# Arxiv Papers in cs.CV on 2022-04-13
### Baseline Computation for Attribution Methods Based on Interpolated Inputs
- **Arxiv ID**: http://arxiv.org/abs/2204.06120v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T07, K.3.2
- **Links**: [PDF](http://arxiv.org/pdf/2204.06120v1)
- **Published**: 2022-04-13 00:11:45+00:00
- **Updated**: 2022-04-13 00:11:45+00:00
- **Authors**: Miguel Lerma, Mirtha Lucas
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: We discuss a way to find a well behaved baseline for attribution methods that work by feeding a neural network with a sequence of interpolated inputs between two given inputs. Then, we test it with our novel Riemann-Stieltjes Integrated Gradient-weighted Class Activation Mapping (RSI-Grad-CAM) attribution method.



### Hierarchical Text-Conditional Image Generation with CLIP Latents
- **Arxiv ID**: http://arxiv.org/abs/2204.06125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06125v1)
- **Published**: 2022-04-13 01:10:33+00:00
- **Updated**: 2022-04-13 01:10:33+00:00
- **Authors**: Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.



### Neural Texture Extraction and Distribution for Controllable Person Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2204.06160v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.06160v1)
- **Published**: 2022-04-13 03:51:07+00:00
- **Updated**: 2022-04-13 03:51:07+00:00
- **Authors**: Yurui Ren, Xiaoqing Fan, Ge Li, Shan Liu, Thomas H. Li
- **Comment**: None
- **Journal**: None
- **Summary**: We deal with the controllable person image synthesis task which aims to re-render a human from a reference image with explicit control over body pose and appearance. Observing that person images are highly structured, we propose to generate desired images by extracting and distributing semantic entities of reference images. To achieve this goal, a neural texture extraction and distribution operation based on double attention is described. This operation first extracts semantic neural textures from reference feature maps. Then, it distributes the extracted neural textures according to the spatial distributions learned from target poses. Our model is trained to predict human images in arbitrary poses, which encourages it to extract disentangled and expressive neural textures representing the appearance of different semantic entities. The disentangled representation further enables explicit appearance control. Neural textures of different reference images can be fused to control the appearance of the interested areas. Experimental comparisons show the superiority of the proposed model. Code is available at https://github.com/RenYurui/Neural-Texture-Extraction-Distribution.



### Estimating Structural Disparities for Face Models
- **Arxiv ID**: http://arxiv.org/abs/2204.06562v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.06562v1)
- **Published**: 2022-04-13 05:30:53+00:00
- **Updated**: 2022-04-13 05:30:53+00:00
- **Authors**: Shervin Ardeshir, Cristina Segalin, Nathan Kallus
- **Comment**: None
- **Journal**: Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2022
- **Summary**: In machine learning, disparity metrics are often defined by measuring the difference in the performance or outcome of a model, across different sub-populations (groups) of datapoints. Thus, the inputs to disparity quantification consist of a model's predictions $\hat{y}$, the ground-truth labels for the predictions $y$, and group labels $g$ for the data points. Performance of the model for each group is calculated by comparing $\hat{y}$ and $y$ for the datapoints within a specific group, and as a result, disparity of performance across the different groups can be calculated. In many real world scenarios however, group labels ($g$) may not be available at scale during training and validation time, or collecting them might not be feasible or desirable as they could often be sensitive information. As a result, evaluating disparity metrics across categorical groups would not be feasible. On the other hand, in many scenarios noisy groupings may be obtainable using some form of a proxy, which would allow measuring disparity metrics across sub-populations. Here we explore performing such analysis on computer vision models trained on human faces, and on tasks such as face attribute prediction and affect estimation. Our experiments indicate that embeddings resulting from an off-the-shelf face recognition model, could meaningfully serve as a proxy for such estimation.



### Migrating Face Swap to Mobile Devices: A lightweight Framework and A Supervised Training Solution
- **Arxiv ID**: http://arxiv.org/abs/2204.08339v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08339v1)
- **Published**: 2022-04-13 05:35:11+00:00
- **Updated**: 2022-04-13 05:35:11+00:00
- **Authors**: Haiming Yu, Hao Zhu, Xiangju Lu, Junhui Liu
- **Comment**: Accepted to IEEE International Conference on Multimedia and Expo 2022
- **Journal**: None
- **Summary**: Existing face swap methods rely heavily on large-scale networks for adequate capacity to generate visually plausible results, which inhibits its applications on resource-constraint platforms. In this work, we propose MobileFSGAN, a novel lightweight GAN for face swap that can run on mobile devices with much fewer parameters while achieving competitive performance. A lightweight encoder-decoder structure is designed especially for image synthesis tasks, which is only 10.2MB and can run on mobile devices at a real-time speed. To tackle the unstability of training such a small network, we construct the FSTriplets dataset utilizing facial attribute editing techniques. FSTriplets provides source-target-result training triplets, yielding pixel-level labels thus for the first time making the training process supervised. We also designed multi-scale gradient losses for efficient back-propagation, resulting in faster and better convergence. Experimental results show that our model reaches comparable performance towards state-of-the-art methods, while significantly reducing the number of network parameters. Codes and the dataset have been released.



### Character-focused Video Thumbnail Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2204.06563v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.06563v1)
- **Published**: 2022-04-13 05:40:42+00:00
- **Updated**: 2022-04-13 05:40:42+00:00
- **Authors**: Shervin Ardeshir, Nagendra Kamath, Hossein Taghavi
- **Comment**: International Conference on Machine Learning. Machine Learning for
  Media Discovery (ML4MD) Workshop 2020
- **Journal**: None
- **Summary**: We explore retrieving character-focused video frames as candidates for being video thumbnails. To evaluate each frame of the video based on the character(s) present in it, characters (faces) are evaluated in two aspects: Facial-expression: We train a CNN model to measure whether a face has an acceptable facial expression for being in a video thumbnail. This model is trained to distinguish faces extracted from artworks/thumbnails, from faces extracted from random frames of videos. Prominence and interactions: Character(s) in the thumbnail should be important character(s) in the video, to prevent the algorithm from suggesting non-representative frames as candidates. We use face clustering to identify the characters in the video, and form a graph in which the prominence (frequency of appearance) of the character(s), and their interactions (co-occurrence) are captured. We use this graph to infer the relevance of the characters present in each candidate frame. Once every face is scored based on the two criteria above, we infer frame level scores by combining the scores for all the faces within a frame.



### Dynamic Neural Textures: Generating Talking-Face Videos with Continuously Controllable Expressions
- **Arxiv ID**: http://arxiv.org/abs/2204.06180v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.06180v1)
- **Published**: 2022-04-13 05:56:12+00:00
- **Updated**: 2022-04-13 05:56:12+00:00
- **Authors**: Zipeng Ye, Zhiyao Sun, Yu-Hui Wen, Yanan Sun, Tian Lv, Ran Yi, Yong-Jin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, talking-face video generation has received considerable attention. So far most methods generate results with neutral expressions or expressions that are implicitly determined by neural networks in an uncontrollable way. In this paper, we propose a method to generate talking-face videos with continuously controllable expressions in real-time. Our method is based on an important observation: In contrast to facial geometry of moderate resolution, most expression information lies in textures. Then we make use of neural textures to generate high-quality talking face videos and design a novel neural network that can generate neural textures for image frames (which we called dynamic neural textures) based on the input expression and continuous intensity expression coding (CIEC). Our method uses 3DMM as a 3D model to sample the dynamic neural texture. The 3DMM does not cover the teeth area, so we propose a teeth submodule to complete the details in teeth. Results and an ablation study show the effectiveness of our method in generating high-quality talking-face videos with continuously controllable expressions. We also set up four baseline methods by combining existing representative methods and compare them with our method. Experimental results including a user study show that our method has the best performance.



### ViViD++: Vision for Visibility Dataset
- **Arxiv ID**: http://arxiv.org/abs/2204.06183v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.06183v2)
- **Published**: 2022-04-13 06:01:27+00:00
- **Updated**: 2022-04-14 00:38:12+00:00
- **Authors**: Alex Junho Lee, Younggun Cho, Young-sik Shin, Ayoung Kim, Hyun Myung
- **Comment**: 8 pages, 8 figures, Accepted to IEEE Robotics and Automation Letters
  (RA-L)
- **Journal**: None
- **Summary**: In this paper, we present a dataset capturing diverse visual data formats that target varying luminance conditions. While RGB cameras provide nourishing and intuitive information, changes in lighting conditions potentially result in catastrophic failure for robotic applications based on vision sensors. Approaches overcoming illumination problems have included developing more robust algorithms or other types of visual sensors, such as thermal and event cameras. Despite the alternative sensors' potential, there still are few datasets with alternative vision sensors. Thus, we provided a dataset recorded from alternative vision sensors, by handheld or mounted on a car, repeatedly in the same space but in different conditions. We aim to acquire visible information from co-aligned alternative vision sensors. Our sensor system collects data more independently from visible light intensity by measuring the amount of infrared dissipation, depth by structured reflection, and instantaneous temporal changes in luminance. We provide these measurements along with inertial sensors and ground-truth for developing robust visual SLAM under poor illumination. The full dataset is available at: https://visibilitydataset.github.io/



### COAP: Compositional Articulated Occupancy of People
- **Arxiv ID**: http://arxiv.org/abs/2204.06184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06184v1)
- **Published**: 2022-04-13 06:02:20+00:00
- **Updated**: 2022-04-13 06:02:20+00:00
- **Authors**: Marko Mihajlovic, Shunsuke Saito, Aayush Bansal, Michael Zollhoefer, Siyu Tang
- **Comment**: To appear at CVPR 2022. The project page is available at
  https://neuralbodies.github.io/COAP/index.html
- **Journal**: None
- **Summary**: We present a novel neural implicit representation for articulated human bodies. Compared to explicit template meshes, neural implicit body representations provide an efficient mechanism for modeling interactions with the environment, which is essential for human motion reconstruction and synthesis in 3D scenes. However, existing neural implicit bodies suffer from either poor generalization on highly articulated poses or slow inference time. In this work, we observe that prior knowledge about the human body's shape and kinematic structure can be leveraged to improve generalization and efficiency. We decompose the full-body geometry into local body parts and employ a part-aware encoder-decoder architecture to learn neural articulated occupancy that models complex deformations locally. Our local shape encoder represents the body deformation of not only the corresponding body part but also the neighboring body parts. The decoder incorporates the geometric constraints of local body shape which significantly improves pose generalization. We demonstrate that our model is suitable for resolving self-intersections and collisions with 3D environments. Quantitative and qualitative experiments show that our method largely outperforms existing solutions in terms of both efficiency and accuracy. The code and models are available at https://neuralbodies.github.io/COAP/index.html



### Calibrating Class Weights with Multi-Modal Information for Partial Video Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2204.06187v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06187v3)
- **Published**: 2022-04-13 06:12:17+00:00
- **Updated**: 2022-07-11 06:51:00+00:00
- **Authors**: Xiyu Wang, Yuecong Xu, Kezhi Mao, Jianfei Yang
- **Comment**: Accepted by ACM Multimedia (ACMMM) 2022, update to camera-ready
  version. 8 pages of text, 5 figures, 2 tables
- **Journal**: None
- **Summary**: Assuming the source label space subsumes the target one, Partial Video Domain Adaptation (PVDA) is a more general and practical scenario for cross-domain video classification problems. The key challenge of PVDA is to mitigate the negative transfer caused by the source-only outlier classes. To tackle this challenge, a crucial step is to aggregate target predictions to assign class weights by up-weighing target classes and down-weighing outlier classes. However, the incorrect predictions of class weights can mislead the network and lead to negative transfer. Previous works improve the class weight accuracy by utilizing temporal features and attention mechanisms, but these methods may fall short when trying to generate accurate class weight when domain shifts are significant, as in most real-world scenarios. To deal with these challenges, we propose the Multi-modality Cluster-calibrated partial Adversarial Network (MCAN). MCAN enhances video feature extraction with multi-modal features from multiple temporal scales to form more robust overall features. It utilizes a novel class weight calibration method to alleviate the negative transfer caused by incorrect class weights. The calibration method tries to identify and weigh correct and incorrect predictions using distributional information implied by unsupervised clustering. Extensive experiments are conducted on prevailing PVDA benchmarks, and the proposed MCAN achieves significant improvements when compared to state-of-the-art PVDA methods.



### Deep Learning Model with GA based Feature Selection and Context Integration
- **Arxiv ID**: http://arxiv.org/abs/2204.06189v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.06189v1)
- **Published**: 2022-04-13 06:28:41+00:00
- **Updated**: 2022-04-13 06:28:41+00:00
- **Authors**: Ranju Mandal, Basim Azam, Brijesh Verma, Mengjie Zhang
- **Comment**: None
- **Journal**: in 2021 IEEE Congress on Evolutionary Computation (CEC) 2021, pp.
  288-295
- **Summary**: Deep learning models have been very successful in computer vision and image processing applications. Since its inception, Many top-performing methods for image segmentation are based on deep CNN models. However, deep CNN models fail to integrate global and local context alongside visual features despite having complex multi-layer architectures. We propose a novel three-layered deep learning model that assiminlate or learns independently global and local contextual information alongside visual features. The novelty of the proposed model is that One-vs-All binary class-based learners are introduced to learn Genetic Algorithm (GA) optimized features in the visual layer, followed by the contextual layer that learns global and local contexts of an image, and finally the third layer integrates all the information optimally to obtain the final class label. Stanford Background and CamVid benchmark image parsing datasets were used for our model evaluation, and our model shows promising results. The empirical analysis reveals that optimized visual features with global and local contextual information play a significant role to improve accuracy and produce stable predictions comparable to state-of-the-art deep CNN models.



### Semantic-Aware Pretraining for Dense Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2204.07449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07449v1)
- **Published**: 2022-04-13 06:57:23+00:00
- **Updated**: 2022-04-13 06:57:23+00:00
- **Authors**: Teng Wang, Zhu Liu, Feng Zheng, Zhichao Lu, Ran Cheng, Ping Luo
- **Comment**: The 2nd place solution to ActivityNet Event Dense-Captioning
  Challenge 2021
- **Journal**: None
- **Summary**: This report describes the details of our approach for the event dense-captioning task in ActivityNet Challenge 2021. We present a semantic-aware pretraining method for dense video captioning, which empowers the learned features to recognize high-level semantic concepts. Diverse video features of different modalities are fed into an event captioning module to generate accurate and meaningful sentences. Our final ensemble model achieves a 10.00 METEOR score on the test set.



### 5G Features and Standards for Vehicle Data Exploitation
- **Arxiv ID**: http://arxiv.org/abs/2204.06211v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/2204.06211v1)
- **Published**: 2022-04-13 07:33:50+00:00
- **Updated**: 2022-04-13 07:33:50+00:00
- **Authors**: Gorka Velez, Edoardo Bonetto, Daniele Brevi, Angel Martin, Gianluca Rizzi, Oscar Castañeda, Arslane Hamza Cherif, Marcos Nieto, Oihana Otaegui
- **Comment**: 12 pages. To be published in ITS European Congress 2022
- **Journal**: None
- **Summary**: Cars capture and generate huge volumes of data in real-time about the driving dynamics, the environment, and the driver and passengers' activities. Due to the proliferation of cooperative, connected and automated mobility (CCAM), the value of data from vehicles is getting strategic, not just for the automotive industry, but also for many diverse stakeholders including small and medium-sized enterprises (SMEs) and start-ups. 5G can enable car-captured data to feed innovative applications and services deployed in the cloud ensuring lower latency and higher throughput than previous cellular technologies. This paper identifies and discusses the relevance of the main 5G features that can contribute to a scalable, flexible, reliable and secure data pipeline, pointing to the standards and technical reports that specify their implementation.



### Defensive Patches for Robust Recognition in the Physical World
- **Arxiv ID**: http://arxiv.org/abs/2204.06213v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06213v1)
- **Published**: 2022-04-13 07:34:51+00:00
- **Updated**: 2022-04-13 07:34:51+00:00
- **Authors**: Jiakai Wang, Zixin Yin, Pengfei Hu, Aishan Liu, Renshuai Tao, Haotong Qin, Xianglong Liu, Dacheng Tao
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: To operate in real-world high-stakes environments, deep learning systems have to endure noises that have been continuously thwarting their robustness. Data-end defense, which improves robustness by operations on input data instead of modifying models, has attracted intensive attention due to its feasibility in practice. However, previous data-end defenses show low generalization against diverse noises and weak transferability across multiple models. Motivated by the fact that robust recognition depends on both local and global features, we propose a defensive patch generation framework to address these problems by helping models better exploit these features. For the generalization against diverse noises, we inject class-specific identifiable patterns into a confined local patch prior, so that defensive patches could preserve more recognizable features towards specific classes, leading models for better recognition under noises. For the transferability across multiple models, we guide the defensive patches to capture more global feature correlations within a class, so that they could activate model-shared global perceptions and transfer better among models. Our defensive patches show great potentials to improve application robustness in practice by simply sticking them around target objects. Extensive experiments show that we outperform others by large margins (improve 20+\% accuracy for both adversarial and corruption robustness on average in the digital and physical world). Our codes are available at https://github.com/nlsde-safety-team/DefensivePatch



### Context-based Deep Learning Architecture with Optimal Integration Layer for Image Parsing
- **Arxiv ID**: http://arxiv.org/abs/2204.06214v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2204.06214v1)
- **Published**: 2022-04-13 07:35:39+00:00
- **Updated**: 2022-04-13 07:35:39+00:00
- **Authors**: Ranju Mandal, Basim Azam, Brijesh Verma
- **Comment**: ICONIP2021
- **Journal**: None
- **Summary**: Deep learning models have been efficient lately on image parsing tasks. However, deep learning models are not fully capable of exploiting visual and contextual information simultaneously. The proposed three-layer context-based deep architecture is capable of integrating context explicitly with visual information. The novel idea here is to have a visual layer to learn visual characteristics from binary class-based learners, a contextual layer to learn context, and then an integration layer to learn from both via genetic algorithm-based optimal fusion to produce a final decision. The experimental outcomes when evaluated on benchmark datasets are promising. Further analysis shows that optimized network weights can improve performance and make stable predictions.



### Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization
- **Arxiv ID**: http://arxiv.org/abs/2204.06228v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06228v2)
- **Published**: 2022-04-13 08:02:11+00:00
- **Updated**: 2023-05-04 00:41:33+00:00
- **Authors**: Zhixi Cai, Kalin Stefanov, Abhinav Dhall, Munawar Hayat
- **Comment**: DICTA 2022
- **Journal**: None
- **Summary**: Due to its high societal impact, deepfake detection is getting active attention in the computer vision community. Most deepfake detection methods rely on identity, facial attributes, and adversarial perturbation-based spatio-temporal modifications at the whole video or random locations while keeping the meaning of the content intact. However, a sophisticated deepfake may contain only a small segment of video/audio manipulation, through which the meaning of the content can be, for example, completely inverted from a sentiment perspective. We introduce a content-driven audio-visual deepfake dataset, termed Localized Audio Visual DeepFake (LAV-DF), explicitly designed for the task of learning temporal forgery localization. Specifically, the content-driven audio-visual manipulations are performed strategically to change the sentiment polarity of the whole video. Our baseline method for benchmarking the proposed dataset is a 3DCNN model, termed as Boundary Aware Temporal Forgery Detection (BA-TFD), which is guided via contrastive, boundary matching, and frame classification loss functions. Our extensive quantitative and qualitative analysis demonstrates the proposed method's strong performance for temporal forgery localization and deepfake detection tasks.



### Rapid model transfer for medical image segmentation via iterative human-in-the-loop update: from labelled public to unlabelled clinical datasets for multi-organ segmentation in CT
- **Arxiv ID**: http://arxiv.org/abs/2204.06243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06243v1)
- **Published**: 2022-04-13 08:22:42+00:00
- **Updated**: 2022-04-13 08:22:42+00:00
- **Authors**: Wenao Ma, Shuang Zheng, Lei Zhang, Huimao Zhang, Qi Dou
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the remarkable success on medical image analysis with deep learning, it is still under exploration regarding how to rapidly transfer AI models from one dataset to another for clinical applications. This paper presents a novel and generic human-in-the-loop scheme for efficiently transferring a segmentation model from a small-scale labelled dataset to a larger-scale unlabelled dataset for multi-organ segmentation in CT. To achieve this, we propose to use an igniter network which can learn from a small-scale labelled dataset and generate coarse annotations to start the process of human-machine interaction. Then, we use a sustainer network for our larger-scale dataset, and iteratively updated it on the new annotated data. Moreover, we propose a flexible labelling strategy for the annotator to reduce the initial annotation workload. The model performance and the time cost of annotation in each subject evaluated on our private dataset are reported and analysed. The results show that our scheme can not only improve the performance by 19.7% on Dice, but also expedite the cost time of manual labelling from 13.87 min to 1.51 min per CT volume during the model transfer, demonstrating the clinical usefulness with promising potentials.



### What Matters in Language Conditioned Robotic Imitation Learning over Unstructured Data
- **Arxiv ID**: http://arxiv.org/abs/2204.06252v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.06252v2)
- **Published**: 2022-04-13 08:45:32+00:00
- **Updated**: 2022-08-30 12:10:35+00:00
- **Authors**: Oier Mees, Lukas Hermann, Wolfram Burgard
- **Comment**: Accepted for publication at IEEE Robotics and Automation Letters
  (RAL). Codebase and trained models available at
  http://hulc.cs.uni-freiburg.de
- **Journal**: None
- **Summary**: A long-standing goal in robotics is to build robots that can perform a wide range of daily tasks from perceptions obtained with their onboard sensors and specified only via natural language. While recently substantial advances have been achieved in language-driven robotics by leveraging end-to-end learning from pixels, there is no clear and well-understood process for making various design choices due to the underlying variation in setups. In this paper, we conduct an extensive study of the most critical challenges in learning language conditioned policies from offline free-form imitation datasets. We further identify architectural and algorithmic techniques that improve performance, such as a hierarchical decomposition of the robot control learning, a multimodal transformer encoder, discrete latent plans and a self-supervised contrastive loss that aligns video and language representations. By combining the results of our investigation with our improved model components, we are able to present a novel approach that significantly outperforms the state of the art on the challenging language conditioned long-horizon robot manipulation CALVIN benchmark. We have open-sourced our implementation to facilitate future research in learning to perform many complex manipulation skills in a row specified with natural language. Codebase and trained models available at http://hulc.cs.uni-freiburg.de



### 3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection
- **Arxiv ID**: http://arxiv.org/abs/2204.06272v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06272v1)
- **Published**: 2022-04-13 09:46:27+00:00
- **Updated**: 2022-04-13 09:46:27+00:00
- **Authors**: Junyu Luo, Jiahui Fu, Xianghao Kong, Chen Gao, Haibing Ren, Hao Shen, Huaxia Xia, Si Liu
- **Comment**: CVPR 2022, Oral
- **Journal**: None
- **Summary**: 3D visual grounding aims to locate the referred target object in 3D point cloud scenes according to a free-form language description. Previous methods mostly follow a two-stage paradigm, i.e., language-irrelevant detection and cross-modal matching, which is limited by the isolated architecture. In such a paradigm, the detector needs to sample keypoints from raw point clouds due to the inherent properties of 3D point clouds (irregular and large-scale), to generate the corresponding object proposal for each keypoint. However, sparse proposals may leave out the target in detection, while dense proposals may confuse the matching model. Moreover, the language-irrelevant detection stage can only sample a small proportion of keypoints on the target, deteriorating the target prediction. In this paper, we propose a 3D Single-Stage Referred Point Progressive Selection (3D-SPS) method, which progressively selects keypoints with the guidance of language and directly locates the target. Specifically, we propose a Description-aware Keypoint Sampling (DKS) module to coarsely focus on the points of language-relevant objects, which are significant clues for grounding. Besides, we devise a Target-oriented Progressive Mining (TPM) module to finely concentrate on the points of the target, which is enabled by progressive intra-modal relation modeling and inter-modal target mining. 3D-SPS bridges the gap between detection and matching in the 3D visual grounding task, localizing the target at a single stage. Experiments demonstrate that 3D-SPS achieves state-of-the-art performance on both ScanRefer and Nr3D/Sr3D datasets.



### Assessing cloudiness in nonwovens
- **Arxiv ID**: http://arxiv.org/abs/2204.06275v2
- **DOI**: None
- **Categories**: **cs.CV**, 62M40, 62P30, I.4.7; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2204.06275v2)
- **Published**: 2022-04-13 09:52:02+00:00
- **Updated**: 2023-04-03 10:40:32+00:00
- **Authors**: Michael Godehardt, Ali Moghiseh, Christine Oetjen, Joachim Ohser, Simon Ringger, Katja Schladitz, Ingo Windschiegel
- **Comment**: FILTECH 2022 - Filter Media Quality Control / Pore Size Analysis
- **Journal**: FILTECH 2022 - 8-10 March - Cologne - Germany - Conference
  Proceedings - ISBN: 978-3-941655-19-5
- **Summary**: The homogeneity of filter media is important for material selection and quality control, along with the specific weight (nominal grammage) and the distribution of the local weight. Cloudiness or formation is a concept used to describe deviations from homogeneity in filter media. We suggest to derive the cloudiness index from the power spectrum of the relative local areal weight, integrated over a selected frequency range. The power spectrum captures the energy density in a broad spectral range. Moreover, under certain conditions, the structure of a nonwoven is fully characterized by the areal weight, the variance of the local areal weight, and the power spectrum. Consequently, the power spectrum is the parameter that exclusively reflects the cloudiness. Here, we address questions arising from practical application. The most prominent is the choice of the spectral band. It certainly depends on the characteristic "size of the clouds", but is limited by the size and lateral resolution of the images. We show that the cloudiness index based on the power spectrum of the relative local areal weight is theoretically well founded and can be robustly measured from image data. Choosing the spectral band allows to capture the cloudiness either visually perceived or found to be decisive for product properties. It is thus well suited to build a technical standard on it.



### A high-resolution canopy height model of the Earth
- **Arxiv ID**: http://arxiv.org/abs/2204.08322v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08322v1)
- **Published**: 2022-04-13 10:34:32+00:00
- **Updated**: 2022-04-13 10:34:32+00:00
- **Authors**: Nico Lang, Walter Jetz, Konrad Schindler, Jan Dirk Wegner
- **Comment**: None
- **Journal**: None
- **Summary**: The worldwide variation in vegetation height is fundamental to the global carbon cycle and central to the functioning of ecosystems and their biodiversity. Geospatially explicit and, ideally, highly resolved information is required to manage terrestrial ecosystems, mitigate climate change, and prevent biodiversity loss. Here, we present the first global, wall-to-wall canopy height map at 10 m ground sampling distance for the year 2020. No single data source meets these requirements: dedicated space missions like GEDI deliver sparse height data, with unprecedented coverage, whereas optical satellite images like Sentinel-2 offer dense observations globally, but cannot directly measure vertical structures. By fusing GEDI with Sentinel-2, we have developed a probabilistic deep learning model to retrieve canopy height from Sentinel-2 images anywhere on Earth, and to quantify the uncertainty in these estimates. The presented approach reduces the saturation effect commonly encountered when estimating canopy height from satellite images, allowing to resolve tall canopies with likely high carbon stocks. According to our map, only 5% of the global landmass is covered by trees taller than 30 m. Such data play an important role for conservation, e.g., we find that only 34% of these tall canopies are located within protected areas. Our model enables consistent, uncertainty-informed worldwide mapping and supports an ongoing monitoring to detect change and inform decision making. The approach can serve ongoing efforts in forest conservation, and has the potential to foster advances in climate, carbon, and biodiversity modelling.



### Reuse your features: unifying retrieval and feature-metric alignment
- **Arxiv ID**: http://arxiv.org/abs/2204.06292v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06292v2)
- **Published**: 2022-04-13 10:42:00+00:00
- **Updated**: 2023-05-08 12:10:26+00:00
- **Authors**: Javier Morlana, J. M. M. Montiel
- **Comment**: ICRA 2023
- **Journal**: None
- **Summary**: We propose a compact pipeline to unify all the steps of Visual Localization: image retrieval, candidate re-ranking and initial pose estimation, and camera pose refinement. Our key assumption is that the deep features used for these individual tasks share common characteristics, so we should reuse them in all the procedures of the pipeline. Our DRAN (Deep Retrieval and image Alignment Network) is able to extract global descriptors for efficient image retrieval, use intermediate hierarchical features to re-rank the retrieval list and produce an initial pose guess, which is finally refined by means of a feature-metric optimization based on learned deep multi-scale dense features. DRAN is the first single network able to produce the features for the three steps of visual localization. DRAN achieves competitive performance in terms of robustness and accuracy under challenging conditions in public benchmarks, outperforming other unified approaches and consuming lower computational and memory cost than its counterparts using multiple networks. Code and models will be publicly available at https://github.com/jmorlana/DRAN.



### Active Diffusion and VCA-Assisted Image Segmentation of Hyperspectral Images
- **Arxiv ID**: http://arxiv.org/abs/2204.06298v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2204.06298v1)
- **Published**: 2022-04-13 11:00:52+00:00
- **Updated**: 2022-04-13 11:00:52+00:00
- **Authors**: Sam L. Polk, Kangning Cui, Robert J. Plemmons, James M. Murphy
- **Comment**: (6 pages, 2 figures). Accepted to Proceedings of IEEE IGARSS 2022
- **Journal**: None
- **Summary**: Hyperspectral images encode rich structure that can be exploited for material discrimination by machine learning algorithms. This article introduces the Active Diffusion and VCA-Assisted Image Segmentation (ADVIS) for active material discrimination. ADVIS selects high-purity, high-density pixels that are far in diffusion distance (a data-dependent metric) from other high-purity, high-density pixels in the hyperspectral image. The ground truth labels of these pixels are queried and propagated to the rest of the image. The ADVIS active learning algorithm is shown to strongly outperform its fully unsupervised clustering algorithm counterpart, suggesting that the incorporation of a very small number of carefully-selected ground truth labels can result in substantially superior material discrimination in hyperspectral images.



### TIB-VA at SemEval-2022 Task 5: A Multimodal Architecture for the Detection and Classification of Misogynous Memes
- **Arxiv ID**: http://arxiv.org/abs/2204.06299v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.06299v1)
- **Published**: 2022-04-13 11:03:21+00:00
- **Updated**: 2022-04-13 11:03:21+00:00
- **Authors**: Sherzod Hakimov, Gullal S. Cheema, Ralph Ewerth
- **Comment**: Accepted for publication at SemEval-2022 Workshop, Task 5: MAMI -
  Multimedia Automatic Misogyny Identification co-located with NAACL 2022
- **Journal**: None
- **Summary**: The detection of offensive, hateful content on social media is a challenging problem that affects many online users on a daily basis. Hateful content is often used to target a group of people based on ethnicity, gender, religion and other factors. The hate or contempt toward women has been increasing on social platforms. Misogynous content detection is especially challenging when textual and visual modalities are combined to form a single context, e.g., an overlay text embedded on top of an image, also known as meme. In this paper, we present a multimodal architecture that combines textual and visual features in order to detect misogynous meme content. The proposed architecture is evaluated in the SemEval-2022 Task 5: MAMI - Multimedia Automatic Misogyny Identification challenge under the team name TIB-VA. Our solution obtained the best result in the Task-B where the challenge is to classify whether a given document is misogynous and further identify the main sub-classes of shaming, stereotype, objectification, and violence.



### Multi-View Consistent Generative Adversarial Networks for 3D-aware Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2204.06307v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06307v1)
- **Published**: 2022-04-13 11:23:09+00:00
- **Updated**: 2022-04-13 11:23:09+00:00
- **Authors**: Xuanmeng Zhang, Zhedong Zheng, Daiheng Gao, Bang Zhang, Pan Pan, Yi Yang
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: 3D-aware image synthesis aims to generate images of objects from multiple views by learning a 3D representation. However, one key challenge remains: existing approaches lack geometry constraints, hence usually fail to generate multi-view consistent images. To address this challenge, we propose Multi-View Consistent Generative Adversarial Networks (MVCGAN) for high-quality 3D-aware image synthesis with geometry constraints. By leveraging the underlying 3D geometry information of generated images, i.e., depth and camera transformation matrix, we explicitly establish stereo correspondence between views to perform multi-view joint optimization. In particular, we enforce the photometric consistency between pairs of views and integrate a stereo mixup mechanism into the training process, encouraging the model to reason about the correct 3D shape. Besides, we design a two-stage training strategy with feature-level multi-view joint optimization to improve the image quality. Extensive experiments on three datasets demonstrate that MVCGAN achieves the state-of-the-art performance for 3D-aware image synthesis.



### Deep Learning-based Framework for Automatic Cranial Defect Reconstruction and Implant Modeling
- **Arxiv ID**: http://arxiv.org/abs/2204.06310v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.06310v1)
- **Published**: 2022-04-13 11:33:26+00:00
- **Updated**: 2022-04-13 11:33:26+00:00
- **Authors**: Marek Wodzinski, Mateusz Daniol, Miroslaw Socha, Daria Hemmerling, Maciej Stanuch, Andrzej Skalski
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this work is to propose a robust, fast, and fully automatic method for personalized cranial defect reconstruction and implant modeling.   We propose a two-step deep learning-based method using a modified U-Net architecture to perform the defect reconstruction, and a dedicated iterative procedure to improve the implant geometry, followed by automatic generation of models ready for 3-D printing. We propose a cross-case augmentation based on imperfect image registration combining cases from different datasets. We perform ablation studies regarding different augmentation strategies and compare them to other state-of-the-art methods.   We evaluate the method on three datasets introduced during the AutoImplant 2021 challenge, organized jointly with the MICCAI conference. We perform the quantitative evaluation using the Dice and boundary Dice coefficients, and the Hausdorff distance. The average Dice coefficient, boundary Dice coefficient, and the 95th percentile of Hausdorff distance are 0.91, 0.94, and 1.53 mm respectively. We perform an additional qualitative evaluation by 3-D printing and visualization in mixed reality to confirm the implant's usefulness.   We propose a complete pipeline that enables one to create the cranial implant model ready for 3-D printing. The described method is a greatly extended version of the method that scored 1st place in all AutoImplant 2021 challenge tasks. We freely release the source code, that together with the open datasets, makes the results fully reproducible. The automatic reconstruction of cranial defects may enable manufacturing personalized implants in a significantly shorter time, possibly allowing one to perform the 3-D printing process directly during a given intervention. Moreover, we show the usability of the defect reconstruction in mixed reality that may further reduce the surgery time.



### Recognition of Freely Selected Keypoints on Human Limbs
- **Arxiv ID**: http://arxiv.org/abs/2204.06326v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06326v1)
- **Published**: 2022-04-13 11:58:28+00:00
- **Updated**: 2022-04-13 11:58:28+00:00
- **Authors**: Katja Ludwig, Daniel Kienzle, Rainer Lienhart
- **Comment**: accepted at CVSports (CVPR 2022 Workshops)
- **Journal**: None
- **Summary**: Nearly all Human Pose Estimation (HPE) datasets consist of a fixed set of keypoints. Standard HPE models trained on such datasets can only detect these keypoints. If more points are desired, they have to be manually annotated and the model needs to be retrained. Our approach leverages the Vision Transformer architecture to extend the capability of the model to detect arbitrary keypoints on the limbs of persons. We propose two different approaches to encode the desired keypoints. (1) Each keypoint is defined by its position along the line between the two enclosing keypoints from the fixed set and its relative distance between this line and the edge of the limb. (2) Keypoints are defined as coordinates on a norm pose. Both approaches are based on the TokenPose architecture, while the keypoint tokens that correspond to the fixed keypoints are replaced with our novel module. Experiments show that our approaches achieve similar results to TokenPose on the fixed keypoints and are capable of detecting arbitrary keypoints on the limbs.



### Transparent Shape from a Single View Polarization Image
- **Arxiv ID**: http://arxiv.org/abs/2204.06331v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06331v6)
- **Published**: 2022-04-13 12:24:32+00:00
- **Updated**: 2023-08-12 11:52:29+00:00
- **Authors**: Mingqi Shao, Chongkun Xia, Zhendong Yang, Junnan Huang, Xueqian Wang
- **Comment**: Proceedings of the IEEE International Conference on Computer Vision,
  ICCV, 2023
- **Journal**: None
- **Summary**: This paper presents a learning-based method for transparent surface estimation from a single view polarization image. Existing shape from polarization(SfP) methods have the difficulty in estimating transparent shape since the inherent transmission interference heavily reduces the reliability of physics-based prior. To address this challenge, we propose the concept of physics-based prior, which is inspired by the characteristic that the transmission component in the polarization image has more noise than reflection. The confidence is used to determine the contribution of the interfered physics-based prior. Then, we build a network(TransSfP) with multi-branch architecture to avoid the destruction of relationships between different hierarchical inputs. To train and test our method, we construct a dataset for transparent shape from polarization with paired polarization images and ground-truth normal maps. Extensive experiments and comparisons demonstrate the superior accuracy of our method.



### Mitigating Bias in Facial Analysis Systems by Incorporating Label Diversity
- **Arxiv ID**: http://arxiv.org/abs/2204.06364v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.06364v2)
- **Published**: 2022-04-13 13:17:27+00:00
- **Updated**: 2022-06-14 17:04:42+00:00
- **Authors**: Camila Kolling, Victor Araujo, Adriano Veloso, Soraia Raupp Musse
- **Comment**: None
- **Journal**: None
- **Summary**: Facial analysis models are increasingly applied in real-world applications that have significant impact on peoples' lives. However, as literature has shown, models that automatically classify facial attributes might exhibit algorithmic discrimination behavior with respect to protected groups, potentially posing negative impacts on individuals and society. It is therefore critical to develop techniques that can mitigate unintended biases in facial classifiers. Hence, in this work, we introduce a novel learning method that combines both subjective human-based labels and objective annotations based on mathematical definitions of facial traits. Specifically, we generate new objective annotations from two large-scale human-annotated dataset, each capturing a different perspective of the analyzed facial trait. We then propose an ensemble learning method, which combines individual models trained on different types of annotations. We provide an in-depth analysis of the annotation procedure as well as the datasets distribution. Moreover, we empirically demonstrate that, by incorporating label diversity, our method successfully mitigates unintended biases, while maintaining significant accuracy on the downstream tasks.



### 3D Convolutional Networks for Action Recognition: Application to Sport Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.08460v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08460v1)
- **Published**: 2022-04-13 13:21:07+00:00
- **Updated**: 2022-04-13 13:21:07+00:00
- **Authors**: Pierre-Etienne Martin, J Benois-Pineau, R Péteri, A Zemmari, J Morlier
- **Comment**: Multi-faceted Deep Learning, 2021
- **Journal**: None
- **Summary**: 3D convolutional networks is a good means to perform tasks such as video segmentation into coherent spatio-temporal chunks and classification of them with regard to a target taxonomy. In the chapter we are interested in the classification of continuous video takes with repeatable actions, such as strokes of table tennis. Filmed in a free marker less ecological environment, these videos represent a challenge from both segmentation and classification point of view. The 3D convnets are an efficient tool for solving these problems with window-based approaches.



### Deep learning based automatic detection of offshore oil slicks using SAR data and contextual information
- **Arxiv ID**: http://arxiv.org/abs/2204.06371v1
- **DOI**: 10.1117/12.2598032
- **Categories**: **cs.CV**, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.06371v1)
- **Published**: 2022-04-13 13:30:16+00:00
- **Updated**: 2022-04-13 13:30:16+00:00
- **Authors**: Emna Amri, Hermann Courteille, A Benoit, Philippe Bolon, Dominique Dubucq, Gilles Poulain, Anthony Credoz
- **Comment**: SPIE Conference on Remote Sensing of the Ocean, Sea Ice, Coastal
  Waters, and Large Water Regions, Sep 2021, Online, Spain
- **Journal**: None
- **Summary**: Ocean surface monitoring, especially oil slick detection, has become mandatory due to its importance for oil exploration and risk prevention on ecosystems. For years, the detection task has been performed manually by photo-interpreters using Synthetic Aperture Radar (SAR) images with the help of contextual data such as wind. This tedious manual work cannot handle the increasing amount of data collected by the available sensors and thus requires automation. Literature reports conventional and semi-automated detection methods that generally focus either on oil slicks originating from anthropogenic (spills) or natural (seeps) sources on limited data collections. As an extension, this paper presents the automation of offshore oil slicks on an extensive database with both kinds of slicks. It builds upon the slick annotations of specialized photo-interpreters on Sentinel-1 SAR data for 4 years over 3 exploration and monitoring areas worldwide. All the considered SAR images and related annotation relate to real oil slick monitoring scenarios. Further, wind estimation is systematically computed to enrich the data collection. Paper contributions are the following : (i) a performance comparison of two deep learning approaches: semantic segmentation using FC-DenseNet and instance segmentation using Mask-RCNN. (ii) the introduction of meteorological information (wind speed) is deemed valuable for oil slick detection in the performance evaluation. The main results of this study show the effectiveness of slick detection by deep learning approaches, in particular FC-DenseNet, which captures more than 92% of oil instances in our test set. Furthermore, a strong correlation between model performances and contextual information such as slick size and wind speed is demonstrated in the performance evaluation. This work opens perspectives to design models that can fuse SAR and wind information to reduce the false alarm rate.



### Investigating Temporal Convolutional Neural Networks for Satellite Image Time Series Classification: A survey
- **Arxiv ID**: http://arxiv.org/abs/2204.08461v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.08461v2)
- **Published**: 2022-04-13 14:08:14+00:00
- **Updated**: 2023-04-20 13:58:18+00:00
- **Authors**: James Brock, Zahraa S. Abdallah
- **Comment**: 24 pages (not inc. 6 pages for appendices and references), 8 figures,
  15 tables. Submitted for publishing
- **Journal**: None
- **Summary**: Satellite Image Time Series (SITS) of the Earth's surface provide detailed land cover maps, with their quality in the spatial and temporal dimensions consistently improving. These image time series are integral for developing systems that aim to produce accurate, up-to-date land cover maps of the Earth's surface. Applications are wide-ranging, with notable examples including ecosystem mapping, vegetation process monitoring and anthropogenic land-use change tracking. Recently proposed methods for SITS classification have demonstrated respectable merit, but these methods tend to lack native mechanisms that exploit the temporal dimension of the data; commonly resulting in extensive data pre-processing contributing to prohibitively long training times. To overcome these shortcomings, Temporal CNNs have recently been employed for SITS classification tasks with encouraging results. This paper seeks to survey this method against a plethora of other contemporary methods for SITS classification to validate the existing findings in recent literature. Comprehensive experiments are carried out on two benchmark SITS datasets with the results demonstrating that Temporal CNNs display a superior performance to the comparative benchmark algorithms across both studied datasets, achieving accuracies of 95.0\% and 87.3\% respectively. Investigations into the Temporal CNN architecture also highlighted the non-trivial task of optimising the model for a new dataset.



### Receding Neuron Importances for Structured Pruning
- **Arxiv ID**: http://arxiv.org/abs/2204.06404v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.06404v1)
- **Published**: 2022-04-13 14:08:27+00:00
- **Updated**: 2022-04-13 14:08:27+00:00
- **Authors**: Mihai Suteu, Yike Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Structured pruning efficiently compresses networks by identifying and removing unimportant neurons. While this can be elegantly achieved by applying sparsity-inducing regularisation on BatchNorm parameters, an L1 penalty would shrink all scaling factors rather than just those of superfluous neurons. To tackle this issue, we introduce a simple BatchNorm variation with bounded scaling parameters, based on which we design a novel regularisation term that suppresses only neurons with low importance. Under our method, the weights of unnecessary neurons effectively recede, producing a polarised bimodal distribution of importances. We show that neural networks trained this way can be pruned to a larger extent and with less deterioration. We one-shot prune VGG and ResNet architectures at different ratios on CIFAR and ImagenNet datasets. In the case of VGG-style networks, our method significantly outperforms existing approaches particularly under a severe pruning regime.



### DMCNet: Diversified Model Combination Network for Understanding Engagement from Video Screengrabs
- **Arxiv ID**: http://arxiv.org/abs/2204.06454v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06454v1)
- **Published**: 2022-04-13 15:24:38+00:00
- **Updated**: 2022-04-13 15:24:38+00:00
- **Authors**: Sarthak Batra, Hewei Wang, Avishek Nag, Philippe Brodeur, Marianne Checkley, Annette Klinkert, Soumyabrata Dev
- **Comment**: Published in Systems and Soft Computing, 2022
- **Journal**: None
- **Summary**: Engagement is an essential indicator of the Quality-of-Learning Experience (QoLE) and plays a major role in developing intelligent educational interfaces. The number of people learning through Massively Open Online Courses (MOOCs) and other online resources has been increasing rapidly because they provide us with the flexibility to learn from anywhere at any time. This provides a good learning experience for the students. However, such learning interface requires the ability to recognize the level of engagement of the students for a holistic learning experience. This is useful for both students and educators alike. However, understanding engagement is a challenging task, because of its subjectivity and ability to collect data. In this paper, we propose a variety of models that have been trained on an open-source dataset of video screengrabs. Our non-deep learning models are based on the combination of popular algorithms such as Histogram of Oriented Gradient (HOG), Support Vector Machine (SVM), Scale Invariant Feature Transform (SIFT) and Speeded Up Robust Features (SURF). The deep learning methods include Densely Connected Convolutional Networks (DenseNet-121), Residual Network (ResNet-18) and MobileNetV1. We show the performance of each models using a variety of metrics such as the Gini Index, Adjusted F-Measure (AGF), and Area Under receiver operating characteristic Curve (AUC). We use various dimensionality reduction techniques such as Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) to understand the distribution of data in the feature sub-space. Our work will thereby assist the educators and students in obtaining a fruitful and efficient online learning experience.



### WSSS4LUAD: Grand Challenge on Weakly-supervised Tissue Semantic Segmentation for Lung Adenocarcinoma
- **Arxiv ID**: http://arxiv.org/abs/2204.06455v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.06455v2)
- **Published**: 2022-04-13 15:27:05+00:00
- **Updated**: 2022-04-14 01:45:02+00:00
- **Authors**: Chu Han, Xipeng Pan, Lixu Yan, Huan Lin, Bingbing Li, Su Yao, Shanshan Lv, Zhenwei Shi, Jinhai Mai, Jiatai Lin, Bingchao Zhao, Zeyan Xu, Zhizhen Wang, Yumeng Wang, Yuan Zhang, Huihui Wang, Chao Zhu, Chunhui Lin, Lijian Mao, Min Wu, Luwen Duan, Jingsong Zhu, Dong Hu, Zijie Fang, Yang Chen, Yongbing Zhang, Yi Li, Yiwen Zou, Yiduo Yu, Xiaomeng Li, Haiming Li, Yanfen Cui, Guoqiang Han, Yan Xu, Jun Xu, Huihua Yang, Chunming Li, Zhenbing Liu, Cheng Lu, Xin Chen, Changhong Liang, Qingling Zhang, Zaiyi Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Lung cancer is the leading cause of cancer death worldwide, and adenocarcinoma (LUAD) is the most common subtype. Exploiting the potential value of the histopathology images can promote precision medicine in oncology. Tissue segmentation is the basic upstream task of histopathology image analysis. Existing deep learning models have achieved superior segmentation performance but require sufficient pixel-level annotations, which is time-consuming and expensive. To enrich the label resources of LUAD and to alleviate the annotation efforts, we organize this challenge WSSS4LUAD to call for the outstanding weakly-supervised semantic segmentation (WSSS) techniques for histopathology images of LUAD. Participants have to design the algorithm to segment tumor epithelial, tumor-associated stroma and normal tissue with only patch-level labels. This challenge includes 10,091 patch-level annotations (the training set) and over 130 million labeled pixels (the validation and test sets), from 87 WSIs (67 from GDPH, 20 from TCGA). All the labels were generated by a pathologist-in-the-loop pipeline with the help of AI models and checked by the label review board. Among 532 registrations, 28 teams submitted the results in the test phase with over 1,000 submissions. Finally, the first place team achieved mIoU of 0.8413 (tumor: 0.8389, stroma: 0.7931, normal: 0.8919). According to the technical reports of the top-tier teams, CAM is still the most popular approach in WSSS. Cutmix data augmentation has been widely adopted to generate more reliable samples. With the success of this challenge, we believe that WSSS approaches with patch-level annotations can be a complement to the traditional pixel annotations while reducing the annotation efforts. The entire dataset has been released to encourage more researches on computational pathology in LUAD and more novel WSSS techniques.



### SpoofGAN: Synthetic Fingerprint Spoof Images
- **Arxiv ID**: http://arxiv.org/abs/2204.06498v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06498v2)
- **Published**: 2022-04-13 16:27:27+00:00
- **Updated**: 2022-04-15 20:51:13+00:00
- **Authors**: Steven A. Grosz, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: A major limitation to advances in fingerprint spoof detection is the lack of publicly available, large-scale fingerprint spoof datasets, a problem which has been compounded by increased concerns surrounding privacy and security of biometric data. Furthermore, most state-of-the-art spoof detection algorithms rely on deep networks which perform best in the presence of a large amount of training data. This work aims to demonstrate the utility of synthetic (both live and spoof) fingerprints in supplying these algorithms with sufficient data to improve the performance of fingerprint spoof detection algorithms beyond the capabilities when training on a limited amount of publicly available real datasets. First, we provide details of our approach in modifying a state-of-the-art generative architecture to synthesize high quality live and spoof fingerprints. Then, we provide quantitative and qualitative analysis to verify the quality of our synthetic fingerprints in mimicking the distribution of real data samples. We showcase the utility of our synthetic live and spoof fingerprints in training a deep network for fingerprint spoof detection, which dramatically boosts the performance across three different evaluation datasets compared to an identical model trained on real data alone. Finally, we demonstrate that only 25% of the original (real) dataset is required to obtain similar detection performance when augmenting the training dataset with synthetic data.



### Scaling Cross-Domain Content-Based Image Retrieval for E-commerce Snap and Search Application
- **Arxiv ID**: http://arxiv.org/abs/2204.11593v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.11593v1)
- **Published**: 2022-04-13 16:41:42+00:00
- **Updated**: 2022-04-13 16:41:42+00:00
- **Authors**: Isaac Kwan Yin Chung, Minh Tran, Eran Nussinovitch
- **Comment**: ECIR 2022 Industry Day
- **Journal**: None
- **Summary**: In this industry talk at ECIR 2022, we illustrate how we approach the main challenges from large scale cross-domain content-based image retrieval using a cascade method and a combination of our visual search and classification capabilities. Specifically, we present a system that is able to handle the scale of the data for e-commerce usage and the cross-domain nature of the query and gallery image pools. We showcase the approach applied in real-world e-commerce snap and search use case and its impact on ranking and latency performance.



### DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization
- **Arxiv ID**: http://arxiv.org/abs/2204.06504v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.AI, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.06504v1)
- **Published**: 2022-04-13 16:42:32+00:00
- **Updated**: 2022-04-13 16:42:32+00:00
- **Authors**: Chaoli Wang, Jun Han
- **Comment**: 20 pages, 2 figures, and 12 tables. To Appear in IEEE Transactions on
  Visualization and Computer Graphics
- **Journal**: None
- **Summary**: Since 2016, we have witnessed the tremendous growth of artificial intelligence+visualization (AI+VIS) research. However, existing survey papers on AI+VIS focus on visual analytics and information visualization, not scientific visualization (SciVis). In this paper, we survey related deep learning (DL) works in SciVis, specifically in the direction of DL4SciVis: designing DL solutions for solving SciVis problems. To stay focused, we primarily consider works that handle scalar and vector field data but exclude mesh data. We classify and discuss these works along six dimensions: domain setting, research task, learning type, network architecture, loss function, and evaluation metric. The paper concludes with a discussion of the remaining gaps to fill along the discussed dimensions and the grand challenges we need to tackle as a community. This state-of-the-art survey guides SciVis researchers in gaining an overview of this emerging topic and points out future directions to grow this research.



### Out-of-Distribution Detection with Deep Nearest Neighbors
- **Arxiv ID**: http://arxiv.org/abs/2204.06507v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.06507v3)
- **Published**: 2022-04-13 16:45:21+00:00
- **Updated**: 2022-12-08 00:04:40+00:00
- **Authors**: Yiyou Sun, Yifei Ming, Xiaojin Zhu, Yixuan Li
- **Comment**: 15 pages, 4 figures, accepted in ICML 2022
- **Journal**: None
- **Summary**: Out-of-distribution (OOD) detection is a critical task for deploying machine learning models in the open world. Distance-based methods have demonstrated promise, where testing samples are detected as OOD if they are relatively far away from in-distribution (ID) data. However, prior methods impose a strong distributional assumption of the underlying feature space, which may not always hold. In this paper, we explore the efficacy of non-parametric nearest-neighbor distance for OOD detection, which has been largely overlooked in the literature. Unlike prior works, our method does not impose any distributional assumption, hence providing stronger flexibility and generality. We demonstrate the effectiveness of nearest-neighbor-based OOD detection on several benchmarks and establish superior performance. Under the same model trained on ImageNet-1k, our method substantially reduces the false positive rate (FPR@TPR95) by 24.77% compared to a strong baseline SSD+, which uses a parametric approach Mahalanobis distance in detection. Code is available: https://github.com/deeplearning-wisc/knn-ood.



### Does depth estimation help object detection?
- **Arxiv ID**: http://arxiv.org/abs/2204.06512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06512v1)
- **Published**: 2022-04-13 17:03:25+00:00
- **Updated**: 2022-04-13 17:03:25+00:00
- **Authors**: Bedrettin Cetinkaya, Sinan Kalkan, Emre Akbas
- **Comment**: Accepted to Image and Vision Computing
- **Journal**: None
- **Summary**: Ground-truth depth, when combined with color data, helps improve object detection accuracy over baseline models that only use color. However, estimated depth does not always yield improvements. Many factors affect the performance of object detection when estimated depth is used. In this paper, we comprehensively investigate these factors with detailed experiments, such as using ground-truth vs. estimated depth, effects of different state-of-the-art depth estimation networks, effects of using different indoor and outdoor RGB-D datasets as training data for depth estimation, and different architectural choices for integrating depth to the base object detector network. We propose an early concatenation strategy of depth, which yields higher mAP than previous works' while using significantly fewer parameters.



### A9-Dataset: Multi-Sensor Infrastructure-Based Dataset for Mobility Research
- **Arxiv ID**: http://arxiv.org/abs/2204.06527v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06527v2)
- **Published**: 2022-04-13 17:12:16+00:00
- **Updated**: 2022-05-13 16:27:37+00:00
- **Authors**: Christian Creß, Walter Zimmer, Leah Strand, Venkatnarayanan Lakshminarasimhan, Maximilian Fortkord, Siyi Dai, Alois Knoll
- **Comment**: Accepted for IEEE Intelligent Vehicles Symposium 2022 (IV22)
- **Journal**: None
- **Summary**: Data-intensive machine learning based techniques increasingly play a prominent role in the development of future mobility solutions - from driver assistance and automation functions in vehicles, to real-time traffic management systems realized through dedicated infrastructure. The availability of high quality real-world data is often an important prerequisite for the development and reliable deployment of such systems in large scale. Towards this endeavour, we present the A9-Dataset based on roadside sensor infrastructure from the 3 km long Providentia++ test field near Munich in Germany. The dataset includes anonymized and precision-timestamped multi-modal sensor and object data in high resolution, covering a variety of traffic situations. As part of the first set of data, which we describe in this paper, we provide camera and LiDAR frames from two overhead gantry bridges on the A9 autobahn with the corresponding objects labeled with 3D bounding boxes. The first set includes in total more than 1000 sensor frames and 14000 traffic objects. The dataset is available for download at https://a9-dataset.com.



### Neural Vector Fields for Implicit Surface Representation and Inference
- **Arxiv ID**: http://arxiv.org/abs/2204.06552v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06552v3)
- **Published**: 2022-04-13 17:53:34+00:00
- **Updated**: 2023-04-07 15:15:46+00:00
- **Authors**: Edoardo Mello Rella, Ajad Chhatkuli, Ender Konukoglu, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Implicit fields have recently shown increasing success in representing and learning 3D shapes accurately. Signed distance fields and occupancy fields are decades old and still the preferred representations, both with well-studied properties, despite their restriction to closed surfaces. With neural networks, several other variations and training principles have been proposed with the goal to represent all classes of shapes. In this paper, we develop a novel and yet a fundamental representation considering unit vectors in 3D space and call it Vector Field (VF): at each point in $\mathbb{R}^3$, VF is directed at the closest point on the surface. We theoretically demonstrate that VF can be easily transformed to surface density by computing the flux density. Unlike other standard representations, VF directly encodes an important physical property of the surface, its normal. We further show the advantages of VF representation, in learning open, closed, or multi-layered as well as piecewise planar surfaces. We compare our method on several datasets including ShapeNet where the proposed new neural implicit field shows superior accuracy in representing any type of shape, outperforming other standard methods. Code is available at https://github.com/edomel/ImplicitVF.



### Controllable Video Generation through Global and Local Motion Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2204.06558v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.06558v1)
- **Published**: 2022-04-13 17:57:53+00:00
- **Updated**: 2022-04-13 17:57:53+00:00
- **Authors**: Aram Davtyan, Paolo Favaro
- **Comment**: None
- **Journal**: None
- **Summary**: We present GLASS, a method for Global and Local Action-driven Sequence Synthesis. GLASS is a generative model that is trained on video sequences in an unsupervised manner and that can animate an input image at test time. The method learns to segment frames into foreground-background layers and to generate transitions of the foregrounds over time through a global and local action representation. Global actions are explicitly related to 2D shifts, while local actions are instead related to (both geometric and photometric) local deformations. GLASS uses a recurrent neural network to transition between frames and is trained through a reconstruction loss. We also introduce W-Sprites (Walking Sprites), a novel synthetic dataset with a predefined action space. We evaluate our method on both W-Sprites and real datasets, and find that GLASS is able to generate realistic video sequences from a single input image and to successfully learn a more advanced action space than in prior work.



### OccAM's Laser: Occlusion-based Attribution Maps for 3D Object Detectors on LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/2204.06577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06577v1)
- **Published**: 2022-04-13 18:00:30+00:00
- **Updated**: 2022-04-13 18:00:30+00:00
- **Authors**: David Schinagl, Georg Krispel, Horst Possegger, Peter M. Roth, Horst Bischof
- **Comment**: CVPR 2022, code is available at https://github.com/dschinagl/occam
- **Journal**: None
- **Summary**: While 3D object detection in LiDAR point clouds is well-established in academia and industry, the explainability of these models is a largely unexplored field. In this paper, we propose a method to generate attribution maps for the detected objects in order to better understand the behavior of such models. These maps indicate the importance of each 3D point in predicting the specific objects. Our method works with black-box models: We do not require any prior knowledge of the architecture nor access to the model's internals, like parameters, activations or gradients. Our efficient perturbation-based approach empirically estimates the importance of each point by testing the model with randomly generated subsets of the input point cloud. Our sub-sampling strategy takes into account the special characteristics of LiDAR data, such as the depth-dependent point density. We show a detailed evaluation of the attribution maps and demonstrate that they are interpretable and highly informative. Furthermore, we compare the attribution maps of recent 3D object detection architectures to provide insights into their decision-making processes.



### Illumination-Invariant Active Camera Relocalization for Fine-Grained Change Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2204.06580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06580v1)
- **Published**: 2022-04-13 18:00:55+00:00
- **Updated**: 2022-04-13 18:00:55+00:00
- **Authors**: Nan Li, Wei Feng, Qian Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Active camera relocalization (ACR) is a new problem in computer vision that significantly reduces the false alarm caused by image distortions due to camera pose misalignment in fine-grained change detection (FGCD). Despite the fruitful achievements that ACR can support, it still remains a challenging problem caused by the unstable results of relative pose estimation, especially for outdoor scenes, where the lighting condition is out of control, i.e., the twice observations may have highly varied illuminations. This paper studies an illumination-invariant active camera relocalization method, it improves both in relative pose estimation and scale estimation. We use plane segments as an intermediate representation to facilitate feature matching, thus further boosting pose estimation robustness and reliability under lighting variances. Moreover, we construct a linear system to obtain the absolute scale in each ACR iteration by minimizing the image warping error, thus, significantly reduce the time consume of ACR process, it is nearly $1.6$ times faster than the state-of-the-art ACR strategy. Our work greatly expands the feasibility of real-world fine-grained change monitoring tasks for cultural heritages. Extensive experiments tests and real-world applications verify the effectiveness and robustness of the proposed pose estimation method using for ACR tasks.



### Deep Relation Learning for Regression and Its Application to Brain Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/2204.06598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06598v1)
- **Published**: 2022-04-13 18:40:34+00:00
- **Updated**: 2022-04-13 18:40:34+00:00
- **Authors**: Sheng He, Yanfang Feng, P. Ellen Grant, Yangming Ou
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging. 2022
- **Summary**: Most deep learning models for temporal regression directly output the estimation based on single input images, ignoring the relationships between different images. In this paper, we propose deep relation learning for regression, aiming to learn different relations between a pair of input images. Four non-linear relations are considered: "cumulative relation", "relative relation", "maximal relation" and "minimal relation". These four relations are learned simultaneously from one deep neural network which has two parts: feature extraction and relation regression. We use an efficient convolutional neural network to extract deep features from the pair of input images and apply a Transformer for relation learning. The proposed method is evaluated on a merged dataset with 6,049 subjects with ages of 0-97 years using 5-fold cross-validation for the task of brain age estimation. The experimental results have shown that the proposed method achieved a mean absolute error (MAE) of 2.38 years, which is lower than the MAEs of 8 other state-of-the-art algorithms with statistical significance (p$<$0.05) in paired T-test (two-side).



### CapillaryX: A Software Design Pattern for Analyzing Medical Images in Real-time using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.08462v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.08462v1)
- **Published**: 2022-04-13 18:47:04+00:00
- **Updated**: 2022-04-13 18:47:04+00:00
- **Authors**: Maged Abdalla Helmy Abdou, Paulo Ferreira, Eric Jul, Tuyen Trung Truong
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in digital imaging, e.g., increased number of pixels captured, have meant that the volume of data to be processed and analyzed from these images has also increased. Deep learning algorithms are state-of-the-art for analyzing such images, given their high accuracy when trained with a large data volume of data. Nevertheless, such analysis requires considerable computational power, making such algorithms time- and resource-demanding. Such high demands can be met by using third-party cloud service providers. However, analyzing medical images using such services raises several legal and privacy challenges and does not necessarily provide real-time results. This paper provides a computing architecture that locally and in parallel can analyze medical images in real-time using deep learning thus avoiding the legal and privacy challenges stemming from uploading data to a third-party cloud provider. To make local image processing efficient on modern multi-core processors, we utilize parallel execution to offset the resource-intensive demands of deep neural networks. We focus on a specific medical-industrial case study, namely the quantifying of blood vessels in microcirculation images for which we have developed a working system. It is currently used in an industrial, clinical research setting as part of an e-health application. Our results show that our system is approximately 78% faster than its serial system counterpart and 12% faster than a master-slave parallel system architecture.



### Towards Metrical Reconstruction of Human Faces
- **Arxiv ID**: http://arxiv.org/abs/2204.06607v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06607v2)
- **Published**: 2022-04-13 18:57:33+00:00
- **Updated**: 2022-10-19 17:29:53+00:00
- **Authors**: Wojciech Zielonka, Timo Bolkart, Justus Thies
- **Comment**: Video: https://youtu.be/vzzEbvv08VA Website:
  https://zielon.github.io/mica/ Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric benchmarks (15% and 24% lower average error on NoW, respectively).



### Adaptive Memory Management for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.06626v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06626v1)
- **Published**: 2022-04-13 19:59:07+00:00
- **Updated**: 2022-04-13 19:59:07+00:00
- **Authors**: Ali Pourganjalikhan, Charalambos Poullis
- **Comment**: In proceeding of the 19th Conference on Robots and Vision (CRV), 2022
- **Journal**: None
- **Summary**: Matching-based networks have achieved state-of-the-art performance for video object segmentation (VOS) tasks by storing every-k frames in an external memory bank for future inference. Storing the intermediate frames' predictions provides the network with richer cues for segmenting an object in the current frame. However, the size of the memory bank gradually increases with the length of the video, which slows down inference speed and makes it impractical to handle arbitrary length videos.   This paper proposes an adaptive memory bank strategy for matching-based networks for semi-supervised video object segmentation (VOS) that can handle videos of arbitrary length by discarding obsolete features. Features are indexed based on their importance in the segmentation of the objects in previous frames. Based on the index, we discard unimportant features to accommodate new features. We present our experiments on DAVIS 2016, DAVIS 2017, and Youtube-VOS that demonstrate that our method outperforms state-of-the-art that employ first-and-latest strategy with fixed-sized memory banks and achieves comparable performance to the every-k strategy with increasing-sized memory banks. Furthermore, experiments show that our method increases inference speed by up to 80% over the every-k and 35% over first-and-latest strategies.



### A Novel Approach for Optimum-Path Forest Classification Using Fuzzy Logic
- **Arxiv ID**: http://arxiv.org/abs/2204.06635v1
- **DOI**: 10.1109/TFUZZ.2019.2949771
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06635v1)
- **Published**: 2022-04-13 20:55:30+00:00
- **Updated**: 2022-04-13 20:55:30+00:00
- **Authors**: Renato W. R. de Souza, João V. C. de Oliveira, Leandro A. Passos, Weiping Ding, João P. Papa, Victor Hugo C. de Albuquerque
- **Comment**: None
- **Journal**: IEEE Transactions on Fuzzy Systems 28.12 (2019): 3076-3086
- **Summary**: In the past decades, fuzzy logic has played an essential role in many research areas. Alongside, graph-based pattern recognition has shown to be of great importance due to its flexibility in partitioning the feature space using the background from graph theory. Some years ago, a new framework for both supervised, semi-supervised, and unsupervised learning named Optimum-Path Forest (OPF) was proposed with competitive results in several applications, besides comprising a low computational burden. In this paper, we propose the Fuzzy Optimum-Path Forest, an improved version of the standard OPF classifier that learns the samples' membership in an unsupervised fashion, which are further incorporated during supervised training. Such information is used to identify the most relevant training samples, thus improving the classification step. Experiments conducted over twelve public datasets highlight the robustness of the proposed approach, which behaves similarly to standard OPF in worst-case scenarios.



### Efficient Deep Learning-based Estimation of the Vital Signs on Smartphones
- **Arxiv ID**: http://arxiv.org/abs/2204.08989v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.AI, cs.CV, cs.LG, eess.IV, I.5.4; I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2204.08989v2)
- **Published**: 2022-04-13 21:20:42+00:00
- **Updated**: 2022-11-06 10:03:00+00:00
- **Authors**: Taha Samavati, Mahdi Farvardin, Aboozar Ghaffari
- **Comment**: 6 pages, 9 figures, 4 tables
- **Journal**: None
- **Summary**: Nowadays, due to the widespread use of smartphones in everyday life and the improvement of computational capabilities of these devices, many complex tasks can now be deployed on them. Concerning the need for continuous monitoring of vital signs, especially for the elderly or those with certain types of diseases, the development of algorithms that can estimate vital signs using smartphones has attracted researchers worldwide. Such algorithms estimate vital signs (heart rate and oxygen saturation level) by processing an input PPG signal. These methods often apply multiple pre-processing steps to the input signal before the prediction step. This can increase the computational complexity of these methods, meaning only a limited number of mobile devices can run them. Furthermore, multiple pre-processing steps also require the design of a couple of hand-crafted stages to obtain an optimal result. This research proposes a novel end-to-end solution to mobile-based vital sign estimation by deep learning. The proposed method does not require any pre-processing. Due to the use of fully convolutional architecture, the parameter count of our proposed model is, on average, a quarter of the ordinary architectures that use fully-connected layers as the prediction heads. As a result, the proposed model has less over-fitting chance and computational complexity. A public dataset for vital sign estimation, including 62 videos collected from 35 men and 27 women, is also provided. The experimental results demonstrate state-of-the-art estimation accuracy.



### Wassmap: Wasserstein Isometric Mapping for Image Manifold Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.06645v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T10, 49Q22
- **Links**: [PDF](http://arxiv.org/pdf/2204.06645v3)
- **Published**: 2022-04-13 21:43:28+00:00
- **Updated**: 2023-02-21 18:56:35+00:00
- **Authors**: Keaton Hamm, Nick Henscheid, Shujie Kang
- **Comment**: Accepted to SIAM Journal on Mathematics of Data Science
- **Journal**: None
- **Summary**: In this paper, we propose Wasserstein Isometric Mapping (Wassmap), a nonlinear dimensionality reduction technique that provides solutions to some drawbacks in existing global nonlinear dimensionality reduction algorithms in imaging applications. Wassmap represents images via probability measures in Wasserstein space, then uses pairwise Wasserstein distances between the associated measures to produce a low-dimensional, approximately isometric embedding. We show that the algorithm is able to exactly recover parameters of some image manifolds including those generated by translations or dilations of a fixed generating measure. Additionally, we show that a discrete version of the algorithm retrieves parameters from manifolds generated from discrete measures by providing a theoretical bridge to transfer recovery results from functional data to discrete data. Testing of the proposed algorithms on various image data manifolds show that Wassmap yields good embeddings compared with other global and local techniques.



### A deep learning algorithm for reducing false positives in screening mammography
- **Arxiv ID**: http://arxiv.org/abs/2204.06671v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.06671v1)
- **Published**: 2022-04-13 23:37:40+00:00
- **Updated**: 2022-04-13 23:37:40+00:00
- **Authors**: Stefano Pedemonte, Trevor Tsue, Brent Mombourquette, Yen Nhi Truong Vu, Thomas Matthews, Rodrigo Morales Hoil, Meet Shah, Nikita Ghare, Naomi Zingman-Daniels, Susan Holley, Catherine M. Appleton, Jason Su, Richard L. Wahl
- **Comment**: None
- **Journal**: None
- **Summary**: Screening mammography improves breast cancer outcomes by enabling early detection and treatment. However, false positive callbacks for additional imaging from screening exams cause unnecessary procedures, patient anxiety, and financial burden. This work demonstrates an AI algorithm that reduces false positives by identifying mammograms not suspicious for breast cancer. We trained the algorithm to determine the absence of cancer using 123,248 2D digital mammograms (6,161 cancers) and performed a retrospective study on 14,831 screening exams (1,026 cancers) from 15 US and 3 UK sites. Retrospective evaluation of the algorithm on the largest of the US sites (11,592 mammograms, 101 cancers) a) left the cancer detection rate unaffected (p=0.02, non-inferiority margin 0.25 cancers per 1000 exams), b) reduced callbacks for diagnostic exams by 31.1% compared to standard clinical readings, c) reduced benign needle biopsies by 7.4%, and d) reduced screening exams requiring radiologist interpretation by 41.6% in the simulated clinical workflow. This work lays the foundation for semi-autonomous breast cancer screening systems that could benefit patients and healthcare systems by reducing false positives, unnecessary procedures, patient anxiety, and expenses.



### Geometric Understanding of Sketches
- **Arxiv ID**: http://arxiv.org/abs/2204.06675v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2204.06675v1)
- **Published**: 2022-04-13 23:55:51+00:00
- **Updated**: 2022-04-13 23:55:51+00:00
- **Authors**: Raghav Brahmadesam Venkataramaiyer
- **Comment**: PhD thesis, based on arXiv:1910.07860, arXiv:2011.06822
- **Journal**: None
- **Summary**: Sketching is used as a ubiquitous tool of expression by novices and experts alike. In this thesis I explore two methods that help a system provide a geometric machine-understanding of sketches, and in-turn help a user accomplish a downstream task.   The first work deals with interpretation of a 2D-line drawing as a graph structure, and also illustrates its effectiveness through its physical reconstruction by a robot. We setup a two-step pipeline to solve the problem. Formerly, we estimate the vertices of the graph with sub-pixel level accuracy. We achieve this using a combination of deep convolutional neural networks learned under a supervised setting for pixel-level estimation followed by the connected component analysis for clustering. Later we follow it up with a feedback-loop-based edge estimation method. To complement the graph-interpretation, we further perform data-interchange to a robot legible ASCII format, and thus teach a robot to replicate a line drawing.   In the second work, we test the 3D-geometric understanding of a sketch-based system without explicit access to the information about 3D-geometry. The objective is to complete a contour-like sketch of a 3D-object, with illumination and texture information. We propose a data-driven approach to learn a conditional distribution modelled as deep convolutional neural networks to be trained under an adversarial setting; and we validate it against a human-in-the-loop. The method itself is further supported by synthetic data generation using constructive solid geometry following a standard graphics pipeline. In order to validate the efficacy of our method, we design a user-interface plugged into a popular sketch-based workflow, and setup a simple task-based exercise, for an artist. Thereafter, we also discover that form-exploration is an additional utility of our application.



