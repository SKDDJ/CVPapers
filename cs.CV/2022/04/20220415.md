# Arxiv Papers in cs.CV on 2022-04-15
### Model-agnostic Multi-Domain Learning with Domain-Specific Adapters for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.07270v2
- **DOI**: 10.1587/transinf.2022EDP7058
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07270v2)
- **Published**: 2022-04-15 00:02:13+00:00
- **Updated**: 2022-11-14 01:41:26+00:00
- **Authors**: Kazuki Omi, Jun Kimata, Toru Tamaki
- **Comment**: IEICE Transactions on Information and Systems, Vol. E105-D, No. 12,
  Dec. 2022
- **Journal**: None
- **Summary**: In this paper, we propose a multi-domain learning model for action recognition. The proposed method inserts domain-specific adapters between layers of domain-independent layers of a backbone network. Unlike a multi-head network that switches classification heads only, our model switches not only the heads, but also the adapters for facilitating to learn feature representations universal to multiple domains. Unlike prior works, the proposed method is model-agnostic and doesn't assume model structures unlike prior works. Experimental results on three popular action recognition datasets (HMDB51, UCF101, and Kinetics-400) demonstrate that the proposed method is more effective than a multi-head architecture and more efficient than separately training models for each domain.



### Invisible-to-Visible: Privacy-Aware Human Instance Segmentation using Airborne Ultrasound via Collaborative Learning Variational Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2204.07280v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07280v1)
- **Published**: 2022-04-15 00:56:01+00:00
- **Updated**: 2022-04-15 00:56:01+00:00
- **Authors**: Risako Tanigawa, Yasunori Ishii, Kazuki Kozuka, Takayoshi Yamashita
- **Comment**: None
- **Journal**: None
- **Summary**: In action understanding in indoor, we have to recognize human pose and action considering privacy. Although camera images can be used for highly accurate human action recognition, camera images do not preserve privacy. Therefore, we propose a new task for human instance segmentation from invisible information, especially airborne ultrasound, for action recognition. To perform instance segmentation from invisible information, we first convert sound waves to reflected sound directional images (sound images). Although the sound images can roughly identify the location of a person, the detailed shape is ambiguous. To address this problem, we propose a collaborative learning variational autoencoder (CL-VAE) that simultaneously uses sound and RGB images during training. In inference, it is possible to obtain instance segmentation results only from sound images. As a result of performance verification, CL-VAE could estimate human instance segmentations more accurately than conventional variational autoencoder and some other models. Since this method can obtain human segmentations individually, it could be applied to human action recognition tasks with privacy protection.



### Guided Co-Modulated GAN for 360° Field of View Extrapolation
- **Arxiv ID**: http://arxiv.org/abs/2204.07286v2
- **DOI**: 10.1109/3DV57658.2022.00059
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07286v2)
- **Published**: 2022-04-15 01:48:35+00:00
- **Updated**: 2022-08-23 00:40:24+00:00
- **Authors**: Mohammad Reza Karimi Dastjerdi, Yannick Hold-Geoffroy, Jonathan Eisenmann, Siavash Khodadadeh, Jean-François Lalonde
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: We propose a method to extrapolate a 360{\deg} field of view from a single image that allows for user-controlled synthesis of the out-painted content. To do so, we propose improvements to an existing GAN-based in-painting architecture for out-painting panoramic image representation. Our method obtains state-of-the-art results and outperforms previous methods on standard image quality metrics. To allow controlled synthesis of out-painting, we introduce a novel guided co-modulation framework, which drives the image generation process with a common pretrained discriminative model. Doing so maintains the high visual quality of generated panoramas while enabling user-controlled semantic content in the extrapolated field of view. We demonstrate the state-of-the-art results of our method on field of view extrapolation both qualitatively and quantitatively, providing thorough analysis of our novel editing capabilities. Finally, we demonstrate that our approach benefits the photorealistic virtual insertion of highly glossy objects in photographs.



### Dense Learning based Semi-Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.07300v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07300v1)
- **Published**: 2022-04-15 02:31:02+00:00
- **Updated**: 2022-04-15 02:31:02+00:00
- **Authors**: Binghui Chen, Pengyu Li, Xiang Chen, Biao Wang, Lei Zhang, Xian-Sheng Hua
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Semi-supervised object detection (SSOD) aims to facilitate the training and deployment of object detectors with the help of a large amount of unlabeled data. Though various self-training based and consistency-regularization based SSOD methods have been proposed, most of them are anchor-based detectors, ignoring the fact that in many real-world applications anchor-free detectors are more demanded. In this paper, we intend to bridge this gap and propose a DenSe Learning (DSL) based anchor-free SSOD algorithm. Specifically, we achieve this goal by introducing several novel techniques, including an Adaptive Filtering strategy for assigning multi-level and accurate dense pixel-wise pseudo-labels, an Aggregated Teacher for producing stable and precise pseudo-labels, and an uncertainty-consistency-regularization term among scales and shuffled patches for improving the generalization capability of the detector. Extensive experiments are conducted on MS-COCO and PASCAL-VOC, and the results show that our proposed DSL method records new state-of-the-art SSOD performance, surpassing existing methods by a large margin. Codes can be found at \textcolor{blue}{https://github.com/chenbinghui1/DSL}.



### Improving Cross-Modal Understanding in Visual Dialog via Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.07302v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2204.07302v1)
- **Published**: 2022-04-15 02:36:52+00:00
- **Updated**: 2022-04-15 02:36:52+00:00
- **Authors**: Feilong Chen, Xiuyi Chen, Shuang Xu, Bo Xu
- **Comment**: ICASSP 2022
- **Journal**: None
- **Summary**: Visual Dialog is a challenging vision-language task since the visual dialog agent needs to answer a series of questions after reasoning over both the image content and dialog history. Though existing methods try to deal with the cross-modal understanding in visual dialog, they are still not enough in ranking candidate answers based on their understanding of visual and textual contexts. In this paper, we analyze the cross-modal understanding in visual dialog based on the vision-language pre-training model VD-BERT and propose a novel approach to improve the cross-modal understanding for visual dialog, named ICMU. ICMU enhances cross-modal understanding by distinguishing different pulled inputs (i.e. pulled images, questions or answers) based on four-way contrastive learning. In addition, ICMU exploits the single-turn visual question answering to enhance the visual dialog model's cross-modal understanding to handle a multi-turn visually-grounded conversation. Experiments show that the proposed approach improves the visual dialog model's cross-modal understanding and brings satisfactory gain to the VisDial dataset.



### Pushing the Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make a Difference
- **Arxiv ID**: http://arxiv.org/abs/2204.07305v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07305v1)
- **Published**: 2022-04-15 02:55:58+00:00
- **Updated**: 2022-04-15 02:55:58+00:00
- **Authors**: Shell Xu Hu, Da Li, Jan Stühmer, Minyoung Kim, Timothy M. Hospedales
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Few-shot learning (FSL) is an important and topical problem in computer vision that has motivated extensive research into numerous methods spanning from sophisticated meta-learning methods to simple transfer learning baselines. We seek to push the limits of a simple-but-effective pipeline for more realistic and practical settings of few-shot image classification. To this end, we explore few-shot learning from the perspective of neural network architecture, as well as a three stage pipeline of network updates under different data supplies, where unsupervised external data is considered for pre-training, base categories are used to simulate few-shot tasks for meta-training, and the scarcely labelled data of an novel task is taken for fine-tuning. We investigate questions such as: (1) How pre-training on external data benefits FSL? (2) How state-of-the-art transformer architectures can be exploited? and (3) How fine-tuning mitigates domain shift? Ultimately, we show that a simple transformer-based pipeline yields surprisingly good performance on standard benchmarks such as Mini-ImageNet, CIFAR-FS, CDFSL and Meta-Dataset. Our code and demo are available at https://hushell.github.io/pmf.



### MetaSets: Meta-Learning on Point Sets for Generalizable Representations
- **Arxiv ID**: http://arxiv.org/abs/2204.07311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07311v1)
- **Published**: 2022-04-15 03:24:39+00:00
- **Updated**: 2022-04-15 03:24:39+00:00
- **Authors**: Chao Huang, Zhangjie Cao, Yunbo Wang, Jianmin Wang, Mingsheng Long
- **Comment**: 13 pages, CVPR 2021
- **Journal**: None
- **Summary**: Deep learning techniques for point clouds have achieved strong performance on a range of 3D vision tasks. However, it is costly to annotate large-scale point sets, making it critical to learn generalizable representations that can transfer well across different point sets. In this paper, we study a new problem of 3D Domain Generalization (3DDG) with the goal to generalize the model to other unseen domains of point clouds without any access to them in the training process. It is a challenging problem due to the substantial geometry shift from simulated to real data, such that most existing 3D models underperform due to overfitting the complete geometries in the source domain. We propose to tackle this problem via MetaSets, which meta-learns point cloud representations from a group of classification tasks on carefully-designed transformed point sets containing specific geometry priors. The learned representations are more generalizable to various unseen domains of different geometries. We design two benchmarks for Sim-to-Real transfer of 3D point clouds. Experimental results show that MetaSets outperforms existing 3D deep learning methods by large margins.



### Feature Compression for Rate Constrained Object Detection on the Edge
- **Arxiv ID**: http://arxiv.org/abs/2204.07314v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2204.07314v1)
- **Published**: 2022-04-15 03:39:30+00:00
- **Updated**: 2022-04-15 03:39:30+00:00
- **Authors**: Zhongzheng Yuan, Samyak Rawlekar, Siddharth Garg, Elza Erkip, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in computer vision has led to a growth of interest in deploying visual analytics model on mobile devices. However, most mobile devices have limited computing power, which prohibits them from running large scale visual analytics neural networks. An emerging approach to solve this problem is to offload the computation of these neural networks to computing resources at an edge server. Efficient computation offloading requires optimizing the trade-off between multiple objectives including compressed data rate, analytics performance, and computation speed. In this work, we consider a "split computation" system to offload a part of the computation of the YOLO object detection model. We propose a learnable feature compression approach to compress the intermediate YOLO features with light-weight computation. We train the feature compression and decompression module together with the YOLO model to optimize the object detection accuracy under a rate constraint. Compared to baseline methods that apply either standard image compression or learned image compression at the mobile and perform image decompression and YOLO at the edge, the proposed system achieves higher detection accuracy at the low to medium rate range. Furthermore, the proposed system requires substantially lower computation time on the mobile device with CPU only.



### A Keypoint-based Global Association Network for Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.07335v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07335v1)
- **Published**: 2022-04-15 05:24:04+00:00
- **Updated**: 2022-04-15 05:24:04+00:00
- **Authors**: Jinsheng Wang, Yinchao Ma, Shaofei Huang, Tianrui Hui, Fei Wang, Chen Qian, Tianzhu Zhang
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Lane detection is a challenging task that requires predicting complex topology shapes of lane lines and distinguishing different types of lanes simultaneously. Earlier works follow a top-down roadmap to regress predefined anchors into various shapes of lane lines, which lacks enough flexibility to fit complex shapes of lanes due to the fixed anchor shapes. Lately, some works propose to formulate lane detection as a keypoint estimation problem to describe the shapes of lane lines more flexibly and gradually group adjacent keypoints belonging to the same lane line in a point-by-point manner, which is inefficient and time-consuming during postprocessing. In this paper, we propose a Global Association Network (GANet) to formulate the lane detection problem from a new perspective, where each keypoint is directly regressed to the starting point of the lane line instead of point-by-point extension. Concretely, the association of keypoints to their belonged lane line is conducted by predicting their offsets to the corresponding starting points of lanes globally without dependence on each other, which could be done in parallel to greatly improve efficiency. In addition, we further propose a Lane-aware Feature Aggregator (LFA), which adaptively captures the local correlations between adjacent keypoints to supplement local information to the global association. Extensive experiments on two popular lane detection benchmarks show that our method outperforms previous methods with F1 score of 79.63% on CULane and 97.71% on Tusimple dataset with high FPS. The code will be released at https://github.com/Wolfwjs/GANet.



### CAiD: Context-Aware Instance Discrimination for Self-supervised Learning in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2204.07344v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.07344v1)
- **Published**: 2022-04-15 06:45:10+00:00
- **Updated**: 2022-04-15 06:45:10+00:00
- **Authors**: Mohammad Reza Hosseinzadeh Taher, Fatemeh Haghighi, Michael B. Gotway, Jianming Liang
- **Comment**: Accepted at MIDL 2022 [main conference]
- **Journal**: None
- **Summary**: Recently, self-supervised instance discrimination methods have achieved significant success in learning visual representations from unlabeled photographic images. However, given the marked differences between photographic and medical images, the efficacy of instance-based objectives, focusing on learning the most discriminative global features in the image (i.e., wheels in bicycle), remains unknown in medical imaging. Our preliminary analysis showed that high global similarity of medical images in terms of anatomy hampers instance discrimination methods for capturing a set of distinct features, negatively impacting their performance on medical downstream tasks. To alleviate this limitation, we have developed a simple yet effective self-supervised framework, called Context-Aware instance Discrimination (CAiD). CAiD aims to improve instance discrimination learning by providing finer and more discriminative information encoded from a diverse local context of unlabeled medical images. We conduct a systematic analysis to investigate the utility of the learned features from a three-pronged perspective: (i) generalizability and transferability, (ii) separability in the embedding space, and (iii) reusability. Our extensive experiments demonstrate that CAiD (1) enriches representations learned from existing instance discrimination methods; (2) delivers more discriminative features by adequately capturing finer contextual information from individual medial images; and (3) improves reusability of low/mid-level features compared to standard instance discriminative methods. As open science, all codes and pre-trained models are available on our GitHub page: https://github.com/JLiangLab/CAiD.



### MVSTER: Epipolar Transformer for Efficient Multi-View Stereo
- **Arxiv ID**: http://arxiv.org/abs/2204.07346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07346v1)
- **Published**: 2022-04-15 06:47:57+00:00
- **Updated**: 2022-04-15 06:47:57+00:00
- **Authors**: Xiaofeng Wang, Zheng Zhu, Fangbo Qin, Yun Ye, Guan Huang, Xu Chi, Yijia He, Xingang Wang
- **Comment**: Code: https://github.com/JeffWang987/MVSTER
- **Journal**: None
- **Summary**: Learning-based Multi-View Stereo (MVS) methods warp source images into the reference camera frustum to form 3D volumes, which are fused as a cost volume to be regularized by subsequent networks. The fusing step plays a vital role in bridging 2D semantics and 3D spatial associations. However, previous methods utilize extra networks to learn 2D information as fusing cues, underusing 3D spatial correlations and bringing additional computation costs. Therefore, we present MVSTER, which leverages the proposed epipolar Transformer to learn both 2D semantics and 3D spatial associations efficiently. Specifically, the epipolar Transformer utilizes a detachable monocular depth estimator to enhance 2D semantics and uses cross-attention to construct data-dependent 3D associations along epipolar line. Additionally, MVSTER is built in a cascade structure, where entropy-regularized optimal transport is leveraged to propagate finer depth estimations in each stage. Extensive experiments show MVSTER achieves state-of-the-art reconstruction performance with significantly higher efficiency: Compared with MVSNet and CasMVSNet, our MVSTER achieves 34% and 14% relative improvements on the DTU benchmark, with 80% and 51% relative reductions in running time. MVSTER also ranks first on Tanks&Temples-Advanced among all published works. Code is released at https://github.com/JeffWang987.



### Crowd counting with crowd attention convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2204.07347v1
- **DOI**: 10.1016/j.neucom.2019.11.064
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07347v1)
- **Published**: 2022-04-15 06:51:58+00:00
- **Updated**: 2022-04-15 06:51:58+00:00
- **Authors**: Jiwei Chen, Wen Su, Zengfu Wang
- **Comment**: Accepted by Neurocomputing
- **Journal**: Neurocomputing, 382, pages 210--220, (2020), Elsevier
- **Summary**: Crowd counting is a challenging problem due to the scene complexity and scale variation. Although deep learning has achieved great improvement in crowd counting, scene complexity affects the judgement of these methods and they usually regard some objects as people mistakenly; causing potentially enormous errors in the crowd counting result. To address the problem, we propose a novel end-to-end model called Crowd Attention Convolutional Neural Network (CAT-CNN). Our CAT-CNN can adaptively assess the importance of a human head at each pixel location by automatically encoding a confidence map. With the guidance of the confidence map, the position of human head in estimated density map gets more attention to encode the final density map, which can avoid enormous misjudgements effectively. The crowd count can be obtained by integrating the final density map. To encode a highly refined density map, the total crowd count of each image is classified in a designed classification task and we first explicitly map the prior of the population-level category to feature maps. To verify the efficiency of our proposed method, extensive experiments are conducted on three highly challenging datasets. Results establish the superiority of our method over many state-of-the-art methods.



### Condition-Invariant and Compact Visual Place Description by Convolutional Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2204.07350v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.07350v1)
- **Published**: 2022-04-15 07:09:23+00:00
- **Updated**: 2022-04-15 07:09:23+00:00
- **Authors**: Hanjing Ye, Weinan Chen, Jingwen Yu, Li He, Yisheng Guan, Hong Zhang
- **Comment**: under review in Journal Intelligent and Robotic Systems (JINT), 2022
- **Journal**: None
- **Summary**: Visual place recognition (VPR) in condition-varying environments is still an open problem. Popular solutions are CNN-based image descriptors, which have been shown to outperform traditional image descriptors based on hand-crafted visual features. However, there are two drawbacks of current CNN-based descriptors: a) their high dimension and b) lack of generalization, leading to low efficiency and poor performance in applications. In this paper, we propose to use a convolutional autoencoder (CAE) to tackle this problem. We employ a high-level layer of a pre-trained CNN to generate features, and train a CAE to map the features to a low-dimensional space to improve the condition invariance property of the descriptor and reduce its dimension at the same time. We verify our method in three challenging datasets involving significant illumination changes, and our method is shown to be superior to the state-of-the-art. For the benefit of the community, we make public the source code.



### Vision-and-Language Pretrained Models: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2204.07356v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2204.07356v5)
- **Published**: 2022-04-15 07:33:06+00:00
- **Updated**: 2022-05-03 21:31:58+00:00
- **Authors**: Siqu Long, Feiqi Cao, Soyeon Caren Han, Haiqin Yang
- **Comment**: Accepted in IJCAI 2022
- **Journal**: None
- **Summary**: Pretrained models have produced great success in both Computer Vision (CV) and Natural Language Processing (NLP). This progress leads to learning joint representations of vision and language pretraining by feeding visual and linguistic contents into a multi-layer transformer, Visual-Language Pretrained Models (VLPMs). In this paper, we present an overview of the major advances achieved in VLPMs for producing joint representations of vision and language. As the preliminaries, we briefly describe the general task definition and genetic architecture of VLPMs. We first discuss the language and vision data encoding methods and then present the mainstream VLPM structure as the core content. We further summarise several essential pretraining and fine-tuning strategies. Finally, we highlight three future directions for both CV and NLP researchers to provide insightful guidance.



### ResT V2: Simpler, Faster and Stronger
- **Arxiv ID**: http://arxiv.org/abs/2204.07366v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07366v3)
- **Published**: 2022-04-15 07:57:40+00:00
- **Updated**: 2022-09-27 07:01:18+00:00
- **Authors**: Qing-Long Zhang, Yu-Bin Yang
- **Comment**: ResTv2, a simpler, faster, and stronger multi-scale vision
  Transformer for visual recognition
- **Journal**: None
- **Summary**: This paper proposes ResTv2, a simpler, faster, and stronger multi-scale vision Transformer for visual recognition. ResTv2 simplifies the EMSA structure in ResTv1 (i.e., eliminating the multi-head interaction part) and employs an upsample operation to reconstruct the lost medium- and high-frequency information caused by the downsampling operation. In addition, we explore different techniques for better apply ResTv2 backbones to downstream tasks. We found that although combining EMSAv2 and window attention can greatly reduce the theoretical matrix multiply FLOPs, it may significantly decrease the computation density, thus causing lower actual speed. We comprehensively validate ResTv2 on ImageNet classification, COCO detection, and ADE20K semantic segmentation. Experimental results show that the proposed ResTv2 can outperform the recently state-of-the-art backbones by a large margin, demonstrating the potential of ResTv2 as solid backbones. The code and models will be made publicly available at \url{https://github.com/wofmanaf/ResT}



### 2D Human Pose Estimation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2204.07370v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07370v1)
- **Published**: 2022-04-15 08:09:43+00:00
- **Updated**: 2022-04-15 08:09:43+00:00
- **Authors**: Haoming Chen, Runyang Feng, Sifan Wu, Hao Xu, Fengcheng Zhou, Zhenguang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Human pose estimation aims at localizing human anatomical keypoints or body parts in the input data (e.g., images, videos, or signals). It forms a crucial component in enabling machines to have an insightful understanding of the behaviors of humans, and has become a salient problem in computer vision and related fields. Deep learning techniques allow learning feature representations directly from the data, significantly pushing the performance boundary of human pose estimation. In this paper, we reap the recent achievements of 2D human pose estimation methods and present a comprehensive survey. Briefly, existing approaches put their efforts in three directions, namely network architecture design, network training refinement, and post processing. Network architecture design looks at the architecture of human pose estimation models, extracting more robust features for keypoint recognition and localization. Network training refinement tap into the training of neural networks and aims to improve the representational ability of models. Post processing further incorporates model-agnostic polishing strategies to improve the performance of keypoint detection. More than 200 research contributions are involved in this survey, covering methodological frameworks, common benchmark datasets, evaluation metrics, and performance comparisons. We seek to provide researchers with a more comprehensive and systematic review on human pose estimation, allowing them to acquire a grand panorama and better identify future directions.



### Revisiting the Adversarial Robustness-Accuracy Tradeoff in Robot Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.07373v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07373v2)
- **Published**: 2022-04-15 08:12:15+00:00
- **Updated**: 2023-01-25 21:36:00+00:00
- **Authors**: Mathias Lechner, Alexander Amini, Daniela Rus, Thomas A. Henzinger
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial training (i.e., training on adversarially perturbed input data) is a well-studied method for making neural networks robust to potential adversarial attacks during inference. However, the improved robustness does not come for free but rather is accompanied by a decrease in overall model accuracy and performance. Recent work has shown that, in practical robot learning applications, the effects of adversarial training do not pose a fair trade-off but inflict a net loss when measured in holistic robot performance. This work revisits the robustness-accuracy trade-off in robot learning by systematically analyzing if recent advances in robust training methods and theory in conjunction with adversarial robot learning, are capable of making adversarial training suitable for real-world robot applications. We evaluate three different robot learning tasks ranging from autonomous driving in a high-fidelity environment amenable to sim-to-real deployment to mobile robot navigation and gesture recognition. Our results demonstrate that, while these techniques make incremental improvements on the trade-off on a relative scale, the negative impact on the nominal accuracy caused by adversarial training still outweighs the improved robustness by an order of magnitude. We conclude that although progress is happening, further advances in robust learning methods are necessary before they can benefit robot learning tasks in practice.



### Image Captioning In the Transformer Age
- **Arxiv ID**: http://arxiv.org/abs/2204.07374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07374v1)
- **Published**: 2022-04-15 08:13:39+00:00
- **Updated**: 2022-04-15 08:13:39+00:00
- **Authors**: Yang Xu, Li Li, Haiyang Xu, Songfang Huang, Fei Huang, Jianfei Cai
- **Comment**: 8pages,2 figures
- **Journal**: None
- **Summary**: Image Captioning (IC) has achieved astonishing developments by incorporating various techniques into the CNN-RNN encoder-decoder architecture. However, since CNN and RNN do not share the basic network component, such a heterogeneous pipeline is hard to be trained end-to-end where the visual encoder will not learn anything from the caption supervision. This drawback inspires the researchers to develop a homogeneous architecture that facilitates end-to-end training, for which Transformer is the perfect one that has proven its huge potential in both vision and language domains and thus can be used as the basic component of the visual encoder and language decoder in an IC pipeline. Meantime, self-supervised learning releases the power of the Transformer architecture that a pre-trained large-scale one can be generalized to various tasks including IC. The success of these large-scale models seems to weaken the importance of the single IC task. However, we demonstrate that IC still has its specific significance in this age by analyzing the connections between IC with some popular self-supervised learning paradigms. Due to the page limitation, we only refer to highly important papers in this short survey and more related works can be found at https://github.com/SjokerLily/awesome-image-captioning.



### Crowd counting with segmentation attention convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2204.07380v1
- **DOI**: 10.1049/ipr2.12099
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07380v1)
- **Published**: 2022-04-15 08:40:38+00:00
- **Updated**: 2022-04-15 08:40:38+00:00
- **Authors**: Jiwei Chen, Zengfu Wang
- **Comment**: Accepted by IET Image Processing
- **Journal**: IET Image Processing, 15, 6, pages 1221--1231, (2021)
- **Summary**: Deep learning occupies an undisputed dominance in crowd counting. In this paper, we propose a novel convolutional neural network (CNN) architecture called SegCrowdNet. Despite the complex background in crowd scenes, the proposeSegCrowdNet still adaptively highlights the human head region and suppresses the non-head region by segmentation. With the guidance of an attention mechanism, the proposed SegCrowdNet pays more attention to the human head region and automatically encodes the highly refined density map. The crowd count can be obtained by integrating the density map. To adapt the variation of crowd counts, SegCrowdNet intelligently classifies the crowd count of each image into several groups. In addition, the multi-scale features are learned and extracted in the proposed SegCrowdNet to overcome the scale variations of the crowd. To verify the effectiveness of our proposed method, extensive experiments are conducted on four challenging datasets. The results demonstrate that our proposed SegCrowdNet achieves excellent performance compared with the state-of-the-art methods.



### FasterVideo: Efficient Online Joint Object Detection And Tracking
- **Arxiv ID**: http://arxiv.org/abs/2204.07394v1
- **DOI**: 10.1007/978-3-031-06433-3_32
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07394v1)
- **Published**: 2022-04-15 09:25:34+00:00
- **Updated**: 2022-04-15 09:25:34+00:00
- **Authors**: Issa Mouawad, Francesca Odone
- **Comment**: Accepted at 21st International Conference on Image Analysis and
  Processing (ICIAP 2021)
- **Journal**: None
- **Summary**: Object detection and tracking in videos represent essential and computationally demanding building blocks for current and future visual perception systems. In order to reduce the efficiency gap between available methods and computational requirements of real-world applications, we propose to re-think one of the most successful methods for image object detection, Faster R-CNN, and extend it to the video domain. Specifically, we extend the detection framework to learn instance-level embeddings which prove beneficial for data association and re-identification purposes. Focusing on the computational aspects of detection and tracking, our proposed method reaches a very high computational efficiency necessary for relevant applications, while still managing to compete with recent and state-of-the-art methods as shown in the experiments we conduct on standard object tracking benchmarks



### SSR-HEF: Crowd Counting with Multi-Scale Semantic Refining and Hard Example Focusing
- **Arxiv ID**: http://arxiv.org/abs/2204.07406v1
- **DOI**: 10.1109/TII.2022.3160634
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07406v1)
- **Published**: 2022-04-15 10:01:35+00:00
- **Updated**: 2022-04-15 10:01:35+00:00
- **Authors**: Jiwei Chen, Kewei Wang, Wen Su, Zengfu Wang
- **Comment**: Accepted by IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS
- **Journal**: IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS (2022.03)
- **Summary**: Crowd counting based on density maps is generally regarded as a regression task.Deep learning is used to learn the mapping between image content and crowd density distribution. Although great success has been achieved, some pedestrians far away from the camera are difficult to be detected. And the number of hard examples is often larger. Existing methods with simple Euclidean distance algorithm indiscriminately optimize the hard and easy examples so that the densities of hard examples are usually incorrectly predicted to be lower or even zero, which results in large counting errors. To address this problem, we are the first to propose the Hard Example Focusing(HEF) algorithm for the regression task of crowd counting. The HEF algorithm makes our model rapidly focus on hard examples by attenuating the contribution of easy examples.Then higher importance will be given to the hard examples with wrong estimations. Moreover, the scale variations in crowd scenes are large, and the scale annotations are labor-intensive and expensive. By proposing a multi-Scale Semantic Refining (SSR) strategy, lower layers of our model can break through the limitation of deep learning to capture semantic features of different scales to sufficiently deal with the scale variation. We perform extensive experiments on six benchmark datasets to verify the proposed method. Results indicate the superiority of our proposed method over the state-of-the-art methods. Moreover, our designed model is smaller and faster.



### End-to-End Sensitivity-Based Filter Pruning
- **Arxiv ID**: http://arxiv.org/abs/2204.07412v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07412v1)
- **Published**: 2022-04-15 10:21:05+00:00
- **Updated**: 2022-04-15 10:21:05+00:00
- **Authors**: Zahra Babaiee, Lucas Liebenwein, Ramin Hasani, Daniela Rus, Radu Grosu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel sensitivity-based filter pruning algorithm (SbF-Pruner) to learn the importance scores of filters of each layer end-to-end. Our method learns the scores from the filter weights, enabling it to account for the correlations between the filters of each layer. Moreover, by training the pruning scores of all layers simultaneously our method can account for layer interdependencies, which is essential to find a performant sparse sub-network. Our proposed method can train and generate a pruned network from scratch in a straightforward, one-stage training process without requiring a pretrained network. Ultimately, we do not need layer-specific hyperparameters and pre-defined layer budgets, since SbF-Pruner can implicitly determine the appropriate number of channels in each layer. Our experimental results on different network architectures suggest that SbF-Pruner outperforms advanced pruning methods. Notably, on CIFAR-10, without requiring a pretrained baseline network, we obtain 1.02% and 1.19% accuracy gain on ResNet56 and ResNet110, compared to the baseline reported for state-of-the-art pruning algorithms. This is while SbF-Pruner reduces parameter-count by 52.3% (for ResNet56) and 54% (for ResNet101), which is better than the state-of-the-art pruning algorithms with a high margin of 9.5% and 6.6%.



### SOTVerse: A User-defined Task Space of Single Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2204.07414v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07414v2)
- **Published**: 2022-04-15 10:31:12+00:00
- **Updated**: 2023-02-02 07:22:36+00:00
- **Authors**: Shiyu Hu, Xin Zhao, Kaiqi Huang
- **Comment**: This paper is submitted to an international journal
- **Journal**: None
- **Summary**: Single object tracking (SOT) research falls into a cycle -- trackers perform well on most benchmarks but quickly fail in challenging scenarios, causing researchers to doubt the insufficient data content and take more effort to construct larger datasets with more challenging situations. However, inefficient data utilization and limited evaluation methods more seriously hinder SOT research. The former causes existing datasets can not be exploited comprehensively, while the latter neglects challenging factors in the evaluation process. In this article, we systematize the representative benchmarks and form a Single Object Tracking metaverse (SOTVerse) -- a user-defined SOT task space to break through the bottleneck. We first propose a 3E Paradigm to describe tasks by three components (i.e., environment, evaluation, and executor). Then, we summarize task characteristics, clarify the organization standards, and construct SOTVerse with 12.56 million frames. Specifically, SOTVerse automatically labels challenging factors per frame, allowing users to generate user-defined spaces efficiently via construction rules. Besides, SOTVerse provides two mechanisms with new indicators and successfully evaluates trackers under various subtasks. Consequently, SOTVerse first provides a strategy to improve resource utilization in the computer vision area, making research more standardized and scientific. The SOTVerse, toolkit, evaluation server, and results are available at http://metaverse.aitestunion.com.



### Deep CardioSound-An Ensembled Deep Learning Model for Heart Sound MultiLabelling
- **Arxiv ID**: http://arxiv.org/abs/2204.07420v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.07420v2)
- **Published**: 2022-04-15 11:13:11+00:00
- **Updated**: 2022-04-20 19:47:10+00:00
- **Authors**: Li Guo, Steven Davenport, Yonghong Peng
- **Comment**: None
- **Journal**: None
- **Summary**: Heart sound diagnosis and classification play an essential role in detecting cardiovascular disorders, especially when the remote diagnosis becomes standard clinical practice. Most of the current work is designed for single category based heard sound classification tasks. To further extend the landscape of the automatic heart sound diagnosis landscape, this work proposes a deep multilabel learning model that can automatically annotate heart sound recordings with labels from different label groups, including murmur's timing, pitch, grading, quality, and shape. Our experiment results show that the proposed method has achieved outstanding performance on the holdout data for the multi-labelling task with sensitivity=0.990, specificity=0.999, F1=0.990 at the segments level, and an overall accuracy=0.969 at the patient's recording level.



### Transfer Learning for Instance Segmentation of Waste Bottles using Mask R-CNN Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2204.07437v1
- **DOI**: 10.1007/978-3-030-71187-0_13
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07437v1)
- **Published**: 2022-04-15 12:19:24+00:00
- **Updated**: 2022-04-15 12:19:24+00:00
- **Authors**: Punitha Jaikumar, Remy Vandaele, Varun Ojha
- **Comment**: None
- **Journal**: Intelligent Systems Design and Applications. ISDA 2020
- **Summary**: This paper proposes a methodological approach with a transfer learning scheme for plastic waste bottle detection and instance segmentation using the \textit{mask region proposal convolutional neural network} (Mask R-CNN). Plastic bottles constitute one of the major pollutants posing a serious threat to the environment both in oceans and on land. The automated identification and segregation of bottles can facilitate plastic waste recycling. We prepare a custom-made dataset of 192 bottle images with pixel-by pixel-polygon annotation for the automatic segmentation task. The proposed transfer learning scheme makes use of a Mask R-CNN model pre-trained on the Microsoft COCO dataset. We present a comprehensive scheme for fine-tuning the base pre-trained Mask-RCNN model on our custom dataset. Our final fine-tuned model has achieved 59.4 \textit{mean average precision} (mAP), which corresponds to the MS COCO metric. The results indicate a promising application of deep learning for detecting waste bottles.



### INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold
- **Arxiv ID**: http://arxiv.org/abs/2204.07439v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07439v2)
- **Published**: 2022-04-15 12:30:02+00:00
- **Updated**: 2022-04-18 17:09:44+00:00
- **Authors**: Changhun Lee, Hyungjun Kim, Eunhyeok Park, Jae-Joon Kim
- **Comment**: 19 pages, 7 figures; excluded axessibility package
- **Journal**: None
- **Summary**: Binary Neural Networks (BNNs) have emerged as a promising solution for reducing the memory footprint and compute costs of deep neural networks. BNNs, on the other hand, suffer from information loss because binary activations are limited to only two values, resulting in reduced accuracy. To improve the accuracy, previous studies have attempted to control the distribution of binary activation by manually shifting the threshold of the activation function or making the shift amount trainable. During the process, they usually depended on statistical information computed from a batch. We argue that using statistical data from a batch fails to capture the crucial information for each input instance in BNN computations, and the differences between statistical information computed from each instance need to be considered when determining the binary activation threshold of each instance. Based on the concept, we propose the Binary Neural Network with INSTAnce-aware threshold (INSTA-BNN), which decides the activation threshold value considering the difference between statistical data computed from a batch and each instance. The proposed INSTA-BNN outperforms the baseline by 2.5% and 2.3% on the ImageNet classification task with comparable computing cost, achieving 68.0% and 71.7% top-1 accuracy on ResNet-18 and MobileNetV1 based models, respectively.



### COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2204.07441v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2204.07441v2)
- **Published**: 2022-04-15 12:34:47+00:00
- **Updated**: 2022-05-20 13:23:30+00:00
- **Authors**: Haoyu Lu, Nanyi Fei, Yuqi Huo, Yizhao Gao, Zhiwu Lu, Ji-Rong Wen
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Large-scale single-stream pre-training has shown dramatic performance in image-text retrieval. Regrettably, it faces low inference efficiency due to heavy attention layers. Recently, two-stream methods like CLIP and ALIGN with high inference efficiency have also shown promising performance, however, they only consider instance-level alignment between the two streams (thus there is still room for improvement). To overcome these limitations, we propose a novel COllaborative Two-Stream vision-language pretraining model termed COTS for image-text retrieval by enhancing cross-modal interaction. In addition to instance level alignment via momentum contrastive learning, we leverage two extra levels of cross-modal interactions in our COTS: (1) Token-level interaction - a masked visionlanguage modeling (MVLM) learning objective is devised without using a cross-stream network module, where variational autoencoder is imposed on the visual encoder to generate visual tokens for each image. (2) Task-level interaction - a KL-alignment learning objective is devised between text-to-image and image-to-text retrieval tasks, where the probability distribution per task is computed with the negative queues in momentum contrastive learning. Under a fair comparison setting, our COTS achieves the highest performance among all two-stream methods and comparable performance (but with 10,800X faster in inference) w.r.t. the latest single-stream methods. Importantly, our COTS is also applicable to text-to-video retrieval, yielding new state-ofthe-art on the widely-used MSR-VTT dataset.



### Scalable and Real-time Multi-Camera Vehicle Detection, Re-Identification, and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2204.07442v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.07442v1)
- **Published**: 2022-04-15 12:47:01+00:00
- **Updated**: 2022-04-15 12:47:01+00:00
- **Authors**: Pirazh Khorramshahi, Vineet Shenoy, Michael Pack, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-camera vehicle tracking is one of the most complicated tasks in Computer Vision as it involves distinct tasks including Vehicle Detection, Tracking, and Re-identification. Despite the challenges, multi-camera vehicle tracking has immense potential in transportation applications including speed, volume, origin-destination (O-D), and routing data generation. Several recent works have addressed the multi-camera tracking problem. However, most of the effort has gone towards improving accuracy on high-quality benchmark datasets while disregarding lower camera resolutions, compression artifacts and the overwhelming amount of computational power and time needed to carry out this task on its edge and thus making it prohibitive for large-scale and real-time deployment. Therefore, in this work we shed light on practical issues that should be addressed for the design of a multi-camera tracking system to provide actionable and timely insights. Moreover, we propose a real-time city-scale multi-camera vehicle tracking system that compares favorably to computationally intensive alternatives and handles real-world, low-resolution CCTV instead of idealized and curated video streams. To show its effectiveness, in addition to integration into the Regional Integrated Transportation Information System (RITIS), we participated in the 2021 NVIDIA AI City multi-camera tracking challenge and our method is ranked among the top five performers on the public leaderboard.



### Detecting Violence in Video Based on Deep Features Fusion Technique
- **Arxiv ID**: http://arxiv.org/abs/2204.07443v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07443v1)
- **Published**: 2022-04-15 12:51:20+00:00
- **Updated**: 2022-04-15 12:51:20+00:00
- **Authors**: Heyam M. Bin Jahlan, Lamiaa A. Elrefaei
- **Comment**: The IIXth International Workshop on Representation, analysis and
  recognition of shape and motion FroM Imaging data (RFMI 2019), December
  11-13, 2019, Sidi Bou Said, Tunis
- **Journal**: None
- **Summary**: With the rapid growth of surveillance cameras in many public places to mon-itor human activities such as in malls, streets, schools and, prisons, there is a strong demand for such systems to detect violence events automatically. Au-tomatic analysis of video to detect violence is significant for law enforce-ment. Moreover, it helps to avoid any social, economic and environmental damages. Mostly, all systems today require manual human supervisors to de-tect violence scenes in the video which is inefficient and inaccurate. in this work, we interest in physical violence that involved two persons or more. This work proposed a novel method to detect violence using a fusion tech-nique of two significantly different convolutional neural networks (CNNs) which are AlexNet and SqueezeNet networks. Each network followed by separate Convolution Long Short Term memory (ConvLSTM) to extract ro-bust and richer features from a video in the final hidden state. Then, making a fusion of these two obtained states and fed to the max-pooling layer. Final-ly, features were classified using a series of fully connected layers and soft-max classifier. The performance of the proposed method is evaluated using three standard benchmark datasets in terms of detection accuracy: Hockey Fight dataset, Movie dataset and Violent Flow dataset. The results show an accuracy of 97%, 100%, and 96% respectively. A comparison of the results with the state of the art techniques revealed the promising capability of the proposed method in recognizing violent videos.



### ORCNet: A context-based network to simultaneously segment the ocular region components
- **Arxiv ID**: http://arxiv.org/abs/2204.07456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07456v1)
- **Published**: 2022-04-15 13:31:06+00:00
- **Updated**: 2022-04-15 13:31:06+00:00
- **Authors**: Diego Rafael Lucio, Luiz A. Zanlorensi, Yandre Maldonado e Gomes da Costa, David Menotti
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate extraction of the Region of Interest is critical for successful ocular region-based biometrics. In this direction, we propose a new context-based segmentation approach, entitled Ocular Region Context Network (ORCNet), introducing a specific loss function, i.e., he Punish Context Loss (PC-Loss). The PC-Loss punishes the segmentation losses of a network by using a percentage difference value between the ground truth and the segmented masks. We obtain the percentage difference by taking into account Biederman's semantic relationship concepts, in which we use three contexts (semantic, spatial, and scale) to evaluate the relationships of the objects in an image. Our proposal achieved promising results in the evaluated scenarios: iris, sclera, and ALL (iris + sclera) segmentations, utperforming the literature baseline techniques. The ORCNet with ResNet-152 outperforms the best baseline (EncNet with ResNet-152) on average by 2.27%, 28.26% and 6.43% in terms of F-Score, Error Rate and Intersection Over Union, respectively. We also provide (for research purposes) 3,191 manually labeled masks for the MICHE-I database, as another contribution of our work.



### Sensitivity of sparse codes to image distortions
- **Arxiv ID**: http://arxiv.org/abs/2204.07466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07466v1)
- **Published**: 2022-04-15 13:58:00+00:00
- **Updated**: 2022-04-15 13:58:00+00:00
- **Authors**: Kyle Luther, H. Sebastian Seung
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse coding has been proposed as a theory of visual cortex and as an unsupervised algorithm for learning representations. We show empirically with the MNIST dataset that sparse codes can be very sensitive to image distortions, a behavior that may hinder invariant object recognition. A locally linear analysis suggests that the sensitivity is due to the existence of linear combinations of active dictionary elements with high cancellation. A nearest neighbor classifier is shown to perform worse on sparse codes than original images. For a linear classifier with a sufficiently large number of labeled examples, sparse codes are shown to yield higher accuracy than original images, but no higher than a representation computed by a random feedforward net. Sensitivity to distortions seems to be a basic property of sparse codes, and one should be aware of this property when applying sparse codes to invariant object recognition.



### Guiding Attention using Partial-Order Relationships for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2204.07476v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07476v1)
- **Published**: 2022-04-15 14:22:09+00:00
- **Updated**: 2022-04-15 14:22:09+00:00
- **Authors**: Murad Popattia, Muhammad Rafi, Rizwan Qureshi, Shah Nawaz
- **Comment**: Accepted at CVPRW
- **Journal**: None
- **Summary**: The use of attention models for automated image captioning has enabled many systems to produce accurate and meaningful descriptions for images. Over the years, many novel approaches have been proposed to enhance the attention process using different feature representations. In this paper, we extend this approach by creating a guided attention network mechanism, that exploits the relationship between the visual scene and text-descriptions using spatial features from the image, high-level information from the topics, and temporal context from caption generation, which are embedded together in an ordered embedding space. A pairwise ranking objective is used for training this embedding space which allows similar images, topics and captions in the shared semantic space to maintain a partial order in the visual-semantic hierarchy and hence, helps the model to produce more visually accurate captions. The experimental results based on MSCOCO dataset shows the competitiveness of our approach, with many state-of-the-art models on various evaluation metrics.



### Towards PAC Multi-Object Detection and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2204.07482v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07482v1)
- **Published**: 2022-04-15 14:33:42+00:00
- **Updated**: 2022-04-15 14:33:42+00:00
- **Authors**: Shuo Li, Sangdon Park, Xiayan Ji, Insup Lee, Osbert Bastani
- **Comment**: 15 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Accurately detecting and tracking multi-objects is important for safety-critical applications such as autonomous navigation. However, it remains challenging to provide guarantees on the performance of state-of-the-art techniques based on deep learning. We consider a strategy known as conformal prediction, which predicts sets of labels instead of a single label; in the classification and regression settings, these algorithms can guarantee that the true label lies within the prediction set with high probability. Building on these ideas, we propose multi-object detection and tracking algorithms that come with probably approximately correct (PAC) guarantees. They do so by constructing both a prediction set around each object detection as well as around the set of edge transitions; given an object, the detection prediction set contains its true bounding box with high probability, and the edge prediction set contains its true transition across frames with high probability. We empirically demonstrate that our method can detect and track objects with PAC guarantees on the COCO and MOT-17 datasets.



### Patch-wise Contrastive Style Learning for Instagram Filter Removal
- **Arxiv ID**: http://arxiv.org/abs/2204.07486v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07486v1)
- **Published**: 2022-04-15 14:38:28+00:00
- **Updated**: 2022-04-15 14:38:28+00:00
- **Authors**: Furkan Kınlı, Barış Özcan, Furkan Kıraç
- **Comment**: Accepted to NTIRE: New Trends in Image Restoration and Enhancement
  workshop and challenges at CVPR 2022
- **Journal**: None
- **Summary**: Image-level corruptions and perturbations degrade the performance of CNNs on different downstream vision tasks. Social media filters are one of the most common resources of various corruptions and perturbations for real-world visual analysis applications. The negative effects of these distractive factors can be alleviated by recovering the original images with their pure style for the inference of the downstream vision tasks. Assuming these filters substantially inject a piece of additional style information to the social media images, we can formulate the problem of recovering the original versions as a reverse style transfer problem. We introduce Contrastive Instagram Filter Removal Network (CIFR), which enhances this idea for Instagram filter removal by employing a novel multi-layer patch-wise contrastive style learning mechanism. Experiments show our proposed strategy produces better qualitative and quantitative results than the previous studies. Moreover, we present the results of our additional experiments for proposed architecture within different settings. Finally, we present the inference outputs and quantitative comparison of filtered and recovered images on localization and segmentation tasks to encourage the main motivation for this problem.



### Synthesizing Informative Training Samples with GAN
- **Arxiv ID**: http://arxiv.org/abs/2204.07513v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.07513v2)
- **Published**: 2022-04-15 15:16:01+00:00
- **Updated**: 2022-12-21 03:01:34+00:00
- **Authors**: Bo Zhao, Hakan Bilen
- **Comment**: NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research,
  https://openreview.net/forum?id=frAv0jtUMfS
- **Journal**: None
- **Summary**: Remarkable progress has been achieved in synthesizing photo-realistic images with generative adversarial networks (GANs). Recently, GANs are utilized as the training sample generator when obtaining or storing real training data is expensive even infeasible. However, traditional GANs generated images are not as informative as the real training samples when being used to train deep neural networks. In this paper, we propose a novel method to synthesize Informative Training samples with GAN (IT-GAN). Specifically, we freeze a pre-trained GAN model and learn the informative latent vectors that correspond to informative training samples. The synthesized images are required to preserve information for training deep neural networks rather than visual reality or fidelity. Experiments verify that the deep neural networks can learn faster and achieve better performance when being trained with our IT-GAN generated images. We also show that our method is a promising solution to dataset condensation problem.



### Unconditional Image-Text Pair Generation with Multimodal Cross Quantizer
- **Arxiv ID**: http://arxiv.org/abs/2204.07537v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07537v2)
- **Published**: 2022-04-15 16:29:55+00:00
- **Updated**: 2022-10-14 13:01:42+00:00
- **Authors**: Hyungyung Lee, Sungjin Park, Joonseok Lee, Edward Choi
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Although deep generative models have gained a lot of attention, most of the existing works are designed for unimodal generation. In this paper, we explore a new method for unconditional image-text pair generation. We design Multimodal Cross-Quantization VAE (MXQ-VAE), a novel vector quantizer for joint image-text representations, with which we discover that a joint image-text representation space is effective for semantically consistent image-text pair generation. To learn a multimodal semantic correlation in a quantized space, we combine VQ-VAE with a Transformer encoder and apply an input masking strategy. Specifically, MXQ-VAE accepts a masked image-text pair as input and learns a quantized joint representation space, so that the input can be converted to a unified code sequence, then we perform unconditional image-text pair generation with the code sequence. Extensive experiments show the correlation between the quantized joint space and the multimodal generation capability on synthetic and real-world datasets. In addition, we demonstrate the superiority of our approach in these two aspects over several baselines. The source code is publicly available at: https://github.com/ttumyche/MXQ-VAE.



### Semi-supervised atmospheric component learning in low-light image problem
- **Arxiv ID**: http://arxiv.org/abs/2204.07546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07546v1)
- **Published**: 2022-04-15 17:06:33+00:00
- **Updated**: 2022-04-15 17:06:33+00:00
- **Authors**: Masud An Nur Islam Fahim, Nazmus Saqib, Jung Ho Yub
- **Comment**: None
- **Journal**: None
- **Summary**: Ambient lighting conditions play a crucial role in determining the perceptual quality of images from photographic devices. In general, inadequate transmission light and undesired atmospheric conditions jointly degrade the image quality. If we know the desired ambient factors associated with the given low-light image, we can recover the enhanced image easily \cite{b1}. Typical deep networks perform enhancement mappings without investigating the light distribution and color formulation properties. This leads to a lack of image instance-adaptive performance in practice. On the other hand, physical model-driven schemes suffer from the need for inherent decompositions and multiple objective minimizations. Moreover, the above approaches are rarely data efficient or free of postprediction tuning. Influenced by the above issues, this study presents a semisupervised training method using no-reference image quality metrics for low-light image restoration. We incorporate the classical haze distribution model \cite{b2} to explore the physical properties of the given image in order to learn the effect of atmospheric components and minimize a single objective for restoration. We validate the performance of our network for six widely used low-light datasets. The experiments show that the proposed study achieves state-of-the-art or comparable performance.



### Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.07548v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07548v2)
- **Published**: 2022-04-15 17:10:48+00:00
- **Updated**: 2022-07-07 13:24:47+00:00
- **Authors**: Damien Robert, Bruno Vallet, Loic Landrieu
- **Comment**: Accepted to CVPR 2022 with an Oral presentation and Best Paper
  candidate; camera ready version. 17 pages, 11 figures. Code and data
  available at https://github.com/drprojects/DeepViewAgg
- **Journal**: In Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition, pp. 5575-5584. 2022
- **Summary**: Recent works on 3D semantic segmentation propose to exploit the synergy between images and point clouds by processing each modality with a dedicated network and projecting learned 2D features onto 3D points. Merging large-scale point clouds and images raises several challenges, such as constructing a mapping between points and pixels, and aggregating features between multiple views. Current methods require mesh reconstruction or specialized sensors to recover occlusions, and use heuristics to select and aggregate available images. In contrast, we propose an end-to-end trainable multi-view aggregation model leveraging the viewing conditions of 3D points to merge features from images taken at arbitrary positions. Our method can combine standard 2D and 3D networks and outperforms both 3D models operating on colorized point clouds and hybrid 2D/3D networks without requiring colorization, meshing, or true depth maps. We set a new state-of-the-art for large-scale indoor/outdoor semantic segmentation on S3DIS (74.7 mIoU 6-Fold) and on KITTI-360 (58.3 mIoU). Our full pipeline is accessible at https://github.com/drprojects/DeepViewAgg, and only requires raw 3D scans and a set of images and poses.



### Y-Net: A Spatiospectral Dual-Encoder Networkfor Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.07613v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07613v2)
- **Published**: 2022-04-15 18:51:28+00:00
- **Updated**: 2022-07-07 16:44:02+00:00
- **Authors**: Azade Farshad, Yousef Yeganeh, Peter Gehlbach, Nassir Navab
- **Comment**: Accepted to MICCAI 2022
- **Journal**: None
- **Summary**: Automated segmentation of retinal optical coherence tomography (OCT) images has become an important recent direction in machine learning for medical applications. We hypothesize that the anatomic structure of layers and their high-frequency variation in OCT images make retinal OCT a fitting choice for extracting spectral-domain features and combining them with spatial domain features. In this work, we present $\Upsilon$-Net, an architecture that combines the frequency domain features with the image domain to improve the segmentation performance of OCT images. The results of this work demonstrate that the introduction of two branches, one for spectral and one for spatial domain features, brings a very significant improvement in fluid segmentation performance and allows outperformance as compared to the well-known U-Net model. Our improvement was 13% on the fluid segmentation dice score and 1.9% on the average dice score. Finally, removing selected frequency ranges in the spectral domain demonstrates the impact of these features on the fluid segmentation outperformance.



### Multi-Frame Self-Supervised Depth with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2204.07616v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07616v2)
- **Published**: 2022-04-15 19:04:57+00:00
- **Updated**: 2022-06-10 21:56:34+00:00
- **Authors**: Vitor Guizilini, Rares Ambrus, Dian Chen, Sergey Zakharov, Adrien Gaidon
- **Comment**: Accepted to CVPR 2022 (correct project page)
- **Journal**: None
- **Summary**: Multi-frame depth estimation improves over single-frame approaches by also leveraging geometric relationships between images via feature matching, in addition to learning appearance-based features. In this paper we revisit feature matching for self-supervised monocular depth estimation, and propose a novel transformer architecture for cost volume generation. We use depth-discretized epipolar sampling to select matching candidates, and refine predictions through a series of self- and cross-attention layers. These layers sharpen the matching probability between pixel features, improving over standard similarity metrics prone to ambiguities and local minima. The refined cost volume is decoded into depth estimates, and the whole pipeline is trained end-to-end from videos using only a photometric objective. Experiments on the KITTI and DDAD datasets show that our DepthFormer architecture establishes a new state of the art in self-supervised monocular depth estimation, and is even competitive with highly specialized supervised single-frame architectures. We also show that our learned cross-attention network yields representations transferable across datasets, increasing the effectiveness of pre-training strategies. Project page: https://sites.google.com/tri.global/depthformer



### Lagrangian Motion Magnification with Double Sparse Optical Flow Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2204.07636v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07636v1)
- **Published**: 2022-04-15 20:24:11+00:00
- **Updated**: 2022-04-15 20:24:11+00:00
- **Authors**: Philipp Flotho, Cosmas Heiss, Gabriele Steidl, Daniel J. Strauss
- **Comment**: None
- **Journal**: None
- **Summary**: Motion magnification techniques aim at amplifying and hence revealing subtle motion in videos. There are basically two main approaches to reach this goal, namely via Eulerian or Lagrangian techniques. While the first one magnifies motion implicitly by operating directly on image pixels, the Lagrangian approach uses optical flow techniques to extract and amplify pixel trajectories. Microexpressions are fast and spatially small facial expressions that are difficult to detect. In this paper, we propose a novel approach for local Lagrangian motion magnification of facial micromovements. Our contribution is three-fold: first, we fine-tune the recurrent all-pairs field transforms for optical flows (RAFT) deep learning approach for faces by adding ground truth obtained from the variational dense inverse search (DIS) for optical flow algorithm applied to the CASME II video set of faces. This enables us to produce optical flows of facial videos in an efficient and sufficiently accurate way. Second, since facial micromovements are both local in space and time, we propose to approximate the optical flow field by sparse components both in space and time leading to a double sparse decomposition. Third, we use this decomposition to magnify micro-motions in specific areas of the face, where we introduce a new forward warping strategy using a triangular splitting of the image grid and barycentric interpolation of the RGB vectors at the corners of the transformed triangles. We demonstrate the very good performance of our approach by various examples.



### Event-aided Direct Sparse Odometry
- **Arxiv ID**: http://arxiv.org/abs/2204.07640v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07640v2)
- **Published**: 2022-04-15 20:40:29+00:00
- **Updated**: 2022-04-24 12:02:08+00:00
- **Authors**: Javier Hidalgo-Carrió, Guillermo Gallego, Davide Scaramuzza
- **Comment**: 16 pages, 14 Figures, Page: https://rpg.ifi.uzh.ch/eds
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  New Orleans, 2022
- **Summary**: We introduce EDS, a direct monocular visual odometry using events and frames. Our algorithm leverages the event generation model to track the camera motion in the blind time between frames. The method formulates a direct probabilistic approach of observed brightness increments. Per-pixel brightness increments are predicted using a sparse number of selected 3D points and are compared to the events via the brightness increment error to estimate camera motion. The method recovers a semi-dense 3D map using photometric bundle adjustment. EDS is the first method to perform 6-DOF VO using events and frames with a direct approach. By design, it overcomes the problem of changing appearance in indirect methods. We also show that, for a target error performance, EDS can work at lower frame rates than state-of-the-art frame-based VO solutions. This opens the door to low-power motion-tracking applications where frames are sparingly triggered "on demand" and our method tracks the motion in between. We release code and datasets to the public.



### MultiEarth 2022 -- Multimodal Learning for Earth and Environment Workshop and Challenge
- **Arxiv ID**: http://arxiv.org/abs/2204.07649v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07649v3)
- **Published**: 2022-04-15 20:59:02+00:00
- **Updated**: 2022-05-31 13:34:06+00:00
- **Authors**: Miriam Cha, Kuan Wei Huang, Morgan Schmidt, Gregory Angelides, Mark Hamilton, Sam Goldberg, Armando Cabrera, Phillip Isola, Taylor Perron, Bill Freeman, Yen-Chen Lin, Brandon Swenson, Jean Piou
- **Comment**: None
- **Journal**: None
- **Summary**: The Multimodal Learning for Earth and Environment Challenge (MultiEarth 2022) will be the first competition aimed at the monitoring and analysis of deforestation in the Amazon rainforest at any time and in any weather conditions. The goal of the Challenge is to provide a common benchmark for multimodal information processing and to bring together the earth and environmental science communities as well as multimodal representation learning communities to compare the relative merits of the various multimodal learning methods to deforestation estimation under well-defined and strictly comparable conditions. MultiEarth 2022 will have three sub-challenges: 1) matrix completion, 2) deforestation estimation, and 3) image-to-image translation. This paper presents the challenge guidelines, datasets, and evaluation metrics for the three sub-challenges. Our challenge website is available at https://sites.google.com/view/rainforest-challenge.



### Deep Unlearning via Randomized Conditionally Independent Hessians
- **Arxiv ID**: http://arxiv.org/abs/2204.07655v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07655v2)
- **Published**: 2022-04-15 21:44:48+00:00
- **Updated**: 2022-07-13 21:22:31+00:00
- **Authors**: Ronak Mehta, Sourav Pal, Vikas Singh, Sathya N. Ravi
- **Comment**: CVPR 2022. Supplement appended to end of main paper (total 15 pages).
  Ronak Mehta and Sourav Pal equal contribution
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), 2022, pp. 10422-10431
- **Summary**: Recent legislation has led to interest in machine unlearning, i.e., removing specific training samples from a predictive model as if they never existed in the training dataset. Unlearning may also be required due to corrupted/adversarial data or simply a user's updated privacy requirement. For models which require no training (k-NN), simply deleting the closest original sample can be effective. But this idea is inapplicable to models which learn richer representations. Recent ideas leveraging optimization-based updates scale poorly with the model dimension d, due to inverting the Hessian of the loss function. We use a variant of a new conditional independence coefficient, L-CODEC, to identify a subset of the model parameters with the most semantic overlap on an individual sample level. Our approach completely avoids the need to invert a (possibly) huge matrix. By utilizing a Markov blanket selection, we premise that L-CODEC is also suitable for deep unlearning, as well as other applications in vision. Compared to alternatives, L-CODEC makes approximate unlearning possible in settings that would otherwise be infeasible, including vision models used for face recognition, person re-identification and NLP models that may require unlearning samples identified for exclusion. Code can be found at https://github.com/vsingh-group/LCODEC-deep-unlearning/



### It is Okay to Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning by Contrastive Data Collection
- **Arxiv ID**: http://arxiv.org/abs/2204.07660v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07660v1)
- **Published**: 2022-04-15 22:08:45+00:00
- **Updated**: 2022-04-15 22:08:45+00:00
- **Authors**: Youssef Mohamed, Faizan Farooq Khan, Kilichbek Haydarov, Mohamed Elhoseiny
- **Comment**: 8 pages, Accepted at CVPR 22, for more details see
  https://www.artemisdataset-v2.org
- **Journal**: None
- **Summary**: Datasets that capture the connection between vision, language, and affection are limited, causing a lack of understanding of the emotional aspect of human intelligence. As a step in this direction, the ArtEmis dataset was recently introduced as a large-scale dataset of emotional reactions to images along with language explanations of these chosen emotions. We observed a significant emotional bias towards instance-rich emotions, making trained neural speakers less accurate in describing under-represented emotions. We show that collecting new data, in the same way, is not effective in mitigating this emotional bias. To remedy this problem, we propose a contrastive data collection approach to balance ArtEmis with a new complementary dataset such that a pair of similar images have contrasting emotions (one positive and one negative). We collected 260,533 instances using the proposed method, we combine them with ArtEmis, creating a second iteration of the dataset. The new combined dataset, dubbed ArtEmis v2.0, has a balanced distribution of emotions with explanations revealing more fine details in the associated painting. Our experiments show that neural speakers trained on the new dataset improve CIDEr and METEOR evaluation metrics by 20% and 7%, respectively, compared to the biased dataset. Finally, we also show that the performance per emotion of neural speakers is improved across all the emotion categories, significantly on under-represented emotions. The collected dataset and code are available at https://artemisdataset-v2.org.



### Self-Similarity Priors: Neural Collages as Differentiable Fractal Representations
- **Arxiv ID**: http://arxiv.org/abs/2204.07673v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.07673v1)
- **Published**: 2022-04-15 22:54:23+00:00
- **Updated**: 2022-04-15 22:54:23+00:00
- **Authors**: Michael Poli, Winnie Xu, Stefano Massaroli, Chenlin Meng, Kuno Kim, Stefano Ermon
- **Comment**: None
- **Journal**: None
- **Summary**: Many patterns in nature exhibit self-similarity: they can be compactly described via self-referential transformations. Said patterns commonly appear in natural and artificial objects, such as molecules, shorelines, galaxies and even images. In this work, we investigate the role of learning in the automated discovery of self-similarity and in its utilization for downstream tasks. To this end, we design a novel class of implicit operators, Neural Collages, which (1) represent data as the parameters of a self-referential, structured transformation, and (2) employ hypernetworks to amortize the cost of finding these parameters to a single forward pass. We investigate how to leverage the representations produced by Neural Collages in various tasks, including data compression and generation. Neural Collages image compressors are orders of magnitude faster than other self-similarity-based algorithms during encoding and offer compression rates competitive with implicit methods. Finally, we showcase applications of Neural Collages for fractal art and as deep generative models.



