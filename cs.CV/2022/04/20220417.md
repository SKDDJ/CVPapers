# Arxiv Papers in cs.CV on 2022-04-17
### SymForce: Symbolic Computation and Code Generation for Robotics
- **Arxiv ID**: http://arxiv.org/abs/2204.07889v2
- **DOI**: 10.15607/RSS.2022.XVIII.041
- **Categories**: **cs.RO**, cs.CV, cs.SC
- **Links**: [PDF](http://arxiv.org/pdf/2204.07889v2)
- **Published**: 2022-04-17 00:15:10+00:00
- **Updated**: 2022-05-06 17:15:46+00:00
- **Authors**: Hayk Martiros, Aaron Miller, Nathan Bucki, Bradley Solliday, Ryan Kennedy, Jack Zhu, Tung Dang, Dominic Pattison, Harrison Zheng, Teo Tomic, Peter Henry, Gareth Cross, Josiah VanderMey, Alvin Sun, Samuel Wang, Kristen Holtz
- **Comment**: 10 pages, 5 figures. RSS 2022
- **Journal**: None
- **Summary**: We present SymForce, a library for fast symbolic computation, code generation, and nonlinear optimization for robotics applications like computer vision, motion planning, and controls. SymForce combines the development speed and flexibility of symbolic math with the performance of autogenerated, highly optimized code in C++ or any target runtime language. SymForce provides geometry and camera types, Lie group operations, and branchless singularity handling for creating and analyzing complex symbolic expressions in Python, built on top of SymPy. Generated functions can be integrated as factors into our tangent-space nonlinear optimizer, which is highly optimized for real-time production use. We introduce novel methods to automatically compute tangent-space Jacobians, eliminating the need for bug-prone handwritten derivatives. This workflow enables faster runtime code, faster development time, and fewer lines of handwritten code versus the state-of-the-art. Our experiments demonstrate that our approach can yield order of magnitude speedups on computational tasks core to robotics. Code is available at https://github.com/symforce-org/symforce.



### Video Action Detection: Analysing Limitations and Challenges
- **Arxiv ID**: http://arxiv.org/abs/2204.07892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07892v1)
- **Published**: 2022-04-17 00:42:14+00:00
- **Updated**: 2022-04-17 00:42:14+00:00
- **Authors**: Rajat Modi, Aayush Jung Rana, Akash Kumar, Praveen Tirupattur, Shruti Vyas, Yogesh Singh Rawat, Mubarak Shah
- **Comment**: CVPRW'22
- **Journal**: None
- **Summary**: Beyond possessing large enough size to feed data hungry machines (eg, transformers), what attributes measure the quality of a dataset? Assuming that the definitions of such attributes do exist, how do we quantify among their relative existences? Our work attempts to explore these questions for video action detection. The task aims to spatio-temporally localize an actor and assign a relevant action class. We first analyze the existing datasets on video action detection and discuss their limitations. Next, we propose a new dataset, Multi Actor Multi Action (MAMA) which overcomes these limitations and is more suitable for real world applications. In addition, we perform a biasness study which analyzes a key property differentiating videos from static images: the temporal aspect. This reveals if the actions in these datasets really need the motion information of an actor, or whether they predict the occurrence of an action even by looking at a single frame. Finally, we investigate the widely held assumptions on the importance of temporal ordering: is temporal ordering important for detecting these actions? Such extreme experiments show existence of biases which have managed to creep into existing methods inspite of careful modeling.



### ParkPredict+: Multimodal Intent and Motion Prediction for Vehicles in Parking Lots with CNN and Transformer
- **Arxiv ID**: http://arxiv.org/abs/2204.10777v2
- **DOI**: 10.1109/ITSC55140.2022.9922162
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2204.10777v2)
- **Published**: 2022-04-17 01:54:25+00:00
- **Updated**: 2023-01-10 23:39:42+00:00
- **Authors**: Xu Shen, Matthew Lacayo, Nidhir Guggilla, Francesco Borrelli
- **Comment**: Published at IEEE ITSC 2022
- **Journal**: None
- **Summary**: The problem of multimodal intent and trajectory prediction for human-driven vehicles in parking lots is addressed in this paper. Using models designed with CNN and Transformer networks, we extract temporal-spatial and contextual information from trajectory history and local bird's eye view (BEV) semantic images, and generate predictions about intent distribution and future trajectory sequences. Our methods outperform existing models in accuracy, while allowing an arbitrary number of modes, encoding complex multi-agent scenarios, and adapting to different parking maps. To train and evaluate our method, we present the first public 4K video dataset of human driving in parking lots with accurate annotation, high frame rate, and rich traffic scenarios.



### MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2204.07908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07908v1)
- **Published**: 2022-04-17 02:39:32+00:00
- **Updated**: 2022-04-17 02:39:32+00:00
- **Authors**: Yuanhao Cai, Jing Lin, Zudi Lin, Haoqian Wang, Yulun Zhang, Hanspeter Pfister, Radu Timofte, Luc Van Gool
- **Comment**: Winner of NTIRE 2022 Challenge on Spectral Reconstruction from RGB;
  The First Transformer-based Method for Spectral Reconstruction
- **Journal**: CVPRW 2022
- **Summary**: Existing leading methods for spectral reconstruction (SR) focus on designing deeper or wider convolutional neural networks (CNNs) to learn the end-to-end mapping from the RGB image to its hyperspectral image (HSI). These CNN-based methods achieve impressive restoration performance while showing limitations in capturing the long-range dependencies and self-similarity prior. To cope with this problem, we propose a novel Transformer-based method, Multi-stage Spectral-wise Transformer (MST++), for efficient spectral reconstruction. In particular, we employ Spectral-wise Multi-head Self-attention (S-MSA) that is based on the HSI spatially sparse while spectrally self-similar nature to compose the basic unit, Spectral-wise Attention Block (SAB). Then SABs build up Single-stage Spectral-wise Transformer (SST) that exploits a U-shaped structure to extract multi-resolution contextual information. Finally, our MST++, cascaded by several SSTs, progressively improves the reconstruction quality from coarse to fine. Comprehensive experiments show that our MST++ significantly outperforms other state-of-the-art methods. In the NTIRE 2022 Spectral Reconstruction Challenge, our approach won the First place. Code and pre-trained models are publicly available at https://github.com/caiyuanhao1998/MST-plus-plus.



### What Goes beyond Multi-modal Fusion in One-stage Referring Expression Comprehension: An Empirical Study
- **Arxiv ID**: http://arxiv.org/abs/2204.07913v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07913v1)
- **Published**: 2022-04-17 03:04:03+00:00
- **Updated**: 2022-04-17 03:04:03+00:00
- **Authors**: Gen Luo, Yiyi Zhou, Jiamu Sun, Shubin Huang, Xiaoshuai Sun, Qixiang Ye, Yongjian Wu, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the existing work in one-stage referring expression comprehension (REC) mainly focuses on multi-modal fusion and reasoning, while the influence of other factors in this task lacks in-depth exploration. To fill this gap, we conduct an empirical study in this paper. Concretely, we first build a very simple REC network called SimREC, and ablate 42 candidate designs/settings, which covers the entire process of one-stage REC from network design to model training. Afterwards, we conduct over 100 experimental trials on three benchmark datasets of REC. The extensive experimental results not only show the key factors that affect REC performance in addition to multi-modal fusion, e.g., multi-scale features and data augmentation, but also yield some findings that run counter to conventional understanding. For example, as a vision and language (V&L) task, REC does is less impacted by language prior. In addition, with a proper combination of these findings, we can improve the performance of SimREC by a large margin, e.g., +27.12% on RefCOCO+, which outperforms all existing REC methods. But the most encouraging finding is that with much less training overhead and parameters, SimREC can still achieve better performance than a set of large-scale pre-trained models, e.g., UNITER and VILLA, portraying the special role of REC in existing V&L research.



### Fast Multi-grid Methods for Minimizing Curvature Energy
- **Arxiv ID**: http://arxiv.org/abs/2204.07921v3
- **DOI**: 10.1109/TIP.2023.3251024
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2204.07921v3)
- **Published**: 2022-04-17 04:34:38+00:00
- **Updated**: 2023-03-12 02:05:50+00:00
- **Authors**: Zhenwei Zhang, Ke Chen, Ke Tang, Yuping Duan
- **Comment**: None
- **Journal**: None
- **Summary**: The geometric high-order regularization methods such as mean curvature and Gaussian curvature, have been intensively studied during the last decades due to their abilities in preserving geometric properties including image edges, corners, and contrast. However, the dilemma between restoration quality and computational efficiency is an essential roadblock for high-order methods. In this paper, we propose fast multi-grid algorithms for minimizing both mean curvature and Gaussian curvature energy functionals without sacrificing accuracy for efficiency. Unlike the existing approaches based on operator splitting and the Augmented Lagrangian method (ALM), no artificial parameters are introduced in our formulation, which guarantees the robustness of the proposed algorithm. Meanwhile, we adopt the domain decomposition method to promote parallel computing and use the fine-to-coarse structure to accelerate convergence. Numerical experiments are presented on image denoising, CT, and MRI reconstruction problems to demonstrate the superiority of our method in preserving geometric structures and fine details. The proposed method is also shown effective in dealing with large-scale image processing problems by recovering an image of size $1024\times 1024$ within $40$s, while the ALM method requires around $200$s.



### Accelerated MRI With Deep Linear Convolutional Transform Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.07923v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2204.07923v2)
- **Published**: 2022-04-17 04:47:32+00:00
- **Updated**: 2022-08-19 06:29:32+00:00
- **Authors**: Hongyi Gu, Burhaneddin Yaman, Steen Moeller, Il Yong Chun, Mehmet Akçakaya
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies show that deep learning (DL) based MRI reconstruction outperforms conventional methods, such as parallel imaging and compressed sensing (CS), in multiple applications. Unlike CS that is typically implemented with pre-determined linear representations for regularization, DL inherently uses a non-linear representation learned from a large database. Another line of work uses transform learning (TL) to bridge the gap between these two approaches by learning linear representations from data. In this work, we combine ideas from CS, TL and DL reconstructions to learn deep linear convolutional transforms as part of an algorithm unrolling approach. Using end-to-end training, our results show that the proposed technique can reconstruct MR images to a level comparable to DL methods, while supporting uniform undersampling patterns unlike conventional CS methods. Our proposed method relies on convex sparse image reconstruction with linear representation at inference time, which may be beneficial for characterizing robustness, stability and generalizability.



### StyleT2F: Generating Human Faces from Textual Description Using StyleGAN2
- **Arxiv ID**: http://arxiv.org/abs/2204.07924v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07924v1)
- **Published**: 2022-04-17 04:51:30+00:00
- **Updated**: 2022-04-17 04:51:30+00:00
- **Authors**: Mohamed Shawky Sabae, Mohamed Ahmed Dardir, Remonda Talaat Eskarous, Mohamed Ramzy Ebbed
- **Comment**: None
- **Journal**: None
- **Summary**: AI-driven image generation has improved significantly in recent years. Generative adversarial networks (GANs), like StyleGAN, are able to generate high-quality realistic data and have artistic control over the output, as well. In this work, we present StyleT2F, a method of controlling the output of StyleGAN2 using text, in order to be able to generate a detailed human face from textual description. We utilize StyleGAN's latent space to manipulate different facial features and conditionally sample the required latent code, which embeds the facial features mentioned in the input text. Our method proves to capture the required features correctly and shows consistency between the input text and the output images. Moreover, our method guarantees disentanglement on manipulating a wide range of facial features that sufficiently describes a human face.



### In Defense of Subspace Tracker: Orthogonal Embedding for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2204.07927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07927v1)
- **Published**: 2022-04-17 05:03:31+00:00
- **Updated**: 2022-04-17 05:03:31+00:00
- **Authors**: Yao Sui, Guanghui Wang, Li Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The paper focuses on a classical tracking model, subspace learning, grounded on the fact that the targets in successive frames are considered to reside in a low-dimensional subspace or manifold due to the similarity in their appearances. In recent years, a number of subspace trackers have been proposed and obtained impressive results. Inspired by the most recent results that the tracking performance is boosted by the subspace with discrimination capability learned over the recently localized targets and their immediately surrounding background, this work aims at solving such a problem: how to learn a robust low-dimensional subspace to accurately and discriminatively represent these target and background samples. To this end, a discriminative approach, which reliably separates the target from its surrounding background, is injected into the subspace learning by means of joint learning, achieving a dimension-adaptive subspace with superior discrimination capability. The proposed approach is extensively evaluated and compared with the state-of-the-art trackers on four popular tracking benchmarks. The experimental results demonstrate that the proposed tracker performs competitively against its counterparts. In particular, it achieves more than 9% performance increase compared with the state-of-the-art subspace trackers.



### Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.07935v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07935v1)
- **Published**: 2022-04-17 05:42:41+00:00
- **Updated**: 2022-04-17 05:42:41+00:00
- **Authors**: Yingjie Chen, Diqi Chen, Tao Wang, Yizhou Wang, Yun Liang
- **Comment**: Accepted by AAAI2022
- **Journal**: None
- **Summary**: Subject-invariant facial action unit (AU) recognition remains challenging for the reason that the data distribution varies among subjects. In this paper, we propose a causal inference framework for subject-invariant facial action unit recognition. To illustrate the causal effect existing in AU recognition task, we formulate the causalities among facial images, subjects, latent AU semantic relations, and estimated AU occurrence probabilities via a structural causal model. By constructing such a causal diagram, we clarify the causal effect among variables and propose a plug-in causal intervention module, CIS, to deconfound the confounder \emph{Subject} in the causal diagram. Extensive experiments conducted on two commonly used AU benchmark datasets, BP4D and DISFA, show the effectiveness of our CIS, and the model with CIS inserted, CISNet, has achieved state-of-the-art performance.



### Wound Severity Classification using Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2204.07942v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07942v1)
- **Published**: 2022-04-17 06:39:40+00:00
- **Updated**: 2022-04-17 06:39:40+00:00
- **Authors**: D. M. Anisuzzaman, Yash Patel, Jeffrey Niezgoda, Sandeep Gopalakrishnan, Zeyun Yu
- **Comment**: 19 pages, 5 figures, 5 tables
- **Journal**: None
- **Summary**: The classification of wound severity is a critical step in wound diagnosis. An effective classifier can help wound professionals categorize wound conditions more quickly and affordably, allowing them to choose the best treatment option. This study used wound photos to construct a deep neural network-based wound severity classifier that classified them into one of three classes: green, yellow, or red. The green class denotes wounds still in the early stages of healing and are most likely to recover with adequate care. Wounds in the yellow category require more attention and treatment than those in the green category. Finally, the red class denotes the most severe wounds that require prompt attention and treatment. A dataset containing different types of wound images is designed with the help of wound specialists. Nine deep learning models are used with applying the concept of transfer learning. Several stacked models are also developed by concatenating these transfer learning models. The maximum accuracy achieved on multi-class classification is 68.49%. In addition, we achieved 78.79%, 81.40%, and 77.57% accuracies on green vs. yellow, green vs. red, and yellow vs. red classifications for binary classifications.



### Global-Supervised Contrastive Loss and View-Aware-Based Post-Processing for Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2204.07943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07943v1)
- **Published**: 2022-04-17 06:39:52+00:00
- **Updated**: 2022-04-17 06:39:52+00:00
- **Authors**: Zhijun Hu, Yong Xu, Jie Wen, Xianjing Cheng, Zaijun Zhang, Lilei Sun, Yaowei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a Global-Supervised Contrastive loss and a view-aware-based post-processing (VABPP) method for the field of vehicle re-identification. The traditional supervised contrastive loss calculates the distances of features within the batch, so it has the local attribute. While the proposed Global-Supervised Contrastive loss has new properties and has good global attributes, the positive and negative features of each anchor in the training process come from the entire training set. The proposed VABPP method is the first time that the view-aware-based method is used as a post-processing method in the field of vehicle re-identification. The advantages of VABPP are that, first, it is only used during testing and does not affect the training process. Second, as a post-processing method, it can be easily integrated into other trained re-id models. We directly apply the view-pair distance scaling coefficient matrix calculated by the model trained in this paper to another trained re-id model, and the VABPP method greatly improves its performance, which verifies the feasibility of the VABPP method.



### DR-GAN: Distribution Regularization for Text-to-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2204.07945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07945v1)
- **Published**: 2022-04-17 07:14:37+00:00
- **Updated**: 2022-04-17 07:14:37+00:00
- **Authors**: Hongchen Tan, Xiuping Liu, Baocai Yin, Xin Li
- **Comment**: Accepted by TNNLS
- **Journal**: None
- **Summary**: This paper presents a new Text-to-Image generation model, named Distribution Regularization Generative Adversarial Network (DR-GAN), to generate images from text descriptions from improved distribution learning. In DR-GAN, we introduce two novel modules: a Semantic Disentangling Module (SDM) and a Distribution Normalization Module (DNM). SDM combines the spatial self-attention mechanism and a new Semantic Disentangling Loss (SDL) to help the generator distill key semantic information for the image generation. DNM uses a Variational Auto-Encoder (VAE) to normalize and denoise the image latent distribution, which can help the discriminator better distinguish synthesized images from real images. DNM also adopts a Distribution Adversarial Loss (DAL) to guide the generator to align with normalized real image distributions in the latent space. Extensive experiments on two public datasets demonstrated that our DR-GAN achieved a competitive performance in the Text-to-Image task.



### Integrated In-vehicle Monitoring System Using 3D Human Pose Estimation and Seat Belt Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.07946v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07946v2)
- **Published**: 2022-04-17 07:20:50+00:00
- **Updated**: 2023-03-02 01:42:42+00:00
- **Authors**: Ginam Kim, Hyunsung Kim, Joseph Kihoon Kim, Sung-Sik Cho, Yeong-Hun Park, Suk-Ju Kang
- **Comment**: AAAI 2022 workshop AI for Transportation accepted
- **Journal**: None
- **Summary**: Recently, along with interest in autonomous vehicles, the importance of monitoring systems for both drivers and passengers inside vehicles has been increasing. This paper proposes a novel in-vehicle monitoring system the combines 3D pose estimation, seat-belt segmentation, and seat-belt status classification networks. Our system outputs various information necessary for monitoring by accurately considering the data characteristics of the in-vehicle environment. Specifically, the proposed 3D pose estimation directly estimates the absolute coordinates of keypoints for a driver and passengers, and the proposed seat-belt segmentation is implemented by applying a structure based on the feature pyramid. In addition, we propose a classification task to distinguish between normal and abnormal states of wearing a seat belt using results that combine 3D pose estimation with seat-belt segmentation. These tasks can be learned simultaneously and operate in real-time. Our method was evaluated on a private dataset we newly created and annotated. The experimental results show that our method has significantly high performance that can be applied directly to real in-vehicle monitoring systems.



### Learning with Signatures
- **Arxiv ID**: http://arxiv.org/abs/2204.07953v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07953v3)
- **Published**: 2022-04-17 08:36:15+00:00
- **Updated**: 2022-05-19 15:52:48+00:00
- **Authors**: J. de Curtò, I. de Zarzà, Hong Yan, Carlos T. Calafate
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we investigate the use of the Signature Transform in the context of Learning. Under this assumption, we advance a supervised framework that potentially provides state-of-the-art classification accuracy with the use of few labels without the need of credit assignment and with minimal or no overfitting. We leverage tools from harmonic analysis by the use of the signature and log-signature, and use as a score function RMSE and MAE Signature and log-signature. We develop a closed-form equation to compute probably good optimal scale factors, as well as the formulation to obtain them by optimization. Techniques of Signal Processing are addressed to further characterize the problem. Classification is performed at the CPU level orders of magnitude faster than other methods. We report results on AFHQ, MNIST and CIFAR10, achieving 100% accuracy on all tasks assuming we can determine at test time which probably good optimal scale factor to use for each category.



### Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis
- **Arxiv ID**: http://arxiv.org/abs/2204.07955v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.07955v2)
- **Published**: 2022-04-17 08:44:00+00:00
- **Updated**: 2022-04-21 12:46:38+00:00
- **Authors**: Yan Ling, Jianfei Yu, Rui Xia
- **Comment**: Accepted by ACL 2022 (long paper)
- **Journal**: None
- **Summary**: As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment Analysis (MABSA) has attracted increasing attention in recent years. However, previous approaches either (i) use separately pre-trained visual and textual models, which ignore the crossmodal alignment or (ii) use vision-language models pre-trained with general pre-training tasks, which are inadequate to identify finegrained aspects, opinions, and their alignments across modalities. To tackle these limitations, we propose a task-specific Vision-Language Pre-training framework for MABSA (VLPMABSA), which is a unified multimodal encoder-decoder architecture for all the pretraining and downstream tasks. We further design three types of task-specific pre-training tasks from the language, vision, and multimodal modalities, respectively. Experimental results show that our approach generally outperforms the state-of-the-art approaches on three MABSA subtasks. Further analysis demonstrates the effectiveness of each pretraining task. The source code is publicly released at https://github.com/NUSTM/VLP-MABSA.



### An Extendable, Efficient and Effective Transformer-based Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2204.07962v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07962v1)
- **Published**: 2022-04-17 09:27:45+00:00
- **Updated**: 2022-04-17 09:27:45+00:00
- **Authors**: Hwanjun Song, Deqing Sun, Sanghyuk Chun, Varun Jampani, Dongyoon Han, Byeongho Heo, Wonjae Kim, Ming-Hsuan Yang
- **Comment**: An extension of the ICLR paper, ViDT: An Efficient and Effective
  Fully Transformer-based Object Detector. arXiv admin note: substantial text
  overlap with arXiv:2110.03921
- **Journal**: None
- **Summary**: Transformers have been widely used in numerous vision problems especially for visual recognition and detection. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to construct an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. In addition, we extend it to ViDT+ to support joint-task learning for object detection and instance segmentation. Specifically, we attach an efficient multi-scale feature fusion layer and utilize two more auxiliary training losses, IoU-aware loss and token labeling loss. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and its extended ViDT+ achieves 53.2AP owing to its high scalability for large models. The source code and trained models are available at https://github.com/naver-ai/vidt.



### AFSC: Adaptive Fourier Space Compression for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.07963v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.07963v1)
- **Published**: 2022-04-17 09:42:05+00:00
- **Updated**: 2022-04-17 09:42:05+00:00
- **Authors**: Haote Xu, Yunlong Zhang, Liyan Sun, Chenxin Li, Yue Huang, Xinghao Ding
- **Comment**: 9 pages, 2 figures
- **Journal**: None
- **Summary**: Anomaly Detection (AD) on medical images enables a model to recognize any type of anomaly pattern without lesion-specific supervised learning. Data augmentation based methods construct pseudo-healthy images by "pasting" fake lesions on real healthy ones, and a network is trained to predict healthy images in a supervised manner. The lesion can be found by difference between the unhealthy input and pseudo-healthy output. However, using only manually designed fake lesions fail to approximate to irregular real lesions, hence limiting the model generalization. We assume by exploring the intrinsic data property within images, we can distinguish previously unseen lesions from healthy regions in an unhealthy image. In this study, we propose an Adaptive Fourier Space Compression (AFSC) module to distill healthy feature for AD. The compression of both magnitude and phase in frequency domain addresses the hyper intensity and diverse position of lesions. Experimental results on the BraTS and MS-SEG datasets demonstrate an AFSC baseline is able to produce promising detection results, and an AFSC module can be effectively embedded into existing AD methods.



### Target-Relevant Knowledge Preservation for Multi-Source Domain Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.07964v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07964v1)
- **Published**: 2022-04-17 09:50:48+00:00
- **Updated**: 2022-04-17 09:50:48+00:00
- **Authors**: Jiaxi Wu, Jiaxin Chen, Mengzhe He, Yiru Wang, Bo Li, Bingqi Ma, Weihao Gan, Wei Wu, Yali Wang, Di Huang
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: Domain adaptive object detection (DAOD) is a promising way to alleviate performance drop of detectors in new scenes. Albeit great effort made in single source domain adaptation, a more generalized task with multiple source domains remains not being well explored, due to knowledge degradation during their combination. To address this issue, we propose a novel approach, namely target-relevant knowledge preservation (TRKP), to unsupervised multi-source DAOD. Specifically, TRKP adopts the teacher-student framework, where the multi-head teacher network is built to extract knowledge from labeled source domains and guide the student network to learn detectors in unlabeled target domain. The teacher network is further equipped with an adversarial multi-source disentanglement (AMSD) module to preserve source domain-specific knowledge and simultaneously perform cross-domain alignment. Besides, a holistic target-relevant mining (HTRM) scheme is developed to re-weight the source images according to the source-target relevance. By this means, the teacher network is enforced to capture target-relevant knowledge, thus benefiting decreasing domain shift when mentoring object detection in the target domain. Extensive experiments are conducted on various widely used benchmarks with new state-of-the-art scores reported, highlighting the effectiveness.



### Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint
- **Arxiv ID**: http://arxiv.org/abs/2204.07965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07965v1)
- **Published**: 2022-04-17 09:51:12+00:00
- **Updated**: 2022-04-17 09:51:12+00:00
- **Authors**: Jiaxi Wu, Jiaxin Chen, Di Huang
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: Active learning is a promising alternative to alleviate the issue of high annotation cost in the computer vision tasks by consciously selecting more informative samples to label. Active learning for object detection is more challenging and existing efforts on it are relatively rare. In this paper, we propose a novel hybrid approach to address this problem, where the instance-level uncertainty and diversity are jointly considered in a bottom-up manner. To balance the computational complexity, the proposed approach is designed as a two-stage procedure. At the first stage, an Entropy-based Non-Maximum Suppression (ENMS) is presented to estimate the uncertainty of every image, which performs NMS according to the entropy in the feature space to remove predictions with redundant information gains. At the second stage, a diverse prototype (DivProto) strategy is explored to ensure the diversity across images by progressively converting it into the intra-class and inter-class diversities of the entropy-based class-specific prototypes. Extensive experiments are conducted on MS COCO and Pascal VOC, and the proposed approach achieves state of the art results and significantly outperforms the other counterparts, highlighting its superiority.



### Augmentation Invariance and Adaptive Sampling in Semantic Segmentation of Agricultural Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2204.07969v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07969v1)
- **Published**: 2022-04-17 10:19:07+00:00
- **Updated**: 2022-04-17 10:19:07+00:00
- **Authors**: Antonio Tavera, Edoardo Arnaudo, Carlo Masone, Barbara Caputo
- **Comment**: CVPR 2022 Workshop - Agriculture Vision
- **Journal**: None
- **Summary**: In this paper, we investigate the problem of Semantic Segmentation for agricultural aerial imagery. We observe that the existing methods used for this task are designed without considering two characteristics of the aerial data: (i) the top-down perspective implies that the model cannot rely on a fixed semantic structure of the scene, because the same scene may be experienced with different rotations of the sensor; (ii) there can be a strong imbalance in the distribution of semantic classes because the relevant objects of the scene may appear at extremely different scales (e.g., a field of crops and a small vehicle). We propose a solution to these problems based on two ideas: (i) we use together a set of suitable augmentation and a consistency loss to guide the model to learn semantic representations that are invariant to the photometric and geometric shifts typical of the top-down perspective (Augmentation Invariance); (ii) we use a sampling method (Adaptive Sampling) that selects the training images based on a measure of pixel-wise distribution of classes and actual network confidence. With an extensive set of experiments conducted on the Agriculture-Vision dataset, we demonstrate that our proposed strategies improve the performance of the current state-of-the-art method.



### Automatic spinal curvature measurement on ultrasound spine images using Faster R-CNN
- **Arxiv ID**: http://arxiv.org/abs/2204.07988v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.07988v2)
- **Published**: 2022-04-17 12:09:29+00:00
- **Updated**: 2022-04-20 08:32:06+00:00
- **Authors**: Zhichao Liu, Liyue Qian, Wenke Jing, Desen Zhou, Xuming He, Edmond Lou, Rui Zheng
- **Comment**: Accepted by IUS2021
- **Journal**: None
- **Summary**: Ultrasound spine imaging technique has been applied to the assessment of spine deformity. However, manual measurements of scoliotic angles on ultrasound images are time-consuming and heavily rely on raters experience. The objectives of this study are to construct a fully automatic framework based on Faster R-CNN for detecting vertebral lamina and to measure the fitting spinal curves from the detected lamina pairs. The framework consisted of two closely linked modules: 1) the lamina detector for identifying and locating each lamina pairs on ultrasound coronal images, and 2) the spinal curvature estimator for calculating the scoliotic angles based on the chain of detected lamina. Two hundred ultrasound images obtained from AIS patients were identified and used for the training and evaluation of the proposed method. The experimental results showed the 0.76 AP on the test set, and the Mean Absolute Difference (MAD) between automatic and manual measurement which was within the clinical acceptance error. Meanwhile the correlation between automatic measurement and Cobb angle from radiographs was 0.79. The results revealed that our proposed technique could provide accurate and reliable automatic curvature measurements on ultrasound spine images for spine deformities.



### PiouCrypt: Decentralized Lattice-based Method for Visual Symmetric Cryptography
- **Arxiv ID**: http://arxiv.org/abs/2204.08017v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, 68P25 (Primary) 94A60, 15B52, 06B30, 11Y05, 68P27 (Secondary), E.3; G.1.3; F.2.1; F.2.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2204.08017v1)
- **Published**: 2022-04-17 13:28:32+00:00
- **Updated**: 2022-04-17 13:28:32+00:00
- **Authors**: Navid Abapour, Mohsen Ebadpour
- **Comment**: 21 pages, 23 figures, for accessing source code, see
  https://bitbucket.org/my_piou/pioucrypt
- **Journal**: None
- **Summary**: In recent years, establishing secure visual communications has turned into one of the essential problems for security engineers and researchers. However, only limited novel solutions are provided for image encryption, and limiting the visual cryptography to only limited schemes can bring up negative consequences, especially with emerging quantum computational systems. This paper presents a novel algorithm for establishing secure private visual communication. The proposed method has a layered architecture with several cohesive components, and corresponded with an NP-hard problem, despite its symmetric structure. This two-step technique is not limited to gray-scale pictures, and furthermore, utilizing a lattice structure causes to proposed method has optimal resistance for the post-quantum era, and is relatively secure from the theoretical dimension.



### VDTR: Video Deblurring with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2204.08023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08023v1)
- **Published**: 2022-04-17 14:22:14+00:00
- **Updated**: 2022-04-17 14:22:14+00:00
- **Authors**: Mingdeng Cao, Yanbo Fan, Yong Zhang, Jue Wang, Yujiu Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Video deblurring is still an unsolved problem due to the challenging spatio-temporal modeling process. While existing convolutional neural network-based methods show a limited capacity for effective spatial and temporal modeling for video deblurring. This paper presents VDTR, an effective Transformer-based model that makes the first attempt to adapt Transformer for video deblurring. VDTR exploits the superior long-range and relation modeling capabilities of Transformer for both spatial and temporal modeling. However, it is challenging to design an appropriate Transformer-based model for video deblurring due to the complicated non-uniform blurs, misalignment across multiple frames and the high computational costs for high-resolution spatial modeling. To address these problems, VDTR advocates performing attention within non-overlapping windows and exploiting the hierarchical structure for long-range dependencies modeling. For frame-level spatial modeling, we propose an encoder-decoder Transformer that utilizes multi-scale features for deblurring. For multi-frame temporal modeling, we adapt Transformer to fuse multiple spatial features efficiently. Compared with CNN-based methods, the proposed method achieves highly competitive results on both synthetic and real-world video deblurring benchmarks, including DVD, GOPRO, REDS and BSD. We hope such a Transformer-based architecture can serve as a powerful alternative baseline for video deblurring and other video restoration tasks. The source code will be available at \url{https://github.com/ljzycmd/VDTR}.



### The Z-axis, X-axis, Weight and Disambiguation Methods for Constructing Local Reference Frame in 3D Registration: An Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2204.08024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08024v1)
- **Published**: 2022-04-17 14:29:06+00:00
- **Updated**: 2022-04-17 14:29:06+00:00
- **Authors**: Bao Zhao, Xianyong Fang, Jiahui Yue, Xiaobo Chen, Xinyi Le, Chanjuan Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: The local reference frame (LRF), as an independent coordinate system generated on a local 3D surface, is widely used in 3D local feature descriptor construction and 3D transformation estimation which are two key steps in the local method-based surface matching. There are numerous LRF methods have been proposed in literatures. In these methods, the x- and z-axis are commonly generated by different methods or strategies, and some x-axis methods are implemented on the basis of a z-axis being given. In addition, the weight and disambiguation methods are commonly used in these LRF methods. In existing evaluations of LRF, each LRF method is evaluated with a complete form. However, the merits and demerits of the z-axis, x-axis, weight and disambiguation methods in LRF construction are unclear. In this paper, we comprehensively analyze the z-axis, x-axis, weight and disambiguation methods in existing LRFs, and obtain six z-axis and eight x-axis, five weight and two disambiguation methods. The performance of these methods are comprehensively evaluated on six standard datasets with different application scenarios and nuisances. Considering the evaluation outcomes, the merits and demerits of different weight, disambiguation, z- and x-axis methods are analyzed and summarized. The experimental result also shows that some new designed LRF axes present superior performance compared with the state-of-the-art ones.



### Attention Mechanism based Cognition-level Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2204.08027v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.08027v2)
- **Published**: 2022-04-17 15:04:44+00:00
- **Updated**: 2022-04-19 02:40:42+00:00
- **Authors**: Xuejiao Tang, Tai Le Quy, Eirini Ntoutsi, Kea Turner, Vasile Palade, Israat Haque, Peng Xu, Chris Brown, Wenbin Zhang
- **Comment**: arXiv admin note: text overlap with arXiv:2108.02924,
  arXiv:2107.01671
- **Journal**: None
- **Summary**: Given a question-image input, the Visual Commonsense Reasoning (VCR) model can predict an answer with the corresponding rationale, which requires inference ability from the real world. The VCR task, which calls for exploiting the multi-source information as well as learning different levels of understanding and extensive commonsense knowledge, is a cognition-level scene understanding task. The VCR task has aroused researchers' interest due to its wide range of applications, including visual question answering, automated vehicle systems, and clinical decision support. Previous approaches to solving the VCR task generally rely on pre-training or exploiting memory with long dependency relationship encoded models. However, these approaches suffer from a lack of generalizability and losing information in long sequences. In this paper, we propose a parallel attention-based cognitive VCR network PAVCR, which fuses visual-textual information efficiently and encodes semantic information in parallel to enable the model to capture rich information for cognition-level inference. Extensive experiments show that the proposed model yields significant improvements over existing methods on the benchmark VCR dataset. Moreover, the proposed model provides intuitive interpretation into visual commonsense reasoning.



### Deep Learning based Automatic Detection of Dicentric Chromosome
- **Arxiv ID**: http://arxiv.org/abs/2204.08029v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08029v1)
- **Published**: 2022-04-17 15:11:13+00:00
- **Updated**: 2022-04-17 15:11:13+00:00
- **Authors**: Angad Singh Wadhwa, Nikhil Tyagi, Pinaki Roy Chowdhury
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic detection of dicentric chromosomes is an essential step to estimate radiation exposure and development of end to end emergency bio dosimetry systems. During accidents, a large amount of data is required to be processed for extensive testing to formulate a medical treatment plan for the masses, which requires this process to be automated. Current approaches require human adjustments according to the data and therefore need a human expert to calibrate the system. This paper proposes a completely data driven framework which requires minimum intervention of field experts and can be deployed in emergency cases with relative ease. Our approach involves YOLOv4 to detect the chromosomes and remove the debris in each image, followed by a classifier that differentiates between an analysable chromosome and a non-analysable one. Images are extracted from YOLOv4 based on the protocols described by WHO-BIODOSNET. The analysable chromosome is classified as Monocentric or Dicentric and an image is accepted for consideration of dose estimation based on the analysable chromosome count. We report an accuracy in dicentric identification of 94.33% on a 1:1 split of Dicentric and Monocentric Chromosomes.



### An Adaptive Task-Related Component Analysis Method for SSVEP recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.08030v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2204.08030v1)
- **Published**: 2022-04-17 15:12:40+00:00
- **Updated**: 2022-04-17 15:12:40+00:00
- **Authors**: Vangelis P. Oikonomou
- **Comment**: 23 pages, 3 Figures, 6 Tables
- **Journal**: None
- **Summary**: Steady-state visual evoked potential (SSVEP) recognition methods are equipped with learning from the subject's calibration data, and they can achieve extra high performance in the SSVEP-based brain-computer interfaces (BCIs), however their performance deteriorate drastically if the calibration trials are insufficient. This study develops a new method to learn from limited calibration data and it proposes and evaluates a novel adaptive data-driven spatial filtering approach for enhancing SSVEPs detection. The spatial filter learned from each stimulus utilizes temporal information from the corresponding EEG trials. To introduce the temporal information into the overall procedure, an multitask learning approach, based on the bayesian framework, is adopted. The performance of the proposed method was evaluated into two publicly available benchmark datasets, and the results demonstrated that our method outperform competing methods by a significant margin.



### U-Net and its variants for Medical Image Segmentation : A short review
- **Arxiv ID**: http://arxiv.org/abs/2204.08470v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08470v1)
- **Published**: 2022-04-17 15:26:51+00:00
- **Updated**: 2022-04-17 15:26:51+00:00
- **Authors**: Vinay Ummadi
- **Comment**: None
- **Journal**: None
- **Summary**: The paper is a short review of medical image segmentation using U-Net and its variants. As we understand going through a medical images is not an easy job for any clinician either radiologist or pathologist. Analysing medical images is the only way to perform non-invasive diagnosis. Segmenting out the regions of interest has significant importance in medical images and is key for diagnosis. This paper also gives a bird eye view of how medical image segmentation has evolved. Also discusses challenge's and success of the deep neural architectures. Following how different hybrid architectures have built upon strong techniques from visual recognition tasks. In the end we will see current challenges and future directions for medical image segmentation(MIS).



### NICO++: Towards Better Benchmarking for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2204.08040v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.08040v2)
- **Published**: 2022-04-17 15:57:12+00:00
- **Updated**: 2022-04-21 11:44:33+00:00
- **Authors**: Xingxuan Zhang, Yue He, Renzhe Xu, Han Yu, Zheyan Shen, Peng Cui
- **Comment**: The NICO challenge based on NICO++ can be found at
  https://nicochallenge.com/
- **Journal**: None
- **Summary**: Despite the remarkable performance that modern deep neural networks have achieved on independent and identically distributed (I.I.D.) data, they can crash under distribution shifts. Most current evaluation methods for domain generalization (DG) adopt the leave-one-out strategy as a compromise on the limited number of domains. We propose a large-scale benchmark with extensive labeled domains named NICO++ along with more rational evaluation methods for comprehensively evaluating DG algorithms. To evaluate DG datasets, we propose two metrics to quantify covariate shift and concept shift, respectively. Two novel generalization bounds from the perspective of data construction are proposed to prove that limited concept shift and significant covariate shift favor the evaluation capability for generalization. Through extensive experiments, NICO++ shows its superior evaluation capability compared with current DG datasets and its contribution in alleviating unfairness caused by the leak of oracle knowledge in model selection.



### Continual Hippocampus Segmentation with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2204.08043v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08043v1)
- **Published**: 2022-04-17 16:13:04+00:00
- **Updated**: 2022-04-17 16:13:04+00:00
- **Authors**: Amin Ranem, Camila González, Anirban Mukhopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: In clinical settings, where acquisition conditions and patient populations change over time, continual learning is key for ensuring the safe use of deep neural networks. Yet most existing work focuses on convolutional architectures and image classification. Instead, radiologists prefer to work with segmentation models that outline specific regions-of-interest, for which Transformer-based architectures are gaining traction. The self-attention mechanism of Transformers could potentially mitigate catastrophic forgetting, opening the way for more robust medical image segmentation. In this work, we explore how recently-proposed Transformer mechanisms for semantic segmentation behave in sequential learning scenarios, and analyse how best to adapt continual learning strategies for this setting. Our evaluation on hippocampus segmentation shows that Transformer mechanisms mitigate catastrophic forgetting for medical image segmentation compared to purely convolutional architectures, and demonstrates that regularising ViT modules should be done with caution.



### A Pre-study on Data Processing Pipelines for Roadside Object Detection Systems Towards Safer Road Infrastructure
- **Arxiv ID**: http://arxiv.org/abs/2205.01783v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.01783v1)
- **Published**: 2022-04-17 16:27:26+00:00
- **Updated**: 2022-04-17 16:27:26+00:00
- **Authors**: Yinan Yu, Samuel Scheidegger, John-Fredrik Grönvall, Magnus Palm, Erik Svanberg, Johan Amoruso Wennerby, Jörg Bakker
- **Comment**: None
- **Journal**: None
- **Summary**: Single-vehicle accidents are the most common type of fatal accidents in Sweden, where a car drives off the road and runs into hazardous roadside objects. Proper installation and maintenance of protective objects, such as crash cushions and guard rails, may reduce the chance and severity of such accidents. Moreover, efficient detection and management of hazardous roadside objects also plays an important role in improving road safety. To better understand the state-of-the-art and system requirements, in this pre-study, we investigate the feasibility, implementation, limitations and scaling up of data processing pipelines for roadside object detection. In particular, we divide our investigation into three parts: the target of interest, the sensors of choice and the algorithm design. The data sources we consider in this study cover two common setups: 1) road surveying fleet - annual scans conducted by Trafikverket, the Swedish Transport Administration, and 2) consumer vehicle - data collected using a research vehicle from the laboratory of Resource for vehicle research at Chalmers (REVERE). The goal of this report is to investigate how to implement a scalable roadside object detection system towards safe road infrastructure and Sweden's Vision Zero.



### MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration
- **Arxiv ID**: http://arxiv.org/abs/2204.08058v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.08058v3)
- **Published**: 2022-04-17 17:59:09+00:00
- **Updated**: 2022-04-28 14:32:18+00:00
- **Authors**: Thomas Hayes, Songyang Zhang, Xi Yin, Guan Pang, Sasha Sheng, Harry Yang, Songwei Ge, Qiyuan Hu, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal video-audio-text understanding and generation can benefit from datasets that are narrow but rich. The narrowness allows bite-sized challenges that the research community can make progress on. The richness ensures we are making progress along the core challenges. To this end, we present a large-scale video-audio-text dataset MUGEN, collected using the open-sourced platform game CoinRun [11]. We made substantial modifications to make the game richer by introducing audio and enabling new interactions. We trained RL agents with different objectives to navigate the game and interact with 13 objects and characters. This allows us to automatically extract a large collection of diverse videos and associated audio. We sample 375K video clips (3.2s each) and collect text descriptions from human annotators. Each video has additional annotations that are extracted automatically from the game engine, such as accurate semantic maps for each frame and templated textual descriptions. Altogether, MUGEN can help progress research in many tasks in multimodal understanding and generation. We benchmark representative approaches on tasks involving video-audio-text retrieval and generation. Our dataset and code are released at: https://mugen-org.github.io/.



### Learning 3D Semantics from Pose-Noisy 2D Images with Hierarchical Full Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2204.08084v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08084v3)
- **Published**: 2022-04-17 20:24:26+00:00
- **Updated**: 2022-04-27 08:32:58+00:00
- **Authors**: Yuhang He, Lin Chen, Junkun Xie, Long Chen
- **Comment**: 3D Point Cloud Semantic Segmentation, Semantic Aggregation from
  Images, Pose Noise
- **Journal**: None
- **Summary**: We propose a novel framework to learn 3D point cloud semantics from 2D multi-view image observations containing pose error. On the one hand, directly learning from the massive, unstructured and unordered 3D point cloud is computationally and algorithmically more difficult than learning from compactly-organized and context-rich 2D RGB images. On the other hand, both LiDAR point cloud and RGB images are captured in standard automated-driving datasets. This motivates us to conduct a "task transfer" paradigm so that 3D semantic segmentation benefits from aggregating 2D semantic cues, albeit pose noises are contained in 2D image observations. Among all difficulties, pose noise and erroneous prediction from 2D semantic segmentation approaches are the main challenges for the task transfer. To alleviate the influence of those factor, we perceive each 3D point using multi-view images and for each single image a patch observation is associated. Moreover, the semantic labels of a block of neighboring 3D points are predicted simultaneously, enabling us to exploit the point structure prior to further improve the performance. A hierarchical full attention network~(HiFANet) is designed to sequentially aggregates patch, bag-of-frames and inter-point semantic cues, with hierarchical attention mechanism tailored for different level of semantic cues. Also, each preceding attention block largely reduces the feature size before feeding to the next attention block, making our framework slim. Experiment results on Semantic-KITTI show that the proposed framework outperforms existing 3D point cloud based methods significantly, it requires much less training data and exhibits tolerance to pose noise. The code is available at https://github.com/yuhanghe01/HiFANet.



### Learning Compositional Representations for Effective Low-Shot Generalization
- **Arxiv ID**: http://arxiv.org/abs/2204.08090v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.08090v1)
- **Published**: 2022-04-17 21:31:11+00:00
- **Updated**: 2022-04-17 21:31:11+00:00
- **Authors**: Samarth Mishra, Pengkai Zhu, Venkatesh Saligrama
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Recognition as Part Composition (RPC), an image encoding approach inspired by human cognition. It is based on the cognitive theory that humans recognize complex objects by components, and that they build a small compact vocabulary of concepts to represent each instance with. RPC encodes images by first decomposing them into salient parts, and then encoding each part as a mixture of a small number of prototypes, each representing a certain concept. We find that this type of learning inspired by human cognition can overcome hurdles faced by deep convolutional networks in low-shot generalization tasks, like zero-shot learning, few-shot learning and unsupervised domain adaptation. Furthermore, we find a classifier using an RPC image encoder is fairly robust to adversarial attacks, that deep neural networks are known to be prone to. Given that our image encoding principle is based on human cognition, one would expect the encodings to be interpretable by humans, which we find to be the case via crowd-sourcing experiments. Finally, we propose an application of these interpretable encodings in the form of generating synthetic attribute annotations for evaluating zero-shot learning methods on new datasets.



### Synthetic Distracted Driving (SynDD2) dataset for analyzing distracted behaviors and various gaze zones of a driver
- **Arxiv ID**: http://arxiv.org/abs/2204.08096v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.08096v3)
- **Published**: 2022-04-17 22:31:41+00:00
- **Updated**: 2023-04-10 07:11:01+00:00
- **Authors**: Mohammed Shaiqur Rahman, Jiyang Wang, Senem Velipasalar Gursoy, David Anastasiu, Shuo Wang, Anuj Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: This article presents a synthetic distracted driving (SynDD2 - a continuum of SynDD1) dataset for machine learning models to detect and analyze drivers' various distracted behavior and different gaze zones. We collected the data in a stationary vehicle using three in-vehicle cameras positioned at locations: on the dashboard, near the rearview mirror, and on the top right-side window corner. The dataset contains two activity types: distracted activities and gaze zones for each participant, and each activity type has two sets: without appearance blocks and with appearance blocks such as wearing a hat or sunglasses. The order and duration of each activity for each participant are random. In addition, the dataset contains manual annotations for each activity, having its start and end time annotated. Researchers could use this dataset to evaluate the performance of machine learning algorithms to classify various distracting activities and gaze zones of drivers.



### Exploiting Embodied Simulation to Detect Novel Object Classes Through Interaction
- **Arxiv ID**: http://arxiv.org/abs/2204.08107v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.08107v1)
- **Published**: 2022-04-17 23:16:55+00:00
- **Updated**: 2022-04-17 23:16:55+00:00
- **Authors**: Nikhil Krishnaswamy, Sadaf Ghaffari
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a novel method for a naive agent to detect novel objects it encounters in an interaction. We train a reinforcement learning policy on a stacking task given a known object type, and then observe the results of the agent attempting to stack various other objects based on the same trained policy. By extracting embedding vectors from a convolutional neural net trained over the results of the aforementioned stacking play, we can determine the similarity of a given object to known object types, and determine if the given object is likely dissimilar enough to the known types to be considered a novel class of object. We present the results of this method on two datasets gathered using two different policies and demonstrate what information the agent needs to extract from its environment to make these novelty judgments.



