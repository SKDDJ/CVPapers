# Arxiv Papers in cs.CV on 2022-04-09
### Searching for Efficient Neural Architectures for On-Device ML on Edge TPUs
- **Arxiv ID**: http://arxiv.org/abs/2204.14007v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.14007v1)
- **Published**: 2022-04-09 00:35:19+00:00
- **Updated**: 2022-04-09 00:35:19+00:00
- **Authors**: Berkin Akin, Suyog Gupta, Yun Long, Anton Spiridonov, Zhuo Wang, Marie White, Hao Xu, Ping Zhou, Yanqi Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: On-device ML accelerators are becoming a standard in modern mobile system-on-chips (SoC). Neural architecture search (NAS) comes to the rescue for efficiently utilizing the high compute throughput offered by these accelerators. However, existing NAS frameworks have several practical limitations in scaling to multiple tasks and different target platforms. In this work, we provide a two-pronged approach to this challenge: (i) a NAS-enabling infrastructure that decouples model cost evaluation, search space design, and the NAS algorithm to rapidly target various on-device ML tasks, and (ii) search spaces crafted from group convolution based inverted bottleneck (IBN) variants that provide flexible quality/performance trade-offs on ML accelerators, complementing the existing full and depthwise convolution based IBNs. Using this approach we target a state-of-the-art mobile platform, Google Tensor SoC, and demonstrate neural architectures that improve the quality-performance pareto frontier for various computer vision (classification, detection, segmentation) as well as natural language processing tasks.



### Segmenting across places: The need for fair transfer learning with satellite imagery
- **Arxiv ID**: http://arxiv.org/abs/2204.04358v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04358v3)
- **Published**: 2022-04-09 02:14:56+00:00
- **Updated**: 2022-04-15 15:04:14+00:00
- **Authors**: Miao Zhang, Harvineet Singh, Lazarus Chok, Rumi Chunara
- **Comment**: None
- **Journal**: None
- **Summary**: The increasing availability of high-resolution satellite imagery has enabled the use of machine learning to support land-cover measurement and inform policy-making. However, labelling satellite images is expensive and is available for only some locations. This prompts the use of transfer learning to adapt models from data-rich locations to others. Given the potential for high-impact applications of satellite imagery across geographies, a systematic assessment of transfer learning implications is warranted. In this work, we consider the task of land-cover segmentation and study the fairness implications of transferring models across locations. We leverage a large satellite image segmentation benchmark with 5987 images from 18 districts (9 urban and 9 rural). Via fairness metrics we quantify disparities in model performance along two axes -- across urban-rural locations and across land-cover classes. Findings show that state-of-the-art models have better overall accuracy in rural areas compared to urban areas, through unsupervised domain adaptation methods transfer learning better to urban versus rural areas and enlarge fairness gaps. In analysis of reasons for these findings, we show that raw satellite images are overall more dissimilar between source and target districts for rural than for urban locations. This work highlights the need to conduct fairness analysis for satellite imagery segmentation models and motivates the development of methods for fair transfer learning in order not to introduce disparities between places, particularly urban and rural locations.



### Attention guided global enhancement and local refinement network for semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.04363v1
- **DOI**: 10.1109/TIP.2022.3166673
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2204.04363v1)
- **Published**: 2022-04-09 02:32:24+00:00
- **Updated**: 2022-04-09 02:32:24+00:00
- **Authors**: Jiangyun Li, Sen Zha, Chen Chen, Meng Ding, Tianxiang Zhang, Hong Yu
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: The encoder-decoder architecture is widely used as a lightweight semantic segmentation network. However, it struggles with a limited performance compared to a well-designed Dilated-FCN model for two major problems. First, commonly used upsampling methods in the decoder such as interpolation and deconvolution suffer from a local receptive field, unable to encode global contexts. Second, low-level features may bring noises to the network decoder through skip connections for the inadequacy of semantic concepts in early encoder layers. To tackle these challenges, a Global Enhancement Method is proposed to aggregate global information from high-level feature maps and adaptively distribute them to different decoder layers, alleviating the shortage of global contexts in the upsampling process. Besides, a Local Refinement Module is developed by utilizing the decoder features as the semantic guidance to refine the noisy encoder features before the fusion of these two (the decoder features and the encoder features). Then, the two methods are integrated into a Context Fusion Block, and based on that, a novel Attention guided Global enhancement and Local refinement Network (AGLN) is elaborately designed. Extensive experiments on PASCAL Context, ADE20K, and PASCAL VOC 2012 datasets have demonstrated the effectiveness of the proposed approach. In particular, with a vanilla ResNet-101 backbone, AGLN achieves the state-of-the-art result (56.23% mean IoU) on the PASCAL Context dataset. The code is available at https://github.com/zhasen1996/AGLN.



### Channel Pruning In Quantization-aware Training: An Adaptive Projection-gradient Descent-shrinkage-splitting Method
- **Arxiv ID**: http://arxiv.org/abs/2204.04375v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2204.04375v1)
- **Published**: 2022-04-09 03:13:16+00:00
- **Updated**: 2022-04-09 03:13:16+00:00
- **Authors**: Zhijian Li, Jack Xin
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an adaptive projection-gradient descent-shrinkage-splitting method (APGDSSM) to integrate penalty based channel pruning into quantization-aware training (QAT). APGDSSM concurrently searches weights in both the quantized subspace and the sparse subspace. APGDSSM uses shrinkage operator and a splitting technique to create sparse weights, as well as the Group Lasso penalty to push the weight sparsity into channel sparsity. In addition, we propose a novel complementary transformed l1 penalty to stabilize the training for extreme compression.



### Robotic Surgery Remote Mentoring via AR with 3D Scene Streaming and Hand Interaction
- **Arxiv ID**: http://arxiv.org/abs/2204.04377v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04377v1)
- **Published**: 2022-04-09 03:17:15+00:00
- **Updated**: 2022-04-09 03:17:15+00:00
- **Authors**: Yonghao Long, Chengkun Li, Qi Dou
- **Comment**: None
- **Journal**: None
- **Summary**: With the growing popularity of robotic surgery, education becomes increasingly important and urgently needed for the sake of patient safety. However, experienced surgeons have limited accessibility due to their busy clinical schedule or working in a distant city, thus can hardly provide sufficient education resources for novices. Remote mentoring, as an effective way, can help solve this problem, but traditional methods are limited to plain text, audio, or 2D video, which are not intuitive nor vivid. Augmented reality (AR), a thriving technique being widely used for various education scenarios, is promising to offer new possibilities of visual experience and interactive teaching. In this paper, we propose a novel AR-based robotic surgery remote mentoring system with efficient 3D scene visualization and natural 3D hand interaction. Using a head-mounted display (i.e., HoloLens), the mentor can remotely monitor the procedure streamed from the trainee's operation side. The mentor can also provide feedback directly with hand gestures, which is in-turn transmitted to the trainee and viewed in surgical console as guidance. We comprehensively validate the system on both real surgery stereo videos and ex-vivo scenarios of common robotic training tasks (i.e., peg-transfer and suturing). Promising results are demonstrated regarding the fidelity of streamed scene visualization, the accuracy of feedback with hand interaction, and the low-latency of each component in the entire remote mentoring system. This work showcases the feasibility of leveraging AR technology for reliable, flexible and low-cost solutions to robotic surgical education, and holds great potential for clinical applications.



### Beyond 3DMM: Learning to Capture High-fidelity 3D Face Shape
- **Arxiv ID**: http://arxiv.org/abs/2204.04379v1
- **DOI**: 10.1109/TPAMI.2022.3164131
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04379v1)
- **Published**: 2022-04-09 03:46:18+00:00
- **Updated**: 2022-04-09 03:46:18+00:00
- **Authors**: Xiangyu Zhu, Chang Yu, Di Huang, Zhen Lei, Hao Wang, Stan Z. Li
- **Comment**: Accepted by T-PAMI 2022. see IEEE page
  http://ieeexplore.ieee.org/document/9748011
- **Journal**: None
- **Summary**: 3D Morphable Model (3DMM) fitting has widely benefited face analysis due to its strong 3D priori. However, previous reconstructed 3D faces suffer from degraded visual verisimilitude due to the loss of fine-grained geometry, which is attributed to insufficient ground-truth 3D shapes, unreliable training strategies and limited representation power of 3DMM. To alleviate this issue, this paper proposes a complete solution to capture the personalized shape so that the reconstructed shape looks identical to the corresponding person. Specifically, given a 2D image as the input, we virtually render the image in several calibrated views to normalize pose variations while preserving the original image geometry. A many-to-one hourglass network serves as the encode-decoder to fuse multiview features and generate vertex displacements as the fine-grained geometry. Besides, the neural network is trained by directly optimizing the visual effect, where two 3D shapes are compared by measuring the similarity between the multiview images rendered from the shapes. Finally, we propose to generate the ground-truth 3D shapes by registering RGB-D images followed by pose and shape augmentation, providing sufficient data for network training. Experiments on several challenging protocols demonstrate the superior reconstruction accuracy of our proposal on the face shape.



### A dataset of ant colonies motion trajectories in indoor and outdoor scenes for social cluster behavior study
- **Arxiv ID**: http://arxiv.org/abs/2204.04380v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04380v1)
- **Published**: 2022-04-09 03:49:55+00:00
- **Updated**: 2022-04-09 03:49:55+00:00
- **Authors**: Meihong Wu, Xiaoyan Cao, Xiaoyu Cao, Shihui Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Motion and interaction of social insects (such as ants) have been studied by many researchers to understand the clustering mechanism. Most studies in the field of ant behavior have only focused on indoor environments, while outdoor environments are still underexplored. In this paper, we collect 10 videos of ant colonies from different indoor and outdoor scenes. And we develop an image sequence marking software named VisualMarkData, which enables us to provide annotations of ants in the video. In all 5354 frames, the location information and the identification number of each ant are recorded for a total of 712 ants and 114112 annotations. Moreover, we provide visual analysis tools to assess and validate the technical quality and reproducibility of our data. It is hoped that this dataset will contribute to a deeper exploration on the behavior of the ant colony.



### Federated Unsupervised Domain Adaptation for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.04382v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2204.04382v1)
- **Published**: 2022-04-09 04:02:03+00:00
- **Updated**: 2022-04-09 04:02:03+00:00
- **Authors**: Weiming Zhuang, Xin Gan, Yonggang Wen, Xuesen Zhang, Shuai Zhang, Shuai Yi
- **Comment**: ICME'22. arXiv admin note: substantial text overlap with
  arXiv:2105.07606
- **Journal**: None
- **Summary**: Given labeled data in a source domain, unsupervised domain adaptation has been widely adopted to generalize models for unlabeled data in a target domain, whose data distributions are different. However, existing works are inapplicable to face recognition under privacy constraints because they require sharing of sensitive face images between domains. To address this problem, we propose federated unsupervised domain adaptation for face recognition, FedFR. FedFR jointly optimizes clustering-based domain adaptation and federated learning to elevate performance on the target domain. Specifically, for unlabeled data in the target domain, we enhance a clustering algorithm with distance constrain to improve the quality of predicted pseudo labels. Besides, we propose a new domain constraint loss (DCL) to regularize source domain training in federated learning. Extensive experiments on a newly constructed benchmark demonstrate that FedFR outperforms the baseline and classic methods on the target domain by 3% to 14% on different evaluation metrics.



### The Two Dimensions of Worst-case Training and the Integrated Effect for Out-of-domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2204.04384v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04384v1)
- **Published**: 2022-04-09 04:14:55+00:00
- **Updated**: 2022-04-09 04:14:55+00:00
- **Authors**: Zeyi Huang, Haohan Wang, Dong Huang, Yong Jae Lee, Eric P. Xing
- **Comment**: to appear at CVPR2022
- **Journal**: None
- **Summary**: Training with an emphasis on "hard-to-learn" components of the data has been proven as an effective method to improve the generalization of machine learning models, especially in the settings where robustness (e.g., generalization across distributions) is valued. Existing literature discussing this "hard-to-learn" concept are mainly expanded either along the dimension of the samples or the dimension of the features. In this paper, we aim to introduce a simple view merging these two dimensions, leading to a new, simple yet effective, heuristic to train machine learning models by emphasizing the worst-cases on both the sample and the feature dimensions. We name our method W2D following the concept of "Worst-case along Two Dimensions". We validate the idea and demonstrate its empirical strength over standard benchmarks.



### Divergence-aware Federated Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.04385v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2204.04385v1)
- **Published**: 2022-04-09 04:15:02+00:00
- **Updated**: 2022-04-09 04:15:02+00:00
- **Authors**: Weiming Zhuang, Yonggang Wen, Shuai Zhang
- **Comment**: ICLR'22
- **Journal**: None
- **Summary**: Self-supervised learning (SSL) is capable of learning remarkable representations from centrally available data. Recent works further implement federated learning with SSL to learn from rapidly growing decentralized unlabeled images (e.g., from cameras and phones), often resulted from privacy constraints. Extensive attention has been paid to SSL approaches based on Siamese networks. However, such an effort has not yet revealed deep insights into various fundamental building blocks for the federated self-supervised learning (FedSSL) architecture. We aim to fill in this gap via in-depth empirical study and propose a new method to tackle the non-independently and identically distributed (non-IID) data problem of decentralized data. Firstly, we introduce a generalized FedSSL framework that embraces existing SSL methods based on Siamese networks and presents flexibility catering to future methods. In this framework, a server coordinates multiple clients to conduct SSL training and periodically updates local models of clients with the aggregated global model. Using the framework, our study uncovers unique insights of FedSSL: 1) stop-gradient operation, previously reported to be essential, is not always necessary in FedSSL; 2) retaining local knowledge of clients in FedSSL is particularly beneficial for non-IID data. Inspired by the insights, we then propose a new approach for model update, Federated Divergence-aware Exponential Moving Average update (FedEMA). FedEMA updates local models of clients adaptively using EMA of the global model, where the decay rate is dynamically measured by model divergence. Extensive experiments demonstrate that FedEMA outperforms existing methods by 3-4% on linear evaluation. We hope that this work will provide useful insights for future research.



### Dual-Stage Approach Toward Hyperspectral Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2204.04387v1
- **DOI**: 10.1109/TIP.2022.3221287
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04387v1)
- **Published**: 2022-04-09 04:36:44+00:00
- **Updated**: 2022-04-09 04:36:44+00:00
- **Authors**: Qiang Li, Yuan Yuan, Xiuping Jia, Qi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral image produces high spectral resolution at the sacrifice of spatial resolution. Without reducing the spectral resolution, improving the resolution in the spatial domain is a very challenging problem. Motivated by the discovery that hyperspectral image exhibits high similarity between adjacent bands in a large spectral range, in this paper, we explore a new structure for hyperspectral image super-resolution (DualSR), leading to a dual-stage design, i.e., coarse stage and fine stage. In coarse stage, five bands with high similarity in a certain spectral range are divided into three groups, and the current band is guided to study the potential knowledge. Under the action of alternative spectral fusion mechanism, the coarse SR image is super-resolved in band-by-band. In order to build model from a global perspective, an enhanced back-projection method via spectral angle constraint is developed in fine stage to learn the content of spatial-spectral consistency, dramatically improving the performance gain. Extensive experiments demonstrate the effectiveness of the proposed coarse stage and fine stage. Besides, our network produces state-of-the-art results against existing works in terms of spatial reconstruction and spectral fidelity.



### E^2TAD: An Energy-Efficient Tracking-based Action Detector
- **Arxiv ID**: http://arxiv.org/abs/2204.04416v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04416v4)
- **Published**: 2022-04-09 07:52:11+00:00
- **Updated**: 2022-10-30 03:45:04+00:00
- **Authors**: Xin Hu, Zhenyu Wu, Hao-Yu Miao, Siqi Fan, Taiyu Long, Zhenyu Hu, Pengcheng Pi, Yi Wu, Zhou Ren, Zhangyang Wang, Gang Hua
- **Comment**: None
- **Journal**: None
- **Summary**: Video action detection (spatio-temporal action localization) is usually the starting point for human-centric intelligent analysis of videos nowadays. It has high practical impacts for many applications across robotics, security, healthcare, etc. The two-stage paradigm of Faster R-CNN inspires a standard paradigm of video action detection in object detection, i.e., firstly generating person proposals and then classifying their actions. However, none of the existing solutions could provide fine-grained action detection to the "who-when-where-what" level. This paper presents a tracking-based solution to accurately and efficiently localize predefined key actions spatially (by predicting the associated target IDs and locations) and temporally (by predicting the time in exact frame indices). This solution won first place in the UAV-Video Track of 2021 Low-Power Computer Vision Challenge (LPCVC).



### Mapping Temporary Slums from Satellite Imagery using a Semi-Supervised Approach
- **Arxiv ID**: http://arxiv.org/abs/2204.04419v1
- **DOI**: 10.1109/LGRS.2022.3180162
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04419v1)
- **Published**: 2022-04-09 08:02:32+00:00
- **Updated**: 2022-04-09 08:02:32+00:00
- **Authors**: M. Fasi ur Rehman, Izza Ali, Waqas Sultani, Mohsen Ali
- **Comment**: None
- **Journal**: None
- **Summary**: One billion people worldwide are estimated to be living in slums, and documenting and analyzing these regions is a challenging task. As compared to regular slums; the small, scattered and temporary nature of temporary slums makes data collection and labeling tedious and time-consuming. To tackle this challenging problem of temporary slums detection, we present a semi-supervised deep learning segmentation-based approach; with the strategy to detect initial seed images in the zero-labeled data settings. A small set of seed samples (32 in our case) are automatically discovered by analyzing the temporal changes, which are manually labeled to train a segmentation and representation learning module. The segmentation module gathers high dimensional image representations, and the representation learning module transforms image representations into embedding vectors. After that, a scoring module uses the embedding vectors to sample images from a large pool of unlabeled images and generates pseudo-labels for the sampled images. These sampled images with their pseudo-labels are added to the training set to update the segmentation and representation learning modules iteratively. To analyze the effectiveness of our technique, we construct a large geographically marked dataset of temporary slums. This dataset constitutes more than 200 potential temporary slum locations (2.28 square kilometers) found by sieving sixty-eight thousand images from 12 metropolitan cities of Pakistan covering 8000 square kilometers. Furthermore, our proposed method outperforms several competitive semi-supervised semantic segmentation baselines on a similar setting. The code and the dataset will be made publicly available.



### Unbiased Directed Object Attention Graph for Object Navigation
- **Arxiv ID**: http://arxiv.org/abs/2204.04421v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.04421v2)
- **Published**: 2022-04-09 08:13:05+00:00
- **Updated**: 2022-07-08 01:41:38+00:00
- **Authors**: Ronghao Dang, Zhuofan Shi, Liuyi Wang, Zongtao He, Chengju Liu, Qijun Chen
- **Comment**: 13 pages, accepted by ACM Mutimedia 2022
- **Journal**: None
- **Summary**: Object navigation tasks require agents to locate specific objects in unknown environments based on visual information. Previously, graph convolutions were used to implicitly explore the relationships between objects. However, due to differences in visibility among objects, it is easy to generate biases in object attention. Thus, in this paper, we propose a directed object attention (DOA) graph to guide the agent in explicitly learning the attention relationships between objects, thereby reducing the object attention bias. In particular, we use the DOA graph to perform unbiased adaptive object attention (UAOA) on the object features and unbiased adaptive image attention (UAIA) on the raw images, respectively. To distinguish features in different branches, a concise adaptive branch energy distribution (ABED) method is proposed. We assess our methods on the AI2-Thor dataset. Compared with the state-of-the-art (SOTA) method, our method reports 7.4%, 8.1% and 17.6% increase in success rate (SR), success weighted by path length (SPL) and success weighted by action efficiency (SAE), respectively.



### Adaptive Differential Filters for Fast and Communication-Efficient Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.04424v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2204.04424v1)
- **Published**: 2022-04-09 08:23:25+00:00
- **Updated**: 2022-04-09 08:23:25+00:00
- **Authors**: Daniel Becking, Heiner Kirchhoffer, Gerhard Tech, Paul Haase, Karsten Müller, Heiko Schwarz, Wojciech Samek
- **Comment**: CVPR 2022 FedVision Workshop (CVPRW), 12 pages, 5 figures, 2 tables,
  supplementary material
- **Journal**: None
- **Summary**: Federated learning (FL) scenarios inherently generate a large communication overhead by frequently transmitting neural network updates between clients and server. To minimize the communication cost, introducing sparsity in conjunction with differential updates is a commonly used technique. However, sparse model updates can slow down convergence speed or unintentionally skip certain update aspects, e.g., learned features, if error accumulation is not properly addressed. In this work, we propose a new scaling method operating at the granularity of convolutional filters which 1) compensates for highly sparse updates in FL processes, 2) adapts the local models to new data domains by enhancing some features in the filter space while diminishing others and 3) motivates extra sparsity in updates and thus achieves higher compression ratios, i.e., savings in the overall data transfer. Compared to unscaled updates and previous work, experimental results on different computer vision tasks (Pascal VOC, CIFAR10, Chest X-Ray) and neural networks (ResNets, MobileNets, VGGs) in uni-, bidirectional and partial update FL settings show that the proposed method improves the performance of the central server model while converging faster and reducing the total amount of transmitted data by up to 377 times.



### ManiTrans: Entity-Level Text-Guided Image Manipulation via Token-wise Semantic Alignment and Generation
- **Arxiv ID**: http://arxiv.org/abs/2204.04428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04428v1)
- **Published**: 2022-04-09 09:01:19+00:00
- **Updated**: 2022-04-09 09:01:19+00:00
- **Authors**: Jianan Wang, Guansong Lu, Hang Xu, Zhenguo Li, Chunjing Xu, Yanwei Fu
- **Comment**: Accepted by CVPR2022 (Oral)
- **Journal**: None
- **Summary**: Existing text-guided image manipulation methods aim to modify the appearance of the image or to edit a few objects in a virtual or simple scenario, which is far from practical application. In this work, we study a novel task on text-guided image manipulation on the entity level in the real world. The task imposes three basic requirements, (1) to edit the entity consistent with the text descriptions, (2) to preserve the text-irrelevant regions, and (3) to merge the manipulated entity into the image naturally. To this end, we propose a new transformer-based framework based on the two-stage image synthesis method, namely \textbf{ManiTrans}, which can not only edit the appearance of entities but also generate new entities corresponding to the text guidance. Our framework incorporates a semantic alignment module to locate the image regions to be manipulated, and a semantic loss to help align the relationship between the vision and language. We conduct extensive experiments on the real datasets, CUB, Oxford, and COCO datasets to verify that our method can distinguish the relevant and irrelevant regions and achieve more precise and flexible manipulation compared with baseline methods. The project homepage is \url{https://jawang19.github.io/manitrans}.



### HSTR-Net: High Spatio-Temporal Resolution Video Generation For Wide Area Surveillance
- **Arxiv ID**: http://arxiv.org/abs/2204.04435v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04435v1)
- **Published**: 2022-04-09 09:23:58+00:00
- **Updated**: 2022-04-09 09:23:58+00:00
- **Authors**: H. Umut Suluhan, Hasan F. Ates, Bahadir K. Gunturk
- **Comment**: None
- **Journal**: None
- **Summary**: Wide area surveillance has many applications and tracking of objects under observation is an important task, which often needs high spatio-temporal resolution (HSTR) video for better precision. This paper presents the usage of multiple video feeds for the generation of HSTR video as an extension of reference based super resolution (RefSR). One feed captures video at high spatial resolution with low frame rate (HSLF) while the other captures low spatial resolution and high frame rate (LSHF) video simultaneously for the same scene. The main purpose is to create an HSTR video from the fusion of HSLF and LSHF videos. In this paper we propose an end-to-end trainable deep network that performs optical flow estimation and frame reconstruction by combining inputs from both video feeds. The proposed architecture provides significant improvement over existing video frame interpolation and RefSR techniques in terms of objective PSNR and SSIM metrics.



### Guided deep learning by subaperture decomposition: ocean patterns from SAR imagery
- **Arxiv ID**: http://arxiv.org/abs/2204.04438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04438v1)
- **Published**: 2022-04-09 09:49:05+00:00
- **Updated**: 2022-04-09 09:49:05+00:00
- **Authors**: Nicolae-Catalin Ristea, Andrei Anghel, Mihai Datcu, Bertrand Chapron
- **Comment**: None
- **Journal**: None
- **Summary**: Spaceborne synthetic aperture radar can provide meters scale images of the ocean surface roughness day or night in nearly all weather conditions. This makes it a unique asset for many geophysical applications. Sentinel 1 SAR wave mode vignettes have made possible to capture many important oceanic and atmospheric phenomena since 2014. However, considering the amount of data provided, expanding applications requires a strategy to automatically process and extract geophysical parameters. In this study, we propose to apply subaperture decomposition as a preprocessing stage for SAR deep learning models. Our data centring approach surpassed the baseline by 0.7, obtaining state of the art on the TenGeoPSARwv data set. In addition, we empirically showed that subaperture decomposition could bring additional information over the original vignette, by rising the number of clusters for an unsupervised segmentation method. Overall, we encourage the development of data centring approaches, showing that, data preprocessing could bring significant performance improvements over existing deep learning models.



### Noise-based Enhancement for Foveated Rendering
- **Arxiv ID**: http://arxiv.org/abs/2204.04455v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04455v1)
- **Published**: 2022-04-09 12:00:28+00:00
- **Updated**: 2022-04-09 12:00:28+00:00
- **Authors**: Taimoor Tariq, Cara Tursun, Piotr Didyk
- **Comment**: 14 pages including refences
- **Journal**: None
- **Summary**: Human visual sensitivity to spatial details declines towards the periphery. Novel image synthesis techniques, so-called foveated rendering, exploit this observation and reduce the spatial resolution of synthesized images for the periphery, avoiding the synthesis of high-spatial-frequency details that are costly to generate but not perceived by a viewer. However, contemporary techniques do not make a clear distinction between the range of spatial frequencies that must be reproduced and those that can be omitted. For a given eccentricity, there is a range of frequencies that are detectable but not resolvable. While the accurate reproduction of these frequencies is not required, an observer can detect their absence if completely omitted. We use this observation to improve the performance of existing foveated rendering techniques. We demonstrate that this specific range of frequencies can be efficiently replaced with procedural noise whose parameters are carefully tuned to image content and human perception. Consequently, these frequencies do not have to be synthesized during rendering, allowing more aggressive foveation, and they can be replaced by noise generated in a less expensive post-processing step, leading to improved performance of the rendering system. Our main contribution is a perceptually-inspired technique for deriving the parameters of the noise required for the enhancement and its calibration. The method operates on rendering output and runs at rates exceeding 200FPS at 4K resolution, making it suitable for integration with real-time foveated rendering systems for VR and AR devices. We validate our results and compare them to the existing contrast enhancement technique in user experiments.



### Refining time-space traffic diagrams: A multiple linear regression model
- **Arxiv ID**: http://arxiv.org/abs/2204.04457v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04457v3)
- **Published**: 2022-04-09 12:02:50+00:00
- **Updated**: 2023-01-20 07:05:59+00:00
- **Authors**: Zhengbing He
- **Comment**: None
- **Journal**: None
- **Summary**: A time-space traffic (TS) diagram, which presents traffic states in time-space cells with color, is an important traffic analysis and visualization tool. Despite its importance for transportation research and engineering, most TS diagrams that have already existed or are being produced are too coarse to exhibit detailed traffic dynamics due to the limitations of existing information technology and traffic infrastructure investment. To increase the resolution of a TS diagram and enable it to present ample traffic details, this paper introduces the TS diagram refinement problem and proposes a multiple linear regression-based model to solve the problem. Two tests, which attempt to increase the resolution of a TS diagram 4 and 16 times, are carried out to evaluate the performance of the proposed model. Data collected at different times, in different locations and even in different countries are employed to thoroughly evaluate the accuracy and transferability of the proposed model. Strict tests with diverse data show that the proposed model, despite its simplicity, is able to refine a TS diagram with promising accuracy and reliable transferability. The proposed refinement model will "save" widely existing TS diagrams from their blurry "faces" and enable TS diagrams to show more traffic details.



### A3CLNN: Spatial, Spectral and Multiscale Attention ConvLSTM Neural Network for Multisource Remote Sensing Data Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.04462v1
- **DOI**: 10.1109/TNNLS.2020.3028945
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04462v1)
- **Published**: 2022-04-09 12:43:32+00:00
- **Updated**: 2022-04-09 12:43:32+00:00
- **Authors**: Heng-Chao Li, Wen-Shuai Hu, Wei Li, Jun Li, Qian Du, Antonio Plaza
- **Comment**: 16 pages, 10 figures
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems, vol.
  33, no. 2, pp. 747-761, Feb. 2022
- **Summary**: The problem of effectively exploiting the information multiple data sources has become a relevant but challenging research topic in remote sensing. In this paper, we propose a new approach to exploit the complementarity of two data sources: hyperspectral images (HSIs) and light detection and ranging (LiDAR) data. Specifically, we develop a new dual-channel spatial, spectral and multiscale attention convolutional long short-term memory neural network (called dual-channel A3CLNN) for feature extraction and classification of multisource remote sensing data. Spatial, spectral and multiscale attention mechanisms are first designed for HSI and LiDAR data in order to learn spectral- and spatial-enhanced feature representations, and to represent multiscale information for different classes. In the designed fusion network, a novel composite attention learning mechanism (combined with a three-level fusion strategy) is used to fully integrate the features in these two data sources. Finally, inspired by the idea of transfer learning, a novel stepwise training strategy is designed to yield a final classification result. Our experimental results, conducted on several multisource remote sensing data sets, demonstrate that the newly proposed dual-channel A3CLNN exhibits better feature representation ability (leading to more competitive classification performance) than other state-of-the-art methods.



### Ultrasound Signal Processing: From Models to Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.04466v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04466v1)
- **Published**: 2022-04-09 13:04:36+00:00
- **Updated**: 2022-04-09 13:04:36+00:00
- **Authors**: Ben Luijten, Nishith Chennakeshava, Yonina C. Eldar, Massimo Mischi, Ruud J. G. van Sloun
- **Comment**: None
- **Journal**: None
- **Summary**: Medical ultrasound imaging relies heavily on high-quality signal processing algorithms to provide reliable and interpretable image reconstructions. Hand-crafted reconstruction methods, often based on approximations of the underlying measurement model, are useful in practice, but notoriously fall behind in terms of image quality. More sophisticated solutions, based on statistical modelling, careful parameter tuning, or through increased model complexity, can be sensitive to different environments. Recently, deep learning based methods have gained popularity, which are optimized in a data-driven fashion. These model-agnostic methods often rely on generic model structures, and require vast training data to converge to a robust solution. A relatively new paradigm combines the power of the two: leveraging data-driven deep learning, as well as exploiting domain knowledge. These model-based solutions yield high robustness, and require less trainable parameters and training data than conventional neural networks. In this work we provide an overview of these methods from the recent literature, and discuss a wide variety of ultrasound applications. We aim to inspire the reader to further research in this area, and to address the opportunities within the field of ultrasound signal processing. We conclude with a future perspective on these model-based deep learning techniques for medical ultrasound applications.



### S4OD: Semi-Supervised learning for Single-Stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.04492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04492v1)
- **Published**: 2022-04-09 15:19:37+00:00
- **Updated**: 2022-04-09 15:19:37+00:00
- **Authors**: Yueming Zhang, Xingxu Yao, Chao Liu, Feng Chen, Xiaolin Song, Tengfei Xing, Runbo Hu, Hua Chai, Pengfei Xu, Guoshan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Single-stage detectors suffer from extreme foreground-background class imbalance, while two-stage detectors do not. Therefore, in semi-supervised object detection, two-stage detectors can deliver remarkable performance by only selecting high-quality pseudo labels based on classification scores. However, directly applying this strategy to single-stage detectors would aggravate the class imbalance with fewer positive samples. Thus, single-stage detectors have to consider both quality and quantity of pseudo labels simultaneously. In this paper, we design a dynamic self-adaptive threshold (DSAT) strategy in classification branch, which can automatically select pseudo labels to achieve an optimal trade-off between quality and quantity. Besides, to assess the regression quality of pseudo labels in single-stage detectors, we propose a module to compute the regression uncertainty of boxes based on Non-Maximum Suppression. By leveraging only 10% labeled data from COCO, our method achieves 35.0% AP on anchor-free detector (FCOS) and 32.9% on anchor-based detector (RetinaNet).



### DeepLIIF: An Online Platform for Quantification of Clinical Pathology Slides
- **Arxiv ID**: http://arxiv.org/abs/2204.04494v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04494v1)
- **Published**: 2022-04-09 15:30:15+00:00
- **Updated**: 2022-04-09 15:30:15+00:00
- **Authors**: Parmida Ghahremani, Joseph Marino, Ricardo Dodds, Saad Nadeem
- **Comment**: CVPR 2022. First three authors contributed equally. Demo paper
  accompanying DeepLIIF Nature Machine Intelligence paper
  (https://www.nature.com/articles/s42256-022-00471-x)
- **Journal**: None
- **Summary**: In the clinic, resected tissue samples are stained with Hematoxylin-and-Eosin (H&E) and/or Immunhistochemistry (IHC) stains and presented to the pathologists on glass slides or as digital scans for diagnosis and assessment of disease progression. Cell-level quantification, e.g. in IHC protein expression scoring, can be extremely inefficient and subjective. We present DeepLIIF (https://deepliif.org), a first free online platform for efficient and reproducible IHC scoring. DeepLIIF outperforms current state-of-the-art approaches (relying on manual error-prone annotations) by virtually restaining clinical IHC slides with more informative multiplex immunofluorescence staining. Our DeepLIIF cloud-native platform supports (1) more than 150 proprietary/non-proprietary input formats via the Bio-Formats standard, (2) interactive adjustment, visualization, and downloading of the IHC quantification results and the accompanying restained images, (3) consumption of an exposed workflow API programmatically or through interactive plugins for open source whole slide image viewers such as QuPath/ImageJ, and (4) auto scaling to efficiently scale GPU resources based on user demand.



### On the Exploitation of Deepfake Model Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.04513v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04513v1)
- **Published**: 2022-04-09 16:48:23+00:00
- **Updated**: 2022-04-09 16:48:23+00:00
- **Authors**: Luca Guarnera, Oliver Giudice, Matthias Niessner, Sebastiano Battiato
- **Comment**: None
- **Journal**: None
- **Summary**: Despite recent advances in Generative Adversarial Networks (GANs), with special focus to the Deepfake phenomenon there is no a clear understanding neither in terms of explainability nor of recognition of the involved models. In particular, the recognition of a specific GAN model that generated the deepfake image compared to many other possible models created by the same generative architecture (e.g. StyleGAN) is a task not yet completely addressed in the state-of-the-art. In this work, a robust processing pipeline to evaluate the possibility to point-out analytic fingerprints for Deepfake model recognition is presented. After exploiting the latent space of 50 slightly different models through an in-depth analysis on the generated images, a proper encoder was trained to discriminate among these models obtaining a classification accuracy of over 96%. Once demonstrated the possibility to discriminate extremely similar images, a dedicated metric exploiting the insights discovered in the latent space was introduced. By achieving a final accuracy of more than 94% for the Model Recognition task on images generated by models not employed in the training phase, this study takes an important step in countering the Deepfake phenomenon introducing a sort of signature in some sense similar to those employed in the multimedia forensics field (e.g. for camera source identification task, image ballistics task, etc).



### Uncertainty-Informed Deep Learning Models Enable High-Confidence Predictions for Digital Histopathology
- **Arxiv ID**: http://arxiv.org/abs/2204.04516v1
- **DOI**: 10.1038/s41467-022-34025-x
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04516v1)
- **Published**: 2022-04-09 17:35:37+00:00
- **Updated**: 2022-04-09 17:35:37+00:00
- **Authors**: James M Dolezal, Andrew Srisuwananukorn, Dmitry Karpeyev, Siddhi Ramesh, Sara Kochanny, Brittany Cody, Aaron Mansfield, Sagar Rakshit, Radhika Bansa, Melanie Bois, Aaron O Bungum, Jefree J Schulte, Everett E Vokes, Marina Chiara Garassino, Aliya N Husain, Alexander T Pearson
- **Comment**: None
- **Journal**: None
- **Summary**: A model's ability to express its own predictive uncertainty is an essential attribute for maintaining clinical user confidence as computational biomarkers are deployed into real-world medical settings. In the domain of cancer digital histopathology, we describe a novel, clinically-oriented approach to uncertainty quantification (UQ) for whole-slide images, estimating uncertainty using dropout and calculating thresholds on training data to establish cutoffs for low- and high-confidence predictions. We train models to identify lung adenocarcinoma vs. squamous cell carcinoma and show that high-confidence predictions outperform predictions without UQ, in both cross-validation and testing on two large external datasets spanning multiple institutions. Our testing strategy closely approximates real-world application, with predictions generated on unsupervised, unannotated slides using predetermined thresholds. Furthermore, we show that UQ thresholding remains reliable in the setting of domain shift, with accurate high-confidence predictions of adenocarcinoma vs. squamous cell carcinoma for out-of-distribution, non-lung cancer cohorts.



### Knowledge-Free Black-Box Watermark and Ownership Proof for Image Classification Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.04522v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.04522v1)
- **Published**: 2022-04-09 18:09:02+00:00
- **Updated**: 2022-04-09 18:09:02+00:00
- **Authors**: Fangqi Li, Shilin Wang
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Watermarking has become a plausible candidate for ownership verification and intellectual property protection of deep neural networks. Regarding image classification neural networks, current watermarking schemes uniformly resort to backdoor triggers. However, injecting a backdoor into a neural network requires knowledge of the training dataset, which is usually unavailable in the real-world commercialization. Meanwhile, established watermarking schemes oversight the potential damage of exposed evidence during ownership verification and the watermarking algorithms themselves. Those concerns decline current watermarking schemes from industrial applications. To confront these challenges, we propose a knowledge-free black-box watermarking scheme for image classification neural networks. The image generator obtained from a data-free distillation process is leveraged to stabilize the network's performance during the backdoor injection. A delicate encoding and verification protocol is designed to ensure the scheme's security against knowledgable adversaries. We also give a pioneering analysis of the capacity of the watermarking scheme. Experiment results proved the functionality-preserving capability and security of the proposed watermarking scheme.



### Self-Labeling Refinement for Robust Representation Learning with Bootstrap Your Own Latent
- **Arxiv ID**: http://arxiv.org/abs/2204.04545v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.04545v1)
- **Published**: 2022-04-09 20:30:20+00:00
- **Updated**: 2022-04-09 20:30:20+00:00
- **Authors**: Siddhant Garg, Dhruval Jain
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we have worked towards two major goals. Firstly, we have investigated the importance of Batch Normalisation (BN) layers in a non-contrastive representation learning framework called Bootstrap Your Own Latent (BYOL). We conducted several experiments to conclude that BN layers are not necessary for representation learning in BYOL. Moreover, BYOL only learns from the positive pairs of images but ignores other semantically similar images in the same input batch. For the second goal, we have introduced two new loss functions to determine the semantically similar pairs in the same input batch of images and reduce the distance between their representations. These loss functions are Cross-Cosine Similarity Loss (CCSL) and Cross-Sigmoid Similarity Loss (CSSL). Using the proposed loss functions, we are able to surpass the performance of Vanilla BYOL (71.04%) by training the BYOL framework using CCSL loss (76.87%) on the STL10 dataset. BYOL trained using CSSL loss performs comparably with Vanilla BYOL.



### Adaptive search area for fast motion estimation
- **Arxiv ID**: http://arxiv.org/abs/2204.04546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04546v1)
- **Published**: 2022-04-09 20:44:34+00:00
- **Updated**: 2022-04-09 20:44:34+00:00
- **Authors**: S. M. Reza Soroushmehr, Shadrokh Samavi, Shahram Shirani
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: This paper suggests a new method for determining the search area for a motion estimation algorithm based on block matching. The search area is adaptively found in the proposed method for each frame block. This search area is similar to that of the full search (FS) algorithm but smaller for most blocks of a frame. Therefore, the proposed algorithm is analogous to FS in terms of regularity but has much less computational complexity. The temporal and spatial correlations among the motion vectors of blocks are used to find the search area. The matched block is chosen from a rectangular area that the prediction vectors set out. Simulation results indicate that the speed of the proposed algorithm is at least seven times better than the FS algorithm.



### Multimodal Transformer for Nursing Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.04564v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2204.04564v1)
- **Published**: 2022-04-09 23:01:00+00:00
- **Updated**: 2022-04-09 23:01:00+00:00
- **Authors**: Momal Ijaz, Renato Diaz, Chen Chen
- **Comment**: CVPR-2022 Workshop
- **Journal**: None
- **Summary**: In an aging population, elderly patient safety is a primary concern at hospitals and nursing homes, which demands for increased nurse care. By performing nurse activity recognition, we can not only make sure that all patients get an equal desired care, but it can also free nurses from manual documentation of activities they perform, leading to a fair and safe place of care for the elderly. In this work, we present a multimodal transformer-based network, which extracts features from skeletal joints and acceleration data, and fuses them to perform nurse activity recognition. Our method achieves state-of-the-art performance of 81.8% accuracy on the benchmark dataset available for nurse activity recognition from the Nurse Care Activity Recognition Challenge. We perform ablation studies to show that our fusion model is better than single modality transformer variants (using only acceleration or skeleton joints data). Our solution also outperforms state-of-the-art ST-GCN, GRU and other classical hand-crafted-feature-based classifier solutions by a margin of 1.6%, on the NCRC dataset. Code is available at \url{https://github.com/Momilijaz96/MMT_for_NCRC}.



### Joint Distribution Matters: Deep Brownian Distance Covariance for Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.04567v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2204.04567v1)
- **Published**: 2022-04-09 23:37:55+00:00
- **Updated**: 2022-04-09 23:37:55+00:00
- **Authors**: Jiangtao Xie, Fei Long, Jiaming Lv, Qilong Wang, Peihua Li
- **Comment**: Accepted to CVPR 2022 as an oral presentation. Equal contribution
  from first two authors
- **Journal**: None
- **Summary**: Few-shot classification is a challenging problem as only very few training examples are given for each new task. One of the effective research lines to address this challenge focuses on learning deep representations driven by a similarity measure between a query image and few support images of some class. Statistically, this amounts to measure the dependency of image features, viewed as random vectors in a high-dimensional embedding space. Previous methods either only use marginal distributions without considering joint distributions, suffering from limited representation capability, or are computationally expensive though harnessing joint distributions. In this paper, we propose a deep Brownian Distance Covariance (DeepBDC) method for few-shot classification. The central idea of DeepBDC is to learn image representations by measuring the discrepancy between joint characteristic functions of embedded features and product of the marginals. As the BDC metric is decoupled, we formulate it as a highly modular and efficient layer. Furthermore, we instantiate DeepBDC in two different few-shot classification frameworks. We make experiments on six standard few-shot image benchmarks, covering general object recognition, fine-grained categorization and cross-domain classification. Extensive evaluations show our DeepBDC significantly outperforms the counterparts, while establishing new state-of-the-art results. The source code is available at http://www.peihuali.org/DeepBDC



