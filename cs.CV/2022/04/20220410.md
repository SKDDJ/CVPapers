# Arxiv Papers in cs.CV on 2022-04-10
### Robust Cross-Modal Representation Learning with Progressive Self-Distillation
- **Arxiv ID**: http://arxiv.org/abs/2204.04588v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.04588v1)
- **Published**: 2022-04-10 03:28:18+00:00
- **Updated**: 2022-04-10 03:28:18+00:00
- **Authors**: Alex Andonian, Shixing Chen, Raffay Hamid
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: The learning objective of vision-language approach of CLIP does not effectively account for the noisy many-to-many correspondences found in web-harvested image captioning datasets, which contributes to its compute and data inefficiency. To address this challenge, we introduce a novel training framework based on cross-modal contrastive learning that uses progressive self-distillation and soft image-text alignments to more efficiently learn robust representations from noisy data. Our model distills its own knowledge to dynamically generate soft-alignment targets for a subset of images and captions in every minibatch, which are then used to update its parameters. Extensive evaluation across 14 benchmark datasets shows that our method consistently outperforms its CLIP counterpart in multiple settings, including: (a) zero-shot classification, (b) linear probe transfer, and (c) image-text retrieval, without incurring added computational cost. Analysis using an ImageNet-based robustness test-bed reveals that our method offers better effective robustness to natural distribution shifts compared to both ImageNet-trained models and CLIP itself. Lastly, pretraining with datasets spanning two orders of magnitude in size shows that our improvements over CLIP tend to scale with number of training examples.



### Explaining Deep Convolutional Neural Networks via Latent Visual-Semantic Filter Attention
- **Arxiv ID**: http://arxiv.org/abs/2204.04601v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.04601v1)
- **Published**: 2022-04-10 04:57:56+00:00
- **Updated**: 2022-04-10 04:57:56+00:00
- **Authors**: Yu Yang, Seungbae Kim, Jungseock Joo
- **Comment**: To appear in CVPR 2022 (oral presentation)
- **Journal**: None
- **Summary**: Interpretability is an important property for visual models as it helps researchers and users understand the internal mechanism of a complex model. However, generating semantic explanations about the learned representation is challenging without direct supervision to produce such explanations. We propose a general framework, Latent Visual Semantic Explainer (LaViSE), to teach any existing convolutional neural network to generate text descriptions about its own latent representations at the filter level. Our method constructs a mapping between the visual and semantic spaces using generic image datasets, using images and category names. It then transfers the mapping to the target domain which does not have semantic labels. The proposed framework employs a modular structure and enables to analyze any trained network whether or not its original training data is available. We show that our method can generate novel descriptions for learned filters beyond the set of categories defined in the training dataset and perform an extensive evaluation on multiple datasets. We also demonstrate a novel application of our method for unsupervised dataset bias analysis which allows us to automatically discover hidden biases in datasets or compare different subsets without using additional labels. The dataset and code are made public to facilitate further research.



### Self-Supervised Video Representation Learning with Motion-Contrastive Perception
- **Arxiv ID**: http://arxiv.org/abs/2204.04607v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.04607v1)
- **Published**: 2022-04-10 05:34:46+00:00
- **Updated**: 2022-04-10 05:34:46+00:00
- **Authors**: Jinyu Liu, Ying Cheng, Yuejie Zhang, Rui-Wei Zhao, Rui Feng
- **Comment**: Accepted by ICME 2022
- **Journal**: None
- **Summary**: Visual-only self-supervised learning has achieved significant improvement in video representation learning. Existing related methods encourage models to learn video representations by utilizing contrastive learning or designing specific pretext tasks. However, some models are likely to focus on the background, which is unimportant for learning video representations. To alleviate this problem, we propose a new view called long-range residual frame to obtain more motion-specific information. Based on this, we propose the Motion-Contrastive Perception Network (MCPNet), which consists of two branches, namely, Motion Information Perception (MIP) and Contrastive Instance Perception (CIP), to learn generic video representations by focusing on the changing areas in videos. Specifically, the MIP branch aims to learn fine-grained motion features, and the CIP branch performs contrastive learning to learn overall semantics information for each instance. Experiments on two benchmark datasets UCF-101 and HMDB-51 show that our method outperforms current state-of-the-art visual-only self-supervised approaches.



### Learning Pixel-Level Distinctions for Video Highlight Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.04615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04615v1)
- **Published**: 2022-04-10 06:41:16+00:00
- **Updated**: 2022-04-10 06:41:16+00:00
- **Authors**: Fanyue Wei, Biao Wang, Tiezheng Ge, Yuning Jiang, Wen Li, Lixin Duan
- **Comment**: Accepted at CVPR 2022
- **Journal**: None
- **Summary**: The goal of video highlight detection is to select the most attractive segments from a long video to depict the most interesting parts of the video. Existing methods typically focus on modeling relationship between different video segments in order to learning a model that can assign highlight scores to these segments; however, these approaches do not explicitly consider the contextual dependency within individual segments. To this end, we propose to learn pixel-level distinctions to improve the video highlight detection. This pixel-level distinction indicates whether or not each pixel in one video belongs to an interesting section. The advantages of modeling such fine-level distinctions are two-fold. First, it allows us to exploit the temporal and spatial relations of the content in one video, since the distinction of a pixel in one frame is highly dependent on both the content before this frame and the content around this pixel in this frame. Second, learning the pixel-level distinction also gives a good explanation to the video highlight task regarding what contents in a highlight segment will be attractive to people. We design an encoder-decoder network to estimate the pixel-level distinction, in which we leverage the 3D convolutional neural networks to exploit the temporal context information, and further take advantage of the visual saliency to model the spatial distinction. State-of-the-art performance on three public benchmarks clearly validates the effectiveness of our framework for video highlight detection.



### On Principal Curve-Based Classifiers and Similarity-Based Selective Sampling in Time-Series
- **Arxiv ID**: http://arxiv.org/abs/2204.04620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04620v1)
- **Published**: 2022-04-10 07:28:18+00:00
- **Updated**: 2022-04-10 07:28:18+00:00
- **Authors**: Aref Hakimzadeh, Koorush Ziarati, Mohammad Taheri
- **Comment**: 13 double column pages
- **Journal**: None
- **Summary**: Considering the concept of time-dilation, there exist some major issues with recurrent neural Architectures. Any variation in time spans between input data points causes performance attenuation in recurrent neural network architectures. Principal curve-based classifiers have the ability of handling any kind of variation in time spans. In other words, principal curve-based classifiers preserve the relativity of time while neural network architecture violates this property of time. On the other hand, considering the labeling costs and problems in online monitoring devices, there should be an algorithm that finds the data points which knowing their labels will cause in better performance of the classifier. Current selective sampling algorithms have lack of reliability due to the randomness of the proposed algorithms. This paper proposes a classifier and also a deterministic selective sampling algorithm with the same computational steps, both by use of principal curve as their building block in model definition.



### Unsupervised Manga Character Re-identification via Face-body and Spatial-temporal Associated Clustering
- **Arxiv ID**: http://arxiv.org/abs/2204.04621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04621v1)
- **Published**: 2022-04-10 07:28:41+00:00
- **Updated**: 2022-04-10 07:28:41+00:00
- **Authors**: Zhimin Zhang, Zheng Wang, Wei Hu
- **Comment**: None
- **Journal**: None
- **Summary**: In the past few years, there has been a dramatic growth in e-manga (electronic Japanese-style comics). Faced with the booming demand for manga research and the large amount of unlabeled manga data, we raised a new task, called unsupervised manga character re-identification. However, the artistic expression and stylistic limitations of manga pose many challenges to the re-identification problem. Inspired by the idea that some content-related features may help clustering, we propose a Face-body and Spatial-temporal Associated Clustering method (FSAC). In the face-body combination module, a face-body graph is constructed to solve problems such as exaggeration and deformation in artistic creation by using the integrity of the image. In the spatial-temporal relationship correction module, we analyze the appearance features of characters and design a temporal-spatial-related triplet loss to fine-tune the clustering. Extensive experiments on a manga book dataset with 109 volumes validate the superiority of our method in unsupervised manga character re-identification.



### Stripformer: Strip Transformer for Fast Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2204.04627v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04627v2)
- **Published**: 2022-04-10 08:01:00+00:00
- **Updated**: 2022-07-22 10:01:04+00:00
- **Authors**: Fu-Jen Tsai, Yan-Tsung Peng, Yen-Yu Lin, Chung-Chi Tsai, Chia-Wen Lin
- **Comment**: ECCV 2022 Oral Presentation
- **Journal**: None
- **Summary**: Images taken in dynamic scenes may contain unwanted motion blur, which significantly degrades visual quality. Such blur causes short- and long-range region-specific smoothing artifacts that are often directional and non-uniform, which is difficult to be removed. Inspired by the current success of transformers on computer vision and image processing tasks, we develop, Stripformer, a transformer-based architecture that constructs intra- and inter-strip tokens to reweight image features in the horizontal and vertical directions to catch blurred patterns with different orientations. It stacks interlaced intra-strip and inter-strip attention layers to reveal blur magnitudes. In addition to detecting region-specific blurred patterns of various orientations and magnitudes, Stripformer is also a token-efficient and parameter-efficient transformer model, demanding much less memory usage and computation cost than the vanilla transformer but works better without relying on tremendous training data. Experimental results show that Stripformer performs favorably against state-of-the-art models in dynamic scene deblurring.



### Intersection Prediction from Single 360° Image via Deep Detection of Possible Direction of Travel
- **Arxiv ID**: http://arxiv.org/abs/2204.04634v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.04634v1)
- **Published**: 2022-04-10 08:53:14+00:00
- **Updated**: 2022-04-10 08:53:14+00:00
- **Authors**: Naoki Sugimoto, Satoshi Ikehata, Kiyoharu Aizawa
- **Comment**: Accepted for publication in BMVC
- **Journal**: None
- **Summary**: Movie-Map, an interactive first-person-view map that engages the user in a simulated walking experience, comprises short 360{\deg} video segments separated by traffic intersections that are seamlessly connected according to the viewer's direction of travel. However, in wide urban-scale areas with numerous intersecting roads, manual intersection segmentation requires significant human effort. Therefore, automatic identification of intersections from 360{\deg} videos is an important problem for scaling up Movie-Map. In this paper, we propose a novel method that identifies an intersection from individual frames in 360{\deg} videos. Instead of formulating the intersection identification as a standard binary classification task with a 360{\deg} image as input, we identify an intersection based on the number of the possible directions of travel (PDoT) in perspective images projected in eight directions from a single 360{\deg} image detected by the neural network for handling various types of intersections. We constructed a large-scale 360{\deg} Image Intersection Identification (iii360) dataset for training and evaluation where 360{\deg} videos were collected from various areas such as school campus, downtown, suburb, and china town and demonstrate that our PDoT-based method achieves 88\% accuracy, which is significantly better than that achieved by the direct naive binary classification based method. The source codes and a partial dataset will be shared in the community after the paper is published.



### ConsInstancy: Learning Instance Representations for Semi-Supervised Panoptic Segmentation of Concrete Aggregate Particles
- **Arxiv ID**: http://arxiv.org/abs/2204.04635v1
- **DOI**: 10.1007/s00138-022-01313-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04635v1)
- **Published**: 2022-04-10 09:08:08+00:00
- **Updated**: 2022-04-10 09:08:08+00:00
- **Authors**: Max Coenen, Tobias Schack, Dries Beyer, Christian Heipke, Michael Haist
- **Comment**: None
- **Journal**: None
- **Summary**: We present a semi-supervised method for panoptic segmentation based on ConsInstancy regularisation, a novel strategy for semi-supervised learning. It leverages completely unlabelled data by enforcing consistency between predicted instance representations and semantic segmentations during training in order to improve the segmentation performance. To this end, we also propose new types of instance representations that can be predicted by one simple forward path through a fully convolutional network (FCN), delivering a convenient and simple-to-train framework for panoptic segmentation. More specifically, we propose the prediction of a three-dimensional instance orientation map as intermediate representation and two complementary distance transform maps as final representation, providing unique instance representations for a panoptic segmentation. We test our method on two challenging data sets of both, hardened and fresh concrete, the latter being proposed by the authors in this paper demonstrating the effectiveness of our approach, outperforming the results achieved by state-of-the-art methods for semi-supervised segmentation. In particular, we are able to show that by leveraging completely unlabeled data in our semi-supervised approach the achieved overall accuracy (OA) is increased by up to 5% compared to an entirely supervised training using only labeled data. Furthermore, we exceed the OA achieved by state-of-the-art semi-supervised methods by up to 1.5%.



### Spectral Unmixing of Hyperspectral Images Based on Block Sparse Structure
- **Arxiv ID**: http://arxiv.org/abs/2204.04638v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04638v2)
- **Published**: 2022-04-10 09:37:41+00:00
- **Updated**: 2023-02-17 09:24:56+00:00
- **Authors**: Seyed Hossein Mosavi Azarang, Roozbeh Rajabi, Hadi Zayyani, Amin Zehtabian
- **Comment**: 25 pages, 8 figures, 2 tables, accepted for publication in journal
- **Journal**: None
- **Summary**: Spectral unmixing (SU) of hyperspectral images (HSIs) is one of the important areas in remote sensing (RS) that needs to be carefully addressed in different RS applications. Despite the high spectral resolution of the hyperspectral data, the relatively low spatial resolution of the sensors may lead to mixture of different pure materials within the image pixels. In this case, the spectrum of a given pixel recorded by the sensor can be a combination of multiple spectra each belonging to a unique material in that pixel. Spectral unmixing is then used as a technique to extract the spectral characteristics of the different materials within the mixed pixels and to recover the spectrum of each pure spectral signature, called endmember. Block-sparsity exists in hyperspectral images as a result of spectral similarity between neighboring pixels. In block-sparse signals, the nonzero samples occur in clusters and the pattern of the clusters is often supposed to be unavailable as prior information. This paper presents an innovative spectral unmixing approach for HSIs based on block-sparse structure. Hyperspectral unmixing problem is solved using pattern coupled sparse Bayesian learning strategy (PCSBL). To evaluate the performance of the proposed SU algorithm, it is tested on both synthetic and real hyperspectral data and the quantitative results are compared to those of other state-of-the-art methods in terms of abundance angle distance and mean squared error. The achieved results show the superiority of the proposed algorithm over the other competing methods by a significant margin.



### Counting in the 2020s: Binned Representations and Inclusive Performance Measures for Deep Crowd Counting Approaches
- **Arxiv ID**: http://arxiv.org/abs/2204.04653v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.04653v1)
- **Published**: 2022-04-10 11:02:13+00:00
- **Updated**: 2022-04-10 11:02:13+00:00
- **Authors**: Sravya Vardhani Shivapuja, Ashwin Gopinath, Ayush Gupta, Ganesh Ramakrishnan, Ravi Kiran Sarvadevabhatla
- **Comment**: Extended version of arXiv:2108.08784. In review
- **Journal**: None
- **Summary**: The data distribution in popular crowd counting datasets is typically heavy tailed and discontinuous. This skew affects all stages within the pipelines of deep crowd counting approaches. Specifically, the approaches exhibit unacceptably large standard deviation wrt statistical measures (MSE, MAE). To address such concerns in a holistic manner, we make two fundamental contributions. Firstly, we modify the training pipeline to accommodate the knowledge of dataset skew. To enable principled and balanced minibatch sampling, we propose a novel smoothed Bayesian binning approach. More specifically, we propose a novel cost function which can be readily incorporated into existing crowd counting deep networks to encourage bin-aware optimization. As the second contribution, we introduce additional performance measures which are more inclusive and throw light on various comparative performance aspects of the deep networks. We also show that our binning-based modifications retain their superiority wrt the newly proposed performance measures. Overall, our contributions enable a practically useful and detail-oriented characterization of performance for crowd counting approaches.



### Fashionformer: A simple, Effective and Unified Baseline for Human Fashion Segmentation and Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.04654v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04654v2)
- **Published**: 2022-04-10 11:11:10+00:00
- **Updated**: 2022-07-10 09:33:58+00:00
- **Authors**: Shilin Xu, Xiangtai Li, Jingbo Wang, Guangliang Cheng, Yunhai Tong, Dacheng Tao
- **Comment**: ECCV-2022
- **Journal**: None
- **Summary**: Human fashion understanding is one crucial computer vision task since it has comprehensive information for real-world applications. This focus on joint human fashion segmentation and attribute recognition. Contrary to the previous works that separately model each task as a multi-head prediction problem, our insight is to bridge these two tasks with one unified model via vision transformer modeling to benefit each task. In particular, we introduce the object query for segmentation and the attribute query for attribute prediction. Both queries and their corresponding features can be linked via mask prediction. Then we adopt a two-stream query learning framework to learn the decoupled query representations.We design a novel Multi-Layer Rendering module for attribute stream to explore more fine-grained features. The decoder design shares the same spirit as DETR. Thus we name the proposed method \textit{Fahsionformer}. Extensive experiments on three human fashion datasets illustrate the effectiveness of our approach. In particular, our method with the same backbone achieve \textbf{relative 10\% improvements} than previous works in case of \textit{a joint metric (AP$^{\text{mask}}_{\text{IoU+F}_1}$) for both segmentation and attribute recognition}. To the best of our knowledge, we are the first unified end-to-end vision transformer framework for human fashion analysis. We hope this simple yet effective method can serve as a new flexible baseline for fashion analysis. Code is available at https://github.com/xushilin1/FashionFormer.



### Panoptic-PartFormer: Learning a Unified Model for Panoptic Part Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.04655v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04655v2)
- **Published**: 2022-04-10 11:16:45+00:00
- **Updated**: 2022-07-10 09:30:39+00:00
- **Authors**: Xiangtai Li, Shilin Xu, Yibo Yang, Guangliang Cheng, Yunhai Tong, Dacheng Tao
- **Comment**: ECCV-2022
- **Journal**: None
- **Summary**: Panoptic Part Segmentation (PPS) aims to unify panoptic segmentation and part segmentation into one task. Previous work mainly utilizes separated approaches to handle thing, stuff, and part predictions individually without performing any shared computation and task association. In this work, we aim to unify these tasks at the architectural level, designing the first end-to-end unified method named Panoptic-PartFormer. In particular, motivated by the recent progress in Vision Transformer, we model things, stuff, and part as object queries and directly learn to optimize the all three predictions as unified mask prediction and classification problem. We design a decoupled decoder to generate part feature and thing/stuff feature respectively. Then we propose to utilize all the queries and corresponding features to perform reasoning jointly and iteratively. The final mask can be obtained via inner product between queries and the corresponding features. The extensive ablation studies and analysis prove the effectiveness of our framework. Our Panoptic-PartFormer achieves the new state-of-the-art results on both Cityscapes PPS and Pascal Context PPS datasets with at least 70% GFlops and 50% parameters decrease. In particular, we get 3.4% relative improvements with ResNet50 backbone and 10% improvements after adopting Swin Transformer on Pascal Context PPS dataset. To the best of our knowledge, we are the first to solve the PPS problem via \textit{a unified and end-to-end transformer model. Given its effectiveness and conceptual simplicity, we hope our Panoptic-PartFormer can serve as a good baseline and aid future unified research for PPS. Our code and models are available at https://github.com/lxtGH/Panoptic-PartFormer.



### Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.04656v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04656v2)
- **Published**: 2022-04-10 11:24:47+00:00
- **Updated**: 2022-10-19 08:30:00+00:00
- **Authors**: Xiangtai Li, Wenwei Zhang, Jiangmiao Pang, Kai Chen, Guangliang Cheng, Yunhai Tong, Chen Change Loy
- **Comment**: Accepted by CVPR-2022(oral); Add more experiments. Code is available
  at https://github.com/lxtGH/Video-K-Net
- **Journal**: None
- **Summary**: This paper presents Video K-Net, a simple, strong, and unified framework for fully end-to-end video panoptic segmentation. The method is built upon K-Net, a method that unifies image segmentation via a group of learnable kernels. We observe that these learnable kernels from K-Net, which encode object appearances and contexts, can naturally associate identical instances across video frames. Motivated by this observation, Video K-Net learns to simultaneously segment and track "things" and "stuff" in a video with simple kernel-based appearance modeling and cross-temporal kernel interaction. Despite the simplicity, it achieves state-of-the-art video panoptic segmentation results on Citscapes-VPS, KITTI-STEP, and VIPSeg without bells and whistles. In particular, on KITTI-STEP, the simple method can boost almost 12\% relative improvements over previous methods. On VIPSeg, Video K-Net boosts almost 15\% relative improvements and results in 39.8 % VPQ. We also validate its generalization on video semantic segmentation, where we boost various baselines by 2\% on the VSPW dataset. Moreover, we extend K-Net into clip-level video framework for video instance segmentation, where we obtain 40.5% mAP for ResNet50 backbone and 54.1% mAP for Swin-base on YouTube-2019 validation set. We hope this simple, yet effective method can serve as a new, flexible baseline in unified video segmentation design. Both code and models are released at https://github.com/lxtGH/Video-K-Net.



### FOSTER: Feature Boosting and Compression for Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.04662v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.04662v2)
- **Published**: 2022-04-10 11:38:33+00:00
- **Updated**: 2022-07-20 11:37:42+00:00
- **Authors**: Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan
- **Comment**: Accepted to ECCV 2022. Code is available at:
  https://github.com/G-U-N/ECCV22-FOSTER
- **Journal**: None
- **Summary**: The ability to learn new concepts continually is necessary in this ever-changing world. However, deep neural networks suffer from catastrophic forgetting when learning new categories. Many works have been proposed to alleviate this phenomenon, whereas most of them either fall into the stability-plasticity dilemma or take too much computation or storage overhead. Inspired by the gradient boosting algorithm to gradually fit the residuals between the target model and the previous ensemble model, we propose a novel two-stage learning paradigm FOSTER, empowering the model to learn new categories adaptively. Specifically, we first dynamically expand new modules to fit the residuals between the target and the output of the original model. Next, we remove redundant parameters and feature dimensions through an effective distillation strategy to maintain the single backbone model. We validate our method FOSTER on CIFAR-100 and ImageNet-100/1000 under different settings. Experimental results show that our method achieves state-of-the-art performance. Code is available at: https://github.com/G-U-N/ECCV22-FOSTER.



### Effective Out-of-Distribution Detection in Classifier Based on PEDCC-Loss
- **Arxiv ID**: http://arxiv.org/abs/2204.04665v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.04665v1)
- **Published**: 2022-04-10 11:47:29+00:00
- **Updated**: 2022-04-10 11:47:29+00:00
- **Authors**: Qiuyu Zhu, Guohui Zheng, Yingying Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks suffer from the overconfidence issue in the open world, meaning that classifiers could yield confident, incorrect predictions for out-of-distribution (OOD) samples. Thus, it is an urgent and challenging task to detect these samples drawn far away from training distribution based on the security considerations of artificial intelligence. Many current methods based on neural networks mainly rely on complex processing strategies, such as temperature scaling and input preprocessing, to obtain satisfactory results. In this paper, we propose an effective algorithm for detecting out-of-distribution examples utilizing PEDCC-Loss. We mathematically analyze the nature of the confidence score output by the PEDCC (Predefined Evenly-Distribution Class Centroids) classifier, and then construct a more effective scoring function to distinguish in-distribution (ID) and out-of-distribution. In this method, there is no need to preprocess the input samples and the computational burden of the algorithm is reduced. Experiments demonstrate that our method can achieve better OOD detection performance.



### Linear Complexity Randomized Self-attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2204.04667v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04667v2)
- **Published**: 2022-04-10 12:10:28+00:00
- **Updated**: 2022-06-15 13:56:23+00:00
- **Authors**: Lin Zheng, Chong Wang, Lingpeng Kong
- **Comment**: ICML 2022 camera ready with 37 pages
- **Journal**: None
- **Summary**: Recently, random feature attentions (RFAs) are proposed to approximate the softmax attention in linear time and space complexity by linearizing the exponential kernel. In this paper, we first propose a novel perspective to understand the bias in such approximation by recasting RFAs as self-normalized importance samplers. This perspective further sheds light on an \emph{unbiased} estimator for the whole softmax attention, called randomized attention (RA). RA constructs positive random features via query-specific distributions and enjoys greatly improved approximation fidelity, albeit exhibiting quadratic complexity. By combining the expressiveness in RA and the efficiency in RFA, we develop a novel linear complexity self-attention mechanism called linear randomized attention (LARA). Extensive experiments across various domains demonstrate that RA and LARA significantly improve the performance of RFAs by a substantial margin.



### NAN: Noise-Aware NeRFs for Burst-Denoising
- **Arxiv ID**: http://arxiv.org/abs/2204.04668v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04668v2)
- **Published**: 2022-04-10 12:10:59+00:00
- **Updated**: 2022-04-12 16:21:08+00:00
- **Authors**: Naama Pearl, Tali Treibitz, Simon Korman
- **Comment**: to appear at CVPR 2022
- **Journal**: None
- **Summary**: Burst denoising is now more relevant than ever, as computational photography helps overcome sensitivity issues inherent in mobile phones and small cameras. A major challenge in burst-denoising is in coping with pixel misalignment, which was so far handled with rather simplistic assumptions of simple motion, or the ability to align in pre-processing. Such assumptions are not realistic in the presence of large motion and high levels of noise. We show that Neural Radiance Fields (NeRFs), originally suggested for physics-based novel-view rendering, can serve as a powerful framework for burst denoising. NeRFs have an inherent capability of handling noise as they integrate information from multiple images, but they are limited in doing so, mainly since they build on pixel-wise operations which are suitable to ideal imaging conditions. Our approach, termed NAN, leverages inter-view and spatial information in NeRFs to better deal with noise. It achieves state-of-the-art results in burst denoising and is especially successful in coping with large movement and occlusions, under very high levels of noise. With the rapid advances in accelerating NeRFs, it could provide a powerful platform for denoising in challenging environments.



### A Novel Region Duplication Detection Algorithm Based on Hybrid Approach
- **Arxiv ID**: http://arxiv.org/abs/2204.08545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08545v1)
- **Published**: 2022-04-10 12:17:13+00:00
- **Updated**: 2022-04-10 12:17:13+00:00
- **Authors**: Kshipra Tatkare, Manoj Devare
- **Comment**: 9 pages, 9 figures, image forgery detection
- **Journal**: None
- **Summary**: The digital images from various sources are ubiquitous due to easy availability of high bandwidth Internet. Digital images are easy to tamper with good or bad intentions. Non-availability of pre-embedded information in digital images makes the tampering detection process more difficult in case of digital forensics. Thus, passive image tampering is difficult to detect. There are various algorithms available for detecting image tampering. However, these algorithms have some drawbacks, due to which all types of tampering cannot be detected. In this paper researchers intend to present the types of image tampering and its detection techniques with example based approach. This paper also illustrates insights into the various existing algorithms and tries to find out efficient algorithm out of them.



### Is my Driver Observation Model Overconfident? Input-guided Calibration Networks for Reliable and Interpretable Confidence Estimates
- **Arxiv ID**: http://arxiv.org/abs/2204.04674v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.04674v1)
- **Published**: 2022-04-10 12:43:58+00:00
- **Updated**: 2022-04-10 12:43:58+00:00
- **Authors**: Alina Roitberg, Kunyu Peng, David Schneider, Kailun Yang, Marios Koulakis, Manuel Martinez, Rainer Stiefelhagen
- **Comment**: None
- **Journal**: None
- **Summary**: Driver observation models are rarely deployed under perfect conditions. In practice, illumination, camera placement and type differ from the ones present during training and unforeseen behaviours may occur at any time. While observing the human behind the steering wheel leads to more intuitive human-vehicle-interaction and safer driving, it requires recognition algorithms which do not only predict the correct driver state, but also determine their prediction quality through realistic and interpretable confidence measures. Reliable uncertainty estimates are crucial for building trust and are a serious obstacle for deploying activity recognition networks in real driving systems. In this work, we for the first time examine how well the confidence values of modern driver observation models indeed match the probability of the correct outcome and show that raw neural network-based approaches tend to significantly overestimate their prediction quality. To correct this misalignment between the confidence values and the actual uncertainty, we consider two strategies. First, we enhance two activity recognition models often used for driver observation with temperature scaling-an off-the-shelf method for confidence calibration in image classification. Then, we introduce Calibrated Action Recognition with Input Guidance (CARING)-a novel approach leveraging an additional neural network to learn scaling the confidences depending on the video representation. Extensive experiments on the Drive&Act dataset demonstrate that both strategies drastically improve the quality of model confidences, while our CARING model out-performs both, the original architectures and their temperature scaling enhancement, leading to best uncertainty estimates.



### Simple Baselines for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2204.04676v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04676v4)
- **Published**: 2022-04-10 12:48:38+00:00
- **Updated**: 2022-08-01 19:47:35+00:00
- **Authors**: Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, Jian Sun
- **Comment**: Accepted to ECCV 2022; Code:
  https://github.com/megvii-research/NAFNet/
- **Journal**: None
- **Summary**: Although there have been significant advances in the field of image restoration recently, the system complexity of the state-of-the-art (SOTA) methods is increasing as well, which may hinder the convenient analysis and comparison of methods. In this paper, we propose a simple baseline that exceeds the SOTA methods and is computationally efficient. To further simplify the baseline, we reveal that the nonlinear activation functions, e.g. Sigmoid, ReLU, GELU, Softmax, etc. are not necessary: they could be replaced by multiplication or removed. Thus, we derive a Nonlinear Activation Free Network, namely NAFNet, from the baseline. SOTA results are achieved on various challenging benchmarks, e.g. 33.69 dB PSNR on GoPro (for image deblurring), exceeding the previous SOTA 0.38 dB with only 8.4% of its computational costs; 40.30 dB PSNR on SIDD (for image denoising), exceeding the previous SOTA 0.28 dB with less than half of its computational costs. The code and the pre-trained models are released at https://github.com/megvii-research/NAFNet.



### FedCorr: Multi-Stage Federated Learning for Label Noise Correction
- **Arxiv ID**: http://arxiv.org/abs/2204.04677v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04677v1)
- **Published**: 2022-04-10 12:51:18+00:00
- **Updated**: 2022-04-10 12:51:18+00:00
- **Authors**: Jingyi Xu, Zihan Chen, Tony Q. S. Quek, Kai Fong Ernest Chong
- **Comment**: Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2022. 18 pages, 10 figures (including supplementary
  material). First two authors contributed equally. Code is available at:
  https://github.com/Xu-Jingyi/FedCorr
- **Journal**: None
- **Summary**: Federated learning (FL) is a privacy-preserving distributed learning paradigm that enables clients to jointly train a global model. In real-world FL implementations, client data could have label noise, and different clients could have vastly different label noise levels. Although there exist methods in centralized learning for tackling label noise, such methods do not perform well on heterogeneous label noise in FL settings, due to the typically smaller sizes of client datasets and data privacy requirements in FL. In this paper, we propose $\texttt{FedCorr}$, a general multi-stage framework to tackle heterogeneous label noise in FL, without making any assumptions on the noise models of local clients, while still maintaining client data privacy. In particular, (1) $\texttt{FedCorr}$ dynamically identifies noisy clients by exploiting the dimensionalities of the model prediction subspaces independently measured on all clients, and then identifies incorrect labels on noisy clients based on per-sample losses. To deal with data heterogeneity and to increase training stability, we propose an adaptive local proximal regularization term that is based on estimated local noise levels. (2) We further finetune the global model on identified clean clients and correct the noisy labels for the remaining noisy clients after finetuning. (3) Finally, we apply the usual training on all clients to make full use of all local data. Experiments conducted on CIFAR-10/100 with federated synthetic label noise, and on a real-world noisy dataset, Clothing1M, demonstrate that $\texttt{FedCorr}$ is robust to label noise and substantially outperforms the state-of-the-art methods at multiple noise levels.



### Scale Invariant Semantic Segmentation with RGB-D Fusion
- **Arxiv ID**: http://arxiv.org/abs/2204.04679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04679v1)
- **Published**: 2022-04-10 12:54:27+00:00
- **Updated**: 2022-04-10 12:54:27+00:00
- **Authors**: Mohammad Dawud Ansari, Alwi Husada, Didier Stricker
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: In this paper, we propose a neural network architecture for scale-invariant semantic segmentation using RGB-D images. We utilize depth information as an additional modality apart from color images only. Especially in an outdoor scene which consists of different scale objects due to the distance of the objects from the camera. The near distance objects consist of significantly more pixels than the far ones. We propose to incorporate depth information to the RGB data for pixel-wise semantic segmentation to address the different scale objects in an outdoor scene. We adapt to a well-known DeepLab-v2(ResNet-101) model as our RGB baseline. Depth images are passed separately as an additional input with a distinct branch. The intermediate feature maps of both color and depth image branch are fused using a novel fusion block. Our model is compact and can be easily applied to the other RGB model. We perform extensive qualitative and quantitative evaluation on a challenging dataset Cityscapes. The results obtained are comparable to the state-of-the-art. Additionally, we evaluated our model on a self-recorded real dataset. For the shake of extended evaluation of a driving scene with ground truth we generated a synthetic dataset using popular vehicle simulation project CARLA. The results obtained from the real and synthetic dataset shows the effectiveness of our approach.



### Reasoning with Multi-Structure Commonsense Knowledge in Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/2204.04680v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.04680v1)
- **Published**: 2022-04-10 13:12:10+00:00
- **Updated**: 2022-04-10 13:12:10+00:00
- **Authors**: Shunyu Zhang, Xiaoze Jiang, Zequn Yang, Tao Wan, Zengchang Qin
- **Comment**: MULA Workshop, CVPR 2022
- **Journal**: None
- **Summary**: Visual Dialog requires an agent to engage in a conversation with humans grounded in an image. Many studies on Visual Dialog focus on the understanding of the dialog history or the content of an image, while a considerable amount of commonsense-required questions are ignored. Handling these scenarios depends on logical reasoning that requires commonsense priors. How to capture relevant commonsense knowledge complementary to the history and the image remains a key challenge. In this paper, we propose a novel model by Reasoning with Multi-structure Commonsense Knowledge (RMK). In our model, the external knowledge is represented with sentence-level facts and graph-level facts, to properly suit the scenario of the composite of dialog history and image. On top of these multi-structure representations, our model can capture relevant knowledge and incorporate them into the vision and semantic features, via graph-based interaction and transformer-based fusion. Experimental results and analysis on VisDial v1.0 and VisDialCK datasets show that our proposed model effectively outperforms comparative methods.



### Enhancing the Robustness, Efficiency, and Diversity of Differentiable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2204.04681v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.04681v1)
- **Published**: 2022-04-10 13:25:36+00:00
- **Updated**: 2022-04-10 13:25:36+00:00
- **Authors**: Chao Li, Jia Ning, Han Hu, Kun He
- **Comment**: None
- **Journal**: None
- **Summary**: Differentiable architecture search (DARTS) has attracted much attention due to its simplicity and significant improvement in efficiency. However, the excessive accumulation of the skip connection makes it suffer from long-term weak stability and low robustness. Many works attempt to restrict the accumulation of skip connections by indicators or manual design, however, these methods are susceptible to thresholds and human priors. In this work, we suggest a more subtle and direct approach that removes skip connections from the operation space. Then, by introducing an adaptive channel allocation strategy, we redesign the DARTS framework to automatically refill the skip connections in the evaluation stage, resolving the performance degradation caused by the absence of skip connections. Our method, dubbed Adaptive-Channel-Allocation-DARTS (ACA-DRATS), could eliminate the inconsistency in operation strength and significantly expand the architecture diversity. We continue to explore smaller search space under our framework, and offer a direct search on the entire ImageNet dataset. Experiments show that ACA-DRATS improves the search stability and significantly speeds up DARTS by more than ten times while yielding higher accuracy.



### Coreset of Hyperspectral Images on Small Quantum Computer
- **Arxiv ID**: http://arxiv.org/abs/2204.04691v2
- **DOI**: None
- **Categories**: **quant-ph**, cs.CV, cs.ET
- **Links**: [PDF](http://arxiv.org/pdf/2204.04691v2)
- **Published**: 2022-04-10 14:14:20+00:00
- **Updated**: 2022-04-12 09:06:16+00:00
- **Authors**: Soronzonbold Otgonbaatar, Mihai Datcu, Begüm Demir
- **Comment**: Accepted to IGARSS2022. You may not be able to access this article
  after the publication in the conference
- **Journal**: None
- **Summary**: Machine Learning (ML) techniques are employed to analyze and process big Remote Sensing (RS) data, and one well-known ML technique is a Support Vector Machine (SVM). An SVM is a quadratic programming (QP) problem, and a D-Wave quantum annealer (D-Wave QA) promises to solve this QP problem more efficiently than a conventional computer. However, the D-Wave QA cannot solve directly the SVM due to its very few input qubits. Hence, we use a coreset ("core of a dataset") of given EO data for training an SVM on this small D-Wave QA. The coreset is a small, representative weighted subset of an original dataset, and any training models generate competitive classes by using the coreset in contrast to by using its original dataset. We measured the closeness between an original dataset and its coreset by employing a Kullback-Leibler (KL) divergence measure. Moreover, we trained the SVM on the coreset data by using both a D-Wave QA and a conventional method. We conclude that the coreset characterizes the original dataset with very small KL divergence measure. In addition, we present our KL divergence results for demonstrating the closeness between our original data and its coreset. As practical RS data, we use Hyperspectral Image (HSI) of Indian Pine, USA.



### An Efficient Pattern Mining Convolution Neural Network (CNN) algorithm with Grey Wolf Optimization (GWO)
- **Arxiv ID**: http://arxiv.org/abs/2204.04704v1
- **DOI**: 10.1080/13682199.2023.2166193
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.04704v1)
- **Published**: 2022-04-10 15:18:42+00:00
- **Updated**: 2022-04-10 15:18:42+00:00
- **Authors**: Aatif Jamshed, Bhawna Mallick, Rajendra Kumar Bharti
- **Comment**: None
- **Journal**: The Imaging Science Journal 2023
- **Summary**: Automation of feature analysis in the dynamic image frame dataset deals with complexity of intensity mapping with normal and abnormal class. The threshold-based data clustering and feature analysis requires iterative model to learn the component of image frame in multi-pattern for different image frame data type. This paper proposed a novel model of feature analysis method with the CNN based on Convoluted Pattern of Wavelet Transform (CPWT) feature vectors that are optimized by Grey Wolf Optimization (GWO) algorithm. Initially, the image frame gets normalized by applying median filter to the image frame that reduce the noise and apply smoothening on it. From that, the edge information represents the boundary region of bright spot in the image frame. Neural network-based image frame classification performs repeated learning of the feature with minimum training of dataset to cluster the image frame pixels. Features of the filtered image frame was analyzed in different pattern of feature extraction model based on the convoluted model of wavelet transformation method. These features represent the different class of image frame in spatial and textural pattern of it. Convolutional Neural Network (CNN) classifier supports to analyze the features and classify the action label for the image frame dataset. This process enhances the classification with minimum number of training dataset. The performance of this proposed method can be validated by comparing with traditional state-of-art methods.



### Generative Adversarial Networks for Image Augmentation in Agriculture: A Systematic Review
- **Arxiv ID**: http://arxiv.org/abs/2204.04707v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04707v2)
- **Published**: 2022-04-10 15:33:05+00:00
- **Updated**: 2022-04-12 23:49:12+00:00
- **Authors**: Ebenezer Olaniyi, Dong Chen, Yuzhen Lu, Yanbo Huang
- **Comment**: 32 pages, 15 figures
- **Journal**: None
- **Summary**: In agricultural image analysis, optimal model performance is keenly pursued for better fulfilling visual recognition tasks (e.g., image classification, segmentation, object detection and localization), in the presence of challenges with biological variability and unstructured environments. Large-scale, balanced and ground-truthed image datasets, however, are often difficult to obtain to fuel the development of advanced, high-performance models. As artificial intelligence through deep learning is impacting analysis and modeling of agricultural images, data augmentation plays a crucial role in boosting model performance while reducing manual efforts for data preparation, by algorithmically expanding training datasets. Beyond traditional data augmentation techniques, generative adversarial network (GAN) invented in 2014 in the computer vision community, provides a suite of novel approaches that can learn good data representations and generate highly realistic samples. Since 2017, there has been a growth of research into GANs for image augmentation or synthesis in agriculture for improved model performance. This paper presents an overview of the evolution of GAN architectures followed by a systematic review of their application to agriculture (https://github.com/Derekabc/GANs-Agriculture), involving various vision tasks for plant health, weeds, fruits, aquaculture, animal farming, plant phenotyping as well as postharvest detection of fruit defects. Challenges and opportunities of GANs are discussed for future research.



### Image Harmonization by Matching Regional References
- **Arxiv ID**: http://arxiv.org/abs/2204.04715v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04715v1)
- **Published**: 2022-04-10 16:23:06+00:00
- **Updated**: 2022-04-10 16:23:06+00:00
- **Authors**: Ziyue Zhu, Zhao Zhang, Zheng Lin, Ruiqi Wu, Zhi Chai, Chun-Le Guo
- **Comment**: None
- **Journal**: None
- **Summary**: To achieve visual consistency in composite images, recent image harmonization methods typically summarize the appearance pattern of global background and apply it to the global foreground without location discrepancy. However, for a real image, the appearances (illumination, color temperature, saturation, hue, texture, etc) of different regions can vary significantly. So previous methods, which transfer the appearance globally, are not optimal. Trying to solve this issue, we firstly match the contents between the foreground and background and then adaptively adjust every foreground location according to the appearance of its content-related background regions. Further, we design a residual reconstruction strategy, that uses the predicted residual to adjust the appearance, and the composite foreground to reserve the image details. Extensive experiments demonstrate the effectiveness of our method. The source code will be available publicly.



### TOV: The Original Vision Model for Optical Remote Sensing Image Understanding via Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.04716v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GT
- **Links**: [PDF](http://arxiv.org/pdf/2204.04716v1)
- **Published**: 2022-04-10 16:25:05+00:00
- **Updated**: 2022-04-10 16:25:05+00:00
- **Authors**: Chao Tao, Ji Qia, Guo Zhang, Qing Zhu, Weipeng Lu, Haifeng Li
- **Comment**: 38 pages, 5 figures, 8 Tables
- **Journal**: None
- **Summary**: Do we on the right way for remote sensing image understanding (RSIU) by training models via supervised data-dependent and task-dependent way, instead of human vision in a label-free and task-independent way? We argue that a more desirable RSIU model should be trained with intrinsic structure from data rather that extrinsic human labels to realize generalizability across a wide range of RSIU tasks. According to this hypothesis, we proposed \textbf{T}he \textbf{O}riginal \textbf{V}ision model (TOV) in remote sensing filed. Trained by massive unlabeled optical data along a human-like self-supervised learning (SSL) path that is from general knowledge to specialized knowledge, TOV model can be easily adapted to various RSIU tasks, including scene classification, object detection, and semantic segmentation, and outperforms dominant ImageNet supervised pretrained method as well as two recently proposed SSL pretrained methods on majority of 12 publicly available benchmarks. Moreover, we analyze the influences of two key factors on the performance of building TOV model for RSIU, including the influence of using different data sampling methods and the selection of learning paths during self-supervised optimization. We believe that a general model which is trained by a label-free and task-independent way may be the next paradigm for RSIU and hope the insights distilled from this study can help to foster the development of an original vision model for RSIU.



### Deep Non-rigid Structure-from-Motion: A Sequence-to-Sequence Translation Perspective
- **Arxiv ID**: http://arxiv.org/abs/2204.04730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04730v1)
- **Published**: 2022-04-10 17:13:52+00:00
- **Updated**: 2022-04-10 17:13:52+00:00
- **Authors**: Hui Deng, Tong Zhang, Yuchao Dai, Jiawei Shi, Yiran Zhong, Hongdong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Directly regressing the non-rigid shape and camera pose from the individual 2D frame is ill-suited to the Non-Rigid Structure-from-Motion (NRSfM) problem. This frame-by-frame 3D reconstruction pipeline overlooks the inherent spatial-temporal nature of NRSfM, i.e., reconstructing the whole 3D sequence from the input 2D sequence. In this paper, we propose to model deep NRSfM from a sequence-to-sequence translation perspective, where the input 2D frame sequence is taken as a whole to reconstruct the deforming 3D non-rigid shape sequence. First, we apply a shape-motion predictor to estimate the initial non-rigid shape and camera motion from a single frame. Then we propose a context modeling module to model camera motions and complex non-rigid shapes. To tackle the difficulty in enforcing the global structure constraint within the deep framework, we propose to impose the union-of-subspace structure by replacing the self-expressiveness layer with multi-head attention and delayed regularizers, which enables end-to-end batch-wise training. Experimental results across different datasets such as Human3.6M, CMU Mocap and InterHand prove the superiority of our framework. The code will be made publicly available



### A Comparative Analysis of Decision-Level Fusion for Multimodal Driver Behaviour Understanding
- **Arxiv ID**: http://arxiv.org/abs/2204.04734v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.04734v1)
- **Published**: 2022-04-10 17:49:22+00:00
- **Updated**: 2022-04-10 17:49:22+00:00
- **Authors**: Alina Roitberg, Kunyu Peng, Zdravko Marinov, Constantin Seibold, David Schneider, Rainer Stiefelhagen
- **Comment**: Accepted at Intelligent Vehicles Symposium 2022, IEEE
- **Journal**: None
- **Summary**: Visual recognition inside the vehicle cabin leads to safer driving and more intuitive human-vehicle interaction but such systems face substantial obstacles as they need to capture different granularities of driver behaviour while dealing with highly limited body visibility and changing illumination. Multimodal recognition mitigates a number of such issues: prediction outcomes of different sensors complement each other due to different modality-specific strengths and weaknesses. While several late fusion methods have been considered in previously published frameworks, they constantly feature different architecture backbones and building blocks making it very hard to isolate the role of the chosen late fusion strategy itself. This paper presents an empirical evaluation of different paradigms for decision-level late fusion in video-based driver observation. We compare seven different mechanisms for joining the results of single-modal classifiers which have been both popular, (e.g. score averaging) and not yet considered (e.g. rank-level fusion) in the context of driver observation evaluating them based on different criteria and benchmark settings. This is the first systematic study of strategies for fusing outcomes of multimodal predictors inside the vehicles, conducted with the goal to provide guidance for fusion scheme selection.



### CholecTriplet2021: A benchmark challenge for surgical action triplet recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.04746v2
- **DOI**: 10.1016/j.media.2023.102803
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04746v2)
- **Published**: 2022-04-10 18:51:55+00:00
- **Updated**: 2022-12-29 20:11:19+00:00
- **Authors**: Chinedu Innocent Nwoye, Deepak Alapatt, Tong Yu, Armine Vardazaryan, Fangfang Xia, Zixuan Zhao, Tong Xia, Fucang Jia, Yuxuan Yang, Hao Wang, Derong Yu, Guoyan Zheng, Xiaotian Duan, Neil Getty, Ricardo Sanchez-Matilla, Maria Robu, Li Zhang, Huabin Chen, Jiacheng Wang, Liansheng Wang, Bokai Zhang, Beerend Gerats, Sista Raviteja, Rachana Sathish, Rong Tao, Satoshi Kondo, Winnie Pang, Hongliang Ren, Julian Ronald Abbing, Mohammad Hasan Sarhan, Sebastian Bodenstedt, Nithya Bhasker, Bruno Oliveira, Helena R. Torres, Li Ling, Finn Gaida, Tobias Czempiel, João L. Vilaça, Pedro Morais, Jaime Fonseca, Ruby Mae Egging, Inge Nicole Wijma, Chen Qian, Guibin Bian, Zhen Li, Velmurugan Balasubramanian, Debdoot Sheet, Imanol Luengo, Yuanbo Zhu, Shuai Ding, Jakob-Anton Aschenbrenner, Nicolas Elini van der Kar, Mengya Xu, Mobarakol Islam, Lalithkumar Seenivasan, Alexander Jenke, Danail Stoyanov, Didier Mutter, Pietro Mascagni, Barbara Seeliger, Cristians Gonzalez, Nicolas Padoy
- **Comment**: CholecTriplet2021 challenge report. Paper accepted at Elsevier
  journal of Medical Image Analysis. 22 pages, 8 figures, 11 tables. Challenge
  website: https://cholectriplet2021.grand-challenge.org
- **Journal**: Medical Image Analysis 86 (2023) 102803
- **Summary**: Context-aware decision support in the operating room can foster surgical safety and efficiency by leveraging real-time feedback from surgical workflow analysis. Most existing works recognize surgical activities at a coarse-grained level, such as phases, steps or events, leaving out fine-grained interaction details about the surgical activity; yet those are needed for more helpful AI assistance in the operating room. Recognizing surgical actions as triplets of <instrument, verb, target> combination delivers comprehensive details about the activities taking place in surgical videos. This paper presents CholecTriplet2021: an endoscopic vision challenge organized at MICCAI 2021 for the recognition of surgical action triplets in laparoscopic videos. The challenge granted private access to the large-scale CholecT50 dataset, which is annotated with action triplet information. In this paper, we present the challenge setup and assessment of the state-of-the-art deep learning methods proposed by the participants during the challenge. A total of 4 baseline methods from the challenge organizers and 19 new deep learning algorithms by competing teams are presented to recognize surgical action triplets directly from surgical videos, achieving mean average precision (mAP) ranging from 4.2% to 38.1%. This study also analyzes the significance of the results obtained by the presented approaches, performs a thorough methodological comparison between them, in-depth result analysis, and proposes a novel ensemble method for enhanced recognition. Our analysis shows that surgical workflow analysis is not yet solved, and also highlights interesting directions for future research on fine-grained surgical activity recognition which is of utmost importance for the development of AI in surgery.



### Beyond Cross-view Image Retrieval: Highly Accurate Vehicle Localization Using Satellite Image
- **Arxiv ID**: http://arxiv.org/abs/2204.04752v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.04752v2)
- **Published**: 2022-04-10 19:16:58+00:00
- **Updated**: 2022-09-04 12:37:24+00:00
- **Authors**: Yujiao Shi, Hongdong Li
- **Comment**: accepted to CVPR2022
- **Journal**: None
- **Summary**: This paper addresses the problem of vehicle-mounted camera localization by matching a ground-level image with an overhead-view satellite map. Existing methods often treat this problem as cross-view image retrieval, and use learned deep features to match the ground-level query image to a partition (eg, a small patch) of the satellite map. By these methods, the localization accuracy is limited by the partitioning density of the satellite map (often in the order of tens meters). Departing from the conventional wisdom of image retrieval, this paper presents a novel solution that can achieve highly-accurate localization. The key idea is to formulate the task as pose estimation and solve it by neural-net based optimization. Specifically, we design a two-branch {CNN} to extract robust features from the ground and satellite images, respectively. To bridge the vast cross-view domain gap, we resort to a Geometry Projection module that projects features from the satellite map to the ground-view, based on a relative camera pose. Aiming to minimize the differences between the projected features and the observed features, we employ a differentiable Levenberg-Marquardt ({LM}) module to search for the optimal camera pose iteratively. The entire pipeline is differentiable and runs end-to-end. Extensive experiments on standard autonomous vehicle localization datasets have confirmed the superiority of the proposed method. Notably, e.g., starting from a coarse estimate of camera location within a wide region of 40m x 40m, with an 80% likelihood our method quickly reduces the lateral location error to be within 5m on a new KITTI cross-view dataset.



### Deflectometry for specular surfaces: an overview
- **Arxiv ID**: http://arxiv.org/abs/2204.11592v1
- **DOI**: None
- **Categories**: **physics.optics**, cs.CV, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2204.11592v1)
- **Published**: 2022-04-10 22:17:47+00:00
- **Updated**: 2022-04-10 22:17:47+00:00
- **Authors**: Jan Burke, Alexey Pak, Sebastian Höfer, Mathias Ziebarth, Masoud Roschani, Jürgen Beyerer
- **Comment**: 45 pages, 26 figures
- **Journal**: None
- **Summary**: Deflectometry as a technical approach to assessing reflective surfaces has now existed for almost 40 years. Different aspects and variations of the method have been studied in multiple theses and research articles, and reviews are also becoming available for certain subtopics. Still a field of active development with many unsolved problems, deflectometry now encompasses a large variety of application domains, hardware setup types, and processing workflows designed for different purposes, and spans a range from qualitative defect inspection of large vehicles to precision measurements of microscopic optics. Over these years, many exciting developments have accumulated in the underlying theory, in the systems design, and in the implementation specifics. This diversity of topics is difficult to grasp for experts and non-experts alike and may present an obstacle to a wider acceptance of deflectometry as a useful tool in other research fields and in the industry.   This paper presents an attempt to summarize the status of deflectometry, and to map relations between its notable "spin-off" branches. The intention of the paper is to provide a common communication basis for practitioners and at the same time to offer a convenient entry point for those interested in learning and using the method. The list of references is extensive but definitely not exhaustive, introducing some prominent trends and established research groups in order to facilitate further self-directed exploration by the reader.



### Representation Learning by Detecting Incorrect Location Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2204.04788v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.04788v2)
- **Published**: 2022-04-10 22:58:02+00:00
- **Updated**: 2023-03-13 10:13:00+00:00
- **Authors**: Sepehr Sameni, Simon Jenni, Paolo Favaro
- **Comment**: accepted at AAAI2023, https://github.com/Separius/DILEMMA
- **Journal**: None
- **Summary**: In this paper, we introduce a novel self-supervised learning (SSL) loss for image representation learning. There is a growing belief that generalization in deep neural networks is linked to their ability to discriminate object shapes. Since object shape is related to the location of its parts, we propose to detect those that have been artificially misplaced. We represent object parts with image tokens and train a ViT to detect which token has been combined with an incorrect positional embedding. We then introduce sparsity in the inputs to make the model more robust to occlusions and to speed up the training. We call our method DILEMMA, which stands for Detection of Incorrect Location EMbeddings with MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an improvement in their performance of respectively 4.41%, 3.97%, and 0.5% under the same training time and with a linear probing transfer on ImageNet-1K. We also show full fine-tuning improvements of MAE combined with our method on ImageNet-100. We evaluate our method via fine-tuning on common SSL benchmarks. Moreover, we show that when downstream tasks are strongly reliant on shape (such as in the YOGA-82 pose dataset), our pre-trained features yield a significant gain over prior work.



### SOS! Self-supervised Learning Over Sets Of Handled Objects In Egocentric Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.04796v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.04796v2)
- **Published**: 2022-04-10 23:27:19+00:00
- **Updated**: 2022-05-02 23:39:56+00:00
- **Authors**: Victor Escorcia, Ricardo Guerrero, Xiatian Zhu, Brais Martinez
- **Comment**: None
- **Journal**: None
- **Summary**: Learning an egocentric action recognition model from video data is challenging due to distractors (e.g., irrelevant objects) in the background. Further integrating object information into an action model is hence beneficial. Existing methods often leverage a generic object detector to identify and represent the objects in the scene. However, several important issues remain. Object class annotations of good quality for the target domain (dataset) are still required for learning good object representation. Besides, previous methods deeply couple the existing action models and need to retrain them jointly with object representation, leading to costly and inflexible integration. To overcome both limitations, we introduce Self-Supervised Learning Over Sets (SOS), an approach to pre-train a generic Objects In Contact (OIC) representation model from video object regions detected by an off-the-shelf hand-object contact detector. Instead of augmenting object regions individually as in conventional self-supervised learning, we view the action process as a means of natural data transformations with unique spatio-temporal continuity and exploit the inherent relationships among per-video object sets. Extensive experiments on two datasets, EPIC-KITCHENS-100 and EGTEA, show that our OIC significantly boosts the performance of multiple state-of-the-art video classification models.



### DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.04799v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.04799v2)
- **Published**: 2022-04-10 23:36:55+00:00
- **Updated**: 2022-08-05 11:26:06+00:00
- **Authors**: Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, Tomas Pfister
- **Comment**: Published at ECCV 2022 as a conference paper
- **Journal**: None
- **Summary**: Continual learning aims to enable a single model to learn a sequence of tasks without catastrophic forgetting. Top-performing methods usually require a rehearsal buffer to store past pristine examples for experience replay, which, however, limits their practical value due to privacy and memory constraints. In this work, we present a simple yet effective framework, DualPrompt, which learns a tiny set of parameters, called prompts, to properly instruct a pre-trained model to learn tasks arriving sequentially without buffering past examples. DualPrompt presents a novel approach to attach complementary prompts to the pre-trained backbone, and then formulates the objective as learning task-invariant and task-specific "instructions". With extensive experimental validation, DualPrompt consistently sets state-of-the-art performance under the challenging class-incremental setting. In particular, DualPrompt outperforms recent advanced continual learning methods with relatively large buffer sizes. We also introduce a more challenging benchmark, Split ImageNet-R, to help generalize rehearsal-free continual learning research. Source code is available at https://github.com/google-research/l2p.



