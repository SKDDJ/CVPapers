# Arxiv Papers in cs.CV on 2022-04-04
### Revisiting Sliced Wasserstein on Images: From Vectorization to Convolution
- **Arxiv ID**: http://arxiv.org/abs/2204.01188v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2204.01188v4)
- **Published**: 2022-04-04 00:17:28+00:00
- **Updated**: 2022-09-23 14:58:23+00:00
- **Authors**: Khai Nguyen, Nhat Ho
- **Comment**: Accepted to NeurIPS 2022, 29 pages, 9 figures, 11 tables
- **Journal**: None
- **Summary**: The conventional sliced Wasserstein is defined between two probability measures that have realizations as vectors. When comparing two probability measures over images, practitioners first need to vectorize images and then project them to one-dimensional space by using matrix multiplication between the sample matrix and the projection matrix. After that, the sliced Wasserstein is evaluated by averaging the two corresponding one-dimensional projected probability measures. However, this approach has two limitations. The first limitation is that the spatial structure of images is not captured efficiently by the vectorization step; therefore, the later slicing process becomes harder to gather the discrepancy information. The second limitation is memory inefficiency since each slicing direction is a vector that has the same dimension as the images. To address these limitations, we propose novel slicing methods for sliced Wasserstein between probability measures over images that are based on the convolution operators. We derive convolution sliced Wasserstein (CSW) and its variants via incorporating stride, dilation, and non-linear activation function into the convolution operators. We investigate the metricity of CSW as well as its sample complexity, its computational complexity, and its connection to conventional sliced Wasserstein distances. Finally, we demonstrate the favorable performance of CSW over the conventional sliced Wasserstein in comparing probability measures over images and in training deep generative modeling on images.



### Unsupervised Change Detection Based on Image Reconstruction Loss
- **Arxiv ID**: http://arxiv.org/abs/2204.01200v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01200v2)
- **Published**: 2022-04-04 01:40:34+00:00
- **Updated**: 2022-04-05 01:16:15+00:00
- **Authors**: Hyeoncheol Noh, Jingi Ju, Minseok Seo, Jongchan Park, Dong-Geol Choi
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: To train the change detector, bi-temporal images taken at different times in the same area are used. However, collecting labeled bi-temporal images is expensive and time consuming. To solve this problem, various unsupervised change detection methods have been proposed, but they still require unlabeled bi-temporal images. In this paper, we propose unsupervised change detection based on image reconstruction loss using only unlabeled single temporal single image. The image reconstruction model is trained to reconstruct the original source image by receiving the source image and the photometrically transformed source image as a pair. During inference, the model receives bi-temporal images as the input, and tries to reconstruct one of the inputs. The changed region between bi-temporal images shows high reconstruction loss. Our change detector showed significant performance in various change detection benchmark datasets even though only a single temporal single source image was used. The code and trained models will be publicly available for reproducibility.



### A Novel Mask R-CNN Model to Segment Heterogeneous Brain Tumors through Image Subtraction
- **Arxiv ID**: http://arxiv.org/abs/2204.01201v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01201v1)
- **Published**: 2022-04-04 01:45:11+00:00
- **Updated**: 2022-04-04 01:45:11+00:00
- **Authors**: Sanskriti Singh
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: The segmentation of diseases is a popular topic explored by researchers in the field of machine learning. Brain tumors are extremely dangerous and require the utmost precision to segment for a successful surgery. Patients with tumors usually take 4 MRI scans, T1, T1gd, T2, and FLAIR, which are then sent to radiologists to segment and analyze for possible future surgery. To create a second segmentation, it would be beneficial to both radiologists and patients in being more confident in their conclusions. We propose using a method performed by radiologists called image segmentation and applying it to machine learning models to prove a better segmentation. Using Mask R-CNN, its ResNet backbone being pre-trained on the RSNA pneumonia detection challenge dataset, we can train a model on the Brats2020 Brain Tumor dataset. Center for Biomedical Image Computing & Analytics provides MRI data on patients with and without brain tumors and the corresponding segmentations. We can see how well the method of image subtraction works by comparing it to models without image subtraction through DICE coefficient (F1 score), recall, and precision on the untouched test set. Our model performed with a DICE coefficient of 0.75 in comparison to 0.69 without image subtraction. To further emphasize the usefulness of image subtraction, we compare our final model to current state-of-the-art models to segment tumors from MRI scans.



### Attribute Prototype Network for Any-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.01208v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01208v1)
- **Published**: 2022-04-04 02:25:40+00:00
- **Updated**: 2022-04-04 02:25:40+00:00
- **Authors**: Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, Zeynep Akata
- **Comment**: arXiv admin note: text overlap with arXiv:2008.08290
- **Journal**: None
- **Summary**: Any-shot image classification allows to recognize novel classes with only a few or even zero samples. For the task of zero-shot learning, visual attributes have been shown to play an important role, while in the few-shot regime, the effect of attributes is under-explored. To better transfer attribute-based knowledge from seen to unseen classes, we argue that an image representation with integrated attribute localization ability would be beneficial for any-shot, i.e. zero-shot and few-shot, image classification tasks. To this end, we propose a novel representation learning framework that jointly learns discriminative global and local features using only class-level attributes. While a visual-semantic embedding layer learns global features, local features are learned through an attribute prototype network that simultaneously regresses and decorrelates attributes from intermediate features. Furthermore, we introduce a zoom-in module that localizes and crops the informative regions to encourage the network to learn informative features explicitly. We show that our locality augmented image representations achieve a new state-of-the-art on challenging benchmarks, i.e. CUB, AWA2, and SUN. As an additional benefit, our model points to the visual evidence of the attributes in an image, confirming the improved attribute localization ability of our image representation. The attribute localization is evaluated quantitatively with ground truth part annotations, qualitatively with visualizations, and through well-designed user studies.



### EResFD: Rediscovery of the Effectiveness of Standard Convolution for Lightweight Face Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.01209v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01209v2)
- **Published**: 2022-04-04 02:30:43+00:00
- **Updated**: 2022-06-13 08:40:00+00:00
- **Authors**: Joonhyun Jeong, Beomyoung Kim, Joonsang Yu, Youngjoon Yoo
- **Comment**: None
- **Journal**: None
- **Summary**: This paper analyses the design choices of face detection architecture that improve efficiency between computation cost and accuracy. Specifically, we re-examine the effectiveness of the standard convolutional block as a lightweight backbone architecture on face detection. Unlike the current tendency of lightweight architecture design, which heavily utilizes depthwise separable convolution layers, we show that heavily channel-pruned standard convolution layer can achieve better accuracy and inference speed when using a similar parameter size. This observation is supported by the analyses concerning the characteristics of the target data domain, face. Based on our observation, we propose to employ ResNet with a highly reduced channel, which surprisingly allows high efficiency compared to other mobile-friendly networks (e.g., MobileNet-V1,-V2,-V3). From the extensive experiments, we show that the proposed backbone can replace that of the state-of-the-art face detector with a faster inference speed. Also, we further propose a new feature aggregation method maximizing the detection performance. Our proposed detector EResFD obtained 80.4% mAP on WIDER FACE Hard subset which only takes 37.7 ms for VGA image inference in on CPU. Code will be available at https://github.com/clovaai/EResFD.



### Co-Teaching for Unsupervised Domain Adaptation and Expansion
- **Arxiv ID**: http://arxiv.org/abs/2204.01210v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01210v2)
- **Published**: 2022-04-04 02:34:26+00:00
- **Updated**: 2022-07-27 06:28:03+00:00
- **Authors**: Kaibin Tian, Qijie Wei, Xirong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) is known to trade a model's performance on a source domain for improving its performance on a target domain. To resolve the issue, Unsupervised Domain Expansion (UDE) has been proposed recently to adapt the model for the target domain as UDA does, and in the meantime maintain its performance on the source domain. For both UDA and UDE, a model tailored to a given domain, let it be the source or the target domain, is assumed to well handle samples from the given domain. We question the assumption by reporting the existence of cross-domain visual ambiguity: Due to the lack of a crystally clear boundary between the two domains, samples from one domain can be visually close to the other domain. We exploit this finding and accordingly propose in this paper Co-Teaching (CT) that consists of knowledge distillation based CT (kdCT) and mixup based CT (miCT). Specifically, kdCT transfers knowledge from a leader-teacher network and an assistant-teacher network to a student network, so the cross-domain visual ambiguity will be better handled by the student. Meanwhile, miCT further enhances the generalization ability of the student. Comprehensive experiments on two image-classification benchmarks and two driving-scene-segmentation benchmarks justify the viability of the proposed method.



### Neural Rendering of Humans in Novel View and Pose from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2204.01218v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01218v2)
- **Published**: 2022-04-04 03:09:20+00:00
- **Updated**: 2023-04-20 04:08:04+00:00
- **Authors**: Tiantian Wang, Nikolaos Sarafianos, Ming-Hsuan Yang, Tony Tung
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We introduce a new method that generates photo-realistic humans under novel views and poses given a monocular video as input. Despite the significant progress recently on this topic, with several methods exploring shared canonical neural radiance fields in dynamic scene scenarios, learning a user-controlled model for unseen poses remains a challenging task. To tackle this problem, we introduce an effective method to a) integrate observations across several frames and b) encode the appearance at each individual frame. We accomplish this by utilizing both the human pose that models the body shape as well as point clouds that partially cover the human as input. Our approach simultaneously learns a shared set of latent codes anchored to the human pose among several frames, and an appearance-dependent code anchored to incomplete point clouds generated by each frame and its predicted depth. The former human pose-based code models the shape of the performer whereas the latter point cloud-based code predicts fine-level details and reasons about missing structures at the unseen poses. To further recover non-visible regions in query frames, we employ a temporal transformer to integrate features of points in query frames and tracked body points from automatically-selected key frames. Experiments on various sequences of dynamic humans from different datasets including ZJU-MoCap show that our method significantly outperforms existing approaches under unseen poses and novel views given monocular videos as input.



### Explicit and Implicit Pattern Relation Analysis for Discovering Actionable Negative Sequences
- **Arxiv ID**: http://arxiv.org/abs/2204.03571v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.03571v1)
- **Published**: 2022-04-04 04:26:09+00:00
- **Updated**: 2022-04-04 04:26:09+00:00
- **Authors**: Wei Wang, Longbing Cao
- **Comment**: 17 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: Real-life events, behaviors and interactions produce sequential data. An important but rarely explored problem is to analyze those nonoccurring (also called negative) yet important sequences, forming negative sequence analysis (NSA). A typical NSA area is to discover negative sequential patterns (NSPs) consisting of important non-occurring and occurring elements and patterns. The limited existing work on NSP mining relies on frequentist and downward closure property-based pattern selection, producing large and highly redundant NSPs, nonactionable for business decision-making. This work makes the first attempt for actionable NSP discovery. It builds an NSP graph representation, quantify both explicit occurrence and implicit non-occurrence-based element and pattern relations, and then discover significant, diverse and informative NSPs in the NSP graph to represent the entire NSP set for discovering actionable NSPs. A DPP-based NSP representation and actionable NSP discovery method EINSP introduces novel and significant contributions for NSA and sequence analysis: (1) it represents NSPs by a determinantal point process (DPP) based graph; (2) it quantifies actionable NSPs in terms of their statistical significance, diversity, and strength of explicit/implicit element/pattern relations; and (3) it models and measures both explicit and implicit element/pattern relations in the DPP-based NSP graph to represent direct and indirect couplings between NSP items, elements and patterns. We substantially analyze the effectiveness of EINSP in terms of various theoretical and empirical aspects including complexity, item/pattern coverage, pattern size and diversity, implicit pattern relation strength, and data factors.



### Soft Threshold Ternary Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.01234v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01234v1)
- **Published**: 2022-04-04 04:43:42+00:00
- **Updated**: 2022-04-04 04:43:42+00:00
- **Authors**: Weixiang Xu, Xiangyu He, Tianli Zhao, Qinghao Hu, Peisong Wang, Jian Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Large neural networks are difficult to deploy on mobile devices because of intensive computation and storage. To alleviate it, we study ternarization, a balance between efficiency and accuracy that quantizes both weights and activations into ternary values. In previous ternarized neural networks, a hard threshold {\Delta} is introduced to determine quantization intervals. Although the selection of {\Delta} greatly affects the training results, previous works estimate {\Delta} via an approximation or treat it as a hyper-parameter, which is suboptimal. In this paper, we present the Soft Threshold Ternary Networks (STTN), which enables the model to automatically determine quantization intervals instead of depending on a hard threshold. Concretely, we replace the original ternary kernel with the addition of two binary kernels at training time, where ternary values are determined by the combination of two corresponding binary values. At inference time, we add up the two binary kernels to obtain a single ternary kernel. Our method dramatically outperforms current state-of-the-arts, lowering the performance gap between full-precision networks and extreme low bit networks. Experiments on ImageNet with ResNet-18 (Top-1 66.2%) achieves new state-of-the-art.   Update: In this version, we further fine-tune the experimental hyperparameters and training procedure. The latest STTN shows that ResNet-18 with ternary weights and ternary activations achieves up to 68.2% Top-1 accuracy on ImageNet. Code is available at: github.com/WeixiangXu/STTN.



### Dynamic Focus-aware Positional Queries for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.01244v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01244v3)
- **Published**: 2022-04-04 05:16:41+00:00
- **Updated**: 2023-03-28 02:42:17+00:00
- **Authors**: Haoyu He, Jianfei Cai, Zizheng Pan, Jing Liu, Jing Zhang, Dacheng Tao, Bohan Zhuang
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: The DETR-like segmentors have underpinned the most recent breakthroughs in semantic segmentation, which end-to-end train a set of queries representing the class prototypes or target segments. Recently, masked attention is proposed to restrict each query to only attend to the foreground regions predicted by the preceding decoder block for easier optimization. Although promising, it relies on the learnable parameterized positional queries which tend to encode the dataset statistics, leading to inaccurate localization for distinct individual queries. In this paper, we propose a simple yet effective query design for semantic segmentation termed Dynamic Focus-aware Positional Queries (DFPQ), which dynamically generates positional queries conditioned on the cross-attention scores from the preceding decoder block and the positional encodings for the corresponding image features, simultaneously. Therefore, our DFPQ preserves rich localization information for the target segments and provides accurate and fine-grained positional priors. In addition, we propose to efficiently deal with high-resolution cross-attention by only aggregating the contextual tokens based on the low-resolution cross-attention scores to perform local relation aggregation. Extensive experiments on ADE20K and Cityscapes show that with the two modifications on Mask2former, our framework achieves SOTA performance and outperforms Mask2former by clear margins of 1.1%, 1.9%, and 1.1% single-scale mIoU with ResNet-50, Swin-T, and Swin-B backbones on the ADE20K validation set, respectively. Source code is available at https://github.com/ziplab/FASeg



### Differentiable Rendering for Synthetic Aperture Radar Imagery
- **Arxiv ID**: http://arxiv.org/abs/2204.01248v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01248v2)
- **Published**: 2022-04-04 05:27:40+00:00
- **Updated**: 2023-08-07 22:21:24+00:00
- **Authors**: Michael Wilmanski, Jonathan Tamir
- **Comment**: This version of the manuscript is an updated preprint which has been
  recently accepted by IEEE Transactions on Aerospace Electronic Systems, but
  has not yet been published or processed by IEEE
- **Journal**: None
- **Summary**: There is rising interest in differentiable rendering, which allows explicitly modeling geometric priors and constraints in optimization pipelines using first-order methods such as backpropagation. Incorporating such domain knowledge can lead to deep neural networks that are trained more robustly and with limited data, as well as the capability to solve ill-posed inverse problems. Existing efforts in differentiable rendering have focused on imagery from electro-optical sensors, particularly conventional RGB-imagery. In this work, we propose an approach for differentiable rendering of Synthetic Aperture Radar (SAR) imagery, which combines methods from 3D computer graphics with neural rendering. We demonstrate the approach on the inverse graphics problem of 3D Object Reconstruction from limited SAR imagery using high-fidelity simulated SAR data.



### BatchFormerV2: Exploring Sample Relationships for Dense Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.01254v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.01254v1)
- **Published**: 2022-04-04 05:53:42+00:00
- **Updated**: 2022-04-04 05:53:42+00:00
- **Authors**: Zhi Hou, Baosheng Yu, Chaoyue Wang, Yibing Zhan, Dacheng Tao
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Attention mechanisms have been very popular in deep neural networks, where the Transformer architecture has achieved great success in not only natural language processing but also visual recognition applications. Recently, a new Transformer module, applying on batch dimension rather than spatial/channel dimension, i.e., BatchFormer [18], has been introduced to explore sample relationships for overcoming data scarcity challenges. However, it only works with image-level representations for classification. In this paper, we devise a more general batch Transformer module, BatchFormerV2, which further enables exploring sample relationships for dense representation learning. Specifically, when applying the proposed module, it employs a two-stream pipeline during training, i.e., either with or without a BatchFormerV2 module, where the batchformer stream can be removed for testing. Therefore, the proposed method is a plug-and-play module and can be easily integrated into different vision Transformers without any extra inference cost. Without bells and whistles, we show the effectiveness of the proposed method for a variety of popular visual recognition tasks, including image classification and two important dense prediction tasks: object detection and panoptic segmentation. Particularly, BatchFormerV2 consistently improves current DETR-based detection methods (e.g., DETR, Deformable-DETR, Conditional DETR, and SMCA) by over 1.3%. Code will be made publicly available.



### Direct Dense Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2204.01263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01263v1)
- **Published**: 2022-04-04 06:14:38+00:00
- **Updated**: 2022-04-04 06:14:38+00:00
- **Authors**: Liqian Ma, Lingjie Liu, Christian Theobalt, Luc Van Gool
- **Comment**: Accepted to 3DV 2021. Project page
  http://charliememory.github.io/3DV21_DDP/
- **Journal**: None
- **Summary**: Dense human pose estimation is the problem of learning dense correspondences between RGB images and the surfaces of human bodies, which finds various applications, such as human body reconstruction, human pose transfer, and human action recognition. Prior dense pose estimation methods are all based on Mask R-CNN framework and operate in a top-down manner of first attempting to identify a bounding box for each person and matching dense correspondences in each bounding box. Consequently, these methods lack robustness due to their critical dependence on the Mask R-CNN detection, and the runtime increases drastically as the number of persons in the image increases. We therefore propose a novel alternative method for solving the dense pose estimation problem, called Direct Dense Pose (DDP). DDP first predicts the instance mask and global IUV representation separately and then combines them together. We also propose a simple yet effective 2D temporal-smoothing scheme to alleviate the temporal jitters when dealing with video data. Experiments demonstrate that DDP overcomes the limitations of previous top-down baseline methods and achieves competitive accuracy. In addition, DDP is computationally more efficient than previous dense pose estimation methods, and it reduces jitters when applied to a video sequence, which is a problem plaguing the previous methods.



### Probabilistic Implicit Scene Completion
- **Arxiv ID**: http://arxiv.org/abs/2204.01264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01264v1)
- **Published**: 2022-04-04 06:16:54+00:00
- **Updated**: 2022-04-04 06:16:54+00:00
- **Authors**: Dongsu Zhang, Changwoon Choi, Inbum Park, Young Min Kim
- **Comment**: Accepted to ICLR 2022 as spotlight, code available at
  https://github.com/96lives/gca
- **Journal**: None
- **Summary**: We propose a probabilistic shape completion method extended to the continuous geometry of large-scale 3D scenes. Real-world scans of 3D scenes suffer from a considerable amount of missing data cluttered with unsegmented objects. The problem of shape completion is inherently ill-posed, and high-quality result requires scalable solutions that consider multiple possible outcomes. We employ the Generative Cellular Automata that learns the multi-modal distribution and transform the formulation to process large-scale continuous geometry. The local continuous shape is incrementally generated as a sparse voxel embedding, which contains the latent code for each occupied cell. We formally derive that our training objective for the sparse voxel embedding maximizes the variational lower bound of the complete shape distribution and therefore our progressive generation constitutes a valid generative model. Experiments show that our model successfully generates diverse plausible scenes faithful to the input, especially when the input suffers from a significant amount of missing data. We also demonstrate that our approach outperforms deterministic models even in less ambiguous cases with a small amount of missing data, which infers that probabilistic formulation is crucial for high-quality geometry completion on input scans exhibiting any levels of completeness.



### Multi-modality Associative Bridging through Memory: Speech Sound Recollected from Face Video
- **Arxiv ID**: http://arxiv.org/abs/2204.01265v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2204.01265v1)
- **Published**: 2022-04-04 06:19:41+00:00
- **Updated**: 2022-04-04 06:19:41+00:00
- **Authors**: Minsu Kim, Joanna Hong, Se Jin Park, Yong Man Ro
- **Comment**: Published at ICCV 2021
- **Journal**: None
- **Summary**: In this paper, we introduce a novel audio-visual multi-modal bridging framework that can utilize both audio and visual information, even with uni-modal inputs. We exploit a memory network that stores source (i.e., visual) and target (i.e., audio) modal representations, where source modal representation is what we are given, and target modal representations are what we want to obtain from the memory network. We then construct an associative bridge between source and target memories that considers the interrelationship between the two memories. By learning the interrelationship through the associative bridge, the proposed bridging framework is able to obtain the target modal representations inside the memory network, even with the source modal input only, and it provides rich information for its downstream tasks. We apply the proposed framework to two tasks: lip reading and speech reconstruction from silent video. Through the proposed associative bridge and modality-specific memories, each task knowledge is enriched with the recalled audio context, achieving state-of-the-art performance. We also verify that the associative bridge properly relates the source and target memories.



### FoV-Net: Field-of-View Extrapolation Using Self-Attention and Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2204.01267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01267v1)
- **Published**: 2022-04-04 06:24:03+00:00
- **Updated**: 2022-04-04 06:24:03+00:00
- **Authors**: Liqian Ma, Stamatios Georgoulis, Xu Jia, Luc Van Gool
- **Comment**: Accepted to IEEE Robotics and Automation Letters and ICRA2021.
  Project page http://charliememory.github.io/RAL21_FoV
- **Journal**: None
- **Summary**: The ability to make educated predictions about their surroundings, and associate them with certain confidence, is important for intelligent systems, like autonomous vehicles and robots. It allows them to plan early and decide accordingly. Motivated by this observation, in this paper we utilize information from a video sequence with a narrow field-of-view to infer the scene at a wider field-of-view. To this end, we propose a temporally consistent field-of-view extrapolation framework, namely FoV-Net, that: (1) leverages 3D information to propagate the observed scene parts from past frames; (2) aggregates the propagated multi-frame information using an attention-based feature aggregation module and a gated self-attention module, simultaneously hallucinating any unobserved scene parts; and (3) assigns an interpretable uncertainty value at each pixel. Extensive experiments show that FoV-Net does not only extrapolate the temporally consistent wide field-of-view scene better than existing alternatives, but also provides the associated uncertainty which may benefit critical decision-making downstream applications. Project page is at http://charliememory.github.io/RAL21_FoV.



### Improving Monocular Visual Odometry Using Learned Depth
- **Arxiv ID**: http://arxiv.org/abs/2204.01268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01268v1)
- **Published**: 2022-04-04 06:26:46+00:00
- **Updated**: 2022-04-04 06:26:46+00:00
- **Authors**: Libo Sun, Wei Yin, Enze Xie, Zhengrong Li, Changming Sun, Chunhua Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular visual odometry (VO) is an important task in robotics and computer vision. Thus far, how to build accurate and robust monocular VO systems that can work well in diverse scenarios remains largely unsolved. In this paper, we propose a framework to exploit monocular depth estimation for improving VO. The core of our framework is a monocular depth estimation module with a strong generalization capability for diverse scenes. It consists of two separate working modes to assist the localization and mapping. With a single monocular image input, the depth estimation module predicts a relative depth to help the localization module on improving the accuracy. With a sparse depth map and an RGB image input, the depth estimation module can generate accurate scale-consistent depth for dense mapping. Compared with current learning-based VO methods, our method demonstrates a stronger generalization ability to diverse scenes. More significantly, our framework is able to boost the performances of existing geometry-based VO methods by a large margin.



### Distinguishing Homophenes Using Multi-Head Visual-Audio Memory for Lip Reading
- **Arxiv ID**: http://arxiv.org/abs/2204.01725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01725v1)
- **Published**: 2022-04-04 06:29:35+00:00
- **Updated**: 2022-04-04 06:29:35+00:00
- **Authors**: Minsu Kim, Jeong Hun Yeo, Yong Man Ro
- **Comment**: Published at AAAI 2022
- **Journal**: None
- **Summary**: Recognizing speech from silent lip movement, which is called lip reading, is a challenging task due to 1) the inherent information insufficiency of lip movement to fully represent the speech, and 2) the existence of homophenes that have similar lip movement with different pronunciations. In this paper, we try to alleviate the aforementioned two challenges in lip reading by proposing a Multi-head Visual-audio Memory (MVM). Firstly, MVM is trained with audio-visual datasets and remembers audio representations by modelling the inter-relationships of paired audio-visual representations. At the inference stage, visual input alone can extract the saved audio representation from the memory by examining the learned inter-relationships. Therefore, the lip reading model can complement the insufficient visual information with the extracted audio representations. Secondly, MVM is composed of multi-head key memories for saving visual features and one value memory for saving audio knowledge, which is designed to distinguish the homophenes. With the multi-head key memories, MVM extracts possible candidate audio features from the memory, which allows the lip reading model to consider the possibility of which pronunciations can be represented from the input lip movement. This also can be viewed as an explicit implementation of the one-to-many mapping of viseme-to-phoneme. Moreover, MVM is employed in multi-temporal levels to consider the context when retrieving the memory and distinguish the homophenes. Extensive experimental results verify the effectiveness of the proposed method in lip reading and in distinguishing the homophenes.



### Lip to Speech Synthesis with Visual Context Attentional GAN
- **Arxiv ID**: http://arxiv.org/abs/2204.01726v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2204.01726v1)
- **Published**: 2022-04-04 06:49:05+00:00
- **Updated**: 2022-04-04 06:49:05+00:00
- **Authors**: Minsu Kim, Joanna Hong, Yong Man Ro
- **Comment**: Published at NeurIPS 2021
- **Journal**: None
- **Summary**: In this paper, we propose a novel lip-to-speech generative adversarial network, Visual Context Attentional GAN (VCA-GAN), which can jointly model local and global lip movements during speech synthesis. Specifically, the proposed VCA-GAN synthesizes the speech from local lip visual features by finding a mapping function of viseme-to-phoneme, while global visual context is embedded into the intermediate layers of the generator to clarify the ambiguity in the mapping induced by homophene. To achieve this, a visual context attention module is proposed where it encodes global representations from the local visual features, and provides the desired global visual context corresponding to the given coarse speech representation to the generator through audio-visual attention. In addition to the explicit modelling of local and global visual representations, synchronization learning is introduced as a form of contrastive learning that guides the generator to synthesize a speech in sync with the given input lip movements. Extensive experiments demonstrate that the proposed VCA-GAN outperforms existing state-of-the-art and is able to effectively synthesize the speech from multi-speaker that has been barely handled in the previous works.



### Aligning Silhouette Topology for Self-Adaptive 3D Human Pose Recovery
- **Arxiv ID**: http://arxiv.org/abs/2204.01276v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.01276v1)
- **Published**: 2022-04-04 06:58:15+00:00
- **Updated**: 2022-04-04 06:58:15+00:00
- **Authors**: Mugalodi Rakesh, Jogendra Nath Kundu, Varun Jampani, R. Venkatesh Babu
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Articulation-centric 2D/3D pose supervision forms the core training objective in most existing 3D human pose estimation techniques. Except for synthetic source environments, acquiring such rich supervision for each real target domain at deployment is highly inconvenient. However, we realize that standard foreground silhouette estimation techniques (on static camera feeds) remain unaffected by domain-shifts. Motivated by this, we propose a novel target adaptation framework that relies only on silhouette supervision to adapt a source-trained model-based regressor. However, in the absence of any auxiliary cue (multi-view, depth, or 2D pose), an isolated silhouette loss fails to provide a reliable pose-specific gradient and requires to be employed in tandem with a topology-centric loss. To this end, we develop a series of convolution-friendly spatial transformations in order to disentangle a topological-skeleton representation from the raw silhouette. Such a design paves the way to devise a Chamfer-inspired spatial topological-alignment loss via distance field computation, while effectively avoiding any gradient hindering spatial-to-pointset mapping. Experimental results demonstrate our superiority against prior-arts in self-adapting a source trained model to diverse unlabeled target domains, such as a) in-the-wild datasets, b) low-resolution image domains, and c) adversarially perturbed image domains (via UAP).



### SPFNet:Subspace Pyramid Fusion Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.01278v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.01278v1)
- **Published**: 2022-04-04 07:04:50+00:00
- **Updated**: 2022-04-04 07:04:50+00:00
- **Authors**: Mohammed A. M. Elhassan, Chenhui Yang, Chenxi Huang, Tewodros Legesse Munea
- **Comment**: None
- **Journal**: None
- **Summary**: The encoder-decoder structure has significantly improved performance in many vision tasks by fusing low-level and high-level feature maps. However, this approach can hardly extract sufficient context information for pixel-wise segmentation. In addition, extracting similar low-level features at multiple scales could lead to redundant information. To tackle these issues, we propose Subspace Pyramid Fusion Network (SPFNet). Specifically, we combine pyramidal module and context aggregation module to exploit the impact of multi-scale/global context information. At first, we construct a Subspace Pyramid Fusion Module (SPFM) based on Reduced Pyramid Pooling (RPP). Then, we propose the Efficient Global Context Aggregation (EGCA) module to capture discriminative features by fusing multi-level global context features. Finally, we add decoder-based subpixel convolution to retrieve the high-resolution feature maps, which can help select category localization details. SPFM learns separate RPP for each feature subspace to capture multi-scale feature representations, which is more useful for semantic segmentation. EGCA adopts shuffle attention mechanism to enhance communication across different sub-features. Experimental results on two well-known semantic segmentation datasets, including Camvid and Cityscapes, show that our proposed method is competitive with other state-of-the-art methods.



### Learning Constrained Dynamic Correlations in Spatiotemporal Graphs for Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2204.01297v5
- **DOI**: 10.1109/TNNLS.2023.3277476
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01297v5)
- **Published**: 2022-04-04 08:11:06+00:00
- **Updated**: 2023-06-03 05:36:56+00:00
- **Authors**: Jiajun Fu, Fuxing Yang, Yonghao Dang, Xiaoli Liu, Jianqin Yin
- **Comment**: Accepted by TNNLS. Codes are available at
  https://github.com/Jaakk0F/DSTD-GCN
- **Journal**: None
- **Summary**: Human motion prediction is challenging due to the complex spatiotemporal feature modeling. Among all methods, graph convolution networks (GCNs) are extensively utilized because of their superiority in explicit connection modeling. Within a GCN, the graph correlation adjacency matrix drives feature aggregation and is the key to extracting predictive motion features. State-of-the-art methods decompose the spatiotemporal correlation into spatial correlations for each frame and temporal correlations for each joint. Directly parameterizing these correlations introduces redundant parameters to represent common relations shared by all frames and all joints. Besides, the spatiotemporal graph adjacency matrix is the same for different motion samples and cannot reflect sample-wise correspondence variances. To overcome these two bottlenecks, we propose dynamic spatiotemporal decompose GC (DSTD-GC), which only takes 28.6% parameters of the state-of-the-art GC. The key of DSTD-GC is constrained dynamic correlation modeling, which explicitly parameterizes the common static constraints as a spatial/temporal vanilla adjacency matrix shared by all frames/joints and dynamically extracts correspondence variances for each frame/joint with an adjustment modeling function. For each sample, the common constrained adjacency matrices are fixed to represent generic motion patterns, while the extracted variances complete the matrices with specific pattern adjustments. Meanwhile, we mathematically reformulate GCs on spatiotemporal graphs into a unified form and find that DSTD-GC relaxes certain constraints of other GC, which contributes to a better representation capability. By combining DSTD-GC with prior knowledge, we propose a powerful spatiotemporal GCN called DSTD-GCN, which outperforms SOTA methods by $3.9\% \sim 8.7\%$ in prediction accuracy with $55.0\% \sim 96.9\%$ fewer parameters.



### REM: Routing Entropy Minimization for Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.01298v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.01298v1)
- **Published**: 2022-04-04 08:13:16+00:00
- **Updated**: 2022-04-04 08:13:16+00:00
- **Authors**: Riccardo Renzulli, Enzo Tartaglione, Marco Grangetto
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule Networks ambition is to build an explainable and biologically-inspired neural network model. One of their main innovations relies on the routing mechanism which extracts a parse tree: its main purpose is to explicitly build relationships between capsules. However, their true potential in terms of explainability has not surfaced yet: these relationships are extremely heterogeneous and difficult to understand. This paper proposes REM, a technique which minimizes the entropy of the parse tree-like structure, improving its explainability. We accomplish this by driving the model parameters distribution towards low entropy configurations, using a pruning mechanism as a proxy. We also generate static parse trees with no performance loss, showing that, with REM, Capsule Networks build stronger relationships between capsules.



### Flexible Portrait Image Editing with Fine-Grained Control
- **Arxiv ID**: http://arxiv.org/abs/2204.01318v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.01318v1)
- **Published**: 2022-04-04 08:39:37+00:00
- **Updated**: 2022-04-04 08:39:37+00:00
- **Authors**: Linlin Liu, Qian Fu, Fei Hou, Ying He
- **Comment**: None
- **Journal**: None
- **Summary**: We develop a new method for portrait image editing, which supports fine-grained editing of geometries, colors, lights and shadows using a single neural network model. We adopt a novel asymmetric conditional GAN architecture: the generators take the transformed conditional inputs, such as edge maps, color palette, sliders and masks, that can be directly edited by the user; the discriminators take the conditional inputs in the way that can guide controllable image generation more effectively. Taking color editing as an example, we feed color palettes (which can be edited easily) into the generator, and color maps (which contain positional information of colors) into the discriminator. We also design a region-weighted discriminator so that higher weights are assigned to more important regions, like eyes and skin. Using a color palette, the user can directly specify the desired colors of hair, skin, eyes, lip and background. Color sliders allow the user to blend colors in an intuitive manner. The user can also edit lights and shadows by modifying the corresponding masks. We demonstrate the effectiveness of our method by evaluating it on the CelebAMask-HQ dataset with a wide range of tasks, including geometry/color/shadow/light editing, hand-drawn sketch to image translation, and color transfer. We also present ablation studies to justify our design.



### RayMVSNet: Learning Ray-based 1D Implicit Fields for Accurate Multi-View Stereo
- **Arxiv ID**: http://arxiv.org/abs/2204.01320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01320v1)
- **Published**: 2022-04-04 08:43:38+00:00
- **Updated**: 2022-04-04 08:43:38+00:00
- **Authors**: Junhua Xi, Yifei Shi, Yijie Wang, Yulan Guo, Kai Xu
- **Comment**: cvpr 2022, 11 pages
- **Journal**: None
- **Summary**: Learning-based multi-view stereo (MVS) has by far centered around 3D convolution on cost volumes. Due to the high computation and memory consumption of 3D CNN, the resolution of output depth is often considerably limited. Different from most existing works dedicated to adaptive refinement of cost volumes, we opt to directly optimize the depth value along each camera ray, mimicking the range (depth) finding of a laser scanner. This reduces the MVS problem to ray-based depth optimization which is much more light-weight than full cost volume optimization. In particular, we propose RayMVSNet which learns sequential prediction of a 1D implicit field along each camera ray with the zero-crossing point indicating scene depth. This sequential modeling, conducted based on transformer features, essentially learns the epipolar line search in traditional multi-view stereo. We also devise a multi-task learning for better optimization convergence and depth accuracy. Our method ranks top on both the DTU and the Tanks \& Temples datasets over all previous learning-based methods, achieving overall reconstruction score of 0.33mm on DTU and f-score of 59.48% on Tanks & Temples.



### IMOT: General-Purpose, Fast and Robust Estimation for Spatial Perception Problems with Outliers
- **Arxiv ID**: http://arxiv.org/abs/2204.01324v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.01324v1)
- **Published**: 2022-04-04 08:57:34+00:00
- **Updated**: 2022-04-04 08:57:34+00:00
- **Authors**: Lei Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Spatial perception problems are the fundamental building blocks of robotics and computer vision. However, in many real-world situations, they inevitably suffer from the issue of outliers, which hinders traditional solvers from making correct estimates. In this paper, we present a novel, general-purpose robust estimator IMOT (Iterative Multi-layered Otsu's Thresholding) using standard non-minimal solvers to rapidly reject outliers for spatial perception problems. First, we propose a new outlier-robust iterative optimizing framework where in each iteration all the measurement data are separated into two groups according to the residual errors and only the group with lower residual errors can be preserved for estimation in the next iteration. Second, we introduce and employ the well-known Otsu's method (from image processing) to conduct thresholding on the residual errors so as to obtain the best separation (grouping) statistically which maximizes the between-class variance. Third, to enhance robustness, we design a multi-layered Otsu's thresholding approach in combination with our framework to sift out the true inliers from outliers that might even occupy the majority of measurements. We test our robust estimator IMOT on 5 different spatial perception problems including: rotation averaging, rotation search, point cloud registration, category-level registration, and SLAM. Experiments show that IMOT is robust against 70%--90% of outliers and can typically converge in only 3--10 iterations, being 3--125 times faster than existing robust estimators: GNC and ADAPT. Moreover, IMOT is able to return robust results even without noise bound information.



### Interpretable Saliency Maps And Self-Supervised Learning For Generalized Zero Shot Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.01728v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01728v2)
- **Published**: 2022-04-04 09:30:08+00:00
- **Updated**: 2022-08-29 07:04:36+00:00
- **Authors**: Dwarikanath Mahapatra
- **Comment**: None
- **Journal**: None
- **Summary**: In many real world medical image classification settings we do not have access to samples of all possible disease classes, while a robust system is expected to give high performance in recognizing novel test data. We propose a generalized zero shot learning (GZSL) method that uses self supervised learning (SSL) for: 1) selecting anchor vectors of different disease classes; and 2) training a feature generator. Our approach does not require class attribute vectors which are available for natural images but not for medical images. SSL ensures that the anchor vectors are representative of each class. SSL is also used to generate synthetic features of unseen classes. Using a simpler architecture, our method matches a state of the art SSL based GZSL method for natural images and outperforms all methods for medical images. Our method is adaptable enough to accommodate class attribute vectors when they are available for natural images.



### An application of Pixel Interval Down-sampling (PID) for dense tiny microorganism counting on environmental microorganism images
- **Arxiv ID**: http://arxiv.org/abs/2204.01341v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.01341v3)
- **Published**: 2022-04-04 09:31:16+00:00
- **Updated**: 2022-07-22 13:46:54+00:00
- **Authors**: Jiawei Zhang, Xin Zhao, Tao Jiang, Md Mamunur Rahaman, Yudong Yao, Yu-Hao Lin, Jinghua Zhang, Ao Pan, Marcin Grzegorzek, Chen Li
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel pixel interval down-sampling network (PID-Net) for dense tiny object (yeast cells) counting tasks with higher accuracy. The PID-Net is an end-to-end convolutional neural network (CNN) model with an encoder--decoder architecture. The pixel interval down-sampling operations are concatenated with max-pooling operations to combine the sparse and dense features. This addresses the limitation of contour conglutination of dense objects while counting. The evaluation was conducted using classical segmentation metrics (the Dice, Jaccard and Hausdorff distance) as well as counting metrics. The experimental results show that the proposed PID-Net had the best performance and potential for dense tiny object counting tasks, which achieved 96.97\% counting accuracy on the dataset with 2448 yeast cell images. By comparing with the state-of-the-art approaches, such as Attention U-Net, Swin U-Net and Trans U-Net, the proposed PID-Net can segment dense tiny objects with clearer boundaries and fewer incorrect debris, which shows the great potential of PID-Net in the task of accurate counting.



### Analyzing the Effects of Handling Data Imbalance on Learned Features from Medical Images by Looking Into the Models
- **Arxiv ID**: http://arxiv.org/abs/2204.01729v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01729v1)
- **Published**: 2022-04-04 09:38:38+00:00
- **Updated**: 2022-04-04 09:38:38+00:00
- **Authors**: Ashkan Khakzar, Yawei Li, Yang Zhang, Mirac Sanisoglu, Seong Tae Kim, Mina Rezaei, Bernd Bischl, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: One challenging property lurking in medical datasets is the imbalanced data distribution, where the frequency of the samples between the different classes is not balanced. Training a model on an imbalanced dataset can introduce unique challenges to the learning problem where a model is biased towards the highly frequent class. Many methods are proposed to tackle the distributional differences and the imbalanced problem. However, the impact of these approaches on the learned features is not well studied. In this paper, we look deeper into the internal units of neural networks to observe how handling data imbalance affects the learned features. We study several popular cost-sensitive approaches for handling data imbalance and analyze the feature maps of the convolutional neural networks from multiple perspectives: analyzing the alignment of salient features with pathologies and analyzing the pathology-related concepts encoded by the networks. Our study reveals differences and insights regarding the trained models that are not reflected by quantitative metrics such as AUROC and AP and show up only by looking at the models through a lens.



### Extended Reality for Anxiety and Depression Therapy amidst Mental Disorders -- A Systematic Review
- **Arxiv ID**: http://arxiv.org/abs/2204.01348v1
- **DOI**: 10.2196/preprints.38413
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01348v1)
- **Published**: 2022-04-04 09:46:30+00:00
- **Updated**: 2022-04-04 09:46:30+00:00
- **Authors**: Omisore Olatunji, Ifeanyi Odenigbo, Joseph Orji, Amelia Beltran, Rita Orji, Nilufar Baghaei, Meier Sandra
- **Comment**: None
- **Journal**: None
- **Summary**: This systematic study is aimed to investigate the implementation level of different extended reality (XR) techniques in the care of mental disorder. We point out some XR technologies used to deliver care for mental disorders, and to evaluate the effectiveness of using XR systems for anxiety and depression amidst other mental disorders. A search period of May 2017 and August 2021 was defined to filter out articles related to the usage of virtual reality (VR), augmented reality (AR) and mixed reality (AR) in a mental health context. Search done on three databases namely Google Scholar, PubMED, and Association for Computing Machinery Digital Library yielded 689 articles. Also, 10 articles were recommended. Upon eligibility filtering, only 72 articles were found relevant and were utilized for the study. Results show that the 72 studies were done in only 23 countries across the globe, with the majority of studies being reported for developed countries such as USA (20.64%) and Germany (11.11%). Thus this could rapidly aid intervention of mental health disorder with XR. Meanwhile, none of the studies observed was from an African country. The majority of the articles reported that XR techniques led to significant reduction in symptoms of anxiety or depression. The majority of studies (23, 36.51%) were published in the year 2021 of the total studies included. In a sense, this data might be attributed to COVID-19 pandemic. Most studies (30, 47.62%) focused a population with age range of 18 to 65 years, while fewer studies (4, 6.35%) focused on each of adolescents (10 - 19 years) and seniors (over 64 years). Also, more studies were done experimentally (52, 82.54%) rather than by analytical and modeling approach (5, 7.94%) as found in other XR studies domain. This review study could aid the development of XR systems for effective cognitive behavioral and exposure therapies of mental disorders.



### MGRR-Net: Multi-level Graph Relational Reasoning Network for Facial Action Units Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.01349v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01349v3)
- **Published**: 2022-04-04 09:47:22+00:00
- **Updated**: 2022-05-05 13:55:06+00:00
- **Authors**: Xuri Ge, Joemon M. Jose, Songpei Xu, Xiao Liu, Hu Han
- **Comment**: 10 pages, 4 figures, 8 tables; submitted to IEEE TCyb for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible
- **Journal**: None
- **Summary**: The Facial Action Coding System (FACS) encodes the action units (AUs) in facial images, which has attracted extensive research attention due to its wide use in facial expression analysis. Many methods that perform well on automatic facial action unit (AU) detection primarily focus on modeling various types of AU relations between corresponding local muscle areas, or simply mining global attention-aware facial features, however, neglect the dynamic interactions among local-global features. We argue that encoding AU features just from one perspective may not capture the rich contextual information between regional and global face features, as well as the detailed variability across AUs, because of the diversity in expression and individual characteristics. In this paper, we propose a novel Multi-level Graph Relational Reasoning Network (termed MGRR-Net) for facial AU detection. Each layer of MGRR-Net performs a multi-level (i.e., region-level, pixel-wise and channel-wise level) feature learning. While the region-level feature learning from local face patches features via graph neural network can encode the correlation across different AUs, the pixel-wise and channel-wise feature learning via graph attention network can enhance the discrimination ability of AU features from global face features. The fused features from the three levels lead to improved AU discriminative ability. Extensive experiments on DISFA and BP4D AU datasets show that the proposed approach achieves superior performance than the state-of-the-art methods.



### Learning to solve Minimum Cost Multicuts efficiently using Edge-Weighted Graph Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.01366v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01366v1)
- **Published**: 2022-04-04 10:21:02+00:00
- **Updated**: 2022-04-04 10:21:02+00:00
- **Authors**: Steffen Jung, Margret Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: The minimum cost multicut problem is the NP-hard/APX-hard combinatorial optimization problem of partitioning a real-valued edge-weighted graph such as to minimize the total cost of the partition. While graph convolutional neural networks (GNN) have proven to be promising in the context of combinatorial optimization, most of them are only tailored to or tested on positive-valued edge weights, i.e. they do not comply to the nature of the multicut problem. We therefore adapt various GNN architectures including Graph Convolutional Networks, Signed Graph Convolutional Networks and Graph Isomorphic Networks to facilitate the efficient encoding of real-valued edge costs. Moreover, we employ a reformulation of the multicut ILP constraints to a polynomial program as loss function that allows to learn feasible multicut solutions in a scalable way. Thus, we provide the first approach towards end-to-end trainable multicuts. Our findings support that GNN approaches can produce good solutions in practice while providing lower computation times and largely improved scalability compared to LP solvers and optimized heuristics, especially when considering large instances.



### Multi-Modal Hypergraph Diffusion Network with Dual Prior for Alzheimer Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.02399v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.02399v3)
- **Published**: 2022-04-04 10:31:42+00:00
- **Updated**: 2022-09-06 16:40:55+00:00
- **Authors**: Angelica I. Aviles-Rivero, Christina Runkel, Nicolas Papadakis, Zoe Kourtzi, Carola-Bibiane Schönlieb
- **Comment**: None
- **Journal**: MICCAI 2022
- **Summary**: The automatic early diagnosis of prodromal stages of Alzheimer's disease is of great relevance for patient treatment to improve quality of life. We address this problem as a multi-modal classification task. Multi-modal data provides richer and complementary information. However, existing techniques only consider either lower order relations between the data and single/multi-modal imaging data. In this work, we introduce a novel semi-supervised hypergraph learning framework for Alzheimer's disease diagnosis. Our framework allows for higher-order relations among multi-modal imaging and non-imaging data whilst requiring a tiny labelled set. Firstly, we introduce a dual embedding strategy for constructing a robust hypergraph that preserves the data semantics. We achieve this by enforcing perturbation invariance at the image and graph levels using a contrastive based mechanism. Secondly, we present a dynamically adjusted hypergraph diffusion model, via a semi-explicit flow, to improve the predictive uncertainty. We demonstrate, through our experiments, that our framework is able to outperform current techniques for Alzheimer's disease diagnosis.



### Dressi: A Hardware-Agnostic Differentiable Renderer with Reactive Shader Packing and Soft Rasterization
- **Arxiv ID**: http://arxiv.org/abs/2204.01386v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01386v1)
- **Published**: 2022-04-04 11:07:03+00:00
- **Updated**: 2022-04-04 11:07:03+00:00
- **Authors**: Yusuke Takimoto, Hiroyuki Sato, Hikari Takehara, Keishiro Uragaki, Takehiro Tawara, Xiao Liang, Kentaro Oku, Wataru Kishimoto, Bo Zheng
- **Comment**: 13 pages, 17 figures, EUROGRAPHICS 2022
- **Journal**: None
- **Summary**: Differentiable rendering (DR) enables various computer graphics and computer vision applications through gradient-based optimization with derivatives of the rendering equation. Most rasterization-based approaches are built on general-purpose automatic differentiation (AD) libraries and DR-specific modules handcrafted using CUDA. Such a system design mixes DR algorithm implementation and algorithm building blocks, resulting in hardware dependency and limited performance. In this paper, we present a practical hardware-agnostic differentiable renderer called Dressi, which is based on a new full AD design. The DR algorithms of Dressi are fully written in our Vulkan-based AD for DR, Dressi-AD, which supports all primitive operations for DR. Dressi-AD and our inverse UV technique inside it bring hardware independence and acceleration by graphics hardware. Stage packing, our runtime optimization technique, can adapt hardware constraints and efficiently execute complex computational graphs of DR with reactive cache considering the render pass hierarchy of Vulkan. HardSoftRas, our novel rendering process, is designed for inverse rendering with a graphics pipeline. Under the limited functionalities of the graphics pipeline, HardSoftRas can propagate the gradients of pixels from the screen space to far-range triangle attributes. Our experiments and applications demonstrate that Dressi establishes hardware independence, high-quality and robust optimization with fast speed, and photorealistic rendering.



### How stable are Transferability Metrics evaluations?
- **Arxiv ID**: http://arxiv.org/abs/2204.01403v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01403v3)
- **Published**: 2022-04-04 11:38:40+00:00
- **Updated**: 2022-10-20 15:26:41+00:00
- **Authors**: Andrea Agostinelli, Michal Pándy, Jasper Uijlings, Thomas Mensink, Vittorio Ferrari
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Transferability metrics is a maturing field with increasing interest, which aims at providing heuristics for selecting the most suitable source models to transfer to a given target dataset, without fine-tuning them all. However, existing works rely on custom experimental setups which differ across papers, leading to inconsistent conclusions about which transferability metrics work best. In this paper we conduct a large-scale study by systematically constructing a broad range of 715k experimental setup variations. We discover that even small variations to an experimental setup lead to different conclusions about the superiority of a transferability metric over another. Then we propose better evaluations by aggregating across many experiments, enabling to reach more stable conclusions. As a result, we reveal the superiority of LogME at selecting good source datasets to transfer from in a semantic segmentation scenario, NLEEP at selecting good source architectures in an image classification scenario, and GBC at determining which target task benefits most from a given source model. Yet, no single transferability metric works best in all scenarios.



### Re-examining Distillation For Continual Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.01407v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01407v2)
- **Published**: 2022-04-04 11:50:54+00:00
- **Updated**: 2022-10-07 10:17:28+00:00
- **Authors**: Eli Verwimp, Kuo Yang, Sarah Parisot, Hong Lanqing, Steven McDonagh, Eduardo Pérez-Pellitero, Matthias De Lange, Tinne Tuytelaars
- **Comment**: Accepted at BMVC '22
- **Journal**: None
- **Summary**: Training models continually to detect and classify objects, from new classes and new domains, remains an open problem. In this work, we conduct a thorough analysis of why and how object detection models forget catastrophically. We focus on distillation-based approaches in two-stage networks; the most-common strategy employed in contemporary continual object detection work.Distillation aims to transfer the knowledge of a model trained on previous tasks -- the teacher -- to a new model -- the student -- while it learns the new task. We show that this works well for the region proposal network, but that wrong, yet overly confident teacher predictions prevent student models from effective learning of the classification head. Our analysis provides a foundation that allows us to propose improvements for existing techniques by detecting incorrect teacher predictions, based on current ground-truth labels, and by employing an adaptive Huber loss as opposed to the mean squared error for the distillation loss in the classification heads. We evidence that our strategy works not only in a class incremental setting, but also in domain incremental settings, which constitute a realistic context, likely to be the setting of representative real-world problems.



### Computer-Aided Extraction of Select MRI Markers of Cerebral Small Vessel Disease: A Systematic Review
- **Arxiv ID**: http://arxiv.org/abs/2204.01411v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2204.01411v1)
- **Published**: 2022-04-04 12:01:39+00:00
- **Updated**: 2022-04-04 12:01:39+00:00
- **Authors**: Jiyang Jiang, Dadong Wang, Yang Song, Perminder S. Sachdev, Wei Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Cerebral small vessel disease (CSVD) is a major vascular contributor to cognitive impairment in ageing, including dementias. Imaging remains the most promising method for in vivo studies of CSVD. To replace the subjective and laborious visual rating approaches, emerging studies have applied state-of-the-art artificial intelligence to extract imaging biomarkers of CSVD from MRI scans. We aimed to summarise published computer-aided methods to examine three imaging biomarkers of CSVD, namely cerebral microbleeds (CMB), dilated perivascular spaces (PVS), and lacunes of presumed vascular origin. Seventy-one classical image processing, classical machine learning, and deep learning studies were identified. CMB and PVS have been better studied, compared to lacunes. While good performance metrics have been achieved in local test datasets, there have not been generalisable pipelines validated in different research or clinical cohorts. Transfer learning and weak supervision techniques have been applied to accommodate the limitations in training data. Future studies could consider pooling data from multiple sources to increase diversity, and validating the performance of the methods using both image processing metrics and associations with clinical measures.



### Degradation-agnostic Correspondence from Resolution-asymmetric Stereo
- **Arxiv ID**: http://arxiv.org/abs/2204.01429v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01429v1)
- **Published**: 2022-04-04 12:24:34+00:00
- **Updated**: 2022-04-04 12:24:34+00:00
- **Authors**: Xihao Chen, Zhiwei Xiong, Zhen Cheng, Jiayong Peng, Yueyi Zhang, Zheng-Jun Zha
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: In this paper, we study the problem of stereo matching from a pair of images with different resolutions, e.g., those acquired with a tele-wide camera system. Due to the difficulty of obtaining ground-truth disparity labels in diverse real-world systems, we start from an unsupervised learning perspective. However, resolution asymmetry caused by unknown degradations between two views hinders the effectiveness of the generally assumed photometric consistency. To overcome this challenge, we propose to impose the consistency between two views in a feature space instead of the image space, named feature-metric consistency. Interestingly, we find that, although a stereo matching network trained with the photometric loss is not optimal, its feature extractor can produce degradation-agnostic and matching-specific features. These features can then be utilized to formulate a feature-metric loss to avoid the photometric inconsistency. Moreover, we introduce a self-boosting strategy to optimize the feature extractor progressively, which further strengthens the feature-metric consistency. Experiments on both simulated datasets with various degradations and a self-collected real-world dataset validate the superior performance of the proposed method over existing solutions.



### WildNet: Learning Domain Generalized Semantic Segmentation from the Wild
- **Arxiv ID**: http://arxiv.org/abs/2204.01446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01446v1)
- **Published**: 2022-04-04 12:57:23+00:00
- **Updated**: 2022-04-04 12:57:23+00:00
- **Authors**: Suhyeon Lee, Hongje Seong, Seongwon Lee, Euntai Kim
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: We present a new domain generalized semantic segmentation network named WildNet, which learns domain-generalized features by leveraging a variety of contents and styles from the wild. In domain generalization, the low generalization ability for unseen target domains is clearly due to overfitting to the source domain. To address this problem, previous works have focused on generalizing the domain by removing or diversifying the styles of the source domain. These alleviated overfitting to the source-style but overlooked overfitting to the source-content. In this paper, we propose to diversify both the content and style of the source domain with the help of the wild. Our main idea is for networks to naturally learn domain-generalized semantic information from the wild. To this end, we diversify styles by augmenting source features to resemble wild styles and enable networks to adapt to a variety of styles. Furthermore, we encourage networks to learn class-discriminant features by providing semantic variations borrowed from the wild to source contents in the feature space. Finally, we regularize networks to capture consistent semantic information even when both the content and style of the source domain are extended to the wild. Extensive experiments on five different datasets validate the effectiveness of our WildNet, and we significantly outperform state-of-the-art methods. The source code and model are available online: https://github.com/suhyeonlee/WildNet.



### Learning Commonsense-aware Moment-Text Alignment for Fast Video Temporal Grounding
- **Arxiv ID**: http://arxiv.org/abs/2204.01450v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01450v2)
- **Published**: 2022-04-04 13:07:05+00:00
- **Updated**: 2022-04-12 11:55:17+00:00
- **Authors**: Ziyue Wu, Junyu Gao, Shucheng Huang, Changsheng Xu
- **Comment**: Submitted to IEEE TMM; Code is available at
  https://github.com/ZiyueWu59/CCA
- **Journal**: None
- **Summary**: Grounding temporal video segments described in natural language queries effectively and efficiently is a crucial capability needed in vision-and-language fields. In this paper, we deal with the fast video temporal grounding (FVTG) task, aiming at localizing the target segment with high speed and favorable accuracy. Most existing approaches adopt elaborately designed cross-modal interaction modules to improve the grounding performance, which suffer from the test-time bottleneck. Although several common space-based methods enjoy the high-speed merit during inference, they can hardly capture the comprehensive and explicit relations between visual and textual modalities. In this paper, to tackle the dilemma of speed-accuracy tradeoff, we propose a commonsense-aware cross-modal alignment (CCA) framework, which incorporates commonsense-guided visual and text representations into a complementary common space for fast video temporal grounding. Specifically, the commonsense concepts are explored and exploited by extracting the structural semantic information from a language corpus. Then, a commonsense-aware interaction module is designed to obtain bridged visual and text features by utilizing the learned commonsense concepts. Finally, to maintain the original semantic information of textual queries, a cross-modal complementary common space is optimized to obtain matching scores for performing FVTG. Extensive results on two challenging benchmarks show that our CCA method performs favorably against state-of-the-arts while running at high speed. Our code is available at https://github.com/ZiyueWu59/CCA.



### Correlation Verification for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2204.01458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01458v1)
- **Published**: 2022-04-04 13:18:49+00:00
- **Updated**: 2022-04-04 13:18:49+00:00
- **Authors**: Seongwon Lee, Hongje Seong, Suhyeon Lee, Euntai Kim
- **Comment**: Accepted to CVPR 2022 (Oral Presentation)
- **Journal**: None
- **Summary**: Geometric verification is considered a de facto solution for the re-ranking task in image retrieval. In this study, we propose a novel image retrieval re-ranking network named Correlation Verification Networks (CVNet). Our proposed network, comprising deeply stacked 4D convolutional layers, gradually compresses dense feature correlation into image similarity while learning diverse geometric matching patterns from various image pairs. To enable cross-scale matching, it builds feature pyramids and constructs cross-scale feature correlations within a single inference, replacing costly multi-scale inferences. In addition, we use curriculum learning with the hard negative mining and Hide-and-Seek strategy to handle hard samples without losing generality. Our proposed re-ranking network shows state-of-the-art performance on several retrieval benchmarks with a significant margin (+12.6% in mAP on ROxford-Hard+1M set) over state-of-the-art methods. The source code and models are available online: https://github.com/sungonce/CVNet.



### Optimizing the Consumption of Spiking Neural Networks with Activity Regularization
- **Arxiv ID**: http://arxiv.org/abs/2204.01460v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01460v1)
- **Published**: 2022-04-04 13:19:47+00:00
- **Updated**: 2022-04-04 13:19:47+00:00
- **Authors**: Simon Narduzzi, Siavash A. Bigdeli, Shih-Chii Liu, L. Andrea Dunbar
- **Comment**: 5 pages, 3 figures; accepted at IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP), Singapore, 2022
- **Journal**: None
- **Summary**: Reducing energy consumption is a critical point for neural network models running on edge devices. In this regard, reducing the number of multiply-accumulate (MAC) operations of Deep Neural Networks (DNNs) running on edge hardware accelerators will reduce the energy consumption during inference. Spiking Neural Networks (SNNs) are an example of bio-inspired techniques that can further save energy by using binary activations, and avoid consuming energy when not spiking. The networks can be configured for equivalent accuracy on a task through DNN-to-SNN conversion frameworks but their conversion is based on rate coding therefore the synaptic operations can be high. In this work, we look into different techniques to enforce sparsity on the neural network activation maps and compare the effect of different training regularizers on the efficiency of the optimized DNNs and SNNs.



### Unsupervised Learning of Accurate Siamese Tracking
- **Arxiv ID**: http://arxiv.org/abs/2204.01475v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.01475v1)
- **Published**: 2022-04-04 13:39:43+00:00
- **Updated**: 2022-04-04 13:39:43+00:00
- **Authors**: Qiuhong Shen, Lei Qiao, Jinyang Guo, Peixia Li, Xin Li, Bo Li, Weitao Feng, Weihao Gan, Wei Wu, Wanli Ouyang
- **Comment**: 13 pages, 7 figures, to appear in CVPR 2022
- **Journal**: None
- **Summary**: Unsupervised learning has been popular in various computer vision tasks, including visual object tracking. However, prior unsupervised tracking approaches rely heavily on spatial supervision from template-search pairs and are still unable to track objects with strong variation over a long time span. As unlimited self-supervision signals can be obtained by tracking a video along a cycle in time, we investigate evolving a Siamese tracker by tracking videos forward-backward. We present a novel unsupervised tracking framework, in which we can learn temporal correspondence both on the classification branch and regression branch. Specifically, to propagate reliable template feature in the forward propagation process so that the tracker can be trained in the cycle, we first propose a consistency propagation transformation. We then identify an ill-posed penalty problem in conventional cycle training in backward propagation process. Thus, a differentiable region mask is proposed to select features as well as to implicitly penalize tracking errors on intermediate frames. Moreover, since noisy labels may degrade training, we propose a mask-guided loss reweighting strategy to assign dynamic weights based on the quality of pseudo labels. In extensive experiments, our tracker outperforms preceding unsupervised methods by a substantial margin, performing on par with supervised methods on large-scale datasets such as TrackingNet and LaSOT. Code is available at https://github.com/FlorinShum/ULAST.



### Adaptive Network Combination for Single-Image Reflection Removal: A Domain Generalization Perspective
- **Arxiv ID**: http://arxiv.org/abs/2204.01505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01505v1)
- **Published**: 2022-04-04 14:06:11+00:00
- **Updated**: 2022-04-04 14:06:11+00:00
- **Authors**: Ming Liu, Jianan Pan, Zifei Yan, Wangmeng Zuo, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, multiple synthetic and real-world datasets have been built to facilitate the training of deep single image reflection removal (SIRR) models. Meanwhile, diverse testing sets are also provided with different types of reflection and scenes. However, the non-negligible domain gaps between training and testing sets make it difficult to learn deep models generalizing well to testing images. The diversity of reflections and scenes further makes it a mission impossible to learn a single model being effective to all testing sets and real-world reflections. In this paper, we tackle these issues by learning SIRR models from a domain generalization perspective. Particularly, for each source set, a specific SIRR model is trained to serve as a domain expert of relevant reflection types. For a given reflection-contaminated image, we present a reflection type-aware weighting (RTAW) module to predict expert-wise weights. RTAW can then be incorporated with adaptive network combination (AdaNEC) for handling different reflection types and scenes, i.e., generalizing to unknown domains. Two representative AdaNEC methods, i.e., output fusion (OF) and network interpolation (NI), are provided by considering both adaptation levels and efficiency. For images from one source set, we train RTAW to only predict expert-wise weights of other domain experts for improving generalization ability, while the weights of all experts are predicted and employed during testing. An in-domain expert (IDE) loss is presented for training RTAW. Extensive experiments show the appealing performance gain of our AdaNEC on different state-of-the-art SIRR networks. Source code and pre-trained models will available at https://github.com/csmliu/AdaNEC.



### The Group Loss++: A deeper look into group loss for deep metric learning
- **Arxiv ID**: http://arxiv.org/abs/2204.01509v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01509v1)
- **Published**: 2022-04-04 14:09:58+00:00
- **Updated**: 2022-04-04 14:09:58+00:00
- **Authors**: Ismail Elezi, Jenny Seidenschwarz, Laurin Wagner, Sebastiano Vascon, Alessandro Torcinovich, Marcello Pelillo, Laura Leal-Taixe
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (tPAMI), 2022. Includes supplementary material
- **Journal**: None
- **Summary**: Deep metric learning has yielded impressive results in tasks such as clustering and image retrieval by leveraging neural networks to obtain highly discriminative feature embeddings, which can be used to group samples into different classes. Much research has been devoted to the design of smart loss functions or data mining strategies for training such networks. Most methods consider only pairs or triplets of samples within a mini-batch to compute the loss function, which is commonly based on the distance between embeddings. We propose Group Loss, a loss function based on a differentiable label-propagation method that enforces embedding similarity across all samples of a group while promoting, at the same time, low-density regions amongst data points belonging to different groups. Guided by the smoothness assumption that "similar objects should belong to the same group", the proposed loss trains the neural network for a classification task, enforcing a consistent labelling amongst samples within a class. We design a set of inference strategies tailored towards our algorithm, named Group Loss++ that further improve the results of our model. We show state-of-the-art results on clustering and image retrieval on four retrieval datasets, and present competitive results on two person re-identification datasets, providing a unified framework for retrieval and re-identification.



### Context-aware Visual Tracking with Joint Meta-updating
- **Arxiv ID**: http://arxiv.org/abs/2204.01513v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.01513v1)
- **Published**: 2022-04-04 14:16:00+00:00
- **Updated**: 2022-04-04 14:16:00+00:00
- **Authors**: Qiuhong Shen, Xin Li, Fanyang Meng, Yongsheng Liang
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Visual object tracking acts as a pivotal component in various emerging video applications. Despite the numerous developments in visual tracking, existing deep trackers are still likely to fail when tracking against objects with dramatic variation. These deep trackers usually do not perform online update or update single sub-branch of the tracking model, for which they cannot adapt to the appearance variation of objects. Efficient updating methods are therefore crucial for tracking while previous meta-updater optimizes trackers directly over parameter space, which is prone to over-fit even collapse on longer sequences. To address these issues, we propose a context-aware tracking model to optimize the tracker over the representation space, which jointly meta-update both branches by exploiting information along the whole sequence, such that it can avoid the over-fitting problem. First, we note that the embedded features of the localization branch and the box-estimation branch, focusing on the local and global information of the target, are effective complements to each other. Based on this insight, we devise a context-aggregation module to fuse information in historical frames, followed by a context-aware module to learn affinity vectors for both branches of the tracker. Besides, we develop a dedicated meta-learning scheme, on account of fast and stable updating with limited training samples. The proposed tracking method achieves an EAO score of 0.514 on VOT2018 with the speed of 40FPS, demonstrating its capability of improving the accuracy and robustness of the underlying tracker with little speed drop.



### Transient motion classification through turbid volumes via parallelized single-photon detection and deep contrastive embedding
- **Arxiv ID**: http://arxiv.org/abs/2204.01733v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2204.01733v2)
- **Published**: 2022-04-04 14:27:36+00:00
- **Updated**: 2022-06-13 00:02:34+00:00
- **Authors**: Shiqi Xu, Wenhui Liu, Xi Yang, Joakim Jönsson, Ruobing Qian, Paul McKee, Kanghyun Kim, Pavan Chandra Konda, Kevin C. Zhou, Lucas Kreiß, Haoqian Wang, Edouard Berrocal, Scott Huettel, Roarke Horstmeyer
- **Comment**: Journal submission
- **Journal**: None
- **Summary**: Fast noninvasive probing of spatially varying decorrelating events, such as cerebral blood flow beneath the human skull, is an essential task in various scientific and clinical settings. One of the primary optical techniques used is diffuse correlation spectroscopy (DCS), whose classical implementation uses a single or few single-photon detectors, resulting in poor spatial localization accuracy and relatively low temporal resolution. Here, we propose a technique termed Classifying Rapid decorrelation Events via Parallelized single photon dEtection (CREPE)}, a new form of DCS that can probe and classify different decorrelating movements hidden underneath turbid volume with high sensitivity using parallelized speckle detection from a $32\times32$ pixel SPAD array. We evaluate our setup by classifying different spatiotemporal-decorrelating patterns hidden beneath a 5mm tissue-like phantom made with rapidly decorrelating dynamic scattering media. Twelve multi-mode fibers are used to collect scattered light from different positions on the surface of the tissue phantom. To validate our setup, we generate perturbed decorrelation patterns by both a digital micromirror device (DMD) modulated at multi-kilo-hertz rates, as well as a vessel phantom containing flowing fluid. Along with a deep contrastive learning algorithm that outperforms classic unsupervised learning methods, we demonstrate our approach can accurately detect and classify different transient decorrelation events (happening in 0.1-0.4s) underneath turbid scattering media, without any data labeling. This has the potential to be applied to noninvasively monitor deep tissue motion patterns, for example identifying normal or abnormal cerebral blood flow events, at multi-Hertz rates within a compact and static detection probe.



### Con$^{2}$DA: Simplifying Semi-supervised Domain Adaptation by Learning Consistent and Contrastive Feature Representations
- **Arxiv ID**: http://arxiv.org/abs/2204.01558v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01558v2)
- **Published**: 2022-04-04 15:05:45+00:00
- **Updated**: 2023-08-11 09:20:55+00:00
- **Authors**: Manuel Pérez-Carrasco, Pavlos Protopapas, Guillermo Cabrera-Vives
- **Comment**: Accepted to NeurIPS 2021 Workshop on Distribution Shifts: Connecting
  Methods and Applications
- **Journal**: None
- **Summary**: In this work, we present Con$^{2}$DA, a simple framework that extends recent advances in semi-supervised learning to the semi-supervised domain adaptation (SSDA) problem. Our framework generates pairs of associated samples by performing stochastic data transformations to a given input. Associated data pairs are mapped to a feature representation space using a feature extractor. We use different loss functions to enforce consistency between the feature representations of associated data pairs of samples. We show that these learned representations are useful to deal with differences in data distributions in the domain adaptation problem. We performed experiments to study the main components of our model and we show that (i) learning of the consistent and contrastive feature representations is crucial to extract good discriminative features across different domains, and ii) our model benefits from the use of strong augmentation policies. With these findings, our method achieves state-of-the-art performances in three benchmark datasets for SSDA.



### HiT-DVAE: Human Motion Generation via Hierarchical Transformer Dynamical VAE
- **Arxiv ID**: http://arxiv.org/abs/2204.01565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01565v1)
- **Published**: 2022-04-04 15:12:34+00:00
- **Updated**: 2022-04-04 15:12:34+00:00
- **Authors**: Xiaoyu Bie, Wen Guo, Simon Leglaive, Lauren Girin, Francesc Moreno-Noguer, Xavier Alameda-Pineda
- **Comment**: None
- **Journal**: None
- **Summary**: Studies on the automatic processing of 3D human pose data have flourished in the recent past. In this paper, we are interested in the generation of plausible and diverse future human poses following an observed 3D pose sequence. Current methods address this problem by injecting random variables from a single latent space into a deterministic motion prediction framework, which precludes the inherent multi-modality in human motion generation. In addition, previous works rarely explore the use of attention to select which frames are to be used to inform the generation process up to our knowledge. To overcome these limitations, we propose Hierarchical Transformer Dynamical Variational Autoencoder, HiT-DVAE, which implements auto-regressive generation with transformer-like attention mechanisms. HiT-DVAE simultaneously learns the evolution of data and latent space distribution with time correlated probabilistic dependencies, thus enabling the generative model to learn a more complex and time-varying latent space as well as diverse and realistic human motions. Furthermore, the auto-regressive generation brings more flexibility on observation and prediction, i.e. one can have any length of observation and predict arbitrary large sequences of poses with a single pre-trained model. We evaluate the proposed method on HumanEva-I and Human3.6M with various evaluation methods, and outperform the state-of-the-art methods on most of the metrics.



### DAD: Data-free Adversarial Defense at Test Time
- **Arxiv ID**: http://arxiv.org/abs/2204.01568v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01568v2)
- **Published**: 2022-04-04 15:16:13+00:00
- **Updated**: 2022-04-08 16:03:30+00:00
- **Authors**: Gaurav Kumar Nayak, Ruchit Rawal, Anirban Chakraborty
- **Comment**: WACV 2022. Project page: https://sites.google.com/view/dad-wacv22
- **Journal**: None
- **Summary**: Deep models are highly susceptible to adversarial attacks. Such attacks are carefully crafted imperceptible noises that can fool the network and can cause severe consequences when deployed. To encounter them, the model requires training data for adversarial training or explicit regularization-based techniques. However, privacy has become an important concern, restricting access to only trained models but not the training data (e.g. biometric data). Also, data curation is expensive and companies may have proprietary rights over it. To handle such situations, we propose a completely novel problem of 'test-time adversarial defense in absence of training data and even their statistics'. We solve it in two stages: a) detection and b) correction of adversarial samples. Our adversarial sample detection framework is initially trained on arbitrary data and is subsequently adapted to the unlabelled test data through unsupervised domain adaptation. We further correct the predictions on detected adversarial samples by transforming them in Fourier domain and obtaining their low frequency component at our proposed suitable radius for model prediction. We demonstrate the efficacy of our proposed technique via extensive experiments against several adversarial attacks and for different model architectures and datasets. For a non-robust Resnet-18 model pre-trained on CIFAR-10, our detection method correctly identifies 91.42% adversaries. Also, we significantly improve the adversarial accuracy from 0% to 37.37% with a minimal drop of 0.02% in clean accuracy on state-of-the-art 'Auto Attack' without having to retrain the model.



### Coarse-to-Fine Q-attention with Learned Path Ranking
- **Arxiv ID**: http://arxiv.org/abs/2204.01571v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01571v1)
- **Published**: 2022-04-04 15:23:14+00:00
- **Updated**: 2022-04-04 15:23:14+00:00
- **Authors**: Stephen James, Pieter Abbeel
- **Comment**: Project page and code: https://sites.google.com/view/q-attention-lpr
- **Journal**: None
- **Summary**: We propose Learned Path Ranking (LPR), a method that accepts an end-effector goal pose, and learns to rank a set of goal-reaching paths generated from an array of path generating methods, including: path planning, Bezier curve sampling, and a learned policy. The core idea being that each of the path generation modules will be useful in different tasks, or at different stages in a task. When LPR is added as an extension to C2F-ARM, our new system, C2F-ARM+LPR, retains the sample efficiency of its predecessor, while also being able to accomplish a larger set of tasks; in particular, tasks that require very specific motions (e.g. opening toilet seat) that need to be inferred from both demonstrations and exploration data. In addition to benchmarking our approach across 16 RLBench tasks, we also learn real-world tasks, tabula rasa, in 10-15 minutes, with only 3 demonstrations.



### Object Level Depth Reconstruction for Category Level 6D Object Pose Estimation From Monocular RGB Image
- **Arxiv ID**: http://arxiv.org/abs/2204.01586v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01586v2)
- **Published**: 2022-04-04 15:33:28+00:00
- **Updated**: 2022-08-28 04:02:27+00:00
- **Authors**: Zhaoxin Fan, Zhenbo Song, Jian Xu, Zhicheng Wang, Kejian Wu, Hongyan Liu, Jun He
- **Comment**: Accept to ECCV 2022
- **Journal**: None
- **Summary**: Recently, RGBD-based category-level 6D object pose estimation has achieved promising improvement in performance, however, the requirement of depth information prohibits broader applications. In order to relieve this problem, this paper proposes a novel approach named Object Level Depth reconstruction Network (OLD-Net) taking only RGB images as input for category-level 6D object pose estimation. We propose to directly predict object-level depth from a monocular RGB image by deforming the category-level shape prior into object-level depth and the canonical NOCS representation. Two novel modules named Normalized Global Position Hints (NGPH) and Shape-aware Decoupled Depth Reconstruction (SDDR) module are introduced to learn high fidelity object-level depth and delicate shape representations. At last, the 6D object pose is solved by aligning the predicted canonical representation with the back-projected object-level depth. Extensive experiments on the challenging CAMERA25 and REAL275 datasets indicate that our model, though simple, achieves state-of-the-art performance.



### FIFO: Learning Fog-invariant Features for Foggy Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.01587v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01587v1)
- **Published**: 2022-04-04 15:33:42+00:00
- **Updated**: 2022-04-04 15:33:42+00:00
- **Authors**: Sohyun Lee, Taeyoung Son, Suha Kwak
- **Comment**: Accepted to CVPR 2022 (Oral)
- **Journal**: None
- **Summary**: Robust visual recognition under adverse weather conditions is of great importance in real-world applications. In this context, we propose a new method for learning semantic segmentation models robust against fog. Its key idea is to consider the fog condition of an image as its style and close the gap between images with different fog conditions in neural style spaces of a segmentation model. In particular, since the neural style of an image is in general affected by other factors as well as fog, we introduce a fog-pass filter module that learns to extract a fog-relevant factor from the style. Optimizing the fog-pass filter and the segmentation model alternately gradually closes the style gap between different fog conditions and allows to learn fog-invariant features in consequence. Our method substantially outperforms previous work on three real foggy image datasets. Moreover, it improves performance on both foggy and clear weather images, while existing methods often degrade performance on clear scenes.



### On Explaining Multimodal Hateful Meme Detection Models
- **Arxiv ID**: http://arxiv.org/abs/2204.01734v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2204.01734v2)
- **Published**: 2022-04-04 15:35:41+00:00
- **Updated**: 2022-04-06 08:56:40+00:00
- **Authors**: Ming Shan Hee, Roy Ka-Wei Lee, Wen-Haw Chong
- **Comment**: None
- **Journal**: None
- **Summary**: Hateful meme detection is a new multimodal task that has gained significant traction in academic and industry research communities. Recently, researchers have applied pre-trained visual-linguistic models to perform the multimodal classification task, and some of these solutions have yielded promising results. However, what these visual-linguistic models learn for the hateful meme classification task remains unclear. For instance, it is unclear if these models are able to capture the derogatory or slurs references in multimodality (i.e., image and text) of the hateful memes. To fill this research gap, this paper propose three research questions to improve our understanding of these visual-linguistic models performing the hateful meme classification task. We found that the image modality contributes more to the hateful meme classification task, and the visual-linguistic models are able to perform visual-text slurs grounding to a certain extent. Our error analysis also shows that the visual-linguistic models have acquired biases, which resulted in false-positive predictions.



### DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.01599v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01599v2)
- **Published**: 2022-04-04 15:52:55+00:00
- **Updated**: 2022-07-21 02:45:14+00:00
- **Authors**: Runyu Ding, Jihan Yang, Li Jiang, Xiaojuan Qi
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Deep learning approaches achieve prominent success in 3D semantic segmentation. However, collecting densely annotated real-world 3D datasets is extremely time-consuming and expensive. Training models on synthetic data and generalizing on real-world scenarios becomes an appealing alternative, but unfortunately suffers from notorious domain shifts. In this work, we propose a Data-Oriented Domain Adaptation (DODA) framework to mitigate pattern and context gaps caused by different sensing mechanisms and layout placements across domains. Our DODA encompasses virtual scan simulation to imitate real-world point cloud patterns and tail-aware cuboid mixing to alleviate the interior context gap with a cuboid-based intermediate domain. The first unsupervised sim-to-real adaptation benchmark on 3D indoor semantic segmentation is also built on 3D-FRONT, ScanNet and S3DIS along with 7 popular Unsupervised Domain Adaptation (UDA) methods. Our DODA surpasses existing UDA approaches by over 13% on both 3D-FRONT -> ScanNet and 3D-FRONT -> S3DIS. Code is available at https://github.com/CVMI-Lab/DODA.



### APP: Anytime Progressive Pruning
- **Arxiv ID**: http://arxiv.org/abs/2204.01640v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01640v2)
- **Published**: 2022-04-04 16:38:55+00:00
- **Updated**: 2022-06-01 18:02:45+00:00
- **Authors**: Diganta Misra, Bharat Runwal, Tianlong Chen, Zhangyang Wang, Irina Rish
- **Comment**: 21 pages including 4 pages of references. Preprint version
- **Journal**: None
- **Summary**: With the latest advances in deep learning, there has been a lot of focus on the online learning paradigm due to its relevance in practical settings. Although many methods have been investigated for optimal learning settings in scenarios where the data stream is continuous over time, sparse networks training in such settings have often been overlooked. In this paper, we explore the problem of training a neural network with a target sparsity in a particular case of online learning: the anytime learning at macroscale paradigm (ALMA). We propose a novel way of progressive pruning, referred to as \textit{Anytime Progressive Pruning} (APP); the proposed approach significantly outperforms the baseline dense and Anytime OSP models across multiple architectures and datasets under short, moderate, and long-sequence training. Our method, for example, shows an improvement in accuracy of $\approx 7\%$ and a reduction in the generalization gap by $\approx 22\%$, while being $\approx 1/3$ rd the size of the dense baseline model in few-shot restricted imagenet training. We further observe interesting nonmonotonic transitions in the generalization gap in the high number of megabatches-based ALMA. The code and experiment dashboards can be accessed at \url{https://github.com/landskape-ai/Progressive-Pruning} and \url{https://wandb.ai/landskape/APP}, respectively.



### Three-dimensional Microstructural Image Synthesis from 2D Backscattered Electron Image of Cement Paste
- **Arxiv ID**: http://arxiv.org/abs/2204.01645v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01645v1)
- **Published**: 2022-04-04 16:50:03+00:00
- **Updated**: 2022-04-04 16:50:03+00:00
- **Authors**: Xin Zhao, Xu Wu, Lin Wang, Pengkun Hou, Qinfei Li, Yuxuan Zhang, Bo Yang
- **Comment**: 25 pages, 9 figures
- **Journal**: None
- **Summary**: The microstructure is significant for exploring the physical properties of hardened cement paste. In general, the microstructures of hardened cement paste are obtained by microscopy. As a popular method, scanning electron microscopy (SEM) can acquire high-quality 2D images but fails to obtain 3D microstructures.Although several methods, such as microtomography (Micro-CT) and Focused Ion Beam Scanning Electron Microscopy (FIB-SEM), can acquire 3D microstructures, these fail to obtain high-quality 3D images or consume considerable cost. To address these issues, a method based on solid texture synthesis is proposed, synthesizing high-quality 3D microstructural image of hardened cement paste. This method includes 2D backscattered electron (BSE) image acquisition and 3D microstructure synthesis phases. In the approach, the synthesis model is based on solid texture synthesis, capturing microstructure information of the acquired 2D BSE image and generating high-quality 3D microstructures. In experiments, the method is verified on actual 3D Micro-CT images and 2D BSE images. Finally, qualitative experiments demonstrate that the 3D microstructures generated by our method have similar visual characteristics to the given 2D example. Furthermore, quantitative experiments prove that the synthetic 3D results are consistent with the actual instance in terms of porosity, particle size distribution, and grey scale co-occurrence matrix.



### Evolving Neural Selection with Adaptive Regularization
- **Arxiv ID**: http://arxiv.org/abs/2204.01662v1
- **DOI**: 10.1145/3449726.3463189
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01662v1)
- **Published**: 2022-04-04 17:19:52+00:00
- **Updated**: 2022-04-04 17:19:52+00:00
- **Authors**: Li Ding, Lee Spector
- **Comment**: None
- **Journal**: Proceedings of the Genetic and Evolutionary Computation Conference
  Companion (pp. 1717-1725). 2021
- **Summary**: Over-parameterization is one of the inherent characteristics of modern deep neural networks, which can often be overcome by leveraging regularization methods, such as Dropout. Usually, these methods are applied globally and all the input cases are treated equally. However, given the natural variation of the input space for real-world tasks such as image recognition and natural language understanding, it is unlikely that a fixed regularization pattern will have the same effectiveness for all the input cases. In this work, we demonstrate a method in which the selection of neurons in deep neural networks evolves, adapting to the difficulty of prediction. We propose the Adaptive Neural Selection (ANS) framework, which evolves to weigh neurons in a layer to form network variants that are suitable to handle different input cases. Experimental results show that the proposed method can significantly improve the performance of commonly-used neural network architectures on standard image recognition benchmarks. Ablation studies also validate the effectiveness and contribution of each component in the proposed framework.



### Tracking Urbanization in Developing Regions with Remote Sensing Spatial-Temporal Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2204.01736v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01736v1)
- **Published**: 2022-04-04 17:21:20+00:00
- **Updated**: 2022-04-04 17:21:20+00:00
- **Authors**: Yutong He, William Zhang, Chenlin Meng, Marshall Burke, David B. Lobell, Stefano Ermon
- **Comment**: Presented at Workshop on Machine Learning for the Developing World
  (ML4D) at the 35th Conference on Neural Information Processing Systems
  (NeurIPS) 2021
- **Journal**: None
- **Summary**: Automated tracking of urban development in areas where construction information is not available became possible with recent advancements in machine learning and remote sensing. Unfortunately, these solutions perform best on high-resolution imagery, which is expensive to acquire and infrequently available, making it difficult to scale over long time spans and across large geographies. In this work, we propose a pipeline that leverages a single high-resolution image and a time series of publicly available low-resolution images to generate accurate high-resolution time series for object tracking in urban construction. Our method achieves significant improvement in comparison to baselines using single image super-resolution, and can assist in extending the accessibility and scalability of building construction tracking across the developing world.



### A Novel Capsule Neural Network Based Model for Drowsiness Detection Using Electroencephalography Signals
- **Arxiv ID**: http://arxiv.org/abs/2204.01666v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01666v1)
- **Published**: 2022-04-04 17:23:53+00:00
- **Updated**: 2022-04-04 17:23:53+00:00
- **Authors**: Luis Guarda, Juan Tapia, Enrique Lopez Droguett, Marcelo Ramos
- **Comment**: None
- **Journal**: None
- **Summary**: The early detection of drowsiness has become vital to ensure the correct and safe development of several industries' tasks. Due to the transient mental state of a human subject between alertness and drowsiness, automated drowsiness detection is a complex problem to tackle. The electroencephalography signals allow us to record variations in an individual's brain's electrical potential, where each of them gives specific information about a subject's mental state. However, due to this type of signal's nature, its acquisition, in general, is complex, so it is hard to have a large volume of data to apply techniques of Deep Learning for processing and classification optimally. Nevertheless, Capsule Neural Networks are a brand-new Deep Learning algorithm proposed for work with reduced amounts of data. It is a robust algorithm to handle the data's hierarchical relationships, which is an essential characteristic for work with biomedical signals. Therefore, this paper presents a Deep Learning-based method for drowsiness detection with CapsNet by using a concatenation of spectrogram images of the electroencephalography signals channels. The proposed CapsNet model is compared with a Convolutional Neural Network, which is outperformed by the proposed model, which obtains an average accuracy of 86,44% and 87,57% of sensitivity against an average accuracy of 75,86% and 79,47% sensitivity for the CNN, showing that CapsNet is more suitable for this kind of datasets and tasks.



### Exemplar-based Pattern Synthesis with Implicit Periodic Field Network
- **Arxiv ID**: http://arxiv.org/abs/2204.01671v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.01671v2)
- **Published**: 2022-04-04 17:36:16+00:00
- **Updated**: 2022-04-15 03:06:32+00:00
- **Authors**: Haiwei Chen, Jiayi Liu, Weikai Chen, Shichen Liu, Yajie Zhao
- **Comment**: 8 pages, CVPR 2022
- **Journal**: None
- **Summary**: Synthesis of ergodic, stationary visual patterns is widely applicable in texturing, shape modeling, and digital content creation. The wide applicability of this technique thus requires the pattern synthesis approaches to be scalable, diverse, and authentic. In this paper, we propose an exemplar-based visual pattern synthesis framework that aims to model the inner statistics of visual patterns and generate new, versatile patterns that meet the aforementioned requirements. To this end, we propose an implicit network based on generative adversarial network (GAN) and periodic encoding, thus calling our network the Implicit Periodic Field Network (IPFN). The design of IPFN ensures scalability: the implicit formulation directly maps the input coordinates to features, which enables synthesis of arbitrary size and is computationally efficient for 3D shape synthesis. Learning with a periodic encoding scheme encourages diversity: the network is constrained to model the inner statistics of the exemplar based on spatial latent codes in a periodic field. Coupled with continuously designed GAN training procedures, IPFN is shown to synthesize tileable patterns with smooth transitions and local variations. Last but not least, thanks to both the adversarial training technique and the encoded Fourier features, IPFN learns high-frequency functions that produce authentic, high-quality results. To validate our approach, we present novel experimental results on various applications in 2D texture synthesis and 3D shape synthesis.



### Feature robustness and sex differences in medical imaging: a case study in MRI-based Alzheimer's disease detection
- **Arxiv ID**: http://arxiv.org/abs/2204.01737v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01737v3)
- **Published**: 2022-04-04 17:37:54+00:00
- **Updated**: 2022-07-14 16:35:45+00:00
- **Authors**: Eike Petersen, Aasa Feragen, Maria Luise da Costa Zemsch, Anders Henriksen, Oskar Eiler Wiese Christensen, Melanie Ganz
- **Comment**: Accepted for presentation at MICCAI 2022
- **Journal**: None
- **Summary**: Convolutional neural networks have enabled significant improvements in medical image-based diagnosis. It is, however, increasingly clear that these models are susceptible to performance degradation when facing spurious correlations and dataset shift, leading, e.g., to underperformance on underrepresented patient groups. In this paper, we compare two classification schemes on the ADNI MRI dataset: a simple logistic regression model using manually selected volumetric features, and a convolutional neural network trained on 3D MRI data. We assess the robustness of the trained models in the face of varying dataset splits, training set sex composition, and stage of disease. In contrast to earlier work in other imaging modalities, we do not observe a clear pattern of improved model performance for the majority group in the training dataset. Instead, while logistic regression is fully robust to dataset composition, we find that CNN performance is generally improved for both male and female subjects when including more female subjects in the training dataset. We hypothesize that this might be due to inherent differences in the pathology of the two sexes. Moreover, in our analysis, the logistic regression model outperforms the 3D CNN, emphasizing the utility of manual feature specification based on prior knowledge, and the need for more robust automatic feature selection.



### MultiMAE: Multi-modal Multi-task Masked Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2204.01678v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01678v1)
- **Published**: 2022-04-04 17:50:41+00:00
- **Updated**: 2022-04-04 17:50:41+00:00
- **Authors**: Roman Bachmann, David Mizrahi, Andrei Atanov, Amir Zamir
- **Comment**: Project page at https://multimae.epfl.ch
- **Journal**: None
- **Summary**: We propose a pre-training strategy called Multi-modal Multi-task Masked Autoencoders (MultiMAE). It differs from standard Masked Autoencoding in two key aspects: I) it can optionally accept additional modalities of information in the input besides the RGB image (hence "multi-modal"), and II) its training objective accordingly includes predicting multiple outputs besides the RGB image (hence "multi-task").   We make use of masking (across image patches and input modalities) to make training MultiMAE tractable as well as to ensure cross-modality predictive coding is indeed learned by the network. We show this pre-training strategy leads to a flexible, simple, and efficient framework with improved transfer results to downstream tasks. In particular, the same exact pre-trained network can be flexibly used when additional information besides RGB images is available or when no information other than RGB is available - in all configurations yielding competitive to or significantly better results than the baselines. To avoid needing training datasets with multiple modalities and tasks, we train MultiMAE entirely using pseudo labeling, which makes the framework widely applicable to any RGB dataset.   The experiments are performed on multiple transfer tasks (image classification, semantic segmentation, depth estimation) and datasets (ImageNet, ADE20K, Taskonomy, Hypersim, NYUv2). The results show an intriguingly impressive capability by the model in cross-modal/task predictive coding and transfer.



### TALLFormer: Temporal Action Localization with a Long-memory Transformer
- **Arxiv ID**: http://arxiv.org/abs/2204.01680v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01680v2)
- **Published**: 2022-04-04 17:51:20+00:00
- **Updated**: 2022-07-26 18:33:10+00:00
- **Authors**: Feng Cheng, Gedas Bertasius
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Most modern approaches in temporal action localization divide this problem into two parts: (i) short-term feature extraction and (ii) long-range temporal boundary localization. Due to the high GPU memory cost caused by processing long untrimmed videos, many methods sacrifice the representational power of the short-term feature extractor by either freezing the backbone or using a small spatial video resolution. This issue becomes even worse with the recent video transformer models, many of which have quadratic memory complexity. To address these issues, we propose TALLFormer, a memory-efficient and end-to-end trainable Temporal Action Localization Transformer with Long-term memory. Our long-term memory mechanism eliminates the need for processing hundreds of redundant video frames during each training iteration, thus, significantly reducing the GPU memory consumption and training time. These efficiency savings allow us (i) to use a powerful video transformer feature extractor without freezing the backbone or reducing the spatial video resolution, while (ii) also maintaining long-range temporal boundary localization capability. With only RGB frames as input and no external action recognition classifier, TALLFormer outperforms previous state-of-the-arts by a large margin, achieving an average mAP of 59.1% on THUMOS14 and 35.6% on ActivityNet-1.3. The code is public available: https://github.com/klauscc/TALLFormer.



### End-to-end multi-particle reconstruction in high occupancy imaging calorimeters with graph neural networks
- **Arxiv ID**: http://arxiv.org/abs/2204.01681v3
- **DOI**: 10.1140/epjc/s10052-022-10665-7
- **Categories**: **physics.ins-det**, cs.CV, cs.LG, hep-ex
- **Links**: [PDF](http://arxiv.org/pdf/2204.01681v3)
- **Published**: 2022-04-04 17:51:43+00:00
- **Updated**: 2022-09-30 12:22:08+00:00
- **Authors**: Shah Rukh Qasim, Nadezda Chernyavskaya, Jan Kieseler, Kenneth Long, Oleksandr Viazlo, Maurizio Pierini, Raheel Nawaz
- **Comment**: None
- **Journal**: Eur. Phys. J. C 82, 753 (2022)
- **Summary**: We present an end-to-end reconstruction algorithm to build particle candidates from detector hits in next-generation granular calorimeters similar to that foreseen for the high-luminosity upgrade of the CMS detector. The algorithm exploits a distance-weighted graph neural network, trained with object condensation, a graph segmentation technique. Through a single-shot approach, the reconstruction task is paired with energy regression. We describe the reconstruction performance in terms of efficiency as well as in terms of energy resolution. In addition, we show the jet reconstruction performance of our method and discuss its inference computational cost. To our knowledge, this work is the first-ever example of single-shot calorimetric reconstruction of ${\cal O}(1000)$ particles in high-luminosity conditions with 200 pileup.



### Long Movie Clip Classification with State-Space Video Models
- **Arxiv ID**: http://arxiv.org/abs/2204.01692v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01692v3)
- **Published**: 2022-04-04 17:58:02+00:00
- **Updated**: 2023-01-04 11:54:58+00:00
- **Authors**: Md Mohaiminul Islam, Gedas Bertasius
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Most modern video recognition models are designed to operate on short video clips (e.g., 5-10s in length). Thus, it is challenging to apply such models to long movie understanding tasks, which typically require sophisticated long-range temporal reasoning. The recently introduced video transformers partially address this issue by using long-range temporal self-attention. However, due to the quadratic cost of self-attention, such models are often costly and impractical to use. Instead, we propose ViS4mer, an efficient long-range video model that combines the strengths of self-attention and the recently introduced structured state-space sequence (S4) layer. Our model uses a standard Transformer encoder for short-range spatiotemporal feature extraction, and a multi-scale temporal S4 decoder for subsequent long-range temporal reasoning. By progressively reducing the spatiotemporal feature resolution and channel dimension at each decoder layer, ViS4mer learns complex long-range spatiotemporal dependencies in a video. Furthermore, ViS4mer is $2.63\times$ faster and requires $8\times$ less GPU memory than the corresponding pure self-attention-based model. Additionally, ViS4mer achieves state-of-the-art results in $6$ out of $9$ long-form movie video classification tasks on the Long Video Understanding (LVU) benchmark. Furthermore, we show that our approach successfully generalizes to other domains, achieving competitive results on the Breakfast and the COIN procedural activity datasets. The code is publicly available at: https://github.com/md-mohaiminul/ViS4mer.



### Monitoring social distancing with single image depth estimation
- **Arxiv ID**: http://arxiv.org/abs/2204.01693v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01693v2)
- **Published**: 2022-04-04 17:58:02+00:00
- **Updated**: 2022-04-29 15:21:54+00:00
- **Authors**: Alessio Mingozzi, Andrea Conti, Filippo Aleotti, Matteo Poggi, Stefano Mattoccia
- **Comment**: Accepted for pubblication on IEEE Transactions on Emerging Topics in
  Computational Intelligence (TETCI)
- **Journal**: None
- **Summary**: The recent pandemic emergency raised many challenges regarding the countermeasures aimed at containing the virus spread, and constraining the minimum distance between people resulted in one of the most effective strategies. Thus, the implementation of autonomous systems capable of monitoring the so-called social distance gained much interest. In this paper, we aim to address this task leveraging a single RGB frame without additional depth sensors. In contrast to existing single-image alternatives failing when ground localization is not available, we rely on single image depth estimation to perceive the 3D structure of the observed scene and estimate the distance between people. During the setup phase, a straightforward calibration procedure, leveraging a scale-aware SLAM algorithm available even on consumer smartphones, allows us to address the scale ambiguity affecting single image depth estimation. We validate our approach through indoor and outdoor images employing a calibrated LiDAR + RGB camera asset. Experimental results highlight that our proposal enables sufficiently reliable estimation of the inter-personal distance to monitor social distancing effectively. This fact confirms that despite its intrinsic ambiguity, if appropriately driven single image depth estimation can be a viable alternative to other depth perception techniques, more expensive and not always feasible in practical applications. Our evaluation also highlights that our framework can run reasonably fast and comparably to competitors, even on pure CPU systems. Moreover, its practical deployment on low-power systems is around the corner.



### "This is my unicorn, Fluffy": Personalizing frozen vision-language representations
- **Arxiv ID**: http://arxiv.org/abs/2204.01694v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01694v3)
- **Published**: 2022-04-04 17:58:11+00:00
- **Updated**: 2022-08-02 13:42:20+00:00
- **Authors**: Niv Cohen, Rinon Gal, Eli A. Meirom, Gal Chechik, Yuval Atzmon
- **Comment**: Accepted to ECCV (Oral). Compared to the ECCV camera ready version,
  we moved the ablation study to the main text, and updated the related work
- **Journal**: None
- **Summary**: Large Vision & Language models pretrained on web-scale data provide representations that are invaluable for numerous V&L problems. However, it is unclear how they can be used for reasoning about user-specific visual concepts in unstructured language. This problem arises in multiple domains, from personalized image retrieval to personalized interaction with smart devices. We introduce a new learning setup called Personalized Vision & Language (PerVL) with two new benchmark datasets for retrieving and segmenting user-specific "personalized" concepts "in the wild". In PerVL, one should learn personalized concepts (1) independently of the downstream task (2) allowing a pretrained model to reason about them with free language, and (3) does not require personalized negative examples. We propose an architecture for solving PerVL that operates by extending the input vocabulary of a pretrained model with new word embeddings for the new personalized concepts. The model can then reason about them by simply using them in a sentence. We demonstrate that our approach learns personalized visual concepts from a few examples and can effectively apply them in image retrieval and semantic segmentation using rich textual queries.



### LISA: Learning Implicit Shape and Appearance of Hands
- **Arxiv ID**: http://arxiv.org/abs/2204.01695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01695v1)
- **Published**: 2022-04-04 17:59:03+00:00
- **Updated**: 2022-04-04 17:59:03+00:00
- **Authors**: Enric Corona, Tomas Hodan, Minh Vo, Francesc Moreno-Noguer, Chris Sweeney, Richard Newcombe, Lingni Ma
- **Comment**: Published at CVPR 2022
- **Journal**: None
- **Summary**: This paper proposes a do-it-all neural model of human hands, named LISA. The model can capture accurate hand shape and appearance, generalize to arbitrary hand subjects, provide dense surface correspondences, be reconstructed from images in the wild and easily animated. We train LISA by minimizing the shape and appearance losses on a large set of multi-view RGB image sequences annotated with coarse 3D poses of the hand skeleton. For a 3D point in the hand local coordinate, our model predicts the color and the signed distance with respect to each hand bone independently, and then combines the per-bone predictions using predicted skinning weights. The shape, color and pose representations are disentangled by design, allowing to estimate or animate only selected parameters. We experimentally demonstrate that LISA can accurately reconstruct a dynamic hand from monocular or multi-view sequences, achieving a noticeably higher quality of reconstructed hand shapes compared to baseline approaches. Project page: https://www.iri.upc.edu/people/ecorona/lisa/.



### Joint Hand Motion and Interaction Hotspots Prediction from Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/2204.01696v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01696v1)
- **Published**: 2022-04-04 17:59:03+00:00
- **Updated**: 2022-04-04 17:59:03+00:00
- **Authors**: Shaowei Liu, Subarna Tripathi, Somdeb Majumdar, Xiaolong Wang
- **Comment**: CVPR 2022, Project page: https://stevenlsw.github.io/hoi-forecast
- **Journal**: None
- **Summary**: We propose to forecast future hand-object interactions given an egocentric video. Instead of predicting action labels or pixels, we directly predict the hand motion trajectory and the future contact points on the next active object (i.e., interaction hotspots). This relatively low-dimensional representation provides a concrete description of future interactions. To tackle this task, we first provide an automatic way to collect trajectory and hotspots labels on large-scale data. We then use this data to train an Object-Centric Transformer (OCT) model for prediction. Our model performs hand and object interaction reasoning via the self-attention mechanism in Transformers. OCT also provides a probabilistic framework to sample the future trajectory and hotspots to handle uncertainty in prediction. We perform experiments on the Epic-Kitchens-55, Epic-Kitchens-100, and EGTEA Gaze+ datasets, and show that OCT significantly outperforms state-of-the-art approaches by a large margin. Project page is available at https://stevenlsw.github.io/hoi-forecast .



### Learning Neural Acoustic Fields
- **Arxiv ID**: http://arxiv.org/abs/2204.00628v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, cs.RO, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2204.00628v2)
- **Published**: 2022-04-04 17:59:37+00:00
- **Updated**: 2023-01-15 02:41:34+00:00
- **Authors**: Andrew Luo, Yilun Du, Michael J. Tarr, Joshua B. Tenenbaum, Antonio Torralba, Chuang Gan
- **Comment**: NeurIPS 2022. Project page:
  https://www.andrew.cmu.edu/user/afluo/Neural_Acoustic_Fields/
- **Journal**: None
- **Summary**: Our environment is filled with rich and dynamic acoustic information. When we walk into a cathedral, the reverberations as much as appearance inform us of the sanctuary's wide open space. Similarly, as an object moves around us, we expect the sound emitted to also exhibit this movement. While recent advances in learned implicit functions have led to increasingly higher quality representations of the visual world, there have not been commensurate advances in learning spatial auditory representations. To address this gap, we introduce Neural Acoustic Fields (NAFs), an implicit representation that captures how sounds propagate in a physical scene. By modeling acoustic propagation in a scene as a linear time-invariant system, NAFs learn to continuously map all emitter and listener location pairs to a neural impulse response function that can then be applied to arbitrary sounds. We demonstrate that the continuous nature of NAFs enables us to render spatial acoustics for a listener at an arbitrary location, and can predict sound propagation at novel locations. We further show that the representation learned by NAFs can help improve visual learning with sparse views. Finally, we show that a representation informative of scene structure emerges during the learning of NAFs.



### MaxViT: Multi-Axis Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2204.01697v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01697v4)
- **Published**: 2022-04-04 17:59:44+00:00
- **Updated**: 2022-09-09 17:57:10+00:00
- **Authors**: Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li
- **Comment**: ECCV 2022; code: https://github.com/google-research/maxvit v1:
  initials; v2: added GAN visuals; v3: fixed ImageNet-1k acc typos for Maxvit @
  384
- **Journal**: None
- **Summary**: Transformers have recently gained significant attention in the computer vision community. However, the lack of scalability of self-attention mechanisms with respect to image size has limited their wide adoption in state-of-the-art vision backbones. In this paper we introduce an efficient and scalable attention model we call multi-axis attention, which consists of two aspects: blocked local and dilated global attention. These design choices allow global-local spatial interactions on arbitrary input resolutions with only linear complexity. We also present a new architectural element by effectively blending our proposed attention model with convolutions, and accordingly propose a simple hierarchical vision backbone, dubbed MaxViT, by simply repeating the basic building block over multiple stages. Notably, MaxViT is able to ''see'' globally throughout the entire network, even in earlier, high-resolution stages. We demonstrate the effectiveness of our model on a broad spectrum of vision tasks. On image classification, MaxViT achieves state-of-the-art performance under various settings: without extra data, MaxViT attains 86.5% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our model achieves 88.7% top-1 accuracy. For downstream tasks, MaxViT as a backbone delivers favorable performance on object detection as well as visual aesthetic assessment. We also show that our proposed model expresses strong generative modeling capability on ImageNet, demonstrating the superior potential of MaxViT blocks as a universal vision module. The source code and trained models will be available at https://github.com/google-research/maxvit.



### Face Recognition In Children: A Longitudinal Study
- **Arxiv ID**: http://arxiv.org/abs/2204.01760v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2204.01760v1)
- **Published**: 2022-04-04 18:00:45+00:00
- **Updated**: 2022-04-04 18:00:45+00:00
- **Authors**: Keivan Bahmani, Stephanie Schuckers
- **Comment**: None
- **Journal**: None
- **Summary**: The lack of high fidelity and publicly available longitudinal children face datasets is one of the main limiting factors in the development of face recognition systems for children. In this work, we introduce the Young Face Aging (YFA) dataset for analyzing the performance of face recognition systems over short age-gaps in children. We expand previous work by comparing YFA with several publicly available cross-age adult datasets to quantify the effects of short age-gap in adults and children. Our analysis confirms a statistically significant and matcher independent decaying relationship between the match scores of ArcFace-Focal, MagFace, and Facenet matchers and the age-gap between the gallery and probe images in children, even at the short age-gap of 6 months. However, our result indicates that the low verification performance reported in previous work might be due to the intra-class structure of the matcher and the lower quality of the samples. Our experiment using YFA and a state-of-the-art, quality-aware face matcher (MagFace) indicates 98.3% and 94.9% TAR at 0.1% FAR over 6 and 36 Months age-gaps, respectively, suggesting that face recognition may be feasible for children for age-gaps of up to three years.



### The First Principles of Deep Learning and Compression
- **Arxiv ID**: http://arxiv.org/abs/2204.01782v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2204.01782v1)
- **Published**: 2022-04-04 18:24:25+00:00
- **Updated**: 2022-04-04 18:24:25+00:00
- **Authors**: Max Ehrlich
- **Comment**: Doctoral Dissertation, more information at
  https://maxehr.umiacs.io/dissertation
- **Journal**: None
- **Summary**: The deep learning revolution incited by the 2012 Alexnet paper has been transformative for the field of computer vision. Many problems which were severely limited using classical solutions are now seeing unprecedented success. The rapid proliferation of deep learning methods has led to a sharp increase in their use in consumer and embedded applications. One consequence of consumer and embedded applications is lossy multimedia compression which is required to engineer the efficient storage and transmission of data in these real-world scenarios. As such, there has been increased interest in a deep learning solution for multimedia compression which would allow for higher compression ratios and increased visual quality.   The deep learning approach to multimedia compression, so called Learned Multimedia Compression, involves computing a compressed representation of an image or video using a deep network for the encoder and the decoder. While these techniques have enjoyed impressive academic success, their industry adoption has been essentially non-existent. Classical compression techniques like JPEG and MPEG are too entrenched in modern computing to be easily replaced. This dissertation takes an orthogonal approach and leverages deep learning to improve the compression fidelity of these classical algorithms. This allows the incredible advances in deep learning to be used for multimedia compression without threatening the ubiquity of the classical methods.   The key insight of this work is that methods which are motivated by first principles, i.e., the underlying engineering decisions that were made when the compression algorithms were developed, are more effective than general methods. By encoding prior knowledge into the design of the algorithm, the flexibility, performance, and/or accuracy are improved at the cost of generality...



### Object Permanence Emerges in a Random Walk along Memory
- **Arxiv ID**: http://arxiv.org/abs/2204.01784v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01784v2)
- **Published**: 2022-04-04 18:28:24+00:00
- **Updated**: 2022-06-13 16:31:22+00:00
- **Authors**: Pavel Tokmakov, Allan Jabri, Jie Li, Adrien Gaidon
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a self-supervised objective for learning representations that localize objects under occlusion - a property known as object permanence. A central question is the choice of learning signal in cases of total occlusion. Rather than directly supervising the locations of invisible objects, we propose a self-supervised objective that requires neither human annotation, nor assumptions about object dynamics. We show that object permanence can emerge by optimizing for temporal coherence of memory: we fit a Markov walk along a space-time graph of memories, where the states in each time step are non-Markovian features from a sequence encoder. This leads to a memory representation that stores occluded objects and predicts their motion, to better localize them. The resulting model outperforms existing approaches on several datasets of increasing complexity and realism, despite requiring minimal supervision, and hence being broadly applicable.



### Lightweight HDR Camera ISP for Robust Perception in Dynamic Illumination Conditions via Fourier Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.01795v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.01795v2)
- **Published**: 2022-04-04 18:48:51+00:00
- **Updated**: 2022-05-14 15:16:16+00:00
- **Authors**: Pranjay Shyam, Sandeep Singh Sengar, Kuk-Jin Yoon, Kyung-Soo Kim
- **Comment**: Accepted in BMVC 2021
- **Journal**: None
- **Summary**: The limited dynamic range of commercial compact camera sensors results in an inaccurate representation of scenes with varying illumination conditions, adversely affecting image quality and subsequently limiting the performance of underlying image processing algorithms. Current state-of-the-art (SoTA) convolutional neural networks (CNN) are developed as post-processing techniques to independently recover under-/over-exposed images. However, when applied to images containing real-world degradations such as glare, high-beam, color bleeding with varying noise intensity, these algorithms amplify the degradations, further degrading image quality. We propose a lightweight two-stage image enhancement algorithm sequentially balancing illumination and noise removal using frequency priors for structural guidance to overcome these limitations. Furthermore, to ensure realistic image quality, we leverage the relationship between frequency and spatial domain properties of an image and propose a Fourier spectrum-based adversarial framework (AFNet) for consistent image enhancement under varying illumination conditions. While current formulations of image enhancement are envisioned as post-processing techniques, we examine if such an algorithm could be extended to integrate the functionality of the Image Signal Processing (ISP) pipeline within the camera sensor benefiting from RAW sensor data and lightweight CNN architecture. Based on quantitative and qualitative evaluations, we also examine the practicality and effects of image enhancement techniques on the performance of common perception tasks such as object detection and semantic segmentation in varying illumination conditions.



### Revisiting Near/Remote Sensing with Geospatial Attention
- **Arxiv ID**: http://arxiv.org/abs/2204.01807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01807v1)
- **Published**: 2022-04-04 19:19:50+00:00
- **Updated**: 2022-04-04 19:19:50+00:00
- **Authors**: Scott Workman, M. Usman Rafique, Hunter Blanton, Nathan Jacobs
- **Comment**: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
  2022
- **Journal**: None
- **Summary**: This work addresses the task of overhead image segmentation when auxiliary ground-level images are available. Recent work has shown that performing joint inference over these two modalities, often called near/remote sensing, can yield significant accuracy improvements. Extending this line of work, we introduce the concept of geospatial attention, a geometry-aware attention mechanism that explicitly considers the geospatial relationship between the pixels in a ground-level image and a geographic location. We propose an approach for computing geospatial attention that incorporates geometric features and the appearance of the overhead and ground-level imagery. We introduce a novel architecture for near/remote sensing that is based on geospatial attention and demonstrate its use for five segmentation tasks. The results demonstrate that our method significantly outperforms the previous state-of-the-art methods.



### Towards Infield Navigation: leveraging simulated data for crop row detection
- **Arxiv ID**: http://arxiv.org/abs/2204.01811v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.01811v1)
- **Published**: 2022-04-04 19:28:30+00:00
- **Updated**: 2022-04-04 19:28:30+00:00
- **Authors**: Rajitha de Silva, Grzegorz Cielniak, Junfeng Gao
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. arXiv admin note: substantial text overlap with
  arXiv:2109.08247
- **Journal**: None
- **Summary**: Agricultural datasets for crop row detection are often bound by their limited number of images. This restricts the researchers from developing deep learning based models for precision agricultural tasks involving crop row detection. We suggest the utilization of small real-world datasets along with additional data generated by simulations to yield similar crop row detection performance as that of a model trained with a large real world dataset. Our method could reach the performance of a deep learning based crop row detection model trained with real-world data by using 60% less labelled real-world data. Our model performed well against field variations such as shadows, sunlight and grow stages. We introduce an automated pipeline to generate labelled images for crop row detection in simulation domain. An extensive comparison is done to analyze the contribution of simulated data towards reaching robust crop row detection in various real-world field scenarios.



### High Efficiency Pedestrian Crossing Prediction
- **Arxiv ID**: http://arxiv.org/abs/2204.01862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.01862v1)
- **Published**: 2022-04-04 21:37:57+00:00
- **Updated**: 2022-04-04 21:37:57+00:00
- **Authors**: Zhuoran Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting pedestrian crossing intention is an indispensable aspect of deploying advanced driving systems (ADS) or advanced driver-assistance systems (ADAS) to real life. State-of-the-art methods in predicting pedestrian crossing intention often rely on multiple streams of information as inputs, each of which requires massive computational resources and heavy network architectures to generate. However, such reliance limits the practical application of the systems. In this paper, driven the the real-world demands of pedestrian crossing intention prediction models with both high efficiency and accuracy, we introduce a network with only frames of pedestrians as the input. Every component in the introduced network is driven by the goal of light weight. Specifically, we reduce the multi-source input dependency and employ light neural networks that are tailored for mobile devices. These smaller neural networks can fit into computer memory and can be transmitted over a computer network more easily, thus making them more suitable for real-life deployment and real-time prediction. To compensate the removal of the multi-source input, we enhance the network effectiveness by adopting a multi-task learning training, named "side task learning", to include multiple auxiliary tasks to jointly learn the feature extractor for improved robustness. Each head handles a specific task that potentially shares knowledge with other heads. In the meantime, the feature extractor is shared across all tasks to ensure the sharing of basic knowledge across all layers. The light weight but high efficiency characteristics of our model endow it the potential of being deployed on vehicle-based systems. Experiments validate that our model consistently delivers outstanding performances.



### Around View Monitoring System for Hydraulic Excavators
- **Arxiv ID**: http://arxiv.org/abs/2205.11224v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.11224v1)
- **Published**: 2022-04-04 21:54:43+00:00
- **Updated**: 2022-04-04 21:54:43+00:00
- **Authors**: Dong Jun Yeom, Yu Na Hong, Yoojun Kim, Hyun Seok Yoo, Youngsuk Kim
- **Comment**: 9 pages, 11 figures
- **Journal**: The 7th International Conference on Construction Engineering and
  Project Management (ICCEPM 2017), Oct. 27-30, 2017, Chengdu, China
- **Summary**: This paper describes the Around View Monitoring (AVM) system for hydraulic excavators that prevents the safety accidents caused by blind spots and increases the operational efficiency. To verify the developed system, experiments were conducted with its prototype. The experimental results demonstrate its applicability in the field with the following values: 7m of a visual range, 15fps of image refresh rate, 300ms of working information data reception rate, and 300ms of surface condition data reception rate.



### Truck Axle Detection with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.01868v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2204.01868v2)
- **Published**: 2022-04-04 22:11:49+00:00
- **Updated**: 2023-03-03 12:41:10+00:00
- **Authors**: Leandro Arab Marcomini, André Luiz Cunha
- **Comment**: Code and dataset available for donwload, links provided
- **Journal**: None
- **Summary**: Axle count in trucks is important to the classification of vehicles and to the operation of road systems. It is used in the determination of service fees and in the impact on the pavement. Although axle count can be achieved with traditional methods, such as manual labor, it is increasingly possible to count axles using deep learning and computer vision methods. This paper aims to compare three deep-learning object detection algorithms, YOLO, Faster R-CNN, and SSD, for the detection of truck axles. A dataset was built to provide training and testing examples for the neural networks. The training was done on different base models, to increase training time efficiency and to compare results. We evaluated results based on five metrics: precision, recall, mAP, F1-score, and FPS count. Results indicate that YOLO and SSD have similar accuracy and performance, with more than 96\% mAP for both models. Datasets and codes are publicly available for download.



### MonoTrack: Shuttle trajectory reconstruction from monocular badminton video
- **Arxiv ID**: http://arxiv.org/abs/2204.01899v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.01899v2)
- **Published**: 2022-04-04 23:57:57+00:00
- **Updated**: 2022-05-18 17:59:57+00:00
- **Authors**: Paul Liu, Jui-Hsien Wang
- **Comment**: To appear in CVSports@CVPR 2022
- **Journal**: None
- **Summary**: Trajectory estimation is a fundamental component of racket sport analytics, as the trajectory contains information not only about the winning and losing of each point, but also how it was won or lost. In sports such as badminton, players benefit from knowing the full 3D trajectory, as the height of shuttlecock or ball provides valuable tactical information. Unfortunately, 3D reconstruction is a notoriously hard problem, and standard trajectory estimators can only track 2D pixel coordinates. In this work, we present the first complete end-to-end system for the extraction and segmentation of 3D shuttle trajectories from monocular badminton videos. Our system integrates badminton domain knowledge such as court dimension, shot placement, physical laws of motion, along with vision-based features such as player poses and shuttle tracking. We find that significant engineering efforts and model improvements are needed to make the overall system robust, and as a by-product of our work, improve state-of-the-art results on court recognition, 2D trajectory estimation, and hit recognition.



