# Arxiv Papers in cs.CV on 2022-04-20
### Learned Monocular Depth Priors in Visual-Inertial Initialization
- **Arxiv ID**: http://arxiv.org/abs/2204.09171v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09171v2)
- **Published**: 2022-04-20 00:30:04+00:00
- **Updated**: 2022-08-01 22:48:33+00:00
- **Authors**: Yunwen Zhou, Abhishek Kar, Eric Turner, Adarsh Kowdle, Chao X. Guo, Ryan C. DuToit, Konstantine Tsotsos
- **Comment**: to be published in 2022 European Conference on Computer Vision
- **Journal**: None
- **Summary**: Visual-inertial odometry (VIO) is the pose estimation backbone for most AR/VR and autonomous robotic systems today, in both academia and industry. However, these systems are highly sensitive to the initialization of key parameters such as sensor biases, gravity direction, and metric scale. In practical scenarios where high-parallax or variable acceleration assumptions are rarely met (e.g. hovering aerial robot, smartphone AR user not gesticulating with phone), classical visual-inertial initialization formulations often become ill-conditioned and/or fail to meaningfully converge. In this paper we target visual-inertial initialization specifically for these low-excitation scenarios critical to in-the-wild usage. We propose to circumvent the limitations of classical visual-inertial structure-from-motion (SfM) initialization by incorporating a new learning-based measurement as a higher-level input. We leverage learned monocular depth images (mono-depth) to constrain the relative depth of features, and upgrade the mono-depths to metric scale by jointly optimizing for their scales and shifts. Our experiments show a significant improvement in problem conditioning compared to a classical formulation for visual-inertial initialization, and demonstrate significant accuracy and robustness improvements relative to the state-of-the-art on public benchmarks, particularly under low-excitation scenarios. We further extend this improvement to implementation within an existing odometry system to illustrate the impact of our improved initialization method on resulting tracking trajectories.



### Reconstruction-Aware Prior Distillation for Semi-supervised Point Cloud Completion
- **Arxiv ID**: http://arxiv.org/abs/2204.09186v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09186v4)
- **Published**: 2022-04-20 02:14:20+00:00
- **Updated**: 2023-07-24 03:20:19+00:00
- **Authors**: Zhaoxin Fan, Yulin He, Zhicheng Wang, Kejian Wu, Hongyan Liu, Jun He
- **Comment**: Accepted to IJCAI 2023
- **Journal**: None
- **Summary**: Real-world sensors often produce incomplete, irregular, and noisy point clouds, making point cloud completion increasingly important. However, most existing completion methods rely on large paired datasets for training, which is labor-intensive. This paper proposes RaPD, a novel semi-supervised point cloud completion method that reduces the need for paired datasets. RaPD utilizes a two-stage training scheme, where a deep semantic prior is learned in stage 1 from unpaired complete and incomplete point clouds, and a semi-supervised prior distillation process is introduced in stage 2 to train a completion network using only a small number of paired samples. Additionally, a self-supervised completion module is introduced to improve performance using unpaired incomplete point clouds. Experiments on multiple datasets show that RaPD outperforms previous methods in both homologous and heterologous scenarios.



### NTIRE 2022 Challenge on Stereo Image Super-Resolution: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2204.09197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09197v1)
- **Published**: 2022-04-20 02:55:37+00:00
- **Updated**: 2022-04-20 02:55:37+00:00
- **Authors**: Longguang Wang, Yulan Guo, Yingqian Wang, Juncheng Li, Shuhang Gu, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we summarize the 1st NTIRE challenge on stereo image super-resolution (restoration of rich details in a pair of low-resolution stereo images) with a focus on new solutions and results. This challenge has 1 track aiming at the stereo image super-resolution problem under a standard bicubic degradation. In total, 238 participants were successfully registered, and 21 teams competed in the final testing phase. Among those participants, 20 teams successfully submitted results with PSNR (RGB) scores better than the baseline. This challenge establishes a new benchmark for stereo image SR.



### Interventional Multi-Instance Learning with Deconfounded Instance-Level Prediction
- **Arxiv ID**: http://arxiv.org/abs/2204.09204v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.09204v2)
- **Published**: 2022-04-20 03:17:36+00:00
- **Updated**: 2022-04-22 06:00:15+00:00
- **Authors**: Tiancheng Lin, Hongteng Xu, Canqian Yang, Yi Xu
- **Comment**: 7 pages. Accepted by AAAI2022
- **Journal**: None
- **Summary**: When applying multi-instance learning (MIL) to make predictions for bags of instances, the prediction accuracy of an instance often depends on not only the instance itself but also its context in the corresponding bag. From the viewpoint of causal inference, such bag contextual prior works as a confounder and may result in model robustness and interpretability issues. Focusing on this problem, we propose a novel interventional multi-instance learning (IMIL) framework to achieve deconfounded instance-level prediction. Unlike traditional likelihood-based strategies, we design an Expectation-Maximization (EM) algorithm based on causal intervention, providing a robust instance selection in the training phase and suppressing the bias caused by the bag contextual prior. Experiments on pathological image analysis demonstrate that our IMIL method substantially reduces false positives and outperforms state-of-the-art MIL methods.



### Does Interference Exist When Training a Once-For-All Network?
- **Arxiv ID**: http://arxiv.org/abs/2204.09210v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09210v1)
- **Published**: 2022-04-20 03:33:10+00:00
- **Updated**: 2022-04-20 03:33:10+00:00
- **Authors**: Jordan Shipard, Arnold Wiliem, Clinton Fookes
- **Comment**: Accepted to CVPR Embedded Vision Workshop 2022
- **Journal**: None
- **Summary**: The Once-For-All (OFA) method offers an excellent pathway to deploy a trained neural network model into multiple target platforms by utilising the supernet-subnet architecture. Once trained, a subnet can be derived from the supernet (both architecture and trained weights) and deployed directly to the target platform with little to no retraining or fine-tuning. To train the subnet population, OFA uses a novel training method called Progressive Shrinking (PS) which is designed to limit the negative impact of interference during training. It is believed that higher interference during training results in lower subnet population accuracies. In this work we take a second look at this interference effect. Surprisingly, we find that interference mitigation strategies do not have a large impact on the overall subnet population performance. Instead, we find the subnet architecture selection bias during training to be a more important aspect. To show this, we propose a simple-yet-effective method called Random Subnet Sampling (RSS), which does not have mitigation on the interference effect. Despite no mitigation, RSS is able to produce a better performing subnet population than PS in four small-to-medium-sized datasets; suggesting that the interference effect does not play a pivotal role in these datasets. Due to its simplicity, RSS provides a $1.9\times$ reduction in training times compared to PS. A $6.1\times$ reduction can also be achieved with a reasonable drop in performance when the number of RSS training epochs are reduced. Code available at https://github.com/Jordan-HS/RSS-Interference-CVPRW2022.



### Efficient Progressive High Dynamic Range Image Restoration via Attention and Alignment Network
- **Arxiv ID**: http://arxiv.org/abs/2204.09213v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09213v1)
- **Published**: 2022-04-20 03:42:53+00:00
- **Updated**: 2022-04-20 03:42:53+00:00
- **Authors**: Gaocheng Yu, Jin Zhang, Zhe Ma, Hongbin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: HDR is an important part of computational photography technology. In this paper, we propose a lightweight neural network called Efficient Attention-and-alignment-guided Progressive Network (EAPNet) for the challenge NTIRE 2022 HDR Track 1 and Track 2. We introduce a multi-dimensional lightweight encoding module to extract features. Besides, we propose Progressive Dilated U-shape Block (PDUB) that can be a progressive plug-and-play module for dynamically tuning MAccs and PSNR. Finally, we use fast and low-power feature-align module to deal with misalignment problem in place of the time-consuming Deformable Convolutional Network (DCN). The experiments show that our method achieves about 20 times compression on MAccs with better mu-PSNR and PSNR compared to the state-of-the-art method. We got the second place of both two tracks during the testing phase. Figure1. shows the visualized result of NTIRE 2022 HDR challenge.



### Vision System of Curling Robots: Thrower and Skip
- **Arxiv ID**: http://arxiv.org/abs/2204.09221v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.09221v1)
- **Published**: 2022-04-20 04:46:36+00:00
- **Updated**: 2022-04-20 04:46:36+00:00
- **Authors**: Seongwook Yoon, Gayoung Kim, Myungpyo Hong, Sanghoon Sull
- **Comment**: None
- **Journal**: None
- **Summary**: We built a vision system of curling robot which can be expected to play with human curling player. Basically, we built two types of vision systems for thrower and skip robots, respectively. First, the thrower robot drives towards a given point of curling sheet to release a stone. Our vision system in the thrower robot initialize 3DoF pose on two dimensional curling sheet and updates the pose to decide for the decision of stone release. Second, the skip robot stands at the opposite side of the thrower robot and monitors the state of the game to make a strategic decision. Our vision system in the skip robot recognize every stones on the curling sheet precisely. Since the viewpoint is quite perspective, many stones are occluded by each others so it is challenging to estimate the accurate position of stone. Thus, we recognize the ellipses of stone handles outline to find the exact midpoint of the stones using perspective Hough transform. Furthermore, we perform tracking of a thrown stone to produce a trajectory for ice condition analysis. Finally, we implemented our vision systems on two mobile robots and successfully perform a single turn and even careful gameplay. Specifically, our vision system includes three cameras with different viewpoint for their respective purposes.



### K-LITE: Learning Transferable Visual Models with External Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2204.09222v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.09222v2)
- **Published**: 2022-04-20 04:47:01+00:00
- **Updated**: 2022-10-22 01:35:48+00:00
- **Authors**: Sheng Shen, Chunyuan Li, Xiaowei Hu, Jianwei Yang, Yujia Xie, Pengchuan Zhang, Zhe Gan, Lijuan Wang, Lu Yuan, Ce Liu, Kurt Keutzer, Trevor Darrell, Anna Rohrbach, Jianfeng Gao
- **Comment**: NeurIPS 2022 camera ready
- **Journal**: None
- **Summary**: The new generation of state-of-the-art computer vision systems are trained from natural language supervision, ranging from simple object category names to descriptive captions. This form of supervision ensures high generality and usability of the learned visual models, due to the broad concept coverage achieved via large-scale data collection process. Alternatively, we argue that learning with external knowledge is a promising way which leverages a much more structured source of supervision and offers sample efficiency. We propose K-LITE, a simple strategy to leverage external knowledge for building transferable visual systems: In training, it enriches entities in text with WordNet and Wiktionary knowledge, leading to an efficient and scalable approach to learning image representations that uses knowledge about the visual concepts. In evaluation, the text is also augmented with external knowledge and then used to reference learned visual concepts (or describe new ones) to enable zero-shot and few-shot transfer of the pre-trained models. We study the performance of K-LITE on two important computer vision problems, image classification and object detection, benchmarking on 20 and 13 different existing datasets, respectively. The proposed knowledge-augmented models show significant improvement in transfer learning performance over existing methods. Our code is available at https://github.com/microsoft/klite.



### Dark Spot Detection from SAR Images Based on Superpixel Deeper Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2204.09230v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09230v1)
- **Published**: 2022-04-20 05:23:04+00:00
- **Updated**: 2022-04-20 05:23:04+00:00
- **Authors**: Xiaojian Liu, Yansheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic Aperture Radar (SAR) is the main instrument utilized for the detection of oil slicks on the ocean surface. In SAR images, some areas affected by ocean phenomena, such as rain cells, upwellings, and internal waves, or discharge from oil spills appear as dark spots on images. Dark spot detection is the first step in the detection of oil spills, which then become oil slick candidates. The accuracy of dark spot segmentation ultimately affects the accuracy of oil slick identification. Although some advanced deep learning methods that use pixels as processing units perform well in remote sensing image semantic segmentation, detecting some dark spots with weak boundaries from noisy SAR images remains a huge challenge. We propose a dark spot detection method based on superpixels deeper graph convolutional networks (SGDCN) in this paper, which takes the superpixels as the processing units and extracts features for each superpixel. The features calculated from superpixel regions are more robust than those from fixed pixel neighborhoods. To reduce the difficulty of learning tasks, we discard irrelevant features and obtain an optimal subset of features. After superpixel segmentation, the images are transformed into graphs with superpixels as nodes, which are fed into the deeper graph convolutional neural network for node classification. This graph neural network uses a differentiable aggregation function to aggregate the features of nodes and neighbors to form more advanced features. It is the first time using it for dark spot detection. To validate our method, we mark all dark spots on six SAR images covering the Baltic Sea and construct a dark spots detection dataset, which has been made publicly available (https://drive.google.com/drive/folders/12UavrntkDSPrItISQ8iGefXn2gIZHxJ6?usp=sharing). The experimental results demonstrate that our proposed SGDCN is robust and effective.



### Visual-based Positioning and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2204.09232v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09232v1)
- **Published**: 2022-04-20 05:30:34+00:00
- **Updated**: 2022-04-20 05:30:34+00:00
- **Authors**: Somnuk Phon-Amnuaisuk, Ken T. Murata, La-Or Kovavisaruch, Tiong-Hoo Lim, Praphan Pavarangkoon, Takamichi Mizuhara
- **Comment**: This paper is the expanded version of our paper titled Visual-based
  Positioning and Pose Estimation, ICONIP (4) 2020: 410-417
- **Journal**: None
- **Summary**: Recent advances in deep learning and computer vision offer an excellent opportunity to investigate high-level visual analysis tasks such as human localization and human pose estimation. Although the performance of human localization and human pose estimation has significantly improved in recent reports, they are not perfect and erroneous localization and pose estimation can be expected among video frames. Studies on the integration of these techniques into a generic pipeline that is robust to noise introduced from those errors are still lacking. This paper fills the missing study. We explored and developed two working pipelines that suited the visual-based positioning and pose estimation tasks. Analyses of the proposed pipelines were conducted on a badminton game. We showed that the concept of tracking by detection could work well, and errors in position and pose could be effectively handled by a linear interpolation technique using information from nearby frames. The results showed that the Visual-based Positioning and Pose Estimation could deliver position and pose estimations with good spatial and temporal resolutions.



### Solving The Long-Tailed Problem via Intra- and Inter-Category Balance
- **Arxiv ID**: http://arxiv.org/abs/2204.09234v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09234v2)
- **Published**: 2022-04-20 05:36:20+00:00
- **Updated**: 2022-04-22 05:59:47+00:00
- **Authors**: Renhui Zhang, Tiancheng Lin, Rui Zhang, Yi Xu
- **Comment**: 4 pages. Accepted by ICASSP2022
- **Journal**: None
- **Summary**: Benchmark datasets for visual recognition assume that data is uniformly distributed, while real-world datasets obey long-tailed distribution. Current approaches handle the long-tailed problem to transform the long-tailed dataset to uniform distribution by re-sampling or re-weighting strategies. These approaches emphasize the tail classes but ignore the hard examples in head classes, which result in performance degradation. In this paper, we propose a novel gradient harmonized mechanism with category-wise adaptive precision to decouple the difficulty and sample size imbalance in the long-tailed problem, which are correspondingly solved via intra- and inter-category balance strategies. Specifically, intra-category balance focuses on the hard examples in each category to optimize the decision boundary, while inter-category balance aims to correct the shift of decision boundary by taking each category as a unit. Extensive experiments demonstrate that the proposed method consistently outperforms other approaches on all the datasets.



### FS-NCSR: Increasing Diversity of the Super-Resolution Space via Frequency Separation and Noise-Conditioned Normalizing Flow
- **Arxiv ID**: http://arxiv.org/abs/2204.09679v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09679v1)
- **Published**: 2022-04-20 06:44:56+00:00
- **Updated**: 2022-04-20 06:44:56+00:00
- **Authors**: Ki-Ung Song, Dongseok Shim, Kang-wook Kim, Jae-young Lee, Younggeun Kim
- **Comment**: CVPRW 2022, First three authors are equally contributed
- **Journal**: None
- **Summary**: Super-resolution suffers from an innate ill-posed problem that a single low-resolution (LR) image can be from multiple high-resolution (HR) images. Recent studies on the flow-based algorithm solve this ill-posedness by learning the super-resolution space and predicting diverse HR outputs. Unfortunately, the diversity of the super-resolution outputs is still unsatisfactory, and the outputs from the flow-based model usually suffer from undesired artifacts which causes low-quality outputs. In this paper, we propose FS-NCSR which produces diverse and high-quality super-resolution outputs using frequency separation and noise conditioning compared to the existing flow-based approaches. As the sharpness and high-quality detail of the image rely on its high-frequency information, FS-NCSR only estimates the high-frequency information of the high-resolution outputs without redundant low-frequency components. Through this, FS-NCSR significantly improves the diversity score without significant image quality degradation compared to the NCSR, the winner of the previous NTIRE 2021 challenge.



### Uncertainty-based Cross-Modal Retrieval with Probabilistic Representations
- **Arxiv ID**: http://arxiv.org/abs/2204.09268v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2204.09268v1)
- **Published**: 2022-04-20 07:24:20+00:00
- **Updated**: 2022-04-20 07:24:20+00:00
- **Authors**: Leila Pishdad, Ran Zhang, Konstantinos G. Derpanis, Allan Jepson, Afsaneh Fazly
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: Probabilistic embeddings have proven useful for capturing polysemous word meanings, as well as ambiguity in image matching. In this paper, we study the advantages of probabilistic embeddings in a cross-modal setting (i.e., text and images), and propose a simple approach that replaces the standard vector point embeddings in extant image-text matching models with probabilistic distributions that are parametrically learned. Our guiding hypothesis is that the uncertainty encoded in the probabilistic embeddings captures the cross-modal ambiguity in the input instances, and that it is through capturing this uncertainty that the probabilistic models can perform better at downstream tasks, such as image-to-text or text-to-image retrieval. Through extensive experiments on standard and new benchmarks, we show a consistent advantage for probabilistic representations in cross-modal retrieval, and validate the ability of our embeddings to capture uncertainty.



### A Survey of Video-based Action Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2204.09271v1
- **DOI**: 10.1109/INSAI54028.2021.00029
- **Categories**: **cs.CV**, I.4.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2204.09271v1)
- **Published**: 2022-04-20 07:29:00+00:00
- **Updated**: 2022-04-20 07:29:00+00:00
- **Authors**: Shunli Wang, Dingkang Yang, Peng Zhai, Qing Yu, Tao Suo, Zhan Sun, Ka Li, Lihua Zhang
- **Comment**: 9 pages, 6 figures, conference paper
- **Journal**: None
- **Summary**: Human action recognition and analysis have great demand and important application significance in video surveillance, video retrieval, and human-computer interaction. The task of human action quality evaluation requires the intelligent system to automatically and objectively evaluate the action completed by the human. The action quality assessment model can reduce the human and material resources spent in action evaluation and reduce subjectivity. In this paper, we provide a comprehensive survey of existing papers on video-based action quality assessment. Different from human action recognition, the application scenario of action quality assessment is relatively narrow. Most of the existing work focuses on sports and medical care. We first introduce the definition and challenges of human action quality assessment. Then we present the existing datasets and evaluation metrics. In addition, we summarized the methods of sports and medical care according to the model categories and publishing institutions according to the characteristics of the two fields. At the end, combined with recent work, the promising development direction in action quality assessment is discussed.



### Sound-Guided Semantic Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2204.09273v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.09273v4)
- **Published**: 2022-04-20 07:33:10+00:00
- **Updated**: 2022-10-21 06:10:08+00:00
- **Authors**: Seung Hyun Lee, Gyeongrok Oh, Wonmin Byeon, Chanyoung Kim, Won Jeong Ryoo, Sang Ho Yoon, Hyunjun Cho, Jihyun Bae, Jinkyu Kim, Sangpil Kim
- **Comment**: None
- **Journal**: None
- **Summary**: The recent success in StyleGAN demonstrates that pre-trained StyleGAN latent space is useful for realistic video generation. However, the generated motion in the video is usually not semantically meaningful due to the difficulty of determining the direction and magnitude in the StyleGAN latent space. In this paper, we propose a framework to generate realistic videos by leveraging multimodal (sound-image-text) embedding space. As sound provides the temporal contexts of the scene, our framework learns to generate a video that is semantically consistent with sound. First, our sound inversion module maps the audio directly into the StyleGAN latent space. We then incorporate the CLIP-based multimodal embedding space to further provide the audio-visual relationships. Finally, the proposed frame generator learns to find the trajectory in the latent space which is coherent with the corresponding sound and generates a video in a hierarchical manner. We provide the new high-resolution landscape video dataset (audio-visual pair) for the sound-guided video generation task. The experiments show that our model outperforms the state-of-the-art methods in terms of video quality. We further show several applications including image and video editing to verify the effectiveness of our method.



### Situational Perception Guided Image Matting
- **Arxiv ID**: http://arxiv.org/abs/2204.09276v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2204.09276v3)
- **Published**: 2022-04-20 07:35:51+00:00
- **Updated**: 2022-04-22 11:01:16+00:00
- **Authors**: Bo Xu, Jiake Xie, Han Huang, Ziwen Li, Cheng Lu, Yong Tang, Yandong Guo
- **Comment**: 14 pages, 8 figures
- **Journal**: None
- **Summary**: Most automatic matting methods try to separate the salient foreground from the background. However, the insufficient quantity and subjective bias of the current existing matting datasets make it difficult to fully explore the semantic association between object-to-object and object-to-environment in a given image. In this paper, we propose a Situational Perception Guided Image Matting (SPG-IM) method that mitigates subjective bias of matting annotations and captures sufficient situational perception information for better global saliency distilled from the visual-to-textual task. SPG-IM can better associate inter-objects and object-to-environment saliency, and compensate the subjective nature of image matting and its expensive annotation. We also introduce a textual Semantic Transformation (TST) module that can effectively transform and integrate the semantic feature stream to guide the visual representations. In addition, an Adaptive Focal Transformation (AFT) Refinement Network is proposed to adaptively switch multi-scale receptive fields and focal points to enhance both global and local details. Extensive experiments demonstrate the effectiveness of situational perception guidance from the visual-to-textual tasks on image matting, and our model outperforms the state-of-the-art methods. We also analyze the significance of different components in our model. The code will be released soon.



### Reinforced Structured State-Evolution for Vision-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2204.09280v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09280v2)
- **Published**: 2022-04-20 07:51:20+00:00
- **Updated**: 2022-05-26 03:38:38+00:00
- **Authors**: Jinyu Chen, Chen Gao, Erli Meng, Qiong Zhang, Si Liu
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Vision-and-language Navigation (VLN) task requires an embodied agent to navigate to a remote location following a natural language instruction. Previous methods usually adopt a sequence model (e.g., Transformer and LSTM) as the navigator. In such a paradigm, the sequence model predicts action at each step through a maintained navigation state, which is generally represented as a one-dimensional vector. However, the crucial navigation clues (i.e., object-level environment layout) for embodied navigation task is discarded since the maintained vector is essentially unstructured. In this paper, we propose a novel Structured state-Evolution (SEvol) model to effectively maintain the environment layout clues for VLN. Specifically, we utilise the graph-based feature to represent the navigation state instead of the vector-based state. Accordingly, we devise a Reinforced Layout clues Miner (RLM) to mine and detect the most crucial layout graph for long-term navigation via a customised reinforcement learning strategy. Moreover, the Structured Evolving Module (SEM) is proposed to maintain the structured graph-based state during navigation, where the state is gradually evolved to learn the object-level spatial-temporal relationship. The experiments on the R2R and R4R datasets show that the proposed SEvol model improves VLN models' performance by large margins, e.g., +3% absolute SPL accuracy for NvEM and +8% for EnvDrop on the R2R test set.



### Human-Object Interaction Detection via Disentangled Transformer
- **Arxiv ID**: http://arxiv.org/abs/2204.09290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09290v1)
- **Published**: 2022-04-20 08:15:04+00:00
- **Updated**: 2022-04-20 08:15:04+00:00
- **Authors**: Desen Zhou, Zhichao Liu, Jian Wang, Leshan Wang, Tao Hu, Errui Ding, Jingdong Wang
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Human-Object Interaction Detection tackles the problem of joint localization and classification of human object interactions. Existing HOI transformers either adopt a single decoder for triplet prediction, or utilize two parallel decoders to detect individual objects and interactions separately, and compose triplets by a matching process. In contrast, we decouple the triplet prediction into human-object pair detection and interaction classification. Our main motivation is that detecting the human-object instances and classifying interactions accurately needs to learn representations that focus on different regions. To this end, we present Disentangled Transformer, where both encoder and decoder are disentangled to facilitate learning of two sub-tasks. To associate the predictions of disentangled decoders, we first generate a unified representation for HOI triplets with a base decoder, and then utilize it as input feature of each disentangled decoder. Extensive experiments show that our method outperforms prior work on two public HOI benchmarks by a sizeable margin. Code will be available.



### A 3-stage Spectral-spatial Method for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.09294v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2204.09294v1)
- **Published**: 2022-04-20 08:23:05+00:00
- **Updated**: 2022-04-20 08:23:05+00:00
- **Authors**: Raymond H. Chan, Ruoning Li
- **Comment**: 18 pages, 9 figures
- **Journal**: None
- **Summary**: Hyperspectral images often have hundreds of spectral bands of different wavelengths captured by aircraft or satellites that record land coverage. Identifying detailed classes of pixels becomes feasible due to the enhancement in spectral and spatial resolution of hyperspectral images. In this work, we propose a novel framework that utilizes both spatial and spectral information for classifying pixels in hyperspectral images. The method consists of three stages. In the first stage, the pre-processing stage, Nested Sliding Window algorithm is used to reconstruct the original data by {enhancing the consistency of neighboring pixels} and then Principal Component Analysis is used to reduce the dimension of data. In the second stage, Support Vector Machines are trained to estimate the pixel-wise probability map of each class using the spectral information from the images. Finally, a smoothed total variation model is applied to smooth the class probability vectors by {ensuring spatial connectivity} in the images. We demonstrate the superiority of our method against three state-of-the-art algorithms on six benchmark hyperspectral data sets with 10 to 50 training labels for each class. The results show that our method gives the overall best performance in accuracy. Especially, our gain in accuracy increases when the number of labeled pixels decreases and therefore our method is more advantageous to be applied to problems with small training set. Hence it is of great practical significance since expert annotations are often expensive and difficult to collect.



### Image Restoration in Non-Linear Filtering Domain using MDB approach
- **Arxiv ID**: http://arxiv.org/abs/2204.09296v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2204.09296v1)
- **Published**: 2022-04-20 08:23:52+00:00
- **Updated**: 2022-04-20 08:23:52+00:00
- **Authors**: S. K. Satpathy, S. Panda, K. K. Nagwanshi, C. Ardil
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1003.1803 by
  other authors without attribution
- **Journal**: World Academy of Science, Engineering and Technology, 68, 352-359
  (2010)
- **Summary**: This paper proposes a new technique based on a non-linear Minmax Detector Based (MDB) filter for image restoration. The aim of image enhancement is to reconstruct the true image from the corrupted image. The process of image acquisition frequently leads to degradation and the quality of the digitized image becomes inferior to the original image. Image degradation can be due to the addition of different types of noise in the original image. Image noise can be modelled of many types and impulse noise is one of them. Impulse noise generates pixels with gray value not consistent with their local neighbourhood. It appears as a sprinkle of both light and dark or only light spots in the image. Filtering is a technique for enhancing the image. Linear filter is the filtering in which the value of an output pixel is a linear combination of neighborhood values, which can produce blur in the image. Thus a variety of smoothing techniques have been developed that are non linear. Median filter is the one of the most popular non-linear filter. When considering a small neighborhood it is highly efficient but for large window and in case of high noise it gives rise to more blurring to image. The Centre Weighted Mean (CWM) filter has got a better average performance over the median filter. However the original pixel corrupted and noise reduction is substantial under high noise condition. Hence this technique has also blurring affect on the image. To illustrate the superiority of the proposed approach, the proposed new scheme has been simulated along with the standard ones and various restored performance measures have been compared.



### Adaptive Non-linear Filtering Technique for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2204.09302v1
- **DOI**: None
- **Categories**: **cs.CV**, I.6, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2204.09302v1)
- **Published**: 2022-04-20 08:36:59+00:00
- **Updated**: 2022-04-20 08:36:59+00:00
- **Authors**: S. K. Satpathy, S. Panda, K. K. Nagwanshi, S. K. Nayak, C. Ardil
- **Comment**: Accepted. arXiv admin note: text overlap with arXiv:1003.1827 by
  other authors
- **Journal**: World Academy of Science, Engineering and Technology, 68, 352-359
  (2010)
- **Summary**: Removing noise from the any processed images is very important. Noise should be removed in such a way that important information of image should be preserved. A decisionbased nonlinear algorithm for elimination of band lines, drop lines, mark, band lost and impulses in images is presented in this paper. The algorithm performs two simultaneous operations, namely, detection of corrupted pixels and evaluation of new pixels for replacing the corrupted pixels. Removal of these artifacts is achieved without damaging edges and details. However, the restricted window size renders median operation less effective whenever noise is excessive in that case the proposed algorithm automatically switches to mean filtering. The performance of the algorithm is analyzed in terms of Mean Square Error [MSE], Peak-Signal-to-Noise Ratio [PSNR], Signal-to-Noise Ratio Improved [SNRI], Percentage Of Noise Attenuated [PONA], and Percentage Of Spoiled Pixels [POSP]. This is compared with standard algorithms already in use and improved performance of the proposed algorithm is presented. The advantage of the proposed algorithm is that a single algorithm can replace several independent algorithms which are required for removal of different artifacts.



### Attention in Attention: Modeling Context Correlation for Efficient Video Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.09303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09303v1)
- **Published**: 2022-04-20 08:37:52+00:00
- **Updated**: 2022-04-20 08:37:52+00:00
- **Authors**: Yanbin Hao, Shuo Wang, Pei Cao, Xinjian Gao, Tong Xu, Jinmeng Wu, Xiangnan He
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Attention mechanisms have significantly boosted the performance of video classification neural networks thanks to the utilization of perspective contexts. However, the current research on video attention generally focuses on adopting a specific aspect of contexts (e.g., channel, spatial/temporal, or global context) to refine the features and neglects their underlying correlation when computing attentions. This leads to incomplete context utilization and hence bears the weakness of limited performance improvement. To tackle the problem, this paper proposes an efficient attention-in-attention (AIA) method for element-wise feature refinement, which investigates the feasibility of inserting the channel context into the spatio-temporal attention learning module, referred to as CinST, and also its reverse variant, referred to as STinC. Specifically, we instantiate the video feature contexts as dynamics aggregated along a specific axis with global average and max pooling operations. The workflow of an AIA module is that the first attention block uses one kind of context information to guide the gating weights calculation of the second attention that targets at the other context. Moreover, all the computational operations in attention units act on the pooled dimension, which results in quite few computational cost increase ($<$0.02\%). To verify our method, we densely integrate it into two classical video network backbones and conduct extensive experiments on several standard video classification benchmarks. The source code of our AIA is available at \url{https://github.com/haoyanbin918/Attention-in-Attention}.



### A Deeper Look into Aleatoric and Epistemic Uncertainty Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2204.09308v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09308v1)
- **Published**: 2022-04-20 08:41:37+00:00
- **Updated**: 2022-04-20 08:41:37+00:00
- **Authors**: Matias Valdenegro-Toro, Daniel Saromo
- **Comment**: 8 pages, 12 figures, with supplementary. LatinX in CV Workshop @ CVPR
  2022 Camera Ready
- **Journal**: None
- **Summary**: Neural networks are ubiquitous in many tasks, but trusting their predictions is an open issue. Uncertainty quantification is required for many applications, and disentangled aleatoric and epistemic uncertainties are best. In this paper, we generalize methods to produce disentangled uncertainties to work with different uncertainty quantification methods, and evaluate their capability to produce disentangled uncertainties. Our results show that: there is an interaction between learning aleatoric and epistemic uncertainty, which is unexpected and violates assumptions on aleatoric uncertainty, some methods like Flipout produce zero epistemic uncertainty, aleatoric uncertainty is unreliable in the out-of-distribution setting, and Ensembles provide overall the best disentangling quality. We also explore the error produced by the number of samples hyper-parameter in the sampling softmax function, recommending N > 100 samples. We expect that our formulation and results help practitioners and researchers choose uncertainty methods and expand the use of disentangled uncertainties, as well as motivate additional research into this topic.



### NTIRE 2022 Challenge on Super-Resolution and Quality Enhancement of Compressed Video: Dataset, Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2204.09314v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09314v2)
- **Published**: 2022-04-20 08:50:02+00:00
- **Updated**: 2022-04-25 07:59:48+00:00
- **Authors**: Ren Yang, Radu Timofte, Meisong Zheng, Qunliang Xing, Minglang Qiao, Mai Xu, Lai Jiang, Huaida Liu, Ying Chen, Youcheng Ben, Xiao Zhou, Chen Fu, Pei Cheng, Gang Yu, Junyi Li, Renlong Wu, Zhilu Zhang, Wei Shang, Zhengyao Lv, Yunjin Chen, Mingcai Zhou, Dongwei Ren, Kai Zhang, Wangmeng Zuo, Pavel Ostyakov, Vyal Dmitry, Shakarim Soltanayev, Chervontsev Sergey, Zhussip Magauiya, Xueyi Zou, Youliang Yan, Pablo Navarrete Michelini, Yunhua Lu, Diankai Zhang, Shaoli Liu, Si Gao, Biao Wu, Chengjian Zheng, Xiaofeng Zhang, Kaidi Lu, Ning Wang, Thuong Nguyen Canh, Thong Bach, Qing Wang, Xiaopeng Sun, Haoyu Ma, Shijie Zhao, Junlin Li, Liangbin Xie, Shuwei Shi, Yujiu Yang, Xintao Wang, Jinjin Gu, Chao Dong, Xiaodi Shi, Chunmei Nian, Dong Jiang, Jucai Lin, Zhihuai Xie, Mao Ye, Dengyan Luo, Liuhan Peng, Shengjie Chen, Xin Liu, Qian Wang, Xin Liu, Boyang Liang, Hang Dong, Yuhao Huang, Kai Chen, Xingbei Guo, Yujing Sun, Huilei Wu, Pengxu Wei, Yulin Huang, Junying Chen, Ik Hyun Lee, Sunder Ali Khowaja, Jiseok Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: This paper reviews the NTIRE 2022 Challenge on Super-Resolution and Quality Enhancement of Compressed Video. In this challenge, we proposed the LDV 2.0 dataset, which includes the LDV dataset (240 videos) and 95 additional videos. This challenge includes three tracks. Track 1 aims at enhancing the videos compressed by HEVC at a fixed QP. Track 2 and Track 3 target both the super-resolution and quality enhancement of HEVC compressed video. They require x2 and x4 super-resolution, respectively. The three tracks totally attract more than 600 registrations. In the test phase, 8 teams, 8 teams and 12 teams submitted the final results to Tracks 1, 2 and 3, respectively. The proposed methods and solutions gauge the state-of-the-art of super-resolution and quality enhancement of compressed video. The proposed LDV 2.0 dataset is available at https://github.com/RenYang-home/LDV_dataset. The homepage of this challenge (including open-sourced codes) is at https://github.com/RenYang-home/NTIRE22_VEnh_SR.



### Logarithmic Morphological Neural Nets robust to lighting variations
- **Arxiv ID**: http://arxiv.org/abs/2204.09319v2
- **DOI**: 10.1007/978-3-031-19897-7_36
- **Categories**: **cs.CV**, cs.NA, eess.SP, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2204.09319v2)
- **Published**: 2022-04-20 08:54:49+00:00
- **Updated**: 2022-11-29 09:39:21+00:00
- **Authors**: Guillaume Noyel, Emile Barbier--Renard, Michel Jourlin, Thierry Fournel
- **Comment**: None
- **Journal**: Discrete Geometry and Mathematical Morphology. DGMM 2022., 13493,
  Springer, 2022, Lecture Notes in Computer Science
- **Summary**: Morphological neural networks allow to learn the weights of a structuring function knowing the desired output image. However, those networks are not intrinsically robust to lighting variations in images with an optical cause, such as a change of light intensity. In this paper, we introduce a morphological neural network which possesses such a robustness to lighting variations. It is based on the recent framework of Logarithmic Mathematical Morphology (LMM), i.e. Mathematical Morphology defined with the Logarithmic Image Processing (LIP) model. This model has a LIP additive law which simulates in images a variation of the light intensity. We especially learn the structuring function of a LMM operator robust to those variations, namely : the map of LIP-additive Asplund distances. Results in images show that our neural network verifies the required property.



### SpiderNet: Hybrid Differentiable-Evolutionary Architecture Search via Train-Free Metrics
- **Arxiv ID**: http://arxiv.org/abs/2204.09320v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2204.09320v1)
- **Published**: 2022-04-20 08:55:01+00:00
- **Updated**: 2022-04-20 08:55:01+00:00
- **Authors**: Rob Geada, Andrew Stephen McGough
- **Comment**: Published in CVPR-NAS 2022
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) algorithms are intended to remove the burden of manual neural network design, and have shown to be capable of designing excellent models for a variety of well-known problems. However, these algorithms require a variety of design parameters in the form of user configuration or hard-coded decisions which limit the variety of networks that can be discovered. This means that NAS algorithms do not eliminate model design tuning, they instead merely shift the burden of where that tuning needs to be applied. In this paper, we present SpiderNet, a hybrid differentiable-evolutionary and hardware-aware algorithm that rapidly and efficiently produces state-of-the-art networks. More importantly, SpiderNet is a proof-of-concept of a minimally-configured NAS algorithm; the majority of design choices seen in other algorithms are incorporated into SpiderNet's dynamically-evolving search space, minimizing the number of user choices to just two: reduction cell count and initial channel count. SpiderNet produces models highly-competitive with the state-of-the-art, and outperforms random search in accuracy, runtime, memory size, and parameter count.



### Self-supervised Learning for Sonar Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.09323v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.09323v1)
- **Published**: 2022-04-20 08:58:35+00:00
- **Updated**: 2022-04-20 08:58:35+00:00
- **Authors**: Alan Preciado-Grijalva, Bilal Wehbe, Miguel Bande Firvida, Matias Valdenegro-Toro
- **Comment**: 8 pages, 10 figures, with supplementary. LatinX in CV Workshop @ CVPR
  2022 Camera Ready
- **Journal**: None
- **Summary**: Self-supervised learning has proved to be a powerful approach to learn image representations without the need of large labeled datasets. For underwater robotics, it is of great interest to design computer vision algorithms to improve perception capabilities such as sonar image classification. Due to the confidential nature of sonar imaging and the difficulty to interpret sonar images, it is challenging to create public large labeled sonar datasets to train supervised learning algorithms. In this work, we investigate the potential of three self-supervised learning methods (RotNet, Denoising Autoencoders, and Jigsaw) to learn high-quality sonar image representation without the need of human labels. We present pre-training and transfer learning results on real-life sonar image datasets. Our results indicate that self-supervised pre-training yields classification performance comparable to supervised pre-training in a few-shot transfer learning setup across all three methods. Code and self-supervised pre-trained models are be available at https://github.com/agrija9/ssl-sonar-images



### NFormer: Robust Person Re-identification with Neighbor Transformer
- **Arxiv ID**: http://arxiv.org/abs/2204.09331v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.09331v1)
- **Published**: 2022-04-20 09:06:47+00:00
- **Updated**: 2022-04-20 09:06:47+00:00
- **Authors**: Haochen Wang, Jiayi Shen, Yongtuo Liu, Yan Gao, Efstratios Gavves
- **Comment**: 8 pages, 7 figures, CVPR2022 poster
- **Journal**: None
- **Summary**: Person re-identification aims to retrieve persons in highly varying settings across different cameras and scenarios, in which robust and discriminative representation learning is crucial. Most research considers learning representations from single images, ignoring any potential interactions between them. However, due to the high intra-identity variations, ignoring such interactions typically leads to outlier features. To tackle this issue, we propose a Neighbor Transformer Network, or NFormer, which explicitly models interactions across all input images, thus suppressing outlier features and leading to more robust representations overall. As modelling interactions between enormous amount of images is a massive task with lots of distractors, NFormer introduces two novel modules, the Landmark Agent Attention, and the Reciprocal Neighbor Softmax. Specifically, the Landmark Agent Attention efficiently models the relation map between images by a low-rank factorization with a few landmarks in feature space. Moreover, the Reciprocal Neighbor Softmax achieves sparse attention to relevant -- rather than all -- neighbors only, which alleviates interference of irrelevant representations and further relieves the computational burden. In experiments on four large-scale datasets, NFormer achieves a new state-of-the-art. The code is released at \url{https://github.com/haochenheheda/NFormer}.



### Unsupervised Domain Adaptation for Cardiac Segmentation: Towards Structure Mutual Information Maximization
- **Arxiv ID**: http://arxiv.org/abs/2204.09334v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09334v3)
- **Published**: 2022-04-20 09:10:18+00:00
- **Updated**: 2023-05-27 15:42:45+00:00
- **Authors**: Changjie Lu, Shen Zheng, Gaurav Gupta
- **Comment**: CVPR Workshop Paper v3: Fix some description errors
- **Journal**: None
- **Summary**: Unsupervised domain adaptation approaches have recently succeeded in various medical image segmentation tasks. The reported works often tackle the domain shift problem by aligning the domain-invariant features and minimizing the domain-specific discrepancies. That strategy works well when the difference between a specific domain and between different domains is slight. However, the generalization ability of these models on diverse imaging modalities remains a significant challenge. This paper introduces UDA-VAE++, an unsupervised domain adaptation framework for cardiac segmentation with a compact loss function lower bound. To estimate this new lower bound, we develop a novel Structure Mutual Information Estimation (SMIE) block with a global estimator, a local estimator, and a prior information matching estimator to maximize the mutual information between the reconstruction and segmentation tasks. Specifically, we design a novel sequential reparameterization scheme that enables information flow and variance correction from the low-resolution latent space to the high-resolution latent space. Comprehensive experiments on benchmark cardiac segmentation datasets demonstrate that our model outperforms previous state-of-the-art qualitatively and quantitatively. The code is available at https://github.com/LOUEY233/Toward-Mutual-Information}{https://github.com/LOUEY233/Toward-Mutual-Information



### Sequential Point Clouds: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2204.09337v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09337v2)
- **Published**: 2022-04-20 09:14:20+00:00
- **Updated**: 2022-04-21 02:10:05+00:00
- **Authors**: Haiyan Wang, Yingli Tian
- **Comment**: Survey paper on sequential point clouds
- **Journal**: None
- **Summary**: Point cloud has drawn more and more research attention as well as real-world applications. However, many of these applications (e.g. autonomous driving and robotic manipulation) are actually based on sequential point clouds (i.e. four dimensions) because the information of the static point cloud data could provide is still limited. Recently, researchers put more and more effort into sequential point clouds. This paper presents an extensive review of the deep learning-based methods for sequential point cloud research including dynamic flow estimation, object detection \& tracking, point cloud segmentation, and point cloud forecasting. This paper further summarizes and compares the quantitative results of the reviewed methods over the public benchmark datasets. Finally, this paper is concluded by discussing the challenges in the current sequential point cloud research and pointing out insightful potential future research directions.



### OutCast: Outdoor Single-image Relighting with Cast Shadows
- **Arxiv ID**: http://arxiv.org/abs/2204.09341v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.09341v1)
- **Published**: 2022-04-20 09:24:14+00:00
- **Updated**: 2022-04-20 09:24:14+00:00
- **Authors**: David Griffiths, Tobias Ritschel, Julien Philip
- **Comment**: Eurographics 2022 - Accepted
- **Journal**: None
- **Summary**: We propose a relighting method for outdoor images. Our method mainly focuses on predicting cast shadows in arbitrary novel lighting directions from a single image while also accounting for shading and global effects such the sun light color and clouds. Previous solutions for this problem rely on reconstructing occluder geometry, e.g. using multi-view stereo, which requires many images of the scene. Instead, in this work we make use of a noisy off-the-shelf single-image depth map estimation as a source of geometry. Whilst this can be a good guide for some lighting effects, the resulting depth map quality is insufficient for directly ray-tracing the shadows. Addressing this, we propose a learned image space ray-marching layer that converts the approximate depth map into a deep 3D representation that is fused into occlusion queries using a learned traversal. Our proposed method achieves, for the first time, state-of-the-art relighting results, with only a single image as input. For supplementary material visit our project page at: https://dgriffiths.uk/outcast.



### Utilizing unsupervised learning to improve sward content prediction and herbage mass estimation
- **Arxiv ID**: http://arxiv.org/abs/2204.09343v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09343v1)
- **Published**: 2022-04-20 09:28:11+00:00
- **Updated**: 2022-04-20 09:28:11+00:00
- **Authors**: Paul Albert, Mohamed Saadeldin, Badri Narayanan, Brian Mac Namee, Deirdre Hennessy, Aisling H. O'Connor, Noel E. O'Connor, Kevin McGuinness
- **Comment**: 3 pages. Accepted at the 29th EGF General Meeting 2022
- **Journal**: None
- **Summary**: Sward species composition estimation is a tedious one. Herbage must be collected in the field, manually separated into components, dried and weighed to estimate species composition. Deep learning approaches using neural networks have been used in previous work to propose faster and more cost efficient alternatives to this process by estimating the biomass information from a picture of an area of pasture alone. Deep learning approaches have, however, struggled to generalize to distant geographical locations and necessitated further data collection to retrain and perform optimally in different climates. In this work, we enhance the deep learning solution by reducing the need for ground-truthed (GT) images when training the neural network. We demonstrate how unsupervised contrastive learning can be used in the sward composition prediction problem and compare with the state-of-the-art on the publicly available GrassClover dataset collected in Denmark as well as a more recent dataset from Ireland where we tackle herbage mass and height estimation.



### Cyber-Forensic Review of Human Footprint and Gait for Personal Identification
- **Arxiv ID**: http://arxiv.org/abs/2204.09344v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2204.09344v1)
- **Published**: 2022-04-20 09:29:43+00:00
- **Updated**: 2022-04-20 09:29:43+00:00
- **Authors**: Kapil Kumar Nagwanshi
- **Comment**: None
- **Journal**: IAENG International Journal of Computer Science, vol. 46, no.4,
  pp645-661, 2019
- **Summary**: The human footprint is having a unique set of ridges unmatched by any other human being, and therefore it can be used in different identity documents for example birth certificate, Indian biometric identification system AADHAR card, driving license, PAN card, and passport. There are many instances of the crime scene where an accused must walk around and left the footwear impressions as well as barefoot prints and therefore, it is very crucial to recovering the footprints from identifying the criminals. Footprint-based biometric is a considerably newer technique for personal identification. Fingerprints, retina, iris and face recognition are the methods most useful for attendance record of the person. This time the world is facing the problem of global terrorism. It is challenging to identify the terrorist because they are living as regular as the citizens do. Their soft target includes the industries of special interests such as defence, silicon and nanotechnology chip manufacturing units, pharmacy sectors. They pretend themselves as religious persons, so temples and other holy places, even in markets is in their targets. These are the places where one can obtain their footprints quickly. The gait itself is sufficient to predict the behaviour of the suspects. The present research is driven to identify the usefulness of footprint and gait as an alternative to personal identification.



### Attentive Dual Stream Siamese U-net for Flood Detection on Multi-temporal Sentinel-1 Data
- **Arxiv ID**: http://arxiv.org/abs/2204.09387v1
- **DOI**: 10.1109/IGARSS46834.2022.9883132
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09387v1)
- **Published**: 2022-04-20 10:56:39+00:00
- **Updated**: 2022-04-20 10:56:39+00:00
- **Authors**: Ritu Yadav, Andrea Nascetti, Yifang Ban
- **Comment**: Accepted in IGARSS2022
- **Journal**: None
- **Summary**: Due to climate and land-use change, natural disasters such as flooding have been increasing in recent years. Timely and reliable flood detection and mapping can help emergency response and disaster management. In this work, we propose a flood detection network using bi-temporal SAR acquisitions. The proposed segmentation network has an encoder-decoder architecture with two Siamese encoders for pre and post-flood images. The network's feature maps are fused and enhanced using attention blocks to achieve more accurate detection of the flooded areas. Our proposed network is evaluated on publicly available Sen1Flood11 benchmark dataset. The network outperformed the existing state-of-the-art (uni-temporal) flood detection method by 6\% IOU. The experiments highlight that the combination of bi-temporal SAR data with an effective network architecture achieves more accurate flood detection than uni-temporal methods.



### Epistemic Uncertainty-Weighted Loss for Visual Bias Mitigation
- **Arxiv ID**: http://arxiv.org/abs/2204.09389v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.09389v1)
- **Published**: 2022-04-20 11:01:51+00:00
- **Updated**: 2022-04-20 11:01:51+00:00
- **Authors**: Rebecca S Stone, Nishant Ravikumar, Andrew J Bulpitt, David C Hogg
- **Comment**: To be published in 2022 IEEE CVPR Workshop on Fair, Data Efficient
  and Trusted Computer Vision
- **Journal**: None
- **Summary**: Deep neural networks are highly susceptible to learning biases in visual data. While various methods have been proposed to mitigate such bias, the majority require explicit knowledge of the biases present in the training data in order to mitigate. We argue the relevance of exploring methods which are completely ignorant of the presence of any bias, but are capable of identifying and mitigating them. Furthermore, we propose using Bayesian neural networks with an epistemic uncertainty-weighted loss function to dynamically identify potential bias in individual training samples and to weight them during training. We find a positive correlation between samples subject to bias and higher epistemic uncertainties. Finally, we show the method has potential to mitigate visual bias on a bias benchmark dataset and on a real-world face detection problem, and we consider the merits and weaknesses of our approach.



### Adversarial Scratches: Deployable Attacks to CNN Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2204.09397v3
- **DOI**: 10.1016/j.patcog.2022.108985
- **Categories**: **cs.LG**, cs.CR, cs.CV, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2204.09397v3)
- **Published**: 2022-04-20 11:42:24+00:00
- **Updated**: 2023-05-18 07:55:02+00:00
- **Authors**: Loris Giulivi, Malhar Jere, Loris Rossi, Farinaz Koushanfar, Gabriela Ciocarlie, Briland Hitaj, Giacomo Boracchi
- **Comment**: This work is published at Pattern Recognition (Elsevier). This paper
  stems from 'Scratch that! An Evolution-based Adversarial Attack against
  Neural Networks' for which an arXiv preprint is available at
  arXiv:1912.02316. Further studies led to a complete overhaul of the work,
  resulting in this paper
- **Journal**: Pattern Recognition, Volume 133, January 2023, 108985
- **Summary**: A growing body of work has shown that deep neural networks are susceptible to adversarial examples. These take the form of small perturbations applied to the model's input which lead to incorrect predictions. Unfortunately, most literature focuses on visually imperceivable perturbations to be applied to digital images that often are, by design, impossible to be deployed to physical targets. We present Adversarial Scratches: a novel L0 black-box attack, which takes the form of scratches in images, and which possesses much greater deployability than other state-of-the-art attacks. Adversarial Scratches leverage B\'ezier Curves to reduce the dimension of the search space and possibly constrain the attack to a specific location. We test Adversarial Scratches in several scenarios, including a publicly available API and images of traffic signs. Results show that, often, our attack achieves higher fooling rate than other deployable state-of-the-art methods, while requiring significantly fewer queries and modifying very few pixels.



### Case-Aware Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2204.09398v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09398v2)
- **Published**: 2022-04-20 11:43:58+00:00
- **Updated**: 2023-08-30 08:18:15+00:00
- **Authors**: Mingyuan Fan, Yang Liu, Cen Chen
- **Comment**: None
- **Journal**: None
- **Summary**: The neural network (NN) becomes one of the most heated type of models in various signal processing applications. However, NNs are extremely vulnerable to adversarial examples (AEs). To defend AEs, adversarial training (AT) is believed to be the most effective method while due to the intensive computation, AT is limited to be applied in most applications. In this paper, to resolve the problem, we design a generic and efficient AT improvement scheme, namely case-aware adversarial training (CAT). Specifically, the intuition stems from the fact that a very limited part of informative samples can contribute to most of model performance. Alternatively, if only the most informative AEs are used in AT, we can lower the computation complexity of AT significantly as maintaining the defense effect. To achieve this, CAT achieves two breakthroughs. First, a method to estimate the information degree of adversarial examples is proposed for AE filtering. Second, to further enrich the information that the NN can obtain from AEs, CAT involves a weight estimation and class-level balancing based sampling strategy to increase the diversity of AT at each iteration. Extensive experiments show that CAT is faster than vanilla AT by up to 3x while achieving competitive defense effect.



### A Probabilistic Time-Evolving Approach to Scanpath Prediction
- **Arxiv ID**: http://arxiv.org/abs/2204.09404v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.09404v1)
- **Published**: 2022-04-20 11:50:29+00:00
- **Updated**: 2022-04-20 11:50:29+00:00
- **Authors**: Daniel Martin, Diego Gutierrez, Belen Masia
- **Comment**: Under submission
- **Journal**: None
- **Summary**: Human visual attention is a complex phenomenon that has been studied for decades. Within it, the particular problem of scanpath prediction poses a challenge, particularly due to the inter- and intra-observer variability, among other reasons. Besides, most existing approaches to scanpath prediction have focused on optimizing the prediction of a gaze point given the previous ones. In this work, we present a probabilistic time-evolving approach to scanpath prediction, based on Bayesian deep learning. We optimize our model using a novel spatio-temporal loss function based on a combination of Kullback-Leibler divergence and dynamic time warping, jointly considering the spatial and temporal dimensions of scanpaths. Our scanpath prediction framework yields results that outperform those of current state-of-the-art approaches, and are almost on par with the human baseline, suggesting that our model is able to generate scanpaths whose behavior closely resembles those of the real ones.



### Video Moment Retrieval from Text Queries via Single Frame Annotation
- **Arxiv ID**: http://arxiv.org/abs/2204.09409v3
- **DOI**: 10.1145/3477495.3532078
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09409v3)
- **Published**: 2022-04-20 11:59:17+00:00
- **Updated**: 2022-06-18 12:56:41+00:00
- **Authors**: Ran Cui, Tianwen Qian, Pai Peng, Elena Daskalaki, Jingjing Chen, Xiaowei Guo, Huyang Sun, Yu-Gang Jiang
- **Comment**: Accepted as full paper in SIGIR 2022
- **Journal**: None
- **Summary**: Video moment retrieval aims at finding the start and end timestamps of a moment (part of a video) described by a given natural language query. Fully supervised methods need complete temporal boundary annotations to achieve promising results, which is costly since the annotator needs to watch the whole moment. Weakly supervised methods only rely on the paired video and query, but the performance is relatively poor. In this paper, we look closer into the annotation process and propose a new paradigm called "glance annotation". This paradigm requires the timestamp of only one single random frame, which we refer to as a "glance", within the temporal boundary of the fully supervised counterpart. We argue this is beneficial because comparing to weak supervision, trivial cost is added yet more potential in performance is provided. Under the glance annotation setting, we propose a method named as Video moment retrieval via Glance Annotation (ViGA) based on contrastive learning. ViGA cuts the input video into clips and contrasts between clips and queries, in which glance guided Gaussian distributed weights are assigned to all clips. Our extensive experiments indicate that ViGA achieves better results than the state-of-the-art weakly supervised methods by a large margin, even comparable to fully supervised methods in some cases.



### HRPose: Real-Time High-Resolution 6D Pose Estimation Network Using Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2204.09429v1
- **DOI**: 10.1049/cje.2021.00.211
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.09429v1)
- **Published**: 2022-04-20 12:43:39+00:00
- **Updated**: 2022-04-20 12:43:39+00:00
- **Authors**: Qi Guan, Zihao Sheng, Shibei Xue
- **Comment**: 8 pages, 4 figures, and 5 tables, accepted by Chinese Journal of
  Electronics
- **Journal**: None
- **Summary**: Real-time 6D object pose estimation is essential for many real-world applications, such as robotic grasping and augmented reality. To achieve an accurate object pose estimation from RGB images in real-time, we propose an effective and lightweight model, namely High-Resolution 6D Pose Estimation Network (HRPose). We adopt the efficient and small HRNetV2-W18 as a feature extractor to reduce computational burdens while generating accurate 6D poses. With only 33\% of the model size and lower computational costs, our HRPose achieves comparable performance compared with state-of-the-art models. Moreover, by transferring knowledge from a large model to our proposed HRPose through output and feature-similarity distillations, the performance of our HRPose is improved in effectiveness and efficiency. Numerical experiments on the widely-used benchmark LINEMOD demonstrate the superiority of our proposed HRPose against state-of-the-art methods.



### A Mobile Food Recognition System for Dietary Assessment
- **Arxiv ID**: http://arxiv.org/abs/2204.09432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09432v1)
- **Published**: 2022-04-20 12:49:36+00:00
- **Updated**: 2022-04-20 12:49:36+00:00
- **Authors**: Şeymanur Aktı, Marwa Qaraqe, Hazım Kemal Ekenel
- **Comment**: Accepted at GoodBrotherVI4IAAL Workshop @ICIAP2021
- **Journal**: None
- **Summary**: Food recognition is an important task for a variety of applications, including managing health conditions and assisting visually impaired people. Several food recognition studies have focused on generic types of food or specific cuisines, however, food recognition with respect to Middle Eastern cuisines has remained unexplored. Therefore, in this paper we focus on developing a mobile friendly, Middle Eastern cuisine focused food recognition application for assisted living purposes. In order to enable a low-latency, high-accuracy food classification system, we opted to utilize the Mobilenet-v2 deep learning model. As some of the foods are more popular than the others, the number of samples per class in the used Middle Eastern food dataset is relatively imbalanced. To compensate for this problem, data augmentation methods are applied on the underrepresented classes. Experimental results show that using Mobilenet-v2 architecture for this task is beneficial in terms of both accuracy and the memory usage. With the model achieving 94% accuracy on 23 food classes, the developed mobile application has potential to serve the visually impaired in automatic food recognition via images.



### PP-Matting: High-Accuracy Natural Image Matting
- **Arxiv ID**: http://arxiv.org/abs/2204.09433v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.09433v1)
- **Published**: 2022-04-20 12:54:06+00:00
- **Updated**: 2022-04-20 12:54:06+00:00
- **Authors**: Guowei Chen, Yi Liu, Jian Wang, Juncai Peng, Yuying Hao, Lutao Chu, Shiyu Tang, Zewu Wu, Zeyu Chen, Zhiliang Yu, Yuning Du, Qingqing Dang, Xiaoguang Hu, Dianhai Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Natural image matting is a fundamental and challenging computer vision task. It has many applications in image editing and composition. Recently, deep learning-based approaches have achieved great improvements in image matting. However, most of them require a user-supplied trimap as an auxiliary input, which limits the matting applications in the real world. Although some trimap-free approaches have been proposed, the matting quality is still unsatisfactory compared to trimap-based ones. Without the trimap guidance, the matting models suffer from foreground-background ambiguity easily, and also generate blurry details in the transition area. In this work, we propose PP-Matting, a trimap-free architecture that can achieve high-accuracy natural image matting. Our method applies a high-resolution detail branch (HRDB) that extracts fine-grained details of the foreground with keeping feature resolution unchanged. Also, we propose a semantic context branch (SCB) that adopts a semantic segmentation subtask. It prevents the detail prediction from local ambiguity caused by semantic context missing. In addition, we conduct extensive experiments on two well-known benchmarks: Composition-1k and Distinctions-646. The results demonstrate the superiority of PP-Matting over previous methods. Furthermore, we provide a qualitative evaluation of our method on human matting which shows its outstanding performance in the practical application. The code and pre-trained models will be available at PaddleSeg: https://github.com/PaddlePaddle/PaddleSeg.



### FenceNet: Fine-grained Footwork Recognition in Fencing
- **Arxiv ID**: http://arxiv.org/abs/2204.09434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09434v1)
- **Published**: 2022-04-20 12:54:31+00:00
- **Updated**: 2022-04-20 12:54:31+00:00
- **Authors**: Kevin Zhu, Alexander Wong, John McPhee
- **Comment**: None
- **Journal**: None
- **Summary**: Current data analysis for the Canadian Olympic fencing team is primarily done manually by coaches and analysts. Due to the highly repetitive, yet dynamic and subtle movements in fencing, manual data analysis can be inefficient and inaccurate. We propose FenceNet as a novel architecture to automate the classification of fine-grained footwork techniques in fencing. FenceNet takes 2D pose data as input and classifies actions using a skeleton-based action recognition approach that incorporates temporal convolutional networks to capture temporal information. We train and evaluate FenceNet on the Fencing Footwork Dataset (FFD), which contains 10 fencers performing 6 different footwork actions for 10-11 repetitions each (652 total videos). FenceNet achieves 85.4% accuracy under 10-fold cross-validation, where each fencer is left out as the test set. This accuracy is within 1% of the current state-of-the-art method, JLJA (86.3%), which selects and fuses features engineered from skeleton data, depth videos, and inertial measurement units. BiFenceNet, a variant of FenceNet that captures the "bidirectionality" of human movement through two separate networks, achieves 87.6% accuracy, outperforming JLJA. Since neither FenceNet nor BiFenceNet requires data from wearable sensors, unlike JLJA, they could be directly applied to most fencing videos, using 2D pose data as input extracted from off-the-shelf 2D human pose estimators. In comparison to JLJA, our methods are also simpler as they do not require manual feature engineering, selection, or fusion.



### Hephaestus: A large scale multitask dataset towards InSAR understanding
- **Arxiv ID**: http://arxiv.org/abs/2204.09435v1
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.IM, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.09435v1)
- **Published**: 2022-04-20 12:58:18+00:00
- **Updated**: 2022-04-20 12:58:18+00:00
- **Authors**: Nikolaos Ioannis Bountos, Ioannis Papoutsis, Dimitrios Michail, Andreas Karavias, Panagiotis Elias, Isaak Parcharidis
- **Comment**: This work has been accepted for publication in EARTHVISION 2022, in
  conjuction with the Computer Vision and Pattern Recognition (CVPR) 2022
  Conference
- **Journal**: None
- **Summary**: Synthetic Aperture Radar (SAR) data and Interferometric SAR (InSAR) products in particular, are one of the largest sources of Earth Observation data. InSAR provides unique information on diverse geophysical processes and geology, and on the geotechnical properties of man-made structures. However, there are only a limited number of applications that exploit the abundance of InSAR data and deep learning methods to extract such knowledge. The main barrier has been the lack of a large curated and annotated InSAR dataset, which would be costly to create and would require an interdisciplinary team of experts experienced on InSAR data interpretation. In this work, we put the effort to create and make available the first of its kind, manually annotated dataset that consists of 19,919 individual Sentinel-1 interferograms acquired over 44 different volcanoes globally, which are split into 216,106 InSAR patches. The annotated dataset is designed to address different computer vision problems, including volcano state classification, semantic segmentation of ground deformation, detection and classification of atmospheric signals in InSAR imagery, interferogram captioning, text to InSAR generation, and InSAR image quality assessment.



### DAM-GAN : Image Inpainting using Dynamic Attention Map based on Fake Texture Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.09442v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09442v1)
- **Published**: 2022-04-20 13:15:52+00:00
- **Updated**: 2022-04-20 13:15:52+00:00
- **Authors**: Dongmin Cha, Daijin Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural advancements have recently brought remarkable image synthesis performance to the field of image inpainting. The adaptation of generative adversarial networks (GAN) in particular has accelerated significant progress in high-quality image reconstruction. However, although many notable GAN-based networks have been proposed for image inpainting, still pixel artifacts or color inconsistency occur in synthesized images during the generation process, which are usually called fake textures. To reduce pixel inconsistency disorder resulted from fake textures, we introduce a GAN-based model using dynamic attention map (DAM-GAN). Our proposed DAM-GAN concentrates on detecting fake texture and products dynamic attention maps to diminish pixel inconsistency from the feature maps in the generator. Evaluation results on CelebA-HQ and Places2 datasets with other image inpainting approaches show the superiority of our network.



### GIMO: Gaze-Informed Human Motion Prediction in Context
- **Arxiv ID**: http://arxiv.org/abs/2204.09443v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09443v2)
- **Published**: 2022-04-20 13:17:39+00:00
- **Updated**: 2022-07-19 16:01:02+00:00
- **Authors**: Yang Zheng, Yanchao Yang, Kaichun Mo, Jiaman Li, Tao Yu, Yebin Liu, C. Karen Liu, Leonidas J. Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting human motion is critical for assistive robots and AR/VR applications, where the interaction with humans needs to be safe and comfortable. Meanwhile, an accurate prediction depends on understanding both the scene context and human intentions. Even though many works study scene-aware human motion prediction, the latter is largely underexplored due to the lack of ego-centric views that disclose human intent and the limited diversity in motion and scenes. To reduce the gap, we propose a large-scale human motion dataset that delivers high-quality body pose sequences, scene scans, as well as ego-centric views with the eye gaze that serves as a surrogate for inferring human intent. By employing inertial sensors for motion capture, our data collection is not tied to specific scenes, which further boosts the motion dynamics observed from our subjects. We perform an extensive study of the benefits of leveraging the eye gaze for ego-centric human motion prediction with various state-of-the-art architectures. Moreover, to realize the full potential of the gaze, we propose a novel network architecture that enables bidirectional communication between the gaze and motion branches. Our network achieves the top performance in human motion prediction on the proposed dataset, thanks to the intent information from eye gaze and the denoised gaze feature modulated by the motion. Code and data can be found at https://github.com/y-zheng18/GIMO.



### STAU: A SpatioTemporal-Aware Unit for Video Prediction and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2204.09456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09456v1)
- **Published**: 2022-04-20 13:42:51+00:00
- **Updated**: 2022-04-20 13:42:51+00:00
- **Authors**: Zheng Chang, Xinfeng Zhang, Shanshe Wang, Siwei Ma, Wen Gao
- **Comment**: This work has been submitted to TPAMI
- **Journal**: None
- **Summary**: Video prediction aims to predict future frames by modeling the complex spatiotemporal dynamics in videos. However, most of the existing methods only model the temporal information and the spatial information for videos in an independent manner but haven't fully explored the correlations between both terms. In this paper, we propose a SpatioTemporal-Aware Unit (STAU) for video prediction and beyond by exploring the significant spatiotemporal correlations in videos. On the one hand, the motion-aware attention weights are learned from the spatial states to help aggregate the temporal states in the temporal domain. On the other hand, the appearance-aware attention weights are learned from the temporal states to help aggregate the spatial states in the spatial domain. In this way, the temporal information and the spatial information can be greatly aware of each other in both domains, during which, the spatiotemporal receptive field can also be greatly broadened for more reliable spatiotemporal modeling. Experiments are not only conducted on traditional video prediction tasks but also other tasks beyond video prediction, including the early action recognition and object detection tasks. Experimental results show that our STAU can outperform other methods on all tasks in terms of performance and computation efficiency.



### THORN: Temporal Human-Object Relation Network for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.09468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09468v1)
- **Published**: 2022-04-20 14:00:24+00:00
- **Updated**: 2022-04-20 14:00:24+00:00
- **Authors**: Mohammed Guermal, Rui Dai, Francois Bremond
- **Comment**: None
- **Journal**: None
- **Summary**: Most action recognition models treat human activities as unitary events. However, human activities often follow a certain hierarchy. In fact, many human activities are compositional. Also, these actions are mostly human-object interactions. In this paper we propose to recognize human action by leveraging the set of interactions that define an action. In this work, we present an end-to-end network: THORN, that can leverage important human-object and object-object interactions to predict actions. This model is built on top of a 3D backbone network. The key components of our model are: 1) An object representation filter for modeling object. 2) An object relation reasoning module to capture object relations. 3) A classification layer to predict the action labels. To show the robustness of THORN, we evaluate it on EPIC-Kitchen55 and EGTEA Gaze+, two of the largest and most challenging first-person and human-object interaction datasets. THORN achieves state-of-the-art performance on both datasets.



### GazeOnce: Real-Time Multi-Person Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/2204.09480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09480v1)
- **Published**: 2022-04-20 14:21:47+00:00
- **Updated**: 2022-04-20 14:21:47+00:00
- **Authors**: Mingfang Zhang, Yunfei Liu, Feng Lu
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Appearance-based gaze estimation aims to predict the 3D eye gaze direction from a single image. While recent deep learning-based approaches have demonstrated excellent performance, they usually assume one calibrated face in each input image and cannot output multi-person gaze in real time. However, simultaneous gaze estimation for multiple people in the wild is necessary for real-world applications. In this paper, we propose the first one-stage end-to-end gaze estimation method, GazeOnce, which is capable of simultaneously predicting gaze directions for multiple faces (>10) in an image. In addition, we design a sophisticated data generation pipeline and propose a new dataset, MPSGaze, which contains full images of multiple people with 3D gaze ground truth. Experimental results demonstrate that our unified framework not only offers a faster speed, but also provides a lower gaze estimation error compared with state-of-the-art methods. This technique can be useful in real-time applications with multiple users.



### Fetal Brain Tissue Annotation and Segmentation Challenge Results
- **Arxiv ID**: http://arxiv.org/abs/2204.09573v1
- **DOI**: 10.1016/j.media.2023.102833
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09573v1)
- **Published**: 2022-04-20 16:14:43+00:00
- **Updated**: 2022-04-20 16:14:43+00:00
- **Authors**: Kelly Payette, Hongwei Li, Priscille de Dumast, Roxane Licandro, Hui Ji, Md Mahfuzur Rahman Siddiquee, Daguang Xu, Andriy Myronenko, Hao Liu, Yuchen Pei, Lisheng Wang, Ying Peng, Juanying Xie, Huiquan Zhang, Guiming Dong, Hao Fu, Guotai Wang, ZunHyan Rieu, Donghyeon Kim, Hyun Gi Kim, Davood Karimi, Ali Gholipour, Helena R. Torres, Bruno Oliveira, João L. Vilaça, Yang Lin, Netanell Avisdris, Ori Ben-Zvi, Dafna Ben Bashat, Lucas Fidon, Michael Aertsen, Tom Vercauteren, Daniel Sobotka, Georg Langs, Mireia Alenyà, Maria Inmaculada Villanueva, Oscar Camara, Bella Specktor Fadida, Leo Joskowicz, Liao Weibin, Lv Yi, Li Xuesong, Moona Mazher, Abdul Qayyum, Domenec Puig, Hamza Kebiri, Zelin Zhang, Xinyi Xu, Dan Wu, KuanLun Liao, YiXuan Wu, JinTai Chen, Yunzhi Xu, Li Zhao, Lana Vasung, Bjoern Menze, Meritxell Bach Cuadra, Andras Jakab
- **Comment**: Results from FeTA Challenge 2021, held at MICCAI; Manuscript
  submitted
- **Journal**: None
- **Summary**: In-utero fetal MRI is emerging as an important tool in the diagnosis and analysis of the developing human brain. Automatic segmentation of the developing fetal brain is a vital step in the quantitative analysis of prenatal neurodevelopment both in the research and clinical context. However, manual segmentation of cerebral structures is time-consuming and prone to error and inter-observer variability. Therefore, we organized the Fetal Tissue Annotation (FeTA) Challenge in 2021 in order to encourage the development of automatic segmentation algorithms on an international level. The challenge utilized FeTA Dataset, an open dataset of fetal brain MRI reconstructions segmented into seven different tissues (external cerebrospinal fluid, grey matter, white matter, ventricles, cerebellum, brainstem, deep grey matter). 20 international teams participated in this challenge, submitting a total of 21 algorithms for evaluation. In this paper, we provide a detailed analysis of the results from both a technical and clinical perspective. All participants relied on deep learning methods, mainly U-Nets, with some variability present in the network architecture, optimization, and image pre- and post-processing. The majority of teams used existing medical imaging deep learning frameworks. The main differences between the submissions were the fine tuning done during training, and the specific pre- and post-processing steps performed. The challenge results showed that almost all submissions performed similarly. Four of the top five teams used ensemble learning methods. However, one team's algorithm performed significantly superior to the other submissions, and consisted of an asymmetrical U-Net network architecture. This paper provides a first of its kind benchmark for future automatic multi-tissue segmentation algorithms for the developing human brain in utero.



### Fast and Robust Femur Segmentation from Computed Tomography Images for Patient-Specific Hip Fracture Risk Screening
- **Arxiv ID**: http://arxiv.org/abs/2204.09575v1
- **DOI**: 10.1080/21681163.2022.2068160
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09575v1)
- **Published**: 2022-04-20 16:16:16+00:00
- **Updated**: 2022-04-20 16:16:16+00:00
- **Authors**: Pall Asgeir Bjornsson, Alexander Baker, Ingmar Fleps, Yves Pauchard, Halldor Palsson, Stephen J. Ferguson, Sigurdur Sigurdsson, Vilmundur Gudnason, Benedikt Helgason, Lotta Maria Ellingsen
- **Comment**: This article has been accepted for publication in Computer Methods in
  Biomechanics and Biomedical Engineering: Imaging & Visualization, published
  by Taylor & Francis
- **Journal**: None
- **Summary**: Osteoporosis is a common bone disease that increases the risk of bone fracture. Hip-fracture risk screening methods based on finite element analysis depend on segmented computed tomography (CT) images; however, current femur segmentation methods require manual delineations of large data sets. Here we propose a deep neural network for fully automated, accurate, and fast segmentation of the proximal femur from CT. Evaluation on a set of 1147 proximal femurs with ground truth segmentations demonstrates that our method is apt for hip-fracture risk screening, bringing us one step closer to a clinically viable option for screening at-risk patients for hip-fracture susceptibility.



### Assembly Planning from Observations under Physical Constraints
- **Arxiv ID**: http://arxiv.org/abs/2204.09616v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.09616v2)
- **Published**: 2022-04-20 16:51:07+00:00
- **Updated**: 2022-10-25 14:17:27+00:00
- **Authors**: Thomas Chabal, Robin Strudel, Etienne Arlaud, Jean Ponce, Cordelia Schmid
- **Comment**: IROS 2022. See the project webpage at
  https://www.di.ens.fr/willow/research/assembly-planning/
- **Journal**: None
- **Summary**: This paper addresses the problem of copying an unknown assembly of primitives with known shape and appearance using information extracted from a single photograph by an off-the-shelf procedure for object detection and pose estimation. The proposed algorithm uses a simple combination of physical stability constraints, convex optimization and Monte Carlo tree search to plan assemblies as sequences of pick-and-place operations represented by STRIPS operators. It is efficient and, most importantly, robust to the errors in object detection and pose estimation unavoidable in any real robotic system. The proposed approach is demonstrated with thorough experiments on a UR5 manipulator.



### Residual Mixture of Experts
- **Arxiv ID**: http://arxiv.org/abs/2204.09636v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09636v3)
- **Published**: 2022-04-20 17:29:48+00:00
- **Updated**: 2022-10-04 10:07:03+00:00
- **Authors**: Lemeng Wu, Mengchen Liu, Yinpeng Chen, Dongdong Chen, Xiyang Dai, Lu Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Mixture of Experts (MoE) is able to scale up vision transformers effectively. However, it requires prohibiting computation resources to train a large MoE transformer. In this paper, we propose Residual Mixture of Experts (RMoE), an efficient training pipeline for MoE vision transformers on downstream tasks, such as segmentation and detection. RMoE achieves comparable results with the upper-bound MoE training, while only introducing minor additional training cost than the lower-bound non-MoE training pipelines. The efficiency is supported by our key observation: the weights of an MoE transformer can be factored into an input-independent core and an input-dependent residual. Compared with the weight core, the weight residual can be efficiently trained with much less computation resource, e.g., finetuning on the downstream data. We show that, compared with the current MoE training pipeline, we get comparable results while saving over 30% training cost. When compared with state-of-the-art non- MoE transformers, such as Swin-T / CvT-13 / Swin-L, we get +1.1 / 0.9 / 1.0 mIoU gain on ADE20K segmentation and +1.4 / 1.6 / 0.6 AP gain on MS-COCO object detection task with less than 3% additional training cost.



### One-Class Model for Fabric Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.09648v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09648v1)
- **Published**: 2022-04-20 17:46:30+00:00
- **Updated**: 2022-04-20 17:46:30+00:00
- **Authors**: Hao Zhou, Yixin Chen, David Troendle, Byunghyun Jang
- **Comment**: None
- **Journal**: None
- **Summary**: An automated and accurate fabric defect inspection system is in high demand as a replacement for slow, inconsistent, error-prone, and expensive human operators in the textile industry. Previous efforts focused on certain types of fabrics or defects, which is not an ideal solution. In this paper, we propose a novel one-class model that is capable of detecting various defects on different fabric types. Our model takes advantage of a well-designed Gabor filter bank to analyze fabric texture. We then leverage an advanced deep learning algorithm, autoencoder, to learn general feature representations from the outputs of the Gabor filter bank. Lastly, we develop a nearest neighbor density estimator to locate potential defects and draw them on the fabric images. We demonstrate the effectiveness and robustness of the proposed model by testing it on various types of fabrics such as plain, patterned, and rotated fabrics. Our model also achieves a true positive rate (a.k.a recall) value of 0.895 with no false alarms on our dataset based upon the Standard Fabric Defect Glossary.



### Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments
- **Arxiv ID**: http://arxiv.org/abs/2204.09667v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.09667v2)
- **Published**: 2022-04-20 17:57:11+00:00
- **Updated**: 2022-04-24 20:10:20+00:00
- **Authors**: Jacob Krantz, Stefan Lee
- **Comment**: Changes: figure compression for accessibility
- **Journal**: None
- **Summary**: Recent work in Vision-and-Language Navigation (VLN) has presented two environmental paradigms with differing realism -- the standard VLN setting built on topological environments where navigation is abstracted away, and the VLN-CE setting where agents must navigate continuous 3D environments using low-level actions. Despite sharing the high-level task and even the underlying instruction-path data, performance on VLN-CE lags behind VLN significantly. In this work, we explore this gap by transferring an agent from the abstract environment of VLN to the continuous environment of VLN-CE. We find that this sim-2-sim transfer is highly effective, improving over the prior state of the art in VLN-CE by +12% success rate. While this demonstrates the potential for this direction, the transfer does not fully retain the original performance of the agent in the abstract setting. We present a sequence of experiments to identify what differences result in performance degradation, providing clear directions for further improvement.



### Complete identification of complex salt geometries from inaccurate migrated subsurface offset gathers using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2204.09710v3
- **DOI**: 10.1190/geo2021-0586.1
- **Categories**: **physics.geo-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09710v3)
- **Published**: 2022-04-20 18:00:27+00:00
- **Updated**: 2022-12-05 16:04:43+00:00
- **Authors**: Ana Paula O. Muller, Jesse C. Costa, Clecio R. Bom, Elisangela L. Faria, Matheus Klatt, Gabriel Teixeira, Marcelo P. de Albuquerque, Marcio P. de Albuquerque
- **Comment**: Manuscript published at Geophysics
- **Journal**: Geophysics 87 2022
- **Summary**: Delimiting salt inclusions from migrated images is a time-consuming activity that relies on highly human-curated analysis and is subject to interpretation errors or limitations of the methods available. We propose to use migrated images produced from an inaccurate velocity model (with a reasonable approximation of sediment velocity, but without salt inclusions) to predict the correct salt inclusions shape using a Convolutional Neural Network (CNN). Our approach relies on subsurface Common Image Gathers to focus the sediments' reflections around the zero offset and to spread the energy of salt reflections over large offsets. Using synthetic data, we trained a U-Net to use common-offset subsurface images as input channels for the CNN and the correct salt-masks as network output. The network learned to predict the salt inclusions masks with high accuracy; moreover, it also performed well when applied to synthetic benchmark data sets that were not previously introduced. Our training process tuned the U-Net to successfully learn the shape of complex salt bodies from partially focused subsurface offset images.



### Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2204.09730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09730v1)
- **Published**: 2022-04-20 18:19:24+00:00
- **Updated**: 2022-04-20 18:19:24+00:00
- **Authors**: Mustafa Shukor, Guillaume Couairon, Asya Grechka, Matthieu Cord
- **Comment**: Accepted at CVPR 2022, MULA Workshop. Code is available at
  https://github.com/mshukor/TFood
- **Journal**: None
- **Summary**: Cross-modal image-recipe retrieval has gained significant attention in recent years. Most work focuses on improving cross-modal embeddings using unimodal encoders, that allow for efficient retrieval in large-scale databases, leaving aside cross-attention between modalities which is more computationally expensive. We propose a new retrieval framework, T-Food (Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval) that exploits the interaction between modalities in a novel regularization scheme, while using only unimodal encoders at test time for efficient retrieval. We also capture the intra-dependencies between recipe entities with a dedicated recipe encoder, and propose new variants of triplet losses with dynamic margins that adapt to the difficulty of the task. Finally, we leverage the power of the recent Vision and Language Pretraining (VLP) models such as CLIP for the image encoder. Our approach outperforms existing approaches by a large margin on the Recipe1M dataset. Specifically, we achieve absolute improvements of 8.1 % (72.6 R@1) and +10.9 % (44.6 R@1) on the 1k and 10k test sets respectively. The code is available here:https://github.com/mshukor/TFood



### Time-based Self-supervised Learning for Wireless Capsule Endoscopy
- **Arxiv ID**: http://arxiv.org/abs/2204.09773v1
- **DOI**: 10.1016/j.compbiomed.2022.105631
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09773v1)
- **Published**: 2022-04-20 20:31:06+00:00
- **Updated**: 2022-04-20 20:31:06+00:00
- **Authors**: Guillem Pascual, Pablo Laiz, Albert García, Hagen Wenzek, Jordi Vitrià, Santi Seguí
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art machine learning models, and especially deep learning ones, are significantly data-hungry; they require vast amounts of manually labeled samples to function correctly. However, in most medical imaging fields, obtaining said data can be challenging. Not only the volume of data is a problem, but also the imbalances within its classes; it is common to have many more images of healthy patients than of those with pathology. Computer-aided diagnostic systems suffer from these issues, usually over-designing their models to perform accurately. This work proposes using self-supervised learning for wireless endoscopy videos by introducing a custom-tailored method that does not initially need labels or appropriate balance. We prove that using the inferred inherent structure learned by our method, extracted from the temporal axis, improves the detection rate on several domain-specific applications even under severe imbalance.



### Attention in Reasoning: Dataset, Analysis, and Modeling
- **Arxiv ID**: http://arxiv.org/abs/2204.09774v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09774v1)
- **Published**: 2022-04-20 20:32:31+00:00
- **Updated**: 2022-04-20 20:32:31+00:00
- **Authors**: Shi Chen, Ming Jiang, Jinhui Yang, Qi Zhao
- **Comment**: To be published in TPAMI. arXiv admin note: substantial text overlap
  with arXiv:2007.14419
- **Journal**: None
- **Summary**: While attention has been an increasingly popular component in deep neural networks to both interpret and boost the performance of models, little work has examined how attention progresses to accomplish a task and whether it is reasonable. In this work, we propose an Attention with Reasoning capability (AiR) framework that uses attention to understand and improve the process leading to task outcomes. We first define an evaluation metric based on a sequence of atomic reasoning operations, enabling a quantitative measurement of attention that considers the reasoning process. We then collect human eye-tracking and answer correctness data, and analyze various machine and human attention mechanisms on their reasoning capability and how they impact task performance. To improve the attention and reasoning ability of visual question answering models, we propose to supervise the learning of attention progressively along the reasoning process and to differentiate the correct and incorrect attention patterns. We demonstrate the effectiveness of the proposed framework in analyzing and modeling attention with better reasoning capability and task performance. The code and data are available at https://github.com/szzexpoi/AiR



### Multi-Focus Image Fusion based on Gradient Transform
- **Arxiv ID**: http://arxiv.org/abs/2204.09777v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.0; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2204.09777v1)
- **Published**: 2022-04-20 20:35:12+00:00
- **Updated**: 2022-04-20 20:35:12+00:00
- **Authors**: Sultan Sevgi Turgut, Mustafa Oral
- **Comment**: 20 pages, 9 Figures
- **Journal**: None
- **Summary**: Multi-focus image fusion is a challenging field of study that aims to provide a completely focused image by integrating focused and un-focused pixels. Most existing methods suffer from shift variance, misregistered images, and data-dependent. In this study, we introduce a novel gradient information-based multi-focus image fusion method that is robust for the aforementioned problems. The proposed method first generates gradient images from original images by using Halftoning-Inverse Halftoning (H-IH) transform. Then, Energy of Gradient (EOG) and Standard Deviation functions are used as the focus measurement on the gradient images to form a fused image. Finally, in order to enhance the fused image a decision fusion approach is applied with the majority voting method. The proposed method is compared with 17 different novel and conventional techniques both visually and objectively. For objective evaluation, 6 different quantitative metrics are used. It is observed that the proposed method is promising according to visual evaluation and 83.3% success is achieved by being first in five out of six metrics according to objective evaluation.



### Multi-Scale Features and Parallel Transformers Based Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2204.09779v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09779v1)
- **Published**: 2022-04-20 20:38:23+00:00
- **Updated**: 2022-04-20 20:38:23+00:00
- **Authors**: Abhisek Keshari, Komal, Sadbhawna, Badri Subudhi
- **Comment**: None
- **Journal**: None
- **Summary**: With the increase in multimedia content, the type of distortions associated with multimedia is also increasing. This problem of image quality assessment is expanded well in the PIPAL dataset, which is still an open problem to solve for researchers. Although, recently proposed transformers networks have already been used in the literature for image quality assessment. At the same time, we notice that multi-scale feature extraction has proven to be a promising approach for image quality assessment. However, the way transformer networks are used for image quality assessment until now lacks these properties of multi-scale feature extraction. We utilized this fact in our approach and proposed a new architecture by integrating these two promising quality assessment techniques of images. Our experimentation on various datasets, including the PIPAL dataset, demonstrates that the proposed integration technique outperforms existing algorithms. The source code of the proposed algorithm is available online: https://github.com/KomalPal9610/IQA



### MultiPathGAN: Structure Preserving Stain Normalization using Unsupervised Multi-domain Adversarial Network with Perception Loss
- **Arxiv ID**: http://arxiv.org/abs/2204.09782v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09782v2)
- **Published**: 2022-04-20 20:48:17+00:00
- **Updated**: 2022-08-02 09:41:08+00:00
- **Authors**: Haseeb Nazki, Ognjen Arandjelović, InHwa Um, David Harrison
- **Comment**: None
- **Journal**: None
- **Summary**: Histopathology relies on the analysis of microscopic tissue images to diagnose disease. A crucial part of tissue preparation is staining whereby a dye is used to make the salient tissue components more distinguishable. However, differences in laboratory protocols and scanning devices result in significant confounding appearance variation in the corresponding images. This variation increases both human error and the inter-rater variability, as well as hinders the performance of automatic or semi-automatic methods. In the present paper we introduce an unsupervised adversarial network to translate (and hence normalize) whole slide images across multiple data acquisition domains. Our key contributions are: (i) an adversarial architecture which learns across multiple domains with a single generator-discriminator network using an information flow branch which optimizes for perceptual loss, and (ii) the inclusion of an additional feature extraction network during training which guides the transformation network to keep all the structural features in the tissue image intact. We: (i) demonstrate the effectiveness of the proposed method firstly on H\&E slides of 120 cases of kidney cancer, as well as (ii) show the benefits of the approach on more general problems, such as flexible illumination based natural image enhancement and light source adaptation.



### SELMA: SEmantic Large-scale Multimodal Acquisitions in Variable Weather, Daytime and Viewpoints
- **Arxiv ID**: http://arxiv.org/abs/2204.09788v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.09788v1)
- **Published**: 2022-04-20 21:22:56+00:00
- **Updated**: 2022-04-20 21:22:56+00:00
- **Authors**: Paolo Testolina, Francesco Barbato, Umberto Michieli, Marco Giordani, Pietro Zanuttigh, Michele Zorzi
- **Comment**: 14 figures, 14 tables. This paper has been submitted to IEEE.
  Copyright may change without notice
- **Journal**: None
- **Summary**: Accurate scene understanding from multiple sensors mounted on cars is a key requirement for autonomous driving systems. Nowadays, this task is mainly performed through data-hungry deep learning techniques that need very large amounts of data to be trained. Due to the high cost of performing segmentation labeling, many synthetic datasets have been proposed. However, most of them miss the multi-sensor nature of the data, and do not capture the significant changes introduced by the variation of daytime and weather conditions. To fill these gaps, we introduce SELMA, a novel synthetic dataset for semantic segmentation that contains more than 30K unique waypoints acquired from 24 different sensors including RGB, depth, semantic cameras and LiDARs, in 27 different atmospheric and daytime conditions, for a total of more than 20M samples. SELMA is based on CARLA, an open-source simulator for generating synthetic data in autonomous driving scenarios, that we modified to increase the variability and the diversity in the scenes and class sets, and to align it with other benchmark datasets. As shown by the experimental evaluation, SELMA allows the efficient training of standard and multi-modal deep learning architectures, and achieves remarkable results on real-world data. SELMA is free and publicly available, thus supporting open science and research.



### Weighted Bayesian Gaussian Mixture Model for Roadside LiDAR Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.09804v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09804v4)
- **Published**: 2022-04-20 22:48:05+00:00
- **Updated**: 2023-03-24 06:22:03+00:00
- **Authors**: Tianya Zhang, Yi Ge, Peter J. Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Background modeling is widely used for intelligent surveillance systems to detect moving targets by subtracting the static background components. Most roadside LiDAR object detection methods filter out foreground points by comparing new data points to pre-trained background references based on descriptive statistics over many frames (e.g., voxel density, number of neighbors, maximum distance). However, these solutions are inefficient under heavy traffic, and parameter values are hard to transfer from one scenario to another. In early studies, the probabilistic background modeling methods widely used for the video-based system were considered unsuitable for roadside LiDAR surveillance systems due to the sparse and unstructured point cloud data. In this paper, the raw LiDAR data were transformed into a structured representation based on the elevation and azimuth value of each LiDAR point. With this high-order tensor representation, we break the barrier to allow efficient high-dimensional multivariate analysis for roadside LiDAR background modeling. The Bayesian Nonparametric (BNP) approach integrates the intensity value and 3D measurements to exploit the measurement data using 3D and intensity info entirely. The proposed method was compared against two state-of-the-art roadside LiDAR background models, computer vision benchmark, and deep learning baselines, evaluated at point, object, and path levels under heavy traffic and challenging weather. This multimodal Weighted Bayesian Gaussian Mixture Model (GMM) can handle dynamic backgrounds with noisy measurements and substantially enhances the infrastructure-based LiDAR object detection, whereby various 3D modeling for smart city applications could be created.



