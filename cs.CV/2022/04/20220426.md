# Arxiv Papers in cs.CV on 2022-04-26
### Assessing the ability of generative adversarial networks to learn canonical medical image statistics
- **Arxiv ID**: http://arxiv.org/abs/2204.12007v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2204.12007v2)
- **Published**: 2022-04-26 00:30:01+00:00
- **Updated**: 2022-04-27 01:28:32+00:00
- **Authors**: Varun A. Kelkar, Dimitrios S. Gotsis, Frank J. Brooks, Prabhat KC, Kyle J. Myers, Rongping Zeng, Mark A. Anastasio
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, generative adversarial networks (GANs) have gained tremendous popularity for potential applications in medical imaging, such as medical image synthesis, restoration, reconstruction, translation, as well as objective image quality assessment. Despite the impressive progress in generating high-resolution, perceptually realistic images, it is not clear if modern GANs reliably learn the statistics that are meaningful to a downstream medical imaging application. In this work, the ability of a state-of-the-art GAN to learn the statistics of canonical stochastic image models (SIMs) that are relevant to objective assessment of image quality is investigated. It is shown that although the employed GAN successfully learned several basic first- and second-order statistics of the specific medical SIMs under consideration and generated images with high perceptual quality, it failed to correctly learn several per-image statistics pertinent to the these SIMs, highlighting the urgent need to assess medical image GANs in terms of objective measures of image quality.



### Estimating the Resize Parameter in End-to-end Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2204.12022v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12022v1)
- **Published**: 2022-04-26 01:35:02+00:00
- **Updated**: 2022-04-26 01:35:02+00:00
- **Authors**: Li-Heng Chen, Christos G. Bampis, Zhi Li, Lukáš Krasula, Alan C. Bovik
- **Comment**: None
- **Journal**: None
- **Summary**: We describe a search-free resizing framework that can further improve the rate-distortion tradeoff of recent learned image compression models. Our approach is simple: compose a pair of differentiable downsampling/upsampling layers that sandwich a neural compression model. To determine resize factors for different inputs, we utilize another neural network jointly trained with the compression model, with the end goal of minimizing the rate-distortion objective. Our results suggest that "compression friendly" downsampled representations can be quickly determined during encoding by using an auxiliary network and differentiable image warping. By conducting extensive experimental tests on existing deep image compression models, we show results that our new resizing parameter estimation framework can provide Bj{\o}ntegaard-Delta rate (BD-rate) improvement of about 10% against leading perceptual quality engines. We also carried out a subjective quality study, the results of which show that our new approach yields favorable compressed images. To facilitate reproducible research in this direction, the implementation used in this paper is being made freely available online at: https://github.com/treammm/ResizeCompression.



### Information Fusion: Scaling Subspace-Driven Approaches
- **Arxiv ID**: http://arxiv.org/abs/2204.12035v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12035v1)
- **Published**: 2022-04-26 02:16:01+00:00
- **Updated**: 2022-04-26 02:16:01+00:00
- **Authors**: Sally Ghanem, Hamid Krim
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we seek to exploit the deep structure of multi-modal data to robustly exploit the group subspace distribution of the information using the Convolutional Neural Network (CNN) formalism. Upon unfolding the set of subspaces constituting each data modality, and learning their corresponding encoders, an optimized integration of the generated inherent information is carried out to yield a characterization of various classes. Referred to as deep Multimodal Robust Group Subspace Clustering (DRoGSuRe), this approach is compared against the independently developed state-of-the-art approach named Deep Multimodal Subspace Clustering (DMSC). Experiments on different multimodal datasets show that our approach is competitive and more robust in the presence of noise.



### Causal Reasoning Meets Visual Representation Learning: A Prospective Study
- **Arxiv ID**: http://arxiv.org/abs/2204.12037v8
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.12037v8)
- **Published**: 2022-04-26 02:22:28+00:00
- **Updated**: 2023-03-22 02:41:58+00:00
- **Authors**: Yang Liu, Yushen Wei, Hong Yan, Guanbin Li, Liang Lin
- **Comment**: 35 pages, 14 figures. This work has been accepted by Machine
  Intelligence Research. The arxiv version is kept updating by adding more
  novel methods, datasets and insights. The official video interpretation of
  this paper can be referred at https://youtu.be/2lfNaTkcTHI
- **Journal**: None
- **Summary**: Visual representation learning is ubiquitous in various real-world applications, including visual comprehension, video understanding, multi-modal analysis, human-computer interaction, and urban computing. Due to the emergence of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal data in big data era, the lack of interpretability, robustness, and out-of-distribution generalization are becoming the challenges of the existing visual models. The majority of the existing methods tend to fit the original data/variable distributions and ignore the essential causal relations behind the multi-modal knowledge, which lacks unified guidance and analysis about why modern visual representation learning methods easily collapse into data bias and have limited generalization and cognitive abilities. Inspired by the strong inference ability of human-level agents, recent years have therefore witnessed great effort in developing causal reasoning paradigms to realize robust representation and model learning with good cognitive ability. In this paper, we conduct a comprehensive review of existing causal reasoning methods for visual representation learning, covering fundamental theories, models, and datasets. The limitations of current methods and datasets are also discussed. Moreover, we propose some prospective challenges, opportunities, and future research directions for benchmarking causal reasoning algorithms in visual representation learning. This paper aims to provide a comprehensive overview of this emerging field, attract attention, encourage discussions, bring to the forefront the urgency of developing novel causal reasoning methods, publicly available benchmarks, and consensus-building standards for reliable visual representation learning and related real-world applications more efficiently.



### Learning Weighting Map for Bit-Depth Expansion within a Rational Range
- **Arxiv ID**: http://arxiv.org/abs/2204.12039v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12039v1)
- **Published**: 2022-04-26 02:27:39+00:00
- **Updated**: 2022-04-26 02:27:39+00:00
- **Authors**: Yuqing Liu, Qi Jia, Jian Zhang, Xin Fan, Shanshe Wang, Siwei Ma, Wen Gao
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Bit-depth expansion (BDE) is one of the emerging technologies to display high bit-depth (HBD) image from low bit-depth (LBD) source. Existing BDE methods have no unified solution for various BDE situations, and directly learn a mapping for each pixel from LBD image to the desired value in HBD image, which may change the given high-order bits and lead to a huge deviation from the ground truth. In this paper, we design a bit restoration network (BRNet) to learn a weight for each pixel, which indicates the ratio of the replenished value within a rational range, invoking an accurate solution without modifying the given high-order bit information. To make the network adaptive for any bit-depth degradation, we investigate the issue in an optimization perspective and train the network under progressive training strategy for better performance. Moreover, we employ Wasserstein distance as a visual quality indicator to evaluate the difference of color distribution between restored image and the ground truth. Experimental results show our method can restore colorful images with fewer artifacts and false contours, and outperforms state-of-the-art methods with higher PSNR/SSIM results and lower Wasserstein distance. The source code will be made available at https://github.com/yuqing-liu-dut/bit-depth-expansion



### Deep Learning-based Automatic Player Identification and Logging in American Football Videos
- **Arxiv ID**: http://arxiv.org/abs/2204.13809v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.13809v1)
- **Published**: 2022-04-26 02:59:03+00:00
- **Updated**: 2022-04-26 02:59:03+00:00
- **Authors**: Hongshan Liu, Colin Aderon, Noah Wagon, Huapu Liu, Steven MacCall, Yu Gan
- **Comment**: None
- **Journal**: None
- **Summary**: American football games attract significant worldwide attention every year. Game analysis systems generate crucial information that can help analyze the games by providing fans and coaches with a convenient means to track and evaluate player performance. Identifying participating players in each play is also important for the video indexing of player participation per play. Processing football game video presents challenges such as crowded setting, distorted objects, and imbalanced data for identifying players, especially jersey numbers. In this work, we propose a deep learning-based football video analysis system to automatically track players and index their participation per play. It is a multi-stage network design to highlight area of interest and identify jersey number information with high accuracy. First, we utilize an object detection network, a detection transformer, to tackle the player detection problem in crowded context. Second, we identify players using jersey number recognition with a secondary convolutional neural network, then synchronize it with a game clock subsystem. Finally, the system outputs a complete log in a database for play indexing. We demonstrate the effectiveness and reliability of player identification and the logging system by analyzing the qualitative and quantitative results on football videos. The proposed system shows great potential for implementation in and analysis of football broadcast video.



### Self-recoverable Adversarial Examples: A New Effective Protection Mechanism in Social Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.12050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12050v1)
- **Published**: 2022-04-26 03:08:13+00:00
- **Updated**: 2022-04-26 03:08:13+00:00
- **Authors**: Jiawei Zhang, Jinwei Wang, Hao Wang, Xiangyang Luo
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: Malicious intelligent algorithms greatly threaten the security of social users' privacy by detecting and analyzing the uploaded photos to social network platforms. The destruction to DNNs brought by the adversarial attack sparks the potential that adversarial examples serve as a new protection mechanism for privacy security in social networks. However, the existing adversarial example does not have recoverability for serving as an effective protection mechanism. To address this issue, we propose a recoverable generative adversarial network to generate self-recoverable adversarial examples. By modeling the adversarial attack and recovery as a united task, our method can minimize the error of the recovered examples while maximizing the attack ability, resulting in better recoverability of adversarial examples. To further boost the recoverability of these examples, we exploit a dimension reducer to optimize the distribution of adversarial perturbation. The experimental results prove that the adversarial examples generated by the proposed method present superior recoverability, attack ability, and robustness on different datasets and network architectures, which ensure its effectiveness as a protection mechanism in social networks.



### An Overview of Recent Work in Media Forensics: Methods and Threats
- **Arxiv ID**: http://arxiv.org/abs/2204.12067v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.12067v2)
- **Published**: 2022-04-26 04:17:19+00:00
- **Updated**: 2022-05-12 20:09:42+00:00
- **Authors**: Kratika Bhagtani, Amit Kumar Singh Yadav, Emily R. Bartusiak, Ziyue Xiang, Ruiting Shao, Sriram Baireddy, Edward J. Delp
- **Comment**: This is a longer version of a paper accepted to the 2022 IEEE
  International Conference on Multimedia Information Processing and Retrieval
  entitled "An Overview of Recent Work in Multimedia Forensics"
- **Journal**: None
- **Summary**: In this paper, we review recent work in media forensics for digital images, video, audio (specifically speech), and documents. For each data modality, we discuss synthesis and manipulation techniques that can be used to create and modify digital media. We then review technological advancements for detecting and quantifying such manipulations. Finally, we consider open issues and suggest directions for future research.



### AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2204.12077v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.12077v3)
- **Published**: 2022-04-26 05:12:00+00:00
- **Updated**: 2022-10-31 08:52:47+00:00
- **Authors**: Gongping Chen, Yu Dai, Jianxun Zhang, Moi Hoon Yap
- **Comment**: Breast cancer segmentation, Ultrasound images, Hybrid attention,
  Adaptive learning, Deep learning
- **Journal**: None
- **Summary**: Various deep learning methods have been proposed to segment breast lesion from ultrasound images. However, similar intensity distributions, variable tumor morphology and blurred boundaries present challenges for breast lesions segmentation, especially for malignant tumors with irregular shapes. Considering the complexity of ultrasound images, we develop an adaptive attention U-net (AAU-net) to segment breast lesions automatically and stably from ultrasound images. Specifically, we introduce a hybrid adaptive attention module, which mainly consists of a channel self-attention block and a spatial self-attention block, to replace the traditional convolution operation. Compared with the conventional convolution operation, the design of the hybrid adaptive attention module can help us capture more features under different receptive fields. Different from existing attention mechanisms, the hybrid adaptive attention module can guide the network to adaptively select more robust representation in channel and space dimensions to cope with more complex breast lesions segmentation. Extensive experiments with several state-of-the-art deep learning segmentation methods on three public breast ultrasound datasets show that our method has better performance on breast lesion segmentation. Furthermore, robustness analysis and external experiments demonstrate that our proposed AAU-net has better generalization performance on the segmentation of breast lesions. Moreover, the hybrid adaptive attention module can be flexibly applied to existing network frameworks.



### U-Net with ResNet Backbone for Garment Landmarking Purpose
- **Arxiv ID**: http://arxiv.org/abs/2204.12084v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2204.12084v1)
- **Published**: 2022-04-26 05:47:27+00:00
- **Updated**: 2022-04-26 05:47:27+00:00
- **Authors**: Khay Boon Hong
- **Comment**: A draft for purpose of archive, not intended for official academic
  uses
- **Journal**: None
- **Summary**: We build a heatmap-based landmark detection model to locate important landmarks on 2D RGB garment images. The main goal is to detect edges, corners and suitable interior region of the garments. This let us re-create 3D garments in modern 3D editing software by incorporate landmark detection model and texture unwrapping. We use a U-net architecture with ResNet backbone to build the model. With an appropriate loss function, we are able to train a moderately robust model.



### Acquiring a Dynamic Light Field through a Single-Shot Coded Image
- **Arxiv ID**: http://arxiv.org/abs/2204.12089v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12089v1)
- **Published**: 2022-04-26 06:00:02+00:00
- **Updated**: 2022-04-26 06:00:02+00:00
- **Authors**: Ryoya Mizuno, Keita Takahashi, Michitaka Yoshida, Chihiro Tsutake, Toshiaki Fujii, Hajime Nagahara
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method for compressively acquiring a dynamic light field (a 5-D volume) through a single-shot coded image (a 2-D measurement). We designed an imaging model that synchronously applies aperture coding and pixel-wise exposure coding within a single exposure time. This coding scheme enables us to effectively embed the original information into a single observed image. The observed image is then fed to a convolutional neural network (CNN) for light-field reconstruction, which is jointly trained with the camera-side coding patterns. We also developed a hardware prototype to capture a real 3-D scene moving over time. We succeeded in acquiring a dynamic light field with 5x5 viewpoints over 4 temporal sub-frames (100 views in total) from a single observed image. Repeating capture and reconstruction processes over time, we can acquire a dynamic light field at 4x the frame rate of the camera. To our knowledge, our method is the first to achieve a finer temporal resolution than the camera itself in compressive light-field acquisition. Our software is available from our project webpage



### Learning Dual-Pixel Alignment for Defocus Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2204.12105v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12105v2)
- **Published**: 2022-04-26 07:02:58+00:00
- **Updated**: 2023-02-20 03:25:44+00:00
- **Authors**: Yu Li, Yaling Yi, Dongwei Ren, Qince Li, Wangmeng Zuo
- **Comment**: Project page: https://github.com/liyucs/DPANet
- **Journal**: None
- **Summary**: It is a challenging task to recover sharp image from a single defocus blurry image in real-world applications. On many modern cameras, dual-pixel (DP) sensors create two-image views, based on which stereo information can be exploited to benefit defocus deblurring. Despite the impressive results achieved by existing DP defocus deblurring methods, the misalignment between DP image views is still not studied, leaving room for improving DP defocus deblurring. In this work, we propose a Dual-Pixel Alignment Network (DPANet) for defocus deblurring. Generally, DPANet is an encoder-decoder with skip-connections, where two branches with shared parameters in the encoder are employed to extract and align deep features from left and right views, and one decoder is adopted to fuse aligned features for predicting the sharp image. Due to that DP views suffer from different blur amounts, it is not trivial to align left and right views. To this end, we propose novel encoder alignment module (EAM) and decoder alignment module (DAM). In particular, a correlation layer is suggested in EAM to measure the disparity between DP views, whose deep features can then be accordingly aligned using deformable convolutions. DAM can further enhance the alignment of skip-connected features from encoder and deep features in decoder. By introducing several EAMs and DAMs, DP views in DPANet can be well aligned for better predicting latent sharp image. Experimental results on real-world datasets show that our DPANet is notably superior to state-of-the-art deblurring methods in reducing defocus blur while recovering visually plausible sharp structures and textures.



### Instance-Specific Feature Propagation for Referring Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.12109v1
- **DOI**: 10.1109/TMM.2022.3163578
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12109v1)
- **Published**: 2022-04-26 07:08:14+00:00
- **Updated**: 2022-04-26 07:08:14+00:00
- **Authors**: Chang Liu, Xudong Jiang, Henghui Ding
- **Comment**: TMM
- **Journal**: None
- **Summary**: Referring segmentation aims to generate a segmentation mask for the target instance indicated by a natural language expression. There are typically two kinds of existing methods: one-stage methods that directly perform segmentation on the fused vision and language features; and two-stage methods that first utilize an instance segmentation model for instance proposal and then select one of these instances via matching them with language features. In this work, we propose a novel framework that simultaneously detects the target-of-interest via feature propagation and generates a fine-grained segmentation mask. In our framework, each instance is represented by an Instance-Specific Feature (ISF), and the target-of-referring is identified by exchanging information among all ISFs using our proposed Feature Propagation Module (FPM). Our instance-aware approach learns the relationship among all objects, which helps to better locate the target-of-interest than one-stage methods. Comparing to two-stage methods, our approach collaboratively and interactively utilizes both vision and language information for synchronous identification and segmentation. In the experimental tests, our method outperforms previous state-of-the-art methods on all three RefCOCO series datasets.



### Neural Maximum A Posteriori Estimation on Unpaired Data for Motion Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2204.12139v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12139v1)
- **Published**: 2022-04-26 08:09:47+00:00
- **Updated**: 2022-04-26 08:09:47+00:00
- **Authors**: Youjian Zhang, Chaoyue Wang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world dynamic scene deblurring has long been a challenging task since paired blurry-sharp training data is unavailable. Conventional Maximum A Posteriori estimation and deep learning-based deblurring methods are restricted by handcrafted priors and synthetic blurry-sharp training pairs respectively, thereby failing to generalize to real dynamic blurriness. To this end, we propose a Neural Maximum A Posteriori (NeurMAP) estimation framework for training neural networks to recover blind motion information and sharp content from unpaired data. The proposed NeruMAP consists of a motion estimation network and a deblurring network which are trained jointly to model the (re)blurring process (i.e. likelihood function). Meanwhile, the motion estimation network is trained to explore the motion information in images by applying implicit dynamic motion prior, and in return enforces the deblurring network training (i.e. providing sharp image prior). The proposed NeurMAP is an orthogonal approach to existing deblurring neural networks, and is the first framework that enables training image deblurring networks on unpaired datasets. Experiments demonstrate our superiority on both quantitative metrics and visual quality over state-of-the-art methods. Codes are available on https://github.com/yjzhang96/NeurMAP-deblur.



### Deeper Insights into the Robustness of ViTs towards Common Corruptions
- **Arxiv ID**: http://arxiv.org/abs/2204.12143v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12143v3)
- **Published**: 2022-04-26 08:22:34+00:00
- **Updated**: 2022-08-19 11:58:54+00:00
- **Authors**: Rui Tian, Zuxuan Wu, Qi Dai, Han Hu, Yu-Gang Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: With Vision Transformers (ViTs) making great advances in a variety of computer vision tasks, recent literature have proposed various variants of vanilla ViTs to achieve better efficiency and efficacy. However, it remains unclear how their unique architecture impact robustness towards common corruptions. In this paper, we make the first attempt to probe into the robustness gap among ViT variants and explore underlying designs that are essential for robustness. Through an extensive and rigorous benchmarking, we demonstrate that simple architecture designs such as overlapping patch embedding and convolutional feed-forward network (FFN) can promote the robustness of ViTs. Moreover, since training ViTs relies heavily on data augmentation, whether previous CNN-based augmentation strategies that are targeted at robustness purposes can still be useful is worth investigating. We explore different data augmentation on ViTs and verify that adversarial noise training is powerful while fourier-domain augmentation is inferior. Based on these findings, we introduce a novel conditional method of generating dynamic augmentation parameters conditioned on input images, offering state-of-the-art robustness towards common corruptions.



### Where and What: Driver Attention-based Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.12150v2
- **DOI**: 10.1145/3530887
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2204.12150v2)
- **Published**: 2022-04-26 08:38:22+00:00
- **Updated**: 2022-05-22 15:45:57+00:00
- **Authors**: Yao Rong, Naemi-Rebecca Kassautzki, Wolfgang Fuhl, Enkelejda Kasneci
- **Comment**: 22 pages
- **Journal**: Proceedings of the ACM on Human-Computer Interaction, 2022
- **Summary**: Human drivers use their attentional mechanisms to focus on critical objects and make decisions while driving. As human attention can be revealed from gaze data, capturing and analyzing gaze information has emerged in recent years to benefit autonomous driving technology. Previous works in this context have primarily aimed at predicting "where" human drivers look at and lack knowledge of "what" objects drivers focus on. Our work bridges the gap between pixel-level and object-level attention prediction. Specifically, we propose to integrate an attention prediction module into a pretrained object detection framework and predict the attention in a grid-based style. Furthermore, critical objects are recognized based on predicted attended-to areas. We evaluate our proposed method on two driver attention datasets, BDD-A and DR(eye)VE. Our framework achieves competitive state-of-the-art performance in the attention prediction on both pixel-level and object-level but is far more efficient (75.3 GFLOPs less) in computation.



### ClothFormer:Taming Video Virtual Try-on in All Module
- **Arxiv ID**: http://arxiv.org/abs/2204.12151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12151v1)
- **Published**: 2022-04-26 08:40:28+00:00
- **Updated**: 2022-04-26 08:40:28+00:00
- **Authors**: Jianbin Jiang, Tan Wang, He Yan, Junhui Liu
- **Comment**: CVPR2022 Oral, project page https://cloth-former.github.io
- **Journal**: None
- **Summary**: The task of video virtual try-on aims to fit the target clothes to a person in the video with spatio-temporal consistency. Despite tremendous progress of image virtual try-on, they lead to inconsistency between frames when applied to videos. Limited work also explored the task of video-based virtual try-on but failed to produce visually pleasing and temporally coherent results. Moreover, there are two other key challenges: 1) how to generate accurate warping when occlusions appear in the clothing region; 2) how to generate clothes and non-target body parts (e.g. arms, neck) in harmony with the complicated background; To address them, we propose a novel video virtual try-on framework, ClothFormer, which successfully synthesizes realistic, harmonious, and spatio-temporal consistent results in complicated environment. In particular, ClothFormer involves three major modules. First, a two-stage anti-occlusion warping module that predicts an accurate dense flow mapping between the body regions and the clothing regions. Second, an appearance-flow tracking module utilizes ridge regression and optical flow correction to smooth the dense flow sequence and generate a temporally smooth warped clothing sequence. Third, a dual-stream transformer extracts and fuses clothing textures, person features, and environment information to generate realistic try-on videos. Through rigorous experiments, we demonstrate that our method highly surpasses the baselines in terms of synthesized video quality both qualitatively and quantitatively.



### A Comparative Study on Approaches to Acoustic Scene Classification using CNNs
- **Arxiv ID**: http://arxiv.org/abs/2204.12177v1
- **DOI**: 10.1007/978-3-030-89817-5_6
- **Categories**: **cs.SD**, cs.AI, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2204.12177v1)
- **Published**: 2022-04-26 09:23:29+00:00
- **Updated**: 2022-04-26 09:23:29+00:00
- **Authors**: Ishrat Jahan Ananya, Sarah Suad, Shadab Hafiz Choudhury, Mohammad Ashrafuzzaman Khan
- **Comment**: Presented at 2021 Mexican International Conference on Artificial
  Intelligence. Published in Advances in Computational Intelligence, MICAI
  2021, Lecture Notes in Computer Science. 12 pages, 3 figures, 5 tables
- **Journal**: Advances in Computational Intelligence, MICAI 2021, Lecture Notes
  in Artificial Intelligence vol. 13067, pp. 81-91 (2021)
- **Summary**: Acoustic scene classification is a process of characterizing and classifying the environments from sound recordings. The first step is to generate features (representations) from the recorded sound and then classify the background environments. However, different kinds of representations have dramatic effects on the accuracy of the classification. In this paper, we explored the three such representations on classification accuracy using neural networks. We investigated the spectrograms, MFCCs, and embeddings representations using different CNN networks and autoencoders. Our dataset consists of sounds from three settings of indoors and outdoors environments - thus the dataset contains sound from six different kinds of environments. We found that the spectrogram representation has the highest classification accuracy while MFCC has the lowest classification accuracy. We reported our findings, insights as well as some guidelines to achieve better accuracy for environment classification using sounds.



### TranSiam: Fusing Multimodal Visual Features Using Transformer for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.12185v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12185v1)
- **Published**: 2022-04-26 09:39:10+00:00
- **Updated**: 2022-04-26 09:39:10+00:00
- **Authors**: Xuejian Li, Shiqiang Ma, Jijun Tang, Fei Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic segmentation of medical images based on multi-modality is an important topic for disease diagnosis. Although the convolutional neural network (CNN) has been proven to have excellent performance in image segmentation tasks, it is difficult to obtain global information. The lack of global information will seriously affect the accuracy of the segmentation results of the lesion area. In addition, there are visual representation differences between multimodal data of the same patient. These differences will affect the results of the automatic segmentation methods. To solve these problems, we propose a segmentation method suitable for multimodal medical images that can capture global information, named TranSiam. TranSiam is a 2D dual path network that extracts features of different modalities. In each path, we utilize convolution to extract detailed information in low level stage, and design a ICMT block to extract global information in high level stage. ICMT block embeds convolution in the transformer, which can extract global information while retaining spatial and detailed information. Furthermore, we design a novel fusion mechanism based on cross attention and selfattention, called TMM block, which can effectively fuse features between different modalities. On the BraTS 2019 and BraTS 2020 multimodal datasets, we have a significant improvement in accuracy over other popular methods.



### Stochastic Coherence Over Attention Trajectory For Continuous Learning In Video Streams
- **Arxiv ID**: http://arxiv.org/abs/2204.12193v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.12193v1)
- **Published**: 2022-04-26 09:52:31+00:00
- **Updated**: 2022-04-26 09:52:31+00:00
- **Authors**: Matteo Tiezzi, Simone Marullo, Lapo Faggi, Enrico Meloni, Alessandro Betti, Stefano Melacci
- **Comment**: Accepted for publication in the 31st International Joint Conference
  on Artificial Intelligence (IJCAI-ECAI 2022)
- **Journal**: None
- **Summary**: Devising intelligent agents able to live in an environment and learn by observing the surroundings is a longstanding goal of Artificial Intelligence. From a bare Machine Learning perspective, challenges arise when the agent is prevented from leveraging large fully-annotated dataset, but rather the interactions with supervisory signals are sparsely distributed over space and time. This paper proposes a novel neural-network-based approach to progressively and autonomously develop pixel-wise representations in a video stream. The proposed method is based on a human-like attention mechanism that allows the agent to learn by observing what is moving in the attended locations. Spatio-temporal stochastic coherence along the attention trajectory, paired with a contrastive term, leads to an unsupervised learning criterion that naturally copes with the considered setting. Differently from most existing works, the learned representations are used in open-set class-incremental classification of each frame pixel, relying on few supervisions. Our experiments leverage 3D virtual environments and they show that the proposed agents can learn to distinguish objects just by observing the video stream. Inheriting features from state-of-the art models is not as powerful as one might expect.



### Adaptive Split-Fusion Transformer
- **Arxiv ID**: http://arxiv.org/abs/2204.12196v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12196v2)
- **Published**: 2022-04-26 10:00:28+00:00
- **Updated**: 2023-08-16 17:09:41+00:00
- **Authors**: Zixuan Su, Hao Zhang, Jingjing Chen, Lei Pang, Chong-Wah Ngo, Yu-Gang Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks for visual content understanding have recently evolved from convolutional ones (CNNs) to transformers. The prior (CNN) relies on small-windowed kernels to capture the regional clues, demonstrating solid local expressiveness. On the contrary, the latter (transformer) establishes long-range global connections between localities for holistic learning. Inspired by this complementary nature, there is a growing interest in designing hybrid models to best utilize each technique. Current hybrids merely replace convolutions as simple approximations of linear projection or juxtapose a convolution branch with attention, without concerning the importance of local/global modeling. To tackle this, we propose a new hybrid named Adaptive Split-Fusion Transformer (ASF-former) to treat convolutional and attention branches differently with adaptive weights. Specifically, an ASF-former encoder equally splits feature channels into half to fit dual-path inputs. Then, the outputs of dual-path are fused with weighting scalars calculated from visual cues. We also design the convolutional path compactly for efficiency concerns. Extensive experiments on standard benchmarks, such as ImageNet-1K, CIFAR-10, and CIFAR-100, show that our ASF-former outperforms its CNN, transformer counterparts, and hybrid pilots in terms of accuracy (83.9% on ImageNet-1K), under similar conditions (12.9G MACs/56.7M Params, without large-scale pre-training). The code is available at: https://github.com/szx503045266/ASF-former.



### Urban Change Detection Using a Dual-Task Siamese Network and Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.12202v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12202v2)
- **Published**: 2022-04-26 10:12:07+00:00
- **Updated**: 2022-07-03 09:40:35+00:00
- **Authors**: Sebastian Hafner, Yifang Ban, Andrea Nascetti
- **Comment**: 4 pages, 4 figures, to be published in 2022 IEEE International
  Geoscience and Remote Sensing Symposium IGARSS
- **Journal**: None
- **Summary**: In this study, a Semi-Supervised Learning (SSL) method for improving urban change detection from bi-temporal image pairs was presented. The proposed method adapted a Dual-Task Siamese Difference network that not only predicts changes with the difference decoder, but also segments buildings for both images with a semantics decoder. First, the architecture was modified to produce a second change prediction derived from the semantics predictions. Second, SSL was adopted to improve supervised change detection. For unlabeled data, we introduced a loss that encourages the network to predict consistent changes across the two change outputs. The proposed method was tested on urban change detection using the SpaceNet7 dataset. SSL achieved improved results compared to three fully supervised benchmarks.



### Boosting Adversarial Transferability of MLP-Mixer
- **Arxiv ID**: http://arxiv.org/abs/2204.12204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12204v1)
- **Published**: 2022-04-26 10:18:59+00:00
- **Updated**: 2022-04-26 10:18:59+00:00
- **Authors**: Haoran Lyu, Yajie Wang, Yu-an Tan, Huipeng Zhou, Yuhang Zhao, Quanxin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The security of models based on new architectures such as MLP-Mixer and ViTs needs to be studied urgently. However, most of the current researches are mainly aimed at the adversarial attack against ViTs, and there is still relatively little adversarial work on MLP-mixer. We propose an adversarial attack method against MLP-Mixer called Maxwell's demon Attack (MA). MA breaks the channel-mixing and token-mixing mechanism of MLP-Mixer by controlling the part input of MLP-Mixer's each Mixer layer, and disturbs MLP-Mixer to obtain the main information of images. Our method can mask the part input of the Mixer layer, avoid overfitting of the adversarial examples to the source model, and improve the transferability of cross-architecture. Extensive experimental evaluation demonstrates the effectiveness and superior performance of the proposed MA. Our method can be easily combined with existing methods and can improve the transferability by up to 38.0% on MLP-based ResMLP. Adversarial examples produced by our method on MLP-Mixer are able to exceed the transferability of adversarial examples produced using DenseNet against CNNs. To the best of our knowledge, we are the first work to study adversarial transferability of MLP-Mixer.



### Unsupervised Learning of Unbiased Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2204.12941v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2204.12941v1)
- **Published**: 2022-04-26 10:51:50+00:00
- **Updated**: 2022-04-26 10:51:50+00:00
- **Authors**: Carlo Alberto Barbano, Enzo Tartaglione, Marco Grangetto
- **Comment**: 14 pages, 8 figures
- **Journal**: None
- **Summary**: Deep neural networks are known for their inability to learn robust representations when biases exist in the dataset. This results in a poor generalization to unbiased datasets, as the predictions strongly rely on peripheral and confounding factors, which are erroneously learned by the network. Many existing works deal with this issue by either employing an explicit supervision on the bias attributes, or assuming prior knowledge about the bias. In this work we study this problem in a more difficult scenario, in which no explicit annotation about the bias is available, and without any prior knowledge about its nature. We propose a fully unsupervised debiasing framework, consisting of three steps: first, we exploit the natural preference for learning malignant biases, obtaining a bias-capturing model; then, we perform a pseudo-labelling step to obtain bias labels; finally we employ state-of-the-art supervised debiasing techniques to obtain an unbiased model. We also propose a theoretical framework to assess the biasness of a model, and provide a detailed analysis on how biases affect the training of neural networks. We perform experiments on synthetic and real-world datasets, showing that our method achieves state-of-the-art performance in a variety of settings, sometimes even higher than fully supervised debiasing approaches.



### Context-Aware Sequence Alignment using 4D Skeletal Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.12223v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12223v1)
- **Published**: 2022-04-26 10:59:29+00:00
- **Updated**: 2022-04-26 10:59:29+00:00
- **Authors**: Taein Kwon, Bugra Tekin, Siyu Tang, Marc Pollefeys
- **Comment**: Project page: http://www.taeinkwon.com/projects/casa. Accepted to
  CVPR 2022 Oral
- **Journal**: None
- **Summary**: Temporal alignment of fine-grained human actions in videos is important for numerous applications in computer vision, robotics, and mixed reality. State-of-the-art methods directly learn image-based embedding space by leveraging powerful deep convolutional neural networks. While being straightforward, their results are far from satisfactory, the aligned videos exhibit severe temporal discontinuity without additional post-processing steps. The recent advancements in human body and hand pose estimation in the wild promise new ways of addressing the task of human action alignment in videos. In this work, based on off-the-shelf human pose estimators, we propose a novel context-aware self-supervised learning architecture to align sequences of actions. We name it CASA. Specifically, CASA employs self-attention and cross-attention mechanisms to incorporate the spatial and temporal context of human actions, which can solve the temporal discontinuity problem. Moreover, we introduce a self-supervised learning scheme that is empowered by novel 4D augmentation techniques for 3D skeleton representations. We systematically evaluate the key components of our method. Our experiments on three public datasets demonstrate CASA significantly improves phase progress and Kendall's Tau scores over the previous state-of-the-art methods.



### Intercategorical Label Interpolation for Emotional Face Generation with Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.12237v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.12237v1)
- **Published**: 2022-04-26 11:36:32+00:00
- **Updated**: 2022-04-26 11:36:32+00:00
- **Authors**: Silvan Mertes, Dominik Schiller, Florian Lingenfelser, Thomas Kiderle, Valentin Kroner, Lama Diab, Elisabeth André
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks offer the possibility to generate deceptively real images that are almost indistinguishable from actual photographs. Such systems however rely on the presence of large datasets to realistically replicate the corresponding domain. This is especially a problem if not only random new images are to be generated, but specific (continuous) features are to be co-modeled. A particularly important use case in \emph{Human-Computer Interaction} (HCI) research is the generation of emotional images of human faces, which can be used for various use cases, such as the automatic generation of avatars. The problem hereby lies in the availability of training data. Most suitable datasets for this task rely on categorical emotion models and therefore feature only discrete annotation labels. This greatly hinders the learning and modeling of smooth transitions between displayed affective states. To overcome this challenge, we explore the potential of label interpolation to enhance networks trained on categorical datasets with the ability to generate images conditioned on continuous features.



### Attentive Fine-Grained Structured Sparsity for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2204.12266v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12266v2)
- **Published**: 2022-04-26 12:44:55+00:00
- **Updated**: 2022-07-15 08:40:19+00:00
- **Authors**: Junghun Oh, Heewon Kim, Seungjun Nah, Cheeun Hong, Jonghyun Choi, Kyoung Mu Lee
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Image restoration tasks have witnessed great performance improvement in recent years by developing large deep models. Despite the outstanding performance, the heavy computation demanded by the deep models has restricted the application of image restoration. To lift the restriction, it is required to reduce the size of the networks while maintaining accuracy. Recently, N:M structured pruning has appeared as one of the effective and practical pruning approaches for making the model efficient with the accuracy constraint. However, it fails to account for different computational complexities and performance requirements for different layers of an image restoration network. To further optimize the trade-off between the efficiency and the restoration accuracy, we propose a novel pruning method that determines the pruning ratio for N:M structured sparsity at each layer. Extensive experimental results on super-resolution and deblurring tasks demonstrate the efficacy of our method which outperforms previous pruning methods significantly. PyTorch implementation for the proposed methods will be publicly available at https://github.com/JungHunOh/SLS_CVPR2022.



### Contrastive Language-Action Pre-training for Temporal Localization
- **Arxiv ID**: http://arxiv.org/abs/2204.12293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12293v1)
- **Published**: 2022-04-26 13:17:50+00:00
- **Updated**: 2022-04-26 13:17:50+00:00
- **Authors**: Mengmeng Xu, Erhan Gundogdu, Maksim Lapin, Bernard Ghanem, Michael Donoser, Loris Bazzani
- **Comment**: 18 pages, 4 figures
- **Journal**: None
- **Summary**: Long-form video understanding requires designing approaches that are able to temporally localize activities or language. End-to-end training for such tasks is limited by the compute device memory constraints and lack of temporal annotations at large-scale. These limitations can be addressed by pre-training on large datasets of temporally trimmed videos supervised by class annotations. Once the video encoder is pre-trained, it is common practice to freeze it during fine-tuning. Therefore, the video encoder does not learn temporal boundaries and unseen classes, causing a domain gap with respect to the downstream tasks. Moreover, using temporally trimmed videos prevents to capture the relations between different action categories and the background context in a video clip which results in limited generalization capacity. To address these limitations, we propose a novel post-pre-training approach without freezing the video encoder which leverages language. We introduce a masked contrastive learning loss to capture visio-linguistic relations between activities, background video clips and language in the form of captions. Our experiments show that the proposed approach improves the state-of-the-art on temporal action localization, few-shot temporal action localization, and video language grounding tasks.



### Unsupervised Segmentation of Hyperspectral Remote Sensing Images with Superpixels
- **Arxiv ID**: http://arxiv.org/abs/2204.12296v2
- **DOI**: 10.1016/j.rsase.2022.100823
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12296v2)
- **Published**: 2022-04-26 13:20:33+00:00
- **Updated**: 2022-08-26 14:34:49+00:00
- **Authors**: Mirko Paolo Barbato, Paolo Napoletano, Flavio Piccoli, Raimondo Schettini
- **Comment**: 23 pages, 15 figures, 12 tables
- **Journal**: Volume 28, Year 2022, Page 100823
- **Summary**: In this paper, we propose an unsupervised method for hyperspectral remote sensing image segmentation. The method exploits the mean-shift clustering algorithm that takes as input a preliminary hyperspectral superpixels segmentation together with the spectral pixel information. The proposed method does not require the number of segmentation classes as input parameter, and it does not exploit any a-priori knowledge about the type of land-cover or land-use to be segmented (e.g. water, vegetation, building etc.). Experiments on Salinas, SalinasA, Pavia Center and Pavia University datasets are carried out. Performance are measured in terms of normalized mutual information, adjusted Rand index and F1-score. Results demonstrate the validity of the proposed method in comparison with the state of the art.



### Unified GCNs: Towards Connecting GCNs with CNNs
- **Arxiv ID**: http://arxiv.org/abs/2204.12300v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12300v1)
- **Published**: 2022-04-26 13:29:53+00:00
- **Updated**: 2022-04-26 13:29:53+00:00
- **Authors**: Ziyan Zhang, Bo Jiang, Bin Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Graph Convolutional Networks (GCNs) have been widely demonstrated their powerful ability in graph data representation and learning. Existing graph convolution layers are mainly designed based on graph signal processing and transform aspect which usually suffer from some limitations, such as over-smoothing, over-squashing and non-robustness, etc. As we all know that Convolution Neural Networks (CNNs) have received great success in many computer vision and machine learning. One main aspect is that CNNs leverage many learnable convolution filters (kernels) to obtain rich feature descriptors and thus can have high capacity to encode complex patterns in visual data analysis. Also, CNNs are flexible in designing their network architecture, such as MobileNet, ResNet, Xception, etc. Therefore, it is natural to arise a question: can we design graph convolutional layer as flexibly as that in CNNs? Innovatively, in this paper, we consider connecting GCNs with CNNs deeply from a general perspective of depthwise separable convolution operation. Specifically, we show that GCN and GAT indeed perform some specific depthwise separable convolution operations. This novel interpretation enables us to better understand the connections between GCNs (GCN, GAT) and CNNs and further inspires us to design more Unified GCNs (UGCNs). As two showcases, we implement two UGCNs, i.e., Separable UGCN (S-UGCN) and General UGCN (G-UGCN) for graph data representation and learning. Promising experiments on several graph representation benchmarks demonstrate the effectiveness and advantages of the proposed UGCNs.



### Evaluating the Quality of a Synthesized Motion with the Fréchet Motion Distance
- **Arxiv ID**: http://arxiv.org/abs/2204.12318v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12318v2)
- **Published**: 2022-04-26 13:53:40+00:00
- **Updated**: 2022-04-27 08:40:43+00:00
- **Authors**: Antoine Maiorca, Youngwoo Yoon, Thierry Dutoit
- **Comment**: 2 pages, 2 figures
- **Journal**: None
- **Summary**: Evaluating the Quality of a Synthesized Motion with the Fr\'echet Motion Distance



### RAPQ: Rescuing Accuracy for Power-of-Two Low-bit Post-training Quantization
- **Arxiv ID**: http://arxiv.org/abs/2204.12322v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12322v2)
- **Published**: 2022-04-26 14:02:04+00:00
- **Updated**: 2022-09-25 15:37:50+00:00
- **Authors**: Hongyi Yao, Pu Li, Jian Cao, Xiangcheng Liu, Chenying Xie, Bingzhang Wang
- **Comment**: This paper was accepted by IJCAI 2022. Pytorch implementation of
  RAPQ: https://github.com/BillAmihom/RAPQ
- **Journal**: None
- **Summary**: We introduce a Power-of-Two low-bit post-training quantization(PTQ) method for deep neural network that meets hardware requirements and does not call for long-time retraining. Power-of-Two quantization can convert the multiplication introduced by quantization and dequantization to bit-shift that is adopted by many efficient accelerators. However, the Power-of-Two scale factors have fewer candidate values, which leads to more rounding or clipping errors. We propose a novel Power-of-Two PTQ framework, dubbed RAPQ, which dynamically adjusts the Power-of-Two scales of the whole network instead of statically determining them layer by layer. It can theoretically trade off the rounding error and clipping error of the whole network. Meanwhile, the reconstruction method in RAPQ is based on the BN information of every unit. Extensive experiments on ImageNet prove the excellent performance of our proposed method. Without bells and whistles, RAPQ can reach accuracy of 65% and 48% on ResNet-18 and MobileNetV2 respectively with weight INT2 activation INT4. We are the first to propose the more constrained but hardware-friendly Power-of-Two quantization scheme for low-bit PTQ specially and prove that it can achieve nearly the same accuracy as SOTA PTQ method. The code was released.



### An Algorithm for the Labeling and Interactive Visualization of the Cerebrovascular System of Ischemic Strokes
- **Arxiv ID**: http://arxiv.org/abs/2204.12333v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.HC, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2204.12333v1)
- **Published**: 2022-04-26 14:20:26+00:00
- **Updated**: 2022-04-26 14:20:26+00:00
- **Authors**: Florian Thamm, Markus Jürgens, Oliver Taubmann, Aleksandra Thamm, Leonhard Rist, Hendrik Ditt, Andreas Maier
- **Comment**: None
- **Journal**: None
- **Summary**: During the diagnosis of ischemic strokes, the Circle of Willis and its surrounding vessels are the arteries of interest. Their visualization in case of an acute stroke is often enabled by Computed Tomography Angiography (CTA). Still, the identification and analysis of the cerebral arteries remain time consuming in such scans due to a large number of peripheral vessels which may disturb the visual impression. In previous work we proposed VirtualDSA++, an algorithm designed to segment and label the cerebrovascular tree on CTA scans. Especially with stroke patients, labeling is a delicate procedure, as in the worst case whole hemispheres may not be present due to impeded perfusion. Hence, we extended the labeling mechanism for the cerebral arteries to identify occluded vessels. In the work at hand, we place the algorithm in a clinical context by evaluating the labeling and occlusion detection on stroke patients, where we have achieved labeling sensitivities comparable to other works between 92\,\% and 95\,\%. To the best of our knowledge, ours is the first work to address labeling and occlusion detection at once, whereby a sensitivity of 67\,\% and a specificity of 81\,\% were obtained for the latter. VirtualDSA++ also automatically segments and models the intracranial system, which we further used in a deep learning driven follow up work. We present the generic concept of iterative systematic search for pathways on all nodes of said model, which enables new interactive features. Exemplary, we derive in detail, firstly, the interactive planning of vascular interventions like the mechanical thrombectomy and secondly, the interactive suppression of vessel structures that are not of interest in diagnosing strokes (like veins). We discuss both features as well as further possibilities emerging from the proposed concept.



### Generating Topological Structure of Floorplans from Room Attributes
- **Arxiv ID**: http://arxiv.org/abs/2204.12338v1
- **DOI**: 10.1145/3512527.3531384
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12338v1)
- **Published**: 2022-04-26 14:24:58+00:00
- **Updated**: 2022-04-26 14:24:58+00:00
- **Authors**: Yin Yu, Hutchcroft Will, Khosravan Naji, Boyadzhiev Ivaylo, Fu Yun, Kang Sing Bing
- **Comment**: None
- **Journal**: None
- **Summary**: Analysis of indoor spaces requires topological information. In this paper, we propose to extract topological information from room attributes using what we call Iterative and adaptive graph Topology Learning (ITL). ITL progressively predicts multiple relations between rooms; at each iteration, it improves node embeddings, which in turn facilitates generation of a better topological graph structure. This notion of iterative improvement of node embeddings and topological graph structure is in the same spirit as \cite{chen2020iterative}. However, while \cite{chen2020iterative} computes the adjacency matrix based on node similarity, we learn the graph metric using a relational decoder to extract room correlations. Experiments using a new challenging indoor dataset validate our proposed method. Qualitative and quantitative evaluation for layout topology prediction and floorplan generation applications also demonstrate the effectiveness of ITL.



### Restricted Black-box Adversarial Attack Against DeepFake Face Swapping
- **Arxiv ID**: http://arxiv.org/abs/2204.12347v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2204.12347v1)
- **Published**: 2022-04-26 14:36:06+00:00
- **Updated**: 2022-04-26 14:36:06+00:00
- **Authors**: Junhao Dong, Yuan Wang, Jianhuang Lai, Xiaohua Xie
- **Comment**: None
- **Journal**: None
- **Summary**: DeepFake face swapping presents a significant threat to online security and social media, which can replace the source face in an arbitrary photo/video with the target face of an entirely different person. In order to prevent this fraud, some researchers have begun to study the adversarial methods against DeepFake or face manipulation. However, existing works focus on the white-box setting or the black-box setting driven by abundant queries, which severely limits the practical application of these methods. To tackle this problem, we introduce a practical adversarial attack that does not require any queries to the facial image forgery model. Our method is built on a substitute model persuing for face reconstruction and then transfers adversarial examples from the substitute model directly to inaccessible black-box DeepFake models. Specially, we propose the Transferable Cycle Adversary Generative Adversarial Network (TCA-GAN) to construct the adversarial perturbation for disrupting unknown DeepFake systems. We also present a novel post-regularization module for enhancing the transferability of generated adversarial examples. To comprehensively measure the effectiveness of our approaches, we construct a challenging benchmark of DeepFake adversarial attacks for future development. Extensive experiments impressively show that the proposed adversarial attack method makes the visual quality of DeepFake face images plummet so that they are easier to be detected by humans and algorithms. Moreover, we demonstrate that the proposed algorithm can be generalized to offer face image protection against various face translation methods.



### Causal Transportability for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.12363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12363v1)
- **Published**: 2022-04-26 15:02:11+00:00
- **Updated**: 2022-04-26 15:02:11+00:00
- **Authors**: Chengzhi Mao, Kevin Xia, James Wang, Hao Wang, Junfeng Yang, Elias Bareinboim, Carl Vondrick
- **Comment**: None
- **Journal**: None
- **Summary**: Visual representations underlie object recognition tasks, but they often contain both robust and non-robust features. Our main observation is that image classifiers may perform poorly on out-of-distribution samples because spurious correlations between non-robust features and labels can be changed in a new environment. By analyzing procedures for out-of-distribution generalization with a causal graph, we show that standard classifiers fail because the association between images and labels is not transportable across settings. However, we then show that the causal effect, which severs all sources of confounding, remains invariant across domains. This motivates us to develop an algorithm to estimate the causal effect for image classification, which is transportable (i.e., invariant) across source and target environments. Without observing additional variables, we show that we can derive an estimand for the causal effect under empirical assumptions using representations in deep models as proxies. Theoretical analysis, empirical results, and visualizations show that our approach captures causal invariances and improves overall generalization.



### ROMA: Cross-Domain Region Similarity Matching for Unpaired Nighttime Infrared to Daytime Visible Video Translation
- **Arxiv ID**: http://arxiv.org/abs/2204.12367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12367v1)
- **Published**: 2022-04-26 15:08:15+00:00
- **Updated**: 2022-04-26 15:08:15+00:00
- **Authors**: Zhenjie Yu, Kai Chen, Shuang Li, Bingfeng Han, Chi Harold Liu, Shuigen Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Infrared cameras are often utilized to enhance the night vision since the visible light cameras exhibit inferior efficacy without sufficient illumination. However, infrared data possesses inadequate color contrast and representation ability attributed to its intrinsic heat-related imaging principle. This makes it arduous to capture and analyze information for human beings, meanwhile hindering its application. Although, the domain gaps between unpaired nighttime infrared and daytime visible videos are even huger than paired ones that captured at the same time, establishing an effective translation mapping will greatly contribute to various fields. In this case, the structural knowledge within nighttime infrared videos and semantic information contained in the translated daytime visible pairs could be utilized simultaneously. To this end, we propose a tailored framework ROMA that couples with our introduced cRoss-domain regiOn siMilarity mAtching technique for bridging the huge gaps. To be specific, ROMA could efficiently translate the unpaired nighttime infrared videos into fine-grained daytime visible ones, meanwhile maintain the spatiotemporal consistency via matching the cross-domain region similarity. Furthermore, we design a multiscale region-wise discriminator to distinguish the details from synthesized visible results and real references. Extensive experiments and evaluations for specific applications indicate ROMA outperforms the state-of-the-art methods. Moreover, we provide a new and challenging dataset encouraging further research for unpaired nighttime infrared and daytime visible video translation, named InfraredCity. In particular, it consists of 9 long video clips including City, Highway and Monitor scenarios. All clips could be split into 603,142 frames in total, which are 20 times larger than the recently released daytime infrared-to-visible dataset IRVI.



### On Fragile Features and Batch Normalization in Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2204.12393v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2204.12393v1)
- **Published**: 2022-04-26 15:49:33+00:00
- **Updated**: 2022-04-26 15:49:33+00:00
- **Authors**: Nils Philipp Walter, David Stutz, Bernt Schiele
- **Comment**: None
- **Journal**: None
- **Summary**: Modern deep learning architecture utilize batch normalization (BN) to stabilize training and improve accuracy. It has been shown that the BN layers alone are surprisingly expressive. In the context of robustness against adversarial examples, however, BN is argued to increase vulnerability. That is, BN helps to learn fragile features. Nevertheless, BN is still used in adversarial training, which is the de-facto standard to learn robust features. In order to shed light on the role of BN in adversarial training, we investigate to what extent the expressiveness of BN can be used to robustify fragile features in comparison to random features. On CIFAR10, we find that adversarially fine-tuning just the BN layers can result in non-trivial adversarial robustness. Adversarially training only the BN layers from scratch, in contrast, is not able to convey meaningful adversarial robustness. Our results indicate that fragile features can be used to learn models with moderate adversarial robustness, while random features cannot



### Understanding the Impact of Edge Cases from Occluded Pedestrians for ML Systems
- **Arxiv ID**: http://arxiv.org/abs/2204.12402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12402v1)
- **Published**: 2022-04-26 16:00:42+00:00
- **Updated**: 2022-04-26 16:00:42+00:00
- **Authors**: Jens Henriksson, Christian Berger, Stig Ursing
- **Comment**: Preprint SEAA 2021
- **Journal**: None
- **Summary**: Machine learning (ML)-enabled approaches are considered a substantial support technique of detection and classification of obstacles of traffic participants in self-driving vehicles. Major breakthroughs have been demonstrated the past few years, even covering complete end-to-end data processing chain from sensory inputs through perception and planning to vehicle control of acceleration, breaking and steering. YOLO (you-only-look-once) is a state-of-the-art perception neural network (NN) architecture providing object detection and classification through bounding box estimations on camera images. As the NN is trained on well annotated images, in this paper we study the variations of confidence levels from the NN when tested on hand-crafted occlusion added to a test set. We compare regular pedestrian detection to upper and lower body detection. Our findings show that the two NN using only partial information perform similarly well like the NN for the full body when the full body NN's performance is 0.75 or better. Furthermore and as expected, the network, which is only trained on the lower half body is least prone to disturbances from occlusions of the upper half and vice versa.



### A survey on attention mechanisms for medical applications: are we moving towards better algorithms?
- **Arxiv ID**: http://arxiv.org/abs/2204.12406v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, 68T07, 68T45, I.2.10; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2204.12406v1)
- **Published**: 2022-04-26 16:04:19+00:00
- **Updated**: 2022-04-26 16:04:19+00:00
- **Authors**: Tiago Gonçalves, Isabel Rio-Torto, Luís F. Teixeira, Jaime S. Cardoso
- **Comment**: Pre-print submitted to Nature Scientific Reports
- **Journal**: None
- **Summary**: The increasing popularity of attention mechanisms in deep learning algorithms for computer vision and natural language processing made these models attractive to other research domains. In healthcare, there is a strong need for tools that may improve the routines of the clinicians and the patients. Naturally, the use of attention-based algorithms for medical applications occurred smoothly. However, being healthcare a domain that depends on high-stake decisions, the scientific community must ponder if these high-performing algorithms fit the needs of medical applications. With this motto, this paper extensively reviews the use of attention mechanisms in machine learning (including Transformers) for several medical applications. This work distinguishes itself from its predecessors by proposing a critical analysis of the claims and potentialities of attention mechanisms presented in the literature through an experimental case study on medical image classification with three different use cases. These experiments focus on the integrating process of attention mechanisms into established deep learning architectures, the analysis of their predictive power, and a visual assessment of their saliency maps generated by post-hoc explanation methods. This paper concludes with a critical analysis of the claims and potentialities presented in the literature about attention mechanisms and proposes future research lines in medical applications that may benefit from these frameworks.



### MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2204.12408v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12408v1)
- **Published**: 2022-04-26 16:06:31+00:00
- **Updated**: 2022-04-26 16:06:31+00:00
- **Authors**: Yuying Ge, Yixiao Ge, Xihui Liu, Alex Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie, Ping Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Dominant pre-training work for video-text retrieval mainly adopt the "dual-encoder" architectures to enable efficient retrieval, where two separate encoders are used to contrast global video and text representations, but ignore detailed local semantics. The recent success of image BERT pre-training with masked visual modeling that promotes the learning of local visual context, motivates a possible solution to address the above limitation. In this work, we for the first time investigate masked visual modeling in video-text pre-training with the "dual-encoder" architecture. We perform Masked visual modeling with Injected LanguagE Semantics (MILES) by employing an extra snapshot video encoder as an evolving "tokenizer" to produce reconstruction targets for masked video patch prediction. Given the corrupted video, the video encoder is trained to recover text-aligned features of the masked patches via reasoning with the visible regions along the spatial and temporal dimensions, which enhances the discriminativeness of local visual features and the fine-grained cross-modality alignment. Our method outperforms state-of-the-art methods for text-to-video retrieval on four datasets with both zero-shot and fine-tune evaluation protocols. Our approach also surpasses the baseline models significantly on zero-shot action recognition, which can be cast as video-to-text retrieval.



### RadioPathomics: Multimodal Learning in Non-Small Cell Lung Cancer for Adaptive Radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/2204.12423v1
- **DOI**: 10.1109/ACCESS.2023.3275126
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12423v1)
- **Published**: 2022-04-26 16:32:52+00:00
- **Updated**: 2022-04-26 16:32:52+00:00
- **Authors**: Matteo Tortora, Ermanno Cordelli, Rosa Sicilia, Lorenzo Nibid, Edy Ippolito, Giuseppe Perrone, Sara Ramella, Paolo Soda
- **Comment**: None
- **Journal**: None
- **Summary**: The current cancer treatment practice collects multimodal data, such as radiology images, histopathology slides, genomics and clinical data. The importance of these data sources taken individually has fostered the recent raise of radiomics and pathomics, i.e. the extraction of quantitative features from radiology and histopathology images routinely collected to predict clinical outcomes or to guide clinical decisions using artificial intelligence algorithms. Nevertheless, how to combine them into a single multimodal framework is still an open issue. In this work we therefore develop a multimodal late fusion approach that combines hand-crafted features computed from radiomics, pathomics and clinical data to predict radiation therapy treatment outcomes for non-small-cell lung cancer patients. Within this context, we investigate eight different late fusion rules (i.e. product, maximum, minimum, mean, decision template, Dempster-Shafer, majority voting, and confidence rule) and two patient-wise aggregation rules leveraging the richness of information given by computer tomography images and whole-slide scans. The experiments in leave-one-patient-out cross-validation on an in-house cohort of 33 patients show that the proposed multimodal paradigm with an AUC equal to $90.9\%$ outperforms each unimodal approach, suggesting that data integration can advance precision medicine. As a further contribution, we also compare the hand-crafted representations with features automatically computed by deep networks, and the late fusion paradigm with early fusion, another popular multimodal approach. In both cases, the experiments show that the proposed multimodal approach provides the best results.



### Understanding The Robustness in Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2204.12451v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12451v4)
- **Published**: 2022-04-26 17:16:32+00:00
- **Updated**: 2022-11-08 15:52:39+00:00
- **Authors**: Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Anima Anandkumar, Jiashi Feng, Jose M. Alvarez
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies show that Vision Transformers(ViTs) exhibit strong robustness against various corruptions. Although this property is partly attributed to the self-attention mechanism, there is still a lack of systematic understanding. In this paper, we examine the role of self-attention in learning robust representations. Our study is motivated by the intriguing properties of the emerging visual grouping in Vision Transformers, which indicates that self-attention may promote robustness through improved mid-level representations. We further propose a family of fully attentional networks (FANs) that strengthen this capability by incorporating an attentional channel processing design. We validate the design comprehensively on various hierarchical backbones. Our model achieves a state-of-the-art 87.1% accuracy and 35.8% mCE on ImageNet-1k and ImageNet-C with 76.8M parameters. We also demonstrate state-of-the-art accuracy and robustness in two downstream tasks: semantic segmentation and object detection. Code is available at: https://github.com/NVlabs/FAN.



### Differentiable Zooming for Multiple Instance Learning on Whole-Slide Images
- **Arxiv ID**: http://arxiv.org/abs/2204.12454v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12454v4)
- **Published**: 2022-04-26 17:20:50+00:00
- **Updated**: 2022-07-26 14:51:02+00:00
- **Authors**: Kevin Thandiackal, Boqi Chen, Pushpak Pati, Guillaume Jaume, Drew F. K. Williamson, Maria Gabrani, Orcun Goksel
- **Comment**: Typos corrected; Changed dataset name from INSEC to CRC upon dataset
  creators' request; Update affiliation and fix typos;
- **Journal**: None
- **Summary**: Multiple Instance Learning (MIL) methods have become increasingly popular for classifying giga-pixel sized Whole-Slide Images (WSIs) in digital pathology. Most MIL methods operate at a single WSI magnification, by processing all the tissue patches. Such a formulation induces high computational requirements, and constrains the contextualization of the WSI-level representation to a single scale. A few MIL methods extend to multiple scales, but are computationally more demanding. In this paper, inspired by the pathological diagnostic process, we propose ZoomMIL, a method that learns to perform multi-level zooming in an end-to-end manner. ZoomMIL builds WSI representations by aggregating tissue-context information from multiple magnifications. The proposed method outperforms the state-of-the-art MIL methods in WSI classification on two large datasets, while significantly reducing the computational demands with regard to Floating-Point Operations (FLOPs) and processing time by up to 40x.



### Focal Sparse Convolutional Networks for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.12463v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.12463v1)
- **Published**: 2022-04-26 17:34:10+00:00
- **Updated**: 2022-04-26 17:34:10+00:00
- **Authors**: Yukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, Jiaya Jia
- **Comment**: CVPR 2022 Oral. Code is at
  http://github.com/dvlab-research/FocalsConv
- **Journal**: None
- **Summary**: Non-uniformed 3D sparse data, e.g., point clouds or voxels in different spatial positions, make contribution to the task of 3D object detection in different ways. Existing basic components in sparse convolutional networks (Sparse CNNs) process all sparse data, regardless of regular or submanifold sparse convolution. In this paper, we introduce two new modules to enhance the capability of Sparse CNNs, both are based on making feature sparsity learnable with position-wise importance prediction. They are focal sparse convolution (Focals Conv) and its multi-modal variant of focal sparse convolution with fusion, or Focals Conv-F for short. The new modules can readily substitute their plain counterparts in existing Sparse CNNs and be jointly trained in an end-to-end fashion. For the first time, we show that spatially learnable sparsity in sparse convolution is essential for sophisticated 3D object detection. Extensive experiments on the KITTI, nuScenes and Waymo benchmarks validate the effectiveness of our approach. Without bells and whistles, our results outperform all existing single-model entries on the nuScenes test benchmark at the paper submission time. Code and models are at https://github.com/dvlab-research/FocalsConv.



### Meta-free few-shot learning via representation learning with weight averaging
- **Arxiv ID**: http://arxiv.org/abs/2204.12466v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12466v2)
- **Published**: 2022-04-26 17:36:34+00:00
- **Updated**: 2022-04-30 22:37:22+00:00
- **Authors**: Kuilin Chen, Chi-Guhn Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies on few-shot classification using transfer learning pose challenges to the effectiveness and efficiency of episodic meta-learning algorithms. Transfer learning approaches are a natural alternative, but they are restricted to few-shot classification. Moreover, little attention has been on the development of probabilistic models with well-calibrated uncertainty from few-shot samples, except for some Bayesian episodic learning algorithms. To tackle the aforementioned issues, we propose a new transfer learning method to obtain accurate and reliable models for few-shot regression and classification. The resulting method does not require episodic meta-learning and is called meta-free representation learning (MFRL). MFRL first finds low-rank representation generalizing well on meta-test tasks. Given the learned representation, probabilistic linear models are fine-tuned with few-shot samples to obtain models with well-calibrated uncertainty. The proposed method not only achieves the highest accuracy on a wide range of few-shot learning benchmark datasets but also correctly quantifies the prediction uncertainty. In addition, weight averaging and temperature scaling are effective in improving the accuracy and reliability of few-shot learning in existing meta-learning algorithms with a wide range of learning paradigms and model architectures.



### Coarse-to-fine Q-attention with Tree Expansion
- **Arxiv ID**: http://arxiv.org/abs/2204.12471v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.12471v2)
- **Published**: 2022-04-26 17:41:28+00:00
- **Updated**: 2022-05-02 15:38:48+00:00
- **Authors**: Stephen James, Pieter Abbeel
- **Comment**: Project page and code: https://sites.google.com/view/q-attention-qte
- **Journal**: None
- **Summary**: Coarse-to-fine Q-attention enables sample-efficient robot manipulation by discretizing the translation space in a coarse-to-fine manner, where the resolution gradually increases at each layer in the hierarchy. Although effective, Q-attention suffers from "coarse ambiguity" - when voxelization is significantly coarse, it is not feasible to distinguish similar-looking objects without first inspecting at a finer resolution. To combat this, we propose to envision Q-attention as a tree that can be expanded and used to accumulate value estimates across the top-k voxels at each Q-attention depth. When our extension, Q-attention with Tree Expansion (QTE), replaces standard Q-attention in the Attention-driven Robot Manipulation (ARM) system, we are able to accomplish a larger set of tasks; especially on those that suffer from "coarse ambiguity". In addition to evaluating our approach across 12 RLBench tasks, we also show that the improved performance is visible in a real-world task involving small objects.



### ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2204.12484v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12484v3)
- **Published**: 2022-04-26 17:55:04+00:00
- **Updated**: 2022-10-13 01:53:23+00:00
- **Authors**: Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao
- **Comment**: Neurips 2022. 81.1 mAP on MS COCO Keypoint Detection test-dev set.
  V2: Update Multi-task training results: 92.8 AP on OCHuman, 78.3 AP on
  CrowdPose, 94.3 PCKh on MPII, and 43.2 AP on AI Challenger
- **Journal**: None
- **Summary**: Although no specific domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for pose estimation tasks. In this paper, we show the surprisingly good capabilities of plain vision transformers for pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm, and transferability of knowledge between models, through a simple baseline model called ViTPose. Specifically, ViTPose employs plain and non-hierarchical vision transformers as backbones to extract features for a given person instance and a lightweight decoder for pose estimation. It can be scaled up from 100M to 1B parameters by taking the advantages of the scalable model capacity and high parallelism of transformers, setting a new Pareto front between throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, pre-training and finetuning strategy, as well as dealing with multiple pose tasks. We also empirically demonstrate that the knowledge of large ViTPose models can be easily transferred to small ones via a simple knowledge token. Experimental results show that our basic ViTPose model outperforms representative methods on the challenging MS COCO Keypoint Detection benchmark, while the largest model sets a new state-of-the-art. The code and models are available at https://github.com/ViTAE-Transformer/ViTPose.



### Sound Localization by Self-Supervised Time Delay Estimation
- **Arxiv ID**: http://arxiv.org/abs/2204.12489v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2204.12489v3)
- **Published**: 2022-04-26 17:59:01+00:00
- **Updated**: 2023-01-28 21:19:30+00:00
- **Authors**: Ziyang Chen, David F. Fouhey, Andrew Owens
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Sounds reach one microphone in a stereo pair sooner than the other, resulting in an interaural time delay that conveys their directions. Estimating a sound's time delay requires finding correspondences between the signals recorded by each microphone. We propose to learn these correspondences through self-supervision, drawing on recent techniques from visual tracking. We adapt the contrastive random walk of Jabri et al. to learn a cycle-consistent representation from unlabeled stereo sounds, resulting in a model that performs on par with supervised methods on "in the wild" internet recordings. We also propose a multimodal contrastive learning model that solves a visually-guided localization task: estimating the time delay for a particular person in a multi-speaker mixture, given a visual representation of their face. Project site: https://ificl.github.io/stereocrw/



### From One Hand to Multiple Hands: Imitation Learning for Dexterous Manipulation from Single-Camera Teleoperation
- **Arxiv ID**: http://arxiv.org/abs/2204.12490v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.12490v2)
- **Published**: 2022-04-26 17:59:51+00:00
- **Updated**: 2023-01-18 23:40:47+00:00
- **Authors**: Yuzhe Qin, Hao Su, Xiaolong Wang
- **Comment**: https://yzqin.github.io/dex-teleop-imitation/
- **Journal**: None
- **Summary**: We propose to perform imitation learning for dexterous manipulation with multi-finger robot hand from human demonstrations, and transfer the policy to the real robot hand. We introduce a novel single-camera teleoperation system to collect the 3D demonstrations efficiently with only an iPad and a computer. One key contribution of our system is that we construct a customized robot hand for each user in the physical simulator, which is a manipulator resembling the same kinematics structure and shape of the operator's hand. This provides an intuitive interface and avoid unstable human-robot hand retargeting for data collection, leading to large-scale and high quality data. Once the data is collected, the customized robot hand trajectories can be converted to different specified robot hands (models that are manufactured) to generate training demonstrations. With imitation learning using our data, we show large improvement over baselines with multiple complex manipulation tasks. Importantly, we show our learned policy is significantly more robust when transferring to the real robot. More videos can be found in the https://yzqin.github.io/dex-teleop-imitation .



### PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions
- **Arxiv ID**: http://arxiv.org/abs/2204.12511v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12511v2)
- **Published**: 2022-04-26 18:00:04+00:00
- **Updated**: 2022-05-10 18:51:24+00:00
- **Authors**: Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Xiaojie Shi, Shuyang Cheng, Dragomir Anguelov
- **Comment**: Add ablation studies on COCO detection using RetinaNet (Section 8)
- **Journal**: International Conference on Learning Representations. 2021
- **Summary**: Cross-entropy loss and focal loss are the most common choices when training deep neural networks for classification problems. Generally speaking, however, a good loss function can take on much more flexible forms, and should be tailored for different tasks and datasets. Motivated by how functions can be approximated via Taylor expansion, we propose a simple framework, named PolyLoss, to view and design loss functions as a linear combination of polynomial functions. Our PolyLoss allows the importance of different polynomial bases to be easily adjusted depending on the targeting tasks and datasets, while naturally subsuming the aforementioned cross-entropy loss and focal loss as special cases. Extensive experimental results show that the optimal choice within the PolyLoss is indeed dependent on the task and dataset. Simply by introducing one extra hyperparameter and adding one line of code, our Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D image classification, instance segmentation, object detection, and 3D object detection tasks, sometimes by a large margin.



### Coupled Iterative Refinement for 6D Multi-Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2204.12516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12516v1)
- **Published**: 2022-04-26 18:00:08+00:00
- **Updated**: 2022-04-26 18:00:08+00:00
- **Authors**: Lahav Lipson, Zachary Teed, Ankit Goyal, Jia Deng
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: We address the task of 6D multi-object pose: given a set of known 3D objects and an RGB or RGB-D input image, we detect and estimate the 6D pose of each object. We propose a new approach to 6D object pose estimation which consists of an end-to-end differentiable architecture that makes use of geometric knowledge. Our approach iteratively refines both pose and correspondence in a tightly coupled manner, allowing us to dynamically remove outliers to improve accuracy. We use a novel differentiable layer to perform pose refinement by solving an optimization problem we refer to as Bidirectional Depth-Augmented Perspective-N-Point (BD-PnP). Our method achieves state-of-the-art accuracy on standard 6D Object Pose benchmarks. Code is available at https://github.com/princeton-vl/Coupled-Iterative-Refinement.



### Leveraging Unlabeled Data for Sketch-based Understanding
- **Arxiv ID**: http://arxiv.org/abs/2204.12522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12522v1)
- **Published**: 2022-04-26 18:13:30+00:00
- **Updated**: 2022-04-26 18:13:30+00:00
- **Authors**: Javier Morales, Nils Murrugarra-Llerena, Jose M. Saavedra
- **Comment**: SketchDL at CVPR 2022
- **Journal**: None
- **Summary**: Sketch-based understanding is a critical component of human cognitive learning and is a primitive communication means between humans. This topic has recently attracted the interest of the computer vision community as sketching represents a powerful tool to express static objects and dynamic scenes. Unfortunately, despite its broad application domains, the current sketch-based models strongly rely on labels for supervised training, ignoring knowledge from unlabeled data, thus limiting the underlying generalization and the applicability. Therefore, we present a study about the use of unlabeled data to improve a sketch-based model. To this end, we evaluate variations of VAE and semi-supervised VAE, and present an extension of BYOL to deal with sketches. Our results show the superiority of sketch-BYOL, which outperforms other self-supervised approaches increasing the retrieval performance for known and unknown categories. Furthermore, we show how other tasks can benefit from our proposal.



### Expanding the Latent Space of StyleGAN for Real Face Editing
- **Arxiv ID**: http://arxiv.org/abs/2204.12530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12530v1)
- **Published**: 2022-04-26 18:27:53+00:00
- **Updated**: 2022-04-26 18:27:53+00:00
- **Authors**: Yin Yu, Ghasedi Kamran, Wu HsiangTao, Yang Jiaolong, Tong Xi, Fu Yun
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, a surge of face editing techniques have been proposed to employ the pretrained StyleGAN for semantic manipulation. To successfully edit a real image, one must first convert the input image into StyleGAN's latent variables. However, it is still challenging to find latent variables, which have the capacity for preserving the appearance of the input subject (e.g., identity, lighting, hairstyles) as well as enabling meaningful manipulations. In this paper, we present a method to expand the latent space of StyleGAN with additional content features to break down the trade-off between low-distortion and high-editability. Specifically, we proposed a two-branch model, where the style branch first tackles the entanglement issue by the sparse manipulation of latent codes, and the content branch then mitigates the distortion issue by leveraging the content and appearance details from the input image. We confirm the effectiveness of our method using extensive qualitative and quantitative experiments on real face editing and reconstruction tasks.



### AccMPEG: Optimizing Video Encoding for Video Analytics
- **Arxiv ID**: http://arxiv.org/abs/2204.12534v1
- **DOI**: None
- **Categories**: **cs.NI**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.12534v1)
- **Published**: 2022-04-26 18:42:07+00:00
- **Updated**: 2022-04-26 18:42:07+00:00
- **Authors**: Kuntai Du, Qizheng Zhang, Anton Arapin, Haodong Wang, Zhengxu Xia, Junchen Jiang
- **Comment**: Accepted by MLSys 2022
- **Journal**: None
- **Summary**: With more videos being recorded by edge sensors (cameras) and analyzed by computer-vision deep neural nets (DNNs), a new breed of video streaming systems has emerged, with the goal to compress and stream videos to remote servers in real time while preserving enough information to allow highly accurate inference by the server-side DNNs. An ideal design of the video streaming system should simultaneously meet three key requirements: (1) low latency of encoding and streaming, (2) high accuracy of server-side DNNs, and (3) low compute overheads on the camera. Unfortunately, despite many recent efforts, such video streaming system has hitherto been elusive, especially when serving advanced vision tasks such as object detection or semantic segmentation. This paper presents AccMPEG, a new video encoding and streaming system that meets all the three requirements. The key is to learn how much the encoding quality at each (16x16) macroblock can influence the server-side DNN accuracy, which we call accuracy gradient. Our insight is that these macroblock-level accuracy gradient can be inferred with sufficient precision by feeding the video frames through a cheap model. AccMPEG provides a suite of techniques that, given a new server-side DNN, can quickly create a cheap model to infer the accuracy gradient on any new frame in near realtime. Our extensive evaluation of AccMPEG on two types of edge devices (one Intel Xeon Silver 4100 CPU or NVIDIA Jetson Nano) and three vision tasks (six recent pre-trained DNNs) shows that AccMPEG (with the same camera-side compute resources) can reduce the end-to-end inference delay by 10-43% without hurting accuracy compared to the state-of-the-art baselines



### Building Change Detection using Multi-Temporal Airborne LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/2204.12535v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12535v1)
- **Published**: 2022-04-26 18:43:29+00:00
- **Updated**: 2022-04-26 18:43:29+00:00
- **Authors**: Ritu Yadav, Andrea Nascetti, Yifang Ban
- **Comment**: Accepted in ISPRS 2022
- **Journal**: None
- **Summary**: Building change detection is essential for monitoring urbanization, disaster assessment, urban planning and frequently updating the maps. 3D structure information from airborne light detection and ranging (LiDAR) is very effective for detecting urban changes. But the 3D point cloud from airborne LiDAR(ALS) holds an enormous amount of unordered and irregularly sparse information. Handling such data is tricky and consumes large memory for processing. Most of this information is not necessary when we are looking for a particular type of urban change. In this study, we propose an automatic method that reduces the 3D point clouds into a much smaller representation without losing the necessary information required for detecting Building changes. The method utilizes the Deep Learning(DL) model U-Net for segmenting the buildings from the background. Produced segmentation maps are then processed further for detecting changes and the results are refined using morphological methods. For the change detection task, we used multi-temporal airborne LiDAR data. The data is acquired over Stockholm in the years 2017 and 2019. The changes in buildings are classified into four types: 'newly built', 'demolished', 'taller' and 'shorter'. The detected changes are visualized in one map for better interpretation.



### Multi stain graph fusion for multimodal integration in pathology
- **Arxiv ID**: http://arxiv.org/abs/2204.12541v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.12541v1)
- **Published**: 2022-04-26 18:49:24+00:00
- **Updated**: 2022-04-26 18:49:24+00:00
- **Authors**: Chaitanya Dwivedi, Shima Nofallah, Maryam Pouryahya, Janani Iyer, Kenneth Leidal, Chuhan Chung, Timothy Watkins, Andrew Billin, Robert Myers, John Abel, Ali Behrooz
- **Comment**: None
- **Journal**: None
- **Summary**: In pathology, tissue samples are assessed using multiple staining techniques to enhance contrast in unique histologic features. In this paper, we introduce a multimodal CNN-GNN based graph fusion approach that leverages complementary information from multiple non-registered histopathology images to predict pathologic scores. We demonstrate this approach in nonalcoholic steatohepatitis (NASH) by predicting CRN fibrosis stage and NAFLD Activity Score (NAS). Primary assessment of NASH typically requires liver biopsy evaluation on two histological stains: Trichrome (TC) and hematoxylin and eosin (H&E). Our multimodal approach learns to extract complementary information from TC and H&E graphs corresponding to each stain while simultaneously learning an optimal policy to combine this information. We report up to 20% improvement in predicting fibrosis stage and NAS component grades over single-stain modeling approaches, measured by computing linearly weighted Cohen's kappa between machine-derived vs. pathologist consensus scores. Broadly, this paper demonstrates the value of leveraging diverse pathology images for improved ML-powered histologic assessment.



### A Close Look into Human Activity Recognition Models using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.13589v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2204.13589v1)
- **Published**: 2022-04-26 19:43:21+00:00
- **Updated**: 2022-04-26 19:43:21+00:00
- **Authors**: Wei Zhong Tee, Rushit Dave, Naeem Seliya, Mounika Vanamala
- **Comment**: None
- **Journal**: None
- **Summary**: Human activity recognition using deep learning techniques has become increasing popular because of its high effectivity with recognizing complex tasks, as well as being relatively low in costs compared to more traditional machine learning techniques. This paper surveys some state-of-the-art human activity recognition models that are based on deep learning architecture and has layers containing Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), or a mix of more than one type for a hybrid system. The analysis outlines how the models are implemented to maximize its effectivity and some of the potential limitations it faces.



### The Influence of the Other-Race Effect on Susceptibility to Face Morphing Attacks
- **Arxiv ID**: http://arxiv.org/abs/2204.12591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12591v1)
- **Published**: 2022-04-26 20:59:08+00:00
- **Updated**: 2022-04-26 20:59:08+00:00
- **Authors**: Snipta Mallick, Geraldine Jeckeln, Connor J. Parde, Carlos D. Castillo, Alice J. O'Toole
- **Comment**: 4 figures, 11 pages
- **Journal**: None
- **Summary**: Facial morphs created between two identities resemble both of the faces used to create the morph. Consequently, humans and machines are prone to mistake morphs made from two identities for either of the faces used to create the morph. This vulnerability has been exploited in "morph attacks" in security scenarios. Here, we asked whether the "other-race effect" (ORE) -- the human advantage for identifying own- vs. other-race faces -- exacerbates morph attack susceptibility for humans. We also asked whether face-identification performance in a deep convolutional neural network (DCNN) is affected by the race of morphed faces. Caucasian (CA) and East-Asian (EA) participants performed a face-identity matching task on pairs of CA and EA face images in two conditions. In the morph condition, different-identity pairs consisted of an image of identity "A" and a 50/50 morph between images of identity "A" and "B". In the baseline condition, morphs of different identities never appeared. As expected, morphs were identified mistakenly more often than original face images. Moreover, CA participants showed an advantage for CA faces in comparison to EA faces (a partial ORE). Of primary interest, morph identification was substantially worse for cross-race faces than for own-race faces. Similar to humans, the DCNN performed more accurately for original face images than for morphed image pairs. Notably, the deep network proved substantially more accurate than humans in both cases. The results point to the possibility that DCNNs might be useful for improving face identification accuracy when morphed faces are presented. They also indicate the significance of the ORE in morph attack susceptibility in applied settings.



### GPUNet: Searching the Deployable Convolution Neural Networks for GPUs
- **Arxiv ID**: http://arxiv.org/abs/2205.00841v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.00841v1)
- **Published**: 2022-04-26 21:48:35+00:00
- **Updated**: 2022-04-26 21:48:35+00:00
- **Authors**: Linnan Wang, Chenhan Yu, Satish Salian, Slawomir Kierat, Szymon Migacz, Alex Fit Florea
- **Comment**: to appear at CVPR-2022
- **Journal**: None
- **Summary**: Customizing Convolution Neural Networks (CNN) for production use has been a challenging task for DL practitioners. This paper intends to expedite the model customization with a model hub that contains the optimized models tiered by their inference latency using Neural Architecture Search (NAS). To achieve this goal, we build a distributed NAS system to search on a novel search space that consists of prominent factors to impact latency and accuracy. Since we target GPU, we name the NAS optimized models as GPUNet, which establishes a new SOTA Pareto frontier in inference latency and accuracy. Within 1$ms$, GPUNet is 2x faster than EfficientNet-X and FBNetV3 with even better accuracy. We also validate GPUNet on detection tasks, and GPUNet consistently outperforms EfficientNet-X and FBNetV3 on COCO detection tasks in both latency and accuracy. All of these data validate that our NAS system is effective and generic to handle different design tasks. With this NAS system, we expand GPUNet to cover a wide range of latency targets such that DL practitioners can deploy our models directly in different scenarios.



### OHM: GPU Based Occupancy Map Generation
- **Arxiv ID**: http://arxiv.org/abs/2206.06079v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, I.2.9 Robotics
- **Links**: [PDF](http://arxiv.org/pdf/2206.06079v1)
- **Published**: 2022-04-26 22:44:41+00:00
- **Updated**: 2022-04-26 22:44:41+00:00
- **Authors**: Kazys Stepanas, Jason Williams, Emili Hernández, Fabio Ruetz, Thomas Hines
- **Comment**: Under review
- **Journal**: None
- **Summary**: Occupancy grid maps (OGMs) are fundamental to most systems for autonomous robotic navigation. However, CPU-based implementations struggle to keep up with data rates from modern 3D lidar sensors, and provide little capacity for modern extensions which maintain richer voxel representations. This paper presents OHM, our open source, GPU-based OGM framework. We show how the algorithms can be mapped to GPU resources, resolving difficulties with contention to obtain a successful implementation. The implementation supports many modern OGM algorithms including NDT-OM, NDT-TM, decay-rate and TSDF. A thorough performance evaluation is presented based on tracked and quadruped UGV platforms and UAVs, and data sets from both outdoor and subterranean environments. The results demonstrate excellent performance improvements both offline, and for online processing in embedded platforms. Finally, we describe how OHM was a key enabler for the UGV navigation solution for our entry in the DARPA Subterranean Challenge, which placed second at the Final Event.



### Evaluation of Self-taught Learning-based Representations for Facial Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.12624v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.12624v1)
- **Published**: 2022-04-26 22:48:15+00:00
- **Updated**: 2022-04-26 22:48:15+00:00
- **Authors**: Bruna Delazeri, Leonardo L. Veras, Alceu de S. Britto Jr., Jean Paul Barddal, Alessandro L. Koerich
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: This work describes different strategies to generate unsupervised representations obtained through the concept of self-taught learning for facial emotion recognition (FER). The idea is to create complementary representations promoting diversity by varying the autoencoders' initialization, architecture, and training data. SVM, Bagging, Random Forest, and a dynamic ensemble selection method are evaluated as final classification methods. Experimental results on Jaffe and Cohn-Kanade datasets using a leave-one-subject-out protocol show that FER methods based on the proposed diverse representations compare favorably against state-of-the-art approaches that also explore unsupervised feature learning.



