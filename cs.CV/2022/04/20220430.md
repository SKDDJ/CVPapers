# Arxiv Papers in cs.CV on 2022-04-30
### Unsupervised Contrastive Learning based Transformer for Lung Nodule Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.00122v1
- **DOI**: 10.1088/1361-6560/ac92ba
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.00122v1)
- **Published**: 2022-04-30 01:19:00+00:00
- **Updated**: 2022-04-30 01:19:00+00:00
- **Authors**: Chuang Niu, Ge Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Early detection of lung nodules with computed tomography (CT) is critical for the longer survival of lung cancer patients and better quality of life. Computer-aided detection/diagnosis (CAD) is proven valuable as a second or concurrent reader in this context. However, accurate detection of lung nodules remains a challenge for such CAD systems and even radiologists due to not only the variability in size, location, and appearance of lung nodules but also the complexity of lung structures. This leads to a high false-positive rate with CAD, compromising its clinical efficacy. Motivated by recent computer vision techniques, here we present a self-supervised region-based 3D transformer model to identify lung nodules among a set of candidate regions. Specifically, a 3D vision transformer (ViT) is developed that divides a CT image volume into a sequence of non-overlap cubes, extracts embedding features from each cube with an embedding layer, and analyzes all embedding features with a self-attention mechanism for the prediction. To effectively train the transformer model on a relatively small dataset, the region-based contrastive learning method is used to boost the performance by pre-training the 3D transformer with public CT images. Our experiments show that the proposed method can significantly improve the performance of lung nodule screening in comparison with the commonly used 3D convolutional neural networks.



### Gaze-enhanced Crossmodal Embeddings for Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.00129v1
- **DOI**: 10.1145/3530879
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.00129v1)
- **Published**: 2022-04-30 02:05:39+00:00
- **Updated**: 2022-04-30 02:05:39+00:00
- **Authors**: Ahmed Abdou, Ekta Sood, Philipp MÃ¼ller, Andreas Bulling
- **Comment**: None
- **Journal**: None
- **Summary**: Emotional expressions are inherently multimodal -- integrating facial behavior, speech, and gaze -- but their automatic recognition is often limited to a single modality, e.g. speech during a phone call. While previous work proposed crossmodal emotion embeddings to improve monomodal recognition performance, despite its importance, an explicit representation of gaze was not included. We propose a new approach to emotion recognition that incorporates an explicit representation of gaze in a crossmodal emotion embedding framework. We show that our method outperforms the previous state of the art for both audio-only and video-only emotion classification on the popular One-Minute Gradual Emotion Recognition dataset. Furthermore, we report extensive ablation experiments and provide detailed insights into the performance of different state-of-the-art gaze representations and integration strategies. Our results not only underline the importance of gaze for emotion recognition but also demonstrate a practical and highly effective approach to leveraging gaze information for this task.



### Learn to Understand Negation in Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2205.00132v2
- **DOI**: 10.1145/3503161.3547968
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.00132v2)
- **Published**: 2022-04-30 02:22:18+00:00
- **Updated**: 2022-07-13 11:51:28+00:00
- **Authors**: Ziyue Wang, Aozhu Chen, Fan Hu, Xirong Li
- **Comment**: Accepted by ACMMM2022
- **Journal**: None
- **Summary**: Negation is a common linguistic skill that allows human to express what we do NOT want. Naturally, one might expect video retrieval to support natural-language queries with negation, e.g., finding shots of kids sitting on the floor and not playing with a dog. However, the state-of-the-art deep learning based video retrieval models lack such ability, as they are typically trained on video description datasets such as MSR-VTT and VATEX that lack negated descriptions. Their retrieved results basically ignore the negator in the sample query, incorrectly returning videos showing kids playing with dog. This paper presents the first study on learning to understand negation in video retrieval and make contributions as follows. By re-purposing two existing datasets (MSR-VTT and VATEX), we propose a new evaluation protocol for video retrieval with negation. We propose a learning based method for training a negation-aware video retrieval model. The key idea is to first construct a soft negative caption for a specific training video by partially negating its original caption, and then compute a bidirectionally constrained loss on the triplet. This auxiliary loss is weightedly added to a standard retrieval loss. Experiments on the re-purposed benchmarks show that re-training the CLIP (Contrastive Language-Image Pre-Training) model by the proposed method clearly improves its ability to handle queries with negation. In addition, the model performance on the original benchmarks is also improved.



### Multimodal Representation Learning With Text and Images
- **Arxiv ID**: http://arxiv.org/abs/2205.00142v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.00142v1)
- **Published**: 2022-04-30 03:25:01+00:00
- **Updated**: 2022-04-30 03:25:01+00:00
- **Authors**: Aishwarya Jayagopal, Ankireddy Monica Aiswarya, Ankita Garg, Srinivasan Kolumam Nandakumar
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, multimodal AI has seen an upward trend as researchers are integrating data of different types such as text, images, speech into modelling to get the best results. This project leverages multimodal AI and matrix factorization techniques for representation learning, on text and image data simultaneously, thereby employing the widely used techniques of Natural Language Processing (NLP) and Computer Vision. The learnt representations are evaluated using downstream classification and regression tasks. The methodology adopted can be extended beyond the scope of this project as it uses Auto-Encoders for unsupervised representation learning.



### Look Closer to Supervise Better: One-Shot Font Generation via Component-Based Discriminator
- **Arxiv ID**: http://arxiv.org/abs/2205.00146v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00146v2)
- **Published**: 2022-04-30 03:41:49+00:00
- **Updated**: 2022-05-08 15:39:53+00:00
- **Authors**: Yuxin Kong, Canjie Luo, Weihong Ma, Qiyuan Zhu, Shenggao Zhu, Nicholas Yuan, Lianwen Jin
- **Comment**: Accepted by CVPR2022(oral)
- **Journal**: None
- **Summary**: Automatic font generation remains a challenging research issue due to the large amounts of characters with complicated structures. Typically, only a few samples can serve as the style/content reference (termed few-shot learning), which further increases the difficulty to preserve local style patterns or detailed glyph structures. We investigate the drawbacks of previous studies and find that a coarse-grained discriminator is insufficient for supervising a font generator. To this end, we propose a novel Component-Aware Module (CAM), which supervises the generator to decouple content and style at a more fine-grained level, i.e., the component level. Different from previous studies struggling to increase the complexity of generators, we aim to perform more effective supervision for a relatively simple generator to achieve its full potential, which is a brand new perspective for font generation. The whole framework achieves remarkable results by coupling component-level supervision with adversarial learning, hence we call it Component-Guided GAN, shortly CG-GAN. Extensive experiments show that our approach outperforms state-of-the-art one-shot font generation methods. Furthermore, it can be applied to handwritten word synthesis and scene text image editing, suggesting the generalization of our approach.



### AnimalTrack: A Benchmark for Multi-Animal Tracking in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2205.00158v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00158v2)
- **Published**: 2022-04-30 04:23:59+00:00
- **Updated**: 2022-11-08 15:50:07+00:00
- **Authors**: Libo Zhang, Junyuan Gao, Zhen Xiao, Heng Fan
- **Comment**: Tech. report
- **Journal**: None
- **Summary**: Multi-animal tracking (MAT), a multi-object tracking (MOT) problem, is crucial for animal motion and behavior analysis and has many crucial applications such as biology, ecology and animal conservation. Despite its importance, MAT is largely under-explored compared to other MOT problems such as multi-human tracking due to the scarcity of dedicated benchmarks. To address this problem, we introduce AnimalTrack, a dedicated benchmark for multi-animal tracking in the wild. Specifically, AnimalTrack consists of 58 sequences from a diverse selection of 10 common animal categories. On average, each sequence comprises of 33 target objects for tracking. In order to ensure high quality, every frame in AnimalTrack is manually labeled with careful inspection and refinement. To our best knowledge, AnimalTrack is the first benchmark dedicated to multi-animal tracking. In addition, to understand how existing MOT algorithms perform on AnimalTrack and provide baselines for future comparison, we extensively evaluate 14 state-of-the-art representative trackers. The evaluation results demonstrate that, not surprisingly, most of these trackers become degenerated due to the differences between pedestrians and animals in various aspects (e.g., pose, motion, and appearance), and more efforts are desired to improve multi-animal tracking. We hope that AnimalTrack together with evaluation and analysis will foster further progress on multi-animal tracking. The dataset and evaluation as well as our analysis will be made available at https://hengfan2010.github.io/projects/AnimalTrack/.



### SVTR: Scene Text Recognition with a Single Visual Model
- **Arxiv ID**: http://arxiv.org/abs/2205.00159v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00159v2)
- **Published**: 2022-04-30 04:37:01+00:00
- **Updated**: 2022-05-23 05:52:33+00:00
- **Authors**: Yongkun Du, Zhineng Chen, Caiyan Jia, Xiaoting Yin, Tianlun Zheng, Chenxia Li, Yuning Du, Yu-Gang Jiang
- **Comment**: Accepted by IJCAI 2022
- **Journal**: None
- **Summary**: Dominant scene text recognition models commonly contain two building blocks, a visual model for feature extraction and a sequence model for text transcription. This hybrid architecture, although accurate, is complex and less efficient. In this study, we propose a Single Visual model for Scene Text recognition within the patch-wise image tokenization framework, which dispenses with the sequential modeling entirely. The method, termed SVTR, firstly decomposes an image text into small patches named character components. Afterward, hierarchical stages are recurrently carried out by component-level mixing, merging and/or combining. Global and local mixing blocks are devised to perceive the inter-character and intra-character patterns, leading to a multi-grained character component perception. Thus, characters are recognized by a simple linear prediction. Experimental results on both English and Chinese scene text recognition tasks demonstrate the effectiveness of SVTR. SVTR-L (Large) achieves highly competitive accuracy in English and outperforms existing methods by a large margin in Chinese, while running faster. In addition, SVTR-T (Tiny) is an effective and much smaller model, which shows appealing speed at inference. The code is publicly available at https://github.com/PaddlePaddle/PaddleOCR.



### Elucidating Meta-Structures of Noisy Labels in Semantic Segmentation by Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.00160v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00160v3)
- **Published**: 2022-04-30 04:54:31+00:00
- **Updated**: 2022-10-08 00:54:27+00:00
- **Authors**: Yaoru Luo, Guole Liu, Yuanhao Guo, Ge Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised training of deep neural networks (DNNs) by noisy labels has been studied extensively in image classification but much less in image segmentation. Our understanding of the learning behavior of DNNs trained by noisy segmentation labels remains limited. We address this deficiency in both binary segmentation of biological microscopy images and multi-class segmentation of natural images. We classify segmentation labels according to their noise transition matrices (NTMs) and compare performance of DNNs trained by different types of labels. When we randomly sample a small fraction (e.g., 10%) or flip a large fraction (e.g., 90%) of the ground-truth labels to train DNNs, their segmentation performance remains largely unchanged. This indicates that DNNs learn structures hidden in labels rather than pixel-level labels per se in their supervised training for semantic segmentation. We call these hidden structures meta-structures. When labels with different perturbations to the meta-structures are used to train DNNs, their performance in feature extraction and segmentation degrades consistently. In contrast, addition of meta-structure information substantially improves performance of an unsupervised model in binary semantic segmentation. We formulate meta-structures mathematically as spatial density distributions. We show theoretically and experimentally how this formulation explains key observed learning behavior of DNNs.



### Towards Feature Distribution Alignment and Diversity Enhancement for Data-Free Quantization
- **Arxiv ID**: http://arxiv.org/abs/2205.00179v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00179v2)
- **Published**: 2022-04-30 06:58:56+00:00
- **Updated**: 2022-12-19 13:51:52+00:00
- **Authors**: Yangcheng Gao, Zhao Zhang, Richang Hong, Haijun Zhang, Jicong Fan, Shuicheng Yan
- **Comment**: Please cite this work as: Yangcheng Gao, Zhao Zhang, Richang Hong,
  Haijun Zhang, Jicong Fan and Shuicheng Yan, "Towards Feature Distribution
  Alignment and Diversity Enhancement for Data-Free Quantization," In:
  Proceedings of the 22nd IEEE International Conference on Data Mining (ICDM),
  Orlando, FL, USA, pp.1-10, Aug 2022
- **Journal**: None
- **Summary**: To obtain lower inference latency and less memory footprint of deep neural networks, model quantization has been widely employed in deep model deployment, by converting the floating points to low-precision integers. However, previous methods (such as quantization aware training and post training quantization) require original data for the fine-tuning or calibration of quantized model, which makes them inapplicable to the cases that original data are not accessed due to privacy or security. This gives birth to the data-free quantization method with synthetic data generation. While current data-free quantization methods still suffer from severe performance degradation when quantizing a model into lower bit, caused by the low inter-class separability of semantic features. To this end, we propose a new and effective data-free quantization method termed ClusterQ, which utilizes the feature distribution alignment for synthetic data generation. To obtain high inter-class separability of semantic features, we cluster and align the feature distribution statistics to imitate the distribution of real data, so that the performance degradation is alleviated. Moreover, we incorporate the diversity enhancement to solve class-wise mode collapse. We also employ the exponential moving average to update the centroid of each cluster for further feature distribution improvement. Extensive experiments based on different deep models (e.g., ResNet-18 and MobileNet-V2) over the ImageNet dataset demonstrate that our proposed ClusterQ model obtains state-of-the-art performance.



### HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding
- **Arxiv ID**: http://arxiv.org/abs/2205.09753v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.09753v2)
- **Published**: 2022-04-30 07:08:30+00:00
- **Updated**: 2023-07-20 08:41:46+00:00
- **Authors**: Xiaosong Jia, Penghao Wu, Li Chen, Yu Liu, Hongyang Li, Junchi Yan
- **Comment**: Accepted at IEEE TPAMI in 2023. Code url:
  https://github.com/OpenDriveLab/HDGT
- **Journal**: None
- **Summary**: Encoding a driving scene into vector representations has been an essential task for autonomous driving that can benefit downstream tasks e.g. trajectory prediction. The driving scene often involves heterogeneous elements such as the different types of objects (agents, lanes, traffic signs) and the semantic relations between objects are rich and diverse. Meanwhile, there also exist relativity across elements, which means that the spatial relation is a relative concept and need be encoded in a ego-centric manner instead of in a global coordinate system. Based on these observations, we propose Heterogeneous Driving Graph Transformer (HDGT), a backbone modelling the driving scene as a heterogeneous graph with different types of nodes and edges. For heterogeneous graph construction, we connect different types of nodes according to diverse semantic relations. For spatial relation encoding, the coordinates of the node as well as its in-edges are in the local node-centric coordinate system. For the aggregation module in the graph neural network (GNN), we adopt the transformer structure in a hierarchical way to fit the heterogeneous nature of inputs. Experimental results show that HDGT achieves state-of-the-art performance for the task of trajectory prediction, on INTERACTION Prediction Challenge and Waymo Open Motion Challenge.



### Reliable Label Correction is a Good Booster When Learning with Extremely Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2205.00186v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00186v2)
- **Published**: 2022-04-30 07:19:03+00:00
- **Updated**: 2022-07-19 17:08:46+00:00
- **Authors**: Kai Wang, Xiangyu Peng, Shuo Yang, Jianfei Yang, Zheng Zhu, Xinchao Wang, Yang You
- **Comment**: LC-Booster is an efficient method for LNL problems
- **Journal**: None
- **Summary**: Learning with noisy labels has aroused much research interest since data annotations, especially for large-scale datasets, may be inevitably imperfect. Recent approaches resort to a semi-supervised learning problem by dividing training samples into clean and noisy sets. This paradigm, however, is prone to significant degeneration under heavy label noise, as the number of clean samples is too small for conventional methods to behave well. In this paper, we introduce a novel framework, termed as LC-Booster, to explicitly tackle learning under extreme noise. The core idea of LC-Booster is to incorporate label correction into the sample selection, so that more purified samples, through the reliable label correction, can be utilized for training, thereby alleviating the confirmation bias. Experiments show that LC-Booster advances state-of-the-art results on several noisy-label benchmarks, including CIFAR-10, CIFAR-100, Clothing1M and WebVision. Remarkably, under the extreme 90\% noise ratio, LC-Booster achieves 92.9\% and 48.4\% accuracy on CIFAR-10 and CIFAR-100, surpassing state-of-the-art methods by a large margin.



### Cracking White-box DNN Watermarks via Invariant Neuron Transforms
- **Arxiv ID**: http://arxiv.org/abs/2205.00199v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.00199v2)
- **Published**: 2022-04-30 08:33:32+00:00
- **Updated**: 2022-05-19 07:28:53+00:00
- **Authors**: Yifan Yan, Xudong Pan, Yining Wang, Mi Zhang, Min Yang
- **Comment**: in submission; a preprint version
- **Journal**: None
- **Summary**: Recently, how to protect the Intellectual Property (IP) of deep neural networks (DNN) becomes a major concern for the AI industry. To combat potential model piracy, recent works explore various watermarking strategies to embed secret identity messages into the prediction behaviors or the internals (e.g., weights and neuron activation) of the target model. Sacrificing less functionality and involving more knowledge about the target model, the latter branch of watermarking schemes (i.e., white-box model watermarking) is claimed to be accurate, credible and secure against most known watermark removal attacks, with emerging research efforts and applications in the industry.   In this paper, we present the first effective removal attack which cracks almost all the existing white-box watermarking schemes with provably no performance overhead and no required prior knowledge. By analyzing these IP protection mechanisms at the granularity of neurons, we for the first time discover their common dependence on a set of fragile features of a local neuron group, all of which can be arbitrarily tampered by our proposed chain of invariant neuron transforms. On $9$ state-of-the-art white-box watermarking schemes and a broad set of industry-level DNN architectures, our attack for the first time reduces the embedded identity message in the protected models to be almost random. Meanwhile, unlike known removal attacks, our attack requires no prior knowledge on the training data distribution or the adopted watermark algorithms, and leaves model functionality intact.



### DefakeHop++: An Enhanced Lightweight Deepfake Detector
- **Arxiv ID**: http://arxiv.org/abs/2205.00211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00211v1)
- **Published**: 2022-04-30 08:50:25+00:00
- **Updated**: 2022-04-30 08:50:25+00:00
- **Authors**: Hong-Shuo Chen, Shuowen Hu, Suya You, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: On the basis of DefakeHop, an enhanced lightweight Deepfake detector called DefakeHop++ is proposed in this work. The improvements lie in two areas. First, DefakeHop examines three facial regions (i.e., two eyes and mouth) while DefakeHop++ includes eight more landmarks for broader coverage. Second, for discriminant features selection, DefakeHop uses an unsupervised approach while DefakeHop++ adopts a more effective approach with supervision, called the Discriminant Feature Test (DFT). In DefakeHop++, rich spatial and spectral features are first derived from facial regions and landmarks automatically. Then, DFT is used to select a subset of discriminant features for classifier training. As compared with MobileNet v3 (a lightweight CNN model of 1.5M parameters targeting at mobile applications), DefakeHop++ has a model of 238K parameters, which is 16% of MobileNet v3. Furthermore, DefakeHop++ outperforms MobileNet v3 in Deepfake image detection performance in a weakly-supervised setting.



### Coarse-to-Fine Video Denoising with Dual-Stage Spatial-Channel Transformer
- **Arxiv ID**: http://arxiv.org/abs/2205.00214v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00214v2)
- **Published**: 2022-04-30 09:01:21+00:00
- **Updated**: 2023-01-17 03:35:30+00:00
- **Authors**: Wulian Yun, Mengshi Qi, Chuanming Wang, Huiyuan Fu, Huadong Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Video denoising aims to recover high-quality frames from the noisy video. While most existing approaches adopt convolutional neural networks~(CNNs) to separate the noise from the original visual content, however, CNNs focus on local information and ignore the interactions between long-range regions in the frame. Furthermore, most related works directly take the output after basic spatio-temporal denoising as the final result, leading to neglect the fine-grained denoising process. In this paper, we propose a Dual-stage Spatial-Channel Transformer for coarse-to-fine video denoising, which inherits the advantages of both Transformer and CNNs. Specifically, DSCT is proposed based on a progressive dual-stage architecture, namely a coarse-level and a fine-level stage to extract dynamic features and static features, respectively. At both stages, a Spatial-Channel Encoding Module is designed to model the long-range contextual dependencies at both spatial and channel levels. Meanwhile, we design a Multi-Scale Residual Structure to preserve multiple aspects of information at different stages, which contains a Temporal Features Aggregation Module to summarize the dynamic representation. Extensive experiments on four publicly available datasets demonstrate our proposed method achieves significant improvements compared to the state-of-the-art methods.



### Recognising Known Configurations of Garments For Dual-Arm Robotic Flattening
- **Arxiv ID**: http://arxiv.org/abs/2205.00225v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.00225v2)
- **Published**: 2022-04-30 10:24:17+00:00
- **Updated**: 2022-06-17 16:17:39+00:00
- **Authors**: Li Duan, Gerardo Argon-Camarasa
- **Comment**: ICRCV 2022
- **Journal**: None
- **Summary**: Robotic deformable-object manipulation is a challenge in the robotic industry because deformable objects have complicated and various object states. Predicting those object states and updating manipulation planning is time-consuming and computationally expensive. In this paper, we propose learning known configurations of garments to allow a robot to recognise garment states and choose a pre-designed manipulation plan for garment flattening.



### Unsupervised Visible-light Images Guided Cross-Spectrum Depth Estimation from Dual-Modality Cameras
- **Arxiv ID**: http://arxiv.org/abs/2205.00257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00257v1)
- **Published**: 2022-04-30 12:58:35+00:00
- **Updated**: 2022-04-30 12:58:35+00:00
- **Authors**: Yubin Guo, Haobo Jiang, Xinlei Qi, Jin Xie, Cheng-Zhong Xu, Hui Kong
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-spectrum depth estimation aims to provide a depth map in all illumination conditions with a pair of dual-spectrum images. It is valuable for autonomous vehicle applications when the vehicle is equipped with two cameras of different modalities. However, images captured by different-modality cameras can be photometrically quite different. Therefore, cross-spectrum depth estimation is a very challenging problem. Moreover, the shortage of large-scale open-source datasets also retards further research in this field. In this paper, we propose an unsupervised visible-light image guided cross-spectrum (i.e., thermal and visible-light, TIR-VIS in short) depth estimation framework given a pair of RGB and thermal images captured from a visible-light camera and a thermal one. We first adopt a base depth estimation network using RGB-image pairs. Then we propose a multi-scale feature transfer network to transfer features from the TIR-VIS domain to the VIS domain at the feature level to fit the trained depth estimation network. At last, we propose a cross-spectrum depth cycle consistency to improve the depth result of dual-spectrum image pairs. Meanwhile, we release a large dual-spectrum depth estimation dataset with visible-light and far-infrared stereo images captured in different scenes to the society. The experiment result shows that our method achieves better performance than the compared existing methods. Our datasets is available at https://github.com/whitecrow1027/VIS-TIR-Datasets.



### Improving Visual Grounding with Visual-Linguistic Verification and Iterative Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2205.00272v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00272v2)
- **Published**: 2022-04-30 13:48:15+00:00
- **Updated**: 2022-06-08 16:28:15+00:00
- **Authors**: Li Yang, Yan Xu, Chunfeng Yuan, Wei Liu, Bing Li, Weiming Hu
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Visual grounding is a task to locate the target indicated by a natural language expression. Existing methods extend the generic object detection framework to this problem. They base the visual grounding on the features from pre-generated proposals or anchors, and fuse these features with the text embeddings to locate the target mentioned by the text. However, modeling the visual features from these predefined locations may fail to fully exploit the visual context and attribute information in the text query, which limits their performance. In this paper, we propose a transformer-based framework for accurate visual grounding by establishing text-conditioned discriminative features and performing multi-stage cross-modal reasoning. Specifically, we develop a visual-linguistic verification module to focus the visual features on regions relevant to the textual descriptions while suppressing the unrelated areas. A language-guided feature encoder is also devised to aggregate the visual contexts of the target object to improve the object's distinctiveness. To retrieve the target from the encoded visual features, we further propose a multi-stage cross-modal decoder to iteratively speculate on the correlations between the image and text for accurate target localization. Extensive experiments on five widely used datasets validate the efficacy of our proposed components and demonstrate state-of-the-art performance. Our code is public at https://github.com/yangli18/VLTVG.



### Dynamic Curriculum Learning for Great Ape Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2205.00275v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00275v2)
- **Published**: 2022-04-30 14:02:52+00:00
- **Updated**: 2023-01-02 13:52:29+00:00
- **Authors**: Xinyu Yang, Tilo Burghardt, Majid Mirmehdi
- **Comment**: Accepted at IJCV
- **Journal**: None
- **Summary**: We propose a novel end-to-end curriculum learning approach for sparsely labelled animal datasets leveraging large volumes of unlabelled data to improve supervised species detectors. We exemplify the method in detail on the task of finding great apes in camera trap footage taken in challenging real-world jungle environments. In contrast to previous semi-supervised methods, our approach adjusts learning parameters dynamically over time and gradually improves detection quality by steering training towards virtuous self-reinforcement. To achieve this, we propose integrating pseudo-labelling with curriculum learning policies and show how learning collapse can be avoided. We discuss theoretical arguments, ablations, and significant performance improvements against various state-of-the-art systems when evaluating on the Extended PanAfrican Dataset holding approx. 1.8M frames. We also demonstrate our method can outperform supervised baselines with significant margins on sparse label versions of other animal datasets such as Bees and Snapshot Serengeti. We note that performance advantages are strongest for smaller labelled ratios common in ecological applications. Finally, we show that our approach achieves competitive benchmarks for generic object detection in MS-COCO and PASCAL-VOC indicating wider applicability of the dynamic learning concepts introduced. We publish all relevant source code, network weights, and data access details for full reproducibility. The code is available at https://github.com/youshyee/DCL-Detection.



### ONCE-3DLanes: Building Monocular 3D Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.00301v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00301v2)
- **Published**: 2022-04-30 16:35:25+00:00
- **Updated**: 2022-05-14 16:51:38+00:00
- **Authors**: Fan Yan, Ming Nie, Xinyue Cai, Jianhua Han, Hang Xu, Zhen Yang, Chaoqiang Ye, Yanwei Fu, Michael Bi Mi, Li Zhang
- **Comment**: CVPR 2022. Project page at https://once-3dlanes.github.io
- **Journal**: None
- **Summary**: We present ONCE-3DLanes, a real-world autonomous driving dataset with lane layout annotation in 3D space. Conventional 2D lane detection from a monocular image yields poor performance of following planning and control tasks in autonomous driving due to the case of uneven road. Predicting the 3D lane layout is thus necessary and enables effective and safe driving. However, existing 3D lane detection datasets are either unpublished or synthesized from a simulated environment, severely hampering the development of this field. In this paper, we take steps towards addressing these issues. By exploiting the explicit relationship between point clouds and image pixels, a dataset annotation pipeline is designed to automatically generate high-quality 3D lane locations from 2D lane annotations in 211K road scenes. In addition, we present an extrinsic-free, anchor-free method, called SALAD, regressing the 3D coordinates of lanes in image view without converting the feature map into the bird's-eye view (BEV). To facilitate future research on 3D lane detection, we benchmark the dataset and provide a novel evaluation metric, performing extensive experiments of both existing approaches and our proposed method. The aim of our work is to revive the interest of 3D lane detection in a real-world scenario. We believe our work can lead to the expected and unexpected innovations in both academia and industry.



### Composition-aware Graphic Layout GAN for Visual-textual Presentation Designs
- **Arxiv ID**: http://arxiv.org/abs/2205.00303v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00303v3)
- **Published**: 2022-04-30 16:42:13+00:00
- **Updated**: 2022-07-13 05:09:51+00:00
- **Authors**: Min Zhou, Chenchen Xu, Ye Ma, Tiezheng Ge, Yuning Jiang, Weiwei Xu
- **Comment**: Accepted by IJCAI 2022 (AI, THE ARTS AND CREATIVITY TRACK)
- **Journal**: None
- **Summary**: In this paper, we study the graphic layout generation problem of producing high-quality visual-textual presentation designs for given images. We note that image compositions, which contain not only global semantics but also spatial information, would largely affect layout results. Hence, we propose a deep generative model, dubbed as composition-aware graphic layout GAN (CGL-GAN), to synthesize layouts based on the global and spatial visual contents of input images. To obtain training images from images that already contain manually designed graphic layout data, previous work suggests masking design elements (e.g., texts and embellishments) as model inputs, which inevitably leaves hint of the ground truth. We study the misalignment between the training inputs (with hint masks) and test inputs (without masks), and design a novel domain alignment module (DAM) to narrow this gap. For training, we built a large-scale layout dataset which consists of 60,548 advertising posters with annotated layout information. To evaluate the generated layouts, we propose three novel metrics according to aesthetic intuitions. Through both quantitative and qualitative evaluations, we demonstrate that the proposed model can synthesize high-quality graphic layouts according to image compositions.



### Source Domain Subset Sampling for Semi-Supervised Domain Adaptation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.00312v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00312v2)
- **Published**: 2022-04-30 17:29:56+00:00
- **Updated**: 2022-05-03 05:48:45+00:00
- **Authors**: Daehan Kim, Minseok Seo, Jinsun Park, Dong-Geol Choi
- **Comment**: 10pages, 4figures
- **Journal**: None
- **Summary**: In this paper, we introduce source domain subset sampling (SDSS) as a new perspective of semi-supervised domain adaptation. We propose domain adaptation by sampling and exploiting only a meaningful subset from source data for training. Our key assumption is that the entire source domain data may contain samples that are unhelpful for the adaptation. Therefore, the domain adaptation can benefit from a subset of source data composed solely of helpful and relevant samples. The proposed method effectively subsamples full source data to generate a small-scale meaningful subset. Therefore, training time is reduced, and performance is improved with our subsampled source data. To further verify the scalability of our method, we construct a new dataset called Ocean Ship, which comprises 500 real and 200K synthetic sample images with ground-truth labels. The SDSS achieved a state-of-the-art performance when applied on GTA5 to Cityscapes and SYNTHIA to Cityscapes public benchmark datasets and a 9.13 mIoU improvement on our Ocean Ship dataset over a baseline model.



### Mosaic Zonotope Shadow Matching for Risk-Aware Autonomous Localization in Harsh Urban Environments
- **Arxiv ID**: http://arxiv.org/abs/2205.10223v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.10223v1)
- **Published**: 2022-04-30 21:01:03+00:00
- **Updated**: 2022-04-30 21:01:03+00:00
- **Authors**: Daniel Neamati, Sriramya Bhamidipati, Grace Gao
- **Comment**: Submitted to AIJ Special Issue on Risk-Aware Autonomous Systems:
  Theory and Practice
- **Journal**: None
- **Summary**: Risk-aware urban localization with the Global Navigation Satellite System (GNSS) remains an unsolved problem with frequent misdetection of the user's street or side of the street. Significant advances in 3D map-aided GNSS use grid-based GNSS shadow matching alongside AI-driven line-of-sight (LOS) classifiers and server-based processing to improve localization accuracy, especially in the cross-street direction. Our prior work introduces a new paradigm for shadow matching that proposes set-valued localization with computationally efficient zonotope set representations. While existing literature improved accuracy and efficiency, the current state of shadow matching theory does not address the needs of risk-aware autonomous systems. We extend our prior work to propose Mosaic Zonotope Shadow Matching (MZSM) that employs a classifier-agnostic polytope mosaic architecture to provide risk-awareness and certifiable guarantees on urban positioning. We formulate a recursively expanding binary tree that refines an initial location estimate with set operations into smaller polytopes. Together, the smaller polytopes form a mosaic. We weight the tree branches with the probability that the user is in line of sight of the satellite and expand the tree with each new satellite observation. Our method yields an exact shadow matching distribution from which we guarantee uncertainty bounds on the user localization. We perform high-fidelity simulations using a 3D building map of San Francisco to validate our algorithm's risk-aware improvements. We demonstrate that MZSM provides certifiable guarantees across varied data-driven LOS classifier accuracies and yields a more precise understanding of the uncertainty over existing methods. We validate that our tree-based construction is efficient and tractable, computing a mosaic from 14 satellites in 0.63 seconds and growing quadratically in the satellite number.



### LayoutBERT: Masked Language Layout Model for Object Insertion
- **Arxiv ID**: http://arxiv.org/abs/2205.00347v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.00347v1)
- **Published**: 2022-04-30 21:35:38+00:00
- **Updated**: 2022-04-30 21:35:38+00:00
- **Authors**: Kerem Turgutlu, Sanat Sharma, Jayant Kumar
- **Comment**: 8 pages main paper, 6 pages supplemental material. Accepted to AI4CC
  Workshop @CVPR 2022
- **Journal**: None
- **Summary**: Image compositing is one of the most fundamental steps in creative workflows. It involves taking objects/parts of several images to create a new image, called a composite. Currently, this process is done manually by creating accurate masks of objects to be inserted and carefully blending them with the target scene or images, usually with the help of tools such as Photoshop or GIMP. While there have been several works on automatic selection of objects for creating masks, the problem of object placement within an image with the correct position, scale, and harmony remains a difficult problem with limited exploration. Automatic object insertion in images or designs is a difficult problem as it requires understanding of the scene geometry and the color harmony between objects. We propose LayoutBERT for the object insertion task. It uses a novel self-supervised masked language model objective and bidirectional multi-head self-attention. It outperforms previous layout-based likelihood models and shows favorable properties in terms of model capacity. We demonstrate the effectiveness of our approach for object insertion in the image compositing setting and other settings like documents and design templates. We further demonstrate the usefulness of the learned representations for layout-based retrieval tasks. We provide both qualitative and quantitative evaluations on datasets from diverse domains like COCO, PublayNet, and two new datasets which we call Image Layouts and Template Layouts. Image Layouts which consists of 5.8 million images with layout annotations is the largest image layout dataset to our knowledge. We also share ablation study results on the effect of dataset size, model size and class sample size for this task.



### Visual Spatial Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2205.00363v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.00363v3)
- **Published**: 2022-04-30 23:03:49+00:00
- **Updated**: 2023-03-22 15:42:50+00:00
- **Authors**: Fangyu Liu, Guy Emerson, Nigel Collier
- **Comment**: TACL camera-ready version; code and data available at
  https://github.com/cambridgeltl/visual-spatial-reasoning
- **Journal**: None
- **Summary**: Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (such as: under, in front of, and facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: the human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs' by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.



### RADNet: A Deep Neural Network Model for Robust Perception in Moving Autonomous Systems
- **Arxiv ID**: http://arxiv.org/abs/2205.00364v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00364v1)
- **Published**: 2022-04-30 23:14:08+00:00
- **Updated**: 2022-04-30 23:14:08+00:00
- **Authors**: Burhan A. Mudassar, Sho Ko, Maojingjing Li, Priyabrata Saha, Saibal Mukhopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Interactive autonomous applications require robustness of the perception engine to artifacts in unconstrained videos. In this paper, we examine the effect of camera motion on the task of action detection. We develop a novel ranking method to rank videos based on the degree of global camera motion. For the high ranking camera videos we show that the accuracy of action detection is decreased. We propose an action detection pipeline that is robust to the camera motion effect and verify it empirically. Specifically, we do actor feature alignment across frames and couple global scene features with local actor-specific features. We do feature alignment using a novel formulation of the Spatio-temporal Sampling Network (STSN) but with multi-scale offset prediction and refinement using a pyramid structure. We also propose a novel input dependent weighted averaging strategy for fusing local and global features. We show the applicability of our network on our dataset of moving camera videos with high camera motion (MOVE dataset) with a 4.1% increase in frame mAP and 17% increase in video mAP.



### Fractional Vegetation Cover Estimation using Hough Lines and Linear Iterative Clustering
- **Arxiv ID**: http://arxiv.org/abs/2205.00366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00366v1)
- **Published**: 2022-04-30 23:33:31+00:00
- **Updated**: 2022-04-30 23:33:31+00:00
- **Authors**: Venkat Margapuri, Trevor Rife, Chaney Courtney, Brandon Schlautman, Kai Zhao, Mitchell Neilsen
- **Comment**: None
- **Journal**: None
- **Summary**: A common requirement of plant breeding programs across the country is companion planting -- growing different species of plants in close proximity so they can mutually benefit each other. However, the determination of companion plants requires meticulous monitoring of plant growth. The technique of ocular monitoring is often laborious and error prone. The availability of image processing techniques can be used to address the challenge of plant growth monitoring and provide robust solutions that assist plant scientists to identify companion plants. This paper presents a new image processing algorithm to determine the amount of vegetation cover present in a given area, called fractional vegetation cover. The proposed technique draws inspiration from the trusted Daubenmire method for vegetation cover estimation and expands upon it. Briefly, the idea is to estimate vegetation cover from images containing multiple rows of plant species growing in close proximity separated by a multi-segment PVC frame of known size. The proposed algorithm applies a Hough Transform and Simple Linear Iterative Clustering (SLIC) to estimate the amount of vegetation cover within each segment of the PVC frame. The analysis when repeated over images captured at regular intervals of time provides crucial insights into plant growth. As a means of comparison, the proposed algorithm is compared with SamplePoint and Canopeo, two trusted applications used for vegetation cover estimation. The comparison shows a 99% similarity with both SamplePoint and Canopeo demonstrating the accuracy and feasibility of the algorithm for fractional vegetation cover estimation.



