# Arxiv Papers in cs.CV on 2022-04-12
### Glass Segmentation with RGB-Thermal Image Pairs
- **Arxiv ID**: http://arxiv.org/abs/2204.05453v4
- **DOI**: 10.1109/TIP.2023.3256762
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05453v4)
- **Published**: 2022-04-12 00:20:22+00:00
- **Updated**: 2023-03-16 19:21:26+00:00
- **Authors**: Dong Huo, Jian Wang, Yiming Qian, Yee-Hong Yang
- **Comment**: IEEE Transactions on Image Processing (TIP), 2023
- **Journal**: None
- **Summary**: This paper proposes a new glass segmentation method utilizing paired RGB and thermal images. Due to the large difference between the transmission property of visible light and that of the thermal energy through the glass where most glass is transparent to the visible light but opaque to thermal energy, glass regions of a scene are made more distinguishable with a pair of RGB and thermal images than solely with an RGB image. To exploit such a unique property, we propose a neural network architecture that effectively combines an RGB-thermal image pair with a new multi-modal fusion module based on attention, and integrate CNN and transformer to extract local features and non-local dependencies, respectively. As well, we have collected a new dataset containing 5551 RGB-thermal image pairs with ground-truth segmentation annotations. The qualitative and quantitative evaluations demonstrate the effectiveness of the proposed approach on fusing RGB and thermal data for glass segmentation. Our code and data are available at https://github.com/Dong-Huo/RGB-T-Glass-Segmentation.



### Are Multimodal Transformers Robust to Missing Modality?
- **Arxiv ID**: http://arxiv.org/abs/2204.05454v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05454v1)
- **Published**: 2022-04-12 00:21:31+00:00
- **Updated**: 2022-04-12 00:21:31+00:00
- **Authors**: Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine, Xi Peng
- **Comment**: In CVPR 2022
- **Journal**: None
- **Summary**: Multimodal data collected from the real world are often imperfect due to missing modalities. Therefore multimodal models that are robust against modal-incomplete data are highly preferred. Recently, Transformer models have shown great success in processing multimodal data. However, existing work has been limited to either architecture designs or pre-training strategies; whether Transformer models are naturally robust against missing-modal data has rarely been investigated. In this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of Transformers in the presence of modal-incomplete data. Unsurprising, we find Transformer models are sensitive to missing modalities while different modal fusion strategies will significantly affect the robustness. What surprised us is that the optimal fusion strategy is dataset dependent even for the same Transformer model; there does not exist a universal strategy that works in general cases. Based on these findings, we propose a principle method to improve the robustness of Transformer models by automatically searching for an optimal fusion strategy regarding input data. Experimental validations on three benchmarks support the superior performance of the proposed method.



### Out-Of-Distribution Detection In Unsupervised Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.05462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05462v1)
- **Published**: 2022-04-12 01:24:54+00:00
- **Updated**: 2022-04-12 01:24:54+00:00
- **Authors**: Jiangpeng He, Fengqing Zhu
- **Comment**: Accpeted paper for CVPR 2022, CLVision Workshop
- **Journal**: None
- **Summary**: Unsupervised continual learning aims to learn new tasks incrementally without requiring human annotations. However, most existing methods, especially those targeted on image classification, only work in a simplified scenario by assuming all new data belong to new tasks, which is not realistic if the class labels are not provided. Therefore, to perform unsupervised continual learning in real life applications, an out-of-distribution detector is required at beginning to identify whether each new data corresponds to a new task or already learned tasks, which still remains under-explored yet. In this work, we formulate the problem for Out-of-distribution Detection in Unsupervised Continual Learning (OOD-UCL) with the corresponding evaluation protocol. In addition, we propose a novel OOD detection method by correcting the output bias at first and then enhancing the output confidence for in-distribution data based on task discriminativeness, which can be applied directly without modifying the learning procedures and objectives of continual learning. Our method is evaluated on CIFAR-100 dataset by following the proposed evaluation protocol and we show improved performance compared with existing OOD detection methods under the unsupervised continual learning scenario.



### HiTPR: Hierarchical Transformer for Place Recognition in Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2204.05481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05481v1)
- **Published**: 2022-04-12 02:12:38+00:00
- **Updated**: 2022-04-12 02:12:38+00:00
- **Authors**: Zhixing Hou, Yan Yan, Chengzhong Xu, Hui Kong
- **Comment**: Accepted paper in ICRA 2022
- **Journal**: None
- **Summary**: Place recognition or loop closure detection is one of the core components in a full SLAM system. In this paper, aiming at strengthening the relevancy of local neighboring points and the contextual dependency among global points simultaneously, we investigate the exploitation of transformer-based network for feature extraction, and propose a Hierarchical Transformer for Place Recognition (HiTPR). The HiTPR consists of four major parts: point cell generation, short-range transformer (SRT), long-range transformer (LRT) and global descriptor aggregation. Specifically, the point cloud is initially divided into a sequence of small cells by downsampling and nearest neighbors searching. In the SRT, we extract the local feature for each point cell. While in the LRT, we build the global dependency among all of the point cells in the whole point cloud. Experiments on several standard benchmarks demonstrate the superiority of the HiTPR in terms of average recall rate, achieving 93.71% at top 1% and 86.63% at top 1 on the Oxford RobotCar dataset for example.



### Neural Graph Matching for Modification Similarity Applied to Electronic Document Comparison
- **Arxiv ID**: http://arxiv.org/abs/2204.05486v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.05486v2)
- **Published**: 2022-04-12 02:37:54+00:00
- **Updated**: 2022-11-02 16:46:32+00:00
- **Authors**: Po-Fang Hsu, Chiching Wei
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel neural graph matching approach applied to document comparison. Document comparison is a common task in the legal and financial industries. In some cases, the most important differences may be the addition or omission of words, sentences, clauses, or paragraphs. However, it is a challenging task without recording or tracing whole edited process. Under many temporal uncertainties, we explore the potentiality of our approach to proximate the accurate comparison to make sure which element blocks have a relation of edition with others. In beginning, we apply a document layout analysis that combining traditional and modern technics to segment layout in blocks of various types appropriately. Then we transform this issue to a problem of layout graph matching with textual awareness. About graph matching, it is a long-studied problem with a broad range of applications. However, different from previous works focusing on visual images or structural layout, we also bring textual features into our model for adapting this domain. Specifically, based on the electronic document, we introduce an encoder to deal with the visual presentation decoding from PDF. Additionally, because the modifications can cause the inconsistency of document layout analysis between modified documents and the blocks can be merged and split, Sinkhorn divergence is adopted in our graph neural approach, which tries to overcome both these issues with many-to-many block matching. We demonstrate this on two categories of layouts, as follows., legal agreement and scientific articles, collected from our real-case datasets.



### Few-shot Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2204.05494v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2204.05494v2)
- **Published**: 2022-04-12 03:05:53+00:00
- **Updated**: 2022-07-31 19:10:59+00:00
- **Authors**: Kevin J Liang, Samrudhdhi B. Rangrej, Vladan Petrovic, Tal Hassner
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Few-shot learning (FSL) methods typically assume clean support sets with accurately labeled samples when training on novel classes. This assumption can often be unrealistic: support sets, no matter how small, can still include mislabeled samples. Robustness to label noise is therefore essential for FSL methods to be practical, but this problem surprisingly remains largely unexplored. To address mislabeled samples in FSL settings, we make several technical contributions. (1) We offer simple, yet effective, feature aggregation methods, improving the prototypes used by ProtoNet, a popular FSL technique. (2) We describe a novel Transformer model for Noisy Few-Shot Learning (TraNFS). TraNFS leverages a transformer's attention mechanism to weigh mislabeled versus correct samples. (3) Finally, we extensively test these methods on noisy versions of MiniImageNet and TieredImageNet. Our results show that TraNFS is on-par with leading FSL methods on clean support sets, yet outperforms them, by far, in the presence of label noise.



### Position-aware Location Regression Network for Temporal Video Grounding
- **Arxiv ID**: http://arxiv.org/abs/2204.05499v1
- **DOI**: 10.1109/AVSS52988.2021.9663815
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05499v1)
- **Published**: 2022-04-12 03:14:16+00:00
- **Updated**: 2022-04-12 03:14:16+00:00
- **Authors**: Sunoh Kim, Kimin Yun, Jin Young Choi
- **Comment**: Accepted in AVSS 2021
- **Journal**: None
- **Summary**: The key to successful grounding for video surveillance is to understand a semantic phrase corresponding to important actors and objects. Conventional methods ignore comprehensive contexts for the phrase or require heavy computation for multiple phrases. To understand comprehensive contexts with only one semantic phrase, we propose Position-aware Location Regression Network (PLRN) which exploits position-aware features of a query and a video. Specifically, PLRN first encodes both the video and query using positional information of words and video segments. Then, a semantic phrase feature is extracted from an encoded query with attention. The semantic phrase feature and encoded video are merged and made into a context-aware feature by reflecting local and global contexts. Finally, PLRN predicts start, end, center, and width values of a grounding boundary. Our experiments show that PLRN achieves competitive performance over existing methods with less computation time and memory.



### CoupleFace: Relation Matters for Face Recognition Distillation
- **Arxiv ID**: http://arxiv.org/abs/2204.05502v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05502v2)
- **Published**: 2022-04-12 03:25:42+00:00
- **Updated**: 2022-11-17 07:17:33+00:00
- **Authors**: Jiaheng Liu, Haoyu Qin, Yichao Wu, Jinyang Guo, Ding Liang, Ke Xu
- **Comment**: Accepted by ECCV22
- **Journal**: None
- **Summary**: Knowledge distillation is an effective method to improve the performance of a lightweight neural network (i.e., student model) by transferring the knowledge of a well-performed neural network (i.e., teacher model), which has been widely applied in many computer vision tasks, including face recognition. Nevertheless, the current face recognition distillation methods usually utilize the Feature Consistency Distillation (FCD) (e.g., L2 distance) on the learned embeddings extracted by the teacher and student models for each sample, which is not able to fully transfer the knowledge from the teacher to the student for face recognition. In this work, we observe that mutual relation knowledge between samples is also important to improve the discriminative ability of the learned representation of the student model, and propose an effective face recognition distillation method called CoupleFace by additionally introducing the Mutual Relation Distillation (MRD) into existing distillation framework. Specifically, in MRD, we first propose to mine the informative mutual relations, and then introduce the Relation-Aware Distillation (RAD) loss to transfer the mutual relation knowledge of the teacher model to the student model. Extensive experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed CoupleFace for face recognition. Moreover, based on our proposed CoupleFace, we have won the first place in the ICCV21 Masked Face Recognition Challenge (MS1M track).



### FSOINet: Feature-Space Optimization-Inspired Network for Image Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/2204.05503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05503v1)
- **Published**: 2022-04-12 03:30:22+00:00
- **Updated**: 2022-04-12 03:30:22+00:00
- **Authors**: Wenjun Chen, Chunling Yang, Xin Yang
- **Comment**: ICASSP2022 accepted
- **Journal**: None
- **Summary**: In recent years, deep learning-based image compressive sensing (ICS) methods have achieved brilliant success. Many optimization-inspired networks have been proposed to bring the insights of optimization algorithms into the network structure design and have achieved excellent reconstruction quality with low computational complexity. But they keep the information flow in pixel space as traditional algorithms by updating and transferring the image in pixel space, which does not fully use the information in the image features. In this paper, we propose the idea of achieving information flow phase by phase in feature space and design a Feature-Space Optimization-Inspired Network (dubbed FSOINet) to implement it by mapping both steps of proximal gradient descent algorithm from pixel space to feature space. Moreover, the sampling matrix is learned end-to-end with other network parameters. Experiments show that the proposed FSOINet outperforms the existing state-of-the-art methods by a large margin both quantitatively and qualitatively. The source code is available on https://github.com/cwjjun/FSOINet.



### End-to-end Autonomous Driving with Semantic Depth Cloud Mapping and Multi-agent
- **Arxiv ID**: http://arxiv.org/abs/2204.05513v2
- **DOI**: 10.1109/TIV.2022.3185303
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.05513v2)
- **Published**: 2022-04-12 03:57:01+00:00
- **Updated**: 2022-06-22 04:21:30+00:00
- **Authors**: Oskar Natan, Jun Miura
- **Comment**: This work has been accepted for publication in IEEE Transactions on
  Intelligent Vehicles. The published version can be accessed at
  https://ieeexplore.ieee.org/document/9802907 or
  https://doi.org/10.1109/TIV.2022.3185303
- **Journal**: None
- **Summary**: Focusing on the task of point-to-point navigation for an autonomous driving vehicle, we propose a novel deep learning model trained with end-to-end and multi-task learning manners to perform both perception and control tasks simultaneously. The model is used to drive the ego vehicle safely by following a sequence of routes defined by the global planner. The perception part of the model is used to encode high-dimensional observation data provided by an RGBD camera while performing semantic segmentation, semantic depth cloud (SDC) mapping, and traffic light state and stop sign prediction. Then, the control part decodes the encoded features along with additional information provided by GPS and speedometer to predict waypoints that come with a latent feature space. Furthermore, two agents are employed to process these outputs and make a control policy that determines the level of steering, throttle, and brake as the final action. The model is evaluated on CARLA simulator with various scenarios made of normal-adversarial situations and different weathers to mimic real-world conditions. In addition, we do a comparative study with some recent models to justify the performance in multiple aspects of driving. Moreover, we also conduct an ablation study on SDC mapping and multi-agent to understand their roles and behavior. As a result, our model achieves the highest driving score even with fewer parameters and computation load. To support future studies, we share our codes at https://github.com/oskarnatan/end-to-end-driving.



### TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.05525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05525v1)
- **Published**: 2022-04-12 04:51:42+00:00
- **Updated**: 2022-04-12 04:51:42+00:00
- **Authors**: Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen, Xinggang Wang, Wenyu Liu, Gang Yu, Chunhua Shen
- **Comment**: To Appear at CVPR 2022
- **Journal**: None
- **Summary**: Although vision transformers (ViTs) have achieved great success in computer vision, the heavy computational cost hampers their applications to dense prediction tasks such as semantic segmentation on mobile devices. In this paper, we present a mobile-friendly architecture named \textbf{To}ken \textbf{P}yramid Vision Trans\textbf{former} (\textbf{TopFormer}). The proposed \textbf{TopFormer} takes Tokens from various scales as input to produce scale-aware semantic features, which are then injected into the corresponding tokens to augment the representation. Experimental results demonstrate that our method significantly outperforms CNN- and ViT-based networks across several semantic segmentation datasets and achieves a good trade-off between accuracy and latency. On the ADE20K dataset, TopFormer achieves 5\% higher accuracy in mIoU than MobileNetV3 with lower latency on an ARM-based mobile device. Furthermore, the tiny version of TopFormer achieves real-time inference on an ARM-based mobile device with competitive results. The code and models are available at: https://github.com/hustvl/TopFormer



### Unidirectional Video Denoising by Mimicking Backward Recurrent Modules with Look-ahead Forward Ones
- **Arxiv ID**: http://arxiv.org/abs/2204.05532v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05532v3)
- **Published**: 2022-04-12 05:33:15+00:00
- **Updated**: 2022-07-31 03:25:28+00:00
- **Authors**: Junyi Li, Xiaohe Wu, Zhenxing Niu, Wangmeng Zuo
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: While significant progress has been made in deep video denoising, it remains very challenging for exploiting historical and future frames. Bidirectional recurrent networks (BiRNN) have exhibited appealing performance in several video restoration tasks. However, BiRNN is intrinsically offline because it uses backward recurrent modules to propagate from the last to current frames, which causes high latency and large memory consumption. To address the offline issue of BiRNN, we present a novel recurrent network consisting of forward and look-ahead recurrent modules for unidirectional video denoising. Particularly, look-ahead module is an elaborate forward module for leveraging information from near-future frames. When denoising the current frame, the hidden features by forward and look-ahead recurrent modules are combined, thereby making it feasible to exploit both historical and near-future frames. Due to the scene motion between non-neighboring frames, border pixels missing may occur when warping look-ahead feature from near-future frame to current frame, which can be largely alleviated by incorporating forward warping and proposed border enlargement. Experiments show that our method achieves state-of-the-art performance with constant latency and memory consumption. Code is avaliable at https://github.com/nagejacob/FloRNN.



### Open-set Text Recognition via Character-Context Decoupling
- **Arxiv ID**: http://arxiv.org/abs/2204.05535v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05535v1)
- **Published**: 2022-04-12 05:43:46+00:00
- **Updated**: 2022-04-12 05:43:46+00:00
- **Authors**: Chang Liu, Chun Yang, Xu-Cheng Yin
- **Comment**: Accepted at CVPR 2022 (Poster)
- **Journal**: None
- **Summary**: The open-set text recognition task is an emerging challenge that requires an extra capability to cognize novel characters during evaluation. We argue that a major cause of the limited performance for current methods is the confounding effect of contextual information over the visual information of individual characters. Under open-set scenarios, the intractable bias in contextual information can be passed down to visual information, consequently impairing the classification performance. In this paper, a Character-Context Decoupling framework is proposed to alleviate this problem by separating contextual information and character-visual information. Contextual information can be decomposed into temporal information and linguistic information. Here, temporal information that models character order and word length is isolated with a detached temporal attention module. Linguistic information that models n-gram and other linguistic statistics is separated with a decoupled context anchor mechanism. A variety of quantitative and qualitative experiments show that our method achieves promising performance on open-set, zero-shot, and close-set text recognition datasets.



### NightLab: A Dual-level Architecture with Hardness Detection for Segmentation at Night
- **Arxiv ID**: http://arxiv.org/abs/2204.05538v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05538v1)
- **Published**: 2022-04-12 05:50:22+00:00
- **Updated**: 2022-04-12 05:50:22+00:00
- **Authors**: Xueqing Deng, Peng Wang, Xiaochen Lian, Shawn Newsam
- **Comment**: 8pages, 6 figures, accept at CVPR 2022
- **Journal**: None
- **Summary**: The semantic segmentation of nighttime scenes is a challenging problem that is key to impactful applications like self-driving cars. Yet, it has received little attention compared to its daytime counterpart. In this paper, we propose NightLab, a novel nighttime segmentation framework that leverages multiple deep learning models imbued with night-aware features to yield State-of-The-Art (SoTA) performance on multiple night segmentation benchmarks. Notably, NightLab contains models at two levels of granularity, i.e. image and regional, and each level is composed of light adaptation and segmentation modules. Given a nighttime image, the image level model provides an initial segmentation estimate while, in parallel, a hardness detection module identifies regions and their surrounding context that need further analysis. A regional level model focuses on these difficult regions to provide a significantly improved segmentation. All the models in NightLab are trained end-to-end using a set of proposed night-aware losses without handcrafted heuristics. Extensive experiments on the NightCity and BDD100K datasets show NightLab achieves SoTA performance compared to concurrent methods.



### Content and Style Aware Generation of Text-line Images for Handwriting Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.05539v1
- **DOI**: 10.1109/TPAMI.2021.3122572
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05539v1)
- **Published**: 2022-04-12 05:52:03+00:00
- **Updated**: 2022-04-12 05:52:03+00:00
- **Authors**: Lei Kang, Pau Riba, Marçal Rusiñol, Alicia Fornés, Mauricio Villegas
- **Comment**: Accepted to TPAMI
- **Journal**: None
- **Summary**: Handwritten Text Recognition has achieved an impressive performance in public benchmarks. However, due to the high inter- and intra-class variability between handwriting styles, such recognizers need to be trained using huge volumes of manually labeled training data. To alleviate this labor-consuming problem, synthetic data produced with TrueType fonts has been often used in the training loop to gain volume and augment the handwriting style variability. However, there is a significant style bias between synthetic and real data which hinders the improvement of recognition performance. To deal with such limitations, we propose a generative method for handwritten text-line images, which is conditioned on both visual appearance and textual content. Our method is able to produce long text-line samples with diverse handwriting styles. Once properly trained, our method can also be adapted to new target data by only accessing unlabeled text-line images to mimic handwritten styles and produce images with any textual content. Extensive experiments have been done on making use of the generated samples to boost Handwritten Text Recognition performance. Both qualitative and quantitative results demonstrate that the proposed approach outperforms the current state of the art.



### Towards Reliable Image Outpainting: Learning Structure-Aware Multimodal Fusion with Depth Guidance
- **Arxiv ID**: http://arxiv.org/abs/2204.05543v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.05543v2)
- **Published**: 2022-04-12 06:06:50+00:00
- **Updated**: 2023-02-16 06:18:56+00:00
- **Authors**: Lei Zhang, Kang Liao, Chunyu Lin, Yao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Image outpainting technology generates visually plausible content regardless of authenticity, making it unreliable to be applied in practice. Thus, we propose a reliable image outpainting task, introducing the sparse depth from LiDARs to extrapolate authentic RGB scenes. The large field view of LiDARs allows it to serve for data enhancement and further multimodal tasks. Concretely, we propose a Depth-Guided Outpainting Network to model different feature representations of two modalities and learn the structure-aware cross-modal fusion. And two components are designed: 1) The Multimodal Learning Module produces unique depth and RGB feature representations from the perspectives of different modal characteristics. 2) The Depth Guidance Fusion Module leverages the complete depth modality to guide the establishment of RGB contents by progressive multimodal feature fusion. Furthermore, we specially design an additional constraint strategy consisting of Cross-modal Loss and Edge Loss to enhance ambiguous contours and expedite reliable content generation. Extensive experiments on KITTI and Waymo datasets demonstrate our superiority over the state-of-the-art method, quantitatively and qualitatively.



### Undoing the Damage of Label Shift for Cross-domain Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.05546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05546v1)
- **Published**: 2022-04-12 06:18:50+00:00
- **Updated**: 2022-04-12 06:18:50+00:00
- **Authors**: Yahao Liu, Jinhong Deng, Jiale Tao, Tong Chu, Lixin Duan, Wen Li
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Existing works typically treat cross-domain semantic segmentation (CDSS) as a data distribution mismatch problem and focus on aligning the marginal distribution or conditional distribution. However, the label shift issue is unfortunately overlooked, which actually commonly exists in the CDSS task, and often causes a classifier bias in the learnt model. In this paper, we give an in-depth analysis and show that the damage of label shift can be overcome by aligning the data conditional distribution and correcting the posterior probability. To this end, we propose a novel approach to undo the damage of the label shift problem in CDSS. In implementation, we adopt class-level feature alignment for conditional distribution alignment, as well as two simple yet effective methods to rectify the classifier bias from source to target by remolding the classifier predictions. We conduct extensive experiments on the benchmark datasets of urban scenes, including GTA5 to Cityscapes and SYNTHIA to Cityscapes, where our proposed approach outperforms previous methods by a large margin. For instance, our model equipped with a self-training strategy reaches 59.3% mIoU on GTA5 to Cityscapes, pushing to a new state-of-the-art. The code will be available at https://github.com/manmanjun/Undoing UDA.



### DistPro: Searching A Fast Knowledge Distillation Process via Meta Optimization
- **Arxiv ID**: http://arxiv.org/abs/2204.05547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05547v1)
- **Published**: 2022-04-12 06:22:24+00:00
- **Updated**: 2022-04-12 06:22:24+00:00
- **Authors**: Xueqing Deng, Dawei Sun, Shawn Newsam, Peng Wang
- **Comment**: 14 pages, 5 figures
- **Journal**: None
- **Summary**: Recent Knowledge distillation (KD) studies show that different manually designed schemes impact the learned results significantly. Yet, in KD, automatically searching an optimal distillation scheme has not yet been well explored. In this paper, we propose DistPro, a novel framework which searches for an optimal KD process via differentiable meta-learning. Specifically, given a pair of student and teacher networks, DistPro first sets up a rich set of KD connection from the transmitting layers of the teacher to the receiving layers of the student, and in the meanwhile, various transforms are also proposed for comparing feature maps along its pathway for the distillation. Then, each combination of a connection and a transform choice (pathway) is associated with a stochastic weighting process which indicates its importance at every step during the distillation. In the searching stage, the process can be effectively learned through our proposed bi-level meta-optimization strategy. In the distillation stage, DistPro adopts the learned processes for knowledge distillation, which significantly improves the student accuracy especially when faster training is required. Lastly, we find the learned processes can be generalized between similar tasks and networks. In our experiments, DistPro produces state-of-the-art (SoTA) accuracy under varying number of learning epochs on popular datasets, i.e. CIFAR100 and ImageNet, which demonstrate the effectiveness of our framework.



### Compact Model Training by Low-Rank Projection with Energy Transfer
- **Arxiv ID**: http://arxiv.org/abs/2204.05566v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05566v2)
- **Published**: 2022-04-12 06:53:25+00:00
- **Updated**: 2022-11-28 13:57:13+00:00
- **Authors**: Kailing Guo, Zhenquan Lin, Xiaofen Xing, Fang Liu, Xiangmin Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Low-rankness plays an important role in traditional machine learning, but is not so popular in deep learning. Most previous low-rank network compression methods compress the networks by approximating pre-trained models and re-training. However, the optimal solution in the Euclidean space may be quite different from the one in the low-rank manifold. A well-pre-trained model is not a good initialization for the model with low-rank constraints. Thus, the performance of a low-rank compressed network degrades significantly. Compared to other network compression methods such as pruning, low-rank methods attracts less attention in recent years. In this paper, we devise a new training method, low-rank projection with energy transfer (LRPET), that trains low-rank compressed networks from scratch and achieves competitive performance. First, we propose to alternately perform stochastic gradient descent training and projection onto the low-rank manifold. Compared to re-training on the compact model, this enables full utilization of model capacity since solution space is relaxed back to Euclidean space after projection. Second, the matrix energy (the sum of squares of singular values) reduction caused by projection is compensated by energy transfer. We uniformly transfer the energy of the pruned singular values to the remaining ones. We theoretically show that energy transfer eases the trend of gradient vanishing caused by projection. Third, we propose batch normalization (BN) rectification to cut off its effect on the optimal low-rank approximation of the weight matrix, which further improves the performance. Comprehensive experiments on CIFAR-10 and ImageNet have justified that our method is superior to other low-rank compression methods and also outperforms recent state-of-the-art pruning methods. Our code is available at https://github.com/BZQLin/LRPET.



### DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.05575v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.05575v1)
- **Published**: 2022-04-12 07:13:33+00:00
- **Updated**: 2022-04-12 07:13:33+00:00
- **Authors**: Haibao Yu, Yizhen Luo, Mao Shu, Yiyi Huo, Zebang Yang, Yifeng Shi, Zhenglong Guo, Hanyu Li, Xing Hu, Jirui Yuan, Zaiqing Nie
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: Autonomous driving faces great safety challenges for a lack of global perspective and the limitation of long-range perception capabilities. It has been widely agreed that vehicle-infrastructure cooperation is required to achieve Level 5 autonomy. However, there is still NO dataset from real scenarios available for computer vision researchers to work on vehicle-infrastructure cooperation-related problems. To accelerate computer vision research and innovation for Vehicle-Infrastructure Cooperative Autonomous Driving (VICAD), we release DAIR-V2X Dataset, which is the first large-scale, multi-modality, multi-view dataset from real scenarios for VICAD. DAIR-V2X comprises 71254 LiDAR frames and 71254 Camera frames, and all frames are captured from real scenes with 3D annotations. The Vehicle-Infrastructure Cooperative 3D Object Detection problem (VIC3D) is introduced, formulating the problem of collaboratively locating and identifying 3D objects using sensory inputs from both vehicle and infrastructure. In addition to solving traditional 3D object detection problems, the solution of VIC3D needs to consider the temporal asynchrony problem between vehicle and infrastructure sensors and the data transmission cost between them. Furthermore, we propose Time Compensation Late Fusion (TCLF), a late fusion framework for the VIC3D task as a benchmark based on DAIR-V2X. Find data, code, and more up-to-date information at https://thudair.baai.ac.cn/index and https://github.com/AIR-THU/DAIR-V2X.



### SwinNet: Swin Transformer drives edge-aware RGB-D and RGB-T salient object detection
- **Arxiv ID**: http://arxiv.org/abs/2204.05585v1
- **DOI**: 10.1109/TCSVT.2021.3127149
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05585v1)
- **Published**: 2022-04-12 07:37:39+00:00
- **Updated**: 2022-04-12 07:37:39+00:00
- **Authors**: Zhengyi Liu, Yacheng Tan, Qian He, Yun Xiao
- **Comment**: Online published in TCSVT
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology,
  2021
- **Summary**: Convolutional neural networks (CNNs) are good at extracting contexture features within certain receptive fields, while transformers can model the global long-range dependency features. By absorbing the advantage of transformer and the merit of CNN, Swin Transformer shows strong feature representation ability. Based on it, we propose a cross-modality fusion model SwinNet for RGB-D and RGB-T salient object detection. It is driven by Swin Transformer to extract the hierarchical features, boosted by attention mechanism to bridge the gap between two modalities, and guided by edge information to sharp the contour of salient object. To be specific, two-stream Swin Transformer encoder first extracts multi-modality features, and then spatial alignment and channel re-calibration module is presented to optimize intra-level cross-modality features. To clarify the fuzzy boundary, edge-guided decoder achieves inter-level cross-modality fusion under the guidance of edge features. The proposed model outperforms the state-of-the-art models on RGB-D and RGB-T datasets, showing that it provides more insight into the cross-modality complementarity task.



### Automatic detection of glaucoma via fundus imaging and artificial intelligence: A review
- **Arxiv ID**: http://arxiv.org/abs/2204.05591v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.05591v1)
- **Published**: 2022-04-12 07:47:13+00:00
- **Updated**: 2022-04-12 07:47:13+00:00
- **Authors**: Lauren Coan, Bryan Williams, Krishna Adithya Venkatesh, Swati Upadhyaya, Silvester Czanner, Rengaraj Venkatesh, Colin E. Willoughby, Srinivasan Kavitha, Gabriela Czanner
- **Comment**: None
- **Journal**: None
- **Summary**: Glaucoma is a leading cause of irreversible vision impairment globally and cases are continuously rising worldwide. Early detection is crucial, allowing timely intervention which can prevent further visual field loss. To detect glaucoma, examination of the optic nerve head via fundus imaging can be performed, at the centre of which is the assessment of the optic cup and disc boundaries. Fundus imaging is non-invasive and low-cost; however, the image examination relies on subjective, time-consuming, and costly expert assessments. A timely question to ask is can artificial intelligence mimic glaucoma assessments made by experts. Namely, can artificial intelligence automatically find the boundaries of the optic cup and disc (providing a so-called segmented fundus image) and then use the segmented image to identify glaucoma with high accuracy. We conducted a comprehensive review on artificial intelligence-enabled glaucoma detection frameworks that produce and use segmented fundus images. We found 28 papers and identified two main approaches: 1) logical rule-based frameworks, based on a set of simplistic decision rules; and 2) machine learning/statistical modelling based frameworks. We summarise the state-of-art of the two approaches and highlight the key hurdles to overcome for artificial intelligence-enabled glaucoma detection frameworks to be translated into clinical practice.



### On the Equity of Nuclear Norm Maximization in Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2204.05596v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05596v1)
- **Published**: 2022-04-12 07:55:47+00:00
- **Updated**: 2022-04-12 07:55:47+00:00
- **Authors**: Wenju Zhang, Xiang Zhang, Qing Liao, Long Lan, Mengzhu Wang, Wei Wang, Baoyun Peng, Zhengming Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Nuclear norm maximization has shown the power to enhance the transferability of unsupervised domain adaptation model (UDA) in an empirical scheme. In this paper, we identify a new property termed equity, which indicates the balance degree of predicted classes, to demystify the efficacy of nuclear norm maximization for UDA theoretically. With this in mind, we offer a new discriminability-and-equity maximization paradigm built on squares loss, such that predictions are equalized explicitly. To verify its feasibility and flexibility, two new losses termed Class Weighted Squares Maximization (CWSM) and Normalized Squares Maximization (NSM), are proposed to maximize both predictive discriminability and equity, from the class level and the sample level, respectively. Importantly, we theoretically relate these two novel losses (i.e., CWSM and NSM) to the equity maximization under mild conditions, and empirically suggest the importance of the predictive equity in UDA. Moreover, it is very efficient to realize the equity constraints in both losses. Experiments of cross-domain image classification on three popular benchmark datasets show that both CWSM and NSM contribute to outperforming the corresponding counterparts.



### HyperDet3D: Learning a Scene-conditioned 3D Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2204.05599v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05599v1)
- **Published**: 2022-04-12 07:57:58+00:00
- **Updated**: 2022-04-12 07:57:58+00:00
- **Authors**: Yu Zheng, Yueqi Duan, Jiwen Lu, Jie Zhou, Qi Tian
- **Comment**: to be published on CVPR2022
- **Journal**: None
- **Summary**: A bathtub in a library, a sink in an office, a bed in a laundry room -- the counter-intuition suggests that scene provides important prior knowledge for 3D object detection, which instructs to eliminate the ambiguous detection of similar objects. In this paper, we propose HyperDet3D to explore scene-conditioned prior knowledge for 3D object detection. Existing methods strive for better representation of local elements and their relations without scene-conditioned knowledge, which may cause ambiguity merely based on the understanding of individual points and object candidates. Instead, HyperDet3D simultaneously learns scene-agnostic embeddings and scene-specific knowledge through scene-conditioned hypernetworks. More specifically, our HyperDet3D not only explores the sharable abstracts from various 3D scenes, but also adapts the detector to the given scene at test time. We propose a discriminative Multi-head Scene-specific Attention (MSA) module to dynamically control the layer parameters of the detector conditioned on the fusion of scene-conditioned knowledge. Our HyperDet3D achieves state-of-the-art results on the 3D object detection benchmark of the ScanNet and SUN RGB-D datasets. Moreover, through cross-dataset evaluation, we show the acquired scene-conditioned prior knowledge still takes effect when facing 3D scenes with domain gap.



### Towards Open-Set Object Detection and Discovery
- **Arxiv ID**: http://arxiv.org/abs/2204.05604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05604v1)
- **Published**: 2022-04-12 08:07:01+00:00
- **Updated**: 2022-04-12 08:07:01+00:00
- **Authors**: Jiyang Zheng, Weihao Li, Jie Hong, Lars Petersson, Nick Barnes
- **Comment**: CVPRW 2022
- **Journal**: None
- **Summary**: With the human pursuit of knowledge, open-set object detection (OSOD) has been designed to identify unknown objects in a dynamic world. However, an issue with the current setting is that all the predicted unknown objects share the same category as "unknown", which require incremental learning via a human-in-the-loop approach to label novel classes. In order to address this problem, we present a new task, namely Open-Set Object Detection and Discovery (OSODD). This new task aims to extend the ability of open-set object detectors to further discover the categories of unknown objects based on their visual appearance without human effort. We propose a two-stage method that first uses an open-set object detector to predict both known and unknown objects. Then, we study the representation of predicted objects in an unsupervised manner and discover new categories from the set of unknown objects. With this method, a detector is able to detect objects belonging to known classes and define novel categories for objects of unknown classes with minimal supervision. We show the performance of our model on the MS-COCO dataset under a thorough evaluation protocol. We hope that our work will promote further research towards a more robust real-world detection system.



### Regression or Classification? Reflection on BP prediction from PPG data using Deep Neural Networks in the scope of practical applications
- **Arxiv ID**: http://arxiv.org/abs/2204.05605v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.05605v1)
- **Published**: 2022-04-12 08:07:38+00:00
- **Updated**: 2022-04-12 08:07:38+00:00
- **Authors**: Fabian Schrumpf, Paul Rudi Serdack, Mirco Fuchs
- **Comment**: Submitted to International Workshop on Computer Vision for
  Physiological Measurement (CVPM 2022); Workshop at the Conference on Computer
  Vision and Pattern Recognition (CVPR) 2022
- **Journal**: None
- **Summary**: Photoplethysmographic (PPG) signals offer diagnostic potential beyond heart rate analysis or blood oxygen level monitoring. In the recent past, research focused extensively on non-invasive PPG-based approaches to blood pressure (BP) estimation. These approaches can be subdivided into regression and classification methods. The latter assign PPG signals to predefined BP intervals that represent clinically relevant ranges. The former predict systolic (SBP) and diastolic (DBP) BP as continuous variables and are of particular interest to the research community. However, the reported accuracies of BP regression methods vary widely among publications with some authors even questioning the feasibility of PPG-based BP regression altogether. In our work, we compare BP regression and classification approaches. We argue that BP classification might provide diagnostic value that is equivalent to regression in many clinically relevant scenarios while being similar or even superior in terms of performance. We compare several established neural architectures using publicly available PPG data for SBP regression and classification with and without personalization using subject-specific data. We found that classification and regression models perform similar before personalization. However, after personalization, the accuracy of classification based methods outperformed regression approaches. We conclude that BP classification might be preferable over BP regression in certain scenarios where a coarser segmentation of the BP range is sufficient.



### Continual Predictive Learning from Videos
- **Arxiv ID**: http://arxiv.org/abs/2204.05624v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.05624v1)
- **Published**: 2022-04-12 08:32:26+00:00
- **Updated**: 2022-04-12 08:32:26+00:00
- **Authors**: Geng Chen, Wendong Zhang, Han Lu, Siyu Gao, Yunbo Wang, Mingsheng Long, Xiaokang Yang
- **Comment**: Accepted by CVPR 2022 (Oral). Code is available at
  https://github.com/jc043/CPL
- **Journal**: None
- **Summary**: Predictive learning ideally builds the world model of physical processes in one or more given environments. Typical setups assume that we can collect data from all environments at all times. In practice, however, different prediction tasks may arrive sequentially so that the environments may change persistently throughout the training procedure. Can we develop predictive learning algorithms that can deal with more realistic, non-stationary physical environments? In this paper, we study a new continual learning problem in the context of video prediction, and observe that most existing methods suffer from severe catastrophic forgetting in this setup. To tackle this problem, we propose the continual predictive learning (CPL) approach, which learns a mixture world model via predictive experience replay and performs test-time adaptation with non-parametric task inference. We construct two new benchmarks based on RoboNet and KTH, in which different tasks correspond to different physical robotic environments or human actions. Our approach is shown to effectively mitigate forgetting and remarkably outperform the na\"ive combinations of previous art in video prediction and continual learning.



### X-DETR: A Versatile Architecture for Instance-wise Vision-Language Tasks
- **Arxiv ID**: http://arxiv.org/abs/2204.05626v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.05626v1)
- **Published**: 2022-04-12 08:34:42+00:00
- **Updated**: 2022-04-12 08:34:42+00:00
- **Authors**: Zhaowei Cai, Gukyeong Kwon, Avinash Ravichandran, Erhan Bas, Zhuowen Tu, Rahul Bhotika, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the challenging instance-wise vision-language tasks, where the free-form language is required to align with the objects instead of the whole image. To address these tasks, we propose X-DETR, whose architecture has three major components: an object detector, a language encoder, and vision-language alignment. The vision and language streams are independent until the end and they are aligned using an efficient dot-product operation. The whole network is trained end-to-end, such that the detector is optimized for the vision-language tasks instead of an off-the-shelf component. To overcome the limited size of paired object-language annotations, we leverage other weak types of supervision to expand the knowledge coverage. This simple yet effective architecture of X-DETR shows good accuracy and fast speeds for multiple instance-wise vision-language tasks, e.g., 16.4 AP on LVIS detection of 1.2K categories at ~20 frames per second without using any LVIS annotation during training.



### How to Register a Live onto a Liver ? Partial Matching in the Space of Varifolds
- **Arxiv ID**: http://arxiv.org/abs/2204.05665v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph, 65D19 (Primary) 68U10, 68U07 (Secondary), I.5.4; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2204.05665v1)
- **Published**: 2022-04-12 09:50:47+00:00
- **Updated**: 2022-04-12 09:50:47+00:00
- **Authors**: Pierre-Louis Antonsanti, Thomas Benseghir, Vincent Jugnon, Mario Ghosn, Perrine Chassat, Irène Kaltenmark, Joan Glaunès
- **Comment**: 30 pages, 11 figures, Special Issue: Information Processing in
  Medical Imaging (IPMI) 2021, Accepted for publication at the Journal of
  Machine Learning for Biomedical Imaging (MELBA) https://www.melba-journal.org
- **Journal**: Journal of Machine Learning for Biomedical Imaging (MELBA) 2022
- **Summary**: Partial shapes correspondences is a problem that often occurs in computer vision (occlusion, evolution in time...). In medical imaging, data may come from different modalities and be acquired under different conditions which leads to variations in shapes and topologies. In this paper we use an asymmetric data dissimilarity term applicable to various geometric shapes like sets of curves or surfaces, assessing the embedding of a shape into another one without relying on correspondences. It is designed as a data attachment for the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework, allowing to compute a meaningful deformation of one shape onto a subset of the other. We refine it in order to control the resulting non-rigid deformations and provide consistent deformations of the shapes along with their ambient space. We show that partial matching can be used for robust multi-modal liver registration between a Computed Tomography (CT) volume and a Cone Beam Computed Tomography (CBCT) volume. The 3D imaging of the patient CBCT at point of care that we call live is truncated while the CT pre-intervention provides a full visualization of the liver. The proposed method allows the truncated surfaces from CBCT to be aligned non-rigidly, yet realistically, with surfaces from CT with an average distance of 2.6mm(+/- 2.2). The generated deformations extend consistently to the liver volume, and are evaluated on points of interest for the physicians, with an average distance of 5.8mm (+/- 2.7) for vessels bifurcations and 5.13mm (+/- 2.5) for tumors landmarks. Such multi-modality volumes registrations would help the physicians in the perspective of navigating their tools in the patient's anatomy to locate structures that are hardly visible in the CBCT used during their procedures. Our code is available at https://github.com/plantonsanti/PartialMatchingVarifolds.



### Three-Stream Joint Network for Zero-Shot Sketch-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2204.05666v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05666v1)
- **Published**: 2022-04-12 09:52:17+00:00
- **Updated**: 2022-04-12 09:52:17+00:00
- **Authors**: Yu-Wei Zhan, Xin Luo, Yongxin Wang, Zhen-Duo Chen, Xin-Shun Xu
- **Comment**: None
- **Journal**: None
- **Summary**: The Zero-Shot Sketch-based Image Retrieval (ZS-SBIR) is a challenging task because of the large domain gap between sketches and natural images as well as the semantic inconsistency between seen and unseen categories. Previous literature bridges seen and unseen categories by semantic embedding, which requires prior knowledge of the exact class names and additional extraction efforts. And most works reduce domain gap by mapping sketches and natural images into a common high-level space using constructed sketch-image pairs, which ignore the unpaired information between images and sketches. To address these issues, in this paper, we propose a novel Three-Stream Joint Training Network (3JOIN) for the ZS-SBIR task. To narrow the domain differences between sketches and images, we extract edge maps for natural images and treat them as a bridge between images and sketches, which have similar content to images and similar style to sketches. For exploiting a sufficient combination of sketches, natural images, and edge maps, a novel three-stream joint training network is proposed. In addition, we use a teacher network to extract the implicit semantics of the samples without the aid of other semantics and transfer the learned knowledge to unseen classes. Extensive experiments conducted on two real-world datasets demonstrate the superiority of our proposed method.



### 3DeformRS: Certifying Spatial Deformations on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2204.05687v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05687v1)
- **Published**: 2022-04-12 10:24:31+00:00
- **Updated**: 2022-04-12 10:24:31+00:00
- **Authors**: Gabriel Pérez S., Juan C. Pérez, Motasem Alfarra, Silvio Giancola, Bernard Ghanem
- **Comment**: Accepted at CVPR 2022
- **Journal**: None
- **Summary**: 3D computer vision models are commonly used in security-critical applications such as autonomous driving and surgical robotics. Emerging concerns over the robustness of these models against real-world deformations must be addressed practically and reliably. In this work, we propose 3DeformRS, a method to certify the robustness of point cloud Deep Neural Networks (DNNs) against real-world deformations. We developed 3DeformRS by building upon recent work that generalized Randomized Smoothing (RS) from pixel-intensity perturbations to vector-field deformations. In particular, we specialized RS to certify DNNs against parameterized deformations (e.g. rotation, twisting), while enjoying practical computational costs. We leverage the virtues of 3DeformRS to conduct a comprehensive empirical study on the certified robustness of four representative point cloud DNNs on two datasets and against seven different deformations. Compared to previous approaches for certifying point cloud DNNs, 3DeformRS is fast, scales well with point cloud size, and provides comparable-to-better certificates. For instance, when certifying a plain PointNet against a 3{\deg} z-rotation on 1024-point clouds, 3DeformRS grants a certificate 3x larger and 20x faster than previous work.



### Super-Resolution for Selfie Biometrics: Introduction and Application to Face and Iris
- **Arxiv ID**: http://arxiv.org/abs/2204.05688v1
- **DOI**: 10.1007/978-3-030-26972-2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05688v1)
- **Published**: 2022-04-12 10:28:31+00:00
- **Updated**: 2022-04-12 10:28:31+00:00
- **Authors**: Fernando Alonso-Fernandez, Reuben A. Farrugia, Julian Fierrez, Josef Bigun
- **Comment**: Published at Springer book on "Selfie Biometrics" (A Rattani, R.
  Derakhshani, A. Ross, Eds.), pp. 105-128, Springer 2019, ISBN
  978-3-030-26972-2
- **Journal**: None
- **Summary**: The lack of resolution has a negative impact on the performance of image-based biometrics. Many applications which are becoming ubiquitous in mobile devices do not operate in a controlled environment, and their performance significantly drops due to the lack of pixel resolution. While many generic super-resolution techniques have been studied to restore low-resolution images for biometrics, the results obtained are not always as desired. Those generic methods are usually aimed to enhance the visual appearance of the scene. However, producing an overall visual enhancement of biometric images does not necessarily correlate with a better recognition performance. Such techniques are designed to restore generic images and therefore do not exploit the specific structure found in biometric images (e.g. iris or faces), which causes the solution to be sub-optimal. For this reason, super-resolution techniques have to be adapted for the particularities of images from a specific biometric modality. In recent years, there has been an increased interest in the application of super-resolution to different biometric modalities, such as face iris, gait or fingerprint. This chapter presents an overview of recent advances in super-resolution reconstruction of face and iris images, which are the two prevalent modalities in selfie biometrics. We also provide experimental results using several state-of-the-art reconstruction algorithms, demonstrating the benefits of using super-resolution to improve the quality of face and iris images prior to classification. In the reported experiments, we study the application of super-resolution to face and iris images captured in the visible range, using experimental setups that represent well the selfie biometrics scenario.



### Unsupervised Anomaly and Change Detection with Multivariate Gaussianization
- **Arxiv ID**: http://arxiv.org/abs/2204.05699v1
- **DOI**: 10.1109/TGRS.2021.3116186
- **Categories**: **cs.LG**, cs.CV, physics.comp-ph, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2204.05699v1)
- **Published**: 2022-04-12 10:52:33+00:00
- **Updated**: 2022-04-12 10:52:33+00:00
- **Authors**: José A. Padrón-Hidalgo, Valero Laparra, Gustau Camps-Valls
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp.
  1-10, 2022, Art no. 5513010
- **Summary**: Anomaly detection is a field of intense research. Identifying low probability events in data/images is a challenging problem given the high-dimensionality of the data, especially when no (or little) information about the anomaly is available a priori. While plenty of methods are available, the vast majority of them do not scale well to large datasets and require the choice of some (very often critical) hyperparameters. Therefore, unsupervised and computationally efficient detection methods become strictly necessary. We propose an unsupervised method for detecting anomalies and changes in remote sensing images by means of a multivariate Gaussianization methodology that allows to estimate multivariate densities accurately, a long-standing problem in statistics and machine learning. The methodology transforms arbitrarily complex multivariate data into a multivariate Gaussian distribution. Since the transformation is differentiable, by applying the change of variables formula one can estimate the probability at any point of the original domain. The assumption is straightforward: pixels with low estimated probability are considered anomalies. Our method can describe any multivariate distribution, makes an efficient use of memory and computational resources, and is parameter-free. We show the efficiency of the method in experiments involving both anomaly detection and change detection in different remote sensing image sets. Results show that our approach outperforms other linear and nonlinear methods in terms of detection power in both anomaly and change detection scenarios, showing robustness and scalability to dimensionality and sample sizes.



### Back to the Roots: Reconstructing Large and Complex Cranial Defects using an Image-based Statistical Shape Model
- **Arxiv ID**: http://arxiv.org/abs/2204.05703v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.05703v1)
- **Published**: 2022-04-12 10:58:05+00:00
- **Updated**: 2022-04-12 10:58:05+00:00
- **Authors**: Jianning Li, David G. Ellis, Antonio Pepe, Christina Gsaxner, Michele R. Aizenberg, Jens Kleesiek, Jan Egger
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Designing implants for large and complex cranial defects is a challenging task, even for professional designers. Current efforts on automating the design process focused mainly on convolutional neural networks (CNN), which have produced state-of-the-art results on reconstructing synthetic defects. However, existing CNN-based methods have been difficult to translate to clinical practice in cranioplasty, as their performance on complex and irregular cranial defects remains unsatisfactory. In this paper, a statistical shape model (SSM) built directly on the segmentation masks of the skulls is presented. We evaluate the SSM on several cranial implant design tasks, and the results show that, while the SSM performs suboptimally on synthetic defects compared to CNN-based approaches, it is capable of reconstructing large and complex defects with only minor manual corrections. The quality of the resulting implants is examined and assured by experienced neurosurgeons. In contrast, CNN-based approaches, even with massive data augmentation, fail or produce less-than-satisfactory implants for these cases. Codes are publicly available at https://github.com/Jianningli/ssm



### GARF: Gaussian Activated Radiance Fields for High Fidelity Reconstruction and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2204.05735v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05735v1)
- **Published**: 2022-04-12 12:14:39+00:00
- **Updated**: 2022-04-12 12:14:39+00:00
- **Authors**: Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, Simon Lucey
- **Comment**: Project page: https://sfchng.github.io/garf/
- **Journal**: None
- **Summary**: Despite Neural Radiance Fields (NeRF) showing compelling results in photorealistic novel views synthesis of real-world scenes, most existing approaches require accurate prior camera poses. Although approaches for jointly recovering the radiance field and camera pose exist (BARF), they rely on a cumbersome coarse-to-fine auxiliary positional embedding to ensure good performance. We present Gaussian Activated neural Radiance Fields (GARF), a new positional embedding-free neural radiance field architecture - employing Gaussian activations - that outperforms the current state-of-the-art in terms of high fidelity reconstruction and pose estimation.



### LifeLonger: A Benchmark for Continual Disease Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.05737v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2204.05737v2)
- **Published**: 2022-04-12 12:25:05+00:00
- **Updated**: 2022-06-30 11:45:09+00:00
- **Authors**: Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Tom van Sonsbeek, Xiantong Zhen, Dwarikanath Mahapatra, Marcel Worring, Cees G. M. Snoek
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have shown a great effectiveness in recognition of findings in medical images. However, they cannot handle the ever-changing clinical environment, bringing newly annotated medical data from different sources. To exploit the incoming streams of data, these models would benefit largely from sequentially learning from new samples, without forgetting the previously obtained knowledge. In this paper we introduce LifeLonger, a benchmark for continual disease classification on the MedMNIST collection, by applying existing state-of-the-art continual learning methods. In particular, we consider three continual learning scenarios, namely, task and class incremental learning and the newly defined cross-domain incremental learning. Task and class incremental learning of diseases address the issue of classifying new samples without re-training the models from scratch, while cross-domain incremental learning addresses the issue of dealing with datasets originating from different institutions while retaining the previously obtained knowledge. We perform a thorough analysis of the performance and examine how the well-known challenges of continual learning, such as the catastrophic forgetting exhibit themselves in this setting. The encouraging results demonstrate that continual learning has a major potential to advance disease classification and to produce a more robust and efficient learning framework for clinical settings. The code repository, data partitions and baseline results for the complete benchmark will be made publicly available.



### Examining the Proximity of Adversarial Examples to Class Manifolds in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.05764v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.05764v1)
- **Published**: 2022-04-12 12:55:37+00:00
- **Updated**: 2022-04-12 12:55:37+00:00
- **Authors**: Štefan Pócoš, Iveta Bečková, Igor Farkaš
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks achieve remarkable performance in multiple fields. However, after proper training they suffer from an inherent vulnerability against adversarial examples (AEs). In this work we shed light on inner representations of the AEs by analysing their activations on the hidden layers. We test various types of AEs, each crafted using a specific norm constraint, which affects their visual appearance and eventually their behavior in the trained networks. Our results in image classification tasks (MNIST and CIFAR-10) reveal qualitative differences between the individual types of AEs, when comparing their proximity to the class-specific manifolds on the inner representations. We propose two methods that can be used to compare the distances to class-specific manifolds, regardless of the changing dimensions throughout the network. Using these methods, we consistently confirm that some of the adversarials do not necessarily leave the proximity of the manifold of the correct class, not even in the last hidden layer of the neural network. Next, using UMAP visualisation technique, we project the class activations to 2D space. The results indicate that the activations of the individual AEs are entangled with the activations of the test set. This, however, does not hold for a group of crafted inputs called the rubbish class. We also confirm the entanglement of adversarials with the test set numerically using the soft nearest neighbour loss.



### GORDA: Graph-based ORientation Distribution Analysis of SLI scatterometry Patterns of Nerve Fibres
- **Arxiv ID**: http://arxiv.org/abs/2204.05776v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2204.05776v1)
- **Published**: 2022-04-12 13:02:45+00:00
- **Updated**: 2022-04-12 13:02:45+00:00
- **Authors**: Esteban Vaca, Miriam Menzel, Katrin Amunts, Markus Axer, Timo Dickscheid
- **Comment**: 5 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: Scattered Light Imaging (SLI) is a novel approach for microscopically revealing the fibre architecture of unstained brain sections. The measurements are obtained by illuminating brain sections from different angles and measuring the transmitted (scattered) light under normal incidence. The evaluation of scattering profiles commonly relies on a peak picking technique and feature extraction from the peaks, which allows quantitative determination of parallel and crossing in-plane nerve fibre directions for each image pixel. However, the estimation of the 3D orientation of the fibres cannot be assessed with the traditional methodology. We propose an unsupervised learning approach using spherical convolutions for estimating the 3D orientation of neural fibres, resulting in a more detailed interpretation of the fibre orientation distributions in the brain.



### Unsupervised Anomaly Detection in 3D Brain MRI using Deep Learning with impured training data
- **Arxiv ID**: http://arxiv.org/abs/2204.05778v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.05778v1)
- **Published**: 2022-04-12 13:05:18+00:00
- **Updated**: 2022-04-12 13:05:18+00:00
- **Authors**: Finn Behrendt, Marcel Bengs, Frederik Rogge, Julia Krüger, Roland Opfer, Alexander Schlaefer
- **Comment**: Accepted for publication at the ISBI22 conference
- **Journal**: None
- **Summary**: The detection of lesions in magnetic resonance imaging (MRI)-scans of human brains remains challenging, time-consuming and error-prone. Recently, unsupervised anomaly detection (UAD) methods have shown promising results for this task. These methods rely on training data sets that solely contain healthy samples. Compared to supervised approaches, this significantly reduces the need for an extensive amount of labeled training data. However, data labelling remains error-prone. We study how unhealthy samples within the training data affect anomaly detection performance for brain MRI-scans. For our evaluations, we consider three publicly available data sets and use autoencoders (AE) as a well-established baseline method for UAD. We systematically evaluate the effect of impured training data by injecting different quantities of unhealthy samples to our training set of healthy samples from T1-weighted MRI-scans. We evaluate a method to identify falsely labeled samples directly during training based on the reconstruction error of the AE. Our results show that training with impured data decreases the UAD performance notably even with few falsely labeled samples. By performing outlier removal directly during training based on the reconstruction-loss, we demonstrate that falsely labeled data can be detected and removed to mitigate the effect of falsely labeled data. Overall, we highlight the importance of clean data sets for UAD in brain MRI and demonstrate an approach for detecting falsely labeled data directly during training.



### Hypercomplex Neural Architectures for Multi-View Breast Cancer Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.05798v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.05798v2)
- **Published**: 2022-04-12 13:32:31+00:00
- **Updated**: 2022-12-13 23:32:39+00:00
- **Authors**: Eleonora Lopez, Eleonora Grassucci, Martina Valleriani, Danilo Comminiello
- **Comment**: This paper has been submitted to IEEE Transactions on Neural Networks
  and Learning Systems
- **Journal**: None
- **Summary**: Traditionally, deep learning methods for breast cancer classification perform a single-view analysis. However, radiologists simultaneously analyze all four views that compose a mammography exam, owing to the correlations contained in mammography views, which present crucial information for identifying tumors. In light of this, some studies have started to propose multi-view methods. Nevertheless, in such existing architectures, mammogram views are processed as independent images by separate convolutional branches, thus losing correlations among them. To overcome such limitations, in this paper we propose a novel approach for multi-view breast cancer classification based on parameterized hypercomplex neural networks. Thanks to hypercomplex algebra properties, our networks are able to model, and thus leverage, existing correlations between the different views that comprise a mammogram, thus mimicking the reading process performed by clinicians. The proposed methods are able to handle the information of a patient altogether without breaking the multi-view nature of the exam. We define architectures designed to process two-view exams, namely PHResNets, and four-view exams, i.e., PHYSEnet and PHYBOnet. Through an extensive experimental evaluation conducted with publicly available datasets, we demonstrate that our proposed models clearly outperform real-valued counterparts and also state-of-the-art methods, proving that breast cancer classification benefits from the proposed multi-view architectures. We also assess the method's robustness beyond mammogram analysis by considering different benchmarks, as well as a finer-scaled task such as segmentation. Full code and pretrained models for complete reproducibility of our experiments are freely available at: https://github.com/ispamm/PHBreast.



### EVOPS Benchmark: Evaluation of Plane Segmentation from RGBD and LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/2204.05799v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.05799v2)
- **Published**: 2022-04-12 13:34:40+00:00
- **Updated**: 2022-08-24 11:01:14+00:00
- **Authors**: Anastasiia Kornilova, Dmitrii Iarosh, Denis Kukushkin, Nikolai Goncharov, Pavel Mokeev, Arthur Saliou, Gonzalo Ferrer
- **Comment**: Accepted to IROS'2022
- **Journal**: None
- **Summary**: This paper provides the EVOPS dataset for plane segmentation from 3D data, both from RGBD images and LiDAR point clouds. We have designed two annotation methodologies (RGBD and LiDAR) running on well-known and widely-used datasets for SLAM evaluation and we have provided a complete set of benchmarking tools including point, planes and segmentation metrics. The data includes a total number of 10k RGBD and 7K LiDAR frames over different selected scenes which consist of high quality segmented planes. The experiments report quality of SOTA methods for RGBD plane segmentation on our annotated data. We also have provided learnable baseline for plane segmentation in LiDAR point clouds. All labeled data and benchmark tools used have been made publicly available at https://evops.netlify.app/.



### Adaptive Cross-Attention-Driven Spatial-Spectral Graph Convolutional Network for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.05823v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.05823v1)
- **Published**: 2022-04-12 14:06:11+00:00
- **Updated**: 2022-04-12 14:06:11+00:00
- **Authors**: Jin-Yu Yang, Heng-Chao Li, Wen-Shuai Hu, Lei Pan, Qian Du
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, graph convolutional networks (GCNs) have been developed to explore spatial relationship between pixels, achieving better classification performance of hyperspectral images (HSIs). However, these methods fail to sufficiently leverage the relationship between spectral bands in HSI data. As such, we propose an adaptive cross-attention-driven spatial-spectral graph convolutional network (ACSS-GCN), which is composed of a spatial GCN (Sa-GCN) subnetwork, a spectral GCN (Se-GCN) subnetwork, and a graph cross-attention fusion module (GCAFM). Specifically, Sa-GCN and Se-GCN are proposed to extract the spatial and spectral features by modeling correlations between spatial pixels and between spectral bands, respectively. Then, by integrating attention mechanism into information aggregation of graph, the GCAFM, including three parts, i.e., spatial graph attention block, spectral graph attention block, and fusion block, is designed to fuse the spatial and spectral features and suppress noise interference in Sa-GCN and Se-GCN. Moreover, the idea of the adaptive graph is introduced to explore an optimal graph through back propagation during the training process. Experiments on two HSI data sets show that the proposed method achieves better performance than other classification methods.



### Generative Negative Replay for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.05842v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2204.05842v1)
- **Published**: 2022-04-12 14:38:00+00:00
- **Updated**: 2022-04-12 14:38:00+00:00
- **Authors**: Gabriele Graffieti, Davide Maltoni, Lorenzo Pellegrini, Vincenzo Lomonaco
- **Comment**: 18 pages, 10 figures, 16 tables, 2 algorithms. Under review
- **Journal**: None
- **Summary**: Learning continually is a key aspect of intelligence and a necessary ability to solve many real-life problems. One of the most effective strategies to control catastrophic forgetting, the Achilles' heel of continual learning, is storing part of the old data and replaying them interleaved with new experiences (also known as the replay approach). Generative replay, which is using generative models to provide replay patterns on demand, is particularly intriguing, however, it was shown to be effective mainly under simplified assumptions, such as simple scenarios and low-dimensional data. In this paper, we show that, while the generated data are usually not able to improve the classification accuracy for the old classes, they can be effective as negative examples (or antagonists) to better learn the new classes, especially when the learning experiences are small and contain examples of just one or few classes. The proposed approach is validated on complex class-incremental and data-incremental continual learning scenarios (CORe50 and ImageNet-1000) composed of high-dimensional data and a large number of training experiences: a setup where existing generative replay approaches usually fail.



### Probabilistic Compositional Embeddings for Multimodal Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2204.05845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.05845v1)
- **Published**: 2022-04-12 14:45:37+00:00
- **Updated**: 2022-04-12 14:45:37+00:00
- **Authors**: Andrei Neculai, Yanbei Chen, Zeynep Akata
- **Comment**: CVPR2022 MULA workshop
- **Journal**: None
- **Summary**: Existing works in image retrieval often consider retrieving images with one or two query inputs, which do not generalize to multiple queries. In this work, we investigate a more challenging scenario for composing multiple multimodal queries in image retrieval. Given an arbitrary number of query images and (or) texts, our goal is to retrieve target images containing the semantic concepts specified in multiple multimodal queries. To learn an informative embedding that can flexibly encode the semantics of various queries, we propose a novel multimodal probabilistic composer (MPC). Specifically, we model input images and texts as probabilistic embeddings, which can be further composed by a probabilistic composition rule to facilitate image retrieval with multiple multimodal queries. We propose a new benchmark based on the MS-COCO dataset and evaluate our model on various setups that compose multiple images and (or) text queries for multimodal image retrieval. Without bells and whistles, we show that our probabilistic model formulation significantly outperforms existing related methods on multimodal image retrieval while generalizing well to query with different amounts of inputs given in arbitrary visual and (or) textual modalities. Code is available here: https://github.com/andreineculai/MPC.



### Bootstrap Motion Forecasting With Self-Consistent Constraints
- **Arxiv ID**: http://arxiv.org/abs/2204.05859v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.05859v3)
- **Published**: 2022-04-12 14:59:48+00:00
- **Updated**: 2023-08-17 13:47:44+00:00
- **Authors**: Maosheng Ye, Jiamiao Xu, Xunnong Xu, Tengfei Wang, Tongyi Cao, Qifeng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel framework for motion forecasting with Dual Consistency Constraints and Multi-Pseudo-Target supervision. The motion forecasting task predicts future trajectories of vehicles by incorporating spatial and temporal information from the past. A key design of DCMS is the proposed Dual Consistency Constraints that regularize the predicted trajectories under spatial and temporal perturbation during the training stage. In addition, we design a novel self-ensembling scheme to obtain accurate pseudo targets to model the multi-modality in motion forecasting through supervision with multiple targets explicitly, namely Multi-Pseudo-Target supervision. Our experimental results on the Argoverse motion forecasting benchmark show that DCMS significantly outperforms the state-of-the-art methods, achieving 1st place on the leaderboard. We also demonstrate that our proposed strategies can be incorporated into other motion forecasting approaches as general training schemes.



### Semantic keypoint-based pose estimation from single RGB frames
- **Arxiv ID**: http://arxiv.org/abs/2204.05864v1
- **DOI**: 10.55417/fr.2022006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05864v1)
- **Published**: 2022-04-12 15:03:51+00:00
- **Updated**: 2022-04-12 15:03:51+00:00
- **Authors**: Karl Schmeckpeper, Philip R. Osteen, Yufu Wang, Georgios Pavlakos, Kenneth Chaney, Wyatt Jordan, Xiaowei Zhou, Konstantinos G. Derpanis, Kostas Daniilidis
- **Comment**: https://sites.google.com/view/rcta-object-keypoints-dataset/home.
  arXiv admin note: substantial text overlap with arXiv:1703.04670
- **Journal**: Field Robotics, 2, 147-171, 2022
- **Summary**: This paper presents an approach to estimating the continuous 6-DoF pose of an object from a single RGB image. The approach combines semantic keypoints predicted by a convolutional network (convnet) with a deformable shape model. Unlike prior investigators, we are agnostic to whether the object is textured or textureless, as the convnet learns the optimal representation from the available training-image data. Furthermore, the approach can be applied to instance- and class-based pose recovery. Additionally, we accompany our main pipeline with a technique for semi-automatic data generation from unlabeled videos. This procedure allows us to train the learnable components of our method with minimal manual intervention in the labeling process. Empirically, we show that our approach can accurately recover the 6-DoF object pose for both instance- and class-based scenarios even against a cluttered background. We apply our approach both to several, existing, large-scale datasets - including PASCAL3D+, LineMOD-Occluded, YCB-Video, and TUD-Light - and, using our labeling pipeline, to a new dataset with novel object classes that we introduce here. Extensive empirical evaluations show that our approach is able to provide pose estimation results comparable to the state of the art.



### Exploring Event Camera-based Odometry for Planetary Robots
- **Arxiv ID**: http://arxiv.org/abs/2204.05880v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.05880v2)
- **Published**: 2022-04-12 15:19:50+00:00
- **Updated**: 2022-07-29 14:26:36+00:00
- **Authors**: Florian Mahlknecht, Daniel Gehrig, Jeremy Nash, Friedrich M. Rockenbauer, Benjamin Morrell, Jeff Delaune, Davide Scaramuzza
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters (RA-L), 2022
- **Summary**: Due to their resilience to motion blur and high robustness in low-light and high dynamic range conditions, event cameras are poised to become enabling sensors for vision-based exploration on future Mars helicopter missions. However, existing event-based visual-inertial odometry (VIO) algorithms either suffer from high tracking errors or are brittle, since they cannot cope with significant depth uncertainties caused by an unforeseen loss of tracking or other effects. In this work, we introduce EKLT-VIO, which addresses both limitations by combining a state-of-the-art event-based frontend with a filter-based backend. This makes it both accurate and robust to uncertainties, outperforming event- and frame-based VIO algorithms on challenging benchmarks by 32%. In addition, we demonstrate accurate performance in hover-like conditions (outperforming existing event-based methods) as well as high robustness in newly collected Mars-like and high-dynamic-range sequences, where existing frame-based methods fail. In doing so, we show that event-based VIO is the way forward for vision-based exploration on Mars.



### VisCUIT: Visual Auditor for Bias in CNN Image Classifier
- **Arxiv ID**: http://arxiv.org/abs/2204.05899v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.05899v2)
- **Published**: 2022-04-12 15:51:09+00:00
- **Updated**: 2022-04-13 13:27:30+00:00
- **Authors**: Seongmin Lee, Zijie J. Wang, Judy Hoffman, Duen Horng Chau
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: CNN image classifiers are widely used, thanks to their efficiency and accuracy. However, they can suffer from biases that impede their practical applications. Most existing bias investigation techniques are either inapplicable to general image classification tasks or require significant user efforts in perusing all data subgroups to manually specify which data attributes to inspect. We present VisCUIT, an interactive visualization system that reveals how and why a CNN classifier is biased. VisCUIT visually summarizes the subgroups on which the classifier underperforms and helps users discover and characterize the cause of the underperformances by revealing image concepts responsible for activating neurons that contribute to misclassifications. VisCUIT runs in modern browsers and is open-source, allowing people to easily access and extend the tool to other model architectures and datasets. VisCUIT is available at the following public demo link: https://poloclub.github.io/VisCUIT. A video demo is available at https://youtu.be/eNDbSyM4R_4.



### Label Distribution Learning for Generalizable Multi-source Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2204.05903v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05903v3)
- **Published**: 2022-04-12 15:59:10+00:00
- **Updated**: 2022-08-25 01:11:44+00:00
- **Authors**: Lei Qi, Jiaying Shen, Jiaqi Liu, Yinghuan Shi, Xin Geng
- **Comment**: Accepted by IEEE Transactions on Information Forensics and Security
  (TIFS). arXiv admin note: text overlap with arXiv:2201.09846
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) is a critical technique in the video surveillance system, which has achieved significant success in the supervised setting. However, it is difficult to directly apply the supervised model to arbitrary unseen domains due to the domain gap between the available source domains and unseen target domains. In this paper, we propose a novel label distribution learning (LDL) method to address the generalizable multi-source person Re-ID task (i.e., there are multiple available source domains, and the testing domain is unseen during training), which aims to explore the relation of different classes and mitigate the domain-shift across different domains so as to improve the discrimination of the model and learn the domain-invariant feature, simultaneously. Specifically, during the training process, we produce the label distribution via the online manner to mine the relation information of different classes, thus it is beneficial for extracting the discriminative feature. Besides, for the label distribution of each class, we further revise it to give more and equal attention to the other domains that the class does not belong to, which can effectively reduce the domain gap across different domains and obtain the domain-invariant feature. Furthermore, we also give the theoretical analysis to demonstrate that the proposed method can effectively deal with the domain-shift issue. Extensive experiments on multiple benchmark datasets validate the effectiveness of the proposed method and show that the proposed method can outperform the state-of-the-art methods. Besides, further analysis also reveals the superiority of the proposed method.



### Few-shot Forgery Detection via Guided Adversarial Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2204.05905v2
- **DOI**: 10.1016/j.patcog.2023.109863
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05905v2)
- **Published**: 2022-04-12 16:05:10+00:00
- **Updated**: 2023-08-27 16:55:27+00:00
- **Authors**: Haonan Qiu, Siyu Chen, Bei Gan, Kun Wang, Huafeng Shi, Jing Shao, Ziwei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The increase in face manipulation models has led to a critical issue in society - the synthesis of realistic visual media. With the emergence of new forgery approaches at an unprecedented rate, existing forgery detection methods suffer from significant performance drops when applied to unseen novel forgery approaches. In this work, we address the few-shot forgery detection problem by 1) designing a comprehensive benchmark based on coverage analysis among various forgery approaches, and 2) proposing Guided Adversarial Interpolation (GAI). Our key insight is that there exist transferable distribution characteristics between majority and minority forgery classes1. Specifically, we enhance the discriminative ability against novel forgery approaches via adversarially interpolating the forgery artifacts of the minority samples to the majority samples under the guidance of a teacher network. Unlike the standard re-balancing method which usually results in over-fitting to minority classes, our method simultaneously takes account of the diversity of majority information as well as the significance of minority information. Extensive experiments demonstrate that our GAI achieves state-of-the-art performances on the established few-shot forgery detection benchmark. Notably, our method is also validated to be robust to choices of majority and minority forgery approaches. The formal publication version is available in Pattern Recognition.



### Arch-Graph: Acyclic Architecture Relation Predictor for Task-Transferable Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2204.05941v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.05941v1)
- **Published**: 2022-04-12 16:46:06+00:00
- **Updated**: 2022-04-12 16:46:06+00:00
- **Authors**: Minbin Huang, Zhijian Huang, Changlin Li, Xin Chen, Hang Xu, Zhenguo Li, Xiaodan Liang
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) aims to find efficient models for multiple tasks. Beyond seeking solutions for a single task, there are surging interests in transferring network design knowledge across multiple tasks. In this line of research, effectively modeling task correlations is vital yet highly neglected. Therefore, we propose \textbf{Arch-Graph}, a transferable NAS method that predicts task-specific optimal architectures with respect to given task embeddings. It leverages correlations across multiple tasks by using their embeddings as a part of the predictor's input for fast adaptation. We also formulate NAS as an architecture relation graph prediction problem, with the relational graph constructed by treating candidate architectures as nodes and their pairwise relations as edges. To enforce some basic properties such as acyclicity in the relational graph, we add additional constraints to the optimization process, converting NAS into the problem of finding a Maximal Weighted Acyclic Subgraph (MWAS). Our algorithm then strives to eliminate cycles and only establish edges in the graph if the rank results can be trusted. Through MWAS, Arch-Graph can effectively rank candidate models for each task with only a small budget to finetune the predictor. With extensive experiments on TransNAS-Bench-101, we show Arch-Graph's transferability and high sample efficiency across numerous tasks, beating many NAS methods designed for both single-task and multi-task search. It is able to find top 0.16\% and 0.29\% architectures on average on two search spaces under the budget of only 50 models.



### RL-CoSeg : A Novel Image Co-Segmentation Algorithm with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.05951v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05951v1)
- **Published**: 2022-04-12 17:06:15+00:00
- **Updated**: 2022-04-12 17:06:15+00:00
- **Authors**: Xin Duan, Xiabi Liu, Xiaopeng Gong, Mengqiao Han
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an automatic image co-segmentation algorithm based on deep reinforcement learning (RL). Existing co-segmentation tasks mainly rely on deep learning methods, and the obtained foreground edges are often rough. In order to obtain more precise foreground edges, we use deep RL to solve this problem and achieve the finer segmentation. To our best knowledge, this is the first work to apply RL methods to co-segmentation. We define the problem as a Markov Decision Process (MDP) and optimize it by RL with asynchronous advantage actor-critic (A3C). The RL image co-segmentation network uses the correlation between images to segment common and salient objects from a set of related images. In order to achieve automatic segmentation, our RL-CoSeg method eliminates user's hints. For the image co-segmentation problem, we propose a collaborative RL algorithm based on the A3C model. We propose a Siamese RL co-segmentation network structure to obtain the co-attention of images for co-segmentation. We improve the self-attention for automatic RL algorithm to obtain long-distance dependence and enlarge the receptive field. The image feature information obtained by self-attention can be used to supplement the deleted user's hints and help to obtain more accurate actions. Experimental results have shown that our method can improve the performance effectively on both coarse and fine initial segmentations, and it achieves the state-of-the-art performance on Internet dataset, iCoseg dataset and MLMR-COS dataset.



### Localization Distillation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.05957v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05957v2)
- **Published**: 2022-04-12 17:14:34+00:00
- **Updated**: 2022-12-08 01:55:12+00:00
- **Authors**: Zhaohui Zheng, Rongguang Ye, Qibin Hou, Dongwei Ren, Ping Wang, Wangmeng Zuo, Ming-Ming Cheng
- **Comment**: Journal extension of our previous conference paper arXiv:2102.12252
- **Journal**: None
- **Summary**: Previous knowledge distillation (KD) methods for object detection mostly focus on feature imitation instead of mimicking the prediction logits due to its inefficiency in distilling the localization information. In this paper, we investigate whether logit mimicking always lags behind feature imitation. Towards this goal, we first present a novel localization distillation (LD) method which can efficiently transfer the localization knowledge from the teacher to the student. Second, we introduce the concept of valuable localization region that can aid to selectively distill the classification and localization knowledge for a certain region. Combining these two new components, for the first time, we show that logit mimicking can outperform feature imitation and the absence of localization distillation is a critical reason for why logit mimicking underperforms for years. The thorough studies exhibit the great potential of logit mimicking that can significantly alleviate the localization ambiguity, learn robust feature representation, and ease the training difficulty in the early stage. We also provide the theoretical connection between the proposed LD and the classification KD, that they share the equivalent optimization effect. Our distillation scheme is simple as well as effective and can be easily applied to both dense horizontal object detectors and rotated object detectors. Extensive experiments on the MS COCO, PASCAL VOC, and DOTA benchmarks demonstrate that our method can achieve considerable AP improvement without any sacrifice on the inference speed. Our source code and pretrained models are publicly available at https://github.com/HikariTJU/LD.



### Video Captioning: a comparative review of where we are and which could be the route
- **Arxiv ID**: http://arxiv.org/abs/2204.05976v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.05976v2)
- **Published**: 2022-04-12 17:42:53+00:00
- **Updated**: 2022-04-13 16:13:43+00:00
- **Authors**: Daniela Moctezuma, Tania Ramírez-delReal, Guillermo Ruiz, Othón González-Chávez
- **Comment**: review, 20 pages
- **Journal**: None
- **Summary**: Video captioning is the process of describing the content of a sequence of images capturing its semantic relationships and meanings. Dealing with this task with a single image is arduous, not to mention how difficult it is for a video (or images sequence). The amount and relevance of the applications of video captioning are vast, mainly to deal with a significant amount of video recordings in video surveillance, or assisting people visually impaired, to mention a few. To analyze where the efforts of our community to solve the video captioning task are, as well as what route could be better to follow, this manuscript presents an extensive review of more than 105 papers for the period of 2016 to 2021. As a result, the most-used datasets and metrics are identified. Also, the main approaches used and the best ones. We compute a set of rankings based on several performance metrics to obtain, according to its performance, the best method with the best result on the video captioning task. Finally, some insights are concluded about which could be the next steps or opportunity areas to improve dealing with this complex task.



### Machine Learning Security against Data Poisoning: Are We There Yet?
- **Arxiv ID**: http://arxiv.org/abs/2204.05986v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.05986v2)
- **Published**: 2022-04-12 17:52:09+00:00
- **Updated**: 2023-03-09 13:51:06+00:00
- **Authors**: Antonio Emanuele Cinà, Kathrin Grosse, Ambra Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo
- **Comment**: preprint, 10 pages, 3 figures
- **Journal**: None
- **Summary**: The recent success of machine learning (ML) has been fueled by the increasing availability of computing power and large amounts of data in many different applications. However, the trustworthiness of the resulting models can be compromised when such data is maliciously manipulated to mislead the learning process. In this article, we first review poisoning attacks that compromise the training data used to learn ML models, including attacks that aim to reduce the overall performance, manipulate the predictions on specific test samples, and even implant backdoors in the model. We then discuss how to mitigate these attacks using basic security principles, or by deploying ML-oriented defensive mechanisms. We conclude our article by formulating some relevant open challenges which are hindering the development of testing methods and benchmarks suitable for assessing and improving the trustworthiness of ML models against data poisoning attacks



### ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension
- **Arxiv ID**: http://arxiv.org/abs/2204.05991v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2204.05991v2)
- **Published**: 2022-04-12 17:55:38+00:00
- **Updated**: 2022-05-02 20:08:17+00:00
- **Authors**: Sanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, Anna Rohrbach
- **Comment**: ACL 2022
- **Journal**: None
- **Summary**: Training a referring expression comprehension (ReC) model for a new visual domain requires collecting referring expressions, and potentially corresponding bounding boxes, for images in the domain. While large-scale pre-trained models are useful for image classification across domains, it remains unclear if they can be applied in a zero-shot manner to more complex tasks like ReC. We present ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a state-of-the-art large-scale model, for ReC. Motivated by the close connection between ReC and CLIP's contrastive pre-training objective, the first component of ReCLIP is a region-scoring method that isolates object proposals via cropping and blurring, and passes them to CLIP. However, through controlled experiments on a synthetic dataset, we find that CLIP is largely incapable of performing spatial reasoning off-the-shelf. Thus, the second component of ReCLIP is a spatial relation resolver that handles several types of spatial relations. We reduce the gap between zero-shot baselines from prior work and supervised models by as much as 29% on RefCOCOg, and on RefGTA (video game imagery), ReCLIP's relative improvement over supervised ReC models trained on real images is 8%.



### Malceiver: Perceiver with Hierarchical and Multi-modal Features for Android Malware Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.05994v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.05994v1)
- **Published**: 2022-04-12 17:59:17+00:00
- **Updated**: 2022-04-12 17:59:17+00:00
- **Authors**: Niall McLaughlin
- **Comment**: 13 pages, 2 figures
- **Journal**: None
- **Summary**: We propose the Malceiver, a hierarchical Perceiver model for Android malware detection that makes use of multi-modal features. The primary inputs are the opcode sequence and the requested permissions of a given Android APK file. To reach a malware classification decision the model combines hierarchical features extracted from the opcode sequence together with the requested permissions. The model's architecture is based on the Perceiver/PerceiverIO which allows for very long opcode sequences to be processed efficiently. Our proposed model can be easily extended to use multi-modal features. We show experimentally that this model outperforms a conventional CNN architecture for opcode sequence based malware detection. We then show that using additional modalities improves performance. Our proposed architecture opens new avenues for the use of Transformer-style networks in malware research.



### AGQA 2.0: An Updated Benchmark for Compositional Spatio-Temporal Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2204.06105v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06105v1)
- **Published**: 2022-04-12 22:30:12+00:00
- **Updated**: 2022-04-12 22:30:12+00:00
- **Authors**: Madeleine Grunde-McLaughlin, Ranjay Krishna, Maneesh Agrawala
- **Comment**: 7 pages, 2 figures, 7 tables, update to AGQA arXiv:2103.16002
- **Journal**: None
- **Summary**: Prior benchmarks have analyzed models' answers to questions about videos in order to measure visual compositional reasoning. Action Genome Question Answering (AGQA) is one such benchmark. AGQA provides a training/test split with balanced answer distributions to reduce the effect of linguistic biases. However, some biases remain in several AGQA categories. We introduce AGQA 2.0, a version of this benchmark with several improvements, most namely a stricter balancing procedure. We then report results on the updated benchmark for all experiments.



### Open-World Instance Segmentation: Exploiting Pseudo Ground Truth From Learned Pairwise Affinity
- **Arxiv ID**: http://arxiv.org/abs/2204.06107v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.06107v1)
- **Published**: 2022-04-12 22:37:49+00:00
- **Updated**: 2022-04-12 22:37:49+00:00
- **Authors**: Weiyao Wang, Matt Feiszli, Heng Wang, Jitendra Malik, Du Tran
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Open-world instance segmentation is the task of grouping pixels into object instances without any pre-determined taxonomy. This is challenging, as state-of-the-art methods rely on explicit class semantics obtained from large labeled datasets, and out-of-domain evaluation performance drops significantly. Here we propose a novel approach for mask proposals, Generic Grouping Networks (GGNs), constructed without semantic supervision. Our approach combines a local measure of pixel affinity with instance-level mask supervision, producing a training regimen designed to make the model as generic as the data diversity allows. We introduce a method for predicting Pairwise Affinities (PA), a learned local relationship between pairs of pixels. PA generalizes very well to unseen categories. From PA we construct a large set of pseudo-ground-truth instance masks; combined with human-annotated instance masks we train GGNs and significantly outperform the SOTA on open-world instance segmentation on various benchmarks including COCO, LVIS, ADE20K, and UVO. Code is available on project website: https://sites.google.com/view/generic-grouping/.



### SRMD: Sparse Random Mode Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2204.06108v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2204.06108v2)
- **Published**: 2022-04-12 22:40:10+00:00
- **Updated**: 2023-03-15 19:06:38+00:00
- **Authors**: Nicholas Richardson, Hayden Schaeffer, Giang Tran
- **Comment**: None
- **Journal**: None
- **Summary**: Signal decomposition and multiscale signal analysis provide many useful tools for time-frequency analysis. We proposed a random feature method for analyzing time-series data by constructing a sparse approximation to the spectrogram. The randomization is both in the time window locations and the frequency sampling, which lowers the overall sampling and computational cost. The sparsification of the spectrogram leads to a sharp separation between time-frequency clusters which makes it easier to identify intrinsic modes, and thus leads to a new data-driven mode decomposition. The applications include signal representation, outlier removal, and mode decomposition. On the benchmark tests, we show that our approach outperforms other state-of-the-art decomposition methods.



