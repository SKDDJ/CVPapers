# Arxiv Papers in cs.CV on 2022-04-19
### A Tour of Visualization Techniques for Computer Vision Datasets
- **Arxiv ID**: http://arxiv.org/abs/2204.08601v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2204.08601v1)
- **Published**: 2022-04-19 01:04:28+00:00
- **Updated**: 2022-04-19 01:04:28+00:00
- **Authors**: Bilal Alsallakh, Pamela Bhattacharya, Vanessa Feng, Narine Kokhlikyan, Orion Reblitz-Richardson, Rahul Rajan, David Yan
- **Comment**: None
- **Journal**: None
- **Summary**: We survey a number of data visualization techniques for analyzing Computer Vision (CV) datasets. These techniques help us understand properties and latent patterns in such data, by applying dataset-level analysis. We present various examples of how such analysis helps predict the potential impact of the dataset properties on CV models and informs appropriate mitigation of their shortcomings. Finally, we explore avenues for further visualization techniques of different modalities of CV datasets as well as ones that are tailored to support specific CV tasks and analysis needs.



### Image Data Augmentation for Deep Learning: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2204.08610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08610v1)
- **Published**: 2022-04-19 02:05:56+00:00
- **Updated**: 2022-04-19 02:05:56+00:00
- **Authors**: Suorong Yang, Weikang Xiao, Mengcheng Zhang, Suhan Guo, Jian Zhao, Furao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has achieved remarkable results in many computer vision tasks. Deep neural networks typically rely on large amounts of training data to avoid overfitting. However, labeled data for real-world applications may be limited. By improving the quantity and diversity of training data, data augmentation has become an inevitable part of deep learning model training with image data.   As an effective way to improve the sufficiency and diversity of training data, data augmentation has become a necessary part of successful application of deep learning models on image data. In this paper, we systematically review different image data augmentation methods. We propose a taxonomy of reviewed methods and present the strengths and limitations of these methods. We also conduct extensive experiments with various data augmentation methods on three typical computer vision tasks, including semantic segmentation, image classification and object detection. Finally, we discuss current challenges faced by data augmentation and future research directions to put forward some useful research guidance.



### Metamorphic Testing-based Adversarial Attack to Fool Deepfake Detectors
- **Arxiv ID**: http://arxiv.org/abs/2204.08612v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08612v2)
- **Published**: 2022-04-19 02:24:30+00:00
- **Updated**: 2022-06-01 01:24:47+00:00
- **Authors**: Nyee Thoang Lim, Meng Yi Kuan, Muxin Pu, Mei Kuan Lim, Chun Yong Chong
- **Comment**: paper accepted at 26TH International Conference on Pattern
  Recognition (ICPR2022)
- **Journal**: None
- **Summary**: Deepfakes utilise Artificial Intelligence (AI) techniques to create synthetic media where the likeness of one person is replaced with another. There are growing concerns that deepfakes can be maliciously used to create misleading and harmful digital contents. As deepfakes become more common, there is a dire need for deepfake detection technology to help spot deepfake media. Present deepfake detection models are able to achieve outstanding accuracy (>90%). However, most of them are limited to within-dataset scenario, where the same dataset is used for training and testing. Most models do not generalise well enough in cross-dataset scenario, where models are tested on unseen datasets from another source. Furthermore, state-of-the-art deepfake detection models rely on neural network-based classification models that are known to be vulnerable to adversarial attacks. Motivated by the need for a robust deepfake detection model, this study adapts metamorphic testing (MT) principles to help identify potential factors that could influence the robustness of the examined model, while overcoming the test oracle problem in this domain. Metamorphic testing is specifically chosen as the testing technique as it fits our demand to address learning-based system testing with probabilistic outcomes from largely black-box components, based on potentially large input domains. We performed our evaluations on MesoInception-4 and TwoStreamNet models, which are the state-of-the-art deepfake detection models. This study identified makeup application as an adversarial attack that could fool deepfake detectors. Our experimental results demonstrate that both the MesoInception-4 and TwoStreamNet models degrade in their performance by up to 30\% when the input data is perturbed with makeup.



### Self-Supervised Equivariant Learning for Oriented Keypoint Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.08613v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08613v1)
- **Published**: 2022-04-19 02:26:07+00:00
- **Updated**: 2022-04-19 02:26:07+00:00
- **Authors**: Jongmin Lee, Byungjin Kim, Minsu Cho
- **Comment**: CVPR 2022 accepted paper, Project page:
  http://cvlab.postech.ac.kr/research/REKD/
- **Journal**: None
- **Summary**: Detecting robust keypoints from an image is an integral part of many computer vision problems, and the characteristic orientation and scale of keypoints play an important role for keypoint description and matching. Existing learning-based methods for keypoint detection rely on standard translation-equivariant CNNs but often fail to detect reliable keypoints against geometric variations. To learn to detect robust oriented keypoints, we introduce a self-supervised learning framework using rotation-equivariant CNNs. We propose a dense orientation alignment loss by an image pair generated by synthetic transformations for training a histogram-based orientation map. Our method outperforms the previous methods on an image matching benchmark and a camera pose estimation benchmark.



### CorrGAN: Input Transformation Technique Against Natural Corruptions
- **Arxiv ID**: http://arxiv.org/abs/2204.08623v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08623v1)
- **Published**: 2022-04-19 02:56:46+00:00
- **Updated**: 2022-04-19 02:56:46+00:00
- **Authors**: Mirazul Haque, Christof J. Budnik, Wei Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Because of the increasing accuracy of Deep Neural Networks (DNNs) on different tasks, a lot of real times systems are utilizing DNNs. These DNNs are vulnerable to adversarial perturbations and corruptions. Specifically, natural corruptions like fog, blur, contrast etc can affect the prediction of DNN in an autonomous vehicle. In real time, these corruptions are needed to be detected and also the corrupted inputs are needed to be de-noised to be predicted correctly. In this work, we propose CorrGAN approach, which can generate benign input when a corrupted input is provided. In this framework, we train Generative Adversarial Network (GAN) with novel intermediate output-based loss function. The GAN can denoise the corrupted input and generate benign input. Through experimentation, we show that up to 75.2% of the corrupted misclassified inputs can be classified correctly by DNN using CorrGAN.



### Topology and geometry of data manifold in deep learning
- **Arxiv ID**: http://arxiv.org/abs/2204.08624v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.AT
- **Links**: [PDF](http://arxiv.org/pdf/2204.08624v1)
- **Published**: 2022-04-19 02:57:47+00:00
- **Updated**: 2022-04-19 02:57:47+00:00
- **Authors**: German Magai, Anton Ayzenberg
- **Comment**: 12 pages, 15 figures
- **Journal**: None
- **Summary**: Despite significant advances in the field of deep learning in applications to various fields, explaining the inner processes of deep learning models remains an important and open question. The purpose of this article is to describe and substantiate the geometric and topological view of the learning process of neural networks. Our attention is focused on the internal representation of neural networks and on the dynamics of changes in the topology and geometry of the data manifold on different layers. We also propose a method for assessing the generalizing ability of neural networks based on topological descriptors. In this paper, we use the concepts of topological data analysis and intrinsic dimension, and we present a wide range of experiments on different datasets and different configurations of convolutional neural network architectures. In addition, we consider the issue of the geometry of adversarial attacks in the classification task and spoofing attacks on face recognition systems. Our work is a contribution to the development of an important area of explainable and interpretable AI through the example of computer vision.



### Quaternion Optimized Model with Sparse Regularization for Color Image Recovery
- **Arxiv ID**: http://arxiv.org/abs/2204.08629v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2204.08629v1)
- **Published**: 2022-04-19 03:07:12+00:00
- **Updated**: 2022-04-19 03:07:12+00:00
- **Authors**: Liqiao Yang, Yang Liu, Kit Ian Kou
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the color image completion problem in accordance with low-rank quatenrion matrix optimization that is characterized by sparse regularization in a transformed domain. This research was inspired by an appreciation of the fact that different signal types, including audio formats and images, possess structures that are inherently sparse in respect of their respective bases. Since color images can be processed as a whole in the quaternion domain, we depicted the sparsity of the color image in the quaternion discrete cosine transform (QDCT) domain. In addition, the representation of a low-rank structure that is intrinsic to the color image is a vital issue in the quaternion matrix completion problem. To achieve a more superior low-rank approximation, the quatenrion-based truncated nuclear norm (QTNN) is employed in the proposed model. Moreover, this model is facilitated by a competent alternating direction method of multipliers (ADMM) based on the algorithm. Extensive experimental results demonstrate that the proposed method can yield vastly superior completion performance in comparison with the state-of-the-art low-rank matrix/quaternion matrix approximation methods tested on color image recovery.



### Interaction-Aware Labeled Multi-Bernoulli Filter
- **Arxiv ID**: http://arxiv.org/abs/2204.08655v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08655v1)
- **Published**: 2022-04-19 04:23:32+00:00
- **Updated**: 2022-04-19 04:23:32+00:00
- **Authors**: Nida Ishtiaq, Amirali Khodadadian Gostar, Alireza Bab-Hadiashar, Reza Hoseinnezhad
- **Comment**: 13 pages including references, 9 figures, submitted and undergoing
  second round of review with IEEE Transactions on Intelligent Transportation
  Systems (ITS)
- **Journal**: None
- **Summary**: Tracking multiple objects through time is an important part of an intelligent transportation system. Random finite set (RFS)-based filters are one of the emerging techniques for tracking multiple objects. In multi-object tracking (MOT), a common assumption is that each object is moving independent of its surroundings. But in many real-world applications, target objects interact with one another and the environment. Such interactions, when considered for tracking, are usually modeled by an interactive motion model which is application specific. In this paper, we present a novel approach to incorporate target interactions within the prediction step of an RFS-based multi-target filter, i.e. labeled multi-Bernoulli (LMB) filter. The method has been developed for two practical applications of tracking a coordinated swarm and vehicles. The method has been tested for a complex vehicle tracking dataset and compared with the LMB filter through the OSPA and OSPA$^{(2)}$ metrics. The results demonstrate that the proposed interaction-aware method depicts considerable performance enhancement over the LMB filter in terms of the selected metrics.



### ActAR: Actor-Driven Pose Embeddings for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.08671v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08671v1)
- **Published**: 2022-04-19 05:12:24+00:00
- **Updated**: 2022-04-19 05:12:24+00:00
- **Authors**: Soufiane Lamghari, Guillaume-Alexandre Bilodeau, Nicolas Saunier
- **Comment**: None
- **Journal**: None
- **Summary**: Human action recognition (HAR) in videos is one of the core tasks of video understanding. Based on video sequences, the goal is to recognize actions performed by humans. While HAR has received much attention in the visible spectrum, action recognition in infrared videos is little studied. Accurate recognition of human actions in the infrared domain is a highly challenging task because of the redundant and indistinguishable texture features present in the sequence. Furthermore, in some cases, challenges arise from the irrelevant information induced by the presence of multiple active persons not contributing to the actual action of interest. Therefore, most existing methods consider a standard paradigm that does not take into account these challenges, which is in some part due to the ambiguous definition of the recognition task in some cases. In this paper, we propose a new method that simultaneously learns to recognize efficiently human actions in the infrared spectrum, while automatically identifying the key-actors performing the action without using any prior knowledge or explicit annotations. Our method is composed of three stages. In the first stage, optical flow-based key-actor identification is performed. Then for each key-actor, we estimate key-poses that will guide the frame selection process. A scale-invariant encoding process along with embedded pose filtering are performed in order to enhance the quality of action representations. Experimental results on InfAR dataset show that our proposed model achieves promising recognition performance and learns useful action representations.



### Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer
- **Arxiv ID**: http://arxiv.org/abs/2204.08680v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08680v3)
- **Published**: 2022-04-19 05:38:16+00:00
- **Updated**: 2022-04-21 14:50:57+00:00
- **Authors**: Wang Zeng, Sheng Jin, Wentao Liu, Chen Qian, Ping Luo, Wanli Ouyang, Xiaogang Wang
- **Comment**: CVPR 2022 oral
- **Journal**: None
- **Summary**: Vision transformers have achieved great successes in many computer vision tasks. Most methods generate vision tokens by splitting an image into a regular and fixed grid and treating each cell as a token. However, not all regions are equally important in human-centric vision tasks, e.g., the human body needs a fine representation with many tokens, while the image background can be modeled by a few tokens. To address this problem, we propose a novel Vision Transformer, called Token Clustering Transformer (TCFormer), which merges tokens by progressive clustering, where the tokens can be merged from different locations with flexible shapes and sizes. The tokens in TCFormer can not only focus on important areas but also adjust the token shapes to fit the semantic concept and adopt a fine resolution for regions containing critical details, which is beneficial to capturing detailed information. Extensive experiments show that TCFormer consistently outperforms its counterparts on different challenging human-centric tasks and datasets, including whole-body pose estimation on COCO-WholeBody and 3D human mesh reconstruction on 3DPW. Code is available at https://github.com/zengwang430521/TCFormer.git



### A Thin Format Vision-Based Tactile Sensor with A Micro Lens Array (MLA)
- **Arxiv ID**: http://arxiv.org/abs/2204.08691v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08691v1)
- **Published**: 2022-04-19 06:17:49+00:00
- **Updated**: 2022-04-19 06:17:49+00:00
- **Authors**: Xia Chen, Guanlan Zhang, Michael Yu Wang, Hongyu Yu
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: Vision-based tactile sensors have been widely studied in the robotics field for high spatial resolution and compatibility with machine learning algorithms. However, the currently employed sensor's imaging system is bulky limiting its further application. Here we present a micro lens array (MLA) based vison system to achieve a low thickness format of the sensor package with high tactile sensing performance. Multiple micromachined micro lens units cover the whole elastic touching layer and provide a stitched clear tactile image, enabling high spatial resolution with a thin thickness of 5 mm. The thermal reflow and soft lithography method ensure the uniform spherical profile and smooth surface of micro lens. Both optical and mechanical characterization demonstrated the sensor's stable imaging and excellent tactile sensing, enabling precise 3D tactile information, such as displacement mapping and force distribution with an ultra compact-thin structure.



### CTCNet: A CNN-Transformer Cooperation Network for Face Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2204.08696v3
- **DOI**: 10.1109/TIP.2023.3261747
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08696v3)
- **Published**: 2022-04-19 06:38:29+00:00
- **Updated**: 2023-03-23 09:44:22+00:00
- **Authors**: Guangwei Gao, Zixiang Xu, Juncheng Li, Jian Yang, Tieyong Zeng, Guo-Jun Qi
- **Comment**: IEEE Transactions on Image Processing, 12 figures, 9 tables
- **Journal**: None
- **Summary**: Recently, deep convolution neural networks (CNNs) steered face super-resolution methods have achieved great progress in restoring degraded facial details by jointly training with facial priors. However, these methods have some obvious limitations. On the one hand, multi-task joint learning requires additional marking on the dataset, and the introduced prior network will significantly increase the computational cost of the model. On the other hand, the limited receptive field of CNN will reduce the fidelity and naturalness of the reconstructed facial images, resulting in suboptimal reconstructed images. In this work, we propose an efficient CNN-Transformer Cooperation Network (CTCNet) for face super-resolution tasks, which uses the multi-scale connected encoder-decoder architecture as the backbone. Specifically, we first devise a novel Local-Global Feature Cooperation Module (LGCM), which is composed of a Facial Structure Attention Unit (FSAU) and a Transformer block, to promote the consistency of local facial detail and global facial structure restoration simultaneously. Then, we design an efficient Feature Refinement Module (FRM) to enhance the encoded features. Finally, to further improve the restoration of fine facial details, we present a Multi-scale Feature Fusion Unit (MFFU) to adaptively fuse the features from different stages in the encoder procedure. Extensive evaluations on various datasets have assessed that the proposed CTCNet can outperform other state-of-the-art methods significantly. Source code will be available at https://github.com/IVIPLab/CTCNet.



### Software Engineering Approaches for TinyML based IoT Embedded Vision: A Systematic Literature Review
- **Arxiv ID**: http://arxiv.org/abs/2204.08702v1
- **DOI**: 10.1145/3528227.3528569
- **Categories**: **cs.SE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.08702v1)
- **Published**: 2022-04-19 07:07:41+00:00
- **Updated**: 2022-04-19 07:07:41+00:00
- **Authors**: Shashank Bangalore Lakshman, Nasir U. Eisty
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Internet of Things (IoT) has catapulted human ability to control our environments through ubiquitous sensing, communication, computation, and actuation. Over the past few years, IoT has joined forces with Machine Learning (ML) to embed deep intelligence at the far edge. TinyML (Tiny Machine Learning) has enabled the deployment of ML models for embedded vision on extremely lean edge hardware, bringing the power of IoT and ML together. However, TinyML powered embedded vision applications are still in a nascent stage, and they are just starting to scale to widespread real-world IoT deployment. To harness the true potential of IoT and ML, it is necessary to provide product developers with robust, easy-to-use software engineering (SE) frameworks and best practices that are customized for the unique challenges faced in TinyML engineering. Through this systematic literature review, we aggregated the key challenges reported by TinyML developers and identified state-of-art SE approaches in large-scale Computer Vision, Machine Learning, and Embedded Systems that can help address key challenges in TinyML based IoT embedded vision. In summary, our study draws synergies between SE expertise that embedded systems developers and ML developers have independently developed to help address the unique challenges in the engineering of TinyML based IoT embedded vision.



### Unsupervised Contrastive Hashing for Cross-Modal Retrieval in Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/2204.08707v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08707v1)
- **Published**: 2022-04-19 07:25:25+00:00
- **Updated**: 2022-04-19 07:25:25+00:00
- **Authors**: Georgii Mikriukov, Mahdyar Ravanbakhsh, Begüm Demir
- **Comment**: Our code is publicly available at https://git.tu-berlin.de/rsim/duch.
  arXiv admin note: substantial text overlap with arXiv:2201.08125
- **Journal**: None
- **Summary**: The development of cross-modal retrieval systems that can search and retrieve semantically relevant data across different modalities based on a query in any modality has attracted great attention in remote sensing (RS). In this paper, we focus our attention on cross-modal text-image retrieval, where queries from one modality (e.g., text) can be matched to archive entries from another (e.g., image). Most of the existing cross-modal text-image retrieval systems in RS require a high number of labeled training samples and also do not allow fast and memory-efficient retrieval. These issues limit the applicability of the existing cross-modal retrieval systems for large-scale applications in RS. To address this problem, in this paper we introduce a novel unsupervised cross-modal contrastive hashing (DUCH) method for text-image retrieval in RS. To this end, the proposed DUCH is made up of two main modules: 1) feature extraction module, which extracts deep representations of two modalities; 2) hashing module that learns to generate cross-modal binary hash codes from the extracted representations. We introduce a novel multi-objective loss function including: i) contrastive objectives that enable similarity preservation in intra- and inter-modal similarities; ii) an adversarial objective that is enforced across two modalities for cross-modal representation consistency; and iii) binarization objectives for generating hash codes. Experimental results show that the proposed DUCH outperforms state-of-the-art methods. Our code is publicly available at https://git.tu-berlin.de/rsim/duch.



### NAFSSR: Stereo Image Super-Resolution Using NAFNet
- **Arxiv ID**: http://arxiv.org/abs/2204.08714v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08714v2)
- **Published**: 2022-04-19 07:38:10+00:00
- **Updated**: 2022-04-26 07:04:33+00:00
- **Authors**: Xiaojie Chu, Liangyu Chen, Wenqing Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Stereo image super-resolution aims at enhancing the quality of super-resolution results by utilizing the complementary information provided by binocular systems. To obtain reasonable performance, most methods focus on finely designing modules, loss functions, and etc. to exploit information from another viewpoint. This has the side effect of increasing system complexity, making it difficult for researchers to evaluate new ideas and compare methods. This paper inherits a strong and simple image restoration model, NAFNet, for single-view feature extraction and extends it by adding cross attention modules to fuse features between views to adapt to binocular scenarios. The proposed baseline for stereo image super-resolution is noted as NAFSSR. Furthermore, training/testing strategies are proposed to fully exploit the performance of NAFSSR. Extensive experiments demonstrate the effectiveness of our method. In particular, NAFSSR outperforms the state-of-the-art methods on the KITTI 2012, KITTI 2015, Middlebury, and Flickr1024 datasets. With NAFSSR, we won 1st place in the NTIRE 2022 Stereo Image Super-resolution Challenge. Codes and models will be released at https://github.com/megvii-research/NAFNet.



### Shape-Aware Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.08717v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08717v2)
- **Published**: 2022-04-19 07:43:56+00:00
- **Updated**: 2022-04-24 07:29:21+00:00
- **Authors**: Wei Chen, Jie Zhao, Wan-Lei Zhao, Song-Yuan Wu
- **Comment**: 8 pages; 6 figures. typo fixed; reference changed
- **Journal**: None
- **Summary**: The detection of 3D objects through a single perspective camera is a challenging issue. The anchor-free and keypoint-based models receive increasing attention recently due to their effectiveness and simplicity. However, most of these methods are vulnerable to occluded and truncated objects. In this paper, a single-stage monocular 3D object detection model is proposed. An instance-segmentation head is integrated into the model training, which allows the model to be aware of the visible shape of a target object. The detection largely avoids interference from irrelevant regions surrounding the target objects. In addition, we also reveal that the popular IoU-based evaluation metrics, which were originally designed for evaluating stereo or LiDAR-based detection methods, are insensitive to the improvement of monocular 3D object detection algorithms. A novel evaluation metric, namely average depth similarity (ADS) is proposed for the monocular 3D object detection models. Our method outperforms the baseline on both the popular and the proposed evaluation metrics while maintaining real-time efficiency.



### Multimodal Token Fusion for Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2204.08721v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08721v2)
- **Published**: 2022-04-19 07:47:50+00:00
- **Updated**: 2022-07-15 11:00:23+00:00
- **Authors**: Yikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang, Fuchun Sun, Yunhe Wang
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Many adaptations of transformers have emerged to address the single-modal vision tasks, where self-attention modules are stacked to handle input sources like images. Intuitively, feeding multiple modalities of data to vision transformers could improve the performance, yet the inner-modal attentive weights may also be diluted, which could thus undermine the final performance. In this paper, we propose a multimodal token fusion method (TokenFusion), tailored for transformer-based vision tasks. To effectively fuse multiple modalities, TokenFusion dynamically detects uninformative tokens and substitutes these tokens with projected and aggregated inter-modal features. Residual positional alignment is also adopted to enable explicit utilization of the inter-modal alignments after fusion. The design of TokenFusion allows the transformer to learn correlations among multimodal features, while the single-modal transformer architecture remains largely intact. Extensive experiments are conducted on a variety of homogeneous and heterogeneous modalities and demonstrate that TokenFusion surpasses state-of-the-art methods in three typical vision tasks: multimodal image-to-image translation, RGB-depth semantic segmentation, and 3D object detection with point cloud and images. Our code is available at https://github.com/yikaiw/TokenFusion.



### Jacobian Ensembles Improve Robustness Trade-offs to Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2204.08726v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08726v1)
- **Published**: 2022-04-19 08:04:38+00:00
- **Updated**: 2022-04-19 08:04:38+00:00
- **Authors**: Kenneth T. Co, David Martinez-Rego, Zhongyuan Hau, Emil C. Lupu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have become an integral part of our software infrastructure and are being deployed in many widely-used and safety-critical applications. However, their integration into many systems also brings with it the vulnerability to test time attacks in the form of Universal Adversarial Perturbations (UAPs). UAPs are a class of perturbations that when applied to any input causes model misclassification. Although there is an ongoing effort to defend models against these adversarial attacks, it is often difficult to reconcile the trade-offs in model accuracy and robustness to adversarial attacks. Jacobian regularization has been shown to improve the robustness of models against UAPs, whilst model ensembles have been widely adopted to improve both predictive performance and model robustness. In this work, we propose a novel approach, Jacobian Ensembles-a combination of Jacobian regularization and model ensembles to significantly increase the robustness against UAPs whilst maintaining or improving model accuracy. Our results show that Jacobian Ensembles achieves previously unseen levels of accuracy and robustness, greatly improving over previous methods that tend to skew towards only either accuracy or robustness.



### Proposal-free Lidar Panoptic Segmentation with Pillar-level Affinity
- **Arxiv ID**: http://arxiv.org/abs/2204.08744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08744v1)
- **Published**: 2022-04-19 08:39:37+00:00
- **Updated**: 2022-04-19 08:39:37+00:00
- **Authors**: Qi Chen, Sourabh Vora
- **Comment**: CVPRW 2022 Workshop on Autonomous Driving
- **Journal**: None
- **Summary**: We propose a simple yet effective proposal-free architecture for lidar panoptic segmentation. We jointly optimize both semantic segmentation and class-agnostic instance classification in a single network using a pillar-based bird's-eye view representation. The instance classification head learns pairwise affinity between pillars to determine whether the pillars belong to the same instance or not. We further propose a local clustering algorithm to propagate instance ids by merging semantic segmentation and affinity predictions. Our experiments on nuScenes dataset show that our approach outperforms previous proposal-free methods and is comparable to proposal-based methods which requires extra annotation from object detection.



### Augmentation of Atmospheric Turbulence Effects on Thermal Adapted Object Detection Models
- **Arxiv ID**: http://arxiv.org/abs/2204.08745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08745v1)
- **Published**: 2022-04-19 08:40:00+00:00
- **Updated**: 2022-04-19 08:40:00+00:00
- **Authors**: Engin Uzun, Ahmet Anil Dursun, Erdem Akagunduz
- **Comment**: Accepted to CVPR 2022 Perception Beyond the Visible Spectrum (PBVS)
  Workshop
- **Journal**: None
- **Summary**: Atmospheric turbulence has a degrading effect on the image quality of long-range observation systems. As a result of various elements such as temperature, wind velocity, humidity, etc., turbulence is characterized by random fluctuations in the refractive index of the atmosphere. It is a phenomenon that may occur in various imaging spectra such as the visible or the infrared bands. In this paper, we analyze the effects of atmospheric turbulence on object detection performance in thermal imagery. We use a geometric turbulence model to simulate turbulence effects on a medium-scale thermal image set, namely "FLIR ADAS v2". We apply thermal domain adaptation to state-of-the-art object detectors and propose a data augmentation strategy to increase the performance of object detectors which utilizes turbulent images in different severity levels as training data. Our results show that the proposed data augmentation strategy yields an increase in performance for both turbulent and non-turbulent thermal test images.



### Multi-View Spatial-Temporal Network for Continuous Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.08747v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.7; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2204.08747v1)
- **Published**: 2022-04-19 08:43:03+00:00
- **Updated**: 2022-04-19 08:43:03+00:00
- **Authors**: Ronghui Li, Lu Meng
- **Comment**: 12 pages, 4 figures
- **Journal**: None
- **Summary**: Sign language is a beautiful visual language and is also the primary language used by speaking and hearing-impaired people. However, sign language has many complex expressions, which are difficult for the public to understand and master. Sign language recognition algorithms will significantly facilitate communication between hearing-impaired people and normal people. Traditional continuous sign language recognition often uses a sequence learning method based on Convolutional Neural Network (CNN) and Long Short-Term Memory Network (LSTM). These methods can only learn spatial and temporal features separately, which cannot learn the complex spatial-temporal features of sign language. LSTM is also difficult to learn long-term dependencies. To alleviate these problems, this paper proposes a multi-view spatial-temporal continuous sign language recognition network. The network consists of three parts. The first part is a Multi-View Spatial-Temporal Feature Extractor Network (MSTN), which can directly extract the spatial-temporal features of RGB and skeleton data; the second is a sign language encoder network based on Transformer, which can learn long-term dependencies; the third is a Connectionist Temporal Classification (CTC) decoder network, which is used to predict the whole meaning of the continuous sign language. Our algorithm is tested on two public sign language datasets SLR-100 and PHOENIX-Weather 2014T (RWTH). As a result, our method achieves excellent performance on both datasets. The word error rate on the SLR-100 dataset is 1.9%, and the word error rate on the RWTHPHOENIX-Weather dataset is 22.8%.



### Dynamic Point Cloud Denoising via Gradient Fields
- **Arxiv ID**: http://arxiv.org/abs/2204.08755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08755v1)
- **Published**: 2022-04-19 08:51:53+00:00
- **Updated**: 2022-04-19 08:51:53+00:00
- **Authors**: Qianjiang Hu, Wei Hu
- **Comment**: None
- **Journal**: None
- **Summary**: 3D dynamic point clouds provide a discrete representation of real-world objects or scenes in motion, which have been widely applied in immersive telepresence, autonomous driving, surveillance, etc. However, point clouds acquired from sensors are usually perturbed by noise, which affects downstream tasks such as surface reconstruction and analysis. Although many efforts have been made for static point cloud denoising, dynamic point cloud denoising remains under-explored. In this paper, we propose a novel gradient-field-based dynamic point cloud denoising method, exploiting the temporal correspondence via the estimation of gradient fields -- a fundamental problem in dynamic point cloud processing and analysis. The gradient field is the gradient of the log-probability function of the noisy point cloud, based on which we perform gradient ascent so as to converge each point to the underlying clean surface. We estimate the gradient of each surface patch and exploit the temporal correspondence, where the temporally corresponding patches are searched leveraging on rigid motion in classical mechanics. In particular, we treat each patch as a rigid object, which moves in the gradient field of an adjacent frame via force until reaching a balanced state, i.e., when the sum of gradients over the patch reaches 0. Since the gradient would be smaller when the point is closer to the underlying surface, the balanced patch would fit the underlying surface well, thus leading to the temporal correspondence. Finally, the position of each point in the patch is updated along the direction of the gradient averaged from corresponding patches in adjacent frames. Experimental results demonstrate that the proposed model outperforms state-of-the-art methods under both synthetic noise and simulated real-world noise.



### Edge-enhanced Feature Distillation Network for Efficient Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2204.08759v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08759v2)
- **Published**: 2022-04-19 08:57:35+00:00
- **Updated**: 2022-06-22 12:17:23+00:00
- **Authors**: Yan Wang
- **Comment**: Accepted to NTIRE workshop at CVPR 2022
- **Journal**: None
- **Summary**: With the recently massive development in convolution neural networks, numerous lightweight CNN-based image super-resolution methods have been proposed for practical deployments on edge devices. However, most existing methods focus on one specific aspect: network or loss design, which leads to the difficulty of minimizing the model size. To address the issue, we conclude block devising, architecture searching, and loss design to obtain a more efficient SR structure. In this paper, we proposed an edge-enhanced feature distillation network, named EFDN, to preserve the high-frequency information under constrained resources. In detail, we build an edge-enhanced convolution block based on the existing reparameterization methods. Meanwhile, we propose edge-enhanced gradient loss to calibrate the reparameterized path training. Experimental results show that our edge-enhanced strategies preserve the edge and significantly improve the final restoration quality. Code is available at https://github.com/icandle/EFDN.



### Incorporating Semi-Supervised and Positive-Unlabeled Learning for Boosting Full Reference Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2204.08763v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08763v1)
- **Published**: 2022-04-19 09:10:06+00:00
- **Updated**: 2022-04-19 09:10:06+00:00
- **Authors**: Yue Cao, Zhaolin Wan, Dongwei Ren, Zifei Yan, Wangmeng Zuo
- **Comment**: CVPR 2022. The source code and model are available at
  https://github.com/happycaoyue/JSPL
- **Journal**: None
- **Summary**: Full-reference (FR) image quality assessment (IQA) evaluates the visual quality of a distorted image by measuring its perceptual difference with pristine-quality reference, and has been widely used in low-level vision tasks. Pairwise labeled data with mean opinion score (MOS) are required in training FR-IQA model, but is time-consuming and cumbersome to collect. In contrast, unlabeled data can be easily collected from an image degradation or restoration process, making it encouraging to exploit unlabeled training data to boost FR-IQA performance. Moreover, due to the distribution inconsistency between labeled and unlabeled data, outliers may occur in unlabeled data, further increasing the training difficulty. In this paper, we suggest to incorporate semi-supervised and positive-unlabeled (PU) learning for exploiting unlabeled data while mitigating the adverse effect of outliers. Particularly, by treating all labeled data as positive samples, PU learning is leveraged to identify negative samples (i.e., outliers) from unlabeled data. Semi-supervised learning (SSL) is further deployed to exploit positive unlabeled data by dynamically generating pseudo-MOS. We adopt a dual-branch network including reference and distortion branches. Furthermore, spatial attention is introduced in the reference branch to concentrate more on the informative regions, and sliced Wasserstein distance is used for robust difference map computation to address the misalignment issues caused by images recovered by GAN models. Extensive experiments show that our method performs favorably against state-of-the-arts on the benchmark datasets PIPAL, KADID-10k, TID2013, LIVE and CSIQ.



### Modeling Missing Annotations for Incremental Learning in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.08766v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08766v2)
- **Published**: 2022-04-19 09:22:50+00:00
- **Updated**: 2022-04-21 15:48:57+00:00
- **Authors**: Fabio Cermelli, Antonino Geraci, Dario Fontanel, Barbara Caputo
- **Comment**: Accepted in CVPR-Workshop (CLVISION) 2022
- **Journal**: None
- **Summary**: Despite the recent advances in the field of object detection, common architectures are still ill-suited to incrementally detect new categories over time. They are vulnerable to catastrophic forgetting: they forget what has been already learned while updating their parameters in absence of the original training data. Previous works extended standard classification methods in the object detection task, mainly adopting the knowledge distillation framework. However, we argue that object detection introduces an additional problem, which has been overlooked. While objects belonging to new classes are learned thanks to their annotations, if no supervision is provided for other objects that may still be present in the input, the model learns to associate them to background regions. We propose to handle these missing annotations by revisiting the standard knowledge distillation framework. Our approach outperforms current state-of-the-art methods in every setting of the Pascal-VOC dataset. We further propose an extension to instance segmentation, outperforming the other baselines. Code can be found here: https://github.com/fcdl94/MMA



### Binary Multi Channel Morphological Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2204.08768v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.08768v1)
- **Published**: 2022-04-19 09:26:11+00:00
- **Updated**: 2022-04-19 09:26:11+00:00
- **Authors**: Theodore Aouad, Hugues Talbot
- **Comment**: 18 pages, 9 figures, 3 tables
- **Journal**: None
- **Summary**: Neural networks and particularly Deep learning have been comparatively little studied from the theoretical point of view. Conversely, Mathematical Morphology is a discipline with solid theoretical foundations. We combine these domains to propose a new type of neural architecture that is theoretically more explainable. We introduce a Binary Morphological Neural Network (BiMoNN) built upon the convolutional neural network. We design it for learning morphological networks with binary inputs and outputs. We demonstrate an equivalence between BiMoNNs and morphological operators that we can use to binarize entire networks. These can learn classical morphological operators and show promising results on a medical imaging application.



### GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction with Relational Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2204.08770v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.08770v2)
- **Published**: 2022-04-19 09:36:20+00:00
- **Updated**: 2022-04-20 04:28:46+00:00
- **Authors**: Chenxin Xu, Maosen Li, Zhenyang Ni, Ya Zhang, Siheng Chen
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Demystifying the interactions among multiple agents from their past trajectories is fundamental to precise and interpretable trajectory prediction. However, previous works only consider pair-wise interactions with limited relational reasoning. To promote more comprehensive interaction modeling for relational reasoning, we propose GroupNet, a multiscale hypergraph neural network, which is novel in terms of both interaction capturing and representation learning. From the aspect of interaction capturing, we propose a trainable multiscale hypergraph to capture both pair-wise and group-wise interactions at multiple group sizes. From the aspect of interaction representation learning, we propose a three-element format that can be learnt end-to-end and explicitly reason some relational factors including the interaction strength and category. We apply GroupNet into both CVAE-based prediction system and previous state-of-the-art prediction systems for predicting socially plausible trajectories with relational reasoning. To validate the ability of relational reasoning, we experiment with synthetic physics simulations to reflect the ability to capture group behaviors, reason interaction strength and interaction category. To validate the effectiveness of prediction, we conduct extensive experiments on three real-world trajectory prediction datasets, including NBA, SDD and ETH-UCY; and we show that with GroupNet, the CVAE-based prediction system outperforms state-of-the-art methods. We also show that adding GroupNet will further improve the performance of previous state-of-the-art prediction systems.



### Sensor Data Fusion in Top-View Grid Maps using Evidential Reasoning with Advanced Conflict Resolution
- **Arxiv ID**: http://arxiv.org/abs/2204.08780v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08780v1)
- **Published**: 2022-04-19 10:02:21+00:00
- **Updated**: 2022-04-19 10:02:21+00:00
- **Authors**: Sven Richter, Frank Bieder, Sascha Wirges, Christoph Stiller
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new method to combine evidential top-view grid maps estimated based on heterogeneous sensor sources. Dempster's combination rule that is usually applied in this context provides undesired results with highly conflicting inputs. Therefore, we use more advanced evidential reasoning techniques and improve the conflict resolution by modeling the reliability of the evidence sources. We propose a data-driven reliability estimation to optimize the fusion quality using the Kitti-360 dataset. We apply the proposed method to the fusion of LiDAR and stereo camera data and evaluate the results qualitatively and quantitatively. The results demonstrate that our proposed method robustly combines measurements from heterogeneous sensors and successfully resolves sensor conflicts.



### ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models
- **Arxiv ID**: http://arxiv.org/abs/2204.08790v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.08790v6)
- **Published**: 2022-04-19 10:23:42+00:00
- **Updated**: 2022-10-13 04:32:12+00:00
- **Authors**: Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, Jianfeng Gao
- **Comment**: NeurIPS 2022 (Datasets and Benchmarks Track). The first two authors
  contribute equally. Benchmark page:
  https://computer-vision-in-the-wild.github.io/ELEVATER/
- **Journal**: None
- **Summary**: Learning visual representations from natural language supervision has recently shown great promise in a number of pioneering works. In general, these language-augmented visual models demonstrate strong transferability to a variety of datasets and tasks. However, it remains challenging to evaluate the transferablity of these models due to the lack of easy-to-use evaluation toolkits and public benchmarks. To tackle this, we build ELEVATER (Evaluation of Language-augmented Visual Task-level Transfer), the first benchmark and toolkit for evaluating(pre-trained) language-augmented visual models. ELEVATER is composed of three components. (i) Datasets. As downstream evaluation suites, it consists of 20 image classification datasets and 35 object detection datasets, each of which is augmented with external knowledge. (ii) Toolkit. An automatic hyper-parameter tuning toolkit is developed to facilitate model evaluation on downstream tasks. (iii) Metrics. A variety of evaluation metrics are used to measure sample-efficiency (zero-shot and few-shot) and parameter-efficiency (linear probing and full model fine-tuning). ELEVATER is a platform for Computer Vision in the Wild (CVinW), and is publicly released at at https://computer-vision-in-the-wild.github.io/ELEVATER/



### A qualitative investigation of optical flow algorithms for video denoising
- **Arxiv ID**: http://arxiv.org/abs/2204.08791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08791v1)
- **Published**: 2022-04-19 10:25:04+00:00
- **Updated**: 2022-04-19 10:25:04+00:00
- **Authors**: Hannes Fassold
- **Comment**: Submitted for FTC 2022 conference
- **Journal**: None
- **Summary**: A good optical flow estimation is crucial in many video analysis and restoration algorithms employed in application fields like media industry, industrial inspection and automotive. In this work, we investigate how well optical flow algorithms perform qualitatively when integrated into a state of the art video denoising algorithm. Both classic optical flow algorithms (e.g. TV-L1) as well as recent deep learning based algorithm (like RAFT or BMBC) will be taken into account. For the qualitative investigation, we will employ realistic content with challenging characteristic (noisy content, large motion etc.) instead of the standard images used in most publications.



### Two-Stream Graph Convolutional Network for Intra-oral Scanner Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.08797v1
- **DOI**: 10.1109/TMI.2021.3124217
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08797v1)
- **Published**: 2022-04-19 10:41:09+00:00
- **Updated**: 2022-04-19 10:41:09+00:00
- **Authors**: Yue Zhao, Lingming Zhang, Yang Liu, Deyu Meng, Zhiming Cui, Chenqiang Gao, Xinbo Gao, Chunfeng Lian, Dinggang Shen
- **Comment**: 11 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:2012.13697
- **Journal**: IEEE Transactions on Medical Images, 41(4): 826-835, 2022
- **Summary**: Precise segmentation of teeth from intra-oral scanner images is an essential task in computer-aided orthodontic surgical planning. The state-of-the-art deep learning-based methods often simply concatenate the raw geometric attributes (i.e., coordinates and normal vectors) of mesh cells to train a single-stream network for automatic intra-oral scanner image segmentation. However, since different raw attributes reveal completely different geometric information, the naive concatenation of different raw attributes at the (low-level) input stage may bring unnecessary confusion in describing and differentiating between mesh cells, thus hampering the learning of high-level geometric representations for the segmentation task. To address this issue, we design a two-stream graph convolutional network (i.e., TSGCN), which can effectively handle inter-view confusion between different raw attributes to more effectively fuse their complementary information and learn discriminative multi-view geometric representations. Specifically, our TSGCN adopts two input-specific graph-learning streams to extract complementary high-level geometric representations from coordinates and normal vectors, respectively. Then, these single-view representations are further fused by a self-attention module to adaptively balance the contributions of different views in learning more discriminative multi-view representations for accurate and fully automatic tooth segmentation. We have evaluated our TSGCN on a real-patient dataset of dental (mesh) models acquired by 3D intraoral scanners. Experimental results show that our TSGCN significantly outperforms state-of-the-art methods in 3D tooth (surface) segmentation. Github: https://github.com/ZhangLingMing1/TSGCNet.



### An Energy-Based Prior for Generative Saliency
- **Arxiv ID**: http://arxiv.org/abs/2204.08803v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08803v3)
- **Published**: 2022-04-19 10:51:00+00:00
- **Updated**: 2023-06-27 06:51:25+00:00
- **Authors**: Jing Zhang, Jianwen Xie, Nick Barnes, Ping Li
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence 2023. arXiv admin note: text overlap with arXiv:2112.13528
- **Journal**: None
- **Summary**: We propose a novel generative saliency prediction framework that adopts an informative energy-based model as a prior distribution. The energy-based prior model is defined on the latent space of a saliency generator network that generates the saliency map based on a continuous latent variables and an observed image. Both the parameters of saliency generator and the energy-based prior are jointly trained via Markov chain Monte Carlo-based maximum likelihood estimation, in which the sampling from the intractable posterior and prior distributions of the latent variables are performed by Langevin dynamics. With the generative saliency model, we can obtain a pixel-wise uncertainty map from an image, indicating model confidence in the saliency prediction. Different from existing generative models, which define the prior distribution of the latent variables as a simple isotropic Gaussian distribution, our model uses an energy-based informative prior which can be more expressive in capturing the latent space of the data. With the informative energy-based prior, we extend the Gaussian distribution assumption of generative models to achieve a more representative distribution of the latent space, leading to more reliable uncertainty estimation. We apply the proposed frameworks to both RGB and RGB-D salient object detection tasks with both transformer and convolutional neural network backbones. We further propose an adversarial learning algorithm and a variational inference algorithm as alternatives to train the proposed generative framework. Experimental results show that our generative saliency model with an energy-based prior can achieve not only accurate saliency predictions but also reliable uncertainty maps that are consistent with human perception. Results and code are available at \url{https://github.com/JingZhang617/EBMGSOD}.



### SePiCo: Semantic-Guided Pixel Contrast for Domain Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.08808v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08808v2)
- **Published**: 2022-04-19 11:16:29+00:00
- **Updated**: 2023-02-20 03:05:56+00:00
- **Authors**: Binhui Xie, Shuang Li, Mingjia Li, Chi Harold Liu, Gao Huang, Guoren Wang
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI). Code is available at https://github.com/BIT-DA/SePiCo
- **Journal**: None
- **Summary**: Domain adaptive semantic segmentation attempts to make satisfactory dense predictions on an unlabeled target domain by utilizing the supervised model trained on a labeled source domain. In this work, we propose Semantic-Guided Pixel Contrast (SePiCo), a novel one-stage adaptation framework that highlights the semantic concepts of individual pixels to promote learning of class-discriminative and class-balanced pixel representations across domains, eventually boosting the performance of self-training methods. Specifically, to explore proper semantic concepts, we first investigate a centroid-aware pixel contrast that employs the category centroids of the entire source domain or a single source image to guide the learning of discriminative features. Considering the possible lack of category diversity in semantic concepts, we then blaze a trail of distributional perspective to involve a sufficient quantity of instances, namely distribution-aware pixel contrast, in which we approximate the true distribution of each semantic category from the statistics of labeled source data. Moreover, such an optimization objective can derive a closed-form upper bound by implicitly involving an infinite number of (dis)similar pairs, making it computationally efficient. Extensive experiments show that SePiCo not only helps stabilize training but also yields discriminative representations, making significant progress on both synthetic-to-real and daytime-to-nighttime adaptation scenarios.



### UID2021: An Underwater Image Dataset for Evaluation of No-reference Quality Assessment Metrics
- **Arxiv ID**: http://arxiv.org/abs/2204.08813v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08813v1)
- **Published**: 2022-04-19 11:28:08+00:00
- **Updated**: 2022-04-19 11:28:08+00:00
- **Authors**: Guojia Hou, Yuxuan Li, Huan Yang, Kunqian Li, Zhenkuan Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Achieving subjective and objective quality assessment of underwater images is of high significance in underwater visual perception and image/video processing. However, the development of underwater image quality assessment (UIQA) is limited for the lack of comprehensive human subjective user study with publicly available dataset and reliable objective UIQA metric. To address this issue, we establish a large-scale underwater image dataset, dubbed UID2021, for evaluating no-reference UIQA metrics. The constructed dataset contains 60 multiply degraded underwater images collected from various sources, covering six common underwater scenes (i.e. bluish scene, bluish-green scene, greenish scene, hazy scene, low-light scene, and turbid scene), and their corresponding 900 quality improved versions generated by employing fifteen state-of-the-art underwater image enhancement and restoration algorithms. Mean opinion scores (MOS) for UID2021 are also obtained by using the pair comparison sorting method with 52 observers. Both in-air NR-IQA and underwater-specific algorithms are tested on our constructed dataset to fairly compare the performance and analyze their strengths and weaknesses. Our proposed UID2021 dataset enables ones to evaluate NR UIQA algorithms comprehensively and paves the way for further research on UIQA. Our UID2021 will be a free download and utilized for research purposes at: https://github.com/Hou-Guojia/UID2021.



### An Efficient Domain-Incremental Learning Approach to Drive in All Weather Conditions
- **Arxiv ID**: http://arxiv.org/abs/2204.08817v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08817v2)
- **Published**: 2022-04-19 11:39:20+00:00
- **Updated**: 2022-04-21 14:16:52+00:00
- **Authors**: M. Jehanzeb Mirza, Marc Masana, Horst Possegger, Horst Bischof
- **Comment**: Accepted to CVPR Workshops - Camera Ready Version
- **Journal**: None
- **Summary**: Although deep neural networks enable impressive visual perception performance for autonomous driving, their robustness to varying weather conditions still requires attention. When adapting these models for changed environments, such as different weather conditions, they are prone to forgetting previously learned information. This catastrophic forgetting is typically addressed via incremental learning approaches which usually re-train the model by either keeping a memory bank of training samples or keeping a copy of the entire model or model parameters for each scenario. While these approaches show impressive results, they can be prone to scalability issues and their applicability for autonomous driving in all weather conditions has not been shown. In this paper we propose DISC -- Domain Incremental through Statistical Correction -- a simple online zero-forgetting approach which can incrementally learn new tasks (i.e weather conditions) without requiring re-training or expensive memory banks. The only information we store for each task are the statistical parameters as we categorize each domain by the change in first and second order statistics. Thus, as each task arrives, we simply 'plug and play' the statistical vectors for the corresponding task into the model and it immediately starts to perform well on that task. We show the efficacy of our approach by testing it for object detection in a challenging domain-incremental autonomous driving scenario where we encounter different adverse weather conditions, such as heavy rain, fog, and snow.



### Semi-supervised 3D shape segmentation with multilevel consistency and part substitution
- **Arxiv ID**: http://arxiv.org/abs/2204.08824v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08824v2)
- **Published**: 2022-04-19 11:48:24+00:00
- **Updated**: 2022-04-20 06:35:40+00:00
- **Authors**: Chun-Yu Sun, Yu-Qi Yang, Hao-Xiang Guo, Peng-Shuai Wang, Xin Tong, Yang Liu, Heung-Yeung Shum
- **Comment**: Computational Visual Media 2022. Project page:
  https://isunchy.github.io/projects/semi_supervised_3d_segmentation.html
- **Journal**: None
- **Summary**: The lack of fine-grained 3D shape segmentation data is the main obstacle to developing learning-based 3D segmentation techniques. We propose an effective semi-supervised method for learning 3D segmentations from a few labeled 3D shapes and a large amount of unlabeled 3D data. For the unlabeled data, we present a novel multilevel consistency loss to enforce consistency of network predictions between perturbed copies of a 3D shape at multiple levels: point-level, part-level, and hierarchical level. For the labeled data, we develop a simple yet effective part substitution scheme to augment the labeled 3D shapes with more structural variations to enhance training. Our method has been extensively validated on the task of 3D object semantic segmentation on PartNet and ShapeNetPart, and indoor scene semantic segmentation on ScanNet. It exhibits superior performance to existing semi-supervised and unsupervised pre-training 3D approaches. Our code and trained models are publicly available at https://github.com/isunchy/semi_supervised_3d_segmentation.



### Detect-and-describe: Joint learning framework for detection and description of objects
- **Arxiv ID**: http://arxiv.org/abs/2204.08828v1
- **DOI**: 10.1051/matecconf/201927702028
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08828v1)
- **Published**: 2022-04-19 11:57:30+00:00
- **Updated**: 2022-04-19 11:57:30+00:00
- **Authors**: Addel Zafar, Umar Khalid
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional object detection answers two questions; "what" (what the object is?) and "where" (where the object is?). "what" part of the object detection can be fine-grained further i.e. "what type", "what shape" and "what material" etc. This results in the shifting of the object detection tasks to the object description paradigm. Describing an object provides additional detail that enables us to understand the characteristics and attributes of the object ("plastic boat" not just boat, "glass bottle" not just bottle). This additional information can implicitly be used to gain insight into unseen objects (e.g. unknown object is "metallic", "has wheels"), which is not possible in traditional object detection. In this paper, we present a new approach to simultaneously detect objects and infer their attributes, we call it Detect and Describe (DaD) framework. DaD is a deep learning-based approach that extends object detection to object attribute prediction as well. We train our model on aPascal train set and evaluate our approach on aPascal test set. We achieve 97.0% in Area Under the Receiver Operating Characteristic Curve (AUC) for object attributes prediction on aPascal test set. We also show qualitative results for object attribute prediction on unseen objects, which demonstrate the effectiveness of our approach for describing unknown objects.



### Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations
- **Arxiv ID**: http://arxiv.org/abs/2204.08839v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08839v2)
- **Published**: 2022-04-19 12:10:18+00:00
- **Updated**: 2022-09-27 08:00:14+00:00
- **Authors**: Atsuhiro Noguchi, Xiao Sun, Stephen Lin, Tatsuya Harada
- **Comment**: 26 pages, ECCV2022, project page
  https://nogu-atsu.github.io/ENARF-GAN/
- **Journal**: None
- **Summary**: We propose an unsupervised method for 3D geometry-aware representation learning of articulated objects, in which no image-pose pairs or foreground masks are used for training. Though photorealistic images of articulated objects can be rendered with explicit pose control through existing 3D neural representations, these methods require ground truth 3D pose and foreground masks for training, which are expensive to obtain. We obviate this need by learning the representations with GAN training. The generator is trained to produce realistic images of articulated objects from random poses and latent vectors by adversarial training. To avoid a high computational cost for GAN training, we propose an efficient neural representation for articulated objects based on tri-planes and then present a GAN-based framework for its unsupervised training. Experiments demonstrate the efficiency of our method and show that GAN-based training enables the learning of controllable 3D representations without paired supervision.



### Core Box Image Recognition and its Improvement with a New Augmentation Technique
- **Arxiv ID**: http://arxiv.org/abs/2204.08853v2
- **DOI**: 10.1016/j.cageo.2022.105099
- **Categories**: **cs.CV**, I.4.8; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2204.08853v2)
- **Published**: 2022-04-19 12:40:21+00:00
- **Updated**: 2022-04-20 10:54:18+00:00
- **Authors**: E. E. Baraboshkin, A. E. Demidov, D. M. Orlov, D. A. Koroteev
- **Comment**: 20 pages, 16 figures, 1 table, the augmentation pipeline code samples
  published as Open-Source code for TLA at
  https://github.com/BEEugene/TemplateArtification/, continue of the research
  from arXiv:1909.10227
- **Journal**: Computers & Geosciences, vol.162, 2022
- **Summary**: Most methods for automated full-bore rock core image analysis (description, colour, properties distribution, etc.) are based on separate core column analyses. The core is usually imaged in a box because of the significant amount of time taken to get an image for each core column. The work presents an innovative method and algorithm for core columns extraction from core boxes. The conditions for core boxes imaging may differ tremendously. Such differences are disastrous for machine learning algorithms which need a large dataset describing all possible data variations. Still, such images have some standard features - a box and core. Thus, we can emulate different environments with a unique augmentation described in this work. It is called template-like augmentation (TLA). The method is described and tested on various environments, and results are compared on an algorithm trained on both 'traditional' data and a mix of traditional and TLA data. The algorithm trained with TLA data provides better metrics and can detect core on most new images, unlike the algorithm trained on data without TLA. The algorithm for core column extraction implemented in an automated core description system speeds up the core box processing by a factor of 20.



### OpenGlue: Open Source Graph Neural Net Based Pipeline for Image Matching
- **Arxiv ID**: http://arxiv.org/abs/2204.08870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08870v1)
- **Published**: 2022-04-19 13:11:16+00:00
- **Updated**: 2022-04-19 13:11:16+00:00
- **Authors**: Ostap Viniavskyi, Mariia Dobko, Dmytro Mishkin, Oles Dobosevych
- **Comment**: None
- **Journal**: None
- **Summary**: We present OpenGlue: a free open-source framework for image matching, that uses a Graph Neural Network-based matcher inspired by SuperGlue \cite{sarlin20superglue}. We show that including additional geometrical information, such as local feature scale, orientation, and affine geometry, when available (e.g. for SIFT features), significantly improves the performance of the OpenGlue matcher. We study the influence of the various attention mechanisms on accuracy and speed. We also present a simple architectural improvement by combining local descriptors with context-aware descriptors. The code and pretrained OpenGlue models for the different local features are publicly available.



### Less than Few: Self-Shot Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.08874v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08874v1)
- **Published**: 2022-04-19 13:14:43+00:00
- **Updated**: 2022-04-19 13:14:43+00:00
- **Authors**: Pengwan Yang, Yuki M. Asano, Pascal Mettes, Cees G. M. Snoek
- **Comment**: 25 pages, 5 figures, 13 tables
- **Journal**: None
- **Summary**: The goal of this paper is to bypass the need for labelled examples in few-shot video understanding at run time. While proven effective, in many practical video settings even labelling a few examples appears unrealistic. This is especially true as the level of details in spatio-temporal video understanding and with it, the complexity of annotations continues to increase. Rather than performing few-shot learning with a human oracle to provide a few densely labelled support videos, we propose to automatically learn to find appropriate support videos given a query. We call this self-shot learning and we outline a simple self-supervised learning method to generate an embedding space well-suited for unsupervised retrieval of relevant samples. To showcase this novel setting, we tackle, for the first time, video instance segmentation in a self-shot (and few-shot) setting, where the goal is to segment instances at the pixel-level across the spatial and temporal domains. We provide strong baseline performances that utilize a novel transformer-based model and show that self-shot learning can even surpass few-shot and can be positively combined for further performance gains. Experiments on new benchmarks show that our approach achieves strong performance, is competitive to oracle support in some settings, scales to large unlabelled video collections, and can be combined in a semi-supervised setting.



### Invertible Mask Network for Face Privacy-Preserving
- **Arxiv ID**: http://arxiv.org/abs/2204.08895v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.08895v1)
- **Published**: 2022-04-19 13:44:46+00:00
- **Updated**: 2022-04-19 13:44:46+00:00
- **Authors**: Yang Yang, Yiyang Huang, Ming Shi, Kejiang Chen, Weiming Zhang, Nenghai Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Face privacy-preserving is one of the hotspots that arises dramatic interests of research. However, the existing face privacy-preserving methods aim at causing the missing of semantic information of face and cannot preserve the reusability of original facial information. To achieve the naturalness of the processed face and the recoverability of the original protected face, this paper proposes face privacy-preserving method based on Invertible "Mask" Network (IMN). In IMN, we introduce a Mask-net to generate "Mask" face firstly. Then, put the "Mask" face onto the protected face and generate the masked face, in which the masked face is indistinguishable from "Mask" face. Finally, "Mask" face can be put off from the masked face and obtain the recovered face to the authorized users, in which the recovered face is visually indistinguishable from the protected face. The experimental results show that the proposed method can not only effectively protect the privacy of the protected face, but also almost perfectly recover the protected face from the masked face.



### Towards Efficient Single Image Dehazing and Desnowing
- **Arxiv ID**: http://arxiv.org/abs/2204.08899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08899v1)
- **Published**: 2022-04-19 13:51:02+00:00
- **Updated**: 2022-04-19 13:51:02+00:00
- **Authors**: Tian Ye, Sixiang Chen, Yun Liu, Erkang Chen, Yuche Li
- **Comment**: None
- **Journal**: None
- **Summary**: Removing adverse weather conditions like rain, fog, and snow from images is a challenging problem. Although the current recovery algorithms targeting a specific condition have made impressive progress, it is not flexible enough to deal with various degradation types. We propose an efficient and compact image restoration network named DAN-Net (Degradation-Adaptive Neural Network) to address this problem, which consists of multiple compact expert networks with one adaptive gated neural. A single expert network efficiently addresses specific degradation in nasty winter scenes relying on the compact architecture and three novel components. Based on the Mixture of Experts strategy, DAN-Net captures degradation information from each input image to adaptively modulate the outputs of task-specific expert networks to remove various adverse winter weather conditions. Specifically, it adopts a lightweight Adaptive Gated Neural Network to estimate gated attention maps of the input image, while different task-specific experts with the same topology are jointly dispatched to process the degraded image. Such novel image restoration pipeline handles different types of severe weather scenes effectively and efficiently. It also enjoys the benefit of coordinate boosting in which the whole network outperforms each expert trained without coordination.   Extensive experiments demonstrate that the presented manner outperforms the state-of-the-art single-task methods on image quality and has better inference efficiency. Furthermore, we have collected the first real-world winter scenes dataset to evaluate winter image restoration methods, which contains various hazy and snowy images snapped in winter. Both the dataset and source code will be publicly available.



### Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing
- **Arxiv ID**: http://arxiv.org/abs/2204.08906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08906v1)
- **Published**: 2022-04-19 14:06:16+00:00
- **Updated**: 2022-04-19 14:06:16+00:00
- **Authors**: Thiemo Alldieck, Mihai Zanfir, Cristian Sminchisescu
- **Comment**: https://phorhum.github.io/
- **Journal**: None
- **Summary**: We present PHORHUM, a novel, end-to-end trainable, deep neural network methodology for photorealistic 3D human reconstruction given just a monocular RGB image. Our pixel-aligned method estimates detailed 3D geometry and, for the first time, the unshaded surface color together with the scene illumination. Observing that 3D supervision alone is not sufficient for high fidelity color reconstruction, we introduce patch-based rendering losses that enable reliable color reconstruction on visible parts of the human, and detailed and plausible color estimation for the non-visible parts. Moreover, our method specifically addresses methodological and practical limitations of prior work in terms of representing geometry, albedo, and illumination effects, in an end-to-end model where factors can be effectively disentangled. In extensive experiments, we demonstrate the versatility and robustness of our approach. Our state-of-the-art results validate the method qualitatively and for different metrics, for both geometric and color reconstruction.



### Self-Calibrated Efficient Transformer for Lightweight Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2204.08913v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08913v1)
- **Published**: 2022-04-19 14:20:32+00:00
- **Updated**: 2022-04-19 14:20:32+00:00
- **Authors**: Wenbin Zou, Tian Ye, Weixin Zheng, Yunchen Zhang, Liang Chen, Yi Wu
- **Comment**: 10 pages, 3 figures, CVPRWorkshop
- **Journal**: None
- **Summary**: Recently, deep learning has been successfully applied to the single-image super-resolution (SISR) with remarkable performance. However, most existing methods focus on building a more complex network with a large number of layers, which can entail heavy computational costs and memory storage. To address this problem, we present a lightweight Self-Calibrated Efficient Transformer (SCET) network to solve this problem. The architecture of SCET mainly consists of the self-calibrated module and efficient transformer block, where the self-calibrated module adopts the pixel attention mechanism to extract image features effectively. To further exploit the contextual information from features, we employ an efficient transformer to help the network obtain similar features over long distances and thus recover sufficient texture details. We provide comprehensive results on different settings of the overall network. Our proposed method achieves more remarkable performance than baseline methods. The source code and pre-trained models are available at https://github.com/AlexZou14/SCET.



### Global-and-Local Collaborative Learning for Co-Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.08917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08917v1)
- **Published**: 2022-04-19 14:32:41+00:00
- **Updated**: 2022-04-19 14:32:41+00:00
- **Authors**: Runmin Cong, Ning Yang, Chongyi Li, Huazhu Fu, Yao Zhao, Qingming Huang, Sam Kwong
- **Comment**: Accepted by IEEE Transactions on Cybernetics 2022, project page:
  https://rmcong.github.io/proj_GLNet.html
- **Journal**: None
- **Summary**: The goal of co-salient object detection (CoSOD) is to discover salient objects that commonly appear in a query group containing two or more relevant images. Therefore, how to effectively extract inter-image correspondence is crucial for the CoSOD task. In this paper, we propose a global-and-local collaborative learning architecture, which includes a global correspondence modeling (GCM) and a local correspondence modeling (LCM) to capture comprehensive inter-image corresponding relationship among different images from the global and local perspectives. Firstly, we treat different images as different time slices and use 3D convolution to integrate all intra features intuitively, which can more fully extract the global group semantics. Secondly, we design a pairwise correlation transformation (PCT) to explore similarity correspondence between pairwise images and combine the multiple local pairwise correspondences to generate the local inter-image relationship. Thirdly, the inter-image relationships of the GCM and LCM are integrated through a global-and-local correspondence aggregation (GLA) module to explore more comprehensive inter-image collaboration cues. Finally, the intra- and inter-features are adaptively integrated by an intra-and-inter weighting fusion (AEWF) module to learn co-saliency features and predict the co-saliency map. The proposed GLNet is evaluated on three prevailing CoSOD benchmark datasets, demonstrating that our model trained on a small dataset (about 3k images) still outperforms eleven state-of-the-art competitors trained on some large datasets (about 8k-200k images).



### Learning to Imagine: Diversify Memory for Incremental Learning using Unlabeled Data
- **Arxiv ID**: http://arxiv.org/abs/2204.08932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08932v1)
- **Published**: 2022-04-19 15:15:18+00:00
- **Updated**: 2022-04-19 15:15:18+00:00
- **Authors**: Yu-Ming Tang, Yi-Xing Peng, Wei-Shi Zheng
- **Comment**: Accepted to CVPR2022
- **Journal**: None
- **Summary**: Deep neural network (DNN) suffers from catastrophic forgetting when learning incrementally, which greatly limits its applications. Although maintaining a handful of samples (called `exemplars`) of each task could alleviate forgetting to some extent, existing methods are still limited by the small number of exemplars since these exemplars are too few to carry enough task-specific knowledge, and therefore the forgetting remains. To overcome this problem, we propose to `imagine` diverse counterparts of given exemplars referring to the abundant semantic-irrelevant information from unlabeled data. Specifically, we develop a learnable feature generator to diversify exemplars by adaptively generating diverse counterparts of exemplars based on semantic information from exemplars and semantically-irrelevant information from unlabeled data. We introduce semantic contrastive learning to enforce the generated samples to be semantic consistent with exemplars and perform semanticdecoupling contrastive learning to encourage diversity of generated samples. The diverse generated samples could effectively prevent DNN from forgetting when learning new tasks. Our method does not bring any extra inference cost and outperforms state-of-the-art methods on two benchmarks CIFAR-100 and ImageNet-Subset by a clear margin.



### Missingness Bias in Model Debugging
- **Arxiv ID**: http://arxiv.org/abs/2204.08945v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.08945v2)
- **Published**: 2022-04-19 15:26:12+00:00
- **Updated**: 2022-06-13 21:30:08+00:00
- **Authors**: Saachi Jain, Hadi Salman, Eric Wong, Pengchuan Zhang, Vibhav Vineet, Sai Vemprala, Aleksander Madry
- **Comment**: Published at ICLR 2022
- **Journal**: None
- **Summary**: Missingness, or the absence of features from an input, is a concept fundamental to many model debugging tools. However, in computer vision, pixels cannot simply be removed from an image. One thus tends to resort to heuristics such as blacking out pixels, which may in turn introduce bias into the debugging process. We study such biases and, in particular, show how transformer-based architectures can enable a more natural implementation of missingness, which side-steps these issues and improves the reliability of model debugging in practice. Our code is available at https://github.com/madrylab/missingness



### Revisiting Vicinal Risk Minimization for Partially Supervised Multi-Label Classification Under Data Scarcity
- **Arxiv ID**: http://arxiv.org/abs/2204.08954v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2204.08954v1)
- **Published**: 2022-04-19 15:50:16+00:00
- **Updated**: 2022-04-19 15:50:16+00:00
- **Authors**: Nanqing Dong, Jiayi Wang, Irina Voiculescu
- **Comment**: Accepted by CVPR 2022 Workshop on Learning with Limited Labelled Data
  for Image and Video Understanding
- **Journal**: None
- **Summary**: Due to the high human cost of annotation, it is non-trivial to curate a large-scale medical dataset that is fully labeled for all classes of interest. Instead, it would be convenient to collect multiple small partially labeled datasets from different matching sources, where the medical images may have only been annotated for a subset of classes of interest. This paper offers an empirical understanding of an under-explored problem, namely partially supervised multi-label classification (PSMLC), where a multi-label classifier is trained with only partially labeled medical images. In contrast to the fully supervised counterpart, the partial supervision caused by medical data scarcity has non-trivial negative impacts on the model performance. A potential remedy could be augmenting the partial labels. Though vicinal risk minimization (VRM) has been a promising solution to improve the generalization ability of the model, its application to PSMLC remains an open question. To bridge the methodological gap, we provide the first VRM-based solution to PSMLC. The empirical results also provide insights into future research directions on partially supervised learning under data scarcity.



### MANIQA: Multi-dimension Attention Network for No-Reference Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2204.08958v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08958v2)
- **Published**: 2022-04-19 15:56:43+00:00
- **Updated**: 2022-04-21 03:08:48+00:00
- **Authors**: Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, Yujiu Yang
- **Comment**: None
- **Journal**: None
- **Summary**: No-Reference Image Quality Assessment (NR-IQA) aims to assess the perceptual quality of images in accordance with human subjective perception. Unfortunately, existing NR-IQA methods are far from meeting the needs of predicting accurate quality scores on GAN-based distortion images. To this end, we propose Multi-dimension Attention Network for no-reference Image Quality Assessment (MANIQA) to improve the performance on GAN-based distortion. We firstly extract features via ViT, then to strengthen global and local interactions, we propose the Transposed Attention Block (TAB) and the Scale Swin Transformer Block (SSTB). These two modules apply attention mechanisms across the channel and spatial dimension, respectively. In this multi-dimensional manner, the modules cooperatively increase the interaction among different regions of images globally and locally. Finally, a dual branch structure for patch-weighted quality prediction is applied to predict the final score depending on the weight of each patch's score. Experimental results demonstrate that MANIQA outperforms state-of-the-art methods on four standard datasets (LIVE, TID2013, CSIQ, and KADID-10K) by a large margin. Besides, our method ranked first place in the final testing phase of the NTIRE 2022 Perceptual Image Quality Assessment Challenge Track 2: No-Reference. Codes and models are available at https://github.com/IIGROUP/MANIQA.



### Rendering Nighttime Image Via Cascaded Color and Brightness Compensation
- **Arxiv ID**: http://arxiv.org/abs/2204.08970v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08970v2)
- **Published**: 2022-04-19 16:15:31+00:00
- **Updated**: 2022-04-21 17:23:11+00:00
- **Authors**: Zhihao Li, Si Yi, Zhan Ma
- **Comment**: Accepted by NTIRE 2022 (CVPR Workshop)
- **Journal**: None
- **Summary**: Image signal processing (ISP) is crucial for camera imaging, and neural networks (NN) solutions are extensively deployed for daytime scenes. The lack of sufficient nighttime image dataset and insights on nighttime illumination characteristics poses a great challenge for high-quality rendering using existing NN ISPs. To tackle it, we first built a high-resolution nighttime RAW-RGB (NR2R) dataset with white balance and tone mapping annotated by expert professionals. Meanwhile, to best capture the characteristics of nighttime illumination light sources, we develop the CBUnet, a two-stage NN ISP to cascade the compensation of color and brightness attributes. Experiments show that our method has better visual quality compared to traditional ISP pipeline, and is ranked at the second place in the NTIRE 2022 Night Photography Rendering Challenge for two tracks by respective People's and Professional Photographer's choices. The code and relevant materials are avaiable on our website: https://njuvision.github.io/CBUnet.



### Shallow camera pipeline for night photography rendering
- **Arxiv ID**: http://arxiv.org/abs/2204.08972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08972v1)
- **Published**: 2022-04-19 16:18:21+00:00
- **Updated**: 2022-04-19 16:18:21+00:00
- **Authors**: Simone Zini, Claudio Rota, Marco Buzzelli, Simone Bianco, Raimondo Schettini
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a camera pipeline for rendering visually pleasing photographs in low light conditions, as part of the NTIRE2022 Night Photography Rendering challenge. Given the nature of the task, where the objective is verbally defined by an expert photographer instead of relying on explicit ground truth images, we design an handcrafted solution, characterized by a shallow structure and by a low parameter count. Our pipeline exploits a local light enhancer as a form of high dynamic range correction, followed by a global adjustment of the image histogram to prevent washed-out results. We proportionally apply image denoising to darker regions, where it is more easily perceived, without losing details on brighter regions. The solution reached the fifth place in the competition, with a preference vote count comparable to those of other entries, based on deep convolutional neural networks. Code is available at www.github.com/AvailableAfterAcceptance.



### A comparison of different atmospheric turbulence simulation methods for image restoration
- **Arxiv ID**: http://arxiv.org/abs/2204.08974v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.08974v1)
- **Published**: 2022-04-19 16:21:36+00:00
- **Updated**: 2022-04-19 16:21:36+00:00
- **Authors**: Nithin Gopalakrishnan Nair, Kangfu Mei, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Atmospheric turbulence deteriorates the quality of images captured by long-range imaging systems by introducing blur and geometric distortions to the captured scene. This leads to a drastic drop in performance when computer vision algorithms like object/face recognition and detection are performed on these images. In recent years, various deep learning-based atmospheric turbulence mitigation methods have been proposed in the literature. These methods are often trained using synthetically generated images and tested on real-world images. Hence, the performance of these restoration methods depends on the type of simulation used for training the network. In this paper, we systematically evaluate the effectiveness of various turbulence simulation methods on image restoration. In particular, we evaluate the performance of two state-or-the-art restoration networks using six simulations method on a real-world LRFID dataset consisting of face images degraded by turbulence. This paper will provide guidance to the researchers and practitioners working in this field to choose the suitable data generation models for training deep models for turbulence mitigation. The implementation codes for the simulation methods, source codes for the networks, and the pre-trained models will be publicly made available.



### Real-Time Face Recognition System
- **Arxiv ID**: http://arxiv.org/abs/2204.08978v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08978v1)
- **Published**: 2022-04-19 16:26:48+00:00
- **Updated**: 2022-04-19 16:26:48+00:00
- **Authors**: Adarsh Ghimire, Naoufel Werghi, Sajid Javed, Jorge Dias
- **Comment**: Poster
- **Journal**: Graduate Students Research Conference 2022
- **Summary**: Over the past few decades, interest in algorithms for face recognition has been growing rapidly and has even surpassed human-level performance. Despite their accomplishments, their practical integration with a real-time performance-hungry system is not feasible due to high computational costs. So in this paper, we explore the recent, fast, and accurate face recognition system that can be easily integrated with real-time devices, and tested the algorithms on robot hardware platforms to confirm their robustness and speed.



### Dual-Domain Image Synthesis using Segmentation-Guided GAN
- **Arxiv ID**: http://arxiv.org/abs/2204.09015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09015v1)
- **Published**: 2022-04-19 17:25:54+00:00
- **Updated**: 2022-04-19 17:25:54+00:00
- **Authors**: Dena Bazazian, Andrew Calway, Dima Damen
- **Comment**: CVPR2022 Workshops. 14 pages, 19 figures
- **Journal**: None
- **Summary**: We introduce a segmentation-guided approach to synthesise images that integrate features from two distinct domains. Images synthesised by our dual-domain model belong to one domain within the semantic mask, and to another in the rest of the image - smoothly integrated. We build on the successes of few-shot StyleGAN and single-shot semantic segmentation to minimise the amount of training required in utilising two domains. The method combines a few-shot cross-domain StyleGAN with a latent optimiser to achieve images containing features of two distinct domains. We use a segmentation-guided perceptual loss, which compares both pixel-level and activations between domain-specific and dual-domain synthetic images. Results demonstrate qualitatively and quantitatively that our model is capable of synthesising dual-domain images on a variety of objects (faces, horses, cats, cars), domains (natural, caricature, sketches) and part-based masks (eyes, nose, mouth, hair, car bonnet). The code is publicly available at: https://github.com/denabazazian/Dual-Domain-Synthesis.



### Unsupervised detection of ash dieback disease (Hymenoscyphus fraxineus) using diffusion-based hyperspectral image clustering
- **Arxiv ID**: http://arxiv.org/abs/2204.09041v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2204.09041v1)
- **Published**: 2022-04-19 17:58:49+00:00
- **Updated**: 2022-04-19 17:58:49+00:00
- **Authors**: Sam L. Polk, Aland H. Y. Chan, Kangning Cui, Robert J. Plemmons, David A. Coomes, James M. Murphy
- **Comment**: (6 pages, 2 figures). Accepted to Proceedings of IEEE IGARSS 2022
- **Journal**: None
- **Summary**: Ash dieback (Hymenoscyphus fraxineus) is an introduced fungal disease that is causing the widespread death of ash trees across Europe. Remote sensing hyperspectral images encode rich structure that has been exploited for the detection of dieback disease in ash trees using supervised machine learning techniques. However, to understand the state of forest health at landscape-scale, accurate unsupervised approaches are needed. This article investigates the use of the unsupervised Diffusion and VCA-Assisted Image Segmentation (D-VIS) clustering algorithm for the detection of ash dieback disease in a forest site near Cambridge, United Kingdom. The unsupervised clustering presented in this work has high overlap with the supervised classification of previous work on this scene (overall accuracy = 71%). Thus, unsupervised learning may be used for the remote detection of ash dieback disease without the need for expert labeling.



### Learning Enriched Features for Fast Image Restoration and Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2205.01649v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01649v1)
- **Published**: 2022-04-19 17:59:45+00:00
- **Updated**: 2022-04-19 17:59:45+00:00
- **Authors**: Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, Ling Shao
- **Comment**: This article supersedes arXiv:2003.06792. Accepted for publication in
  TPAMI
- **Journal**: None
- **Summary**: Given a degraded input image, image restoration aims to recover the missing high-quality image content. Numerous applications demand effective image restoration, e.g., computational photography, surveillance, autonomous vehicles, and remote sensing. Significant advances in image restoration have been made in recent years, dominated by convolutional neural networks (CNNs). The widely-used CNN-based methods typically operate either on full-resolution or on progressively low-resolution representations. In the former case, spatial details are preserved but the contextual information cannot be precisely encoded. In the latter case, generated outputs are semantically reliable but spatially less accurate. This paper presents a new architecture with a holistic goal of maintaining spatially-precise high-resolution representations through the entire network, and receiving complementary contextual information from the low-resolution representations. The core of our approach is a multi-scale residual block containing the following key elements: (a) parallel multi-resolution convolution streams for extracting multi-scale features, (b) information exchange across the multi-resolution streams, (c) non-local attention mechanism for capturing contextual information, and (d) attention based multi-scale feature aggregation. Our approach learns an enriched set of features that combines contextual information from multiple scales, while simultaneously preserving the high-resolution spatial details. Extensive experiments on six real image benchmark datasets demonstrate that our method, named as MIRNet-v2 , achieves state-of-the-art results for a variety of image processing tasks, including defocus deblurring, image denoising, super-resolution, and image enhancement. The source code and pre-trained models are available at https://github.com/swz30/MIRNetv2



### Embodied Navigation at the Art Gallery
- **Arxiv ID**: http://arxiv.org/abs/2204.09069v1
- **DOI**: 10.1007/978-3-031-06427-2_61
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.09069v1)
- **Published**: 2022-04-19 18:00:06+00:00
- **Updated**: 2022-04-19 18:00:06+00:00
- **Authors**: Roberto Bigazzi, Federico Landi, Silvia Cascianelli, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
- **Comment**: Accepted by 21st International Conference on Image Analysis and
  Processing (ICIAP 2021)
- **Journal**: None
- **Summary**: Embodied agents, trained to explore and navigate indoor photorealistic environments, have achieved impressive results on standard datasets and benchmarks. So far, experiments and evaluations have involved domestic and working scenes like offices, flats, and houses. In this paper, we build and release a new 3D space with unique characteristics: the one of a complete art museum. We name this environment ArtGallery3D (AG3D). Compared with existing 3D scenes, the collected space is ampler, richer in visual features, and provides very sparse occupancy information. This feature is challenging for occupancy-based agents which are usually trained in crowded domestic environments with plenty of occupancy information. Additionally, we annotate the coordinates of the main points of interest inside the museum, such as paintings, statues, and other items. Thanks to this manual process, we deliver a new benchmark for PointGoal navigation inside this new space. Trajectories in this dataset are far more complex and lengthy than existing ground-truth paths for navigation in Gibson and Matterport3D. We carry on extensive experimental evaluation using our new space for evaluation and prove that existing methods hardly adapt to this scenario. As such, we believe that the availability of this 3D model will foster future research and help improve existing solutions.



### Detection of Tool based Edited Images from Error Level Analysis and Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2204.09075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09075v1)
- **Published**: 2022-04-19 18:03:55+00:00
- **Updated**: 2022-04-19 18:03:55+00:00
- **Authors**: Abhishek Gupta, Raunak Joshi, Ronald Laban
- **Comment**: 6 Pages, 9 Figures
- **Journal**: None
- **Summary**: Image Forgery is a problem of image forensics and its detection can be leveraged using Deep Learning. In this paper we present an approach for identification of authentic and tampered images done using image editing tools with Error Level Analysis and Convolutional Neural Network. The process is performed on CASIA ITDE v2 dataset and trained for 50 and 100 epochs respectively. The respective accuracies of the training and validation sets are represented using graphs.



### Photometric single-view dense 3D reconstruction in endoscopy
- **Arxiv ID**: http://arxiv.org/abs/2204.09083v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09083v1)
- **Published**: 2022-04-19 18:23:31+00:00
- **Updated**: 2022-04-19 18:23:31+00:00
- **Authors**: Victor M. Batlle, J. M. M. Montiel, Juan D. Tardos
- **Comment**: 7 pages, 7 figures, submitted to IROS 2022
- **Journal**: None
- **Summary**: Visual SLAM inside the human body will open the way to computer-assisted navigation in endoscopy. However, due to space limitations, medical endoscopes only provide monocular images, leading to systems lacking true scale. In this paper, we exploit the controlled lighting in colonoscopy to achieve the first in-vivo 3D reconstruction of the human colon using photometric stereo on a calibrated monocular endoscope. Our method works in a real medical environment, providing both a suitable in-place calibration procedure and a depth estimation technique adapted to the colon's tubular geometry. We validate our method on simulated colonoscopies, obtaining a mean error of 7% on depth estimation, which is below 3 mm on average. Our qualitative results on the EndoMapper dataset show that the method is able to correctly estimate the colon shape in real human colonoscopies, paving the ground for true-scale monocular SLAM in endoscopy.



### 4D-MultispectralNet: Multispectral Stereoscopic Disparity Estimation using Human Masks
- **Arxiv ID**: http://arxiv.org/abs/2204.09089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09089v1)
- **Published**: 2022-04-19 18:49:16+00:00
- **Updated**: 2022-04-19 18:49:16+00:00
- **Authors**: Philippe Duplessis-Guindon, Guillaume-Alexandre Bilodeau
- **Comment**: 6 pages, 2 figures
- **Journal**: None
- **Summary**: Multispectral stereoscopy is an emerging field. A lot of work has been done in classical stereoscopy, but multispectral stereoscopy is not studied as frequently. This type of stereoscopy can be used in autonomous vehicles to complete the information given by RGB cameras. It helps to identify objects in the surroundings when the conditions are more difficult, such as in night scenes. This paper focuses on the RGB-LWIR spectrum. RGB-LWIR stereoscopy has the same challenges as classical stereoscopy, that is occlusions, textureless surfaces and repetitive patterns, plus specific ones related to the different modalities. Finding matches between two spectrums adds another layer of complexity. Color, texture and shapes are more likely to vary from a spectrum to another. To address this additional challenge, this paper focuses on estimating the disparity of people present in a scene. Given the fact that people's shape is captured in both RGB and LWIR, we propose a novel method that uses segmentation masks of the human in both spectrum and than concatenate them to the original images before the first layer of a Siamese Network. This method helps to improve the accuracy, particularly within the one pixel error range.



### Behind the Machine's Gaze: Neural Networks with Biologically-inspired Constraints Exhibit Human-like Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/2204.09093v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.09093v2)
- **Published**: 2022-04-19 18:57:47+00:00
- **Updated**: 2022-11-19 05:13:45+00:00
- **Authors**: Leo Schwinn, Doina Precup, Björn Eskofier, Dario Zanca
- **Comment**: 31 pages, 14 figures, 4 tables
- **Journal**: Transactions on Machine Learning Research, October 2022, Open
  Access
- **Summary**: By and large, existing computational models of visual attention tacitly assume perfect vision and full access to the stimulus and thereby deviate from foveated biological vision. Moreover, modeling top-down attention is generally reduced to the integration of semantic features without incorporating the signal of a high-level visual tasks that have been shown to partially guide human attention. We propose the Neural Visual Attention (NeVA) algorithm to generate visual scanpaths in a top-down manner. With our method, we explore the ability of neural networks on which we impose a biologically-inspired foveated vision constraint to generate human-like scanpaths without directly training for this objective. The loss of a neural network performing a downstream visual task (i.e., classification or reconstruction) flexibly provides top-down guidance to the scanpath. Extensive experiments show that our method outperforms state-of-the-art unsupervised human attention models in terms of similarity to human scanpaths. Additionally, the flexibility of the framework allows to quantitatively investigate the role of different tasks in the generated visual behaviors. Finally, we demonstrate the superiority of the approach in a novel experiment that investigates the utility of scanpaths in real-world applications, where imperfect viewing conditions are given.



### Importance is in your attention: agent importance prediction for autonomous driving
- **Arxiv ID**: http://arxiv.org/abs/2204.09121v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09121v1)
- **Published**: 2022-04-19 20:34:30+00:00
- **Updated**: 2022-04-19 20:34:30+00:00
- **Authors**: Christopher Hazard, Akshay Bhagat, Balarama Raju Buddharaju, Zhongtao Liu, Yunming Shao, Lu Lu, Sammy Omari, Henggang Cui
- **Comment**: Accepted at CVPR 2022 Precognition workshop
- **Journal**: None
- **Summary**: Trajectory prediction is an important task in autonomous driving. State-of-the-art trajectory prediction models often use attention mechanisms to model the interaction between agents. In this paper, we show that the attention information from such models can also be used to measure the importance of each agent with respect to the ego vehicle's future planned trajectory. Our experiment results on the nuPlans dataset show that our method can effectively find and rank surrounding agents by their impact on the ego's plan.



### Diverse Imagenet Models Transfer Better
- **Arxiv ID**: http://arxiv.org/abs/2204.09134v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T07, 68T10, 68T45, I.2.10; I.2.6; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2204.09134v1)
- **Published**: 2022-04-19 21:26:58+00:00
- **Updated**: 2022-04-19 21:26:58+00:00
- **Authors**: Niv Nayman, Avram Golbert, Asaf Noy, Tan Ping, Lihi Zelnik-Manor
- **Comment**: None
- **Journal**: None
- **Summary**: A commonly accepted hypothesis is that models with higher accuracy on Imagenet perform better on other downstream tasks, leading to much research dedicated to optimizing Imagenet accuracy. Recently this hypothesis has been challenged by evidence showing that self-supervised models transfer better than their supervised counterparts, despite their inferior Imagenet accuracy. This calls for identifying the additional factors, on top of Imagenet accuracy, that make models transferable. In this work we show that high diversity of the features learnt by the model promotes transferability jointly with Imagenet accuracy. Encouraged by the recent transferability results of self-supervised models, we propose a method that combines self-supervised and supervised pretraining to generate models with both high diversity and high accuracy, and as a result high transferability. We demonstrate our results on several architectures and multiple downstream tasks, including both single-label and multi-label classification.



### RangeUDF: Semantic Surface Reconstruction from 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2204.09138v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.09138v1)
- **Published**: 2022-04-19 21:39:45+00:00
- **Updated**: 2022-04-19 21:39:45+00:00
- **Authors**: Bing Wang, Zhengdi Yu, Bo Yang, Jie Qin, Toby Breckon, Ling Shao, Niki Trigoni, Andrew Markham
- **Comment**: None
- **Journal**: None
- **Summary**: We present RangeUDF, a new implicit representation based framework to recover the geometry and semantics of continuous 3D scene surfaces from point clouds. Unlike occupancy fields or signed distance fields which can only model closed 3D surfaces, our approach is not restricted to any type of topology. Being different from the existing unsigned distance fields, our framework does not suffer from any surface ambiguity. In addition, our RangeUDF can jointly estimate precise semantics for continuous surfaces. The key to our approach is a range-aware unsigned distance function together with a surface-oriented semantic segmentation module. Extensive experiments show that RangeUDF clearly surpasses state-of-the-art approaches for surface reconstruction on four point cloud datasets. Moreover, RangeUDF demonstrates superior generalization capability across multiple unseen datasets, which is nearly impossible for all existing approaches.



### Spatially-Preserving Flattening for Location-Aware Classification of Findings in Chest X-Rays
- **Arxiv ID**: http://arxiv.org/abs/2204.09676v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09676v1)
- **Published**: 2022-04-19 22:12:30+00:00
- **Updated**: 2022-04-19 22:12:30+00:00
- **Authors**: Neha Srivathsa, Razi Mahmood, Tanveer Syeda-Mahmood
- **Comment**: 5 pages, Paper presented as a poster at the IEEE International
  Symposium on Biomedical Imaging, 2022, Paper Number 828
- **Journal**: 2022 IEEE International Symposium on Biomedical Imaging (ISBI)
- **Summary**: Chest X-rays have become the focus of vigorous deep learning research in recent years due to the availability of large labeled datasets. While classification of anomalous findings is now possible, ensuring that they are correctly localized still remains challenging, as this requires recognition of anomalies within anatomical regions. Existing deep learning networks for fine-grained anomaly classification learn location-specific findings using architectures where the location and spatial contiguity information is lost during the flattening step before classification. In this paper, we present a new spatially preserving deep learning network that preserves location and shape information through auto-encoding of feature maps during flattening. The feature maps, auto-encoder and classifier are then trained in an end-to-end fashion to enable location aware classification of findings in chest X-rays. Results are shown on a large multi-hospital chest X-ray dataset indicating a significant improvement in the quality of finding classification over state-of-the-art methods.



### Multi-Camera Multiple 3D Object Tracking on the Move for Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2204.09151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09151v1)
- **Published**: 2022-04-19 22:50:36+00:00
- **Updated**: 2022-04-19 22:50:36+00:00
- **Authors**: Pha Nguyen, Kha Gia Quach, Chi Nhan Duong, Ngan Le, Xuan-Bac Nguyen, Khoa Luu
- **Comment**: Accepted at CVPRW 2022
- **Journal**: None
- **Summary**: The development of autonomous vehicles provides an opportunity to have a complete set of camera sensors capturing the environment around the car. Thus, it is important for object detection and tracking to address new challenges, such as achieving consistent results across views of cameras. To address these challenges, this work presents a new Global Association Graph Model with Link Prediction approach to predict existing tracklets location and link detections with tracklets via cross-attention motion modeling and appearance re-identification. This approach aims at solving issues caused by inconsistent 3D object detection. Moreover, our model exploits to improve the detection accuracy of a standard 3D object detector in the nuScenes detection challenge. The experimental results on the nuScenes dataset demonstrate the benefits of the proposed method to produce SOTA performance on the existing vision-based tracking dataset.



### Performance Evaluation of Action Recognition Models on Low Quality Videos
- **Arxiv ID**: http://arxiv.org/abs/2204.09166v2
- **DOI**: 10.1109/ACCESS.2022.3204755
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09166v2)
- **Published**: 2022-04-19 23:56:56+00:00
- **Updated**: 2022-11-14 01:41:32+00:00
- **Authors**: Aoi Otani, Ryota Hashiguchi, Kazuki Omi, Norishige Fukushima, Toru Tamaki
- **Comment**: IEEE Access, vol. 10, pp. 94898-94907, 2022
- **Journal**: None
- **Summary**: In the design of action recognition models, the quality of videos is an important issue; however, the trade-off between the quality and performance is often ignored. In general, action recognition models are trained on high-quality videos, hence it is not known how the model performance degrades when tested on low-quality videos, and how much the quality of training videos affects the performance. The issue of video quality is important, however, it has not been studied so far. The goal of this study is to show the trade-off between the performance and the quality of training and test videos by quantitative performance evaluation of several action recognition models for transcoded videos in different qualities. First, we show how the video quality affects the performance of pre-trained models. We transcode the original validation videos of Kinetics400 by changing quality control parameters of JPEG (compression strength) and H.264/AVC (CRF). Then we use the transcoded videos to validate the pre-trained models. Second, we show how the models perform when trained on transcoded videos. We transcode the original training videos of Kinetics400 by changing the quality parameters of JPEG and H.264/AVC. Then we train the models on the transcoded training videos and validate them with the original and transcoded validation videos. Experimental results with JPEG transcoding show that there is no severe performance degradation (up to -1.5%) for compression strength smaller than 70 where no quality degradation is visually observed, and for larger than 80 the performance degrades linearly with respect to the quality index. Experiments with H.264/AVC transcoding show that there is no significant performance loss (up to -1%) with CRF30 while the total size of video files is reduced to 30%.



