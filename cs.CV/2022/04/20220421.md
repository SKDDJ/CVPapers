# Arxiv Papers in cs.CV on 2022-04-21
### Parametric Level-sets Enhanced To Improve Reconstruction (PaLEnTIR)
- **Arxiv ID**: http://arxiv.org/abs/2204.09815v2
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, eess.IV, 65F22, 65F99, 65N21
- **Links**: [PDF](http://arxiv.org/pdf/2204.09815v2)
- **Published**: 2022-04-21 00:03:44+00:00
- **Updated**: 2022-07-18 20:06:10+00:00
- **Authors**: Ege Ozsar, Misha Kilmer, Eric Miller, Eric de Sturler, Arvind Saibaba
- **Comment**: 37 pages, 62 figures
- **Journal**: None
- **Summary**: In this paper, we consider the restoration and reconstruction of piecewise constant objects in two and three dimensions using PaLEnTIR, a significantly enhanced Parametric level set (PaLS) model relative to the current state-of-the-art. The primary contribution of this paper is a new PaLS formulation which requires only a single level set function to recover a scene with piecewise constant objects possessing multiple unknown contrasts. Our model offers distinct advantages over current approaches to the multi-contrast, multi-object problem, all of which require multiple level sets and explicit estimation of the contrast magnitudes. Given upper and lower bounds on the contrast, our approach is able to recover objects with any distribution of contrasts and eliminates the need to know either the number of contrasts in a given scene or their values. We provide an iterative process for finding these space-varying contrast limits. Relative to most PaLS methods which employ radial basis functions (RBFs), our model makes use of non-isotropic basis functions, thereby expanding the class of shapes that a PaLS model of a given complexity can approximate. Finally, PaLEnTIR improves the conditioning of the Jacobian matrix required as part of the parameter identification process and consequently accelerates the optimization methods by controlling the magnitude of the PaLS expansion coefficients, fixing the centers of the basis functions, and the uniqueness of parametric to image mappings provided by the new parameterization. We demonstrate the performance of the new approach using both 2D and 3D variants of X-ray computed tomography, diffuse optical tomography (DOT), denoising, deconvolution problems. Application to experimental sparse CT data and simulated data with different types of noise are performed to further validate the proposed method.



### Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing
- **Arxiv ID**: http://arxiv.org/abs/2204.09817v4
- **DOI**: 10.1007/978-3-031-20059-5_1
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2204.09817v4)
- **Published**: 2022-04-21 00:04:35+00:00
- **Updated**: 2022-07-21 14:46:17+00:00
- **Authors**: Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, Hoifung Poon, Ozan Oktay
- **Comment**: To appear in ECCV 2022. Code: https://aka.ms/biovil-code Dataset:
  https://aka.ms/ms-cxr Demo Notebook: https://aka.ms/biovil-demo-notebook
- **Journal**: Computer Vision - ECCV 2022, LNCS vol 13696, pp 1-21
- **Summary**: Multi-modal data abounds in biomedicine, such as radiology images and reports. Interpreting this data at scale is essential for improving clinical care and accelerating clinical research. Biomedical text with its complex semantics poses additional challenges in vision--language modelling compared to the general domain, and previous work has used insufficiently adapted models that lack domain-specific language understanding. In this paper, we show that principled textual semantic modelling can substantially improve contrastive learning in self-supervised vision--language processing. We release a language model that achieves state-of-the-art results in radiology natural language inference through its improved vocabulary and novel language pretraining objective leveraging semantics and discourse characteristics in radiology reports. Further, we propose a self-supervised joint vision--language approach with a focus on better text modelling. It establishes new state of the art results on a wide range of publicly available benchmarks, in part by leveraging our new domain-specific language model. We release a new dataset with locally-aligned phrase grounding annotations by radiologists to facilitate the study of complex semantic modelling in biomedical vision--language processing. A broad evaluation, including on this new dataset, shows that our contrastive learning approach, aided by textual-semantic modelling, outperforms prior methods in segmentation tasks, despite only using a global-alignment objective.



### SimMC: Simple Masked Contrastive Learning of Skeleton Representations for Unsupervised Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2204.09826v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.09826v4)
- **Published**: 2022-04-21 00:19:38+00:00
- **Updated**: 2022-06-19 05:15:53+00:00
- **Authors**: Haocong Rao, Chunyan Miao
- **Comment**: Accepted at IJCAI 2022 Main Track. The Appendix A for Proof (4 pages)
  and Appendix B for Experiments (9 pages) are included in the version [v3] at
  arXiv:2204.09826
- **Journal**: None
- **Summary**: Recent advances in skeleton-based person re-identification (re-ID) obtain impressive performance via either hand-crafted skeleton descriptors or skeleton representation learning with deep learning paradigms. However, they typically require skeletal pre-modeling and label information for training, which leads to limited applicability of these methods. In this paper, we focus on unsupervised skeleton-based person re-ID, and present a generic Simple Masked Contrastive learning (SimMC) framework to learn effective representations from unlabeled 3D skeletons for person re-ID. Specifically, to fully exploit skeleton features within each skeleton sequence, we first devise a masked prototype contrastive learning (MPC) scheme to cluster the most typical skeleton features (skeleton prototypes) from different subsequences randomly masked from raw sequences, and contrast the inherent similarity between skeleton features and different prototypes to learn discriminative skeleton representations without using any label. Then, considering that different subsequences within the same sequence usually enjoy strong correlations due to the nature of motion continuity, we propose the masked intra-sequence contrastive learning (MIC) to capture intra-sequence pattern consistency between subsequences, so as to encourage learning more effective skeleton representations for person re-ID. Extensive experiments validate that the proposed SimMC outperforms most state-of-the-art skeleton-based methods. We further show its scalability and efficiency in enhancing the performance of existing models. Our codes are available at https://github.com/Kali-Hac/SimMC.



### Fast AdvProp
- **Arxiv ID**: http://arxiv.org/abs/2204.09838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09838v1)
- **Published**: 2022-04-21 01:23:57+00:00
- **Updated**: 2022-04-21 01:23:57+00:00
- **Authors**: Jieru Mei, Yucheng Han, Yutong Bai, Yixiao Zhang, Yingwei Li, Xianhang Li, Alan Yuille, Cihang Xie
- **Comment**: ICLR 2022 camera ready version
- **Journal**: None
- **Summary**: Adversarial Propagation (AdvProp) is an effective way to improve recognition models, leveraging adversarial examples. Nonetheless, AdvProp suffers from the extremely slow training speed, mainly because: a) extra forward and backward passes are required for generating adversarial examples; b) both original samples and their adversarial counterparts are used for training (i.e., 2$\times$ data). In this paper, we introduce Fast AdvProp, which aggressively revamps AdvProp's costly training components, rendering the method nearly as cheap as the vanilla training. Specifically, our modifications in Fast AdvProp are guided by the hypothesis that disentangled learning with adversarial examples is the key for performance improvements, while other training recipes (e.g., paired clean and adversarial training samples, multi-step adversarial attackers) could be largely simplified.   Our empirical results show that, compared to the vanilla training baseline, Fast AdvProp is able to further model performance on a spectrum of visual benchmarks, without incurring extra training cost. Additionally, our ablations find Fast AdvProp scales better if larger models are used, is compatible with existing data augmentation methods (i.e., Mixup and CutMix), and can be easily adapted to other recognition tasks like object detection. The code is available here: https://github.com/meijieru/fast_advprop.



### Multiscale Analysis for Improving Texture Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.09841v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09841v1)
- **Published**: 2022-04-21 01:32:22+00:00
- **Updated**: 2022-04-21 01:32:22+00:00
- **Authors**: Steve T. M. Ataky, Diego Saqui, Jonathan de Matos, Alceu S. Britto Jr., Alessandro L. Koerich
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Information from an image occurs over multiple and distinct spatial scales. Image pyramid multiresolution representations are a useful data structure for image analysis and manipulation over a spectrum of spatial scales. This paper employs the Gaussian-Laplacian pyramid to treat different spatial frequency bands of a texture separately. First, we generate three images corresponding to three levels of the Gaussian-Laplacian pyramid for an input image to capture intrinsic details. Then we aggregate features extracted from gray and color texture images using bio-inspired texture descriptors, information-theoretic measures, gray-level co-occurrence matrix features, and Haralick statistical features into a single feature vector. Such an aggregation aims at producing features that characterize textures to their maximum extent, unlike employing each descriptor separately, which may lose some relevant textural information and reduce the classification performance. The experimental results on texture and histopathologic image datasets have shown the advantages of the proposed method compared to state-of-the-art approaches. Such findings emphasize the importance of multiscale image analysis and corroborate that the descriptors mentioned above are complementary.



### Unseen Object Instance Segmentation with Fully Test-time RGB-D Embeddings Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2204.09847v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.09847v2)
- **Published**: 2022-04-21 02:35:20+00:00
- **Updated**: 2023-02-22 05:33:23+00:00
- **Authors**: Lu Zhang, Siqi Zhang, Xu Yang, Hong Qiao, Zhiyong Liu
- **Comment**: Accepted to ICRA 2023
- **Journal**: None
- **Summary**: Segmenting unseen objects is a crucial ability for the robot since it may encounter new environments during the operation. Recently, a popular solution is leveraging RGB-D features of large-scale synthetic data and directly applying the model to unseen real-world scenarios. However, the domain shift caused by the sim2real gap is inevitable, posing a crucial challenge to the segmentation model. In this paper, we emphasize the adaptation process across sim2real domains and model it as a learning problem on the BatchNorm parameters of a simulation-trained model. Specifically, we propose a novel non-parametric entropy objective, which formulates the learning objective for the test-time adaptation in an open-world manner. Then, a cross-modality knowledge distillation objective is further designed to encourage the test-time knowledge transfer for feature enhancement. Our approach can be efficiently implemented with only test images, without requiring annotations or revisiting the large-scale synthetic training data. Besides significant time savings, the proposed method consistently improves segmentation results on the overlap and boundary metrics, achieving state-of-the-art performance on unseen object instance segmentation.



### Weakly Aligned Feature Fusion for Multimodal Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.09848v1
- **DOI**: 10.1109/TNNLS.2021.3105143
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09848v1)
- **Published**: 2022-04-21 02:35:23+00:00
- **Updated**: 2022-04-21 02:35:23+00:00
- **Authors**: Lu Zhang, Zhiyong Liu, Xiangyu Zhu, Zhan Song, Xu Yang, Zhen Lei, Hong Qiao
- **Comment**: Journal extension of the previous conference paper arXiv:1901.02645,
  see IEEE page https://ieeexplore.ieee.org/document/9523596
- **Journal**: None
- **Summary**: To achieve accurate and robust object detection in the real-world scenario, various forms of images are incorporated, such as color, thermal, and depth. However, multimodal data often suffer from the position shift problem, i.e., the image pair is not strictly aligned, making one object has different positions in different modalities. For the deep learning method, this problem makes it difficult to fuse multimodal features and puzzles the convolutional neural network (CNN) training. In this article, we propose a general multimodal detector named aligned region CNN (AR-CNN) to tackle the position shift problem. First, a region feature (RF) alignment module with adjacent similarity constraint is designed to consistently predict the position shift between two modalities and adaptively align the cross-modal RFs. Second, we propose a novel region of interest (RoI) jitter strategy to improve the robustness to unexpected shift patterns. Third, we present a new multimodal feature fusion method that selects the more reliable feature and suppresses the less useful one via feature reweighting. In addition, by locating bounding boxes in both modalities and building their relationships, we provide novel multimodal labeling named KAIST-Paired. Extensive experiments on 2-D and 3-D object detection, RGB-T, and RGB-D datasets demonstrate the effectiveness and robustness of our method.



### Self-Supervised Learning to Guide Scientifically Relevant Categorization of Martian Terrain Images
- **Arxiv ID**: http://arxiv.org/abs/2204.09854v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T07, I.2.10; I.4.8; I.5.1; I.5.3; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2204.09854v1)
- **Published**: 2022-04-21 02:48:40+00:00
- **Updated**: 2022-04-21 02:48:40+00:00
- **Authors**: Tejas Panambur, Deep Chakraborty, Melissa Meyer, Ralph Milliken, Erik Learned-Miller, Mario Parente
- **Comment**: Earthvision at CVPR Workshops 2022, Code and datasets are available
  at https://github.com/TejasPanambur/mastcam
- **Journal**: None
- **Summary**: Automatic terrain recognition in Mars rover images is an important problem not just for navigation, but for scientists interested in studying rock types, and by extension, conditions of the ancient Martian paleoclimate and habitability. Existing approaches to label Martian terrain either involve the use of non-expert annotators producing taxonomies of limited granularity (e.g. soil, sand, bedrock, float rock, etc.), or rely on generic class discovery approaches that tend to produce perceptual classes such as rover parts and landscape, which are irrelevant to geologic analysis. Expert-labeled datasets containing granular geological/geomorphological terrain categories are rare or inaccessible to public, and sometimes require the extraction of relevant categorical information from complex annotations. In order to facilitate the creation of a dataset with detailed terrain categories, we present a self-supervised method that can cluster sedimentary textures in images captured from the Mast camera onboard the Curiosity rover (Mars Science Laboratory). We then present a qualitative analysis of these clusters and describe their geologic significance via the creation of a set of granular terrain categories. The precision and geologic validation of these automatically discovered clusters suggest that our methods are promising for the rapid classification of important geologic features and will therefore facilitate our long-term goal of producing a large, granular, and publicly available dataset for Mars terrain recognition.



### Remote Sensing Cross-Modal Text-Image Retrieval Based on Global and Local Information
- **Arxiv ID**: http://arxiv.org/abs/2204.09860v1
- **DOI**: 10.1109/TGRS.2022.3163706
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.09860v1)
- **Published**: 2022-04-21 03:18:09+00:00
- **Updated**: 2022-04-21 03:18:09+00:00
- **Authors**: Zhiqiang Yuan, Wenkai Zhang, Changyuan Tian, Xuee Rong, Zhengyuan Zhang, Hongqi Wang, Kun Fu, Xian Sun
- **Comment**: None
- **Journal**: in IEEE Transactions on Geoscience and Remote Sensing, vol. 60,
  pp. 1-16, 2022, Art no. 5620616
- **Summary**: Cross-modal remote sensing text-image retrieval (RSCTIR) has recently become an urgent research hotspot due to its ability of enabling fast and flexible information extraction on remote sensing (RS) images. However, current RSCTIR methods mainly focus on global features of RS images, which leads to the neglect of local features that reflect target relationships and saliency. In this article, we first propose a novel RSCTIR framework based on global and local information (GaLR), and design a multi-level information dynamic fusion (MIDF) module to efficaciously integrate features of different levels. MIDF leverages local information to correct global information, utilizes global information to supplement local information, and uses the dynamic addition of the two to generate prominent visual representation. To alleviate the pressure of the redundant targets on the graph convolution network (GCN) and to improve the model s attention on salient instances during modeling local features, the de-noised representation matrix and the enhanced adjacency matrix (DREA) are devised to assist GCN in producing superior local representations. DREA not only filters out redundant features with high similarity, but also obtains more powerful local features by enhancing the features of prominent objects. Finally, to make full use of the information in the similarity matrix during inference, we come up with a plug-and-play multivariate rerank (MR) algorithm. The algorithm utilizes the k nearest neighbors of the retrieval results to perform a reverse search, and improves the performance by combining multiple components of bidirectional retrieval. Extensive experiments on public datasets strongly demonstrate the state-of-the-art performance of GaLR methods on the RSCTIR task. The code of GaLR method, MR algorithm, and corresponding files have been made available at https://github.com/xiaoyuan1996/GaLR .



### Pixel2Mesh++: 3D Mesh Generation and Refinement from Multi-View Images
- **Arxiv ID**: http://arxiv.org/abs/2204.09866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09866v1)
- **Published**: 2022-04-21 03:42:31+00:00
- **Updated**: 2022-04-21 03:42:31+00:00
- **Authors**: Chao Wen, Yinda Zhang, Chenjie Cao, Zhuwen Li, Xiangyang Xue, Yanwei Fu
- **Comment**: Accepted by TPAMI2022. arXiv admin note: substantial text overlap
  with arXiv:1908.01491
- **Journal**: None
- **Summary**: We study the problem of shape generation in 3D mesh representation from a small number of color images with or without camera poses. While many previous works learn to hallucinate the shape directly from priors, we adopt to further improve the shape quality by leveraging cross-view information with a graph convolution network. Instead of building a direct mapping function from images to 3D shape, our model learns to predict series of deformations to improve a coarse shape iteratively. Inspired by traditional multiple view geometry methods, our network samples nearby area around the initial mesh's vertex locations and reasons an optimal deformation using perceptual feature statistics built from multiple input images. Extensive experiments show that our model produces accurate 3D shapes that are not only visually plausible from the input perspectives, but also well aligned to arbitrary viewpoints. With the help of physically driven architecture, our model also exhibits generalization capability across different semantic categories, and the number of input images. Model analysis experiments show that our model is robust to the quality of the initial mesh and the error of camera pose, and can be combined with a differentiable renderer for test-time optimization.



### Denoising of Three-Dimensional Fast Spin Echo Magnetic Resonance Images of Knee Joints using Spatial-Variant Noise-Relevant Residual Learning of Convolution Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2204.10773v1
- **DOI**: 10.1016/j.compbiomed.2022.106295
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2204.10773v1)
- **Published**: 2022-04-21 03:45:11+00:00
- **Updated**: 2022-04-21 03:45:11+00:00
- **Authors**: Shutian Zhao, Donal G. Cahill, Siyue Li, Fan Xiao, Thierry Blu, James F Griffith, Weitian Chen
- **Comment**: 6 figures, abstract accepted by Joint Annual Meeting ISMRM-ESMRMB &
  ISMRT 31st Annual Meeting
- **Journal**: Computers in Biology and Medicine, Volume 151, Part A, 2022,
  106295, ISSN 0010-4825
- **Summary**: Two-dimensional (2D) fast spin echo (FSE) techniques play a central role in the clinical magnetic resonance imaging (MRI) of knee joints. Moreover, three-dimensional (3D) FSE provides high-isotropic-resolution magnetic resonance (MR) images of knee joints, but it has a reduced signal-to-noise ratio compared to 2D FSE. Deep-learning denoising methods are a promising approach for denoising MR images, but they are often trained using synthetic noise due to challenges in obtaining true noise distributions for MR images. In this study, inherent true noise information from 2-NEX acquisition was used to develop a deep-learning model based on residual learning of convolutional neural network (CNN), and this model was used to suppress the noise in 3D FSE MR images of knee joints. The proposed CNN used two-step residual learning over parallel transporting and residual blocks and was designed to comprehensively learn real noise features from 2-NEX training data. The results of an ablation study validated the network design. The new method achieved improved denoising performance of 3D FSE knee MR images compared with current state-of-the-art methods, based on the peak signal-to-noise ratio and structural similarity index measure. The improved image quality after denoising using the new method was verified by radiological evaluation. A deep CNN using the inherent spatial-varying noise information in 2-NEX acquisitions was developed. This method showed promise for clinical MRI assessments of the knee, and has potential applications for the assessment of other anatomical structures.



### Exploring a Fine-Grained Multiscale Method for Cross-Modal Remote Sensing Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2204.09868v1
- **DOI**: 10.1109/TGRS.2021.3078451
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.09868v1)
- **Published**: 2022-04-21 03:53:19+00:00
- **Updated**: 2022-04-21 03:53:19+00:00
- **Authors**: Zhiqiang Yuan, Wenkai Zhang, Kun Fu, Xuan Li, Chubo Deng, Hongqi Wang, Xian Sun
- **Comment**: None
- **Journal**: in IEEE Transactions on Geoscience and Remote Sensing, vol. 60,
  pp. 1-19, 2022, Art no. 4404119
- **Summary**: Remote sensing (RS) cross-modal text-image retrieval has attracted extensive attention for its advantages of flexible input and efficient query. However, traditional methods ignore the characteristics of multi-scale and redundant targets in RS image, leading to the degradation of retrieval accuracy. To cope with the problem of multi-scale scarcity and target redundancy in RS multimodal retrieval task, we come up with a novel asymmetric multimodal feature matching network (AMFMN). Our model adapts to multi-scale feature inputs, favors multi-source retrieval methods, and can dynamically filter redundant features. AMFMN employs the multi-scale visual self-attention (MVSA) module to extract the salient features of RS image and utilizes visual features to guide the text representation. Furthermore, to alleviate the positive samples ambiguity caused by the strong intraclass similarity in RS image, we propose a triplet loss function with dynamic variable margin based on prior similarity of sample pairs. Finally, unlike the traditional RS image-text dataset with coarse text and higher intraclass similarity, we construct a fine-grained and more challenging Remote sensing Image-Text Match dataset (RSITMD), which supports RS image retrieval through keywords and sentence separately and jointly. Experiments on four RS text-image datasets demonstrate that the proposed model can achieve state-of-the-art performance in cross-modal RS text-image retrieval task.



### Physics vs. Learned Priors: Rethinking Camera and Algorithm Design for Task-Specific Imaging
- **Arxiv ID**: http://arxiv.org/abs/2204.09871v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09871v2)
- **Published**: 2022-04-21 04:09:32+00:00
- **Updated**: 2023-01-11 16:30:04+00:00
- **Authors**: Tzofi Klinghoffer, Siddharth Somasundaram, Kushagra Tiwary, Ramesh Raskar
- **Comment**: Published at the International Conference on Computational
  Photography (ICCP), 2022
- **Journal**: None
- **Summary**: Cameras were originally designed using physics-based heuristics to capture aesthetic images. In recent years, there has been a transformation in camera design from being purely physics-driven to increasingly data-driven and task-specific. In this paper, we present a framework to understand the building blocks of this nascent field of end-to-end design of camera hardware and algorithms. As part of this framework, we show how methods that exploit both physics and data have become prevalent in imaging and computer vision, underscoring a key trend that will continue to dominate the future of task-specific camera design. Finally, we share current barriers to progress in end-to-end design, and hypothesize how these barriers can be overcome.



### Persistent-Transient Duality in Human Behavior Modeling
- **Arxiv ID**: http://arxiv.org/abs/2204.09875v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.09875v1)
- **Published**: 2022-04-21 04:29:57+00:00
- **Updated**: 2022-04-21 04:29:57+00:00
- **Authors**: Hung Tran, Vuong Le, Svetha Venkatesh, Truyen Tran
- **Comment**: Accepted at CVPR Precognition Workshop 2022
- **Journal**: None
- **Summary**: We propose to model the persistent-transient duality in human behavior using a parent-child multi-channel neural network, which features a parent persistent channel that manages the global dynamics and children transient channels that are initiated and terminated on-demand to handle detailed interactive actions. The short-lived transient sessions are managed by a proposed Transient Switch. The neural framework is trained to discover the structure of the duality automatically. Our model shows superior performances in human-object interaction motion prediction.



### CNLL: A Semi-supervised Approach For Continual Noisy Label Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.09881v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.09881v1)
- **Published**: 2022-04-21 05:01:10+00:00
- **Updated**: 2022-04-21 05:01:10+00:00
- **Authors**: Nazmul Karim, Umar Khalid, Ashkan Esmaeili, Nazanin Rahnavard
- **Comment**: To Appear in IEEE CVPR 2022 Workshop on Continual Learning in Vision.
  arXiv admin note: text overlap with arXiv:2110.07735 by other authors
- **Journal**: None
- **Summary**: The task of continual learning requires careful design of algorithms that can tackle catastrophic forgetting. However, the noisy label, which is inevitable in a real-world scenario, seems to exacerbate the situation. While very few studies have addressed the issue of continual learning under noisy labels, long training time and complicated training schemes limit their applications in most cases. In contrast, we propose a simple purification technique to effectively cleanse the online data stream that is both cost-effective and more accurate. After purification, we perform fine-tuning in a semi-supervised fashion that ensures the participation of all available samples. Training in this fashion helps us learn a better representation that results in state-of-the-art (SOTA) performance. Through extensive experimentation on 3 benchmark datasets, MNIST, CIFAR10 and CIFAR100, we show the effectiveness of our proposed approach. We achieve a 24.8% performance gain for CIFAR10 with 20% noise over previous SOTA methods. Our code is publicly available.



### Color Invariant Skin Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.09882v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09882v2)
- **Published**: 2022-04-21 05:07:21+00:00
- **Updated**: 2022-04-22 19:48:00+00:00
- **Authors**: Han Xu, Abhijit Sarkar, A. Lynn Abbott
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of automatically detecting human skin in images without reliance on color information. A primary motivation of the work has been to achieve results that are consistent across the full range of skin tones, even while using a training dataset that is significantly biased toward lighter skin tones. Previous skin-detection methods have used color cues almost exclusively, and we present a new approach that performs well in the absence of such information. A key aspect of the work is dataset repair through augmentation that is applied strategically during training, with the goal of color invariant feature learning to enhance generalization. We have demonstrated the concept using two architectures, and experimental results show improvements in both precision and recall for most Fitzpatrick skin tones in the benchmark ECU dataset. We further tested the system with the RFW dataset to show that the proposed method performs much more consistently across different ethnicities, thereby reducing the chance of bias based on skin color. To demonstrate the effectiveness of our work, extensive experiments were performed on grayscale images as well as images obtained under unconstrained illumination and with artificial filters. Source code: https://github.com/HanXuMartin/Color-Invariant-Skin-Segmentation



### Unsupervised Video Interpolation by Learning Multilayered 2.5D Motion Fields
- **Arxiv ID**: http://arxiv.org/abs/2204.09900v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09900v1)
- **Published**: 2022-04-21 06:17:05+00:00
- **Updated**: 2022-04-21 06:17:05+00:00
- **Authors**: Ziang Cheng, Shihao Jiang, Hongdong Li
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of video frame interpolation is to increase the temporal resolution of a low frame-rate video, by interpolating novel frames between existing temporally sparse frames. This paper presents a self-supervised approach to video frame interpolation that requires only a single video. We pose the video as a set of layers. Each layer is parameterized by two implicit neural networks -- one for learning a static frame and the other for a time-varying motion field corresponding to video dynamics. Together they represent an occlusion-free subset of the scene with a pseudo-depth channel. To model inter-layer occlusions, all layers are lifted to the 2.5D space so that the frontal layer occludes distant layers. This is done by assigning each layer a depth channel, which we call `pseudo-depth', whose partial order defines the occlusion between layers. The pseudo-depths are converted to visibility values through a fully differentiable SoftMin function so that closer layers are more visible than layers in a distance. On the other hand, we parameterize the video motions by solving an ordinary differentiable equation (ODE) defined on a time-varying neural velocity field that guarantees valid motions. This implicit neural representation learns the video as a space-time continuum, allowing frame interpolation at any temporal resolution. We demonstrate the effectiveness of our method on real-world datasets, where our method achieves comparable performance to state-of-the-arts that require ground truth labels for training.



### Beyond the Prototype: Divide-and-conquer Proxies for Few-shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.09903v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09903v2)
- **Published**: 2022-04-21 06:21:14+00:00
- **Updated**: 2022-05-30 12:28:14+00:00
- **Authors**: Chunbo Lang, Binfei Tu, Gong Cheng, Junwei Han
- **Comment**: accepted to IJCAI 2022 Long Oral
- **Journal**: None
- **Summary**: Few-shot segmentation, which aims to segment unseen-class objects given only a handful of densely labeled samples, has received widespread attention from the community. Existing approaches typically follow the prototype learning paradigm to perform meta-inference, which fails to fully exploit the underlying information from support image-mask pairs, resulting in various segmentation failures, e.g., incomplete objects, ambiguous boundaries, and distractor activation. To this end, we propose a simple yet versatile framework in the spirit of divide-and-conquer. Specifically, a novel self-reasoning scheme is first implemented on the annotated support image, and then the coarse segmentation mask is divided into multiple regions with different properties. Leveraging effective masked average pooling operations, a series of support-induced proxies are thus derived, each playing a specific role in conquering the above challenges. Moreover, we devise a unique parallel decoder structure that integrates proxies with similar attributes to boost the discrimination power. Our proposed approach, named divide-and-conquer proxies (DCP), allows for the development of appropriate and reliable information as a guide at the "episode" level, not just about the object cues themselves. Extensive experiments on PASCAL-5i and COCO-20i demonstrate the superiority of DCP over conventional prototype-based approaches (up to 5~10% on average), which also establishes a new state-of-the-art. Code is available at github.com/chunbolang/DCP.



### Infographics Wizard: Flexible Infographics Authoring and Design Exploration
- **Arxiv ID**: http://arxiv.org/abs/2204.09904v2
- **DOI**: 10.1111/cgf.14527
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.LG, stat.ML, H.5.2; I.4.6; J.5
- **Links**: [PDF](http://arxiv.org/pdf/2204.09904v2)
- **Published**: 2022-04-21 06:26:06+00:00
- **Updated**: 2022-05-08 09:07:47+00:00
- **Authors**: Anjul Tyagi, Jian Zhao, Pushkar Patel, Swasti Khurana, Klaus Mueller
- **Comment**: Preprint of the EUROVIS 22 accepted paper. arXiv admin note:
  substantial text overlap with arXiv:2108.11914
- **Journal**: Computer Graphics Forum, 2022, 41: 121-132
- **Summary**: Infographics are an aesthetic visual representation of information following specific design principles of human perception. Designing infographics can be a tedious process for non-experts and time-consuming, even for professional designers. With the help of designers, we propose a semi-automated infographic framework for general structured and flow-based infographic design generation. For novice designers, our framework automatically creates and ranks infographic designs for a user-provided text with no requirement for design input. However, expert designers can still provide custom design inputs to customize the infographics. We will also contribute an individual visual group (VG) designs dataset (in SVG), along with a 1k complete infographic image dataset with segmented VGs in this work. Evaluation results confirm that by using our framework, designers from all expertise levels can generate generic infographic designs faster than existing methods while maintaining the same quality as hand-designed infographics templates.



### An Efficient End-to-End Deep Neural Network for Interstitial Lung Disease Recognition and Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.09909v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09909v1)
- **Published**: 2022-04-21 06:36:10+00:00
- **Updated**: 2022-04-21 06:36:10+00:00
- **Authors**: Masum Shah Junayed, Afsana Ahsan Jeny, Md Baharul Islam, Ikhtiar Ahmed, A F M Shahen Shah
- **Comment**: Turkish Journal of Electrical Engineering and Computer Sciences
- **Journal**: None
- **Summary**: The automated Interstitial Lung Diseases (ILDs) classification technique is essential for assisting clinicians during the diagnosis process. Detecting and classifying ILDs patterns is a challenging problem. This paper introduces an end-to-end deep convolution neural network (CNN) for classifying ILDs patterns. The proposed model comprises four convolutional layers with different kernel sizes and Rectified Linear Unit (ReLU) activation function, followed by batch normalization and max-pooling with a size equal to the final feature map size well as four dense layers. We used the ADAM optimizer to minimize categorical cross-entropy. A dataset consisting of 21328 image patches of 128 CT scans with five classes is taken to train and assess the proposed model. A comparison study showed that the presented model outperformed pre-trained CNNs and five-fold cross-validation on the same dataset. For ILDs pattern classification, the proposed approach achieved the accuracy scores of 99.09% and the average F score of 97.9%, outperforming three pre-trained CNNs. These outcomes show that the proposed model is relatively state-of-the-art in precision, recall, f score, and accuracy.



### CPGNet: Cascade Point-Grid Fusion Network for Real-Time LiDAR Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.09914v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09914v3)
- **Published**: 2022-04-21 06:56:30+00:00
- **Updated**: 2022-06-06 07:45:59+00:00
- **Authors**: Xiaoyan Li, Gang Zhang, Hongyu Pan, Zhenhua Wang
- **Comment**: Accepted in the 2022 IEEE International Conference on Robotics and
  Automation (ICRA 2022)
- **Journal**: None
- **Summary**: LiDAR semantic segmentation essential for advanced autonomous driving is required to be accurate, fast, and easy-deployed on mobile platforms. Previous point-based or sparse voxel-based methods are far away from real-time applications since time-consuming neighbor searching or sparse 3D convolution are employed. Recent 2D projection-based methods, including range view and multi-view fusion, can run in real time, but suffer from lower accuracy due to information loss during the 2D projection. Besides, to improve the performance, previous methods usually adopt test time augmentation (TTA), which further slows down the inference process. To achieve a better speed-accuracy trade-off, we propose Cascade Point-Grid Fusion Network (CPGNet), which ensures both effectiveness and efficiency mainly by the following two techniques: 1) the novel Point-Grid (PG) fusion block extracts semantic features mainly on the 2D projected grid for efficiency, while summarizes both 2D and 3D features on 3D point for minimal information loss; 2) the proposed transformation consistency loss narrows the gap between the single-time model inference and TTA. The experiments on the SemanticKITTI and nuScenes benchmarks demonstrate that the CPGNet without ensemble models or TTA is comparable with the state-of-the-art RPVNet, while it runs 4.7 times faster.



### Perception Visualization: Seeing Through the Eyes of a DNN
- **Arxiv ID**: http://arxiv.org/abs/2204.09920v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2204.09920v1)
- **Published**: 2022-04-21 07:18:55+00:00
- **Updated**: 2022-04-21 07:18:55+00:00
- **Authors**: Loris Giulivi, Mark James Carman, Giacomo Boracchi
- **Comment**: Accepted paper at BMVC 2021 (Proceedings not available yet)
- **Journal**: None
- **Summary**: Artificial intelligence (AI) systems power the world we live in. Deep neural networks (DNNs) are able to solve tasks in an ever-expanding landscape of scenarios, but our eagerness to apply these powerful models leads us to focus on their performance and deprioritises our ability to understand them. Current research in the field of explainable AI tries to bridge this gap by developing various perturbation or gradient-based explanation techniques. For images, these techniques fail to fully capture and convey the semantic information needed to elucidate why the model makes the predictions it does. In this work, we develop a new form of explanation that is radically different in nature from current explanation methods, such as Grad-CAM. Perception visualization provides a visual representation of what the DNN perceives in the input image by depicting what visual patterns the latent representation corresponds to. Visualizations are obtained through a reconstruction model that inverts the encoded features, such that the parameters and predictions of the original models are not modified. Results of our user study demonstrate that humans can better understand and predict the system's decisions when perception visualizations are available, thus easing the debugging and deployment of deep models as trusted systems.



### Progressive Training of A Two-Stage Framework for Video Restoration
- **Arxiv ID**: http://arxiv.org/abs/2204.09924v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.09924v2)
- **Published**: 2022-04-21 07:24:14+00:00
- **Updated**: 2023-02-04 13:45:13+00:00
- **Authors**: Meisong Zheng, Qunliang Xing, Minglang Qiao, Mai Xu, Lai Jiang, Huaida Liu, Ying Chen
- **Comment**: Winning two championships and one runner-up in the NTIRE 2022
  challenge on super-resolution and quality enhancement of compressed video;
  Accepted to CVPRW 2022
- **Journal**: None
- **Summary**: As a widely studied task, video restoration aims to enhance the quality of the videos with multiple potential degradations, such as noises, blurs and compression artifacts. Among video restorations, compressed video quality enhancement and video super-resolution are two of the main tacks with significant values in practical scenarios. Recently, recurrent neural networks and transformers attract increasing research interests in this field, due to their impressive capability in sequence-to-sequence modeling. However, the training of these models is not only costly but also relatively hard to converge, with gradient exploding and vanishing problems. To cope with these problems, we proposed a two-stage framework including a multi-frame recurrent network and a single-frame transformer. Besides, multiple training strategies, such as transfer learning and progressive training, are developed to shorten the training time and improve the model performance. Benefiting from the above technical contributions, our solution wins two champions and a runner-up in the NTIRE 2022 super-resolution and quality enhancement of compressed video challenges. Code is available at https://github.com/ryanxingql/winner-ntire22-vqe.



### Learning to Purification for Unsupervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2204.09931v2
- **DOI**: 10.1109/TIP.2023.3278860
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.09931v2)
- **Published**: 2022-04-21 07:46:00+00:00
- **Updated**: 2022-06-22 07:28:56+00:00
- **Authors**: Long Lan, Xiao Teng, Jing Zhang, Xiang Zhang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised person re-identification is a challenging and promising task in computer vision. Nowadays unsupervised person re-identification methods have achieved great progress by training with pseudo labels. However, how to purify feature and label noise is less explicitly studied in the unsupervised manner. To purify the feature, we take into account two types of additional features from different local views to enrich the feature representation. The proposed multi-view features are carefully integrated into our cluster contrast learning to leverage more discriminative cues that the global feature easily ignored and biased. To purify the label noise, we propose to take advantage of the knowledge of teacher model in an offline scheme. Specifically, we first train a teacher model from noisy pseudo labels, and then use the teacher model to guide the learning of our student model. In our setting, the student model could converge fast with the supervision of the teacher model thus reduce the interference of noisy labels as the teacher model greatly suffered. After carefully handling the noise and bias in the feature learning, our purification modules are proven to be very effective for unsupervised person re-identification. Extensive experiments on three popular person re-identification datasets demonstrate the superiority of our method. Especially, our approach achieves a state-of-the-art accuracy 85.8\% @mAP and 94.5\% @Rank-1 on the challenging Market-1501 benchmark with ResNet-50 under the fully unsupervised setting. The code will be released.



### Domain Invariant Model with Graph Convolutional Network for Mammogram Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.09954v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09954v1)
- **Published**: 2022-04-21 08:23:44+00:00
- **Updated**: 2022-04-21 08:23:44+00:00
- **Authors**: Churan Wang, Jing Li, Xinwei Sun, Fandong Zhang, Yizhou Yu, Yizhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Due to its safety-critical property, the image-based diagnosis is desired to achieve robustness on out-of-distribution (OOD) samples. A natural way towards this goal is capturing only clinically disease-related features, which is composed of macroscopic attributes (e.g., margins, shapes) and microscopic image-based features (e.g., textures) of lesion-related areas. However, such disease-related features are often interweaved with data-dependent (but disease irrelevant) biases during learning, disabling the OOD generalization. To resolve this problem, we propose a novel framework, namely Domain Invariant Model with Graph Convolutional Network (DIM-GCN), which only exploits invariant disease-related features from multiple domains. Specifically, we first propose a Bayesian network, which explicitly decomposes the latent variables into disease-related and other disease-irrelevant parts that are provable to be disentangled from each other. Guided by this, we reformulate the objective function based on Variational Auto-Encoder, in which the encoder in each domain has two branches: the domain-independent and -dependent ones, which respectively encode disease-related and -irrelevant features. To better capture the macroscopic features, we leverage the observed clinical attributes as a goal for reconstruction, via Graph Convolutional Network (GCN). Finally, we only implement the disease-related features for prediction. The effectiveness and utility of our method are demonstrated by the superior OOD generalization performance over others on mammogram benign/malignant diagnosis.



### Self-paced Multi-grained Cross-modal Interaction Modeling for Referring Expression Comprehension
- **Arxiv ID**: http://arxiv.org/abs/2204.09957v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09957v2)
- **Published**: 2022-04-21 08:32:47+00:00
- **Updated**: 2022-10-09 09:30:11+00:00
- **Authors**: Peihan Miao, Wei Su, Gaoang Wang, Xuewei Li, Xi Li
- **Comment**: None
- **Journal**: None
- **Summary**: As an important and challenging problem in vision-language tasks, referring expression comprehension (REC) generally requires a large amount of multi-grained information of visual and linguistic modalities to realize accurate reasoning. In addition, due to the diversity of visual scenes and the variation of linguistic expressions, some hard examples have much more abundant multi-grained information than others. How to aggregate multi-grained information from different modalities and extract abundant knowledge from hard examples is crucial in the REC task. To address aforementioned challenges, in this paper, we propose a Self-paced Multi-grained Cross-modal Interaction Modeling framework, which improves the language-to-vision localization ability through innovations in network structure and learning mechanism. Concretely, we design a transformer-based multi-grained cross-modal attention, which effectively utilizes the inherent multi-grained information in visual and linguistic encoders. Furthermore, considering the large variance of samples, we propose a self-paced sample informativeness learning to adaptively enhance the network learning for samples containing abundant multi-grained information. The proposed framework significantly outperforms state-of-the-art methods on widely used datasets, such as RefCOCO, RefCOCO+, RefCOCOg, and ReferItGame datasets, demonstrating the effectiveness of our method.



### ChildPredictor: A Child Face Prediction Framework with Disentangled Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.09962v1
- **DOI**: 10.1109/TMM.2022.3164785
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.09962v1)
- **Published**: 2022-04-21 08:38:08+00:00
- **Updated**: 2022-04-21 08:38:08+00:00
- **Authors**: Yuzhi Zhao, Lai-Man Po, Xuehui Wang, Qiong Yan, Wei Shen, Yujia Zhang, Wei Liu, Chun-Kit Wong, Chiu-Sing Pang, Weifeng Ou, Wing-Yin Yu, Buhua Liu
- **Comment**: accepted to IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: The appearances of children are inherited from their parents, which makes it feasible to predict them. Predicting realistic children's faces may help settle many social problems, such as age-invariant face recognition, kinship verification, and missing child identification. It can be regarded as an image-to-image translation task. Existing approaches usually assume domain information in the image-to-image translation can be interpreted by "style", i.e., the separation of image content and style. However, such separation is improper for the child face prediction, because the facial contours between children and parents are not the same. To address this issue, we propose a new disentangled learning strategy for children's face prediction. We assume that children's faces are determined by genetic factors (compact family features, e.g., face contour), external factors (facial attributes irrelevant to prediction, such as moustaches and glasses), and variety factors (individual properties for each child). On this basis, we formulate predictions as a mapping from parents' genetic factors to children's genetic factors, and disentangle them from external and variety factors. In order to obtain accurate genetic factors and perform the mapping, we propose a ChildPredictor framework. It transfers human faces to genetic factors by encoders and back by generators. Then, it learns the relationship between the genetic factors of parents and children through a mapping function. To ensure the generated faces are realistic, we collect a large Family Face Database to train ChildPredictor and evaluate it on the FF-Database validation set. Experimental results demonstrate that ChildPredictor is superior to other well-known image-to-image translation methods in predicting realistic and diverse child faces. Implementation codes can be found at https://github.com/zhaoyuzhi/ChildPredictor.



### Transformer-Guided Convolutional Neural Network for Cross-View Geolocalization
- **Arxiv ID**: http://arxiv.org/abs/2204.09967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09967v1)
- **Published**: 2022-04-21 08:46:41+00:00
- **Updated**: 2022-04-21 08:46:41+00:00
- **Authors**: Teng Wang, Shujuan Fan, Daikun Liu, Changyin Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Ground-to-aerial geolocalization refers to localizing a ground-level query image by matching it to a reference database of geo-tagged aerial imagery. This is very challenging due to the huge perspective differences in visual appearances and geometric configurations between these two views. In this work, we propose a novel Transformer-guided convolutional neural network (TransGCNN) architecture, which couples CNN-based local features with Transformer-based global representations for enhanced representation learning. Specifically, our TransGCNN consists of a CNN backbone extracting feature map from an input image and a Transformer head modeling global context from the CNN map. In particular, our Transformer head acts as a spatial-aware importance generator to select salient CNN features as the final feature representation. Such a coupling procedure allows us to leverage a lightweight Transformer network to greatly enhance the discriminative capability of the embedded features. Furthermore, we design a dual-branch Transformer head network to combine image features from multi-scale windows in order to improve details of the global feature representation. Extensive experiments on popular benchmark datasets demonstrate that our model achieves top-1 accuracy of 94.12\% and 84.92\% on CVUSA and CVACT_val, respectively, which outperforms the second-performing baseline with less than 50% parameters and almost 2x higher frame rate, therefore achieving a preferable accuracy-efficiency tradeoff.



### DGECN: A Depth-Guided Edge Convolutional Network for End-to-End 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2204.09983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09983v1)
- **Published**: 2022-04-21 09:19:50+00:00
- **Updated**: 2022-04-21 09:19:50+00:00
- **Authors**: Tuo Cao, Fei Luo, Yanping Fu, Wenxiao Zhang, Shengjie Zheng, Chunxia Xiao
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Monocular 6D pose estimation is a fundamental task in computer vision. Existing works often adopt a two-stage pipeline by establishing correspondences and utilizing a RANSAC algorithm to calculate 6 degrees-of-freedom (6DoF) pose. Recent works try to integrate differentiable RANSAC algorithms to achieve an end-to-end 6D pose estimation. However, most of them hardly consider the geometric features in 3D space, and ignore the topology cues when performing differentiable RANSAC algorithms. To this end, we proposed a Depth-Guided Edge Convolutional Network (DGECN) for 6D pose estimation task. We have made efforts from the following three aspects: 1) We take advantages ofestimated depth information to guide both the correspondences-extraction process and the cascaded differentiable RANSAC algorithm with geometric information. 2)We leverage the uncertainty ofthe estimated depth map to improve accuracy and robustness ofthe output 6D pose. 3) We propose a differentiable Perspective-n-Point(PnP) algorithm via edge convolution to explore the topology relations between 2D-3D correspondences. Experiments demonstrate that our proposed network outperforms current works on both effectiveness and efficiency.



### Arbitrary Bit-width Network: A Joint Layer-Wise Quantization and Adaptive Inference Approach
- **Arxiv ID**: http://arxiv.org/abs/2204.09992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.09992v1)
- **Published**: 2022-04-21 09:36:43+00:00
- **Updated**: 2022-04-21 09:36:43+00:00
- **Authors**: Chen Tang, Haoyu Zhai, Kai Ouyang, Zhi Wang, Yifei Zhu, Wenwu Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional model quantization methods use a fixed quantization scheme to different data samples, which ignores the inherent "recognition difficulty" differences between various samples. We propose to feed different data samples with varying quantization schemes to achieve a data-dependent dynamic inference, at a fine-grained layer level. However, enabling this adaptive inference with changeable layer-wise quantization schemes is challenging because the combination of bit-widths and layers is growing exponentially, making it extremely difficult to train a single model in such a vast searching space and use it in practice. To solve this problem, we present the Arbitrary Bit-width Network (ABN), where the bit-widths of a single deep network can change at runtime for different data samples, with a layer-wise granularity. Specifically, first we build a weight-shared layer-wise quantizable "super-network" in which each layer can be allocated with multiple bit-widths and thus quantized differently on demand. The super-network provides a considerably large number of combinations of bit-widths and layers, each of which can be used during inference without retraining or storing myriad models. Second, based on the well-trained super-network, each layer's runtime bit-width selection decision is modeled as a Markov Decision Process (MDP) and solved by an adaptive inference strategy accordingly. Experiments show that the super-network can be built without accuracy degradation, and the bit-widths allocation of each layer can be adjusted to deal with various inputs on the fly. On ImageNet classification, we achieve 1.1% top1 accuracy improvement while saving 36.2% BitOps.



### On Learning the Invisible in Photoacoustic Tomography with Flat Directionally Sensitive Detector
- **Arxiv ID**: http://arxiv.org/abs/2204.10001v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.10001v2)
- **Published**: 2022-04-21 09:57:01+00:00
- **Updated**: 2022-11-06 12:49:46+00:00
- **Authors**: Bolin Pan, Marta M. Betcke
- **Comment**: Submitted to SIAM Journal on Imaging Sciences 2nd Review
- **Journal**: None
- **Summary**: In photoacoustic tomography (PAT) with flat sensor, we routinely encounter two types of limited data. The first is due to using a finite sensor and is especially perceptible if the region of interest is large relative to the sensor or located farther away from the sensor. In this paper, we focus on the second type caused by a varying sensitivity of the sensor to the incoming wavefront direction which can be modelled as binary i.e. by a cone of sensitivity. Such visibility conditions result, in the Fourier domain, in a restriction of both the image and the data to a bow-tie, akin to the one corresponding to the range of the forward operator. The visible wavefrontsets in image and data domains, are related by the wavefront direction mapping. We adapt the wedge restricted Curvelet decomposition, we previously proposed for the representation of the full PAT data, to separate the visible and invisible wavefronts in the image. We optimally combine fast approximate operators with tailored deep neural network architectures into efficient learned reconstruction methods which perform reconstruction of the visible coefficients and the invisible coefficients are learned from a training set of similar data.



### Fluctuation-based Outlier Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.10007v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.10007v1)
- **Published**: 2022-04-21 10:09:14+00:00
- **Updated**: 2022-04-21 10:09:14+00:00
- **Authors**: Xusheng Du, Enguang Zuo, Zhenzhen He, Jiong Yu
- **Comment**: 20pages,11figures
- **Journal**: None
- **Summary**: Outlier detection is an important topic in machine learning and has been used in a wide range of applications. Outliers are objects that are few in number and deviate from the majority of objects. As a result of these two properties, we show that outliers are susceptible to a mechanism called fluctuation. This article proposes a method called fluctuation-based outlier detection (FBOD) that achieves a low linear time complexity and detects outliers purely based on the concept of fluctuation without employing any distance, density or isolation measure. Fundamentally different from all existing methods. FBOD first converts the Euclidean structure datasets into graphs by using random links, then propagates the feature value according to the connection of the graph. Finally, by comparing the difference between the fluctuation of an object and its neighbors, FBOD determines the object with a larger difference as an outlier. The results of experiments comparing FBOD with seven state-of-the-art algorithms on eight real-world tabular datasets and three video datasets show that FBOD outperforms its competitors in the majority of cases and that FBOD has only 5% of the execution time of the fastest algorithm. The experiment codes are available at: https://github.com/FluctuationOD/Fluctuation-based-Outlier-Detection.



### Towards Fewer Labels: Support Pair Active Learning for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2204.10008v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10008v1)
- **Published**: 2022-04-21 10:10:18+00:00
- **Updated**: 2022-04-21 10:10:18+00:00
- **Authors**: Dapeng Jin, Minxian Li
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised-learning based person re-identification (re-id) require a large amount of manual labeled data, which is not applicable in practical re-id deployment. In this work, we propose a Support Pair Active Learning (SPAL) framework to lower the manual labeling cost for large-scale person reidentification. The support pairs can provide the most informative relationships and support the discriminative feature learning. Specifically, we firstly design a dual uncertainty selection strategy to iteratively discover support pairs and require human annotations. Afterwards, we introduce a constrained clustering algorithm to propagate the relationships of labeled support pairs to other unlabeled samples. Moreover, a hybrid learning strategy consisting of an unsupervised contrastive loss and a supervised support pair loss is proposed to learn the discriminative re-id feature representation. The proposed overall framework can effectively lower the labeling cost by mining and leveraging the critical support pairs. Extensive experiments demonstrate the superiority of the proposed method over state-of-the-art active learning methods on large-scale person re-id benchmarks.



### Understanding the Domain Gap in LiDAR Object Detection Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.10024v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.10024v1)
- **Published**: 2022-04-21 11:18:48+00:00
- **Updated**: 2022-04-21 11:18:48+00:00
- **Authors**: Jasmine Richter, Florian Faion, Di Feng, Paul Benedikt Becker, Piotr Sielecki, Claudius Glaeser
- **Comment**: 14. Uni-DAS e.V. Workshop Fahrerassistenz und automatisiertes Fahren
- **Journal**: None
- **Summary**: In order to make autonomous driving a reality, artificial neural networks have to work reliably in the open-world. However, the open-world is vast and continuously changing, so it is not technically feasible to collect and annotate training datasets which accurately represent this domain. Therefore, there are always domain gaps between training datasets and the open-world which must be understood. In this work, we investigate the domain gaps between high-resolution and low-resolution LiDAR sensors in object detection networks. Using a unique dataset, which enables us to study sensor resolution domain gaps independent of other effects, we show two distinct domain gaps - an inference domain gap and a training domain gap. The inference domain gap is characterised by a strong dependence on the number of LiDAR points per object, while the training gap shows no such dependence. These fndings show that different approaches are required to close these inference and training domain gaps.



### Is Neuron Coverage Needed to Make Person Detection More Robust?
- **Arxiv ID**: http://arxiv.org/abs/2204.10027v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.10027v1)
- **Published**: 2022-04-21 11:23:33+00:00
- **Updated**: 2022-04-21 11:23:33+00:00
- **Authors**: Svetlana Pavlitskaya, Şiyar Yıkmış, J. Marius Zöllner
- **Comment**: Accepted for publication at CVPR 2022 TCV workshop
- **Journal**: None
- **Summary**: The growing use of deep neural networks (DNNs) in safety- and security-critical areas like autonomous driving raises the need for their systematic testing. Coverage-guided testing (CGT) is an approach that applies mutation or fuzzing according to a predefined coverage metric to find inputs that cause misbehavior. With the introduction of a neuron coverage metric, CGT has also recently been applied to DNNs. In this work, we apply CGT to the task of person detection in crowded scenes. The proposed pipeline uses YOLOv3 for person detection and includes finding DNN bugs via sampling and mutation, and subsequent DNN retraining on the updated training set. To be a bug, we require a mutated image to cause a significant performance drop compared to a clean input. In accordance with the CGT, we also consider an additional requirement of increased coverage in the bug definition. In order to explore several types of robustness, our approach includes natural image transformations, corruptions, and adversarial examples generated with the Daedalus attack. The proposed framework has uncovered several thousand cases of incorrect DNN behavior. The relative change in mAP performance of the retrained models reached on average between 26.21\% and 64.24\% for different robustness types. However, we have found no evidence that the investigated coverage metrics can be advantageously used to improve robustness.



### A New Dataset and Transformer for Stereoscopic Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2204.10039v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10039v1)
- **Published**: 2022-04-21 11:49:29+00:00
- **Updated**: 2022-04-21 11:49:29+00:00
- **Authors**: Hassan Imani, Md Baharul Islam, Lai-Kuan Wong
- **Comment**: Conference on Computer Vision and Pattern Recognition (CVPR 2022)
- **Journal**: None
- **Summary**: Stereo video super-resolution (SVSR) aims to enhance the spatial resolution of the low-resolution video by reconstructing the high-resolution video. The key challenges in SVSR are preserving the stereo-consistency and temporal-consistency, without which viewers may experience 3D fatigue. There are several notable works on stereoscopic image super-resolution, but there is little research on stereo video super-resolution. In this paper, we propose a novel Transformer-based model for SVSR, namely Trans-SVSR. Trans-SVSR comprises two key novel components: a spatio-temporal convolutional self-attention layer and an optical flow-based feed-forward layer that discovers the correlation across different video frames and aligns the features. The parallax attention mechanism (PAM) that uses the cross-view information to consider the significant disparities is used to fuse the stereo views. Due to the lack of a benchmark dataset suitable for the SVSR task, we collected a new stereoscopic video dataset, SVSR-Set, containing 71 full high-definition (HD) stereo videos captured using a professional stereo camera. Extensive experiments on the collected dataset, along with two other datasets, demonstrate that the Trans-SVSR can achieve competitive performance compared to the state-of-the-art methods. Project code and additional results are available at https://github.com/H-deep/Trans-SVSR/



### Implicit Shape Completion via Adversarial Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/2204.10060v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10060v1)
- **Published**: 2022-04-21 12:49:59+00:00
- **Updated**: 2022-04-21 12:49:59+00:00
- **Authors**: Abhishek Saroha, Marvin Eisenberger, Tarun Yenamandra, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel neural implicit shape method for partial point cloud completion. To that end, we combine a conditional Deep-SDF architecture with learned, adversarial shape priors. More specifically, our network converts partial inputs into a global latent code and then recovers the full geometry via an implicit, signed distance generator. Additionally, we train a PointNet++ discriminator that impels the generator to produce plausible, globally consistent reconstructions. In that way, we effectively decouple the challenges of predicting shapes that are both realistic, i.e. imitate the training set's pose distribution, and accurate in the sense that they replicate the partial input observations. In our experiments, we demonstrate state-of-the-art performance for completing partial shapes, considering both man-made objects (e.g. airplanes, chairs, ...) and deformable shape categories (human bodies). Finally, we show that our adversarial training approach leads to visually plausible reconstructions that are highly consistent in recovering missing parts of a given object.



### Absolute Wrong Makes Better: Boosting Weakly Supervised Object Detection via Negative Deterministic Information
- **Arxiv ID**: http://arxiv.org/abs/2204.10068v2
- **DOI**: 10.24963/ijcai.2022/192
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10068v2)
- **Published**: 2022-04-21 12:55:27+00:00
- **Updated**: 2023-05-17 06:17:01+00:00
- **Authors**: Guanchun Wang, Xiangrong Zhang, Zelin Peng, Xu Tang, Huiyu Zhou, Licheng Jiao
- **Comment**: 7 pages, 5 figures, accepted by IJCAI 2022
- **Journal**: None
- **Summary**: Weakly supervised object detection (WSOD) is a challenging task, in which image-level labels (e.g., categories of the instances in the whole image) are used to train an object detector. Many existing methods follow the standard multiple instance learning (MIL) paradigm and have achieved promising performance. However, the lack of deterministic information leads to part domination and missing instances. To address these issues, this paper focuses on identifying and fully exploiting the deterministic information in WSOD. We discover that negative instances (i.e. absolutely wrong instances), ignored in most of the previous studies, normally contain valuable deterministic information. Based on this observation, we here propose a negative deterministic information (NDI) based method for improving WSOD, namely NDI-WSOD. Specifically, our method consists of two stages: NDI collecting and exploiting. In the collecting stage, we design several processes to identify and distill the NDI from negative instances online. In the exploiting stage, we utilize the extracted NDI to construct a novel negative contrastive learning mechanism and a negative guided instance selection strategy for dealing with the issues of part domination and missing instances, respectively. Experimental results on several public benchmarks including VOC 2007, VOC 2012 and MS COCO show that our method achieves satisfactory performance.



### Learn from Unpaired Data for Image Restoration: A Variational Bayes Approach
- **Arxiv ID**: http://arxiv.org/abs/2204.10090v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.10090v3)
- **Published**: 2022-04-21 13:27:17+00:00
- **Updated**: 2022-09-11 12:15:32+00:00
- **Authors**: Dihan Zheng, Xiaowen Zhang, Kaisheng Ma, Chenglong Bao
- **Comment**: None
- **Journal**: None
- **Summary**: Collecting paired training data is difficult in practice, but the unpaired samples broadly exist. Current approaches aim at generating synthesized training data from unpaired samples by exploring the relationship between the corrupted and clean data. This work proposes LUD-VAE, a deep generative method to learn the joint probability density function from data sampled from marginal distributions. Our approach is based on a carefully designed probabilistic graphical model in which the clean and corrupted data domains are conditionally independent. Using variational inference, we maximize the evidence lower bound (ELBO) to estimate the joint probability density function. Furthermore, we show that the ELBO is computable without paired samples under the inference invariant assumption. This property provides the mathematical rationale of our approach in the unpaired setting. Finally, we apply our method to real-world image denoising, super-resolution, and low-light image enhancement tasks and train the models using the synthetic data generated by the LUD-VAE. Experimental results validate the advantages of our method over other approaches.



### R2-Trans:Fine-Grained Visual Categorization with Redundancy Reduction
- **Arxiv ID**: http://arxiv.org/abs/2204.10095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.10095v1)
- **Published**: 2022-04-21 13:35:38+00:00
- **Updated**: 2022-04-21 13:35:38+00:00
- **Authors**: Yu Wang, Shuo Ye, Shujian Yu, Xinge You
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained visual categorization (FGVC) aims to discriminate similar subcategories, whose main challenge is the large intraclass diversities and subtle inter-class differences. Existing FGVC methods usually select discriminant regions found by a trained model, which is prone to neglect other potential discriminant information. On the other hand, the massive interactions between the sequence of image patches in ViT make the resulting class-token contain lots of redundant information, which may also impacts FGVC performance. In this paper, we present a novel approach for FGVC, which can simultaneously make use of partial yet sufficient discriminative information in environmental cues and also compress the redundant information in class-token with respect to the target. Specifically, our model calculates the ratio of high-weight regions in a batch, adaptively adjusts the masking threshold and achieves moderate extraction of background information in the input space. Moreover, we also use the Information Bottleneck~(IB) approach to guide our network to learn a minimum sufficient representations in the feature space. Experimental results on three widely-used benchmark datasets verify that our approach can achieve outperforming performance than other state-of-the-art approaches and baseline models.



### GAF-NAU: Gramian Angular Field encoded Neighborhood Attention U-Net for Pixel-Wise Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.10099v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.10099v1)
- **Published**: 2022-04-21 13:45:18+00:00
- **Updated**: 2022-04-21 13:45:18+00:00
- **Authors**: Sidike Paheding, Abel A. Reyes, Anush Kasaragod, Thomas Oommen
- **Comment**: 8 Pages, 9 Figures
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) classification is the most vibrant area of research in the hyperspectral community due to the rich spectral information contained in HSI can greatly aid in identifying objects of interest. However, inherent non-linearity between materials and the corresponding spectral profiles brings two major challenges in HSI classification: interclass similarity and intraclass variability. Many advanced deep learning methods have attempted to address these issues from the perspective of a region/patch-based approach, instead of a pixel-based alternate. However, the patch-based approaches hypothesize that neighborhood pixels of a target pixel in a fixed spatial window belong to the same class. And this assumption is not always true. To address this problem, we herein propose a new deep learning architecture, namely Gramian Angular Field encoded Neighborhood Attention U-Net (GAF-NAU), for pixel-based HSI classification. The proposed method does not require regions or patches centered around a raw target pixel to perform 2D-CNN based classification, instead, our approach transforms 1D pixel vector in HSI into 2D angular feature space using Gramian Angular Field (GAF) and then embed it to a new neighborhood attention network to suppress irrelevant angular feature while emphasizing on pertinent features useful for HSI classification task. Evaluation results on three publicly available HSI datasets demonstrate the superior performance of the proposed model.



### Working memory inspired hierarchical video decomposition with transformative representations
- **Arxiv ID**: http://arxiv.org/abs/2204.10105v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2204.10105v3)
- **Published**: 2022-04-21 13:49:43+00:00
- **Updated**: 2022-05-05 23:52:44+00:00
- **Authors**: Binjie Qin, Haohao Mao, Ruipeng Zhang, Yueqi Zhu, Song Ding, Xu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Video decomposition is very important to extract moving foreground objects from complex backgrounds in computer vision, machine learning, and medical imaging, e.g., extracting moving contrast-filled vessels from the complex and noisy backgrounds of X-ray coronary angiography (XCA). However, the challenges caused by dynamic backgrounds, overlapping heterogeneous environments and complex noises still exist in video decomposition. To solve these problems, this study is the first to introduce a flexible visual working memory model in video decomposition tasks to provide interpretable and high-performance hierarchical deep architecture, integrating the transformative representations between sensory and control layers from the perspective of visual and cognitive neuroscience. Specifically, robust PCA unrolling networks acting as a structure-regularized sensor layer decompose XCA into sparse/low-rank structured representations to separate moving contrast-filled vessels from noisy and complex backgrounds. Then, patch recurrent convolutional LSTM networks with a backprojection module embody unstructured random representations of the control layer in working memory, recurrently projecting spatiotemporally decomposed nonlocal patches into orthogonal subspaces for heterogeneous vessel retrieval and interference suppression. This video decomposition deep architecture effectively restores the heterogeneous profiles of intensity and the geometries of moving objects against the complex background interferences. Experiments show that the proposed method significantly outperforms state-of-the-art methods in accurate moving contrast-filled vessel extraction with excellent flexibility and computational efficiency.



### Convex Augmentation for Total Variation Based Phase Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2205.00834v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.00834v1)
- **Published**: 2022-04-21 13:55:14+00:00
- **Updated**: 2022-04-21 13:55:14+00:00
- **Authors**: Jianwei Niu, Hok Shing Wong, Tieyong Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Phase retrieval is an important problem with significant physical and industrial applications. In this paper, we consider the case where the magnitude of the measurement of an underlying signal is corrupted by Gaussian noise. We introduce a convex augmentation approach for phase retrieval based on total variation regularization. In contrast to popular convex relaxation models like PhaseLift, our model can be efficiently solved by a modified semi-proximal alternating direction method of multipliers (sPADMM). The modified sPADMM is more general and flexible than the standard one, and its convergence is also established in this paper. Extensive numerical experiments are conducted to showcase the effectiveness of the proposed method.



### Deep Model-Based Super-Resolution with Non-uniform Blur
- **Arxiv ID**: http://arxiv.org/abs/2204.10109v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.10109v3)
- **Published**: 2022-04-21 13:57:21+00:00
- **Updated**: 2022-10-20 14:25:01+00:00
- **Authors**: Charles Laroche, Andrés Almansa, Matias Tassano
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a state-of-the-art method for super-resolution with non-uniform blur. Single-image super-resolution methods seek to restore a high-resolution image from blurred, subsampled, and noisy measurements. Despite their impressive performance, existing techniques usually assume a uniform blur kernel. Hence, these techniques do not generalize well to the more general case of non-uniform blur. Instead, in this paper, we address the more realistic and computationally challenging case of spatially-varying blur. To this end, we first propose a fast deep plug-and-play algorithm, based on linearized ADMM splitting techniques, which can solve the super-resolution problem with spatially-varying blur. Second, we unfold our iterative algorithm into a single network and train it end-to-end. In this way, we overcome the intricacy of manually tuning the parameters involved in the optimization scheme. Our algorithm presents remarkable performance and generalizes well after a single training to a large family of spatially-varying blur kernels, noise levels and scale factors.



### OSSO: Obtaining Skeletal Shape from Outside
- **Arxiv ID**: http://arxiv.org/abs/2204.10129v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10129v1)
- **Published**: 2022-04-21 14:33:42+00:00
- **Updated**: 2022-04-21 14:33:42+00:00
- **Authors**: Marilyn Keller, Silvia Zuffi, Michael J. Black, Sergi Pujades
- **Comment**: Project page: https://osso.is.tue.mpg.de/. Accepted in CVPR 2022
- **Journal**: None
- **Summary**: We address the problem of inferring the anatomic skeleton of a person, in an arbitrary pose, from the 3D surface of the body; i.e. we predict the inside (bones) from the outside (skin). This has many applications in medicine and biomechanics. Existing state-of-the-art biomechanical skeletons are detailed but do not easily generalize to new subjects. Additionally, computer vision and graphics methods that predict skeletons are typically heuristic, not learned from data, do not leverage the full 3D body surface, and are not validated against ground truth. To our knowledge, our system, called OSSO (Obtaining Skeletal Shape from Outside), is the first to learn the mapping from the 3D body surface to the internal skeleton from real data. We do so using 1000 male and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit a parametric 3D body shape model (STAR) to capture the body surface and a novel part-based 3D skeleton model to capture the bones. This provides inside/outside training pairs. We model the statistical variation of full skeletons using PCA in a pose-normalized space. We then train a regressor from body shape parameters to skeleton shape parameters and refine the skeleton to satisfy constraints on physical plausibility. Given an arbitrary 3D body shape and pose, OSSO predicts a realistic skeleton inside. In contrast to previous work, we evaluate the accuracy of the skeleton shape quantitatively on held-out DXA scans, outperforming the state-of-the-art. We also show 3D skeleton prediction from varied and challenging 3D bodies. The code to infer a skeleton from a body shape is available for research at https://osso.is.tue.mpg.de/, and the dataset of paired outer surface (skin) and skeleton (bone) meshes is available as a Biobank Returned Dataset. This research has been conducted using the UK Biobank Resource.



### Toward Fast, Flexible, and Robust Low-Light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2204.10137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10137v1)
- **Published**: 2022-04-21 14:40:32+00:00
- **Updated**: 2022-04-21 14:40:32+00:00
- **Authors**: Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, Zhongxuan Luo
- **Comment**: CVPR 2022 (Oral). Project page: https://github.com/vis-opt-group/SCI
- **Journal**: None
- **Summary**: Existing low-light image enhancement techniques are mostly not only difficult to deal with both visual quality and computational efficiency but also commonly invalid in unknown complex scenarios. In this paper, we develop a new Self-Calibrated Illumination (SCI) learning framework for fast, flexible, and robust brightening images in real-world low-light scenarios. To be specific, we establish a cascaded illumination learning process with weight sharing to handle this task. Considering the computational burden of the cascaded pattern, we construct the self-calibrated module which realizes the convergence between results of each stage, producing the gains that only use the single basic block for inference (yet has not been exploited in previous works), which drastically diminishes computation cost. We then define the unsupervised training loss to elevate the model capability that can adapt to general scenes. Further, we make comprehensive explorations to excavate SCI's inherent properties (lacking in existing works) including operation-insensitive adaptability (acquiring stable performance under the settings of different simple operations) and model-irrelevant generality (can be applied to illumination-based existing works to improve performance). Finally, plenty of experiments and ablation studies fully indicate our superiority in both quality and efficiency. Applications on low-light face detection and nighttime semantic segmentation fully reveal the latent practical values for SCI. The source code is available at https://github.com/vis-opt-group/SCI.



### Multiple EffNet/ResNet Architectures for Melanoma Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.10142v1
- **DOI**: 10.1109/ICCEA53728.2021.00061
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.10142v1)
- **Published**: 2022-04-21 14:46:55+00:00
- **Updated**: 2022-04-21 14:46:55+00:00
- **Authors**: Jiaqi Xue, Chentian Ma, Li Li, Xuan Wen
- **Comment**: 16 pages,20 figures
- **Journal**: 2021 International Conference on Computer Engineering and
  Application (ICCEA)
- **Summary**: Melanoma is the most malignant skin tumor and usually cancerates from normal moles, which is difficult to distinguish benign from malignant in the early stage. Therefore, many machine learning methods are trying to make auxiliary prediction. However, these methods attach more attention to the image data of suspected tumor, and focus on improving the accuracy of image classification, but ignore the significance of patient-level contextual information for disease diagnosis in actual clinical diagnosis. To make more use of patient information and improve the accuracy of diagnosis, we propose a new melanoma classification model based on EffNet and Resnet. Our model not only uses images within the same patient but also consider patient-level contextual information for better cancer prediction. The experimental results demonstrated that the proposed model achieved 0.981 ACC. Furthermore, we note that the overall ROC value of the model is 0.976 which is better than the previous state-of-the-art approaches.



### A case for using rotation invariant features in state of the art feature matchers
- **Arxiv ID**: http://arxiv.org/abs/2204.10144v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10144v2)
- **Published**: 2022-04-21 14:49:45+00:00
- **Updated**: 2022-07-03 12:54:16+00:00
- **Authors**: Georg Bökman, Fredrik Kahl
- **Comment**: CVPRW 2022, updated version
- **Journal**: None
- **Summary**: The aim of this paper is to demonstrate that a state of the art feature matcher (LoFTR) can be made more robust to rotations by simply replacing the backbone CNN with a steerable CNN which is equivariant to translations and image rotations. It is experimentally shown that this boost is obtained without reducing performance on ordinary illumination and viewpoint matching sequences.



### Recommendations on test datasets for evaluating AI solutions in pathology
- **Arxiv ID**: http://arxiv.org/abs/2204.14226v1
- **DOI**: 10.1038/s41379-022-01147-y
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2204.14226v1)
- **Published**: 2022-04-21 14:52:47+00:00
- **Updated**: 2022-04-21 14:52:47+00:00
- **Authors**: André Homeyer, Christian Geißler, Lars Ole Schwen, Falk Zakrzewski, Theodore Evans, Klaus Strohmenger, Max Westphal, Roman David Bülow, Michaela Kargl, Aray Karjauv, Isidre Munné-Bertran, Carl Orge Retzlaff, Adrià Romero-López, Tomasz Sołtysiński, Markus Plass, Rita Carvalho, Peter Steinbach, Yu-Chia Lan, Nassim Bouteldja, David Haber, Mateo Rojas-Carulla, Alireza Vafaei Sadr, Matthias Kraft, Daniel Krüger, Rutger Fick, Tobias Lang, Peter Boor, Heimo Müller, Peter Hufnagl, Norman Zerbe
- **Comment**: None
- **Journal**: Mod Pathol (2022)
- **Summary**: Artificial intelligence (AI) solutions that automatically extract information from digital histology images have shown great promise for improving pathological diagnosis. Prior to routine use, it is important to evaluate their predictive performance and obtain regulatory approval. This assessment requires appropriate test datasets. However, compiling such datasets is challenging and specific recommendations are missing.   A committee of various stakeholders, including commercial AI developers, pathologists, and researchers, discussed key aspects and conducted extensive literature reviews on test datasets in pathology. Here, we summarize the results and derive general recommendations for the collection of test datasets.   We address several questions: Which and how many images are needed? How to deal with low-prevalence subsets? How can potential bias be detected? How should datasets be reported? What are the regulatory requirements in different countries?   The recommendations are intended to help AI developers demonstrate the utility of their products and to help regulatory agencies and end users verify reported performance measures. Further research is needed to formulate criteria for sufficiently representative test datasets so that AI solutions can operate with less user intervention and better support diagnostic workflows in the future.



### WebFace260M: A Benchmark for Million-Scale Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.10149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10149v1)
- **Published**: 2022-04-21 14:56:53+00:00
- **Updated**: 2022-04-21 14:56:53+00:00
- **Authors**: Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie Huang, Xinze Chen, Jiagang Zhu, Tian Yang, Dalong Du, Jiwen Lu, Jie Zhou
- **Comment**: Accepted by T-PAMI. Extension of our CVPR-2021 work:
  arXiv:2103.04098. Project website is https://www.face-benchmark.org
- **Journal**: None
- **Summary**: Face benchmarks empower the research community to train and evaluate high-performance face recognition systems. In this paper, we contribute a new million-scale recognition benchmark, containing uncurated 4M identities/260M faces (WebFace260M) and cleaned 2M identities/42M faces (WebFace42M) training data, as well as an elaborately designed time-constrained evaluation protocol. Firstly, we collect 4M name lists and download 260M faces from the Internet. Then, a Cleaning Automatically utilizing Self-Training (CAST) pipeline is devised to purify the tremendous WebFace260M, which is efficient and scalable. To the best of our knowledge, the cleaned WebFace42M is the largest public face recognition training set and we expect to close the data gap between academia and industry. Referring to practical deployments, Face Recognition Under Inference Time conStraint (FRUITS) protocol and a new test set with rich attributes are constructed. Besides, we gather a large-scale masked face sub-set for biometrics assessment under COVID-19. For a comprehensive evaluation of face matchers, three recognition tasks are performed under standard, masked and unbiased settings, respectively. Equipped with this benchmark, we delve into million-scale face recognition problems. A distributed framework is developed to train face recognition models efficiently without tampering with the performance. Enabled by WebFace42M, we reduce 40% failure rate on the challenging IJB-C set and rank 3rd among 430 entries on NIST-FRVT. Even 10% data (WebFace4M) shows superior performance compared with the public training sets. Furthermore, comprehensive baselines are established under the FRUITS-100/500/1000 milliseconds protocols. The proposed benchmark shows enormous potential on standard, masked and unbiased face recognition scenarios. Our WebFace260M website is https://www.face-benchmark.org.



### A Multi-Person Video Dataset Annotation Method of Spatio-Temporally Actions
- **Arxiv ID**: http://arxiv.org/abs/2204.10160v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10160v2)
- **Published**: 2022-04-21 15:14:02+00:00
- **Updated**: 2022-04-25 23:37:46+00:00
- **Authors**: Fan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Spatio-temporal action detection is an important and challenging problem in video understanding. However, the application of the existing large-scale spatio-temporal action datasets in specific fields is limited, and there is currently no public tool for making spatio-temporal action datasets, it takes a lot of time and effort for researchers to customize the spatio-temporal action datasets, so we propose a multi-Person video dataset Annotation Method of spatio-temporally actions.First, we use ffmpeg to crop the videos and frame the videos; then use yolov5 to detect human in the video frame, and then use deep sort to detect the ID of the human in the video frame. By processing the detection results of yolov5 and deep sort, we can get the annotation file of the spatio-temporal action dataset to complete the work of customizing the spatio-temporal action dataset. https://github.com/Whiffe/Custom-ava-dataset_Custom-Spatio-Temporally-Action-Video-Dataset



### Automated analysis of fibrous cap in intravascular optical coherence tomography images of coronary arteries
- **Arxiv ID**: http://arxiv.org/abs/2204.10162v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.10162v2)
- **Published**: 2022-04-21 15:15:23+00:00
- **Updated**: 2022-12-12 15:00:11+00:00
- **Authors**: Juhwan Lee, Gabriel T. R. Pereira, Yazan Gharaibeh, Chaitanya Kolluru, Vladislav N. Zimin, Luis A. P. Dallan, Justin N. Kim, Ammar Hoori, Sadeer G. Al-Kindi, Giulio Guagliumi, Hiram G. Bezerra, David L. Wilson
- **Comment**: 18 pages, 9 figures
- **Journal**: None
- **Summary**: Thin-cap fibroatheroma (TCFA) and plaque rupture have been recognized as the most frequent risk factor for thrombosis and acute coronary syndrome. Intravascular optical coherence tomography (IVOCT) can identify TCFA and assess cap thickness, which provides an opportunity to assess plaque vulnerability. We developed an automated method that can detect lipidous plaque and assess fibrous cap thickness in IVOCT images. This study analyzed a total of 4,360 IVOCT image frames of 77 lesions among 41 patients. To improve segmentation performance, preprocessing included lumen segmentation, pixel-shifting, and noise filtering on the raw polar (r, theta) IVOCT images. We used the DeepLab-v3 plus deep learning model to classify lipidous plaque pixels. After lipid detection, we automatically detected the outer border of the fibrous cap using a special dynamic programming algorithm and assessed the cap thickness. Our method provided excellent discriminability of lipid plaque with a sensitivity of 85.8% and A-line Dice coefficient of 0.837. By comparing lipid angle measurements between two analysts following editing of our automated software, we found good agreement by Bland-Altman analysis (difference 6.7+/-17 degree; mean 196 degree). Our method accurately detected the fibrous cap from the detected lipid plaque. Automated analysis required a significant modification for only 5.5% frames. Furthermore, our method showed a good agreement of fibrous cap thickness between two analysts with Bland-Altman analysis (4.2+/-14.6 micron; mean 175 micron), indicating little bias between users and good reproducibility of the measurement. We developed a fully automated method for fibrous cap quantification in IVOCT images, resulting in good agreement with determinations by analysts. The method has great potential to enable highly automated, repeatable, and comprehensive evaluations of TCFAs.



### BTranspose: Bottleneck Transformers for Human Pose Estimation with Self-Supervised Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2204.10209v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.10209v1)
- **Published**: 2022-04-21 15:45:05+00:00
- **Updated**: 2022-04-21 15:45:05+00:00
- **Authors**: Kaushik Balakrishnan, Devesh Upadhyay
- **Comment**: 24 pages, 10 figures
- **Journal**: None
- **Summary**: The task of 2D human pose estimation is challenging as the number of keypoints is typically large (~ 17) and this necessitates the use of robust neural network architectures and training pipelines that can capture the relevant features from the input image. These features are then aggregated to make accurate heatmap predictions from which the final keypoints of human body parts can be inferred. Many papers in literature use CNN-based architectures for the backbone, and/or combine it with a transformer, after which the features are aggregated to make the final keypoint predictions [1]. In this paper, we consider the recently proposed Bottleneck Transformers [2], which combine CNN and multi-head self attention (MHSA) layers effectively, and we integrate it with a Transformer encoder and apply it to the task of 2D human pose estimation. We consider different backbone architectures and pre-train them using the DINO self-supervised learning method [3], this pre-training is found to improve the overall prediction accuracy. We call our model BTranspose, and experiments show that on the COCO validation set, our model achieves an AP of 76.4, which is competitive with other methods such as [1] and has fewer network parameters. Furthermore, we also present the dependencies of the final predicted keypoints on both the MHSA block and the Transformer encoder layers, providing clues on the image sub-regions the network attends to at the mid and high levels.



### SmartPortraits: Depth Powered Handheld Smartphone Dataset of Human Portraits for State Estimation, Reconstruction and Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2204.10211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10211v1)
- **Published**: 2022-04-21 15:47:38+00:00
- **Updated**: 2022-04-21 15:47:38+00:00
- **Authors**: Anastasiia Kornilova, Marsel Faizullin, Konstantin Pakulev, Andrey Sadkov, Denis Kukushkin, Azat Akhmetyanov, Timur Akhtyamov, Hekmat Taherinejad, Gonzalo Ferrer
- **Comment**: Accepted to CVPR'2022
- **Journal**: None
- **Summary**: We present a dataset of 1000 video sequences of human portraits recorded in real and uncontrolled conditions by using a handheld smartphone accompanied by an external high-quality depth camera. The collected dataset contains 200 people captured in different poses and locations and its main purpose is to bridge the gap between raw measurements obtained from a smartphone and downstream applications, such as state estimation, 3D reconstruction, view synthesis, etc. The sensors employed in data collection are the smartphone's camera and Inertial Measurement Unit (IMU), and an external Azure Kinect DK depth camera software synchronized with sub-millisecond precision to the smartphone system. During the recording, the smartphone flash is used to provide a periodic secondary source of lightning. Accurate mask of the foremost person is provided as well as its impact on the camera alignment accuracy. For evaluation purposes, we compare multiple state-of-the-art camera alignment methods by using a Motion Capture system. We provide a smartphone visual-inertial benchmark for portrait capturing, where we report results for multiple methods and motivate further use of the provided trajectories, available in the dataset, in view synthesis and 3D reconstruction tasks.



### OCTOPUS -- optical coherence tomography plaque and stent analysis software
- **Arxiv ID**: http://arxiv.org/abs/2204.10212v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.10212v1)
- **Published**: 2022-04-21 15:49:03+00:00
- **Updated**: 2022-04-21 15:49:03+00:00
- **Authors**: Juhwan Lee, Justin N. Kim, Yazan Gharaibeh, Vladislav N. Zimin, Luis A. P. Dallan, Gabriel T. R. Pereira, Armando Vergara-Martel, Chaitanya Kolluru, Ammar Hoori, Hiram G. Bezerra, David L. Wilson
- **Comment**: 16 pages, 8 figures
- **Journal**: None
- **Summary**: Compared with other imaging modalities, intravascular optical coherence tomography (IVOCT) has significant advantages for guiding percutaneous coronary interventions. To aid IVOCT research studies, we developed the Optical Coherence TOmography PlaqUe and Stent (OCTOPUS) analysis software. To automate image analysis results, the software includes several important algorithmic steps: pre-processing, deep learning plaque segmentation, machine learning identification of stent struts, and registration of pullbacks. Interactive visualization and manual editing of segmentations were included in the software. Quantifications include stent deployment characteristics (e.g., stent strut malapposition), strut level analysis, calcium angle, and calcium thickness measurements. Interactive visualizations include (x,y) anatomical, en face, and longitudinal views with optional overlays. Underlying plaque segmentation algorithm yielded excellent pixel-wise results (86.2% sensitivity and 0.781 F1 score). Using OCTOPUS on 34 new pullbacks, we determined that following automated segmentation, only 13% and 23% of frames needed any manual touch up for detailed lumen and calcification labeling, respectively. Only up to 3.8% of plaque pixels were modified, leading to an average editing time of only 7.5 seconds/frame, an approximately 80% reduction compared to manual analysis. Regarding stent analysis, sensitivity and precision were both greater than 90%, and each strut was successfully classified as either covered or uncovered with high sensitivity (94%) and specificity (90%). We introduced and evaluated the clinical application of a highly automated software package, OCTOPUS, for quantitative plaque and stent analysis in IVOCT images. The software is currently used as an offline tool for research purposes; however, the software's embedded algorithms may also be useful for real-time treatment planning.



### Enhancing Core Image Classification Using Generative Adversarial Networks (GANs)
- **Arxiv ID**: http://arxiv.org/abs/2204.14224v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.14224v2)
- **Published**: 2022-04-21 16:00:21+00:00
- **Updated**: 2023-08-25 19:30:58+00:00
- **Authors**: Galymzhan Abdimanap, Kairat Bostanbekov, Abdelrahman Abdallah, Anel Alimova, Darkhan Kurmangaliyev, Daniyar Nurseitov
- **Comment**: None
- **Journal**: None
- **Summary**: In the thrilling world of oil exploration, drill core samples are key to unlocking geological information critical to finding lucrative oil deposits. Despite the importance of these samples, traditional core logging techniques are known to be laborious and, worse still, subjective. Thankfully, the industry has embraced an innovative solution core imaging that allows for nondestructive and noninvasive rapid characterization of large quantities of drill cores. Our preeminent research paper aims to tackle the pressing problem of core detection and classification. Using state-of-the-art techniques, we present a groundbreaking solution that will transform the industry. Our first challenge is detecting the cores and segmenting the holes in images, which we will achieve using the Faster RCNN and Mask RCNN models, respectively. Then, we will address the problem of filling the hole in the core image, utilizing the powerful Generative Adversarial Networks (GANs) and employing Contextual Residual Aggregation (CRA) to create high-frequency residuals for missing contents in images. Finally, we will apply sophisticated texture recognition models for the classification of core images, revealing crucial information to oil companies in their quest to uncover valuable oil deposits. Our research paper presents an innovative and groundbreaking approach to tackling the complex issues surrounding core detection and classification. By harnessing cutting-edge techniques and technologies, we are poised to revolutionize the industry and make significant contributions to the field of oil exploration.



### Planes vs. Chairs: Category-guided 3D shape learning without any 3D cues
- **Arxiv ID**: http://arxiv.org/abs/2204.10235v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10235v1)
- **Published**: 2022-04-21 16:13:31+00:00
- **Updated**: 2022-04-21 16:13:31+00:00
- **Authors**: Zixuan Huang, Stefan Stojanov, Anh Thai, Varun Jampani, James M. Rehg
- **Comment**: Project page: https://zixuanh.com/multiclass3D
- **Journal**: None
- **Summary**: We present a novel 3D shape reconstruction method which learns to predict an implicit 3D shape representation from a single RGB image. Our approach uses a set of single-view images of multiple object categories without viewpoint annotation, forcing the model to learn across multiple object categories without 3D supervision. To facilitate learning with such minimal supervision, we use category labels to guide shape learning with a novel categorical metric learning approach. We also utilize adversarial and viewpoint regularization techniques to further disentangle the effects of viewpoint and shape. We obtain the first results for large-scale (more than 50 categories) single-viewpoint shape prediction using a single model without any 3D cues. We are also the first to examine and quantify the benefit of class information in single-view supervised 3D shape reconstruction. Our method achieves superior performance over state-of-the-art methods on ShapeNet-13, ShapeNet-55 and Pascal3D+.



### HEATGait: Hop-Extracted Adjacency Technique in Graph Convolution based Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.10238v1
- **DOI**: 10.1109/CTISC54888.2022.9849799
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10238v1)
- **Published**: 2022-04-21 16:13:58+00:00
- **Updated**: 2022-04-21 16:13:58+00:00
- **Authors**: Md. Bakhtiar Hasan, Tasnim Ahmed, Md. Hasanul Kabir
- **Comment**: Accepted in 2022 4th International Conference on Advances in Computer
  Technology, Information Science and Communications (CTISC 2022). 6 pages, 4
  figures, 2 tables
- **Journal**: None
- **Summary**: Biometric authentication using gait has become a promising field due to its unobtrusive nature. Recent approaches in model-based gait recognition techniques utilize spatio-temporal graphs for the elegant extraction of gait features. However, existing methods often rely on multi-scale operators for extracting long-range relationships among joints resulting in biased weighting. In this paper, we present HEATGait, a gait recognition system that improves the existing multi-scale graph convolution by efficient hop-extraction technique to alleviate the issue. Combined with preprocessing and augmentation techniques, we propose a powerful feature extractor that utilizes ResGCN to achieve state-of-the-art performance in model-based gait recognition on the CASIA-B gait dataset.



### An Examination of Bias of Facial Analysis based BMI Prediction Models
- **Arxiv ID**: http://arxiv.org/abs/2204.10262v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10262v1)
- **Published**: 2022-04-21 17:01:59+00:00
- **Updated**: 2022-04-21 17:01:59+00:00
- **Authors**: Hera Siddiqui, Ajita Rattani, Karl Ricanek, Twyla Hill
- **Comment**: None
- **Journal**: None
- **Summary**: Obesity is one of the most important public health problems that the world is facing today. A recent trend is in the development of intervention tools that predict BMI using facial images for weight monitoring and management to combat obesity. Most of these studies used BMI annotated facial image datasets that mainly consisted of Caucasian subjects. Research on bias evaluation of face-based gender-, age-classification, and face recognition systems suggest that these technologies perform poorly for women, dark-skinned people, and older adults. The bias of facial analysis-based BMI prediction tools has not been studied until now. This paper evaluates the bias of facial-analysis-based BMI prediction models across Caucasian and African-American Males and Females. Experimental investigations on the gender, race, and BMI balanced version of the modified MORPH-II dataset suggested that the error rate in BMI prediction was least for Black Males and highest for White Females. Further, the psychology-related facial features correlated with weight suggested that as the BMI increases, the changes in the facial region are more prominent for Black Males and the least for White Females. This is the reason for the least error rate of the facial analysis-based BMI prediction tool for Black Males and highest for White Females.



### DooDLeNet: Double DeepLab Enhanced Feature Fusion for Thermal-color Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.10266v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.10266v1)
- **Published**: 2022-04-21 17:06:57+00:00
- **Updated**: 2022-04-21 17:06:57+00:00
- **Authors**: Oriel Frigo, Lucien Martin-Gaffé, Catherine Wacongne
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: In this paper we present a new approach for feature fusion between RGB and LWIR Thermal images for the task of semantic segmentation for driving perception. We propose DooDLeNet, a double DeepLab architecture with specialized encoder-decoders for thermal and color modalities and a shared decoder for final segmentation. We combine two strategies for feature fusion: confidence weighting and correlation weighting. We report state-of-the-art mean IoU results on the MF dataset.



### Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency
- **Arxiv ID**: http://arxiv.org/abs/2204.10310v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2204.10310v3)
- **Published**: 2022-04-21 17:47:35+00:00
- **Updated**: 2022-07-25 07:57:02+00:00
- **Authors**: Tom Monnier, Matthew Fisher, Alexei A. Efros, Mathieu Aubry
- **Comment**: ECCV 2022. Project webpage with code and videos:
  http://imagine.enpc.fr/~monniert/UNICORN/
- **Journal**: None
- **Summary**: Approaches for single-view reconstruction typically rely on viewpoint annotations, silhouettes, the absence of background, multiple views of the same instance, a template shape, or symmetry. We avoid all such supervision and assumptions by explicitly leveraging the consistency between images of different object instances. As a result, our method can learn from large collections of unlabelled images depicting the same object category. Our main contributions are two ways for leveraging cross-instance consistency: (i) progressive conditioning, a training strategy to gradually specialize the model from category to instances in a curriculum learning fashion; and (ii) neighbor reconstruction, a loss enforcing consistency between instances having similar shape or texture. Also critical to the success of our method are: our structured autoencoding architecture decomposing an image into explicit shape, texture, pose, and background; an adapted formulation of differential rendering; and a new optimization scheme alternating between 3D and pose learning. We compare our approach, UNICORN, both on the diverse synthetic ShapeNet dataset - the classical benchmark for methods requiring multiple views as supervision - and on standard real-image benchmarks (Pascal3D+ Car, CUB) for which most methods require known templates and silhouette annotations. We also showcase applicability to more challenging real-world collections (CompCars, LSUN), where silhouettes are not available and images are not cropped around the object.



### Unsupervised Human Action Recognition with Skeletal Graph Laplacian and Self-Supervised Viewpoints Invariance
- **Arxiv ID**: http://arxiv.org/abs/2204.10312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10312v1)
- **Published**: 2022-04-21 17:47:42+00:00
- **Updated**: 2022-04-21 17:47:42+00:00
- **Authors**: Giancarlo Paoletti, Jacopo Cavazza, Cigdem Beyan, Alessio Del Bue
- **Comment**: None
- **Journal**: The 32nd British Machine Vision Conference (BMVC) 2021
- **Summary**: This paper presents a novel end-to-end method for the problem of skeleton-based unsupervised human action recognition. We propose a new architecture with a convolutional autoencoder that uses graph Laplacian regularization to model the skeletal geometry across the temporal dynamics of actions. Our approach is robust towards viewpoint variations by including a self-supervised gradient reverse layer that ensures generalization across camera views. The proposed method is validated on NTU-60 and NTU-120 large-scale datasets in which it outperforms all prior unsupervised skeleton-based approaches on the cross-subject, cross-view, and cross-setup protocols. Although unsupervised, our learnable representation allows our method even to surpass a few supervised skeleton-based action recognition methods. The code is available in: www.github.com/IIT-PAVIS/UHAR_Skeletal_Laplacian



### Adversarial Contrastive Learning by Permuting Cluster Assignments
- **Arxiv ID**: http://arxiv.org/abs/2204.10314v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.10314v1)
- **Published**: 2022-04-21 17:49:52+00:00
- **Updated**: 2022-04-21 17:49:52+00:00
- **Authors**: Muntasir Wahed, Afrina Tabassum, Ismini Lourentzou
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive learning has gained popularity as an effective self-supervised representation learning technique. Several research directions improve traditional contrastive approaches, e.g., prototypical contrastive methods better capture the semantic similarity among instances and reduce the computational burden by considering cluster prototypes or cluster assignments, while adversarial instance-wise contrastive methods improve robustness against a variety of attacks. To the best of our knowledge, no prior work jointly considers robustness, cluster-wise semantic similarity and computational efficiency. In this work, we propose SwARo, an adversarial contrastive framework that incorporates cluster assignment permutations to generate representative adversarial samples. We evaluate SwARo on multiple benchmark datasets and against various white-box and black-box attacks, obtaining consistent improvements over state-of-the-art baselines.



### Feature anomaly detection system (FADS) for intelligent manufacturing
- **Arxiv ID**: http://arxiv.org/abs/2204.10318v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.10318v1)
- **Published**: 2022-04-21 17:54:37+00:00
- **Updated**: 2022-04-21 17:54:37+00:00
- **Authors**: Anthony Garland, Kevin Potter, Matt Smith
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection is important for industrial automation and part quality assurance, and while humans can easily detect anomalies in components given a few examples, designing a generic automated system that can perform at human or above human capabilities remains a challenge. In this work, we present a simple new anomaly detection algorithm called FADS (feature-based anomaly detection system) which leverages pretrained convolutional neural networks (CNN) to generate a statistical model of nominal inputs by observing the activation of the convolutional filters. During inference the system compares the convolutional filter activation of the new input to the statistical model and flags activations that are outside the expected range of values and therefore likely an anomaly. By using a pretrained network, FADS demonstrates excellent performance similar to or better than other machine learning approaches to anomaly detection while at the same time FADS requires no tuning of the CNN weights. We demonstrate FADS ability by detecting process parameter changes on a custom dataset of additively manufactured lattices. The FADS localization algorithm shows that textural differences that are visible on the surface can be used to detect process parameter changes. In addition, we test FADS on benchmark datasets, such as the MVTec Anomaly Detection dataset, and report good results.



### TorchSparse: Efficient Point Cloud Inference Engine
- **Arxiv ID**: http://arxiv.org/abs/2204.10319v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/2204.10319v1)
- **Published**: 2022-04-21 17:58:30+00:00
- **Updated**: 2022-04-21 17:58:30+00:00
- **Authors**: Haotian Tang, Zhijian Liu, Xiuyu Li, Yujun Lin, Song Han
- **Comment**: MLSys 2022. The first three authors contributed equally to this work.
  Project page: https://torchsparse.mit.edu
- **Journal**: None
- **Summary**: Deep learning on point clouds has received increased attention thanks to its wide applications in AR/VR and autonomous driving. These applications require low latency and high accuracy to provide real-time user experience and ensure user safety. Unlike conventional dense workloads, the sparse and irregular nature of point clouds poses severe challenges to running sparse CNNs efficiently on the general-purpose hardware. Furthermore, existing sparse acceleration techniques for 2D images do not translate to 3D point clouds. In this paper, we introduce TorchSparse, a high-performance point cloud inference engine that accelerates the sparse convolution computation on GPUs. TorchSparse directly optimizes the two bottlenecks of sparse convolution: irregular computation and data movement. It applies adaptive matrix multiplication grouping to trade computation for better regularity, achieving 1.4-1.5x speedup for matrix multiplication. It also optimizes the data movement by adopting vectorized, quantized and fused locality-aware memory access, reducing the memory movement cost by 2.7x. Evaluated on seven representative models across three benchmark datasets, TorchSparse achieves 1.6x and 1.5x measured end-to-end speedup over the state-of-the-art MinkowskiEngine and SpConv, respectively.



### SelfD: Self-Learning Large-Scale Driving Policies From the Web
- **Arxiv ID**: http://arxiv.org/abs/2204.10320v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.10320v1)
- **Published**: 2022-04-21 17:58:36+00:00
- **Updated**: 2022-04-21 17:58:36+00:00
- **Authors**: Jimuyang Zhang, Ruizhao Zhu, Eshed Ohn-Bar
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Effectively utilizing the vast amounts of ego-centric navigation data that is freely available on the internet can advance generalized intelligent systems, i.e., to robustly scale across perspectives, platforms, environmental conditions, scenarios, and geographical locations. However, it is difficult to directly leverage such large amounts of unlabeled and highly diverse data for complex 3D reasoning and planning tasks. Consequently, researchers have primarily focused on its use for various auxiliary pixel- and image-level computer vision tasks that do not consider an ultimate navigational objective. In this work, we introduce SelfD, a framework for learning scalable driving by utilizing large amounts of online monocular images. Our key idea is to leverage iterative semi-supervised training when learning imitative agents from unlabeled data. To handle unconstrained viewpoints, scenes, and camera parameters, we train an image-based model that directly learns to plan in the Bird's Eye View (BEV) space. Next, we use unlabeled data to augment the decision-making knowledge and robustness of an initially trained model via self-training. In particular, we propose a pseudo-labeling step which enables making full use of highly diverse demonstration data through "hypothetical" planning-based data augmentation. We employ a large dataset of publicly available YouTube videos to train SelfD and comprehensively analyze its generalization benefits across challenging navigation scenarios. Without requiring any additional data collection or annotation efforts, SelfD demonstrates consistent improvements (by up to 24%) in driving performance evaluation on nuScenes, Argoverse, Waymo, and CARLA.



### Future Object Detection with Spatiotemporal Transformers
- **Arxiv ID**: http://arxiv.org/abs/2204.10321v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.10321v2)
- **Published**: 2022-04-21 17:58:36+00:00
- **Updated**: 2022-10-17 13:45:29+00:00
- **Authors**: Adam Tonderski, Joakim Johnander, Christoffer Petersson, Kalle Åström
- **Comment**: 14 pages, 6 figures
- **Journal**: None
- **Summary**: We propose the task Future Object Detection, in which the goal is to predict the bounding boxes for all visible objects in a future video frame. While this task involves recognizing temporal and kinematic patterns, in addition to the semantic and geometric ones, it only requires annotations in the standard form for individual, single (future) frames, in contrast to expensive full sequence annotations. We propose to tackle this task with an end-to-end method, in which a detection transformer is trained to directly output the future objects. In order to make accurate predictions about the future, it is necessary to capture the dynamics in the scene, both object motion and the movement of the ego-camera. To this end, we extend existing detection transformers in two ways. First, we experiment with three different mechanisms that enable the network to spatiotemporally process multiple frames. Second, we provide ego-motion information to the model in a learnable manner. We show that both of these extensions improve the future object detection performance substantially. Our final approach learns to capture the dynamics and makes predictions on par with an oracle for prediction horizons up to 100 ms, and outperforms all baselines for longer prediction horizons. By visualizing the attention maps, we observe that a form of tracking emerges within the network. Code is available at github.com/atonderski/future-object-detection.



### Interactive Segmentation and Visualization for Tiny Objects in Multi-megapixel Images
- **Arxiv ID**: http://arxiv.org/abs/2204.10356v1
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.IM, cs.HC, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2204.10356v1)
- **Published**: 2022-04-21 18:26:48+00:00
- **Updated**: 2022-04-21 18:26:48+00:00
- **Authors**: Chengyuan Xu, Boning Dong, Noah Stier, Curtis McCully, D. Andrew Howell, Pradeep Sen, Tobias Höllerer
- **Comment**: 6 pages, 4 figures. Accepted by CVPR 2022 Demo Program
- **Journal**: None
- **Summary**: We introduce an interactive image segmentation and visualization framework for identifying, inspecting, and editing tiny objects (just a few pixels wide) in large multi-megapixel high-dynamic-range (HDR) images. Detecting cosmic rays (CRs) in astronomical observations is a cumbersome workflow that requires multiple tools, so we developed an interactive toolkit that unifies model inference, HDR image visualization, segmentation mask inspection and editing into a single graphical user interface. The feature set, initially designed for astronomical data, makes this work a useful research-supporting tool for human-in-the-loop tiny-object segmentation in scientific areas like biomedicine, materials science, remote sensing, etc., as well as computer vision. Our interface features mouse-controlled, synchronized, dual-window visualization of the image and the segmentation mask, a critical feature for locating tiny objects in multi-megapixel images. The browser-based tool can be readily hosted on the web to provide multi-user access and GPU acceleration for any device. The toolkit can also be used as a high-precision annotation tool, or adapted as the frontend for an interactive machine learning framework. Our open-source dataset, CR detection model, and visualization toolkit are available at https://github.com/cy-xu/cosmic-conn.



### Contrastive Test-Time Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2204.10377v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10377v1)
- **Published**: 2022-04-21 19:17:22+00:00
- **Updated**: 2022-04-21 19:17:22+00:00
- **Authors**: Dian Chen, Dequan Wang, Trevor Darrell, Sayna Ebrahimi
- **Comment**: CVPR 2022 camera-ready version
- **Journal**: None
- **Summary**: Test-time adaptation is a special setting of unsupervised domain adaptation where a trained model on the source domain has to adapt to the target domain without accessing source data. We propose a novel way to leverage self-supervised contrastive learning to facilitate target feature learning, along with an online pseudo labeling scheme with refinement that significantly denoises pseudo labels. The contrastive learning task is applied jointly with pseudo labeling, contrasting positive and negative pairs constructed similarly as MoCo but with source-initialized encoder, and excluding same-class negative pairs indicated by pseudo labels. Meanwhile, we produce pseudo labels online and refine them via soft voting among their nearest neighbors in the target feature space, enabled by maintaining a memory queue. Our method, AdaContrast, achieves state-of-the-art performance on major benchmarks while having several desirable properties compared to existing works, including memory efficiency, insensitivity to hyper-parameters, and better model calibration. Project page: sites.google.com/view/adacontrast.



### The 6th AI City Challenge
- **Arxiv ID**: http://arxiv.org/abs/2204.10380v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10380v4)
- **Published**: 2022-04-21 19:24:17+00:00
- **Updated**: 2022-06-09 22:52:22+00:00
- **Authors**: Milind Naphade, Shuo Wang, David C. Anastasiu, Zheng Tang, Ming-Ching Chang, Yue Yao, Liang Zheng, Mohammed Shaiqur Rahman, Archana Venkatachalapathy, Anuj Sharma, Qi Feng, Vitaly Ablavsky, Stan Sclaroff, Pranamesh Chakraborty, Alice Li, Shangru Li, Rama Chellappa
- **Comment**: Summary of the 6th AI City Challenge Workshop in conjunction with
  CVPR 2022. arXiv admin note: text overlap with arXiv:2104.12233
- **Journal**: None
- **Summary**: The 6th edition of the AI City Challenge specifically focuses on problems in two domains where there is tremendous unlocked potential at the intersection of computer vision and artificial intelligence: Intelligent Traffic Systems (ITS), and brick and mortar retail businesses. The four challenge tracks of the 2022 AI City Challenge received participation requests from 254 teams across 27 countries. Track 1 addressed city-scale multi-target multi-camera (MTMC) vehicle tracking. Track 2 addressed natural-language-based vehicle track retrieval. Track 3 was a brand new track for naturalistic driving analysis, where the data were captured by several cameras mounted inside the vehicle focusing on driver safety, and the task was to classify driver actions. Track 4 was another new track aiming to achieve retail store automated checkout using only a single view camera. We released two leader boards for submissions based on different methods, including a public leader board for the contest, where no use of external data is allowed, and a general leader board for all submitted results. The top performance of participating teams established strong baselines and even outperformed the state-of-the-art in the proposed challenge tracks.



### Monocular Depth Estimation Using Cues Inspired by Biological Vision Systems
- **Arxiv ID**: http://arxiv.org/abs/2204.10384v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10384v2)
- **Published**: 2022-04-21 19:42:36+00:00
- **Updated**: 2022-05-12 13:56:32+00:00
- **Authors**: Dylan Auty, Krystian Mikolajczyk
- **Comment**: 7 pages, 2 figures. Accepted to International Conference on Pattern
  Recognition (ICPR) 2022. Code available at
  https://github.com/DylanAuty/MDE-biological-vision-systems
- **Journal**: None
- **Summary**: Monocular depth estimation (MDE) aims to transform an RGB image of a scene into a pixelwise depth map from the same camera view. It is fundamentally ill-posed due to missing information: any single image can have been taken from many possible 3D scenes. Part of the MDE task is, therefore, to learn which visual cues in the image can be used for depth estimation, and how. With training data limited by cost of annotation or network capacity limited by computational power, this is challenging. In this work we demonstrate that explicitly injecting visual cue information into the model is beneficial for depth estimation. Following research into biological vision systems, we focus on semantic information and prior knowledge of object sizes and their relations, to emulate the biological cues of relative size, familiar size, and absolute size. We use state-of-the-art semantic and instance segmentation models to provide external information, and exploit language embeddings to encode relational information between classes. We also provide a prior on the average real-world size of objects. This external information overcomes the limitation in data availability, and ensures that the limited capacity of a given network is focused on known-helpful cues, therefore improving performance. We experimentally validate our hypothesis and evaluate the proposed model on the widely used NYUD2 indoor depth estimation benchmark. The results show improvements in depth prediction when the semantic information, size prior and instance size are explicitly provided along with the RGB images, and our method can be easily adapted to any depth estimation system.



### Learning Sequential Latent Variable Models from Multimodal Time Series Data
- **Arxiv ID**: http://arxiv.org/abs/2204.10419v2
- **DOI**: 10.1007/978-3-031-22216-0_35
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.10419v2)
- **Published**: 2022-04-21 21:59:24+00:00
- **Updated**: 2023-01-20 07:11:44+00:00
- **Authors**: Oliver Limoyo, Trevor Ablett, Jonathan Kelly
- **Comment**: In: Petrovic, I., Menegatti, E., Markovi\'c, I. (eds) Intelligent
  Autonomous Systems 17. IAS 2022. Lecture Notes in Networks and Systems, vol
  577. Springer, Cham
- **Journal**: None
- **Summary**: Sequential modelling of high-dimensional data is an important problem that appears in many domains including model-based reinforcement learning and dynamics identification for control. Latent variable models applied to sequential data (i.e., latent dynamics models) have been shown to be a particularly effective probabilistic approach to solve this problem, especially when dealing with images. However, in many application areas (e.g., robotics), information from multiple sensing modalities is available -- existing latent dynamics methods have not yet been extended to effectively make use of such multimodal sequential data. Multimodal sensor streams can be correlated in a useful manner and often contain complementary information across modalities. In this work, we present a self-supervised generative modelling framework to jointly learn a probabilistic latent state representation of multimodal data and the respective dynamics. Using synthetic and real-world datasets from a multimodal robotic planar pushing task, we demonstrate that our approach leads to significant improvements in prediction and representation quality. Furthermore, we compare to the common learning baseline of concatenating each modality in the latent space and show that our principled probabilistic formulation performs better. Finally, despite being fully self-supervised, we demonstrate that our method is nearly as effective as an existing supervised approach that relies on ground truth labels.



### PreTraM: Self-Supervised Pre-training via Connecting Trajectory and Map
- **Arxiv ID**: http://arxiv.org/abs/2204.10435v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.10435v1)
- **Published**: 2022-04-21 23:01:21+00:00
- **Updated**: 2022-04-21 23:01:21+00:00
- **Authors**: Chenfeng Xu, Tian Li, Chen Tang, Lingfeng Sun, Kurt Keutzer, Masayoshi Tomizuka, Alireza Fathi, Wei Zhan
- **Comment**: The code is available at https://github.com/chenfengxu714/PreTraM.git
- **Journal**: None
- **Summary**: Deep learning has recently achieved significant progress in trajectory forecasting. However, the scarcity of trajectory data inhibits the data-hungry deep-learning models from learning good representations. While mature representation learning methods exist in computer vision and natural language processing, these pre-training methods require large-scale data. It is hard to replicate these approaches in trajectory forecasting due to the lack of adequate trajectory data (e.g., 34K samples in the nuScenes dataset). To work around the scarcity of trajectory data, we resort to another data modality closely related to trajectories-HD-maps, which is abundantly provided in existing datasets. In this paper, we propose PreTraM, a self-supervised pre-training scheme via connecting trajectories and maps for trajectory forecasting. Specifically, PreTraM consists of two parts: 1) Trajectory-Map Contrastive Learning, where we project trajectories and maps to a shared embedding space with cross-modal contrastive learning, and 2) Map Contrastive Learning, where we enhance map representation with contrastive learning on large quantities of HD-maps. On top of popular baselines such as AgentFormer and Trajectron++, PreTraM boosts their performance by 5.5% and 6.9% relatively in FDE-10 on the challenging nuScenes dataset. We show that PreTraM improves data efficiency and scales well with model size.



### Scale-Equivariant Unrolled Neural Networks for Data-Efficient Accelerated MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2204.10436v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.10436v1)
- **Published**: 2022-04-21 23:29:52+00:00
- **Updated**: 2022-04-21 23:29:52+00:00
- **Authors**: Beliz Gunel, Arda Sahiner, Arjun D. Desai, Akshay S. Chaudhari, Shreyas Vasanawala, Mert Pilanci, John Pauly
- **Comment**: None
- **Journal**: None
- **Summary**: Unrolled neural networks have enabled state-of-the-art reconstruction performance and fast inference times for the accelerated magnetic resonance imaging (MRI) reconstruction task. However, these approaches depend on fully-sampled scans as ground truth data which is either costly or not possible to acquire in many clinical medical imaging applications; hence, reducing dependence on data is desirable. In this work, we propose modeling the proximal operators of unrolled neural networks with scale-equivariant convolutional neural networks in order to improve the data-efficiency and robustness to drifts in scale of the images that might stem from the variability of patient anatomies or change in field-of-view across different MRI scanners. Our approach demonstrates strong improvements over the state-of-the-art unrolled neural networks under the same memory constraints both with and without data augmentations on both in-distribution and out-of-distribution scaled images without significantly increasing the train or inference time.



### DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2204.10437v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.10437v1)
- **Published**: 2022-04-21 23:52:52+00:00
- **Updated**: 2022-04-21 23:52:52+00:00
- **Authors**: Fatemeh Haghighi, Mohammad Reza Hosseinzadeh Taher, Michael B. Gotway, Jianming Liang
- **Comment**: Accepted at CVPR 2022 [main conference]
- **Journal**: None
- **Summary**: Discriminative learning, restorative learning, and adversarial learning have proven beneficial for self-supervised learning schemes in computer vision and medical imaging. Existing efforts, however, omit their synergistic effects on each other in a ternary setup, which, we envision, can significantly benefit deep semantic representation learning. To realize this vision, we have developed DiRA, the first framework that unites discriminative, restorative, and adversarial learning in a unified manner to collaboratively glean complementary visual information from unlabeled medical images for fine-grained semantic representation learning. Our extensive experiments demonstrate that DiRA (1) encourages collaborative learning among three learning ingredients, resulting in more generalizable representation across organs, diseases, and modalities; (2) outperforms fully supervised ImageNet models and increases robustness in small data regimes, reducing annotation cost across multiple medical imaging applications; (3) learns fine-grained semantic representation, facilitating accurate lesion localization with only image-level annotation; and (4) enhances state-of-the-art restorative approaches, revealing that DiRA is a general mechanism for united representation learning. All code and pre-trained models are available at https: //github.com/JLiangLab/DiRA.



