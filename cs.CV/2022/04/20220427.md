# Arxiv Papers in cs.CV on 2022-04-27
### SCGC : Self-Supervised Contrastive Graph Clustering
- **Arxiv ID**: http://arxiv.org/abs/2204.12656v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.12656v1)
- **Published**: 2022-04-27 01:38:46+00:00
- **Updated**: 2022-04-27 01:38:46+00:00
- **Authors**: Gayan K. Kulatilleke, Marius Portmann, Shekhar S. Chandra
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Graph clustering discovers groups or communities within networks. Deep learning methods such as autoencoders (AE) extract effective clustering and downstream representations but cannot incorporate rich structural information. While Graph Neural Networks (GNN) have shown great success in encoding graph structure, typical GNNs based on convolution or attention variants suffer from over-smoothing, noise, heterophily, are computationally expensive and typically require the complete graph being present. Instead, we propose Self-Supervised Contrastive Graph Clustering (SCGC), which imposes graph-structure via contrastive loss signals to learn discriminative node representations and iteratively refined soft cluster labels. We also propose SCGC*, with a more effective, novel, Influence Augmented Contrastive (IAC) loss to fuse richer structural information, and half the original model parameters. SCGC(*) is faster with simple linear units, completely eliminate convolutions and attention of traditional GNNs, yet efficiently incorporates structure. It is impervious to layer depth and robust to over-smoothing, incorrect edges and heterophily. It is scalable by batching, a limitation in many prior GNN models, and trivially parallelizable. We obtain significant improvements over state-of-the-art on a wide range of benchmark graph datasets, including images, sensor data, text, and citation networks efficiently. Specifically, 20% on ARI and 18% on NMI for DBLP; overall 55% reduction in training time and overall, 81% reduction on inference time. Our code is available at : https://github.com/gayanku/SCGC



### MM-TTA: Multi-Modal Test-Time Adaptation for 3D Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.12667v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12667v1)
- **Published**: 2022-04-27 02:28:12+00:00
- **Updated**: 2022-04-27 02:28:12+00:00
- **Authors**: Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, Kuk-Jin Yoon
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Test-time adaptation approaches have recently emerged as a practical solution for handling domain shift without access to the source domain data. In this paper, we propose and explore a new multi-modal extension of test-time adaptation for 3D semantic segmentation. We find that directly applying existing methods usually results in performance instability at test time because multi-modal input is not considered jointly. To design a framework that can take full advantage of multi-modality, where each modality provides regularized self-supervisory signals to other modalities, we propose two complementary modules within and across the modalities. First, Intra-modal Pseudolabel Generation (Intra-PG) is introduced to obtain reliable pseudo labels within each modality by aggregating information from two models that are both pre-trained on source data but updated with target data at different paces. Second, Inter-modal Pseudo-label Refinement (Inter-PR) adaptively selects more reliable pseudo labels from different modalities based on a proposed consistency scheme. Experiments demonstrate that our regularized pseudo labels produce stable self-learning signals in numerous multi-modal test-time adaptation scenarios for 3D semantic segmentation. Visit our project website at https://www.nec-labs.com/~mas/MM-TTA.



### Optimized latent-code selection for explainable conditional text-to-image GANs
- **Arxiv ID**: http://arxiv.org/abs/2204.12678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12678v1)
- **Published**: 2022-04-27 03:12:55+00:00
- **Updated**: 2022-04-27 03:12:55+00:00
- **Authors**: Zhenxing Zhang, Lambert Schomaker
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2202.12929
- **Journal**: None
- **Summary**: The task of text-to-image generation has achieved remarkable progress due to the advances in the conditional generative adversarial networks (GANs). However, existing conditional text-to-image GANs approaches mostly concentrate on improving both image quality and semantic relevance but ignore the explainability of the model which plays a vital role in real-world applications. In this paper, we present a variety of techniques to take a deep look into the latent space and semantic space of the conditional text-to-image GANs model. We introduce pairwise linear interpolation of latent codes and `linguistic' linear interpolation to study what the model has learned within the latent space and `linguistic' embeddings. Subsequently, we extend linear interpolation to triangular interpolation conditioned on three corners to further analyze the model. After that, we build a Good/Bad data set containing unsuccessfully and successfully synthetic samples and corresponding latent codes for the image-quality research. Based on this data set, we propose a framework for finding good latent codes by utilizing a linear SVM. Experimental results on the recent DiverGAN generator trained on two benchmark data sets qualitatively prove the effectiveness of our presented techniques, with a better than 94\% accuracy in predicting ${Good}$/${Bad}$ classes for latent vectors. The Good/Bad data set is publicly available at https://zenodo.org/record/5850224#.YeGMwP7MKUk.



### Improving the Transferability of Adversarial Examples with Restructure Embedded Patches
- **Arxiv ID**: http://arxiv.org/abs/2204.12680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12680v1)
- **Published**: 2022-04-27 03:22:55+00:00
- **Updated**: 2022-04-27 03:22:55+00:00
- **Authors**: Huipeng Zhou, Yu-an Tan, Yajie Wang, Haoran Lyu, Shangbo Wu, Yuanzhang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Vision transformers (ViTs) have demonstrated impressive performance in various computer vision tasks. However, the adversarial examples generated by ViTs are challenging to transfer to other networks with different structures. Recent attack methods do not consider the specificity of ViTs architecture and self-attention mechanism, which leads to poor transferability of the generated adversarial samples by ViTs. We attack the unique self-attention mechanism in ViTs by restructuring the embedded patches of the input. The restructured embedded patches enable the self-attention mechanism to obtain more diverse patches connections and help ViTs keep regions of interest on the object. Therefore, we propose an attack method against the unique self-attention mechanism in ViTs, called Self-Attention Patches Restructure (SAPR). Our method is simple to implement yet efficient and applicable to any self-attention based network and gradient transferability-based attack methods. We evaluate attack transferability on black-box models with different structures. The result show that our method generates adversarial examples on white-box ViTs with higher transferability and higher image quality. Our research advances the development of black-box transfer attacks on ViTs and demonstrates the feasibility of using white-box ViTs to attack other black-box models.



### Density-preserving Deep Point Cloud Compression
- **Arxiv ID**: http://arxiv.org/abs/2204.12684v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12684v1)
- **Published**: 2022-04-27 03:42:15+00:00
- **Updated**: 2022-04-27 03:42:15+00:00
- **Authors**: Yun He, Xinlin Ren, Danhang Tang, Yinda Zhang, Xiangyang Xue, Yanwei Fu
- **Comment**: Accepted by CVPR 2022. Project page is available at
  https://yunhe20.github.io/D-PCC
- **Journal**: None
- **Summary**: Local density of point clouds is crucial for representing local details, but has been overlooked by existing point cloud compression methods. To address this, we propose a novel deep point cloud compression method that preserves local density information. Our method works in an auto-encoder fashion: the encoder downsamples the points and learns point-wise features, while the decoder upsamples the points using these features. Specifically, we propose to encode local geometry and density with three embeddings: density embedding, local position embedding and ancestor embedding. During the decoding, we explicitly predict the upsampling factor for each point, and the directions and scales of the upsampled points. To mitigate the clustered points issue in existing methods, we design a novel sub-point convolution layer, and an upsampling block with adaptive scale. Furthermore, our method can also compress point-wise attributes, such as normal. Extensive qualitative and quantitative results on SemanticKITTI and ShapeNet demonstrate that our method achieves the state-of-the-art rate-distortion trade-off.



### Robust Face Anti-Spoofing with Dual Probabilistic Modeling
- **Arxiv ID**: http://arxiv.org/abs/2204.12685v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12685v1)
- **Published**: 2022-04-27 03:44:18+00:00
- **Updated**: 2022-04-27 03:44:18+00:00
- **Authors**: Yuanhan Zhang, Yichao Wu, Zhenfei Yin, Jing Shao, Ziwei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The field of face anti-spoofing (FAS) has witnessed great progress with the surge of deep learning. Due to its data-driven nature, existing FAS methods are sensitive to the noise in the dataset, which will hurdle the learning process. However, very few works consider noise modeling in FAS. In this work, we attempt to fill this gap by automatically addressing the noise problem from both label and data perspectives in a probabilistic manner. Specifically, we propose a unified framework called Dual Probabilistic Modeling (DPM), with two dedicated modules, DPM-LQ (Label Quality aware learning) and DPM-DQ (Data Quality aware learning). Both modules are designed based on the assumption that data and label should form coherent probabilistic distributions. DPM-LQ is able to produce robust feature representations without overfitting to the distribution of noisy semantic labels. DPM-DQ can eliminate data noise from `False Reject' and `False Accept' during inference by correcting the prediction confidence of noisy data based on its quality distribution. Both modules can be incorporated into existing deep networks seamlessly and efficiently. Furthermore, we propose the generalized DPM to address the noise problem in practical usage without the need of semantic annotations. Extensive experiments demonstrate that this probabilistic modeling can 1) significantly improve the accuracy, and 2) make the model robust to the noise in real-world datasets. Without bells and whistles, our proposed DPM achieves state-of-the-art performance on multiple standard FAS benchmarks.



### Grasping the Arrow of Time from the Singularity: Decoding Micromotion in Low-dimensional Latent Spaces from StyleGAN
- **Arxiv ID**: http://arxiv.org/abs/2204.12696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12696v1)
- **Published**: 2022-04-27 04:38:39+00:00
- **Updated**: 2022-04-27 04:38:39+00:00
- **Authors**: Qiucheng Wu, Yifan Jiang, Junru Wu, Kai Wang, Gong Zhang, Humphrey Shi, Zhangyang Wang, Shiyu Chang
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: The disentanglement of StyleGAN latent space has paved the way for realistic and controllable image editing, but does StyleGAN know anything about temporal motion, as it was only trained on static images? To study the motion features in the latent space of StyleGAN, in this paper, we hypothesize and demonstrate that a series of meaningful, natural, and versatile small, local movements (referred to as "micromotion", such as expression, head movement, and aging effect) can be represented in low-rank spaces extracted from the latent space of a conventionally pre-trained StyleGAN-v2 model for face generation, with the guidance of proper "anchors" in the form of either short text or video clips. Starting from one target face image, with the editing direction decoded from the low-rank space, its micromotion features can be represented as simple as an affine transformation over its latent feature. Perhaps more surprisingly, such micromotion subspace, even learned from just single target face, can be painlessly transferred to other unseen face images, even those from vastly different domains (such as oil painting, cartoon, and sculpture faces). It demonstrates that the local feature geometry corresponding to one type of micromotion is aligned across different face subjects, and hence that StyleGAN-v2 is indeed "secretly" aware of the subject-disentangled feature variations caused by that micromotion. We present various successful examples of applying our low-dimensional micromotion subspace technique to directly and effortlessly manipulate faces, showing high robustness, low computational overhead, and impressive domain transferability. Our codes are available at https://github.com/wuqiuche/micromotion-StyleGAN.



### Mapping suburban bicycle lanes using street scene images and deep learning
- **Arxiv ID**: http://arxiv.org/abs/2204.12701v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12701v1)
- **Published**: 2022-04-27 04:56:26+00:00
- **Updated**: 2022-04-27 04:56:26+00:00
- **Authors**: Tyler Saxton
- **Comment**: 77 pages, 24 figures. A minor thesis submitted in partial fulfilment
  of the requirements for the degree of Master of Data Science
- **Journal**: None
- **Summary**: On-road bicycle lanes improve safety for cyclists, and encourage participation in cycling for active transport and recreation. With many local authorities responsible for portions of the infrastructure, official maps and datasets of bicycle lanes may be out-of-date and incomplete. Even "crowdsourced" databases may have significant gaps, especially outside popular metropolitan areas. This thesis presents a method to create a map of bicycle lanes in a survey area by taking sample street scene images from each road, and then applying a deep learning model that has been trained to recognise bicycle lane symbols. The list of coordinates where bicycle lane markings are detected is then correlated to geospatial data about the road network to record bicycle lane routes. The method was applied to successfully build a map for a survey area in the outer suburbs of Melbourne. It was able to identify bicycle lanes not previously recorded in the official state government dataset, OpenStreetMap, or the "biking" layer of Google Maps.



### Dataset for Robust and Accurate Leading Vehicle Velocity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.12717v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12717v1)
- **Published**: 2022-04-27 06:06:54+00:00
- **Updated**: 2022-04-27 06:06:54+00:00
- **Authors**: Genya Ogawa, Toru Saito, Noriyuki Aoi
- **Comment**: 5 pages, 9 figures
- **Journal**: None
- **Summary**: Recognition of the surrounding environment using a camera is an important technology in Advanced Driver-Assistance Systems and Autonomous Driving, and recognition technology is often solved by machine learning approaches such as deep learning in recent years. Machine learning requires datasets for learning and evaluation. To develop robust recognition technology in the real world, in addition to normal driving environment, data in environments that are difficult for cameras such as rainy weather or nighttime are essential. We have constructed a dataset that one can benchmark the technology, targeting the velocity recognition of the leading vehicle. This task is an important one for the Advanced Driver-Assistance Systems and Autonomous Driving. The dataset is available at https://signate.jp/competitions/657



### PRE-NAS: Predictor-assisted Evolutionary Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2204.12726v1
- **DOI**: 10.1145/3512290.3528727
- **Categories**: **cs.CV**, I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2204.12726v1)
- **Published**: 2022-04-27 06:40:39+00:00
- **Updated**: 2022-04-27 06:40:39+00:00
- **Authors**: Yameng Peng, Andy Song, Vic Ciesielski, Haytham M. Fayek, Xiaojun Chang
- **Comment**: Accepted by GECCO 2022
- **Journal**: None
- **Summary**: Neural architecture search (NAS) aims to automate architecture engineering in neural networks. This often requires a high computational overhead to evaluate a number of candidate networks from the set of all possible networks in the search space during the search. Prediction of the networks' performance can alleviate this high computational overhead by mitigating the need for evaluating every candidate network. Developing such a predictor typically requires a large number of evaluated architectures which may be difficult to obtain. We address this challenge by proposing a novel evolutionary-based NAS strategy, Predictor-assisted E-NAS (PRE-NAS), which can perform well even with an extremely small number of evaluated architectures. PRE-NAS leverages new evolutionary search strategies and integrates high-fidelity weight inheritance over generations. Unlike one-shot strategies, which may suffer from bias in the evaluation due to weight sharing, offspring candidates in PRE-NAS are topologically homogeneous, which circumvents bias and leads to more accurate predictions. Extensive experiments on NAS-Bench-201 and DARTS search spaces show that PRE-NAS can outperform state-of-the-art NAS methods. With only a single GPU searching for 0.6 days, competitive architecture can be found by PRE-NAS which achieves 2.40% and 24% test error rates on CIFAR-10 and ImageNet respectively.



### Human-Centered Prior-Guided and Task-Dependent Multi-Task Representation Learning for Action Recognition Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2204.12729v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.12729v1)
- **Published**: 2022-04-27 06:51:31+00:00
- **Updated**: 2022-04-27 06:51:31+00:00
- **Authors**: Guanhong Wang, Keyu Lu, Yang Zhou, Zhanhao He, Gaoang Wang
- **Comment**: This paper has been accepted by ICME 2022
- **Journal**: None
- **Summary**: Recently, much progress has been made for self-supervised action recognition. Most existing approaches emphasize the contrastive relations among videos, including appearance and motion consistency. However, two main issues remain for existing pre-training methods: 1) the learned representation is neutral and not informative for a specific task; 2) multi-task learning-based pre-training sometimes leads to sub-optimal solutions due to inconsistent domains of different tasks. To address the above issues, we propose a novel action recognition pre-training framework, which exploits human-centered prior knowledge that generates more informative representation, and avoids the conflict between multiple tasks by using task-dependent representations. Specifically, we distill knowledge from a human parsing model to enrich the semantic capability of representation. In addition, we combine knowledge distillation with contrastive learning to constitute a task-dependent multi-task framework. We achieve state-of-the-art performance on two popular benchmarks for action recognition task, i.e., UCF101 and HMDB51, verifying the effectiveness of our method.



### A Multi-Head Convolutional Neural Network With Multi-path Attention improves Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2204.12736v2
- **DOI**: 10.1007/978-3-031-20868-3_25
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12736v2)
- **Published**: 2022-04-27 07:11:41+00:00
- **Updated**: 2022-11-04 02:21:48+00:00
- **Authors**: Jiahong Zhang, Meijun Qu, Ye Wang, Lihong Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, convolutional neural networks (CNNs) and attention mechanisms have been widely used in image denoising and achieved satisfactory performance. However, the previous works mostly use a single head to receive the noisy image, limiting the richness of extracted features. Therefore, a novel CNN with multiple heads (MH) named MHCNN is proposed in this paper, whose heads will receive the input images rotated by different rotation angles. MH makes MHCNN simultaneously utilize features of rotated images to remove noise. To integrate these features effectively, we present a novel multi-path attention mechanism (MPA). Unlike previous attention mechanisms that handle pixel-level, channel-level, or patch-level features, MPA focuses on features at the image level. Experiments show MHCNN surpasses other state-of-the-art CNN models on additive white Gaussian noise (AWGN) denoising and real-world image denoising. Its peak signal-to-noise ratio (PSNR) results are higher than other networks, such as BRDNet, RIDNet, PAN-Net, and CSANN. The code is accessible at https://github.com/JiaHongZ/MHCNN.



### Self-Supervised Text Erasing with Controllable Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2204.12743v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.12743v1)
- **Published**: 2022-04-27 07:21:55+00:00
- **Updated**: 2022-04-27 07:21:55+00:00
- **Authors**: Gangwei Jiang, Shiyao Wang, Tiezheng Ge, Yuning Jiang, Ying Wei, Defu Lian
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Recent efforts on scene text erasing have shown promising results. However, existing methods require rich yet costly label annotations to obtain robust models, which limits the use for practical applications. To this end, we study an unsupervised scenario by proposing a novel Self-supervised Text Erasing (STE) framework that jointly learns to synthesize training images with erasure ground-truth and accurately erase texts in the real world. We first design a style-aware image synthesis function to generate synthetic images with diverse styled texts based on two synthetic mechanisms. To bridge the text style gap between the synthetic and real-world data, a policy network is constructed to control the synthetic mechanisms by picking style parameters with the guidance of two specifically designed rewards. The synthetic training images with erasure ground-truth are then fed to train a coarse-to-fine erasing network. To produce better erasing outputs, a triplet erasure loss is designed to enforce the refinement stage to recover background textures. Moreover, we provide a new dataset (called PosterErase), which contains 60K high-resolution posters with texts and is more challenging for the text erasing task. The proposed method has been extensively evaluated with both PosterErase and the widely-used SCUT-Enstext dataset. Notably, on PosterErase, our unsupervised method achieves 5.07 in terms of FID, with a relative performance of 20.9% over existing supervised baselines.



### Self-Driving Car Steering Angle Prediction: Let Transformer Be a Car Again
- **Arxiv ID**: http://arxiv.org/abs/2204.12748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12748v1)
- **Published**: 2022-04-27 07:42:42+00:00
- **Updated**: 2022-04-27 07:42:42+00:00
- **Authors**: Chingis Oinar, Eunmin Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Self-driving vehicles are expected to be a massive economic influence over the coming decades. Udacity https://www.udacity.com/ has been working on a completely open-source self driving car. Thus, it regularly organizes various competitions, one of which was dedicated to steering angle prediction task. In this work, we perform an extensive study on this particular task by exploring the Udacity Self-driving Car Challenge 2. We provide insights on the previous teams' solutions. Moreover, we propose our new architecture that is inspired by some of the teams. We report our performance and compare it with multiple baseline architectures as well as other teams' solutions. We make our work available on GitHub and hope it is useful for the Udacity community and brings insights for future works https://github.com/chingisooinar/AI_self-driving-car



### Talking Head Generation Driven by Speech-Related Facial Action Units and Audio- Based on Multimodal Representation Fusion
- **Arxiv ID**: http://arxiv.org/abs/2204.12756v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.12756v1)
- **Published**: 2022-04-27 08:05:24+00:00
- **Updated**: 2022-04-27 08:05:24+00:00
- **Authors**: Sen Chen, Zhilei Liu, Jiaxing Liu, Longbiao Wang
- **Comment**: arXiv admin note: text overlap with arXiv:2110.09951
- **Journal**: None
- **Summary**: Talking head generation is to synthesize a lip-synchronized talking head video by inputting an arbitrary face image and corresponding audio clips. Existing methods ignore not only the interaction and relationship of cross-modal information, but also the local driving information of the mouth muscles. In this study, we propose a novel generative framework that contains a dilated non-causal temporal convolutional self-attention network as a multimodal fusion module to promote the relationship learning of cross-modal features. In addition, our proposed method uses both audio- and speech-related facial action units (AUs) as driving information. Speech-related AU information can guide mouth movements more accurately. Because speech is highly correlated with speech-related AUs, we propose an audio-to-AU module to predict speech-related AU information. We utilize pre-trained AU classifier to ensure that the generated images contain correct AU information. We verify the effectiveness of the proposed model on the GRID and TCD-TIMIT datasets. An ablation study is also conducted to verify the contribution of each component. The results of quantitative and qualitative experiments demonstrate that our method outperforms existing methods in terms of both image quality and lip-sync accuracy.



### A Scalable Combinatorial Solver for Elastic Geometrically Consistent 3D Shape Matching
- **Arxiv ID**: http://arxiv.org/abs/2204.12805v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2204.12805v1)
- **Published**: 2022-04-27 09:47:47+00:00
- **Updated**: 2022-04-27 09:47:47+00:00
- **Authors**: Paul Roetzer, Paul Swoboda, Daniel Cremers, Florian Bernard
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: We present a scalable combinatorial algorithm for globally optimizing over the space of geometrically consistent mappings between 3D shapes. We use the mathematically elegant formalism proposed by Windheuser et al. (ICCV 2011) where 3D shape matching was formulated as an integer linear program over the space of orientation-preserving diffeomorphisms. Until now, the resulting formulation had limited practical applicability due to its complicated constraint structure and its large size. We propose a novel primal heuristic coupled with a Lagrange dual problem that is several orders of magnitudes faster compared to previous solvers. This allows us to handle shapes with substantially more triangles than previously solvable. We demonstrate compelling results on diverse datasets, and, even showcase that we can address the challenging setting of matching two partial shapes without availability of complete shapes. Our code is publicly available at http://github.com/paul0noah/sm-comb .



### The MeVer DeepFake Detection Service: Lessons Learnt from Developing and Deploying in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2204.12816v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.12816v1)
- **Published**: 2022-04-27 10:20:44+00:00
- **Updated**: 2022-04-27 10:20:44+00:00
- **Authors**: Spyridon Baxevanakis, Giorgos Kordopatis-Zilos, Panagiotis Galopoulos, Lazaros Apostolidis, Killian Levacher, Ipek B. Schlicht, Denis Teyssou, Ioannis Kompatsiaris, Symeon Papadopoulos
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Enabled by recent improvements in generation methodologies, DeepFakes have become mainstream due to their increasingly better visual quality, the increase in easy-to-use generation tools and the rapid dissemination through social media. This fact poses a severe threat to our societies with the potential to erode social cohesion and influence our democracies. To mitigate the threat, numerous DeepFake detection schemes have been introduced in the literature but very few provide a web service that can be used in the wild. In this paper, we introduce the MeVer DeepFake detection service, a web service detecting deep learning manipulations in images and video. We present the design and implementation of the proposed processing pipeline that involves a model ensemble scheme, and we endow the service with a model card for transparency. Experimental results show that our service performs robustly on the three benchmark datasets while being vulnerable to Adversarial Attacks. Finally, we outline our experience and lessons learned when deploying a research system into production in the hopes that it will be useful to other academic and industry teams.



### CATrans: Context and Affinity Transformer for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.12817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12817v1)
- **Published**: 2022-04-27 10:20:47+00:00
- **Updated**: 2022-04-27 10:20:47+00:00
- **Authors**: Shan Zhang, Tianyi Wu, Sitong Wu, Guodong Guo
- **Comment**: Accepted by IJCAI 2022
- **Journal**: None
- **Summary**: Few-shot segmentation (FSS) aims to segment novel categories given scarce annotated support images. The crux of FSS is how to aggregate dense correlations between support and query images for query segmentation while being robust to the large variations in appearance and context. To this end, previous Transformer-based methods explore global consensus either on context similarity or affinity map between support-query pairs. In this work, we effectively integrate the context and affinity information via the proposed novel Context and Affinity Transformer (CATrans) in a hierarchical architecture. Specifically, the Relation-guided Context Transformer (RCT) propagates context information from support to query images conditioned on more informative support features. Based on the observation that a huge feature distinction between support and query pairs brings barriers for context knowledge transfer, the Relation-guided Affinity Transformer (RAT) measures attention-aware affinity as auxiliary information for FSS, in which the self-affinity is responsible for more reliable cross-affinity. We conduct experiments to demonstrate the effectiveness of the proposed model, outperforming the state-of-the-art methods.



### Conformer and Blind Noisy Students for Improved Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2204.12819v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12819v1)
- **Published**: 2022-04-27 10:21:08+00:00
- **Updated**: 2022-04-27 10:21:08+00:00
- **Authors**: Marcos V. Conde, Maxime Burchi, Radu Timofte
- **Comment**: CVPR NTIRE 2022
- **Journal**: None
- **Summary**: Generative models for image restoration, enhancement, and generation have significantly improved the quality of the generated images. Surprisingly, these models produce more pleasant images to the human eye than other methods, yet, they may get a lower perceptual quality score using traditional perceptual quality metrics such as PSNR or SSIM. Therefore, it is necessary to develop a quantitative metric to reflect the performance of new algorithms, which should be well-aligned with the person's mean opinion score (MOS). Learning-based approaches for perceptual image quality assessment (IQA) usually require both the distorted and reference image for measuring the perceptual quality accurately. However, commonly only the distorted or generated image is available. In this work, we explore the performance of transformer-based full-reference IQA models. We also propose a method for IQA based on semi-supervised knowledge distillation from full-reference teacher models into blind student models using noisy pseudo-labeled data. Our approaches achieved competitive results on the NTIRE 2022 Perceptual Image Quality Assessment Challenge: our full-reference model was ranked 4th, and our blind noisy student was ranked 3rd among 70 participants, each in their respective track.



### Power Bundle Adjustment for Large-Scale 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2204.12834v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12834v4)
- **Published**: 2022-04-27 10:38:33+00:00
- **Updated**: 2023-04-17 13:52:06+00:00
- **Authors**: Simon Weber, Nikolaus Demmel, Tin Chon Chan, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Power Bundle Adjustment as an expansion type algorithm for solving large-scale bundle adjustment problems. It is based on the power series expansion of the inverse Schur complement and constitutes a new family of solvers that we call inverse expansion methods. We theoretically justify the use of power series and we prove the convergence of our approach. Using the real-world BAL dataset we show that the proposed solver challenges the state-of-the-art iterative methods and significantly accelerates the solution of the normal equation, even for reaching a very high accuracy. This easy-to-implement solver can also complement a recently presented distributed bundle adjustment framework. We demonstrate that employing the proposed Power Bundle Adjustment as a sub-problem solver significantly improves speed and accuracy of the distributed optimization.



### BBBD: Bounding Box Based Detector for Occlusion Detection and Order Recovery
- **Arxiv ID**: http://arxiv.org/abs/2204.12841v1
- **DOI**: 10.5220/0000159200003209
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2204.12841v1)
- **Published**: 2022-04-27 10:56:18+00:00
- **Updated**: 2022-04-27 10:56:18+00:00
- **Authors**: Kaziwa Saleh, Zoltan Vamossy
- **Comment**: 7 pages, 4 figures
- **Journal**: In Proceedings of the 2nd International Conference on Image
  Processing and Vision Engineering (IMPROVE 2022), pages 78-84, ISBN:
  978-989-758-563-0
- **Summary**: Occlusion handling is one of the challenges of object detection and segmentation, and scene understanding. Because objects appear differently when they are occluded in varying degree, angle, and locations. Therefore, determining the existence of occlusion between objects and their order in a scene is a fundamental requirement for semantic understanding. Existing works mostly use deep learning based models to retrieve the order of the instances in an image or for occlusion detection. This requires labelled occluded data and it is time consuming. In this paper, we propose a simpler and faster method that can perform both operations without any training and only requires the modal segmentation masks. For occlusion detection, instead of scanning the two objects entirely, we only focus on the intersected area between their bounding boxes. Similarly, we use the segmentation mask inside the same area to recover the depth-ordering. When tested on COCOA dataset, our method achieves +8% and +5% more accuracy than the baselines in order recovery and occlusion detection respectively.



### Forecasting Urban Development from Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2204.12875v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12875v1)
- **Published**: 2022-04-27 12:22:20+00:00
- **Updated**: 2022-04-27 12:22:20+00:00
- **Authors**: Nando Metzger
- **Comment**: 7 Pages short-paper, Master Thesis, 2021
- **Journal**: None
- **Summary**: Forecasting where and when new buildings will emerge is a rather unexplored niche topic, but relevant in disciplines such as urban planning, agriculture, resource management, and even autonomous flight. In this work, we present a method that accomplishes this task using satellite images and a custom neural network training procedure. In stage A, a DeepLapv3+ backbone is pretrained through a Siamese network architecture aimed at solving a building change detection task. In stage B, we transfer the backbone into a change forecasting model that relies solely on the initial input image. We also transfer the backbone into a forecasting model predicting the correct time range of the future change. For our experiments, we use the SpaceNet7 dataset with 960 km2 spatial extension and 24 monthly frames. We found that our training strategy consistently outperforms the traditional pretraining on the ImageNet dataset. Especially with longer forecasting ranges of 24 months, we observe F1 scores of 24% instead of 16%. Furthermore, we found that our method performed well in forecasting the times of future building constructions. Hereby, the strengths of our custom pretraining become especially apparent when we increase the difficulty of the task by predicting finer time windows.



### Low-rank Meets Sparseness: An Integrated Spatial-Spectral Total Variation Approach to Hyperspectral Denoising
- **Arxiv ID**: http://arxiv.org/abs/2204.12879v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12879v1)
- **Published**: 2022-04-27 12:31:55+00:00
- **Updated**: 2022-04-27 12:31:55+00:00
- **Authors**: Haijin Zeng, Shaoguang Huang, Yongyong Chen, Hiep Luong, Wilfried Philips
- **Comment**: None
- **Journal**: None
- **Summary**: Spatial-Spectral Total Variation (SSTV) can quantify local smoothness of image structures, so it is widely used in hyperspectral image (HSI) processing tasks. Essentially, SSTV assumes a sparse structure of gradient maps calculated along the spatial and spectral directions. In fact, these gradient tensors are not only sparse, but also (approximately) low-rank under FFT, which we have verified by numerical tests and theoretical analysis. Based on this fact, we propose a novel TV regularization to simultaneously characterize the sparsity and low-rank priors of the gradient map (LRSTV). The new regularization not only imposes sparsity on the gradient map itself, but also penalize the rank on the gradient map after Fourier transform along the spectral dimension. It naturally encodes the sparsity and lowrank priors of the gradient map, and thus is expected to reflect the inherent structure of the original image more faithfully. Further, we use LRSTV to replace conventional SSTV and embed it in the HSI processing model to improve its performance. Experimental results on multiple public data-sets with heavy mixed noise show that the proposed model can get 1.5dB improvement of PSNR.



### Gleo-Det: Deep Convolution Feature-Guided Detector with Local Entropy Optimization for Salient Points
- **Arxiv ID**: http://arxiv.org/abs/2204.12884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12884v1)
- **Published**: 2022-04-27 12:40:21+00:00
- **Updated**: 2022-04-27 12:40:21+00:00
- **Authors**: Chao Li, Yanan You, Wenli Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Feature detection is an important procedure for image matching, where unsupervised feature detection methods are the detection approaches that have been mostly studied recently, including the ones that are based on repeatability requirement to define loss functions, and the ones that attempt to use descriptor matching to drive the optimization of the pipelines. For the former type, mean square error (MSE) is usually used which cannot provide strong constraint for training and can make the model easy to be stuck into the collapsed solution. For the later one, due to the down sampling operation and the expansion of receptive fields, the details can be lost for local descriptors can be lost, making the constraint not fine enough. Considering the issues above, we propose to combine both ideas, which including three aspects. 1) We propose to achieve fine constraint based on the requirement of repeatability while coarse constraint with guidance of deep convolution features. 2) To address the issue that optimization with MSE is limited, entropy-based cost function is utilized, both soft cross-entropy and self-information. 3) With the guidance of convolution features, we define the cost function from both positive and negative sides. Finally, we study the effect of each modification proposed and experiments demonstrate that our method achieves competitive results over the state-of-the-art approaches.



### Cross-Camera Trajectories Help Person Retrieval in a Camera Network
- **Arxiv ID**: http://arxiv.org/abs/2204.12900v3
- **DOI**: 10.1109/TIP.2023.3290515
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.12900v3)
- **Published**: 2022-04-27 13:10:48+00:00
- **Updated**: 2023-07-04 02:20:38+00:00
- **Authors**: Xin Zhang, Xiaohua Xie, Jianhuang Lai, Wei-Shi Zheng
- **Comment**: IEEE Transactions on Image Processing (TIP), 2023
- **Journal**: None
- **Summary**: We are concerned with retrieving a query person from multiple videos captured by a non-overlapping camera network. Existing methods often rely on purely visual matching or consider temporal constraints but ignore the spatial information of the camera network. To address this issue, we propose a pedestrian retrieval framework based on cross-camera trajectory generation, which integrates both temporal and spatial information. To obtain pedestrian trajectories, we propose a novel cross-camera spatio-temporal model that integrates pedestrians' walking habits and the path layout between cameras to form a joint probability distribution. Such a spatio-temporal model among a camera network can be specified using sparsely sampled pedestrian data. Based on the spatio-temporal model, cross-camera trajectories can be extracted by the conditional random field model and further optimized by restricted non-negative matrix factorization. Finally, a trajectory re-ranking technique is proposed to improve the pedestrian retrieval results. To verify the effectiveness of our method, we construct the first cross-camera pedestrian trajectory dataset, the Person Trajectory Dataset, in real surveillance scenarios. Extensive experiments verify the effectiveness and robustness of the proposed method.



### Epicardial Adipose Tissue Segmentation from CT Images with A Semi-3D Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2204.12904v1
- **DOI**: 10.1109/ELMAR52657.2021.9550936
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.12904v1)
- **Published**: 2022-04-27 13:15:44+00:00
- **Updated**: 2022-04-27 13:15:44+00:00
- **Authors**: Marin Benčević, Marija Habijan, Irena Galić
- **Comment**: None
- **Journal**: 2021 International Symposium ELMAR, 2021, pp. 87-90
- **Summary**: Epicardial adipose tissue is a type of adipose tissue located between the heart wall and a protective layer around the heart called the pericardium. The volume and thickness of epicardial adipose tissue are linked to various cardiovascular diseases. It is shown to be an independent cardiovascular disease risk factor. Fully automatic and reliable measurements of epicardial adipose tissue from CT scans could provide better disease risk assessment and enable the processing of large CT image data sets for a systemic epicardial adipose tissue study. This paper proposes a method for fully automatic semantic segmentation of epicardial adipose tissue from CT images using a deep neural network. The proposed network uses a U-Net-based architecture with slice depth information embedded in the input image to segment a pericardium region of interest, which is used to obtain an epicardial adipose tissue segmentation. Image augmentation is used to increase model robustness. Cross-validation of the proposed method yields a Dice score of 0.86 on the CT scans of 20 patients.



### An Iterative Labeling Method for Annotating Fisheries Imagery
- **Arxiv ID**: http://arxiv.org/abs/2204.12934v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2204.12934v2)
- **Published**: 2022-04-27 13:36:50+00:00
- **Updated**: 2022-06-08 06:47:15+00:00
- **Authors**: Zhiyong Zhang, Pushyami Kaveti, Hanumant Singh, Abigail Powell, Erica Fruh, M. Elizabeth Clarke
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a methodology for fisheries-related data that allows us to converge on a labeled image dataset by iterating over the dataset with multiple training and production loops that can exploit crowdsourcing interfaces. We present our algorithm and its results on two separate sets of image data collected using the Seabed autonomous underwater vehicle. The first dataset comprises of 2,026 completely unlabeled images, while the second consists of 21,968 images that were point annotated by experts. Our results indicate that training with a small subset and iterating on that to build a larger set of labeled data allows us to converge to a fully annotated dataset with a small number of iterations. Even in the case of a dataset labeled by experts, a single iteration of the methodology improves the labels by discovering additional complicated examples of labels associated with fish that overlap, are very small, or obscured by the contrast limitations associated with underwater imagery.



### MAPLE-Edge: A Runtime Latency Predictor for Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/2204.12950v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12950v1)
- **Published**: 2022-04-27 14:00:48+00:00
- **Updated**: 2022-04-27 14:00:48+00:00
- **Authors**: Saeejith Nair, Saad Abbasi, Alexander Wong, Mohammad Javad Shafiee
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has enabled automatic discovery of more efficient neural network architectures, especially for mobile and embedded vision applications. Although recent research has proposed ways of quickly estimating latency on unseen hardware devices with just a few samples, little focus has been given to the challenges of estimating latency on runtimes using optimized graphs, such as TensorRT and specifically for edge devices. In this work, we propose MAPLE-Edge, an edge device-oriented extension of MAPLE, the state-of-the-art latency predictor for general purpose hardware, where we train a regression network on architecture-latency pairs in conjunction with a hardware-runtime descriptor to effectively estimate latency on a diverse pool of edge devices. Compared to MAPLE, MAPLE-Edge can describe the runtime and target device platform using a much smaller set of CPU performance counters that are widely available on all Linux kernels, while still achieving up to +49.6% accuracy gains against previous state-of-the-art baseline methods on optimized edge device runtimes, using just 10 measurements from an unseen target device. We also demonstrate that unlike MAPLE which performs best when trained on a pool of devices sharing a common runtime, MAPLE-Edge can effectively generalize across runtimes by applying a trick of normalizing performance counters by the operator latency, in the measured hardware-runtime descriptor. Lastly, we show that for runtimes exhibiting lower than desired accuracy, performance can be boosted by collecting additional samples from the target device, with an extra 90 samples translating to gains of nearly +40%.



### Towards assessing agricultural land suitability with causal machine learning
- **Arxiv ID**: http://arxiv.org/abs/2204.12956v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12956v1)
- **Published**: 2022-04-27 14:13:47+00:00
- **Updated**: 2022-04-27 14:13:47+00:00
- **Authors**: Georgios Giannarakis, Vasileios Sitokonstantinou, Roxanne Suzette Lorilla, Charalampos Kontoes
- **Comment**: This work has been accepted for publication in EARTHVISION 2022, in
  conjunction with the Computer Vision and Pattern Recognition (CVPR) 2022
  Conference
- **Journal**: None
- **Summary**: Understanding the suitability of agricultural land for applying specific management practices is of great importance for sustainable and resilient agriculture against climate change. Recent developments in the field of causal machine learning enable the estimation of intervention impacts on an outcome of interest, for samples described by a set of observed characteristics. We introduce an extensible data-driven framework that leverages earth observations and frames agricultural land suitability as a geospatial impact assessment problem, where the estimated effects of agricultural practices on agroecosystems serve as a land suitability score and guide decision making. We formulate this as a causal machine learning task and discuss how this approach can be used for agricultural planning in a changing climate. Specifically, we extract the agricultural management practices of "crop rotation" and "landscape crop diversity" from crop type maps, account for climate and land use data, and use double machine learning to estimate their heterogeneous effect on Net Primary Productivity (NPP), within the Flanders region of Belgium from 2010 to 2020. We find that the effect of crop rotation was insignificant, while landscape crop diversity had a small negative effect on NPP. Finally, we observe considerable effect heterogeneity in space for both practices and analyze it.



### CapOnImage: Context-driven Dense-Captioning on Image
- **Arxiv ID**: http://arxiv.org/abs/2204.12974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12974v1)
- **Published**: 2022-04-27 14:40:31+00:00
- **Updated**: 2022-04-27 14:40:31+00:00
- **Authors**: Yiqi Gao, Xinglin Hou, Yuanmeng Zhang, Tiezheng Ge, Yuning Jiang, Peng Wang
- **Comment**: 13pages, 10figures
- **Journal**: None
- **Summary**: Existing image captioning systems are dedicated to generating narrative captions for images, which are spatially detached from the image in presentation. However, texts can also be used as decorations on the image to highlight the key points and increase the attractiveness of images. In this work, we introduce a new task called captioning on image (CapOnImage), which aims to generate dense captions at different locations of the image based on contextual information. To fully exploit the surrounding visual context to generate the most suitable caption for each location, we propose a multi-modal pre-training model with multi-level pre-training tasks that progressively learn the correspondence between texts and image locations from easy to difficult. Since the model may generate redundant captions for nearby locations, we further enhance the location embedding with neighbor locations as context. For this new task, we also introduce a large-scale benchmark called CapOnImage2M, which contains 2.1 million product images, each with an average of 4.8 spatially localized captions. Compared with other image captioning model variants, our model achieves the best results in both captioning accuracy and diversity aspects. We will make code and datasets public to facilitate future research.



### DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2204.12997v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.12997v2)
- **Published**: 2022-04-27 15:11:04+00:00
- **Updated**: 2022-04-28 14:36:21+00:00
- **Authors**: Xianing Chen, Qiong Cao, Yujie Zhong, Jing Zhang, Shenghua Gao, Dacheng Tao
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Transformers are successfully applied to computer vision due to their powerful modeling capacity with self-attention. However, the excellent performance of transformers heavily depends on enormous training images. Thus, a data-efficient transformer solution is urgently needed. In this work, we propose an early knowledge distillation framework, which is termed as DearKD, to improve the data efficiency required by transformers. Our DearKD is a two-stage framework that first distills the inductive biases from the early intermediate layers of a CNN and then gives the transformer full play by training without distillation. Further, our DearKD can be readily applied to the extreme data-free case where no real images are available. In this case, we propose a boundary-preserving intra-divergence loss based on DeepInversion to further close the performance gap against the full-data counterpart. Extensive experiments on ImageNet, partial ImageNet, data-free setting and other downstream tasks prove the superiority of DearKD over its baselines and state-of-the-art methods.



### Relevance-based Margin for Contrastively-trained Video Retrieval Models
- **Arxiv ID**: http://arxiv.org/abs/2204.13001v1
- **DOI**: 10.1145/3512527.3531395
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13001v1)
- **Published**: 2022-04-27 15:16:11+00:00
- **Updated**: 2022-04-27 15:16:11+00:00
- **Authors**: Alex Falcon, Swathikiran Sudhakaran, Giuseppe Serra, Sergio Escalera, Oswald Lanz
- **Comment**: Accepted for presentation at International Conference on Multimedia
  Retrieval (ICMR '22)
- **Journal**: None
- **Summary**: Video retrieval using natural language queries has attracted increasing interest due to its relevance in real-world applications, from intelligent access in private media galleries to web-scale video search. Learning the cross-similarity of video and text in a joint embedding space is the dominant approach. To do so, a contrastive loss is usually employed because it organizes the embedding space by putting similar items close and dissimilar items far. This framework leads to competitive recall rates, as they solely focus on the rank of the groundtruth items. Yet, assessing the quality of the ranking list is of utmost importance when considering intelligent retrieval systems, since multiple items may share similar semantics, hence a high relevance. Moreover, the aforementioned framework uses a fixed margin to separate similar and dissimilar items, treating all non-groundtruth items as equally irrelevant. In this paper we propose to use a variable margin: we argue that varying the margin used during training based on how much relevant an item is to a given query, i.e. a relevance-based margin, easily improves the quality of the ranking lists measured through nDCG and mAP. We demonstrate the advantages of our technique using different models on EPIC-Kitchens-100 and YouCook2. We show that even if we carefully tuned the fixed margin, our technique (which does not have the margin as a hyper-parameter) would still achieve better performance. Finally, extensive ablation studies and qualitative analysis support the robustness of our approach. Code will be released at \url{https://github.com/aranciokov/RelevanceMargin-ICMR22}.



### Defending Person Detection Against Adversarial Patch Attack by using Universal Defensive Frame
- **Arxiv ID**: http://arxiv.org/abs/2204.13004v2
- **DOI**: 10.1109/TIP.2022.3217375
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13004v2)
- **Published**: 2022-04-27 15:18:08+00:00
- **Updated**: 2022-10-20 06:59:15+00:00
- **Authors**: Youngjoon Yu, Hong Joo Lee, Hakmin Lee, Yong Man Ro
- **Comment**: Accepted at IEEE Transactions on Image Processing (TIP), 2022
- **Journal**: None
- **Summary**: Person detection has attracted great attention in the computer vision area and is an imperative element in human-centric computer vision. Although the predictive performances of person detection networks have been improved dramatically, they are vulnerable to adversarial patch attacks. Changing the pixels in a restricted region can easily fool the person detection network in safety-critical applications such as autonomous driving and security systems. Despite the necessity of countering adversarial patch attacks, very few efforts have been dedicated to defending person detection against adversarial patch attack. In this paper, we propose a novel defense strategy that defends against an adversarial patch attack by optimizing a defensive frame for person detection. The defensive frame alleviates the effect of the adversarial patch while maintaining person detection performance with clean person. The proposed defensive frame in the person detection is generated with a competitive learning algorithm which makes an iterative competition between detection threatening module and detection shielding module in person detection. Comprehensive experimental results demonstrate that the proposed method effectively defends person detection against adversarial patch attacks.



### Ollivier-Ricci Curvature For Head Pose Estimation From a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2204.13006v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.13006v1)
- **Published**: 2022-04-27 15:20:26+00:00
- **Updated**: 2022-04-27 15:20:26+00:00
- **Authors**: Lucia Cascone, Riccardo Distasi, Michele Nappi
- **Comment**: None
- **Journal**: None
- **Summary**: Head pose estimation is a crucial challenge for many real-world applications, such as attention and human behavior analysis. This paper aims to estimate head pose from a single image by applying notions of network curvature. In the real world, many complex networks have groups of nodes that are well connected to each other with significant functional roles. Similarly, the interactions of facial landmarks can be represented as complex dynamic systems modeled by weighted graphs. The functionalities of such systems are therefore intrinsically linked to the topology and geometry of the underlying graph. In this work, using the geometric notion of Ollivier-Ricci curvature (ORC) on weighted graphs as input to the XGBoost regression model, we show that the intrinsic geometric basis of ORC offers a natural approach to discovering underlying common structure within a pool of poses. Experiments on the BIWI, AFLW2000 and Pointing'04 datasets show that the ORC_XGB method performs well compared to state-of-the-art methods, both landmark-based and image-only.



### Dropout Inference with Non-Uniform Weight Scaling
- **Arxiv ID**: http://arxiv.org/abs/2204.13047v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.13047v1)
- **Published**: 2022-04-27 16:41:12+00:00
- **Updated**: 2022-04-27 16:41:12+00:00
- **Authors**: Zhaoyuan Yang, Arpit Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Dropout as regularization has been used extensively to prevent overfitting for training neural networks. During training, units and their connections are randomly dropped, which could be considered as sampling many different submodels from the original model. At test time, weight scaling and Monte Carlo approximation are two widely applied approaches to approximate the outputs. Both approaches work well practically when all submodels are low-bias complex learners. However, in this work, we demonstrate scenarios where some submodels behave closer to high-bias models and a non-uniform weight scaling is a better approximation for inference.



### Collaborative Learning for Hand and Object Reconstruction with Attention-guided Graph Convolution
- **Arxiv ID**: http://arxiv.org/abs/2204.13062v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13062v1)
- **Published**: 2022-04-27 17:00:54+00:00
- **Updated**: 2022-04-27 17:00:54+00:00
- **Authors**: Tze Ho Elden Tse, Kwang In Kim, Ales Leonardis, Hyung Jin Chang
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Estimating the pose and shape of hands and objects under interaction finds numerous applications including augmented and virtual reality. Existing approaches for hand and object reconstruction require explicitly defined physical constraints and known objects, which limits its application domains. Our algorithm is agnostic to object models, and it learns the physical rules governing hand-object interaction. This requires automatically inferring the shapes and physical interaction of hands and (potentially unknown) objects. We seek to approach this challenging problem by proposing a collaborative learning strategy where two-branches of deep networks are learning from each other. Specifically, we transfer hand mesh information to the object branch and vice versa for the hand branch. The resulting optimisation (training) problem can be unstable, and we address this via two strategies: (i) attention-guided graph convolution which helps identify and focus on mutual occlusion and (ii) unsupervised associative loss which facilitates the transfer of information between the branches. Experiments using four widely-used benchmarks show that our framework achieves beyond state-of-the-art accuracy in 3D pose estimation, as well as recovers dense 3D hand and object shapes. Each technical component above contributes meaningfully in the ablation study.



### Attention Consistency on Visual Corruptions for Single-Source Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2204.13091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13091v1)
- **Published**: 2022-04-27 17:39:13+00:00
- **Updated**: 2022-04-27 17:39:13+00:00
- **Authors**: Ilke Cugu, Massimiliano Mancini, Yanbei Chen, Zeynep Akata
- **Comment**: CVPRW 2022 - Camera ready version
- **Journal**: None
- **Summary**: Generalizing visual recognition models trained on a single distribution to unseen input distributions (i.e. domains) requires making them robust to superfluous correlations in the training set. In this work, we achieve this goal by altering the training images to simulate new domains and imposing consistent visual attention across the different views of the same sample. We discover that the first objective can be simply and effectively met through visual corruptions. Specifically, we alter the content of the training images using the nineteen corruptions of the ImageNet-C benchmark and three additional transformations based on Fourier transform. Since these corruptions preserve object locations, we propose an attention consistency loss to ensure that class activation maps across original and corrupted versions of the same training sample are aligned. We name our model Attention Consistency on Visual Corruptions (ACVC). We show that ACVC consistently achieves the state of the art on three single-source domain generalization benchmarks, PACS, COCO, and the large-scale DomainNet.



### 3D Magic Mirror: Clothing Reconstruction from a Single Image via a Causal Perspective
- **Arxiv ID**: http://arxiv.org/abs/2204.13096v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13096v2)
- **Published**: 2022-04-27 17:46:55+00:00
- **Updated**: 2023-03-13 15:28:11+00:00
- **Authors**: Zhedong Zheng, Jiayin Zhu, Wei Ji, Yi Yang, Tat-Seng Chua
- **Comment**: Update results. Report person re-id performance. Add details in
  Appendix
- **Journal**: None
- **Summary**: This research aims to study a self-supervised 3D clothing reconstruction method, which recovers the geometry shape and texture of human clothing from a single image. Compared with existing methods, we observe that three primary challenges remain: (1) 3D ground-truth meshes of clothing are usually inaccessible due to annotation difficulties and time costs; (2) Conventional template-based methods are limited to modeling non-rigid objects, e.g., handbags and dresses, which are common in fashion images; (3) The inherent ambiguity compromises the model training, such as the dilemma between a large shape with a remote camera or a small shape with a close camera.   In an attempt to address the above limitations, we propose a causality-aware self-supervised learning method to adaptively reconstruct 3D non-rigid objects from 2D images without 3D annotations. In particular, to solve the inherent ambiguity among four implicit variables, i.e., camera position, shape, texture, and illumination, we introduce an explainable structural causal map (SCM) to build our model. The proposed model structure follows the spirit of the causal map, which explicitly considers the prior template in the camera estimation and shape prediction. When optimization, the causality intervention tool, i.e., two expectation-maximization loops, is deeply embedded in our algorithm to (1) disentangle four encoders and (2) facilitate the prior template. Extensive experiments on two 2D fashion benchmarks (ATR and Market-HQ) show that the proposed method could yield high-fidelity 3D reconstruction. Furthermore, we also verify the scalability of the proposed method on a fine-grained bird dataset, i.e., CUB. The code is available at https://github.com/layumi/ 3D-Magic-Mirror .



### Few-Shot Head Swapping in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2204.13100v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2204.13100v1)
- **Published**: 2022-04-27 17:52:51+00:00
- **Updated**: 2022-04-27 17:52:51+00:00
- **Authors**: Changyong Shu, Hemao Wu, Hang Zhou, Jiaming Liu, Zhibin Hong, Changxing Ding, Junyu Han, Jingtuo Liu, Errui Ding, Jingdong Wang
- **Comment**: Accepted to CVPR 2022 as Oral. Demo videos and code are available at
  https://jmliu88.github.io/HeSer
- **Journal**: None
- **Summary**: The head swapping task aims at flawlessly placing a source head onto a target body, which is of great importance to various entertainment scenarios. While face swapping has drawn much attention, the task of head swapping has rarely been explored, particularly under the few-shot setting. It is inherently challenging due to its unique needs in head modeling and background blending. In this paper, we present the Head Swapper (HeSer), which achieves few-shot head swapping in the wild through two delicately designed modules. Firstly, a Head2Head Aligner is devised to holistically migrate pose and expression information from the target to the source head by examining multi-scale information. Secondly, to tackle the challenges of skin color variations and head-background mismatches in the swapping procedure, a Head2Scene Blender is introduced to simultaneously modify facial skin color and fill mismatched gaps in the background around the head. Particularly, seamless blending is achieved with the help of a Semantic-Guided Color Reference Creation procedure and a Blending UNet. Extensive experiments demonstrate that the proposed method produces superior head swapping results in a variety of scenes.



### Self-Supervised Learning of Object Parts for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.13101v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13101v2)
- **Published**: 2022-04-27 17:55:17+00:00
- **Updated**: 2022-06-21 01:05:28+00:00
- **Authors**: Adrian Ziegler, Yuki M. Asano
- **Comment**: Accepted at CVPR 2022
- **Journal**: None
- **Summary**: Progress in self-supervised learning has brought strong general image representation learning methods. Yet so far, it has mostly focused on image-level learning. In turn, tasks such as unsupervised image segmentation have not benefited from this trend as they require spatially-diverse representations. However, learning dense representations is challenging, as in the unsupervised context it is not clear how to guide the model to learn representations that correspond to various potential object categories. In this paper, we argue that self-supervised learning of object parts is a solution to this issue. Object parts are generalizable: they are a priori independent of an object definition, but can be grouped to form objects a posteriori. To this end, we leverage the recently proposed Vision Transformer's capability of attending to objects and combine it with a spatially dense clustering task for fine-tuning the spatial tokens. Our method surpasses the state-of-the-art on three semantic segmentation benchmarks by 17%-3%, showing that our representations are versatile under various object definitions. Finally, we extend this to fully unsupervised segmentation - which refrains completely from using label information even at test-time - and demonstrate that a simple method for automatically merging discovered object parts based on community detection yields substantial gains.



### HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.13132v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13132v2)
- **Published**: 2022-04-27 18:00:26+00:00
- **Updated**: 2022-07-26 15:06:17+00:00
- **Authors**: Lukas Hoyer, Dengxin Dai, Luc Van Gool
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) aims to adapt a model trained on the source domain (e.g. synthetic data) to the target domain (e.g. real-world data) without requiring further annotations on the target domain. This work focuses on UDA for semantic segmentation as real-world pixel-wise annotations are particularly expensive to acquire. As UDA methods for semantic segmentation are usually GPU memory intensive, most previous methods operate only on downscaled images. We question this design as low-resolution predictions often fail to preserve fine details. The alternative of training with random crops of high-resolution images alleviates this problem but falls short in capturing long-range, domain-robust context information. Therefore, we propose HRDA, a multi-resolution training approach for UDA, that combines the strengths of small high-resolution crops to preserve fine segmentation details and large low-resolution crops to capture long-range context dependencies with a learned scale attention, while maintaining a manageable GPU memory footprint. HRDA enables adapting small objects and preserving fine segmentation details. It significantly improves the state-of-the-art performance by 5.5 mIoU for GTA-to-Cityscapes and 4.9 mIoU for Synthia-to-Cityscapes, resulting in unprecedented 73.8 and 65.8 mIoU, respectively. The implementation is available at https://github.com/lhoyer/HRDA.



### An Improved Nearest Neighbour Classifier
- **Arxiv ID**: http://arxiv.org/abs/2204.13141v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13141v2)
- **Published**: 2022-04-27 18:28:12+00:00
- **Updated**: 2023-02-11 18:11:21+00:00
- **Authors**: Eric Setterqvist, Natan Kruglyak, Robert Forchheimer
- **Comment**: None
- **Journal**: None
- **Summary**: A windowed version of the Nearest Neighbour (WNN) classifier for images is described. While its construction is inspired by the architecture of Artificial Neural Networks, the underlying theoretical framework is based on approximation theory. We illustrate WNN on the datasets MNIST and EMNIST of images of handwritten digits. In order to calibrate the parameters of WNN, we first study it on the classical MNIST dataset. We then apply WNN with these parameters to the challenging EMNIST dataset. It is demonstrated that WNN misclassifies 0.42% of the images of EMNIST and therefore significantly outperforms predictions by humans and shallow ANNs that both have more than 1.3% of errors.



### SSR-GNNs: Stroke-based Sketch Representation with Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.13153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13153v1)
- **Published**: 2022-04-27 19:18:01+00:00
- **Updated**: 2022-04-27 19:18:01+00:00
- **Authors**: Sheng Cheng, Yi Ren, Yezhou Yang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper follows cognitive studies to investigate a graph representation for sketches, where the information of strokes, i.e., parts of a sketch, are encoded on vertices and information of inter-stroke on edges. The resultant graph representation facilitates the training of a Graph Neural Networks for classification tasks, and achieves accuracy and robustness comparable to the state-of-the-art against translation and rotation attacks, as well as stronger attacks on graph vertices and topologies, i.e., modifications and addition of strokes, all without resorting to adversarial training. Prior studies on sketches, e.g., graph transformers, encode control points of stroke on vertices, which are not invariant to spatial transformations. In contrary, we encode vertices and edges using pairwise distances among control points to achieve invariance. Compared with existing generative sketch model for one-shot classification, our method does not rely on run-time statistical inference. Lastly, the proposed representation enables generation of novel sketches that are structurally similar to while separable from the existing dataset.



### Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2204.13158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13158v1)
- **Published**: 2022-04-27 19:37:42+00:00
- **Updated**: 2022-04-27 19:37:42+00:00
- **Authors**: Mustafa Ebrahim Chasmai, Tamajit Banerjee
- **Comment**: None
- **Journal**: None
- **Summary**: Person Re-Identification (Re-ID) is an important problem in computer vision-based surveillance applications, in which one aims to identify a person across different surveillance photographs taken from different cameras having varying orientations and field of views. Due to the increasing demand for intelligent video surveillance, Re-ID has gained significant interest in the computer vision community. In this work, we experiment on some existing Re-ID methods that obtain state of the art performance in some open benchmarks. We qualitatively and quantitaively analyse their performance on a provided dataset, and then propose methods to improve the results. This work was the report submitted for COL780 final project at IIT Delhi.



### AdaBest: Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation
- **Arxiv ID**: http://arxiv.org/abs/2204.13170v4
- **DOI**: 10.1007/978-3-031-20050-2_41
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.DC, cs.MA, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2204.13170v4)
- **Published**: 2022-04-27 20:04:24+00:00
- **Updated**: 2023-07-24 13:35:28+00:00
- **Authors**: Farshid Varno, Marzie Saghayi, Laya Rafiee Sevyeri, Sharut Gupta, Stan Matwin, Mohammad Havaei
- **Comment**: Published as a conference paper at ECCV 2022; Corrected some typos in
  the text and a baseline algorithm
- **Journal**: None
- **Summary**: In Federated Learning (FL), a number of clients or devices collaborate to train a model without sharing their data. Models are optimized locally at each client and further communicated to a central hub for aggregation. While FL is an appealing decentralized training paradigm, heterogeneity among data from different clients can cause the local optimization to drift away from the global objective. In order to estimate and therefore remove this drift, variance reduction techniques have been incorporated into FL optimization recently. However, these approaches inaccurately estimate the clients' drift and ultimately fail to remove it properly. In this work, we propose an adaptive algorithm that accurately estimates drift across clients. In comparison to previous works, our approach necessitates less storage and communication bandwidth, as well as lower compute costs. Additionally, our proposed methodology induces stability by constraining the norm of estimates for client drift, making it more practical for large scale FL. Experimental findings demonstrate that the proposed algorithm converges significantly faster and achieves higher accuracy than the baselines across various FL benchmarks.



### Interpretable Graph Convolutional Network of Multi-Modality Brain Imaging for Alzheimer's Disease Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2204.13188v1
- **DOI**: 10.1109/ISBI52829.2022.9761449
- **Categories**: **cs.LG**, cs.CV, eess.IV, 68T07, 68T20, 68T45, I.2.6; I.2.10; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2204.13188v1)
- **Published**: 2022-04-27 20:43:11+00:00
- **Updated**: 2022-04-27 20:43:11+00:00
- **Authors**: Houliang Zhou, Lifang He, Yu Zhang, Li Shen, Brian Chen
- **Comment**: This paper has been published to IEEE 19th International Symposium on
  Biomedical Imaging (ISBI), 2022
- **Journal**: None
- **Summary**: Identification of brain regions related to the specific neurological disorders are of great importance for biomarker and diagnostic studies. In this paper, we propose an interpretable Graph Convolutional Network (GCN) framework for the identification and classification of Alzheimer's disease (AD) using multi-modality brain imaging data. Specifically, we extended the Gradient Class Activation Mapping (Grad-CAM) technique to quantify the most discriminative features identified by GCN from brain connectivity patterns. We then utilized them to find signature regions of interest (ROIs) by detecting the difference of features between regions in healthy control (HC), mild cognitive impairment (MCI), and AD groups. We conducted the experiments on the ADNI database with imaging data from three modalities, including VBM-MRI, FDG-PET, and AV45-PET, and showed that the ROI features learned by our method were effective for enhancing the performances of both clinical score prediction and disease status identification. It also successfully identified biomarkers associated with AD and MCI.



### Channel Pruned YOLOv5-based Deep Learning Approach for Rapid and Accurate Outdoor Obstacles Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.13699v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.13699v2)
- **Published**: 2022-04-27 21:06:04+00:00
- **Updated**: 2022-08-15 11:27:18+00:00
- **Authors**: Zeqian Li, Yuwei Wang, Kexun Chen, Zhibin Yu
- **Comment**: None
- **Journal**: None
- **Summary**: One-stage algorithm have been widely used in target detection systems that need to be trained with massive data. Most of them perform well both in real-time and accuracy. However, due to their convolutional structure, they need more computing power and greater memory consumption. Hence, we applied pruning strategy to target detection networks to reduce the number of parameters and the size of model. To demonstrate the practicality of the pruning method, we select the YOLOv5 model for experiments and provide a data set of outdoor obstacles to show the effect of model. In this specific data set, in the best circumstances, the volume of the network model is reduced by 49.7% compared with the original model, and the reasoning time is reduced by 52.5%. Meanwhile, it also uses data processing methods to compensate for the drop in accuracy caused by pruning.



### Use All The Labels: A Hierarchical Multi-Label Contrastive Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/2204.13207v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.13207v1)
- **Published**: 2022-04-27 21:41:44+00:00
- **Updated**: 2022-04-27 21:41:44+00:00
- **Authors**: Shu Zhang, Ran Xu, Caiming Xiong, Chetan Ramaiah
- **Comment**: Accepted by CVPR, 2022
- **Journal**: None
- **Summary**: Current contrastive learning frameworks focus on leveraging a single supervisory signal to learn representations, which limits the efficacy on unseen data and downstream tasks. In this paper, we present a hierarchical multi-label representation learning framework that can leverage all available labels and preserve the hierarchical relationship between classes. We introduce novel hierarchy preserving losses, which jointly apply a hierarchical penalty to the contrastive loss, and enforce the hierarchy constraint. The loss function is data driven and automatically adapts to arbitrary multi-label structures. Experiments on several datasets show that our relationship-preserving embedding performs well on a variety of tasks and outperform the baseline supervised and self-supervised approaches. Code is available at https://github.com/salesforce/hierarchicalContrastiveLearning.



### Offline Visual Representation Learning for Embodied Navigation
- **Arxiv ID**: http://arxiv.org/abs/2204.13226v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.13226v1)
- **Published**: 2022-04-27 23:22:43+00:00
- **Updated**: 2022-04-27 23:22:43+00:00
- **Authors**: Karmesh Yadav, Ram Ramrakhya, Arjun Majumdar, Vincent-Pierre Berges, Sachit Kuhar, Dhruv Batra, Alexei Baevski, Oleksandr Maksymets
- **Comment**: 15 pages, 4 figures, 7 tables and supplementary
- **Journal**: None
- **Summary**: How should we learn visual representations for embodied agents that must see and move? The status quo is tabula rasa in vivo, i.e. learning visual representations from scratch while also learning to move, potentially augmented with auxiliary tasks (e.g. predicting the action taken between two successive observations). In this paper, we show that an alternative 2-stage strategy is far more effective: (1) offline pretraining of visual representations with self-supervised learning (SSL) using large-scale pre-rendered images of indoor environments (Omnidata), and (2) online finetuning of visuomotor representations on specific tasks with image augmentations under long learning schedules. We call this method Offline Visual Representation Learning (OVRL). We conduct large-scale experiments - on 3 different 3D datasets (Gibson, HM3D, MP3D), 2 tasks (ImageNav, ObjectNav), and 2 policy learning algorithms (RL, IL) - and find that the OVRL representations lead to significant across-the-board improvements in state of art, on ImageNav from 29.2% to 54.2% (+25% absolute, 86% relative) and on ObjectNav from 18.1% to 23.2% (+5.1% absolute, 28% relative). Importantly, both results were achieved by the same visual encoder generalizing to datasets that were not seen during pretraining. While the benefits of pretraining sometimes diminish (or entirely disappear) with long finetuning schedules, we find that OVRL's performance gains continue to increase (not decrease) as the agent is trained for 2 billion frames of experience.



