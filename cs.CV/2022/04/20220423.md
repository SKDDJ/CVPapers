# Arxiv Papers in cs.CV on 2022-04-23
### Visual Attention Emerges from Recurrent Sparse Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2204.10962v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.10962v3)
- **Published**: 2022-04-23 00:35:02+00:00
- **Updated**: 2022-06-12 23:57:50+00:00
- **Authors**: Baifeng Shi, Yale Song, Neel Joshi, Trevor Darrell, Xin Wang
- **Comment**: Released code: https://github.com/bfshi/VARS
- **Journal**: None
- **Summary**: Visual attention helps achieve robust perception under noise, corruption, and distribution shifts in human vision, which are areas where modern neural networks still fall short. We present VARS, Visual Attention from Recurrent Sparse reconstruction, a new attention formulation built on two prominent features of the human visual attention mechanism: recurrency and sparsity. Related features are grouped together via recurrent connections between neurons, with salient objects emerging via sparse regularization. VARS adopts an attractor network with recurrent connections that converges toward a stable pattern over time. Network layers are represented as ordinary differential equations (ODEs), formulating attention as a recurrent attractor network that equivalently optimizes the sparse reconstruction of input using a dictionary of "templates" encoding underlying patterns of data. We show that self-attention is a special case of VARS with a single-step optimization and no sparsity constraint. VARS can be readily used as a replacement for self-attention in popular vision transformers, consistently improving their robustness across various benchmarks. Code is released on GitHub (https://github.com/bfshi/VARS).



### CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.10965v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.10965v5)
- **Published**: 2022-04-23 00:40:02+00:00
- **Updated**: 2023-06-05 17:49:40+00:00
- **Authors**: Tuomas Oikarinen, Tsui-Wei Weng
- **Comment**: Published in ICLR 2023 Conference (Spotlight). New v5(5 June 2023) -
  Added crowdsourced user study in Appendix B, not included in ICLR publication
- **Journal**: None
- **Summary**: In this paper, we propose CLIP-Dissect, a new technique to automatically describe the function of individual hidden neurons inside vision networks. CLIP-Dissect leverages recent advances in multimodal vision/language models to label internal neurons with open-ended concepts without the need for any labeled data or human examples. We show that CLIP-Dissect provides more accurate descriptions than existing methods for last layer neurons where the ground-truth is available as well as qualitatively good descriptions for hidden layer neurons. In addition, our method is very flexible: it is model agnostic, can easily handle new concepts and can be extended to take advantage of better multimodal models in the future. Finally CLIP-Dissect is computationally efficient and can label all neurons from five layers of ResNet-50 in just 4 minutes, which is more than 10 times faster than existing methods. Our code is available at https://github.com/Trustworthy-ML-Lab/CLIP-dissect. Finally, crowdsourced user study results are available at Appendix B to further support the effectiveness of our method.



### Unsupervised Restoration of Weather-affected Images using Deep Gaussian Process-based CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/2204.10970v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10970v1)
- **Published**: 2022-04-23 01:30:47+00:00
- **Updated**: 2022-04-23 01:30:47+00:00
- **Authors**: Rajeev Yasarla, Vishwanath A. Sindagi, Vishal M. Patel
- **Comment**: Accepted at ICPR 2022
- **Journal**: None
- **Summary**: Existing approaches for restoring weather-degraded images follow a fully-supervised paradigm and they require paired data for training. However, collecting paired data for weather degradations is extremely challenging, and existing methods end up training on synthetic data. To overcome this issue, we describe an approach for supervising deep networks that are based on CycleGAN, thereby enabling the use of unlabeled real-world data for training. Specifically, we introduce new losses for training CycleGAN that lead to more effective training, resulting in high-quality reconstructions. These new losses are obtained by jointly modeling the latent space embeddings of predicted clean images and original clean images through Deep Gaussian Processes. This enables the CycleGAN architecture to transfer the knowledge from one domain (weather-degraded) to another (clean) more effectively. We demonstrate that the proposed method can be effectively applied to different restoration tasks like de-raining, de-hazing and de-snowing and it outperforms other unsupervised techniques (that leverage weather-based characteristics) by a considerable margin.



### GRM: Gradient Rectification Module for Visual Place Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2204.10972v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.10972v2)
- **Published**: 2022-04-23 01:40:06+00:00
- **Updated**: 2023-02-28 02:46:40+00:00
- **Authors**: Boshu Lei, Wenjie Ding, Limeng Qiao, Xi Qiu
- **Comment**: Accepted to the 2023 International Conference on Robotics and
  Automation (ICRA 2023)
- **Journal**: None
- **Summary**: Visual place retrieval aims to search images in the database that depict similar places as the query image. However, global descriptors encoded by the network usually fall into a low dimensional principal space, which is harmful to the retrieval performance. We first analyze the cause of this phenomenon, pointing out that it is due to degraded distribution of the gradients of descriptors. Then, we propose Gradient Rectification Module(GRM) to alleviate this issue. GRM is appended after the final pooling layer and can rectify gradients to the complementary space of the principal space. With GRM, the network is encouraged to generate descriptors more uniformly in the whole space. At last, we conduct experiments on multiple datasets and generalize our method to classification task under prototype learning framework.



### Detecting Recolored Image by Spatial Correlation
- **Arxiv ID**: http://arxiv.org/abs/2204.10973v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.10973v1)
- **Published**: 2022-04-23 01:54:06+00:00
- **Updated**: 2022-04-23 01:54:06+00:00
- **Authors**: Yushu Zhang, Nuo Chen, Shuren Qi, Mingfu Xue, Xiaochun Cao
- **Comment**: 11 pages, 13 figures
- **Journal**: None
- **Summary**: Image forensics, aiming to ensure the authenticity of the image, has made great progress in dealing with common image manipulation such as copy-move, splicing, and inpainting in the past decades. However, only a few researchers pay attention to an emerging editing technique called image recoloring, which can manipulate the color values of an image to give it a new style. To prevent it from being used maliciously, the previous approaches address the conventional recoloring from the perspective of inter-channel correlation and illumination consistency. In this paper, we try to explore a solution from the perspective of the spatial correlation, which exhibits the generic detection capability for both conventional and deep learning-based recoloring. Through theoretical and numerical analysis, we find that the recoloring operation will inevitably destroy the spatial correlation between pixels, implying a new prior of statistical discriminability. Based on such fact, we generate a set of spatial correlation features and learn the informative representation from the set via a convolutional neural network. To train our network, we use three recoloring methods to generate a large-scale and high-quality data set. Extensive experimental results in two recoloring scenes demonstrate that the spatial correlation features are highly discriminative. Our method achieves the state-of-the-art detection accuracy on multiple benchmark datasets and exhibits well generalization for unknown types of recoloring methods.



### Federated Contrastive Learning for Volumetric Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.10983v1
- **DOI**: 10.1007/978-3-030-87199-4_35
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.10983v1)
- **Published**: 2022-04-23 03:47:23+00:00
- **Updated**: 2022-04-23 03:47:23+00:00
- **Authors**: Yawen Wu, Dewen Zeng, Zhepeng Wang, Yiyu Shi, Jingtong Hu
- **Comment**: International Conference on Medical Image Computing and
  Computer-Assisted Intervention. Springer, Cham, 2021
- **Journal**: None
- **Summary**: Supervised deep learning needs a large amount of labeled data to achieve high performance. However, in medical imaging analysis, each site may only have a limited amount of data and labels, which makes learning ineffective. Federated learning (FL) can help in this regard by learning a shared model while keeping training data local for privacy. Traditional FL requires fully-labeled data for training, which is inconvenient or sometimes infeasible to obtain due to high labeling cost and the requirement of expertise. Contrastive learning (CL), as a self-supervised learning approach, can effectively learn from unlabeled data to pre-train a neural network encoder, followed by fine-tuning for downstream tasks with limited annotations. However, when adopting CL in FL, the limited data diversity on each client makes federated contrastive learning (FCL) ineffective. In this work, we propose an FCL framework for volumetric medical image segmentation with limited annotations. More specifically, we exchange the features in the FCL pre-training process such that diverse contrastive data are provided to each site for effective local CL while keeping raw data private. Based on the exchanged features, global structural matching further leverages the structural similarity to align local features to the remote ones such that a unified feature space can be learned among different sites. Experiments on a cardiac MRI dataset show the proposed framework substantially improves the segmentation performance compared with state-of-the-art techniques.



### TerrainMesh: Metric-Semantic Terrain Reconstruction from Aerial Images Using Joint 2D-3D Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.10993v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2204.10993v1)
- **Published**: 2022-04-23 05:18:39+00:00
- **Updated**: 2022-04-23 05:18:39+00:00
- **Authors**: Qiaojun Feng, Nikolay Atanasov
- **Comment**: 15 pages, 14 figures. arXiv admin note: text overlap with
  arXiv:2101.01844
- **Journal**: None
- **Summary**: This paper considers outdoor terrain mapping using RGB images obtained from an aerial vehicle. While feature-based localization and mapping techniques deliver real-time vehicle odometry and sparse keypoint depth reconstruction, a dense model of the environment geometry and semantics (vegetation, buildings, etc.) is usually recovered offline with significant computation and storage. This paper develops a joint 2D-3D learning approach to reconstruct a local metric-semantic mesh at each camera keyframe maintained by a visual odometry algorithm. Given the estimated camera trajectory, the local meshes can be assembled into a global environment model to capture the terrain topology and semantics during online operation. A local mesh is reconstructed using an initialization and refinement stage. In the initialization stage, we estimate the mesh vertex elevation by solving a least squares problem relating the vertex barycentric coordinates to the sparse keypoint depth measurements. In the refinement stage, we associate 2D image and semantic features with the 3D mesh vertices using camera projection and apply graph convolution to refine the mesh vertex spatial coordinates and semantic features based on joint 2D and 3D supervision. Quantitative and qualitative evaluation using real aerial images show the potential of our method to support environmental monitoring and surveillance applications.



### Cerebral Palsy Prediction with Frequency Attention Informed Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.10997v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.10997v2)
- **Published**: 2022-04-23 05:48:17+00:00
- **Updated**: 2023-03-28 16:14:46+00:00
- **Authors**: Haozheng Zhang, Hubert P. H. Shum, Edmond S. L. Ho
- **Comment**: None
- **Journal**: None
- **Summary**: Early diagnosis and intervention are clinically considered the paramount part of treating cerebral palsy (CP), so it is essential to design an efficient and interpretable automatic prediction system for CP. We highlight a significant difference between CP infants' frequency of human movement and that of the healthy group, which improves prediction performance. However, the existing deep learning-based methods did not use the frequency information of infants' movement for CP prediction. This paper proposes a frequency attention informed graph convolutional network and validates it on two consumer-grade RGB video datasets, namely MINI-RGBD and RVI-38 datasets. Our proposed frequency attention module aids in improving both classification performance and system interpretability. In addition, we design a frequency-binning method that retains the critical frequency of the human joint position data while filtering the noise. Our prediction performance achieves state-of-the-art research on both datasets. Our work demonstrates the effectiveness of frequency information in supporting the prediction of CP non-intrusively and provides a way for supporting the early diagnosis of CP in the resource-limited regions where the clinical resources are not abundant.



### Training and challenging models for text-guided fashion image retrieval
- **Arxiv ID**: http://arxiv.org/abs/2204.11004v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.11004v1)
- **Published**: 2022-04-23 06:24:23+00:00
- **Updated**: 2022-04-23 06:24:23+00:00
- **Authors**: Eric Dodds, Jack Culpepper, Gaurav Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Retrieving relevant images from a catalog based on a query image together with a modifying caption is a challenging multimodal task that can particularly benefit domains like apparel shopping, where fine details and subtle variations may be best expressed through natural language. We introduce a new evaluation dataset, Challenging Fashion Queries (CFQ), as well as a modeling approach that achieves state-of-the-art performance on the existing Fashion IQ (FIQ) dataset. CFQ complements existing benchmarks by including relative captions with positive and negative labels of caption accuracy and conditional image similarity, where others provided only positive labels with a combined meaning. We demonstrate the importance of multimodal pretraining for the task and show that domain-specific weak supervision based on attribute labels can augment generic large-scale pretraining. While previous modality fusion mechanisms lose the benefits of multimodal pretraining, we introduce a residual attention fusion mechanism that improves performance. We release CFQ and our code to the research community.



### Surface Reconstruction from Point Clouds by Learning Predictive Context Priors
- **Arxiv ID**: http://arxiv.org/abs/2204.11015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.11015v1)
- **Published**: 2022-04-23 08:11:33+00:00
- **Updated**: 2022-04-23 08:11:33+00:00
- **Authors**: Baorui Ma, Yu-Shen Liu, Matthias Zwicker, Zhizhong Han
- **Comment**: To appear at CVPR2022. Project page:this https URL
  https://mabaorui.github.io/PredictableContextPrior_page/
- **Journal**: None
- **Summary**: Surface reconstruction from point clouds is vital for 3D computer vision. State-of-the-art methods leverage large datasets to first learn local context priors that are represented as neural network-based signed distance functions (SDFs) with some parameters encoding the local contexts. To reconstruct a surface at a specific query location at inference time, these methods then match the local reconstruction target by searching for the best match in the local prior space (by optimizing the parameters encoding the local context) at the given query location. However, this requires the local context prior to generalize to a wide variety of unseen target regions, which is hard to achieve. To resolve this issue, we introduce Predictive Context Priors by learning Predictive Queries for each specific point cloud at inference time. Specifically, we first train a local context prior using a large point cloud dataset similar to previous techniques. For surface reconstruction at inference time, however, we specialize the local context prior into our Predictive Context Prior by learning Predictive Queries, which predict adjusted spatial query locations as displacements of the original locations. This leads to a global SDF that fits the specific point cloud the best. Intuitively, the query prediction enables us to flexibly search the learned local context prior over the entire prior space, rather than being restricted to the fixed query locations, and this improves the generalizability. Our method does not require ground truth signed distances, normals, or any additional procedure of signed distance fusion across overlapping regions. Our experimental results in surface reconstruction for single shapes or complex scenes show significant improvements over the state-of-the-art under widely used benchmarks.



### Exploring Negatives in Contrastive Learning for Unpaired Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2204.11018v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.11018v2)
- **Published**: 2022-04-23 08:31:18+00:00
- **Updated**: 2022-07-13 06:44:26+00:00
- **Authors**: Yupei Lin, Sen Zhang, Tianshui Chen, Yongyi Lu, Guangping Li, Yukai Shi
- **Comment**: Accepted at ACM MM 2022; Contrastive learning shows better effects by
  adopting a pruning constraint for negatives
- **Journal**: None
- **Summary**: Unpaired image-to-image translation aims to find a mapping between the source domain and the target domain. To alleviate the problem of the lack of supervised labels for the source images, cycle-consistency based methods have been proposed for image structure preservation by assuming a reversible relationship between unpaired images. However, this assumption only uses limited correspondence between image pairs. Recently, contrastive learning (CL) has been used to further investigate the image correspondence in unpaired image translation by using patch-based positive/negative learning. Patch-based contrastive routines obtain the positives by self-similarity computation and recognize the rest patches as negatives. This flexible learning paradigm obtains auxiliary contextualized information at a low cost. As the negatives own an impressive sample number, with curiosity, we make an investigation based on a question: are all negatives necessary for feature contrastive learning? Unlike previous CL approaches that use negatives as much as possible, in this paper, we study the negatives from an information-theoretic perspective and introduce a new negative Pruning technology for Unpaired image-to-image Translation (PUT) by sparsifying and ranking the patches. The proposed algorithm is efficient, flexible and enables the model to learn essential information between corresponding patches stably. By putting quality over quantity, only a few negative patches are required to achieve better results. Lastly, we validate the superiority, stability, and versatility of our model through comparative experiments.



### Indoor simultaneous localization and mapping based on fringe projection profilometry
- **Arxiv ID**: http://arxiv.org/abs/2204.11020v1
- **DOI**: 10.1364/OE.483667
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.11020v1)
- **Published**: 2022-04-23 08:35:58+00:00
- **Updated**: 2022-04-23 08:35:58+00:00
- **Authors**: Yang Zhao, Kai Zhang, Haotian Yu, Yi Zhang, Dongliang Zheng, Jing Han
- **Comment**: None
- **Journal**: None
- **Summary**: Simultaneous Localization and Mapping (SLAM) plays an important role in outdoor and indoor applications ranging from autonomous driving to indoor robotics. Outdoor SLAM has been widely used with the assistance of LiDAR or GPS. For indoor applications, the LiDAR technique does not satisfy the accuracy requirement and the GPS signals will be lost. An accurate and efficient scene sensing technique is required for indoor SLAM. As the most promising 3D sensing technique, the opportunities for indoor SLAM with fringe projection profilometry (FPP) systems are obvious, but methods to date have not fully leveraged the accuracy and speed of sensing that such systems offer. In this paper, we propose a novel FPP-based indoor SLAM method based on the coordinate transformation relationship of FPP, where the 2D-to-3D descriptor-assisted is used for mapping and localization. The correspondences generated by matching descriptors are used for fast and accurate mapping, and the transform estimation between the 2D and 3D descriptors is used to localize the sensor. The provided experimental results demonstrate that the proposed indoor SLAM can achieve the localization and mapping accuracy around one millimeter.



### Towards Data-Free Model Stealing in a Hard Label Setting
- **Arxiv ID**: http://arxiv.org/abs/2204.11022v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.11022v1)
- **Published**: 2022-04-23 08:44:51+00:00
- **Updated**: 2022-04-23 08:44:51+00:00
- **Authors**: Sunandini Sanyal, Sravanti Addepalli, R. Venkatesh Babu
- **Comment**: CVPR 2022, Project Page: https://sites.google.com/view/dfms-hl
- **Journal**: None
- **Summary**: Machine learning models deployed as a service (MLaaS) are susceptible to model stealing attacks, where an adversary attempts to steal the model within a restricted access framework. While existing attacks demonstrate near-perfect clone-model performance using softmax predictions of the classification network, most of the APIs allow access to only the top-1 labels. In this work, we show that it is indeed possible to steal Machine Learning models by accessing only top-1 predictions (Hard Label setting) as well, without access to model gradients (Black-Box setting) or even the training dataset (Data-Free setting) within a low query budget. We propose a novel GAN-based framework that trains the student and generator in tandem to steal the model effectively while overcoming the challenge of the hard label setting by utilizing gradients of the clone network as a proxy to the victim's gradients. We propose to overcome the large query costs associated with a typical Data-Free setting by utilizing publicly available (potentially unrelated) datasets as a weak image prior. We additionally show that even in the absence of such data, it is possible to achieve state-of-the-art results within a low query budget using synthetically crafted samples. We are the first to demonstrate the scalability of Model Stealing in a restricted access setting on a 100 class dataset as well.



### VISTA: Vision Transformer enhanced by U-Net and Image Colorfulness Frame Filtration for Automatic Retail Checkout
- **Arxiv ID**: http://arxiv.org/abs/2204.11024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.11024v1)
- **Published**: 2022-04-23 08:54:28+00:00
- **Updated**: 2022-04-23 08:54:28+00:00
- **Authors**: Md. Istiak Hossain Shihab, Nazia Tasnim, Hasib Zunair, Labiba Kanij Rupty, Nabeel Mohammed
- **Comment**: accepted at AI City Challenge workshop - CVPR 2022
- **Journal**: None
- **Summary**: Multi-class product counting and recognition identifies product items from images or videos for automated retail checkout. The task is challenging due to the real-world scenario of occlusions where product items overlap, fast movement in the conveyor belt, large similarity in overall appearance of the items being scanned, novel products, and the negative impact of misidentifying items. Further, there is a domain bias between training and test sets, specifically, the provided training dataset consists of synthetic images and the test set videos consist of foreign objects such as hands and tray. To address these aforementioned issues, we propose to segment and classify individual frames from a video sequence. The segmentation method consists of a unified single product item- and hand-segmentation followed by entropy masking to address the domain bias problem. The multi-class classification method is based on Vision Transformers (ViT). To identify the frames with target objects, we utilize several image processing methods and propose a custom metric to discard frames not having any product items. Combining all these mechanisms, our best system achieves 3rd place in the AI City Challenge 2022 Track 4 with an F1 score of 0.4545. Code will be available at



### Learning by Erasing: Conditional Entropy based Transferable Out-Of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.11041v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.11041v2)
- **Published**: 2022-04-23 10:19:58+00:00
- **Updated**: 2023-08-07 22:47:07+00:00
- **Authors**: Meng Xing, Zhiyong Feng, Yong Su, Changjae Oh
- **Comment**: update new experimental results
- **Journal**: None
- **Summary**: Out-of-distribution (OOD) detection is essential to handle the distribution shifts between training and test scenarios. For a new in-distribution (ID) dataset, existing methods require retraining to capture the dataset-specific feature representation or data distribution. In this paper, we propose a deep generative models (DGM) based transferable OOD detection method, which is unnecessary to retrain on a new ID dataset. We design an image erasing strategy to equip exclusive conditional entropy distribution for each ID dataset, which determines the discrepancy of DGM's posteriori ucertainty distribution on different ID datasets. Owing to the powerful representation capacity of convolutional neural networks, the proposed model trained on complex dataset can capture the above discrepancy between ID datasets without retraining and thus achieve transferable OOD detection. We validate the proposed method on five datasets and verity that ours achieves comparable performance to the state-of-the-art group based OOD detection methods that need to be retrained to deploy on new ID datasets. Our code is available at https://github.com/oOHCIOo/CETOOD.



### Investigating Neural Architectures by Synthetic Dataset Design
- **Arxiv ID**: http://arxiv.org/abs/2204.11045v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.11045v1)
- **Published**: 2022-04-23 10:50:52+00:00
- **Updated**: 2022-04-23 10:50:52+00:00
- **Authors**: Adrien Courtois, Jean-Michel Morel, Pablo Arias
- **Comment**: Accepted at the VDU2022 workshop hosted at CVPR2022
- **Journal**: None
- **Summary**: Recent years have seen the emergence of many new neural network structures (architectures and layers). To solve a given task, a network requires a certain set of abilities reflected in its structure. The required abilities depend on each task. There is so far no systematic study of the real capacities of the proposed neural structures. The question of what each structure can and cannot achieve is only partially answered by its performance on common benchmarks. Indeed, natural data contain complex unknown statistical cues. It is therefore impossible to know what cues a given neural structure is taking advantage of in such data. In this work, we sketch a methodology to measure the effect of each structure on a network's ability, by designing ad hoc synthetic datasets. Each dataset is tailored to assess a given ability and is reduced to its simplest form: each input contains exactly the amount of information needed to solve the task. We illustrate our methodology by building three datasets to evaluate each of the three following network properties: a) the ability to link local cues to distant inferences, b) the translation covariance and c) the ability to group pixels with the same characteristics and share information among them. Using a first simplified depth estimation dataset, we pinpoint a serious nonlocal deficit of the U-Net. We then evaluate how to resolve this limitation by embedding its structure with nonlocal layers, which allow computing complex features with long-range dependencies. Using a second dataset, we compare different positional encoding methods and use the results to further improve the U-Net on the depth estimation task. The third introduced dataset serves to demonstrate the need for self-attention-like mechanisms for resolving more realistic depth estimation tasks.



### Class Balanced PixelNet for Neurological Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.11048v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.11048v1)
- **Published**: 2022-04-23 10:57:54+00:00
- **Updated**: 2022-04-23 10:57:54+00:00
- **Authors**: Mobarakol Islam, Hongliang Ren
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an automatic brain tumor segmentation approach (e.g., PixelNet) using a pixel-level convolutional neural network (CNN). The model extracts feature from multiple convolutional layers and concatenate them to form a hyper-column where samples a modest number of pixels for optimization. Hyper-column ensures both local and global contextual information for pixel-wise predictors. The model confirms the statistical efficiency by sampling a few pixels in the training phase where spatial redundancy limits the information learning among the neighboring pixels in conventional pixel-level semantic segmentation approaches. Besides, label skewness in training data leads the convolutional model often converge to certain classes which is a common problem in the medical dataset. We deal with this problem by selecting an equal number of pixels for all the classes in sampling time. The proposed model has achieved promising results in brain tumor and ischemic stroke lesion segmentation datasets.



### Uncertain Label Correction via Auxiliary Action Unit Graphs for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.11053v2
- **DOI**: 10.1109/ICPR56361.2022.9956650
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.11053v2)
- **Published**: 2022-04-23 11:09:43+00:00
- **Updated**: 2022-12-14 10:42:14+00:00
- **Authors**: Yang Liu, Xingming Zhang, Janne Kauttonen, Guoying Zhao
- **Comment**: 7 pages, 7 figures, accecpted by ICPR 2022
- **Journal**: None
- **Summary**: High-quality annotated images are significant to deep facial expression recognition (FER) methods. However, uncertain labels, mostly existing in large-scale public datasets, often mislead the training process. In this paper, we achieve uncertain label correction of facial expressions using auxiliary action unit (AU) graphs, called ULC-AG. Specifically, a weighted regularization module is introduced to highlight valid samples and suppress category imbalance in every batch. Based on the latent dependency between emotions and AUs, an auxiliary branch using graph convolutional layers is added to extract the semantic information from graph topologies. Finally, a re-labeling strategy corrects the ambiguous annotations by comparing their feature similarities with semantic templates. Experiments show that our ULC-AG achieves 89.31% and 61.57% accuracy on RAF-DB and AffectNet datasets, respectively, outperforming the baseline and state-of-the-art methods.



### MLP-Hash: Protecting Face Templates via Hashing of Randomized Multi-Layer Perceptron
- **Arxiv ID**: http://arxiv.org/abs/2204.11054v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2204.11054v1)
- **Published**: 2022-04-23 11:18:22+00:00
- **Updated**: 2022-04-23 11:18:22+00:00
- **Authors**: Hatef Otroshi Shahreza, Vedrana Krivokuća Hahn, Sébastien Marcel
- **Comment**: None
- **Journal**: None
- **Summary**: Applications of face recognition systems for authentication purposes are growing rapidly. Although state-of-the-art (SOTA) face recognition systems have high recognition performance, the features which are extracted for each user and are stored in the system's database contain privacy-sensitive information. Accordingly, compromising this data would jeopardize users' privacy. In this paper, we propose a new cancelable template protection method, dubbed MLP-hash, which generates protected templates by passing the extracted features through a user-specific randomly-weighted multi-layer perceptron (MLP) and binarizing the MLP output. We evaluated the unlinkability, irreversibility, and recognition performance of our proposed biometric template protection method to fulfill the ISO/IEC 30136 standard requirements. Our experiments with SOTA face recognition systems on the MOBIO and LFW datasets show that our method has competitive performance with the BioHashing and IoM Hashing (IoM-GRP and IoM-URP) template protection algorithms. We provide an open-source implementation of all the experiments presented in this paper so that other researchers can verify our findings and build upon our work.



### Transformation Invariant Cancerous Tissue Classification Using Spatially Transformed DenseNet
- **Arxiv ID**: http://arxiv.org/abs/2204.11066v1
- **DOI**: 10.1109/ASET53988.2022.9734997
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.11066v1)
- **Published**: 2022-04-23 13:12:50+00:00
- **Updated**: 2022-04-23 13:12:50+00:00
- **Authors**: Omar Mahdi, Ali Bou Nassif
- **Comment**: None
- **Journal**: 2022 Advances in Science and Engineering Technology International
  Conferences (ASET)
- **Summary**: In this work, we introduce a spatially transformed DenseNet architecture for transformation invariant classification of cancer tissue. Our architecture increases the accuracy of the base DenseNet architecture while adding the ability to operate in a transformation invariant way while simultaneously being simpler than other models that try to provide some form of invariance.



### On Leveraging Variational Graph Embeddings for Open World Compositional Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.11848v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.11848v1)
- **Published**: 2022-04-23 13:30:08+00:00
- **Updated**: 2022-04-23 13:30:08+00:00
- **Authors**: Muhammad Umer Anwaar, Zhihui Pan, Martin Kleinsteuber
- **Comment**: Submitted to a conference
- **Journal**: None
- **Summary**: Humans are able to identify and categorize novel compositions of known concepts. The task in Compositional Zero-Shot learning (CZSL) is to learn composition of primitive concepts, i.e. objects and states, in such a way that even their novel compositions can be zero-shot classified. In this work, we do not assume any prior knowledge on the feasibility of novel compositions i.e.open-world setting, where infeasible compositions dominate the search space. We propose a Compositional Variational Graph Autoencoder (CVGAE) approach for learning the variational embeddings of the primitive concepts (nodes) as well as feasibility of their compositions (via edges). Such modelling makes CVGAE scalable to real-world application scenarios. This is in contrast to SOTA method, CGE, which is computationally very expensive. e.g.for benchmark C-GQA dataset, CGE requires 3.94 x 10^5 nodes, whereas CVGAE requires only 1323 nodes. We learn a mapping of the graph and image embeddings onto a common embedding space. CVGAE adopts a deep metric learning approach and learns a similarity metric in this space via bi-directional contrastive loss between projected graph and image embeddings. We validate the effectiveness of our approach on three benchmark datasets.We also demonstrate via an image retrieval task that the representations learnt by CVGAE are better suited for compositional generalization.



### Learning Shape Priors by Pairwise Comparison for Robust Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.11090v1
- **DOI**: 10.1109/ISBI48211.2021.9433936
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.11090v1)
- **Published**: 2022-04-23 15:29:34+00:00
- **Updated**: 2022-04-23 15:29:34+00:00
- **Authors**: Cong Xie, Hualuo Liu, Shilei Cao, Dong Wei, Kai Ma, Liansheng Wang, Yefeng Zheng
- **Comment**: IEEE ISBI 2021
- **Journal**: None
- **Summary**: Semantic segmentation is important in medical image analysis. Inspired by the strong ability of traditional image analysis techniques in capturing shape priors and inter-subject similarity, many deep learning (DL) models have been recently proposed to exploit such prior information and achieved robust performance. However, these two types of important prior information are usually studied separately in existing models. In this paper, we propose a novel DL model to model both type of priors within a single framework. Specifically, we introduce an extra encoder into the classic encoder-decoder structure to form a Siamese structure for the encoders, where one of them takes a target image as input (the image-encoder), and the other concatenates a template image and its foreground regions as input (the template-encoder). The template-encoder encodes the shape priors and appearance characteristics of each foreground class in the template image. A cosine similarity based attention module is proposed to fuse the information from both encoders, to utilize both types of prior information encoded by the template-encoder and model the inter-subject similarity for each foreground class. Extensive experiments on two public datasets demonstrate that our proposed method can produce superior performance to competing methods.



### Can domain adaptation make object recognition work for everyone?
- **Arxiv ID**: http://arxiv.org/abs/2204.11122v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.11122v1)
- **Published**: 2022-04-23 18:51:13+00:00
- **Updated**: 2022-04-23 18:51:13+00:00
- **Authors**: Viraj Prabhu, Ramprasaath R. Selvaraju, Judy Hoffman, Nikhil Naik
- **Comment**: Published at the L3D-IVU workshop at CVPR 2022
- **Journal**: None
- **Summary**: Despite the rapid progress in deep visual recognition, modern computer vision datasets significantly overrepresent the developed world and models trained on such datasets underperform on images from unseen geographies. We investigate the effectiveness of unsupervised domain adaptation (UDA) of such models across geographies at closing this performance gap. To do so, we first curate two shifts from existing datasets to study the Geographical DA problem, and discover new challenges beyond data distribution shift: context shift, wherein object surroundings may change significantly across geographies, and subpopulation shift, wherein the intra-category distributions may shift. We demonstrate the inefficacy of standard DA methods at Geographical DA, highlighting the need for specialized geographical adaptation solutions to address the challenge of making object recognition work for everyone.



### Towards Bundle Adjustment for Satellite Imaging via Quantum Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.11133v1
- **DOI**: None
- **Categories**: **quant-ph**, cs.CV, stat.ML, C.3; I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2204.11133v1)
- **Published**: 2022-04-23 19:33:14+00:00
- **Updated**: 2022-04-23 19:33:14+00:00
- **Authors**: Nico Piatkowski, Thore Gerlach, Romain Hugues, Rafet Sifa, Christian Bauckhage, Frederic Barbaresco
- **Comment**: None
- **Journal**: None
- **Summary**: Given is a set of images, where all images show views of the same area at different points in time and from different viewpoints. The task is the alignment of all images such that relevant information, e.g., poses, changes, and terrain, can be extracted from the fused image. In this work, we focus on quantum methods for keypoint extraction and feature matching, due to the demanding computational complexity of these sub-tasks. To this end, k-medoids clustering, kernel density clustering, nearest neighbor search, and kernel methods are investigated and it is explained how these methods can be re-formulated for quantum annealers and gate-based quantum computers. Experimental results obtained on digital quantum emulation hardware, quantum annealers, and quantum gate computers show that classical systems still deliver superior results. However, the proposed methods are ready for the current and upcoming generations of quantum computing devices which have the potential to outperform classical systems in the near future.



### A Novel Framework for Characterization of Tumor-Immune Spatial Relationships in Tumor Microenvironment
- **Arxiv ID**: http://arxiv.org/abs/2204.12283v3
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.12283v3)
- **Published**: 2022-04-23 21:00:30+00:00
- **Updated**: 2022-05-01 22:52:27+00:00
- **Authors**: Mahmudul Hasan, Jakub R. Kaczmarzyk, David Paredes, Lyanne Oblein, Jaymie Oentoro, Shahira Abousamra, Michael Horowitz, Dimitris Samaras, Chao Chen, Tahsin Kurc, Kenneth R. Shroyer, Joel Saltz
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the impact of tumor biology on the composition of nearby cells often requires characterizing the impact of biologically distinct tumor regions. Biomarkers have been developed to label biologically distinct tumor regions, but challenges arise because of differences in the spatial extent and distribution of differentially labeled regions. In this work, we present a framework for systematically investigating the impact of distinct tumor regions on cells near the tumor borders, accounting their cross spatial distributions. We apply the framework to multiplex immunohistochemistry (mIHC) studies of pancreatic cancer and show its efficacy in demonstrating how biologically different tumor regions impact the immune response in the tumor microenvironment. Furthermore, we show that the proposed framework can be extended to largescale whole slide image analysis.



### Supplementing Missing Visions via Dialog for Scene Graph Generations
- **Arxiv ID**: http://arxiv.org/abs/2204.11143v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.11143v1)
- **Published**: 2022-04-23 21:46:17+00:00
- **Updated**: 2022-04-23 21:46:17+00:00
- **Authors**: Ye Zhu, Xiaoguang Zhu, Yuzhang Shang, Zhenghao Zhao, Yan Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Most current AI systems rely on the premise that the input visual data are sufficient to achieve competitive performance in various computer vision tasks. However, the classic task setup rarely considers the challenging, yet common practical situations where the complete visual data may be inaccessible due to various reasons (e.g., restricted view range and occlusions). To this end, we investigate a computer vision task setting with incomplete visual input data. Specifically, we exploit the Scene Graph Generation (SGG) task with various levels of visual data missingness as input. While insufficient visual input intuitively leads to performance drop, we propose to supplement the missing visions via the natural language dialog interactions to better accomplish the task objective. We design a model-agnostic Supplementary Interactive Dialog (SI-Dial) framework that can be jointly learned with most existing models, endowing the current AI systems with the ability of question-answer interactions in natural language. We demonstrate the feasibility of such a task setting with missing visual input and the effectiveness of our proposed dialog module as the supplementary information source through extensive experiments and analysis, by achieving promising performance improvement over multiple baselines.



### Gabor is Enough: Interpretable Deep Denoising with a Gabor Synthesis Dictionary Prior
- **Arxiv ID**: http://arxiv.org/abs/2204.11146v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2204.11146v1)
- **Published**: 2022-04-23 22:21:54+00:00
- **Updated**: 2022-04-23 22:21:54+00:00
- **Authors**: Nikola Janjušević, Amirhossein Khalilian-Gourtani, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Image processing neural networks, natural and artificial, have a long history with orientation-selectivity, often described mathematically as Gabor filters. Gabor-like filters have been observed in the early layers of CNN classifiers and even throughout low-level image processing networks. In this work, we take this observation to the extreme and explicitly constrain the filters of a natural-image denoising CNN to be learned 2D real Gabor filters. Surprisingly, we find that the proposed network (GDLNet) can achieve near state-of-the-art denoising performance amongst popular fully convolutional neural networks, with only a fraction of the learned parameters. We further verify that this parameterization maintains the noise-level generalization (training vs. inference mismatch) characteristics of the base network, and investigate the contribution of individual Gabor filter parameters to the performance of the denoiser. We present positive findings for the interpretation of dictionary learning networks as performing accelerated sparse-coding via the importance of untied learned scale parameters between network layers. Our network's success suggests that representations used by low-level image processing CNNs can be as simple and interpretable as Gabor filterbanks.



