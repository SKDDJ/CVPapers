# Arxiv Papers in cs.CV on 2022-04-06
### Emphasis on the Minimization of False Negatives or False Positives in Binary Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.02526v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.02526v1)
- **Published**: 2022-04-06 00:33:40+00:00
- **Updated**: 2022-04-06 00:33:40+00:00
- **Authors**: Sanskriti Singh
- **Comment**: 5 pages, 6 figures
- **Journal**: None
- **Summary**: The minimization of specific cases in binary classification, such as false negatives or false positives, grows increasingly important as humans begin to implement more machine learning into current products. While there are a few methods to put a bias towards the reduction of specific cases, these methods aren't very effective, hence their minimal use in models. To this end, a new method is introduced to reduce the False Negatives or False positives without drastically changing the overall performance or F1 score of the model. This method involving the careful change to the real value of the input after pre-training the model. Presenting the results of this method being applied on various datasets, some being more complex than others. Through experimentation on multiple model architectures on these datasets, the best model was found. In all the models, an increase in the recall or precision, minimization of False Negatives or False Positives respectively, was shown without a large drop in F1 score.



### Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.02547v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2204.02547v1)
- **Published**: 2022-04-06 02:42:33+00:00
- **Updated**: 2022-04-06 02:42:33+00:00
- **Authors**: Wangbo Zhao, Kai Wang, Xiangxiang Chu, Fuzhao Xue, Xinchao Wang, Yang You
- **Comment**: Accepted to CVPR2022
- **Journal**: None
- **Summary**: Text-based video segmentation aims to segment the target object in a video based on a describing sentence. Incorporating motion information from optical flow maps with appearance and linguistic modalities is crucial yet has been largely ignored by previous work. In this paper, we design a method to fuse and align appearance, motion, and linguistic features to achieve accurate segmentation. Specifically, we propose a multi-modal video transformer, which can fuse and aggregate multi-modal and temporal features between frames. Furthermore, we design a language-guided feature fusion module to progressively fuse appearance and motion features in each feature level with guidance from linguistic features. Finally, a multi-modal alignment loss is proposed to alleviate the semantic gap between features from different modalities. Extensive experiments on A2D Sentences and J-HMDB Sentences verify the performance and the generalization ability of our method compared to the state-of-the-art methods.



### Style-Hallucinated Dual Consistency Learning for Domain Generalized Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.02548v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02548v2)
- **Published**: 2022-04-06 02:49:06+00:00
- **Updated**: 2022-07-19 13:24:11+00:00
- **Authors**: Yuyang Zhao, Zhun Zhong, Na Zhao, Nicu Sebe, Gim Hee Lee
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: In this paper, we study the task of synthetic-to-real domain generalized semantic segmentation, which aims to learn a model that is robust to unseen real-world scenes using only synthetic data. The large domain shift between synthetic and real-world data, including the limited source environmental variations and the large distribution gap between synthetic and real-world data, significantly hinders the model performance on unseen real-world scenes. In this work, we propose the Style-HAllucinated Dual consistEncy learning (SHADE) framework to handle such domain shift. Specifically, SHADE is constructed based on two consistency constraints, Style Consistency (SC) and Retrospection Consistency (RC). SC enriches the source situations and encourages the model to learn consistent representation across style-diversified samples. RC leverages real-world knowledge to prevent the model from overfitting to synthetic data and thus largely keeps the representation consistent between the synthetic and real-world models. Furthermore, we present a novel style hallucination module (SHM) to generate style-diversified samples that are essential to consistency learning. SHM selects basis styles from the source distribution, enabling the model to dynamically generate diverse and realistic samples during training. Experiments show that our SHADE yields significant improvement and outperforms state-of-the-art methods by 5.05% and 8.35% on the average mIoU of three real-world datasets on single- and multi-source settings, respectively.



### RODD: A Self-Supervised Approach for Robust Out-of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.02553v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02553v3)
- **Published**: 2022-04-06 03:05:58+00:00
- **Updated**: 2022-10-15 00:41:28+00:00
- **Authors**: Umar Khalid, Ashkan Esmaeili, Nazmul Karim, Nazanin Rahnavard
- **Comment**: Accepted in CVPR Art of Robustness Workshop Proceedings
- **Journal**: None
- **Summary**: Recent studies have addressed the concern of detecting and rejecting the out-of-distribution (OOD) samples as a major challenge in the safe deployment of deep learning (DL) models. It is desired that the DL model should only be confident about the in-distribution (ID) data which reinforces the driving principle of the OOD detection. In this paper, we propose a simple yet effective generalized OOD detection method independent of out-of-distribution datasets. Our approach relies on self-supervised feature learning of the training samples, where the embeddings lie on a compact low-dimensional space. Motivated by the recent studies that show self-supervised adversarial contrastive learning helps robustify the model, we empirically show that a pre-trained model with self-supervised contrastive learning yields a better model for uni-dimensional feature learning in the latent space. The method proposed in this work referred to as RODD outperforms SOTA detection performance on an extensive suite of benchmark datasets on OOD detection tasks. On the CIFAR-100 benchmarks, RODD achieves a 26.97 $\%$ lower false-positive rate (FPR@95) compared to SOTA methods.



### MixFormer: Mixing Features across Windows and Dimensions
- **Arxiv ID**: http://arxiv.org/abs/2204.02557v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02557v2)
- **Published**: 2022-04-06 03:13:50+00:00
- **Updated**: 2022-04-12 03:02:07+00:00
- **Authors**: Qiang Chen, Qiman Wu, Jian Wang, Qinghao Hu, Tao Hu, Errui Ding, Jian Cheng, Jingdong Wang
- **Comment**: CVPR2022 Oral
- **Journal**: None
- **Summary**: While local-window self-attention performs notably in vision tasks, it suffers from limited receptive field and weak modeling capability issues. This is mainly because it performs self-attention within non-overlapped windows and shares weights on the channel dimension. We propose MixFormer to find a solution. First, we combine local-window self-attention with depth-wise convolution in a parallel design, modeling cross-window connections to enlarge the receptive fields. Second, we propose bi-directional interactions across branches to provide complementary clues in the channel and spatial dimensions. These two designs are integrated to achieve efficient feature mixing among windows and dimensions. Our MixFormer provides competitive results on image classification with EfficientNet and shows better results than RegNet and Swin Transformer. Performance in downstream tasks outperforms its alternatives by significant margins with less computational costs in 5 dense prediction tasks on MS COCO, ADE20k, and LVIS. Code is available at \url{https://github.com/PaddlePaddle/PaddleClas}.



### Gait Recognition in the Wild with Dense 3D Representations and A Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2204.02569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02569v1)
- **Published**: 2022-04-06 03:54:06+00:00
- **Updated**: 2022-04-06 03:54:06+00:00
- **Authors**: Jinkai Zheng, Xinchen Liu, Wu Liu, Lingxiao He, Chenggang Yan, Tao Mei
- **Comment**: 16 pages, 11 figures, CVPR 2022 accepted, project page:
  https://gait3d.github.io/
- **Journal**: None
- **Summary**: Existing studies for gait recognition are dominated by 2D representations like the silhouette or skeleton of the human body in constrained scenes. However, humans live and walk in the unconstrained 3D space, so projecting the 3D human body onto the 2D plane will discard a lot of crucial information like the viewpoint, shape, and dynamics for gait recognition. Therefore, this paper aims to explore dense 3D representations for gait recognition in the wild, which is a practical yet neglected problem. In particular, we propose a novel framework to explore the 3D Skinned Multi-Person Linear (SMPL) model of the human body for gait recognition, named SMPLGait. Our framework has two elaborately-designed branches of which one extracts appearance features from silhouettes, the other learns knowledge of 3D viewpoints and shapes from the 3D SMPL model. In addition, due to the lack of suitable datasets, we build the first large-scale 3D representation-based gait recognition dataset, named Gait3D. It contains 4,000 subjects and over 25,000 sequences extracted from 39 cameras in an unconstrained indoor scene. More importantly, it provides 3D SMPL models recovered from video frames which can provide dense 3D information of body shape, viewpoint, and dynamics. Based on Gait3D, we comprehensively compare our method with existing gait recognition approaches, which reflects the superior performance of our framework and the potential of 3D representations for gait recognition in the wild. The code and dataset are available at https://gait3d.github.io.



### Detecting key Soccer match events to create highlights using Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2204.02573v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.02573v1)
- **Published**: 2022-04-06 04:28:27+00:00
- **Updated**: 2022-04-06 04:28:27+00:00
- **Authors**: Narayana Darapaneni, Prashant Kumar, Nikhil Malhotra, Vigneswaran Sundaramurthy, Abhaya Thakur, Shivam Chauhan, Krishna Chaitanya Thangeda, Anwesh Reddy Paduri
- **Comment**: None
- **Journal**: None
- **Summary**: The research and data science community has been fascinated with the development of automatic systems for the detection of key events in a video. Special attention in this field is given to sports video analytics which could help in identifying key events during a match and help in preparing a strategy for the games going forward. For this paper, we have chosen Football (soccer) as a sport where we would want to create highlights for a given match video, through a computer vision model that aims to identify important events in a Soccer match to create highlights of the match. We built the models based on Faster RCNN and YoloV5 architectures and noticed that for the amount of data we used for training Faster RCNN did better than YoloV5 in detecting the events in the match though it was much slower. Within Faster RCNN using ResNet50 as a base model gave a better class accuracy of 95.5% as compared to 92% with VGG16 as base model completely outperforming YoloV5 for our training dataset. We tested with an original video of size 23 minutes and our model could reduce it to 4:50 minutes of highlights capturing almost all important events in the match.



### FocalClick: Towards Practical Interactive Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.02574v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02574v2)
- **Published**: 2022-04-06 04:32:01+00:00
- **Updated**: 2022-04-17 14:13:14+00:00
- **Authors**: Xi Chen, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, Hengshuang Zhao
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: Interactive segmentation allows users to extract target masks by making positive/negative clicks. Although explored by many previous works, there is still a gap between academic approaches and industrial needs: first, existing models are not efficient enough to work on low power devices; second, they perform poorly when used to refine preexisting masks as they could not avoid destroying the correct part. FocalClick solves both issues at once by predicting and updating the mask in localized areas. For higher efficiency, we decompose the slow prediction on the entire image into two fast inferences on small crops: a coarse segmentation on the Target Crop, and a local refinement on the Focus Crop. To make the model work with preexisting masks, we formulate a sub-task termed Interactive Mask Correction, and propose Progressive Merge as the solution. Progressive Merge exploits morphological information to decide where to preserve and where to update, enabling users to refine any preexisting mask effectively. FocalClick achieves competitive results against SOTA methods with significantly smaller FLOPs. It also shows significant superiority when making corrections on preexisting masks. Code and data will be released at github.com/XavierCHEN34/ClickSEG



### Banana Sub-Family Classification and Quality Prediction using Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2204.02581v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.02581v1)
- **Published**: 2022-04-06 05:06:51+00:00
- **Updated**: 2022-04-06 05:06:51+00:00
- **Authors**: Narayana Darapaneni, Arjun Tanndalam, Mohit Gupta, Neeta Taneja, Prabu Purushothaman, Swati Eswar, Anwesh Reddy Paduri, Thangaselvi Arichandrapandian
- **Comment**: None
- **Journal**: None
- **Summary**: India is the second largest producer of fruits and vegetables in the world, and one of the largest consumers of fruits like Banana, Papaya and Mangoes through retail and ecommerce giants like BigBasket, Grofers and Amazon Fresh. However, adoption of technology in supply chain and retail stores is still low and there is a great potential to adopt computer-vision based technology for identification and classification of fruits. We have chosen banana fruit to build a computer vision based model to carry out the following three use-cases (a) Identify Banana from a given image (b) Determine sub-family or variety of Banana (c) Determine the quality of Banana. Successful execution of these use-cases using computer-vision model would greatly help with overall inventory management automation, quality control, quick and efficient weighing and billing which all are manual labor intensive currently. In this work, we suggest a machine learning pipeline that combines the ideas of CNNs, transfer learning, and data augmentation towards improving Banana fruit sub family and quality image classification. We have built a basic CNN and then went on to tune a MobileNet Banana classification model using a combination of self-curated and publicly-available dataset of 3064 images. The results show an overall 93.4% and 100% accuracy for sub-family/variety and for quality test classifications respectively.



### SqueezeNeRF: Further factorized FastNeRF for memory-efficient inference
- **Arxiv ID**: http://arxiv.org/abs/2204.02585v3
- **DOI**: 10.1109/CVPRW56347.2022.00307
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02585v3)
- **Published**: 2022-04-06 05:19:47+00:00
- **Updated**: 2022-10-07 07:02:46+00:00
- **Authors**: Krishna Wadhwani, Tamaki Kojima
- **Comment**: 9 pages, 3 figures, 5 tables. Presented in the "5th Efficient Deep
  Learning for Computer Vision" CVPR 2022 Workshop"
- **Journal**: None
- **Summary**: Neural Radiance Fields (NeRF) has emerged as the state-of-the-art method for novel view generation of complex scenes, but is very slow during inference. Recently, there have been multiple works on speeding up NeRF inference, but the state of the art methods for real-time NeRF inference rely on caching the neural network output, which occupies several giga-bytes of disk space that limits their real-world applicability. As caching the neural network of original NeRF network is not feasible, Garbin et al. proposed "FastNeRF" which factorizes the problem into 2 sub-networks - one which depends only on the 3D coordinate of a sample point and one which depends only on the 2D camera viewing direction. Although this factorization enables them to reduce the cache size and perform inference at over 200 frames per second, the memory overhead is still substantial. In this work, we propose SqueezeNeRF, which is more than 60 times memory-efficient than the sparse cache of FastNeRF and is still able to render at more than 190 frames per second on a high spec GPU during inference.



### Learning to Anticipate Future with Dynamic Context Removal
- **Arxiv ID**: http://arxiv.org/abs/2204.02587v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02587v2)
- **Published**: 2022-04-06 05:24:28+00:00
- **Updated**: 2022-06-27 15:28:36+00:00
- **Authors**: Xinyu Xu, Yong-Lu Li, Cewu Lu
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Anticipating future events is an essential feature for intelligent systems and embodied AI. However, compared to the traditional recognition task, the uncertainty of future and reasoning ability requirement make the anticipation task very challenging and far beyond solved. In this filed, previous methods usually care more about the model architecture design or but few attention has been put on how to train an anticipation model with a proper learning policy. To this end, in this work, we propose a novel training scheme called Dynamic Context Removal (DCR), which dynamically schedules the visibility of observed future in the learning procedure. It follows the human-like curriculum learning process, i.e., gradually removing the event context to increase the anticipation difficulty till satisfying the final anticipation target. Our learning scheme is plug-and-play and easy to integrate any reasoning model including transformer and LSTM, with advantages in both effectiveness and efficiency. In extensive experiments, the proposed method achieves state-of-the-art on four widely-used benchmarks. Our code and models are publicly released at https://github.com/AllenXuuu/DCR.



### Contextual Attention Mechanism, SRGAN Based Inpainting System for Eliminating Interruptions from Images
- **Arxiv ID**: http://arxiv.org/abs/2204.02591v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.02591v1)
- **Published**: 2022-04-06 05:51:04+00:00
- **Updated**: 2022-04-06 05:51:04+00:00
- **Authors**: Narayana Darapaneni, Vaibhav Kherde, Kameswara Rao, Deepali Nikam, Swanand Katdare, Anima Shukla, Anagha Lomate, Anwesh Reddy Paduri
- **Comment**: None
- **Journal**: None
- **Summary**: The new alternative is to use deep learning to inpaint any image by utilizing image classification and computer vision techniques. In general, image inpainting is a task of recreating or reconstructing any broken image which could be a photograph or oil/acrylic painting. With the advancement in the field of Artificial Intelligence, this topic has become popular among AI enthusiasts. With our approach, we propose an initial end-to-end pipeline for inpainting images using a complete Machine Learning approach instead of a conventional application-based approach. We first use the YOLO model to automatically identify and localize the object we wish to remove from the image. Using the result obtained from the model we can generate a mask for the same. After this, we provide the masked image and original image to the GAN model which uses the Contextual Attention method to fill in the region. It consists of two generator networks and two discriminator networks and is also called a coarse-to-fine network structure. The two generators use fully convolutional networks while the global discriminator gets hold of the entire image as input while the local discriminator gets the grip of the filled region as input. The contextual Attention mechanism is proposed to effectively borrow the neighbor information from distant spatial locations for reconstructing the missing pixels. The third part of our implementation uses SRGAN to resolve the inpainted image back to its original size. Our work is inspired by the paper Free-Form Image Inpainting with Gated Convolution and Generative Image Inpainting with Contextual Attention.



### Fine-Grained Predicates Learning for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2204.02597v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02597v2)
- **Published**: 2022-04-06 06:20:09+00:00
- **Updated**: 2022-04-08 00:43:13+00:00
- **Authors**: Xinyu Lyu, Lianli Gao, Yuyu Guo, Zhou Zhao, Hao Huang, Heng Tao Shen, Jingkuan Song
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of current Scene Graph Generation models is severely hampered by some hard-to-distinguish predicates, e.g., "woman-on/standing on/walking on-beach" or "woman-near/looking at/in front of-child". While general SGG models are prone to predict head predicates and existing re-balancing strategies prefer tail categories, none of them can appropriately handle these hard-to-distinguish predicates. To tackle this issue, inspired by fine-grained image classification, which focuses on differentiating among hard-to-distinguish object classes, we propose a method named Fine-Grained Predicates Learning (FGPL) which aims at differentiating among hard-to-distinguish predicates for Scene Graph Generation task. Specifically, we first introduce a Predicate Lattice that helps SGG models to figure out fine-grained predicate pairs. Then, utilizing the Predicate Lattice, we propose a Category Discriminating Loss and an Entity Discriminating Loss, which both contribute to distinguishing fine-grained predicates while maintaining learned discriminatory power over recognizable ones. The proposed model-agnostic strategy significantly boosts the performances of three benchmark models (Transformer, VCTree, and Motif) by 22.8\%, 24.1\% and 21.7\% of Mean Recall (mR@100) on the Predicate Classification sub-task, respectively. Our model also outperforms state-of-the-art methods by a large margin (i.e., 6.1\%, 4.6\%, and 3.2\% of Mean Recall (mR@100)) on the Visual Genome dataset.



### Face recognition in a transformed domain
- **Arxiv ID**: http://arxiv.org/abs/2204.02608v1
- **DOI**: 10.1109/CCST.2003.1297575
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02608v1)
- **Published**: 2022-04-06 06:36:57+00:00
- **Updated**: 2022-04-06 06:36:57+00:00
- **Authors**: Marcos Faundez-Zanuy
- **Comment**: 9 pages, published in IEEE 37th Annual 2003 International Carnahan
  Conference on Security Technology, 2003. Proceedings. 14-16 Oct. 2003 Taipei
  (Taiwan)
- **Journal**: IEEE 37th Annual 2003 International Carnahan Conference on
  Security Technology (ICCST), 2003
- **Summary**: This paper proposes the use of a discrete cosine transform (DCT) instead of the eigenfaces method (Karhunen-Loeve Transform) for biometric identification based on frontal face images. Experimental results show better recognition accuracies and reduced computational burden. This paper includes results with different classifiers and a combination of them.



### Cloning Outfits from Real-World Images to 3D Characters for Generalizable Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2204.02611v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02611v2)
- **Published**: 2022-04-06 06:41:08+00:00
- **Updated**: 2022-04-07 08:39:59+00:00
- **Authors**: Yanan Wang, Xuezhi Liang, Shengcai Liao
- **Comment**: The paper is accepted by CVPR 2022, including the appendix
- **Journal**: None
- **Summary**: Recently, large-scale synthetic datasets are shown to be very useful for generalizable person re-identification. However, synthesized persons in existing datasets are mostly cartoon-like and in random dress collocation, which limits their performance. To address this, in this work, an automatic approach is proposed to directly clone the whole outfits from real-world person images to virtual 3D characters, such that any virtual person thus created will appear very similar to its real-world counterpart. Specifically, based on UV texture mapping, two cloning methods are designed, namely registered clothes mapping and homogeneous cloth expansion. Given clothes keypoints detected on person images and labeled on regular UV maps with clear clothes structures, registered mapping applies perspective homography to warp real-world clothes to the counterparts on the UV map. As for invisible clothes parts and irregular UV maps, homogeneous expansion segments a homogeneous area on clothes as a realistic cloth pattern or cell, and expand the cell to fill the UV map. Furthermore, a similarity-diversity expansion strategy is proposed, by clustering person images, sampling images per cluster, and cloning outfits for 3D character generation. This way, virtual persons can be scaled up densely in visual similarity to challenge model learning, and diversely in population to enrich sample distribution. Finally, by rendering the cloned characters in Unity3D scenes, a more realistic virtual dataset called ClonedPerson is created, with 5,621 identities and 887,766 images. Experimental results show that the model trained on ClonedPerson has a better generalization performance, superior to that trained on other popular real-world and synthetic person re-identification datasets. The ClonedPerson project is available at https://github.com/Yanan-Wang-cs/ClonedPerson.



### Towards Robust Adaptive Object Detection under Noisy Annotations
- **Arxiv ID**: http://arxiv.org/abs/2204.02620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02620v1)
- **Published**: 2022-04-06 07:02:37+00:00
- **Updated**: 2022-04-06 07:02:37+00:00
- **Authors**: Xinyu Liu, Wuyang Li, Qiushi Yang, Baopu Li, Yixuan Yuan
- **Comment**: CVPR-2022 Version
- **Journal**: None
- **Summary**: Domain Adaptive Object Detection (DAOD) models a joint distribution of images and labels from an annotated source domain and learns a domain-invariant transformation to estimate the target labels with the given target domain images. Existing methods assume that the source domain labels are completely clean, yet large-scale datasets often contain error-prone annotations due to instance ambiguity, which may lead to a biased source distribution and severely degrade the performance of the domain adaptive detector de facto. In this paper, we represent the first effort to formulate noisy DAOD and propose a Noise Latent Transferability Exploration (NLTE) framework to address this issue. It is featured with 1) Potential Instance Mining (PIM), which leverages eligible proposals to recapture the miss-annotated instances from the background; 2) Morphable Graph Relation Module (MGRM), which models the adaptation feasibility and transition probability of noisy samples with relation matrices; 3) Entropy-Aware Gradient Reconcilement (EAGR), which incorporates the semantic information into the discrimination process and enforces the gradients provided by noisy and clean samples to be consistent towards learning domain-invariant representations. A thorough evaluation on benchmark DAOD datasets with noisy source annotations validates the effectiveness of NLTE. In particular, NLTE improves the mAP by 8.4\% under 60\% corrupted annotations and even approaches the ideal upper bound of training on a clean source dataset.



### IterVM: Iterative Vision Modeling Module for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.02630v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02630v1)
- **Published**: 2022-04-06 07:19:28+00:00
- **Updated**: 2022-04-06 07:19:28+00:00
- **Authors**: Xiaojie Chu, Yongtao Wang
- **Comment**: Accepted by ICPR2022
- **Journal**: None
- **Summary**: Scene text recognition (STR) is a challenging problem due to the imperfect imagery conditions in natural images. State-of-the-art methods utilize both visual cues and linguistic knowledge to tackle this challenging problem. Specifically, they propose iterative language modeling module (IterLM) to repeatedly refine the output sequence from the visual modeling module (VM). Though achieving promising results, the vision modeling module has become the performance bottleneck of these methods. In this paper, we newly propose iterative vision modeling module (IterVM) to further improve the STR accuracy. Specifically, the first VM directly extracts multi-level features from the input image, and the following VMs re-extract multi-level features from the input image and fuse them with the high-level (i.e., the most semantic one) feature extracted by the previous VM. By combining the proposed IterVM with iterative language modeling module, we further propose a powerful scene text recognizer called IterNet. Extensive experiments demonstrate that the proposed IterVM can significantly improve the scene text recognition accuracy, especially on low-quality scene text images. Moreover, the proposed scene text recognizer IterNet achieves new state-of-the-art results on several public benchmarks. Codes will be available at https://github.com/VDIGPKU/IterNet.



### Super-resolved multi-temporal segmentation with deep permutation-invariant networks
- **Arxiv ID**: http://arxiv.org/abs/2204.02631v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.02631v1)
- **Published**: 2022-04-06 07:19:51+00:00
- **Updated**: 2022-04-06 07:19:51+00:00
- **Authors**: Diego Valsesia, Enrico Magli
- **Comment**: IGARSS 2022
- **Journal**: None
- **Summary**: Multi-image super-resolution from multi-temporal satellite acquisitions of a scene has recently enjoyed great success thanks to new deep learning models. In this paper, we go beyond classic image reconstruction at a higher resolution by studying a super-resolved inference problem, namely semantic segmentation at a spatial resolution higher than the one of sensing platform. We expand upon recently proposed models exploiting temporal permutation invariance with a multi-resolution fusion module able to infer the rich semantic information needed by the segmentation task. The model presented in this paper has recently won the AI4EO challenge on Enhanced Sentinel 2 Agriculture.



### The Swiss Army Knife for Image-to-Image Translation: Multi-Task Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2204.02641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02641v1)
- **Published**: 2022-04-06 07:52:06+00:00
- **Updated**: 2022-04-06 07:52:06+00:00
- **Authors**: Julia Wolleb, Robin Sandkühler, Florentin Bieder, Philippe C. Cattin
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, diffusion models were applied to a wide range of image analysis tasks. We build on a method for image-to-image translation using denoising diffusion implicit models and include a regression problem and a segmentation problem for guiding the image generation to the desired output. The main advantage of our approach is that the guidance during the denoising process is done by an external gradient. Consequently, the diffusion model does not need to be retrained for the different tasks on the same dataset. We apply our method to simulate the aging process on facial photos using a regression task, as well as on a brain magnetic resonance (MR) imaging dataset for the simulation of brain tumor growth. Furthermore, we use a segmentation model to inpaint tumors at the desired location in healthy slices of brain MR images. We achieve convincing results for all problems.



### CAIPI in Practice: Towards Explainable Interactive Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.02661v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.02661v2)
- **Published**: 2022-04-06 08:21:34+00:00
- **Updated**: 2022-05-31 06:07:48+00:00
- **Authors**: Emanuel Slany, Yannik Ott, Stephan Scheele, Jan Paulus, Ute Schmid
- **Comment**: Manuscript accepted at IFIP AIAI 2022, correct typo in Discussion
- **Journal**: None
- **Summary**: Would you trust physicians if they cannot explain their decisions to you? Medical diagnostics using machine learning gained enormously in importance within the last decade. However, without further enhancements many state-of-the-art machine learning methods are not suitable for medical application. The most important reasons are insufficient data set quality and the black-box behavior of machine learning algorithms such as Deep Learning models. Consequently, end-users cannot correct the model's decisions and the corresponding explanations. The latter is crucial for the trustworthiness of machine learning in the medical domain. The research field explainable interactive machine learning searches for methods that address both shortcomings. This paper extends the explainable and interactive CAIPI algorithm and provides an interface to simplify human-in-the-loop approaches for image classification. The interface enables the end-user (1) to investigate and (2) to correct the model's prediction and explanation, and (3) to influence the data set quality. After CAIPI optimization with only a single counterexample per iteration, the model achieves an accuracy of $97.48\%$ on the Medical MNIST and $95.02\%$ on the Fashion MNIST. This accuracy is approximately equal to state-of-the-art Deep Learning optimization procedures. Besides, CAIPI reduces the labeling effort by approximately $80\%$.



### Towards An End-to-End Framework for Flow-Guided Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2204.02663v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.02663v2)
- **Published**: 2022-04-06 08:24:47+00:00
- **Updated**: 2022-04-07 13:35:40+00:00
- **Authors**: Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, Ming-Ming Cheng
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Optical flow, which captures motion information across frames, is exploited in recent video inpainting methods through propagating pixels along its trajectories. However, the hand-crafted flow-based processes in these methods are applied separately to form the whole inpainting pipeline. Thus, these methods are less efficient and rely heavily on the intermediate results from earlier stages. In this paper, we propose an End-to-End framework for Flow-Guided Video Inpainting (E$^2$FGVI) through elaborately designed three trainable modules, namely, flow completion, feature propagation, and content hallucination modules. The three modules correspond with the three stages of previous flow-based methods but can be jointly optimized, leading to a more efficient and effective inpainting process. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods both qualitatively and quantitatively and shows promising efficiency. The code is available at https://github.com/MCG-NKU/E2FGVI.



### Follow My Eye: Using Gaze to Supervise Computer-Aided Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2204.02976v1
- **DOI**: 10.1109/TMI.2022.3146973
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.02976v1)
- **Published**: 2022-04-06 08:31:05+00:00
- **Updated**: 2022-04-06 08:31:05+00:00
- **Authors**: Sheng Wang, Xi Ouyang, Tianming Liu, Qian Wang, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: When deep neural network (DNN) was first introduced to the medical image analysis community, researchers were impressed by its performance. However, it is evident now that a large number of manually labeled data is often a must to train a properly functioning DNN. This demand for supervision data and labels is a major bottleneck in current medical image analysis, since collecting a large number of annotations from experienced experts can be time-consuming and expensive. In this paper, we demonstrate that the eye movement of radiologists reading medical images can be a new form of supervision to train the DNN-based computer-aided diagnosis (CAD) system. Particularly, we record the tracks of the radiologists' gaze when they are reading images. The gaze information is processed and then used to supervise the DNN's attention via an Attention Consistency module. To the best of our knowledge, the above pipeline is among the earliest efforts to leverage expert eye movement for deep-learning-based CAD. We have conducted extensive experiments on knee X-ray images for osteoarthritis assessment. The results show that our method can achieve considerable improvement in diagnosis performance, with the help of gaze supervision.



### Multi-Scale Memory-Based Video Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2204.02977v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.02977v1)
- **Published**: 2022-04-06 08:48:56+00:00
- **Updated**: 2022-04-06 08:48:56+00:00
- **Authors**: Bo Ji, Angela Yao
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Video deblurring has achieved remarkable progress thanks to the success of deep neural networks. Most methods solve for the deblurring end-to-end with limited information propagation from the video sequence. However, different frame regions exhibit different characteristics and should be provided with corresponding relevant information. To achieve fine-grained deblurring, we designed a memory branch to memorize the blurry-sharp feature pairs in the memory bank, thus providing useful information for the blurry query input. To enrich the memory of our memory bank, we further designed a bidirectional recurrency and multi-scale strategy based on the memory bank. Experimental results demonstrate that our model outperforms other state-of-the-art methods while keeping the model complexity and inference time low. The code is available at https://github.com/jibo27/MemDeblur.



### Faster-TAD: Towards Temporal Action Detection with Proposal Generation and Classification in a Unified Network
- **Arxiv ID**: http://arxiv.org/abs/2204.02674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02674v1)
- **Published**: 2022-04-06 08:55:35+00:00
- **Updated**: 2022-04-06 08:55:35+00:00
- **Authors**: Shimin Chen, Chen Chen, Wei Li, Xunqiang Tao, Yandong Guo
- **Comment**: 16 pages,5 figures
- **Journal**: None
- **Summary**: Temporal action detection (TAD) aims to detect the semantic labels and boundaries of action instances in untrimmed videos. Current mainstream approaches are multi-step solutions, which fall short in efficiency and flexibility. In this paper, we propose a unified network for TAD, termed Faster-TAD, by re-purposing a Faster-RCNN like architecture. To tackle the unique difficulty in TAD, we make important improvements over the original framework. We propose a new Context-Adaptive Proposal Module and an innovative Fake-Proposal Generation Block. What's more, we use atomic action features to improve the performance. Faster-TAD simplifies the pipeline of TAD and gets remarkable performance on lots of benchmarks, i.e., ActivityNet-1.3 (40.01% mAP), HACS Segments (38.39% mAP), SoccerNet-Action Spotting (54.09% mAP). It outperforms existing single-network detector by a large margin.



### Rolling Colors: Adversarial Laser Exploits against Traffic Light Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.02675v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2204.02675v1)
- **Published**: 2022-04-06 08:57:25+00:00
- **Updated**: 2022-04-06 08:57:25+00:00
- **Authors**: Chen Yan, Zhijian Xu, Zhanyuan Yin, Xiaoyu Ji, Wenyuan Xu
- **Comment**: To be published in USENIX Security 2022
- **Journal**: None
- **Summary**: Traffic light recognition is essential for fully autonomous driving in urban areas. In this paper, we investigate the feasibility of fooling traffic light recognition mechanisms by shedding laser interference on the camera. By exploiting the rolling shutter of CMOS sensors, we manage to inject a color stripe overlapped on the traffic light in the image, which can cause a red light to be recognized as a green light or vice versa. To increase the success rate, we design an optimization method to search for effective laser parameters based on empirical models of laser interference. Our evaluation in emulated and real-world setups on 2 state-of-the-art recognition systems and 5 cameras reports a maximum success rate of 30% and 86.25% for Red-to-Green and Green-to-Red attacks. We observe that the attack is effective in continuous frames from more than 40 meters away against a moving vehicle, which may cause end-to-end impacts on self-driving such as running a red light or emergency stop. To mitigate the threat, we propose redesigning the rolling shutter mechanism.



### PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model
- **Arxiv ID**: http://arxiv.org/abs/2204.02681v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.02681v1)
- **Published**: 2022-04-06 09:02:41+00:00
- **Updated**: 2022-04-06 09:02:41+00:00
- **Authors**: Juncai Peng, Yi Liu, Shiyu Tang, Yuying Hao, Lutao Chu, Guowei Chen, Zewu Wu, Zeyu Chen, Zhiliang Yu, Yuning Du, Qingqing Dang, Baohua Lai, Qiwen Liu, Xiaoguang Hu, Dianhai Yu, Yanjun Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world applications have high demands for semantic segmentation methods. Although semantic segmentation has made remarkable leap-forwards with deep learning, the performance of real-time methods is not satisfactory. In this work, we propose PP-LiteSeg, a novel lightweight model for the real-time semantic segmentation task. Specifically, we present a Flexible and Lightweight Decoder (FLD) to reduce computation overhead of previous decoder. To strengthen feature representations, we propose a Unified Attention Fusion Module (UAFM), which takes advantage of spatial and channel attention to produce a weight and then fuses the input features with the weight. Moreover, a Simple Pyramid Pooling Module (SPPM) is proposed to aggregate global context with low computation cost. Extensive evaluations demonstrate that PP-LiteSeg achieves a superior trade-off between accuracy and speed compared to other methods. On the Cityscapes test set, PP-LiteSeg achieves 72.0% mIoU/273.6 FPS and 77.5% mIoU/102.6 FPS on NVIDIA GTX 1080Ti. Source code and models are available at PaddleSeg: https://github.com/PaddlePaddle/PaddleSeg.



### Domain-Agnostic Prior for Transfer Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.02684v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02684v2)
- **Published**: 2022-04-06 09:13:25+00:00
- **Updated**: 2022-04-20 07:53:49+00:00
- **Authors**: Xinyue Huo, Lingxi Xie, Hengtong Hu, Wengang Zhou, Houqiang Li, Qi Tian
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) is an important topic in the computer vision community. The key difficulty lies in defining a common property between the source and target domains so that the source-domain features can align with the target-domain semantics. In this paper, we present a simple and effective mechanism that regularizes cross-domain representation learning with a domain-agnostic prior (DAP) that constrains the features extracted from source and target domains to align with a domain-agnostic space. In practice, this is easily implemented as an extra loss term that requires a little extra costs. In the standard evaluation protocol of transferring synthesized data to real data, we validate the effectiveness of different types of DAP, especially that borrowed from a text embedding model that shows favorable performance beyond the state-of-the-art UDA approaches in terms of segmentation accuracy. Our research reveals that UDA benefits much from better proxies, possibly from other data modalities.



### SEAL: A Large-scale Video Dataset of Multi-grained Spatio-temporally Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2204.02688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02688v1)
- **Published**: 2022-04-06 09:27:52+00:00
- **Updated**: 2022-04-06 09:27:52+00:00
- **Authors**: Shimin Chen, Wei Li, Chen Chen, Jianyang Gu, Jiaming Chu, Xunqiang Tao, Yandong Guo
- **Comment**: 17 pages,6 figures
- **Journal**: None
- **Summary**: In spite of many dataset efforts for human action recognition, current computer vision algorithms are still limited to coarse-grained spatial and temporal annotations among human daily life. In this paper, we introduce a novel large-scale video dataset dubbed SEAL for multi-grained Spatio-tEmporal Action Localization. SEAL consists of two kinds of annotations, SEAL Tubes and SEAL Clips. We observe that atomic actions can be combined into many complex activities. SEAL Tubes provide both atomic action and complex activity annotations in tubelet level, producing 49.6k atomic actions spanning 172 action categories and 17.7k complex activities spanning 200 activity categories. SEAL Clips localizes atomic actions in space during two-second clips, producing 510.4k action labels with multiple labels per person. Extensive experimental results show that SEAL significantly helps to advance video understanding.



### Aesthetic Text Logo Synthesis via Content-aware Layout Inferring
- **Arxiv ID**: http://arxiv.org/abs/2204.02701v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02701v1)
- **Published**: 2022-04-06 09:51:50+00:00
- **Updated**: 2022-04-06 09:51:50+00:00
- **Authors**: Yizhi Wang, Guo Pu, Wenhan Luo, Yexin Wang, Pengfei Xiong, Hongwen Kang, Zhouhui Lian
- **Comment**: Accepted by CVPR 2022. Code and Dataset:
  https://github.com/yizhiwang96/TextLogoLayout
- **Journal**: None
- **Summary**: Text logo design heavily relies on the creativity and expertise of professional designers, in which arranging element layouts is one of the most important procedures. However, few attention has been paid to this task which needs to take many factors (e.g., fonts, linguistics, topics, etc.) into consideration. In this paper, we propose a content-aware layout generation network which takes glyph images and their corresponding text as input and synthesizes aesthetic layouts for them automatically. Specifically, we develop a dual-discriminator module, including a sequence discriminator and an image discriminator, to evaluate both the character placing trajectories and rendered shapes of synthesized text logos, respectively. Furthermore, we fuse the information of linguistics from texts and visual semantics from glyphs to guide layout prediction, which both play important roles in professional layout design. To train and evaluate our approach, we construct a dataset named as TextLogo3K, consisting of about 3,500 text logo images and their pixel-level annotations. Experimental studies on this dataset demonstrate the effectiveness of our approach for synthesizing visually-pleasing text logos and verify its superiority against the state of the art.



### Georeferencing of Photovoltaic Modules from Aerial Infrared Videos using Structure-from-Motion
- **Arxiv ID**: http://arxiv.org/abs/2204.02733v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02733v1)
- **Published**: 2022-04-06 11:17:08+00:00
- **Updated**: 2022-04-06 11:17:08+00:00
- **Authors**: Lukas Bommes, Claudia Buerhop-Lutz, Tobias Pickel, Jens Hauch, Christoph Brabec, Ian Marius Peters
- **Comment**: None
- **Journal**: None
- **Summary**: To identify abnormal photovoltaic (PV) modules in large-scale PV plants economically, drone-mounted infrared (IR) cameras and automated video processing algorithms are frequently used. While most related works focus on the detection of abnormal modules, little has been done to automatically localize those modules within the plant. In this work, we use incremental structure-from-motion to automatically obtain geocoordinates of all PV modules in a plant based on visual cues and the measured GPS trajectory of the drone. In addition, we extract multiple IR images of each PV module. Using our method, we successfully map 99.3 % of the 35084 modules in four large-scale and one rooftop plant and extract over 2.2 million module images. As compared to our previous work, extraction misses 18 times less modules (one in 140 modules as compared to one in eight). Furthermore, two or three plant rows can be processed simultaneously, increasing module throughput and reducing flight duration by a factor of 2.1 and 3.7, respectively. Comparison with an accurate orthophoto of one of the large-scale plants yields a root mean square error of the estimated module geocoordinates of 5.87 m and a relative error within each plant row of 0.22 m to 0.82 m. Finally, we use the module geocoordinates and extracted IR images to visualize distributions of module temperatures and anomaly predictions of a deep learning classifier on a map. While the temperature distribution helps to identify disconnected strings, we also find that its detection accuracy for module anomalies reaches, or even exceeds, that of a deep learning classifier for seven out of ten common anomaly types. The software is published at https://github.com/LukasBommes/PV-Hawk.



### Emotional Speech Recognition with Pre-trained Deep Visual Models
- **Arxiv ID**: http://arxiv.org/abs/2204.03561v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2204.03561v1)
- **Published**: 2022-04-06 11:27:59+00:00
- **Updated**: 2022-04-06 11:27:59+00:00
- **Authors**: Waleed Ragheb, Mehdi Mirzapour, Ali Delfardi, Hélène Jacquenet, Lawrence Carbon
- **Comment**: None
- **Journal**: Deep Learning for NLP Workshop, Extraction et Gestion des
  Connaissances (EGC), 2022
- **Summary**: In this paper, we propose a new methodology for emotional speech recognition using visual deep neural network models. We employ the transfer learning capabilities of the pre-trained computer vision deep models to have a mandate for the emotion recognition in speech task. In order to achieve that, we propose to use a composite set of acoustic features and a procedure to convert them into images. Besides, we present a training paradigm for these models taking into consideration the different characteristics between acoustic-based images and regular ones. In our experiments, we use the pre-trained VGG-16 model and test the overall methodology on the Berlin EMO-DB dataset for speaker-independent emotion recognition. We evaluate the proposed model on the full list of the seven emotions and the results set a new state-of-the-art.



### Masking Adversarial Damage: Finding Adversarial Saliency for Robust and Sparse Network
- **Arxiv ID**: http://arxiv.org/abs/2204.02738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02738v1)
- **Published**: 2022-04-06 11:28:06+00:00
- **Updated**: 2022-04-06 11:28:06+00:00
- **Authors**: Byung-Kwan Lee, Junho Kim, Yong Man Ro
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Adversarial examples provoke weak reliability and potential security issues in deep neural networks. Although adversarial training has been widely studied to improve adversarial robustness, it works in an over-parameterized regime and requires high computations and large memory budgets. To bridge adversarial robustness and model compression, we propose a novel adversarial pruning method, Masking Adversarial Damage (MAD) that employs second-order information of adversarial loss. By using it, we can accurately estimate adversarial saliency for model parameters and determine which parameters can be pruned without weakening adversarial robustness. Furthermore, we reveal that model parameters of initial layer are highly sensitive to the adversarial examples and show that compressed feature representation retains semantic information for the target objects. Through extensive experiments on three public datasets, we demonstrate that MAD effectively prunes adversarially trained networks without loosing adversarial robustness and shows better performance than previous adversarial pruning methods.



### Drivers' attention detection: a systematic literature review
- **Arxiv ID**: http://arxiv.org/abs/2204.03741v1
- **DOI**: None
- **Categories**: **cs.CV**, A.1; I.2
- **Links**: [PDF](http://arxiv.org/pdf/2204.03741v1)
- **Published**: 2022-04-06 11:36:40+00:00
- **Updated**: 2022-04-06 11:36:40+00:00
- **Authors**: Luiz G. Véras, Anna K. F. Gomes, Guilherme A. R. Dominguez, Alexandre T. Oliveira
- **Comment**: 15 pages, 4 figures, Systematic Literature Review
- **Journal**: None
- **Summary**: Countless traffic accidents often occur because of the inattention of the drivers. Many factors can contribute to distractions while driving, since objects or events to physiological conditions, as drowsiness and fatigue, do not allow the driver to stay attentive. The technological progress allowed the development and application of many solutions to detect the attention in real situations, promoting the interest of the scientific community in these last years. Commonly, these solutions identify the lack of attention and alert the driver, in order to help her/him to recover the attention, avoiding serious accidents and preserving lives. Our work presents a Systematic Literature Review (SLR) of the methods and criteria used to detect attention of drivers at the wheel, focusing on those methods based on images. As results, 50 studies were selected from the literature on drivers' attention detection, in which 22 contain solutions in the desired context. The results of SLR can be used as a resource in the preparation of new research projects in drivers' attention detection.



### Universal Representations: A Unified Look at Multiple Task and Domain Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.02744v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02744v2)
- **Published**: 2022-04-06 11:40:01+00:00
- **Updated**: 2022-08-30 12:02:09+00:00
- **Authors**: Wei-Hong Li, Xialei Liu, Hakan Bilen
- **Comment**: Multi-task Learning, Multi-domain Learning, Cross-domain Few-shot
  Learning, Universal Representation Learning, Balanced Optimization, Dense
  Prediction, Code and models are available at
  https://github.com/VICO-UoE/UniversalRepresentations. arXiv admin note: text
  overlap with arXiv:2103.13841
- **Journal**: None
- **Summary**: We propose a unified look at jointly learning multiple vision tasks and visual domains through universal representations, a single deep neural network. Learning multiple problems simultaneously involves minimizing a weighted sum of multiple loss functions with different magnitudes and characteristics and thus results in unbalanced state of one loss dominating the optimization and poor results compared to learning a separate model for each problem. To this end, we propose distilling knowledge of multiple task/domain-specific networks into a single deep neural network after aligning its representations with the task/domain-specific ones through small capacity adapters. We rigorously show that universal representations achieve state-of-the-art performances in learning of multiple dense prediction problems in NYU-v2 and Cityscapes, multiple image classification problems from diverse domains in Visual Decathlon Dataset and cross-domain few-shot learning in MetaDataset. Finally we also conduct multiple analysis through ablation and qualitative studies.



### Mitosis domain generalization in histopathology images -- The MIDOG challenge
- **Arxiv ID**: http://arxiv.org/abs/2204.03742v1
- **DOI**: 10.1016/j.media.2022.102699
- **Categories**: **eess.IV**, cs.CV, physics.med-ph, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2204.03742v1)
- **Published**: 2022-04-06 11:43:10+00:00
- **Updated**: 2022-04-06 11:43:10+00:00
- **Authors**: Marc Aubreville, Nikolas Stathonikos, Christof A. Bertram, Robert Klopleisch, Natalie ter Hoeve, Francesco Ciompi, Frauke Wilm, Christian Marzahl, Taryn A. Donovan, Andreas Maier, Jack Breen, Nishant Ravikumar, Youjin Chung, Jinah Park, Ramin Nateghi, Fattaneh Pourakpour, Rutger H. J. Fick, Saima Ben Hadj, Mostafa Jahanifar, Nasir Rajpoot, Jakob Dexl, Thomas Wittenberg, Satoshi Kondo, Maxime W. Lafarge, Viktor H. Koelzer, Jingtang Liang, Yubo Wang, Xi Long, Jingxin Liu, Salar Razavi, April Khademi, Sen Yang, Xiyue Wang, Mitko Veta, Katharina Breininger
- **Comment**: 19 pages, 9 figures, summary paper of the 2021 MICCAI MIDOG challenge
- **Journal**: Medical Image Analysis 84 (2023) 102699
- **Summary**: The density of mitotic figures within tumor tissue is known to be highly correlated with tumor proliferation and thus is an important marker in tumor grading. Recognition of mitotic figures by pathologists is known to be subject to a strong inter-rater bias, which limits the prognostic value. State-of-the-art deep learning methods can support the expert in this assessment but are known to strongly deteriorate when applied in a different clinical environment than was used for training. One decisive component in the underlying domain shift has been identified as the variability caused by using different whole slide scanners. The goal of the MICCAI MIDOG 2021 challenge has been to propose and evaluate methods that counter this domain shift and derive scanner-agnostic mitosis detection algorithms. The challenge used a training set of 200 cases, split across four scanning systems. As a test set, an additional 100 cases split across four scanning systems, including two previously unseen scanners, were given. The best approaches performed on an expert level, with the winning algorithm yielding an F_1 score of 0.748 (CI95: 0.704-0.781). In this paper, we evaluate and compare the approaches that were submitted to the challenge and identify methodological factors contributing to better performance.



### BFRnet: A deep learning-based MR background field removal method for QSM of the brain containing significant pathological susceptibility sources
- **Arxiv ID**: http://arxiv.org/abs/2204.02760v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.02760v1)
- **Published**: 2022-04-06 12:05:56+00:00
- **Updated**: 2022-04-06 12:05:56+00:00
- **Authors**: Xuanyu Zhu, Yang Gao, Feng Liu, Stuart Crozier, Hongfu Sun
- **Comment**: 23 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: Introduction: Background field removal (BFR) is a critical step required for successful quantitative susceptibility mapping (QSM). However, eliminating the background field in brains containing significant susceptibility sources, such as intracranial hemorrhages, is challenging due to the relatively large scale of the field induced by these pathological susceptibility sources. Method: This study proposes a new deep learning-based method, BFRnet, to remove background field in healthy and hemorrhagic subjects. The network is built with the dual-frequency octave convolutions on the U-net architecture, trained with synthetic field maps containing significant susceptibility sources. The BFRnet method is compared with three conventional BFR methods and one previous deep learning method using simulated and in vivo brains from 4 healthy and 2 hemorrhagic subjects. Robustness against acquisition field-of-view (FOV) orientation and brain masking are also investigated. Results: For both simulation and in vivo experiments, BFRnet led to the best visually appealing results in the local field and QSM results with the minimum contrast loss and the most accurate hemorrhage susceptibility measurements among all five methods. In addition, BFRnet produced the most consistent local field and susceptibility maps between different sizes of brain masks, while conventional methods depend drastically on precise brain extraction and further brain edge erosions. It is also observed that BFRnet performed the best among all BFR methods for acquisition FOVs oblique to the main magnetic field. Conclusion: The proposed BFRnet improved the accuracy of local field reconstruction in the hemorrhagic subjects compared with conventional BFR algorithms. The BFRnet method was effective for acquisitions of titled orientations and retained whole brains without edge erosion as often required by traditional BFR methods.



### Detail-recovery Image Deraining via Dual Sample-augmented Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.02772v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02772v2)
- **Published**: 2022-04-06 12:35:27+00:00
- **Updated**: 2023-02-14 08:57:25+00:00
- **Authors**: Yiyang Shen, Mingqiang Wei, Sen Deng, Wenhan Yang, Yongzhen Wang, Xiao-Ping Zhang, Meng Wang, Jing Qin
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: The intricacy of rainy image contents often leads cutting-edge deraining models to image degradation including remnant rain, wrongly-removed details, and distorted appearance. Such degradation is further exacerbated when applying the models trained on synthetic data to real-world rainy images. We observe two types of domain gaps between synthetic and real-world rainy images: one exists in rain streak patterns; the other is the pixel-level appearance of rain-free images. To bridge the two domain gaps, we propose a semi-supervised detail-recovery image deraining network (Semi-DRDNet) with dual sample-augmented contrastive learning. Semi-DRDNet consists of three sub-networks:i) for removing rain streaks without remnants, we present a squeeze-and-excitation based rain residual network; ii) for encouraging the lost details to return, we construct a structure detail context aggregation based detail repair network; to our knowledge, this is the first time; and iii) for building efficient contrastive constraints for both rain streaks and clean backgrounds, we exploit a novel dual sample-augmented contrastive regularization network.Semi-DRDNet operates smoothly on both synthetic and real-world rainy data in terms of deraining robustness and detail accuracy. Comparisons on four datasets including our established Real200 show clear improvements of Semi-DRDNet over fifteen state-of-the-art methods. Code and dataset are available at https://github.com/syy-whu/DRD-Net.



### 3D face reconstruction with dense landmarks
- **Arxiv ID**: http://arxiv.org/abs/2204.02776v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02776v2)
- **Published**: 2022-04-06 12:43:34+00:00
- **Updated**: 2022-07-20 22:07:30+00:00
- **Authors**: Erroll Wood, Tadas Baltrusaitis, Charlie Hewitt, Matthew Johnson, Jingjing Shen, Nikola Milosavljevic, Daniel Wilde, Stephan Garbin, Chirag Raman, Jamie Shotton, Toby Sharp, Ivan Stojiljkovic, Tom Cashman, Julien Valentin
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Landmarks often play a key role in face analysis, but many aspects of identity or expression cannot be represented by sparse landmarks alone. Thus, in order to reconstruct faces more accurately, landmarks are often combined with additional signals like depth images or techniques like differentiable rendering. Can we keep things simple by just using more landmarks? In answer, we present the first method that accurately predicts 10x as many landmarks as usual, covering the whole head, including the eyes and teeth. This is accomplished using synthetic training data, which guarantees perfect landmark annotations. By fitting a morphable model to these dense landmarks, we achieve state-of-the-art results for monocular 3D face reconstruction in the wild. We show that dense landmarks are an ideal signal for integrating face shape information across frames by demonstrating accurate and expressive facial performance capture in both monocular and multi-view scenarios. This approach is also highly efficient: we can predict dense landmarks and fit our 3D face model at over 150FPS on a single CPU thread. Please see our website: https://microsoft.github.io/DenseLandmarks/.



### Implicit Motion-Compensated Network for Unsupervised Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.02791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02791v1)
- **Published**: 2022-04-06 13:03:59+00:00
- **Updated**: 2022-04-06 13:03:59+00:00
- **Authors**: Lin Xi, Weihai Chen, Xingming Wu, Zhong Liu, Zhengguo Li
- **Comment**: 14 pages, 13 figures
- **Journal**: None
- **Summary**: Unsupervised video object segmentation (UVOS) aims at automatically separating the primary foreground object(s) from the background in a video sequence. Existing UVOS methods either lack robustness when there are visually similar surroundings (appearance-based) or suffer from deterioration in the quality of their predictions because of dynamic background and inaccurate flow (flow-based). To overcome the limitations, we propose an implicit motion-compensated network (IMCNet) combining complementary cues ($\textit{i.e.}$, appearance and motion) with aligned motion information from the adjacent frames to the current frame at the feature level without estimating optical flows. The proposed IMCNet consists of an affinity computing module (ACM), an attention propagation module (APM), and a motion compensation module (MCM). The light-weight ACM extracts commonality between neighboring input frames based on appearance features. The APM then transmits global correlation in a top-down manner. Through coarse-to-fine iterative inspiring, the APM will refine object regions from multiple resolutions so as to efficiently avoid losing details. Finally, the MCM aligns motion information from temporally adjacent frames to the current frame which achieves implicit motion compensation at the feature level. We perform extensive experiments on $\textit{DAVIS}_{\textit{16}}$ and $\textit{YouTube-Objects}$. Our network achieves favorable performance while running at a faster speed compared to the state-of-the-art methods.



### Expression-preserving face frontalization improves visually assisted speech processing
- **Arxiv ID**: http://arxiv.org/abs/2204.02810v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2204.02810v4)
- **Published**: 2022-04-06 13:22:24+00:00
- **Updated**: 2022-12-15 11:17:07+00:00
- **Authors**: Zhiqi Kang, Mostafa Sadeghi, Radu Horaud, Xavier Alameda-Pineda
- **Comment**: arXiv admin note: text overlap with arXiv:2202.00538
- **Journal**: None
- **Summary**: Face frontalization consists of synthesizing a frontally-viewed face from an arbitrarily-viewed one. The main contribution of this paper is a frontalization methodology that preserves non-rigid facial deformations in order to boost the performance of visually assisted speech communication. The method alternates between the estimation of (i)~the rigid transformation (scale, rotation, and translation) and (ii)~the non-rigid deformation between an arbitrarily-viewed face and a face model. The method has two important merits: it can deal with non-Gaussian errors in the data and it incorporates a dynamical face deformation model. For that purpose, we use the generalized Student t-distribution in combination with a linear dynamic system in order to account for both rigid head motions and time-varying facial deformations caused by speech production. We propose to use the zero-mean normalized cross-correlation (ZNCC) score to evaluate the ability of the method to preserve facial expressions. The method is thoroughly evaluated and compared with several state of the art methods, either based on traditional geometric models or on deep learning. Moreover, we show that the method, when incorporated into deep learning pipelines, namely lip reading and speech enhancement, improves word recognition and speech intelligibilty scores by a considerable margin. Supplemental material is accessible at https://team.inria.fr/robotlearn/research/facefrontalization/



### BMD: A General Class-balanced Multicentric Dynamic Prototype Strategy for Source-free Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2204.02811v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02811v2)
- **Published**: 2022-04-06 13:23:02+00:00
- **Updated**: 2022-07-18 08:00:22+00:00
- **Authors**: Sanqing Qu, Guang Chen, Jing Zhang, Zhijun Li, Wei He, Dacheng Tao
- **Comment**: Camera-ready version of ECCV 2022. Code is available at
  https://github.com/ispc-lab/BMD
- **Journal**: None
- **Summary**: Source-free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to the unlabeled target domain without accessing the well-labeled source data, which is a much more practical setting due to the data privacy, security, and transmission issues. To make up for the absence of source data, most existing methods introduced feature prototype based pseudo-labeling strategies to realize self-training model adaptation. However, feature prototypes are obtained by instance-level predictions based feature clustering, which is category-biased and tends to result in noisy labels since the visual domain gaps between source and target are usually different between categories. In addition, we found that a monocentric feature prototype may be ineffective to represent each category and introduce negative transfer, especially for those hard-transfer data. To address these issues, we propose a general class-Balanced Multicentric Dynamic prototype (BMD) strategy for the SFDA task. Specifically, for each target category, we first introduce a global inter-class balanced sampling strategy to aggregate potential representative target samples. Then, we design an intra-class multicentric clustering strategy to achieve more robust and representative prototypes generation. In contrast to existing strategies that update the pseudo label at a fixed training period, we further introduce a dynamic pseudo labeling strategy to incorporate network update information during model adaptation. Extensive experiments show that the proposed model-agnostic BMD strategy significantly improves representative SFDA methods to yield new state-of-the-art results. The code is available at https://github.com/ispc-lab/BMD.



### ShowFace: Coordinated Face Inpainting with Memory-Disentangled Refinement Networks
- **Arxiv ID**: http://arxiv.org/abs/2204.02824v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02824v3)
- **Published**: 2022-04-06 13:36:41+00:00
- **Updated**: 2022-04-24 14:52:35+00:00
- **Authors**: Zhuojie Wu, Xingqun Qi, Zijian Wang, Wanting Zhou, Kun Yuan, Muyi Sun, Zhenan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Face inpainting aims to complete the corrupted regions of the face images, which requires coordination between the completed areas and the non-corrupted areas. Recently, memory-oriented methods illustrate great prospects in the generation related tasks by introducing an external memory module to improve image coordination. However, such methods still have limitations in restoring the consistency and continuity for specificfacial semantic parts. In this paper, we propose the coarse-to-fine Memory-Disentangled Refinement Networks (MDRNets) for coordinated face inpainting, in which two collaborative modules are integrated, Disentangled Memory Module (DMM) and Mask-Region Enhanced Module (MREM). Specifically, the DMM establishes a group of disentangled memory blocks to store the semantic-decoupled face representations, which could provide the most relevant information to refine the semantic-level coordination. The MREM involves a masked correlation mining mechanism to enhance the feature relationships into the corrupted regions, which could also make up for the correlation loss caused by memory disentanglement. Furthermore, to better improve the inter-coordination between the corrupted and non-corrupted regions and enhance the intra-coordination in corrupted regions, we design InCo2 Loss, a pair of similarity based losses to constrain the feature consistency. Eventually, extensive experiments conducted on CelebA-HQ and FFHQ datasets demonstrate the superiority of our MDRNets compared with previous State-Of-The-Art methods.



### An Empirical Study of Remote Sensing Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2204.02825v4
- **DOI**: 10.1109/TGRS.2022.3176603
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02825v4)
- **Published**: 2022-04-06 13:38:11+00:00
- **Updated**: 2023-05-04 16:53:19+00:00
- **Authors**: Di Wang, Jing Zhang, Bo Du, Gui-Song Xia, Dacheng Tao
- **Comment**: Accepted by IEEE TGRS, codes and pretrained models are moved to
  https://github.com/ViTAE-Transformer/RSP
- **Journal**: None
- **Summary**: Deep learning has largely reshaped remote sensing (RS) research for aerial image understanding and made a great success. Nevertheless, most of the existing deep models are initialized with the ImageNet pretrained weights. Since natural images inevitably present a large domain gap relative to aerial images, probably limiting the finetuning performance on downstream aerial scene tasks. This issue motivates us to conduct an empirical study of remote sensing pretraining (RSP) on aerial images. To this end, we train different networks from scratch with the help of the largest RS scene recognition dataset up to now -- MillionAID, to obtain a series of RS pretrained backbones, including both convolutional neural networks (CNN) and vision transformers such as Swin and ViTAE, which have shown promising performance on computer vision tasks. Then, we investigate the impact of RSP on representative downstream tasks including scene recognition, semantic segmentation, object detection, and change detection using these CNN and vision transformer backbones. Empirical study shows that RSP can help deliver distinctive performances in scene recognition tasks and in perceiving RS related semantics such as "Bridge" and "Airplane". We also find that, although RSP mitigates the data discrepancies of traditional ImageNet pretraining on RS images, it may still suffer from task discrepancies, where downstream tasks require different representations from scene recognition tasks. These findings call for further research efforts on both large-scale pretraining datasets and effective pretraining methods. The codes and pretrained models will be released at https://github.com/ViTAE-Transformer/ViTAE-Transformer-Remote-Sensing.



### CCAT-NET: A Novel Transformer Based Semi-supervised Framework for Covid-19 Lung Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.02839v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.02839v1)
- **Published**: 2022-04-06 14:05:48+00:00
- **Updated**: 2022-04-06 14:05:48+00:00
- **Authors**: Mingyang Liu, Li Xiao, Huiqin Jiang, Qing He
- **Comment**: None
- **Journal**: None
- **Summary**: The spread of the novel coronavirus disease 2019 (COVID-19) has claimed millions of lives. Automatic segmentation of lesions from CT images can assist doctors with screening, treatment, and monitoring. However, accurate segmentation of lesions from CT images can be very challenging due to data and model limitations. Recently, Transformer-based networks have attracted a lot of attention in the area of computer vision, as Transformer outperforms CNN at a bunch of tasks. In this work, we propose a novel network structure that combines CNN and Transformer for the segmentation of COVID-19 lesions. We further propose an efficient semi-supervised learning framework to address the shortage of labeled data. Extensive experiments showed that our proposed network outperforms most existing networks and the semi-supervised learning framework can outperform the base network by 3.0% and 8.2% in terms of Dice coefficient and sensitivity.



### Open-Source Tools for Behavioral Video Analysis: Setup, Methods, and Development
- **Arxiv ID**: http://arxiv.org/abs/2204.02842v4
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2204.02842v4)
- **Published**: 2022-04-06 14:06:43+00:00
- **Updated**: 2023-03-09 16:20:21+00:00
- **Authors**: Kevin Luxem, Jennifer J. Sun, Sean P. Bradley, Keerthi Krishnan, Eric A. Yttri, Jan Zimmermann, Talmo D. Pereira, Mark Laubach
- **Comment**: 26 pages, 2 figures, 3 tables; this is a commentary on video methods
  for analyzing behavior in animals that emerged from a working group organized
  by the OpenBehavior project (openbehavior.com)
- **Journal**: None
- **Summary**: Recently developed methods for video analysis, especially models for pose estimation and behavior classification, are transforming behavioral quantification to be more precise, scalable, and reproducible in fields such as neuroscience and ethology. These tools overcome long-standing limitations of manual scoring of video frames and traditional "center of mass" tracking algorithms to enable video analysis at scale. The expansion of open-source tools for video acquisition and analysis has led to new experimental approaches to understand behavior. Here, we review currently available open-source tools for video analysis and discuss how to set up these methods for labs new to video recording. We also discuss best practices for developing and using video analysis methods, including community-wide standards and critical needs for the open sharing of datasets and code, more widespread comparisons of video analysis methods, and better documentation for these methods especially for new users. We encourage broader adoption and continued development of these tools, which have tremendous potential for accelerating scientific progress in understanding the brain and behavior.



### Learning to Generate Realistic Noisy Images via Pixel-level Noise-aware Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2204.02844v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.02844v2)
- **Published**: 2022-04-06 14:09:02+00:00
- **Updated**: 2022-09-15 03:20:03+00:00
- **Authors**: Yuanhao Cai, Xiaowan Hu, Haoqian Wang, Yulun Zhang, Hanspeter Pfister, Donglai Wei
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: Existing deep learning real denoising methods require a large amount of noisy-clean image pairs for supervision. Nonetheless, capturing a real noisy-clean dataset is an unacceptable expensive and cumbersome procedure. To alleviate this problem, this work investigates how to generate realistic noisy images. Firstly, we formulate a simple yet reasonable noise model that treats each real noisy pixel as a random variable. This model splits the noisy image generation problem into two sub-problems: image domain alignment and noise domain alignment. Subsequently, we propose a novel framework, namely Pixel-level Noise-aware Generative Adversarial Network (PNGAN). PNGAN employs a pre-trained real denoiser to map the fake and real noisy images into a nearly noise-free solution space to perform image domain alignment. Simultaneously, PNGAN establishes a pixel-level adversarial training to conduct noise domain alignment. Additionally, for better noise fitting, we present an efficient architecture Simple Multi-scale Network (SMNet) as the generator. Qualitative validation shows that noise generated by PNGAN is highly similar to real noise in terms of intensity and distribution. Quantitative experiments demonstrate that a series of denoisers trained with the generated noisy images achieve state-of-the-art (SOTA) results on four real denoising benchmarks. Part of codes, pre-trained models, and results are available at https://github.com/caiyuanhao1998/PNGAN for comparisons.



### KNN-Diffusion: Image Generation via Large-Scale Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2204.02849v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.02849v2)
- **Published**: 2022-04-06 14:13:35+00:00
- **Updated**: 2022-10-02 11:55:59+00:00
- **Authors**: Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, Yaniv Taigman
- **Comment**: None
- **Journal**: None
- **Summary**: Recent text-to-image models have achieved impressive results. However, since they require large-scale datasets of text-image pairs, it is impractical to train them on new domains where data is scarce or not labeled. In this work, we propose using large-scale retrieval methods, in particular, efficient k-Nearest-Neighbors (kNN), which offers novel capabilities: (1) training a substantially small and efficient text-to-image diffusion model without any text, (2) generating out-of-distribution images by simply swapping the retrieval database at inference time, and (3) performing text-driven local semantic manipulations while preserving object identity. To demonstrate the robustness of our method, we apply our kNN approach on two state-of-the-art diffusion backbones, and show results on several different datasets. As evaluated by human studies and automatic metrics, our method achieves state-of-the-art results compared to existing approaches that train text-to-image generation models using images only (without paired text data)



### Influence of Color Spaces for Deep Learning Image Colorization
- **Arxiv ID**: http://arxiv.org/abs/2204.02850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02850v1)
- **Published**: 2022-04-06 14:14:07+00:00
- **Updated**: 2022-04-06 14:14:07+00:00
- **Authors**: Coloma Ballester, Aurélie Bugeau, Hernan Carrillo, Michaël Clément, Rémi Giraud, Lara Raad, Patricia Vitoria
- **Comment**: None
- **Journal**: None
- **Summary**: Colorization is a process that converts a grayscale image into a color one that looks as natural as possible. Over the years this task has received a lot of attention. Existing colorization methods rely on different color spaces: RGB, YUV, Lab, etc. In this chapter, we aim to study their influence on the results obtained by training a deep neural network, to answer the question: "Is it crucial to correctly choose the right color space in deep-learning based colorization?". First, we briefly summarize the literature and, in particular, deep learning-based methods. We then compare the results obtained with the same deep neural network architecture with RGB, YUV and Lab color spaces. Qualitative and quantitative analysis do not conclude similarly on which color space is better. We then show the importance of carefully designing the architecture and evaluation protocols depending on the types of images that are being processed and their specificities: strong/small contours, few/many objects, recent/archive images.



### Analysis of Different Losses for Deep Learning Image Colorization
- **Arxiv ID**: http://arxiv.org/abs/2204.02980v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02980v3)
- **Published**: 2022-04-06 14:19:43+00:00
- **Updated**: 2022-05-28 16:56:41+00:00
- **Authors**: Coloma Ballester, Aurélie Bugeau, Hernan Carrillo, Michaël Clément, Rémi Giraud, Lara Raad, Patricia Vitoria
- **Comment**: arXiv admin note: text overlap with arXiv:2204.02850
- **Journal**: None
- **Summary**: Image colorization aims to add color information to a grayscale image in a realistic way. Recent methods mostly rely on deep learning strategies. While learning to automatically colorize an image, one can define well-suited objective functions related to the desired color output. Some of them are based on a specific type of error between the predicted image and ground truth one, while other losses rely on the comparison of perceptual properties. But, is the choice of the objective function that crucial, i.e., does it play an important role in the results? In this chapter, we aim to answer this question by analyzing the impact of the loss function on the estimated colorization results. To that goal, we review the different losses and evaluation metrics that are used in the literature. We then train a baseline network with several of the reviewed objective functions: classic L1 and L2 losses, as well as more complex combinations such as Wasserstein GAN and VGG-based LPIPS loss. Quantitative results show that the models trained with VGG-based LPIPS provide overall slightly better results for most evaluation metrics. Qualitative results exhibit more vivid colors when with Wasserstein GAN plus the L2 loss or again with the VGG-based LPIPS. Finally, the convenience of quantitative user studies is also discussed to overcome the difficulty of properly assessing on colorized images, notably for the case of old archive photographs where no ground truth is available.



### Retrieval-based Spatially Adaptive Normalization for Semantic Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2204.02854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02854v1)
- **Published**: 2022-04-06 14:21:39+00:00
- **Updated**: 2022-04-06 14:21:39+00:00
- **Authors**: Yupeng Shi, Xiao Liu, Yuxiang Wei, Zhongqin Wu, Wangmeng Zuo
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic image synthesis is a challenging task with many practical applications. Albeit remarkable progress has been made in semantic image synthesis with spatially-adaptive normalization and existing methods normalize the feature activations under the coarse-level guidance (e.g., semantic class). However, different parts of a semantic object (e.g., wheel and window of car) are quite different in structures and textures, making blurry synthesis results usually inevitable due to the missing of fine-grained guidance. In this paper, we propose a novel normalization module, termed as REtrieval-based Spatially AdaptIve normaLization (RESAIL), for introducing pixel level fine-grained guidance to the normalization architecture. Specifically, we first present a retrieval paradigm by finding a content patch of the same semantic class from training set with the most similar shape to each test semantic mask. Then, RESAIL is presented to use the retrieved patch for guiding the feature normalization of corresponding region, and can provide pixel level fine-grained guidance, thereby greatly mitigating blurry synthesis results. Moreover, distorted ground-truth images are also utilized as alternatives of retrieval-based guidance for feature normalization, further benefiting model training and improving visual quality of generated images. Experiments on several challenging datasets show that our RESAIL performs favorably against state-of-the-arts in terms of quantitative metrics, visual quality, and subjective evaluation. The source code and pre-trained models will be publicly available.



### Demonstrate Once, Imitate Immediately (DOME): Learning Visual Servoing for One-Shot Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/2204.02863v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.02863v2)
- **Published**: 2022-04-06 14:32:51+00:00
- **Updated**: 2022-07-27 19:24:54+00:00
- **Authors**: Eugene Valassakis, Georgios Papagiannis, Norman Di Palo, Edward Johns
- **Comment**: To be published at IROS 2022. 7 figures, 8 pages. Videos and
  supplementary material are available at: https://www.robot-learning.uk/dome
- **Journal**: None
- **Summary**: We present DOME, a novel method for one-shot imitation learning, where a task can be learned from just a single demonstration and then be deployed immediately, without any further data collection or training. DOME does not require prior task or object knowledge, and can perform the task in novel object configurations and with distractors. At its core, DOME uses an image-conditioned object segmentation network followed by a learned visual servoing network, to move the robot's end-effector to the same relative pose to the object as during the demonstration, after which the task can be completed by replaying the demonstration's end-effector velocities. We show that DOME achieves near 100% success rate on 7 real-world everyday tasks, and we perform several studies to thoroughly understand each individual component of DOME. Videos and supplementary material are available at: https://www.robot-learning.uk/dome .



### ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound
- **Arxiv ID**: http://arxiv.org/abs/2204.02874v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2204.02874v3)
- **Published**: 2022-04-06 14:43:42+00:00
- **Updated**: 2022-08-02 05:57:20+00:00
- **Authors**: Yan-Bo Lin, Jie Lei, Mohit Bansal, Gedas Bertasius
- **Comment**: ECCV 2022 Oral project page: https://yanbo.ml/project_page/eclipse/
- **Journal**: None
- **Summary**: We introduce an audiovisual method for long-range text-to-video retrieval. Unlike previous approaches designed for short video retrieval (e.g., 5-15 seconds in duration), our approach aims to retrieve minute-long videos that capture complex human actions. One challenge of standard video-only approaches is the large computational cost associated with processing hundreds of densely extracted frames from such long videos. To address this issue, we propose to replace parts of the video with compact audio cues that succinctly summarize dynamic audio events and are cheap to process. Our method, named ECLIPSE (Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an audiovisual video setting, by adding a unified audiovisual transformer block that captures complementary cues from the video and audio streams. In addition to being 2.92x faster and 2.34x memory-efficient than long-range video-only approaches, our method also achieves better text-to-video retrieval accuracy on several diverse long-range video datasets such as ActivityNet, QVHighlights, YouCook2, DiDeMo and Charades.



### Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2204.02887v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02887v2)
- **Published**: 2022-04-06 15:12:20+00:00
- **Updated**: 2022-04-24 03:28:20+00:00
- **Authors**: Xu Han, Anmin Liu, Yifeng Xiong, Yanbo Fan, Kun He
- **Comment**: The writing and experiment of the article need to be further
  strengthened
- **Journal**: None
- **Summary**: Deep neural networks have shown to be very vulnerable to adversarial examples crafted by adding human-imperceptible perturbations to benign inputs. After achieving impressive attack success rates in the white-box setting, more focus is shifted to black-box attacks. In either case, the common gradient-based approaches generally use the $sign$ function to generate perturbations at the end of the process. However, only a few works pay attention to the limitation of the $sign$ function. Deviation between the original gradient and the generated noises may lead to inaccurate gradient update estimation and suboptimal solutions for adversarial transferability, which is crucial for black-box attacks. To address this issue, we propose a Sampling-based Fast Gradient Rescaling Method (S-FGRM) to improve the transferability of the crafted adversarial examples. Specifically, we use data rescaling to substitute the inefficient $sign$ function in gradient-based attacks without extra computational cost. We also propose a Depth First Sampling method to eliminate the fluctuation of rescaling and stabilize the gradient update. Our method can be used in any gradient-based optimizations and is extensible to be integrated with various input transformation or ensemble methods for further improving the adversarial transferability. Extensive experiments on the standard ImageNet dataset show that our S-FGRM could significantly boost the transferability of gradient-based attacks and outperform the state-of-the-art baselines.



### DBF: Dynamic Belief Fusion for Combining Multiple Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2204.02890v1
- **DOI**: 10.1109/TPAMI.2019.2952847
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02890v1)
- **Published**: 2022-04-06 15:16:12+00:00
- **Updated**: 2022-04-06 15:16:12+00:00
- **Authors**: Hyungtae Lee, Heesung Kwon
- **Comment**: TPAMI publication
- **Journal**: in IEEE Transactions on Pattern Analysis and Machine Intelligence,
  vol. 43, no. 5, pp. 1499-1514, 1 May 2021
- **Summary**: In this paper, we propose a novel and highly practical score-level fusion approach called dynamic belief fusion (DBF) that directly integrates inference scores of individual detections from multiple object detection methods. To effectively integrate the individual outputs of multiple detectors, the level of ambiguity in each detection score is estimated using a confidence model built on a precision-recall relationship of the corresponding detector. For each detector output, DBF then calculates the probabilities of three hypotheses (target, non-target, and intermediate state (target or non-target)) based on the confidence level of the detection score conditioned on the prior confidence model of individual detectors, which is referred to as basic probability assignment. The probability distributions over three hypotheses of all the detectors are optimally fused via the Dempster's combination rule. Experiments on the ARL, PASCAL VOC 07, and 12 datasets show that the detection accuracy of the DBF is significantly higher than any of the baseline fusion approaches as well as individual detectors used for the fusion.



### End-to-End Instance Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.02898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02898v1)
- **Published**: 2022-04-06 15:32:21+00:00
- **Updated**: 2022-04-06 15:32:21+00:00
- **Authors**: Xueyan Zou, Haotian Liu, Yong Jae Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Edge detection has long been an important problem in the field of computer vision. Previous works have explored category-agnostic or category-aware edge detection. In this paper, we explore edge detection in the context of object instances. Although object boundaries could be easily derived from segmentation masks, in practice, instance segmentation models are trained to maximize IoU to the ground-truth mask, which means that segmentation boundaries are not enforced to precisely align with ground-truth edge boundaries. Thus, the task of instance edge detection itself is different and critical. Since precise edge detection requires high resolution feature maps, we design a novel transformer architecture that efficiently combines a FPN and a transformer decoder to enable cross attention on multi-scale high resolution feature maps within a reasonable computation budget. Further, we propose a light weight dense prediction head that is applicable to both instance edge and mask detection. Finally, we use a penalty reduced focal loss to effectively train the model with point supervision on instance edges, which can reduce annotation costs. We demonstrate highly competitive instance edge detection performance compared to state-of-the-art baselines, and also show that the proposed task and loss are complementary to instance segmentation and object detection.



### An Empirical Study of End-to-End Temporal Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.02932v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02932v1)
- **Published**: 2022-04-06 16:46:30+00:00
- **Updated**: 2022-04-06 16:46:30+00:00
- **Authors**: Xiaolong Liu, Song Bai, Xiang Bai
- **Comment**: Accepted by CVPR 2022. 13 pages, including supplementary
- **Journal**: None
- **Summary**: Temporal action detection (TAD) is an important yet challenging task in video understanding. It aims to simultaneously predict the semantic label and the temporal interval of every action instance in an untrimmed video. Rather than end-to-end learning, most existing methods adopt a head-only learning paradigm, where the video encoder is pre-trained for action classification, and only the detection head upon the encoder is optimized for TAD. The effect of end-to-end learning is not systematically evaluated. Besides, there lacks an in-depth study on the efficiency-accuracy trade-off in end-to-end TAD. In this paper, we present an empirical study of end-to-end temporal action detection. We validate the advantage of end-to-end learning over head-only learning and observe up to 11\% performance improvement. Besides, we study the effects of multiple design choices that affect the TAD performance and speed, including detection head, video encoder, and resolution of input videos. Based on the findings, we build a mid-resolution baseline detector, which achieves the state-of-the-art performance of end-to-end methods while running more than 4$\times$ faster. We hope that this paper can serve as a guide for end-to-end learning and inspire future research in this field. Code and models are available at \url{https://github.com/xlliu7/E2E-TAD}.



### Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations
- **Arxiv ID**: http://arxiv.org/abs/2204.02937v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2204.02937v2)
- **Published**: 2022-04-06 16:55:41+00:00
- **Updated**: 2023-06-30 22:51:42+00:00
- **Authors**: Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson
- **Comment**: ICLR 2023. Code is available at
  https://github.com/PolinaKirichenko/deep_feature_reweighting
- **Journal**: None
- **Summary**: Neural network classifiers can largely rely on simple spurious features, such as backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU.



### S-R2F2U-Net: A single-stage model for teeth segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.02939v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02939v2)
- **Published**: 2022-04-06 17:07:09+00:00
- **Updated**: 2023-02-23 20:26:37+00:00
- **Authors**: Mrinal Kanti Dhar, Mou Deb
- **Comment**: GitHub link is mentioned in the abstract. The main manuscript
  contains 4 figures and 4 tables. The supplementary document contains 7
  figures and 1 table
- **Journal**: None
- **Summary**: Precision tooth segmentation is crucial in the oral sector because it provides location information for orthodontic therapy, clinical diagnosis, and surgical treatments. In this paper, we investigate residual, recurrent, and attention networks to segment teeth from panoramic dental images. Based on our findings, we suggest three single-stage models: Single Recurrent R2U-Net (S-R2U-Net), Single Recurrent Filter Double R2U-Net (S-R2F2U-Net), and Single Recurrent Attention Enabled Filter Double (S-R2F2-Attn-U-Net). Particularly, S-R2F2U-Net outperforms state-of-the-art models in terms of accuracy and dice score. A hybrid loss function combining the cross-entropy loss and dice loss is used to train the model. In addition, it reduces around 45% of model parameters compared to the R2U-Net model. Models are trained and evaluated on a benchmark dataset containing 1500 dental panoramic X-ray images. S-R2F2U-Net achieves 97.31% of accuracy and 93.26% of dice score, showing superiority over the state-of-the-art methods. Codes are available at https://github.com/mrinal054/teethSeg_sr2f2u-net.git.



### Intervertebral Disc Labeling With Learning Shape Information, A Look Once Approach
- **Arxiv ID**: http://arxiv.org/abs/2204.02943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02943v1)
- **Published**: 2022-04-06 17:23:02+00:00
- **Updated**: 2022-04-06 17:23:02+00:00
- **Authors**: Reza Azad, Moein Heidari, Julien Cohen-Adad, Ehsan Adeli, Dorit Merhof
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and automatic segmentation of intervertebral discs from medical images is a critical task for the assessment of spine-related diseases such as osteoporosis, vertebral fractures, and intervertebral disc herniation. To date, various approaches have been developed in the literature which routinely relies on detecting the discs as the primary step. A disadvantage of many cohort studies is that the localization algorithm also yields false-positive detections. In this study, we aim to alleviate this problem by proposing a novel U-Net-based structure to predict a set of candidates for intervertebral disc locations. In our design, we integrate the image shape information (image gradients) to encourage the model to learn rich and generic geometrical information. This additional signal guides the model to selectively emphasize the contextual representation and suppress the less discriminative features. On the post-processing side, to further decrease the false positive rate, we propose a permutation invariant 'look once' model, which accelerates the candidate recovery procedure. In comparison with previous studies, our proposed approach does not need to perform the selection in an iterative fashion. The proposed method was evaluated on the spine generic public multi-center dataset and demonstrated superior performance compared to previous work. We have provided the implementation code in https://github.com/rezazad68/intervertebral-lookonce



### "The Pedestrian next to the Lamppost" Adaptive Object Graphs for Better Instantaneous Mapping
- **Arxiv ID**: http://arxiv.org/abs/2204.02944v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02944v1)
- **Published**: 2022-04-06 17:23:13+00:00
- **Updated**: 2022-04-06 17:23:13+00:00
- **Authors**: Avishkar Saha, Oscar Mendez, Chris Russell, Richard Bowden
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Estimating a semantically segmented bird's-eye-view (BEV) map from a single image has become a popular technique for autonomous control and navigation. However, they show an increase in localization error with distance from the camera. While such an increase in error is entirely expected - localization is harder at distance - much of the drop in performance can be attributed to the cues used by current texture-based models, in particular, they make heavy use of object-ground intersections (such as shadows), which become increasingly sparse and uncertain for distant objects. In this work, we address these shortcomings in BEV-mapping by learning the spatial relationship between objects in a scene. We propose a graph neural network which predicts BEV objects from a monocular image by spatially reasoning about an object within the context of other objects. Our approach sets a new state-of-the-art in BEV estimation from monocular images across three large-scale datasets, including a 50% relative improvement for objects on nuScenes.



### Video Demoireing with Relation-Based Temporal Consistency
- **Arxiv ID**: http://arxiv.org/abs/2204.02957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02957v1)
- **Published**: 2022-04-06 17:45:38+00:00
- **Updated**: 2022-04-06 17:45:38+00:00
- **Authors**: Peng Dai, Xin Yu, Lan Ma, Baoheng Zhang, Jia Li, Wenbo Li, Jiajun Shen, Xiaojuan Qi
- **Comment**: None
- **Journal**: None
- **Summary**: Moire patterns, appearing as color distortions, severely degrade image and video qualities when filming a screen with digital cameras. Considering the increasing demands for capturing videos, we study how to remove such undesirable moire patterns in videos, namely video demoireing. To this end, we introduce the first hand-held video demoireing dataset with a dedicated data collection pipeline to ensure spatial and temporal alignments of captured data. Further, a baseline video demoireing model with implicit feature space alignment and selective feature aggregation is developed to leverage complementary information from nearby frames to improve frame-level video demoireing. More importantly, we propose a relation-based temporal consistency loss to encourage the model to learn temporal consistency priors directly from ground-truth reference videos, which facilitates producing temporally consistent predictions and effectively maintains frame-level qualities. Extensive experiments manifest the superiority of our model. Code is available at \url{https://daipengwa.github.io/VDmoire_ProjectPage/}.



### LEAD: Self-Supervised Landmark Estimation by Aligning Distributions of Feature Similarity
- **Arxiv ID**: http://arxiv.org/abs/2204.02958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02958v1)
- **Published**: 2022-04-06 17:48:18+00:00
- **Updated**: 2022-04-06 17:48:18+00:00
- **Authors**: Tejan Karmali, Abhinav Atrishi, Sai Sree Harsha, Susmit Agrawal, Varun Jampani, R. Venkatesh Babu
- **Comment**: WACV 2022. Project Page at http://sites.google.com/view/lead-wacv22
- **Journal**: None
- **Summary**: In this work, we introduce LEAD, an approach to discover landmarks from an unannotated collection of category-specific images. Existing works in self-supervised landmark detection are based on learning dense (pixel-level) feature representations from an image, which are further used to learn landmarks in a semi-supervised manner. While there have been advances in self-supervised learning of image features for instance-level tasks like classification, these methods do not ensure dense equivariant representations. The property of equivariance is of interest for dense prediction tasks like landmark estimation. In this work, we introduce an approach to enhance the learning of dense equivariant representations in a self-supervised fashion. We follow a two-stage training approach: first, we train a network using the BYOL objective which operates at an instance level. The correspondences obtained through this network are further used to train a dense and compact representation of the image using a lightweight network. We show that having such a prior in the feature extractor helps in landmark detection, even under drastically limited number of annotations while also improving generalization across scale variations.



### Simple and Effective Synthesis of Indoor 3D Scenes
- **Arxiv ID**: http://arxiv.org/abs/2204.02960v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.02960v2)
- **Published**: 2022-04-06 17:54:46+00:00
- **Updated**: 2022-12-01 16:19:57+00:00
- **Authors**: Jing Yu Koh, Harsh Agrawal, Dhruv Batra, Richard Tucker, Austin Waters, Honglak Lee, Yinfei Yang, Jason Baldridge, Peter Anderson
- **Comment**: AAAI 2023
- **Journal**: None
- **Summary**: We study the problem of synthesizing immersive 3D indoor scenes from one or more images. Our aim is to generate high-resolution images and videos from novel viewpoints, including viewpoints that extrapolate far beyond the input images while maintaining 3D consistency. Existing approaches are highly complex, with many separately trained stages and components. We propose a simple alternative: an image-to-image GAN that maps directly from reprojections of incomplete point clouds to full high-resolution RGB-D images. On the Matterport3D and RealEstate10K datasets, our approach significantly outperforms prior work when evaluated by humans, as well as on FID scores. Further, we show that our model is useful for generative data augmentation. A vision-and-language navigation (VLN) agent trained with trajectories spatially-perturbed by our model improves success rate by up to 1.5% over a state of the art baseline on the R2R benchmark. Our code will be made available to facilitate generative data augmentation and applications to downstream robotics and embodied AI tasks.



### SMU-Net: Style matching U-Net for brain tumor segmentation with missing modalities
- **Arxiv ID**: http://arxiv.org/abs/2204.02961v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02961v1)
- **Published**: 2022-04-06 17:55:19+00:00
- **Updated**: 2022-04-06 17:55:19+00:00
- **Authors**: Reza Azad, Nika Khosravi, Dorit Merhof
- **Comment**: None
- **Journal**: None
- **Summary**: Gliomas are one of the most prevalent types of primary brain tumours, accounting for more than 30\% of all cases and they develop from the glial stem or progenitor cells. In theory, the majority of brain tumours could well be identified exclusively by the use of Magnetic Resonance Imaging (MRI). Each MRI modality delivers distinct information on the soft tissue of the human brain and integrating all of them would provide comprehensive data for the accurate segmentation of the glioma, which is crucial for the patient's prognosis, diagnosis, and determining the best follow-up treatment. Unfortunately, MRI is prone to artifacts for a variety of reasons, which might result in missing one or more MRI modalities. Various strategies have been proposed over the years to synthesize the missing modality or compensate for the influence it has on automated segmentation models. However, these methods usually fail to model the underlying missing information. In this paper, we propose a style matching U-Net (SMU-Net) for brain tumour segmentation on MRI images. Our co-training approach utilizes a content and style-matching mechanism to distill the informative features from the full-modality network into a missing modality network. To do so, we encode both full-modality and missing-modality data into a latent space, then we decompose the representation space into a style and content representation. Our style matching module adaptively recalibrates the representation space by learning a matching function to transfer the informative and textural features from a full-modality path into a missing-modality path. Moreover, by modelling the mutual information, our content module surpasses the less informative features and re-calibrates the representation space based on discriminative semantic features. The evaluation process on the BraTS 2018 dataset shows a significant results.



### Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.02964v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02964v2)
- **Published**: 2022-04-06 17:59:04+00:00
- **Updated**: 2022-05-19 03:41:11+00:00
- **Authors**: Yuxin Fang, Shusheng Yang, Shijie Wang, Yixiao Ge, Ying Shan, Xinggang Wang
- **Comment**: v2: more analysis & stronger results. Preprint. Work in progress.
  Code and pre-trained models are available at https://github.com/hustvl/MIMDet
- **Journal**: None
- **Summary**: We present an approach to efficiently and effectively adapt a masked image modeling (MIM) pre-trained vanilla Vision Transformer (ViT) for object detection, which is based on our two novel observations: (i) A MIM pre-trained vanilla ViT encoder can work surprisingly well in the challenging object-level recognition scenario even with randomly sampled partial observations, e.g., only 25% $\sim$ 50% of the input embeddings. (ii) In order to construct multi-scale representations for object detection from single-scale ViT, a randomly initialized compact convolutional stem supplants the pre-trained large kernel patchify stem, and its intermediate features can naturally serve as the higher resolution inputs of a feature pyramid network without further upsampling or other manipulations. While the pre-trained ViT is only regarded as the 3$^{rd}$-stage of our detector's backbone instead of the whole feature extractor. This results in a ConvNet-ViT hybrid feature extractor. The proposed detector, named MIMDet, enables a MIM pre-trained vanilla ViT to outperform hierarchical Swin Transformer by 2.5 box AP and 2.6 mask AP on COCO, and achieves better results compared with the previous best adapted vanilla ViT detector using a more modest fine-tuning recipe while converging 2.8$\times$ faster. Code and pre-trained models are available at https://github.com/hustvl/MIMDet.



### LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification
- **Arxiv ID**: http://arxiv.org/abs/2204.02965v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.02965v1)
- **Published**: 2022-04-06 17:59:10+00:00
- **Updated**: 2022-04-06 17:59:10+00:00
- **Authors**: Sharath Girish, Kamal Gupta, Saurabh Singh, Abhinav Shrivastava
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce LilNetX, an end-to-end trainable technique for neural networks that enables learning models with specified accuracy-rate-computation trade-off. Prior works approach these problems one at a time and often require post-processing or multistage training which become less practical and do not scale very well for large datasets or architectures. Our method constructs a joint training objective that penalizes the self-information of network parameters in a reparameterized latent space to encourage small model size while also introducing priors to increase structured sparsity in the parameter space to reduce computation. We achieve up to 50% smaller model size and 98% model sparsity on ResNet-20 while retaining the same accuracy on the CIFAR-10 dataset as well as 35% smaller model size and 42% structured sparsity on ResNet-50 trained on ImageNet, when compared to existing state-of-the-art model compression methods. Code is available at https://github.com/Sharath-girish/LilNetX.



### Temporal Alignment Networks for Long-term Video
- **Arxiv ID**: http://arxiv.org/abs/2204.02968v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.02968v1)
- **Published**: 2022-04-06 17:59:46+00:00
- **Updated**: 2022-04-06 17:59:46+00:00
- **Authors**: Tengda Han, Weidi Xie, Andrew Zisserman
- **Comment**: CVPR2022 Oral, 16 pages
- **Journal**: None
- **Summary**: The objective of this paper is a temporal alignment network that ingests long term video sequences, and associated text sentences, in order to: (1) determine if a sentence is alignable with the video; and (2) if it is alignable, then determine its alignment. The challenge is to train such networks from large-scale datasets, such as HowTo100M, where the associated text sentences have significant noise, and are only weakly aligned when relevant. Apart from proposing the alignment network, we also make four contributions: (i) we describe a novel co-training method that enables to denoise and train on raw instructional videos without using manual annotation, despite the considerable noise; (ii) to benchmark the alignment performance, we manually curate a 10-hour subset of HowTo100M, totalling 80 videos, with sparse temporal descriptions. Our proposed model, trained on HowTo100M, outperforms strong baselines (CLIP, MIL-NCE) on this alignment dataset by a significant margin; (iii) we apply the trained model in the zero-shot settings to multiple downstream video understanding tasks and achieve state-of-the-art results, including text-video retrieval on YouCook2, and weakly supervised video action segmentation on Breakfast-Action; (iv) we use the automatically aligned HowTo100M annotations for end-to-end finetuning of the backbone model, and obtain improved performance on downstream action recognition tasks.



### EfficientCellSeg: Efficient Volumetric Cell Segmentation Using Context Aware Pseudocoloring
- **Arxiv ID**: http://arxiv.org/abs/2204.03014v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.03014v2)
- **Published**: 2022-04-06 18:02:15+00:00
- **Updated**: 2022-11-28 10:38:56+00:00
- **Authors**: Royden Wagner, Karl Rohr
- **Comment**: Accepted at MIDL 2022 (Oral); Updated link to challenge submission
- **Journal**: None
- **Summary**: Volumetric cell segmentation in fluorescence microscopy images is important to study a wide variety of cellular processes. Applications range from the analysis of cancer cells to behavioral studies of cells in the embryonic stage. Like in other computer vision fields, most recent methods use either large convolutional neural networks (CNNs) or vision transformer models (ViTs). Since the number of available 3D microscopy images is typically limited in applications, we take a different approach and introduce a small CNN for volumetric cell segmentation. Compared to previous CNN models for cell segmentation, our model is efficient and has an asymmetric encoder-decoder structure with very few parameters in the decoder. Training efficiency is further improved via transfer learning. In addition, we introduce Context Aware Pseudocoloring to exploit spatial context in z-direction of 3D images while performing volumetric cell segmentation slice-wise. We evaluated our method using different 3D datasets from the Cell Segmentation Benchmark of the Cell Tracking Challenge. Our segmentation method achieves top-ranking results, while our CNN model has an up to 25x lower number of parameters than other top-ranking methods. Code and pretrained models are available at: https://github.com/roydenwa/efficient-cell-seg



### Learning from Untrimmed Videos: Self-Supervised Video Representation Learning with Hierarchical Consistency
- **Arxiv ID**: http://arxiv.org/abs/2204.03017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03017v1)
- **Published**: 2022-04-06 18:04:54+00:00
- **Updated**: 2022-04-06 18:04:54+00:00
- **Authors**: Zhiwu Qing, Shiwei Zhang, Ziyuan Huang, Yi Xu, Xiang Wang, Mingqian Tang, Changxin Gao, Rong Jin, Nong Sang
- **Comment**: CVPR2022; Project page is: https://hico-cvpr2022.github.io/
- **Journal**: None
- **Summary**: Natural videos provide rich visual contents for self-supervised learning. Yet most existing approaches for learning spatio-temporal representations rely on manually trimmed videos, leading to limited diversity in visual patterns and limited performance gain. In this work, we aim to learn representations by leveraging more abundant information in untrimmed videos. To this end, we propose to learn a hierarchy of consistencies in videos, i.e., visual consistency and topical consistency, corresponding respectively to clip pairs that tend to be visually similar when separated by a short time span and share similar topics when separated by a long time span. Specifically, a hierarchical consistency learning framework HiCo is presented, where the visually consistent pairs are encouraged to have the same representation through contrastive learning, while the topically consistent pairs are coupled through a topical classifier that distinguishes whether they are topic related. Further, we impose a gradual sampling algorithm for proposed hierarchical consistency learning, and demonstrate its theoretical superiority. Empirically, we show that not only HiCo can generate stronger representations on untrimmed videos, it also improves the representation quality when applied to trimmed videos. This is in contrast to standard contrastive learning that fails to learn appropriate representations from untrimmed videos.



### Statistical Model Criticism of Variational Auto-Encoders
- **Arxiv ID**: http://arxiv.org/abs/2204.03030v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2204.03030v1)
- **Published**: 2022-04-06 18:19:29+00:00
- **Updated**: 2022-04-06 18:19:29+00:00
- **Authors**: Claartje Barkhof, Wilker Aziz
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a framework for the statistical evaluation of variational auto-encoders (VAEs) and test two instances of this framework in the context of modelling images of handwritten digits and a corpus of English text. Our take on evaluation is based on the idea of statistical model criticism, popular in Bayesian data analysis, whereby a statistical model is evaluated in terms of its ability to reproduce statistics of an unknown data generating process from which we can obtain samples. A VAE learns not one, but two joint distributions over a shared sample space, each exploiting a choice of factorisation that makes sampling tractable in one of two directions (latent-to-data, data-to-latent). We evaluate samples from these distributions, assessing their (marginal) fit to the observed data and our choice of prior, and we also evaluate samples through a pipeline that connects the two distributions starting from a data sample, assessing whether together they exploit and reveal latent factors of variation that are useful to a practitioner. We show that this methodology offers possibilities for model selection qualitatively beyond intrinsic evaluation metrics and at a finer granularity than commonly used statistics can offer.



### DSGN++: Exploiting Visual-Spatial Relation for Stereo-based 3D Detectors
- **Arxiv ID**: http://arxiv.org/abs/2204.03039v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03039v3)
- **Published**: 2022-04-06 18:43:54+00:00
- **Updated**: 2022-07-21 12:08:06+00:00
- **Authors**: Yilun Chen, Shijia Huang, Shu Liu, Bei Yu, Jiaya Jia
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Camera-based 3D object detectors are welcome due to their wider deployment and lower price than LiDAR sensors. We first revisit the prior stereo detector DSGN for its stereo volume construction ways for representing both 3D geometry and semantics. We polish the stereo modeling and propose the advanced version, DSGN++, aiming to enhance effective information flow throughout the 2D-to-3D pipeline in three main aspects. First, to effectively lift the 2D information to stereo volume, we propose depth-wise plane sweeping (DPS) that allows denser connections and extracts depth-guided features. Second, for grasping differently spaced features, we present a novel stereo volume -- Dual-view Stereo Volume (DSV) that integrates front-view and top-view features and reconstructs sub-voxel depth in the camera frustum. Third, as the foreground region becomes less dominant in 3D space, we propose a multi-modal data editing strategy -- Stereo-LiDAR Copy-Paste, which ensures cross-modal alignment and improves data efficiency. Without bells and whistles, extensive experiments in various modality setups on the popular KITTI benchmark show that our method consistently outperforms other camera-based 3D detectors for all categories. Code is available at https://github.com/chenyilun95/DSGN2.



### Fusing finetuned models for better pretraining
- **Arxiv ID**: http://arxiv.org/abs/2204.03044v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.03044v1)
- **Published**: 2022-04-06 18:54:48+00:00
- **Updated**: 2022-04-06 18:54:48+00:00
- **Authors**: Leshem Choshen, Elad Venezian, Noam Slonim, Yoav Katz
- **Comment**: None
- **Journal**: None
- **Summary**: Pretrained models are the standard starting point for training. This approach consistently outperforms the use of a random initialization. However, pretraining is a costly endeavour that few can undertake.   In this paper, we create better base models at hardly any cost, by fusing multiple existing fine tuned models into one. Specifically, we fuse by averaging the weights of these models. We show that the fused model results surpass the pretrained model ones. We also show that fusing is often better than intertraining.   We find that fusing is less dependent on the target task. Furthermore, weight decay nullifies intertraining effects but not those of fusing.



### Thermal to Visible Image Synthesis under Atmospheric Turbulence
- **Arxiv ID**: http://arxiv.org/abs/2204.03057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03057v1)
- **Published**: 2022-04-06 19:47:41+00:00
- **Updated**: 2022-04-06 19:47:41+00:00
- **Authors**: Kangfu Mei, Yiqun Mei, Vishal M. Patel
- **Comment**: 4 pages, 3 figures
- **Journal**: None
- **Summary**: In many practical applications of long-range imaging such as biometrics and surveillance, thermal imagining modalities are often used to capture images in low-light and nighttime conditions. However, such imaging systems often suffer from atmospheric turbulence, which introduces severe blur and deformation artifacts to the captured images. Such an issue is unavoidable in long-range imaging and significantly decreases the face verification accuracy. In this paper, we first investigate the problem with a turbulence simulation method on real-world thermal images. An end-to-end reconstruction method is then proposed which can directly transform thermal images into visible-spectrum images by utilizing natural image priors based on a pre-trained StyleGAN2 network. Compared with the existing two-steps methods of consecutive turbulence mitigation and thermal to visible image translation, our method is demonstrated to be effective in terms of both the visual quality of the reconstructed results and face verification accuracy. Moreover, to the best of our knowledge, this is the first work that studies the problem of thermal to visible image translation under atmospheric turbulence.



### Late multimodal fusion for image and audio music transcription
- **Arxiv ID**: http://arxiv.org/abs/2204.03063v3
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.IR, cs.SD, eess.AS, H.3; H.4; I.4; I.5; J.6
- **Links**: [PDF](http://arxiv.org/pdf/2204.03063v3)
- **Published**: 2022-04-06 20:00:33+00:00
- **Updated**: 2022-08-26 10:09:51+00:00
- **Authors**: María Alfaro-Contreras, Jose J. Valero-Mas, José M. Iñesta, Jorge Calvo-Zaragoza
- **Comment**: None
- **Journal**: None
- **Summary**: Music transcription, which deals with the conversion of music sources into a structured digital format, is a key problem for Music Information Retrieval (MIR). When addressing this challenge in computational terms, the MIR community follows two lines of research: music documents, which is the case of Optical Music Recognition (OMR), or audio recordings, which is the case of Automatic Music Transcription (AMT). The different nature of the aforementioned input data has conditioned these fields to develop modality-specific frameworks. However, their recent definition in terms of sequence labeling tasks leads to a common output representation, which enables research on a combined paradigm. In this respect, multimodal image and audio music transcription comprises the challenge of effectively combining the information conveyed by image and audio modalities. In this work, we explore this question at a late-fusion level: we study four combination approaches in order to merge, for the first time, the hypotheses regarding end-to-end OMR and AMT systems in a lattice-based search space. The results obtained for a series of performance scenarios -- in which the corresponding single-modality models yield different error rates -- showed interesting benefits of these approaches. In addition, two of the four strategies considered significantly improve the corresponding unimodal standard recognition frameworks.



### The Self-Optimal-Transport Feature Transform
- **Arxiv ID**: http://arxiv.org/abs/2204.03065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03065v1)
- **Published**: 2022-04-06 20:00:39+00:00
- **Updated**: 2022-04-06 20:00:39+00:00
- **Authors**: Daniel Shalam, Simon Korman
- **Comment**: None
- **Journal**: None
- **Summary**: The Self-Optimal-Transport (SOT) feature transform is designed to upgrade the set of features of a data instance to facilitate downstream matching or grouping related tasks. The transformed set encodes a rich representation of high order relations between the instance features. Distances between transformed features capture their direct original similarity and their third party agreement regarding similarity to other features in the set. A particular min-cost-max-flow fractional matching problem, whose entropy regularized version can be approximated by an optimal transport (OT) optimization, results in our transductive transform which is efficient, differentiable, equivariant, parameterless and probabilistically interpretable. Empirically, the transform is highly effective and flexible in its use, consistently improving networks it is inserted into, in a variety of tasks and training schemes. We demonstrate its merits through the problem of unsupervised clustering and its efficiency and wide applicability for few-shot-classification, with state-of-the-art results, and large-scale person re-identification.



### OSCARS: An Outlier-Sensitive Content-Based Radiography Retrieval System
- **Arxiv ID**: http://arxiv.org/abs/2204.03074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03074v1)
- **Published**: 2022-04-06 20:18:35+00:00
- **Updated**: 2022-04-06 20:18:35+00:00
- **Authors**: Xiaoyuan Guo, Jiali Duan, Saptarshi Purkayastha, Hari Trivedi, Judy Wawira Gichoya, Imon Banerjee
- **Comment**: 12 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: Improving the retrieval relevance on noisy datasets is an emerging need for the curation of a large-scale clean dataset in the medical domain. While existing methods can be applied for class-wise retrieval (aka. inter-class), they cannot distinguish the granularity of likeness within the same class (aka. intra-class). The problem is exacerbated on medical external datasets, where noisy samples of the same class are treated equally during training. Our goal is to identify both intra/inter-class similarities for fine-grained retrieval. To achieve this, we propose an Outlier-Sensitive Content-based rAdiologhy Retrieval System (OSCARS), consisting of two steps. First, we train an outlier detector on a clean internal dataset in an unsupervised manner. Then we use the trained detector to generate the anomaly scores on the external dataset, whose distribution will be used to bin intra-class variations. Second, we propose a quadruplet (a, p, nintra, ninter) sampling strategy, where intra-class negatives nintra are sampled from bins of the same class other than the bin anchor a belongs to, while niner are randomly sampled from inter-classes. We suggest a weighted metric learning objective to balance the intra and inter-class feature learning. We experimented on two representative public radiography datasets. Experiments show the effectiveness of our approach. The training and evaluation code can be found in https://github.com/XiaoyuanGuo/oscars.



### PlutoNet: An Efficient Polyp Segmentation Network with Modified Partial Decoder and Decoder Consistency Training
- **Arxiv ID**: http://arxiv.org/abs/2204.03652v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.03652v4)
- **Published**: 2022-04-06 20:29:00+00:00
- **Updated**: 2023-03-18 16:25:15+00:00
- **Authors**: Tugberk Erol, Duygu Sarikaya
- **Comment**: 10 pages, 2 figures, 2 tables
- **Journal**: None
- **Summary**: Deep learning models are used to minimize the number of polyps that goes unnoticed by the experts and to accurately segment the detected polyps during interventions. Although state-of-the-art models are proposed, it remains a challenge to define representations that are able to generalize well and that mediate between capturing low-level features and higher-level semantic details without being redundant. Another challenge with these models is that they require too many parameters, which can pose a problem with real-time applications. To address these problems, we propose PlutoNet for polyp segmentation which requires only 2,626,537 parameters, less than 10\% of the parameters required by its counterparts. With PlutoNet, we propose a novel \emph{decoder consistency training} approach that consists of a shared encoder, the modified partial decoder which is a combination of the partial decoder and full-scale connections that capture salient features at different scales without being redundant, and the auxiliary decoder which focuses on higher-level relevant semantic features. We train the modified partial decoder and the auxiliary decoder with a combined loss to enforce consistency, which helps improve the encoders representations. This way we are able to reduce uncertainty and false positive rates. We perform ablation studies and extensive experiments which show that PlutoNet performs significantly better than the state-of-the-art models, particularly on unseen datasets and datasets across different domains.



### Instance Segmentation of Unlabeled Modalities via Cyclic Segmentation GAN
- **Arxiv ID**: http://arxiv.org/abs/2204.03082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03082v1)
- **Published**: 2022-04-06 20:46:39+00:00
- **Updated**: 2022-04-06 20:46:39+00:00
- **Authors**: Leander Lauenburg, Zudi Lin, Ruihan Zhang, Márcia dos Santos, Siyu Huang, Ignacio Arganda-Carreras, Edward S. Boyden, Hanspeter Pfister, Donglai Wei
- **Comment**: 13 pages with appendix
- **Journal**: None
- **Summary**: Instance segmentation for unlabeled imaging modalities is a challenging but essential task as collecting expert annotation can be expensive and time-consuming. Existing works segment a new modality by either deploying a pre-trained model optimized on diverse training data or conducting domain translation and image segmentation as two independent steps. In this work, we propose a novel Cyclic Segmentation Generative Adversarial Network (CySGAN) that conducts image translation and instance segmentation jointly using a unified framework. Besides the CycleGAN losses for image translation and supervised losses for the annotated source domain, we introduce additional self-supervised and segmentation-based adversarial objectives to improve the model performance by leveraging unlabeled target domain images. We benchmark our approach on the task of 3D neuronal nuclei segmentation with annotated electron microscopy (EM) images and unlabeled expansion microscopy (ExM) data. Our CySGAN outperforms both pretrained generalist models and the baselines that sequentially conduct image translation and segmentation. Our implementation and the newly collected, densely annotated ExM nuclei dataset, named NucExM, are available at https://connectomics-bazaar.github.io/proj/CySGAN/index.html.



### Audio-Visual Person-of-Interest DeepFake Detection
- **Arxiv ID**: http://arxiv.org/abs/2204.03083v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2204.03083v3)
- **Published**: 2022-04-06 20:51:40+00:00
- **Updated**: 2023-05-18 06:56:42+00:00
- **Authors**: Davide Cozzolino, Alessandro Pianese, Matthias Nießner, Luisa Verdoliva
- **Comment**: None
- **Journal**: None
- **Summary**: Face manipulation technology is advancing very rapidly, and new methods are being proposed day by day. The aim of this work is to propose a deepfake detector that can cope with the wide variety of manipulation methods and scenarios encountered in the real world. Our key insight is that each person has specific characteristics that a synthetic generator likely cannot reproduce. Accordingly, we extract audio-visual features which characterize the identity of a person, and use them to create a person-of-interest (POI) deepfake detector. We leverage a contrastive learning paradigm to learn the moving-face and audio segment embeddings that are most discriminative for each identity. As a result, when the video and/or audio of a person is manipulated, its representation in the embedding space becomes inconsistent with the real identity, allowing reliable detection. Training is carried out exclusively on real talking-face video; thus, the detector does not depend on any specific manipulation method and yields the highest generalization ability. In addition, our method can detect both single-modality (audio-only, video-only) and multi-modality (audio-video) attacks, and is robust to low-quality or corrupted videos. Experiments on a wide variety of datasets confirm that our method ensures a SOTA performance, especially on low quality videos. Code is publicly available on-line at https://github.com/grip-unina/poi-forensics.



### Hierarchical Self-supervised Representation Learning for Movie Understanding
- **Arxiv ID**: http://arxiv.org/abs/2204.03101v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03101v1)
- **Published**: 2022-04-06 21:28:41+00:00
- **Updated**: 2022-04-06 21:28:41+00:00
- **Authors**: Fanyi Xiao, Kaustav Kundu, Joseph Tighe, Davide Modolo
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Most self-supervised video representation learning approaches focus on action recognition. In contrast, in this paper we focus on self-supervised video learning for movie understanding and propose a novel hierarchical self-supervised pretraining strategy that separately pretrains each level of our hierarchical movie understanding model (based on [37]). Specifically, we propose to pretrain the low-level video backbone using a contrastive learning objective, while pretrain the higher-level video contextualizer using an event mask prediction task, which enables the usage of different data sources for pretraining different levels of the hierarchy. We first show that our self-supervised pretraining strategies are effective and lead to improved performance on all tasks and metrics on VidSitu benchmark [37] (e.g., improving on semantic role prediction from 47% to 61% CIDEr scores). We further demonstrate the effectiveness of our contextualized event features on LVU tasks [54], both when used alone and when combined with instance features, showing their complementarity.



### AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2204.03105v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.03105v1)
- **Published**: 2022-04-06 21:39:24+00:00
- **Updated**: 2022-04-06 21:39:24+00:00
- **Authors**: Zhiqin Chen, Kangxue Yin, Sanja Fidler
- **Comment**: CVPR 2022. Project page: https://nv-tlabs.github.io/AUV-NET
- **Journal**: None
- **Summary**: In this paper, we address the problem of texture representation for 3D shapes for the challenging and underexplored tasks of texture transfer and synthesis. Previous works either apply spherical texture maps which may lead to large distortions, or use continuous texture fields that yield smooth outputs lacking details. We argue that the traditional way of representing textures with images and linking them to a 3D mesh via UV mapping is more desirable, since synthesizing 2D images is a well-studied problem. We propose AUV-Net which learns to embed 3D surfaces into a 2D aligned UV space, by mapping the corresponding semantic parts of different 3D shapes to the same location in the UV space. As a result, textures are aligned across objects, and can thus be easily synthesized by generative models of images. Texture alignment is learned in an unsupervised manner by a simple yet effective texture alignment module, taking inspiration from traditional works on linear subspace learning. The learned UV mapping and aligned texture representations enable a variety of applications including texture transfer, texture synthesis, and textured single view 3D reconstruction. We conduct experiments on multiple datasets to demonstrate the effectiveness of our method. Project page: https://nv-tlabs.github.io/AUV-NET.



### UIGR: Unified Interactive Garment Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2204.03111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.03111v1)
- **Published**: 2022-04-06 21:54:14+00:00
- **Updated**: 2022-04-06 21:54:14+00:00
- **Authors**: Xiao Han, Sen He, Li Zhang, Yi-Zhe Song, Tao Xiang
- **Comment**: CVPRW 2022
- **Journal**: None
- **Summary**: Interactive garment retrieval (IGR) aims to retrieve a target garment image based on a reference garment image along with user feedback on what to change on the reference garment. Two IGR tasks have been studied extensively: text-guided garment retrieval (TGR) and visually compatible garment retrieval (VCR). The user feedback for the former indicates what semantic attributes to change with the garment category preserved, while the category is the only thing to be changed explicitly for the latter, with an implicit requirement on style preservation. Despite the similarity between these two tasks and the practical need for an efficient system tackling both, they have never been unified and modeled jointly. In this paper, we propose a Unified Interactive Garment Retrieval (UIGR) framework to unify TGR and VCR. To this end, we first contribute a large-scale benchmark suited for both problems. We further propose a strong baseline architecture to integrate TGR and VCR in one model. Extensive experiments suggest that unifying two tasks in one framework is not only more efficient by requiring a single model only, it also leads to better performance. Code and datasets are available at https://github.com/BrandonHanx/CompFashion.



### AutoCOR: Autonomous Condylar Offset Ratio Calculator on TKA-Postoperative Lateral Knee X-ray
- **Arxiv ID**: http://arxiv.org/abs/2204.03120v1
- **DOI**: None
- **Categories**: **cs.CV**, 92C55 (Primary)
- **Links**: [PDF](http://arxiv.org/pdf/2204.03120v1)
- **Published**: 2022-04-06 22:49:58+00:00
- **Updated**: 2022-04-06 22:49:58+00:00
- **Authors**: Gulsade Rabia Cakmak, Ibrahim Ethem Hamamci, Mehmet Kursat Yilmaz, Reda Alhajj, Ibrahim Azboy, Mehmet Kemal Ozdemir
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: The postoperative range of motion is one of the crucial factors indicating the outcome of Total Knee Arthroplasty (TKA). Although the correlation between range of knee flexion and posterior condylar offset (PCO) is controversial in the literature, PCO maintains its importance on evaluation of TKA. Due to limitations on PCO measurement, two novel parameters, posterior condylar offset ratio (PCOR) and anterior condylar offset ratio (ACOR), were introduced. Nowadays, the calculation of PCOR and ACOR on plain lateral radiographs is done manually by orthopedic surgeons. In this regard, we developed a software, AutoCOR, to calculate PCOR and ACOR autonomously, utilizing unsupervised machine learning algorithm (k-means clustering) and digital image processing techniques. The software AutoCOR is capable of detecting the anterior/posterior edge points and anterior/posterior cortex of the femoral shaft on true postoperative lateral conventional radiographs. To test the algorithm, 50 postoperative true lateral radiographs from Istanbul Kosuyolu Medipol Hospital Database were used (32 patients). The mean PCOR was 0.984 (SD 0.235) in software results and 0.972 (SD 0.164) in ground truth values. It shows strong and significant correlation between software and ground truth values (Pearson r=0.845 p<0.0001). The mean ACOR was 0.107 (SD 0.092) in software results and 0.107 (SD 0.070) in ground truth values. It shows moderate and significant correlation between software and ground truth values (Spearman's rs=0.519 p=0.0001412). We suggest that AutoCOR is a useful tool that can be used in clinical practice.



