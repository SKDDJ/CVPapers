# Arxiv Papers in cs.CV on 2022-04-16
### Safe Self-Refinement for Transformer-based Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2204.07683v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07683v1)
- **Published**: 2022-04-16 00:15:46+00:00
- **Updated**: 2022-04-16 00:15:46+00:00
- **Authors**: Tao Sun, Cheng Lu, Tianshuo Zhang, Haibin Ling
- **Comment**: To appear in CVPR 2022
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) aims to leverage a label-rich source domain to solve tasks on a related unlabeled target domain. It is a challenging problem especially when a large domain gap lies between the source and target domains. In this paper we propose a novel solution named SSRT (Safe Self-Refinement for Transformer-based domain adaptation), which brings improvement from two aspects. First, encouraged by the success of vision transformers in various vision tasks, we arm SSRT with a transformer backbone. We find that the combination of vision transformer with simple adversarial adaptation surpasses best reported Convolutional Neural Network (CNN)-based results on the challenging DomainNet benchmark, showing its strong transferable feature representation. Second, to reduce the risk of model collapse and improve the effectiveness of knowledge transfer between domains with large gaps, we propose a Safe Self-Refinement strategy. Specifically, SSRT utilizes predictions of perturbed target domain data to refine the model. Since the model capacity of vision transformer is large and predictions in such challenging tasks can be noisy, a safe training mechanism is designed to adaptively adjust learning configuration. Extensive evaluations are conducted on several widely tested UDA benchmarks and SSRT achieves consistently the best performances, including 85.43% on Office-Home, 88.76% on VisDA-2017 and 45.2% on DomainNet.



### Privacy-Preserving Image Classification Using Isotropic Network
- **Arxiv ID**: http://arxiv.org/abs/2204.07707v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.07707v1)
- **Published**: 2022-04-16 03:15:54+00:00
- **Updated**: 2022-04-16 03:15:54+00:00
- **Authors**: AprilPyone MaungMaung, Hitoshi Kiya
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a privacy-preserving image classification method that uses encrypted images and an isotropic network such as the vision transformer. The proposed method allows us not only to apply images without visual information to deep neural networks (DNNs) for both training and testing but also to maintain a high classification accuracy. In addition, compressible encrypted images, called encryption-then-compression (EtC) images, can be used for both training and testing without any adaptation network. Previously, to classify EtC images, an adaptation network was required before a classification network, so methods with an adaptation network have been only tested on small images. To the best of our knowledge, previous privacy-preserving image classification methods have never considered image compressibility and patch embedding-based isotropic networks. In an experiment, the proposed privacy-preserving image classification was demonstrated to outperform state-of-the-art methods even when EtC images were used in terms of classification accuracy and robustness against various attacks under the use of two isotropic networks: vision transformer and ConvMixer.



### GAUSS: Guided Encoder-Decoder Architecture for Hyperspectral Unmixing with Spatial Smoothness
- **Arxiv ID**: http://arxiv.org/abs/2204.07713v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.07713v2)
- **Published**: 2022-04-16 04:23:47+00:00
- **Updated**: 2022-12-20 16:35:44+00:00
- **Authors**: Yasiru Ranasinghe, Kavinga Weerasooriya, Roshan Godaliyadda, Vijitha Herath, Parakrama Ekanayake, Dhananjaya Jayasundara, Lakshitha Ramanayake, Neranjan Senarath, Dulantha Wickramasinghe
- **Comment**: 16 pages, 6 figures
- **Journal**: None
- **Summary**: In recent hyperspectral unmixing (HU) literature, the application of deep learning (DL) has become more prominent, especially with the autoencoder (AE) architecture. We propose a split architecture and use a pseudo-ground truth for abundances to guide the `unmixing network' (UN) optimization. Preceding the UN, an `approximation network' (AN) is proposed, which will improve the association between the centre pixel and its neighbourhood. Hence, it will accentuate spatial correlation in the abundances as its output is the input to the UN and the reference for the `mixing network' (MN). In the Guided Encoder-Decoder Architecture for Hyperspectral Unmixing with Spatial Smoothness (GAUSS), we proposed using one-hot encoded abundances as the pseudo-ground truth to guide the UN; computed using the k-means algorithm to exclude the use of prior HU methods. Furthermore, we release the single-layer constraint on MN by introducing the UN generated abundances in contrast to the standard AE for HU. Secondly, we experimented with two modifications on the pre-trained network using the GAUSS method. In GAUSS$_\textit{blind}$, we have concatenated the UN and the MN to back-propagate the reconstruction error gradients to the encoder. Then, in the GAUSS$_\textit{prime}$, abundance results of a signal processing (SP) method with reliable abundance results were used as the pseudo-ground truth with the GAUSS architecture. According to quantitative and graphical results for four experimental datasets, the three architectures either transcended or equated the performance of existing HU algorithms from both DL and SP domains.



### Pushing the Performance Limit of Scene Text Recognizer without Human Annotation
- **Arxiv ID**: http://arxiv.org/abs/2204.07714v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07714v2)
- **Published**: 2022-04-16 04:42:02+00:00
- **Updated**: 2022-05-22 09:28:13+00:00
- **Authors**: Caiyuan Zheng, Hui Li, Seon-Min Rhee, Seungju Han, Jae-Joon Han, Peng Wang
- **Comment**: 10 pages, 5 figures, accepted by CVPR-2022
- **Journal**: None
- **Summary**: Scene text recognition (STR) attracts much attention over the years because of its wide application. Most methods train STR model in a fully supervised manner which requires large amounts of labeled data. Although synthetic data contributes a lot to STR, it suffers from the real-tosynthetic domain gap that restricts model performance. In this work, we aim to boost STR models by leveraging both synthetic data and the numerous real unlabeled images, exempting human annotation cost thoroughly. A robust consistency regularization based semi-supervised framework is proposed for STR, which can effectively solve the instability issue due to domain inconsistency between synthetic and real images. A character-level consistency regularization is designed to mitigate the misalignment between characters in sequence recognition. Extensive experiments on standard text recognition benchmarks demonstrate the effectiveness of the proposed method. It can steadily improve existing STR models, and boost an STR model to achieve new state-of-the-art results. To our best knowledge, this is the first consistency regularization based framework that applies successfully to STR.



### Interactiveness Field in Human-Object Interactions
- **Arxiv ID**: http://arxiv.org/abs/2204.07718v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07718v1)
- **Published**: 2022-04-16 05:09:25+00:00
- **Updated**: 2022-04-16 05:09:25+00:00
- **Authors**: Xinpeng Liu, Yong-Lu Li, Xiaoqian Wu, Yu-Wing Tai, Cewu Lu, Chi-Keung Tang
- **Comment**: To appear in CVPR2022
- **Journal**: None
- **Summary**: Human-Object Interaction (HOI) detection plays a core role in activity understanding. Though recent two/one-stage methods have achieved impressive results, as an essential step, discovering interactive human-object pairs remains challenging. Both one/two-stage methods fail to effectively extract interactive pairs instead of generating redundant negative pairs. In this work, we introduce a previously overlooked interactiveness bimodal prior: given an object in an image, after pairing it with the humans, the generated pairs are either mostly non-interactive, or mostly interactive, with the former more frequent than the latter. Based on this interactiveness bimodal prior we propose the "interactiveness field". To make the learned field compatible with real HOI image considerations, we propose new energy constraints based on the cardinality and difference in the inherent "interactiveness field" underlying interactive versus non-interactive pairs. Consequently, our method can detect more precise pairs and thus significantly boost HOI detection performance, which is validated on widely-used benchmarks where we achieve decent improvements over state-of-the-arts. Our code is available at https://github.com/Foruck/Interactiveness-Field.



### Stress-Testing Point Cloud Registration on Automotive LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2204.07719v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07719v2)
- **Published**: 2022-04-16 05:10:55+00:00
- **Updated**: 2022-11-25 13:20:27+00:00
- **Authors**: Amnon Drory, Shai Avidan, Raja Giryes
- **Comment**: Accepted to the NeurIPS 2022 workshop on Machine Learning for
  Autonomous Driving. Project Page:
  https://github.com/AmnonDrory/LidarRegistration
- **Journal**: None
- **Summary**: Rigid Point Cloud Registration (PCR) algorithms aim to estimate the 6-DOF relative motion between two point clouds, which is important in various fields, including autonomous driving. Recent years have seen a significant improvement in global PCR algorithms, i.e. algorithms that can handle a large relative motion. This has been demonstrated in various scenarios, including indoor scenes, but has only been minimally tested in the Automotive setting, where point clouds are produced by vehicle-mounted LiDAR sensors. In this work, we aim to answer questions that are important for automotive applications, including: which of the new algorithms is the most accurate, and which is fastest? How transferable are deep-learning approaches, e.g. what happens when you train a network with data from Boston, and run it in a vehicle in Singapore? How small can the overlap between point clouds be before the algorithms start to deteriorate? To what extent are the algorithms rotation invariant? Our results are at times surprising. When comparing robust parameter estimation methods for registration, we find that the fastest and most accurate is not one of the newest approaches. Instead, it is a modern variant of the well known RANSAC technique. We also suggest a new outlier filtering method, Grid-Prioritized Filtering (GPF), to further improve it. An additional contribution of this work is an algorithm for selecting challenging sets of frame-pairs from automotive LiDAR datasets. This enables meaningful benchmarking in the Automotive LiDAR setting, and can also improve training for learning algorithms.



### Searching Intrinsic Dimensions of Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2204.07722v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.07722v1)
- **Published**: 2022-04-16 05:16:35+00:00
- **Updated**: 2022-04-16 05:16:35+00:00
- **Authors**: Fanghui Xue, Biao Yang, Yingyong Qi, Jack Xin
- **Comment**: None
- **Journal**: None
- **Summary**: It has been shown by many researchers that transformers perform as well as convolutional neural networks in many computer vision tasks. Meanwhile, the large computational costs of its attention module hinder further studies and applications on edge devices. Some pruning methods have been developed to construct efficient vision transformers, but most of them have considered image classification tasks only. Inspired by these results, we propose SiDT, a method for pruning vision transformer backbones on more complicated vision tasks like object detection, based on the search of transformer dimensions. Experiments on CIFAR-100 and COCO datasets show that the backbones with 20\% or 40\% dimensions/parameters pruned can have similar or even better performance than the unpruned models. Moreover, we have also provided the complexity analysis and comparisons with the previous pruning methods.



### Semantic interpretation for convolutional neural networks: What makes a cat a cat?
- **Arxiv ID**: http://arxiv.org/abs/2204.07724v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.07724v1)
- **Published**: 2022-04-16 05:25:17+00:00
- **Updated**: 2022-04-16 05:25:17+00:00
- **Authors**: Hao Xu, Yuntian Chen, Dongxiao Zhang
- **Comment**: 33 pages, 11 figures
- **Journal**: None
- **Summary**: The interpretability of deep neural networks has attracted increasing attention in recent years, and several methods have been created to interpret the "black box" model. Fundamental limitations remain, however, that impede the pace of understanding the networks, especially the extraction of understandable semantic space. In this work, we introduce the framework of semantic explainable AI (S-XAI), which utilizes row-centered principal component analysis to obtain the common traits from the best combination of superpixels discovered by a genetic algorithm, and extracts understandable semantic spaces on the basis of discovered semantically sensitive neurons and visualization techniques. Statistical interpretation of the semantic space is also provided, and the concept of semantic probability is proposed for the first time. Our experimental results demonstrate that S-XAI is effective in providing a semantic interpretation for the CNN, and offers broad usage, including trustworthiness assessment and semantic sample searching.



### Bidirectional Self-Training with Multiple Anisotropic Prototypes for Domain Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.07730v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07730v2)
- **Published**: 2022-04-16 05:56:39+00:00
- **Updated**: 2022-08-04 08:38:50+00:00
- **Authors**: Yulei Lu, Yawei Luo, Li Zhang, Zheyang Li, Yi Yang, Jun Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: A thriving trend for domain adaptive segmentation endeavors to generate the high-quality pseudo labels for target domain and retrain the segmentor on them. Under this self-training paradigm, some competitive methods have sought to the latent-space information, which establishes the feature centroids (a.k.a prototypes) of the semantic classes and determines the pseudo label candidates by their distances from these centroids. In this paper, we argue that the latent space contains more information to be exploited thus taking one step further to capitalize on it. Firstly, instead of merely using the source-domain prototypes to determine the target pseudo labels as most of the traditional methods do, we bidirectionally produce the target-domain prototypes to degrade those source features which might be too hard or disturbed for the adaptation. Secondly, existing attempts simply model each category as a single and isotropic prototype while ignoring the variance of the feature distribution, which could lead to the confusion of similar categories. To cope with this issue, we propose to represent each category with multiple and anisotropic prototypes via Gaussian Mixture Model, in order to fit the de facto distribution of source domain and estimate the likelihood of target samples based on the probability density. We apply our method on GTA5->Cityscapes and Synthia->Cityscapes tasks and achieve 61.2 and 62.8 respectively in terms of mean IoU, substantially outperforming other competitive self-training methods. Noticeably, in some categories which severely suffer from the categorical confusion such as "truck" and "bus", our method achieves 56.4 and 68.8 respectively, which further demonstrates the effectiveness of our design.



### Efficient Linear Attention for Fast and Accurate Keypoint Matching
- **Arxiv ID**: http://arxiv.org/abs/2204.07731v3
- **DOI**: 10.1145/3512527.3531369
- **Categories**: **cs.CV**, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2204.07731v3)
- **Published**: 2022-04-16 06:17:36+00:00
- **Updated**: 2022-04-22 16:16:26+00:00
- **Authors**: Suwichaya Suwanwimolkul, Satoshi Komorita
- **Comment**: To be published in ACM ICMR 2022
- **Journal**: None
- **Summary**: Recently Transformers have provided state-of-the-art performance in sparse matching, crucial to realize high-performance 3D vision applications. Yet, these Transformers lack efficiency due to the quadratic computational complexity of their attention mechanism. To solve this problem, we employ an efficient linear attention for the linear computational complexity. Then, we propose a new attentional aggregation that achieves high accuracy by aggregating both the global and local information from sparse keypoints. To further improve the efficiency, we propose the joint learning of feature matching and description. Our learning enables simpler and faster matching than Sinkhorn, often used in matching the learned descriptors from Transformers. Our method achieves competitive performance with only 0.84M learnable parameters against the bigger SOTAs, SuperGlue (12M parameters) and SGMNet (30M parameters), on three benchmarks, HPatch, ETH, and Aachen Day-Night.



### GitNet: Geometric Prior-based Transformation for Birds-Eye-View Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.07733v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.07733v2)
- **Published**: 2022-04-16 06:46:45+00:00
- **Updated**: 2022-07-21 05:23:17+00:00
- **Authors**: Shi Gong, Xiaoqing Ye, Xiao Tan, Jingdong Wang, Errui Ding, Yu Zhou, Xiang Bai
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Birds-eye-view (BEV) semantic segmentation is critical for autonomous driving for its powerful spatial representation ability. It is challenging to estimate the BEV semantic maps from monocular images due to the spatial gap, since it is implicitly required to realize both the perspective-to-BEV transformation and segmentation. We present a novel two-stage Geometry Prior-based Transformation framework named GitNet, consisting of (i) the geometry-guided pre-alignment and (ii) ray-based transformer. In the first stage, we decouple the BEV segmentation into the perspective image segmentation and geometric prior-based mapping, with explicit supervision by projecting the BEV semantic labels onto the image plane to learn visibility-aware features and learnable geometry to translate into BEV space. Second, the pre-aligned coarse BEV features are further deformed by ray-based transformers to take visibility knowledge into account. GitNet achieves the leading performance on the challenging nuScenes and Argoverse Datasets.



### Robust PCA Unrolling Network for Super-resolution Vessel Extraction in X-ray Coronary Angiography
- **Arxiv ID**: http://arxiv.org/abs/2204.08466v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2204.08466v2)
- **Published**: 2022-04-16 08:19:03+00:00
- **Updated**: 2022-04-24 01:47:29+00:00
- **Authors**: Binjie Qin, Haohao Mao, Yiming Liu, Jun Zhao, Yisong Lv, Yueqi Zhu, Song Ding, Xu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Although robust PCA has been increasingly adopted to extract vessels from X-ray coronary angiography (XCA) images, challenging problems such as inefficient vessel-sparsity modelling, noisy and dynamic background artefacts, and high computational cost still remain unsolved. Therefore, we propose a novel robust PCA unrolling network with sparse feature selection for super-resolution XCA vessel imaging. Being embedded within a patch-wise spatiotemporal super-resolution framework that is built upon a pooling layer and a convolutional long short-term memory network, the proposed network can not only gradually prune complex vessel-like artefacts and noisy backgrounds in XCA during network training but also iteratively learn and select the high-level spatiotemporal semantic information of moving contrast agents flowing in the XCA-imaged vessels. The experimental results show that the proposed method significantly outperforms state-of-the-art methods, especially in the imaging of the vessel network and its distal vessels, by restoring the intensity and geometry profiles of heterogeneous vessels against complex and dynamic backgrounds.



### IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2204.08467v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.08467v2)
- **Published**: 2022-04-16 08:26:19+00:00
- **Updated**: 2023-03-29 07:25:54+00:00
- **Authors**: Meirui Jiang, Hongzheng Yang, Chen Cheng, Qi Dou
- **Comment**: Accepted by IEEE TMI special issue on federated learning for medical
  imaging
- **Journal**: None
- **Summary**: Federated learning (FL) allows multiple medical institutions to collaboratively learn a global model without centralizing client data. It is difficult, if possible at all, for such a global model to commonly achieve optimal performance for each individual client, due to the heterogeneity of medical images from various scanners and patient demographics. This problem becomes even more significant when deploying the global model to unseen clients outside the FL with unseen distributions not presented during federated training. To optimize the prediction accuracy of each individual client for medical imaging tasks, we propose a novel unified framework for both \textit{Inside and Outside model Personalization in FL} (IOP-FL). Our inside personalization uses a lightweight gradient-based approach that exploits the local adapted model for each client, by accumulating both the global gradients for common knowledge and the local gradients for client-specific optimization. Moreover, and importantly, the obtained local personalized models and the global model can form a diverse and informative routing space to personalize an adapted model for outside FL clients. Hence, we design a new test-time routing scheme using the consistency loss with a shape constraint to dynamically incorporate the models, given the distribution information conveyed by the test data. Our extensive experimental results on two medical image segmentation tasks present significant improvements over SOTA methods on both inside and outside personalization, demonstrating the potential of our IOP-FL scheme for clinical practice.



### Visual Attention Methods in Deep Learning: An In-Depth Survey
- **Arxiv ID**: http://arxiv.org/abs/2204.07756v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2204.07756v2)
- **Published**: 2022-04-16 08:57:00+00:00
- **Updated**: 2022-04-21 03:51:18+00:00
- **Authors**: Mohammed Hassanin, Saeed Anwar, Ibrahim Radwan, Fahad S Khan, Ajmal Mian
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the human cognitive system, attention is a mechanism that imitates the human cognitive awareness about specific information, amplifying critical details to focus more on the essential aspects of data. Deep learning has employed attention to boost performance for many applications. Interestingly, the same attention design can suit processing different data modalities and can easily be incorporated into large networks. Furthermore, multiple complementary attention mechanisms can be incorporated in one network. Hence, attention techniques have become extremely attractive. However, the literature lacks a comprehensive survey specific to attention techniques to guide researchers in employing attention in their deep models. Note that, besides being demanding in terms of training data and computational resources, transformers only cover a single category in self-attention out of the many categories available. We fill this gap and provide an in-depth survey of 50 attention techniques categorizing them by their most prominent features. We initiate our discussion by introducing the fundamental concepts behind the success of attention mechanism. Next, we furnish some essentials such as the strengths and limitations of each attention category, describe their fundamental building blocks, basic formulations with primary usage, and applications specifically for computer vision. We also discuss the challenges and open questions related to attention mechanism in general. Finally, we recommend possible future research directions for deep attention.



### Language-Grounded Indoor 3D Semantic Segmentation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2204.07761v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07761v2)
- **Published**: 2022-04-16 09:17:40+00:00
- **Updated**: 2022-07-28 20:42:56+00:00
- **Authors**: David Rozenberszki, Or Litany, Angela Dai
- **Comment**: Project page: https://rozdavid.github.io/scannet200, GitHub:
  https://github.com/RozDavid/LanguageGroundedSemseg, Video:
  https://www.youtube.com/watch?v=Cu-zW1oXrvU
- **Journal**: None
- **Summary**: Recent advances in 3D semantic segmentation with deep neural networks have shown remarkable success, with rapid performance increase on available datasets. However, current 3D semantic segmentation benchmarks contain only a small number of categories -- less than 30 for ScanNet and SemanticKITTI, for instance, which are not enough to reflect the diversity of real environments (e.g., semantic image understanding covers hundreds to thousands of classes). Thus, we propose to study a larger vocabulary for 3D semantic segmentation with a new extended benchmark on ScanNet data with 200 class categories, an order of magnitude more than previously studied. This large number of class categories also induces a large natural class imbalance, both of which are challenging for existing 3D semantic segmentation methods. To learn more robust 3D features in this context, we propose a language-driven pre-training method to encourage learned 3D features that might have limited training examples to lie close to their pre-trained text embeddings. Extensive experiments show that our approach consistently outperforms state-of-the-art 3D pre-training for 3D semantic segmentation on our proposed benchmark (+9% relative mIoU), including limited-data scenarios with +25% relative mIoU using only 5% annotations.



### Biometric verification of humans by means of hand geometry
- **Arxiv ID**: http://arxiv.org/abs/2204.07764v1
- **DOI**: 10.1109/CCST.2005.1594816
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2204.07764v1)
- **Published**: 2022-04-16 09:29:28+00:00
- **Updated**: 2022-04-16 09:29:28+00:00
- **Authors**: Marcos Faundez-Zanuy
- **Comment**: 8 pages, published in Proceedings 39th Annual 2005 International
  Carnahan Conference on Security Technology ICCST2005 Las Palmas, Spain. arXiv
  admin note: substantial text overlap with arXiv:2204.03925
- **Journal**: IEEE Proceedings 39th Annual 2005 International Carnahan
  Conference on Security Technology, 2005, pp. 61-67
- **Summary**: This paper describes a hand geometry biometric identification system. We have acquired a database of 22 people, 10 acquisitions per person, using a conventional document scanner. We propose a feature extraction and classifier. The experimental results reveal a maximum identification rate equal to 93.64%, and a minimum value of the detection cost function equal to 2.92% using a multi layer perceptron classifier.



### Face recognition with small and large size databases
- **Arxiv ID**: http://arxiv.org/abs/2204.08468v1
- **DOI**: 10.1109/CCST.2005.1594843
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.08468v1)
- **Published**: 2022-04-16 09:37:11+00:00
- **Updated**: 2022-04-16 09:37:11+00:00
- **Authors**: Josep roure-Alcobé, Marcos Faundez-Zanuy
- **Comment**: 5 pages, published in Proceedings 39th Annual 2005 International
  Carnahan Conference on Security Technology, Las Palmas (Spain). arXiv admin
  note: text overlap with arXiv:2204.07764, arXiv:2204.03935
- **Journal**: IEEE Proceedings 39th Annual 2005 International Carnahan
  Conference on Security Technology, 2005, pp. 153-156
- **Summary**: This paper presents experimental results using the ORL (40 people) and FERET (994 people) databases. The ORL database can be useful for securing applications where few users attempting to access are expected. This is the case, for instance, of a PDA or PC where the password is the face of the user. On the other hand, the FERET database is useful for studying those situations where the number of authorized users is around a thousand people.



### Hand Geometry Based Recognition with a MLP Classifier
- **Arxiv ID**: http://arxiv.org/abs/2204.08469v1
- **DOI**: 10.1007/11608288_96
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2204.08469v1)
- **Published**: 2022-04-16 10:01:07+00:00
- **Updated**: 2022-04-16 10:01:07+00:00
- **Authors**: Marcos Faundez-Zanuy, Miguel A. Ferrer-Ballester, Carlos M. Travieso-González, Virginia Espinosa-Duro
- **Comment**: 8 pages published in International Conference on Biometrics ICB 2006,
  Advances in Biometrics pages 721 727 Hong Kong, China. arXiv admin note:
  substantial text overlap with arXiv:2204.07764
- **Journal**: Zhang, D., Jain, A.K. (eds) Advances in Biometrics. ICB 2006.
  Lecture Notes in Computer Science, vol 3832. Springer
- **Summary**: This paper presents a biometric recognition system based on hand geometry. We describe a database specially collected for research purposes, which consists of 50 people and 10 different acquisitions of the right hand. This database can be freely downloaded. In addition, we describe a feature extraction procedure and we obtain experimental results using different classification strategies based on Multi Layer Perceptrons (MLP). We have evaluated identification rates and Detection Cost Function (DCF) values for verification applications. Experimental results reveal up to 100% identification and 0% DCF



### De-biasing facial detection system using VAE
- **Arxiv ID**: http://arxiv.org/abs/2204.09556v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.09556v1)
- **Published**: 2022-04-16 11:24:37+00:00
- **Updated**: 2022-04-16 11:24:37+00:00
- **Authors**: Vedant V. Kandge, Siddhant V. Kandge, Kajal Kumbharkar, Prof. Tanuja Pattanshetti
- **Comment**: None
- **Journal**: None
- **Summary**: Bias in AI/ML-based systems is a ubiquitous problem and bias in AI/ML systems may negatively impact society. There are many reasons behind a system being biased. The bias can be due to the algorithm we are using for our problem or may be due to the dataset we are using, having some features over-represented in it. In the face detection system bias due to the dataset is majorly seen. Sometimes models learn only features that are over-represented in data and ignore rare features from data which results in being biased toward those over-represented features. In real life, these biased systems are dangerous to society. The proposed approach uses generative models which are best suited for learning underlying features(latent variables) from the dataset and by using these learned features models try to reduce the threats which are there due to bias in the system. With the help of an algorithm, the bias present in the dataset can be removed. And then we train models on two datasets and compare the results.



### Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks
- **Arxiv ID**: http://arxiv.org/abs/2204.07780v1
- **DOI**: 10.1109/TIP.2021.3139234
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07780v1)
- **Published**: 2022-04-16 11:30:26+00:00
- **Updated**: 2022-04-16 11:30:26+00:00
- **Authors**: Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Yan Wang, Liujuan Cao, Yongjian Wu, Feiyue Huang, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the exciting performance, Transformer is criticized for its excessive parameters and computation cost. However, compressing Transformer remains as an open problem due to its internal complexity of the layer designs, i.e., Multi-Head Attention (MHA) and Feed-Forward Network (FFN). To address this issue, we introduce Group-wise Transformation towards a universal yet lightweight Transformer for vision-and-language tasks, termed as LW-Transformer. LW-Transformer applies Group-wise Transformation to reduce both the parameters and computations of Transformer, while also preserving its two main properties, i.e., the efficient attention modeling on diverse subspaces of MHA, and the expanding-scaling feature transformation of FFN. We apply LW-Transformer to a set of Transformer-based networks, and quantitatively measure them on three vision-and-language tasks and six benchmark datasets. Experimental results show that while saving a large number of parameters and computations, LW-Transformer achieves very competitive performance against the original Transformer networks for vision-and-language tasks. To examine the generalization ability, we also apply our optimization strategy to a recently proposed image Transformer called Swin-Transformer for image classification, where the effectiveness can be also confirmed



### UAMD-Net: A Unified Adaptive Multimodal Neural Network for Dense Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/2204.07791v1
- **DOI**: 10.1109/TCSVT.2023.3254650
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.07791v1)
- **Published**: 2022-04-16 12:49:50+00:00
- **Updated**: 2022-04-16 12:49:50+00:00
- **Authors**: Guancheng Chen, Junli Lin, Huabiao Qin
- **Comment**: 11 pages, 4 figures
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology,
  2023
- **Summary**: Depth prediction is a critical problem in robotics applications especially autonomous driving. Generally, depth prediction based on binocular stereo matching and fusion of monocular image and laser point cloud are two mainstream methods. However, the former usually suffers from overfitting while building cost volume, and the latter has a limited generalization due to the lack of geometric constraint. To solve these problems, we propose a novel multimodal neural network, namely UAMD-Net, for dense depth completion based on fusion of binocular stereo matching and the weak constrain from the sparse point clouds. Specifically, the sparse point clouds are converted to sparse depth map and sent to the multimodal feature encoder (MFE) with binocular image, constructing a cross-modal cost volume. Then, it will be further processed by the multimodal feature aggregator (MFA) and the depth regression layer. Furthermore, the existing multimodal methods ignore the problem of modal dependence, that is, the network will not work when a certain modal input has a problem. Therefore, we propose a new training strategy called Modal-dropout which enables the network to be adaptively trained with multiple modal inputs and inference with specific modal inputs. Benefiting from the flexible network structure and adaptive training method, our proposed network can realize unified training under various modal input conditions. Comprehensive experiments conducted on KITTI depth completion benchmark demonstrate that our method produces robust results and outperforms other state-of-the-art methods.



### FCL-GAN: A Lightweight and Real-Time Baseline for Unsupervised Blind Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2204.07820v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07820v2)
- **Published**: 2022-04-16 15:08:03+00:00
- **Updated**: 2022-07-24 03:24:43+00:00
- **Authors**: Suiyi Zhao, Zhao Zhang, Richang Hong, Mingliang Xu, Yi Yang, Meng Wang
- **Comment**: Please cite this work as: Suiyi Zhao, Zhao Zhang, Richang Hong,
  Mingliang Xu, Yi Yang and Meng Wang, "FCL-GAN: A Lightweight and Real-Time
  Baseline for Unsupervised Blind Image Deblurring," In: Proceedings of the
  30th ACM International Conference on Multimedia (ACM MM), Lisbon, Portugal,
  June 2022
- **Journal**: None
- **Summary**: Blind image deblurring (BID) remains a challenging and significant task. Benefiting from the strong fitting ability of deep learning, paired data-driven supervised BID method has obtained great progress. However, paired data are usually synthesized by hand, and the realistic blurs are more complex than synthetic ones, which makes the supervised methods inept at modeling realistic blurs and hinders their real-world applications. As such, unsupervised deep BID method without paired data offers certain advantages, but current methods still suffer from some drawbacks, e.g., bulky model size, long inference time, and strict image resolution and domain requirements. In this paper, we propose a lightweight and real-time unsupervised BID baseline, termed Frequency-domain Contrastive Loss Constrained Lightweight CycleGAN (shortly, FCL-GAN), with attractive properties, i.e., no image domain limitation, no image resolution limitation, 25x lighter than SOTA, and 5x faster than SOTA. To guarantee the lightweight property and performance superiority, two new collaboration units called lightweight domain conversion unit(LDCU) and parameter-free frequency-domain contrastive unit(PFCU) are designed. LDCU mainly implements inter-domain conversion in lightweight manner. PFCU further explores the similarity measure, external difference and internal connection between the blurred domain and sharp domain images in frequency domain, without involving extra parameters. Extensive experiments on several image datasets demonstrate the effectiveness of our FCL-GAN in terms of performance, model size and reference time.



### Few-Shot Transfer Learning to improve Chest X-Ray pathology detection using limited triplets
- **Arxiv ID**: http://arxiv.org/abs/2204.07824v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.07824v1)
- **Published**: 2022-04-16 15:44:56+00:00
- **Updated**: 2022-04-16 15:44:56+00:00
- **Authors**: Ananth Reddy Bhimireddy, John Lee Burns, Saptarshi Purkayastha, Judy Wawira Gichoya
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning approaches applied to medical imaging have reached near-human or better-than-human performance on many diagnostic tasks. For instance, the CheXpert competition on detecting pathologies in chest x-rays has shown excellent multi-class classification performance. However, training and validating deep learning models require extensive collections of images and still produce false inferences, as identified by a human-in-the-loop. In this paper, we introduce a practical approach to improve the predictions of a pre-trained model through Few-Shot Learning (FSL). After training and validating a model, a small number of false inference images are collected to retrain the model using \textbf{\textit{Image Triplets}} - a false positive or false negative, a true positive, and a true negative. The retrained FSL model produces considerable gains in performance with only a few epochs and few images. In addition, FSL opens rapid retraining opportunities for human-in-the-loop systems, where a radiologist can relabel false inferences, and the model can be quickly retrained. We compare our retrained model performance with existing FSL approaches in medical imaging that train and evaluate models at once.



### A Robust and Scalable Attention Guided Deep Learning Framework for Movement Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2204.07840v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2204.07840v1)
- **Published**: 2022-04-16 16:37:30+00:00
- **Updated**: 2022-04-16 16:37:30+00:00
- **Authors**: Aditya Kanade, Mansi Sharma, Manivannan Muniyandi
- **Comment**: None
- **Journal**: None
- **Summary**: Physical rehabilitation programs frequently begin with a brief stay in the hospital and continue with home-based rehabilitation. Lack of feedback on exercise correctness is a significant issue in home-based rehabilitation. Automated movement quality assessment (MQA) using skeletal movement data (hereafter referred to as skeletal data) collected via depth imaging devices can assist with home-based rehabilitation by providing the necessary quantitative feedback. This paper aims to use recent advances in deep learning to address the problem of MQA. Movement quality score generation is an essential component of MQA. We propose three novel skeletal data augmentation schemes. We show that using the proposed augmentations for generating movement quality scores result in significant performance boosts over existing methods. Finally, we propose a novel transformer based architecture for MQA. Four novel feature extractors are proposed and studied that allow the transformer network to operate on skeletal data. We show that adding the attention mechanism in the design of the proposed feature extractor allows the transformer network to pay attention to specific body parts that make a significant contribution towards executing a movement. We report an improvement in movement quality score prediction of 12% on UI-PRMD dataset and 21% on KIMORE dataset compared to the existing methods.



### Multi-Modal Few-Shot Object Detection with Meta-Learning-Based Cross-Modal Prompting
- **Arxiv ID**: http://arxiv.org/abs/2204.07841v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.07841v3)
- **Published**: 2022-04-16 16:45:06+00:00
- **Updated**: 2023-03-27 15:40:57+00:00
- **Authors**: Guangxing Han, Long Chen, Jiawei Ma, Shiyuan Huang, Rama Chellappa, Shih-Fu Chang
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: We study multi-modal few-shot object detection (FSOD) in this paper, using both few-shot visual examples and class semantic information for detection, which are complementary to each other by definition. Most of the previous works on multi-modal FSOD are fine-tuning-based which are inefficient for online applications. Moreover, these methods usually require expertise like class names to extract class semantic embedding, which are hard to get for rare classes. Our approach is motivated by the high-level conceptual similarity of (metric-based) meta-learning and prompt-based learning to learn generalizable few-shot and zero-shot object detection models respectively without fine-tuning. Specifically, we combine the few-shot visual classifier and text classifier learned via meta-learning and prompt-based learning respectively to build the multi-modal classifier and detection models. In addition, to fully exploit the pre-trained language models, we propose meta-learning-based cross-modal prompting to generate soft prompts for novel classes present in few-shot visual examples, which are then used to learn the text classifier. Knowledge distillation is introduced to learn the soft prompt generator without using human prior knowledge of class names, which may not be available for rare classes. Our insight is that the few-shot support images naturally include related context information and semantics of the class. We comprehensively evaluate the proposed multi-modal FSOD models on multiple few-shot object detection benchmarks, achieving promising results.



### Shape-guided Object Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2204.07845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2204.07845v1)
- **Published**: 2022-04-16 17:19:11+00:00
- **Updated**: 2022-04-16 17:19:11+00:00
- **Authors**: Yu Zeng, Zhe Lin, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Previous works on image inpainting mainly focus on inpainting background or partially missing objects, while the problem of inpainting an entire missing object remains unexplored. This work studies a new image inpainting task, i.e. shape-guided object inpainting. Given an incomplete input image, the goal is to fill in the hole by generating an object based on the context and implicit guidance given by the hole shape. Since previous methods for image inpainting are mainly designed for background inpainting, they are not suitable for this task. Therefore, we propose a new data preparation method and a novel Contextual Object Generator (CogNet) for the object inpainting task. On the data side, we incorporate object priors into training data by using object instances as holes. The CogNet has a two-stream architecture that combines the standard bottom-up image completion process with a top-down object generation process. A predictive class embedding module bridges the two streams by predicting the class of the missing object from the bottom-up features, from which a semantic object map is derived as the input of the top-down stream. Experiments demonstrate that the proposed method can generate realistic objects that fit the context in terms of both visual appearance and semantic meanings. Code can be found at the project page \url{https://zengxianyu.github.io/objpaint}



### Multi-organ Segmentation Network with Adversarial Performance Validator
- **Arxiv ID**: http://arxiv.org/abs/2204.07850v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.07850v1)
- **Published**: 2022-04-16 18:00:29+00:00
- **Updated**: 2022-04-16 18:00:29+00:00
- **Authors**: Haoyu Fang, Yi Fang, Xiaofeng Yang
- **Comment**: None
- **Journal**: None
- **Summary**: CT organ segmentation on computed tomography (CT) images becomes a significant brick for modern medical image analysis, supporting clinic workflows in multiple domains. Previous segmentation methods include 2D convolution neural networks (CNN) based approaches, fed by CT image slices that lack the structural knowledge in axial view, and 3D CNN-based methods with the expensive computation cost in multi-organ segmentation applications. This paper introduces an adversarial performance validation network into a 2D-to-3D segmentation framework. The classifier and performance validator competition contribute to accurate segmentation results via back-propagation. The proposed network organically converts the 2D-coarse result to 3D high-quality segmentation masks in a coarse-to-fine manner, allowing joint optimization to improve segmentation accuracy. Besides, the structural information of one specific organ is depicted by a statistics-meaningful prior bounding box, which is transformed into a global feature leveraging the learning process in 3D fine segmentation. The experiments on the NIH pancreas segmentation dataset demonstrate the proposed network achieves state-of-the-art accuracy on small organ segmentation and outperforms the previous best. High accuracy is also reported on multi-organ segmentation in a dataset collected by ourselves.



### Towards a Deeper Understanding of Skeleton-based Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/2204.07855v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07855v1)
- **Published**: 2022-04-16 18:23:37+00:00
- **Updated**: 2022-04-16 18:23:37+00:00
- **Authors**: Torben Teepe, Johannes Gilg, Fabian Herzog, Stefan Hörmann, Gerhard Rigoll
- **Comment**: 8 Pages, 5 figures, Accepted at 17th IEEE Computer Society Workshop
  on Biometrics 2022 (CVPRW'22)
- **Journal**: None
- **Summary**: Gait recognition is a promising biometric with unique properties for identifying individuals from a long distance by their walking patterns. In recent years, most gait recognition methods used the person's silhouette to extract the gait features. However, silhouette images can lose fine-grained spatial information, suffer from (self) occlusion, and be challenging to obtain in real-world scenarios. Furthermore, these silhouettes also contain other visual clues that are not actual gait features and can be used for identification, but also to fool the system. Model-based methods do not suffer from these problems and are able to represent the temporal motion of body joints, which are actual gait features. The advances in human pose estimation started a new era for model-based gait recognition with skeleton-based gait recognition. In this work, we propose an approach based on Graph Convolutional Networks (GCNs) that combines higher-order inputs, and residual networks to an efficient architecture for gait recognition. Extensive experiments on the two popular gait datasets, CASIA-B and OUMVLP-Pose, show a massive improvement (3x) of the state-of-the-art (SotA) on the largest gait dataset OUMVLP-Pose and strong temporal modeling capabilities. Finally, we visualize our method to understand skeleton-based gait recognition better and to show that we model real gait features.



### GHM Wavelet Transform for Deep Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2204.07862v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2204.07862v1)
- **Published**: 2022-04-16 19:59:48+00:00
- **Updated**: 2022-04-16 19:59:48+00:00
- **Authors**: Ben Lowe, Hadi Salman, Justin Zhan
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: The GHM multi-level discrete wavelet transform is proposed as preprocessing for image super resolution with convolutional neural networks. Previous works perform analysis with the Haar wavelet only. In this work, 37 single-level wavelets are experimentally analyzed from Haar, Daubechies, Biorthogonal, Reverse Biorthogonal, Coiflets, and Symlets wavelet families. All single-level wavelets report similar results indicating that the convolutional neural network is invariant to choice of wavelet in a single-level filter approach. However, the GHM multi-level wavelet achieves higher quality reconstructions than the single-level wavelets. Three large data sets are used for the experiments: DIV2K, a dataset of textures, and a dataset of satellite images. The approximate high resolution images are compared using seven objective error measurements. A convolutional neural network based approach using wavelet transformed images has good results in the literature.



### 3D Human Pose Estimation for Free-from and Moving Activities Using WiFi
- **Arxiv ID**: http://arxiv.org/abs/2204.07878v1
- **DOI**: 10.1145/3485730.3492871
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2204.07878v1)
- **Published**: 2022-04-16 21:58:24+00:00
- **Updated**: 2022-04-16 21:58:24+00:00
- **Authors**: Yili Ren, Jie Yang
- **Comment**: None
- **Journal**: Proceedings of the 19th ACM Conference on Embedded Networked
  Sensor Systems, 2021
- **Summary**: This paper presents GoPose, a 3D skeleton-based human pose estimation system that uses WiFi devices at home. Our system leverages the WiFi signals reflected off the human body for 3D pose estimation. In contrast to prior systems that need specialized hardware or dedicated sensors, our system does not require a user to wear or carry any sensors and can reuse the WiFi devices that already exist in a home environment for mass adoption. To realize such a system, we leverage the 2D AoA spectrum of the signals reflected from the human body and the deep learning techniques. In particular, the 2D AoA spectrum is proposed to locate different parts of the human body as well as to enable environment-independent pose estimation. Deep learning is incorporated to model the complex relationship between the 2D AoA spectrums and the 3D skeletons of the human body for pose tracking. Our evaluation results show GoPose achieves around 4.7cm of accuracy under various scenarios including tracking unseen activities and under NLoS scenarios.



### Mapping LiDAR and Camera Measurements in a Dual Top-View Grid Representation Tailored for Automated Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2204.07887v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2204.07887v2)
- **Published**: 2022-04-16 23:51:20+00:00
- **Updated**: 2022-04-21 12:39:43+00:00
- **Authors**: Sven Richter, Frank Bieder, Sascha Wirges, Christoph Stiller
- **Comment**: None
- **Journal**: None
- **Summary**: We present a generic evidential grid mapping pipeline designed for imaging sensors such as LiDARs and cameras. Our grid-based evidential model contains semantic estimates for cell occupancy and ground separately. We specify the estimation steps for input data represented by point sets, but mainly focus on input data represented by images such as disparity maps or LiDAR range images. Instead of relying on an external ground segmentation only, we deduce occupancy evidence by analyzing the surface orientation around measurements. We conduct experiments and evaluate the presented method using LiDAR and stereo camera data recorded in real traffic scenarios. Our method estimates cell occupancy robustly and with a high level of detail while maximizing efficiency and minimizing the dependency to external processing modules.



