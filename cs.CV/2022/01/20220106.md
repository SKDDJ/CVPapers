# Arxiv Papers in cs.CV on 2022-01-06
### Memory-guided Image De-raining Using Time-Lapse Data
- **Arxiv ID**: http://arxiv.org/abs/2201.01883v1
- **DOI**: 10.1109/TIP.2022.3180561
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.01883v1)
- **Published**: 2022-01-06 01:36:59+00:00
- **Updated**: 2022-01-06 01:36:59+00:00
- **Authors**: Jaehoon Cho, Seungryong Kim, Kwanghoon Sohn
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of single image de-raining, that is, the task of recovering clean and rain-free background scenes from a single image obscured by a rainy artifact. Although recent advances adopt real-world time-lapse data to overcome the need for paired rain-clean images, they are limited to fully exploit the time-lapse data. The main cause is that, in terms of network architectures, they could not capture long-term rain streak information in the time-lapse data during training owing to the lack of memory components. To address this problem, we propose a novel network architecture based on a memory network that explicitly helps to capture long-term rain streak information in the time-lapse data. Our network comprises the encoder-decoder networks and a memory network. The features extracted from the encoder are read and updated in the memory network that contains several memory items to store rain streak-aware feature representations. With the read/update operation, the memory network retrieves relevant memory items in terms of the queries, enabling the memory items to represent the various rain streaks included in the time-lapse data. To boost the discriminative power of memory features, we also present a novel background selective whitening (BSW) loss for capturing only rain streak information in the memory network by erasing the background information. Experimental results on standard benchmarks demonstrate the effectiveness and superiority of our approach.



### 3D Intracranial Aneurysm Classification and Segmentation via Unsupervised Dual-branch Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.02198v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.02198v2)
- **Published**: 2022-01-06 02:03:25+00:00
- **Updated**: 2022-01-17 02:17:48+00:00
- **Authors**: Di Shao, Xuequan Lu, Xiao Liu
- **Comment**: under review (corresponding: {xuequan.lu@deakin.edu.au})
- **Journal**: None
- **Summary**: Intracranial aneurysms are common nowadays and how to detect them intelligently is of great significance in digital health. While most existing deep learning research focused on medical images in a supervised way, we introduce an unsupervised method for the detection of intracranial aneurysms based on 3D point cloud data. In particular, our method consists of two stages: unsupervised pre-training and downstream tasks. As for the former, the main idea is to pair each point cloud with its jittered counterpart and maximise their correspondence. Then we design a dual-branch contrastive network with an encoder for each branch and a subsequent common projection head. As for the latter, we design simple networks for supervised classification and segmentation training. Experiments on the public dataset (IntrA) show that our unsupervised method achieves comparable or even better performance than some state-of-the-art supervised techniques, and it is most prominent in the detection of aneurysmal vessels. Experiments on the ModelNet40 also show that our method achieves the accuracy of 90.79\% which outperforms existing state-of-the-art unsupervised models.



### Flow-Guided Sparse Transformer for Video Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2201.01893v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.01893v3)
- **Published**: 2022-01-06 02:05:32+00:00
- **Updated**: 2022-05-29 07:58:48+00:00
- **Authors**: Jing Lin, Yuanhao Cai, Xiaowan Hu, Haoqian Wang, Youliang Yan, Xueyi Zou, Henghui Ding, Yulun Zhang, Radu Timofte, Luc Van Gool
- **Comment**: ICML 2022; The First Transformer-based method for Video Deblurring
- **Journal**: None
- **Summary**: Exploiting similar and sharper scene patches in spatio-temporal neighborhoods is critical for video deblurring. However, CNN-based methods show limitations in capturing long-range dependencies and modeling non-local self-similarity. In this paper, we propose a novel framework, Flow-Guided Sparse Transformer (FGST), for video deblurring. In FGST, we customize a self-attention module, Flow-Guided Sparse Window-based Multi-head Self-Attention (FGSW-MSA). For each $query$ element on the blurry reference frame, FGSW-MSA enjoys the guidance of the estimated optical flow to globally sample spatially sparse yet highly related $key$ elements corresponding to the same scene patch in neighboring frames. Besides, we present a Recurrent Embedding (RE) mechanism to transfer information from past frames and strengthen long-range temporal dependencies. Comprehensive experiments demonstrate that our proposed FGST outperforms state-of-the-art (SOTA) methods on both DVD and GOPRO datasets and even yields more visually pleasing results in real video deblurring. Code and pre-trained models are publicly available at https://github.com/linjing7/VR-Baseline



### Incremental Object Grounding Using Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/2201.01901v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2201.01901v2)
- **Published**: 2022-01-06 02:55:34+00:00
- **Updated**: 2022-11-13 16:57:00+00:00
- **Authors**: John Seon Keun Yi, Yoonwoo Kim, Sonia Chernova
- **Comment**: None
- **Journal**: None
- **Summary**: Object grounding tasks aim to locate the target object in an image through verbal communications. Understanding human command is an important process needed for effective human-robot communication. However, this is challenging because human commands can be ambiguous and erroneous. This paper aims to disambiguate the human's referring expressions by allowing the agent to ask relevant questions based on semantic data obtained from scene graphs. We test if our agent can use relations between objects from a scene graph to ask semantically relevant questions that can disambiguate the original user command. In this paper, we present Incremental Grounding using Scene Graphs (IGSG), a disambiguation model that uses semantic data from an image scene graph and linguistic structures from a language scene graph to ground objects based on human command. Compared to the baseline, IGSG shows promising results in complex real-world scenes where there are multiple identical target objects. IGSG can effectively disambiguate ambiguous or wrong referring expressions by asking disambiguating questions back to the user.



### Contrastive Neighborhood Alignment
- **Arxiv ID**: http://arxiv.org/abs/2201.01922v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.01922v1)
- **Published**: 2022-01-06 04:58:31+00:00
- **Updated**: 2022-01-06 04:58:31+00:00
- **Authors**: Pengkai Zhu, Zhaowei Cai, Yuanjun Xiong, Zhuowen Tu, Luis Goncalves, Vijay Mahadevan, Stefano Soatto
- **Comment**: 10 pages, 7 tables, 3 figures
- **Journal**: None
- **Summary**: We present Contrastive Neighborhood Alignment (CNA), a manifold learning approach to maintain the topology of learned features whereby data points that are mapped to nearby representations by the source (teacher) model are also mapped to neighbors by the target (student) model. The target model aims to mimic the local structure of the source representation space using a contrastive loss. CNA is an unsupervised learning algorithm that does not require ground-truth labels for the individual samples. CNA is illustrated in three scenarios: manifold learning, where the model maintains the local topology of the original data in a dimension-reduced space; model distillation, where a small student model is trained to mimic a larger teacher; and legacy model update, where an older model is replaced by a more powerful one. Experiments show that CNA is able to capture the manifold in a high-dimensional space and improves performance compared to the competing methods in their domains.



### Egocentric Deep Multi-Channel Audio-Visual Active Speaker Localization
- **Arxiv ID**: http://arxiv.org/abs/2201.01928v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2201.01928v1)
- **Published**: 2022-01-06 05:40:16+00:00
- **Updated**: 2022-01-06 05:40:16+00:00
- **Authors**: Hao Jiang, Calvin Murdock, Vamsi Krishna Ithapu
- **Comment**: None
- **Journal**: None
- **Summary**: Augmented reality devices have the potential to enhance human perception and enable other assistive functionalities in complex conversational environments. Effectively capturing the audio-visual context necessary for understanding these social interactions first requires detecting and localizing the voice activities of the device wearer and the surrounding people. These tasks are challenging due to their egocentric nature: the wearer's head motion may cause motion blur, surrounding people may appear in difficult viewing angles, and there may be occlusions, visual clutter, audio noise, and bad lighting. Under these conditions, previous state-of-the-art active speaker detection methods do not give satisfactory results. Instead, we tackle the problem from a new setting using both video and multi-channel microphone array audio. We propose a novel end-to-end deep learning approach that is able to give robust voice activity detection and localization results. In contrast to previous methods, our method localizes active speakers from all possible directions on the sphere, even outside the camera's field of view, while simultaneously detecting the device wearer's own voice activity. Our experiments show that the proposed method gives superior results, can run in real time, and is robust against noise and clutter.



### Decompose to Adapt: Cross-domain Object Detection via Feature Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2201.01929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01929v1)
- **Published**: 2022-01-06 05:43:01+00:00
- **Updated**: 2022-01-06 05:43:01+00:00
- **Authors**: Dongnan Liu, Chaoyi Zhang, Yang Song, Heng Huang, Chenyu Wang, Michael Barnett, Weidong Cai
- **Comment**: Accepted to appear in IEEE Transactions on Multimedia; source code:
  https://github.com/dliu5812/DDF
- **Journal**: None
- **Summary**: Recent advances in unsupervised domain adaptation (UDA) techniques have witnessed great success in cross-domain computer vision tasks, enhancing the generalization ability of data-driven deep learning architectures by bridging the domain distribution gaps. For the UDA-based cross-domain object detection methods, the majority of them alleviate the domain bias by inducing the domain-invariant feature generation via adversarial learning strategy. However, their domain discriminators have limited classification ability due to the unstable adversarial training process. Therefore, the extracted features induced by them cannot be perfectly domain-invariant and still contain domain-private factors, bringing obstacles to further alleviate the cross-domain discrepancy. To tackle this issue, we design a Domain Disentanglement Faster-RCNN (DDF) to eliminate the source-specific information in the features for detection task learning. Our DDF method facilitates the feature disentanglement at the global and local stages, with a Global Triplet Disentanglement (GTD) module and an Instance Similarity Disentanglement (ISD) module, respectively. By outperforming state-of-the-art methods on four benchmark UDA object detection tasks, our DDF method is demonstrated to be effective with wide applicability.



### Aerial Scene Parsing: From Tile-level Scene Classification to Pixel-wise Semantic Labeling
- **Arxiv ID**: http://arxiv.org/abs/2201.01953v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01953v2)
- **Published**: 2022-01-06 07:40:47+00:00
- **Updated**: 2022-01-09 05:23:03+00:00
- **Authors**: Yang Long, Gui-Song Xia, Liangpei Zhang, Gong Cheng, Deren Li
- **Comment**: None
- **Journal**: None
- **Summary**: Given an aerial image, aerial scene parsing (ASP) targets to interpret the semantic structure of the image content, e.g., by assigning a semantic label to every pixel of the image. With the popularization of data-driven methods, the past decades have witnessed promising progress on ASP by approaching the problem with the schemes of tile-level scene classification or segmentation-based image analysis, when using high-resolution aerial images. However, the former scheme often produces results with tile-wise boundaries, while the latter one needs to handle the complex modeling process from pixels to semantics, which often requires large-scale and well-annotated image samples with pixel-wise semantic labels. In this paper, we address these issues in ASP, with perspectives from tile-level scene classification to pixel-wise semantic labeling. Specifically, we first revisit aerial image interpretation by a literature review. We then present a large-scale scene classification dataset that contains one million aerial images termed Million-AID. With the presented dataset, we also report benchmarking experiments using classical convolutional neural networks (CNNs). Finally, we perform ASP by unifying the tile-level scene classification and object-based image analysis to achieve pixel-wise semantic labeling. Intensive experiments show that Million-AID is a challenging yet useful dataset, which can serve as a benchmark for evaluating newly developed algorithms. When transferring knowledge from Million-AID, fine-tuning CNN models pretrained on Million-AID perform consistently better than those pretrained ImageNet for aerial scene classification. Moreover, our designed hierarchical multi-task learning method achieves the state-of-the-art pixel-wise classification on the challenging GID, bridging the tile-level scene classification toward pixel-wise semantic labeling for aerial image interpretation.



### Diversity-boosted Generalization-Specialization Balancing for Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.01961v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.01961v2)
- **Published**: 2022-01-06 08:04:27+00:00
- **Updated**: 2022-10-23 12:17:38+00:00
- **Authors**: Yun Li, Zhe Liu, Xiaojun Chang, Julian McAuley, Lina Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-Shot Learning (ZSL) aims to transfer classification capability from seen to unseen classes. Recent methods have proved that generalization and specialization are two essential abilities to achieve good performance in ZSL. However, focusing on only one of the abilities may result in models that are either too general with degraded classification ability or too specialized to generalize to unseen classes. In this paper, we propose an end-to-end network, termed as BGSNet, which equips and balances generalization and specialization abilities at the instance and dataset level. Specifically, BGSNet consists of two branches: the Generalization Network (GNet), which applies episodic meta-learning to learn generalized knowledge, and the Balanced Specialization Network (BSNet), which adopts multiple attentive extractors to extract discriminative features and achieve instance-level balance. A novel self-adjusted diversity loss is designed to optimize BSNet with redundancy reduced and diversity boosted. We further propose a differentiable dataset-level balance and update the weights in a linear annealing schedule to simulate network pruning and thus obtain the optimal structure for BSNet with dataset-level balance achieved. Experiments on four benchmark datasets demonstrate our model's effectiveness. Sufficient component ablations prove the necessity of integrating and balancing generalization and specialization abilities.



### Multi-Label Classification on Remote-Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2201.01971v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.01971v1)
- **Published**: 2022-01-06 08:42:32+00:00
- **Updated**: 2022-01-06 08:42:32+00:00
- **Authors**: Aditya Kumar Singh, B. Uma Shankar
- **Comment**: The report consists of 95 Pages, 45 Figures, 31 Tables, 85 References
- **Journal**: None
- **Summary**: Acquiring information on large areas on the earth's surface through satellite cameras allows us to see much more than we can see while standing on the ground. This assists us in detecting and monitoring the physical characteristics of an area like land-use patterns, atmospheric conditions, forest cover, and many unlisted aspects. The obtained images not only keep track of continuous natural phenomena but are also crucial in tackling the global challenge of severe deforestation. Among which Amazon basin accounts for the largest share every year. Proper data analysis would help limit detrimental effects on the ecosystem and biodiversity with a sustainable healthy atmosphere. This report aims to label the satellite image chips of the Amazon rainforest with atmospheric and various classes of land cover or land use through different machine learning and superior deep learning models. Evaluation is done based on the F2 metric, while for loss function, we have both sigmoid cross-entropy as well as softmax cross-entropy. Images are fed indirectly to the machine learning classifiers after only features are extracted using pre-trained ImageNet architectures. Whereas for deep learning models, ensembles of fine-tuned ImageNet pre-trained models are used via transfer learning. Our best score was achieved so far with the F2 metric is 0.927.



### SASA: Semantics-Augmented Set Abstraction for Point-based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.01976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01976v1)
- **Published**: 2022-01-06 08:54:47+00:00
- **Updated**: 2022-01-06 08:54:47+00:00
- **Authors**: Chen Chen, Zhe Chen, Jing Zhang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Although point-based networks are demonstrated to be accurate for 3D point cloud modeling, they are still falling behind their voxel-based competitors in 3D detection. We observe that the prevailing set abstraction design for down-sampling points may maintain too much unimportant background information that can affect feature learning for detecting objects. To tackle this issue, we propose a novel set abstraction method named Semantics-Augmented Set Abstraction (SASA). Technically, we first add a binary segmentation module as the side output to help identify foreground points. Based on the estimated point-wise foreground scores, we then propose a semantics-guided point sampling algorithm to help retain more important foreground points during down-sampling. In practice, SASA shows to be effective in identifying valuable points related to foreground objects and improving feature learning for point-based 3D detection. Additionally, it is an easy-to-plug-in module and able to boost various point-based detectors, including single-stage and two-stage ones. Extensive experiments on the popular KITTI and nuScenes datasets validate the superiority of SASA, lifting point-based detection models to reach comparable performance to state-of-the-art voxel-based methods.



### An Abstraction-Refinement Approach to Verifying Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.01978v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.LO, 68Q60
- **Links**: [PDF](http://arxiv.org/pdf/2201.01978v1)
- **Published**: 2022-01-06 08:57:43+00:00
- **Updated**: 2022-01-06 08:57:43+00:00
- **Authors**: Matan Ostrovsky, Clark Barrett, Guy Katz
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks have gained vast popularity due to their excellent performance in the fields of computer vision, image processing, and others. Unfortunately, it is now well known that convolutional networks often produce erroneous results - for example, minor perturbations of the inputs of these networks can result in severe classification errors. Numerous verification approaches have been proposed in recent years to prove the absence of such errors, but these are typically geared for fully connected networks and suffer from exacerbated scalability issues when applied to convolutional networks. To address this gap, we present here the Cnn-Abs framework, which is particularly aimed at the verification of convolutional networks. The core of Cnn-Abs is an abstraction-refinement technique, which simplifies the verification problem through the removal of convolutional connections in a way that soundly creates an over-approximation of the original problem; and which restores these connections if the resulting problem becomes too abstract. Cnn-Abs is designed to use existing verification engines as a backend, and our evaluation demonstrates that it can significantly boost the performance of a state-of-the-art DNN verification engine, reducing runtime by 15.7% on average.



### Multi-Domain Joint Training for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2201.01983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.01983v1)
- **Published**: 2022-01-06 09:20:59+00:00
- **Updated**: 2022-01-06 09:20:59+00:00
- **Authors**: Lu Yang, Lingqiao Liu, Yunlong Wang, Peng Wang, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based person Re-IDentification (ReID) often requires a large amount of training data to achieve good performance. Thus it appears that collecting more training data from diverse environments tends to improve the ReID performance. This paper re-examines this common belief and makes a somehow surprising observation: using more samples, i.e., training with samples from multiple datasets, does not necessarily lead to better performance by using the popular ReID models. In some cases, training with more samples may even hurt the performance of the evaluation is carried out in one of those datasets. We postulate that this phenomenon is due to the incapability of the standard network in adapting to diverse environments. To overcome this issue, we propose an approach called Domain-Camera-Sample Dynamic network (DCSD) whose parameters can be adaptive to various factors. Specifically, we consider the internal domain-related factor that can be identified from the input features, and external domain-related factors, such as domain information or camera information. Our discovery is that training with such an adaptive model can better benefit from more training samples. Experimental results show that our DCSD can greatly boost the performance (up to 12.3%) while joint training in multiple datasets.



### Compact Bidirectional Transformer for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2201.01984v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2201.01984v1)
- **Published**: 2022-01-06 09:23:18+00:00
- **Updated**: 2022-01-06 09:23:18+00:00
- **Authors**: Yuanen Zhou, Zhenzhen Hu, Daqing Liu, Huixia Ben, Meng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Most current image captioning models typically generate captions from left to right. This unidirectional property makes them can only leverage past context but not future context. Though recent refinement-based models can exploit both past and future context by generating a new caption in the second stage based on pre-retrieved or pre-generated captions in the first stage, the decoder of these models generally consists of two networks~(i.e. a retriever or captioner in the first stage and a refiner in the second stage), which can only be executed sequentially. In this paper, we introduce a Compact Bidirectional Transformer model for image captioning that can leverage bidirectional context implicitly and explicitly while the decoder can be executed parallelly. Specifically, it is implemented by tightly coupling left-to-right(L2R) and right-to-left(R2L) flows into a single compact model~(i.e. implicitly) and optionally allowing interaction of the two flows(i.e. explicitly), while the final caption is chosen from either L2R or R2L flow in a sentence-level ensemble manner. We conduct extensive ablation studies on the MSCOCO benchmark and find that the compact architecture, which serves as a regularization for implicitly exploiting bidirectional context, and the sentence-level ensemble play more important roles than the explicit interaction mechanism. By combining with word-level ensemble seamlessly, the effect of the sentence-level ensemble is further enlarged. We further extend the conventional one-flow self-critical training to the two-flows version under this architecture and achieve new state-of-the-art results in comparison with non-vision-language-pretraining models. Source code is available at {\color{magenta}\url{https://github.com/YuanEZhou/CBTrans}}.



### TransVPR: Transformer-based place recognition with multi-level attention aggregation
- **Arxiv ID**: http://arxiv.org/abs/2201.02001v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02001v4)
- **Published**: 2022-01-06 10:20:24+00:00
- **Updated**: 2022-04-13 05:46:16+00:00
- **Authors**: Ruotong Wang, Yanqing Shen, Weiliang Zuo, Sanping Zhou, Nanning Zheng
- **Comment**: CVPR 2022 oral
- **Journal**: None
- **Summary**: Visual place recognition is a challenging task for applications such as autonomous driving navigation and mobile robot localization. Distracting elements presenting in complex scenes often lead to deviations in the perception of visual place. To address this problem, it is crucial to integrate information from only task-relevant regions into image representations. In this paper, we introduce a novel holistic place recognition model, TransVPR, based on vision Transformers. It benefits from the desirable property of the self-attention operation in Transformers which can naturally aggregate task-relevant features. Attentions from multiple levels of the Transformer, which focus on different regions of interest, are further combined to generate a global image representation. In addition, the output tokens from Transformer layers filtered by the fused attention mask are considered as key-patch descriptors, which are used to perform spatial matching to re-rank the candidates retrieved by the global image features. The whole model allows end-to-end training with a single objective and image-level supervision. TransVPR achieves state-of-the-art performance on several real-world benchmarks while maintaining low computational time and storage requirements.



### Self-Training Vision Language BERTs with a Unified Conditional Model
- **Arxiv ID**: http://arxiv.org/abs/2201.02010v2
- **DOI**: 10.1109/TCSVT.2023.3235704
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2201.02010v2)
- **Published**: 2022-01-06 11:00:52+00:00
- **Updated**: 2023-01-19 08:10:09+00:00
- **Authors**: Xiaofeng Yang, Fengmao Lv, Fayao Liu, Guosheng Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Natural language BERTs are trained with language corpus in a self-supervised manner. Unlike natural language BERTs, vision language BERTs need paired data to train, which restricts the scale of VL-BERT pretraining. We propose a self-training approach that allows training VL-BERTs from unlabeled image data. The proposed method starts with our unified conditional model -- a vision language BERT model that can perform zero-shot conditional generation. Given different conditions, the unified conditional model can generate captions, dense captions, and even questions. We use the labeled image data to train a teacher model and use the trained model to generate pseudo captions on unlabeled image data. We then combine the labeled data and pseudo labeled data to train a student model. The process is iterated by putting the student model as a new teacher. By using the proposed self-training approach and only 300k unlabeled extra data, we are able to get competitive or even better performances compared to the models of similar model size trained with 3 million extra image data.



### An unambiguous cloudiness index for nonwovens
- **Arxiv ID**: http://arxiv.org/abs/2201.02011v2
- **DOI**: 10.1186/s13362-022-00124-z
- **Categories**: **cs.CV**, eess.IV, 62M40, 62P30, I.4.7; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2201.02011v2)
- **Published**: 2022-01-06 11:03:27+00:00
- **Updated**: 2022-04-11 13:04:46+00:00
- **Authors**: Michael Godehardt, Ali Moghiseh, Christine Oetjen, Joachim Ohser, Katja Schladitz
- **Comment**: None
- **Journal**: Journal of Mathematics in Industry, 2022
- **Summary**: Cloudiness or formation is a concept routinely used in industry to address deviations from homogeneity in nonwovens and papers. Measuring a cloudiness index based on image data is a common task in industrial quality assurance. The two most popular ways of quantifying cloudiness are based on power spectrum or correlation function on the one hand or the Laplacian pyramid on the other hand. Here, we recall the mathematical basis of the first approach comprehensively, derive a cloudiness index, and demonstrate its practical estimation. We prove that the Laplacian pyramid as well as other quantities characterizing cloudiness like the range of interaction and the intensity of small-angle scattering are very closely related to the power spectrum. Finally, we show that the power spectrum is easy to be measured image analytically and carries more information than the alternatives.



### Enhancing Egocentric 3D Pose Estimation with Third Person Views
- **Arxiv ID**: http://arxiv.org/abs/2201.02017v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02017v3)
- **Published**: 2022-01-06 11:42:01+00:00
- **Updated**: 2022-06-15 16:09:06+00:00
- **Authors**: Ameya Dhamanaskar, Mariella Dimiccoli, Enric Corona, Albert Pumarola, Francesc Moreno-Noguer
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach to enhance the 3D body pose estimation of a person computed from videos captured from a single wearable camera. The key idea is to leverage high-level features linking first- and third-views in a joint embedding space. To learn such embedding space we introduce First2Third-Pose, a new paired synchronized dataset of nearly 2,000 videos depicting human activities captured from both first- and third-view perspectives. We explicitly consider spatial- and motion-domain features, combined using a semi-Siamese architecture trained in a self-supervised fashion. Experimental results demonstrate that the joint multi-view embedded space learned with our dataset is useful to extract discriminatory features from arbitrary single-view egocentric videos, without needing domain adaptation nor knowledge of camera parameters. We achieve significant improvement of egocentric 3D body pose estimation performance on two unconstrained datasets, over three supervised state-of-the-art approaches. Our dataset and code will be available for research purposes.



### A Light in the Dark: Deep Learning Practices for Industrial Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2201.02028v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.02028v1)
- **Published**: 2022-01-06 12:36:35+00:00
- **Updated**: 2022-01-06 12:36:35+00:00
- **Authors**: Maximilian Harl, Marvin Herchenbach, Sven Kruschel, Nico Hambauer, Patrick Zschech, Mathias Kraus
- **Comment**: Preprint accepted for archival and presentation at the 17th
  International Conference on Wirtschaftsinformatik 2022. 14 pages, 5 figures,
  4 tables
- **Journal**: None
- **Summary**: In recent years, large pre-trained deep neural networks (DNNs) have revolutionized the field of computer vision (CV). Although these DNNs have been shown to be very well suited for general image recognition tasks, application in industry is often precluded for three reasons: 1) large pre-trained DNNs are built on hundreds of millions of parameters, making deployment on many devices impossible, 2) the underlying dataset for pre-training consists of general objects, while industrial cases often consist of very specific objects, such as structures on solar wafers, 3) potentially biased pre-trained DNNs raise legal issues for companies. As a remedy, we study neural networks for CV that we train from scratch. For this purpose, we use a real-world case from a solar wafer manufacturer. We find that our neural networks achieve similar performances as pre-trained DNNs, even though they consist of far fewer parameters and do not rely on third-party datasets.



### A Unified Framework for Attention-Based Few-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.02052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02052v1)
- **Published**: 2022-01-06 13:35:28+00:00
- **Updated**: 2022-01-06 13:35:28+00:00
- **Authors**: Pierre Le Jeune, Anissa Mokraoui
- **Comment**: None
- **Journal**: None
- **Summary**: Few-Shot Object Detection (FSOD) is a rapidly growing field in computer vision. It consists in finding all occurrences of a given set of classes with only a few annotated examples for each class. Numerous methods have been proposed to address this challenge and most of them are based on attention mechanisms. However, the great variety of classic object detection frameworks and training strategies makes performance comparison between methods difficult. In particular, for attention-based FSOD methods, it is laborious to compare the impact of the different attention mechanisms on performance. This paper aims at filling this shortcoming. To do so, a flexible framework is proposed to allow the implementation of most of the attention techniques available in the literature. To properly introduce such a framework, a detailed review of the existing FSOD methods is firstly provided. Some different attention mechanisms are then reimplemented within the framework and compared with all other parameters fixed.



### ASL-Skeleton3D and ASL-Phono: Two Novel Datasets for the American Sign Language
- **Arxiv ID**: http://arxiv.org/abs/2201.02065v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2201.02065v1)
- **Published**: 2022-01-06 14:10:03+00:00
- **Updated**: 2022-01-06 14:10:03+00:00
- **Authors**: Cleison Correia de Amorim, Cleber Zanchettin
- **Comment**: None
- **Journal**: The paper is under consideration at Pattern Recognition Letters
  (2022) (under the manuscript number PRLETTERS-D-22-00140)
- **Summary**: Sign language is an essential resource enabling access to communication and proper socioemotional development for individuals suffering from disabling hearing loss. As this population is expected to reach 700 million by 2050, the importance of the language becomes even more essential as it plays a critical role to ensure the inclusion of such individuals in society. The Sign Language Recognition field aims to bridge the gap between users and non-users of sign languages. However, the scarcity in quantity and quality of datasets is one of the main challenges limiting the exploration of novel approaches that could lead to significant advancements in this research area. Thus, this paper contributes by introducing two new datasets for the American Sign Language: the first is composed of the three-dimensional representation of the signers and, the second, by an unprecedented linguistics-based representation containing a set of phonological attributes of the signs.



### EM-driven unsupervised learning for efficient motion segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.02074v3
- **DOI**: 10.1109/TPAMI.2022.3198480
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02074v3)
- **Published**: 2022-01-06 14:35:45+00:00
- **Updated**: 2022-10-06 08:54:08+00:00
- **Authors**: Etienne Meunier, Anaïs Badoual, Patrick Bouthemy
- **Comment**: Accepted to : IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)
- **Journal**: None
- **Summary**: In this paper, we present a CNN-based fully unsupervised method for motion segmentation from optical flow. We assume that the input optical flow can be represented as a piecewise set of parametric motion models, typically, affine or quadratic motion models. The core idea of our work is to leverage the Expectation-Maximization (EM) framework in order to design in a well-founded manner a loss function and a training procedure of our motion segmentation neural network that does not require either ground-truth or manual annotation. However, in contrast to the classical iterative EM, once the network is trained, we can provide a segmentation for any unseen optical flow field in a single inference step and without estimating any motion models. We investigate different loss functions including robust ones and propose a novel efficient data augmentation technique on the optical flow field, applicable to any network taking optical flow as input. In addition, our method is able by design to segment multiple motions. Our motion segmentation network was tested on four benchmarks, DAVIS2016, SegTrackV2, FBMS59, and MoCA, and performed very well, while being fast at test time.



### Deep Learning Based Classification System For Recognizing Local Spinach
- **Arxiv ID**: http://arxiv.org/abs/2201.02093v1
- **DOI**: 10.1007/978-3-030-85365-5_1
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.02093v1)
- **Published**: 2022-01-06 15:10:41+00:00
- **Updated**: 2022-01-06 15:10:41+00:00
- **Authors**: Mirajul Islam, Nushrat Jahan Ria, Jannatul Ferdous Ani, Abu Kaisar Mohammad Masum, Sheikh Abujar, Syed Akhter Hossain
- **Comment**: 10 pages, 4 figures, supplemental materials. Accepted in 2nd
  International Conference on Deep Learning, Artificial Intelligence and
  Robotics,(ICDLAIR) 2020
- **Journal**: None
- **Summary**: A deep learning model gives an incredible result for image processing by studying from the trained dataset. Spinach is a leaf vegetable that contains vitamins and nutrients. In our research, a Deep learning method has been used that can automatically identify spinach and this method has a dataset of a total of five species of spinach that contains 3785 images. Four Convolutional Neural Network (CNN) models were used to classify our spinach. These models give more accurate results for image classification. Before applying these models there is some preprocessing of the image data. For the preprocessing of data, some methods need to happen. Those are RGB conversion, filtering, resize & rescaling, and categorization. After applying these methods image data are pre-processed and ready to be used in the classifier algorithms. The accuracy of these classifiers is in between 98.68% - 99.79%. Among those models, VGG16 achieved the highest accuracy of 99.79%.



### HyperionSolarNet: Solar Panel Detection from Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2201.02107v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02107v1)
- **Published**: 2022-01-06 15:43:13+00:00
- **Updated**: 2022-01-06 15:43:13+00:00
- **Authors**: Poonam Parhar, Ryan Sawasaki, Alberto Todeschini, Colorado Reed, Hossein Vahabi, Nathan Nusaputra, Felipe Vergara
- **Comment**: None
- **Journal**: None
- **Summary**: With the effects of global climate change impacting the world, collective efforts are needed to reduce greenhouse gas emissions. The energy sector is the single largest contributor to climate change and many efforts are focused on reducing dependence on carbon-emitting power plants and moving to renewable energy sources, such as solar power. A comprehensive database of the location of solar panels is important to assist analysts and policymakers in defining strategies for further expansion of solar energy. In this paper we focus on creating a world map of solar panels. We identify locations and total surface area of solar panels within a given geographic area. We use deep learning methods for automated detection of solar panel locations and their surface area using aerial imagery. The framework, which consists of a two-branch model using an image classifier in tandem with a semantic segmentation model, is trained on our created dataset of satellite images. Our work provides an efficient and scalable method for detecting solar panels, achieving an accuracy of 0.96 for classification and an IoU score of 0.82 for segmentation performance.



### Exploring Kervolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.07264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07264v1)
- **Published**: 2022-01-06 17:30:30+00:00
- **Updated**: 2022-01-06 17:30:30+00:00
- **Authors**: Nicolas Perez
- **Comment**: 5 pages, 8 figures
- **Journal**: None
- **Summary**: A paper published in the CVPR 2019 conference outlines a new technique called 'kervolution' used in a new type of augmented convolutional neural network (CNN) called a 'kervolutional neural network' (KNN). The paper asserts that KNNs achieve faster convergence and higher accuracies than CNNs. This "mini paper" will further examine the findings in the original paper and perform a more in depth analysis of the KNN architecture. This will be done by analyzing the impact of hyper parameters (specifically the learning rate) on KNNs versus CNNs, experimenting with other types of kervolution operations not tested in the original paper, a more rigourous statistical analysis of accuracies and convergence times and additional theoretical analysis. The accompanying code is publicly available.



### Bio-inspired Min-Nets Improve the Performance and Robustness of Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.02149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02149v1)
- **Published**: 2022-01-06 17:31:21+00:00
- **Updated**: 2022-01-06 17:31:21+00:00
- **Authors**: Philipp Grüning, Erhardt Barth
- **Comment**: None
- **Journal**: Gruening, P., & Barth, E. (2021, October). Bio-inspired Min-Nets
  Improve the Performance and Robustness of Deep Networks. In SVRHM 2021
  Workshop@ NeurIPS
- **Summary**: Min-Nets are inspired by end-stopped cortical cells with units that output the minimum of two learned filters. We insert such Min-units into state-of-the-art deep networks, such as the popular ResNet and DenseNet, and show that the resulting Min-Nets perform better on the Cifar-10 benchmark. Moreover, we show that Min-Nets are more robust against JPEG compression artifacts. We argue that the minimum operation is the simplest way of implementing an AND operation on pairs of filters and that such AND operations introduce a bias that is appropriate given the statistics of natural images.



### Realistic Full-Body Anonymization with Surface-Guided GANs
- **Arxiv ID**: http://arxiv.org/abs/2201.02193v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02193v2)
- **Published**: 2022-01-06 18:57:59+00:00
- **Updated**: 2023-06-01 09:52:16+00:00
- **Authors**: Håkon Hukkelås, Morten Smebye, Rudolf Mester, Frank Lindseth
- **Comment**: 8 pages, 7 figures, 6 tables. Source code and appendix available at:
  https://www.github.com/hukkelas/full_body_anonymization. Published at WACV
  2023
- **Journal**: None
- **Summary**: Recent work on image anonymization has shown that generative adversarial networks (GANs) can generate near-photorealistic faces to anonymize individuals. However, scaling up these networks to the entire human body has remained a challenging and yet unsolved task. We propose a new anonymization method that generates realistic humans for in-the-wild images. A key part of our design is to guide adversarial nets by dense pixel-to-surface correspondences between an image and a canonical 3D surface. We introduce Variational Surface-Adaptive Modulation (V-SAM) that embeds surface information throughout the generator. Combining this with our novel discriminator surface supervision loss, the generator can synthesize high quality humans with diverse appearances in complex and varying scenes. We demonstrate that surface guidance significantly improves image quality and diversity of samples, yielding a highly practical generator. Finally, we show that our method preserves data usability without infringing privacy when collecting image datasets for training computer vision models. Source code and appendix is available at: \href{https://github.com/hukkelas/full_body_anonymization}{github.com/hukkelas/full\_body\_anonymization}



### Consistent Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2201.02233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02233v1)
- **Published**: 2022-01-06 20:19:35+00:00
- **Updated**: 2022-01-06 20:19:35+00:00
- **Authors**: Xuan Luo, Zhen Han, Lingkang Yang, Lingling Zhang
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: Recently, attentional arbitrary style transfer methods have been proposed to achieve fine-grained results, which manipulates the point-wise similarity between content and style features for stylization. However, the attention mechanism based on feature points ignores the feature multi-manifold distribution, where each feature manifold corresponds to a semantic region in the image. Consequently, a uniform content semantic region is rendered by highly different patterns from various style semantic regions, producing inconsistent stylization results with visual artifacts. We proposed the progressive attentional manifold alignment (PAMA) to alleviate this problem, which repeatedly applies attention operations and space-aware interpolations. The attention operation rearranges style features dynamically according to the spatial distribution of content features. This makes the content and style manifolds correspond on the feature map. Then the space-aware interpolation adaptively interpolates between the corresponding content and style manifolds to increase their similarity. By gradually aligning the content manifolds to style manifolds, the proposed PAMA achieves state-of-the-art performance while avoiding the inconsistency of semantic regions. Codes are available at https://github.com/computer-vision2022/PAMA.



### A Keypoint Detection and Description Network Based on the Vessel Structure for Multi-Modal Retinal Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2201.02242v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.02242v1)
- **Published**: 2022-01-06 20:43:35+00:00
- **Updated**: 2022-01-06 20:43:35+00:00
- **Authors**: Aline Sindel, Bettina Hohberger, Sebastian Fassihi Dehcordi, Christian Mardin, Robert Lämmer, Andreas Maier, Vincent Christlein
- **Comment**: 6 pages, 4 figures, 1 table, accepted to BVM 2022
- **Journal**: None
- **Summary**: Ophthalmological imaging utilizes different imaging systems, such as color fundus, infrared, fluorescein angiography, optical coherence tomography (OCT) or OCT angiography. Multiple images with different modalities or acquisition times are often analyzed for the diagnosis of retinal diseases. Automatically aligning the vessel structures in the images by means of multi-modal registration can support the ophthalmologists in their work. Our method uses a convolutional neural network to extract features of the vessel structure in multi-modal retinal images. We jointly train a keypoint detection and description network on small patches using a classification and a cross-modal descriptor loss function and apply the network to the full image size in the test phase. Our method demonstrates the best registration performance on our and a public multi-modal dataset in comparison to competing methods.



### CitySurfaces: City-Scale Semantic Segmentation of Sidewalk Materials
- **Arxiv ID**: http://arxiv.org/abs/2201.02260v1
- **DOI**: 10.1016/j.scs.2021.103630
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2201.02260v1)
- **Published**: 2022-01-06 21:58:37+00:00
- **Updated**: 2022-01-06 21:58:37+00:00
- **Authors**: Maryam Hosseini, Fabio Miranda, Jianzhe Lin, Claudio Silva
- **Comment**: Sustainable Cities and Society journal (accepted); Model:
  https://github.com/VIDA-NYU/city-surfaces
- **Journal**: None
- **Summary**: While designing sustainable and resilient urban built environment is increasingly promoted around the world, significant data gaps have made research on pressing sustainability issues challenging to carry out. Pavements are known to have strong economic and environmental impacts; however, most cities lack a spatial catalog of their surfaces due to the cost-prohibitive and time-consuming nature of data collection. Recent advancements in computer vision, together with the availability of street-level images, provide new opportunities for cities to extract large-scale built environment data with lower implementation costs and higher accuracy. In this paper, we propose CitySurfaces, an active learning-based framework that leverages computer vision techniques for classifying sidewalk materials using widely available street-level images. We trained the framework on images from New York City and Boston and the evaluation results show a 90.5% mIoU score. Furthermore, we evaluated the framework using images from six different cities, demonstrating that it can be applied to regions with distinct urban fabrics, even outside the domain of the training data. CitySurfaces can provide researchers and city agencies with a low-cost, accurate, and extensible method to collect sidewalk material data which plays a critical role in addressing major sustainability issues, including climate change and surface water management.



### ITSA: An Information-Theoretic Approach to Automatic Shortcut Avoidance and Domain Generalization in Stereo Matching Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.02263v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.02263v2)
- **Published**: 2022-01-06 22:03:50+00:00
- **Updated**: 2022-03-03 21:53:33+00:00
- **Authors**: WeiQin Chuah, Ruwan Tennakoon, Reza Hoseinnezhad, Alireza Bab-Hadiashar, David Suter
- **Comment**: 11 pages, 4 figures. Accepted by CVPR2022
- **Journal**: None
- **Summary**: State-of-the-art stereo matching networks trained only on synthetic data often fail to generalize to more challenging real data domains. In this paper, we attempt to unfold an important factor that hinders the networks from generalizing across domains: through the lens of shortcut learning. We demonstrate that the learning of feature representations in stereo matching networks is heavily influenced by synthetic data artefacts (shortcut attributes). To mitigate this issue, we propose an Information-Theoretic Shortcut Avoidance~(ITSA) approach to automatically restrict shortcut-related information from being encoded into the feature representations. As a result, our proposed method learns robust and shortcut-invariant features by minimizing the sensitivity of latent features to input variations. To avoid the prohibitive computational cost of direct input sensitivity optimization, we propose an effective yet feasible algorithm to achieve robustness. We show that using this method, state-of-the-art stereo matching networks that are trained purely on synthetic data can effectively generalize to challenging and previously unseen real data scenarios. Importantly, the proposed method enhances the robustness of the synthetic trained networks to the point that they outperform their fine-tuned counterparts (on real data) for challenging out-of-domain stereo datasets.



### De-rendering 3D Objects in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2201.02279v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02279v2)
- **Published**: 2022-01-06 23:50:09+00:00
- **Updated**: 2022-09-27 14:36:16+00:00
- **Authors**: Felix Wimbauer, Shangzhe Wu, Christian Rupprecht
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), 2022, pp. 18490-18499
- **Summary**: With increasing focus on augmented and virtual reality applications (XR) comes the demand for algorithms that can lift objects from images and videos into representations that are suitable for a wide variety of related 3D tasks. Large-scale deployment of XR devices and applications means that we cannot solely rely on supervised learning, as collecting and annotating data for the unlimited variety of objects in the real world is infeasible. We present a weakly supervised method that is able to decompose a single image of an object into shape (depth and normals), material (albedo, reflectivity and shininess) and global lighting parameters. For training, the method only relies on a rough initial shape estimate of the training objects to bootstrap the learning process. This shape supervision can come for example from a pretrained depth network or - more generically - from a traditional structure-from-motion pipeline. In our experiments, we show that the method can successfully de-render 2D images into a decomposed 3D representation and generalizes to unseen object categories. Since in-the-wild evaluation is difficult due to the lack of ground truth data, we also introduce a photo-realistic synthetic test set that allows for quantitative evaluation.



