# Arxiv Papers in cs.CV on 2022-01-08
### Pseudo-labelling and Meta Reweighting Learning for Image Aesthetic Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2201.02714v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.02714v1)
- **Published**: 2022-01-08 00:43:01+00:00
- **Updated**: 2022-01-08 00:43:01+00:00
- **Authors**: Xin Jin, Hao Lou, Huang Heng, Xiaodong Li, Shuai Cui, Xiaokun Zhang, Xiqiao Li
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: In the tasks of image aesthetic quality evaluation, it is difficult to reach both the high score area and low score area due to the normal distribution of aesthetic datasets. To reduce the error in labeling and solve the problem of normal data distribution, we propose a new aesthetic mixed dataset with classification and regression called AMD-CR, and we train a meta reweighting network to reweight the loss of training data differently. In addition, we provide a training strategy acccording to different stages, based on pseudo labels of the binary classification task, and then we use it for aesthetic training acccording to different stages in classification and regression tasks. In the construction of the network structure, we construct an aesthetic adaptive block (AAB) structure that can adapt to any size of the input images. Besides, we also use the efficient channel attention (ECA) to strengthen the feature extracting ability of each task. The experimental result shows that our method improves 0.1112 compared with the conventional methods in SROCC. The method can also help to find best aesthetic path planning for unmanned aerial vehicles (UAV) and vehicles.



### Real-time Rail Recognition Based on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2201.02726v1
- **DOI**: 10.1088/1361-6501/ac750c
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02726v1)
- **Published**: 2022-01-08 01:42:02+00:00
- **Updated**: 2022-01-08 01:42:02+00:00
- **Authors**: Xinyi Yu, Weiqi He, Xuecheng Qian, Yang Yang, Linlin Ou
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate rail location is a crucial part in the railway support driving system for safety monitoring. LiDAR can obtain point clouds that carry 3D information for the railway environment, especially in darkness and terrible weather conditions. In this paper, a real-time rail recognition method based on 3D point clouds is proposed to solve the challenges, such as disorderly, uneven density and large volume of the point clouds. A voxel down-sampling method is first presented for density balanced of railway point clouds, and pyramid partition is designed to divide the 3D scanning area into the voxels with different volumes. Then, a feature encoding module is developed to find the nearest neighbor points and to aggregate their local geometric features for the center point. Finally, a multi-scale neural network is proposed to generate the prediction results of each voxel and the rail location. The experiments are conducted under 9 sequences of 3D point cloud data for the railway. The results show that the method has good performance in detecting straight, curved and other complex topologies rails.



### Expert Knowledge-guided Geometric Representation Learning for Magnetic Resonance Imaging-based Glioma Grading
- **Arxiv ID**: http://arxiv.org/abs/2201.02746v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.02746v1)
- **Published**: 2022-01-08 02:45:11+00:00
- **Updated**: 2022-01-08 02:45:11+00:00
- **Authors**: Yeqi Wang, Longfei Li, Cheng Li, Yan Xi, Hairong Zheng, Yusong Lin, Shanshan Wang
- **Comment**: 10 pages, 9 figures, 2 tables
- **Journal**: None
- **Summary**: Radiomics and deep learning have shown high popularity in automatic glioma grading. Radiomics can extract hand-crafted features that quantitatively describe the expert knowledge of glioma grades, and deep learning is powerful in extracting a large number of high-throughput features that facilitate the final classification. However, the performance of existing methods can still be improved as their complementary strengths have not been sufficiently investigated and integrated. Furthermore, lesion maps are usually needed for the final prediction at the testing phase, which is very troublesome. In this paper, we propose an expert knowledge-guided geometric representation learning (ENROL) framework . Geometric manifolds of hand-crafted features and learned features are constructed to mine the implicit relationship between deep learning and radiomics, and therefore to dig mutual consent and essential representation for the glioma grades. With a specially designed manifold discrepancy measurement, the grading model can exploit the input image data and expert knowledge more effectively in the training phase and get rid of the requirement of lesion segmentation maps at the testing phase. The proposed framework is flexible regarding deep learning architectures to be utilized. Three different architectures have been evaluated and five models have been compared, which show that our framework can always generate promising results.



### QuadTree Attention for Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2201.02767v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02767v2)
- **Published**: 2022-01-08 05:45:32+00:00
- **Updated**: 2022-03-23 19:10:58+00:00
- **Authors**: Shitao Tang, Jiahui Zhang, Siyu Zhu, Ping Tan
- **Comment**: ICLR2022
- **Journal**: None
- **Summary**: Transformers have been successful in many vision tasks, thanks to their capability of capturing long-range dependency. However, their quadratic computational complexity poses a major obstacle for applying them to vision tasks requiring dense predictions, such as object detection, feature matching, stereo, etc. We introduce QuadTree Attention, which reduces the computational complexity from quadratic to linear. Our quadtree transformer builds token pyramids and computes attention in a coarse-to-fine manner. At each level, the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top K patches. We demonstrate that quadtree attention achieves state-of-the-art performance in various vision tasks, e.g. with 4.0% improvement in feature matching on ScanNet, about 50% flops reduction in stereo matching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification, 1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on semantic segmentation over previous state-of-the-art transformers. The codes are available at https://github.com/Tangshitao/QuadtreeAttention.



### A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2201.02771v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.02771v2)
- **Published**: 2022-01-08 05:57:26+00:00
- **Updated**: 2022-01-28 04:47:31+00:00
- **Authors**: Shuyue Guan, Murray Loew
- **Comment**: 8 pages, 10 figures. Accepted by IEEE AIPR 2021 (Oral)
- **Journal**: None
- **Summary**: Instead of using current deep-learning segmentation models (like the UNet and variants), we approach the segmentation problem using trained Convolutional Neural Network (CNN) classifiers, which automatically extract important features from images for classification. Those extracted features can be visualized and formed into heatmaps using Gradient-weighted Class Activation Mapping (Grad-CAM). This study tested whether the heatmaps could be used to segment the classified targets. We also proposed an evaluation method for the heatmaps; that is, to re-train the CNN classifier using images filtered by heatmaps and examine its performance. We used the mean-Dice coefficient to evaluate segmentation results. Results from our experiments show that heatmaps can locate and segment partial tumor areas. But use of only the heatmaps from CNN classifiers may not be an optimal approach for segmentation. We have verified that the predictions of CNN classifiers mainly depend on tumor areas, and dark regions in Grad-CAM's heatmaps also contribute to classification.



### A Comprehensive Empirical Study of Vision-Language Pre-trained Model for Supervised Cross-Modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2201.02772v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.02772v2)
- **Published**: 2022-01-08 06:00:22+00:00
- **Updated**: 2022-04-17 15:32:25+00:00
- **Authors**: Zhixiong Zeng, Wenji Mao
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-Modal Retrieval (CMR) is an important research topic across multimodal computing and information retrieval, which takes one type of data as the query to retrieve relevant data of another type. It has been widely used in many real-world applications. Recently, the vision-language pre-trained models represented by CLIP demonstrate its superiority in learning the visual and textual representations and gain impressive performance on various vision and language related tasks. Although CLIP as well as the previous pre-trained models have shown great performance improvement in the unsupervised CMR, the performance and impact of these pre-trained models on the supervised CMR were rarely explored due to the lack of common representation for the multimodal class-level associations. In this paper, we take CLIP as the current representative vision-language pre-trained model to conduct a comprehensive empirical study. We evaluate its performance and impact on the supervised CMR, and attempt to answer several key research questions. To this end, we first propose a novel model CLIP4CMR (CLIP enhanced network for Cross-Modal Retrieval) that employs the pre-trained CLIP as backbone network to perform the supervised CMR. Then by means of the CLIP4CMR framework, we revisit the design of different learning objectives in current CMR methods to provide new insights on model design. Moreover, we investigate the most concerned aspects in applying CMR, including the robustness to modality imbalance and sensitivity to hyper-parameters, to provide new perspectives for practical applications. Through extensive experiments, we show that CLIP4CMR achieves the SOTA results with prominent improvements on the benchmark datasets, and can be used as a fundamental framework to empirically study the key research issues of the supervised CMR, with significant implications for model design and practical considerations.



### A Baseline Statistical Method For Robust User-Assisted Multiple Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.02779v1
- **DOI**: 10.1109/LSP.2022.3154313
- **Categories**: **cs.CV**, cs.IT, eess.SP, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2201.02779v1)
- **Published**: 2022-01-08 06:55:45+00:00
- **Updated**: 2022-01-08 06:55:45+00:00
- **Authors**: Huseyin Afser
- **Comment**: Submitted to IEEE Signal Processing Letters. Is a continuation to our
  work: H. Af\c{s}er, "Statistical Classification via Robust Hypothesis
  Testing: Non-Asymptotic and Simple Bounds," in IEEE Signal Processing
  Letters, vol. 28, pp. 2112-2116, 2021
- **Journal**: None
- **Summary**: Recently, several image segmentation methods that welcome and leverage different types of user assistance have been developed. In these methods, the user inputs can be provided by drawing bounding boxes over image objects, drawing scribbles or planting seeds that help to differentiate between image boundaries or by interactively refining the missegmented image regions. Due to the variety in the types and the amounts of these inputs, relative assessment of different segmentation methods becomes difficult. As a possible solution, we propose a simple yet effective, statistical segmentation method that can handle and utilize different input types and amounts. The proposed method is based on robust hypothesis testing, specifically the DGL test, and can be implemented with time complexity that is linear in the number of pixels and quadratic in the number of image regions. Therefore, it is suitable to be used as a baseline method for quick benchmarking and assessing the relative performance improvements of different types of user-assisted segmentation algorithms. We provide a mathematical analysis on the operation of the proposed method, discuss its capabilities and limitations, provide design guidelines and present simulations that validate its operation.



### Relieving Long-tailed Instance Segmentation via Pairwise Class Balance
- **Arxiv ID**: http://arxiv.org/abs/2201.02784v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02784v2)
- **Published**: 2022-01-08 07:48:36+00:00
- **Updated**: 2022-04-03 14:57:15+00:00
- **Authors**: Yin-Yin He, Peizhen Zhang, Xiu-Shen Wei, Xiangyu Zhang, Jian Sun
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Long-tailed instance segmentation is a challenging task due to the extreme imbalance of training samples among classes. It causes severe biases of the head classes (with majority samples) against the tailed ones. This renders "how to appropriately define and alleviate the bias" one of the most important issues. Prior works mainly use label distribution or mean score information to indicate a coarse-grained bias. In this paper, we explore to excavate the confusion matrix, which carries the fine-grained misclassification details, to relieve the pairwise biases, generalizing the coarse one. To this end, we propose a novel Pairwise Class Balance (PCB) method, built upon a confusion matrix which is updated during training to accumulate the ongoing prediction preferences. PCB generates fightback soft labels for regularization during training. Besides, an iterative learning paradigm is developed to support a progressive and smooth regularization in such debiasing. PCB can be plugged and played to any existing method as a complement. Experimental results on LVIS demonstrate that our method achieves state-of-the-art performance without bells and whistles. Superior results across various architectures show the generalization ability. The code and trained models are available at https://github.com/megvii-research/PCB.



### RARA: Zero-shot Sim2Real Visual Navigation with Following Foreground Cues
- **Arxiv ID**: http://arxiv.org/abs/2201.02798v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.02798v1)
- **Published**: 2022-01-08 09:53:21+00:00
- **Updated**: 2022-01-08 09:53:21+00:00
- **Authors**: Klaas Kelchtermans, Tinne Tuytelaars
- **Comment**: 7 pages, submitted to IROS, code: github.com/kkelchte/fgbg
- **Journal**: None
- **Summary**: The gap between simulation and the real-world restrains many machine learning breakthroughs in computer vision and reinforcement learning from being applicable in the real world. In this work, we tackle this gap for the specific case of camera-based navigation, formulating it as following a visual cue in the foreground with arbitrary backgrounds. The visual cue in the foreground can often be simulated realistically, such as a line, gate or cone. The challenge then lies in coping with the unknown backgrounds and integrating both. As such, the goal is to train a visual agent on data captured in an empty simulated environment except for this foreground cue and test this model directly in a visually diverse real world. In order to bridge this big gap, we show it's crucial to combine following techniques namely: Randomized augmentation of the fore- and background, regularization with both deep supervision and triplet loss and finally abstraction of the dynamics by using waypoints rather than direct velocity commands. The various techniques are ablated in our experimental results both qualitatively and quantitatively finally demonstrating a successful transfer from simulation to the real world.



### Counteracting Dark Web Text-Based CAPTCHA with Generative Adversarial Learning for Proactive Cyber Threat Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2201.02799v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02799v2)
- **Published**: 2022-01-08 09:53:31+00:00
- **Updated**: 2022-01-14 21:32:22+00:00
- **Authors**: Ning Zhang, Mohammadreza Ebrahimi, Weifeng Li, Hsinchun Chen
- **Comment**: Accepted by ACM Transactions on Management Information Systems
- **Journal**: None
- **Summary**: Automated monitoring of dark web (DW) platforms on a large scale is the first step toward developing proactive Cyber Threat Intelligence (CTI). While there are efficient methods for collecting data from the surface web, large-scale dark web data collection is often hindered by anti-crawling measures. In particular, text-based CAPTCHA serves as the most prevalent and prohibiting type of these measures in the dark web. Text-based CAPTCHA identifies and blocks automated crawlers by forcing the user to enter a combination of hard-to-recognize alphanumeric characters. In the dark web, CAPTCHA images are meticulously designed with additional background noise and variable character length to prevent automated CAPTCHA breaking. Existing automated CAPTCHA breaking methods have difficulties in overcoming these dark web challenges. As such, solving dark web text-based CAPTCHA has been relying heavily on human involvement, which is labor-intensive and time-consuming. In this study, we propose a novel framework for automated breaking of dark web CAPTCHA to facilitate dark web data collection. This framework encompasses a novel generative method to recognize dark web text-based CAPTCHA with noisy background and variable character length. To eliminate the need for human involvement, the proposed framework utilizes Generative Adversarial Network (GAN) to counteract dark web background noise and leverages an enhanced character segmentation algorithm to handle CAPTCHA images with variable character length. Our proposed framework, DW-GAN, was systematically evaluated on multiple dark web CAPTCHA testbeds. DW-GAN significantly outperformed the state-of-the-art benchmark methods on all datasets, achieving over 94.4% success rate on a carefully collected real-world dark web dataset...



### Hyperspectral Image Denoising Using Non-convex Local Low-rank and Sparse Separation with Spatial-Spectral Total Variation Regularization
- **Arxiv ID**: http://arxiv.org/abs/2201.02812v1
- **DOI**: 10.1109/TGRS.2022.3206783
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.02812v1)
- **Published**: 2022-01-08 11:48:46+00:00
- **Updated**: 2022-01-08 11:48:46+00:00
- **Authors**: Chong Peng, Yang Liu, Yongyong Chen, Xinxin Wu, Andrew Cheng, Zhao Kang, Chenglizhao Chen, Qiang Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel nonconvex approach to robust principal component analysis for HSI denoising, which focuses on simultaneously developing more accurate approximations to both rank and column-wise sparsity for the low-rank and sparse components, respectively. In particular, the new method adopts the log-determinant rank approximation and a novel $\ell_{2,\log}$ norm, to restrict the local low-rank or column-wisely sparse properties for the component matrices, respectively. For the $\ell_{2,\log}$-regularized shrinkage problem, we develop an efficient, closed-form solution, which is named $\ell_{2,\log}$-shrinkage operator. The new regularization and the corresponding operator can be generally used in other problems that require column-wise sparsity. Moreover, we impose the spatial-spectral total variation regularization in the log-based nonconvex RPCA model, which enhances the global piece-wise smoothness and spectral consistency from the spatial and spectral views in the recovered HSI. Extensive experiments on both simulated and real HSIs demonstrate the effectiveness of the proposed method in denoising HSIs.



### Classification of Hyperspectral Images by Using Spectral Data and Fully Connected Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2201.02821v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.02821v1)
- **Published**: 2022-01-08 12:45:48+00:00
- **Updated**: 2022-01-08 12:45:48+00:00
- **Authors**: Zumray Dokur, Tamer Olmez
- **Comment**: None
- **Journal**: None
- **Summary**: It is observed that high classification performance is achieved for one- and two-dimensional signals by using deep learning methods. In this context, most researchers have tried to classify hyperspectral images by using deep learning methods and classification success over 90% has been achieved for these images. Deep neural networks (DNN) actually consist of two parts: i) Convolutional neural network (CNN) and ii) fully connected neural network (FCNN). While CNN determines the features, FCNN is used in classification. In classification of the hyperspectral images, it is observed that almost all of the researchers used 2D or 3D convolution filters on the spatial data beside spectral data (features). It is convenient to use convolution filters on images or time signals. In hyperspectral images, each pixel is represented by a signature vector which consists of individual features that are independent of each other. Since the order of the features in the vector can be changed, it doesn't make sense to use convolution filters on these features as on time signals. At the same time, since the hyperspectral images do not have a textural structure, there is no need to use spatial data besides spectral data. In this study, hyperspectral images of Indian pines, Salinas, Pavia centre, Pavia university and Botswana are classified by using only fully connected neural network and the spectral data with one dimensional. An average accuracy of 97.5% is achieved for the test sets of all hyperspectral images.



### CrossMoDA 2021 challenge: Benchmark of Cross-Modality Domain Adaptation techniques for Vestibular Schwannoma and Cochlea Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.02831v3
- **DOI**: 10.1016/j.media.2022.102628
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.02831v3)
- **Published**: 2022-01-08 14:00:34+00:00
- **Updated**: 2022-12-14 15:09:54+00:00
- **Authors**: Reuben Dorent, Aaron Kujawa, Marina Ivory, Spyridon Bakas, Nicola Rieke, Samuel Joutard, Ben Glocker, Jorge Cardoso, Marc Modat, Kayhan Batmanghelich, Arseniy Belkov, Maria Baldeon Calisto, Jae Won Choi, Benoit M. Dawant, Hexin Dong, Sergio Escalera, Yubo Fan, Lasse Hansen, Mattias P. Heinrich, Smriti Joshi, Victoriya Kashtanova, Hyeon Gyu Kim, Satoshi Kondo, Christian N. Kruse, Susana K. Lai-Yuen, Hao Li, Han Liu, Buntheng Ly, Ipek Oguz, Hyungseob Shin, Boris Shirokikh, Zixian Su, Guotai Wang, Jianghao Wu, Yanwu Xu, Kai Yao, Li Zhang, Sebastien Ourselin, Jonathan Shapey, Tom Vercauteren
- **Comment**: In Medical Image Analysis
- **Journal**: None
- **Summary**: Domain Adaptation (DA) has recently raised strong interests in the medical imaging community. While a large variety of DA techniques has been proposed for image segmentation, most of these techniques have been validated either on private datasets or on small publicly available datasets. Moreover, these datasets mostly addressed single-class problems. To tackle these limitations, the Cross-Modality Domain Adaptation (crossMoDA) challenge was organised in conjunction with the 24th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2021). CrossMoDA is the first large and multi-class benchmark for unsupervised cross-modality DA. The challenge's goal is to segment two key brain structures involved in the follow-up and treatment planning of vestibular schwannoma (VS): the VS and the cochleas. Currently, the diagnosis and surveillance in patients with VS are performed using contrast-enhanced T1 (ceT1) MRI. However, there is growing interest in using non-contrast sequences such as high-resolution T2 (hrT2) MRI. Therefore, we created an unsupervised cross-modality segmentation benchmark. The training set provides annotated ceT1 (N=105) and unpaired non-annotated hrT2 (N=105). The aim was to automatically perform unilateral VS and bilateral cochlea segmentation on hrT2 as provided in the testing set (N=137). A total of 16 teams submitted their algorithm for the evaluation phase. The level of performance reached by the top-performing teams is strikingly high (best median Dice - VS:88.4%; Cochleas:85.7%) and close to full supervision (median Dice - VS:92.5%; Cochleas:87.7%). All top-performing methods made use of an image-to-image translation approach to transform the source-domain images into pseudo-target-domain images. A segmentation network was then trained using these generated images and the manual annotations provided for the source image.



### SGUIE-Net: Semantic Attention Guided Underwater Image Enhancement with Multi-Scale Perception
- **Arxiv ID**: http://arxiv.org/abs/2201.02832v1
- **DOI**: 10.1109/TIP.2022.3216208
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.02832v1)
- **Published**: 2022-01-08 14:03:24+00:00
- **Updated**: 2022-01-08 14:03:24+00:00
- **Authors**: Qi Qi, Kunqian Li, Haiyong Zheng, Xiang Gao, Guojia Hou, Kun Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the wavelength-dependent light attenuation, refraction and scattering, underwater images usually suffer from color distortion and blurred details. However, due to the limited number of paired underwater images with undistorted images as reference, training deep enhancement models for diverse degradation types is quite difficult. To boost the performance of data-driven approaches, it is essential to establish more effective learning mechanisms that mine richer supervised information from limited training sample resources. In this paper, we propose a novel underwater image enhancement network, called SGUIE-Net, in which we introduce semantic information as high-level guidance across different images that share common semantic regions. Accordingly, we propose semantic region-wise enhancement module to perceive the degradation of different semantic regions from multiple scales and feed it back to the global attention features extracted from its original scale. This strategy helps to achieve robust and visually pleasant enhancements to different semantic objects, which should thanks to the guidance of semantic information for differentiated enhancement. More importantly, for those degradation types that are not common in the training sample distribution, the guidance connects them with the already well-learned types according to their semantic relevance. Extensive experiments on the publicly available datasets and our proposed dataset demonstrated the impressive performance of SGUIE-Net. The code and proposed dataset are available at: https://trentqq.github.io/SGUIE-Net.html



### Weighted Encoding Optimization for Dynamic Single-pixel Imaging and Sensing
- **Arxiv ID**: http://arxiv.org/abs/2201.02833v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.02833v1)
- **Published**: 2022-01-08 14:11:22+00:00
- **Updated**: 2022-01-08 14:11:22+00:00
- **Authors**: Xinrui Zhan, Liheng Bian, Chunli Zhu, Jun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Using single-pixel detection, the end-to-end neural network that jointly optimizes both encoding and decoding enables high-precision imaging and high-level semantic sensing. However, for varied sampling rates, the large-scale network requires retraining that is laboursome and computation-consuming. In this letter, we report a weighted optimization technique for dynamic rate-adaptive single-pixel imaging and sensing, which only needs to train the network for one time that is available for any sampling rates. Specifically, we introduce a novel weighting scheme in the encoding process to characterize different patterns' modulation efficiency. While the network is training at a high sampling rate, the modulation patterns and corresponding weights are updated iteratively, which produces optimal ranked encoding series when converged. In the experimental implementation, the optimal pattern series with the highest weights are employed for light modulation, thus achieving highly-efficient imaging and sensing. The reported strategy saves the additional training of another low-rate network required by the existing dynamic single-pixel networks, which further doubles training efficiency. Experiments on the MNIST dataset validated that once the network is trained with a sampling rate of 1, the average imaging PSNR reaches 23.50 dB at 0.1 sampling rate, and the image-free classification accuracy reaches up to 95.00\% at a sampling rate of 0.03 and 97.91\% at a sampling rate of 0.1.



### Self-aligned Spatial Feature Extraction Network for UAV Vehicle Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2201.02836v1
- **DOI**: 10.1109/LGRS.2023.3237823
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.02836v1)
- **Published**: 2022-01-08 14:25:54+00:00
- **Updated**: 2022-01-08 14:25:54+00:00
- **Authors**: Aihuan Yao, Jiahao Qi, Ping Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: Compared with existing vehicle re-identification (ReID) tasks conducted with datasets collected by fixed surveillance cameras, vehicle ReID for unmanned aerial vehicle (UAV) is still under-explored and could be more challenging. Vehicles with the same color and type show extremely similar appearance from the UAV's perspective so that mining fine-grained characteristics becomes necessary. Recent works tend to extract distinguishing information by regional features and component features. The former requires input images to be aligned and the latter entails detailed annotations, both of which are difficult to meet in UAV application. In order to extract efficient fine-grained features and avoid tedious annotating work, this letter develops an unsupervised self-aligned network consisting of three branches. The network introduced a self-alignment module to convert the input images with variable orientations to a uniform orientation, which is implemented under the constraint of triple loss function designed with spatial features. On this basis, spatial features, obtained by vertical and horizontal segmentation methods, and global features are integrated to improve the representation ability in embedded space. Extensive experiments are conducted on UAV-VeID dataset, and our method achieves the best performance compared with recent ReID works.



### Mushrooms Detection, Localization and 3D Pose Estimation using RGB-D Sensor for Robotic-picking Applications
- **Arxiv ID**: http://arxiv.org/abs/2201.02837v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.02837v1)
- **Published**: 2022-01-08 14:32:25+00:00
- **Updated**: 2022-01-08 14:32:25+00:00
- **Authors**: Nathanael L. Baisa, Bashir Al-Diri
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose mushrooms detection, localization and 3D pose estimation algorithm using RGB-D data acquired from a low-cost consumer RGB-D sensor. We use the RGB and depth information for different purposes. From RGB color, we first extract initial contour locations of the mushrooms and then provide both the initial contour locations and the original image to active contour for mushrooms segmentation. These segmented mushrooms are then used as input to a circular Hough transform for each mushroom detection including its center and radius. Once each mushroom's center position in the RGB image is known, we then use the depth information to locate it in 3D space i.e. in world coordinate system. In case of missing depth information at the detected center of each mushroom, we estimate from the nearest available depth information within the radius of each mushroom. We also estimate the 3D pose of each mushroom using a pre-prepared upright mushroom model. We use a global registration followed by local refine registration approach for this 3D pose estimation. From the estimated 3D pose, we use only the rotation part expressed in quaternion as an orientation of each mushroom. These estimated (X,Y,Z) positions, diameters and orientations of the mushrooms are used for robotic-picking applications. We carry out extensive experiments on both 3D printed and real mushrooms which show that our method has an interesting performance.



### Learning Sample Importance for Cross-Scenario Video Temporal Grounding
- **Arxiv ID**: http://arxiv.org/abs/2201.02848v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2201.02848v1)
- **Published**: 2022-01-08 15:41:38+00:00
- **Updated**: 2022-01-08 15:41:38+00:00
- **Authors**: Peijun Bao, Yadong Mu
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: The task of temporal grounding aims to locate video moment in an untrimmed video, with a given sentence query. This paper for the first time investigates some superficial biases that are specific to the temporal grounding task, and proposes a novel targeted solution. Most alarmingly, we observe that existing temporal ground models heavily rely on some biases (e.g., high preference on frequent concepts or certain temporal intervals) in the visual modal. This leads to inferior performance when generalizing the model in cross-scenario test setting. To this end, we propose a novel method called Debiased Temporal Language Localizer (DebiasTLL) to prevent the model from naively memorizing the biases and enforce it to ground the query sentence based on true inter-modal relationship. Debias-TLL simultaneously trains two models. By our design, a large discrepancy of these two models' predictions when judging a sample reveals higher probability of being a biased sample. Harnessing the informative discrepancy, we devise a data re-weighing scheme for mitigating the data biases. We evaluate the proposed model in cross-scenario temporal grounding, where the train / test data are heterogeneously sourced. Experiments show large-margin superiority of the proposed method in comparison with state-of-the-art competitors.



### Spatio-Temporal Tuples Transformer for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.02849v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.02849v1)
- **Published**: 2022-01-08 16:03:01+00:00
- **Updated**: 2022-01-08 16:03:01+00:00
- **Authors**: Helei Qiu, Biao Hou, Bo Ren, Xiaohua Zhang
- **Comment**: 14 pages, 5 figures
- **Journal**: None
- **Summary**: Capturing the dependencies between joints is critical in skeleton-based action recognition task. Transformer shows great potential to model the correlation of important joints. However, the existing Transformer-based methods cannot capture the correlation of different joints between frames, which the correlation is very useful since different body parts (such as the arms and legs in "long jump") between adjacent frames move together. Focus on this problem, A novel spatio-temporal tuples Transformer (STTFormer) method is proposed. The skeleton sequence is divided into several parts, and several consecutive frames contained in each part are encoded. And then a spatio-temporal tuples self-attention module is proposed to capture the relationship of different joints in consecutive frames. In addition, a feature aggregation module is introduced between non-adjacent frames to enhance the ability to distinguish similar actions. Compared with the state-of-the-art methods, our method achieves better performance on two large-scale datasets.



### Image-based Automatic Dial Meter Reading in Unconstrained Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2201.02850v2
- **DOI**: 10.1016/j.measurement.2022.112025
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02850v2)
- **Published**: 2022-01-08 16:03:46+00:00
- **Updated**: 2022-10-23 11:56:38+00:00
- **Authors**: Gabriel Salomon, Rayson Laroca, David Menotti
- **Comment**: None
- **Journal**: Measurement, vol. 204, p. 112025, 2022
- **Summary**: The replacement of analog meters with smart meters is costly, laborious, and far from complete in developing countries. The Energy Company of Parana (Copel) (Brazil) performs more than 4 million meter readings (almost entirely of non-smart devices) per month, and we estimate that 850 thousand of them are from dial meters. Therefore, an image-based automatic reading system can reduce human errors, create a proof of reading, and enable the customers to perform the reading themselves through a mobile application. We propose novel approaches for Automatic Dial Meter Reading (ADMR) and introduce a new dataset for ADMR in unconstrained scenarios, called UFPR-ADMR-v2. Our best-performing method combines YOLOv4 with a novel regression approach (AngReg), and explores several postprocessing techniques. Compared to previous works, it decreased the Mean Absolute Error (MAE) from 1,343 to 129 and achieved a meter recognition rate (MRR) of 98.90% -- with an error tolerance of 1 Kilowatt-hour (kWh).



### Fake Hilsa Fish Detection Using Machine Vision
- **Arxiv ID**: http://arxiv.org/abs/2201.02853v1
- **DOI**: 10.1007/978-981-16-0586-4_14
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.02853v1)
- **Published**: 2022-01-08 16:28:21+00:00
- **Updated**: 2022-01-08 16:28:21+00:00
- **Authors**: Mirajul Islam, Jannatul Ferdous Ani, Abdur Rahman, Zakia Zaman
- **Comment**: 12 pages, 8 figures, International Joint Conference on Advances in
  Computational Intelligence (IJCACI 2020)
- **Journal**: None
- **Summary**: Hilsa is the national fish of Bangladesh. Bangladesh is earning a lot of foreign currency by exporting this fish. Unfortunately, in recent days, some unscrupulous businessmen are selling fake Hilsa fishes to gain profit. The Sardines and Sardinella are the most sold in the market as Hilsa. The government agency of Bangladesh, namely Bangladesh Food Safety Authority said that these fake Hilsa fish contain high levels of cadmium and lead which are detrimental for humans. In this research, we have proposed a method that can readily identify original Hilsa fish and fake Hilsa fish. Based on the research available on online literature, we are the first to do research on identifying original Hilsa fish. We have collected more than 16,000 images of original and counterfeit Hilsa fish. To classify these images, we have used several deep learning-based models. Then, the performance has been compared between them. Among those models, DenseNet201 achieved the highest accuracy of 97.02%.



### Decoupling Makes Weakly Supervised Local Feature Better
- **Arxiv ID**: http://arxiv.org/abs/2201.02861v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02861v2)
- **Published**: 2022-01-08 16:51:02+00:00
- **Updated**: 2022-03-26 02:57:30+00:00
- **Authors**: Kunhong Li, Longguang Wang, Li Liu, Qing Ran, Kai Xu, Yulan Guo
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: Weakly supervised learning can help local feature methods to overcome the obstacle of acquiring a large-scale dataset with densely labeled correspondences. However, since weak supervision cannot distinguish the losses caused by the detection and description steps, directly conducting weakly supervised learning within a joint describe-then-detect pipeline suffers limited performance. In this paper, we propose a decoupled describe-then-detect pipeline tailored for weakly supervised local feature learning. Within our pipeline, the detection step is decoupled from the description step and postponed until discriminative and robust descriptors are learned. In addition, we introduce a line-to-window search strategy to explicitly use the camera pose information for better descriptor learning. Extensive experiments show that our method, namely PoSFeat (Camera Pose Supervised Feature), outperforms previous fully and weakly supervised methods and achieves state-of-the-art performance on a wide range of downstream tasks.



### Deep Generative Modeling for Volume Reconstruction in Cryo-Electron Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2201.02867v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2201.02867v3)
- **Published**: 2022-01-08 17:45:18+00:00
- **Updated**: 2022-05-26 03:54:31+00:00
- **Authors**: Claire Donnat, Axel Levy, Frederic Poitevin, Ellen Zhong, Nina Miolane
- **Comment**: None
- **Journal**: None
- **Summary**: Recent breakthroughs in high-resolution imaging of biomolecules in solution with cryo-electron microscopy (cryo-EM) have unlocked new doors for the reconstruction of molecular volumes, thereby promising further advances in biology, chemistry, and pharmacological research. Recent next-generation volume reconstruction algorithms that combine generative modeling with end-to-end unsupervised deep learning techniques have shown promising preliminary results, but still face considerable technical and theoretical hurdles when applied to experimental cryo-EM images. In light of the proliferation of such methods, we propose here a critical review of recent advances in the field of deep generative modeling for cryo-EM volume reconstruction. The present review aims to (i) unify and compare these new methods using a consistent statistical framework, (ii) present them using a terminology familiar to machine learning researchers and computational biologists with no specific background in cryo-EM, and (iii) provide the necessary perspective on current advances to highlight their relative strengths and weaknesses, along with outstanding bottlenecks and avenues for improvements in the field. This review might also raise the interest of computer vision practitioners, as it highlights significant limits of deep generative models in low signal-to-noise regimes -- therefore emphasizing a need for new theoretical and methodological developments.



### Defocus Deblur Microscopy via Head-to-Tail Cross-scale Fusion
- **Arxiv ID**: http://arxiv.org/abs/2201.02876v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.02876v2)
- **Published**: 2022-01-08 18:53:54+00:00
- **Updated**: 2023-05-30 04:33:11+00:00
- **Authors**: Jiahe Wang, Boran Han
- **Comment**: published on ICIP 2022
- **Journal**: None
- **Summary**: Microscopy imaging is vital in biology research and diagnosis. When imaging at the scale of cell or molecule level, mechanical drift on the axial axis can be difficult to correct. Although multi-scale networks have been developed for deblurring, those cascade residual learning approaches fail to accurately capture the end-to-end non-linearity of deconvolution, a relation between in-focus images and their out-of-focus counterparts in microscopy. In our model, we adopt a structure of multi-scale U-Net without cascade residual leaning. Additionally, in contrast to the conventional coarse-to-fine model, our model strengthens the cross-scale interaction by fusing the features from the coarser sub-networks with the finer ones in a head-to-tail manner: the decoder from the coarser scale is fused with the encoder of the finer ones. Such interaction contributes to better feature learning as fusion happens across decoder and encoder at all scales. Numerous experiments demonstrate that our method yields better performance when compared with other existing models.



### Agricultural Plant Cataloging and Establishment of a Data Framework from UAV-based Crop Images by Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2201.02885v2
- **DOI**: 10.1093/gigascience/giac054
- **Categories**: **cs.CV**, cs.CE, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.02885v2)
- **Published**: 2022-01-08 21:14:07+00:00
- **Updated**: 2022-01-11 11:49:09+00:00
- **Authors**: Maurice Günder, Facundo R. Ispizua Yamati, Jana Kierdorf, Ribana Roscher, Anne-Katrin Mahlein, Christian Bauckhage
- **Comment**: Preprint submitted to GigaScience
- **Journal**: GigaScience, Volume 11, 2022
- **Summary**: UAV-based image retrieval in modern agriculture enables gathering large amounts of spatially referenced crop image data. In large-scale experiments, however, UAV images suffer from containing a multitudinous amount of crops in a complex canopy architecture. Especially for the observation of temporal effects, this complicates the recognition of individual plants over several images and the extraction of relevant information tremendously. In this work, we present a hands-on workflow for the automatized temporal and spatial identification and individualization of crop images from UAVs abbreviated as "cataloging" based on comprehensible computer vision methods. We evaluate the workflow on two real-world datasets. One dataset is recorded for observation of Cercospora leaf spot - a fungal disease - in sugar beet over an entire growing cycle. The other one deals with harvest prediction of cauliflower plants. The plant catalog is utilized for the extraction of single plant images seen over multiple time points. This gathers large-scale spatio-temporal image dataset that in turn can be applied to train further machine learning models including various data layers. The presented approach improves analysis and interpretation of UAV data in agriculture significantly. By validation with some reference data, our method shows an accuracy that is similar to more complex deep learning-based recognition techniques. Our workflow is able to automatize plant cataloging and training image extraction, especially for large datasets.



