# Arxiv Papers in cs.CV on 2022-01-27
### HistoKT: Cross Knowledge Transfer in Computational Pathology
- **Arxiv ID**: http://arxiv.org/abs/2201.11246v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11246v1)
- **Published**: 2022-01-27 00:34:19+00:00
- **Updated**: 2022-01-27 00:34:19+00:00
- **Authors**: Ryan Zhang, Jiadai Zhu, Stephen Yang, Mahdi S. Hosseini, Angelo Genovese, Lina Chen, Corwyn Rowsell, Savvas Damaskinos, Sonal Varma, Konstantinos N. Plataniotis
- **Comment**: Accepted in ICASSP2022
- **Journal**: None
- **Summary**: The lack of well-annotated datasets in computational pathology (CPath) obstructs the application of deep learning techniques for classifying medical images. %Since pathologist time is expensive, dataset curation is intrinsically difficult. Many CPath workflows involve transferring learned knowledge between various image domains through transfer learning. Currently, most transfer learning research follows a model-centric approach, tuning network parameters to improve transfer results over few datasets. In this paper, we take a data-centric approach to the transfer learning problem and examine the existence of generalizable knowledge between histopathological datasets. First, we create a standardization workflow for aggregating existing histopathological data. We then measure inter-domain knowledge by training ResNet18 models across multiple histopathological datasets, and cross-transferring between them to determine the quantity and quality of innate shared knowledge. Additionally, we use weight distillation to share knowledge between models without additional training. We find that hard to learn, multi-class datasets benefit most from pretraining, and a two stage learning framework incorporating a large source domain such as ImageNet allows for better utilization of smaller datasets. Furthermore, we find that weight distillation enables models trained on purely histopathological features to outperform models using external natural image data.



### Controlling Directions Orthogonal to a Classifier
- **Arxiv ID**: http://arxiv.org/abs/2201.11259v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11259v1)
- **Published**: 2022-01-27 01:23:08+00:00
- **Updated**: 2022-01-27 01:23:08+00:00
- **Authors**: Yilun Xu, Hao He, Tianxiao Shen, Tommi Jaakkola
- **Comment**: accepted by ICLR 2022
- **Journal**: None
- **Summary**: We propose to identify directions invariant to a given classifier so that these directions can be controlled in tasks such as style transfer. While orthogonal decomposition is directly identifiable when the given classifier is linear, we formally define a notion of orthogonality in the non-linear case. We also provide a surprisingly simple method for constructing the orthogonal classifier (a classifier utilizing directions other than those of the given classifier). Empirically, we present three use cases where controlling orthogonal variation is important: style transfer, domain adaptation, and fairness. The orthogonal classifier enables desired style transfer when domains vary in multiple aspects, improves domain adaptation with label shifts and mitigates the unfairness as a predictor. The code is available at http://github.com/Newbeeer/orthogonal_classifier



### Revisiting RCAN: Improved Training for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2201.11279v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11279v1)
- **Published**: 2022-01-27 02:20:11+00:00
- **Updated**: 2022-01-27 02:20:11+00:00
- **Authors**: Zudi Lin, Prateek Garg, Atmadeep Banerjee, Salma Abdel Magid, Deqing Sun, Yulun Zhang, Luc Van Gool, Donglai Wei, Hanspeter Pfister
- **Comment**: 13 pages with 10 tables and 4 figures
- **Journal**: None
- **Summary**: Image super-resolution (SR) is a fast-moving field with novel architectures attracting the spotlight. However, most SR models were optimized with dated training strategies. In this work, we revisit the popular RCAN model and examine the effect of different training options in SR. Surprisingly (or perhaps as expected), we show that RCAN can outperform or match nearly all the CNN-based SR architectures published after RCAN on standard benchmarks with a proper training strategy and minimal architecture change. Besides, although RCAN is a very large SR architecture with more than four hundred convolutional layers, we draw a notable conclusion that underfitting is still the main problem restricting the model capability instead of overfitting. We observe supportive evidence that increasing training iterations clearly improves the model performance while applying regularization techniques generally degrades the predictions. We denote our simply revised RCAN as RCAN-it and recommend practitioners to use it as baselines for future research. Code is publicly available at https://github.com/zudi-lin/rcan-it.



### Interactive 3D Character Modeling from 2D Orthogonal Drawings with Annotations
- **Arxiv ID**: http://arxiv.org/abs/2201.11284v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2201.11284v1)
- **Published**: 2022-01-27 02:34:32+00:00
- **Updated**: 2022-01-27 02:34:32+00:00
- **Authors**: Zhengyu Huang, Haoran Xie, Tsukasa Fukusato
- **Comment**: 6 pages, 4 figures, accepted in Proceedings of International Workshop
  on Advanced Image Technology 2022
- **Journal**: None
- **Summary**: We propose an interactive 3D character modeling approach from orthographic drawings (e.g., front and side views) based on 2D-space annotations. First, the system builds partial correspondences between the input drawings and generates a base mesh with sweeping splines according to edge information in 2D images. Next, users annotates the desired parts on the input drawings (e.g., the eyes and mouth) by using two type of strokes, called addition and erosion, and the system re-optimizes the shape of the base mesh. By repeating the 2D-space operations (i.e., revising and modifying the annotations), users can design a desired character model. To validate the efficiency and quality of our system, we verified the generated results with state-of-the-art methods.



### Efficient divide-and-conquer registration of UAV and ground LiDAR point clouds through canopy shape context
- **Arxiv ID**: http://arxiv.org/abs/2201.11296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11296v1)
- **Published**: 2022-01-27 03:29:56+00:00
- **Updated**: 2022-01-27 03:29:56+00:00
- **Authors**: Jie Shao, Wei Yao, Peng Wan, Lei Luo, Jiaxin Lyu, Wuming Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Registration of unmanned aerial vehicle laser scanning (ULS) and ground light detection and ranging (LiDAR) point clouds in forests is critical to create a detailed representation of a forest structure and an accurate inversion of forest parameters. However, forest occlusion poses challenges for marker-based registration methods, and some marker-free automated registration methods have low efficiency due to the process of object (e.g., tree, crown) segmentation. Therefore, we use a divide-and-conquer strategy and propose an automated and efficient method to register ULS and ground LiDAR point clouds in forests. Registration involves coarse alignment and fine registration, where the coarse alignment of point clouds is divided into vertical and horizontal alignment. The vertical alignment is achieved by ground alignment, which is achieved by the transformation relationship between normal vectors of the ground point cloud and the horizontal plane, and the horizontal alignment is achieved by canopy projection image matching. During image matching, vegetation points are first distinguished by the ground filtering algorithm, and then, vegetation points are projected onto the horizontal plane to obtain two binary images. To match the two images, a matching strategy is used based on canopy shape context features, which are described by a two-point congruent set and canopy overlap. Finally, we implement coarse alignment of ULS and ground LiDAR datasets by combining the results of ground alignment and image matching and finish fine registration. Also, the effectiveness, accuracy, and efficiency of the proposed method are demonstrated by field measurements of forest plots. Experimental results show that the ULS and ground LiDAR data in different plots are registered, of which the horizontal alignment errors are less than 0.02 m, and the average runtime of the proposed method is less than 1 second.



### Dissecting the impact of different loss functions with gradient surgery
- **Arxiv ID**: http://arxiv.org/abs/2201.11307v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.11307v1)
- **Published**: 2022-01-27 03:55:48+00:00
- **Updated**: 2022-01-27 03:55:48+00:00
- **Authors**: Hong Xuan, Robert Pless
- **Comment**: None
- **Journal**: None
- **Summary**: Pair-wise loss is an approach to metric learning that learns a semantic embedding by optimizing a loss function that encourages images from the same semantic class to be mapped closer than images from different classes. The literature reports a large and growing set of variations of the pair-wise loss strategies. Here we decompose the gradient of these loss functions into components that relate to how they push the relative feature positions of the anchor-positive and anchor-negative pairs. This decomposition allows the unification of a large collection of current pair-wise loss functions. Additionally, explicitly constructing pair-wise gradient updates to separate out these effects gives insights into which have the biggest impact, and leads to a simple algorithm that beats the state of the art for image retrieval on the CAR, CUB and Stanford Online products datasets.



### Transformer Module Networks for Systematic Generalization in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2201.11316v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.11316v2)
- **Published**: 2022-01-27 04:22:25+00:00
- **Updated**: 2023-03-17 11:10:31+00:00
- **Authors**: Moyuru Yamada, Vanessa D'Amario, Kentaro Takemoto, Xavier Boix, Tomotake Sasaki
- **Comment**: None
- **Journal**: None
- **Summary**: Transformers achieve great performance on Visual Question Answering (VQA). However, their systematic generalization capabilities, i.e., handling novel combinations of known concepts, is unclear. We reveal that Neural Module Networks (NMNs), i.e., question-specific compositions of modules that tackle a sub-task, achieve better or similar systematic generalization performance than the conventional Transformers, even though NMNs' modules are CNN-based. In order to address this shortcoming of Transformers with respect to NMNs, in this paper we investigate whether and how modularity can bring benefits to Transformers. Namely, we introduce Transformer Module Network (TMN), a novel NMN based on compositions of Transformer modules. TMNs achieve state-of-the-art systematic generalization performance in three VQA datasets, improving more than 30% over standard Transformers for novel compositions of sub-tasks. We show that not only the module composition but also the module specialization for each sub-task are the key of such performance gain.



### Dynamic Rectification Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2201.11319v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.11319v1)
- **Published**: 2022-01-27 04:38:01+00:00
- **Updated**: 2022-01-27 04:38:01+00:00
- **Authors**: Fahad Rahman Amik, Ahnaf Ismat Tasin, Silvia Ahmed, M. M. Lutfe Elahi, Nabeel Mohammed
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge Distillation is a technique which aims to utilize dark knowledge to compress and transfer information from a vast, well-trained neural network (teacher model) to a smaller, less capable neural network (student model) with improved inference efficiency. This approach of distilling knowledge has gained popularity as a result of the prohibitively complicated nature of such cumbersome models for deployment on edge computing devices. Generally, the teacher models used to teach smaller student models are cumbersome in nature and expensive to train. To eliminate the necessity for a cumbersome teacher model completely, we propose a simple yet effective knowledge distillation framework that we termed Dynamic Rectification Knowledge Distillation (DR-KD). Our method transforms the student into its own teacher, and if the self-teacher makes wrong predictions while distilling information, the error is rectified prior to the knowledge being distilled. Specifically, the teacher targets are dynamically tweaked by the agency of ground-truth while distilling the knowledge gained from traditional training. Our proposed DR-KD performs remarkably well in the absence of a sophisticated cumbersome teacher model and achieves comparable performance to existing state-of-the-art teacher-free knowledge distillation frameworks when implemented by a low-cost dynamic mannered teacher. Our approach is all-encompassing and can be utilized for any deep neural network training that requires categorization or object recognition. DR-KD enhances the test accuracy on Tiny ImageNet by 2.65% over prominent baseline models, which is significantly better than any other knowledge distillation approach while requiring no additional training costs.



### Few-shot Transfer Learning for Holographic Image Reconstruction using a Recurrent Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2201.11333v1
- **DOI**: 10.1063/5.0090582
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.11333v1)
- **Published**: 2022-01-27 05:51:36+00:00
- **Updated**: 2022-01-27 05:51:36+00:00
- **Authors**: Luzhe Huang, Xilin Yang, Tairan Liu, Aydogan Ozcan
- **Comment**: 10 Pages, 3 Figures
- **Journal**: APL Photonics (2022)
- **Summary**: Deep learning-based methods in computational microscopy have been shown to be powerful but in general face some challenges due to limited generalization to new types of samples and requirements for large and diverse training data. Here, we demonstrate a few-shot transfer learning method that helps a holographic image reconstruction deep neural network rapidly generalize to new types of samples using small datasets. We pre-trained a convolutional recurrent neural network on a large dataset with diverse types of samples, which serves as the backbone model. By fixing the recurrent blocks and transferring the rest of the convolutional blocks of the pre-trained model, we reduced the number of trainable parameters by ~90% compared with standard transfer learning, while achieving equivalent generalization. We validated the effectiveness of this approach by successfully generalizing to new types of samples using small holographic datasets for training, and achieved (i) ~2.5-fold convergence speed acceleration, (ii) ~20% computation time reduction per epoch, and (iii) improved reconstruction performance over baseline network models trained from scratch. This few-shot transfer learning approach can potentially be applied in other microscopic imaging methods, helping to generalize to new types of samples without the need for extensive training time and data.



### Exploring Global Diversity and Local Context for Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/2201.11345v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.11345v2)
- **Published**: 2022-01-27 06:56:01+00:00
- **Updated**: 2022-03-27 15:50:58+00:00
- **Authors**: Yingchao Pan, Ouhan Huang, Qinghao Ye, Zhongjin Li, Wenjiang Wang, Guodun Li, Yuxing Chen
- **Comment**: Accepted by IEEE Access
- **Journal**: None
- **Summary**: Video summarization aims to automatically generate a diverse and concise summary which is useful in large-scale video processing. Most of the methods tend to adopt self-attention mechanism across video frames, which fails to model the diversity of video frames. To alleviate this problem, we revisit the pairwise similarity measurement in self-attention mechanism and find that the existing inner-product affinity leads to discriminative features rather than diversified features. In light of this phenomenon, we propose global diverse attention which uses the squared Euclidean distance instead to compute the affinities. Moreover, we model the local contextual information by novel local contextual attention to remove the redundancy in the video. By combining these two attention mechanisms, a video SUMmarization model with Diversified Contextual Attention scheme is developed, namely SUM-DCA. Extensive experiments are conducted on benchmark data sets to verify the effectiveness and the superiority of SUM-DCA in terms of F-score and rank-based evaluation without any bells and whistles.



### Effective Shortcut Technique for GAN
- **Arxiv ID**: http://arxiv.org/abs/2201.11351v1
- **DOI**: 10.1007/s10489-022-03666-2
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.11351v1)
- **Published**: 2022-01-27 07:14:45+00:00
- **Updated**: 2022-01-27 07:14:45+00:00
- **Authors**: Seung Park, Cheol-Hwan Yoo, Yong-Goo Shin
- **Comment**: arXiv admin note: text overlap with arXiv:2112.14968
- **Journal**: Applied Intelligence 2022
- **Summary**: In recent years, generative adversarial network (GAN)-based image generation techniques design their generators by stacking up multiple residual blocks. The residual block generally contains a shortcut, \ie skip connection, which effectively supports information propagation in the network. In this paper, we propose a novel shortcut method, called the gated shortcut, which not only embraces the strength point of the residual block but also further boosts the GAN performance. More specifically, based on the gating mechanism, the proposed method leads the residual block to keep (or remove) information that is relevant (or irrelevant) to the image being generated. To demonstrate that the proposed method brings significant improvements in the GAN performance, this paper provides extensive experimental results on the various standard datasets such as CIFAR-10, CIFAR-100, LSUN, and tiny-ImageNet. Quantitative evaluations show that the gated shortcut achieves the impressive GAN performance in terms of Frechet inception distance (FID) and Inception score (IS). For instance, the proposed method improves the FID and IS scores on the tiny-ImageNet dataset from 35.13 to 27.90 and 20.23 to 23.42, respectively.



### DiriNet: A network to estimate the spatial and spectral degradation functions
- **Arxiv ID**: http://arxiv.org/abs/2201.12346v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12346v1)
- **Published**: 2022-01-27 07:24:52+00:00
- **Updated**: 2022-01-27 07:24:52+00:00
- **Authors**: Ting Hu
- **Comment**: None
- **Journal**: None
- **Summary**: The spatial and spectral degradation functions are critical to hyper- and multi-spectral image fusion. However, few work has been payed on the estimation of the degradation functions. To learn the spatial response function and the point spread function from the image pairs to be fused, we propose a Dirichlet network, where both functions are properly constrained. Specifically, the spatial response function is constrained with positivity, while the Dirichlet distribution along with a total variation is imposed on the point spread function. To the best of our knowledge, the neural netwrok and the Dirichlet regularization are exclusively investigated, for the first time, to estimate the degradation functions. Both image degradation and fusion experiments demonstrate the effectiveness and superiority of the proposed Dirichlet network.



### Deep Confidence Guided Distance for 3D Partial Shape Registration
- **Arxiv ID**: http://arxiv.org/abs/2201.11379v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11379v1)
- **Published**: 2022-01-27 08:40:05+00:00
- **Updated**: 2022-01-27 08:40:05+00:00
- **Authors**: Dvir Ginzburg, Dan Raviv
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel non-iterative learnable method for partial-to-partial 3D shape registration. The partial alignment task is extremely complex, as it jointly tries to match between points and identify which points do not appear in the corresponding shape, causing the solution to be non-unique and ill-posed in most cases.   Until now, two principal methodologies have been suggested to solve this problem: sample a subset of points that are likely to have correspondences or perform soft alignment between the point clouds and try to avoid a match to an occluded part. These heuristics work when the partiality is mild or when the transformation is small but fails for severe occlusions or when outliers are present. We present a unique approach named Confidence Guided Distance Network (CGD-net), where we fuse learnable similarity between point embeddings and spatial distance between point clouds, inducing an optimized solution for the overlapping points while ignoring parts that only appear in one of the shapes. The point feature generation is done by a self-supervised architecture that repels far points to have different embeddings, therefore succeeds to align partial views of shapes, even with excessive internal symmetries or acute rotations. We compare our network to recently presented learning-based and axiomatic methods and report a fundamental boost in performance.



### On scale-invariant properties in natural images and their simulations
- **Arxiv ID**: http://arxiv.org/abs/2201.13312v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.13312v2)
- **Published**: 2022-01-27 08:51:52+00:00
- **Updated**: 2022-04-27 16:07:24+00:00
- **Authors**: Maxim Koroteev, Kirill Aistov
- **Comment**: 7 pages, 13 figures
- **Journal**: None
- **Summary**: We study samples of natural images for which a set of statistical characteristics is computed and scale-invariant properties of samples are demonstrated computationally. Computations of the power spectrum are carried out and a power-law decaying power spectrum is observed on samples taken from van Hateren images of natural scenes. We propose a dynamic model to reproduce the observed slope in the power spectrum qualitatively. For two types of sources for this model the behaviour of power spectrum is investigated and scale-invariance confirmed numerically. We then discuss potential applications of scale-invariant properties of natural images.



### Contrastive Embedding Distribution Refinement and Entropy-Aware Attention for 3D Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/2201.11388v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.11388v1)
- **Published**: 2022-01-27 09:10:28+00:00
- **Updated**: 2022-01-27 09:10:28+00:00
- **Authors**: Feng Yang, Yichao Cao, Qifan Xue, Shuai Jin, Xuanpeng Li, Weigong Zhang
- **Comment**: 15 pages, 10figures
- **Journal**: None
- **Summary**: Learning a powerful representation from point clouds is a fundamental and challenging problem in the field of computer vision. Different from images where RGB pixels are stored in the regular grid, for point clouds, the underlying semantic and structural information of point clouds is the spatial layout of the points. Moreover, the properties of challenging in-context and background noise pose more challenges to point cloud analysis. One assumption is that the poor performance of the classification model can be attributed to the indistinguishable embedding feature that impedes the search for the optimal classifier. This work offers a new strategy for learning powerful representations via a contrastive learning approach that can be embedded into any point cloud classification network. First, we propose a supervised contrastive classification method to implement embedding feature distribution refinement by improving the intra-class compactness and inter-class separability. Second, to solve the confusion problem caused by small inter-class compactness and inter-class separability. Second, to solve the confusion problem caused by small inter-class variations between some similar-looking categories, we propose a confusion-prone class mining strategy to alleviate the confusion effect. Finally, considering that outliers of the sample clusters in the embedding space may cause performance degradation, we design an entropy-aware attention module with information entropy theory to identify the outlier cases and the unstable samples by measuring the uncertainty of predicted probability. The results of extensive experiments demonstrate that our method outperforms the state-of-the-art approaches by achieving 82.9% accuracy on the real-world ScanObjectNN dataset and substantial performance gains up to 2.9% in DCGNN, 3.1% in PointNet++, and 2.4% in GBNet.



### Multi-Frame Quality Enhancement On Compressed Video Using Quantised Data of Deep Belief Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.11389v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.11389v1)
- **Published**: 2022-01-27 09:14:57+00:00
- **Updated**: 2022-01-27 09:14:57+00:00
- **Authors**: Dionne Takudzwa Chasi, Mkhuseli Ngxande
- **Comment**: 7 pages, 11 figures and 3 tables
- **Journal**: None
- **Summary**: In the age of streaming and surveillance compressed video enhancement has become a problem in need of constant improvement. Here, we investigate a way of improving the Multi-Frame Quality Enhancement approach. This approach consists of making use of the frames that have the peak quality in the region to improve those that have a lower quality in that region. This approach consists of obtaining quantized data from the videos using a deep belief network. The quantized data is then fed into the MF-CNN architecture to improve the compressed video. We further investigate the impact of using a Bi-LSTM for detecting the peak quality frames. Our approach obtains better results than the first approach of the MFQE which uses an SVM for PQF detection. On the other hand, our MFQE approach does not outperform the latest version of the MQFE approach that uses a Bi-LSTM for PQF detection.



### Generalised Image Outpainting with U-Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.11403v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11403v5)
- **Published**: 2022-01-27 09:41:58+00:00
- **Updated**: 2022-09-14 09:40:47+00:00
- **Authors**: Penglei Gao, Xi Yang, Rui Zhang, John Y. Goulermas, Yujie Geng, Yuyao Yan, Kaizhu Huang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we develop a novel transformer-based generative adversarial neural network called U-Transformer for generalised image outpainting problem. Different from most present image outpainting methods conducting horizontal extrapolation, our generalised image outpainting could extrapolate visual context all-side around a given image with plausible structure and details even for complicated scenery, building, and art images. Specifically, we design a generator as an encoder-to-decoder structure embedded with the popular Swin Transformer blocks. As such, our novel neural network can better cope with image long-range dependencies which are crucially important for generalised image outpainting. We propose additionally a U-shaped structure and multi-view Temporal Spatial Predictor (TSP) module to reinforce image self-reconstruction as well as unknown-part prediction smoothly and realistically. By adjusting the predicting step in the TSP module in the testing stage, we can generate arbitrary outpainting size given the input sub-image. We experimentally demonstrate that our proposed method could produce visually appealing results for generalized image outpainting against the state-of-the-art image outpainting approaches.



### Non-linear Motion Estimation for Video Frame Interpolation using Space-time Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2201.11407v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.11407v2)
- **Published**: 2022-01-27 09:49:23+00:00
- **Updated**: 2022-04-13 02:37:58+00:00
- **Authors**: Saikat Dutta, Arulkumar Subramaniam, Anurag Mittal
- **Comment**: Accepted at CLIC workshop, CVPR 2022. Code:
  https://github.com/saikatdutta/NME-VFI
- **Journal**: None
- **Summary**: Video frame interpolation aims to synthesize one or multiple frames between two consecutive frames in a video. It has a wide range of applications including slow-motion video generation, frame-rate up-scaling and developing video codecs. Some older works tackled this problem by assuming per-pixel linear motion between video frames. However, objects often follow a non-linear motion pattern in the real domain and some recent methods attempt to model per-pixel motion by non-linear models (e.g., quadratic). A quadratic model can also be inaccurate, especially in the case of motion discontinuities over time (i.e. sudden jerks) and occlusions, where some of the flow information may be invalid or inaccurate.   In our paper, we propose to approximate the per-pixel motion using a space-time convolution network that is able to adaptively select the motion model to be used. Specifically, we are able to softly switch between a linear and a quadratic model. Towards this end, we use an end-to-end 3D CNN encoder-decoder architecture over bidirectional optical flows and occlusion maps to estimate the non-linear motion model of each pixel. Further, a motion refinement module is employed to refine the non-linear motion and the interpolated frames are estimated by a simple warping of the neighboring frames with the estimated per-pixel motion. Through a set of comprehensive experiments, we validate the effectiveness of our model and show that our method outperforms state-of-the-art algorithms on four datasets (Vimeo, DAVIS, HD and GoPro).



### DocSegTr: An Instance-Level End-to-End Document Image Segmentation Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.11438v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11438v2)
- **Published**: 2022-01-27 10:50:22+00:00
- **Updated**: 2022-09-21 15:58:41+00:00
- **Authors**: Sanket Biswas, Ayan Banerjee, Josep Lladós, Umapada Pal
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Understanding documents with rich layouts is an essential step towards information extraction. Business intelligence processes often require the extraction of useful semantic content from documents at a large scale for subsequent decision-making tasks. In this context, instance-level segmentation of different document objects (title, sections, figures etc.) has emerged as an interesting problem for the document analysis and understanding community. To advance the research in this direction, we present a transformer-based model called \emph{DocSegTr} for end-to-end instance segmentation of complex layouts in document images. The method adapts a twin attention module, for semantic reasoning, which helps to become highly computationally efficient compared with the state-of-the-art. To the best of our knowledge, this is the first work on transformer-based document segmentation. Extensive experimentation on competitive benchmarks like PubLayNet, PRIMA, Historical Japanese (HJ) and TableBank demonstrate that our model achieved comparable or better segmentation performance than the existing state-of-the-art approaches with the average precision of 89.4, 40.3, 83.4 and 93.3. This simple and flexible framework could serve as a promising baseline for instance-level recognition tasks in document images.



### An Analysis on Ensemble Learning optimized Medical Image Classification with Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.11440v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.11440v2)
- **Published**: 2022-01-27 10:56:11+00:00
- **Updated**: 2022-04-13 14:19:52+00:00
- **Authors**: Dominik Müller, Iñaki Soto-Rey, Frank Kramer
- **Comment**: Code: https://github.com/frankkramer-lab/ensmic ; Supplementary
  Material: https://doi.org/10.5281/zenodo.6457912
- **Journal**: None
- **Summary**: Novel and high-performance medical image classification pipelines are heavily utilizing ensemble learning strategies. The idea of ensemble learning is to assemble diverse models or multiple predictions and, thus, boost prediction performance. However, it is still an open question to what extent as well as which ensemble learning strategies are beneficial in deep learning based medical image classification pipelines. In this work, we proposed a reproducible medical image classification pipeline for analyzing the performance impact of the following ensemble learning techniques: Augmenting, Stacking, and Bagging. The pipeline consists of state-of-the-art preprocessing and image augmentation methods as well as 9 deep convolution neural network architectures. It was applied on four popular medical imaging datasets with varying complexity. Furthermore, 12 pooling functions for combining multiple predictions were analyzed, ranging from simple statistical functions like unweighted averaging up to more complex learning-based functions like support vector machines. Our results revealed that Stacking achieved the largest performance gain of up to 13% F1-score increase. Augmenting showed consistent improvement capabilities by up to 4% and is also applicable to single model based pipelines. Cross-validation based Bagging demonstrated significant performance gain close to Stacking, which resulted in an F1-score increase up to +11%. Furthermore, we demonstrated that simple statistical pooling functions are equal or often even better than more complex pooling functions. We concluded that the integration of ensemble learning techniques is a powerful method for any medical image classification pipeline to improve robustness and boost performance.



### Pan-tumor CAnine cuTaneous Cancer Histology (CATCH) dataset
- **Arxiv ID**: http://arxiv.org/abs/2201.11446v2
- **DOI**: 10.1038/s41597-022-01692-w
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11446v2)
- **Published**: 2022-01-27 11:16:26+00:00
- **Updated**: 2022-08-26 09:05:19+00:00
- **Authors**: Frauke Wilm, Marco Fragoso, Christian Marzahl, Jingna Qiu, Chloé Puget, Laura Diehl, Christof A. Bertram, Robert Klopfleisch, Andreas Maier, Katharina Breininger, Marc Aubreville
- **Comment**: Submitted to Scientific Data. 15 pages, 9 figures, 6 tables
- **Journal**: Scientific Data vol. 9 (2022)
- **Summary**: Due to morphological similarities, the differentiation of histologic sections of cutaneous tumors into individual subtypes can be challenging. Recently, deep learning-based approaches have proven their potential for supporting pathologists in this regard. However, many of these supervised algorithms require a large amount of annotated data for robust development. We present a publicly available dataset of 350 whole slide images of seven different canine cutaneous tumors complemented by 12,424 polygon annotations for 13 histologic classes, including seven cutaneous tumor subtypes. In inter-rater experiments, we show a high consistency of the provided labels, especially for tumor annotations. We further validate the dataset by training a deep neural network for the task of tissue segmentation and tumor subtype classification. We achieve a class-averaged Jaccard coefficient of 0.7047, and 0.9044 for tumor in particular. For classification, we achieve a slide-level accuracy of 0.9857. Since canine cutaneous tumors possess various histologic homologies to human tumors the added value of this dataset is not limited to veterinary pathology but extends to more general fields of application.



### In Defense of Kalman Filtering for Polyp Tracking from Colonoscopy Videos
- **Arxiv ID**: http://arxiv.org/abs/2201.11450v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11450v1)
- **Published**: 2022-01-27 11:25:58+00:00
- **Updated**: 2022-01-27 11:25:58+00:00
- **Authors**: David Butler, Yuan Zhang, Tim Chen, Seon Ho Shin, Rajvinder Singh, Gustavo Carneiro
- **Comment**: Paper accepted to the International Symposium on Biomedical Imaging
  (ISBI) 2022
- **Journal**: None
- **Summary**: Real-time and robust automatic detection of polyps from colonoscopy videos are essential tasks to help improve the performance of doctors during this exam. The current focus of the field is on the development of accurate but inefficient detectors that will not enable a real-time application. We advocate that the field should instead focus on the development of simple and efficient detectors that an be combined with effective trackers to allow the implementation of real-time polyp detectors. In this paper, we propose a Kalman filtering tracker that can work together with powerful, but efficient detectors, enabling the implementation of real-time polyp detectors. In particular, we show that the combination of our Kalman filtering with the detector PP-YOLO shows state-of-the-art (SOTA) detection accuracy and real-time processing. More specifically, our approach has SOTA results on the CVC-ClinicDB dataset, with a recall of 0.740, precision of 0.869, $F_1$ score of 0.799, an average precision (AP) of 0.837, and can run in real time (i.e., 30 frames per second). We also evaluate our method on a subset of the Hyper-Kvasir annotated by our clinical collaborators, resulting in SOTA results, with a recall of 0.956, precision of 0.875, $F_1$ score of 0.914, AP of 0.952, and can run in real time.



### RelTR: Relation Transformer for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2201.11460v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11460v3)
- **Published**: 2022-01-27 11:53:41+00:00
- **Updated**: 2023-04-14 21:44:13+00:00
- **Authors**: Yuren Cong, Michael Ying Yang, Bodo Rosenhahn
- **Comment**: accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: Different objects in the same scene are more or less related to each other, but only a limited number of these relationships are noteworthy. Inspired by DETR, which excels in object detection, we view scene graph generation as a set prediction problem and propose an end-to-end scene graph generation model RelTR which has an encoder-decoder architecture. The encoder reasons about the visual feature context while the decoder infers a fixed-size set of triplets subject-predicate-object using different types of attention mechanisms with coupled subject and object queries. We design a set prediction loss performing the matching between the ground truth and predicted triplets for the end-to-end training. In contrast to most existing scene graph generation methods, RelTR is a one-stage method that predicts a set of relationships directly only using visual appearance without combining entities and labeling all possible predicates. Extensive experiments on the Visual Genome and Open Images V6 datasets demonstrate the superior performance and fast inference of our model.



### Eye-focused Detection of Bell's Palsy in Videos
- **Arxiv ID**: http://arxiv.org/abs/2201.11479v1
- **DOI**: 10.21428/594757db.d2f8342b
- **Categories**: **cs.CV**, cs.AI, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11479v1)
- **Published**: 2022-01-27 12:34:35+00:00
- **Updated**: 2022-01-27 12:34:35+00:00
- **Authors**: Sharik Ali Ansari, Koteswar Rao Jerripothula, Pragya Nagpal, Ankush Mittal
- **Comment**: Published in the Proceedings of the 34th Canadian Conference on
  Artificial Intelligence. Please cite this paper in the following manner: S.
  A. Ansari, K. R. Jerripothula, P. Nagpal, and A. Mittal. "Eye-focused
  Detection of Bell's Palsy in Videos". In: Proceedings of the 34th Canadian
  Conference on Artificial Intelligence (June 8, 2021). doi:
  10.21428/594757db.d2f8342b
- **Journal**: None
- **Summary**: In this paper, we present how Bell's Palsy, a neurological disorder, can be detected just from a subject's eyes in a video. We notice that Bell's Palsy patients often struggle to blink their eyes on the affected side. As a result, we can observe a clear contrast between the blinking patterns of the two eyes. Although previous works did utilize images/videos to detect this disorder, none have explicitly focused on the eyes. Most of them require the entire face. One obvious advantage of having an eye-focused detection system is that subjects' anonymity is not at risk. Also, our AI decisions based on simple blinking patterns make them explainable and straightforward. Specifically, we develop a novel feature called blink similarity, which measures the similarity between the two blinking patterns. Our extensive experiments demonstrate that the proposed feature is quite robust, for it helps in Bell's Palsy detection even with very few labels. Our proposed eye-focused detection system is not only cheaper but also more convenient than several existing methods.



### Head and eye egocentric gesture recognition for human-robot interaction using eyewear cameras
- **Arxiv ID**: http://arxiv.org/abs/2201.11500v2
- **DOI**: 10.1109/LRA.2022.3180442
- **Categories**: **cs.CV**, cs.HC, cs.RO, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2201.11500v2)
- **Published**: 2022-01-27 13:26:05+00:00
- **Updated**: 2022-06-10 17:29:26+00:00
- **Authors**: Javier Marina-Miranda, V. Javier Traver
- **Comment**: Copyright 2022 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: IEEE Robotics and Automation Letters, 2022
- **Summary**: Non-verbal communication plays a particularly important role in a wide range of scenarios in Human-Robot Interaction (HRI). Accordingly, this work addresses the problem of human gesture recognition. In particular, we focus on head and eye gestures, and adopt an egocentric (first-person) perspective using eyewear cameras. We argue that this egocentric view may offer a number of conceptual and technical benefits over scene- or robot-centric perspectives. A motion-based recognition approach is proposed, which operates at two temporal granularities. Locally, frame-to-frame homographies are estimated with a convolutional neural network (CNN). The output of this CNN is input to a long short-term memory (LSTM) to capture longer-term temporal visual relationships, which are relevant to characterize gestures. Regarding the configuration of the network architecture, one particularly interesting finding is that using the output of an internal layer of the homography CNN increases the recognition rate with respect to using the homography matrix itself. While this work focuses on action recognition, and no robot or user study has been conducted yet, the system has been designed to meet real-time constraints. The encouraging results suggest that the proposed egocentric perspective is viable, and this proof-of-concept work provides novel and useful contributions to the exciting area of HRI.



### Anomaly Detection in Retinal Images using Multi-Scale Deep Feature Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/2201.11506v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11506v1)
- **Published**: 2022-01-27 13:36:22+00:00
- **Updated**: 2022-01-27 13:36:22+00:00
- **Authors**: Sourya Dipta Das, Saikat Dutta, Nisarg A. Shah, Dwarikanath Mahapatra, Zongyuan Ge
- **Comment**: Accepted to ISBI 2022.\copyright IEEE
- **Journal**: None
- **Summary**: Convolutional Neural Network models have successfully detected retinal illness from optical coherence tomography (OCT) and fundus images. These CNN models frequently rely on vast amounts of labeled data for training, difficult to obtain, especially for rare diseases. Furthermore, a deep learning system trained on a data set with only one or a few diseases cannot detect other diseases, limiting the system's practical use in disease identification. We have introduced an unsupervised approach for detecting anomalies in retinal images to overcome this issue. We have proposed a simple, memory efficient, easy to train method which followed a multi-step training technique that incorporated autoencoder training and Multi-Scale Deep Feature Sparse Coding (MDFSC), an extended version of normal sparse coding, to accommodate diverse types of retinal datasets. We achieve relative AUC score improvement of 7.8\%, 6.7\% and 12.1\% over state-of-the-art SPADE on Eye-Q, IDRiD and OCTID datasets respectively.



### Density-Aware Hyper-Graph Neural Networks for Graph-based Semi-supervised Node Classification
- **Arxiv ID**: http://arxiv.org/abs/2201.11511v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11511v1)
- **Published**: 2022-01-27 13:43:14+00:00
- **Updated**: 2022-01-27 13:43:14+00:00
- **Authors**: Jianpeng Liao, Qian Tao, Jun Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Graph-based semi-supervised learning, which can exploit the connectivity relationship between labeled and unlabeled data, has been shown to outperform the state-of-the-art in many artificial intelligence applications. One of the most challenging problems for graph-based semi-supervised node classification is how to use the implicit information among various data to improve the performance of classifying. Traditional studies on graph-based semi-supervised learning have focused on the pairwise connections among data. However, the data correlation in real applications could be beyond pairwise and more complicated. The density information has been demonstrated to be an important clue, but it is rarely explored in depth among existing graph-based semi-supervised node classification methods. To develop a flexible and effective model for graph-based semi-supervised node classification, we propose a novel Density-Aware Hyper-Graph Neural Networks (DA-HGNN). In our proposed approach, hyper-graph is provided to explore the high-order semantic correlation among data, and a density-aware hyper-graph attention network is presented to explore the high-order connection relationship. Extensive experiments are conducted in various benchmark datasets, and the results demonstrate the effectiveness of the proposed approach.



### ResiDualGAN: Resize-Residual DualGAN for Cross-Domain Remote Sensing Images Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.11523v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11523v2)
- **Published**: 2022-01-27 13:56:54+00:00
- **Updated**: 2022-10-28 09:04:11+00:00
- **Authors**: Yang Zhao, Peng Guo, Zihao Sun, Xiuwan Chen, Han Gao
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of a semantic segmentation model for remote sensing (RS) images pretrained on an annotated dataset would greatly decrease when testing on another unannotated dataset because of the domain gap. Adversarial generative methods, e.g., DualGAN, are utilized for unpaired image-to-image translation to minimize the pixel-level domain gap, which is one of the common approaches for unsupervised domain adaptation (UDA). However, the existing image translation methods are facing two problems when performing RS images translation: 1) ignoring the scale discrepancy between two RS datasets which greatly affects the accuracy performance of scale-invariant objects, 2) ignoring the characteristic of real-to-real translation of RS images which brings an unstable factor for the training of the models. In this paper, ResiDualGAN is proposed for RS images translation, where an in-network resizer module is used for addressing the scale discrepancy of RS datasets, and a residual connection is used for strengthening the stability of real-to-real images translation and improving the performance in cross-domain semantic segmentation tasks. Combined with an output space adaptation method, the proposed method greatly improves the accuracy performance on common benchmarks, which demonstrates the superiority and reliability of ResiDuanGAN. At the end of the paper, a thorough discussion is also conducted to give a reasonable explanation for the improvement of ResiDualGAN. Our source code is available at https://github.com/miemieyanga/ResiDualGAN-DRDG.



### Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains
- **Arxiv ID**: http://arxiv.org/abs/2201.11528v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11528v4)
- **Published**: 2022-01-27 14:04:27+00:00
- **Updated**: 2022-03-14 11:42:30+00:00
- **Authors**: Qilong Zhang, Xiaodan Li, Yuefeng Chen, Jingkuan Song, Lianli Gao, Yuan He, Hui Xue
- **Comment**: Accepted by ICLR 2022
- **Journal**: None
- **Summary**: Adversarial examples have posed a severe threat to deep neural networks due to their transferable nature. Currently, various works have paid great efforts to enhance the cross-model transferability, which mostly assume the substitute model is trained in the same domain as the target model. However, in reality, the relevant information of the deployed model is unlikely to leak. Hence, it is vital to build a more practical black-box threat model to overcome this limitation and evaluate the vulnerability of deployed models. In this paper, with only the knowledge of the ImageNet domain, we propose a Beyond ImageNet Attack (BIA) to investigate the transferability towards black-box domains (unknown classification tasks). Specifically, we leverage a generative model to learn the adversarial function for disrupting low-level features of input images. Based on this framework, we further propose two variants to narrow the gap between the source and target domains from the data and model perspectives, respectively. Extensive experiments on coarse-grained and fine-grained domains demonstrate the effectiveness of our proposed methods. Notably, our methods outperform state-of-the-art approaches by up to 7.71\% (towards coarse-grained domains) and 25.91\% (towards fine-grained domains) on average. Our code is available at \url{https://github.com/qilong-zhang/Beyond-ImageNet-Attack}.



### ASOC: Adaptive Self-aware Object Co-localization
- **Arxiv ID**: http://arxiv.org/abs/2201.11547v1
- **DOI**: 10.1109/ICME51207.2021.9428191
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11547v1)
- **Published**: 2022-01-27 14:38:11+00:00
- **Updated**: 2022-01-27 14:38:11+00:00
- **Authors**: Koteswar Rao Jerripothula, Prerana Mukherjee
- **Comment**: Published in IEEE ICME 2021. Please cite this paper in the following
  manner: K. R. Jerripothula and P. Mukherjee, "ASOC: Adaptive Self-Aware
  Object Co-Localization," 2021 IEEE International Conference on Multimedia and
  Expo (ICME), 2021, pp. 1-6, doi: 10.1109/ICME51207.2021.9428191
- **Journal**: 2021 IEEE International Conference on Multimedia and Expo (ICME),
  2021, pp. 1-6
- **Summary**: The primary goal of this paper is to localize objects in a group of semantically similar images jointly, also known as the object co-localization problem. Most related existing works are essentially weakly-supervised, relying prominently on the neighboring images' weak-supervision. Although weak supervision is beneficial, it is not entirely reliable, for the results are quite sensitive to the neighboring images considered. In this paper, we combine it with a self-awareness phenomenon to mitigate this issue. By self-awareness here, we refer to the solution derived from the image itself in the form of saliency cue, which can also be unreliable if applied alone. Nevertheless, combining these two paradigms together can lead to a better co-localization ability. Specifically, we introduce a dynamic mediator that adaptively strikes a proper balance between the two static solutions to provide an optimal solution. Therefore, we call this method \textit{ASOC}: Adaptive Self-aware Object Co-localization. We perform exhaustive experiments on several benchmark datasets and validate that weak-supervision supplemented with self-awareness has superior performance outperforming several compared competing methods.



### A Probabilistic Framework for Dynamic Object Recognition in 3D Environment With A Novel Continuous Ground Estimation Method
- **Arxiv ID**: http://arxiv.org/abs/2201.11608v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.11608v1)
- **Published**: 2022-01-27 16:07:10+00:00
- **Updated**: 2022-01-27 16:07:10+00:00
- **Authors**: Pouria Mehrabi
- **Comment**: Master's Thesis Submitted in Partial Fulfillment of The Requirements
  For The Degree of Master of Science in Electrical Engineerin
- **Journal**: None
- **Summary**: In this thesis a probabilistic framework is developed and proposed for Dynamic Object Recognition in 3D Environments. A software package is developed using C++ and Python in ROS that performs the detection and tracking task. Furthermore, a novel Gaussian Process Regression (GPR) based method is developed to detect ground points in different urban scenarios of regular, sloped and rough. The ground surface behavior is assumed to only demonstrate local input-dependent smoothness. kernel's length-scales are obtained. Bayesian inference is implemented sing \textit{Maximum a Posteriori} criterion. The log-marginal likelihood function is assumed to be a multi-task objective function, to represent a whole-frame unbiased view of the ground at each frame because adjacent segments may not have similar ground structure in an uneven scene while having shared hyper-parameter values. Simulation results shows the effectiveness of the proposed method in uneven and rough scenes which outperforms similar Gaussian process based ground segmentation methods.



### Domain-Invariant Representation Learning from EEG with Private Encoders
- **Arxiv ID**: http://arxiv.org/abs/2201.11613v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2201.11613v2)
- **Published**: 2022-01-27 16:14:26+00:00
- **Updated**: 2022-05-30 08:08:35+00:00
- **Authors**: David Bethge, Philipp Hallgarten, Tobias Grosse-Puppendahl, Mohamed Kari, Ralf Mikut, Albrecht Schmidt, Ozan Özdenizci
- **Comment**: 5 pages, 1 figure
- **Journal**: None
- **Summary**: Deep learning based electroencephalography (EEG) signal processing methods are known to suffer from poor test-time generalization due to the changes in data distribution. This becomes a more challenging problem when privacy-preserving representation learning is of interest such as in clinical settings. To that end, we propose a multi-source learning architecture where we extract domain-invariant representations from dataset-specific private encoders. Our model utilizes a maximum-mean-discrepancy (MMD) based domain alignment approach to impose domain-invariance for encoded representations, which outperforms state-of-the-art approaches in EEG-based emotion classification. Furthermore, representations learned in our pipeline preserve domain privacy as dataset-specific private encoding alleviates the need for conventional, centralized EEG-based deep neural network training approaches with shared parameters.



### Domain generalization in deep learning-based mass detection in mammography: A large-scale multi-center study
- **Arxiv ID**: http://arxiv.org/abs/2201.11620v2
- **DOI**: 10.1016/j.artmed.2022.102386
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T07, 68U10, 65D17
- **Links**: [PDF](http://arxiv.org/pdf/2201.11620v2)
- **Published**: 2022-01-27 16:26:36+00:00
- **Updated**: 2023-01-24 16:18:30+00:00
- **Authors**: Lidia Garrucho, Kaisar Kushibar, Socayna Jouide, Oliver Diaz, Laura Igual, Karim Lekadir
- **Comment**: None
- **Journal**: None
- **Summary**: Computer-aided detection systems based on deep learning have shown great potential in breast cancer detection. However, the lack of domain generalization of artificial neural networks is an important obstacle to their deployment in changing clinical environments. In this work, we explore the domain generalization of deep learning methods for mass detection in digital mammography and analyze in-depth the sources of domain shift in a large-scale multi-center setting. To this end, we compare the performance of eight state-of-the-art detection methods, including Transformer-based models, trained in a single domain and tested in five unseen domains. Moreover, a single-source mass detection training pipeline is designed to improve the domain generalization without requiring images from the new domain. The results show that our workflow generalizes better than state-of-the-art transfer learning-based approaches in four out of five domains while reducing the domain shift caused by the different acquisition protocols and scanner manufacturers. Subsequently, an extensive analysis is performed to identify the covariate shifts with bigger effects on the detection performance, such as due to differences in patient age, breast density, mass size, and mass malignancy. Ultimately, this comprehensive study provides key insights and best practices for future research on domain generalization in deep learning-based breast cancer detection.



### Automatic Classification of Neuromuscular Diseases in Children Using Photoacoustic Imaging
- **Arxiv ID**: http://arxiv.org/abs/2201.11630v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11630v1)
- **Published**: 2022-01-27 16:37:19+00:00
- **Updated**: 2022-01-27 16:37:19+00:00
- **Authors**: Maja Schlereth, Daniel Stromer, Katharina Breininger, Alexandra Wagner, Lina Tan, Andreas Maier, Ferdinand Knieling
- **Comment**: accepted by BVM conference proceedings 2022
- **Journal**: None
- **Summary**: Neuromuscular diseases (NMDs) cause a significant burden for both healthcare systems and society. They can lead to severe progressive muscle weakness, muscle degeneration, contracture, deformity and progressive disability. The NMDs evaluated in this study often manifest in early childhood. As subtypes of disease, e.g. Duchenne Muscular Dystropy (DMD) and Spinal Muscular Atrophy (SMA), are difficult to differentiate at the beginning and worsen quickly, fast and reliable differential diagnosis is crucial. Photoacoustic and ultrasound imaging has shown great potential to visualize and quantify the extent of different diseases. The addition of automatic classification of such image data could further improve standard diagnostic procedures. We compare deep learning-based 2-class and 3-class classifiers based on VGG16 for differentiating healthy from diseased muscular tissue. This work shows promising results with high accuracies above 0.86 for the 3-class problem and can be used as a proof of concept for future approaches for earlier diagnosis and therapeutic monitoring of NMDs.



### Deep Video Prior for Video Consistency and Propagation
- **Arxiv ID**: http://arxiv.org/abs/2201.11632v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.11632v1)
- **Published**: 2022-01-27 16:38:52+00:00
- **Updated**: 2022-01-27 16:38:52+00:00
- **Authors**: Chenyang Lei, Yazhou Xing, Hao Ouyang, Qifeng Chen
- **Comment**: Accepted by TPAMI in Dec 2021; extension of NeurIPS2020 Blind Video
  Temporal Consistency via Deep Video Prior. arXiv admin note: substantial text
  overlap with arXiv:2010.11838
- **Journal**: None
- **Summary**: Applying an image processing algorithm independently to each video frame often leads to temporal inconsistency in the resulting video. To address this issue, we present a novel and general approach for blind video temporal consistency. Our method is only trained on a pair of original and processed videos directly instead of a large dataset. Unlike most previous methods that enforce temporal consistency with optical flow, we show that temporal consistency can be achieved by training a convolutional neural network on a video with Deep Video Prior (DVP). Moreover, a carefully designed iteratively reweighted training strategy is proposed to address the challenging multimodal inconsistency problem. We demonstrate the effectiveness of our approach on 7 computer vision tasks on videos. Extensive quantitative and perceptual experiments show that our approach obtains superior performance than state-of-the-art methods on blind video temporal consistency. We further extend DVP to video propagation and demonstrate its effectiveness in propagating three different types of information (color, artistic style, and object segmentation). A progressive propagation strategy with pseudo labels is also proposed to enhance DVP's performance on video propagation. Our source codes are publicly available at https://github.com/ChenyangLEI/deep-video-prior.



### Vision Checklist: Towards Testable Error Analysis of Image Models to Help System Designers Interrogate Model Capabilities
- **Arxiv ID**: http://arxiv.org/abs/2201.11674v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 62R07, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2201.11674v3)
- **Published**: 2022-01-27 17:20:16+00:00
- **Updated**: 2022-01-31 11:09:19+00:00
- **Authors**: Xin Du, Benedicte Legastelois, Bhargavi Ganesh, Ajitha Rajan, Hana Chockler, Vaishak Belle, Stuart Anderson, Subramanian Ramamoorthy
- **Comment**: 17 pages, 18 figures
- **Journal**: None
- **Summary**: Using large pre-trained models for image recognition tasks is becoming increasingly common owing to the well acknowledged success of recent models like vision transformers and other CNN-based models like VGG and Resnet. The high accuracy of these models on benchmark tasks has translated into their practical use across many domains including safety-critical applications like autonomous driving and medical diagnostics. Despite their widespread use, image models have been shown to be fragile to changes in the operating environment, bringing their robustness into question. There is an urgent need for methods that systematically characterise and quantify the capabilities of these models to help designers understand and provide guarantees about their safety and robustness. In this paper, we propose Vision Checklist, a framework aimed at interrogating the capabilities of a model in order to produce a report that can be used by a system designer for robustness evaluations. This framework proposes a set of perturbation operations that can be applied on the underlying data to generate test samples of different types. The perturbations reflect potential changes in operating environments, and interrogate various properties ranging from the strictly quantitative to more qualitative. Our framework is evaluated on multiple datasets like Tinyimagenet, CIFAR10, CIFAR100 and Camelyon17 and for models like ViT and Resnet. Our Vision Checklist proposes a specific set of evaluations that can be integrated into the previously proposed concept of a model card. Robustness evaluations like our checklist will be crucial in future safety evaluations of visual perception modules, and be useful for a wide range of stakeholders including designers, deployers, and regulators involved in the certification of these systems. Source code of Vision Checklist would be open for public use.



### Unsupervised Change Detection using DRE-CUSUM
- **Arxiv ID**: http://arxiv.org/abs/2201.11678v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2201.11678v1)
- **Published**: 2022-01-27 17:25:42+00:00
- **Updated**: 2022-01-27 17:25:42+00:00
- **Authors**: Sudarshan Adiga, Ravi Tandon
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents DRE-CUSUM, an unsupervised density-ratio estimation (DRE) based approach to determine statistical changes in time-series data when no knowledge of the pre-and post-change distributions are available. The core idea behind the proposed approach is to split the time-series at an arbitrary point and estimate the ratio of densities of distribution (using a parametric model such as a neural network) before and after the split point. The DRE-CUSUM change detection statistic is then derived from the cumulative sum (CUSUM) of the logarithm of the estimated density ratio. We present a theoretical justification as well as accuracy guarantees which show that the proposed statistic can reliably detect statistical changes, irrespective of the split point. While there have been prior works on using density ratio based methods for change detection, to the best of our knowledge, this is the first unsupervised change detection approach with a theoretical justification and accuracy guarantees. The simplicity of the proposed framework makes it readily applicable in various practical settings (including high-dimensional time-series data); we also discuss generalizations for online change detection. We experimentally show the superiority of DRE-CUSUM using both synthetic and real-world datasets over existing state-of-the-art unsupervised algorithms (such as Bayesian online change detection, its variants as well as several other heuristic methods).



### DropNAS: Grouped Operation Dropout for Differentiable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2201.11679v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11679v1)
- **Published**: 2022-01-27 17:28:23+00:00
- **Updated**: 2022-01-27 17:28:23+00:00
- **Authors**: Weijun Hong, Guilin Li, Weinan Zhang, Ruiming Tang, Yunhe Wang, Zhenguo Li, Yong Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Neural architecture search (NAS) has shown encouraging results in automating the architecture design. Recently, DARTS relaxes the search process with a differentiable formulation that leverages weight-sharing and SGD where all candidate operations are trained simultaneously. Our empirical results show that such procedure results in the co-adaption problem and Matthew Effect: operations with fewer parameters would be trained maturely earlier. This causes two problems: firstly, the operations with more parameters may never have the chance to express the desired function since those with less have already done the job; secondly, the system will punish those underperforming operations by lowering their architecture parameter, and they will get smaller loss gradients, which causes the Matthew Effect. In this paper, we systematically study these problems and propose a novel grouped operation dropout algorithm named DropNAS to fix the problems with DARTS. Extensive experiments demonstrate that DropNAS solves the above issues and achieves promising performance. Specifically, DropNAS achieves 2.26% test error on CIFAR-10, 16.39% on CIFAR-100 and 23.4% on ImageNet (with the same training hyperparameters as DARTS for a fair comparison). It is also observed that DropNAS is robust across variants of the DARTS search space. Code is available at https://github.com/wiljohnhong/DropNAS.



### Constrained Structure Learning for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2201.11697v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.11697v1)
- **Published**: 2022-01-27 17:47:37+00:00
- **Updated**: 2022-01-27 17:47:37+00:00
- **Authors**: Daqi Liu, Miroslaw Bober, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: As a structured prediction task, scene graph generation aims to build a visually-grounded scene graph to explicitly model objects and their relationships in an input image. Currently, the mean field variational Bayesian framework is the de facto methodology used by the existing methods, in which the unconstrained inference step is often implemented by a message passing neural network. However, such formulation fails to explore other inference strategies, and largely ignores the more general constrained optimization models. In this paper, we present a constrained structure learning method, for which an explicit constrained variational inference objective is proposed. Instead of applying the ubiquitous message-passing strategy, a generic constrained optimization method - entropic mirror descent - is utilized to solve the constrained variational inference step. We validate the proposed generic model on various popular scene graph generation benchmarks and show that it outperforms the state-of-the-art methods.



### Matched Illumination
- **Arxiv ID**: http://arxiv.org/abs/2201.11700v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11700v1)
- **Published**: 2022-01-27 17:53:58+00:00
- **Updated**: 2022-01-27 17:53:58+00:00
- **Authors**: Yuteng Zhu, Graham D. Finlayson
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: In previous work, it was shown that a camera can theoretically be made more colorimetric - its RGBs become more linearly related to XYZ tristimuli - by placing a specially designed color filter in the optical path. While the prior art demonstrated the principle, the optimal color-correction filters were not actually manufactured. In this paper, we provide a novel way of creating the color filtering effect without making a physical filter: we modulate the spectrum of the light source by using a spectrally tunable lighting system to recast the prefiltering effect from a lighting perspective. According to our method, if we wish to measure color under a D65 light, we relight the scene with a modulated D65 spectrum where the light modulation mimics the effect of color prefiltering in the prior art. We call our optimally modulated light, the matched illumination. In the experiments, using synthetic and real measurements, we show that color measurement errors can be reduced by about 50% or more on simulated data and 25% or more on real images when the matched illumination is used.



### A Systematic Study of Bias Amplification
- **Arxiv ID**: http://arxiv.org/abs/2201.11706v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11706v2)
- **Published**: 2022-01-27 18:04:24+00:00
- **Updated**: 2022-10-19 14:54:49+00:00
- **Authors**: Melissa Hall, Laurens van der Maaten, Laura Gustafson, Maxwell Jones, Aaron Adcock
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research suggests that predictions made by machine-learning models can amplify biases present in the training data. When a model amplifies bias, it makes certain predictions at a higher rate for some groups than expected based on training-data statistics. Mitigating such bias amplification requires a deep understanding of the mechanics in modern machine learning that give rise to that amplification. We perform the first systematic, controlled study into when and how bias amplification occurs. To enable this study, we design a simple image-classification problem in which we can tightly control (synthetic) biases. Our study of this problem reveals that the strength of bias amplification is correlated to measures such as model accuracy, model capacity, model overconfidence, and amount of training data. We also find that bias amplification can vary greatly during training. Finally, we find that bias amplification may depend on the difficulty of the classification task relative to the difficulty of recognizing group membership: bias amplification appears to occur primarily when it is easier to recognize group membership than class membership. Our results suggest best practices for training machine-learning models that we hope will help pave the way for the development of better mitigation strategies. Code can be found at https://github.com/facebookresearch/cv_bias_amplification.



### IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages
- **Arxiv ID**: http://arxiv.org/abs/2201.11732v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11732v2)
- **Published**: 2022-01-27 18:53:22+00:00
- **Updated**: 2022-07-17 13:01:43+00:00
- **Authors**: Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott, Edoardo Maria Ponti, Ivan Vulić
- **Comment**: ICML 2022
- **Journal**: None
- **Summary**: Reliable evaluation benchmarks designed for replicability and comprehensiveness have driven progress in machine learning. Due to the lack of a multilingual benchmark, however, vision-and-language research has mostly focused on English language tasks. To fill this gap, we introduce the Image-Grounded Language Understanding Evaluation benchmark. IGLUE brings together - by both aggregating pre-existing datasets and creating new ones - visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages. Our benchmark enables the evaluation of multilingual multimodal models for transfer learning, not only in a zero-shot setting, but also in newly defined few-shot learning setups. Based on the evaluation of the available state-of-the-art models, we find that translate-test transfer is superior to zero-shot transfer and that few-shot learning is hard to harness for many tasks. Moreover, downstream performance is partially explained by the amount of available unlabelled textual data for pretraining, and only weakly by the typological distance of target-source languages. We hope to encourage future research efforts in this area by releasing the benchmark to the community.



### Ranking Info Noise Contrastive Estimation: Boosting Contrastive Learning via Ranked Positives
- **Arxiv ID**: http://arxiv.org/abs/2201.11736v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11736v1)
- **Published**: 2022-01-27 18:55:32+00:00
- **Updated**: 2022-01-27 18:55:32+00:00
- **Authors**: David T. Hoffmann, Nadine Behrmann, Juergen Gall, Thomas Brox, Mehdi Noroozi
- **Comment**: AAAI 2022 (Main Track)
- **Journal**: None
- **Summary**: This paper introduces Ranking Info Noise Contrastive Estimation (RINCE), a new member in the family of InfoNCE losses that preserves a ranked ordering of positive samples. In contrast to the standard InfoNCE loss, which requires a strict binary separation of the training pairs into similar and dissimilar samples, RINCE can exploit information about a similarity ranking for learning a corresponding embedding space. We show that the proposed loss function learns favorable embeddings compared to the standard InfoNCE whenever at least noisy ranking information can be obtained or when the definition of positives and negatives is blurry. We demonstrate this for a supervised classification task with additional superclass labels and noisy similarity scores. Furthermore, we show that RINCE can also be applied to unsupervised training with experiments on unsupervised representation learning from videos. In particular, the embedding yields higher classification accuracy, retrieval rates and performs better in out-of-distribution detection than the standard InfoNCE loss.



### PRNU Based Source Camera Identification for Webcam and Smartphone Videos
- **Arxiv ID**: http://arxiv.org/abs/2201.11737v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2201.11737v1)
- **Published**: 2022-01-27 18:57:14+00:00
- **Updated**: 2022-01-27 18:57:14+00:00
- **Authors**: Fernando Martín-Rodríguez, Fernando Isasi-de-Vicente
- **Comment**: 4 pages, 5 figures, 4 tables. arXiv admin note: substantial text
  overlap with arXiv:2107.01885
- **Journal**: None
- **Summary**: This communication is about an application of image forensics where we use camera sensor fingerprints to identify source camera (SCI: Source Camera Identification) in webcam/smartphone videos. Sensor or camera fingerprints are based on computing the intrinsic noise that is always present in this kind of sensors due to manufacturing imperfections. This is an unavoidable characteristic that links each sensor with its noise pattern. PRNU (Photo Response Non-Uniformity) has become the default technique to compute a camera fingerprint. There are many applications nowadays dealing with PRNU patterns for camera identification using still images. In this work we focus on video, first on webcam video and afterwards on smartphone video. Webcams and smartphones are the most used video cameras nowadays. Three possible methods for SCI are implemented and assessed in this work.



### Unsupervised Denoising of Retinal OCT with Diffusion Probabilistic Model
- **Arxiv ID**: http://arxiv.org/abs/2201.11760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11760v1)
- **Published**: 2022-01-27 19:02:38+00:00
- **Updated**: 2022-01-27 19:02:38+00:00
- **Authors**: Dewei Hu, Yuankai K. Tao, Ipek Oguz
- **Comment**: SPIE medical imaging, 2022
- **Journal**: None
- **Summary**: Optical coherence tomography (OCT) is a prevalent non-invasive imaging method which provides high resolution volumetric visualization of retina. However, its inherent defect, the speckle noise, can seriously deteriorate the tissue visibility in OCT. Deep learning based approaches have been widely used for image restoration, but most of these require a noise-free reference image for supervision. In this study, we present a diffusion probabilistic model that is fully unsupervised to learn from noise instead of signal. A diffusion process is defined by adding a sequence of Gaussian noise to self-fused OCT b-scans. Then the reverse process of diffusion, modeled by a Markov chain, provides an adjustable level of denoising. Our experiment results demonstrate that our method can significantly improve the image quality with a simple working pipeline and a small amount of training data.



### Network-level Safety Metrics for Overall Traffic Safety Assessment: A Case Study
- **Arxiv ID**: http://arxiv.org/abs/2201.13229v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2201.13229v2)
- **Published**: 2022-01-27 19:07:08+00:00
- **Updated**: 2022-06-13 16:59:37+00:00
- **Authors**: Xiwen Chen, Hao Wang, Abolfazl Razi, Brendan Russo, Jason Pacheco, John Roberts, Jeffrey Wishart, Larry Head, Alonso Granados Baca
- **Comment**: None
- **Journal**: None
- **Summary**: Driving safety analysis has recently experienced unprecedented improvements thanks to technological advances in precise positioning sensors, artificial intelligence (AI)-based safety features, autonomous driving systems, connected vehicles, high-throughput computing, and edge computing servers. Particularly, deep learning (DL) methods empowered volume video processing to extract safety-related features from massive videos captured by roadside units (RSU). Safety metrics are commonly used measures to investigate crashes and near-conflict events. However, these metrics provide limited insight into the overall network-level traffic management. On the other hand, some safety assessment efforts are devoted to processing crash reports and identifying spatial and temporal patterns of crashes that correlate with road geometry, traffic volume, and weather conditions. This approach relies merely on crash reports and ignores the rich information of traffic videos that can help identify the role of safety violations in crashes. To bridge these two perspectives, we define a new set of network-level safety metrics (NSM) to assess the overall safety profile of traffic flow by processing imagery taken by RSU cameras. Our analysis suggests that NSMs show significant statistical associations with crash rates. This approach is different than simply generalizing the results of individual crash analyses, since all vehicles contribute to calculating NSMs, not only the ones involved in crash incidents. This perspective considers the traffic flow as a complex dynamic system where actions of some nodes can propagate through the network and influence the crash risk for other nodes. We also provide a comprehensive review of surrogate safety metrics (SSM) in the Appendix A.



### An Empirical Analysis of Recurrent Learning Algorithms In Neural Lossy Image Compression Systems
- **Arxiv ID**: http://arxiv.org/abs/2201.11782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11782v1)
- **Published**: 2022-01-27 19:47:51+00:00
- **Updated**: 2022-01-27 19:47:51+00:00
- **Authors**: Ankur Mali, Alexander Ororbia, Daniel Kifer, Lee Giles
- **Comment**: Accepted at DCC 2021, 15 pages
- **Journal**: None
- **Summary**: Recent advances in deep learning have resulted in image compression algorithms that outperform JPEG and JPEG 2000 on the standard Kodak benchmark. However, they are slow to train (due to backprop-through-time) and, to the best of our knowledge, have not been systematically evaluated on a large variety of datasets. In this paper, we perform the first large-scale comparison of recent state-of-the-art hybrid neural compression algorithms, while exploring the effects of alternative training strategies (when applicable). The hybrid recurrent neural decoder is a former state-of-the-art model (recently overtaken by a Google model) that can be trained using backprop-through-time (BPTT) or with alternative algorithms like sparse attentive backtracking (SAB), unbiased online recurrent optimization (UORO), and real-time recurrent learning (RTRL). We compare these training alternatives along with the Google models (GOOG and E2E) on 6 benchmark datasets. Surprisingly, we found that the model trained with SAB performs better (outperforming even BPTT), resulting in faster convergence and a better peak signal-to-noise ratio.



### Denoising Diffusion Restoration Models
- **Arxiv ID**: http://arxiv.org/abs/2201.11793v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.11793v3)
- **Published**: 2022-01-27 20:19:07+00:00
- **Updated**: 2022-10-12 18:56:53+00:00
- **Authors**: Bahjat Kawar, Michael Elad, Stefano Ermon, Jiaming Song
- **Comment**: Project page: https://ddrm-ml.github.io/
- **Journal**: None
- **Summary**: Many interesting tasks in image restoration can be cast as linear inverse problems. A recent family of approaches for solving these problems uses stochastic algorithms that sample from the posterior distribution of natural images given the measurements. However, efficient solutions often require problem-specific supervised training to model the posterior, whereas unsupervised methods that are not problem-specific typically rely on inefficient iterative methods. This work addresses these issues by introducing Denoising Diffusion Restoration Models (DDRM), an efficient, unsupervised posterior sampling method. Motivated by variational inference, DDRM takes advantage of a pre-trained denoising diffusion generative model for solving any linear inverse problem. We demonstrate DDRM's versatility on several image datasets for super-resolution, deblurring, inpainting, and colorization under various amounts of measurement noise. DDRM outperforms the current leading unsupervised methods on the diverse ImageNet dataset in reconstruction quality, perceptual quality, and runtime, being 5x faster than the nearest competitor. DDRM also generalizes well for natural images out of the distribution of the observed ImageNet training set.



### A Survey on Visual Transfer Learning using Knowledge Graphs
- **Arxiv ID**: http://arxiv.org/abs/2201.11794v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.11794v1)
- **Published**: 2022-01-27 20:19:55+00:00
- **Updated**: 2022-01-27 20:19:55+00:00
- **Authors**: Sebastian Monka, Lavdim Halilaj, Achim Rettinger
- **Comment**: Semantic Web Journal (SWJ)
- **Journal**: None
- **Summary**: Recent approaches of computer vision utilize deep learning methods as they perform quite well if training and testing domains follow the same underlying data distribution. However, it has been shown that minor variations in the images that occur when using these methods in the real world can lead to unpredictable errors. Transfer learning is the area of machine learning that tries to prevent these errors. Especially, approaches that augment image data using auxiliary knowledge encoded in language embeddings or knowledge graphs (KGs) have achieved promising results in recent years. This survey focuses on visual transfer learning approaches using KGs. KGs can represent auxiliary knowledge either in an underlying graph-structured schema or in a vector-based knowledge graph embedding. Intending to enable the reader to solve visual transfer learning problems with the help of specific KG-DL configurations we start with a description of relevant modeling structures of a KG of various expressions, such as directed labeled graphs, hypergraphs, and hyper-relational graphs. We explain the notion of feature extractor, while specifically referring to visual and semantic features. We provide a broad overview of knowledge graph embedding methods and describe several joint training objectives suitable to combine them with high dimensional visual embeddings. The main section introduces four different categories on how a KG can be combined with a DL pipeline: 1) Knowledge Graph as a Reviewer; 2) Knowledge Graph as a Trainee; 3) Knowledge Graph as a Trainer; and 4) Knowledge Graph as a Peer. To help researchers find evaluation benchmarks, we provide an overview of generic KGs and a set of image processing datasets and benchmarks including various types of auxiliary knowledge. Last, we summarize related surveys and give an outlook about challenges and open issues for future research.



### Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG Encoder-Decoder
- **Arxiv ID**: http://arxiv.org/abs/2201.11795v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11795v2)
- **Published**: 2022-01-27 20:20:03+00:00
- **Updated**: 2022-01-31 05:16:43+00:00
- **Authors**: Ankur Mali, Alexander Ororbia, Daniel Kifer, Lee Giles
- **Comment**: Accepted in DCC 2022, 11 pages
- **Journal**: None
- **Summary**: Recent advances in deep learning have led to superhuman performance across a variety of applications. Recently, these methods have been successfully employed to improve the rate-distortion performance in the task of image compression. However, current methods either use additional post-processing blocks on the decoder end to improve compression or propose an end-to-end compression scheme based on heuristics. For the majority of these, the trained deep neural networks (DNNs) are not compatible with standard encoders and would be difficult to deply on personal computers and cellphones. In light of this, we propose a system that learns to improve the encoding performance by enhancing its internal neural representations on both the encoder and decoder ends, an approach we call Neural JPEG. We propose frequency domain pre-editing and post-editing methods to optimize the distribution of the DCT coefficients at both encoder and decoder ends in order to improve the standard compression (JPEG) method. Moreover, we design and integrate a scheme for jointly learning quantization tables within this hybrid neural compression framework.Experiments demonstrate that our approach successfully improves the rate-distortion performance over JPEG across various quality metrics, such as PSNR and MS-SSIM, and generates visually appealing images with better color retention quality.



### LAP: An Attention-Based Module for Faithful Interpretation and Knowledge Injection in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.11808v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T07, 68T99 (Primary) 68T45 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/2201.11808v4)
- **Published**: 2022-01-27 21:10:20+00:00
- **Updated**: 2023-02-06 10:12:28+00:00
- **Authors**: Rassa Ghavami Modegh, Ahmad Salimi, Alireza Dizaji, Hamid R. Rabiee
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the state-of-the-art performance of deep convolutional neural networks, they are susceptible to bias and malfunction in unseen situations. The complex computation behind their reasoning is not sufficiently human-understandable to develop trust. External explainer methods have tried to interpret the network decisions in a human-understandable way, but they are accused of fallacies due to their assumptions and simplifications. On the other side, the inherent self-interpretability of models, while being more robust to the mentioned fallacies, cannot be applied to the already trained models. In this work, we propose a new attention-based pooling layer, called Local Attention Pooling (LAP), that accomplishes self-interpretability and the possibility for knowledge injection while improving the model's performance. Moreover, several weakly-supervised knowledge injection methodologies are provided to enhance the process of training. We verified our claims by evaluating several LAP-extended models on three different datasets, including Imagenet. The proposed framework offers more valid human-understandable and more faithful-to-the-model interpretations than the commonly used white-box explainer methods.



### A Transfer Learning and Optimized CNN Based Intrusion Detection System for Internet of Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2201.11812v1
- **DOI**: 10.1109/ICC45855.2022.9838780
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG, cs.NI, 68T01, I.2.6; C.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2201.11812v1)
- **Published**: 2022-01-27 21:24:09+00:00
- **Updated**: 2022-01-27 21:24:09+00:00
- **Authors**: Li Yang, Abdallah Shami
- **Comment**: Accepted and to appear in IEEE International Conference on
  Communications (ICC); Code is available at Github link:
  https://github.com/Western-OC2-Lab/Intrusion-Detection-System-Using-CNN-and-Transfer-Learning
- **Journal**: None
- **Summary**: Modern vehicles, including autonomous vehicles and connected vehicles, are increasingly connected to the external world, which enables various functionalities and services. However, the improving connectivity also increases the attack surfaces of the Internet of Vehicles (IoV), causing its vulnerabilities to cyber-threats. Due to the lack of authentication and encryption procedures in vehicular networks, Intrusion Detection Systems (IDSs) are essential approaches to protect modern vehicle systems from network attacks. In this paper, a transfer learning and ensemble learning-based IDS is proposed for IoV systems using convolutional neural networks (CNNs) and hyper-parameter optimization techniques. In the experiments, the proposed IDS has demonstrated over 99.25% detection rates and F1-scores on two well-known public benchmark IoV security datasets: the Car-Hacking dataset and the CICIDS2017 dataset. This shows the effectiveness of the proposed IDS for cyber-attack detection in both intra-vehicle and external vehicular networks.



### Pressure Eye: In-bed Contact Pressure Estimation via Contact-less Imaging
- **Arxiv ID**: http://arxiv.org/abs/2201.11828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11828v1)
- **Published**: 2022-01-27 22:22:17+00:00
- **Updated**: 2022-01-27 22:22:17+00:00
- **Authors**: Shuangjun Liu, Sarah Ostadabbas
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision has achieved great success in interpreting semantic meanings from images, yet estimating underlying (non-visual) physical properties of an object is often limited to their bulk values rather than reconstructing a dense map. In this work, we present our pressure eye (PEye) approach to estimate contact pressure between a human body and the surface she is lying on with high resolution from vision signals directly. PEye approach could ultimately enable the prediction and early detection of pressure ulcers in bed-bound patients, that currently depends on the use of expensive pressure mats. Our PEye network is configured in a dual encoding shared decoding form to fuse visual cues and some relevant physical parameters in order to reconstruct high resolution pressure maps (PMs). We also present a pixel-wise resampling approach based on Naive Bayes assumption to further enhance the PM regression performance. A percentage of correct sensing (PCS) tailored for sensing estimation accuracy evaluation is also proposed which provides another perspective for performance evaluation under varying error tolerances. We tested our approach via a series of extensive experiments using multimodal sensing technologies to collect data from 102 subjects while lying on a bed. The individual's high resolution contact pressure data could be estimated from their RGB or long wavelength infrared (LWIR) images with 91.8% and 91.2% estimation accuracies in $PCS_{efs0.1}$ criteria, superior to state-of-the-art methods in the related image regression/translation tasks.



### Towards an Automatic Diagnosis of Peripheral and Central Palsy Using Machine Learning on Facial Features
- **Arxiv ID**: http://arxiv.org/abs/2201.11852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.11852v1)
- **Published**: 2022-01-27 23:07:02+00:00
- **Updated**: 2022-01-27 23:07:02+00:00
- **Authors**: C. V. Vletter, H. L. Burger, H. Alers, N. Sourlos, Z. Al-Ars
- **Comment**: 9 pages, 10 tables, 10 figures
- **Journal**: None
- **Summary**: Central palsy is a form of facial paralysis that requires urgent medical attention and has to be differentiated from other, similar conditions such as peripheral palsy. To aid in fast and accurate diagnosis of this condition, we propose a machine learning approach to automatically classify peripheral and central facial palsy. The Palda dataset is used, which contains 103 peripheral palsy images, 40 central palsy, and 60 healthy people. Experiments are run on five machine learning algorithms. The best performing algorithms were found to be the SVM (total accuracy of 85.1%) and the Gaussian naive Bayes (80.7%). The lowest false negative rate on central palsy was achieved by the naive Bayes approach (80% compared to 70%). This condition could prove to be the most severe, and thus its sensitivity is another good way to compare algorithms. By extrapolation, a dataset size of 334 total pictures is estimated to achieve a central palsy sensitivity of 95%. All code used for these machine learning experiments is freely available online at https://github.com/cvvletter/palsy.



### Using Shape Metrics to Describe 2D Data Points
- **Arxiv ID**: http://arxiv.org/abs/2201.11857v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.11857v1)
- **Published**: 2022-01-27 23:28:42+00:00
- **Updated**: 2022-01-27 23:28:42+00:00
- **Authors**: William Franz Lamberti
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional machine learning (ML) algorithms, such as multiple regression, require human analysts to make decisions on how to treat the data. These decisions can make the model building process subjective and difficult to replicate for those who did not build the model. Deep learning approaches benefit by allowing the model to learn what features are important once the human analyst builds the architecture. Thus, a method for automating certain human decisions for traditional ML modeling would help to improve the reproducibility and remove subjective aspects of the model building process. To that end, we propose to use shape metrics to describe 2D data to help make analyses more explainable and interpretable. The proposed approach provides a foundation to help automate various aspects of model building in an interpretable and explainable fashion. This is particularly important in applications in the medical community where the `right to explainability' is crucial. We provide various simulated data sets ranging from probability distributions, functions, and model quality control checks (such as QQ-Plots and residual analyses from ordinary least squares) to showcase the breadth of this approach.



