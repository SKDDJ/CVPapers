# Arxiv Papers in cs.CV on 2022-01-21
### Vertical Federated Edge Learning with Distributed Integrated Sensing and Communication
- **Arxiv ID**: http://arxiv.org/abs/2201.08512v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/2201.08512v2)
- **Published**: 2022-01-21 02:05:07+00:00
- **Updated**: 2022-06-07 03:39:42+00:00
- **Authors**: Peixi Liu, Guangxu Zhu, Wei Jiang, Wu Luo, Jie Xu, Shuguang Cui
- **Comment**: 5 pages, 7 figures, accepted by IEEE Communications Letters
- **Journal**: None
- **Summary**: This letter studies a vertical federated edge learning (FEEL) system for collaborative objects/human motion recognition by exploiting the distributed integrated sensing and communication (ISAC). In this system, distributed edge devices first send wireless signals to sense targeted objects/human, and then exchange intermediate computed vectors (instead of raw sensing data) for collaborative recognition while preserving data privacy. To boost the spectrum and hardware utilization efficiency for FEEL, we exploit ISAC for both target sensing and data exchange, by employing dedicated frequency-modulated continuous-wave (FMCW) signals at each edge device. Under this setup, we propose a vertical FEEL framework for realizing the recognition based on the collected multi-view wireless sensing data. In this framework, each edge device owns an individual local L-model to transform its sensing data into an intermediate vector with relatively low dimensions, which is then transmitted to a coordinating edge device for final output via a common downstream S-model. By considering a human motion recognition task, experimental results show that our vertical FEEL based approach achieves recognition accuracy up to 98\% with an improvement up to 8\% compared to the benchmarks, including on-device training and horizontal FEEL.



### What Can Machine Vision Do for Lymphatic Histopathology Image Analysis: A Comprehensive Review
- **Arxiv ID**: http://arxiv.org/abs/2201.08550v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08550v2)
- **Published**: 2022-01-21 05:54:14+00:00
- **Updated**: 2022-05-08 16:19:14+00:00
- **Authors**: Xiaoqi Li, Haoyuan Chen, Chen Li, Md Mamunur Rahaman, Xintong Li, Jian Wu, Xiaoyan Li, Hongzan Sun, Marcin Grzegorzek
- **Comment**: None
- **Journal**: None
- **Summary**: In the past ten years, the computing power of machine vision (MV) has been continuously improved, and image analysis algorithms have developed rapidly. At the same time, histopathological slices can be stored as digital images. Therefore, MV algorithms can provide doctors with diagnostic references. In particular, the continuous improvement of deep learning algorithms has further improved the accuracy of MV in disease detection and diagnosis. This paper reviews the applications of image processing technology based on MV in lymphoma histopathological images in recent years, including segmentation, classification and detection. Finally, the current methods are analyzed, some more potential methods are proposed, and further prospects are made.



### Classroom Slide Narration System
- **Arxiv ID**: http://arxiv.org/abs/2201.08574v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.08574v1)
- **Published**: 2022-01-21 07:20:03+00:00
- **Updated**: 2022-01-21 07:20:03+00:00
- **Authors**: Jobin K. V., Ajoy Mondal, C. V. Jawahar
- **Comment**: None
- **Journal**: CVIP 2021
- **Summary**: Slide presentations are an effective and efficient tool used by the teaching community for classroom communication. However, this teaching model can be challenging for blind and visually impaired (VI) students. The VI student required personal human assistance for understand the presented slide. This shortcoming motivates us to design a Classroom Slide Narration System (CSNS) that generates audio descriptions corresponding to the slide content. This problem poses as an image-to-markup language generation task. The initial step is to extract logical regions such as title, text, equation, figure, and table from the slide image. In the classroom slide images, the logical regions are distributed based on the location of the image. To utilize the location of the logical regions for slide image segmentation, we propose the architecture, Classroom Slide Segmentation Network (CSSN). The unique attributes of this architecture differs from most other semantic segmentation networks. Publicly available benchmark datasets such as WiSe and SPaSe are used to validate the performance of our segmentation architecture. We obtained 9.54 segmentation accuracy improvement in WiSe dataset. We extract content (information) from the slide using four well-established modules such as optical character recognition (OCR), figure classification, equation description, and table structure recognizer. With this information, we build a Classroom Slide Narration System (CSNS) to help VI students understand the slide content. The users have given better feedback on the quality output of the proposed CSNS in comparison to existing systems like Facebooks Automatic Alt-Text (AAT) and Tesseract.



### SegTransVAE: Hybrid CNN -- Transformer with Regularization for medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.08582v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.08582v3)
- **Published**: 2022-01-21 08:02:55+00:00
- **Updated**: 2022-03-04 03:48:21+00:00
- **Authors**: Quan-Dung Pham, Hai Nguyen-Truong, Nam Nguyen Phuong, Khoa N. A. Nguyen
- **Comment**: I would like to withdraw this paper to recheck the result and method
- **Journal**: None
- **Summary**: Current research on deep learning for medical image segmentation exposes their limitations in learning either global semantic information or local contextual information. To tackle these issues, a novel network named SegTransVAE is proposed in this paper. SegTransVAE is built upon encoder-decoder architecture, exploiting transformer with the variational autoencoder (VAE) branch to the network to reconstruct the input images jointly with segmentation. To the best of our knowledge, this is the first method combining the success of CNN, transformer, and VAE. Evaluation on various recently introduced datasets shows that SegTransVAE outperforms previous methods in Dice Score and $95\%$-Haudorff Distance while having comparable inference time to a simple CNN-based architecture network. The source code is available at: https://github.com/itruonghai/SegTransVAE.



### Pseudo-Labeled Auto-Curriculum Learning for Semi-Supervised Keypoint Localization
- **Arxiv ID**: http://arxiv.org/abs/2201.08613v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08613v2)
- **Published**: 2022-01-21 09:51:58+00:00
- **Updated**: 2022-01-24 11:04:47+00:00
- **Authors**: Can Wang, Sheng Jin, Yingda Guan, Wentao Liu, Chen Qian, Ping Luo, Wanli Ouyang
- **Comment**: To appear on ICLR2022
- **Journal**: None
- **Summary**: Localizing keypoints of an object is a basic visual problem. However, supervised learning of a keypoint localization network often requires a large amount of data, which is expensive and time-consuming to obtain. To remedy this, there is an ever-growing interest in semi-supervised learning (SSL), which leverages a small set of labeled data along with a large set of unlabeled data. Among these SSL approaches, pseudo-labeling (PL) is one of the most popular. PL approaches apply pseudo-labels to unlabeled data, and then train the model with a combination of the labeled and pseudo-labeled data iteratively. The key to the success of PL is the selection of high-quality pseudo-labeled samples. Previous works mostly select training samples by manually setting a single confidence threshold. We propose to automatically select reliable pseudo-labeled samples with a series of dynamic thresholds, which constitutes a learning curriculum. Extensive experiments on six keypoint localization benchmark datasets demonstrate that the proposed approach significantly outperforms the previous state-of-the-art SSL approaches.



### Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World
- **Arxiv ID**: http://arxiv.org/abs/2201.08619v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2201.08619v2)
- **Published**: 2022-01-21 10:11:27+00:00
- **Updated**: 2022-05-29 08:34:08+00:00
- **Authors**: Hua Ma, Yinshan Li, Yansong Gao, Alsharif Abuadbba, Zhi Zhang, Anmin Fu, Hyoungshick Kim, Said F. Al-Sarawi, Nepal Surya, Derek Abbott
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have been shown to be vulnerable to recent backdoor attacks. A backdoored model behaves normally for inputs containing no attacker-secretly-chosen trigger and maliciously for inputs with the trigger. To date, backdoor attacks and countermeasures mainly focus on image classification tasks. And most of them are implemented in the digital world with digital triggers. Besides the classification tasks, object detection systems are also considered as one of the basic foundations of computer vision tasks. However, there is no investigation and understanding of the backdoor vulnerability of the object detector, even in the digital world with digital triggers. For the first time, this work demonstrates that existing object detectors are inherently susceptible to physical backdoor attacks. We use a natural T-shirt bought from a market as a trigger to enable the cloaking effect--the person bounding-box disappears in front of the object detector. We show that such a backdoor can be implanted from two exploitable attack scenarios into the object detector, which is outsourced or fine-tuned through a pretrained model. We have extensively evaluated three popular object detection algorithms: anchor-based Yolo-V3, Yolo-V4, and anchor-free CenterNet. Building upon 19 videos shot in real-world scenes, we confirm that the backdoor attack is robust against various factors: movement, distance, angle, non-rigid deformation, and lighting. Specifically, the attack success rate (ASR) in most videos is 100% or close to it, while the clean data accuracy of the backdoored model is the same as its clean counterpart. The latter implies that it is infeasible to detect the backdoor behavior merely through a validation set. The averaged ASR still remains sufficiently high to be 78% in the transfer learning attack scenarios evaluated on CenterNet. See the demo video on https://youtu.be/Q3HOF4OobbY.



### VIPriors 2: Visual Inductive Priors for Data-Efficient Deep Learning Challenges
- **Arxiv ID**: http://arxiv.org/abs/2201.08625v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.08625v1)
- **Published**: 2022-01-21 10:20:52+00:00
- **Updated**: 2022-01-21 10:20:52+00:00
- **Authors**: Attila Lengyel, Robert-Jan Bruintjes, Marcos Baptista Rios, Osman Semih Kayhan, Davide Zambrano, Nergis Tomen, Jan van Gemert
- **Comment**: 11 pages, 11 figures
- **Journal**: None
- **Summary**: The second edition of the "VIPriors: Visual Inductive Priors for Data-Efficient Deep Learning" challenges featured five data-impaired challenges, where models are trained from scratch on a reduced number of training samples for various key computer vision tasks. To encourage new and creative ideas on incorporating relevant inductive biases to improve the data efficiency of deep learning models, we prohibited the use of pre-trained checkpoints and other transfer learning techniques. The provided baselines are outperformed by a large margin in all five challenges, mainly thanks to extensive data augmentation policies, model ensembling, and data efficient network architectures.



### Multi-view Monocular Depth and Uncertainty Prediction with Deep SfM in Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/2201.08633v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08633v1)
- **Published**: 2022-01-21 10:42:57+00:00
- **Updated**: 2022-01-21 10:42:57+00:00
- **Authors**: Christian Homeyer, Oliver Lange, Christoph Schnörr
- **Comment**: 20 pages, 5 figures, 3 tables, submitted to ICPRAI 2022
- **Journal**: None
- **Summary**: 3D reconstruction of depth and motion from monocular video in dynamic environments is a highly ill-posed problem due to scale ambiguities when projecting to the 2D image domain. In this work, we investigate the performance of the current State-of-the-Art (SotA) deep multi-view systems in such environments. We find that current supervised methods work surprisingly well despite not modelling individual object motions, but make systematic errors due to a lack of dense ground truth data. To detect such errors during usage, we extend the cost volume based Deep Video to Depth (DeepV2D) framework \cite{teed2018deepv2d} with a learned uncertainty. Our Deep Video to certain Depth (DeepV2cD) model allows i) to perform en par or better with current SotA and ii) achieve a better uncertainty measure than the naive Shannon entropy. Our experiments show that a simple filter strategy based on the uncertainty can significantly reduce systematic errors. This results in cleaner reconstructions both on static and dynamic parts of the scene.



### Conceptor Learning for Class Activation Mapping
- **Arxiv ID**: http://arxiv.org/abs/2201.08636v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.08636v1)
- **Published**: 2022-01-21 10:51:14+00:00
- **Updated**: 2022-01-21 10:51:14+00:00
- **Authors**: Guangwu Qian, Zhen-Qun Yang, Xu-Lu Zhang, Yaowei Wang, Qing Li, Xiao-Yong Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Class Activation Mapping (CAM) has been widely adopted to generate saliency maps which provides visual explanations for deep neural networks (DNNs). The saliency maps are conventionally generated by fusing the channels of the target feature map using a weighted average scheme. It is a weak model for the inter-channel relation, in the sense that it only models the relation among channels in a contrastive way (i.e., channels that play key roles in the prediction are given higher weights for them to stand out in the fusion). The collaborative relation, which makes the channels work together to provide cross reference, has been ignored. Furthermore, the model has neglected the intra-channel relation thoroughly.In this paper, we address this problem by introducing Conceptor learning into CAM generation. Conceptor leaning has been originally proposed to model the patterns of state changes in recurrent neural networks (RNNs). By relaxing the dependency of Conceptor learning to RNNs, we make Conceptor-CAM not only generalizable to more DNN architectures but also able to learn both the inter- and intra-channel relations for better saliency map generation. Moreover, we have enabled the use of Boolean operations to combine the positive and pseudo-negative evidences, which has made the CAM inference more robust and comprehensive. The effectiveness of Conceptor-CAM has been validated with both formal verifications and experiments on the dataset of the largest scale in literature. The experimental results show that Conceptor-CAM is compatible with and can bring significant improvement to all well recognized CAM-based methods, and has outperformed the state-of-the-art methods by 43.14%~72.79% (88.39%~168.15%) on ILSVRC2012 in Average Increase (Drop), 15.42%~42.55% (47.09%~372.09%) on VOC, and 17.43%~31.32% (47.54%~206.45%) on COCO, respectively.



### Enhancing Pseudo Label Quality for Semi-Supervised Domain-Generalized Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.08657v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.08657v2)
- **Published**: 2022-01-21 12:02:00+00:00
- **Updated**: 2022-03-17 10:20:17+00:00
- **Authors**: Huifeng Yao, Xiaowei Hu, Xiaomeng Li
- **Comment**: Accepted by AAAI 2022
- **Journal**: None
- **Summary**: Generalizing the medical image segmentation algorithms to unseen domains is an important research topic for computer-aided diagnosis and surgery. Most existing methods require a fully labeled dataset in each source domain. Although some researchers developed a semi-supervised domain generalized method, it still requires the domain labels. This paper presents a novel confidence-aware cross pseudo supervision algorithm for semi-supervised domain generalized medical image segmentation. The main goal is to enhance the pseudo label quality for unlabeled images from unknown distributions. To achieve it, we perform the Fourier transformation to learn low-level statistic information across domains and augment the images to incorporate cross-domain information. With these augmentations as perturbations, we feed the input to a confidence-aware cross pseudo supervision network to measure the variance of pseudo labels and regularize the network to learn with more confident pseudo labels. Our method sets new records on public datasets, i.e., M&Ms and SCGM. Notably, without using domain labels, our method surpasses the prior art that even uses domain labels by 11.67% on Dice on M&Ms dataset with 2% labeled data. Code is available at https://github.com/XMed-Lab/EPL_SemiDG.



### Fast Differentiable Matrix Square Root
- **Arxiv ID**: http://arxiv.org/abs/2201.08663v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MS, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2201.08663v1)
- **Published**: 2022-01-21 12:18:06+00:00
- **Updated**: 2022-01-21 12:18:06+00:00
- **Authors**: Yue Song, Nicu Sebe, Wei Wang
- **Comment**: Accpeted by ICLR 2022
- **Journal**: None
- **Summary**: Computing the matrix square root or its inverse in a differentiable manner is important in a variety of computer vision tasks. Previous methods either adopt the Singular Value Decomposition (SVD) to explicitly factorize the matrix or use the Newton-Schulz iteration (NS iteration) to derive the approximate solution. However, both methods are not computationally efficient enough in either the forward pass or in the backward pass. In this paper, we propose two more efficient variants to compute the differentiable matrix square root. For the forward propagation, one method is to use Matrix Taylor Polynomial (MTP), and the other method is to use Matrix Pad\'e Approximants (MPA). The backward gradient is computed by iteratively solving the continuous-time Lyapunov equation using the matrix sign function. Both methods yield considerable speed-up compared with the SVD or the Newton-Schulz iteration. Experimental results on the de-correlated batch normalization and second-order vision transformer demonstrate that our methods can also achieve competitive and even slightly better performances. The code is available at \href{https://github.com/KingJamesSong/FastDifferentiableMatSqrt}{https://github.com/KingJamesSong/FastDifferentiableMatSqrt}.



### Dynamic Deep Convolutional Candlestick Learner
- **Arxiv ID**: http://arxiv.org/abs/2201.08669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08669v1)
- **Published**: 2022-01-21 12:34:59+00:00
- **Updated**: 2022-01-21 12:34:59+00:00
- **Authors**: Jun-Hao Chen, Yun-Cheng Tsai
- **Comment**: 11 pages, 9 figures, 2 tables
- **Journal**: None
- **Summary**: Candlestick pattern is one of the most fundamental and valuable graphical tools in financial trading that supports traders observing the current market conditions to make the proper decision. This task has a long history and, most of the time, human experts. Recently, efforts have been made to automatically classify these patterns with the deep learning models. The GAF-CNN model is a well-suited way to imitate how human traders capture the candlestick pattern by integrating spatial features visually. However, with the great potential of the GAF encoding, this classification task can be extended to a more complicated object detection level. This work presents an innovative integration of modern object detection techniques and GAF time-series encoding on candlestick pattern tasks. We make crucial modifications to the representative yet straightforward YOLO version 1 model based on our time-series encoding method and the property of such data type. Powered by the deep neural networks and the unique architectural design, the proposed model performs pretty well in candlestick classification and location recognition. The results show tremendous potential in applying modern object detection techniques on time-series tasks in a real-time manner.



### Exploring Fusion Strategies for Accurate RGBT Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2201.08673v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08673v1)
- **Published**: 2022-01-21 12:37:43+00:00
- **Updated**: 2022-01-21 12:37:43+00:00
- **Authors**: Zhangyong Tang, Tianyang Xu, Hui Li, Xiao-Jun Wu, Xuefeng Zhu, Josef Kittler
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: We address the problem of multi-modal object tracking in video and explore various options of fusing the complementary information conveyed by the visible (RGB) and thermal infrared (TIR) modalities including pixel-level, feature-level and decision-level fusion. Specifically, different from the existing methods, paradigm of image fusion task is heeded for fusion at pixel level. Feature-level fusion is fulfilled by attention mechanism with channels excited optionally. Besides, at decision level, a novel fusion strategy is put forward since an effortless averaging configuration has shown the superiority. The effectiveness of the proposed decision-level fusion strategy owes to a number of innovative contributions, including a dynamic weighting of the RGB and TIR contributions and a linear template update operation. A variant of which produced the winning tracker at the Visual Object Tracking Challenge 2020 (VOT-RGBT2020). The concurrent exploration of innovative pixel- and feature-level fusion strategies highlights the advantages of the proposed decision-level fusion method. Extensive experimental results on three challenging datasets, \textit{i.e.}, GTOT, VOT-RGBT2019, and VOT-RGBT2020, demonstrate the effectiveness and robustness of the proposed method, compared to the state-of-the-art approaches. Code will be shared at \textcolor{blue}{\emph{https://github.com/Zhangyong-Tang/DFAT}.



### Distance-Ratio-Based Formulation for Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.08676v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2201.08676v1)
- **Published**: 2022-01-21 12:45:23+00:00
- **Updated**: 2022-01-21 12:45:23+00:00
- **Authors**: Hyeongji Kim, Pekka Parviainen, Ketil Malde
- **Comment**: 17 pages. Codes for our experiments are available in
  https://github.com/hjk92g/DR_Formulation_ML . Perhaps, we will write a new
  version with experiments using normalized embedding and common metric
  learning performance metrics
- **Journal**: None
- **Summary**: In metric learning, the goal is to learn an embedding so that data points with the same class are close to each other and data points with different classes are far apart. We propose a distance-ratio-based (DR) formulation for metric learning. Like softmax-based formulation for metric learning, it models $p(y=c|x')$, which is a probability that a query point $x'$ belongs to a class $c$. The DR formulation has two useful properties. First, the corresponding loss is not affected by scale changes of an embedding. Second, it outputs the optimal (maximum or minimum) classification confidence scores on representing points for classes. To demonstrate the effectiveness of our formulation, we conduct few-shot classification experiments using softmax-based and DR formulations on CUB and mini-ImageNet datasets. The results show that DR formulation generally enables faster and more stable metric learning than the softmax-based formulation. As a result, using DR formulation achieves improved or comparable generalization performances.



### A Comprehensive Study of Vision Transformers on Dense Prediction Tasks
- **Arxiv ID**: http://arxiv.org/abs/2201.08683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08683v1)
- **Published**: 2022-01-21 13:18:16+00:00
- **Updated**: 2022-01-21 13:18:16+00:00
- **Authors**: Kishaan Jeeveswaran, Senthilkumar Kathiresan, Arnav Varma, Omar Magdy, Bahram Zonooz, Elahe Arani
- **Comment**: 17th International Conference on Computer Vision Theory and
  Applications (VISAP, 2022)
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs), architectures consisting of convolutional layers, have been the standard choice in vision tasks. Recent studies have shown that Vision Transformers (VTs), architectures based on self-attention modules, achieve comparable performance in challenging tasks such as object detection and semantic segmentation. However, the image processing mechanism of VTs is different from that of conventional CNNs. This poses several questions about their generalizability, robustness, reliability, and texture bias when used to extract features for complex tasks. To address these questions, we study and compare VT and CNN architectures as feature extractors in object detection and semantic segmentation. Our extensive empirical results show that the features generated by VTs are more robust to distribution shifts, natural corruptions, and adversarial attacks in both tasks, whereas CNNs perform better at higher image resolutions in object detection. Furthermore, our results demonstrate that VTs in dense prediction tasks produce more reliable and less texture-biased predictions.



### SparseAlign: A Super-Resolution Algorithm for Automatic Marker Localization and Deformation Estimation in Cryo-Electron Tomography
- **Arxiv ID**: http://arxiv.org/abs/2201.08706v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA, math.OC, q-bio.QM, 65K10, 65M32
- **Links**: [PDF](http://arxiv.org/pdf/2201.08706v1)
- **Published**: 2022-01-21 14:03:32+00:00
- **Updated**: 2022-01-21 14:03:32+00:00
- **Authors**: Poulami Somanya Ganguly, Felix Lucka, Holger Kohr, Erik Franken, Hermen Jan Hupkes, K Joost Batenburg
- **Comment**: None
- **Journal**: None
- **Summary**: Tilt-series alignment is crucial to obtaining high-resolution reconstructions in cryo-electron tomography. Beam-induced local deformation of the sample is hard to estimate from the low-contrast sample alone, and often requires fiducial gold bead markers. The state-of-the-art approach for deformation estimation uses (semi-)manually labelled marker locations in projection data to fit the parameters of a polynomial deformation model. Manually-labelled marker locations are difficult to obtain when data are noisy or markers overlap in projection data. We propose an alternative mathematical approach for simultaneous marker localization and deformation estimation by extending a grid-free super-resolution algorithm first proposed in the context of single-molecule localization microscopy. Our approach does not require labelled marker locations; instead, we use an image-based loss where we compare the forward projection of markers with the observed data. We equip this marker localization scheme with an additional deformation estimation component and solve for a reduced number of deformation parameters. Using extensive numerical studies on marker-only samples, we show that our approach automatically finds markers and reliably estimates sample deformation without labelled marker data. We further demonstrate the applicability of our approach for a broad range of model mismatch scenarios, including experimental electron tomography data of gold markers on ice.



### Improving Across-Dataset Brain Tissue Segmentation Using Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.08741v2
- **DOI**: 10.3389/fnimg.2022.1023481
- **Categories**: **eess.IV**, cs.CV, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2201.08741v2)
- **Published**: 2022-01-21 15:16:39+00:00
- **Updated**: 2023-01-31 20:49:39+00:00
- **Authors**: Vishwanatha M. Rao, Zihan Wan, Soroush Arabshahi, David J. Ma, Pin-Yu Lee, Ye Tian, Xuzhe Zhang, Andrew F. Laine, Jia Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Brain tissue segmentation has demonstrated great utility in quantifying MRI data through Voxel-Based Morphometry and highlighting subtle structural changes associated with various conditions within the brain. However, manual segmentation is highly labor-intensive, and automated approaches have struggled due to properties inherent to MRI acquisition, leaving a great need for an effective segmentation tool. Despite the recent success of deep convolutional neural networks (CNNs) for brain tissue segmentation, many such solutions do not generalize well to new datasets, which is critical for a reliable solution. Transformers have demonstrated success in natural image segmentation and have recently been applied to 3D medical image segmentation tasks due to their ability to capture long-distance relationships in the input where the local receptive fields of CNNs struggle. This study introduces a novel CNN-Transformer hybrid architecture designed for brain tissue segmentation. We validate our model's performance across four multi-site T1w MRI datasets, covering different vendors, field strengths, scan parameters, time points, and neuropsychiatric conditions. In all situations, our model achieved the greatest generality and reliability. Out method is inherently robust and can serve as a valuable tool for brain-related T1w MRI studies. The code for the TABS network is available at: https://github.com/raovish6/TABS.



### ERS: a novel comprehensive endoscopy image dataset for machine learning, compliant with the MST 3.0 specification
- **Arxiv ID**: http://arxiv.org/abs/2201.08746v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08746v1)
- **Published**: 2022-01-21 15:39:45+00:00
- **Updated**: 2022-01-21 15:39:45+00:00
- **Authors**: Jan Cychnerski, Tomasz Dziubich, Adam Brzeski
- **Comment**: None
- **Journal**: None
- **Summary**: The article presents a new multi-label comprehensive image dataset from flexible endoscopy, colonoscopy and capsule endoscopy, named ERS. The collection has been labeled according to the full medical specification of 'Minimum Standard Terminology 3.0' (MST 3.0), describing all possible findings in the gastrointestinal tract (104 possible labels), extended with an additional 19 labels useful in common machine learning applications.   The dataset contains around 6000 precisely and 115,000 approximately labeled frames from endoscopy videos, 3600 precise and 22,600 approximate segmentation masks, and 1.23 million unlabeled frames from flexible and capsule endoscopy videos. The labeled data cover almost entirely the MST 3.0 standard. The data came from 1520 videos of 1135 patients.   Additionally, this paper proposes and describes four exemplary experiments in gastrointestinal image classification task performed using the created dataset. The obtained results indicate the high usefulness and flexibility of the dataset in training and testing machine learning algorithms in the field of endoscopic data analysis.



### Object Detection in Aerial Images: What Improves the Accuracy?
- **Arxiv ID**: http://arxiv.org/abs/2201.08763v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.08763v1)
- **Published**: 2022-01-21 16:22:48+00:00
- **Updated**: 2022-01-21 16:22:48+00:00
- **Authors**: Hashmat Shadab Malik, Ikboljon Sobirov, Abdelrahman Mohamed
- **Comment**: 8 pages, 14 Figures
- **Journal**: None
- **Summary**: Object detection is a challenging and popular computer vision problem. The problem is even more challenging in aerial images due to significant variation in scale and viewpoint in a diverse set of object categories. Recently, deep learning-based object detection approaches have been actively explored for the problem of object detection in aerial images. In this work, we investigate the impact of Faster R-CNN for aerial object detection and explore numerous strategies to improve its performance for aerial images. We conduct extensive experiments on the challenging iSAID dataset. The resulting adapted Faster R-CNN obtains a significant mAP gain of 4.96% over its vanilla baseline counterpart on the iSAID validation set, demonstrating the impact of different strategies investigated in this work.



### Contrastive and Selective Hidden Embeddings for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.08779v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08779v2)
- **Published**: 2022-01-21 16:52:19+00:00
- **Updated**: 2022-04-29 14:00:59+00:00
- **Authors**: Zhuowei Li, Zihao Liu, Zhiqiang Hu, Qing Xia, Ruiqin Xiong, Shaoting Zhang, Dimitris Metaxas, Tingting Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation has been widely recognized as a pivot procedure for clinical diagnosis, analysis, and treatment planning. However, the laborious and expensive annotation process lags down the speed of further advances. Contrastive learning-based weight pre-training provides an alternative by leveraging unlabeled data to learn a good representation. In this paper, we investigate how contrastive learning benefits the general supervised medical segmentation tasks. To this end, patch-dragsaw contrastive regularization (PDCR) is proposed to perform patch-level tugging and repulsing with the extent controlled by a continuous affinity score. And a new structure dubbed uncertainty-aware feature selection block (UAFS) is designed to perform the feature selection process, which can handle the learning target shift caused by minority features with high uncertainty. By plugging the proposed 2 modules into the existing segmentation architecture, we achieve state-of-the-art results across 8 public datasets from 6 domains. Newly designed modules further decrease the amount of training data to a quarter while achieving comparable, if not better, performances. From this perspective, we take the opposite direction of the original self/un-supervised contrastive learning by further excavating information contained within the label.



### AiTLAS: Artificial Intelligence Toolbox for Earth Observation
- **Arxiv ID**: http://arxiv.org/abs/2201.08789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08789v1)
- **Published**: 2022-01-21 17:10:14+00:00
- **Updated**: 2022-01-21 17:10:14+00:00
- **Authors**: Ivica Dimitrovski, Ivan Kitanovski, Panče Panov, Nikola Simidjievski, Dragi Kocev
- **Comment**: None
- **Journal**: None
- **Summary**: The AiTLAS toolbox (Artificial Intelligence Toolbox for Earth Observation) includes state-of-the-art machine learning methods for exploratory and predictive analysis of satellite imagery as well as repository of AI-ready Earth Observation (EO) datasets. It can be easily applied for a variety of Earth Observation tasks, such as land use and cover classification, crop type prediction, localization of specific objects (semantic segmentation), etc. The main goal of AiTLAS is to facilitate better usability and adoption of novel AI methods (and models) by EO experts, while offering easy access and standardized format of EO datasets to AI experts which further allows benchmarking of various existing and novel AI methods tailored for EO data.



### Machine Learning Algorithms for Prediction of Penetration Depth and Geometrical Analysis of Weld in Friction Stir Spot Welding Process
- **Arxiv ID**: http://arxiv.org/abs/2201.09725v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09725v1)
- **Published**: 2022-01-21 17:16:25+00:00
- **Updated**: 2022-01-21 17:16:25+00:00
- **Authors**: Akshansh Mishra, Raheem Al-Sabur, Ahmad K. Jassim
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, manufacturing sectors harness the power of machine learning and data science algorithms to make predictions for the optimization of mechanical and microstructure properties of fabricated mechanical components. The application of these algorithms reduces the experimental cost beside leads to reduce the time of experiments. The present research work is based on the prediction of penetration depth using Supervised Machine Learning algorithms such as Support Vector Machines (SVM), Random Forest Algorithm, and Robust Regression algorithm. A Friction Stir Spot Welding (FSSW) was used to join two elements of AA1230 aluminum alloys. The dataset consists of three input parameters: Rotational Speed (rpm), Dwelling Time (seconds), and Axial Load (KN), on which the machine learning models were trained and tested. It observed that the Robust Regression machine learning algorithm outperformed the rest of the algorithms by resulting in the coefficient of determination of 0.96. The research work also highlights the application of image processing techniques to find the geometrical features of the weld formation.



### Reliable Detection of Doppelgängers based on Deep Face Representations
- **Arxiv ID**: http://arxiv.org/abs/2201.08831v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08831v2)
- **Published**: 2022-01-21 18:37:08+00:00
- **Updated**: 2022-04-05 05:38:31+00:00
- **Authors**: Christian Rathgeb, Daniel Fischer, Pawel Drozdowski, Christoph Busch
- **Comment**: accepted in IET Biometrics
- **Journal**: None
- **Summary**: Doppelg\"angers (or lookalikes) usually yield an increased probability of false matches in a facial recognition system, as opposed to random face image pairs selected for non-mated comparison trials. In this work, we assess the impact of doppelg\"angers on the HDA Doppelg\"anger and Disguised Faces in The Wild databases using a state-of-the-art face recognition system. It is found that doppelg\"anger image pairs yield very high similarity scores resulting in a significant increase of false match rates. Further, we propose a doppelg\"anger detection method which distinguishes doppelg\"angers from mated comparison trials by analysing differences in deep representations obtained from face image pairs. The proposed detection system employs a machine learning-based classifier, which is trained with generated doppelg\"anger image pairs utilising face morphing techniques. Experimental evaluations conducted on the HDA Doppelg\"anger and Look-Alike Face databases reveal a detection equal error rate of approximately 2.7% for the task of separating mated authentication attempts from doppelg\"angers.



### Point-NeRF: Point-based Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2201.08845v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08845v7)
- **Published**: 2022-01-21 18:59:20+00:00
- **Updated**: 2023-03-15 21:51:08+00:00
- **Authors**: Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, Ulrich Neumann
- **Comment**: Accepted to CVPR 2022 (Oral)
- **Journal**: In Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (pp. 5438-5448) (2022)
- **Summary**: Volumetric neural rendering methods like NeRF generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30X faster training time. Point-NeRF can be combined with other 3D reconstruction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism. The experiments on the DTU, the NeRF Synthetics , the ScanNet and the Tanks and Temples datasets demonstrate Point-NeRF can surpass the existing methods and achieve the state-of-the-art results.



### On the in vivo recognition of kidney stones using machine learning
- **Arxiv ID**: http://arxiv.org/abs/2201.08865v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.08865v2)
- **Published**: 2022-01-21 19:18:42+00:00
- **Updated**: 2023-08-24 21:58:18+00:00
- **Authors**: Francisco Lopez-Tiro, Vincent Estrade, Jacques Hubert, Daniel Flores-Araiza, Miguel Gonzalez-Mendoza, Gilberto Ochoa-Ruiz, Christian Daul
- **Comment**: Paper submitted to IEEE Access
- **Journal**: None
- **Summary**: Determining the type of kidney stones allows urologists to prescribe a treatment to avoid recurrence of renal lithiasis. An automated in-vivo image-based classification method would be an important step towards an immediate identification of the kidney stone type required as a first phase of the diagnosis. In the literature it was shown on ex-vivo data (i.e., in very controlled scene and image acquisition conditions) that an automated kidney stone classification is indeed feasible. This pilot study compares the kidney stone recognition performances of six shallow machine learning methods and three deep-learning architectures which were tested with in-vivo images of the four most frequent urinary calculi types acquired with an endoscope during standard ureteroscopies. This contribution details the database construction and the design of the tested kidney stones classifiers. Even if the best results were obtained by the Inception v3 architecture (weighted precision, recall and F1-score of 0.97, 0.98 and 0.97, respectively), it is also shown that choosing an appropriate colour space and texture features allows a shallow machine learning method to approach closely the performances of the most promising deep-learning methods (the XGBoost classifier led to weighted precision, recall and F1-score values of 0.96). This paper is the first one that explores the most discriminant features to be extracted from images acquired during ureteroscopies.



### Image-to-Video Re-Identification via Mutual Discriminative Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2201.08887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08887v1)
- **Published**: 2022-01-21 21:04:39+00:00
- **Updated**: 2022-01-21 21:04:39+00:00
- **Authors**: Pichao Wang, Fan Wang, Hao Li
- **Comment**: accepted by ICASSP 2022
- **Journal**: None
- **Summary**: The gap in representations between image and video makes Image-to-Video Re-identification (I2V Re-ID) challenging, and recent works formulate this problem as a knowledge distillation (KD) process. In this paper, we propose a mutual discriminative knowledge distillation framework to transfer a video-based richer representation to an image based representation more effectively. Specifically, we propose the triplet contrast loss (TCL), a novel loss designed for KD. During the KD process, the TCL loss transfers the local structure, exploits the higher order information, and mitigates the misalignment of the heterogeneous output of teacher and student networks. Compared with other losses for KD, the proposed TCL loss selectively transfers the local discriminative features from teacher to student, making it effective in the ReID. Besides the TCL loss, we adopt mutual learning to regularize both the teacher and student networks training. Extensive experiments demonstrate the effectiveness of our method on the MARS, DukeMTMC-VideoReID and VeRi-776 benchmarks.



### Adaptive Activation-based Structured Pruning
- **Arxiv ID**: http://arxiv.org/abs/2201.10520v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.10520v3)
- **Published**: 2022-01-21 22:21:31+00:00
- **Updated**: 2023-03-09 20:01:31+00:00
- **Authors**: Kaiqi Zhao, Animesh Jain, Ming Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Pruning is a promising approach to compress complex deep learning models in order to deploy them on resource-constrained edge devices. However, many existing pruning solutions are based on unstructured pruning, which yields models that cannot efficiently run on commodity hardware and require users to manually explore and tune the pruning process, which is time-consuming and often leads to sub-optimal results. To address these limitations, this paper presents an adaptive, activation-based, structured pruning approach to automatically and efficiently generate small, accurate, and hardware-efficient models that meet user requirements. First, it proposes iterative structured pruning using activation-based attention feature maps to effectively identify and prune unimportant filters. Then, it proposes adaptive pruning policies for automatically meeting the pruning objectives of accuracy-critical, memory-constrained, and latency-sensitive tasks. A comprehensive evaluation shows that the proposed method can substantially outperform the state-of-the-art structured pruning works on CIFAR-10 and ImageNet datasets. For example, on ResNet-56 with CIFAR-10, without any accuracy drop, our method achieves the largest parameter reduction (79.11%), outperforming the related works by 22.81% to 66.07%, and the largest FLOPs reduction (70.13%), outperforming the related works by 14.13% to 26.53%.



