# Arxiv Papers in cs.CV on 2022-01-31
### Deep Learning Approaches on Image Captioning: A Review
- **Arxiv ID**: http://arxiv.org/abs/2201.12944v5
- **DOI**: 10.1145/3617592
- **Categories**: **cs.CV**, I.2.7; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2201.12944v5)
- **Published**: 2022-01-31 00:39:37+00:00
- **Updated**: 2023-08-22 17:50:41+00:00
- **Authors**: Taraneh Ghandi, Hamidreza Pourreza, Hamidreza Mahyar
- **Comment**: 41 pages, 6 figures
- **Journal**: None
- **Summary**: Image captioning is a research area of immense importance, aiming to generate natural language descriptions for visual content in the form of still images. The advent of deep learning and more recently vision-language pre-training techniques has revolutionized the field, leading to more sophisticated methods and improved performance. In this survey paper, we provide a structured review of deep learning methods in image captioning by presenting a comprehensive taxonomy and discussing each method category in detail. Additionally, we examine the datasets commonly employed in image captioning research, as well as the evaluation metrics used to assess the performance of different captioning models. We address the challenges faced in this field by emphasizing issues such as object hallucination, missing context, illumination conditions, contextual understanding, and referring expressions. We rank different deep learning methods' performance according to widely used evaluation metrics, giving insight into the current state of the art. Furthermore, we identify several potential future directions for research in this area, which include tackling the information misalignment problem between image and text modalities, mitigating dataset bias, incorporating vision-language pre-training methods to enhance caption generation, and developing improved evaluation tools to accurately measure the quality of image captions.



### Plug-In Inversion: Model-Agnostic Inversion for Vision with Data Augmentations
- **Arxiv ID**: http://arxiv.org/abs/2201.12961v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.12961v1)
- **Published**: 2022-01-31 02:12:45+00:00
- **Updated**: 2022-01-31 02:12:45+00:00
- **Authors**: Amin Ghiasi, Hamid Kazemi, Steven Reich, Chen Zhu, Micah Goldblum, Tom Goldstein
- **Comment**: None
- **Journal**: None
- **Summary**: Existing techniques for model inversion typically rely on hard-to-tune regularizers, such as total variation or feature regularization, which must be individually calibrated for each network in order to produce adequate images. In this work, we introduce Plug-In Inversion, which relies on a simple set of augmentations and does not require excessive hyper-parameter tuning. Under our proposed augmentation-based scheme, the same set of augmentation hyper-parameters can be used for inverting a wide range of image classification models, regardless of input dimensions or the architecture. We illustrate the practicality of our approach by inverting Vision Transformers (ViTs) and Multi-Layer Perceptrons (MLPs) trained on the ImageNet dataset, tasks which to the best of our knowledge have not been successfully accomplished by any previous works.



### A Survey on Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2202.07456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07456v1)
- **Published**: 2022-01-31 02:35:45+00:00
- **Updated**: 2022-01-31 02:35:45+00:00
- **Authors**: ChuMiao Li
- **Comment**: None
- **Journal**: None
- **Summary**: With the improvement of social life quality and the real needs of daily work, images are more and more all around us. Image blurring due to camera shake, human movement, etc. has become the key to affecting image quality. How to remove image blur and restore clear image has gradually become an important research direction in the field of computer vision. After more than half a century of unremitting efforts, the majority of scientific and technological workers have made fruitful progress in image deblurring. This article reviews the work of image deblurring and specifically introduces more classic image deblurring methods, which is helpful to understand current research and look forward to future trends. This article reviews the traditional image deblurring methods and depth-represented image deblurring methods, and comprehensively classifies and introduces the corresponding technical methods. This review can provide some guidance for researchers in the field of image deblurring, and at the same time facilitate their subsequent study and research.



### Filtering In Neural Implicit Functions
- **Arxiv ID**: http://arxiv.org/abs/2201.13013v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.13013v4)
- **Published**: 2022-01-31 06:11:28+00:00
- **Updated**: 2022-08-22 16:26:19+00:00
- **Authors**: Yixin Zhuang
- **Comment**: Source code at https://github.com/yixin26/FINN
- **Journal**: None
- **Summary**: Neural implicit functions are highly effective for data representation. However, the implicit functions learned by neural networks usually include unexpected noisy artifacts or lose fine details if the input data has many scales of detail or contains both low-frequency and high-frequency bandwidths. Removing artifacts while preserving fine-scale contents is challenging and usually comes out with over-smoothing or noisy issues. To solve this dilemma, we propose a new framework (FINN) that integrates a filtering module into the MLPs to perform data reconstruction while adapting regions containing different frequencies. The filtering module has a smoothing operator acting on intermediate results of the network that encourages the results to be smooth and a recovering operator bringing high frequencies to regions overly smooth. The two counteractive operators play consecutively in all MLP layers to adaptively influence the reconstruction. We demonstrate the advantage of FINN on several tasks and showcase significant improvement compared to state-of-the-art methods. In addition, FINN also yields better performance in both convergence speed and network stability.



### BOAT: Bilateral Local Attention Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.13027v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.13027v2)
- **Published**: 2022-01-31 07:09:50+00:00
- **Updated**: 2022-10-19 16:10:44+00:00
- **Authors**: Tan Yu, Gangming Zhao, Ping Li, Yizhou Yu
- **Comment**: BMVC2022 oral
- **Journal**: None
- **Summary**: Vision Transformers achieved outstanding performance in many computer vision tasks. Early Vision Transformers such as ViT and DeiT adopt global self-attention, which is computationally expensive when the number of patches is large. To improve efficiency, recent Vision Transformers adopt local self-attention mechanisms, where self-attention is computed within local windows. Despite the fact that window-based local self-attention significantly boosts efficiency, it fails to capture the relationships between distant but similar patches in the image plane. To overcome this limitation of image-space local attention, in this paper, we further exploit the locality of patches in the feature space. We group the patches into multiple clusters using their features, and self-attention is computed within every cluster. Such feature-space local attention effectively captures the connections between patches across different local windows but still relevant. We propose a Bilateral lOcal Attention vision Transformer (BOAT), which integrates feature-space local attention with image-space local attention. We further integrate BOAT with both Swin and CSWin models, and extensive experiments on several benchmark datasets demonstrate that our BOAT-CSWin model clearly and consistently outperforms existing state-of-the-art CNN models and vision Transformers.



### Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2202.00632v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00632v1)
- **Published**: 2022-01-31 08:13:12+00:00
- **Updated**: 2022-01-31 08:13:12+00:00
- **Authors**: Sven Burdorf, Karoline Plum, Daniel Hasenklever
- **Comment**: 6 pages, 4 figures submitted to 33rd IEEE Intelligent Vehicles
  Symposium (IV 22)
- **Journal**: None
- **Summary**: A number of studies have investigated the training of neural networks with synthetic data for applications in the real world. The aim of this study is to quantify how much real world data can be saved when using a mixed dataset of synthetic and real world data. By modeling the relationship between the number of training examples and detection performance by a simple power law, we find that the need for real world data can be reduced by up to 70% without sacrificing detection performance. The training of object detection networks is especially enhanced by enriching the mixed dataset with classes underrepresented in the real world dataset. The results indicate that mixed datasets with real world data ratios between 5% and 20% reduce the need for real world data the most without reducing the detection performance.



### NeuralTailor: Reconstructing Sewing Pattern Structures from 3D Point Clouds of Garments
- **Arxiv ID**: http://arxiv.org/abs/2201.13063v2
- **DOI**: 10.1145/3528223.3530179
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2201.13063v2)
- **Published**: 2022-01-31 08:33:49+00:00
- **Updated**: 2022-06-28 03:15:55+00:00
- **Authors**: Maria Korosteleva, Sung-Hee Lee
- **Comment**: Updated to the version accepted to SIGGRAPH 2022 (Journal Track)
- **Journal**: None
- **Summary**: The fields of SocialVR, performance capture, and virtual try-on are often faced with a need to faithfully reproduce real garments in the virtual world. One critical task is the disentanglement of the intrinsic garment shape from deformations due to fabric properties, physical forces, and contact with the body. We propose to use a garment sewing pattern, a realistic and compact garment descriptor, to facilitate the intrinsic garment shape estimation. Another major challenge is a high diversity of shapes and designs in the domain. The most common approach for Deep Learning on 3D garments is to build specialized models for individual garments or garment types. We argue that building a unified model for various garment designs has the benefit of generalization to novel garment types, hence covering a larger design domain than individual models would. We introduce NeuralTailor, a novel architecture based on point-level attention for set regression with variable cardinality, and apply it to the task of reconstructing 2D garment sewing patterns from the 3D point could garment models. Our experiments show that NeuralTailor successfully reconstructs sewing patterns and generalizes to garment types with pattern topologies unseen during training.



### Rigidity Preserving Image Transformations and Equivariance in Perspective
- **Arxiv ID**: http://arxiv.org/abs/2201.13065v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.13065v2)
- **Published**: 2022-01-31 08:43:10+00:00
- **Updated**: 2022-10-13 09:14:42+00:00
- **Authors**: Lucas Brynte, Georg Bökman, Axel Flinth, Fredrik Kahl
- **Comment**: v2: Substantially revised version. Among other things, experiments
  with the PixLoc model added
- **Journal**: None
- **Summary**: We characterize the class of image plane transformations which realize rigid camera motions and call these transformations `rigidity preserving'. In particular, 2D translations of pinhole images are not rigidity preserving. Hence, when using CNNs for 3D inference tasks, it can be beneficial to modify the inductive bias from equivariance towards translations to equivariance towards rigidity preserving transformations. We investigate how equivariance with respect to rigidity preserving transformations can be approximated in CNNs, and test our ideas on both 6D object pose estimation and visual localization. Experimentally, we improve on several competitive baselines.



### Single Object Tracking: A Survey of Methods, Datasets, and Evaluation Metrics
- **Arxiv ID**: http://arxiv.org/abs/2201.13066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.13066v1)
- **Published**: 2022-01-31 08:45:50+00:00
- **Updated**: 2022-01-31 08:45:50+00:00
- **Authors**: Zahra Soleimanitaleb, Mohammad Ali Keyvanrad
- **Comment**: 15 pages. This paper is about object tracking and review of methods
  in this task. The paper first published in the ICCKE2019 conference and then
  extended in this new paper
- **Journal**: None
- **Summary**: Object tracking is one of the foremost assignments in computer vision that has numerous commonsense applications such as traffic monitoring, robotics, autonomous vehicle tracking, and so on. Different researches have been tried later a long time, but since of diverse challenges such as occlusion, illumination variations, fast motion, etc. researches in this area continues. In this paper, different strategies of the following objects are inspected and a comprehensive classification is displayed that classified the following strategies into four fundamental categories of feature-based, segmentation-based, estimation-based, and learning-based methods that each of which has its claim sub-categories. The most center of this paper is on learning-based strategies, which are classified into three categories of generative strategies, discriminative strategies, and reinforcement learning. One of the sub-categories of the discriminative show is deep learning. Since of high-performance, deep learning has as of late been exceptionally much consider. Finally, the different datasets and the evaluation methods that are most commonly used will be introduced.



### Lymphoma segmentation from 3D PET-CT images using a deep evidential network
- **Arxiv ID**: http://arxiv.org/abs/2201.13078v2
- **DOI**: 10.1016/j.ijar.2022.06.007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.13078v2)
- **Published**: 2022-01-31 09:34:38+00:00
- **Updated**: 2022-06-24 14:11:18+00:00
- **Authors**: Ling Huang, Su Ruan, Pierre Decazes, Thierry Denoeux
- **Comment**: Preprint submitted to International Journal of Approximate Reasoning
- **Journal**: International Journal of Approximate Reasoning, Volume 149, 2022,
  Pages 39-60,
- **Summary**: An automatic evidential segmentation method based on Dempster-Shafer theory and deep learning is proposed to segment lymphomas from three-dimensional Positron Emission Tomography (PET) and Computed Tomography (CT) images. The architecture is composed of a deep feature-extraction module and an evidential layer. The feature extraction module uses an encoder-decoder framework to extract semantic feature vectors from 3D inputs. The evidential layer then uses prototypes in the feature space to compute a belief function at each voxel quantifying the uncertainty about the presence or absence of a lymphoma at this location. Two evidential layers are compared, based on different ways of using distances to prototypes for computing mass functions. The whole model is trained end-to-end by minimizing the Dice loss function. The proposed combination of deep feature extraction and evidential segmentation is shown to outperform the baseline UNet model as well as three other state-of-the-art models on a dataset of 173 patients.



### Unsupervised Anomaly Detection in 3D Brain MRI using Deep Learning with Multi-Task Brain Age Prediction
- **Arxiv ID**: http://arxiv.org/abs/2201.13081v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.13081v1)
- **Published**: 2022-01-31 09:39:52+00:00
- **Updated**: 2022-01-31 09:39:52+00:00
- **Authors**: Marcel Bengs, Finn Behrendt, Max-Heinrich Laves, Julia Krüger, Roland Opfer, Alexander Schlaefer
- **Comment**: Accepted at SPIE Medical Imaging 2022
- **Journal**: None
- **Summary**: Lesion detection in brain Magnetic Resonance Images (MRIs) remains a challenging task. MRIs are typically read and interpreted by domain experts, which is a tedious and time-consuming process. Recently, unsupervised anomaly detection (UAD) in brain MRI with deep learning has shown promising results to provide a quick, initial assessment. So far, these methods only rely on the visual appearance of healthy brain anatomy for anomaly detection. Another biomarker for abnormal brain development is the deviation between the brain age and the chronological age, which is unexplored in combination with UAD. We propose deep learning for UAD in 3D brain MRI considering additional age information. We analyze the value of age information during training, as an additional anomaly score, and systematically study several architecture concepts. Based on our analysis, we propose a novel deep learning approach for UAD with multi-task age prediction. We use clinical T1-weighted MRIs of 1735 healthy subjects and the publicly available BraTs 2019 data set for our study. Our novel approach significantly improves UAD performance with an AUC of 92.60% compared to an AUC-score of 84.37% using previous approaches without age information.



### Crowd-powered Face Manipulation Detection: Fusing Human Examiner Decisions
- **Arxiv ID**: http://arxiv.org/abs/2201.13084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.13084v1)
- **Published**: 2022-01-31 09:49:41+00:00
- **Updated**: 2022-01-31 09:49:41+00:00
- **Authors**: Christian Rathgeb, Robert Nichols, Mathias Ibsen, Pawel Drozdowski, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate the potential of fusing human examiner decisions for the task of digital face manipulation detection. To this end, various decision fusion methods are proposed incorporating the examiners' decision confidence, experience level, and their time to take a decision. Conducted experiments are based on a psychophysical evaluation of digital face image manipulation detection capabilities of humans in which different manipulation techniques were applied, i.e. face morphing, face swapping and retouching. The decisions of 223 participants were fused to simulate crowds of up to seven human examiners. Experimental results reveal that (1) despite the moderate detection performance achieved by single human examiners, a high accuracy can be obtained through decision fusion and (2) a weighted fusion which takes the examiners' decision confidence into account yields the most competitive detection performance.



### Adversarial Masking for Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.13100v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.13100v3)
- **Published**: 2022-01-31 10:23:23+00:00
- **Updated**: 2022-07-06 09:33:16+00:00
- **Authors**: Yuge Shi, N. Siddharth, Philip H. S. Torr, Adam R. Kosiorek
- **Comment**: None
- **Journal**: None
- **Summary**: We propose ADIOS, a masked image model (MIM) framework for self-supervised learning, which simultaneously learns a masking function and an image encoder using an adversarial objective. The image encoder is trained to minimise the distance between representations of the original and that of a masked image. The masking function, conversely, aims at maximising this distance. ADIOS consistently improves on state-of-the-art self-supervised learning (SSL) methods on a variety of tasks and datasets -- including classification on ImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and iNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao et al., 2021) -- while generating semantically meaningful masks. Unlike modern MIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch tokenisation construction of Vision Transformers, and can be implemented with convolutional backbones. We further demonstrate that the masks learned by ADIOS are more effective in improving representation learning of SSL methods than masking schemes used in popular MIM models. Code is available at https://github.com/YugeTen/adios.



### Imperceptible and Multi-channel Backdoor Attack against Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.13164v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.13164v1)
- **Published**: 2022-01-31 12:19:28+00:00
- **Updated**: 2022-01-31 12:19:28+00:00
- **Authors**: Mingfu Xue, Shifeng Ni, Yinghao Wu, Yushu Zhang, Jian Wang, Weiqiang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent researches demonstrate that Deep Neural Networks (DNN) models are vulnerable to backdoor attacks. The backdoored DNN model will behave maliciously when images containing backdoor triggers arrive. To date, existing backdoor attacks are single-trigger and single-target attacks, and the triggers of most existing backdoor attacks are obvious thus are easy to be detected or noticed. In this paper, we propose a novel imperceptible and multi-channel backdoor attack against Deep Neural Networks by exploiting Discrete Cosine Transform (DCT) steganography. Based on the proposed backdoor attack method, we implement two variants of backdoor attacks, i.e., N-to-N backdoor attack and N-to-One backdoor attack. Specifically, for a colored image, we utilize DCT steganography to construct the trigger on different channels of the image. As a result, the trigger is stealthy and natural. Based on the proposed method, we implement multi-target and multi-trigger backdoor attacks. Experimental results demonstrate that the average attack success rate of the N-to-N backdoor attack is 93.95% on CIFAR-10 dataset and 91.55% on TinyImageNet dataset, respectively. The average attack success rate of N-to-One attack is 90.22% and 89.53% on CIFAR-10 and TinyImageNet datasets, respectively. Meanwhile, the proposed backdoor attack does not affect the classification accuracy of the DNN model. Moreover, the proposed attack is demonstrated to be robust to the state-of-the-art backdoor defense (Neural Cleanse).



### SPAGHETTI: Editing Implicit Shapes Through Part Aware Generation
- **Arxiv ID**: http://arxiv.org/abs/2201.13168v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.13168v1)
- **Published**: 2022-01-31 12:31:41+00:00
- **Updated**: 2022-01-31 12:31:41+00:00
- **Authors**: Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: Neural implicit fields are quickly emerging as an attractive representation for learning based techniques. However, adopting them for 3D shape modeling and editing is challenging. We introduce a method for $\mathbf{E}$diting $\mathbf{I}$mplicit $\mathbf{S}$hapes $\mathbf{T}$hrough $\mathbf{P}$art $\mathbf{A}$ware $\mathbf{G}$enera$\mathbf{T}$ion, permuted in short as SPAGHETTI. Our architecture allows for manipulation of implicit shapes by means of transforming, interpolating and combining shape segments together, without requiring explicit part supervision. SPAGHETTI disentangles shape part representation into extrinsic and intrinsic geometric information. This characteristic enables a generative framework with part-level control. The modeling capabilities of SPAGHETTI are demonstrated using an interactive graphical interface, where users can directly edit neural implicit shapes.



### Few-Shot Backdoor Attacks on Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2201.13178v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.13178v2)
- **Published**: 2022-01-31 12:38:58+00:00
- **Updated**: 2022-05-04 09:08:10+00:00
- **Authors**: Yiming Li, Haoxiang Zhong, Xingjun Ma, Yong Jiang, Shu-Tao Xia
- **Comment**: This work is accepted by the ICLR 2022. The first two authors
  contributed equally to this work. In this version, we fix some typos and
  errors contained in the last one. 21 pages
- **Journal**: None
- **Summary**: Visual object tracking (VOT) has been widely adopted in mission-critical applications, such as autonomous driving and intelligent surveillance systems. In current practice, third-party resources such as datasets, backbone networks, and training platforms are frequently used to train high-performance VOT models. Whilst these resources bring certain convenience, they also introduce new security threats into VOT models. In this paper, we reveal such a threat where an adversary can easily implant hidden backdoors into VOT models by tempering with the training process. Specifically, we propose a simple yet effective few-shot backdoor attack (FSBA) that optimizes two losses alternately: 1) a \emph{feature loss} defined in the hidden feature space, and 2) the standard \emph{tracking loss}. We show that, once the backdoor is embedded into the target model by our FSBA, it can trick the model to lose track of specific objects even when the \emph{trigger} only appears in one or a few frames. We examine our attack in both digital and physical-world settings and show that it can significantly degrade the performance of state-of-the-art VOT trackers. We also show that our attack is resistant to potential defenses, highlighting the vulnerability of VOT models to potential backdoor attacks.



### Learning Super-Features for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2201.13182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.13182v1)
- **Published**: 2022-01-31 12:48:42+00:00
- **Updated**: 2022-01-31 12:48:42+00:00
- **Authors**: Philippe Weinzaepfel, Thomas Lucas, Diane Larlus, Yannis Kalantidis
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: Methods that combine local and global features have recently shown excellent performance on multiple challenging deep image retrieval benchmarks, but their use of local features raises at least two issues. First, these local features simply boil down to the localized map activations of a neural network, and hence can be extremely redundant. Second, they are typically trained with a global loss that only acts on top of an aggregation of local features; by contrast, testing is based on local feature matching, which creates a discrepancy between training and testing. In this paper, we propose a novel architecture for deep image retrieval, based solely on mid-level features that we call Super-features. These Super-features are constructed by an iterative attention module and constitute an ordered set in which each element focuses on a localized and discriminant image pattern. For training, they require only image labels. A contrastive loss operates directly at the level of Super-features and focuses on those that match across images. A second complementary loss encourages diversity. Experiments on common landmark retrieval benchmarks validate that Super-features substantially outperform state-of-the-art methods when using the same number of features, and only require a significantly smaller memory footprint to match their performance. Code and models are available at: https://github.com/naver/FIRe.



### Differentiable Neural Radiosity
- **Arxiv ID**: http://arxiv.org/abs/2201.13190v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.13190v1)
- **Published**: 2022-01-31 12:53:37+00:00
- **Updated**: 2022-01-31 12:53:37+00:00
- **Authors**: Saeed Hadadan, Matthias Zwicker
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Differentiable Neural Radiosity, a novel method of representing the solution of the differential rendering equation using a neural network. Inspired by neural radiosity techniques, we minimize the norm of the residual of the differential rendering equation to directly optimize our network. The network is capable of outputting continuous, view-independent gradients of the radiance field with respect to scene parameters, taking into account differential global illumination effects while keeping memory and time complexity constant in path length. To solve inverse rendering problems, we use a pre-trained instance of our network that represents the differential radiance field with respect to a limited number of scene parameters. In our experiments, we leverage this to achieve faster and more accurate convergence compared to other techniques such as Automatic Differentiation, Radiative Backpropagation, and Path Replay Backpropagation.



### Proximal Denoiser for Convergent Plug-and-Play Optimization with Nonconvex Regularization
- **Arxiv ID**: http://arxiv.org/abs/2201.13256v4
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.13256v4)
- **Published**: 2022-01-31 14:05:20+00:00
- **Updated**: 2022-06-21 15:01:41+00:00
- **Authors**: Samuel Hurault, Arthur Leclaire, Nicolas Papadakis
- **Comment**: 21 pages. arXiv admin note: text overlap with arXiv:2110.03220
- **Journal**: None
- **Summary**: Plug-and-Play (PnP) methods solve ill-posed inverse problems through iterative proximal algorithms by replacing a proximal operator by a denoising operation. When applied with deep neural network denoisers, these methods have shown state-of-the-art visual performance for image restoration problems. However, their theoretical convergence analysis is still incomplete. Most of the existing convergence results consider nonexpansive denoisers, which is non-realistic, or limit their analysis to strongly convex data-fidelity terms in the inverse problem to solve. Recently, it was proposed to train the denoiser as a gradient descent step on a functional parameterized by a deep neural network. Using such a denoiser guarantees the convergence of the PnP version of the Half-Quadratic-Splitting (PnP-HQS) iterative algorithm. In this paper, we show that this gradient denoiser can actually correspond to the proximal operator of another scalar function. Given this new result, we exploit the convergence theory of proximal algorithms in the nonconvex setting to obtain convergence results for PnP-PGD (Proximal Gradient Descent) and PnP-ADMM (Alternating Direction Method of Multipliers). When built on top of a smooth gradient denoiser, we show that PnP-PGD and PnP-ADMM are convergent and target stationary points of an explicit functional. These convergence results are confirmed with numerical experiments on deblurring, super-resolution and inpainting.



### StRegA: Unsupervised Anomaly Detection in Brain MRIs using a Compact Context-encoding Variational Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2201.13271v3
- **DOI**: 10.1016/j.compbiomed.2022.106093
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2201.13271v3)
- **Published**: 2022-01-31 14:27:35+00:00
- **Updated**: 2022-09-04 06:51:20+00:00
- **Authors**: Soumick Chatterjee, Alessandro Sciarra, Max Dünnwald, Pavan Tummala, Shubham Kumar Agrawal, Aishwarya Jauhari, Aman Kalra, Steffen Oeltze-Jafra, Oliver Speck, Andreas Nürnberger
- **Comment**: None
- **Journal**: Computers in Biology and Medicine, 106093 (2022)
- **Summary**: Expert interpretation of anatomical images of the human brain is the central part of neuro-radiology. Several machine learning-based techniques have been proposed to assist in the analysis process. However, the ML models typically need to be trained to perform a specific task, e.g., brain tumour segmentation or classification. Not only do the corresponding training data require laborious manual annotations, but a wide variety of abnormalities can be present in a human brain MRI - even more than one simultaneously, which renders representation of all possible anomalies very challenging. Hence, a possible solution is an unsupervised anomaly detection (UAD) system that can learn a data distribution from an unlabelled dataset of healthy subjects and then be applied to detect out of distribution samples. Such a technique can then be used to detect anomalies - lesions or abnormalities, for example, brain tumours, without explicitly training the model for that specific pathology. Several Variational Autoencoder (VAE) based techniques have been proposed in the past for this task. Even though they perform very well on controlled artificially simulated anomalies, many of them perform poorly while detecting anomalies in clinical data. This research proposes a compact version of the "context-encoding" VAE (ceVAE) model, combined with pre and post-processing steps, creating a UAD pipeline (StRegA), which is more robust on clinical data, and shows its applicability in detecting anomalies such as tumours in brain MRIs. The proposed pipeline achieved a Dice score of 0.642$\pm$0.101 while detecting tumours in T2w images of the BraTS dataset and 0.859$\pm$0.112 while detecting artificially induced anomalies, while the best performing baseline achieved 0.522$\pm$0.135 and 0.783$\pm$0.111, respectively.



### Combining Local and Global Pose Estimation for Precise Tracking of Similar Objects
- **Arxiv ID**: http://arxiv.org/abs/2201.13278v1
- **DOI**: 10.5220/0010882700003124
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.13278v1)
- **Published**: 2022-01-31 14:36:57+00:00
- **Updated**: 2022-01-31 14:36:57+00:00
- **Authors**: Niklas Gard, Anna Hilsmann, Peter Eisert
- **Comment**: Accepted at VISAPP 2022
- **Journal**: None
- **Summary**: In this paper, we present a multi-object 6D detection and tracking pipeline for potentially similar and non-textured objects. The combination of a convolutional neural network for object classification and rough pose estimation with a local pose refinement and an automatic mismatch detection enables direct application in real-time AR scenarios. A new network architecture, trained solely with synthetic images, allows simultaneous pose estimation of multiple objects with reduced GPU memory consumption and enhanced performance. In addition, the pose estimates are further improved by a local edge-based refinement step that explicitly exploits known object geometry information. For continuous movements, the sole use of local refinement reduces pose mismatches due to geometric ambiguities or occlusions. We showcase the entire tracking pipeline and demonstrate the benefits of the combined approach. Experiments on a challenging set of non-textured similar objects demonstrate the enhanced quality compared to the baseline method. Finally, we illustrate how the system can be used in a real AR assistance application within the field of construction.



### UQGAN: A Unified Model for Uncertainty Quantification of Deep Classifiers trained via Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/2201.13279v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.13279v5)
- **Published**: 2022-01-31 14:42:35+00:00
- **Updated**: 2023-01-09 11:59:35+00:00
- **Authors**: Philipp Oberdiek, Gernot A. Fink, Matthias Rottmann
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach to quantifying both aleatoric and epistemic uncertainty for deep neural networks in image classification, based on generative adversarial networks (GANs). While most works in the literature that use GANs to generate out-of-distribution (OoD) examples only focus on the evaluation of OoD detection, we present a GAN based approach to learn a classifier that produces proper uncertainties for OoD examples as well as for false positives (FPs). Instead of shielding the entire in-distribution data with GAN generated OoD examples which is state-of-the-art, we shield each class separately with out-of-class examples generated by a conditional GAN and complement this with a one-vs-all image classifier. In our experiments, in particular on CIFAR10, CIFAR100 and Tiny ImageNet, we improve over the OoD detection and FP detection performance of state-of-the-art GAN-training based classifiers. Furthermore, we also find that the generated GAN examples do not significantly affect the calibration error of our classifier and result in a significant gain in model accuracy.



### AI-based Medical e-Diagnosis for Fast and Automatic Ventricular Volume Measurement in the Patients with Normal Pressure Hydrocephalus
- **Arxiv ID**: http://arxiv.org/abs/2202.00650v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00650v1)
- **Published**: 2022-01-31 14:55:20+00:00
- **Updated**: 2022-01-31 14:55:20+00:00
- **Authors**: Xi Zhou, Qinghao Ye, Xiaolin Yang, Jiakuan Chen, Haiqin Ma, Jun Xia, Javier Del Ser, Guang Yang
- **Comment**: 26 pages, 4 figure, accepted by Neural Computing and Applications
  journal
- **Journal**: None
- **Summary**: Based on CT and MRI images acquired from normal pressure hydrocephalus (NPH) patients, using machine learning methods, we aim to establish a multi-modal and high-performance automatic ventricle segmentation method to achieve efficient and accurate automatic measurement of the ventricular volume. First, we extract the brain CT and MRI images of 143 definite NPH patients. Second, we manually label the ventricular volume (VV) and intracranial volume (ICV). Then, we use machine learning method to extract features and establish automatic ventricle segmentation model. Finally, we verify the reliability of the model and achieved automatic measurement of VV and ICV. In CT images, the Dice similarity coefficient (DSC), Intraclass Correlation Coefficient (ICC), Pearson correlation, and Bland-Altman analysis of the automatic and manual segmentation result of the VV were 0.95, 0.99, 0.99, and 4.2$\pm$2.6 respectively. The results of ICV were 0.96, 0.99, 0.99, and 6.0$\pm$3.8 respectively. The whole process takes 3.4$\pm$0.3 seconds. In MRI images, the DSC, ICC, Pearson correlation, and Bland-Altman analysis of the automatic and manual segmentation result of the VV were 0.94, 0.99, 0.99, and 2.0$\pm$0.6 respectively. The results of ICV were 0.93, 0.99, 0.99, and 7.9$\pm$3.8 respectively. The whole process took 1.9$\pm$0.1 seconds. We have established a multi-modal and high-performance automatic ventricle segmentation method to achieve efficient and accurate automatic measurement of the ventricular volume of NPH patients. This can help clinicians quickly and accurately understand the situation of NPH patient's ventricles.



### Metrics for saliency map evaluation of deep learning explanation methods
- **Arxiv ID**: http://arxiv.org/abs/2201.13291v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.13291v3)
- **Published**: 2022-01-31 14:59:36+00:00
- **Updated**: 2022-06-22 12:55:12+00:00
- **Authors**: Tristan Gomez, Thomas Fréour, Harold Mouchère
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the black-box nature of deep learning models, there is a recent development of solutions for visual explanations of CNNs. Given the high cost of user studies, metrics are necessary to compare and evaluate these different methods. In this paper, we critically analyze the Deletion Area Under Curve (DAUC) and Insertion Area Under Curve (IAUC) metrics proposed by Petsiuk et al. (2018). These metrics were designed to evaluate the faithfulness of saliency maps generated by generic methods such as Grad-CAM or RISE. First, we show that the actual saliency score values given by the saliency map are ignored as only the ranking of the scores is taken into account. This shows that these metrics are insufficient by themselves, as the visual appearance of a saliency map can change significantly without the ranking of the scores being modified. Secondly, we argue that during the computation of DAUC and IAUC, the model is presented with images that are out of the training distribution which might lead to an unreliable behavior of the model being explained. To complement DAUC/IAUC, we propose new metrics that quantify the sparsity and the calibration of explanation methods, two previously unstudied properties. Finally, we give general remarks about the metrics studied in this paper and discuss how to evaluate them in a user study.



### Rate Coding or Direct Coding: Which One is Better for Accurate, Robust, and Energy-efficient Spiking Neural Networks?
- **Arxiv ID**: http://arxiv.org/abs/2202.03133v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03133v2)
- **Published**: 2022-01-31 16:18:07+00:00
- **Updated**: 2022-04-12 16:43:19+00:00
- **Authors**: Youngeun Kim, Hyoungseob Park, Abhishek Moitra, Abhiroop Bhattacharjee, Yeshwanth Venkatesha, Priyadarshini Panda
- **Comment**: Accepted to ICASSP2022
- **Journal**: None
- **Summary**: Recent Spiking Neural Networks (SNNs) works focus on an image classification task, therefore various coding techniques have been proposed to convert an image into temporal binary spikes. Among them, rate coding and direct coding are regarded as prospective candidates for building a practical SNN system as they show state-of-the-art performance on large-scale datasets. Despite their usage, there is little attention to comparing these two coding schemes in a fair manner. In this paper, we conduct a comprehensive analysis of the two codings from three perspectives: accuracy, adversarial robustness, and energy-efficiency. First, we compare the performance of two coding techniques with various architectures and datasets. Then, we measure the robustness of the coding techniques on two adversarial attack methods. Finally, we compare the energy-efficiency of two coding schemes on a digital hardware platform. Our results show that direct coding can achieve better accuracy especially for a small number of timesteps. In contrast, rate coding shows better robustness to adversarial attacks owing to the non-differentiable spike generation process. Rate coding also yields higher energy-efficiency than direct coding which requires multi-bit precision for the first layer. Our study explores the characteristics of two codings, which is an important design consideration for building SNNs. The code is made available at https://github.com/Intelligent-Computing-Lab-Yale/Rate-vs-Direct.



### Learning to Hash Naturally Sorts
- **Arxiv ID**: http://arxiv.org/abs/2201.13322v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.13322v2)
- **Published**: 2022-01-31 16:19:02+00:00
- **Updated**: 2022-04-21 11:23:12+00:00
- **Authors**: Jiaguo Yu, Yuming Shen, Menghan Wang, Haofeng Zhang, Philip H. S. Torr
- **Comment**: IJCAI 2022
- **Journal**: None
- **Summary**: Learning to hash pictures a list-wise sorting problem. Its testing metrics, e.g., mean-average precision, count on a sorted candidate list ordered by pair-wise code similarity. However, scarcely does one train a deep hashing model with the sorted results end-to-end because of the non-differentiable nature of the sorting operation. This inconsistency in the objectives of training and test may lead to sub-optimal performance since the training loss often fails to reflect the actual retrieval metric. In this paper, we tackle this problem by introducing Naturally-Sorted Hashing (NSH). We sort the Hamming distances of samples' hash codes and accordingly gather their latent representations for self-supervised training. Thanks to the recent advances in differentiable sorting approximations, the hash head receives gradients from the sorter so that the hash encoder can be optimized along with the training procedure. Additionally, we describe a novel Sorted Noise-Contrastive Estimation (SortedNCE) loss that selectively picks positive and negative samples for contrastive learning, which allows NSH to mine data semantic relations during training in an unsupervised manner. Our extensive experiments show the proposed NSH model significantly outperforms the existing unsupervised hashing methods on three benchmarked datasets.



### Modeling the Background for Incremental and Weakly-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.13338v1
- **DOI**: 10.1109/TPAMI.2021.3133954
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.13338v1)
- **Published**: 2022-01-31 16:33:21+00:00
- **Updated**: 2022-01-31 16:33:21+00:00
- **Authors**: Fabio Cermelli, Massimiliano Mancini, Samuel Rota Buló, Elisa Ricci, Barbara Caputo
- **Comment**: Accepted by T-PAMI (https://ieeexplore.ieee.org/document/9645239/).
  arXiv admin note: substantial text overlap with arXiv:2002.00718
- **Journal**: None
- **Summary**: Deep neural networks have enabled major progresses in semantic segmentation. However, even the most advanced neural architectures suffer from important limitations. First, they are vulnerable to catastrophic forgetting, i.e. they perform poorly when they are required to incrementally update their model as new classes are available. Second, they rely on large amount of pixel-level annotations to produce accurate segmentation maps. To tackle these issues, we introduce a novel incremental class learning approach for semantic segmentation taking into account a peculiar aspect of this task: since each training step provides annotation only for a subset of all possible classes, pixels of the background class exhibit a semantic shift. Therefore, we revisit the traditional distillation paradigm by designing novel loss terms which explicitly account for the background shift. Additionally, we introduce a novel strategy to initialize classifier's parameters at each step in order to prevent biased predictions toward the background class. Finally, we demonstrate that our approach can be extended to point- and scribble-based weakly supervised segmentation, modeling the partial annotations to create priors for unlabeled pixels. We demonstrate the effectiveness of our approach with an extensive evaluation on the Pascal-VOC, ADE20K, and Cityscapes datasets, significantly outperforming state-of-the-art methods.



### Signing the Supermask: Keep, Hide, Invert
- **Arxiv ID**: http://arxiv.org/abs/2201.13361v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.13361v2)
- **Published**: 2022-01-31 17:17:37+00:00
- **Updated**: 2022-02-17 13:32:27+00:00
- **Authors**: Nils Koster, Oliver Grothe, Achim Rettinger
- **Comment**: ICLR 2022 camera ready
- **Journal**: None
- **Summary**: The exponential growth in numbers of parameters of neural networks over the past years has been accompanied by an increase in performance across several fields. However, due to their sheer size, the networks not only became difficult to interpret but also problematic to train and use in real-world applications, since hardware requirements increased accordingly. Tackling both issues, we present a novel approach that either drops a neural network's initial weights or inverts their respective sign. Put simply, a network is trained by weight selection and inversion without changing their absolute values. Our contribution extends previous work on masking by additionally sign-inverting the initial weights and follows the findings of the Lottery Ticket Hypothesis. Through this extension and adaptations of initialization methods, we achieve a pruning rate of up to 99%, while still matching or exceeding the performance of various baseline and previous models. Our approach has two main advantages. First, and most notable, signed Supermask models drastically simplify a model's structure, while still performing well on given tasks. Second, by reducing the neural network to its very foundation, we gain insights into which weights matter for performance. The code is available on GitHub.



### MHSnet: Multi-head and Spatial Attention Network with False-Positive Reduction for Pulmonary Nodules Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.13392v6
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.13392v6)
- **Published**: 2022-01-31 17:56:08+00:00
- **Updated**: 2022-05-12 04:16:25+00:00
- **Authors**: Juanyun Mai, Minghao Wang, Jiayin Zheng, Yanbo Shao, Zhaoqi Diao, Xinliang Fu, Yulong Chen, Jianyu Xiao, Jian You, Airu Yin, Yang Yang, Xiangcheng Qiu, Jinsheng Tao, Bo Wang, Hua Ji
- **Comment**: We have to revise the experiment results and conclusions
- **Journal**: None
- **Summary**: The mortality of lung cancer has ranked high among cancers for many years. Early detection of lung cancer is critical for disease prevention, cure, and mortality rate reduction. However, existing detection methods on pulmonary nodules introduce an excessive number of false positive proposals in order to achieve high sensitivity, which is not practical in clinical situations. In this paper, we propose the multi-head detection and spatial squeeze-and-attention network, MHSnet, to detect pulmonary nodules, in order to aid doctors in the early diagnosis of lung cancers. Specifically, we first introduce multi-head detectors and skip connections to customize for the variety of nodules in sizes, shapes and types and capture multi-scale features. Then, we implement a spatial attention module to enable the network to focus on different regions differently inspired by how experienced clinicians screen CT images, which results in fewer false positive proposals. Lastly, we present a lightweight but effective false positive reduction module with the Linear Regression model to cut down the number of false positive proposals, without any constraints on the front network. Extensive experimental results compared with the state-of-the-art models have shown the superiority of the MHSnet in terms of the average FROC, sensitivity and especially false discovery rate (2.98% and 2.18% improvement in terms of average FROC and sensitivity, 5.62% and 28.33% decrease in terms of false discovery rate and average candidates per scan). The false positive reduction module significantly decreases the average number of candidates generated per scan by 68.11% and the false discovery rate by 13.48%, which is promising to reduce distracted proposals for the downstream tasks based on the detection results.



### Third Time's the Charm? Image and Video Editing with StyleGAN3
- **Arxiv ID**: http://arxiv.org/abs/2201.13433v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.13433v1)
- **Published**: 2022-01-31 18:44:59+00:00
- **Updated**: 2022-01-31 18:44:59+00:00
- **Authors**: Yuval Alaluf, Or Patashnik, Zongze Wu, Asif Zamir, Eli Shechtman, Dani Lischinski, Daniel Cohen-Or
- **Comment**: Project page available at
  https://yuval-alaluf.github.io/stylegan3-editing/
- **Journal**: None
- **Summary**: StyleGAN is arguably one of the most intriguing and well-studied generative models, demonstrating impressive performance in image generation, inversion, and manipulation. In this work, we explore the recent StyleGAN3 architecture, compare it to its predecessor, and investigate its unique advantages, as well as drawbacks. In particular, we demonstrate that while StyleGAN3 can be trained on unaligned data, one can still use aligned data for training, without hindering the ability to generate unaligned imagery. Next, our analysis of the disentanglement of the different latent spaces of StyleGAN3 indicates that the commonly used W/W+ spaces are more entangled than their StyleGAN2 counterparts, underscoring the benefits of using the StyleSpace for fine-grained editing. Considering image inversion, we observe that existing encoder-based techniques struggle when trained on unaligned data. We therefore propose an encoding scheme trained solely on aligned data, yet can still invert unaligned images. Finally, we introduce a novel video inversion and editing workflow that leverages the capabilities of a fine-tuned StyleGAN3 generator to reduce texture sticking and expand the field of view of the edited video.



### Leveraging Bitstream Metadata for Fast, Accurate, Generalized Compressed Video Quality Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2202.00011v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.00011v2)
- **Published**: 2022-01-31 18:56:04+00:00
- **Updated**: 2023-04-25 18:53:04+00:00
- **Authors**: Max Ehrlich, Jon Barker, Namitha Padmanabhan, Larry Davis, Andrew Tao, Bryan Catanzaro, Abhinav Shrivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Video compression is a central feature of the modern internet powering technologies from social media to video conferencing. While video compression continues to mature, for many compression settings, quality loss is still noticeable. These settings nevertheless have important applications to the efficient transmission of videos over bandwidth constrained or otherwise unstable connections. In this work, we develop a deep learning architecture capable of restoring detail to compressed videos which leverages the underlying structure and motion information embedded in the video bitstream. We show that this improves restoration accuracy compared to prior compression correction methods and is competitive when compared with recent deep-learning-based video compression methods on rate-distortion while achieving higher throughput. Furthermore, we condition our model on quantization data which is readily available in the bitstream. This allows our single model to handle a variety of different compression quality settings which required an ensemble of models in prior work.



### Finding Directions in GAN's Latent Space for Neural Face Reenactment
- **Arxiv ID**: http://arxiv.org/abs/2202.00046v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.00046v2)
- **Published**: 2022-01-31 19:14:03+00:00
- **Updated**: 2022-10-06 15:02:35+00:00
- **Authors**: Stella Bounareli, Vasileios Argyriou, Georgios Tzimiropoulos
- **Comment**: Accepted for publication in BMVC 2022. Project page:
  https://stelabou.github.io/stylegan-directions-reenactment/ Code:
  https://github.com/StelaBou/stylegan_directions_face_reenactment
- **Journal**: None
- **Summary**: This paper is on face/head reenactment where the goal is to transfer the facial pose (3D head orientation and expression) of a target face to a source face. Previous methods focus on learning embedding networks for identity and pose disentanglement which proves to be a rather hard task, degrading the quality of the generated images. We take a different approach, bypassing the training of such networks, by using (fine-tuned) pre-trained GANs which have been shown capable of producing high-quality facial images. Because GANs are characterized by weak controllability, the core of our approach is a method to discover which directions in latent GAN space are responsible for controlling facial pose and expression variations. We present a simple pipeline to learn such directions with the aid of a 3D shape model which, by construction, already captures disentangled directions for facial pose, identity and expression. Moreover, we show that by embedding real images in the GAN latent space, our method can be successfully used for the reenactment of real-world faces. Our method features several favorable properties including using a single source image (one-shot) and enabling cross-person reenactment. Our qualitative and quantitative results show that our approach often produces reenacted faces of significantly higher quality than those produced by state-of-the-art methods for the standard benchmarks of VoxCeleb1 & 2. Source code is available at: https://github.com/StelaBou/stylegan_directions_face_reenactment



### Deep-Disaster: Unsupervised Disaster Detection and Localization Using Visual Data
- **Arxiv ID**: http://arxiv.org/abs/2202.00050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00050v1)
- **Published**: 2022-01-31 19:21:44+00:00
- **Updated**: 2022-01-31 19:21:44+00:00
- **Authors**: Soroor Shekarizadeh, Razieh Rastgoo, Saif Al-Kuwari, Mohammad Sabokrou
- **Comment**: None
- **Journal**: None
- **Summary**: Social media plays a significant role in sharing essential information, which helps humanitarian organizations in rescue operations during and after disaster incidents. However, developing an efficient method that can provide rapid analysis of social media images in the early hours of disasters is still largely an open problem, mainly due to the lack of suitable datasets and the sheer complexity of this task. In addition, supervised methods can not generalize well to novel disaster incidents. In this paper, inspired by the success of Knowledge Distillation (KD) methods, we propose an unsupervised deep neural network to detect and localize damages in social media images. Our proposed KD architecture is a feature-based distillation approach that comprises a pre-trained teacher and a smaller student network, with both networks having similar GAN architecture containing a generator and a discriminator. The student network is trained to emulate the behavior of the teacher on training input samples, which, in turn, contain images that do not include any damaged regions. Therefore, the student network only learns the distribution of no damage data and would have different behavior from the teacher network-facing damages. To detect damage, we utilize the difference between features generated by two networks using a defined score function that demonstrates the probability of damages occurring. Our experimental results on the benchmark dataset confirm that our approach outperforms state-of-the-art methods in detecting and localizing the damaged areas, especially for novel disaster types.



### AutoGeoLabel: Automated Label Generation for Geospatial Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.00067v1
- **DOI**: 10.1109/BigData52589.2021.9672060
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.5.4; I.5.2; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2202.00067v1)
- **Published**: 2022-01-31 20:02:22+00:00
- **Updated**: 2022-01-31 20:02:22+00:00
- **Authors**: Conrad M Albrecht, Fernando Marianno, Levente J Klein
- **Comment**: None
- **Journal**: 2021 IEEE International Conference on Big Data (Big Data), pp.
  1779-1786. IEEE, 2021
- **Summary**: A key challenge of supervised learning is the availability of human-labeled data. We evaluate a big data processing pipeline to auto-generate labels for remote sensing data. It is based on rasterized statistical features extracted from surveys such as e.g. LiDAR measurements. Using simple combinations of the rasterized statistical layers, it is demonstrated that multiple classes can be generated at accuracies of ~0.9. As proof of concept, we utilize the big geo-data platform IBM PAIRS to dynamically generate such labels in dense urban areas with multiple land cover classes. The general method proposed here is platform independent, and it can be adapted to generate labels for other satellite modalities in order to enable machine learning on overhead imagery for land use classification and object detection.



### Holistic Fine-grained GGS Characterization: From Detection to Unbalanced Classification
- **Arxiv ID**: http://arxiv.org/abs/2202.00087v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2202.00087v1)
- **Published**: 2022-01-31 20:56:25+00:00
- **Updated**: 2022-01-31 20:56:25+00:00
- **Authors**: Yuzhe Lu, Haichun Yang, Zuhayr Asad, Zheyu Zhu, Tianyuan Yao, Jiachen Xu, Agnes B. Fogo, Yuankai Huo
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies have demonstrated the diagnostic and prognostic values of global glomerulosclerosis (GGS) in IgA nephropathy, aging, and end-stage renal disease. However, the fine-grained quantitative analysis of multiple GGS subtypes (e.g., obsolescent, solidified, and disappearing glomerulosclerosis) is typically a resource extensive manual process. Very few automatic methods, if any, have been developed to bridge this gap for such analytics. In this paper, we present a holistic pipeline to quantify GGS (with both detection and classification) from a whole slide image in a fully automatic manner. In addition, we conduct the fine-grained classification for the sub-types of GGS. Our study releases the open-source quantitative analytical tool for fine-grained GGS characterization while tackling the technical challenges in unbalanced classification and integrating detection and classification.



### Query Efficient Decision Based Sparse Attacks Against Black-Box Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2202.00091v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00091v2)
- **Published**: 2022-01-31 21:10:47+00:00
- **Updated**: 2023-03-24 02:12:06+00:00
- **Authors**: Viet Quoc Vo, Ehsan Abbasnejad, Damith C. Ranasinghe
- **Comment**: Published as a conference paper at the International Conference on
  Learning Representations (ICLR 2022). Code is available at
  https://sparseevoattack.github.io/
- **Journal**: None
- **Summary**: Despite our best efforts, deep learning models remain highly vulnerable to even tiny adversarial perturbations applied to the inputs. The ability to extract information from solely the output of a machine learning model to craft adversarial perturbations to black-box models is a practical threat against real-world systems, such as autonomous cars or machine learning models exposed as a service (MLaaS). Of particular interest are sparse attacks. The realization of sparse attacks in black-box models demonstrates that machine learning models are more vulnerable than we believe. Because these attacks aim to minimize the number of perturbed pixels measured by l_0 norm-required to mislead a model by solely observing the decision (the predicted label) returned to a model query; the so-called decision-based attack setting. But, such an attack leads to an NP-hard optimization problem. We develop an evolution-based algorithm-SparseEvo-for the problem and evaluate against both convolutional deep neural networks and vision transformers. Notably, vision transformers are yet to be investigated under a decision-based attack setting. SparseEvo requires significantly fewer model queries than the state-of-the-art sparse attack Pointwise for both untargeted and targeted attacks. The attack algorithm, although conceptually simple, is also competitive with only a limited query budget against the state-of-the-art gradient-based whitebox attacks in standard computer vision tasks such as ImageNet. Importantly, the query efficient SparseEvo, along with decision-based attacks, in general, raise new questions regarding the safety of deployed systems and poses new directions to study and understand the robustness of machine learning models.



### Semi-supervised Identification and Mapping of Surface Water Extent using Street-level Monitoring Videos
- **Arxiv ID**: http://arxiv.org/abs/2202.00096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00096v1)
- **Published**: 2022-01-31 21:27:37+00:00
- **Updated**: 2022-01-31 21:27:37+00:00
- **Authors**: Ruo-Qian Wang, Yangmin Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Urban flooding is becoming a common and devastating hazard to cause life loss and economic damage. Monitoring and understanding urban flooding in the local scale is a challenging task due to the complicated urban landscape, intricate hydraulic process, and the lack of high-quality and resolution data. The emerging smart city technology such as monitoring cameras provides an unprecedented opportunity to address the data issue. However, estimating the water accumulation on the land surface based on the monitoring footage is unreliable using the traditional segmentation technique because the boundary of the water accumulation, under the influence of varying weather, background, and illumination, is usually too fuzzy to identify, and the oblique angle and image distortion in the video monitoring data prevents georeferencing and object-based measurements. This paper presents a novel semi-supervised segmentation scheme for surface water extent recognition from the footage of an oblique monitoring camera. The semi-supervised segmentation algorithm was found suitable to determine the water boundary and the monoplotting method was successfully applied to georeference the pixels of the monitoring video for the virtual quantification of the local drainage process. The correlation and mechanism-based analysis demonstrates the value of the proposed method in advancing the understanding of local drainage hydraulics. The workflow and created methods in this study has a great potential to study other street-level and earth surface processes.



### Real-Time Facial Expression Recognition using Facial Landmarks and Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.00102v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.00102v1)
- **Published**: 2022-01-31 21:38:30+00:00
- **Updated**: 2022-01-31 21:38:30+00:00
- **Authors**: Mohammad Amin Haghpanah, Ehsan Saeedizade, Mehdi Tale Masouleh, Ahmad Kalhor
- **Comment**: 7 pages, 8 figures, 6 tables
- **Journal**: None
- **Summary**: This paper presents a lightweight algorithm for feature extraction, classification of seven different emotions, and facial expression recognition in a real-time manner based on static images of the human face. In this regard, a Multi-Layer Perceptron (MLP) neural network is trained based on the foregoing algorithm. In order to classify human faces, first, some pre-processing is applied to the input image, which can localize and cut out faces from it. In the next step, a facial landmark detection library is used, which can detect the landmarks of each face. Then, the human face is split into upper and lower faces, which enables the extraction of the desired features from each part. In the proposed model, both geometric and texture-based feature types are taken into account. After the feature extraction phase, a normalized vector of features is created. A 3-layer MLP is trained using these feature vectors, leading to 96% accuracy on the test set.



### AntidoteRT: Run-time Detection and Correction of Poison Attacks on Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.01179v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01179v1)
- **Published**: 2022-01-31 23:42:32+00:00
- **Updated**: 2022-01-31 23:42:32+00:00
- **Authors**: Muhammad Usman, Youcheng Sun, Divya Gopinath, Corina S. Pasareanu
- **Comment**: None
- **Journal**: None
- **Summary**: We study backdoor poisoning attacks against image classification networks, whereby an attacker inserts a trigger into a subset of the training data, in such a way that at test time, this trigger causes the classifier to predict some target class. %There are several techniques proposed in the literature that aim to detect the attack but only a few also propose to defend against it, and they typically involve retraining the network which is not always possible in practice. We propose lightweight automated detection and correction techniques against poisoning attacks, which are based on neuron patterns mined from the network using a small set of clean and poisoned test samples with known labels. The patterns built based on the mis-classified samples are used for run-time detection of new poisoned inputs. For correction, we propose an input correction technique that uses a differential analysis to identify the trigger in the detected poisoned images, which is then reset to a neutral color. Our detection and correction are performed at run-time and input level, which is in contrast to most existing work that is focused on offline model-level defenses. We demonstrate that our technique outperforms existing defenses such as NeuralCleanse and STRIP on popular benchmarks such as MNIST, CIFAR-10, and GTSRB against the popular BadNets attack and the more complex DFST attack.



