# Arxiv Papers in cs.CV on 2022-01-22
### Enabling Deep Learning on Edge Devices through Filter Pruning and Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2201.10947v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.10947v1)
- **Published**: 2022-01-22 00:27:21+00:00
- **Updated**: 2022-01-22 00:27:21+00:00
- **Authors**: Kaiqi Zhao, Yitao Chen, Ming Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have introduced various intelligent applications to edge devices, such as image classification, speech recognition, and augmented reality. There is an increasing need of training such models on the devices in order to deliver personalized, responsive, and private learning. To address this need, this paper presents a new solution for deploying and training state-of-the-art models on the resource-constrained devices. First, the paper proposes a novel filter-pruning-based model compression method to create lightweight trainable models from large models trained in the cloud, without much loss of accuracy. Second, it proposes a novel knowledge transfer method to enable the on-device model to update incrementally in real time or near real time using incremental learning on new data and enable the on-device model to learn the unseen categories with the help of the in-cloud model in an unsupervised fashion. The results show that 1) our model compression method can remove up to 99.36% parameters of WRN-28-10, while preserving a Top-1 accuracy of over 90% on CIFAR-10; 2) our knowledge transfer method enables the compressed models to achieve more than 90% accuracy on CIFAR-10 and retain good accuracy on old categories; 3) it allows the compressed models to converge within real time (three to six minutes) on the edge for incremental learning tasks; 4) it enables the model to classify unseen categories of data (78.92% Top-1 accuracy) that it is never trained with.



### SAR Image Change Detection Based on Multiscale Capsule Network
- **Arxiv ID**: http://arxiv.org/abs/2201.08935v1
- **DOI**: 10.1109/LGRS.2020.2977838
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.08935v1)
- **Published**: 2022-01-22 01:30:36+00:00
- **Updated**: 2022-01-22 01:30:36+00:00
- **Authors**: Yunhao Gao, Feng Gao, Junyu Dong, Heng-Chao Li
- **Comment**: None
- **Journal**: in IEEE Geoscience and Remote Sensing Letters, vol. 18, no. 3, pp.
  484-488, March 2021
- **Summary**: Traditional synthetic aperture radar image change detection methods based on convolutional neural networks (CNNs) face the challenges of speckle noise and deformation sensitivity. To mitigate these issues, we proposed a Multiscale Capsule Network (Ms-CapsNet) to extract the discriminative information between the changed and unchanged pixels. On the one hand, the multiscale capsule module is employed to exploit the spatial relationship of features. Therefore, equivariant properties can be achieved by aggregating the features from different positions. On the other hand, an adaptive fusion convolution (AFC) module is designed for the proposed Ms-CapsNet. Higher semantic features can be captured for the primary capsules. Feature extracted by the AFC module significantly improves the robustness to speckle noise. The effectiveness of the proposed Ms-CapsNet is verified on three real SAR datasets. The comparison experiments with four state-of-the-art methods demonstrate the efficiency of the proposed method. Our codes are available at https://github.com/summitgao/SAR_CD_MS_CapsNet .



### Adaptive DropBlock Enhanced Generative Adversarial Networks for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2201.08938v1
- **DOI**: 10.1109/TGRS.2020.3015843
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.08938v1)
- **Published**: 2022-01-22 01:43:59+00:00
- **Updated**: 2022-01-22 01:43:59+00:00
- **Authors**: Junjie Wang, Feng Gao, Junyu Dong, Qian Du
- **Comment**: None
- **Journal**: in IEEE Transactions on Geoscience and Remote Sensing, vol. 59,
  no. 6, pp. 5040-5053, June 2021
- **Summary**: In recent years, hyperspectral image (HSI) classification based on generative adversarial networks (GAN) has achieved great progress. GAN-based classification methods can mitigate the limited training sample dilemma to some extent. However, several studies have pointed out that existing GAN-based HSI classification methods are heavily affected by the imbalanced training data problem. The discriminator in GAN always contradicts itself and tries to associate fake labels to the minority-class samples, and thus impair the classification performance. Another critical issue is the mode collapse in GAN-based methods. The generator is only capable of producing samples within a narrow scope of the data space, which severely hinders the advancement of GAN-based HSI classification methods. In this paper, we proposed an Adaptive DropBlock-enhanced Generative Adversarial Networks (ADGAN) for HSI classification. First, to solve the imbalanced training data problem, we adjust the discriminator to be a single classifier, and it will not contradict itself. Second, an adaptive DropBlock (AdapDrop) is proposed as a regularization method employed in the generator and discriminator to alleviate the mode collapse issue. The AdapDrop generated drop masks with adaptive shapes instead of a fixed size region, and it alleviates the limitations of DropBlock in dealing with ground objects with various shapes. Experimental results on three HSI datasets demonstrated that the proposed ADGAN achieved superior performance over state-of-the-art GAN-based methods. Our codes are available at https://github.com/summitgao/HC_ADGAN



### DCNGAN: A Deformable Convolutional-Based GAN with QP Adaptation for Perceptual Quality Enhancement of Compressed Video
- **Arxiv ID**: http://arxiv.org/abs/2201.08944v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.08944v3)
- **Published**: 2022-01-22 02:09:50+00:00
- **Updated**: 2022-01-28 01:29:42+00:00
- **Authors**: Saiping Zhang, Luis Herranz, Marta Mrak, Marc Gorriz Blanch, Shuai Wan, Fuzheng Yang
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: In this paper, we propose a deformable convolution-based generative adversarial network (DCNGAN) for perceptual quality enhancement of compressed videos. DCNGAN is also adaptive to the quantization parameters (QPs). Compared with optical flows, deformable convolutions are more effective and efficient to align frames. Deformable convolutions can operate on multiple frames, thus leveraging more temporal information, which is beneficial for enhancing the perceptual quality of compressed videos. Instead of aligning frames in a pairwise manner, the deformable convolution can process multiple frames simultaneously, which leads to lower computational complexity. Experimental results demonstrate that the proposed DCNGAN outperforms other state-of-the-art compressed video quality enhancement algorithms.



### Temporal Aggregation for Adaptive RGBT Tracking
- **Arxiv ID**: http://arxiv.org/abs/2201.08949v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08949v2)
- **Published**: 2022-01-22 02:31:56+00:00
- **Updated**: 2022-01-29 05:55:05+00:00
- **Authors**: Zhangyong Tang, Tianyang Xu, Xiao-Jun Wu
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: Visual object tracking with RGB and thermal infrared (TIR) spectra available, shorted in RGBT tracking, is a novel and challenging research topic which draws increasing attention nowadays. In this paper, we propose an RGBT tracker which takes spatio-temporal clues into account for robust appearance model learning, and simultaneously, constructs an adaptive fusion sub-network for cross-modal interactions. Unlike most existing RGBT trackers that implement object tracking tasks with only spatial information included, temporal information is further considered in this method. Specifically, different from traditional Siamese trackers, which only obtain one search image during the process of picking up template-search image pairs, an extra search sample adjacent to the original one is selected to predict the temporal transformation, resulting in improved robustness of tracking performance.As for multi-modal tracking, constrained to the limited RGBT datasets, the adaptive fusion sub-network is appended to our method at the decision level to reflect the complementary characteristics contained in two modalities. To design a thermal infrared assisted RGB tracker, the outputs of the classification head from the TIR modality are taken into consideration before the residual connection from the RGB modality. Extensive experimental results on three challenging datasets, i.e. VOT-RGBT2019, GTOT and RGBT210, verify the effectiveness of our method. Code will be shared at \textcolor{blue}{\emph{https://github.com/Zhangyong-Tang/TAAT}}.



### Visual Representation Learning with Self-Supervised Attention for Low-Label High-data Regime
- **Arxiv ID**: http://arxiv.org/abs/2201.08951v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.08951v2)
- **Published**: 2022-01-22 02:37:07+00:00
- **Updated**: 2022-01-30 22:21:20+00:00
- **Authors**: Prarthana Bhattacharyya, Chenge Li, Xiaonan Zhao, István Fehérvári, Jason Sun
- **Comment**: Accepted to ICASSP-2022
- **Journal**: None
- **Summary**: Self-supervision has shown outstanding results for natural language processing, and more recently, for image recognition. Simultaneously, vision transformers and its variants have emerged as a promising and scalable alternative to convolutions on various computer vision tasks. In this paper, we are the first to question if self-supervised vision transformers (SSL-ViTs) can be adapted to two important computer vision tasks in the low-label, high-data regime: few-shot image classification and zero-shot image retrieval. The motivation is to reduce the number of manual annotations required to train a visual embedder, and to produce generalizable and semantically meaningful embeddings. For few-shot image classification we train SSL-ViTs without any supervision, on external data, and use this trained embedder to adapt quickly to novel classes with limited number of labels. For zero-shot image retrieval, we use SSL-ViTs pre-trained on a large dataset without any labels and fine-tune them with several metric learning objectives. Our self-supervised attention representations outperforms the state-of-the-art on several public benchmarks for both tasks, namely miniImageNet and CUB200 for few-shot image classification by up-to 6%-10%, and Stanford Online Products, Cars196 and CUB200 for zero-shot image retrieval by up-to 4%-11%. Code is available at \url{https://github.com/AutoVision-cloud/SSL-ViT-lowlabel-highdata}.



### FedMed-GAN: Federated Domain Translation on Unsupervised Cross-Modality Brain Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2201.08953v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.08953v3)
- **Published**: 2022-01-22 02:50:29+00:00
- **Updated**: 2023-04-23 03:43:23+00:00
- **Authors**: Jinbao Wang, Guoyang Xie, Yawen Huang, Jiayi Lyu, Yefeng Zheng, Feng Zheng, Yaochu Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Utilizing multi-modal neuroimaging data has been proved to be effective to investigate human cognitive activities and certain pathologies. However, it is not practical to obtain the full set of paired neuroimaging data centrally since the collection faces several constraints, e.g., high examination cost, long acquisition time, and image corruption. In addition, these data are dispersed into different medical institutions and thus cannot be aggregated for centralized training considering the privacy issues. There is a clear need to launch a federated learning and facilitate the integration of the dispersed data from different institutions. In this paper, we propose a new benchmark for federated domain translation on unsupervised brain image synthesis (termed as FedMed-GAN) to bridge the gap between federated learning and medical GAN. FedMed-GAN mitigates the mode collapse without sacrificing the performance of generators, and is widely applied to different proportions of unpaired and paired data with variation adaptation property. We treat the gradient penalties by federally averaging algorithm and then leveraging differential privacy gradient descent to regularize the training dynamics. A comprehensive evaluation is provided for comparing FedMed-GAN and other centralized methods, which shows the new state-of-the-art performance by our FedMed-GAN. Our code has been released on the website: https://github.com/M-3LAB/FedMed-GAN



### Change Detection from Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network
- **Arxiv ID**: http://arxiv.org/abs/2201.08954v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.08954v2)
- **Published**: 2022-01-22 02:50:50+00:00
- **Updated**: 2022-02-09 07:52:08+00:00
- **Authors**: Junjie Wang, Feng Gao, Junyu Dong, Shan Zhang, Qian Du
- **Comment**: Accepted by IEEE JSTARS
- **Journal**: None
- **Summary**: Synthetic aperture radar (SAR) image change detection is a vital yet challenging task in the field of remote sensing image analysis. Most previous works adopt a self-supervised method which uses pseudo-labeled samples to guide subsequent training and testing. However, deep networks commonly require many high-quality samples for parameter optimization. The noise in pseudo-labels inevitably affects the final change detection performance. To solve the problem, we propose a Graph-based Knowledge Supplement Network (GKSNet). To be more specific, we extract discriminative information from the existing labeled dataset as additional knowledge, to suppress the adverse effects of noisy samples to some extent. Afterwards, we design a graph transfer module to distill contextual information attentively from the labeled dataset to the target dataset, which bridges feature correlation between datasets. To validate the proposed method, we conducted extensive experiments on four SAR datasets, which demonstrated the superiority of the proposed GKSNet as compared to several state-of-the-art baselines. Our codes are available at https://github.com/summitgao/SAR_CD_GKSNet.



### Modality Bank: Learn multi-modality images across data centers without sharing medical data
- **Arxiv ID**: http://arxiv.org/abs/2201.08955v1
- **DOI**: 10.1109/EMBC48229.2022.9871529
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.08955v1)
- **Published**: 2022-01-22 02:59:40+00:00
- **Updated**: 2022-01-22 02:59:40+00:00
- **Authors**: Qi Chang, Hui Qu, Zhennan Yan, Yunhe Gao, Lohendran Baskaran, Dimitris Metaxas
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2012.08604
- **Journal**: 2022 44th Annual International Conference of the IEEE Engineering
  in Medicine & Biology Society (EMBC), 2022, pp. 4758-4763
- **Summary**: Multi-modality images have been widely used and provide comprehensive information for medical image analysis. However, acquiring all modalities among all institutes is costly and often impossible in clinical settings. To leverage more comprehensive multi-modality information, we propose a privacy secured decentralized multi-modality adaptive learning architecture named ModalityBank. Our method could learn a set of effective domain-specific modulation parameters plugged into a common domain-agnostic network. We demonstrate by switching different sets of configurations, the generator could output high-quality images for a specific modality. Our method could also complete the missing modalities across all data centers, thus could be used for modality completion purposes. The downstream task trained from the synthesized multi-modality samples could achieve higher performance than learning from one real data center and achieve close-to-real performance compare with all real images.



### A Review of Deep Learning Based Image Super-resolution Techniques
- **Arxiv ID**: http://arxiv.org/abs/2201.10521v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10521v1)
- **Published**: 2022-01-22 03:13:11+00:00
- **Updated**: 2022-01-22 03:13:11+00:00
- **Authors**: Fangyuan Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Image super-resolution technology is the process of obtaining high-resolution images from one or more low-resolution images. With the development of deep learning, image super-resolution technology based on deep learning method is emerging. This paper reviews the research progress of the application of depth learning method in the field of image super-resolution, introduces this kind of super-resolution work from several aspects, and looks forward to the further application of depth learning method in the field of image super-resolution. By collecting and counting the relevant literature on the application of depth learning in the field of image super-resolution, we preliminarily summarizes the application results of depth learning method in the field of image super-resolution, and reports the latest progress of image super-resolution technology based on depth learning method.



### Learning Efficient Representations for Enhanced Object Detection on Large-scene SAR Images
- **Arxiv ID**: http://arxiv.org/abs/2201.08958v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08958v1)
- **Published**: 2022-01-22 03:25:24+00:00
- **Updated**: 2022-01-22 03:25:24+00:00
- **Authors**: Siyan Li, Yue Xiao, Yuhang Zhang, Lei Chu, Robert C. Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: It is a challenging problem to detect and recognize targets on complex large-scene Synthetic Aperture Radar (SAR) images. Recently developed deep learning algorithms can automatically learn the intrinsic features of SAR images, but still have much room for improvement on large-scene SAR images with limited data. In this paper, based on learning representations and multi-scale features of SAR images, we propose an efficient and robust deep learning based target detection method. Especially, by leveraging the effectiveness of adversarial autoencoder (AAE) which influences the distribution of the investigated data explicitly, the raw SAR dataset is augmented into an enhanced version with a large quantity and diversity. Besides, an auto-labeling scheme is proposed to improve labeling efficiency. Finally, with jointly training small target chips and large-scene images, an integrated YOLO network combining non-maximum suppression on sub-images is used to realize multiple targets detection of high resolution images. The numerical experimental results on the MSTAR dataset show that our method can realize target detection and recognition on large-scene images accurately and efficiently. The superior anti-noise performance is also confirmed by experiments.



### Few-shot Object Counting with Similarity-Aware Feature Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2201.08959v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08959v5)
- **Published**: 2022-01-22 03:27:11+00:00
- **Updated**: 2022-09-11 01:49:36+00:00
- **Authors**: Zhiyuan You, Kai Yang, Wenhan Luo, Xin Lu, Lei Cui, Xinyi Le
- **Comment**: Accepted by WACV 2023
- **Journal**: None
- **Summary**: This work studies the problem of few-shot object counting, which counts the number of exemplar objects (i.e., described by one or several support images) occurring in the query image. The major challenge lies in that the target objects can be densely packed in the query image, making it hard to recognize every single one. To tackle the obstacle, we propose a novel learning block, equipped with a similarity comparison module and a feature enhancement module. Concretely, given a support image and a query image, we first derive a score map by comparing their projected features at every spatial position. The score maps regarding all support images are collected together and normalized across both the exemplar dimension and the spatial dimensions, producing a reliable similarity map. We then enhance the query feature with the support features by employing the developed point-wise similarities as the weighting coefficients. Such a design encourages the model to inspect the query image by focusing more on the regions akin to the support images, leading to much clearer boundaries between different objects. Extensive experiments on various benchmarks and training setups suggest that we surpass the state-of-the-art methods by a sufficiently large margin. For instance, on a recent large-scale FSC-147 dataset, we surpass the state-of-the-art method by improving the mean absolute error from 22.08 to 14.32 (35%$\uparrow$). Code has been released in https://github.com/zhiyuanyou/SAFECount.



### Collaborative Representation for SPD Matrices with Application to Image-Set Classification
- **Arxiv ID**: http://arxiv.org/abs/2201.08962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08962v1)
- **Published**: 2022-01-22 04:56:53+00:00
- **Updated**: 2022-01-22 04:56:53+00:00
- **Authors**: Li Chu, Rui Wang, Xiao-Jun Wu
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: Collaborative representation-based classification (CRC) has demonstrated remarkable progress in the past few years because of its closed-form analytical solutions. However, the existing CRC methods are incapable of processing the nonlinear variational information directly. Recent advances illustrate that how to effectively model these nonlinear variational information and learn invariant representations is an open challenge in the community of computer vision and pattern recognition To this end, we try to design a new algorithm to handle this problem. Firstly, the second-order statistic, i.e., covariance matrix is applied to model the original image sets. Due to the space formed by a set of nonsingular covariance matrices is a well-known Symmetric Positive Definite (SPD) manifold, generalising the Euclidean collaborative representation to the SPD manifold is not an easy task. Then, we devise two strategies to cope with this issue. One attempts to embed the SPD manifold-valued data representations into an associated tangent space via the matrix logarithm map. Another is to embed them into a Reproducing Kernel Hilbert Space (RKHS) by utilizing the Riemannian kernel function. After these two treatments, CRC is applicable to the SPD manifold-valued features. The evaluations on four banchmarking datasets justify its effectiveness.



### Diffractive all-optical computing for quantitative phase imaging
- **Arxiv ID**: http://arxiv.org/abs/2201.08964v1
- **DOI**: 10.1002/adom.202200281
- **Categories**: **physics.optics**, cs.CV, cs.NE, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2201.08964v1)
- **Published**: 2022-01-22 05:28:44+00:00
- **Updated**: 2022-01-22 05:28:44+00:00
- **Authors**: Deniz Mengu, Aydogan Ozcan
- **Comment**: 23 Pages, 5 Figures
- **Journal**: Advanced Optical Materials (2022)
- **Summary**: Quantitative phase imaging (QPI) is a label-free computational imaging technique that provides optical path length information of specimens. In modern implementations, the quantitative phase image of an object is reconstructed digitally through numerical methods running in a computer, often using iterative algorithms. Here, we demonstrate a diffractive QPI network that can synthesize the quantitative phase image of an object by converting the input phase information of a scene into intensity variations at the output plane. A diffractive QPI network is a specialized all-optical processor designed to perform a quantitative phase-to-intensity transformation through passive diffractive surfaces that are spatially engineered using deep learning and image data. Forming a compact, all-optical network that axially extends only ~200-300 times the illumination wavelength, this framework can replace traditional QPI systems and related digital computational burden with a set of passive transmissive layers. All-optical diffractive QPI networks can potentially enable power-efficient, high frame-rate and compact phase imaging systems that might be useful for various applications, including, e.g., on-chip microscopy and sensing.



### Parallel Rectangle Flip Attack: A Query-based Black-box Attack against Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.08970v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08970v1)
- **Published**: 2022-01-22 06:00:17+00:00
- **Updated**: 2022-01-22 06:00:17+00:00
- **Authors**: Siyuan Liang, Baoyuan Wu, Yanbo Fan, Xingxing Wei, Xiaochun Cao
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Object detection has been widely used in many safety-critical tasks, such as autonomous driving. However, its vulnerability to adversarial examples has not been sufficiently studied, especially under the practical scenario of black-box attacks, where the attacker can only access the query feedback of predicted bounding-boxes and top-1 scores returned by the attacked model. Compared with black-box attack to image classification, there are two main challenges in black-box attack to detection. Firstly, even if one bounding-box is successfully attacked, another sub-optimal bounding-box may be detected near the attacked bounding-box. Secondly, there are multiple bounding-boxes, leading to very high attack cost. To address these challenges, we propose a Parallel Rectangle Flip Attack (PRFA) via random search. We explain the difference between our method with other attacks in Fig.~\ref{fig1}. Specifically, we generate perturbations in each rectangle patch to avoid sub-optimal detection near the attacked region. Besides, utilizing the observation that adversarial perturbations mainly locate around objects' contours and critical points under white-box attacks, the search space of attacked rectangles is reduced to improve the attack efficiency. Moreover, we develop a parallel mechanism of attacking multiple rectangles simultaneously to further accelerate the attack process. Extensive experiments demonstrate that our method can effectively and efficiently attack various popular object detectors, including anchor-based and anchor-free, and generate transferable adversarial examples.



### Semi-Supervised Adversarial Recognition of Refined Window Structures for Inverse Procedural Façade Modeling
- **Arxiv ID**: http://arxiv.org/abs/2201.08977v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08977v2)
- **Published**: 2022-01-22 06:34:48+00:00
- **Updated**: 2022-09-24 03:46:55+00:00
- **Authors**: Han Hu, Xinrong Liang, Yulin Ding, Qisen Shang, Bo Xu, Xuming Ge, Min Chen, Ruofei Zhong, Qing Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods are notoriously data-hungry, which requires a large number of labeled samples. Unfortunately, the large amount of interactive sample labeling efforts has dramatically hindered the application of deep learning methods, especially for 3D modeling tasks, which require heterogeneous samples. To alleviate the work of data annotation for learned 3D modeling of fa\c{c}ades, this paper proposed a semi-supervised adversarial recognition strategy embedded in inverse procedural modeling. Beginning with textured LOD-2 (Level-of-Details) models, we use the classical convolutional neural networks to recognize the types and estimate the parameters of windows from image patches. The window types and parameters are then assembled into procedural grammar. A simple procedural engine is built inside an existing 3D modeling software, producing fine-grained window geometries. To obtain a useful model from a few labeled samples, we leverage the generative adversarial network to train the feature extractor in a semi-supervised manner. The adversarial training strategy can also exploit unlabeled data to make the training phase more stable. Experiments using publicly available fa\c{c}ade image datasets reveal that the proposed training strategy can obtain about 10% improvement in classification accuracy and 50% improvement in parameter estimation under the same network structure. In addition, performance gains are more pronounced when testing against unseen data featuring different fa\c{c}ade styles.



### BBA-net: A bi-branch attention network for crowd counting
- **Arxiv ID**: http://arxiv.org/abs/2201.08983v1
- **DOI**: 10.1109/ICASSP40776.2020.9053955
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.08983v1)
- **Published**: 2022-01-22 07:30:52+00:00
- **Updated**: 2022-01-22 07:30:52+00:00
- **Authors**: Yi Hou, Chengyang Li, Fan Yang, Cong Ma, Liping Zhu, Yuan Li, Huizhu Jia, Xiaodong Xie
- **Comment**: None
- **Journal**: ICASSP 2020 - 2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)
- **Summary**: In the field of crowd counting, the current mainstream CNN-based regression methods simply extract the density information of pedestrians without finding the position of each person. This makes the output of the network often found to contain incorrect responses, which may erroneously estimate the total number and not conducive to the interpretation of the algorithm. To this end, we propose a Bi-Branch Attention Network (BBA-NET) for crowd counting, which has three innovation points. i) A two-branch architecture is used to estimate the density information and location information separately. ii) Attention mechanism is used to facilitate feature extraction, which can reduce false responses. iii) A new density map generation method combining geometric adaptation and Voronoi split is introduced. Our method can integrate the pedestrian's head and body information to enhance the feature expression ability of the density map. Extensive experiments performed on two public datasets show that our method achieves a lower crowd counting error compared to other state-of-the-art methods.



### Enhancing and Dissecting Crowd Counting By Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2201.08992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08992v1)
- **Published**: 2022-01-22 08:16:00+00:00
- **Updated**: 2022-01-22 08:16:00+00:00
- **Authors**: Yi Hou, Chengyang Li, Yuheng Lu, Liping Zhu, Yuan Li, Huizhu Jia, Xiaodong Xie
- **Comment**: None
- **Journal**: None
- **Summary**: In this article, we propose a simulated crowd counting dataset CrowdX, which has a large scale, accurate labeling, parameterized realization, and high fidelity. The experimental results of using this dataset as data enhancement show that the performance of the proposed streamlined and efficient benchmark network ESA-Net can be improved by 8.4\%. The other two classic heterogeneous architectures MCNN and CSRNet pre-trained on CrowdX also show significant performance improvements. Considering many influencing factors determine performance, such as background, camera angle, human density, and resolution. Although these factors are important, there is still a lack of research on how they affect crowd counting. Thanks to the CrowdX dataset with rich annotation information, we conduct a large number of data-driven comparative experiments to analyze these factors. Our research provides a reference for a deeper understanding of the crowd counting problem and puts forward some useful suggestions in the actual deployment of the algorithm.



### Blind Image Deblurring: a Review
- **Arxiv ID**: http://arxiv.org/abs/2201.10522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10522v1)
- **Published**: 2022-01-22 08:21:12+00:00
- **Updated**: 2022-01-22 08:21:12+00:00
- **Authors**: Zhengrong Xue
- **Comment**: None
- **Journal**: None
- **Summary**: This is a review on blind image deblurring. First, we formulate the blind image deblurring problem and explain why it is challenging. Next, we bring some psychological and cognitive studies on the way our human vision system deblurs. Then, relying on several previous reviews, we discuss the topic of metrics and datasets, which is non-trivial to blind deblurring. Finally, we introduce some typical optimization-based methods and learning-based methods.



### Linear Array Network for Low-light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2201.08996v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.08996v2)
- **Published**: 2022-01-22 08:44:02+00:00
- **Updated**: 2022-02-16 09:15:27+00:00
- **Authors**: Keqi Wang, Ziteng Cui, Jieru Jia, Hao Xu, Ge Wu, Yin Zhuang, Lu Chen, Zhiguo Hu, Yuhua Qian
- **Comment**: None
- **Journal**: None
- **Summary**: Convolution neural networks (CNNs) based methods have dominated the low-light image enhancement tasks due to their outstanding performance. However, the convolution operation is based on a local sliding window mechanism, which is difficult to construct the long-range dependencies of the feature maps. Meanwhile, the self-attention based global relationship aggregation methods have been widely used in computer vision, but these methods are difficult to handle high-resolution images because of the high computational complexity. To solve this problem, this paper proposes a Linear Array Self-attention (LASA) mechanism, which uses only two 2-D feature encodings to construct 3-D global weights and then refines feature maps generated by convolution layers. Based on LASA, Linear Array Network (LAN) is proposed, which is superior to the existing state-of-the-art (SOTA) methods in both RGB and RAW based low-light enhancement tasks with a smaller amount of parameters. The code is released in https://github.com/cuiziteng/LASA_enhancement.



### Content-aware Warping for View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2201.09023v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09023v3)
- **Published**: 2022-01-22 11:35:05+00:00
- **Updated**: 2023-01-30 03:04:42+00:00
- **Authors**: Mantang Guo, Junhui Hou, Jing Jin, Hui Liu, Huanqiang Zeng, Jiwen Lu
- **Comment**: arXiv admin note: text overlap with arXiv:2108.07408
- **Journal**: None
- **Summary**: Existing image-based rendering methods usually adopt depth-based image warping operation to synthesize novel views. In this paper, we reason the essential limitations of the traditional warping operation to be the limited neighborhood and only distance-based interpolation weights. To this end, we propose content-aware warping, which adaptively learns the interpolation weights for pixels of a relatively large neighborhood from their contextual information via a lightweight neural network. Based on this learnable warping module, we propose a new end-to-end learning-based framework for novel view synthesis from a set of input source views, in which two additional modules, namely confidence-based blending and feature-assistant spatial refinement, are naturally proposed to handle the occlusion issue and capture the spatial correlation among pixels of the synthesized view, respectively. Besides, we also propose a weight-smoothness loss term to regularize the network. Experimental results on light field datasets with wide baselines and multi-view datasets show that the proposed method significantly outperforms state-of-the-art methods both quantitatively and visually. The source code will be publicly available at https://github.com/MantangGuo/CW4VS.



### Inter-Semantic Domain Adversarial in Histopathological Images
- **Arxiv ID**: http://arxiv.org/abs/2201.09041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09041v1)
- **Published**: 2022-01-22 12:55:59+00:00
- **Updated**: 2022-01-22 12:55:59+00:00
- **Authors**: Nicolas Dumas, Valentin Derangère, Laurent Arnould, Sylvain Ladoire, Louis-Oscar Morel, Nathan Vinçon
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: In computer vision, data shift has proven to be a major barrier for safe and robust deep learning applications. In medical applications, histopathological images are often associated with data shift and they are hardly available. It is important to understand to what extent a model can be made robust against data shift using all available data. Here, we first show that domain adversarial methods can be very deleterious if they are wrongly used. We then use domain adversarial methods to transfer data shift invariance from one dataset to another dataset with different semantics and show that domain adversarial methods are efficient inter-semantically with similar performance than intra-semantical domain adversarial methods.



### Uncertainty-aware deep learning methods for robust diabetic retinopathy classification
- **Arxiv ID**: http://arxiv.org/abs/2201.09042v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09042v2)
- **Published**: 2022-01-22 13:00:58+00:00
- **Updated**: 2022-02-02 11:53:42+00:00
- **Authors**: Joel Jaskari, Jaakko Sahlsten, Theodoros Damoulas, Jeremias Knoblauch, Simo Särkkä, Leo Kärkkäinen, Kustaa Hietala, Kimmo Kaski
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic classification of diabetic retinopathy from retinal images has been widely studied using deep neural networks with impressive results. However, there is a clinical need for estimation of the uncertainty in the classifications, a shortcoming of modern neural networks. Recently, approximate Bayesian deep learning methods have been proposed for the task but the studies have only considered the binary referable/non-referable diabetic retinopathy classification applied to benchmark datasets. We present novel results by systematically investigating a clinical dataset and a clinically relevant 5-class classification scheme, in addition to benchmark datasets and the binary classification scheme. Moreover, we derive a connection between uncertainty measures and classifier risk, from which we develop a new uncertainty measure. We observe that the previously proposed entropy-based uncertainty measure generalizes to the clinical dataset on the binary classification scheme but not on the 5-class scheme, whereas our new uncertainty measure generalizes to the latter case.



### Phase-SLAM: Phase Based Simultaneous Localization and Mapping for Mobile Structured Light Illumination Systems
- **Arxiv ID**: http://arxiv.org/abs/2201.09048v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.09048v1)
- **Published**: 2022-01-22 13:47:06+00:00
- **Updated**: 2022-01-22 13:47:06+00:00
- **Authors**: Xi Zheng, Rui Ma, Rui Gao, Qi Hao
- **Comment**: None
- **Journal**: None
- **Summary**: Structured Light Illumination (SLI) systems have been used for reliable indoor dense 3D scanning via phase triangulation. However, mobile SLI systems for 360 degree 3D reconstruction demand 3D point cloud registration, involving high computational complexity. In this paper, we propose a phase based Simultaneous Localization and Mapping (Phase-SLAM) framework for fast and accurate SLI sensor pose estimation and 3D object reconstruction. The novelty of this work is threefold: (1) developing a reprojection model from 3D points to 2D phase data towards phase registration with low computational complexity; (2) developing a local optimizer to achieve SLI sensor pose estimation (odometry) using the derived Jacobian matrix for the 6 DoF variables; (3) developing a compressive phase comparison method to achieve high-efficiency loop closure detection. The whole Phase-SLAM pipeline is then exploited using existing global pose graph optimization techniques. We build datasets from both the unreal simulation platform and a robotic arm based SLI system in real-world to verify the proposed approach. The experiment results demonstrate that the proposed Phase-SLAM outperforms other state-of-the-art methods in terms of the efficiency and accuracy of pose estimation and 3D reconstruction. The open-source code is available at https://github.com/ZHENGXi-git/Phase-SLAM.



### LTC-SUM: Lightweight Client-driven Personalized Video Summarization Framework Using 2D CNN
- **Arxiv ID**: http://arxiv.org/abs/2201.09049v2
- **DOI**: 10.1109/ACCESS.2022.3209275
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.09049v2)
- **Published**: 2022-01-22 13:54:13+00:00
- **Updated**: 2022-10-05 02:30:26+00:00
- **Authors**: Ghulam Mujtaba, Adeel Malik, Eun-Seok Ryu
- **Comment**: 14
- **Journal**: in IEEE Access, vol. 10, pp. 103041-103055, 2022
- **Summary**: This paper proposes a novel lightweight thumbnail container-based summarization (LTC-SUM) framework for full feature-length videos. This framework generates a personalized keyshot summary for concurrent users by using the computational resource of the end-user device. State-of-the-art methods that acquire and process entire video data to generate video summaries are highly computationally intensive. In this regard, the proposed LTC-SUM method uses lightweight thumbnails to handle the complex process of detecting events. This significantly reduces computational complexity and improves communication and storage efficiency by resolving computational and privacy bottlenecks in resource-constrained end-user devices. These improvements were achieved by designing a lightweight 2D CNN model to extract features from thumbnails, which helped select and retrieve only a handful of specific segments. Extensive quantitative experiments on a set of full 18 feature-length videos (approximately 32.9 h in duration) showed that the proposed method is significantly computationally efficient than state-of-the-art methods on the same end-user device configurations. Joint qualitative assessments of the results of 56 participants showed that participants gave higher ratings to the summaries generated using the proposed method. To the best of our knowledge, this is the first attempt in designing a fully client-driven personalized keyshot video summarization framework using thumbnail containers for feature-length videos.



### Explore the Expression: Facial Expression Generation using Auxiliary Classifier Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2201.09061v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09061v2)
- **Published**: 2022-01-22 14:37:13+00:00
- **Updated**: 2022-02-08 06:11:27+00:00
- **Authors**: J. Rafid Siddiqui
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expressions are a form of non-verbal communication that humans perform seamlessly for meaningful transfer of information. Most of the literature addresses the facial expression recognition aspect however, with the advent of Generative Models, it has become possible to explore the affect space in addition to mere classification of a set of expressions. In this article, we propose a generative model architecture which robustly generates a set of facial expressions for multiple character identities and explores the possibilities of generating complex expressions by combining the simple ones.



### A Multi-modal Fusion Framework Based on Multi-task Correlation Learning for Cancer Prognosis Prediction
- **Arxiv ID**: http://arxiv.org/abs/2201.10353v1
- **DOI**: 10.1016/j.artmed.2022.102260
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.10353v1)
- **Published**: 2022-01-22 15:16:24+00:00
- **Updated**: 2022-01-22 15:16:24+00:00
- **Authors**: Kaiwen Tan, Weixian Huang, Xiaofeng Liu, Jinlong Hu, Shoubin Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Morphological attributes from histopathological images and molecular profiles from genomic data are important information to drive diagnosis, prognosis, and therapy of cancers. By integrating these heterogeneous but complementary data, many multi-modal methods are proposed to study the complex mechanisms of cancers, and most of them achieve comparable or better results from previous single-modal methods. However, these multi-modal methods are restricted to a single task (e.g., survival analysis or grade classification), and thus neglect the correlation between different tasks. In this study, we present a multi-modal fusion framework based on multi-task correlation learning (MultiCoFusion) for survival analysis and cancer grade classification, which combines the power of multiple modalities and multiple tasks. Specifically, a pre-trained ResNet-152 and a sparse graph convolutional network (SGCN) are used to learn the representations of histopathological images and mRNA expression data respectively. Then these representations are fused by a fully connected neural network (FCNN), which is also a multi-task shared network. Finally, the results of survival analysis and cancer grade classification output simultaneously. The framework is trained by an alternate scheme. We systematically evaluate our framework using glioma datasets from The Cancer Genome Atlas (TCGA). Results demonstrate that MultiCoFusion learns better representations than traditional feature extraction methods. With the help of multi-task alternating learning, even simple multi-modal concatenation can achieve better performance than other deep learning and traditional methods. Multi-task learning can improve the performance of multiple tasks not just one of them, and it is effective in both single-modal and multi-modal data.



### LTC-GIF: Attracting More Clicks on Feature-length Sports Videos
- **Arxiv ID**: http://arxiv.org/abs/2201.09077v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.09077v1)
- **Published**: 2022-01-22 15:34:10+00:00
- **Updated**: 2022-01-22 15:34:10+00:00
- **Authors**: Ghulam Mujtaba, Jaehyuk Choi, Eun-Seok Ryu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a lightweight method to attract users and increase views of the video by presenting personalized artistic media -- i.e, static thumbnails and animated GIFs. This method analyzes lightweight thumbnail containers (LTC) using computational resources of the client device to recognize personalized events from full-length sports videos. In addition, instead of processing the entire video, small video segments are processed to generate artistic media. This makes the proposed approach more computationally efficient compared to the baseline approaches that create artistic media using the entire video. The proposed method retrieves and uses thumbnail containers and video segments, which reduces the required transmission bandwidth as well as the amount of locally stored data used during artistic media generation. When extensive experiments were conducted on the Nvidia Jetson TX2, the computational complexity of the proposed method was 3.57 times lower than that of the SoA method. In the qualitative assessment, GIFs generated using the proposed method received 1.02 higher overall ratings compared to the SoA method. To the best of our knowledge, this is the first technique that uses LTC to generate artistic media while providing lightweight and high-performance services even on resource-constrained devices.



### Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension
- **Arxiv ID**: http://arxiv.org/abs/2201.09079v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09079v1)
- **Published**: 2022-01-22 15:36:03+00:00
- **Updated**: 2022-01-22 15:36:03+00:00
- **Authors**: Paris V. Giampouras, Benjamin D. Haeffele, René Vidal
- **Comment**: None
- **Journal**: None
- **Summary**: Robust subspace recovery (RSR) is a fundamental problem in robust representation learning. Here we focus on a recently proposed RSR method termed Dual Principal Component Pursuit (DPCP) approach, which aims to recover a basis of the orthogonal complement of the subspace and is amenable to handling subspaces of high relative dimension. Prior work has shown that DPCP can provably recover the correct subspace in the presence of outliers, as long as the true dimension of the subspace is known. We show that DPCP can provably solve RSR problems in the {\it unknown} subspace dimension regime, as long as orthogonality constraints -- adopted in previous DPCP formulations -- are relaxed and random initialization is used instead of spectral one. Namely, we propose a very simple algorithm based on running multiple instances of a projected sub-gradient descent method (PSGM), with each problem instance seeking to find one vector in the null space of the subspace. We theoretically prove that under mild conditions this approach will succeed with high probability. In particular, we show that 1) all of the problem instances will converge to a vector in the nullspace of the subspace and 2) the ensemble of problem instance solutions will be sufficiently diverse to fully span the nullspace of the subspace thus also revealing its true unknown codimension. We provide empirical results that corroborate our theoretical results and showcase the remarkable implicit rank regularization behavior of PSGM algorithm that allows us to perform RSR without being aware of the subspace dimension.



### A Comprehensive Study on Occlusion Invariant Face Recognition under Face Mask Occlusion
- **Arxiv ID**: http://arxiv.org/abs/2201.09089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09089v1)
- **Published**: 2022-01-22 16:09:20+00:00
- **Updated**: 2022-01-22 16:09:20+00:00
- **Authors**: Susith Hemathilaka, Achala Aponso
- **Comment**: None
- **Journal**: None
- **Summary**: The face mask is an essential sanitaryware in daily lives growing during the pandemic period and is a big threat to current face recognition systems. The masks destroy a lot of details in a large area of face, and it makes it difficult to recognize them even for humans. The evaluation report shows the difficulty well when recognizing masked faces. Rapid development and breakthrough of deep learning in the recent past have witnessed most promising results from face recognition algorithms. But they fail to perform far from satisfactory levels in the unconstrained environment during the challenges such as varying lighting conditions, low resolution, facial expressions, pose variation and occlusions. Facial occlusions are considered one of the most intractable problems. Especially when the occlusion occupies a large region of the face because it destroys lots of official features.



### Robust Unpaired Single Image Super-Resolution of Faces
- **Arxiv ID**: http://arxiv.org/abs/2201.09109v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09109v1)
- **Published**: 2022-01-22 18:17:50+00:00
- **Updated**: 2022-01-22 18:17:50+00:00
- **Authors**: Saurabh Goswami, Rajagopalan A. N
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: We propose an adversarial attack for facial class-specific Single Image Super-Resolution (SISR) methods. Existing attacks, such as the Fast Gradient Sign Method (FGSM) or the Projected Gradient Descent (PGD) method, are either fast but ineffective, or effective but prohibitively slow on these networks. By closely inspecting the surface that the MSE loss, used to train such networks, traces under varying degradations, we were able to identify its parameterizable property. We leverage this property to propose an adverasrial attack that is able to locate the optimum degradation (effective) without needing multiple gradient-ascent steps (fast). Our experiments show that the proposed method is able to achieve a better speed vs effectiveness trade-off than the state-of-theart adversarial attacks, such as FGSM and PGD, for the task of unpaired facial as well as class-specific SISR.



### Investigating the Potential of Auxiliary-Classifier GANs for Image Classification in Low Data Regimes
- **Arxiv ID**: http://arxiv.org/abs/2201.09120v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.5.4; I.5.1; I.4.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2201.09120v1)
- **Published**: 2022-01-22 19:33:16+00:00
- **Updated**: 2022-01-22 19:33:16+00:00
- **Authors**: Amil Dravid, Florian Schiffers, Yunan Wu, Oliver Cossairt, Aggelos K. Katsaggelos
- **Comment**: 4 pages content, 1 page references, 3 figures, 2 tables, to appear in
  ICASSP 2022
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have shown promise in augmenting datasets and boosting convolutional neural networks' (CNN) performance on image classification tasks. But they introduce more hyperparameters to tune as well as the need for additional time and computational power to train supplementary to the CNN. In this work, we examine the potential for Auxiliary-Classifier GANs (AC-GANs) as a 'one-stop-shop' architecture for image classification, particularly in low data regimes. Additionally, we explore modifications to the typical AC-GAN framework, changing the generator's latent space sampling scheme and employing a Wasserstein loss with gradient penalty to stabilize the simultaneous training of image synthesis and classification. Through experiments on images of varying resolutions and complexity, we demonstrate that AC-GANs show promise in image classification, achieving competitive performance with standard CNNs. These methods can be employed as an 'all-in-one' framework with particular utility in the absence of large amounts of training data.



### Artificial Intelligence for Suicide Assessment using Audiovisual Cues: A Review
- **Arxiv ID**: http://arxiv.org/abs/2201.09130v2
- **DOI**: 10.1007/s10462-022-10290-6
- **Categories**: **cs.AI**, cs.CV, cs.CY, cs.HC, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2201.09130v2)
- **Published**: 2022-01-22 21:17:19+00:00
- **Updated**: 2022-11-03 13:08:18+00:00
- **Authors**: Sahraoui Dhelim, Liming Chen, Huansheng Ning, Chris Nugent
- **Comment**: Manuscript submitted to Arificial Intelligence Reviews (2022)
- **Journal**: None
- **Summary**: Death by suicide is the seventh leading death cause worldwide. The recent advancement in Artificial Intelligence (AI), specifically AI applications in image and voice processing, has created a promising opportunity to revolutionize suicide risk assessment. Subsequently, we have witnessed fast-growing literature of research that applies AI to extract audiovisual non-verbal cues for mental illness assessment. However, the majority of the recent works focus on depression, despite the evident difference between depression symptoms and suicidal behavior and non-verbal cues. This paper reviews recent works that study suicide ideation and suicide behavior detection through audiovisual feature analysis, mainly suicidal voice/speech acoustic features analysis and suicidal visual cues. Automatic suicide assessment is a promising research direction that is still in the early stages. Accordingly, there is a lack of large datasets that can be used to train machine learning and deep learning models proven to be effective in other, similar tasks.



### MIDAS: Deep learning human action intention prediction from natural eye movement patterns
- **Arxiv ID**: http://arxiv.org/abs/2201.09135v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09135v1)
- **Published**: 2022-01-22 21:52:42+00:00
- **Updated**: 2022-01-22 21:52:42+00:00
- **Authors**: Paul Festor, Ali Shafti, Alex Harston, Michey Li, Pavel Orlov, A. Aldo Faisal
- **Comment**: None
- **Journal**: None
- **Summary**: Eye movements have long been studied as a window into the attentional mechanisms of the human brain and made accessible as novelty style human-machine interfaces. However, not everything that we gaze upon, is something we want to interact with; this is known as the Midas Touch problem for gaze interfaces. To overcome the Midas Touch problem, present interfaces tend not to rely on natural gaze cues, but rather use dwell time or gaze gestures. Here we present an entirely data-driven approach to decode human intention for object manipulation tasks based solely on natural gaze cues. We run data collection experiments where 16 participants are given manipulation and inspection tasks to be performed on various objects on a table in front of them. The subjects' eye movements are recorded using wearable eye-trackers allowing the participants to freely move their head and gaze upon the scene. We use our Semantic Fovea, a convolutional neural network model to obtain the objects in the scene and their relation to gaze traces at every frame. We then evaluate the data and examine several ways to model the classification task for intention prediction. Our evaluation shows that intention prediction is not a naive result of the data, but rather relies on non-linear temporal processing of gaze cues. We model the task as a time series classification problem and design a bidirectional Long-Short-Term-Memory (LSTM) network architecture to decode intentions. Our results show that we can decode human intention of motion purely from natural gaze cues and object relative position, with $91.9\%$ accuracy. Our work demonstrates the feasibility of natural gaze as a Zero-UI interface for human-machine interaction, i.e., users will only need to act naturally, and do not need to interact with the interface itself or deviate from their natural eye movement patterns.



### Dual-Flattening Transformers through Decomposed Row and Column Queries for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.09139v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09139v1)
- **Published**: 2022-01-22 22:38:15+00:00
- **Updated**: 2022-01-22 22:38:15+00:00
- **Authors**: Ying Wang, Chiuman Ho, Wenju Xu, Ziwei Xuan, Xudong Liu, Guo-Jun Qi
- **Comment**: None
- **Journal**: None
- **Summary**: It is critical to obtain high resolution features with long range dependency for dense prediction tasks such as semantic segmentation. To generate high-resolution output of size $H\times W$ from a low-resolution feature map of size $h\times w$ ($hw\ll HW$), a naive dense transformer incurs an intractable complexity of $\mathcal{O}(hwHW)$, limiting its application on high-resolution dense prediction. We propose a Dual-Flattening Transformer (DFlatFormer) to enable high-resolution output by reducing complexity to $\mathcal{O}(hw(H+W))$ that is multiple orders of magnitude smaller than the naive dense transformer. Decomposed queries are presented to retrieve row and column attentions tractably through separate transformers, and their outputs are combined to form a dense feature map at high resolution. To this end, the input sequence fed from an encoder is row-wise and column-wise flattened to align with decomposed queries by preserving their row and column structures, respectively. Row and column transformers also interact with each other to capture their mutual attentions with the spatial crossings between rows and columns. We also propose to perform attentions through efficient grouping and pooling to further reduce the model complexity. Extensive experiments on ADE20K and Cityscapes datasets demonstrate the superiority of the proposed dual-flattening transformer architecture with higher mIoUs.



### Background Invariant Classification on Infrared Imagery by Data Efficient Training and Reducing Bias in CNNs
- **Arxiv ID**: http://arxiv.org/abs/2201.09144v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09144v2)
- **Published**: 2022-01-22 23:29:42+00:00
- **Updated**: 2022-02-09 18:32:16+00:00
- **Authors**: Maliha Arif, Calvin Yong, Abhijit Mahalanobis
- **Comment**: Accepted in AAAI-22 Workshop
- **Journal**: None
- **Summary**: Even though convolutional neural networks can classify objects in images very accurately, it is well known that the attention of the network may not always be on the semantically important regions of the scene. It has been observed that networks often learn background textures which are not relevant to the object of interest. In turn this makes the networks susceptible to variations and changes in the background which negatively affect their performance. We propose a new two-step training procedure called split training to reduce this bias in CNNs on both Infrared imagery and RGB data. Our split training procedure has two steps: using MSE loss first train the layers of the network on images with background to match the activations of the same network when it is trained using images without background; then with these layers frozen, train the rest of the network with cross-entropy loss to classify the objects. Our training method outperforms the traditional training procedure in both a simple CNN architecture, and deep CNNs like VGG and Densenet which use lots of hardware resources, and learns to mimic human vision which focuses more on shape and structure than background with higher accuracy.



