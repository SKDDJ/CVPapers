# Arxiv Papers in cs.CV on 2022-01-01
### PatchTrack: Multiple Object Tracking Using Frame Patches
- **Arxiv ID**: http://arxiv.org/abs/2201.00080v1
- **DOI**: None
- **Categories**: **cs.CV**, ACM-class: I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2201.00080v1)
- **Published**: 2022-01-01 00:16:45+00:00
- **Updated**: 2022-01-01 00:16:45+00:00
- **Authors**: Xiaotong Chen, Seyed Mehdi Iranmanesh, Kuo-Chin Lien
- **Comment**: 11 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Object motion and object appearance are commonly used information in multiple object tracking (MOT) applications, either for associating detections across frames in tracking-by-detection methods or direct track predictions for joint-detection-and-tracking methods. However, not only are these two types of information often considered separately, but also they do not help optimize the usage of visual information from the current frame of interest directly. In this paper, we present PatchTrack, a Transformer-based joint-detection-and-tracking system that predicts tracks using patches of the current frame of interest. We use the Kalman filter to predict the locations of existing tracks in the current frame from the previous frame. Patches cropped from the predicted bounding boxes are sent to the Transformer decoder to infer new tracks. By utilizing both object motion and object appearance information encoded in patches, the proposed method pays more attention to where new tracks are more likely to occur. We show the effectiveness of PatchTrack on recent MOT benchmarks, including MOT16 (MOTA 73.71%, IDF1 65.77%) and MOT17 (MOTA 73.59%, IDF1 65.23%). The results are published on https://motchallenge.net/method/MOT=4725&chl=10.



### Performance Comparison of Deep Learning Architectures for Artifact Removal in Gastrointestinal Endoscopic Imaging
- **Arxiv ID**: http://arxiv.org/abs/2201.00084v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.00084v1)
- **Published**: 2022-01-01 01:04:51+00:00
- **Updated**: 2022-01-01 01:04:51+00:00
- **Authors**: Taira Watanabe, Kensuke Tanioka, Satoru Hiwa, Tomoyuki Hiroyasu
- **Comment**: None
- **Journal**: None
- **Summary**: Endoscopic images typically contain several artifacts. The artifacts significantly impact image analysis result in computer-aided diagnosis. Convolutional neural networks (CNNs), a type of deep learning, can removes such artifacts. Various architectures have been proposed for the CNNs, and the accuracy of artifact removal varies depending on the choice of architecture. Therefore, it is necessary to determine the artifact removal accuracy, depending on the selected architecture. In this study, we focus on endoscopic surgical instruments as artifacts, and determine and discuss the artifact removal accuracy using seven different CNN architectures.



### Computer Vision Based Parking Optimization System
- **Arxiv ID**: http://arxiv.org/abs/2201.00095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.00095v1)
- **Published**: 2022-01-01 02:35:49+00:00
- **Updated**: 2022-01-01 02:35:49+00:00
- **Authors**: Siddharth Chandrasekaran, Jeffrey Matthew Reginald, Wei Wang, Ting Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: An improvement in technology is linearly related to time and time-relevant problems. It has been seen that as time progresses, the number of problems humans face also increases. However, technology to resolve these problems tends to improve as well. One of the earliest existing problems which started with the invention of vehicles was parking. The ease of resolving this problem using technology has evolved over the years but the problem of parking still remains unsolved. The main reason behind this is that parking does not only involve one problem but it consists of a set of problems within itself. One of these problems is the occupancy detection of the parking slots in a distributed parking ecosystem. In a distributed system, users would find preferable parking spaces as opposed to random parking spaces. In this paper, we propose a web-based application as a solution for parking space detection in different parking spaces. The solution is based on Computer Vision (CV) and is built using the Django framework written in Python 3.0. The solution works to resolve the occupancy detection problem along with providing the user the option to determine the block based on availability and his preference. The evaluation results for our proposed system are promising and efficient. The proposed system can also be integrated with different systems and be used for solving other relevant parking problems.



### SalyPath360: Saliency and Scanpath Prediction Framework for Omnidirectional Images
- **Arxiv ID**: http://arxiv.org/abs/2201.00096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.00096v1)
- **Published**: 2022-01-01 02:37:33+00:00
- **Updated**: 2022-01-01 02:37:33+00:00
- **Authors**: Mohamed Amine Kerkouri, Marouane Tliba, Aladine Chetouani, Mohamed Sayeh
- **Comment**: Accepted at Electornic Imaging Sympotium 2022
- **Journal**: None
- **Summary**: This paper introduces a new framework to predict visual attention of omnidirectional images. The key setup of our architecture is the simultaneous prediction of the saliency map and a corresponding scanpath for a given stimulus. The framework implements a fully encoder-decoder convolutional neural network augmented by an attention module to generate representative saliency maps. In addition, an auxiliary network is employed to generate probable viewport center fixation points through the SoftArgMax function. The latter allows to derive fixation points from feature maps. To take advantage of the scanpath prediction, an adaptive joint probability distribution model is then applied to construct the final unbiased saliency map by leveraging the encoder decoder-based saliency map and the scanpath-based saliency heatmap. The proposed framework was evaluated in terms of saliency and scanpath prediction, and the results were compared to state-of-the-art methods on Salient360! dataset. The results showed the relevance of our framework and the benefits of such architecture for further omnidirectional visual attention prediction tasks.



### Adversarial Attack via Dual-Stage Network Erosion
- **Arxiv ID**: http://arxiv.org/abs/2201.00097v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00097v1)
- **Published**: 2022-01-01 02:38:09+00:00
- **Updated**: 2022-01-01 02:38:09+00:00
- **Authors**: Yexin Duan, Junhua Zou, Xingyu Zhou, Wu Zhang, Jin Zhang, Zhisong Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are vulnerable to adversarial examples, which can fool deep models by adding subtle perturbations. Although existing attacks have achieved promising results, it still leaves a long way to go for generating transferable adversarial examples under the black-box setting. To this end, this paper proposes to improve the transferability of adversarial examples, and applies dual-stage feature-level perturbations to an existing model to implicitly create a set of diverse models. Then these models are fused by the longitudinal ensemble during the iterations. The proposed method is termed Dual-Stage Network Erosion (DSNE). We conduct comprehensive experiments both on non-residual and residual networks, and obtain more transferable adversarial examples with the computational cost similar to the state-of-the-art method. In particular, for the residual networks, the transferability of the adversarial examples can be significantly improved by biasing the residual block information to the skip connections. Our work provides new insights into the architectural vulnerability of neural networks and presents new challenges to the robustness of neural networks.



### Boosting RGB-D Saliency Detection by Leveraging Unlabeled RGB Images
- **Arxiv ID**: http://arxiv.org/abs/2201.00100v1
- **DOI**: 10.1109/TIP.2021.3139232
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00100v1)
- **Published**: 2022-01-01 03:02:27+00:00
- **Updated**: 2022-01-01 03:02:27+00:00
- **Authors**: Xiaoqiang Wang, Lei Zhu, Siliang Tang, Huazhu Fu, Ping Li, Fei Wu, Yi Yang, Yueting Zhuang
- **Comment**: Accepted by IEEE TIP
- **Journal**: None
- **Summary**: Training deep models for RGB-D salient object detection (SOD) often requires a large number of labeled RGB-D images. However, RGB-D data is not easily acquired, which limits the development of RGB-D SOD techniques. To alleviate this issue, we present a Dual-Semi RGB-D Salient Object Detection Network (DS-Net) to leverage unlabeled RGB images for boosting RGB-D saliency detection. We first devise a depth decoupling convolutional neural network (DDCNN), which contains a depth estimation branch and a saliency detection branch. The depth estimation branch is trained with RGB-D images and then used to estimate the pseudo depth maps for all unlabeled RGB images to form the paired data. The saliency detection branch is used to fuse the RGB feature and depth feature to predict the RGB-D saliency. Then, the whole DDCNN is assigned as the backbone in a teacher-student framework for semi-supervised learning. Moreover, we also introduce a consistency loss on the intermediate attention and saliency maps for the unlabeled data, as well as a supervised depth and saliency loss for labeled data. Experimental results on seven widely-used benchmark datasets demonstrate that our DDCNN outperforms state-of-the-art methods both quantitatively and qualitatively. We also demonstrate that our semi-supervised DS-Net can further improve the performance, even when using an RGB image with the pseudo depth map.



### Robust Region Feature Synthesizer for Zero-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.00103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.00103v1)
- **Published**: 2022-01-01 03:09:15+00:00
- **Updated**: 2022-01-01 03:09:15+00:00
- **Authors**: Peiliang Huang, Junwei Han, De Cheng, Dingwen Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot object detection aims at incorporating class semantic vectors to realize the detection of (both seen and) unseen classes given an unconstrained test image. In this study, we reveal the core challenges in this research area: how to synthesize robust region features (for unseen objects) that are as intra-class diverse and inter-class separable as the real samples, so that strong unseen object detectors can be trained upon them. To address these challenges, we build a novel zero-shot object detection framework that contains an Intra-class Semantic Diverging component and an Inter-class Structure Preserving component. The former is used to realize the one-to-more mapping to obtain diverse visual features from each class semantic vector, preventing miss-classifying the real unseen objects as image backgrounds. While the latter is used to avoid the synthesized features too scattered to mix up the inter-class and foreground-background relationship. To demonstrate the effectiveness of the proposed approach, comprehensive experiments on PASCAL VOC, COCO, and DIOR datasets are conducted. Notably, our approach achieves the new state-of-the-art performance on PASCAL VOC and COCO and it is the first study to carry out zero-shot object detection in remote sensing imagery.



### Quality-aware Part Models for Occluded Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2201.00107v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.00107v1)
- **Published**: 2022-01-01 03:51:09+00:00
- **Updated**: 2022-01-01 03:51:09+00:00
- **Authors**: Pengfei Wang, Changxing Ding, Zhiyin Shao, Zhibin Hong, Shengli Zhang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Occlusion poses a major challenge for person re-identification (ReID). Existing approaches typically rely on outside tools to infer visible body parts, which may be suboptimal in terms of both computational efficiency and ReID accuracy. In particular, they may fail when facing complex occlusions, such as those between pedestrians. Accordingly, in this paper, we propose a novel method named Quality-aware Part Models (QPM) for occlusion-robust ReID. First, we propose to jointly learn part features and predict part quality scores. As no quality annotation is available, we introduce a strategy that automatically assigns low scores to occluded body parts, thereby weakening the impact of occluded body parts on ReID results. Second, based on the predicted part quality scores, we propose a novel identity-aware spatial attention (ISA) module. In this module, a coarse identity-aware feature is utilized to highlight pixels of the target pedestrian, so as to handle the occlusion between pedestrians. Third, we design an adaptive and efficient approach for generating global features from common non-occluded regions with respect to each image pair. This design is crucial, but is often ignored by existing methods. QPM has three key advantages: 1) it does not rely on any outside tools in either the training or inference stages; 2) it handles occlusions caused by both objects and other pedestrians;3) it is highly computationally efficient. Experimental results on four popular databases for occluded ReID demonstrate that QPM consistently outperforms state-of-the-art methods by significant margins. The code of QPM will be released.



### SurfGen: Adversarial 3D Shape Synthesis with Explicit Surface Discriminators
- **Arxiv ID**: http://arxiv.org/abs/2201.00112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.00112v1)
- **Published**: 2022-01-01 04:44:42+00:00
- **Updated**: 2022-01-01 04:44:42+00:00
- **Authors**: Andrew Luo, Tianqin Li, Wen-Hao Zhang, Tai Sing Lee
- **Comment**: ICCV 2021. Project page: https://github.com/aluo-x/NeuralRaycaster
- **Journal**: None
- **Summary**: Recent advances in deep generative models have led to immense progress in 3D shape synthesis. While existing models are able to synthesize shapes represented as voxels, point-clouds, or implicit functions, these methods only indirectly enforce the plausibility of the final 3D shape surface. Here we present a 3D shape synthesis framework (SurfGen) that directly applies adversarial training to the object surface. Our approach uses a differentiable spherical projection layer to capture and represent the explicit zero isosurface of an implicit 3D generator as functions defined on the unit sphere. By processing the spherical representation of 3D object surfaces with a spherical CNN in an adversarial setting, our generator can better learn the statistics of natural shape surfaces. We evaluate our model on large-scale shape datasets, and demonstrate that the end-to-end trained model is capable of generating high fidelity 3D shapes with diverse topology.



### SAFL: A Self-Attention Scene Text Recognizer with Focal Loss
- **Arxiv ID**: http://arxiv.org/abs/2201.00132v1
- **DOI**: 10.1109/ICMLA51294.2020.00223
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.00132v1)
- **Published**: 2022-01-01 06:51:03+00:00
- **Updated**: 2022-01-01 06:51:03+00:00
- **Authors**: Bao Hieu Tran, Thanh Le-Cong, Huu Manh Nguyen, Duc Anh Le, Thanh Hung Nguyen, Phi Le Nguyen
- **Comment**: Accepted to ICMLA 2020
- **Journal**: 2020 19th IEEE International Conference on Machine Learning and
  Applications (ICMLA)
- **Summary**: In the last decades, scene text recognition has gained worldwide attention from both the academic community and actual users due to its importance in a wide range of applications. Despite achievements in optical character recognition, scene text recognition remains challenging due to inherent problems such as distortions or irregular layout. Most of the existing approaches mainly leverage recurrence or convolution-based neural networks. However, while recurrent neural networks (RNNs) usually suffer from slow training speed due to sequential computation and encounter problems as vanishing gradient or bottleneck, CNN endures a trade-off between complexity and performance. In this paper, we introduce SAFL, a self-attention-based neural network model with the focal loss for scene text recognition, to overcome the limitation of the existing approaches. The use of focal loss instead of negative log-likelihood helps the model focus more on low-frequency samples training. Moreover, to deal with the distortions and irregular texts, we exploit Spatial TransformerNetwork (STN) to rectify text before passing to the recognition network. We perform experiments to compare the performance of the proposed model with seven benchmarks. The numerical results show that our model achieves the best performance.



### Rethinking Feature Uncertainty in Stochastic Neural Networks for Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2201.00148v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00148v1)
- **Published**: 2022-01-01 08:46:06+00:00
- **Updated**: 2022-01-01 08:46:06+00:00
- **Authors**: Hao Yang, Min Wang, Zhengfei Yu, Yun Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: It is well-known that deep neural networks (DNNs) have shown remarkable success in many fields. However, when adding an imperceptible magnitude perturbation on the model input, the model performance might get rapid decrease. To address this issue, a randomness technique has been proposed recently, named Stochastic Neural Networks (SNNs). Specifically, SNNs inject randomness into the model to defend against unseen attacks and improve the adversarial robustness. However, existed studies on SNNs mainly focus on injecting fixed or learnable noises to model weights/activations. In this paper, we find that the existed SNNs performances are largely bottlenecked by the feature representation ability. Surprisingly, simply maximizing the variance per dimension of the feature distribution leads to a considerable boost beyond all previous methods, which we named maximize feature distribution variance stochastic neural network (MFDV-SNN). Extensive experiments on well-known white- and black-box attacks show that MFDV-SNN achieves a significant improvement over existing methods, which indicates that it is a simple but effective method to improve model robustness.



### Adaptive Single Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2201.00155v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00155v1)
- **Published**: 2022-01-01 10:10:19+00:00
- **Updated**: 2022-01-01 10:10:19+00:00
- **Authors**: Maitreya Suin, Kuldeep Purohit, A. N. Rajagopalan
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2004.05343
- **Journal**: None
- **Summary**: This paper tackles the problem of dynamic scene deblurring. Although end-to-end fully convolutional designs have recently advanced the state-of-the-art in non-uniform motion deblurring, their performance-complexity trade-off is still sub-optimal. Existing approaches achieve a large receptive field by a simple increment in the number of generic convolution layers, kernel-size, which comes with the burden of the increase in model size and inference speed. In this work, we propose an efficient pixel adaptive and feature attentive design for handling large blur variations within and across different images. We also propose an effective content-aware global-local filtering module that significantly improves the performance by considering not only the global dependencies of the pixel but also dynamically using the neighboring pixels. We use a patch hierarchical attentive architecture composed of the above module that implicitly discover the spatial variations in the blur present in the input image and in turn perform local and global modulation of intermediate features. Extensive qualitative and quantitative comparisons with prior art on deblurring benchmarks demonstrate the superiority of the proposed network.



### Development of Diabetic Foot Ulcer Datasets: An Overview
- **Arxiv ID**: http://arxiv.org/abs/2201.00163v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00163v1)
- **Published**: 2022-01-01 10:45:07+00:00
- **Updated**: 2022-01-01 10:45:07+00:00
- **Authors**: Moi Hoon Yap, Connah Kendrick, Neil D. Reeves, Manu Goyal, Joseph M. Pappachan, Bill Cassidy
- **Comment**: Preprint (author copy) to be published in MICCAI DFUC2021 Proceedings
- **Journal**: None
- **Summary**: This paper provides conceptual foundation and procedures used in the development of diabetic foot ulcer datasets over the past decade, with a timeline to demonstrate progress. We conduct a survey on data capturing methods for foot photographs, an overview of research in developing private and public datasets, the related computer vision tasks (detection, segmentation and classification), the diabetic foot ulcer challenges and the future direction of the development of the datasets. We report the distribution of dataset users by country and year. Our aim is to share the technical challenges that we encountered together with good practices in dataset development, and provide motivation for other researchers to participate in data sharing in this domain.



### Self-attention Multi-view Representation Learning with Diversity-promoting Complementarity
- **Arxiv ID**: http://arxiv.org/abs/2201.00168v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00168v1)
- **Published**: 2022-01-01 11:17:02+00:00
- **Updated**: 2022-01-01 11:17:02+00:00
- **Authors**: Jian-wei Liu, Xi-hao Ding, Run-kun Lu, Xionglin Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-view learning attempts to generate a model with a better performance by exploiting the consensus and/or complementarity among multi-view data. However, in terms of complementarity, most existing approaches only can find representations with single complementarity rather than complementary information with diversity. In this paper, to utilize both complementarity and consistency simultaneously, give free rein to the potential of deep learning in grasping diversity-promoting complementarity for multi-view representation learning, we propose a novel supervised multi-view representation learning algorithm, called Self-Attention Multi-View network with Diversity-Promoting Complementarity (SAMVDPC), which exploits the consistency by a group of encoders, uses self-attention to find complementary information entailing diversity. Extensive experiments conducted on eight real-world datasets have demonstrated the effectiveness of our proposed method, and show its superiority over several baseline methods, which only consider single complementary information.



### Dynamic Scene Video Deblurring using Non-Local Attention
- **Arxiv ID**: http://arxiv.org/abs/2201.00169v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00169v1)
- **Published**: 2022-01-01 11:17:34+00:00
- **Updated**: 2022-01-01 11:17:34+00:00
- **Authors**: Maitreya Suin, A. N. Rajagopalan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper tackles the challenging problem of video deblurring. Most of the existing works depend on implicit or explicit alignment for temporal information fusion which either increase the computational cost or result in suboptimal performance due to wrong alignment. In this study, we propose a factorized spatio-temporal attention to perform non-local operations across space and time to fully utilize the available information without depending on alignment. It shows superior performance compared to existing fusion techniques while being much efficient. Extensive experiments on multiple datasets demonstrate the superiority of our method.



### Multi-view Subspace Adaptive Learning via Autoencoder and Attention
- **Arxiv ID**: http://arxiv.org/abs/2201.00171v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00171v1)
- **Published**: 2022-01-01 11:31:52+00:00
- **Updated**: 2022-01-01 11:31:52+00:00
- **Authors**: Jian-wei Liu, Hao-jie Xie, Run-kun Lu, Xiong-lin Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-view learning can cover all features of data samples more comprehensively, so multi-view learning has attracted widespread attention. Traditional subspace clustering methods, such as sparse subspace clustering (SSC) and low-ranking subspace clustering (LRSC), cluster the affinity matrix for a single view, thus ignoring the problem of fusion between views. In our article, we propose a new Multiview Subspace Adaptive Learning based on Attention and Autoencoder (MSALAA). This method combines a deep autoencoder and a method for aligning the self-representations of various views in Multi-view Low-Rank Sparse Subspace Clustering (MLRSSC), which can not only increase the capability to non-linearity fitting, but also can meets the principles of consistency and complementarity of multi-view learning. We empirically observe significant improvement over existing baseline methods on six real-life datasets.



### Adaptive Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2201.00177v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.00177v1)
- **Published**: 2022-01-01 12:16:01+00:00
- **Updated**: 2022-01-01 12:16:01+00:00
- **Authors**: Maitreya Suin, Kuldeep Purohit, A. N. Rajagopalan
- **Comment**: None
- **Journal**: None
- **Summary**: Image inpainting methods have shown significant improvements by using deep neural networks recently. However, many of these techniques often create distorted structures or blurry textures inconsistent with surrounding areas. The problem is rooted in the encoder layers' ineffectiveness in building a complete and faithful embedding of the missing regions. To address this problem, two-stage approaches deploy two separate networks for a coarse and fine estimate of the inpainted image. Some approaches utilize handcrafted features like edges or contours to guide the reconstruction process. These methods suffer from huge computational overheads owing to multiple generator networks, limited ability of handcrafted features, and sub-optimal utilization of the information present in the ground truth. Motivated by these observations, we propose a distillation based approach for inpainting, where we provide direct feature level supervision for the encoder layers in an adaptive manner. We deploy cross and self distillation techniques and discuss the need for a dedicated completion-block in encoder to achieve the distillation target. We conduct extensive evaluations on multiple datasets to validate our method.



### Image Restoration using Feature-guidance
- **Arxiv ID**: http://arxiv.org/abs/2201.00187v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.00187v1)
- **Published**: 2022-01-01 13:10:19+00:00
- **Updated**: 2022-01-01 13:10:19+00:00
- **Authors**: Maitreya Suin, Kuldeep Purohit, A. N. Rajagopalan
- **Comment**: None
- **Journal**: None
- **Summary**: Image restoration is the task of recovering a clean image from a degraded version. In most cases, the degradation is spatially varying, and it requires the restoration network to both localize and restore the affected regions. In this paper, we present a new approach suitable for handling the image-specific and spatially-varying nature of degradation in images affected by practically occurring artifacts such as blur, rain-streaks. We decompose the restoration task into two stages of degradation localization and degraded region-guided restoration, unlike existing methods which directly learn a mapping between the degraded and clean images. Our premise is to use the auxiliary task of degradation mask prediction to guide the restoration process. We demonstrate that the model trained for this auxiliary task contains vital region knowledge, which can be exploited to guide the restoration network's training using attentive knowledge distillation technique. Further, we propose mask-guided convolution and global context aggregation module that focuses solely on restoring the degraded regions. The proposed approach's effectiveness is demonstrated by achieving significant improvement over strong baselines.



### Turath-150K: Image Database of Arab Heritage
- **Arxiv ID**: http://arxiv.org/abs/2201.00220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.00220v1)
- **Published**: 2022-01-01 17:36:25+00:00
- **Updated**: 2022-01-01 17:36:25+00:00
- **Authors**: Dani Kiyasseh, Rasheed El-Bouri
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale image databases remain largely biased towards objects and activities encountered in a select few cultures. This absence of culturally-diverse images, which we refer to as the hidden tail, limits the applicability of pre-trained neural networks and inadvertently excludes researchers from under-represented regions. To begin remedying this issue, we curate Turath-150K, a database of images of the Arab world that reflect objects, activities, and scenarios commonly found there. In the process, we introduce three benchmark databases, Turath Standard, Art, and UNESCO, specialised subsets of the Turath dataset. After demonstrating the limitations of existing networks pre-trained on ImageNet when deployed on such benchmarks, we train and evaluate several networks on the task of image classification. As a consequence of Turath, we hope to engage machine learning researchers in under-represented regions, and to inspire the release of additional culture-focused databases. The database can be accessed here: danikiyasseh.github.io/Turath.



### Deep Learning Applications for Lung Cancer Diagnosis: A systematic review
- **Arxiv ID**: http://arxiv.org/abs/2201.00227v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.00227v1)
- **Published**: 2022-01-01 18:29:11+00:00
- **Updated**: 2022-01-01 18:29:11+00:00
- **Authors**: Hesamoddin Hosseini, Reza Monsefi, Shabnam Shadroo
- **Comment**: 32 pages, 14 figures
- **Journal**: None
- **Summary**: Lung cancer has been one of the most prevalent disease in recent years. According to the research of this field, more than 200,000 cases are identified each year in the US. Uncontrolled multiplication and growth of the lung cells result in malignant tumour formation. Recently, deep learning algorithms, especially Convolutional Neural Networks (CNN), have become a superior way to automatically diagnose disease. The purpose of this article is to review different models that lead to different accuracy and sensitivity in the diagnosis of early-stage lung cancer and to help physicians and researchers in this field. The main purpose of this work is to identify the challenges that exist in lung cancer based on deep learning. The survey is systematically written that combines regular mapping and literature review to review 32 conference and journal articles in the field from 2016 to 2021. After analysing and reviewing the articles, the questions raised in the articles are being answered. This research is superior to other review articles in this field due to the complete review of relevant articles and systematic write up.



### SporeAgent: Reinforced Scene-level Plausibility for Object Pose Refinement
- **Arxiv ID**: http://arxiv.org/abs/2201.00239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.00239v1)
- **Published**: 2022-01-01 20:26:19+00:00
- **Updated**: 2022-01-01 20:26:19+00:00
- **Authors**: Dominik Bauer, Timothy Patten, Markus Vincze
- **Comment**: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
  2022
- **Journal**: None
- **Summary**: Observational noise, inaccurate segmentation and ambiguity due to symmetry and occlusion lead to inaccurate object pose estimates. While depth- and RGB-based pose refinement approaches increase the accuracy of the resulting pose estimates, they are susceptible to ambiguity in the observation as they consider visual alignment. We propose to leverage the fact that we often observe static, rigid scenes. Thus, the objects therein need to be under physically plausible poses. We show that considering plausibility reduces ambiguity and, in consequence, allows poses to be more accurately predicted in cluttered environments. To this end, we extend a recent RL-based registration approach towards iterative refinement of object poses. Experiments on the LINEMOD and YCB-VIDEO datasets demonstrate the state-of-the-art performance of our depth-based refinement approach.



### Subspace modeling for fast and high-sensitivity X-ray chemical imaging
- **Arxiv ID**: http://arxiv.org/abs/2201.00259v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.00259v1)
- **Published**: 2022-01-01 23:26:06+00:00
- **Updated**: 2022-01-01 23:26:06+00:00
- **Authors**: Jizhou Li, Bin Chen, Guibin Zan, Guannan Qian, Piero Pianetta, Yijin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Resolving morphological chemical phase transformations at the nanoscale is of vital importance to many scientific and industrial applications across various disciplines. The TXM-XANES imaging technique, by combining full field transmission X-ray microscopy (TXM) and X-ray absorption near edge structure (XANES), has been an emerging tool which operates by acquiring a series of microscopy images with multi-energy X-rays and fitting to obtain the chemical map. Its capability, however, is limited by the poor signal-to-noise ratios due to the system errors and low exposure illuminations for fast acquisition. In this work, by exploiting the intrinsic properties and subspace modeling of the TXM-XANES imaging data, we introduce a simple and robust denoising approach to improve the image quality, which enables fast and high-sensitivity chemical imaging. Extensive experiments on both synthetic and real datasets demonstrate the superior performance of the proposed method.



