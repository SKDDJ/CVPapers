# Arxiv Papers in cs.CV on 2022-01-14
### Deep Leaning-Based Ultra-Fast Stair Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.05275v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05275v2)
- **Published**: 2022-01-14 02:05:01+00:00
- **Updated**: 2022-02-04 09:16:05+00:00
- **Authors**: Chen Wang, Zhongcai Pei, Shuang Qiu, Zhiyong Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Staircases are some of the most common building structures in urban environments. Stair detection is an important task for various applications, including the environmental perception of exoskeleton robots, humanoid robots, and rescue robots and the navigation of visually impaired people. Most existing stair detection algorithms have difficulty dealing with the diversity of stair structure materials, extreme light and serious occlusion. Inspired by human perception, we propose an end-to-end method based on deep learning. Specifically, we treat the process of stair line detection as a multitask involving coarse-grained semantic segmentation and object detection. The input images are divided into cells, and a simple neural network is used to judge whether each cell contains stair lines. For cells containing stair lines, the locations of the stair lines relative to each cell are regressed. Extensive experiments on our dataset show that our method can achieve high performance in terms of both speed and accuracy. A lightweight version can even achieve 300+ frames per second with the same resolution. Our code and dataset will be soon available at GitHub.



### Boundary-aware Self-supervised Learning for Video Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.05277v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05277v1)
- **Published**: 2022-01-14 02:14:07+00:00
- **Updated**: 2022-01-14 02:14:07+00:00
- **Authors**: Jonghwan Mun, Minchul Shin, Gunsoo Han, Sangho Lee, Seongsu Ha, Joonseok Lee, Eun-Sol Kim
- **Comment**: The code is available at https://github.com/kakaobrain/bassl
- **Journal**: None
- **Summary**: Self-supervised learning has drawn attention through its effectiveness in learning in-domain representations with no ground-truth annotations; in particular, it is shown that properly designed pretext tasks (e.g., contrastive prediction task) bring significant performance gains for downstream tasks (e.g., classification task). Inspired from this, we tackle video scene segmentation, which is a task of temporally localizing scene boundaries in a video, with a self-supervised learning framework where we mainly focus on designing effective pretext tasks. In our framework, we discover a pseudo-boundary from a sequence of shots by splitting it into two continuous, non-overlapping sub-sequences and leverage the pseudo-boundary to facilitate the pre-training. Based on this, we introduce three novel boundary-aware pretext tasks: 1) Shot-Scene Matching (SSM), 2) Contextual Group Matching (CGM) and 3) Pseudo-boundary Prediction (PP); SSM and CGM guide the model to maximize intra-scene similarity and inter-scene discrimination while PP encourages the model to identify transitional moments. Through comprehensive analysis, we empirically show that pre-training and transferring contextual representation are both critical to improving the video scene segmentation performance. Lastly, we achieve the new state-of-the-art on the MovieNet-SSeg benchmark. The code is available at https://github.com/kakaobrain/bassl.



### Manifoldron: Direct Space Partition via Manifold Discovery
- **Arxiv ID**: http://arxiv.org/abs/2201.05279v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.05279v2)
- **Published**: 2022-01-14 02:28:17+00:00
- **Updated**: 2022-05-02 23:59:22+00:00
- **Authors**: Dayang Wang, Feng-Lei Fan, Bo-Jian Hou, Hao Zhang, Zhen Jia, Boce Zhou, Rongjie Lai, Hengyong Yu, Fei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: A neural network with the widely-used ReLU activation has been shown to partition the sample space into many convex polytopes for prediction. However, the parameterized way a neural network and other machine learning models use to partition the space has imperfections, \textit{e}.\textit{g}., the compromised interpretability for complex models, the inflexibility in decision boundary construction due to the generic character of the model, and the risk of being trapped into shortcut solutions. In contrast, although the non-parameterized models can adorably avoid or downplay these issues, they are usually insufficiently powerful either due to over-simplification or the failure to accommodate the manifold structures of data. In this context, we first propose a new type of machine learning models referred to as Manifoldron that directly derives decision boundaries from data and partitions the space via manifold structure discovery. Then, we systematically analyze the key characteristics of the Manifoldron such as manifold characterization capability and its link to neural networks. The experimental results on 4 synthetic examples, 20 public benchmark datasets, and 1 real-world application demonstrate that the proposed Manifoldron performs competitively compared to the mainstream machine learning models. We have shared our code in \url{https://github.com/wdayang/Manifoldron} for free download and evaluation.



### Argus++: Robust Real-time Activity Detection for Unconstrained Video Streams with Overlapping Cube Proposals
- **Arxiv ID**: http://arxiv.org/abs/2201.05290v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.05290v1)
- **Published**: 2022-01-14 03:35:22+00:00
- **Updated**: 2022-01-14 03:35:22+00:00
- **Authors**: Lijun Yu, Yijun Qian, Wenhe Liu, Alexander G. Hauptmann
- **Comment**: None
- **Journal**: None
- **Summary**: Activity detection is one of the attractive computer vision tasks to exploit the video streams captured by widely installed cameras. Although achieving impressive performance, conventional activity detection algorithms are usually designed under certain constraints, such as using trimmed and/or object-centered video clips as inputs. Therefore, they failed to deal with the multi-scale multi-instance cases in real-world unconstrained video streams, which are untrimmed and have large field-of-views. Real-time requirements for streaming analysis also mark brute force expansion of them unfeasible.   To overcome these issues, we propose Argus++, a robust real-time activity detection system for analyzing unconstrained video streams. The design of Argus++ introduces overlapping spatio-temporal cubes as an intermediate concept of activity proposals to ensure coverage and completeness of activity detection through over-sampling. The overall system is optimized for real-time processing on standalone consumer-level hardware. Extensive experiments on different surveillance and driving scenarios demonstrated its superior performance in a series of activity detection benchmarks, including CVPR ActivityNet ActEV 2021, NIST ActEV SDL UF/KF, TRECVID ActEV 2020/2021, and ICCV ROAD 2021.



### MMNet: Muscle motion-guided network for micro-expression recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.05297v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05297v2)
- **Published**: 2022-01-14 04:05:49+00:00
- **Updated**: 2022-08-19 11:24:19+00:00
- **Authors**: Hanting Li, Mingzhe Sui, Zhaoqing Zhu, Feng Zhao
- **Comment**: 8 pages, 4 figures
- **Journal**: Proc. 31st Int'l Joint Conf. Artificial Intelligence (IJCAI), 2022
- **Summary**: Facial micro-expressions (MEs) are involuntary facial motions revealing peoples real feelings and play an important role in the early intervention of mental illness, the national security, and many human-computer interaction systems. However, existing micro-expression datasets are limited and usually pose some challenges for training good classifiers. To model the subtle facial muscle motions, we propose a robust micro-expression recognition (MER) framework, namely muscle motion-guided network (MMNet). Specifically, a continuous attention (CA) block is introduced to focus on modeling local subtle muscle motion patterns with little identity information, which is different from most previous methods that directly extract features from complete video frames with much identity information. Besides, we design a position calibration (PC) module based on the vision transformer. By adding the position embeddings of the face generated by PC module at the end of the two branches, the PC module can help to add position information to facial muscle motion pattern features for the MER. Extensive experiments on three public micro-expression datasets demonstrate that our approach outperforms state-of-the-art methods by a large margin.



### A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2201.05299v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2201.05299v1)
- **Published**: 2022-01-14 04:12:46+00:00
- **Updated**: 2022-01-14 04:12:46+00:00
- **Authors**: Feng Gao, Qing Ping, Govind Thattai, Aishwarya Reganti, Ying Nian Wu, Prem Natarajan
- **Comment**: None
- **Journal**: None
- **Summary**: Outside-knowledge visual question answering (OK-VQA) requires the agent to comprehend the image, make use of relevant knowledge from the entire web, and digest all the information to answer the question. Most previous works address the problem by first fusing the image and question in the multi-modal space, which is inflexible for further fusion with a vast amount of external knowledge. In this paper, we call for a paradigm shift for the OK-VQA task, which transforms the image into plain text, so that we can enable knowledge passage retrieval, and generative question-answering in the natural language space. This paradigm takes advantage of the sheer volume of gigantic knowledge bases and the richness of pre-trained language models. A Transform-Retrieve-Generate framework (TRiG) framework is proposed, which can be plug-and-played with alternative image-to-text models and textual knowledge bases. Experimental results show that our TRiG framework outperforms all state-of-the-art supervised methods by at least 11.1% absolute margin.



### Unsupervised Temporal Video Grounding with Deep Semantic Clustering
- **Arxiv ID**: http://arxiv.org/abs/2201.05307v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.05307v1)
- **Published**: 2022-01-14 05:16:33+00:00
- **Updated**: 2022-01-14 05:16:33+00:00
- **Authors**: Daizong Liu, Xiaoye Qu, Yinzhen Wang, Xing Di, Kai Zou, Yu Cheng, Zichuan Xu, Pan Zhou
- **Comment**: Accepted by AAAI2022
- **Journal**: None
- **Summary**: Temporal video grounding (TVG) aims to localize a target segment in a video according to a given sentence query. Though respectable works have made decent achievements in this task, they severely rely on abundant video-query paired data, which is expensive and time-consuming to collect in real-world scenarios. In this paper, we explore whether a video grounding model can be learned without any paired annotations. To the best of our knowledge, this paper is the first work trying to address TVG in an unsupervised setting. Considering there is no paired supervision, we propose a novel Deep Semantic Clustering Network (DSCNet) to leverage all semantic information from the whole query set to compose the possible activity in each video for grounding. Specifically, we first develop a language semantic mining module, which extracts implicit semantic features from the whole query set. Then, these language semantic features serve as the guidance to compose the activity in video via a video-based semantic aggregation module. Finally, we utilize a foreground attention branch to filter out the redundant background activities and refine the grounding results. To validate the effectiveness of our DSCNet, we conduct experiments on both ActivityNet Captions and Charades-STA datasets. The results demonstrate that DSCNet achieves competitive performance, and even outperforms most weakly-supervised approaches.



### A Novel Skeleton-Based Human Activity Discovery Using Particle Swarm Optimization with Gaussian Mutation
- **Arxiv ID**: http://arxiv.org/abs/2201.05314v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.05314v4)
- **Published**: 2022-01-14 06:28:38+00:00
- **Updated**: 2022-10-18 09:25:21+00:00
- **Authors**: Parham Hadikhani, Daphne Teck Ching Lai, Wee-Hong Ong
- **Comment**: None
- **Journal**: None
- **Summary**: Human activity discovery aims to cluster the activities performed by humans without any prior information on what defines each activity. Most methods presented in human activity recognition are supervised, where there are labeled inputs to train the system. In reality, it is difficult to label activities data because of its huge volume and the variety of human activities. This paper proposes an unsupervised framework to perform human activity discovery in 3D skeleton sequences. First, an approach for data pre-processing is presented. In this stage, important frames are selected based on kinetic energy. Next, the displacement of joints, statistical displacements, angles, and orientation features are extracted to represent the activities information. Since not all extracted features have useful information, the dimension of features is reduced using PCA. Most methods proposed for human activity discovery are not fully unsupervised. They use pre-segmented videos before categorizing activities. To deal with this, we have used a sliding time window to segment the time series of activities with some overlapping. Then, activities are discovered by our proposed Hybrid Particle swarm optimization (PSO) with Gaussian Mutation and K-means (HPGMK) algorithm to provide diverse solutions. PSO is used due to its straightforward idea and powerful global search capability which can identify the ideal solution in a few iterations. Finally, k-means is applied to the outcome centroids from each iteration of the PSO to overcome the slow convergence rate of PSO. The experiment results on five datasets show that the proposed framework has superior performance in discovering activities compared to the other state-of-the-art methods and has increased accuracy of at least 4% on average.



### Learning from One and Only One Shot
- **Arxiv ID**: http://arxiv.org/abs/2201.08815v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.08815v1)
- **Published**: 2022-01-14 08:11:21+00:00
- **Updated**: 2022-01-14 08:11:21+00:00
- **Authors**: Haizi Yu, Igor Mineyev, Lav R. Varshney, James A. Evans
- **Comment**: None
- **Journal**: None
- **Summary**: Humans can generalize from only a few examples and from little pre-training on similar tasks. Yet, machine learning (ML) typically requires large data to learn or pre-learn to transfer. Inspired by nativism, we directly model basic human-innate priors in abstract visual tasks e.g., character/doodle recognition. This yields a white-box model that learns general-appearance similarity -- how any two images look in general -- by mimicking how humans naturally "distort" an object at first sight. Using simply the nearest-neighbor classifier on this similarity space, we achieve human-level character recognition using only 1--10 examples per class and nothing else (no pre-training). This differs from few-shot learning (FSL) using significant pre-training. On standard benchmarks MNIST/EMNIST and the Omniglot challenge, we outperform both neural-network-based and classical ML in the "tiny-data" regime, including FSL pre-trained on large data. Our model enables unsupervised learning too: by learning the non-Euclidean, general-appearance similarity space in a k-means style, we can generate human-intuitive archetypes as cluster ``centroids''.



### Semi-automated Virtual Unfolded View Generation Method of Stomach from CT Volumes
- **Arxiv ID**: http://arxiv.org/abs/2201.05331v1
- **DOI**: 10.1007/978-3-642-40811-3_42
- **Categories**: **eess.IV**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2201.05331v1)
- **Published**: 2022-01-14 08:13:04+00:00
- **Updated**: 2022-01-14 08:13:04+00:00
- **Authors**: Masahiro Oda, Tomoaki Suito, Yuichiro Hayashi, Takayuki Kitasaka, Kazuhiro Furukawa, Ryoji Miyahara, Yoshiki Hirooka, Hidemi Goto, Gen Iinuma, Kazunari Misawa, Shigeru Nawano, Kensaku Mori
- **Comment**: Accepted paper as a poster presentation at MICCAI 2013 (International
  Conference on Medical Image Computing and Computer-Assisted Intervention),
  Nagoya, Japan
- **Journal**: Published in Proceedings of MICCAI 2013, LNCS 8149, pp.332-339,
  2013
- **Summary**: CT image-based diagnosis of the stomach is developed as a new way of diagnostic method. A virtual unfolded (VU) view is suitable for displaying its wall. In this paper, we propose a semi-automated method for generating VU views of the stomach. Our method requires minimum manual operations. The determination of the unfolding forces and the termination of the unfolding process are automated. The unfolded shape of the stomach is estimated based on its radius. The unfolding forces are determined so that the stomach wall is deformed to the expected shape. The iterative deformation process is terminated if the difference of the shapes between the deformed shape and expected shape is small. Our experiments using 67 CT volumes showed that our proposed method can generate good VU views for 76.1% cases.



### AWSnet: An Auto-weighted Supervision Attention Network for Myocardial Scar and Edema Segmentation in Multi-sequence Cardiac Magnetic Resonance Images
- **Arxiv ID**: http://arxiv.org/abs/2201.05344v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.05344v1)
- **Published**: 2022-01-14 08:59:54+00:00
- **Updated**: 2022-01-14 08:59:54+00:00
- **Authors**: Kai-Ni Wang, Xin Yang, Juzheng Miao, Lei Li, Jing Yao, Ping Zhou, Wufeng Xue, Guang-Quan Zhou, Xiahai Zhuang, Dong Ni
- **Comment**: 19 pages, 10 figures, accepted by Medical Image Analysis
- **Journal**: None
- **Summary**: Multi-sequence cardiac magnetic resonance (CMR) provides essential pathology information (scar and edema) to diagnose myocardial infarction. However, automatic pathology segmentation can be challenging due to the difficulty of effectively exploring the underlying information from the multi-sequence CMR data. This paper aims to tackle the scar and edema segmentation from multi-sequence CMR with a novel auto-weighted supervision framework, where the interactions among different supervised layers are explored under a task-specific objective using reinforcement learning. Furthermore, we design a coarse-to-fine framework to boost the small myocardial pathology region segmentation with shape prior knowledge. The coarse segmentation model identifies the left ventricle myocardial structure as a shape prior, while the fine segmentation model integrates a pixel-wise attention strategy with an auto-weighted supervision model to learn and extract salient pathological structures from the multi-sequence CMR data. Extensive experimental results on a publicly available dataset from Myocardial pathology segmentation combining multi-sequence CMR (MyoPS 2020) demonstrate our method can achieve promising performance compared with other state-of-the-art methods. Our method is promising in advancing the myocardial pathology assessment on multi-sequence CMR data. To motivate the community, we have made our code publicly available via https://github.com/soleilssss/AWSnet/tree/master.



### Arbitrary Handwriting Image Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2201.05346v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05346v4)
- **Published**: 2022-01-14 09:00:55+00:00
- **Updated**: 2022-03-31 14:51:47+00:00
- **Authors**: Kai Yang, Xiaoman Liang, Huihuang Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposed a method to imitate handwriting style by style transfer. We proposed an neural network model based on conditional generative adversarial networks (cGAN) for handwriting style transfer. This paper improved the loss function on the basis of the GAN. Compared with other handwriting imitation methods, the handwriting style transfer's effect and efficiency have been significantly improved. The experiments showed that the shape of the generated Chinese characters is clear and the analysis of experimental data showed the Generative adversarial networks showed excellent performance in handwriting style transfer. The generated text image is closer to the real handwriting and achieved a better performance in term of handwriting imitation.



### A New Deep Hybrid Boosted and Ensemble Learning-based Brain Tumor Analysis using MRI
- **Arxiv ID**: http://arxiv.org/abs/2201.05373v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.05373v2)
- **Published**: 2022-01-14 10:24:47+00:00
- **Updated**: 2022-02-11 11:48:36+00:00
- **Authors**: Mirza Mumtaz Zahoor, Shahzad Ahmad Qureshi, Saddam Hussain Khan, Asifullah Khan
- **Comment**: 26 pages, 9 figures, 8 tables
- **Journal**: None
- **Summary**: Brain tumors analysis is important in timely diagnosis and effective treatment to cure patients. Tumor analysis is challenging because of tumor morphology like size, location, texture, and heteromorphic appearance in the medical images. In this regard, a novel two-phase deep learning-based framework is proposed to detect and categorize brain tumors in magnetic resonance images (MRIs). In the first phase, a novel deep boosted features and ensemble classifiers (DBF-EC) scheme is proposed to detect tumor MRI images from healthy individuals effectively. The deep boosted feature space is achieved through the customized and well-performing deep convolutional neural networks (CNNs), and consequently, fed into the ensemble of machine learning (ML) classifiers. While in the second phase, a new hybrid features fusion-based brain tumor classification approach is proposed, comprised of dynamic-static feature and ML classifier to categorize different tumor types. The dynamic features are extracted from the proposed BRAIN-RENet CNN, which carefully learns heteromorphic and inconsistent behavior of various tumors, while the static features are extracted using HOG. The effectiveness of the proposed two-phase brain tumor analysis framework is validated on two standard benchmark datasets; collected from Kaggle and Figshare containing different types of tumor, including glioma, meningioma, pituitary, and normal images. Experimental results proved that the proposed DBF-EC detection scheme outperforms and achieved accuracy (99.56%), precision (0.9991), recall (0.9899), F1-Score (0.9945), MCC (0.9892), and AUC-PR (0.9990). While the classification scheme, the joint employment of the deep features fusion of proposed BRAIN-RENet and HOG features improves performance significantly in terms of recall (0.9913), precision (0.9906), F1-Score (0.9909), and accuracy (99.20%) on diverse datasets.



### SRVIO: Super Robust Visual Inertial Odometry for dynamic environments and challenging Loop-closure conditions
- **Arxiv ID**: http://arxiv.org/abs/2201.05386v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05386v2)
- **Published**: 2022-01-14 10:52:04+00:00
- **Updated**: 2022-04-20 23:49:18+00:00
- **Authors**: Ali Samadzadeh, Ahmad Nickabadi
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: There has been extensive research on visual localization and odometry for autonomous robots and virtual reality during the past decades. Traditionally, this problem has been solved with the help of expensive sensors, such as lidars. Nowadays, the focus of the leading research in this field is on robust localization using more economic sensors, such as cameras and IMUs. Consequently, geometric visual localization methods have become more accurate in time. However, these methods still suffer from significant loss and divergence in challenging environments, such as a room full of moving people. Scientists started using deep neural networks (DNNs) to mitigate this problem. The main idea behind using DNNs is to better understand challenging aspects of the data and overcome complex conditions such as the movement of a dynamic object in front of the camera that covers the full view of the camera, extreme lighting conditions, and high speed of the camera. Prior end-to-end DNN methods have overcome some of these challenges. However, no general and robust framework is available to overcome all challenges together. In this paper, we have combined geometric and DNN-based methods to have the generality and speed of geometric SLAM frameworks and overcome most of these challenging conditions with the help of DNNs and deliver the most robust framework so far. To do so, we have designed a framework based on Vins-Mono, and show that it is able to achieve state-of-the-art results on TUM-Dynamic, TUM-VI, ADVIO, and EuRoC datasets compared to geometric and end-to-end DNN based SLAMs. Our proposed framework could also achieve outstanding results on extreme simulated cases resembling the aforementioned challenges.



### Skyline variations allow estimating distance to trees on landscape photos using semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.08816v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2201.08816v1)
- **Published**: 2022-01-14 12:31:02+00:00
- **Updated**: 2022-01-14 12:31:02+00:00
- **Authors**: Laura Martinez-Sanchez, Daniele Borio, Raphaël d'Andrimont, Marijn van der Velde
- **Comment**: None
- **Journal**: None
- **Summary**: Approximate distance estimation can be used to determine fundamental landscape properties including complexity and openness. We show that variations in the skyline of landscape photos can be used to estimate distances to trees on the horizon. A methodology based on the variations of the skyline has been developed and used to investigate potential relationships with the distance to skyline objects. The skyline signal, defined by the skyline height expressed in pixels, was extracted for several Land Use/Cover Area frame Survey (LUCAS) landscape photos. Photos were semantically segmented with DeepLabV3+ trained with the Common Objects in Context (COCO) dataset. This provided pixel-level classification of the objects forming the skyline. A Conditional Random Fields (CRF) algorithm was also applied to increase the details of the skyline signal. Three metrics, able to capture the skyline signal variations, were then considered for the analysis. These metrics shows a functional relationship with distance for the class of trees, whose contours have a fractal nature. In particular, regression analysis was performed against 475 ortho-photo based distance measurements, and, in the best case, a R2 score equal to 0.47 was achieved. This is an encouraging result which shows the potential of skyline variation metrics for inferring distance related information.



### HardBoost: Boosting Zero-Shot Learning with Hard Classes
- **Arxiv ID**: http://arxiv.org/abs/2201.05479v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05479v1)
- **Published**: 2022-01-14 14:33:48+00:00
- **Updated**: 2022-01-14 14:33:48+00:00
- **Authors**: Bo Liu, Lihua Hu, Zhanyi Hu, Qiulei Dong
- **Comment**: 15 pages, 8 figures, submitted to IEEE Transactions on Pattern
  Analysis and Machine Intelligence on Sep.16 2021, This work is an extended
  version of our CVPR2021 work----Hardness sampling for self-training based
  transductive zero-shot learning (arXiv:2106.00264)
- **Journal**: None
- **Summary**: This work is a systematical analysis on the so-called hard class problem in zero-shot learning (ZSL), that is, some unseen classes disproportionally affect the ZSL performances than others, as well as how to remedy the problem by detecting and exploiting hard classes. At first, we report our empirical finding that the hard class problem is a ubiquitous phenomenon and persists regardless of used specific methods in ZSL. Then, we find that high semantic affinity among unseen classes is a plausible underlying cause of hardness and design two metrics to detect hard classes. Finally, two frameworks are proposed to remedy the problem by detecting and exploiting hard classes, one under inductive setting, the other under transductive setting. The proposed frameworks could accommodate most existing ZSL methods to further significantly boost their performances with little efforts. Extensive experiments on three popular benchmarks demonstrate the benefits by identifying and exploiting the hard classes in ZSL.



### Emergence of Machine Language: Towards Symbolic Intelligence with Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.05489v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.05489v1)
- **Published**: 2022-01-14 14:54:58+00:00
- **Updated**: 2022-01-14 14:54:58+00:00
- **Authors**: Yuqi Wang, Xu-Yao Zhang, Cheng-Lin Liu, Zhaoxiang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Representation is a core issue in artificial intelligence. Humans use discrete language to communicate and learn from each other, while machines use continuous features (like vector, matrix, or tensor in deep neural networks) to represent cognitive patterns. Discrete symbols are low-dimensional, decoupled, and have strong reasoning ability, while continuous features are high-dimensional, coupled, and have incredible abstracting capabilities. In recent years, deep learning has developed the idea of continuous representation to the extreme, using millions of parameters to achieve high accuracies. Although this is reasonable from the statistical perspective, it has other major problems like lacking interpretability, poor generalization, and is easy to be attacked. Since both paradigms have strengths and weaknesses, a better choice is to seek reconciliation. In this paper, we make an initial attempt towards this direction. Specifically, we propose to combine symbolism and connectionism principles by using neural networks to derive a discrete representation. This process is highly similar to human language, which is a natural combination of discrete symbols and neural systems, where the brain processes continuous signals and represents intelligence via discrete language. To mimic this functionality, we denote our approach as machine language. By designing an interactive environment and task, we demonstrated that machines could generate a spontaneous, flexible, and semantic language through cooperation. Moreover, through experiments we show that discrete language representation has several advantages compared with continuous feature representation, from the aspects of interpretability, generalization, and robustness.



### Determination of building flood risk maps from LiDAR mobile mapping data
- **Arxiv ID**: http://arxiv.org/abs/2201.05514v1
- **DOI**: 10.1016/j.compenvurbsys.2022.101759
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05514v1)
- **Published**: 2022-01-14 15:36:08+00:00
- **Updated**: 2022-01-14 15:36:08+00:00
- **Authors**: Yu Feng, Qing Xiao, Claus Brenner, Aaron Peche, Juntao Yang, Udo Feuerhake, Monika Sester
- **Comment**: None
- **Journal**: Computers, Environment and Urban Systems, Vol. 93, April 2022,
  101759
- **Summary**: With increasing urbanization, flooding is a major challenge for many cities today. Based on forecast precipitation, topography, and pipe networks, flood simulations can provide early warnings for areas and buildings at risk of flooding. Basement windows, doors, and underground garage entrances are common places where floodwater can flow into a building. Some buildings have been prepared or designed considering the threat of flooding, but others have not. Therefore, knowing the heights of these facade openings helps to identify places that are more susceptible to water ingress. However, such data is not yet readily available in most cities. Traditional surveying of the desired targets may be used, but this is a very time-consuming and laborious process. This research presents a new process for the extraction of windows and doors from LiDAR mobile mapping data. Deep learning object detection models are trained to identify these objects. Usually, this requires to provide large amounts of manual annotations. In this paper, we mitigate this problem by leveraging a rule-based method. In a first step, the rule-based method is used to generate pseudo-labels. A semi-supervised learning strategy is then applied with three different levels of supervision. The results show that using only automatically generated pseudo-labels, the learning-based model outperforms the rule-based approach by 14.6% in terms of F1-score. After five hours of human supervision, it is possible to improve the model by another 6.2%. By comparing the detected facade openings' heights with the predicted water levels from a flood simulation model, a map can be produced which assigns per-building flood risk levels. This information can be combined with flood forecasting to provide a more targeted disaster prevention guide for the city's infrastructure and residential buildings.



### ViT2Hash: Unsupervised Information-Preserving Hashing
- **Arxiv ID**: http://arxiv.org/abs/2201.05541v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05541v1)
- **Published**: 2022-01-14 16:25:30+00:00
- **Updated**: 2022-01-14 16:25:30+00:00
- **Authors**: Qinkang Gong, Liangdao Wang, Hanjiang Lai, Yan Pan, Jian Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised image hashing, which maps images into binary codes without supervision, is a compressor with a high compression rate. Hence, how to preserving meaningful information of the original data is a critical problem. Inspired by the large-scale vision pre-training model, known as ViT, which has shown significant progress for learning visual representations, in this paper, we propose a simple information-preserving compressor to finetune the ViT model for the target unsupervised hashing task. Specifically, from pixels to continuous features, we first propose a feature-preserving module, using the corrupted image as input to reconstruct the original feature from the pre-trained ViT model and the complete image, so that the feature extractor can focus on preserving the meaningful information of original data. Secondly, from continuous features to hash codes, we propose a hashing-preserving module, which aims to keep the semantic information from the pre-trained ViT model by using the proposed Kullback-Leibler divergence loss. Besides, the quantization loss and the similarity loss are added to minimize the quantization error. Our method is very simple and achieves a significantly higher degree of MAP on three benchmark image datasets.



### Multimodal registration of FISH and nanoSIMS images using convolutional neural network models
- **Arxiv ID**: http://arxiv.org/abs/2201.05545v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.05545v2)
- **Published**: 2022-01-14 16:35:10+00:00
- **Updated**: 2022-04-05 20:42:47+00:00
- **Authors**: Xiaojia He, Christof Meile, Suchendra M. Bhandarkar
- **Comment**: None
- **Journal**: None
- **Summary**: Nanoscale secondary ion mass spectrometry (nanoSIMS) and fluorescence in situ hybridization (FISH) microscopy provide high-resolution, multimodal image representations of the identity and cell activity respectively of targeted microbial communities in microbiological research. Despite its importance to microbiologists, multimodal registration of FISH and nanoSIMS images is challenging given the morphological distortion and background noise in both images. In this study, we use convolutional neural networks (CNNs) for multiscale feature extraction, shape context for computation of the minimum transformation cost feature matching and the thin-plate spline (TPS) model for multimodal registration of the FISH and nanoSIMS images. Registration accuracy was quantitatively assessed against manually registered images, at both, the pixel and structural levels using standard metrics. Although all six tested CNN models performed well, ResNet18 was observed to outperform VGG16, VGG19, GoogLeNet and ShuffleNet and ResNet101 based on most metrics. This study demonstrates the utility of CNNs in the registration of multimodal images with significant background noise and morphology distortion. We also show aggregate shape, preserved by binarization, to be a robust feature for registering multimodal microbiology-related images.



### Domain Adaptation in LiDAR Semantic Segmentation via Alternating Skip Connections and Hybrid Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.05585v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.05585v2)
- **Published**: 2022-01-14 18:13:09+00:00
- **Updated**: 2022-03-21 14:59:01+00:00
- **Authors**: Eduardo R. Corral-Soto, Mrigank Rochan, Yannis Y. He, Shubhra Aich, Yang Liu, Liu Bingbing
- **Comment**: 1) Introduced Fig 1, 2) Simplified Fig. 2 diagram, 3) Fixed typos in
  losses, 4) Introduced Fig. 3, 5) Updated evaluation results, included
  evaluation on SemanticPOSS, 6) Introduced Table 3 - effects on covariance
  matrix and mean, 7) Updated Fig. 5, 8) Added more references. Improved
  writing in general, especially the motivation and description of each element
  and contribution from the method
- **Journal**: None
- **Summary**: In this paper we address the challenging problem of domain adaptation in LiDAR semantic segmentation. We consider the setting where we have a fully-labeled data set from source domain and a target domain with a few labeled and many unlabeled examples. We propose a domain adaption framework that mitigates the issue of domain shift and produces appealing performance on the target domain. To this end, we develop a GAN-based image-to-image translation engine that has generators with alternating connections, and couple it with a state-of-the-art LiDAR semantic segmentation network. Our framework is hybrid in nature in the sense that our model learning is composed of self-supervision, semi-supervision and unsupervised learning. Extensive experiments on benchmark LiDAR semantic segmentation data sets demonstrate that our method achieves superior performance in comparison to strong baselines and prior arts.



### When less is more: Simplifying inputs aids neural network understanding
- **Arxiv ID**: http://arxiv.org/abs/2201.05610v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2201.05610v4)
- **Published**: 2022-01-14 18:58:36+00:00
- **Updated**: 2022-02-01 13:15:00+00:00
- **Authors**: Robin Tibor Schirrmeister, Rosanne Liu, Sara Hooker, Tonio Ball
- **Comment**: None
- **Journal**: None
- **Summary**: How do neural network image classifiers respond to simpler and simpler inputs? And what do such responses reveal about the learning process? To answer these questions, we need a clear measure of input simplicity (or inversely, complexity), an optimization objective that correlates with simplification, and a framework to incorporate such objective into training and inference. Lastly we need a variety of testbeds to experiment and evaluate the impact of such simplification on learning. In this work, we measure simplicity with the encoding bit size given by a pretrained generative model, and minimize the bit size to simplify inputs in training and inference. We investigate the effect of such simplification in several scenarios: conventional training, dataset condensation and post-hoc explanations. In all settings, inputs are simplified along with the original classification task, and we investigate the trade-off between input simplicity and task performance. For images with injected distractors, such simplification naturally removes superfluous information. For dataset condensation, we find that inputs can be simplified with almost no accuracy degradation. When used in post-hoc explanation, our learning-based simplification approach offers a valuable new tool to explore the basis of network decisions.



### Disentanglement enables cross-domain Hippocampus Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.05650v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.05650v1)
- **Published**: 2022-01-14 19:49:53+00:00
- **Updated**: 2022-01-14 19:49:53+00:00
- **Authors**: John Kalkhof, Camila González, Anirban Mukhopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Limited amount of labelled training data are a common problem in medical imaging. This makes it difficult to train a well-generalised model and therefore often leads to failure in unknown domains. Hippocampus segmentation from magnetic resonance imaging (MRI) scans is critical for the diagnosis and treatment of neuropsychatric disorders. Domain differences in contrast or shape can significantly affect segmentation. We address this issue by disentangling a T1-weighted MRI image into its content and domain. This separation enables us to perform a domain transfer and thus convert data from new sources into the training domain. This step thus simplifies the segmentation problem, resulting in higher quality segmentations. We achieve the disentanglement with the proposed novel methodology 'Content Domain Disentanglement GAN', and we propose to retrain the UNet on the transformed outputs to deal with GAN-specific artefacts. With these changes, we are able to improve performance on unseen domains by 6-13% and outperform state-of-the-art domain transfer methods.



### Transformers in Action: Weakly Supervised Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.05675v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.05675v2)
- **Published**: 2022-01-14 21:15:58+00:00
- **Updated**: 2022-01-20 19:31:31+00:00
- **Authors**: John Ridley, Huseyin Coskun, David Joseph Tan, Nassir Navab, Federico Tombari
- **Comment**: Under Review
- **Journal**: None
- **Summary**: The video action segmentation task is regularly explored under weaker forms of supervision, such as transcript supervision, where a list of actions is easier to obtain than dense frame-wise labels. In this formulation, the task presents various challenges for sequence modeling approaches due to the emphasis on action transition points, long sequence lengths, and frame contextualization, making the task well-posed for transformers. Given developments enabling transformers to scale linearly, we demonstrate through our architecture how they can be applied to improve action alignment accuracy over the equivalent RNN-based models with the attention mechanism focusing around salient action transition regions. Additionally, given the recent focus on inference-time transcript selection, we propose a supplemental transcript embedding approach to select transcripts more quickly at inference-time. Furthermore, we subsequently demonstrate how this approach can also improve the overall segmentation performance. Finally, we evaluate our proposed methods across the benchmark datasets to better understand the applicability of transformers and the importance of transcript selection on this video-driven weakly-supervised task.



### Active Predictive Coding Networks: A Neural Solution to the Problem of Learning Reference Frames and Part-Whole Hierarchies
- **Arxiv ID**: http://arxiv.org/abs/2201.08813v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.08813v1)
- **Published**: 2022-01-14 21:22:48+00:00
- **Updated**: 2022-01-14 21:22:48+00:00
- **Authors**: Dimitrios C. Gklezakos, Rajesh P. N. Rao
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Active Predictive Coding Networks (APCNs), a new class of neural networks that solve a major problem posed by Hinton and others in the fields of artificial intelligence and brain modeling: how can neural networks learn intrinsic reference frames for objects and parse visual scenes into part-whole hierarchies by dynamically allocating nodes in a parse tree? APCNs address this problem by using a novel combination of ideas: (1) hypernetworks are used for dynamically generating recurrent neural networks that predict parts and their locations within intrinsic reference frames conditioned on higher object-level embedding vectors, and (2) reinforcement learning is used in conjunction with backpropagation for end-to-end learning of model parameters. The APCN architecture lends itself naturally to multi-level hierarchical learning and is closely related to predictive coding models of cortical function. Using the MNIST, Fashion-MNIST and Omniglot datasets, we demonstrate that APCNs can (a) learn to parse images into part-whole hierarchies, (b) learn compositional representations, and (c) transfer their knowledge to unseen classes of objects. With their ability to dynamically generate parse trees with part locations for objects, APCNs offer a new framework for explainable AI that leverages advances in deep learning while retaining interpretability and compositionality.



### Perspective Transformation Layer
- **Arxiv ID**: http://arxiv.org/abs/2201.05706v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.05706v2)
- **Published**: 2022-01-14 23:09:26+00:00
- **Updated**: 2022-10-30 17:36:38+00:00
- **Authors**: Nishan Khatri, Agnibh Dasgupta, Yucong Shen, Xin Zhong, Frank Y. Shih
- **Comment**: This paper has been accepted for publication by the 2022
  International Conference on Computational Science & Computational
  Intelligence (CSCI'22), Research Track on Signal & Image Processing, Computer
  Vision & Pattern Recognition
- **Journal**: None
- **Summary**: Incorporating geometric transformations that reflect the relative position changes between an observer and an object into computer vision and deep learning models has attracted much attention in recent years. However, the existing proposals mainly focus on the affine transformation that is insufficient to reflect such geometric position changes. Furthermore, current solutions often apply a neural network module to learn a single transformation matrix, which not only ignores the importance of multi-view analysis but also includes extra training parameters from the module apart from the transformation matrix parameters that increase the model complexity. In this paper, a perspective transformation layer is proposed in the context of deep learning. The proposed layer can learn homography, therefore reflecting the geometric positions between observers and objects. In addition, by directly training its transformation matrices, a single proposed layer can learn an adjustable number of multiple viewpoints without considering module parameters. The experiments and evaluations confirm the superiority of the proposed layer.



