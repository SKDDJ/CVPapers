# Arxiv Papers in cs.CV on 2022-01-19
### TriCoLo: Trimodal Contrastive Loss for Fine-grained Text to Shape Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2201.07366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07366v1)
- **Published**: 2022-01-19 00:15:15+00:00
- **Updated**: 2022-01-19 00:15:15+00:00
- **Authors**: Yue Ruan, Han-Hung Lee, Ke Zhang, Angel X. Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent work on contrastive losses for learning joint embeddings over multimodal data has been successful at downstream tasks such as retrieval and classification. On the other hand, work on joint representation learning for 3D shapes and text has thus far mostly focused on improving embeddings through modeling of complex attention between representations , or multi-task learning . We show that with large batch contrastive learning we achieve SoTA on text-shape retrieval without complex attention mechanisms or losses. Prior work in 3D and text representations has also focused on bimodal representation learning using either voxels or multi-view images with text. To this end, we propose a trimodal learning scheme to achieve even higher performance and better representations for all modalities.



### The Role of Pleura and Adipose in Lung Ultrasound AI
- **Arxiv ID**: http://arxiv.org/abs/2201.07368v1
- **DOI**: 10.1007/978-3-030-90874-4_14
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.07368v1)
- **Published**: 2022-01-19 00:46:56+00:00
- **Updated**: 2022-01-19 00:46:56+00:00
- **Authors**: Gautam Rajendrakumar Gare, Wanwen Chen, Alex Ling Yu Hung, Edward Chen, Hai V. Tran, Tom Fox, Pete Lowery, Kevin Zamora, Bennett P deBoisblanc, Ricardo Luis Rodriguez, John Michael Galeotti
- **Comment**: Published in MICCAI 2021 workshop on Lessons Learned from the
  development and application of medical imaging-based AI technologies for
  combating COVID-19 (LL-COVID19). The first two authors contributed equally to
  this work
- **Journal**: LL-COVID19 2021. Lecture Notes in Computer Science, vol 12969.
  Springer, Cham
- **Summary**: In this paper, we study the significance of the pleura and adipose tissue in lung ultrasound AI analysis. We highlight their more prominent appearance when using high-frequency linear (HFL) instead of curvilinear ultrasound probes, showing HFL reveals better pleura detail. We compare the diagnostic utility of the pleura and adipose tissue using an HFL ultrasound probe. Masking the adipose tissue during training and inference (while retaining the pleural line and Merlin's space artifacts such as A-lines and B-lines) improved the AI model's diagnostic accuracy.



### Online Deep Learning based on Auto-Encoder
- **Arxiv ID**: http://arxiv.org/abs/2201.07383v1
- **DOI**: 10.1007/s10489-020-02058-8
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.07383v1)
- **Published**: 2022-01-19 02:14:57+00:00
- **Updated**: 2022-01-19 02:14:57+00:00
- **Authors**: Si-si Zhang, Jian-wei Liu, Xin Zuo, Run-kun Lu, Si-ming Lian
- **Comment**: 30 pages
- **Journal**: Applied Intelligence (2021)
- **Summary**: Online learning is an important technical means for sketching massive real-time and high-speed data. Although this direction has attracted intensive attention, most of the literature in this area ignore the following three issues: (1) they think little of the underlying abstract hierarchical latent information existing in examples, even if extracting these abstract hierarchical latent representations is useful to better predict the class labels of examples; (2) the idea of preassigned model on unseen datapoints is not suitable for modeling streaming data with evolving probability distribution. This challenge is referred as model flexibility. And so, with this in minds, the online deep learning model we need to design should have a variable underlying structure; (3) moreover, it is of utmost importance to fusion these abstract hierarchical latent representations to achieve better classification performance, and we should give different weights to different levels of implicit representation information when dealing with the data streaming where the data distribution changes. To address these issues, we propose a two-phase Online Deep Learning based on Auto-Encoder (ODLAE). Based on auto-encoder, considering reconstruction loss, we extract abstract hierarchical latent representations of instances; Based on predictive loss, we devise two fusion strategies: the output-level fusion strategy, which is obtained by fusing the classification results of encoder each hidden layer; and feature-level fusion strategy, which is leveraged self-attention mechanism to fusion every hidden layer output. Finally, in order to improve the robustness of the algorithm, we also try to utilize the denoising auto-encoder to yield hierarchical latent representations. Experimental results on different datasets are presented to verify the validity of our proposed algorithm (ODLAE) outperforms several baselines.



### Swin-Pose: Swin Transformer Based Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2201.07384v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07384v2)
- **Published**: 2022-01-19 02:15:26+00:00
- **Updated**: 2022-06-25 23:08:10+00:00
- **Authors**: Zinan Xiong, Chenxi Wang, Ying Li, Yan Luo, Yu Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been widely utilized in many computer vision tasks. However, CNNs have a fixed reception field and lack the ability of long-range perception, which is crucial to human pose estimation. Due to its capability to capture long-range dependencies between pixels, transformer architecture has been adopted to computer vision applications recently and is proven to be a highly effective architecture. We are interested in exploring its capability in human pose estimation, and thus propose a novel model based on transformer architecture, enhanced with a feature pyramid fusion structure. More specifically, we use pre-trained Swin Transformer as our backbone and extract features from input images, we leverage a feature pyramid structure to extract feature maps from different stages. By fusing the features together, our model predicts the keypoint heatmap. The experiment results of our study have demonstrated that the proposed transformer-based model can achieve better performance compared to the state-of-the-art CNN-based models.



### Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.09671v3
- **DOI**: 10.1109/ICAPAI55158.2022.9801567
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09671v3)
- **Published**: 2022-01-19 02:45:01+00:00
- **Updated**: 2022-04-19 05:17:54+00:00
- **Authors**: Christopher Sun
- **Comment**: IEEE International Conference on Applied Artificial Intelligence (May
  2022)
- **Journal**: None
- **Summary**: Since frequent severe droughts are lengthening the dry season in the Amazon Rainforest, it is important to detect wildfires promptly and forecast possible spread for effective suppression response. Current wildfire detection models are not versatile enough for the low-technology conditions of South American hot spots. This deep learning study first trains a Fully Convolutional Neural Network on Landsat 8 images of Ecuador and the Galapagos, using Green and Short-wave Infrared bands to predict pixel-level binary fire masks. This model achieves a 0.962 validation F2 score and a 0.932 F2 score on test data from Guyana and Suriname. Afterward, image segmentation is conducted on the Cirrus band using K-Means Clustering to simplify continuous pixel values into three discrete classes representing differing degrees of cirrus cloud contamination. Three additional Convolutional Neural Networks are trained to conduct a sensitivity analysis measuring the effect of simplified features on model accuracy and train time. The Experimental model trained on the segmented cirrus images provides a statistically significant decrease in train time compared to the Control model trained on raw cirrus images, without compromising binary accuracy. This proof of concept reveals that feature engineering can improve the performance of wildfire detection models by lowering computational expense.



### KappaFace: Adaptive Additive Angular Margin Loss for Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.07394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07394v1)
- **Published**: 2022-01-19 03:05:24+00:00
- **Updated**: 2022-01-19 03:05:24+00:00
- **Authors**: Chingis Oinar, Binh M. Le, Simon S. Woo
- **Comment**: None
- **Journal**: None
- **Summary**: Feature learning is a widely used method employed for large-scale face recognition. Recently, large-margin softmax loss methods have demonstrated significant enhancements on deep face recognition. These methods propose fixed positive margins in order to enforce intra-class compactness and inter-class diversity. However, the majority of the proposed methods do not consider the class imbalance issue, which is a major challenge in practice for developing deep face recognition models. We hypothesize that it significantly affects the generalization ability of the deep face models. Inspired by this observation, we introduce a novel adaptive strategy, called KappaFace, to modulate the relative importance based on class difficultness and imbalance. With the support of the von Mises-Fisher distribution, our proposed KappaFace loss can intensify the margin's magnitude for hard learning or low concentration classes while relaxing it for counter classes. Experiments conducted on popular facial benchmarks demonstrate that our proposed method achieves superior performance to the state-of-the-art.



### A Review of Deep Transfer Learning and Recent Advancements
- **Arxiv ID**: http://arxiv.org/abs/2201.09679v2
- **DOI**: 10.3390/technologies11020040
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09679v2)
- **Published**: 2022-01-19 04:19:36+00:00
- **Updated**: 2022-12-22 20:15:14+00:00
- **Authors**: Mohammadreza Iman, Khaled Rasheed, Hamid R. Arabnia
- **Comment**: 18 pages, 2 figures, 1 table
- **Journal**: Technologies 2023, 11, 40
- **Summary**: Deep learning has been the answer to many machine learning problems during the past two decades. However, it comes with two major constraints: dependency on extensive labeled data and training costs. Transfer learning in deep learning, known as Deep Transfer Learning (DTL), attempts to reduce such dependency and costs by reusing an obtained knowledge from a source data/task in training on a target data/task. Most applied DTL techniques are network/model-based approaches. These methods reduce the dependency of deep learning models on extensive training data and drastically decrease training costs. As a result, researchers detected Covid-19 infection on chest X-Rays with high accuracy at the beginning of the pandemic with minimal data using DTL techniques. Also, the training cost reduction makes DTL viable on edge devices with limited resources. Like any new advancement, DTL methods have their own limitations, and a successful transfer depends on some adjustments for different scenarios. In this paper, we review the definition and taxonomy of deep transfer learning and well-known methods. Then we investigate the DTL approaches by reviewing recent applied DTL techniques in the past five years. Further, we review some experimental analyses of DTLs to learn the best practice for applying DTL in different scenarios. Moreover, the limitations of DTLs (catastrophic forgetting dilemma and overly biased pre-trained models) are discussed, along with possible solutions and research trends.



### Poseur: Direct Human Pose Regression with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2201.07412v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07412v2)
- **Published**: 2022-01-19 04:31:57+00:00
- **Updated**: 2022-07-20 12:25:18+00:00
- **Authors**: Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, Zhibin Wang, Anton van den Hengel
- **Comment**: Accepted to Proc. Eur. Conf. Comp. Vision (ECCV) 2022
- **Journal**: None
- **Summary**: We propose a direct, regression-based approach to 2D human pose estimation from single images. We formulate the problem as a sequence prediction task, which we solve using a Transformer network. This network directly learns a regression mapping from images to the keypoint coordinates, without resorting to intermediate representations such as heatmaps. This approach avoids much of the complexity associated with heatmap-based approaches. To overcome the feature misalignment issues of previous regression-based methods, we propose an attention mechanism that adaptively attends to the features that are most relevant to the target keypoints, considerably improving the accuracy. Importantly, our framework is end-to-end differentiable, and naturally learns to exploit the dependencies between keypoints. Experiments on MS-COCO and MPII, two predominant pose-estimation datasets, demonstrate that our method significantly improves upon the state-of-the-art in regression-based pose estimation. More notably, ours is the first regression-based approach to perform favorably compared to the best heatmap-based pose estimation methods.



### Self-Supervised Deep Blind Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2201.07422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07422v1)
- **Published**: 2022-01-19 05:18:44+00:00
- **Updated**: 2022-01-19 05:18:44+00:00
- **Authors**: Haoran Bai, Jinshan Pan
- **Comment**: Project website: https://github.com/csbhr/Self-Blind-VSR
- **Journal**: None
- **Summary**: Existing deep learning-based video super-resolution (SR) methods usually depend on the supervised learning approach, where the training data is usually generated by the blurring operation with known or predefined kernels (e.g., Bicubic kernel) followed by a decimation operation. However, this does not hold for real applications as the degradation process is complex and cannot be approximated by these idea cases well. Moreover, obtaining high-resolution (HR) videos and the corresponding low-resolution (LR) ones in real-world scenarios is difficult. To overcome these problems, we propose a self-supervised learning method to solve the blind video SR problem, which simultaneously estimates blur kernels and HR videos from the LR videos. As directly using LR videos as supervision usually leads to trivial solutions, we develop a simple and effective method to generate auxiliary paired data from original LR videos according to the image formation of video SR, so that the networks can be better constrained by the generated paired data for both blur kernel estimation and latent HR video restoration. In addition, we introduce an optical flow estimation module to exploit the information from adjacent frames for HR video restoration. Experiments show that our method performs favorably against state-of-the-art ones on benchmarks and real-world videos.



### WebUAV-3M: A Benchmark for Unveiling the Power of Million-Scale Deep UAV Tracking
- **Arxiv ID**: http://arxiv.org/abs/2201.07425v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07425v4)
- **Published**: 2022-01-19 05:39:42+00:00
- **Updated**: 2022-12-31 02:00:27+00:00
- **Authors**: Chunhui Zhang, Guanjie Huang, Li Liu, Shan Huang, Yinan Yang, Xiang Wan, Shiming Ge, Dacheng Tao
- **Comment**: 25 pages
- **Journal**: None
- **Summary**: Unmanned aerial vehicle (UAV) tracking is of great significance for a wide range of applications, such as delivery and agriculture. Previous benchmarks in this area mainly focused on small-scale tracking problems while ignoring the amounts of data, types of data modalities, diversities of target categories and scenarios, and evaluation protocols involved, greatly hiding the massive power of deep UAV tracking. In this work, we propose WebUAV-3M, the largest public UAV tracking benchmark to date, to facilitate both the development and evaluation of deep UAV trackers. WebUAV-3M contains over 3.3 million frames across 4,500 videos and offers 223 highly diverse target categories. Each video is densely annotated with bounding boxes by an efficient and scalable semiautomatic target annotation (SATA) pipeline. Importantly, to take advantage of the complementary superiority of language and audio, we enrich WebUAV-3M by innovatively providing both natural language specifications and audio descriptions. We believe that such additions will greatly boost future research in terms of exploring language features and audio cues for multimodal UAV tracking. In addition, a fine-grained UAV tracking-under-scenario constraint (UTUSC) evaluation protocol and seven challenging scenario subtest sets are constructed to enable the community to develop, adapt and evaluate various types of advanced trackers. We provide extensive evaluations and detailed analyses of 43 representative trackers and envision future research directions in the field of deep UAV tracking and beyond. The dataset, toolkits and baseline results are available at \url{https://github.com/983632847/WebUAV-3M}.



### Variable Augmented Network for Invertible MR Coil Compression
- **Arxiv ID**: http://arxiv.org/abs/2201.07428v2
- **DOI**: None
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2201.07428v2)
- **Published**: 2022-01-19 05:59:40+00:00
- **Updated**: 2022-03-19 14:39:19+00:00
- **Authors**: Xianghao Liao, Shanshan Wang, Lanlan Tu, Yuhao Wang, Dong Liang, Qiegen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: A large number of coils are able to provide enhanced signal-to-noise ratio and improve imaging performance in parallel imaging. Nevertheless, the increasing growth of coil number simultaneously aggravates the drawbacks of data storage and reconstruction speed, especially in some iterative reconstructions. Coil compression addresses these issues by generating fewer virtual coils. In this work, a novel variable augmentation network for invertible coil compression termed VAN-ICC is presented. It utilizes inherent reversibility of normalizing flow-based models for high-precision compression and invertible recovery. By employing the variable augmentation technology to image/k-space variables from multi-coils, VAN-ICC trains invertible networks by finding an invertible and bijective function, which can map the original data to the compressed counterpart and vice versa. Experiments conducted on both fully-sampled and under-sampled data verified the effectiveness and flexibility of VAN-ICC. Quantitative and qualitative comparisons with traditional non-deep learning-based approaches demonstrated that VAN-ICC can carry much higher compression effects. Additionally, its performance is not susceptible to different number of virtual coils.



### Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth
- **Arxiv ID**: http://arxiv.org/abs/2201.07436v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07436v3)
- **Published**: 2022-01-19 06:37:21+00:00
- **Updated**: 2022-10-29 05:16:36+00:00
- **Authors**: Doyeon Kim, Woonghyun Ka, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim
- **Comment**: 11pages, 5 figures
- **Journal**: None
- **Summary**: Depth estimation from a single image is an important task that can be applied to various fields in computer vision, and has grown rapidly with the development of convolutional neural networks. In this paper, we propose a novel structure and training strategy for monocular depth estimation to further improve the prediction accuracy of the network. We deploy a hierarchical transformer encoder to capture and convey the global context, and design a lightweight yet powerful decoder to generate an estimated depth map while considering local connectivity. By constructing connected paths between multi-scale local features and the global decoding stream with our proposed selective feature fusion module, the network can integrate both representations and recover fine details. In addition, the proposed decoder shows better performance than the previously proposed decoders, with considerably less computational complexity. Furthermore, we improve the depth-specific augmentation method by utilizing an important observation in depth estimation to enhance the model. Our network achieves state-of-the-art performance over the challenging depth dataset NYU Depth V2. Extensive experiments have been conducted to validate and show the effectiveness of the proposed approach. Finally, our model shows better generalisation ability and robustness than other comparative models.



### TransFuse: A Unified Transformer-based Image Fusion Framework using Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.07451v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07451v1)
- **Published**: 2022-01-19 07:30:44+00:00
- **Updated**: 2022-01-19 07:30:44+00:00
- **Authors**: Linhao Qu, Shaolei Liu, Manning Wang, Shiman Li, Siqi Yin, Qin Qiao, Zhijian Song
- **Comment**: None
- **Journal**: None
- **Summary**: Image fusion is a technique to integrate information from multiple source images with complementary information to improve the richness of a single image. Due to insufficient task-specific training data and corresponding ground truth, most existing end-to-end image fusion methods easily fall into overfitting or tedious parameter optimization processes. Two-stage methods avoid the need of large amount of task-specific training data by training encoder-decoder network on large natural image datasets and utilizing the extracted features for fusion, but the domain gap between natural images and different fusion tasks results in limited performance. In this study, we design a novel encoder-decoder based image fusion framework and propose a destruction-reconstruction based self-supervised training scheme to encourage the network to learn task-specific features. Specifically, we propose three destruction-reconstruction self-supervised auxiliary tasks for multi-modal image fusion, multi-exposure image fusion and multi-focus image fusion based on pixel intensity non-linear transformation, brightness transformation and noise transformation, respectively. In order to encourage different fusion tasks to promote each other and increase the generalizability of the trained network, we integrate the three self-supervised auxiliary tasks by randomly choosing one of them to destroy a natural image in model training. In addition, we design a new encoder that combines CNN and Transformer for feature extraction, so that the trained model can exploit both local and global information. Extensive experiments on multi-modal image fusion, multi-exposure image fusion and multi-focus image fusion tasks demonstrate that our proposed method achieves the state-of-the-art performance in both subjective and objective evaluations. The code will be publicly available soon.



### PT4AL: Using Self-Supervised Pretext Tasks for Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.07459v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07459v3)
- **Published**: 2022-01-19 07:58:06+00:00
- **Updated**: 2022-07-26 09:21:37+00:00
- **Authors**: John Seon Keun Yi, Minseok Seo, Jongchan Park, Dong-Geol Choi
- **Comment**: Code is available at https://github.com/johnsk95/PT4AL Updated for
  ECCV 2022 submission
- **Journal**: None
- **Summary**: Labeling a large set of data is expensive. Active learning aims to tackle this problem by asking to annotate only the most informative data from the unlabeled set. We propose a novel active learning approach that utilizes self-supervised pretext tasks and a unique data sampler to select data that are both difficult and representative. We discover that the loss of a simple self-supervised pretext task, such as rotation prediction, is closely correlated to the downstream task loss. Before the active learning iterations, the pretext task learner is trained on the unlabeled set, and the unlabeled data are sorted and split into batches by their pretext task losses. In each active learning iteration, the main task model is used to sample the most uncertain data in a batch to be annotated. We evaluate our method on various image classification and segmentation benchmarks and achieve compelling performances on CIFAR10, Caltech-101, ImageNet, and Cityscapes. We further show that our method performs well on imbalanced datasets, and can be an effective solution to the cold-start problem where active learning performance is affected by the randomly sampled initial labeled set.



### High-fidelity 3D Model Compression based on Key Spheres
- **Arxiv ID**: http://arxiv.org/abs/2201.07486v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07486v2)
- **Published**: 2022-01-19 09:21:54+00:00
- **Updated**: 2022-01-20 04:36:35+00:00
- **Authors**: Yuanzhan Li, Yuqi Liu, Yujie Lu, Siyu Zhang, Shen Cai, Yanting Zhang
- **Comment**: Accepted in Data Compression Conference (DCC) 2022 as a full paper
- **Journal**: None
- **Summary**: In recent years, neural signed distance function (SDF) has become one of the most effective representation methods for 3D models. By learning continuous SDFs in 3D space, neural networks can predict the distance from a given query space point to its closest object surface,whose positive and negative signs denote inside and outside of the object, respectively. Training a specific network for each 3D model, which individually embeds its shape, can realize compressed representation of objects by storing fewer network (and possibly latent) parameters. Consequently, reconstruction through network inference and surface recovery can be achieved. In this paper, we propose an SDF prediction network using explicit key spheres as input. Key spheres are extracted from the internal space of objects, whose centers either have relatively larger SDF values (sphere radii), or are located at essential positions. By inputting the spatial information of multiple spheres which imply different local shapes, the proposed method can significantly improve the reconstruction accuracy with a negligible storage cost. Compared to previous works, our method achieves the high-fidelity and high-compression 3D object coding and reconstruction. Experiments conducted on three datasets verify the superior performance of our method.



### Weakly Supervised Semantic Segmentation of Remote Sensing Images for Tree Species Classification Based on Explanation Methods
- **Arxiv ID**: http://arxiv.org/abs/2201.07495v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/2201.07495v1)
- **Published**: 2022-01-19 09:32:48+00:00
- **Updated**: 2022-01-19 09:32:48+00:00
- **Authors**: Steve Ahlswede, Nimisha Thekke-Madam, Christian Schulz, Birgit Kleinschmit, Begüm Demir
- **Comment**: 4 pages, 1 figure, submitted to IEEE Geosciences and Remote Sensing
  Symposium (2022)
- **Journal**: None
- **Summary**: The collection of a high number of pixel-based labeled training samples for tree species identification is time consuming and costly in operational forestry applications. To address this problem, in this paper we investigate the effectiveness of explanation methods for deep neural networks in performing weakly supervised semantic segmentation using only image-level labels. Specifically, we consider four methods:i) class activation maps (CAM); ii) gradient-based CAM; iii) pixel correlation module; and iv) self-enhancing maps (SEM). We compare these methods with each other using both quantitative and qualitative measures of their segmentation accuracy, as well as their computational requirements. Experimental results obtained on an aerial image archive show that:i) considered explanation techniques are highly relevant for the identification of tree species with weak supervision; and ii) the SEM outperforms the other considered methods. The code for this paper is publicly available at https://git.tu-berlin.de/rsim/rs_wsss.



### Signal Strength and Noise Drive Feature Preference in CNN Image Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2201.08893v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.08893v1)
- **Published**: 2022-01-19 11:32:19+00:00
- **Updated**: 2022-01-19 11:32:19+00:00
- **Authors**: Max Wolff, Stuart Wolff
- **Comment**: Accepted at SVRHM 2021
- **Journal**: None
- **Summary**: Feature preference in Convolutional Neural Network (CNN) image classifiers is integral to their decision making process, and while the topic has been well studied, it is still not understood at a fundamental level. We test a range of task relevant feature attributes (including shape, texture, and color) with varying degrees of signal and noise in highly controlled CNN image classification experiments using synthetic datasets to determine feature preferences. We find that CNNs will prefer features with stronger signal strength and lower noise irrespective of whether the feature is texture, shape, or color. This provides guidance for a predictive model for task relevant feature preferences, demonstrates pathways for bias in machine models that can be avoided with careful controls on experimental setup, and suggests that comparisons between how humans and machines prefer task relevant features in vision classification tasks should be revisited. Code to reproduce experiments in this paper can be found at \url{https://github.com/mwolff31/signal_preference}.



### Virtual Coil Augmentation Technology for MR Coil Extrapolation via Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.07540v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07540v2)
- **Published**: 2022-01-19 11:33:38+00:00
- **Updated**: 2022-03-20 06:44:41+00:00
- **Authors**: Cailian Yang, Xianghao Liao, Yuhao Wang, Minghui Zhang, Qiegen Liu
- **Comment**: arXiv admin note: text overlap with arXiv:2103.15061,
  arXiv:1907.03063, arXiv:1807.03039 by other authors
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) is a widely used medical imaging modality. However, due to the limitations in hardware, scan time, and throughput, it is often clinically challenging to obtain high-quality MR images. In this article, we propose a method of using artificial intelligence to expand the channel to achieve the goal of generating the virtual coils. The main characteristic of our work is utilizing dummy variable technology to expand/extrapolate the receive coils in both image and k-space domains. The high-dimensional information formed by channel expansion is used as the prior information to improve the reconstruction effect of parallel imaging. Two main components are incorporated into the network design, namely variable augmentation technology and sum of squares (SOS) objective function. Variable augmentation provides the network with more high-dimensional prior information, which is helpful for the network to extract the deep feature information of the data. The SOS objective function is employed to solve the deficiency of k-space data training while speeding up convergence. Experimental results demonstrated its great potentials in super-resolution of MR images and accelerated parallel imaging reconstruction.



### Simpler is better: spectral regularization and up-sampling techniques for variational autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2201.07544v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.07544v1)
- **Published**: 2022-01-19 11:49:57+00:00
- **Updated**: 2022-01-19 11:49:57+00:00
- **Authors**: Sara Björk, Jonas Nordhaug Myhre, Thomas Haugland Johansen
- **Comment**: Submitted to ICASSP 2022, 2022 IEEE International Conference on
  Acoustics, Speech and Signal Processing
- **Journal**: None
- **Summary**: Full characterization of the spectral behavior of generative models based on neural networks remains an open issue. Recent research has focused heavily on generative adversarial networks and the high-frequency discrepancies between real and generated images. The current solution to avoid this is to either replace transposed convolutions with bilinear up-sampling or add a spectral regularization term in the generator. It is well known that Variational Autoencoders (VAEs) also suffer from these issues. In this work, we propose a simple 2D Fourier transform-based spectral regularization loss for the VAE and show that it can achieve results equal to, or better than, the current state-of-the-art in frequency-aware losses for generative models. In addition, we experiment with altering the up-sampling procedure in the generator network and investigate how it influences the spectral performance of the model. We include experiments on synthetic and real data sets to demonstrate our results.



### Learned Cone-Beam CT Reconstruction Using Neural Ordinary Differential Equations
- **Arxiv ID**: http://arxiv.org/abs/2201.07562v1
- **DOI**: 10.1117/12.2646442
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.07562v1)
- **Published**: 2022-01-19 12:32:38+00:00
- **Updated**: 2022-01-19 12:32:38+00:00
- **Authors**: Mareike Thies, Fabian Wagner, Mingxuan Gu, Lukas Folle, Lina Felsner, Andreas Maier
- **Comment**: 6 pages
- **Journal**: 7th International Conference on Image Formation in X-Ray Computed
  Tomography, Proc. Vol. 12304 (2022)
- **Summary**: Learned iterative reconstruction algorithms for inverse problems offer the flexibility to combine analytical knowledge about the problem with modules learned from data. This way, they achieve high reconstruction performance while ensuring consistency with the measured data. In computed tomography, extending such approaches from 2D fan-beam to 3D cone-beam data is challenging due to the prohibitively high GPU memory that would be needed to train such models. This paper proposes to use neural ordinary differential equations to solve the reconstruction problem in a residual formulation via numerical integration. For training, there is no need to backpropagate through several unrolled network blocks nor through the internals of the solver. Instead, the gradients are obtained very memory-efficiently in the neural ODE setting allowing for training on a single consumer graphics card. The method is able to reduce the root mean squared error by over 30% compared to the best performing classical iterative reconstruction algorithm and produces high quality cone-beam reconstructions even in a sparse view scenario.



### An Ensemble Model for Face Liveness Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.08901v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08901v1)
- **Published**: 2022-01-19 12:43:39+00:00
- **Updated**: 2022-01-19 12:43:39+00:00
- **Authors**: Shashank Shekhar, Avinash Patel, Mrinal Haloi, Asif Salim
- **Comment**: Accepted and presented at MLDM 2022. To be published in Lattice
  journal
- **Journal**: None
- **Summary**: In this paper, we present a passive method to detect face presentation attack a.k.a face liveness detection using an ensemble deep learning technique. Face liveness detection is one of the key steps involved in user identity verification of customers during the online onboarding/transaction processes. During identity verification, an unauthenticated user tries to bypass the verification system by several means, for example, they can capture a user photo from social media and do an imposter attack using printouts of users faces or using a digital photo from a mobile device and even create a more sophisticated attack like video replay attack. We have tried to understand the different methods of attack and created an in-house large-scale dataset covering all the kinds of attacks to train a robust deep learning model. We propose an ensemble method where multiple features of the face and background regions are learned to predict whether the user is a bonafide or an attacker.



### Superpixel Pre-Segmentation of HER2 Slides for Efficient Annotation
- **Arxiv ID**: http://arxiv.org/abs/2201.07572v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.07572v1)
- **Published**: 2022-01-19 12:51:33+00:00
- **Updated**: 2022-01-19 12:51:33+00:00
- **Authors**: Mathias Öttl, Jana Mönius, Christian Marzahl, Matthias Rübner, Carol I. Geppert, Arndt Hartmann, Matthias W. Beckmann, Peter Fasching, Andreas Maier, Ramona Erber, Katharina Breininger
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised deep learning has shown state-of-the-art performance for medical image segmentation across different applications, including histopathology and cancer research; however, the manual annotation of such data is extremely laborious. In this work, we explore the use of superpixel approaches to compute a pre-segmentation of HER2 stained images for breast cancer diagnosis that facilitates faster manual annotation and correction in a second step. Four methods are compared: Standard Simple Linear Iterative Clustering (SLIC) as a baseline, a domain adapted SLIC, and superpixels based on feature embeddings of a pretrained ResNet-50 and a denoising autoencoder. To tackle oversegmentation, we propose to hierarchically merge superpixels, based on their content in the respective feature space. When evaluating the approaches on fully manually annotated images, we observe that the autoencoder-based superpixels achieve a 23% increase in boundary F1 score compared to the baseline SLIC superpixels. Furthermore, the boundary F1 score increases by 73% when hierarchical clustering is applied on the adapted SLIC and the autoencoder-based superpixels. These evaluations show encouraging first results for a pre-segmentation for efficient manual refinement without the need for an initial set of annotated training data.



### DMF-Net: Dual-Branch Multi-Scale Feature Fusion Network for copy forgery identification of anti-counterfeiting QR code
- **Arxiv ID**: http://arxiv.org/abs/2201.07583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07583v1)
- **Published**: 2022-01-19 13:12:38+00:00
- **Updated**: 2022-01-19 13:12:38+00:00
- **Authors**: Zhongyuan Guo, Hong Zheng, Changhui You, Tianyu Wang, Chang Liu
- **Comment**: 17 pages, 6 figures
- **Journal**: None
- **Summary**: Anti-counterfeiting QR codes are widely used in people's work and life, especially in product packaging. However, the anti-counterfeiting QR code has the risk of being copied and forged in the circulation process. In reality, copying is usually based on genuine anti-counterfeiting QR codes, but the brands and models of copiers are diverse, and it is extremely difficult to determine which individual copier the forged anti-counterfeiting code come from. In response to the above problems, this paper proposes a method for copy forgery identification of anti-counterfeiting QR code based on deep learning. We first analyze the production principle of anti-counterfeiting QR code, and convert the identification of copy forgery to device category forensics, and then a Dual-Branch Multi-Scale Feature Fusion network is proposed. During the design of the network, we conducted a detailed analysis of the data preprocessing layer, single-branch design, etc., combined with experiments, the specific structure of the dual-branch multi-scale feature fusion network is determined. The experimental results show that the proposed method has achieved a high accuracy of copy forgery identification, which exceeds the current series of methods in the field of image forensics.



### Real-time Recognition of Yoga Poses using computer Vision for Smart Health Care
- **Arxiv ID**: http://arxiv.org/abs/2201.07594v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07594v1)
- **Published**: 2022-01-19 13:41:58+00:00
- **Updated**: 2022-01-19 13:41:58+00:00
- **Authors**: Abhishek Sharma, Yash Shah, Yash Agrawal, Prateek Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, yoga has become a part of life for many people. Exercises and sports technological assistance is implemented in yoga pose identification. In this work, a self-assistance based yoga posture identification technique is developed, which helps users to perform Yoga with the correction feature in Real-time. The work also presents Yoga-hand mudra (hand gestures) identification. The YOGI dataset has been developed which include 10 Yoga postures with around 400-900 images of each pose and also contain 5 mudras for identification of mudras postures. It contains around 500 images of each mudra. The feature has been extracted by making a skeleton on the body for yoga poses and hand for mudra poses. Two different algorithms have been used for creating a skeleton one for yoga poses and the second for hand mudras. Angles of the joints have been extracted as a features for different machine learning and deep learning models. among all the models XGBoost with RandomSearch CV is most accurate and gives 99.2\% accuracy. The complete design framework is described in the present paper.



### A Confidence-based Iterative Solver of Depths and Surface Normals for Deep Multi-view Stereo
- **Arxiv ID**: http://arxiv.org/abs/2201.07609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07609v1)
- **Published**: 2022-01-19 14:08:45+00:00
- **Updated**: 2022-01-19 14:08:45+00:00
- **Authors**: Wang Zhao, Shaohui Liu, Yi Wei, Hengkai Guo, Yong-Jin Liu
- **Comment**: 17 pages, 13 figures, 7 tables. ICCV 2021
- **Journal**: None
- **Summary**: In this paper, we introduce a deep multi-view stereo (MVS) system that jointly predicts depths, surface normals and per-view confidence maps. The key to our approach is a novel solver that iteratively solves for per-view depth map and normal map by optimizing an energy potential based on the locally planar assumption. Specifically, the algorithm updates depth map by propagating from neighboring pixels with slanted planes, and updates normal map with local probabilistic plane fitting. Both two steps are monitored by a customized confidence map. This solver is not only effective as a post-processing tool for plane-based depth refinement and completion, but also differentiable such that it can be efficiently integrated into deep learning pipelines. Our multi-view stereo system employs multiple optimization steps of the solver over the initial prediction of depths and surface normals. The whole system can be trained end-to-end, decoupling the challenging problem of matching pixels within poorly textured regions from the cost-volume based neural network. Experimental results on ScanNet and RGB-D Scenes V2 demonstrate state-of-the-art performance of the proposed deep MVS system on multi-view depth estimation, with our proposed solver consistently improving the depth quality over both conventional and deep learning based MVS pipelines. Code is available at https://github.com/thuzhaowang/idn-solver.



### Nonlinear Unknown Input Observability and Unknown Input Reconstruction: The General Analytical Solution
- **Arxiv ID**: http://arxiv.org/abs/2201.07610v4
- **DOI**: 10.1016/j.inffus.2022.03.004
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.07610v4)
- **Published**: 2022-01-19 14:09:14+00:00
- **Updated**: 2023-05-09 06:57:09+00:00
- **Authors**: Agostino Martinelli
- **Comment**: This paper was published by the journal of Information Fusion
- **Journal**: Journal of Information Fusion, Volume 85, September 2022, Pages
  23-51
- **Summary**: Observability is a fundamental structural property of any dynamic system and describes the possibility of reconstructing the state that characterizes the system from observing its inputs and outputs. Despite the huge effort made to study this property and to introduce analytical criteria able to check whether a dynamic system satisfies this property or not, there is no general analytical criterion to automatically check the state observability when the dynamics are also driven by unknown inputs. Here, we introduce the general analytical solution of this fundamental problem, often called the unknown input observability problem. This paper provides the general analytical solution of this problem, namely, it provides the systematic procedure, based on automatic computation (differentiation and matrix rank determination), that allows us to automatically check the state observability even in the presence of unknown inputs (Algorithm 6.1). A first solution of this problem was presented in the second part of the book: "Observability: A New Theory Based on the Group of Invariance" [45]. The solution presented by this paper completes the previous solution in [45]. In particular, the new solution exhaustively accounts for the systems that do not belong to the category of the systems that are "canonic with respect to their unknown inputs". The analytical derivations largely exploit several new concepts and analytical results introduced in [45]. Finally, as a simple consequence of the results here obtained, we also provide the answer to the problem of unknown input reconstruction which is intimately related to the problem of state observability. We illustrate the implementation of the new algorithm by studying the observability properties of a nonlinear system in the framework of visual-inertial sensor fusion, whose dynamics are driven by two unknown inputs and one known input.



### CAST: Character labeling in Animation using Self-supervision by Tracking
- **Arxiv ID**: http://arxiv.org/abs/2201.07619v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07619v1)
- **Published**: 2022-01-19 14:21:43+00:00
- **Updated**: 2022-01-19 14:21:43+00:00
- **Authors**: Oron Nir, Gal Rapoport, Ariel Shamir
- **Comment**: Published as a conference paper at EuroGraphics 2022
- **Journal**: None
- **Summary**: Cartoons and animation domain videos have very different characteristics compared to real-life images and videos. In addition, this domain carries a large variability in styles. Current computer vision and deep-learning solutions often fail on animated content because they were trained on natural images. In this paper we present a method to refine a semantic representation suitable for specific animated content. We first train a neural network on a large-scale set of animation videos and use the mapping to deep features as an embedding space. Next, we use self-supervision to refine the representation for any specific animation style by gathering many examples of animated characters in this style, using a multi-object tracking. These examples are used to define triplets for contrastive loss training. The refined semantic space allows better clustering of animated characters even when they have diverse manifestations. Using this space we can build dictionaries of characters in an animation videos, and define specialized classifiers for specific stylistic content (e.g., characters in a specific animation series) with very little user effort. These classifiers are the basis for automatically labeling characters in animation videos. We present results on a collection of characters in a variety of animation styles.



### A Survey on Training Challenges in Generative Adversarial Networks for Biomedical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2201.07646v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.07646v4)
- **Published**: 2022-01-19 15:23:46+00:00
- **Updated**: 2023-08-11 00:13:54+00:00
- **Authors**: Muhammad Muneeb Saad, Ruairi O'Reilly, Mubashir Husain Rehmani
- **Comment**: Submitted to the AI Review Journal
- **Journal**: None
- **Summary**: In biomedical image analysis, the applicability of deep learning methods is directly impacted by the quantity of image data available. This is due to deep learning models requiring large image datasets to provide high-level performance. Generative Adversarial Networks (GANs) have been widely utilized to address data limitations through the generation of synthetic biomedical images. GANs consist of two models. The generator, a model that learns how to produce synthetic images based on the feedback it receives. The discriminator, a model that classifies an image as synthetic or real and provides feedback to the generator. Throughout the training process, a GAN can experience several technical challenges that impede the generation of suitable synthetic imagery. First, the mode collapse problem whereby the generator either produces an identical image or produces a uniform image from distinct input features. Second, the non-convergence problem whereby the gradient descent optimizer fails to reach a Nash equilibrium. Thirdly, the vanishing gradient problem whereby unstable training behavior occurs due to the discriminator achieving optimal classification performance resulting in no meaningful feedback being provided to the generator. These problems result in the production of synthetic imagery that is blurry, unrealistic, and less diverse. To date, there has been no survey article outlining the impact of these technical challenges in the context of the biomedical imagery domain. This work presents a review and taxonomy based on solutions to the training problems of GANs in the biomedical imaging domain. This survey highlights important challenges and outlines future research directions about the training of GANs in the domain of biomedical imagery.



### Open Source Handwritten Text Recognition on Medieval Manuscripts using Mixed Models and Document-Specific Finetuning
- **Arxiv ID**: http://arxiv.org/abs/2201.07661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07661v1)
- **Published**: 2022-01-19 15:34:19+00:00
- **Updated**: 2022-01-19 15:34:19+00:00
- **Authors**: Christian Reul, Stefan Tomasek, Florian Langhanki, Uwe Springmann
- **Comment**: None
- **Journal**: None
- **Summary**: This paper deals with the task of practical and open source Handwritten Text Recognition (HTR) on German medieval manuscripts. We report on our efforts to construct mixed recognition models which can be applied out-of-the-box without any further document-specific training but also serve as a starting point for finetuning by training a new model on a few pages of transcribed text (ground truth). To train the mixed models we collected a corpus of 35 manuscripts and ca. 12.5k text lines for two widely used handwriting styles, Gothic and Bastarda cursives. Evaluating the mixed models out-of-the-box on four unseen manuscripts resulted in an average Character Error Rate (CER) of 6.22%. After training on 2, 4 and eventually 32 pages the CER dropped to 3.27%, 2.58%, and 1.65%, respectively. While the in-domain recognition and training of models (Bastarda model to Bastarda material, Gothic to Gothic) unsurprisingly yielded the best results, finetuning out-of-domain models to unseen scripts was still shown to be superior to training from scratch.   Our new mixed models have been made openly available to the community.



### Semi-automatic 3D Object Keypoint Annotation and Detection for the Masses
- **Arxiv ID**: http://arxiv.org/abs/2201.07665v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.07665v1)
- **Published**: 2022-01-19 15:41:54+00:00
- **Updated**: 2022-01-19 15:41:54+00:00
- **Authors**: Kenneth Blomqvist, Jen Jen Chung, Lionel Ott, Roland Siegwart
- **Comment**: Code: https://github.com/ethz-asl/object_keypoints
- **Journal**: None
- **Summary**: Creating computer vision datasets requires careful planning and lots of time and effort. In robotics research, we often have to use standardized objects, such as the YCB object set, for tasks such as object tracking, pose estimation, grasping and manipulation, as there are datasets and pre-learned methods available for these objects. This limits the impact of our research since learning-based computer vision methods can only be used in scenarios that are supported by existing datasets.   In this work, we present a full object keypoint tracking toolkit, encompassing the entire process from data collection, labeling, model learning and evaluation. We present a semi-automatic way of collecting and labeling datasets using a wrist mounted camera on a standard robotic arm. Using our toolkit and method, we are able to obtain a working 3D object keypoint detector and go through the whole process of data collection, annotation and learning in just a couple hours of active time.



### GroupGazer: A Tool to Compute the Gaze per Participant in Groups with integrated Calibration to Map the Gaze Online to a Screen or Beamer Projection
- **Arxiv ID**: http://arxiv.org/abs/2201.07692v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07692v2)
- **Published**: 2022-01-19 16:22:02+00:00
- **Updated**: 2023-03-10 08:48:44+00:00
- **Authors**: Wolfgang Fuhl, Daniel Weber, Shahram Eivazi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present GroupGaze. It is a tool that can be used to calculate the gaze direction and the gaze position of whole groups. GroupGazer calculates the gaze direction of every single person in the image and allows to map these gaze vectors to a projection like a projector. In addition to the person-specific gaze direction, the person affiliation of each gaze vector is stored based on the position in the image. Also, it is possible to save the group attention after a calibration. The software is free to use and requires a simple webcam as well as an NVIDIA GPU and the operating system Windows or Linux.   Link: https://es-cloud.cs.uni-tuebingen.de/d/8e2ab8c3fdd444e1a135/?p=%2FGroupGazer&mode=list



### Visualization and Analysis of Wearable Health Data From COVID-19 Patients
- **Arxiv ID**: http://arxiv.org/abs/2201.07698v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.07698v1)
- **Published**: 2022-01-19 16:32:01+00:00
- **Updated**: 2022-01-19 16:32:01+00:00
- **Authors**: Susanne K. Suter, Georg R. Spinner, Bianca Hoelz, Sofia Rey, Sujeanthraa Thanabalasingam, Jens Eckstein, Sven Hirsch
- **Comment**: 17 pages, 9 figures, conference
- **Journal**: None
- **Summary**: Effective visualizations were evaluated to reveal relevant health patterns from multi-sensor real-time wearable devices that recorded vital signs from patients admitted to hospital with COVID-19. Furthermore, specific challenges associated with wearable health data visualizations, such as fluctuating data quality resulting from compliance problems, time needed to charge the device and technical problems are described. As a primary use case, we examined the detection and communication of relevant health patterns visible in the vital signs acquired by the technology. Customized heat maps and bar charts were used to specifically highlight medically relevant patterns in vital signs. A survey of two medical doctors, one clinical project manager and seven health data science researchers was conducted to evaluate the visualization methods. From a dataset of 84 hospitalized COVID-19 patients, we extracted one typical COVID-19 patient history and based on the visualizations showcased the health history of two noteworthy patients. The visualizations were shown to be effective, simple and intuitive in deducing the health status of patients. For clinical staff who are time-constrained and responsible for numerous patients, such visualization methods can be an effective tool to enable continuous acquisition and monitoring of patients' health statuses even remotely.



### Q-ViT: Fully Differentiable Quantization for Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.07703v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07703v2)
- **Published**: 2022-01-19 16:43:17+00:00
- **Updated**: 2022-09-06 02:40:26+00:00
- **Authors**: Zhexin Li, Tong Yang, Peisong Wang, Jian Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a fully differentiable quantization method for vision transformer (ViT) named as Q-ViT, in which both of the quantization scales and bit-widths are learnable parameters. Specifically, based on our observation that heads in ViT display different quantization robustness, we leverage head-wise bit-width to squeeze the size of Q-ViT while preserving performance. In addition, we propose a novel technique named switchable scale to resolve the convergence problem in the joint training of quantization scales and bit-widths. In this way, Q-ViT pushes the limits of ViT quantization to 3-bit without heavy performance drop. Moreover, we analyze the quantization robustness of every architecture component of ViT and show that the Multi-head Self-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key aspects for ViT quantization. This study provides some insights for further research about ViT quantization. Extensive experiments on different ViT models, such as DeiT and Swin Transformer show the effectiveness of our quantization method. In particular, our method outperforms the state-of-the-art uniform quantization method by 1.5% on DeiT-Tiny.



### Object Detection in Autonomous Vehicles: Status and Open Challenges
- **Arxiv ID**: http://arxiv.org/abs/2201.07706v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.07706v1)
- **Published**: 2022-01-19 16:45:16+00:00
- **Updated**: 2022-01-19 16:45:16+00:00
- **Authors**: Abhishek Balasubramaniam, Sudeep Pasricha
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is a computer vision task that has become an integral part of many consumer applications today such as surveillance and security systems, mobile text recognition, and diagnosing diseases from MRI/CT scans. Object detection is also one of the critical components to support autonomous driving. Autonomous vehicles rely on the perception of their surroundings to ensure safe and robust driving performance. This perception system uses object detection algorithms to accurately determine objects such as pedestrians, vehicles, traffic signs, and barriers in the vehicle's vicinity. Deep learning-based object detectors play a vital role in finding and localizing these objects in real-time. This article discusses the state-of-the-art in object detectors and open challenges for their integration into autonomous vehicles.



### A pipeline for automated processing of Corona KH-4 (1962-1972) stereo imagery
- **Arxiv ID**: http://arxiv.org/abs/2201.07756v1
- **DOI**: 10.1109/TGRS.2022.3200151
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.07756v1)
- **Published**: 2022-01-19 17:52:33+00:00
- **Updated**: 2022-01-19 17:52:33+00:00
- **Authors**: Sajid Ghuffar, Tobias Bolch, Ewelina Rupnik, Atanu Bhattacharya
- **Comment**: 24 Pages, 16 Figures
- **Journal**: None
- **Summary**: The Corona KH-4 reconnaissance satellite missions from 1962-1972 acquired panoramic stereo imagery with high spatial resolution of 1.8-7.5 m. The potential of 800,000+ declassified Corona images has not been leveraged due to the complexities arising from handling of panoramic imaging geometry, film distortions and limited availability of the metadata required for georeferencing of the Corona imagery. This paper presents Corona Stereo Pipeline (CoSP): A pipeline for processing of Corona KH-4 stereo panoramic imagery. CoSP utlizes a deep learning based feature matcher SuperGlue to automatically match features point between Corona KH-4 images and recent satellite imagery to generate Ground Control Points (GCPs). To model the imaging geometry and the scanning motion of the panoramic KH-4 cameras, a rigorous camera model consisting of modified collinearity equations with time dependent exterior orientation parameters is employed. The results show that using the entire frame of the Corona image, bundle adjustment using well-distributed GCPs results in an average standard deviation (SD) of less than 2 pixels. The distortion pattern of image residuals of GCPs and y-parallax in epipolar resampled images suggest that film distortions due to long term storage as likely cause of systematic deviations. Compared to the SRTM DEM, the Corona DEM computed using CoSP achieved a Normalized Median Absolute Deviation (NMAD) of elevation differences of ~4 m over an area of approx. 4000 $km^2$. We show that the proposed pipeline can be applied to sequence of complex scenes involving high relief and glacierized terrain and that the resulting DEMs can be used to compute long term glacier elevation changes over large areas.



### Look Closer: Bridging Egocentric and Third-Person Views with Transformers for Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2201.07779v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.07779v2)
- **Published**: 2022-01-19 18:39:03+00:00
- **Updated**: 2022-01-20 10:12:14+00:00
- **Authors**: Rishabh Jangir, Nicklas Hansen, Sambaran Ghosal, Mohit Jain, Xiaolong Wang
- **Comment**: Accepted in Robotics and Automation Letters Journal (RA-L 2022).
  Website at https://jangirrishabh.github.io/lookcloser .8 Pages
- **Journal**: None
- **Summary**: Learning to solve precision-based manipulation tasks from visual feedback using Reinforcement Learning (RL) could drastically reduce the engineering efforts required by traditional robot systems. However, performing fine-grained motor control from visual inputs alone is challenging, especially with a static third-person camera as often used in previous work. We propose a setting for robotic manipulation in which the agent receives visual feedback from both a third-person camera and an egocentric camera mounted on the robot's wrist. While the third-person camera is static, the egocentric camera enables the robot to actively control its vision to aid in precise manipulation. To fuse visual information from both cameras effectively, we additionally propose to use Transformers with a cross-view attention mechanism that models spatial attention from one view to another (and vice-versa), and use the learned features as input to an RL policy. Our method improves learning over strong single-view and multi-view baselines, and successfully transfers to a set of challenging manipulation tasks on a real robot with uncalibrated cameras, no access to state information, and a high degree of task variability. In a hammer manipulation task, our method succeeds in 75% of trials versus 38% and 13% for multi-view and single-view baselines, respectively.



### Towards a General Deep Feature Extractor for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.07781v1
- **DOI**: 10.1109/ICIP42928.2021.9506025
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.07781v1)
- **Published**: 2022-01-19 18:42:23+00:00
- **Updated**: 2022-01-19 18:42:23+00:00
- **Authors**: Liam Schoneveld, Alice Othmani
- **Comment**: Published in: 2021 IEEE International Conference on Image Processing
  (ICIP). arXiv admin note: text overlap with arXiv:2103.09154
- **Journal**: IEEE International Conference on Image Processing (ICIP), 2021,
  pp. 2339-2342
- **Summary**: The human face conveys a significant amount of information. Through facial expressions, the face is able to communicate numerous sentiments without the need for verbalisation. Visual emotion recognition has been extensively studied. Recently several end-to-end trained deep neural networks have been proposed for this task. However, such models often lack generalisation ability across datasets. In this paper, we propose the Deep Facial Expression Vector ExtractoR (DeepFEVER), a new deep learning-based approach that learns a visual feature extractor general enough to be applied to any other facial emotion recognition task or dataset. DeepFEVER outperforms state-of-the-art results on the AffectNet and Google Facial Expression Comparison datasets. DeepFEVER's extracted features also generalise extremely well to other datasets -- even those unseen during training -- namely, the Real-World Affective Faces (RAF) dataset.



### Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation
- **Arxiv ID**: http://arxiv.org/abs/2201.07786v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2201.07786v1)
- **Published**: 2022-01-19 18:54:41+00:00
- **Updated**: 2022-01-19 18:54:41+00:00
- **Authors**: Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne Wu, Bolei Zhou
- **Comment**: 12 pages, 3 figures. Project page:
  https://alvinliu0.github.io/projects/SSP-NeRF
- **Journal**: None
- **Summary**: Animating high-fidelity video portrait with speech audio is crucial for virtual reality and digital entertainment. While most previous studies rely on accurate explicit structural information, recent works explore the implicit scene representation of Neural Radiance Fields (NeRF) for realistic generation. In order to capture the inconsistent motions as well as the semantic difference between human head and torso, some work models them via two individual sets of NeRF, leading to unnatural results. In this work, we propose Semantic-aware Speaking Portrait NeRF (SSP-NeRF), which creates delicate audio-driven portraits using one unified set of NeRF. The proposed model can handle the detailed local facial semantics and the global head-torso relationship through two semantic-aware modules. Specifically, we first propose a Semantic-Aware Dynamic Ray Sampling module with an additional parsing branch that facilitates audio-driven volume rendering. Moreover, to enable portrait rendering in one unified neural radiance field, a Torso Deformation module is designed to stabilize the large-scale non-rigid torso motions. Extensive evaluations demonstrate that our proposed approach renders more realistic video portraits compared to previous methods. Project page: https://alvinliu0.github.io/projects/SSP-NeRF



### ConDor: Self-Supervised Canonicalization of 3D Pose for Partial Shapes
- **Arxiv ID**: http://arxiv.org/abs/2201.07788v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.07788v2)
- **Published**: 2022-01-19 18:57:21+00:00
- **Updated**: 2022-04-14 08:01:46+00:00
- **Authors**: Rahul Sajnani, Adrien Poulenard, Jivitesh Jain, Radhika Dua, Leonidas J. Guibas, Srinath Sridhar
- **Comment**: Accepted to CVPR 2022, New Orleans, Louisiana. For project page and
  code, see https://ivl.cs.brown.edu/ConDor/
- **Journal**: None
- **Summary**: Progress in 3D object understanding has relied on manually canonicalized shape datasets that contain instances with consistent position and orientation (3D pose). This has made it hard to generalize these methods to in-the-wild shapes, eg., from internet model collections or depth sensors. ConDor is a self-supervised method that learns to Canonicalize the 3D orientation and position for full and partial 3D point clouds. We build on top of Tensor Field Networks (TFNs), a class of permutation- and rotation-equivariant, and translation-invariant 3D networks. During inference, our method takes an unseen full or partial 3D point cloud at an arbitrary pose and outputs an equivariant canonical pose. During training, this network uses self-supervision losses to learn the canonical pose from an un-canonicalized collection of full and partial 3D point clouds. ConDor can also learn to consistently co-segment object parts without any supervision. Extensive quantitative results on four new metrics show that our approach outperforms existing methods while enabling new applications such as operation on depth images and annotation transfer.



### BLINC: Lightweight Bimodal Learning for Low-Complexity VVC Intra Coding
- **Arxiv ID**: http://arxiv.org/abs/2201.07823v1
- **DOI**: 10.1007/s11554-022-01223-1
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.07823v1)
- **Published**: 2022-01-19 19:12:41+00:00
- **Updated**: 2022-01-19 19:12:41+00:00
- **Authors**: Farhad Pakdaman, Mohammad Ali Adelimanesh, Mahmoud Reza Hashemi
- **Comment**: None
- **Journal**: Journal of Real-Time Image Processing (2022)
- **Summary**: The latest video coding standard, Versatile Video Coding (VVC), achieves almost twice coding efficiency compared to its predecessor, the High Efficiency Video Coding (HEVC). However, achieving this efficiency (for intra coding) requires 31x computational complexity compared to HEVC, making it challenging for low power and real-time applications. This paper, proposes a novel machine learning approach that jointly and separately employs two modalities of features, to simplify the intra coding decision. First a set of features are extracted that use the existing DCT core of VVC, to assess the texture characteristics, and forms the first modality of data. This produces high quality features with almost no overhead. The distribution of intra modes at the neighboring blocks is also used to form the second modality of data, which provides statistical information about the frame. Second, a two-step feature reduction method is designed that reduces the size of feature set, such that a lightweight model with a limited number of parameters can be used to learn the intra mode decision task. Third, three separate training strategies are proposed (1) an offline training strategy using the first (single) modality of data, (2) an online training strategy that uses the second (single) modality, and (3) a mixed online-offline strategy that uses bimodal learning. Finally, a low-complexity encoding algorithms is proposed based on the proposed learning strategies. Extensive experimental results show that the proposed methods can reduce up to 24% of encoding time, with a negligible loss of coding efficiency. Moreover, it is demonstrated how a bimodal learning strategy can boost the performance of learning. Lastly, the proposed method has a very low computational overhead (0.2%), and uses existing components of a VVC encoder, which makes it much more practical compared to competing solutions.



### ROS georegistration: Aerial Multi-spectral Image Simulator for the Robot Operating System
- **Arxiv ID**: http://arxiv.org/abs/2201.07863v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.07863v1)
- **Published**: 2022-01-19 21:09:50+00:00
- **Updated**: 2022-01-19 21:09:50+00:00
- **Authors**: Andrew R. Willis, Kevin Brink, Kathleen Dipple
- **Comment**: None
- **Journal**: None
- **Summary**: This article describes a software package called ROS georegistration intended for use with the Robot Operating System (ROS) and the Gazebo 3D simulation environment. ROSgeoregistration provides tools for the simulation, test and deployment of aerial georegistration algorithms and is made available with a link provided in the paper. A model creation package is provided which downloads multi-spectral images from the Google Earth Engine database and, if necessary, incorporates these images into a single, possibly very large, reference image. Additionally a Gazebo plugin which uses the real-time sensor pose and image formation model to generate simulated imagery using the specified reference image is provided along with related plugins for UAV relevant data. The novelty of this work is threefold: (1) this is the first system to link the massive multi-spectral imaging database of Google's Earth Engine to the Gazebo simulator, (2) this is the first example of a system that can simulate geospatially and radiometrically accurate imagery from multiple sensor views of the same terrain region, and (3) integration with other UAS tools creates a new holistic UAS simulation environment to support UAS system and subsystem development where real-world testing would generally be prohibitive. Sensed imagery and ground truth registration information is published to client applications which can receive imagery synchronously with telemetry from other payload sensors, e.g., IMU, GPS/GNSS, barometer, and windspeed sensor data. To highlight functionality, we demonstrate ROSgeoregistration for simulating Electro-Optical (EO) and Synthetic Aperture Radar (SAR) image sensors and an example use case for developing and evaluating image-based UAS position feedback, i.e., pose for image-based Guidance Navigation and Control (GNC) applications.



### Enhanced Performance of Pre-Trained Networks by Matched Augmentation Distributions
- **Arxiv ID**: http://arxiv.org/abs/2201.07894v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T45, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2201.07894v1)
- **Published**: 2022-01-19 22:33:00+00:00
- **Updated**: 2022-01-19 22:33:00+00:00
- **Authors**: Touqeer Ahmad, Mohsen Jafarzadeh, Akshay Raj Dhamija, Ryan Rabinowitz, Steve Cruz, Chunchun Li, Terrance E. Boult
- **Comment**: None
- **Journal**: None
- **Summary**: There exists a distribution discrepancy between training and testing, in the way images are fed to modern CNNs. Recent work tried to bridge this gap either by fine-tuning or re-training the network at different resolutions. However re-training a network is rarely cheap and not always viable. To this end, we propose a simple solution to address the train-test distributional shift and enhance the performance of pre-trained models -- which commonly ship as a package with deep learning platforms \eg, PyTorch. Specifically, we demonstrate that running inference on the center crop of an image is not always the best as important discriminatory information may be cropped-off. Instead we propose to combine results for multiple random crops for a test image. This not only matches the train time augmentation but also provides the full coverage of the input image. We explore combining representation of random crops through averaging at different levels \ie, deep feature level, logit level, and softmax level. We demonstrate that, for various families of modern deep networks, such averaging results in better validation accuracy compared to using a single central crop per image. The softmax averaging results in the best performance for various pre-trained networks without requiring any re-training or fine-tuning whatsoever. On modern GPUs with batch processing, the paper's approach to inference of pre-trained networks, is essentially free as all images in a batch can all be processed at once.



### ASL Video Corpora & Sign Bank: Resources Available through the American Sign Language Linguistic Research Project (ASLLRP)
- **Arxiv ID**: http://arxiv.org/abs/2201.07899v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.07899v1)
- **Published**: 2022-01-19 22:48:36+00:00
- **Updated**: 2022-01-19 22:48:36+00:00
- **Authors**: Carol Neidle, Augustine Opoku, Dimitris Metaxas
- **Comment**: None
- **Journal**: None
- **Summary**: The American Sign Language Linguistic Research Project (ASLLRP) provides Internet access to high-quality ASL video data, generally including front and side views and a close-up of the face. The manual and non-manual components of the signing have been linguistically annotated using SignStream(R). The recently expanded video corpora can be browsed and searched through the Data Access Interface (DAI 2) we have designed; it is possible to carry out complex searches. The data from our corpora can also be downloaded; annotations are available in an XML export format. We have also developed the ASLLRP Sign Bank, which contains almost 6,000 sign entries for lexical signs, with distinct English-based glosses, with a total of 41,830 examples of lexical signs (in addition to about 300 gestures, over 1,000 fingerspelled signs, and 475 classifier examples). The Sign Bank is likewise accessible and searchable on the Internet; it can also be accessed from within SignStream(R) (software to facilitate linguistic annotation and analysis of visual language data) to make annotations more accurate and efficient. Here we describe the available resources. These data have been used for many types of research in linguistics and in computer-based sign language recognition from video; examples of such research are provided in the latter part of this article.



### The Role of Facial Expressions and Emotion in ASL
- **Arxiv ID**: http://arxiv.org/abs/2201.07906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07906v1)
- **Published**: 2022-01-19 23:11:48+00:00
- **Updated**: 2022-01-19 23:11:48+00:00
- **Authors**: Lee Kezar, Pei Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: There is little prior work on quantifying the relationships between facial expressions and emotionality in American Sign Language. In this final report, we provide two methods for studying these relationships through probability and prediction. Using a large corpus of natural signing manually annotated with facial features paired with lexical emotion datasets, we find that there exist many relationships between emotionality and the face, and that a simple classifier can predict what someone is saying in terms of broad emotional categories only by looking at the face.



