# Arxiv Papers in cs.CV on 2022-01-09
### Resolving Camera Position for a Practical Application of Gaze Estimation on Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/2201.02946v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.02946v2)
- **Published**: 2022-01-09 07:19:59+00:00
- **Updated**: 2022-01-15 23:20:22+00:00
- **Authors**: Linh Van Ma, Tin Trung Tran, Moongu Jeon
- **Comment**: 6 pages, 11 figures, conference paper
- **Journal**: ICAIIC 2022 (The 4th International Conference on Artificial
  Intelligence in Information and Communication February 21 (Mon.) ~ 24
  (Thur.), 2022, Guam, USA & Virtual Conference)
- **Summary**: Most Gaze estimation research only works on a setup condition that a camera perfectly captures eyes gaze. They have not literarily specified how to set up a camera correctly for a given position of a person. In this paper, we carry out a study on gaze estimation with a logical camera setup position. We further bring our research in a practical application by using inexpensive edge devices with a realistic scenario. That is, we first set up a shopping environment where we want to grasp customers gazing behaviors. This setup needs an optimal camera position in order to maintain estimation accuracy from existing gaze estimation research. We then apply the state-of-the-art of few-shot learning gaze estimation to reduce training sampling in the inference phase. In the experiment, we perform our implemented research on NVIDIA Jetson TX2 and achieve a reasonable speed, 12 FPS which is faster compared with our reference work, without much degradation of gaze estimation accuracy. The source code is released at https://github.com/linh-gist/GazeEstimationTX2.



### Box2Seg: Learning Semantics of 3D Point Clouds with Box-Level Supervision
- **Arxiv ID**: http://arxiv.org/abs/2201.02963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.02963v1)
- **Published**: 2022-01-09 09:07:48+00:00
- **Updated**: 2022-01-09 09:07:48+00:00
- **Authors**: Yan Liu, Qingyong Hu, Yinjie Lei, Kai Xu, Jonathan Li, Yulan Guo
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Learning dense point-wise semantics from unstructured 3D point clouds with fewer labels, although a realistic problem, has been under-explored in literature. While existing weakly supervised methods can effectively learn semantics with only a small fraction of point-level annotations, we find that the vanilla bounding box-level annotation is also informative for semantic segmentation of large-scale 3D point clouds. In this paper, we introduce a neural architecture, termed Box2Seg, to learn point-level semantics of 3D point clouds with bounding box-level supervision. The key to our approach is to generate accurate pseudo labels by exploring the geometric and topological structure inside and outside each bounding box. Specifically, an attention-based self-training (AST) technique and Point Class Activation Mapping (PCAM) are utilized to estimate pseudo-labels. The network is further trained and refined with pseudo labels. Experiments on two large-scale benchmarks including S3DIS and ScanNet demonstrate the competitive performance of the proposed method. In particular, the proposed network can be trained with cheap, or even off-the-shelf bounding box-level annotations and subcloud-level tags.



### MAXIM: Multi-Axis MLP for Image Processing
- **Arxiv ID**: http://arxiv.org/abs/2201.02973v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.02973v2)
- **Published**: 2022-01-09 09:59:32+00:00
- **Updated**: 2022-04-01 21:02:35+00:00
- **Authors**: Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li
- **Comment**: CVPR 2022 Oral; Code: \url{https://github.com/google-research/maxim}
- **Journal**: None
- **Summary**: Recent progress on Transformers and multi-layer perceptron (MLP) models provide new network architectural designs for computer vision tasks. Although these models proved to be effective in many vision tasks such as image recognition, there remain challenges in adapting them for low-level vision. The inflexibility to support high-resolution images and limitations of local attention are perhaps the main bottlenecks. In this work, we present a multi-axis MLP based architecture called MAXIM, that can serve as an efficient and flexible general-purpose vision backbone for image processing tasks. MAXIM uses a UNet-shaped hierarchical structure and supports long-range interactions enabled by spatially-gated MLPs. Specifically, MAXIM contains two MLP-based building blocks: a multi-axis gated MLP that allows for efficient and scalable spatial mixing of local and global visual cues, and a cross-gating block, an alternative to cross-attention, which accounts for cross-feature conditioning. Both these modules are exclusively based on MLPs, but also benefit from being both global and `fully-convolutional', two properties that are desirable for image processing. Our extensive experimental results show that the proposed MAXIM model achieves state-of-the-art performance on more than ten benchmarks across a range of image processing tasks, including denoising, deblurring, deraining, dehazing, and enhancement while requiring fewer or comparable numbers of parameters and FLOPs than competitive models. The source code and trained models will be available at \url{https://github.com/google-research/maxim}.



### Enhanced total variation minimization for stable image reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2201.02979v2
- **DOI**: 10.1088/1361-6420/acd4e1
- **Categories**: **eess.IV**, cs.CV, cs.IT, cs.NA, math.IT, math.NA, 94A08, 94A20, 68U10, 68Q25
- **Links**: [PDF](http://arxiv.org/pdf/2201.02979v2)
- **Published**: 2022-01-09 10:24:02+00:00
- **Updated**: 2022-08-19 04:26:10+00:00
- **Authors**: Congpei An, Hao-Ning Wu, Xiaoming Yuan
- **Comment**: 29 pages, 8 figures
- **Journal**: None
- **Summary**: The total variation (TV) regularization has phenomenally boosted various variational models for image processing tasks. We propose to combine the backward diffusion process in the earlier literature of image enhancement with the TV regularization, and show that the resulting enhanced TV minimization model is particularly effective for reducing the loss of contrast. The main purpose of this paper is to establish stable reconstruction guarantees for the enhanced TV model from noisy subsampled measurements with two sampling strategies, non-adaptive sampling for general linear measurements and variable-density sampling for Fourier measurements. In particular, under some weaker restricted isometry property conditions, the enhanced TV minimization model is shown to have tighter reconstruction error bounds than various TV-based models for the scenario where the level of noise is significant and the amount of measurements is limited. Advantages of the enhanced TV model are also numerically validated by preliminary experiments on the reconstruction of some synthetic, natural, and medical images.



### Invariance encoding in sliced-Wasserstein space for image classification with limited training data
- **Arxiv ID**: http://arxiv.org/abs/2201.02980v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.02980v3)
- **Published**: 2022-01-09 10:25:27+00:00
- **Updated**: 2022-07-24 17:47:52+00:00
- **Authors**: Mohammad Shifat E Rabbi, Yan Zhuang, Shiying Li, Abu Hasnat Mohammad Rubaiyat, Xuwang Yin, Gustavo K. Rohde
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) are broadly considered to be state-of-the-art generic end-to-end image classification systems. However, they are known to underperform when training data are limited and thus require data augmentation strategies that render the method computationally expensive and not always effective. Rather than using a data augmentation strategy to encode invariances as typically done in machine learning, here we propose to mathematically augment a nearest subspace classification model in sliced-Wasserstein space by exploiting certain mathematical properties of the Radon Cumulative Distribution Transform (R-CDT), a recently introduced image transform. We demonstrate that for a particular type of learning problem, our mathematical solution has advantages over data augmentation with deep CNNs in terms of classification accuracy and computational complexity, and is particularly effective under a limited training data setting. The method is simple, effective, computationally efficient, non-iterative, and requires no parameters to be tuned. Python code implementing our method is available at https://github.com/rohdelab/mathematical_augmentation. Our method is integrated as a part of the software package PyTransKit, which is available at https://github.com/rohdelab/PyTransKit.



### A Survey on Face Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/2201.02991v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.02991v1)
- **Published**: 2022-01-09 11:47:29+00:00
- **Updated**: 2022-01-09 11:47:29+00:00
- **Authors**: Jash Dalvi, Sanket Bafna, Devansh Bagaria, Shyamal Virnodkar
- **Comment**: None
- **Journal**: None
- **Summary**: Face Recognition has proven to be one of the most successful technology and has impacted heterogeneous domains. Deep learning has proven to be the most successful at computer vision tasks because of its convolution-based architecture. Since the advent of deep learning, face recognition technology has had a substantial increase in its accuracy. In this paper, some of the most impactful face recognition systems were surveyed. Firstly, the paper gives an overview of a general face recognition system. Secondly, the survey covers various network architectures and training losses that have had a substantial impact. Finally, the paper talks about various databases that are used to evaluate the capabilities of a face recognition system.



### MaskMTL: Attribute prediction in masked facial images with deep multitask learning
- **Arxiv ID**: http://arxiv.org/abs/2201.03002v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03002v2)
- **Published**: 2022-01-09 13:03:29+00:00
- **Updated**: 2022-01-11 11:12:59+00:00
- **Authors**: Prerana Mukherjee, Vinay Kaushik, Ronak Gupta, Ritika Jha, Daneshwari Kankanwadi, Brejesh Lall
- **Comment**: In Proceedings of 9th International Conference on Pattern Recognition
  and Machine Intelligence (PReMI 2021), Kolkata, India
- **Journal**: None
- **Summary**: Predicting attributes in the landmark free facial images is itself a challenging task which gets further complicated when the face gets occluded due to the usage of masks. Smart access control gates which utilize identity verification or the secure login to personal electronic gadgets may utilize face as a biometric trait. Particularly, the Covid-19 pandemic increasingly validates the essentiality of hygienic and contactless identity verification. In such cases, the usage of masks become more inevitable and performing attribute prediction helps in segregating the target vulnerable groups from community spread or ensuring social distancing for them in a collaborative environment. We create a masked face dataset by efficiently overlaying masks of different shape, size and textures to effectively model variability generated by wearing mask. This paper presents a deep Multi-Task Learning (MTL) approach to jointly estimate various heterogeneous attributes from a single masked facial image. Experimental results on benchmark face attribute UTKFace dataset demonstrate that the proposed approach supersedes in performance to other competing techniques. The source code is available at https://github.com/ritikajha/Attribute-prediction-in-masked-facial-images-with-deep-multitask-learning



### ThreshNet: An Efficient DenseNet Using Threshold Mechanism to Reduce Connections
- **Arxiv ID**: http://arxiv.org/abs/2201.03013v2
- **DOI**: 10.1109/ACCESS.2022.3196492
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03013v2)
- **Published**: 2022-01-09 13:52:16+00:00
- **Updated**: 2022-08-07 12:33:45+00:00
- **Authors**: Rui-Yang Ju, Ting-Yu Lin, Jia-Hao Jian, Jen-Shiun Chiang, Wei-Bin Yang
- **Comment**: IEEE Access
- **Journal**: None
- **Summary**: With the continuous development of neural networks for computer vision tasks, more and more network architectures have achieved outstanding success. As one of the most advanced neural network architectures, DenseNet shortcuts all feature maps to solve the model depth problem. Although this network architecture has excellent accuracy with low parameters, it requires an excessive inference time. To solve this problem, HarDNet reduces the connections between the feature maps, making the remaining connections resemble harmonic waves. However, this compression method may result in a decrease in the model accuracy and an increase in the parameters and model size. This network architecture may reduce the memory access time, but its overall performance can still be improved. Therefore, we propose a new network architecture, ThreshNet, using a threshold mechanism to further optimize the connection method. Different numbers of connections for different convolution layers are discarded to accelerate the inference of the network. The proposed network has been evaluated with image classification using CIFAR 10 and SVHN datasets under platforms of NVIDIA RTX 3050 and Raspberry Pi 4. The experimental results show that, compared with HarDNet68, GhostNet, MobileNetV2, ShuffleNet, and EfficientNet, the inference time of the proposed ThreshNet79 is 5%, 9%, 10%, 18%, and 20% faster, respectively. The number of parameters of ThreshNet95 is 55% less than that of HarDNet85. The new model compression and model acceleration methods can speed up the inference time, enabling network models to operate on mobile devices.



### Glance and Focus Networks for Dynamic Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.03014v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03014v2)
- **Published**: 2022-01-09 14:00:56+00:00
- **Updated**: 2022-08-04 10:29:41+00:00
- **Authors**: Gao Huang, Yulin Wang, Kangchen Lv, Haojun Jiang, Wenhui Huang, Pengfei Qi, Shiji Song
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI). Journal version of arXiv:2010.05300 (NeurIPS 2020).
  The first two authors contributed equally
- **Journal**: None
- **Summary**: Spatial redundancy widely exists in visual recognition tasks, i.e., discriminative features in an image or video frame usually correspond to only a subset of pixels, while the remaining regions are irrelevant to the task at hand. Therefore, static models which process all the pixels with an equal amount of computation result in considerable redundancy in terms of time and space consumption. In this paper, we formulate the image recognition problem as a sequential coarse-to-fine feature learning process, mimicking the human visual system. Specifically, the proposed Glance and Focus Network (GFNet) first extracts a quick global representation of the input image at a low resolution scale, and then strategically attends to a series of salient (small) regions to learn finer features. The sequential process naturally facilitates adaptive inference at test time, as it can be terminated once the model is sufficiently confident about its prediction, avoiding further redundant computation. It is worth noting that the problem of locating discriminant regions in our model is formulated as a reinforcement learning task, thus requiring no additional manual annotations other than classification labels. GFNet is general and flexible as it is compatible with any off-the-shelf backbone models (such as MobileNets, EfficientNets and TSM), which can be conveniently deployed as the feature extractor. Extensive experiments on a variety of image classification and video recognition tasks and with various backbone models demonstrate the remarkable efficiency of our method. For example, it reduces the average latency of the highly efficient MobileNet-V3 on an iPhone XS Max by 1.3x without sacrificing accuracy. Code and pre-trained models are available at https://github.com/blackfeather-wang/GFNet-Pytorch.



### Learning from Synthetic InSAR with Vision Transformers: The case of volcanic unrest detection
- **Arxiv ID**: http://arxiv.org/abs/2201.03016v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.03016v2)
- **Published**: 2022-01-09 14:03:00+00:00
- **Updated**: 2022-06-16 13:57:40+00:00
- **Authors**: Nikolaos Ioannis Bountos, Dimitrios Michail, Ioannis Papoutsis
- **Comment**: This work has been submitted to the IEEE for possible publication
- **Journal**: None
- **Summary**: The detection of early signs of volcanic unrest preceding an eruption, in the form of ground deformation in Interferometric Synthetic Aperture Radar (InSAR) data is critical for assessing volcanic hazard. In this work we treat this as a binary classification problem of InSAR images, and propose a novel deep learning methodology that exploits a rich source of synthetically generated interferograms to train quality classifiers that perform equally well in real interferograms. The imbalanced nature of the problem, with orders of magnitude fewer positive samples, coupled with the lack of a curated database with labeled InSAR data, sets a challenging task for conventional deep learning architectures. We propose a new framework for domain adaptation, in which we learn class prototypes from synthetic data with vision transformers. We report detection accuracy that amounts to the highest reported accuracy on a large test set for volcanic unrest detection. Moreover, we built upon this knowledge by learning a new, non-linear, projection between the learnt representations and prototype space, using pseudo labels produced by our model from an unlabeled real InSAR dataset. This leads to the new state of the art with 97.1% accuracy on our test set. We demonstrate the robustness of our approach by training a simple ResNet-18 Convolutional Neural Network on the unlabeled real InSAR dataset with pseudo-labels generated from our top transformer-prototype model. Our methodology provides a significant improvement in performance without the need of manually labeling any sample, opening the road for further exploitation of synthetic InSAR data in various remote sensing applications.



### Self-Supervised Feature Learning from Partial Point Clouds via Pose Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2201.03018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03018v1)
- **Published**: 2022-01-09 14:12:50+00:00
- **Updated**: 2022-01-09 14:12:50+00:00
- **Authors**: Meng-Shiun Tsai, Pei-Ze Chiang, Yi-Hsuan Tsai, Wei-Chen Chiu
- **Comment**: 10 pages, 4 figures and 6 tables
- **Journal**: None
- **Summary**: Self-supervised learning on point clouds has gained a lot of attention recently, since it addresses the label-efficiency and domain-gap problems on point cloud tasks. In this paper, we propose a novel self-supervised framework to learn informative representations from partial point clouds. We leverage partial point clouds scanned by LiDAR that contain both content and pose attributes, and we show that disentangling such two factors from partial point clouds enhances feature representation learning. To this end, our framework consists of three main parts: 1) a completion network to capture holistic semantics of point clouds; 2) a pose regression network to understand the viewing angle where partial data is scanned from; 3) a partial reconstruction network to encourage the model to learn content and pose features. To demonstrate the robustness of the learnt feature representations, we conduct several downstream tasks including classification, part segmentation, and registration, with comparisons against state-of-the-art methods. Our method not only outperforms existing self-supervised methods, but also shows a better generalizability across synthetic and real-world datasets.



### Semantics-driven Attentive Few-shot Learning over Clean and Noisy Samples
- **Arxiv ID**: http://arxiv.org/abs/2201.03043v2
- **DOI**: 10.1016/j.neucom.2022.09.121
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03043v2)
- **Published**: 2022-01-09 16:16:23+00:00
- **Updated**: 2023-02-03 13:53:39+00:00
- **Authors**: Orhun Buğra Baran, Ramazan Gökberk Cinbiş
- **Comment**: 25 pages, 4 figures
- **Journal**: None
- **Summary**: Over the last couple of years few-shot learning (FSL) has attracted great attention towards minimizing the dependency on labeled training examples. An inherent difficulty in FSL is the handling of ambiguities resulting from having too few training samples per class. To tackle this fundamental challenge in FSL, we aim to train meta-learner models that can leverage prior semantic knowledge about novel classes to guide the classifier synthesis process. In particular, we propose semantically-conditioned feature attention and sample attention mechanisms that estimate the importance of representation dimensions and training instances. We also study the problem of sample noise in FSL, towards the utilization of meta-learners in more realistic and imperfect settings. Our experimental results demonstrate the effectiveness of the proposed semantic FSL model with and without sample noise.



### Applying Artificial Intelligence for Age Estimation in Digital Forensic Investigations
- **Arxiv ID**: http://arxiv.org/abs/2201.03045v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03045v1)
- **Published**: 2022-01-09 16:25:37+00:00
- **Updated**: 2022-01-09 16:25:37+00:00
- **Authors**: Thomas Grubl, Harjinder Singh Lallie
- **Comment**: None
- **Journal**: None
- **Summary**: The precise age estimation of child sexual abuse and exploitation (CSAE) victims is one of the most significant digital forensic challenges. Investigators often need to determine the age of victims by looking at images and interpreting the sexual development stages and other human characteristics. The main priority - safeguarding children -- is often negatively impacted by a huge forensic backlog, cognitive bias and the immense psychological stress that this work can entail. This paper evaluates existing facial image datasets and proposes a new dataset tailored to the needs of similar digital forensic research contributions. This small, diverse dataset of 0 to 20-year-old individuals contains 245 images and is merged with 82 unique images from the FG-NET dataset, thus achieving a total of 327 images with high image diversity and low age range density. The new dataset is tested on the Deep EXpectation (DEX) algorithm pre-trained on the IMDB-WIKI dataset. The overall results for young adolescents aged 10 to 15 and older adolescents/adults aged 16 to 20 are very encouraging -- achieving MAEs as low as 1.79, but also suggest that the accuracy for children aged 0 to 10 needs further work. In order to determine the efficacy of the prototype, valuable input of four digital forensic experts, including two forensic investigators, has been taken into account to improve age estimation results. Further research is required to extend datasets both concerning image density and the equal distribution of factors such as gender and racial diversity.



### Lung infection and normal region segmentation from CT volumes of COVID-19 cases
- **Arxiv ID**: http://arxiv.org/abs/2201.03050v1
- **DOI**: 10.1117/12.2582066
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03050v1)
- **Published**: 2022-01-09 16:41:23+00:00
- **Updated**: 2022-01-09 16:41:23+00:00
- **Authors**: Masahiro Oda, Yuichiro Hayashi, Yoshito Otake, Masahiro Hashimoto, Toshiaki Akashi, Kensaku Mori
- **Comment**: Accepted paper as a poster presentation at SPIE Medical Imaging 2021
- **Journal**: Proceedings of SPIE Medical Imaging 2021: Computer-Aided
  Diagnosis, Vol.11597, 115972X-1-6
- **Summary**: This paper proposes an automated segmentation method of infection and normal regions in the lung from CT volumes of COVID-19 patients. From December 2019, novel coronavirus disease 2019 (COVID-19) spreads over the world and giving significant impacts to our economic activities and daily lives. To diagnose the large number of infected patients, diagnosis assistance by computers is needed. Chest CT is effective for diagnosis of viral pneumonia including COVID-19. A quantitative analysis method of condition of the lung from CT volumes by computers is required for diagnosis assistance of COVID-19. This paper proposes an automated segmentation method of infection and normal regions in the lung from CT volumes using a COVID-19 segmentation fully convolutional network (FCN). In diagnosis of lung diseases including COVID-19, analysis of conditions of normal and infection regions in the lung is important. Our method recognizes and segments lung normal and infection regions in CT volumes. To segment infection regions that have various shapes and sizes, we introduced dense pooling connections and dilated convolutions in our FCN. We applied the proposed method to CT volumes of COVID-19 cases. From mild to severe cases of COVID-19, the proposed method correctly segmented normal and infection regions in the lung. Dice scores of normal and infection regions were 0.911 and 0.753, respectively.



### COVID-19 Infection Segmentation from Chest CT Images Based on Scale Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2201.03053v1
- **DOI**: 10.1007/978-3-030-90874-4_9
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03053v1)
- **Published**: 2022-01-09 16:55:22+00:00
- **Updated**: 2022-01-09 16:55:22+00:00
- **Authors**: Masahiro Oda, Tong Zheng, Yuichiro Hayashi, Yoshito Otake, Masahiro Hashimoto, Toshiaki Akashi, Shigeki Aoki, Kensaku Mori
- **Comment**: Accepted paper as a oral presentation at CILP2021, 10th MICCAI CLIP
  Workshop
- **Journal**: DCL 2021, PPML 2021, LL-COVID19 2021, CLIP 2021, Lecture Notes in
  Computer Science (LNCS) 12969, pp.88-97
- **Summary**: This paper proposes a segmentation method of infection regions in the lung from CT volumes of COVID-19 patients. COVID-19 spread worldwide, causing many infected patients and deaths. CT image-based diagnosis of COVID-19 can provide quick and accurate diagnosis results. An automated segmentation method of infection regions in the lung provides a quantitative criterion for diagnosis. Previous methods employ whole 2D image or 3D volume-based processes. Infection regions have a considerable variation in their sizes. Such processes easily miss small infection regions. Patch-based process is effective for segmenting small targets. However, selecting the appropriate patch size is difficult in infection region segmentation. We utilize the scale uncertainty among various receptive field sizes of a segmentation FCN to obtain infection regions. The receptive field sizes can be defined as the patch size and the resolution of volumes where patches are clipped from. This paper proposes an infection segmentation network (ISNet) that performs patch-based segmentation and a scale uncertainty-aware prediction aggregation method that refines the segmentation result. We design ISNet to segment infection regions that have various intensity values. ISNet has multiple encoding paths to process patch volumes normalized by multiple intensity ranges. We collect prediction results generated by ISNets having various receptive field sizes. Scale uncertainty among the prediction results is extracted by the prediction aggregation method. We use an aggregation FCN to generate a refined segmentation result considering scale uncertainty among the predictions. In our experiments using 199 chest CT volumes of COVID-19 cases, the prediction aggregation method improved the dice similarity score from 47.6% to 62.1%.



### The State of Aerial Surveillance: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2201.03080v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03080v2)
- **Published**: 2022-01-09 20:13:27+00:00
- **Updated**: 2022-01-13 01:25:41+00:00
- **Authors**: Kien Nguyen, Clinton Fookes, Sridha Sridharan, Yingli Tian, Feng Liu, Xiaoming Liu, Arun Ross
- **Comment**: None
- **Journal**: None
- **Summary**: The rapid emergence of airborne platforms and imaging sensors are enabling new forms of aerial surveillance due to their unprecedented advantages in scale, mobility, deployment and covert observation capabilities. This paper provides a comprehensive overview of human-centric aerial surveillance tasks from a computer vision and pattern recognition perspective. It aims to provide readers with an in-depth systematic review and technical analysis of the current state of aerial surveillance tasks using drones, UAVs and other airborne platforms. The main object of interest is humans, where single or multiple subjects are to be detected, identified, tracked, re-identified and have their behavior analyzed. More specifically, for each of these four tasks, we first discuss unique challenges in performing these tasks in an aerial setting compared to a ground-based setting. We then review and analyze the aerial datasets publicly available for each task, and delve deep into the approaches in the aerial literature and investigate how they presently address the aerial challenges. We conclude the paper with discussion on the missing gaps and open research questions to inform future research avenues.



### ImageSubject: A Large-scale Dataset for Subject Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.03101v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03101v2)
- **Published**: 2022-01-09 22:49:59+00:00
- **Updated**: 2022-09-15 07:30:48+00:00
- **Authors**: Xin Miao, Jiayi Liu, Huayan Wang, Jun Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Main subjects usually exist in the images or videos, as they are the objects that the photographer wants to highlight. Human viewers can easily identify them but algorithms often confuse them with other objects. Detecting the main subjects is an important technique to help machines understand the content of images and videos. We present a new dataset with the goal of training models to understand the layout of the objects and the context of the image then to find the main subjects among them. This is achieved in three aspects. By gathering images from movie shots created by directors with professional shooting skills, we collect the dataset with strong diversity, specifically, it contains 107\,700 images from 21\,540 movie shots. We labeled them with the bounding box labels for two classes: subject and non-subject foreground object. We present a detailed analysis of the dataset and compare the task with saliency detection and object detection. ImageSubject is the first dataset that tries to localize the subject in an image that the photographer wants to highlight. Moreover, we find the transformer-based detection model offers the best result among other popular model architectures. Finally, we discuss the potential applications and conclude with the importance of the dataset.



### Preserving Domain Private Representation via Mutual Information Maximization
- **Arxiv ID**: http://arxiv.org/abs/2201.03102v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.03102v1)
- **Published**: 2022-01-09 22:55:57+00:00
- **Updated**: 2022-01-09 22:55:57+00:00
- **Authors**: Jiahong Chen, Jing Wang, Weipeng Lin, Kuangen Zhang, Clarence W. de Silva
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in unsupervised domain adaptation have shown that mitigating the domain divergence by extracting the domain-invariant representation could significantly improve the generalization of a model to an unlabeled data domain. Nevertheless, the existing methods fail to effectively preserve the representation that is private to the label-missing domain, which could adversely affect the generalization. In this paper, we propose an approach to preserve such representation so that the latent distribution of the unlabeled domain could represent both the domain-invariant features and the individual characteristics that are private to the unlabeled domain. In particular, we demonstrate that maximizing the mutual information between the unlabeled domain and its latent space while mitigating the domain divergence can achieve such preservation. We also theoretically and empirically validate that preserving the representation that is private to the unlabeled domain is important and of necessity for the cross-domain generalization. Our approach outperforms state-of-the-art methods on several public datasets.



### Signal Reconstruction from Quantized Noisy Samples of the Discrete Fourier Transform
- **Arxiv ID**: http://arxiv.org/abs/2201.03114v1
- **DOI**: 10.1109/TSP.2022.3168127
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.03114v1)
- **Published**: 2022-01-09 23:55:53+00:00
- **Updated**: 2022-01-09 23:55:53+00:00
- **Authors**: Mohak Goyal, Animesh Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present two variations of an algorithm for signal reconstruction from one-bit or two-bit noisy observations of the discrete Fourier transform (DFT). The one-bit observations of the DFT correspond to the sign of its real part, whereas, the two-bit observations of the DFT correspond to the signs of both the real and imaginary parts of the DFT. We focus on images for analysis and simulations, thus using the sign of the 2D-DFT. This choice of the class of signals is inspired by previous works on this problem. For our algorithm, we show that the expected mean squared error (MSE) in signal reconstruction is asymptotically proportional to the inverse of the sampling rate. The samples are affected by additive zero-mean noise of known distribution. We solve this signal estimation problem by designing an algorithm that uses contraction mapping, based on the Banach fixed point theorem. Numerical tests with four benchmark images are provided to show the effectiveness of our algorithm. Various metrics for image reconstruction quality assessment such as PSNR, SSIM, ESSIM, and MS-SSIM are employed. On all four benchmark images, our algorithm outperforms the state-of-the-art in all of these metrics by a significant margin.



