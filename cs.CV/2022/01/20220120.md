# Arxiv Papers in cs.CV on 2022-01-20
### Learning-by-Novel-View-Synthesis for Full-Face Appearance-Based 3D Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/2201.07927v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07927v3)
- **Published**: 2022-01-20 00:29:45+00:00
- **Updated**: 2022-04-27 08:11:57+00:00
- **Authors**: Jiawei Qin, Takuru Shimoyama, Yusuke Sugano
- **Comment**: Camera-ready version for CVPR 2022 Workshop (GAZE 2022)
- **Journal**: None
- **Summary**: Despite recent advances in appearance-based gaze estimation techniques, the need for training data that covers the target head pose and gaze distribution remains a crucial challenge for practical deployment. This work examines a novel approach for synthesizing gaze estimation training data based on monocular 3D face reconstruction. Unlike prior works using multi-view reconstruction, photo-realistic CG models, or generative neural networks, our approach can manipulate and extend the head pose range of existing training data without any additional requirements. We introduce a projective matching procedure to align the reconstructed 3D facial mesh with the camera coordinate system and synthesize face images with accurate gaze labels. We also propose a mask-guided gaze estimation model and data augmentation strategies to further improve the estimation accuracy by taking advantage of synthetic training data. Experiments using multiple public datasets show that our approach significantly improves the estimation performance on challenging cross-dataset settings with non-overlapping gaze distributions.



### Estimating Egocentric 3D Human Pose in the Wild with External Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/2201.07929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07929v1)
- **Published**: 2022-01-20 00:45:13+00:00
- **Updated**: 2022-01-20 00:45:13+00:00
- **Authors**: Jian Wang, Lingjie Liu, Weipeng Xu, Kripasindhu Sarkar, Diogo Luvizon, Christian Theobalt
- **Comment**: None
- **Journal**: None
- **Summary**: Egocentric 3D human pose estimation with a single fisheye camera has drawn a significant amount of attention recently. However, existing methods struggle with pose estimation from in-the-wild images, because they can only be trained on synthetic data due to the unavailability of large-scale in-the-wild egocentric datasets. Furthermore, these methods easily fail when the body parts are occluded by or interacting with the surrounding scene. To address the shortage of in-the-wild data, we collect a large-scale in-the-wild egocentric dataset called Egocentric Poses in the Wild (EgoPW). This dataset is captured by a head-mounted fisheye camera and an auxiliary external camera, which provides an additional observation of the human body from a third-person perspective during training. We present a new egocentric pose estimation method, which can be trained on the new dataset with weak external supervision. Specifically, we first generate pseudo labels for the EgoPW dataset with a spatio-temporal optimization method by incorporating the external-view supervision. The pseudo labels are then used to train an egocentric pose estimation network. To facilitate the network training, we propose a novel learning strategy to supervise the egocentric features with the high-quality features extracted by a pretrained external-view pose estimation model. The experiments show that our method predicts accurate 3D poses from a single in-the-wild egocentric image and outperforms the state-of-the-art methods both quantitatively and qualitatively.



### Improving Specificity in Mammography Using Cross-correlation between Wavelet and Fourier Transform
- **Arxiv ID**: http://arxiv.org/abs/2201.08385v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.08385v2)
- **Published**: 2022-01-20 00:49:33+00:00
- **Updated**: 2022-01-29 02:35:14+00:00
- **Authors**: Liuhua Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer is in the most common malignant tumor in women. It accounted for 30% of new malignant tumor cases. Although the incidence of breast cancer remains high around the world, the mortality rate has been continuously reduced. This is mainly due to recent developments in molecular biology technology and improved level of comprehensive diagnosis and standard treatment. Early detection by mammography is an integral part of that. The most common breast abnormalities that may indicate breast cancer are masses and calcifications. Previous detection approaches usually obtain relatively high sensitivity but unsatisfactory specificity. We will investigate an approach that applies the discrete wavelet transform and Fourier transform to parse the images and extracts statistical features that characterize an image's content, such as the mean intensity and the skewness of the intensity. A naive Bayesian classifier uses these features to classify the images. We expect to achieve an optimal high specificity.



### Experimental Large-Scale Jet Flames' Geometrical Features Extraction for Risk Management Using Infrared Images and Deep Learning Segmentation Methods
- **Arxiv ID**: http://arxiv.org/abs/2201.07931v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.07931v1)
- **Published**: 2022-01-20 00:50:41+00:00
- **Updated**: 2022-01-20 00:50:41+00:00
- **Authors**: Carmina Pérez-Guerrero, Adriana Palacios, Gilberto Ochoa-Ruiz, Christian Mata, Joaquim Casal, Miguel Gonzalez-Mendoza, Luis Eduardo Falcón-Morales
- **Comment**: None
- **Journal**: None
- **Summary**: Jet fires are relatively small and have the least severe effects among the diverse fire accidents that can occur in industrial plants; however, they are usually involved in a process known as the domino effect, that leads to more severe events, such as explosions or the initiation of another fire, making the analysis of such fires an important part of risk analysis. This research work explores the application of deep learning models in an alternative approach that uses the semantic segmentation of jet fires flames to extract main geometrical attributes, relevant for fire risk assessments. A comparison is made between traditional image processing methods and some state-of-the-art deep learning models. It is found that the best approach is a deep learning architecture known as UNet, along with its two improvements, Attention UNet and UNet++. The models are then used to segment a group of vertical jet flames of varying pipe outlet diameters to extract their main geometrical characteristics. Attention UNet obtained the best general performance in the approximation of both height and area of the flames, while also showing a statistically significant difference between it and UNet++. UNet obtained the best overall performance for the approximation of the lift-off distances; however, there is not enough data to prove a statistically significant difference between Attention UNet and UNet++. The only instance where UNet++ outperformed the other models, was while obtaining the lift-off distances of the jet flames with 0.01275 m pipe outlet diameter. In general, the explored models show good agreement between the experimental and predicted values for relatively large turbulent propane jet flames, released in sonic and subsonic regimes; thus, making these radiation zones segmentation models, a suitable approach for different jet flame risk management scenarios.



### GASCN: Graph Attention Shape Completion Network
- **Arxiv ID**: http://arxiv.org/abs/2201.07937v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07937v1)
- **Published**: 2022-01-20 01:03:00+00:00
- **Updated**: 2022-01-20 01:03:00+00:00
- **Authors**: Haojie Huang, Ziyi Yang, Robert Platt
- **Comment**: International Conference on 3D Vision (3DV)
- **Journal**: None
- **Summary**: Shape completion, the problem of inferring the complete geometry of an object given a partial point cloud, is an important problem in robotics and computer vision. This paper proposes the Graph Attention Shape Completion Network (GASCN), a novel neural network model that solves this problem. This model combines a graph-based model for encoding local point cloud information with an MLP-based architecture for encoding global information. For each completed point, our model infers the normal and extent of the local surface patch which is used to produce dense yet precise shape completions. We report experiments that demonstrate that GASCN outperforms standard shape completion methods on a standard benchmark drawn from the Shapenet dataset.



### Self-supervised Video Representation Learning with Cascade Positive Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2201.07989v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07989v4)
- **Published**: 2022-01-20 03:48:57+00:00
- **Updated**: 2022-04-21 02:39:30+00:00
- **Authors**: Cheng-En Wu, Farley Lai, Yu Hen Hu, Asim Kadav
- **Comment**: To appear in CVPR 2022 L3D-IVU Workshop
- **Journal**: None
- **Summary**: Self-supervised video representation learning has been shown to effectively improve downstream tasks such as video retrieval and action recognition. In this paper, we present the Cascade Positive Retrieval (CPR) that successively mines positive examples w.r.t. the query for contrastive learning in a cascade of stages. Specifically, CPR exploits multiple views of a query example in different modalities, where an alternative view may help find another positive example dissimilar in the query view. We explore the effects of possible CPR configurations in ablations including the number of mining stages, the top similar example selection ratio in each stage, and progressive training with an incremental number of the final Top-k selection. The overall mining quality is measured to reflect the recall across training set classes. CPR reaches a median class mining recall of 83.3%, outperforming previous work by 5.5%. Implementation-wise, CPR is complementary to pretext tasks and can be easily applied to previous work. In the evaluation of pretraining on UCF101, CPR consistently improves existing work and even achieves state-of-the-art R@1 of 56.7% and 24.4% in video retrieval as well as 83.8% and 54.8% in action recognition on UCF101 and HMDB51. The code is available at https://github.com/necla-ml/CPR.



### CELESTIAL: Classification Enabled via Labelless Embeddings with Self-supervised Telescope Image Analysis Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.08001v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08001v1)
- **Published**: 2022-01-20 04:59:05+00:00
- **Updated**: 2022-01-20 04:59:05+00:00
- **Authors**: Suhas Kotha, Anirudh Koul, Siddha Ganju, Meher Kasam
- **Comment**: COSPAR 2021 Cross-Disciplinary Workshop on Machine Learning for Space
  Sciences, Sydney, Australia
- **Journal**: None
- **Summary**: A common class of problems in remote sensing is scene classification, a fundamentally important task for natural hazards identification, geographic image retrieval, and environment monitoring. Recent developments in this field rely label-dependent supervised learning techniques which is antithetical to the 35 petabytes of unlabelled satellite imagery in NASA GIBS. To solve this problem, we establish CELESTIAL-a self-supervised learning pipeline for effectively leveraging sparsely-labeled satellite imagery. This pipeline successfully adapts SimCLR, an algorithm that first learns image representations on unlabelled data and then fine-tunes this knowledge on the provided labels. Our results show CELESTIAL requires only a third of the labels that the supervised method needs to attain the same accuracy on an experimental dataset. The first unsupervised tier can enable applications such as reverse image search for NASA Worldview (i.e. searching similar atmospheric phenomenon over years of unlabelled data with minimal samples) and the second supervised tier can lower the necessity of expensive data annotation significantly. In the future, we hope we can generalize the CELESTIAL pipeline to other data types, algorithms, and applications.



### PRMI: A Dataset of Minirhizotron Images for Diverse Plant Root Study
- **Arxiv ID**: http://arxiv.org/abs/2201.08002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08002v1)
- **Published**: 2022-01-20 05:07:41+00:00
- **Updated**: 2022-01-20 05:07:41+00:00
- **Authors**: Weihuang Xu, Guohao Yu, Yiming Cui, Romain Gloaguen, Alina Zare, Jason Bonnette, Joel Reyes-Cabrera, Ashish Rajurkar, Diane Rowland, Roser Matamala, Julie D. Jastrow, Thomas E. Juenger, Felix B. Fritschi
- **Comment**: The 36th AAAI Conference on the AI for Agriculture and Food Systems
  (AIAFS) Workshop
- **Journal**: None
- **Summary**: Understanding a plant's root system architecture (RSA) is crucial for a variety of plant science problem domains including sustainability and climate adaptation. Minirhizotron (MR) technology is a widely-used approach for phenotyping RSA non-destructively by capturing root imagery over time. Precisely segmenting roots from the soil in MR imagery is a critical step in studying RSA features. In this paper, we introduce a large-scale dataset of plant root images captured by MR technology. In total, there are over 72K RGB root images across six different species including cotton, papaya, peanut, sesame, sunflower, and switchgrass in the dataset. The images span a variety of conditions including varied root age, root structures, soil types, and depths under the soil surface. All of the images have been annotated with weak image-level labels indicating whether each image contains roots or not. The image-level labels can be used to support weakly supervised learning in plant root segmentation tasks. In addition, 63K images have been manually annotated to generate pixel-level binary masks indicating whether each pixel corresponds to root or not. These pixel-level binary masks can be used as ground truth for supervised learning in semantic segmentation tasks. By introducing this dataset, we aim to facilitate the automatic segmentation of roots and the research of RSA with deep learning and other image analysis algorithms.



### Steerable Pyramid Transform Enables Robust Left Ventricle Quantification
- **Arxiv ID**: http://arxiv.org/abs/2201.08388v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.08388v1)
- **Published**: 2022-01-20 06:09:56+00:00
- **Updated**: 2022-01-20 06:09:56+00:00
- **Authors**: Xiangyang Zhu, Kede Ma, Wufeng Xue
- **Comment**: 10 pages, 13 figures, journal paper
- **Journal**: None
- **Summary**: Although multifarious variants of convolutional neural networks (CNNs) have proved successful in cardiac index quantification, they seem vulnerable to mild input perturbations, e.g., spatial transformations, image distortions, and adversarial attacks. Such brittleness erodes our trust in CNN-based automated diagnosis of various cardiovascular diseases. In this work, we describe a simple and effective method to learn robust CNNs for left ventricle (LV) quantification, including cavity and myocardium areas, directional dimensions, and regional wall thicknesses. The key to the success of our approach is the use of the biologically-inspired steerable pyramid transform (SPT) as fixed front-end processing, which brings three computational advantages to LV quantification. First, the basis functions of SPT match the anatomical structure of the LV as well as the geometric characteristics of the estimated indices. Second, SPT enables sharing a CNN across different orientations as a form of parameter regularization, and explicitly captures the scale variations of the LV in a natural way. Third, the residual highpass subband can be conveniently discarded to further encourage robust feature learning. A concise and effective metric, named Robustness Ratio, is proposed to evaluate the robustness under various input perturbations. Extensive experiments on 145 cardiac sequences show that our SPT-augmented method performs favorably against state-of-the-art algorithms in terms of prediction accuracy, but is significantly more robust under input perturbations.



### A Joint Morphological Profiles and Patch Tensor Change Detection for Hyperspectral Imagery
- **Arxiv ID**: http://arxiv.org/abs/2201.08027v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2201.08027v1)
- **Published**: 2022-01-20 07:34:17+00:00
- **Updated**: 2022-01-20 07:34:17+00:00
- **Authors**: Zengfu Hou, Wei Li
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-temporal hyperspectral images can be used to detect changed information, which has gradually attracted researchers' attention. However, traditional change detection algorithms have not deeply explored the relevance of spatial and spectral changed features, which leads to low detection accuracy. To better excavate both spectral and spatial information of changed features, a joint morphology and patch-tensor change detection (JMPT) method is proposed. Initially, a patch-based tensor strategy is adopted to exploit similar property of spatial structure, where the non-overlapping local patch image is reshaped into a new tensor cube, and then three-order Tucker decompositon and image reconstruction strategies are adopted to obtain more robust multi-temporal hyperspectral datasets. Meanwhile, multiple morphological profiles including max-tree and min-tree are applied to extract different attributes of multi-temporal images. Finally, these results are fused to general a final change detection map. Experiments conducted on two real hyperspectral datasets demonstrate that the proposed detector achieves better detection performance.



### Domain Generalization via Frequency-domain-based Feature Disentanglement and Interaction
- **Arxiv ID**: http://arxiv.org/abs/2201.08029v2
- **DOI**: 10.1145/3503161.3548267
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08029v2)
- **Published**: 2022-01-20 07:42:12+00:00
- **Updated**: 2022-07-25 16:51:32+00:00
- **Authors**: Jingye Wang, Ruoyi Du, Dongliang Chang, Kongming Liang, Zhanyu Ma
- **Comment**: The paper is accepted by ACM Multimedia 2022
- **Journal**: None
- **Summary**: Adaptation to out-of-distribution data is a meta-challenge for all statistical learning algorithms that strongly rely on the i.i.d. assumption. It leads to unavoidable labor costs and confidence crises in realistic applications. For that, domain generalization aims at mining domain-irrelevant knowledge from multiple source domains that can generalize to unseen target domains. In this paper, by leveraging the frequency domain of an image, we uniquely work with two key observations: (i) the high-frequency information of an image depicts object edge structure, which preserves high-level semantic information of the object is naturally consistent across different domains, and (ii) the low-frequency component retains object smooth structure, while this information is susceptible to domain shifts. Motivated by the above observations, we introduce (i) an encoder-decoder structure to disentangle high- and low-frequency feature of an image, (ii) an information interaction mechanism to ensure the helpful knowledge from both two parts can cooperate effectively, and (iii) a novel data augmentation technique that works on the frequency domain to encourage the robustness of frequency-wise feature disentangling. The proposed method obtains state-of-the-art performance on three widely used domain generalization benchmarks (Digit-DG, Office-Home, and PACS).



### Lightweight Salient Object Detection in Optical Remote Sensing Images via Feature Correlation
- **Arxiv ID**: http://arxiv.org/abs/2201.08049v1
- **DOI**: 10.1109/TGRS.2022.3145483
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08049v1)
- **Published**: 2022-01-20 08:28:01+00:00
- **Updated**: 2022-01-20 08:28:01+00:00
- **Authors**: Gongyang Li, Zhi Liu, Zhen Bai, Weisi Lin, and Haibin Ling
- **Comment**: 11 pages, 6 figures, Accepted by IEEE Transactions on Geoscience and
  Remote Sensing 2022
- **Journal**: None
- **Summary**: Salient object detection in optical remote sensing images (ORSI-SOD) has been widely explored for understanding ORSIs. However, previous methods focus mainly on improving the detection accuracy while neglecting the cost in memory and computation, which may hinder their real-world applications. In this paper, we propose a novel lightweight ORSI-SOD solution, named CorrNet, to address these issues. In CorrNet, we first lighten the backbone (VGG-16) and build a lightweight subnet for feature extraction. Then, following the coarse-to-fine strategy, we generate an initial coarse saliency map from high-level semantic features in a Correlation Module (CorrM). The coarse saliency map serves as the location guidance for low-level features. In CorrM, we mine the object location information between high-level semantic features through the cross-layer correlation operation. Finally, based on low-level detailed features, we refine the coarse saliency map in the refinement subnet equipped with Dense Lightweight Refinement Blocks, and produce the final fine saliency map. By reducing the parameters and computations of each component, CorrNet ends up having only 4.09M parameters and running with 21.09G FLOPs. Experimental results on two public datasets demonstrate that our lightweight CorrNet achieves competitive or even better performance compared with 26 state-of-the-art methods (including 16 large CNN-based methods and 2 lightweight methods), and meanwhile enjoys the clear memory and run time efficiency. The code and results of our method are available at https://github.com/MathLee/CorrNet.



### TerViT: An Efficient Ternary Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.08050v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.08050v2)
- **Published**: 2022-01-20 08:29:19+00:00
- **Updated**: 2022-01-21 05:22:32+00:00
- **Authors**: Sheng Xu, Yanjing Li, Teli Ma, Bohan Zeng, Baochang Zhang, Peng Gao, Jinhu Lv
- **Comment**: None
- **Journal**: None
- **Summary**: Vision transformers (ViTs) have demonstrated great potential in various visual tasks, but suffer from expensive computational and memory cost problems when deployed on resource-constrained devices. In this paper, we introduce a ternary vision transformer (TerViT) to ternarize the weights in ViTs, which are challenged by the large loss surface gap between real-valued and ternary parameters. To address the issue, we introduce a progressive training scheme by first training 8-bit transformers and then TerViT, and achieve a better optimization than conventional methods. Furthermore, we introduce channel-wise ternarization, by partitioning each matrix to different channels, each of which is with an unique distribution and ternarization interval. We apply our methods to popular DeiT and Swin backbones, and extensive results show that we can achieve competitive performance. For example, TerViT can quantize Swin-S to 13.1MB model size while achieving above 79% Top-1 accuracy on ImageNet dataset.



### Predicting Vegetation Stratum Occupancy from Airborne LiDAR Data with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.08051v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08051v1)
- **Published**: 2022-01-20 08:30:27+00:00
- **Updated**: 2022-01-20 08:30:27+00:00
- **Authors**: Ekaterina Kalinicheva, Loic Landrieu, Clément Mallet, Nesrine Chehata
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new deep learning-based method for estimating the occupancy of vegetation strata from airborne 3D LiDAR point clouds. Our model predicts rasterized occupancy maps for three vegetation strata corresponding to lower, medium, and higher cover. Our weakly-supervised training scheme allows our network to only be supervised with vegetation occupancy values aggregated over cylindrical plots containing thousands of points. Such ground truth is easier to produce than pixel-wise or point-wise annotations. Our method outperforms handcrafted and deep learning baselines in terms of precision by up to 30%, while simultaneously providing visual and interpretable predictions. We provide an open-source implementation along with a dataset of 199 agricultural plots to train and evaluate weakly supervised occupancy regression algorithms.



### Temporal Sentence Grounding in Videos: A Survey and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/2201.08071v3
- **DOI**: 10.1109/TPAMI.2023.3258628
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.08071v3)
- **Published**: 2022-01-20 09:10:20+00:00
- **Updated**: 2023-03-13 13:55:30+00:00
- **Authors**: Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)
- **Journal**: None
- **Summary**: Temporal sentence grounding in videos (TSGV), \aka natural language video localization (NLVL) or video moment retrieval (VMR), aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video. Connecting computer vision and natural language, TSGV has drawn significant attention from researchers in both communities. This survey attempts to provide a summary of fundamental concepts in TSGV and current research status, as well as future research directions. As the background, we present a common structure of functional components in TSGV, in a tutorial style: from feature extraction from raw video and language query, to answer prediction of the target moment. Then we review the techniques for multimodal understanding and interaction, which is the key focus of TSGV for effective alignment between the two modalities. We construct a taxonomy of TSGV techniques and elaborate the methods in different categories with their strengths and weaknesses. Lastly, we discuss issues with the current TSGV research and share our insights about promising research directions.



### AirPose: Multi-View Fusion Network for Aerial 3D Human Pose and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/2201.08093v2
- **DOI**: 10.1109/LRA.2022.3145494
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08093v2)
- **Published**: 2022-01-20 09:46:20+00:00
- **Updated**: 2022-02-15 20:37:45+00:00
- **Authors**: Nitin Saini, Elia Bonetto, Eric Price, Aamir Ahmad, Michael J. Black
- **Comment**: None
- **Journal**: None
- **Summary**: In this letter, we present a novel markerless 3D human motion capture (MoCap) system for unstructured, outdoor environments that uses a team of autonomous unmanned aerial vehicles (UAVs) with on-board RGB cameras and computation. Existing methods are limited by calibrated cameras and off-line processing. Thus, we present the first method (AirPose) to estimate human pose and shape using images captured by multiple extrinsically uncalibrated flying cameras. AirPose itself calibrates the cameras relative to the person instead of relying on any pre-calibration. It uses distributed neural networks running on each UAV that communicate viewpoint-independent information with each other about the person (i.e., their 3D shape and articulated pose). The person's shape and pose are parameterized using the SMPL-X body model, resulting in a compact representation, that minimizes communication between the UAVs. The network is trained using synthetic images of realistic virtual environments, and fine-tuned on a small set of real images. We also introduce an optimization-based post-processing method (AirPose$^{+}$) for offline applications that require higher MoCap quality. We make our method's code and data available for research at https://github.com/robot-perception-group/AirPose. A video describing the approach and results is available at https://youtu.be/xLYe1TNHsfs.



### What can we learn from misclassified ImageNet images?
- **Arxiv ID**: http://arxiv.org/abs/2201.08098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08098v1)
- **Published**: 2022-01-20 10:08:38+00:00
- **Updated**: 2022-01-20 10:08:38+00:00
- **Authors**: Shixian Wen, Amanda Sofie Rios, Kiran Lekkala, Laurent Itti
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the patterns of misclassified ImageNet images is particularly important, as it could guide us to design deep neural networks (DNN) that generalize better. However, the richness of ImageNet imposes difficulties for researchers to visually find any useful patterns of misclassification. Here, to help find these patterns, we propose "Superclassing ImageNet dataset". It is a subset of ImageNet which consists of 10 superclasses, each containing 7-116 related subclasses (e.g., 52 bird types, 116 dog types). By training neural networks on this dataset, we found that: (i) Misclassifications are rarely across superclasses, but mainly among subclasses within a superclass. (ii) Ensemble networks trained each only on subclasses of a given superclass perform better than the same network trained on all subclasses of all superclasses. Hence, we propose a two-stage Super-Sub framework, and demonstrate that: (i) The framework improves overall classification performance by 3.3%, by first inferring a superclass using a generalist superclass-level network, and then using a specialized network for final subclass-level classification. (ii) Although the total parameter storage cost increases to a factor N+1 for N superclasses compared to using a single network, with finetuning, delta and quantization aware training techniques this can be reduced to 0.2N+1. Another advantage of this efficient implementation is that the memory cost on the GPU during inference is equivalent to using only one network. The reason is we initiate each subclass-level network through addition of small parameter variations (deltas) to the superclass-level network. (iii) Finally, our framework promises to be more scalable and generalizable than the common alternative of simply scaling up a vanilla network in size, since very large networks often suffer from overfitting and gradient vanishing.



### A Computational Model for Machine Thinking
- **Arxiv ID**: http://arxiv.org/abs/2201.08122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08122v1)
- **Published**: 2022-01-20 12:01:06+00:00
- **Updated**: 2022-01-20 12:01:06+00:00
- **Authors**: Slimane Larabi
- **Comment**: Internal report, RIIMA Laboratory
- **Journal**: None
- **Summary**: A machine thinking model is proposed in this report based on recent advances of computer vision and the recent results of neuroscience devoted to brain understanding. We deliver the result of machine thinking in the form of sentences of natural-language or drawn sketches either informative or decisional. This result is obtained from a reasoning performed on new acquired data and memorized data.



### Deep Unsupervised Contrastive Hashing for Large-Scale Cross-Modal Text-Image Retrieval in Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/2201.08125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08125v1)
- **Published**: 2022-01-20 12:05:10+00:00
- **Updated**: 2022-01-20 12:05:10+00:00
- **Authors**: Georgii Mikriukov, Mahdyar Ravanbakhsh, Begüm Demir
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the availability of large-scale multi-modal data (e.g., satellite images acquired by different sensors, text sentences, etc) archives, the development of cross-modal retrieval systems that can search and retrieve semantically relevant data across different modalities based on a query in any modality has attracted great attention in RS. In this paper, we focus our attention on cross-modal text-image retrieval, where queries from one modality (e.g., text) can be matched to archive entries from another (e.g., image). Most of the existing cross-modal text-image retrieval systems require a high number of labeled training samples and also do not allow fast and memory-efficient retrieval due to their intrinsic characteristics. These issues limit the applicability of the existing cross-modal retrieval systems for large-scale applications in RS. To address this problem, in this paper we introduce a novel deep unsupervised cross-modal contrastive hashing (DUCH) method for RS text-image retrieval. The proposed DUCH is made up of two main modules: 1) feature extraction module (which extracts deep representations of the text-image modalities); and 2) hashing module (which learns to generate cross-modal binary hash codes from the extracted representations). Within the hashing module, we introduce a novel multi-objective loss function including: i) contrastive objectives that enable similarity preservation in both intra- and inter-modal similarities; ii) an adversarial objective that is enforced across two modalities for cross-modal representation consistency; iii) binarization objectives for generating representative hash codes. Experimental results show that the proposed DUCH outperforms state-of-the-art unsupervised cross-modal hashing methods on two multi-modal (image and text) benchmark archives in RS. Our code is publicly available at https://git.tu-berlin.de/rsim/duch.



### GeoFill: Reference-Based Image Inpainting with Better Geometric Understanding
- **Arxiv ID**: http://arxiv.org/abs/2201.08131v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08131v2)
- **Published**: 2022-01-20 12:17:13+00:00
- **Updated**: 2022-10-09 01:10:02+00:00
- **Authors**: Yunhan Zhao, Connelly Barnes, Yuqian Zhou, Eli Shechtman, Sohrab Amirghodsi, Charless Fowlkes
- **Comment**: Accepted to WACV 2023
- **Journal**: None
- **Summary**: Reference-guided image inpainting restores image pixels by leveraging the content from another single reference image. The primary challenge is how to precisely place the pixels from the reference image into the hole region. Therefore, understanding the 3D geometry that relates pixels between two views is a crucial step towards building a better model. Given the complexity of handling various types of reference images, we focus on the scenario where the images are captured by freely moving the same camera around. Compared to the previous work, we propose a principled approach that does not make heuristic assumptions about the planarity of the scene. We leverage a monocular depth estimate and predict relative pose between cameras, then align the reference image to the target by a differentiable 3D reprojection and a joint optimization of relative pose and depth map scale and offset. Our approach achieves state-of-the-art performance on both RealEstate10K and MannequinChallenge dataset with large baselines, complex geometry and extreme camera motions. We experimentally verify our approach is also better at handling large holes.



### SPAMs: Structured Implicit Parametric Models
- **Arxiv ID**: http://arxiv.org/abs/2201.08141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08141v1)
- **Published**: 2022-01-20 12:33:46+00:00
- **Updated**: 2022-01-20 12:33:46+00:00
- **Authors**: Pablo Palafox, Nikolaos Sarafianos, Tony Tung, Angela Dai
- **Comment**: Project page: https://pablopalafox.github.io/spams/ - Video:
  https://youtu.be/ChdjHNGgrzI
- **Journal**: None
- **Summary**: Parametric 3D models have formed a fundamental role in modeling deformable objects, such as human bodies, faces, and hands; however, the construction of such parametric models requires significant manual intervention and domain expertise. Recently, neural implicit 3D representations have shown great expressibility in capturing 3D shape geometry. We observe that deformable object motion is often semantically structured, and thus propose to learn Structured-implicit PArametric Models (SPAMs) as a deformable object representation that structurally decomposes non-rigid object motion into part-based disentangled representations of shape and pose, with each being represented by deep implicit functions. This enables a structured characterization of object movement, with part decomposition characterizing a lower-dimensional space in which we can establish coarse motion correspondence. In particular, we can leverage the part decompositions at test time to fit to new depth sequences of unobserved shapes, by establishing part correspondences between the input observation and our learned part spaces; this guides a robust joint optimization between the shape and pose of all parts, even under dramatic motion sequences. Experiments demonstrate that our part-aware shape and pose understanding lead to state-of-the-art performance in reconstruction and tracking of depth sequences of complex deforming object motion. We plan to release models to the public at https://pablopalafox.github.io/spams.



### Physically Embodied Deep Image Optimisation
- **Arxiv ID**: http://arxiv.org/abs/2201.08142v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.08142v1)
- **Published**: 2022-01-20 12:34:01+00:00
- **Updated**: 2022-01-20 12:34:01+00:00
- **Authors**: Daniela Mihai, Jonathon Hare
- **Comment**: None
- **Journal**: 5th Workshop on Machine Learning for Creativity and Design of the
  Neural Information Processing Systems (NeurIPS) 2021 Conference
- **Summary**: Physical sketches are created by learning programs to control a drawing robot. A differentiable rasteriser is used to optimise sets of drawing strokes to match an input image, using deep networks to provide an encoding for which we can compute a loss. The optimised drawing primitives can then be translated into G-code commands which command a robot to draw the image using drawing instruments such as pens and pencils on a physical support medium.



### WPPNets and WPPFlows: The Power of Wasserstein Patch Priors for Superresolution
- **Arxiv ID**: http://arxiv.org/abs/2201.08157v3
- **DOI**: 10.1137/22M1496542
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.08157v3)
- **Published**: 2022-01-20 13:04:19+00:00
- **Updated**: 2023-01-05 10:09:10+00:00
- **Authors**: Fabian Altekrüger, Johannes Hertrich
- **Comment**: None
- **Journal**: SIAM Journal on Imaging Sciences, vol. 16(3), pp. 1033-1067, 2023
- **Summary**: Exploiting image patches instead of whole images have proved to be a powerful approach to tackle various problems in image processing. Recently, Wasserstein patch priors (WPP), which are based on the comparison of the patch distributions of the unknown image and a reference image, were successfully used as data-driven regularizers in the variational formulation of superresolution. However, for each input image, this approach requires the solution of a non-convex minimization problem which is computationally costly. In this paper, we propose to learn two kind of neural networks in an unsupervised way based on WPP loss functions. First, we show how convolutional neural networks (CNNs) can be incorporated. Once the network, called WPPNet, is learned, it can be very efficiently applied to any input image. Second, we incorporate conditional normalizing flows to provide a tool for uncertainty quantification. Numerical examples demonstrate the very good performance of WPPNets for superresolution in various image classes even if the forward operator is known only approximately.



### HDhuman: High-quality Human Performance Capture with Sparse Views
- **Arxiv ID**: http://arxiv.org/abs/2201.08158v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08158v2)
- **Published**: 2022-01-20 13:04:59+00:00
- **Updated**: 2022-01-24 12:49:11+00:00
- **Authors**: Tiansong Zhou, Tao Yu, Ruizhi Shao, Kun Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce HDhuman, a method that addresses the challenge of novel view rendering of human performers that wear clothes with complex texture patterns using a sparse set of camera views. Although some recent works have achieved remarkable rendering quality on humans with relatively uniform textures using sparse views, the rendering quality remains limited when dealing with complex texture patterns as they are unable to recover the high-frequency geometry details that observed in the input views. To this end, the proposed HDhuman uses a human reconstruction network with a pixel-aligned spatial transformer and a rendering network that uses geometry-guided pixel-wise feature integration to achieve high-quality human reconstruction and rendering. The designed pixel-aligned spatial transformer calculates the correlations between the input views, producing human reconstruction results with high-frequency details. Based on the surface reconstruction results, the geometry-guided pixel-wise visibility reasoning provides guidance for multi-view feature integration, enabling the rendering network to render high-quality images at 2k resolution on novel views. Unlike previous neural rendering works that always need to train or fine-tune an independent network for a different scene, our method is a general framework that is able to generalize to novel subjects. Experiments show that our approach outperforms all the prior generic or specific methods on both synthetic data and real-world data.



### CP-Net: Contour-Perturbed Reconstruction Network for Self-Supervised Point Cloud Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.08215v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08215v2)
- **Published**: 2022-01-20 15:04:12+00:00
- **Updated**: 2022-03-28 05:48:05+00:00
- **Authors**: Mingye Xu, Yali Wang, Zhipeng Zhou, Hongbin Xu, Yu Qiao
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning has not been fully explored for point cloud analysis. Current frameworks are mainly based on point cloud reconstruction. Given only 3D coordinates, such approaches tend to learn local geometric structures and contours, while failing in understanding high level semantic content. Consequently, they achieve unsatisfactory performance in downstream tasks such as classification, segmentation, etc. To fill this gap, we propose a generic Contour-Perturbed Reconstruction Network (CP-Net), which can effectively guide self-supervised reconstruction to learn semantic content in the point cloud, and thus promote discriminative power of point cloud representation. First, we introduce a concise contour-perturbed augmentation module for point cloud reconstruction. With guidance of geometry disentangling, we divide point cloud into contour and content components. Subsequently, we perturb the contour components and preserve the content components on the point cloud. As a result, self supervisor can effectively focus on semantic content, by reconstructing the original point cloud from such perturbed one. Second, we use this perturbed reconstruction as an assistant branch, to guide the learning of basic reconstruction branch via a distinct dual-branch consistency loss. In this case, our CP-Net not only captures structural contour but also learn semantic content for discriminative downstream tasks. Finally, we perform extensive experiments on a number of point cloud benchmarks. Part segmentation results demonstrate that our CP-Net (81.5% of mIoU) outperforms the previous self-supervised models, and narrows the gap with the fully-supervised methods. For classification, we get a competitive result with the fully-supervised methods on ModelNet40 (92.5% accuracy) and ScanObjectNN (87.9% accuracy). The codes and models will be released afterwards.



### Watermarking Pre-trained Encoders in Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.08217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08217v1)
- **Published**: 2022-01-20 15:14:31+00:00
- **Updated**: 2022-01-20 15:14:31+00:00
- **Authors**: Yutong Wu, Han Qiu, Tianwei Zhang, Jiwei L, Meikang Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastive learning has become a popular technique to pre-train image encoders, which could be used to build various downstream classification models in an efficient way. This process requires a large amount of data and computation resources. Hence, the pre-trained encoders are an important intellectual property that needs to be carefully protected. It is challenging to migrate existing watermarking techniques from the classification tasks to the contrastive learning scenario, as the owner of the encoder lacks the knowledge of the downstream tasks which will be developed from the encoder in the future. We propose the \textit{first} watermarking methodology for the pre-trained encoders. We introduce a task-agnostic loss function to effectively embed into the encoder a backdoor as the watermark. This backdoor can still exist in any downstream models transferred from the encoder. Extensive evaluations over different contrastive learning algorithms, datasets, and downstream tasks indicate our watermarks exhibit high effectiveness and robustness against different adversarial operations.



### End-to-end Generative Pretraining for Multimodal Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2201.08264v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2201.08264v2)
- **Published**: 2022-01-20 16:16:21+00:00
- **Updated**: 2022-05-10 09:36:22+00:00
- **Authors**: Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, Cordelia Schmid
- **Comment**: None
- **Journal**: Proceedings of Conference on Computer Vision and Pattern
  Recognition (CVPR) 2022
- **Summary**: Recent video and language pretraining frameworks lack the ability to generate sentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new pretraining framework for learning from unlabelled videos which can be effectively used for generative tasks such as multimodal video captioning. Unlike recent video-language pretraining frameworks, our framework trains both a multimodal video encoder and a sentence decoder jointly. To overcome the lack of captions in unlabelled videos, we leverage the future utterance as an additional text source and propose a bidirectional generation objective -- we generate future utterances given the present mulitmodal context, and also the present utterance given future observations. With this objective, we train an encoder-decoder model end-to-end to generate a caption from raw pixels and transcribed speech directly. Our model achieves state-of-the-art performance for multimodal video captioning on four standard benchmarks, as well as for other video understanding tasks such as VideoQA, video retrieval and action classification.



### A Real-Time Rendering Method for Light Field Display
- **Arxiv ID**: http://arxiv.org/abs/2201.08266v4
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.08266v4)
- **Published**: 2022-01-20 16:16:35+00:00
- **Updated**: 2022-04-27 07:06:50+00:00
- **Authors**: Quanzhen Wan
- **Comment**: We are reminded by our supervisors and peers that we have not taken
  many potential influential factors into consideration, which might lead to a
  rather different outcome. If the whole idea will be certified correctly in
  the future, we will resubmit our updated version at that time
- **Journal**: None
- **Summary**: A real-time elemental image array (EIA) generation method which does not sacrifice accuracy nor rely on high-performance hardware is developed, through raytracing and pre-stored voxel-pixel lookup table (LUT). Benefiting from both offline and online working flow, experiments will verified the effectiveness.



### Modeling and hexahedral meshing of cerebral arterial networks from centerlines
- **Arxiv ID**: http://arxiv.org/abs/2201.08279v2
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.08279v2)
- **Published**: 2022-01-20 16:30:17+00:00
- **Updated**: 2023-06-13 07:50:50+00:00
- **Authors**: Méghane Decroocq, Carole Frindel, Pierre Rougé, Makoto Ohta, Guillaume Lavoué
- **Comment**: None
- **Journal**: None
- **Summary**: Computational fluid dynamics (CFD) simulation provides valuable information on blood flow from the vascular geometry. However, it requires extracting precise models of arteries from low-resolution medical images, which remains challenging. Centerline-based representation is widely used to model large vascular networks with small vessels, as it encodes both the geometric and topological information and facilitates manual editing. In this work, we propose an automatic method to generate a structured hexahedral mesh suitable for CFD directly from centerlines. We addressed both the modeling and meshing tasks. We proposed a vessel model based on penalized splines to overcome the limitations inherent to the centerline representation, such as noise and sparsity. The bifurcations are reconstructed using a parametric model based on the anatomy that we extended to planar n-furcations. Finally, we developed a method to produce a volume mesh with structured, hexahedral, and flow-oriented cells from the proposed vascular network model. The proposed method offers better robustness to the common defects of centerlines and increases the mesh quality compared to state-of-the-art methods. As it relies on centerlines alone, it can be applied to edit the vascular model effortlessly to study the impact of vascular geometry and topology on hemodynamics. We demonstrate the efficiency of our method by entirely meshing a dataset of 60 cerebral vascular networks. 92% of the vessels and 83% of the bifurcations were meshed without defects needing manual intervention, despite the challenging aspect of the input data. The source code is released publicly.



### DIVA-DAF: A Deep Learning Framework for Historical Document Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2201.08295v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08295v2)
- **Published**: 2022-01-20 17:02:46+00:00
- **Updated**: 2022-01-21 14:21:52+00:00
- **Authors**: Lars Vögtlin, Paul Maergner, Rolf Ingold
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a new deep learning framework called DIVA-DAF. We have developed this framework to support our research on historical document image analysis tasks and to develop techniques to reduce the need for manually-labeled ground truth. We want to apply self-supervised learning techniques and use different kinds of training data. Our new framework aids us in performing rapid prototyping and reproducible experiments. We present a first semantic segmentation experiment on DIVA-HisDB using our framework, achieving state-of-the-art results. The DIVA-DAF framework is open-source, and we encourage other research groups to use it for their experiments.



### Accelerating Laue Depth Reconstruction Algorithm with CUDA
- **Arxiv ID**: http://arxiv.org/abs/2201.13309v1
- **DOI**: None
- **Categories**: **physics.data-an**, cs.CV, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/2201.13309v1)
- **Published**: 2022-01-20 18:35:51+00:00
- **Updated**: 2022-01-20 18:35:51+00:00
- **Authors**: Ke Yue, Schwarz Nicholas, Tischler Jonathan Z
- **Comment**: 2015 IEEE International Conference on Cluster Computing
- **Journal**: None
- **Summary**: The Laue diffraction microscopy experiment uses the polychromatic Laue micro-diffraction technique to examine the structure of materials with sub-micron spatial resolution in all three dimensions. During this experiment, local crystallographic orientations, orientation gradients and strains are measured as properties which will be recorded in HDF5 image format. The recorded images will be processed with a depth reconstruction algorithm for future data analysis. But the current depth reconstruction algorithm consumes considerable processing time and might take up to 2 weeks for reconstructing data collected from one single experiment. To improve the depth reconstruction computation speed, we propose a scalable GPU program solution on the depth reconstruction problem in this paper. The test result shows that the running time would be 10 to 20 times faster than the prior CPU design for various size of input data.



### Stitch it in Time: GAN-Based Facial Editing of Real Videos
- **Arxiv ID**: http://arxiv.org/abs/2201.08361v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.08361v2)
- **Published**: 2022-01-20 18:48:20+00:00
- **Updated**: 2022-01-21 17:28:57+00:00
- **Authors**: Rotem Tzaban, Ron Mokady, Rinon Gal, Amit H. Bermano, Daniel Cohen-Or
- **Comment**: Project website: https://stitch-time.github.io/
- **Journal**: None
- **Summary**: The ability of Generative Adversarial Networks to encode rich semantics within their latent space has been widely adopted for facial image editing. However, replicating their success with videos has proven challenging. Sets of high-quality facial videos are lacking, and working with videos introduces a fundamental barrier to overcome - temporal coherency. We propose that this barrier is largely artificial. The source video is already temporally coherent, and deviations from this state arise in part due to careless treatment of individual components in the editing pipeline. We leverage the natural alignment of StyleGAN and the tendency of neural networks to learn low frequency functions, and demonstrate that they provide a strongly consistent prior. We draw on these insights and propose a framework for semantic editing of faces in videos, demonstrating significant improvements over the current state-of-the-art. Our method produces meaningful face manipulations, maintains a higher degree of temporal consistency, and can be applied to challenging, high quality, talking head videos which current methods struggle with.



### Revisiting Weakly Supervised Pre-Training of Visual Perception Models
- **Arxiv ID**: http://arxiv.org/abs/2201.08371v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08371v2)
- **Published**: 2022-01-20 18:55:06+00:00
- **Updated**: 2022-04-02 06:34:29+00:00
- **Authors**: Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollár, Laurens van der Maaten
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Model pre-training is a cornerstone of modern visual recognition systems. Although fully supervised pre-training on datasets like ImageNet is still the de-facto standard, recent studies suggest that large-scale weakly supervised pre-training can outperform fully supervised approaches. This paper revisits weakly-supervised pre-training of models using hashtag supervision with modern versions of residual networks and the largest-ever dataset of images and corresponding hashtags. We study the performance of the resulting models in various transfer-learning settings including zero-shot transfer. We also compare our models with those obtained via large-scale self-supervised learning. We find our weakly-supervised models to be very competitive across all settings, and find they substantially outperform their self-supervised counterparts. We also include an investigation into whether our models learned potentially troubling associations or stereotypes. Overall, our results provide a compelling argument for the use of weakly supervised learning in the development of visual recognition systems. Our models, Supervised Weakly through hashtAGs (SWAG), are available publicly.



### Omnivore: A Single Model for Many Visual Modalities
- **Arxiv ID**: http://arxiv.org/abs/2201.08377v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.08377v2)
- **Published**: 2022-01-20 18:58:03+00:00
- **Updated**: 2022-03-30 20:07:46+00:00
- **Authors**: Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, Ishan Misra
- **Comment**: Accepted at CVPR 2022 (Oral Presentation)
- **Journal**: None
- **Summary**: Prior work has studied different visual modalities in isolation and developed separate architectures for recognition of images, videos, and 3D data. Instead, in this paper, we propose a single model which excels at classifying images, videos, and single-view 3D data using exactly the same model parameters. Our 'Omnivore' model leverages the flexibility of transformer-based architectures and is trained jointly on classification tasks from different modalities. Omnivore is simple to train, uses off-the-shelf standard datasets, and performs at-par or better than modality-specific models of the same size. A single Omnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision tasks and generalize across modalities. Omnivore's shared visual representation naturally enables cross-modal recognition without access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together.



### Learning Pixel Trajectories with Multiscale Contrastive Random Walks
- **Arxiv ID**: http://arxiv.org/abs/2201.08379v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08379v2)
- **Published**: 2022-01-20 18:58:52+00:00
- **Updated**: 2022-04-04 06:05:42+00:00
- **Authors**: Zhangxing Bian, Allan Jabri, Alexei A. Efros, Andrew Owens
- **Comment**: None
- **Journal**: None
- **Summary**: A range of video modeling tasks, from optical flow to multiple object tracking, share the same fundamental challenge: establishing space-time correspondence. Yet, approaches that dominate each space differ. We take a step towards bridging this gap by extending the recent contrastive random walk formulation to much denser, pixel-level space-time graphs. The main contribution is introducing hierarchy into the search problem by computing the transition matrix between two frames in a coarse-to-fine manner, forming a multiscale contrastive random walk when extended in time. This establishes a unified technique for self-supervised learning of optical flow, keypoint tracking, and video object segmentation. Experiments demonstrate that, for each of these tasks, the unified model achieves performance competitive with strong self-supervised approaches specific to that task. Project webpage: https://jasonbian97.github.io/flowwalk



### MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.08383v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08383v2)
- **Published**: 2022-01-20 18:59:54+00:00
- **Updated**: 2022-11-30 19:40:55+00:00
- **Authors**: Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, Christoph Feichtenhofer
- **Comment**: Technical report. arXiv v2: add link to code
- **Journal**: None
- **Summary**: While today's video recognition systems parse snapshots or short clips accurately, they cannot connect the dots and reason across a longer range of time yet. Most existing video architectures can only process <5 seconds of a video without hitting the computation or memory bottlenecks.   In this paper, we propose a new strategy to overcome this challenge. Instead of trying to process more frames at once like most existing methods, we propose to process videos in an online fashion and cache "memory" at each iteration. Through the memory, the model can reference prior context for long-term modeling, with only a marginal cost. Based on this idea, we build MeMViT, a Memory-augmented Multiscale Vision Transformer, that has a temporal support 30x longer than existing models with only 4.5% more compute; traditional methods need >3,000% more compute to do the same. On a wide range of settings, the increased temporal support enabled by MeMViT brings large gains in recognition accuracy consistently. MeMViT obtains state-of-the-art results on the AVA, EPIC-Kitchens-100 action classification, and action anticipation datasets. Code and models are available at https://github.com/facebookresearch/memvit.



### SoftDropConnect (SDC) -- Effective and Efficient Quantification of the Network Uncertainty in Deep MR Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2201.08418v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2201.08418v2)
- **Published**: 2022-01-20 19:22:26+00:00
- **Updated**: 2022-06-01 20:10:10+00:00
- **Authors**: Qing Lyu, Christopher T. Whitlow, Ge Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep learning has achieved remarkable successes in medical image analysis. Although deep neural networks generate clinically important predictions, they have inherent uncertainty. Such uncertainty is a major barrier to report these predictions with confidence. In this paper, we propose a novel yet simple Bayesian inference approach called SoftDropConnect (SDC) to quantify the network uncertainty in medical imaging tasks with gliomas segmentation and metastases classification as initial examples. Our key idea is that during training and testing SDC modulates network parameters continuously so as to allow affected information processing channels still in operation, instead of disabling them as Dropout or DropConnet does. When compared with three popular Bayesian inference methods including Bayes By Backprop, Dropout, and DropConnect, our SDC method (SDC-W after optimization) outperforms the three competing methods with a substantial margin. Quantitatively, our proposed method generates substantial improvements in prediction accuracy (by 3.4%, 2.5%, and 6.7% respectively for whole tumor segmentation in terms of dice score; and by 11.7%, 3.9%, and 8.7% respectively for brain metastases classification) and greatly reduced epistemic and aleatoric uncertainties. Our approach promises to deliver better diagnostic performance and make medical AI imaging more explainable and trustworthy.



### FaceOcc: A Diverse, High-quality Face Occlusion Dataset for Human Face Extraction
- **Arxiv ID**: http://arxiv.org/abs/2201.08425v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.08425v1)
- **Published**: 2022-01-20 19:44:18+00:00
- **Updated**: 2022-01-20 19:44:18+00:00
- **Authors**: Xiangnan Yin, Liming Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Occlusions often occur in face images in the wild, troubling face-related tasks such as landmark detection, 3D reconstruction, and face recognition. It is beneficial to extract face regions from unconstrained face images accurately. However, current face segmentation datasets suffer from small data volumes, few occlusion types, low resolution, and imprecise annotation, limiting the performance of data-driven-based algorithms. This paper proposes a novel face occlusion dataset with manually labeled face occlusions from the CelebA-HQ and the internet. The occlusion types cover sunglasses, spectacles, hands, masks, scarfs, microphones, etc. To the best of our knowledge, it is by far the largest and most comprehensive face occlusion dataset. Combining it with the attribute mask in CelebAMask-HQ, we trained a straightforward face segmentation model but obtained SOTA performance, convincingly demonstrating the effectiveness of the proposed dataset.



### A Visual Analytics Approach to Building Logistic Regression Models and its Application to Health Records
- **Arxiv ID**: http://arxiv.org/abs/2201.08429v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2201.08429v1)
- **Published**: 2022-01-20 19:53:41+00:00
- **Updated**: 2022-01-20 19:53:41+00:00
- **Authors**: Erasmo Artur, Rosane Minghim
- **Comment**: 16 pages and 13 figures
- **Journal**: None
- **Summary**: Multidimensional data analysis has become increasingly important in many fields, mainly due to current vast data availability and the increasing demand to extract knowledge from it. In most applications, the role of the final user is crucial to build proper machine learning models and to explain the patterns found in data. In this paper, we present an open unified approach for generating, evaluating, and applying regression models in high-dimensional data sets within a user-guided process. The approach is based on exposing a broad correlation panorama for attributes, by which the user can select relevant attributes to build and evaluate prediction models for one or more contexts. We name the approach UCReg (User-Centered Regression). We demonstrate effectiveness and efficiency of UCReg through the application of our framework to the analysis of Covid-19 and other synthetic and real health records data.



### An Empirical Investigation of Model-to-Model Distribution Shifts in Trained Convolutional Filters
- **Arxiv ID**: http://arxiv.org/abs/2201.08465v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.08465v1)
- **Published**: 2022-01-20 21:48:12+00:00
- **Updated**: 2022-01-20 21:48:12+00:00
- **Authors**: Paul Gavrikov, Janis Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: We present first empirical results from our ongoing investigation of distribution shifts in image data used for various computer vision tasks. Instead of analyzing the original training and test data, we propose to study shifts in the learned weights of trained models. In this work, we focus on the properties of the distributions of dominantly used 3x3 convolution filter kernels. We collected and publicly provide a data set with over half a billion filters from hundreds of trained CNNs, using a wide range of data sets, architectures, and vision tasks. Our analysis shows interesting distribution shifts (or the lack thereof) between trained filters along different axes of meta-parameters, like data type, task, architecture, or layer depth. We argue, that the observed properties are a valuable source for further investigation into a better understanding of the impact of shifts in the input data to the generalization abilities of CNN models and novel methods for more robust transfer-learning in this domain. Data available at: https://github.com/paulgavrikov/CNN-Filter-DB/.



