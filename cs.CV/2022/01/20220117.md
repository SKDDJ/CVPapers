# Arxiv Papers in cs.CV on 2022-01-17
### Synthesis and Reconstruction of Fingerprints using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.06164v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06164v2)
- **Published**: 2022-01-17 00:18:00+00:00
- **Updated**: 2023-03-12 17:46:08+00:00
- **Authors**: Rafael Bouzaglo, Yosi Keller
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based models have been shown to improve the accuracy of fingerprint recognition. While these algorithms show exceptional performance, they require large-scale fingerprint datasets for training and evaluation. In this work, we propose a novel fingerprint synthesis and reconstruction framework based on the StyleGan2 architecture, to address the privacy issues related to the acquisition of such large-scale datasets. We also derive a computational approach to modify the attributes of the generated fingerprint while preserving their identity. This allows synthesizing multiple different fingerprint images per finger. In particular, we introduce the SynFing synthetic fingerprints dataset consisting of 100K image pairs, each pair corresponding to the same identity. The proposed framework was experimentally shown to outperform contemporary state-of-the-art approaches for both fingerprint synthesis and reconstruction. It significantly improved the realism of the generated fingerprints, both visually and in terms of their ability to spoof fingerprint-based verification systems. The code and fingerprints dataset are publicly available: https://github.com/rafaelbou/fingerprint_generator.



### SunCast: Solar Irradiance Nowcasting from Geosynchronous Satellite Data
- **Arxiv ID**: http://arxiv.org/abs/2201.06173v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06173v1)
- **Published**: 2022-01-17 01:55:26+00:00
- **Updated**: 2022-01-17 01:55:26+00:00
- **Authors**: Dhileeban Kumaresan, Richard Wang, Ernesto Martinez, Richard Cziva, Alberto Todeschini, Colorado J Reed, Hossein Vahabi
- **Comment**: None
- **Journal**: None
- **Summary**: When cloud layers cover photovoltaic (PV) panels, the amount of power the panels produce fluctuates rapidly. Therefore, to maintain enough energy on a power grid to match demand, utilities companies rely on reserve power sources that typically come from fossil fuels and therefore pollute the environment. Accurate short-term PV power prediction enables operators to maximize the amount of power obtained from PV panels and safely reduce the reserve energy needed from fossil fuel sources. While several studies have developed machine learning models to predict solar irradiance at specific PV generation facilities, little work has been done to model short-term solar irradiance on a global scale. Furthermore, models that have been developed are proprietary and have architectures that are not publicly available or rely on computationally demanding Numerical Weather Prediction (NWP) models. Here, we propose a Convolutional Long Short-Term Memory Network model that treats solar nowcasting as a next frame prediction problem, is more efficient than NWP models and has a straightforward, reproducible architecture. Our models can predict solar irradiance for entire North America for up to 3 hours in under 60 seconds on a single machine without a GPU and has a RMSE of 120 W/m2 when evaluated on 2 months of data.



### A novel attention model for salient structure detection in seismic volumes
- **Arxiv ID**: http://arxiv.org/abs/2201.06174v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06174v1)
- **Published**: 2022-01-17 01:56:11+00:00
- **Updated**: 2022-01-17 01:56:11+00:00
- **Authors**: Muhammad Amir Shafiq, Zhiling Long, Haibin Di, Ghassan AlRegib
- **Comment**: Published in Applied Computing and Intelligence, Nov. 2021
- **Journal**: Applied Computing and Intelligence, vol. 1, no. 1, pp. 31-45, Nov.
  2021
- **Summary**: A new approach to seismic interpretation is proposed to leverage visual perception and human visual system modeling. Specifically, a saliency detection algorithm based on a novel attention model is proposed for identifying subsurface structures within seismic data volumes. The algorithm employs 3D-FFT and a multi-dimensional spectral projection, which decomposes local spectra into three distinct components, each depicting variations along different dimensions of the data. Subsequently, a novel directional center-surround attention model is proposed to incorporate directional comparisons around each voxel for saliency detection within each projected dimension. Next, the resulting saliency maps along each dimension are combined adaptively to yield a consolidated saliency map, which highlights various structures characterized by subtle variations and relative motion with respect to their neighboring sections. A priori information about the seismic data can be either embedded into the proposed attention model in the directional comparisons, or incorporated into the algorithm by specifying a template when combining saliency maps adaptively. Experimental results on two real seismic datasets from the North Sea, Netherlands and Great South Basin, New Zealand demonstrate the effectiveness of the proposed algorithm for detecting salient seismic structures of different natures and appearances in one shot, which differs significantly from traditional seismic interpretation algorithms. The results further demonstrate that the proposed method outperforms comparable state-of-the-art saliency detection algorithms for natural images and videos, which are inadequate for seismic imaging data.



### A fast and accurate iris segmentation method using an LoG filter and its zero-crossings
- **Arxiv ID**: http://arxiv.org/abs/2201.06176v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06176v1)
- **Published**: 2022-01-17 02:10:36+00:00
- **Updated**: 2022-01-17 02:10:36+00:00
- **Authors**: Tariq M. Khan, Donald G. bailey, Yinan Kong
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a hybrid approach to achieve iris localization based on a Laplacian of Gaussian (LoG) filter, region growing, and zero-crossings of the LoG filter. In the proposed method, an LoG filter with region growing is used to detect the pupil region. Subsequently, zero-crossings of the LoG filter are used to accurately mark the inner and outer circular boundaries. The use of LoG based blob detection along with zero-crossings makes the inner and outer circle detection fast and robust. The proposed method has been tested on three public databases: MMU version 1.0, CASIA-IrisV1 and CASIA-IrisV3- Lamp. The experimental results demonstrate the segmentation accuracy of the proposed method. The robustness of the proposed method is also validated in the presence of noise, such as eyelashes, a reflection of the pupil, Poisson, Gaussian, speckle and salt-and-pepper noise. The comparison with well-known methods demonstrates the superior performance of the proposed method's accuracy and speed.



### Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/2201.06192v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.06192v1)
- **Published**: 2022-01-17 03:24:31+00:00
- **Updated**: 2022-01-17 03:24:31+00:00
- **Authors**: Wei Jia, Zhaojun Lu, Haichun Zhang, Zhenglin Liu, Jie Wang, Gang Qu
- **Comment**: 17 pages, 15 figures
- **Journal**: None
- **Summary**: Adversarial Examples (AEs) can deceive Deep Neural Networks (DNNs) and have received a lot of attention recently. However, majority of the research on AEs is in the digital domain and the adversarial patches are static, which is very different from many real-world DNN applications such as Traffic Sign Recognition (TSR) systems in autonomous vehicles. In TSR systems, object detectors use DNNs to process streaming video in real time. From the view of object detectors, the traffic sign`s position and quality of the video are continuously changing, rendering the digital AEs ineffective in the physical world.   In this paper, we propose a systematic pipeline to generate robust physical AEs against real-world object detectors. Robustness is achieved in three ways. First, we simulate the in-vehicle cameras by extending the distribution of image transformations with the blur transformation and the resolution transformation. Second, we design the single and multiple bounding boxes filters to improve the efficiency of the perturbation training. Third, we consider four representative attack vectors, namely Hiding Attack, Appearance Attack, Non-Target Attack and Target Attack.   We perform a comprehensive set of experiments under a variety of environmental conditions, and considering illuminations in sunny and cloudy weather as well as at night. The experimental results show that the physical AEs generated from our pipeline are effective and robust when attacking the YOLO v5 based TSR system. The attacks have good transferability and can deceive other state-of-the-art object detectors. We launched HA and NTA on a brand-new 2021 model vehicle. Both attacks are successful in fooling the TSR system, which could be a life-threatening case for autonomous vehicles. Finally, we discuss three defense mechanisms based on image preprocessing, AEs detection, and model enhancing.



### Discourse Analysis for Evaluating Coherence in Video Paragraph Captions
- **Arxiv ID**: http://arxiv.org/abs/2201.06207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06207v1)
- **Published**: 2022-01-17 04:23:08+00:00
- **Updated**: 2022-01-17 04:23:08+00:00
- **Authors**: Arjun R Akula, Song-Chun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Video paragraph captioning is the task of automatically generating a coherent paragraph description of the actions in a video. Previous linguistic studies have demonstrated that coherence of a natural language text is reflected by its discourse structure and relations. However, existing video captioning methods evaluate the coherence of generated paragraphs by comparing them merely against human paragraph annotations and fail to reason about the underlying discourse structure. At UCLA, we are currently exploring a novel discourse based framework to evaluate the coherence of video paragraphs. Central to our approach is the discourse representation of videos, which helps in modeling coherence of paragraphs conditioned on coherence of videos. We also introduce DisNet, a novel dataset containing the proposed visual discourse annotations of 3000 videos and their paragraphs. Our experiment results have shown that the proposed framework evaluates coherence of video paragraphs significantly better than all the baseline methods. We believe that many other multi-discipline Artificial Intelligence problems such as Visual Dialog and Visual Storytelling would also greatly benefit from the proposed visual discourse framework and the DisNet dataset.



### Face Detection in Extreme Conditions: A Machine-learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2201.06220v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.06220v2)
- **Published**: 2022-01-17 05:23:22+00:00
- **Updated**: 2022-02-13 15:49:33+00:00
- **Authors**: Sameer Aqib Hashmi
- **Comment**: 6 pages, 9 figures
- **Journal**: None
- **Summary**: Face detection in unrestricted conditions has been a trouble for years due to various expressions, brightness, and coloration fringing. Recent studies show that deep learning knowledge of strategies can acquire spectacular performance inside the identification of different gadgets and patterns. This face detection in unconstrained surroundings is difficult due to various poses, illuminations, and occlusions. Figuring out someone with a picture has been popularized through the mass media. However, it's miles less sturdy to fingerprint or retina scanning. The latest research shows that deep mastering techniques can gain mind-blowing performance on those two responsibilities. In this paper, I recommend a deep cascaded multi-venture framework that exploits the inherent correlation among them to boost up their performance. In particular, my framework adopts a cascaded shape with 3 layers of cautiously designed deep convolutional networks that expect face and landmark region in a coarse-to-fine way. Besides, within the gaining knowledge of the procedure, I propose a new online tough sample mining method that can enhance the performance robotically without manual pattern choice.



### Improving Clinical Diagnosis Performance with Automated X-ray Scan Quality Enhancement Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2201.06250v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.06250v1)
- **Published**: 2022-01-17 07:27:03+00:00
- **Updated**: 2022-01-17 07:27:03+00:00
- **Authors**: Karthik K, Sowmya Kamath S
- **Comment**: Presented and Accepted in International Conference on Advances in
  Systems, Control and Computing (AISCC-2020) at Malaviya National Institute of
  Technology, Jaipur, India, February 27-28, 2020
- **Journal**: International Conference on Advances in Systems, Control and
  Computing (AISCC-2020) at Malaviya National Institute of Technology, Jaipur,
  India, February 27-28, 2020
- **Summary**: In clinical diagnosis, diagnostic images that are obtained from the scanning devices serve as preliminary evidence for further investigation in the process of delivering quality healthcare. However, often the medical image may contain fault artifacts, introduced due to noise, blur and faulty equipment. The reason for this may be the low-quality or older scanning devices, the test environment or technicians lack of training etc; however, the net result is that the process of fast and reliable diagnosis is hampered. Resolving these issues automatically can have a significant positive impact in a hospital clinical workflow, where often, there is no other way but to work with faulty/older equipment or inadequately qualified radiology technicians. In this paper, automated image quality improvement approaches for adapted and benchmarked for the task of medical image super-resolution. During experimental evaluation on standard open datasets, the observations showed that certain algorithms perform better and show significant improvement in the diagnostic quality of medical scans, thereby enabling better visualization for human diagnostic purposes.



### Automatic Segmentation of Head and Neck Tumor: How Powerful Transformers Are?
- **Arxiv ID**: http://arxiv.org/abs/2201.06251v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.06251v2)
- **Published**: 2022-01-17 07:31:52+00:00
- **Updated**: 2022-05-12 11:36:39+00:00
- **Authors**: Ikboljon Sobirov, Otabek Nazarov, Hussain Alasmawi, Mohammad Yaqub
- **Comment**: 8 pages, 2 figures (3 more figures in Appendix), 2 tables; accepted
  to MIDL conference
- **Journal**: None
- **Summary**: Cancer is one of the leading causes of death worldwide, and head and neck (H&N) cancer is amongst the most prevalent types. Positron emission tomography and computed tomography are used to detect, segment and quantify the tumor region. Clinically, tumor segmentation is extensively time-consuming and prone to error. Machine learning, and deep learning in particular, can assist to automate this process, yielding results as accurate as the results of a clinician. In this paper, we investigate a vision transformer-based method to automatically delineate H&N tumor, and compare its results to leading convolutional neural network (CNN)-based models. We use multi-modal data from CT and PET scans to perform the segmentation task. We show that a solution with a transformer-based model has the potential to achieve comparable results to CNN-based ones. With cross validation, the model achieves a mean dice similarity coefficient (DSC) of 0.736, mean precision of 0.766 and mean recall of 0.766. This is only 0.021 less than the 2020 competition winning model (cross validated in-house) in terms of the DSC score. On the testing set, the model performs similarly, with DSC of 0.736, precision of 0.773, and recall of 0.760, which is only 0.023 lower in DSC than the 2020 competition winning model. This work shows that cancer segmentation via transformer-based models is a promising research area to further explore.



### Segmentation of the Carotid Lumen and Vessel Wall using Deep Learning and Location Priors
- **Arxiv ID**: http://arxiv.org/abs/2201.06259v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.06259v2)
- **Published**: 2022-01-17 07:56:58+00:00
- **Updated**: 2022-05-14 18:31:23+00:00
- **Authors**: Florian Thamm, Felix Denzinger, Leonhard Rist, Celia Martin Vicario, Florian Kordon, Andreas Maier
- **Comment**: Challenge Report - Preprint
- **Journal**: None
- **Summary**: In this report we want to present our method and results for the Carotid Artery Vessel Wall Segmentation Challenge. We propose an image-based pipeline utilizing the U-Net architecture and location priors to solve the segmentation problem at hand.



### Towards Realistic Visual Dubbing with Heterogeneous Sources
- **Arxiv ID**: http://arxiv.org/abs/2201.06260v1
- **DOI**: 10.1145/3474085.3475318
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06260v1)
- **Published**: 2022-01-17 07:57:24+00:00
- **Updated**: 2022-01-17 07:57:24+00:00
- **Authors**: Tianyi Xie, Liucheng Liao, Cheng Bi, Benlai Tang, Xiang Yin, Jianfei Yang, Mingjie Wang, Jiali Yao, Yang Zhang, Zejun Ma
- **Comment**: 9 pages (including references), 7 figures, Accepted in ACM
  Multimedia, 2021
- **Journal**: None
- **Summary**: The task of few-shot visual dubbing focuses on synchronizing the lip movements with arbitrary speech input for any talking head video. Albeit moderate improvements in current approaches, they commonly require high-quality homologous data sources of videos and audios, thus causing the failure to leverage heterogeneous data sufficiently. In practice, it may be intractable to collect the perfect homologous data in some cases, for example, audio-corrupted or picture-blurry videos. To explore this kind of data and support high-fidelity few-shot visual dubbing, in this paper, we novelly propose a simple yet efficient two-stage framework with a higher flexibility of mining heterogeneous data. Specifically, our two-stage paradigm employs facial landmarks as intermediate prior of latent representations and disentangles the lip movements prediction from the core task of realistic talking head generation. By this means, our method makes it possible to independently utilize the training corpus for two-stage sub-networks using more available heterogeneous data easily acquired. Besides, thanks to the disentanglement, our framework allows a further fine-tuning for a given talking head, thereby leading to better speaker-identity preserving in the final synthesized results. Moreover, the proposed method can also transfer appearance features from others to the target speaker. Extensive experimental results demonstrate the superiority of our proposed method in generating highly realistic videos synchronized with the speech over the state-of-the-art.



### Continual Transformers: Redundancy-Free Attention for Online Inference
- **Arxiv ID**: http://arxiv.org/abs/2201.06268v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06268v3)
- **Published**: 2022-01-17 08:20:09+00:00
- **Updated**: 2023-01-24 07:42:08+00:00
- **Authors**: Lukas Hedegaard, Arian Bakhtiarnia, Alexandros Iosifidis
- **Comment**: 16 pages, 6 figures, 7 tables
- **Journal**: International Conference on Learning Representations, 2023
- **Summary**: Transformers in their common form are inherently limited to operate on whole token sequences rather than on one token at a time. Consequently, their use during online inference on time-series data entails considerable redundancy due to the overlap in successive token sequences. In this work, we propose novel formulations of the Scaled Dot-Product Attention, which enable Transformers to perform efficient online token-by-token inference on a continual input stream. Importantly, our modifications are purely to the order of computations, while the outputs and learned weights are identical to those of the original Transformer Encoder. We validate our Continual Transformer Encoder with experiments on the THUMOS14, TVSeries and GTZAN datasets with remarkable results: Our Continual one- and two-block architectures reduce the floating point operations per prediction by up to 63x and 2.6x, respectively, while retaining predictive performance.



### The CLEAR Benchmark: Continual LEArning on Real-World Imagery
- **Arxiv ID**: http://arxiv.org/abs/2201.06289v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.06289v3)
- **Published**: 2022-01-17 09:09:09+00:00
- **Updated**: 2022-06-09 04:41:54+00:00
- **Authors**: Zhiqiu Lin, Jia Shi, Deepak Pathak, Deva Ramanan
- **Comment**: Project site: https://clear-benchmark.github.io
- **Journal**: None
- **Summary**: Continual learning (CL) is widely regarded as crucial challenge for lifelong AI. However, existing CL benchmarks, e.g. Permuted-MNIST and Split-CIFAR, make use of artificial temporal variation and do not align with or generalize to the real-world. In this paper, we introduce CLEAR, the first continual image classification benchmark dataset with a natural temporal evolution of visual concepts in the real world that spans a decade (2004-2014). We build CLEAR from existing large-scale image collections (YFCC100M) through a novel and scalable low-cost approach to visio-linguistic dataset curation. Our pipeline makes use of pretrained vision-language models (e.g. CLIP) to interactively build labeled datasets, which are further validated with crowd-sourcing to remove errors and even inappropriate images (hidden in original YFCC100M). The major strength of CLEAR over prior CL benchmarks is the smooth temporal evolution of visual concepts with real-world imagery, including both high-quality labeled data along with abundant unlabeled samples per time period for continual semi-supervised learning. We find that a simple unsupervised pre-training step can already boost state-of-the-art CL algorithms that only utilize fully-supervised data. Our analysis also reveals that mainstream CL evaluation protocols that train and test on iid data artificially inflate performance of CL system. To address this, we propose novel "streaming" protocols for CL that always test on the (near) future. Interestingly, streaming protocols (a) can simplify dataset curation since today's testset can be repurposed for tomorrow's trainset and (b) can produce more generalizable models with more accurate estimates of performance since all labeled data from each time-period is used for both training and testing (unlike classic iid train-test splits).



### Action Keypoint Network for Efficient Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.06304v1
- **DOI**: 10.1109/TIP.2022.3191461
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06304v1)
- **Published**: 2022-01-17 09:35:34+00:00
- **Updated**: 2022-01-17 09:35:34+00:00
- **Authors**: Xu Chen, Yahong Han, Xiaohan Wang, Yifan Sun, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Reducing redundancy is crucial for improving the efficiency of video recognition models. An effective approach is to select informative content from the holistic video, yielding a popular family of dynamic video recognition methods. However, existing dynamic methods focus on either temporal or spatial selection independently while neglecting a reality that the redundancies are usually spatial and temporal, simultaneously. Moreover, their selected content is usually cropped with fixed shapes, while the realistic distribution of informative content can be much more diverse. With these two insights, this paper proposes to integrate temporal and spatial selection into an Action Keypoint Network (AK-Net). From different frames and positions, AK-Net selects some informative points scattered in arbitrary-shaped regions as a set of action keypoints and then transforms the video recognition into point cloud classification. AK-Net has two steps, i.e., the keypoint selection and the point cloud classification. First, it inputs the video into a baseline network and outputs a feature map from an intermediate layer. We view each pixel on this feature map as a spatial-temporal point and select some informative keypoints using self-attention. Second, AK-Net devises a ranking criterion to arrange the keypoints into an ordered 1D sequence. Consequentially, AK-Net brings two-fold benefits for efficiency: The keypoint selection step collects informative content within arbitrary shapes and increases the efficiency for modeling spatial-temporal dependencies, while the point cloud classification step further reduces the computational cost by compacting the convolutional kernels. Experimental results show that AK-Net can consistently improve the efficiency and performance of baseline methods on several video recognition benchmarks.



### Graph Neural Networks for Cross-Camera Data Association
- **Arxiv ID**: http://arxiv.org/abs/2201.06311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06311v1)
- **Published**: 2022-01-17 09:52:39+00:00
- **Updated**: 2022-01-17 09:52:39+00:00
- **Authors**: Elena Luna, Juan C. SanMiguel, José M. Martínez, Pablo Carballeira
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-camera image data association is essential for many multi-camera computer vision tasks, such as multi-camera pedestrian detection, multi-camera multi-target tracking, 3D pose estimation, etc. This association task is typically stated as a bipartite graph matching problem and often solved by applying minimum-cost flow techniques, which may be computationally inefficient with large data. Furthermore, cameras are usually treated by pairs, obtaining local solutions, rather than finding a global solution at once. Other key issue is that of the affinity measurement: the widespread usage of non-learnable pre-defined distances, such as the Euclidean and Cosine ones. This paper proposes an efficient approach for cross-cameras data-association focused on a global solution, instead of processing cameras by pairs. To avoid the usage of fixed distances, we leverage the connectivity of Graph Neural Networks, previously unused in this scope, using a Message Passing Network to jointly learn features and similarity. We validate the proposal for pedestrian multi-view association, showing results over the EPFL multi-camera pedestrian dataset. Our approach considerably outperforms the literature data association techniques, without requiring to be trained in the same scenario in which it is tested. Our code is available at \url{http://www-vpu.eps.uam.es/publications/gnn_cca}.



### Landscape of Neural Architecture Search across sensors: how much do they differ ?
- **Arxiv ID**: http://arxiv.org/abs/2201.06321v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2201.06321v2)
- **Published**: 2022-01-17 10:14:39+00:00
- **Updated**: 2022-01-20 15:07:49+00:00
- **Authors**: Kalifou René Traoré, Andrés Camero, Xiao Xiang Zhu
- **Comment**: This work is under review for a conference publication
- **Journal**: None
- **Summary**: With the rapid rise of neural architecture search, the ability to understand its complexity from the perspective of a search algorithm is desirable. Recently, Traor\'e et al. have proposed the framework of Fitness Landscape Footprint to help describe and compare neural architecture search problems. It attempts at describing why a search strategy might be successful, struggle or fail on a target task. Our study leverages this methodology in the context of searching across sensors, including sensor data fusion. In particular, we apply the Fitness Landscape Footprint to the real-world image classification problem of So2Sat LCZ42, in order to identify the most beneficial sensor to our neural network hyper-parameter optimization problem. From the perspective of distributions of fitness, our findings indicate a similar behaviour of the search space for all sensors: the longer the training time, the larger the overall fitness, and more flatness in the landscapes (less ruggedness and deviation). Regarding sensors, the better the fitness they enable (Sentinel-2), the better the search trajectories (smoother, higher persistence). Results also indicate very similar search behaviour for sensors that can be decently fitted by the search space (Sentinel-2 and fusion).



### H&E-adversarial network: a convolutional neural network to learn stain-invariant features through Hematoxylin & Eosin regression
- **Arxiv ID**: http://arxiv.org/abs/2201.06329v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06329v2)
- **Published**: 2022-01-17 10:34:23+00:00
- **Updated**: 2022-01-19 16:29:37+00:00
- **Authors**: Niccoló Marini, Manfredo Atzori, Sebastian Otálora, Stephane Marchand-Maillet, Henning Müller
- **Comment**: Errata corrige Proceedings of the IEEE/CVF International Conference
  on Computer Vision 2021
- **Journal**: None
- **Summary**: Computational pathology is a domain that aims to develop algorithms to automatically analyze large digitized histopathology images, called whole slide images (WSI). WSIs are produced scanning thin tissue samples that are stained to make specific structures visible. They show stain colour heterogeneity due to different preparation and scanning settings applied across medical centers. Stain colour heterogeneity is a problem to train convolutional neural networks (CNN), the state-of-the-art algorithms for most computational pathology tasks, since CNNs usually underperform when tested on images including different stain variations than those within data used to train the CNN. Despite several methods that were developed, stain colour heterogeneity is still an unsolved challenge that limits the development of CNNs that can generalize on data from several medical centers. This paper aims to present a novel method to train CNNs that better generalize on data including several colour variations. The method, called H&E-adversarial CNN, exploits H&E matrix information to learn stain-invariant features during the training. The method is evaluated on the classification of colon and prostate histopathology images, involving eleven heterogeneous datasets, and compared with five other techniques used to handle stain colour heterogeneity. H&E-adversarial CNNs show an improvement in performance compared to the other algorithms, demonstrating that it can help to better deal with stain colour heterogeneous images.



### Can We Find Neurons that Cause Unrealistic Images in Deep Generative Networks?
- **Arxiv ID**: http://arxiv.org/abs/2201.06346v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06346v4)
- **Published**: 2022-01-17 11:04:30+00:00
- **Updated**: 2022-06-16 02:01:22+00:00
- **Authors**: Hwanil Choi, Wonjoon Chang, Jaesik Choi
- **Comment**: Accepted at IJCAI-2022
- **Journal**: None
- **Summary**: Even though Generative Adversarial Networks (GANs) have shown a remarkable ability to generate high-quality images, GANs do not always guarantee the generation of photorealistic images. Occasionally, they generate images that have defective or unnatural objects, which are referred to as 'artifacts'. Research to investigate why these artifacts emerge and how they can be detected and removed has yet to be sufficiently carried out. To analyze this, we first hypothesize that rarely activated neurons and frequently activated neurons have different purposes and responsibilities for the progress of generating images. In this study, by analyzing the statistics and the roles for those neurons, we empirically show that rarely activated neurons are related to the failure results of making diverse objects and inducing artifacts. In addition, we suggest a correction method, called 'Sequential Ablation', to repair the defective part of the generated images without high computational cost and manual efforts.



### Disentangled Latent Transformer for Interpretable Monocular Height Estimation
- **Arxiv ID**: http://arxiv.org/abs/2201.06357v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06357v2)
- **Published**: 2022-01-17 11:42:30+00:00
- **Updated**: 2022-02-02 16:18:00+00:00
- **Authors**: Zhitong Xiong, Sining Chen, Yilei Shi, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular height estimation (MHE) from remote sensing imagery has high potential in generating 3D city models efficiently for a quick response to natural disasters. Most existing works pursue higher performance. However, there is little research exploring the interpretability of MHE networks. In this paper, we target at exploring how deep neural networks predict height from a single monocular image. Towards a comprehensive understanding of MHE networks, we propose to interpret them from multiple levels: 1) Neurons: unit-level dissection. Exploring the semantic and height selectivity of the learned internal deep representations; 2) Instances: object-level interpretation. Studying the effects of different semantic classes, scales, and spatial contexts on height estimation; 3) Attribution: pixel-level analysis. Understanding which input pixels are important for the height estimation. Based on the multi-level interpretation, a disentangled latent Transformer network is proposed towards a more compact, reliable, and explainable deep model for monocular height estimation. Furthermore, a novel unsupervised semantic segmentation task based on height estimation is first introduced in this work. Additionally, we also construct a new dataset for joint semantic segmentation and height estimation. Our work provides novel insights for both understanding and designing MHE models.



### Few-shot image segmentation for cross-institution male pelvic organs using registration-assisted prototypical learning
- **Arxiv ID**: http://arxiv.org/abs/2201.06358v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06358v1)
- **Published**: 2022-01-17 11:44:10+00:00
- **Updated**: 2022-01-17 11:44:10+00:00
- **Authors**: Yiwen Li, Yunguan Fu, Qianye Yang, Zhe Min, Wen Yan, Henkjan Huisman, Dean Barratt, Victor Adrian Prisacariu, Yipeng Hu
- **Comment**: To appear in the proceedings of the IEEE International Symposium on
  Biomedical Imaging (ISBI) 2022
- **Journal**: None
- **Summary**: The ability to adapt medical image segmentation networks for a novel class such as an unseen anatomical or pathological structure, when only a few labelled examples of this class are available from local healthcare providers, is sought-after. This potentially addresses two widely recognised limitations in deploying modern deep learning models to clinical practice, expertise-and-labour-intensive labelling and cross-institution generalisation. This work presents the first 3D few-shot interclass segmentation network for medical images, using a labelled multi-institution dataset from prostate cancer patients with eight regions of interest. We propose an image alignment module registering the predicted segmentation of both query and support data, in a standard prototypical learning algorithm, to a reference atlas space. The built-in registration mechanism can effectively utilise the prior knowledge of consistent anatomy between subjects, regardless whether they are from the same institution or not. Experimental results demonstrated that the proposed registration-assisted prototypical learning significantly improved segmentation accuracy (p-values<0.01) on query data from a holdout institution, with varying availability of support data from multiple institutions. We also report the additional benefits of the proposed 3D networks with 75% fewer parameters and an arguably simpler implementation, compared with existing 2D few-shot approaches that segment 2D slices of volumetric medical images.



### RestoreFormer: High-Quality Blind Face Restoration from Undegraded Key-Value Pairs
- **Arxiv ID**: http://arxiv.org/abs/2201.06374v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06374v3)
- **Published**: 2022-01-17 12:21:55+00:00
- **Updated**: 2022-06-25 07:15:48+00:00
- **Authors**: Zhouxia Wang, Jiawei Zhang, Runjian Chen, Wenping Wang, Ping Luo
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Blind face restoration is to recover a high-quality face image from unknown degradations. As face image contains abundant contextual information, we propose a method, RestoreFormer, which explores fully-spatial attentions to model contextual information and surpasses existing works that use local operators. RestoreFormer has several benefits compared to prior arts. First, unlike the conventional multi-head self-attention in previous Vision Transformers (ViTs), RestoreFormer incorporates a multi-head cross-attention layer to learn fully-spatial interactions between corrupted queries and high-quality key-value pairs. Second, the key-value pairs in ResotreFormer are sampled from a reconstruction-oriented high-quality dictionary, whose elements are rich in high-quality facial features specifically aimed for face reconstruction, leading to superior restoration results. Third, RestoreFormer outperforms advanced state-of-the-art methods on one synthetic dataset and three real-world datasets, as well as produces images with better visual quality.



### UWC: Unit-wise Calibration Towards Rapid Network Compression
- **Arxiv ID**: http://arxiv.org/abs/2201.06376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06376v1)
- **Published**: 2022-01-17 12:27:35+00:00
- **Updated**: 2022-01-17 12:27:35+00:00
- **Authors**: Chen Lin, Zheyang Li, Bo Peng, Haoji Hu, Wenming Tan, Ye Ren, Shiliang Pu
- **Comment**: Accepted by BMVC 2021
- **Journal**: None
- **Summary**: This paper introduces a post-training quantization~(PTQ) method achieving highly efficient Convolutional Neural Network~ (CNN) quantization with high performance. Previous PTQ methods usually reduce compression error via performing layer-by-layer parameters calibration. However, with lower representational ability of extremely compressed parameters (e.g., the bit-width goes less than 4), it is hard to eliminate all the layer-wise errors. This work addresses this issue via proposing a unit-wise feature reconstruction algorithm based on an observation of second order Taylor series expansion of the unit-wise error. It indicates that leveraging the interaction between adjacent layers' parameters could compensate layer-wise errors better. In this paper, we define several adjacent layers as a Basic-Unit, and present a unit-wise post-training algorithm which can minimize quantization error. This method achieves near-original accuracy on ImageNet and COCO when quantizing FP32 models to INT4 and INT3.



### Self-Supervised Anomaly Detection by Self-Distillation and Negative Sampling
- **Arxiv ID**: http://arxiv.org/abs/2201.06378v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06378v1)
- **Published**: 2022-01-17 12:33:14+00:00
- **Updated**: 2022-01-17 12:33:14+00:00
- **Authors**: Nima Rafiee, Rahil Gholamipoorfard, Nikolas Adaloglou, Simon Jaxy, Julius Ramakers, Markus Kollmann
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting whether examples belong to a given in-distribution or are Out-Of-Distribution (OOD) requires identifying features specific to the in-distribution. In the absence of labels, these features can be learned by self-supervised techniques under the generic assumption that the most abstract features are those which are statistically most over-represented in comparison to other distributions from the same domain. In this work, we show that self-distillation of the in-distribution training set together with contrasting against negative examples derived from shifting transformation of auxiliary data strongly improves OOD detection. We find that this improvement depends on how the negative samples are generated. In particular, we observe that by leveraging negative samples, which keep the statistics of low-level features while changing the high-level semantics, higher average detection performance is obtained. Furthermore, good negative sampling strategies can be identified from the sensitivity of the OOD detection score. The efficiency of our approach is demonstrated across a diverse range of OOD detection problems, setting new benchmarks for unsupervised OOD detection in the visual domain.



### Dual Perceptual Loss for Single Image Super-Resolution Using ESRGAN
- **Arxiv ID**: http://arxiv.org/abs/2201.06383v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06383v1)
- **Published**: 2022-01-17 12:42:56+00:00
- **Updated**: 2022-01-17 12:42:56+00:00
- **Authors**: Jie Song, Huawei Yi, Wenqian Xu, Xiaohui Li, Bo Li, Yuanyuan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The proposal of perceptual loss solves the problem that per-pixel difference loss function causes the reconstructed image to be overly-smooth, which acquires a significant progress in the field of single image super-resolution reconstruction. Furthermore, the generative adversarial networks (GAN) is applied to the super-resolution field, which effectively improves the visual quality of the reconstructed image. However, under the condtion of high upscaling factors, the excessive abnormal reasoning of the network produces some distorted structures, so that there is a certain deviation between the reconstructed image and the ground-truth image. In order to fundamentally improve the quality of reconstructed images, this paper proposes a effective method called Dual Perceptual Loss (DP Loss), which is used to replace the original perceptual loss to solve the problem of single image super-resolution reconstruction. Due to the complementary property between the VGG features and the ResNet features, the proposed DP Loss considers the advantages of learning two features simultaneously, which significantly improves the reconstruction effect of images. The qualitative and quantitative analysis on benchmark datasets demonstrates the superiority of our proposed method over state-of-the-art super-resolution methods.



### SwinUNet3D -- A Hierarchical Architecture for Deep Traffic Prediction using Shifted Window Transformers
- **Arxiv ID**: http://arxiv.org/abs/2201.06390v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06390v1)
- **Published**: 2022-01-17 12:58:45+00:00
- **Updated**: 2022-01-17 12:58:45+00:00
- **Authors**: Alabi Bojesomo, Hasan Al Marzouqi, Panos Liatsis
- **Comment**: 7 pages, 1 figure
- **Journal**: None
- **Summary**: Traffic forecasting is an important element of mobility management, an important key that drives the logistics industry. Over the years, lots of work have been done in Traffic forecasting using time series as well as spatiotemporal dynamic forecasting. In this paper, we explore the use of vision transformer in a UNet setting. We completely remove all convolution-based building blocks in UNet, while using 3D shifted window transformer in both encoder and decoder branches. In addition, we experiment with the use of feature mixing just before patch encoding to control the inter-relationship of the feature while avoiding contraction of the depth dimension of our spatiotemporal input. The proposed network is tested on the data provided by Traffic Map Movie Forecasting Challenge 2021(Traffic4cast2021), held in the competition track of Neural Information Processing Systems (NeurIPS). Traffic4cast2021 task is to predict an hour (6 frames) of traffic conditions (volume and average speed)from one hour of given traffic state (12 frames averaged in 5 minutes time span). Source code is available online at https://github.com/bojesomo/Traffic4Cast2021-SwinUNet3D.



### Deep Learning-based Quality Assessment of Clinical Protocol Adherence in Fetal Ultrasound Dating Scans
- **Arxiv ID**: http://arxiv.org/abs/2201.06406v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06406v1)
- **Published**: 2022-01-17 13:46:30+00:00
- **Updated**: 2022-01-17 13:46:30+00:00
- **Authors**: Sevim Cengiz, Mohammad Yaqub
- **Comment**: 13 pages, 2 figures, 3 tables. Proceedings of Machine Learning
  Research, Under Review. Full Paper MIDL 2022 submission
- **Journal**: None
- **Summary**: To assess fetal health during pregnancy, doctors use the gestational age (GA) calculation based on the Crown Rump Length (CRL) measurement in order to check for fetal size and growth trajectory. However, GA estimation based on CRL, requires proper positioning of calipers on the fetal crown and rump view, which is not always an easy plane to find, especially for an inexperienced sonographer. Finding a slightly oblique view from the true CRL view could lead to a different CRL value and therefore incorrect estimation of GA. This study presents an AI-based method for a quality assessment of the CRL view by verifying 7 clinical scoring criteria that are used to verify the correctness of the acquired plane. We show how our proposed solution achieves high accuracy on the majority of the scoring criteria when compared to an expert. We also show that if such scoring system is used, it helps identify poorly acquired images accurately and hence may help sonographers acquire better images which could potentially lead to a better assessment of conditions such as Intrauterine Growth Restriction (IUGR).



### Improving Performance of Semantic Segmentation CycleGANs by Noise Injection into the Latent Segmentation Space
- **Arxiv ID**: http://arxiv.org/abs/2201.06415v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06415v1)
- **Published**: 2022-01-17 14:10:27+00:00
- **Updated**: 2022-01-17 14:10:27+00:00
- **Authors**: Jonas Löhdefink, Tim Fingscheidt
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, semantic segmentation has taken benefit from various works in computer vision. Inspired by the very versatile CycleGAN architecture, we combine semantic segmentation with the concept of cycle consistency to enable a multitask training protocol. However, learning is largely prevented by the so-called steganography effect, which expresses itself as watermarks in the latent segmentation domain, making image reconstruction a too easy task. To combat this, we propose a noise injection, based either on quantization noise or on Gaussian noise addition to avoid this disadvantageous information flow in the cycle architecture. We find that noise injection significantly reduces the generation of watermarks and thus allows the recognition of highly relevant classes such as "traffic signs", which are hardly detected by the ERFNet baseline. We report mIoU and PSNR results on the Cityscapes dataset for semantic segmentation and image reconstruction, respectively. The proposed methodology allows to achieve an mIoU improvement on the Cityscapes validation set of 5.7% absolute over the same CycleGAN without noise injection, and still an absolute 4.9% over the ERFNet non-cyclic baseline.



### Homogenization of Existing Inertial-Based Datasets to Support Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.07891v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.07891v1)
- **Published**: 2022-01-17 14:29:48+00:00
- **Updated**: 2022-01-17 14:29:48+00:00
- **Authors**: Hamza Amrani, Daniela Micucci, Marco Mobilio, Paolo Napoletano
- **Comment**: None
- **Journal**: None
- **Summary**: Several techniques have been proposed to address the problem of recognizing activities of daily living from signals. Deep learning techniques applied to inertial signals have proven to be effective, achieving significant classification accuracy. Recently, research in human activity recognition (HAR) models has been almost totally model-centric. It has been proven that the number of training samples and their quality are critical for obtaining deep learning models that both perform well independently of their architecture, and that are more robust to intraclass variability and interclass similarity. Unfortunately, publicly available datasets do not always contain hight quality data and a sufficiently large and diverse number of samples (e.g., number of subjects, type of activity performed, and duration of trials). Furthermore, datasets are heterogeneous among them and therefore cannot be trivially combined to obtain a larger set. The final aim of our work is the definition and implementation of a platform that integrates datasets of inertial signals in order to make available to the scientific community large datasets of homogeneous signals, enriched, when possible, with context information (e.g., characteristics of the subjects and device position). The main focus of our platform is to emphasise data quality, which is essential for training efficient models.



### Masked Faces with Faced Masks
- **Arxiv ID**: http://arxiv.org/abs/2201.06427v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.06427v2)
- **Published**: 2022-01-17 14:37:33+00:00
- **Updated**: 2022-04-12 14:40:12+00:00
- **Authors**: Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Modern face recognition systems (FRS) still fall short when the subjects are wearing facial masks, a common theme in the age of respiratory pandemics. An intuitive partial remedy is to add a mask detector to flag any masked faces so that the FRS can act accordingly for those low-confidence masked faces. In this work, we set out to investigate the potential vulnerability of such FRS equipped with a mask detector, on large-scale masked faces, which might trigger a serious risk, e.g., letting a suspect evade the FRS where both facial identity and mask are undetected. As existing face recognizers and mask detectors have high performance in their respective tasks, it is significantly challenging to simultaneously fool them and preserve the transferability of the attack. We formulate the new task as the generation of realistic & adversarial-faced mask and make three main contributions: First, we study the naive Delanunay-based masking method (DM) to simulate the process of wearing a faced mask that is cropped from a template image, which reveals the main challenges of this new task. Second, we further equip the DM with the adversarial noise attack and propose the adversarial noise Delaunay-based masking method (AdvNoise-DM) that can fool the face recognition and mask detection effectively but make the face less natural. Third, we propose the adversarial filtering Delaunay-based masking method denoted as MF2M by employing the adversarial filtering for AdvNoise-DM and obtain more natural faces. With the above efforts, the final version not only leads to significant performance deterioration of the state-of-the-art (SOTA) deep learning-based FRS, but also remains undetected by the SOTA facial mask detector, thus successfully fooling both systems at the same time.



### Towards deep observation: A systematic survey on artificial intelligence techniques to monitor fetus via Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2201.07935v2
- **DOI**: 10.1016/j.isci.2022.104713
- **Categories**: **cs.LG**, cs.CV, cs.CY, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.07935v2)
- **Published**: 2022-01-17 14:38:53+00:00
- **Updated**: 2022-08-25 14:44:06+00:00
- **Authors**: Mahmood Alzubaidi, Marco Agus, Khalid Alyafei, Khaled A Althelaya, Uzair Shah, Alaa Abd-Alrazaq, Mohammed Anbar, Michel Makhlouf, Mowafa Househ
- **Comment**: 25 pages, 4 figures, submitted to Artificial Intelligence in Medicine
- **Journal**: IScience,Volume 25, Issue 8, 19 August 2022, 104713
- **Summary**: Developing innovative informatics approaches aimed to enhance fetal monitoring is a burgeoning field of study in reproductive medicine. Several reviews have been conducted regarding Artificial intelligence (AI) techniques to improve pregnancy outcomes. They are limited by focusing on specific data such as mother's care during pregnancy. This systematic survey aims to explore how artificial intelligence (AI) can assist with fetal growth monitoring via Ultrasound (US) image. We used eight medical and computer science bibliographic databases, including PubMed, Embase, PsycINFO, ScienceDirect, IEEE explore, ACM Library, Google Scholar, and the Web of Science. We retrieved studies published between 2010 to 2021. Data extracted from studies were synthesized using a narrative approach. Out of 1269 retrieved studies, we included 107 distinct studies from queries that were relevant to the topic in the survey. We found that 2D ultrasound images were more popular (n=88) than 3D and 4D ultrasound images (n=19). Classification is the most used method (n=42), followed by segmentation (n=31), classification integrated with segmentation (n=16) and other miscellaneous such as object-detection, regression and reinforcement learning (n=18). The most common areas within the pregnancy domain were the fetus head (n=43), then fetus body (n=31), fetus heart (n=13), fetus abdomen (n=10), and lastly the fetus face (n=10). In the most recent studies, deep learning techniques were primarily used (n=81), followed by machine learning (n=16), artificial neural network (n=7), and reinforcement learning (n=2). AI techniques played a crucial role in predicting fetal diseases and identifying fetus anatomy structures during pregnancy. More research is required to validate this technology from a physician's perspective, such as pilot studies and randomized controlled trials on AI and its applications in a hospital setting.



### FourierNet: Shape-Preserving Network for Henle's Fiber Layer Segmentation in Optical Coherence Tomography Images
- **Arxiv ID**: http://arxiv.org/abs/2201.06435v1
- **DOI**: 10.1109/JBHI.2022.3225425
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06435v1)
- **Published**: 2022-01-17 14:50:26+00:00
- **Updated**: 2022-01-17 14:50:26+00:00
- **Authors**: Selahattin Cansiz, Cem Kesim, Sevval Nur Bektas, Zeynep Kulali, Murat Hasanreisoglu, Cigdem Gunduz-Demir
- **Comment**: None
- **Journal**: IEEE Journal of Biomedical and Health Informatics, vol. 27, no. 2,
  pp. 1036-1047, Feb. 2023
- **Summary**: The Henle's fiber layer (HFL) in the retina carries valuable information on the macular condition of an eye. However, in the common practice, this layer is not separately segmented but rather included in the outer nuclear layer since it is difficult to perceive HFL contours on standard optical coherence tomography (OCT) imaging. Due to its variable reflectivity under an imaging beam, delineating the HFL contours necessitates directional OCT, which requires additional imaging. This paper addresses this issue by introducing a shape-preserving network, FourierNet, that achieves HFL segmentation in standard OCT scans with the target performance obtained when directional OCT scans are used. FourierNet is a new cascaded network design that puts forward the idea of benefiting the shape prior of HFL in the network training. This design proposes to represent the shape prior by extracting Fourier descriptors on the HFL contours and defining an additional regression task of learning these descriptors. It then formulates HFL segmentation as concurrent learning of regression and classification tasks, in which Fourier descriptors are estimated from an input image to encode the shape prior and used together with the input image to construct the HFL segmentation map. Our experiments on 1470 images of 30 OCT scans reveal that quantifying the HFL shape with Fourier descriptors and concurrently learning them with the main task of HFL segmentation lead to better results. This indicates the effectiveness of designing a shape-preserving network to improve HFL segmentation by reducing the need to perform directional OCT imaging.



### Convolutional Neural Networks for Spherical Signal Processing via Spherical Haar Tight Framelets
- **Arxiv ID**: http://arxiv.org/abs/2201.07890v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, math.FA
- **Links**: [PDF](http://arxiv.org/pdf/2201.07890v1)
- **Published**: 2022-01-17 14:52:34+00:00
- **Updated**: 2022-01-17 14:52:34+00:00
- **Authors**: Jianfei Li, Han Feng, Xiaosheng Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we develop a general theoretical framework for constructing Haar-type tight framelets on any compact set with a hierarchical partition. In particular, we construct a novel area-regular hierarchical partition on the 2-sphere and establish its corresponding spherical Haar tight framelets with directionality. We conclude by evaluating and illustrating the effectiveness of our area-regular spherical Haar tight framelets in several denoising experiments. Furthermore, we propose a convolutional neural network (CNN) model for spherical signal denoising which employs the fast framelet decomposition and reconstruction algorithms. Experiment results show that our proposed CNN model outperforms threshold methods, and processes strong generalization and robustness properties.



### A Novel Framework to Jointly Compress and Index Remote Sensing Images for Efficient Content-Based Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2201.06459v2
- **DOI**: 10.1109/IGARSS46834.2022.9884146
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06459v2)
- **Published**: 2022-01-17 15:13:03+00:00
- **Updated**: 2022-06-03 11:13:42+00:00
- **Authors**: Gencer Sumbul, Jun Xiang, Nimisha Thekke Madam, Begüm Demir
- **Comment**: Accepted at IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS) 2022. Our code is available at
  https://git.tu-berlin.de/rsim/RS-JCIF
- **Journal**: None
- **Summary**: Remote sensing (RS) images are usually stored in compressed format to reduce the storage size of the archives. Thus, existing content-based image retrieval (CBIR) systems in RS require decoding images before applying CBIR (which is computationally demanding in the case of large-scale CBIR problems). To address this problem, in this paper, we present a joint framework that simultaneously learns RS image compression and indexing. Thus, it eliminates the need for decoding RS images before applying CBIR. The proposed framework is made up of two modules. The first module compresses RS images based on an auto-encoder architecture. The second module produces hash codes with a high discrimination capability by employing soft pairwise, bit-balancing and classification loss functions. We also introduce a two stage learning strategy with gradient manipulation techniques to obtain image representations that are compatible with both RS image indexing and compression. Experimental results show the efficacy of the proposed framework when compared to widely used approaches in RS. The code of the proposed framework is available at https://git.tu-berlin.de/rsim/RS-JCIF.



### AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.06493v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06493v2)
- **Published**: 2022-01-17 16:08:57+00:00
- **Updated**: 2022-04-21 01:52:05+00:00
- **Authors**: Zehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinghong Jiang, Feng Zhao, Bolei Zhou, Hang Zhao
- **Comment**: Accepted to IJCAI2022
- **Journal**: None
- **Summary**: Object detection through either RGB images or the LiDAR point clouds has been extensively explored in autonomous driving. However, it remains challenging to make these two data sources complementary and beneficial to each other. In this paper, we propose \textit{AutoAlign}, an automatic feature fusion strategy for 3D object detection. Instead of establishing deterministic correspondence with camera projection matrix, we model the mapping relationship between the image and point clouds with a learnable alignment map. This map enables our model to automate the alignment of non-homogenous features in a dynamic and data-driven manner. Specifically, a cross-attention feature alignment module is devised to adaptively aggregate \textit{pixel-level} image features for each voxel. To enhance the semantic consistency during feature alignment, we also design a self-supervised cross-modal feature interaction module, through which the model can learn feature aggregation with \textit{instance-level} feature guidance. Extensive experimental results show that our approach can lead to 2.3 mAP and 7.0 mAP improvements on the KITTI and nuScenes datasets, respectively. Notably, our best model reaches 70.9 NDS on the nuScenes testing leaderboard, achieving competitive performance among various state-of-the-arts.



### AugLy: Data Augmentations for Robustness
- **Arxiv ID**: http://arxiv.org/abs/2201.06494v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06494v1)
- **Published**: 2022-01-17 16:08:59+00:00
- **Updated**: 2022-01-17 16:08:59+00:00
- **Authors**: Zoe Papakipos, Joanna Bitton
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce AugLy, a data augmentation library with a focus on adversarial robustness. AugLy provides a wide array of augmentations for multiple modalities (audio, image, text, & video). These augmentations were inspired by those that real users perform on social media platforms, some of which were not already supported by existing data augmentation libraries. AugLy can be used for any purpose where data augmentations are useful, but it is particularly well-suited for evaluating robustness and systematically generating adversarial attacks. In this paper we present how AugLy works, benchmark it compared against existing libraries, and use it to evaluate the robustness of various state-of-the-art models to showcase AugLy's utility. The AugLy repository can be found at https://github.com/facebookresearch/AugLy.



### Data Harmonisation for Information Fusion in Digital Healthcare: A State-of-the-Art Systematic Review, Meta-Analysis and Future Research Directions
- **Arxiv ID**: http://arxiv.org/abs/2201.06505v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06505v1)
- **Published**: 2022-01-17 16:30:15+00:00
- **Updated**: 2022-01-17 16:30:15+00:00
- **Authors**: Yang Nan, Javier Del Ser, Simon Walsh, Carola Schönlieb, Michael Roberts, Ian Selby, Kit Howard, John Owen, Jon Neville, Julien Guiot, Benoit Ernst, Ana Pastor, Angel Alberich-Bayarri, Marion I. Menzel, Sean Walsh, Wim Vos, Nina Flerin, Jean-Paul Charbonnier, Eva van Rikxoort, Avishek Chatterjee, Henry Woodruff, Philippe Lambin, Leonor Cerdá-Alberich, Luis Martí-Bonmatí, Francisco Herrera, Guang Yang
- **Comment**: 54 pages, 14 figures, accepted by the Information Fusion journal
- **Journal**: None
- **Summary**: Removing the bias and variance of multicentre data has always been a challenge in large scale digital healthcare studies, which requires the ability to integrate clinical features extracted from data acquired by different scanners and protocols to improve stability and robustness. Previous studies have described various computational approaches to fuse single modality multicentre datasets. However, these surveys rarely focused on evaluation metrics and lacked a checklist for computational data harmonisation studies. In this systematic review, we summarise the computational data harmonisation approaches for multi-modality data in the digital healthcare field, including harmonisation strategies and evaluation metrics based on different theories. In addition, a comprehensive checklist that summarises common practices for data harmonisation studies is proposed to guide researchers to report their research findings more effectively. Last but not least, flowcharts presenting possible ways for methodology and metric selection are proposed and the limitations of different methods have been surveyed for future research.



### Automatic Quantification and Visualization of Street Trees
- **Arxiv ID**: http://arxiv.org/abs/2201.06569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06569v1)
- **Published**: 2022-01-17 18:44:46+00:00
- **Updated**: 2022-01-17 18:44:46+00:00
- **Authors**: Arpit Bahety, Rohit Saluja, Ravi Kiran Sarvadevabhatla, Anbumani Subramanian, C. V. Jawahar
- **Comment**: Accepted at ICVGIP 2021
- **Journal**: None
- **Summary**: Assessing the number of street trees is essential for evaluating urban greenery and can help municipalities employ solutions to identify tree-starved streets. It can also help identify roads with different levels of deforestation and afforestation over time. Yet, there has been little work in the area of street trees quantification. This work first explains a data collection setup carefully designed for counting roadside trees. We then describe a unique annotation procedure aimed at robustly detecting and quantifying trees. We work on a dataset of around 1300 Indian road scenes annotated with over 2500 street trees. We additionally use the five held-out videos covering 25 km of roads for counting trees. We finally propose a street tree detection, counting, and visualization framework using current object detectors and a novel yet simple counting algorithm owing to the thoughtful collection setup. We find that the high-level visualizations based on the density of trees on the routes and Kernel Density Ranking (KDR) provide a quick, accurate, and inexpensive way to recognize tree-starved streets. We obtain a tree detection mAP of 83.74% on the test images, which is a 2.73% improvement over our baseline. We propose Tree Count Density Classification Accuracy (TCDCA) as an evaluation metric to measure tree density. We obtain TCDCA of 96.77% on the test videos, with a remarkable improvement of 22.58% over baseline, and demonstrate that our counting module's performance is close to human level. Source code: https://github.com/iHubData-Mobility/public-tree-counting.



### BDA-SketRet: Bi-Level Domain Adaptation for Zero-Shot SBIR
- **Arxiv ID**: http://arxiv.org/abs/2201.06570v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.06570v1)
- **Published**: 2022-01-17 18:45:55+00:00
- **Updated**: 2022-01-17 18:45:55+00:00
- **Authors**: Ushasi Chaudhuri, Ruchika Chavan, Biplab Banerjee, Anjan Dutta, Zeynep Akata
- **Comment**: None
- **Journal**: None
- **Summary**: The efficacy of zero-shot sketch-based image retrieval (ZS-SBIR) models is governed by two challenges. The immense distributions-gap between the sketches and the images requires a proper domain alignment. Moreover, the fine-grained nature of the task and the high intra-class variance of many categories necessitates a class-wise discriminative mapping among the sketch, image, and the semantic spaces. Under this premise, we propose BDA-SketRet, a novel ZS-SBIR framework performing a bi-level domain adaptation for aligning the spatial and semantic features of the visual data pairs progressively. In order to highlight the shared features and reduce the effects of any sketch or image-specific artifacts, we propose a novel symmetric loss function based on the notion of information bottleneck for aligning the semantic features while a cross-entropy-based adversarial loss is introduced to align the spatial feature maps. Finally, our CNN-based model confirms the discriminativeness of the shared latent space through a novel topology-preserving semantic projection network. Experimental results on the extended Sketchy, TU-Berlin, and QuickDraw datasets exhibit sharp improvements over the literature.



### Neural Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2201.06574v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06574v1)
- **Published**: 2022-01-17 18:50:58+00:00
- **Updated**: 2022-01-17 18:50:58+00:00
- **Authors**: Kunal Gupta, Brendan Colvert, Francisco Contijoch
- **Comment**: https://kunalmgupta.github.io/projects/NeuralCT.html
- **Journal**: None
- **Summary**: Motion during acquisition of a set of projections can lead to significant motion artifacts in computed tomography reconstructions despite fast acquisition of individual views. In cases such as cardiac imaging, motion may be unavoidable and evaluating motion may be of clinical interest. Reconstructing images with reduced motion artifacts has typically been achieved by developing systems with faster gantry rotation or using algorithms which measure and/or estimate the displacements. However, these approaches have had limited success due to both physical constraints as well as the challenge of estimating/measuring non-rigid, temporally varying, and patient-specific motions. We propose a novel reconstruction framework, NeuralCT, to generate time-resolved images free from motion artifacts. Our approaches utilizes a neural implicit approach and does not require estimation or modeling of the underlying motion. Instead, boundaries are represented using a signed distance metric and neural implicit framework. We utilize `analysis-by-synthesis' to identify a solution consistent with the acquired sinogram as well as spatial and temporal consistency constraints. We illustrate the utility of NeuralCT in three progressively more complex scenarios: translation of a small circle, heartbeat-like change in an ellipse's diameter, and complex topological deformation. Without hyperparameter tuning or change to the architecture, NeuralCT provides high quality image reconstruction for all three motions, as compared to filtered backprojection, using mean-square-error and Dice metrics.



### Collapse by Conditioning: Training Class-conditional GANs with Limited Data
- **Arxiv ID**: http://arxiv.org/abs/2201.06578v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.06578v2)
- **Published**: 2022-01-17 18:59:23+00:00
- **Updated**: 2022-03-16 10:44:34+00:00
- **Authors**: Mohamad Shahbazi, Martin Danelljan, Danda Pani Paudel, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Class-conditioning offers a direct means to control a Generative Adversarial Network (GAN) based on a discrete input variable. While necessary in many applications, the additional information provided by the class labels could even be expected to benefit the training of the GAN itself. On the contrary, we observe that class-conditioning causes mode collapse in limited data settings, where unconditional learning leads to satisfactory generative ability. Motivated by this observation, we propose a training strategy for class-conditional GANs (cGANs) that effectively prevents the observed mode-collapse by leveraging unconditional learning. Our training strategy starts with an unconditional GAN and gradually injects the class conditioning into the generator and the objective function. The proposed method for training cGANs with limited data results not only in stable training but also in generating high-quality images, thanks to the early-stage exploitation of the shared information across classes. We analyze the observed mode collapse problem in comprehensive experiments on four datasets. Our approach demonstrates outstanding results compared with state-of-the-art methods and established baselines. The code is available at https://github.com/mshahbazi72/transitional-cGAN



### Using Machine Learning to Detect Rotational Symmetries from Reflectional Symmetries in 2D Images
- **Arxiv ID**: http://arxiv.org/abs/2201.06594v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.06594v1)
- **Published**: 2022-01-17 19:14:58+00:00
- **Updated**: 2022-01-17 19:14:58+00:00
- **Authors**: Koen Ponse, Anna V. Kononova, Maria Loleyt, Bas van Stein
- **Comment**: 8 pages, 12 figures
- **Journal**: None
- **Summary**: Automated symmetry detection is still a difficult task in 2021. However, it has applications in computer vision, and it also plays an important part in understanding art. This paper focuses on aiding the latter by comparing different state-of-the-art automated symmetry detection algorithms. For one of such algorithms aimed at reflectional symmetries, we propose post-processing improvements to find localised symmetries in images, improve the selection of detected symmetries and identify another symmetry type (rotational). In order to detect rotational symmetries, we contribute a machine learning model which detects rotational symmetries based on provided reflection symmetry axis pairs. We demonstrate and analyze the performance of the extended algorithm to detect localised symmetries and the machine learning model to classify rotational symmetries.



### Who supervises the supervisor? Model monitoring in production using deep feature embeddings with applications to workpiece inspection
- **Arxiv ID**: http://arxiv.org/abs/2201.06599v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06599v1)
- **Published**: 2022-01-17 19:25:33+00:00
- **Updated**: 2022-01-17 19:25:33+00:00
- **Authors**: Michael Banf, Gregor Steinhagen
- **Comment**: None
- **Journal**: None
- **Summary**: The automation of condition monitoring and workpiece inspection plays an essential role in maintaining high quality as well as high throughput of the manufacturing process. To this end, the recent rise of developments in machine learning has lead to vast improvements in the area of autonomous process supervision. However, the more complex and powerful these models become, the less transparent and explainable they generally are as well. One of the main challenges is the monitoring of live deployments of these machine learning systems and raising alerts when encountering events that might impact model performance. In particular, supervised classifiers are typically build under the assumption of stationarity in the underlying data distribution. For example, a visual inspection system trained on a set of material surface defects generally does not adapt or even recognize gradual changes in the data distribution - an issue known as "data drift" - such as the emergence of new types of surface defects. This, in turn, may lead to detrimental mispredictions, e.g. samples from new defect classes being classified as non-defective. To this end, it is desirable to provide real-time tracking of a classifier's performance to inform about the putative onset of additional error classes and the necessity for manual intervention with respect to classifier re-training. Here, we propose an unsupervised framework that acts on top of a supervised classification system, thereby harnessing its internal deep feature representations as a proxy to track changes in the data distribution during deployment and, hence, to anticipate classifier performance degradation.



### VAQF: Fully Automatic Software-Hardware Co-Design Framework for Low-Bit Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.06618v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06618v2)
- **Published**: 2022-01-17 20:27:52+00:00
- **Updated**: 2022-02-18 18:54:59+00:00
- **Authors**: Mengshu Sun, Haoyu Ma, Guoliang Kang, Yifan Jiang, Tianlong Chen, Xiaolong Ma, Zhangyang Wang, Yanzhi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The transformer architectures with attention mechanisms have obtained success in Nature Language Processing (NLP), and Vision Transformers (ViTs) have recently extended the application domains to various vision tasks. While achieving high performance, ViTs suffer from large model size and high computation complexity that hinders the deployment of them on edge devices. To achieve high throughput on hardware and preserve the model accuracy simultaneously, we propose VAQF, a framework that builds inference accelerators on FPGA platforms for quantized ViTs with binary weights and low-precision activations. Given the model structure and the desired frame rate, VAQF will automatically output the required quantization precision for activations as well as the optimized parameter settings of the accelerator that fulfill the hardware requirements. The implementations are developed with Vivado High-Level Synthesis (HLS) on the Xilinx ZCU102 FPGA board, and the evaluation results with the DeiT-base model indicate that a frame rate requirement of 24 frames per second (FPS) is satisfied with 8-bit activation quantization, and a target of 30 FPS is met with 6-bit activation quantization. To the best of our knowledge, this is the first time quantization has been incorporated into ViT acceleration on FPGAs with the help of a fully automatic framework to guide the quantization strategy on the software side and the accelerator implementations on the hardware side given the target frame rate. Very small compilation time cost is incurred compared with quantization training, and the generated accelerators show the capability of achieving real-time execution for state-of-the-art ViT models on FPGAs.



### Validation of object detection in UAV-based images using synthetic data
- **Arxiv ID**: http://arxiv.org/abs/2201.06629v1
- **DOI**: 10.1117/12.2586860
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06629v1)
- **Published**: 2022-01-17 20:56:56+00:00
- **Updated**: 2022-01-17 20:56:56+00:00
- **Authors**: Eung-Joo Lee, Damon M. Conover, Shuvra S. Bhattacharyyaa, Heesung Kwon, Jason Hill, Kenneth Evensen
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is increasingly used onboard Unmanned Aerial Vehicles (UAV) for various applications; however, the machine learning (ML) models for UAV-based detection are often validated using data curated for tasks unrelated to the UAV application. This is a concern because training neural networks on large-scale benchmarks have shown excellent capability in generic object detection tasks, yet conventional training approaches can lead to large inference errors for UAV-based images. Such errors arise due to differences in imaging conditions between images from UAVs and images in training. To overcome this problem, we characterize boundary conditions of ML models, beyond which the models exhibit rapid degradation in detection accuracy. Our work is focused on understanding the impact of different UAV-based imaging conditions on detection performance by using synthetic data generated using a game engine. Properties of the game engine are exploited to populate the synthetic datasets with realistic and annotated images. Specifically, it enables the fine control of various parameters, such as camera position, view angle, illumination conditions, and object pose. Using the synthetic datasets, we analyze detection accuracy in different imaging conditions as a function of the above parameters. We use three well-known neural network models with different model complexity in our work. In our experiment, we observe and quantify the following: 1) how detection accuracy drops as the camera moves toward the nadir-view region; 2) how detection accuracy varies depending on different object poses, and 3) the degree to which the robustness of the models changes as illumination conditions vary.



### Towards Adversarial Evaluations for Inexact Machine Unlearning
- **Arxiv ID**: http://arxiv.org/abs/2201.06640v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06640v3)
- **Published**: 2022-01-17 21:49:21+00:00
- **Updated**: 2023-02-22 12:33:14+00:00
- **Authors**: Shashwat Goel, Ameya Prabhu, Amartya Sanyal, Ser-Nam Lim, Philip Torr, Ponnurangam Kumaraguru
- **Comment**: Tech Report
- **Journal**: None
- **Summary**: Machine Learning models face increased concerns regarding the storage of personal user data and adverse impacts of corrupted data like backdoors or systematic bias. Machine Unlearning can address these by allowing post-hoc deletion of affected training data from a learned model. Achieving this task exactly is computationally expensive; consequently, recent works have proposed inexact unlearning algorithms to solve this approximately as well as evaluation methods to test the effectiveness of these algorithms.   In this work, we first outline some necessary criteria for evaluation methods and show no existing evaluation satisfies them all. Then, we design a stronger black-box evaluation method called the Interclass Confusion (IC) test which adversarially manipulates data during training to detect the insufficiency of unlearning procedures. We also propose two analytically motivated baseline methods~(EU-k and CF-k) which outperform several popular inexact unlearning methods. Overall, we demonstrate how adversarial evaluation strategies can help in analyzing various unlearning phenomena which can guide the development of stronger unlearning algorithms.



### Explainable Ensemble Machine Learning for Breast Cancer Diagnosis based on Ultrasound Image Texture Features
- **Arxiv ID**: http://arxiv.org/abs/2201.07227v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.07227v1)
- **Published**: 2022-01-17 22:13:03+00:00
- **Updated**: 2022-01-17 22:13:03+00:00
- **Authors**: Alireza Rezazadeh, Yasamin Jafarian, Ali Kord
- **Comment**: None
- **Journal**: None
- **Summary**: Image classification is widely used to build predictive models for breast cancer diagnosis. Most existing approaches overwhelmingly rely on deep convolutional networks to build such diagnosis pipelines. These model architectures, although remarkable in performance, are black-box systems that provide minimal insight into the inner logic behind their predictions. This is a major drawback as the explainability of prediction is vital for applications such as cancer diagnosis. In this paper, we address this issue by proposing an explainable machine learning pipeline for breast cancer diagnosis based on ultrasound images. We extract first- and second-order texture features of the ultrasound images and use them to build a probabilistic ensemble of decision tree classifiers. Each decision tree learns to classify the input ultrasound image by learning a set of robust decision thresholds for texture features of the image. The decision path of the model predictions can then be interpreted by decomposing the learned decision trees. Our results show that our proposed framework achieves high predictive performance while being explainable.



### HydraFusion: Context-Aware Selective Sensor Fusion for Robust and Efficient Autonomous Vehicle Perception
- **Arxiv ID**: http://arxiv.org/abs/2201.06644v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06644v1)
- **Published**: 2022-01-17 22:19:53+00:00
- **Updated**: 2022-01-17 22:19:53+00:00
- **Authors**: Arnav Vaibhav Malawade, Trier Mortlock, Mohammad Abdullah Al Faruque
- **Comment**: Accepted to be published in the 13th ACM/IEEE International
  Conference on Cyber-Physical Systems (ICCPS 2022)
- **Journal**: None
- **Summary**: Although autonomous vehicles (AVs) are expected to revolutionize transportation, robust perception across a wide range of driving contexts remains a significant challenge. Techniques to fuse sensor data from camera, radar, and lidar sensors have been proposed to improve AV perception. However, existing methods are insufficiently robust in difficult driving contexts (e.g., bad weather, low light, sensor obstruction) due to rigidity in their fusion implementations. These methods fall into two broad categories: (i) early fusion, which fails when sensor data is noisy or obscured, and (ii) late fusion, which cannot leverage features from multiple sensors and thus produces worse estimates. To address these limitations, we propose HydraFusion: a selective sensor fusion framework that learns to identify the current driving context and fuses the best combination of sensors to maximize robustness without compromising efficiency. HydraFusion is the first approach to propose dynamically adjusting between early fusion, late fusion, and combinations in-between, thus varying both how and when fusion is applied. We show that, on average, HydraFusion outperforms early and late fusion approaches by 13.66% and 14.54%, respectively, without increasing computational complexity or energy consumption on the industry-standard Nvidia Drive PX2 AV hardware platform. We also propose and evaluate both static and deep-learning-based context identification strategies. Our open-source code and model implementation are available at https://github.com/AICPS/hydrafusion.



### OmniPrint: A Configurable Printed Character Synthesizer
- **Arxiv ID**: http://arxiv.org/abs/2201.06648v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.06648v1)
- **Published**: 2022-01-17 22:31:35+00:00
- **Updated**: 2022-01-17 22:31:35+00:00
- **Authors**: Haozhe Sun, Wei-Wei Tu, Isabelle Guyon
- **Comment**: Accepted at 35th Conference on Neural Information Processing Systems
  (NeurIPS 2021) Track on Datasets and Benchmarks.
  https://openreview.net/forum?id=R07XwJPmgpl
- **Journal**: 35th Conference on Neural Information Processing Systems (NeurIPS
  2021) Track on Datasets and Benchmarks
- **Summary**: We introduce OmniPrint, a synthetic data generator of isolated printed characters, geared toward machine learning research. It draws inspiration from famous datasets such as MNIST, SVHN and Omniglot, but offers the capability of generating a wide variety of printed characters from various languages, fonts and styles, with customized distortions. We include 935 fonts from 27 scripts and many types of distortions. As a proof of concept, we show various use cases, including an example of meta-learning dataset designed for the upcoming MetaDL NeurIPS 2021 competition. OmniPrint is available at https://github.com/SunHaozhe/OmniPrint.



