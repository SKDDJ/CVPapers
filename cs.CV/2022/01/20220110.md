# Arxiv Papers in cs.CV on 2022-01-10
### Multimodal Representations Learning Based on Mutual Information Maximization and Minimization and Identity Embedding for Multimodal Sentiment Analysis
- **Arxiv ID**: http://arxiv.org/abs/2201.03969v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.03969v2)
- **Published**: 2022-01-10 01:41:39+00:00
- **Updated**: 2022-07-04 15:59:47+00:00
- **Authors**: Jiahao Zheng, Sen Zhang, Xiaoping Wang, Zhigang Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal sentiment analysis (MSA) is a fundamental complex research problem due to the heterogeneity gap between different modalities and the ambiguity of human emotional expression. Although there have been many successful attempts to construct multimodal representations for MSA, there are still two challenges to be addressed: 1) A more robust multimodal representation needs to be constructed to bridge the heterogeneity gap and cope with the complex multimodal interactions, and 2) the contextual dynamics must be modeled effectively throughout the information flow. In this work, we propose a multimodal representation model based on Mutual information Maximization and Minimization and Identity Embedding (MMMIE). We combine mutual information maximization between modal pairs, and mutual information minimization between input data and corresponding features to mine the modal-invariant and task-related information. Furthermore, Identity Embedding is proposed to prompt the downstream network to perceive the contextual information. Experimental results on two public datasets demonstrate the effectiveness of the proposed model.



### Systematic biases when using deep neural networks for annotating large catalogs of astronomical images
- **Arxiv ID**: http://arxiv.org/abs/2201.03131v1
- **DOI**: None
- **Categories**: **astro-ph.GA**, astro-ph.CO, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03131v1)
- **Published**: 2022-01-10 01:51:14+00:00
- **Updated**: 2022-01-10 01:51:14+00:00
- **Authors**: Sanchari Dhar, Lior Shamir
- **Comment**: A&C, accepted
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) have become the most common solution for automatic image annotation due to their non-parametric nature, good performance, and their accessibility through libraries such as TensorFlow. Among other fields, DCNNs are also a common approach to the annotation of large astronomical image databases acquired by digital sky surveys. One of the main downsides of DCNNs is the complex non-intuitive rules that make DCNNs act as a ``black box", providing annotations in a manner that is unclear to the user. Therefore, the user is often not able to know what information is used by the DCNNs for the classification. Here we demonstrate that the training of a DCNN is sensitive to the context of the training data such as the location of the objects in the sky. We show that for basic classification of elliptical and spiral galaxies, the sky location of the galaxies used for training affects the behavior of the algorithm, and leads to a small but consistent and statistically significant bias. That bias exhibits itself in the form of cosmological-scale anisotropy in the distribution of basic galaxy morphology. Therefore, while DCNNs are powerful tools for annotating images of extended sources, the construction of training sets for galaxy morphology should take into consideration more aspects than the visual appearance of the object. In any case, catalogs created with deep neural networks that exhibit signs of cosmological anisotropy should be interpreted with the possibility of consistent bias.



### Multi-Level Attention for Unsupervised Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2201.03141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03141v1)
- **Published**: 2022-01-10 02:47:06+00:00
- **Updated**: 2022-01-10 02:47:06+00:00
- **Authors**: Yi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: The attention mechanism is widely used in deep learning because of its excellent performance in neural networks without introducing additional information. However, in unsupervised person re-identification, the attention module represented by multi-headed self-attention suffers from attention spreading in the condition of non-ground truth. To solve this problem, we design pixel-level attention module to provide constraints for multi-headed self-attention. Meanwhile, for the trait that the identification targets of person re-identification data are all pedestrians in the samples, we design domain-level attention module to provide more comprehensive pedestrian features. We combine head-level, pixel-level and domain-level attention to propose multi-level attention block and validate its performance on for large person re-identification datasets (Market-1501, DukeMTMC-reID and MSMT17 and PersonX).



### Enhancing Low-Light Images in Real World via Cross-Image Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2201.03145v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.3; I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2201.03145v2)
- **Published**: 2022-01-10 03:12:52+00:00
- **Updated**: 2022-07-08 03:31:31+00:00
- **Authors**: Lanqing Guo, Renjie Wan, Wenhan Yang, Alex Kot, Bihan Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Images captured in the low-light condition suffer from low visibility and various imaging artifacts, e.g., real noise. Existing supervised enlightening algorithms require a large set of pixel-aligned training image pairs, which are hard to prepare in practice. Though weakly-supervised or unsupervised methods can alleviate such challenges without using paired training images, some real-world artifacts inevitably get falsely amplified because of the lack of corresponded supervision. In this paper, instead of using perfectly aligned images for training, we creatively employ the misaligned real-world images as the guidance, which are considerably easier to collect. Specifically, we propose a Cross-Image Disentanglement Network (CIDN) to separately extract cross-image brightness and image-specific content features from low/normal-light images. Based on that, CIDN can simultaneously correct the brightness and suppress image artifacts in the feature domain, which largely increases the robustness to the pixel shifts. Furthermore, we collect a new low-light image enhancement dataset consisting of misaligned training images with real-world corruptions. Experimental results show that our model achieves state-of-the-art performances on both the newly proposed dataset and other popular low-light datasets.



### Identification of chicken egg fertility using SVM classifier based on first-order statistical feature extraction
- **Arxiv ID**: http://arxiv.org/abs/2201.04063v1
- **DOI**: 10.33096/ilkom.v13i3.937.285-293
- **Categories**: **cs.CV**, eess.IV, 94A08, I.5.1; I.4.m
- **Links**: [PDF](http://arxiv.org/pdf/2201.04063v1)
- **Published**: 2022-01-10 04:29:49+00:00
- **Updated**: 2022-01-10 04:29:49+00:00
- **Authors**: Shoffan Saifullah, Andiko Putro Suryotomo
- **Comment**: 9 Pages, 5 Figures, 2 Tables
- **Journal**: ILKOM Jurnal Ilmiah, 13(3), (2021), 285-293
- **Summary**: This study aims to identify chicken eggs fertility using the support vector machine (SVM) classifier method. The classification basis used the first-order statistical (FOS) parameters as feature extraction in the identification process. This research was developed based on the process's identification process, which is still manual (conventional). Although currently there are many technologies in the identification process, they still need development. Thus, this research is one of the developments in the field of image processing technology. The sample data uses datasets from previous studies with a total of 100 egg images. The egg object in the image is a single object. From these data, the classification of each fertile and infertile egg is 50 image data. Chicken egg image data became input in image processing, with the initial process is segmentation. This initial segmentation aims to get the cropped image according to the object. The cropped image is repaired using image preprocessing with grayscaling and image enhancement methods. This method (image enhancement) used two combination methods: contrast limited adaptive histogram equalization (CLAHE) and histogram equalization (HE). The improved image becomes the input for feature extraction using the FOS method. The FOS uses five parameters, namely mean, entropy, variance, skewness, and kurtosis. The five parameters entered into the SVM classifier method to identify the fertility of chicken eggs. The results of these experiments, the method proposed in the identification process has a success percentage of 84.57%. Thus, the implementation of this method can be used as a reference for future research improvements. In addition, it may be possible to use a second-order feature extraction method to improve its accuracy and improve supervised learning for classification.



### TFS Recognition: Investigating MPH]{Thai Finger Spelling Recognition: Investigating MediaPipe Hands Potentials
- **Arxiv ID**: http://arxiv.org/abs/2201.03170v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2201.03170v1)
- **Published**: 2022-01-10 05:27:58+00:00
- **Updated**: 2022-01-10 05:27:58+00:00
- **Authors**: Jinnavat Sanalohit, Tatpong Katanyukul
- **Comment**: 19 pages, 10 figures
- **Journal**: None
- **Summary**: Thai Finger Spelling (TFS) sign recognition could benefit a community of hearing-difficulty people in bridging to a major hearing population. With a relatively large number of alphabets, TFS employs multiple signing schemes. Two schemes of more common signing -- static and dynamic single-hand signing, widely used in other sign languages -- have been addressed in several previous works. To complete the TFS sign recognition, the remaining two of quite distinct signing schemes -- static and dynamic point-on-hand signing -- need to be sufficiently addressed.   With the advent of many off-the-shelf hand skeleton prediction models and that training a model to recognize a sign language from scratch is expensive, we explore an approach building upon recently launched MediaPipe Hands (MPH). MPH is a high-precision well-trained model for hand-keypoint detection.   We have investigated MPH on three TFS schemes: static-single-hand (S1), simplified dynamic-single-hand (S2) and static-point-on-hand (P1) schemes.   Our results show that MPH can satisfactorily address single-hand schemes with accuracy of 84.57% on both S1 and S2.   However, our finding reveals a shortcoming of MPH in addressing a point-on-hand scheme, whose accuracy is 23.66% on P1 conferring to 69.19% obtained from conventional classification trained from scratch. This shortcoming has been investigated and attributed to self occlusion and handedness.



### Pedestrian Detection: Domain Generalization, CNNs, Transformers and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2201.03176v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03176v2)
- **Published**: 2022-01-10 06:00:26+00:00
- **Updated**: 2022-03-02 06:39:02+00:00
- **Authors**: Irtiza Hasan, Shengcai Liao, Jinpeng Li, Saad Ullah Akram, Ling Shao
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Pedestrian detection is the cornerstone of many vision based applications, starting from object tracking to video surveillance and more recently, autonomous driving. With the rapid development of deep learning in object detection, pedestrian detection has achieved very good performance in traditional single-dataset training and evaluation setting. However, in this study on generalizable pedestrian detectors, we show that, current pedestrian detectors poorly handle even small domain shifts in cross-dataset evaluation. We attribute the limited generalization to two main factors, the method and the current sources of data. Regarding the method, we illustrate that biasness present in the design choices (e.g anchor settings) of current pedestrian detectors are the main contributing factor to the limited generalization. Most modern pedestrian detectors are tailored towards target dataset, where they do achieve high performance in traditional single training and testing pipeline, but suffer a degrade in performance when evaluated through cross-dataset evaluation. Consequently, a general object detector performs better in cross-dataset evaluation compared with state of the art pedestrian detectors, due to its generic design. As for the data, we show that the autonomous driving benchmarks are monotonous in nature, that is, they are not diverse in scenarios and dense in pedestrians. Therefore, benchmarks curated by crawling the web (which contain diverse and dense scenarios), are an efficient source of pre-training for providing a more robust representation. Accordingly, we propose a progressive fine-tuning strategy which improves generalization. Code and models can accessed at https://github.com/hasanirtiza/Pedestron.



### Swin Transformer coupling CNNs Makes Strong Contextual Encoders for VHR Image Road Extraction
- **Arxiv ID**: http://arxiv.org/abs/2201.03178v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03178v2)
- **Published**: 2022-01-10 06:05:12+00:00
- **Updated**: 2023-05-28 06:57:17+00:00
- **Authors**: Tao Chen, Yiran Liu, Haoyu Jiang, Ruirui Li
- **Comment**: None
- **Journal**: None
- **Summary**: Accurately segmenting roads is challenging due to substantial intra-class variations, indistinct inter-class distinctions, and occlusions caused by shadows, trees, and buildings. To address these challenges, attention to important texture details and perception of global geometric contextual information are essential. Recent research has shown that CNN-Transformer hybrid structures outperform using CNN or Transformer alone. While CNN excels at extracting local detail features, the Transformer naturally perceives global contextual information. In this paper, we propose a dual-branch network block named ConSwin that combines ResNet and SwinTransformers for road extraction tasks. This ConSwin block harnesses the strengths of both approaches to better extract detailed and global features. Based on ConSwin, we construct an hourglass-shaped road extraction network and introduce two novel connection structures to better transmit texture and structural detail information to the decoder. Our proposed method outperforms state-of-the-art methods on both the Massachusetts and CHN6-CUG datasets in terms of overall accuracy, IOU, and F1 indicators. Additional experiments validate the effectiveness of our proposed module, while visualization results demonstrate its ability to obtain better road representations.



### Transfer Learning for Scene Text Recognition in Indian Languages
- **Arxiv ID**: http://arxiv.org/abs/2201.03180v1
- **DOI**: 10.1007/978-3-030-86198-8_14
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03180v1)
- **Published**: 2022-01-10 06:14:49+00:00
- **Updated**: 2022-01-10 06:14:49+00:00
- **Authors**: Sanjana Gunna, Rohit Saluja, C. V. Jawahar
- **Comment**: 16 pages, 5 figures
- **Journal**: ICDAR 2021: Document Analysis and Recognition, ICDAR 2021
  Workshops, pp 182-197
- **Summary**: Scene text recognition in low-resource Indian languages is challenging because of complexities like multiple scripts, fonts, text size, and orientations. In this work, we investigate the power of transfer learning for all the layers of deep scene text recognition networks from English to two common Indian languages. We perform experiments on the conventional CRNN model and STAR-Net to ensure generalisability. To study the effect of change in different scripts, we initially run our experiments on synthetic word images rendered using Unicode fonts. We show that the transfer of English models to simple synthetic datasets of Indian languages is not practical. Instead, we propose to apply transfer learning techniques among Indian languages due to similarity in their n-gram distributions and visual features like the vowels and conjunct characters. We then study the transfer learning among six Indian languages with varying complexities in fonts and word length statistics. We also demonstrate that the learned features of the models transferred from other Indian languages are visually closer (and sometimes even better) to the individual model features than those transferred from English. We finally set new benchmarks for scene-text recognition on Hindi, Telugu, and Malayalam datasets from IIIT-ILST and Bangla dataset from MLT-17 by achieving 6%, 5%, 2%, and 23% gains in Word Recognition Rates (WRRs) compared to previous works. We further improve the MLT-17 Bangla results by plugging in a novel correction BiLSTM into our model. We additionally release a dataset of around 440 scene images containing 500 Gujarati and 2535 Tamil words. WRRs improve over the baselines by 8%, 4%, 5%, and 3% on the MLT-19 Hindi and Bangla datasets and the Gujarati and Tamil datasets.



### Towards Boosting the Accuracy of Non-Latin Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.03185v1
- **DOI**: 10.1007/978-3-030-86198-8_20
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03185v1)
- **Published**: 2022-01-10 06:36:43+00:00
- **Updated**: 2022-01-10 06:36:43+00:00
- **Authors**: Sanjana Gunna, Rohit Saluja, C. V. Jawahar
- **Comment**: 12 pages, 6 figures
- **Journal**: ICDAR 2021: Document Analysis and Recognition, ICDAR 2021
  Workshops, pp 282-293
- **Summary**: Scene-text recognition is remarkably better in Latin languages than the non-Latin languages due to several factors like multiple fonts, simplistic vocabulary statistics, updated data generation tools, and writing systems. This paper examines the possible reasons for low accuracy by comparing English datasets with non-Latin languages. We compare various features like the size (width and height) of the word images and word length statistics. Over the last decade, generating synthetic datasets with powerful deep learning techniques has tremendously improved scene-text recognition. Several controlled experiments are performed on English, by varying the number of (i) fonts to create the synthetic data and (ii) created word images. We discover that these factors are critical for the scene-text recognition systems. The English synthetic datasets utilize over 1400 fonts while Arabic and other non-Latin datasets utilize less than 100 fonts for data generation. Since some of these languages are a part of different regions, we garner additional fonts through a region-based search to improve the scene-text recognition models in Arabic and Devanagari. We improve the Word Recognition Rates (WRRs) on Arabic MLT-17 and MLT-19 datasets by 24.54% and 2.32% compared to previous works or baselines. We achieve WRR gains of 7.88% and 3.72% for IIIT-ILST and MLT-19 Devanagari datasets.



### MyoPS: A Benchmark of Myocardial Pathology Segmentation Combining Three-Sequence Cardiac Magnetic Resonance Images
- **Arxiv ID**: http://arxiv.org/abs/2201.03186v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.03186v1)
- **Published**: 2022-01-10 06:37:23+00:00
- **Updated**: 2022-01-10 06:37:23+00:00
- **Authors**: Lei Li, Fuping Wu, Sihan Wang, Xinzhe Luo, Carlos Martin-Isla, Shuwei Zhai, Jianpeng Zhang, Yanfei Liu7, Zhen Zhang, Markus J. Ankenbrand, Haochuan Jiang, Xiaoran Zhang, Linhong Wang, Tewodros Weldebirhan Arega, Elif Altunok, Zhou Zhao, Feiyan Li, Jun Ma, Xiaoping Yang, Elodie Puybareau, Ilkay Oksuz, Stephanie Bricq, Weisheng Li, Kumaradevan Punithakumar, Sotirios A. Tsaftaris, Laura M. Schreiber, Mingjing Yang, Guocai Liu, Yong Xia, Guotai Wang, Sergio Escalera, Xiahai Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: Assessment of myocardial viability is essential in diagnosis and treatment management of patients suffering from myocardial infarction, and classification of pathology on myocardium is the key to this assessment. This work defines a new task of medical image analysis, i.e., to perform myocardial pathology segmentation (MyoPS) combining three-sequence cardiac magnetic resonance (CMR) images, which was first proposed in the MyoPS challenge, in conjunction with MICCAI 2020. The challenge provided 45 paired and pre-aligned CMR images, allowing algorithms to combine the complementary information from the three CMR sequences for pathology segmentation. In this article, we provide details of the challenge, survey the works from fifteen participants and interpret their methods according to five aspects, i.e., preprocessing, data augmentation, learning strategy, model architecture and post-processing. In addition, we analyze the results with respect to different factors, in order to examine the key obstacles and explore potential of solutions, as well as to provide a benchmark for future research. We conclude that while promising results have been reported, the research is still in the early stage, and more in-depth exploration is needed before a successful application to the clinics. Note that MyoPS data and evaluation tool continue to be publicly available upon registration via its homepage (www.sdspeople.fudan.edu.cn/zhuangxiahai/0/myops20/).



### Reproducing BowNet: Learning Representations by Predicting Bags of Visual Words
- **Arxiv ID**: http://arxiv.org/abs/2201.03556v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03556v2)
- **Published**: 2022-01-10 07:00:22+00:00
- **Updated**: 2022-01-14 19:55:43+00:00
- **Authors**: Harry Nguyen, Stone Yun, Hisham Mohammad
- **Comment**: This is a reproducibility project. Original work is by Gidaris et al.
  published in CVPR 2020. Pytorch implementation is public on Github. v2
  clarifies comments regarding communication with original authors
- **Journal**: None
- **Summary**: This work aims to reproduce results from the CVPR 2020 paper by Gidaris et al. Self-supervised learning (SSL) is used to learn feature representations of an image using an unlabeled dataset. This work proposes to use bag-of-words (BoW) deep feature descriptors as a self-supervised learning target to learn robust, deep representations. BowNet is trained to reconstruct the histogram of visual words (ie. the deep BoW descriptor) of a reference image when presented a perturbed version of the image as input. Thus, this method aims to learn perturbation-invariant and context-aware image features that can be useful for few-shot tasks or supervised downstream tasks. In the paper, the author describes BowNet as a network consisting of a convolutional feature extractor $\Phi(\cdot)$ and a Dense-softmax layer $\Omega(\cdot)$ trained to predict BoW features from images. After BoW training, the features of $\Phi$ are used in downstream tasks. For this challenge we were trying to build and train a network that could reproduce the CIFAR-100 accuracy improvements reported in the original paper. However, we were unsuccessful in reproducing an accuracy improvement comparable to what the authors mentioned. This could be for a variety of factors and we believe that time constraints were the primary bottleneck.



### Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification
- **Arxiv ID**: http://arxiv.org/abs/2201.03194v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03194v2)
- **Published**: 2022-01-10 07:17:24+00:00
- **Updated**: 2022-01-11 06:57:52+00:00
- **Authors**: Jingzhou Chen, Peng Wang, Jian Liu, Yuntao Qian
- **Comment**: None
- **Journal**: None
- **Summary**: Hierarchical multi-granularity classification (HMC) assigns hierarchical multi-granularity labels to each object and focuses on encoding the label hierarchy, e.g., ["Albatross", "Laysan Albatross"] from coarse-to-fine levels. However, the definition of what is fine-grained is subjective, and the image quality may affect the identification. Thus, samples could be observed at any level of the hierarchy, e.g., ["Albatross"] or ["Albatross", "Laysan Albatross"], and examples discerned at coarse categories are often neglected in the conventional setting of HMC. In this paper, we study the HMC problem in which objects are labeled at any level of the hierarchy. The essential designs of the proposed method are derived from two motivations: (1) learning with objects labeled at various levels should transfer hierarchical knowledge between levels; (2) lower-level classes should inherit attributes related to upper-level superclasses. The proposed combinatorial loss maximizes the marginal probability of the observed ground truth label by aggregating information from related labels defined in the tree hierarchy. If the observed label is at the leaf level, the combinatorial loss further imposes the multi-class cross-entropy loss to increase the weight of fine-grained classification loss. Considering the hierarchical feature interaction, we propose a hierarchical residual network (HRN), in which granularity-specific features from parent levels acting as residual connections are added to features of children levels. Experiments on three commonly used datasets demonstrate the effectiveness of our approach compared to the state-of-the-art HMC approaches and fine-grained visual classification (FGVC) methods exploiting the label hierarchy.



### End-to-end lossless compression of high precision depth maps guided by pseudo-residual
- **Arxiv ID**: http://arxiv.org/abs/2201.03195v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.03195v1)
- **Published**: 2022-01-10 07:19:02+00:00
- **Updated**: 2022-01-10 07:19:02+00:00
- **Authors**: Yuyang Wu, Wei Gao
- **Comment**: Data Compression Conference 2022
- **Journal**: None
- **Summary**: As a fundamental data format representing spatial information, depth map is widely used in signal processing and computer vision fields. Massive amount of high precision depth maps are produced with the rapid development of equipment like laser scanner or LiDAR. Therefore, it is urgent to explore a new compression method with better compression ratio for high precision depth maps. Utilizing the wide spread deep learning environment, we propose an end-to-end learning-based lossless compression method for high precision depth maps. The whole process is comprised of two sub-processes, named pre-processing of depth maps and deep lossless compression of processed depth maps. The deep lossless compression network consists of two sub-networks, named lossy compression network and lossless compression network. We leverage the concept of pseudo-residual to guide the generation of distribution for residual and avoid introducing context models. Our end-to-end lossless compression network achieves competitive performance over engineered codecs and has low computational cost.



### Model-Based Image Signal Processors via Learnable Dictionaries
- **Arxiv ID**: http://arxiv.org/abs/2201.03210v1
- **DOI**: 10.1609/aaai.v36i1.19926
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.03210v1)
- **Published**: 2022-01-10 08:36:10+00:00
- **Updated**: 2022-01-10 08:36:10+00:00
- **Authors**: Marcos V. Conde, Steven McDonagh, Matteo Maggioni, Aleš Leonardis, Eduardo Pérez-Pellitero
- **Comment**: AAAI 2022
- **Journal**: Vol. 36 No. 1: AAAI-22 Technical Tracks 1 (2022) 481-489
- **Summary**: Digital cameras transform sensor RAW readings into RGB images by means of their Image Signal Processor (ISP). Computational photography tasks such as image denoising and colour constancy are commonly performed in the RAW domain, in part due to the inherent hardware design, but also due to the appealing simplicity of noise statistics that result from the direct sensor readings. Despite this, the availability of RAW images is limited in comparison with the abundance and diversity of available RGB data. Recent approaches have attempted to bridge this gap by estimating the RGB to RAW mapping: handcrafted model-based methods that are interpretable and controllable usually require manual parameter fine-tuning, while end-to-end learnable neural networks require large amounts of training data, at times with complex training procedures, and generally lack interpretability and parametric control. Towards addressing these existing limitations, we present a novel hybrid model-based and data-driven ISP that builds on canonical ISP operations and is both learnable and interpretable. Our proposed invertible model, capable of bidirectional mapping between RAW and RGB domains, employs end-to-end learning of rich parameter representations, i.e. dictionaries, that are free from direct parametric supervision and additionally enable simple and plausible data augmentation. We evidence the value of our data generation process by extensive experiments under both RAW image reconstruction and RAW image denoising tasks, obtaining state-of-the-art performance in both. Additionally, we show that our ISP can learn meaningful mappings from few data samples, and that denoising models trained with our dictionary-based data augmentation are competitive despite having only few or zero ground-truth labels.



### Why-So-Deep: Towards Boosting Previously Trained Models for Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.03212v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.03212v1)
- **Published**: 2022-01-10 08:39:06+00:00
- **Updated**: 2022-01-10 08:39:06+00:00
- **Authors**: M. Usman Maqbool Bhutta, Yuxiang Sun, Darwin Lau, Ming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based image retrieval techniques for the loop closure detection demonstrate satisfactory performance. However, it is still challenging to achieve high-level performance based on previously trained models in different geographical regions. This paper addresses the problem of their deployment with simultaneous localization and mapping (SLAM) systems in the new environment. The general baseline approach uses additional information, such as GPS, sequential keyframes tracking, and re-training the whole environment to enhance the recall rate. We propose a novel approach for improving image retrieval based on previously trained models. We present an intelligent method, MAQBOOL, to amplify the power of pre-trained models for better image recall and its application to real-time multiagent SLAM systems. We achieve comparable image retrieval results at a low descriptor dimension (512-D), compared to the high descriptor dimension (4096-D) of state-of-the-art methods. We use spatial information to improve the recall rate in image retrieval on pre-trained models.



### Fully automatic scoring of handwritten descriptive answers in Japanese language tests
- **Arxiv ID**: http://arxiv.org/abs/2201.03215v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.03215v1)
- **Published**: 2022-01-10 08:47:52+00:00
- **Updated**: 2022-01-10 08:47:52+00:00
- **Authors**: Hung Tuan Nguyen, Cuong Tuan Nguyen, Haruki Oka, Tsunenori Ishioka, Masaki Nakagawa
- **Comment**: Keywords: handwritten language answers, handwriting recognition,
  automatic scoring, ensemble recognition, deep neural networks Appeared in
  IEICE technical report, PRMU2021-32, pp.45-50 (2021.12)
- **Journal**: None
- **Summary**: This paper presents an experiment of automatically scoring handwritten descriptive answers in the trial tests for the new Japanese university entrance examination, which were made for about 120,000 examinees in 2017 and 2018. There are about 400,000 answers with more than 20 million characters. Although all answers have been scored by human examiners, handwritten characters are not labelled. We present our attempt to adapt deep neural network-based handwriting recognizers trained on a labelled handwriting dataset into this unlabeled answer set. Our proposed method combines different training strategies, ensembles multiple recognizers, and uses a language model built from a large general corpus to avoid overfitting into specific data. In our experiment, the proposed method records character accuracy of over 97% using about 2,000 verified labelled answers that account for less than 0.5% of the dataset. Then, the recognized answers are fed into a pre-trained automatic scoring system based on the BERT model without correcting misrecognized characters and providing rubric annotations. The automatic scoring system achieves from 0.84 to 0.98 of Quadratic Weighted Kappa (QWK). As QWK is over 0.8, it represents acceptable similarity of scoring between the automatic scoring system and the human examiners. These results are promising for further research on end-to-end automatic scoring of descriptive answers.



### Swin Transformer for Fast MRI
- **Arxiv ID**: http://arxiv.org/abs/2201.03230v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03230v2)
- **Published**: 2022-01-10 09:32:32+00:00
- **Updated**: 2022-04-10 14:58:13+00:00
- **Authors**: Jiahao Huang, Yingying Fang, Yinzhe Wu, Huanjun Wu, Zhifan Gao, Yang Li, Javier Del Ser, Jun Xia, Guang Yang
- **Comment**: 55 pages, 19 figures, submitted to Neurocomputing journal
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) is an important non-invasive clinical tool that can produce high-resolution and reproducible images. However, a long scanning time is required for high-quality MR images, which leads to exhaustion and discomfort of patients, inducing more artefacts due to voluntary movements of the patients and involuntary physiological movements. To accelerate the scanning process, methods by k-space undersampling and deep learning based reconstruction have been popularised. This work introduced SwinMR, a novel Swin transformer based method for fast MRI reconstruction. The whole network consisted of an input module (IM), a feature extraction module (FEM) and an output module (OM). The IM and OM were 2D convolutional layers and the FEM was composed of a cascaded of residual Swin transformer blocks (RSTBs) and 2D convolutional layers. The RSTB consisted of a series of Swin transformer layers (STLs). The shifted windows multi-head self-attention (W-MSA/SW-MSA) of STL was performed in shifted windows rather than the multi-head self-attention (MSA) of the original transformer in the whole image space. A novel multi-channel loss was proposed by using the sensitivity maps, which was proved to reserve more textures and details. We performed a series of comparative studies and ablation studies in the Calgary-Campinas public brain MR dataset and conducted a downstream segmentation experiment in the Multi-modal Brain Tumour Segmentation Challenge 2017 dataset. The results demonstrate our SwinMR achieved high-quality reconstruction compared with other benchmark methods, and it shows great robustness with different undersampling masks, under noise interruption and on different datasets. The code is publicly available at https://github.com/ayanglab/SwinMR.



### Small Object Detection using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.03243v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03243v1)
- **Published**: 2022-01-10 09:58:25+00:00
- **Updated**: 2022-01-10 09:58:25+00:00
- **Authors**: Aleena Ajaz, Ayesha Salar, Tauseef Jamal, Asif Ullah Khan
- **Comment**: 21 pages
- **Journal**: None
- **Summary**: Now a days, UAVs such as drones are greatly used for various purposes like that of capturing and target detection from ariel imagery etc. Easy access of these small ariel vehicles to public can cause serious security threats. For instance, critical places may be monitored by spies blended in public using drones. Study in hand proposes an improved and efficient Deep Learning based autonomous system which can detect and track very small drones with great precision. The proposed system consists of a custom deep learning model Tiny YOLOv3, one of the flavors of very fast object detection model You Look Only Once (YOLO) is built and used for detection. The object detection algorithm will efficiently the detect the drones. The proposed architecture has shown significantly better performance as compared to the previous YOLO version. The improvement is observed in the terms of resource usage and time complexity. The performance is measured using the metrics of recall and precision that are 93% and 91% respectively.



### Vision in adverse weather: Augmentation using CycleGANs with various object detectors for robust perception in autonomous racing
- **Arxiv ID**: http://arxiv.org/abs/2201.03246v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03246v3)
- **Published**: 2022-01-10 10:02:40+00:00
- **Updated**: 2023-01-02 16:35:26+00:00
- **Authors**: Izzeddin Teeti, Valentina Musat, Salman Khan, Alexander Rast, Fabio Cuzzolin, Andrew Bradley
- **Comment**: ICML 2022
- **Journal**: None
- **Summary**: In an autonomous driving system, perception - identification of features and objects from the environment - is crucial. In autonomous racing, high speeds and small margins demand rapid and accurate detection systems. During the race, the weather can change abruptly, causing significant degradation in perception, resulting in ineffective manoeuvres. In order to improve detection in adverse weather, deep-learning-based models typically require extensive datasets captured in such conditions - the collection of which is a tedious, laborious, and costly process. However, recent developments in CycleGAN architectures allow the synthesis of highly realistic scenes in multiple weather conditions. To this end, we introduce an approach of using synthesised adverse condition datasets in autonomous racing (generated using CycleGAN) to improve the performance of four out of five state-of-the-art detectors by an average of 42.7 and 4.4 mAP percentage points in the presence of night-time conditions and droplets, respectively. Furthermore, we present a comparative analysis of five object detectors - identifying the optimal pairing of detector and training data for use during autonomous racing in challenging conditions.



### A statistical shape model for radiation-free assessment and classification of craniosynostosis
- **Arxiv ID**: http://arxiv.org/abs/2201.03288v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.03288v2)
- **Published**: 2022-01-10 11:10:54+00:00
- **Updated**: 2022-03-28 13:36:31+00:00
- **Authors**: Matthias Schaufelberger, Reinald Peter Kühle, Andreas Wachter, Frederic Weichel, Niclas Hagen, Friedemann Ringwald, Urs Eisenmann, Jürgen Hoffmann, Michael Engel, Christian Freudlsperger, Werner Nahm
- **Comment**: None
- **Journal**: None
- **Summary**: The assessment of craniofacial deformities requires patient data which is sparsely available. Statistical shape models provide realistic and synthetic data enabling comparisons of existing methods on a common dataset.   We build the first publicly available statistical 3D head model of craniosynostosis patients and the first model focusing on infants younger than 1.5 years. We further present a shape-model-based classification pipeline to distinguish between three different classes of craniosynostosis and a control group on photogrammetric surface scans. To the best of our knowledge, our study uses the largest dataset of craniosynostosis patients in a classification study for craniosynostosis and statistical shape modeling to date.   We demonstrate that our shape model performs similar to other statistical shape models of the human head. Craniosynostosis-specific pathologies are represented in the first eigenmodes of the model. Regarding the automatic classification of craniosynostis, our classification approach yields an accuracy of 97.8%, comparable to other state-of-the-art methods using both computed tomography scans and stereophotogrammetry.   Our publicly available, craniosynostosis-specific statistical shape model enables the assessment of craniosynostosis on realistic and synthetic data. We further present a state-of-the-art shape-model-based classification approach for a radiation-free diagnosis of craniosynostosis.



### GhostNets on Heterogeneous Devices via Cheap Operations
- **Arxiv ID**: http://arxiv.org/abs/2201.03297v1
- **DOI**: 10.1007/s11263-022-01575-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03297v1)
- **Published**: 2022-01-10 11:46:38+00:00
- **Updated**: 2022-01-10 11:46:38+00:00
- **Authors**: Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chunjing Xu, Enhua Wu, Qi Tian
- **Comment**: Accepted by IJCV 2022. Extension of GhostNet CVPR2020 paper
  (arXiv:1911.11907). arXiv admin note: substantial text overlap with
  arXiv:1911.11907
- **Journal**: None
- **Summary**: Deploying convolutional neural networks (CNNs) on mobile devices is difficult due to the limited memory and computation resources. We aim to design efficient neural networks for heterogeneous devices including CPU and GPU, by exploiting the redundancy in feature maps, which has rarely been investigated in neural architecture design. For CPU-like devices, we propose a novel CPU-efficient Ghost (C-Ghost) module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed C-Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. C-Ghost bottlenecks are designed to stack C-Ghost modules, and then the lightweight C-GhostNet can be easily established. We further consider the efficient networks for GPU devices. Without involving too many GPU-inefficient operations (e.g.,, depth-wise convolution) in a building stage, we propose to utilize the stage-wise feature redundancy to formulate GPU-efficient Ghost (G-Ghost) stage structure. The features in a stage are split into two parts where the first part is processed using the original block with fewer output channels for generating intrinsic features, and the other are generated using cheap operations by exploiting stage-wise redundancy. Experiments conducted on benchmarks demonstrate the effectiveness of the proposed C-Ghost module and the G-Ghost stage. C-GhostNet and G-GhostNet can achieve the optimal trade-off of accuracy and latency for CPU and GPU, respectively. Code is available at https://github.com/huawei-noah/CV-Backbones.



### Avoiding Overfitting: A Survey on Regularization Methods for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.03299v1
- **DOI**: 10.1145/3510413
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03299v1)
- **Published**: 2022-01-10 11:54:06+00:00
- **Updated**: 2022-01-10 11:54:06+00:00
- **Authors**: Claudio Filipi Gonçalves dos Santos, João Paulo Papa
- **Comment**: 27 pages
- **Journal**: None
- **Summary**: Several image processing tasks, such as image classification and object detection, have been significantly improved using Convolutional Neural Networks (CNN). Like ResNet and EfficientNet, many architectures have achieved outstanding results in at least one dataset by the time of their creation. A critical factor in training concerns the network's regularization, which prevents the structure from overfitting. This work analyzes several regularization methods developed in the last few years, showing significant improvements for different CNN models. The works are classified into three main areas: the first one is called "data augmentation", where all the techniques focus on performing changes in the input data. The second, named "internal changes", which aims to describe procedures to modify the feature maps generated by the neural network or the kernels. The last one, called "label", concerns transforming the labels of a given input. This work presents two main differences comparing to other available surveys about regularization: (i) the first concerns the papers gathered in the manuscript, which are not older than five years, and (ii) the second distinction is about reproducibility, i.e., all works refered here have their code available in public repositories or they have been directly implemented in some framework, such as TensorFlow or Torch.



### Comparison of Representation Learning Techniques for Tracking in time resolved 3D Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2201.03319v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03319v1)
- **Published**: 2022-01-10 12:38:22+00:00
- **Updated**: 2022-01-10 12:38:22+00:00
- **Authors**: Daniel Wulff, Jannis Hagenah, Floris Ernst
- **Comment**: Presented at Medical Imaging with Deep Learning (MIDL) 2021
- **Journal**: None
- **Summary**: 3D ultrasound (3DUS) becomes more interesting for target tracking in radiation therapy due to its capability to provide volumetric images in real-time without using ionizing radiation. It is potentially usable for tracking without using fiducials. For this, a method for learning meaningful representations would be useful to recognize anatomical structures in different time frames in representation space (r-space). In this study, 3DUS patches are reduced into a 128-dimensional r-space using conventional autoencoder, variational autoencoder and sliced-wasserstein autoencoder. In the r-space, the capability of separating different ultrasound patches as well as recognizing similar patches is investigated and compared based on a dataset of liver images. Two metrics to evaluate the tracking capability in the r-space are proposed. It is shown that ultrasound patches with different anatomical structures can be distinguished and sets of similar patches can be clustered in r-space. The results indicate that the investigated autoencoders have different levels of usability for target tracking in 3DUS.



### Gait Recognition Based on Deep Learning: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2201.03323v1
- **DOI**: 10.1145/3490235
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03323v1)
- **Published**: 2022-01-10 12:44:42+00:00
- **Updated**: 2022-01-10 12:44:42+00:00
- **Authors**: Claudio Filipi Gonçalves dos Santos, Diego de Souza Oliveira, Leandro A. Passos, Rafael Gonçalves Pires, Daniel Felipe Silva Santos, Lucas Pascotti Valem, Thierry P. Moreira, Marcos Cleison S. Santana, Mateus Roder, João Paulo Papa, Danilo Colombo
- **Comment**: None
- **Journal**: None
- **Summary**: In general, biometry-based control systems may not rely on individual expected behavior or cooperation to operate appropriately. Instead, such systems should be aware of malicious procedures for unauthorized access attempts. Some works available in the literature suggest addressing the problem through gait recognition approaches. Such methods aim at identifying human beings through intrinsic perceptible features, despite dressed clothes or accessories. Although the issue denotes a relatively long-time challenge, most of the techniques developed to handle the problem present several drawbacks related to feature extraction and low classification rates, among other issues. However, deep learning-based approaches recently emerged as a robust set of tools to deal with virtually any image and computer-vision related problem, providing paramount results for gait recognition as well. Therefore, this work provides a surveyed compilation of recent works regarding biometric detection through gait recognition with a focus on deep learning approaches, emphasizing their benefits, and exposing their weaknesses. Besides, it also presents categorized and characterized descriptions of the datasets, approaches, and architectures employed to tackle associated constraints.



### Evaluation of Neural Networks Defenses and Attacks using NDCG and Reciprocal Rank Metrics
- **Arxiv ID**: http://arxiv.org/abs/2201.05071v1
- **DOI**: 10.1007/s10207-022-00652-0
- **Categories**: **cs.CR**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2201.05071v1)
- **Published**: 2022-01-10 12:54:45+00:00
- **Updated**: 2022-01-10 12:54:45+00:00
- **Authors**: Haya Brama, Lihi Dery, Tal Grinshpoun
- **Comment**: 12 pages, 5 figures
- **Journal**: International Journal of Information Security 2022
- **Summary**: The problem of attacks on neural networks through input modification (i.e., adversarial examples) has attracted much attention recently. Being relatively easy to generate and hard to detect, these attacks pose a security breach that many suggested defenses try to mitigate. However, the evaluation of the effect of attacks and defenses commonly relies on traditional classification metrics, without adequate adaptation to adversarial scenarios. Most of these metrics are accuracy-based, and therefore may have a limited scope and low distinctive power. Other metrics do not consider the unique characteristics of neural networks functionality, or measure the effect of the attacks indirectly (e.g., through the complexity of their generation). In this paper, we present two metrics which are specifically designed to measure the effect of attacks, or the recovery effect of defenses, on the output of neural networks in multiclass classification tasks. Inspired by the normalized discounted cumulative gain and the reciprocal rank metrics used in information retrieval literature, we treat the neural network predictions as ranked lists of results. Using additional information about the probability of the rank enabled us to define novel metrics that are suited to the task at hand. We evaluate our metrics using various attacks and defenses on a pretrained VGG19 model and the ImageNet dataset. Compared to the common classification metrics, our proposed metrics demonstrate superior informativeness and distinctiveness.



### COIN: Counterfactual Image Generation for VQA Interpretation
- **Arxiv ID**: http://arxiv.org/abs/2201.03342v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.03342v1)
- **Published**: 2022-01-10 13:51:35+00:00
- **Updated**: 2022-01-10 13:51:35+00:00
- **Authors**: Zeyd Boukhers, Timo Hartmann, Jan Jürjens
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the significant advancement of Natural Language Processing and Computer Vision-based models, Visual Question Answering (VQA) systems are becoming more intelligent and advanced. However, they are still error-prone when dealing with relatively complex questions. Therefore, it is important to understand the behaviour of the VQA models before adopting their results. In this paper, we introduce an interpretability approach for VQA models by generating counterfactual images. Specifically, the generated image is supposed to have the minimal possible change to the original image and leads the VQA model to give a different answer. In addition, our approach ensures that the generated image is realistic. Since quantitative metrics cannot be employed to evaluate the interpretability of the model, we carried out a user study to assess different aspects of our approach. In addition to interpreting the result of VQA models on single images, the obtained results and the discussion provides an extensive explanation of VQA models' behaviour.



### GMFIM: A Generative Mask-guided Facial Image Manipulation Model for Privacy Preservation
- **Arxiv ID**: http://arxiv.org/abs/2201.03353v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03353v1)
- **Published**: 2022-01-10 14:09:14+00:00
- **Updated**: 2022-01-10 14:09:14+00:00
- **Authors**: Mohammad Hossein Khojaste, Nastaran Moradzadeh Farid, Ahmad Nickabadi
- **Comment**: None
- **Journal**: None
- **Summary**: The use of social media websites and applications has become very popular and people share their photos on these networks. Automatic recognition and tagging of people's photos on these networks has raised privacy preservation issues and users seek methods for hiding their identities from these algorithms. Generative adversarial networks (GANs) are shown to be very powerful in generating face images in high diversity and also in editing face images. In this paper, we propose a Generative Mask-guided Face Image Manipulation (GMFIM) model based on GANs to apply imperceptible editing to the input face image to preserve the privacy of the person in the image. Our model consists of three main components: a) the face mask module to cut the face area out of the input image and omit the background, b) the GAN-based optimization module for manipulating the face image and hiding the identity and, c) the merge module for combining the background of the input image and the manipulated de-identified face image. Different criteria are considered in the loss function of the optimization step to produce high-quality images that are as similar as possible to the input image while they cannot be recognized by AFR systems. The results of the experiments on different datasets show that our model can achieve better performance against automated face recognition systems in comparison to the state-of-the-art methods and it catches a higher attack success rate in most experiments from a total of 18. Moreover, the generated images of our proposed model have the highest quality and are more pleasing to human eyes.



### High-resolution Ecosystem Mapping in Repetitive Environments Using Dual Camera SLAM
- **Arxiv ID**: http://arxiv.org/abs/2201.03364v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.03364v1)
- **Published**: 2022-01-10 14:29:37+00:00
- **Updated**: 2022-01-10 14:29:37+00:00
- **Authors**: Brian M. Hopkinson, Suchendra M. Bhandarkar
- **Comment**: 6 pages plus references, 5 figures
- **Journal**: None
- **Summary**: Structure from Motion (SfM) techniques are being increasingly used to create 3D maps from images in many domains including environmental monitoring. However, SfM techniques are often confounded in visually repetitive environments as they rely primarily on globally distinct image features. Simultaneous Localization and Mapping (SLAM) techniques offer a potential solution in visually repetitive environments since they use local feature matching, but SLAM approaches work best with wide-angle cameras that are often unsuitable for documenting the environmental system of interest. We resolve this issue by proposing a dual-camera SLAM approach that uses a forward facing wide-angle camera for localization and a downward facing narrower angle, high-resolution camera for documentation. Video frames acquired by the forward facing camera video are processed using a standard SLAM approach providing a trajectory of the imaging system through the environment which is then used to guide the registration of the documentation camera images. Fragmentary maps, initially produced from the documentation camera images via monocular SLAM, are subsequently scaled and aligned with the localization camera trajectory and finally subjected to a global optimization procedure to produce a unified, refined map. An experimental comparison with several state-of-the-art SfM approaches shows the dual-camera SLAM approach to perform better in repetitive environmental systems based on select samples of ground control point markers.



### Demonstrating The Risk of Imbalanced Datasets in Chest X-ray Image-based Diagnostics by Prototypical Relevance Propagation
- **Arxiv ID**: http://arxiv.org/abs/2201.03559v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03559v1)
- **Published**: 2022-01-10 14:57:39+00:00
- **Updated**: 2022-01-10 14:57:39+00:00
- **Authors**: Srishti Gautam, Marina M. -C. Höhne, Stine Hansen, Robert Jenssen, Michael Kampffmeyer
- **Comment**: To appear in ISBI 2022
- **Journal**: None
- **Summary**: The recent trend of integrating multi-source Chest X-Ray datasets to improve automated diagnostics raises concerns that models learn to exploit source-specific correlations to improve performance by recognizing the source domain of an image rather than the medical pathology. We hypothesize that this effect is enforced by and leverages label-imbalance across the source domains, i.e, prevalence of a disease corresponding to a source. Therefore, in this work, we perform a thorough study of the effect of label-imbalance in multi-source training for the task of pneumonia detection on the widely used ChestX-ray14 and CheXpert datasets. The results highlight and stress the importance of using more faithful and transparent self-explaining models for automated diagnosis, thus enabling the inherent detection of spurious learning. They further illustrate that this undesirable effect of learning spurious correlations can be reduced considerably when ensuring label-balanced source domain datasets.



### Iterative training of robust k-space interpolation networks for improved image reconstruction with limited scan specific training samples
- **Arxiv ID**: http://arxiv.org/abs/2201.03560v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2201.03560v2)
- **Published**: 2022-01-10 16:14:27+00:00
- **Updated**: 2022-07-14 11:54:22+00:00
- **Authors**: Peter Dawood, Felix Breuer, Paul R. Burd, István Homolya, Johannes Oberberger, Peter M. Jakob, Martin Blaimer
- **Comment**: Submitted to Magnetic Resonance in Medicine
- **Journal**: None
- **Summary**: Purpose: To evaluate an iterative learning approach for enhanced performance of Robust Artificial-neural-networks for K-space Interpolation (RAKI), when only a limited amount of training data (auto-calibration signals, ACS) are available for accelerated standard 2D imaging. Methods: In a first step, the RAKI model was optimized for the case of strongly limited training data amount. In the iterative learning approach (termed iterative RAKI), the optimized RAKI model is initially trained using original and augmented ACS obtained from a linear parallel imaging reconstruction. Subsequently, the RAKI convolution filters are refined iteratively using original and augmented ACS extracted from the previous RAKI reconstruction. Evaluation was carried out on 200 retrospectively undersampled in-vivo datasets from the fastMRI neuro database with different contrast settings. Results: For limited training data (18 and 22 ACS lines for R=4 and R=5, respectively), iterative RAKI outperforms standard RAKI by reducing residual artefacts and yields strong noise suppression when compared to standard parallel imaging, underlined by quantitative reconstruction quality metrics. In combination with a phase constraint, further reconstruction improvements can be achieved. Additionally, iterative RAKI shows better performance than both GRAPPA and RAKI in case of pre-scan calibration with varying contrast between training- and undersampled data. Conclusion: The iterative learning approach with RAKI benefits from standard RAKIs well known noise suppression feature but requires less original training data for the accurate reconstruction of standard 2D images thereby improving net acceleration.



### 3D Face Morphing Attacks: Generation, Vulnerability and Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.03454v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03454v2)
- **Published**: 2022-01-10 16:53:39+00:00
- **Updated**: 2022-06-22 15:18:48+00:00
- **Authors**: Jag Mohan Singh, Raghavendra Ramachandra
- **Comment**: The paper is currently under review at IEEE Transactions on
  Biometrics, Behavior and Identity Science
- **Journal**: None
- **Summary**: Face Recognition systems (FRS) have been found vulnerable to morphing attacks, where the morphed face image is generated by blending the face images from contributory data subjects. This work presents a novel direction towards generating face morphing attacks in 3D. To this extent, we have introduced a novel approach based on blending the 3D face point clouds corresponding to the contributory data subjects. The proposed method will generate the 3D face morphing by projecting the input 3D face point clouds to depth-maps \& 2D color images followed by the image blending and wrapping operations performed independently on the color images and depth maps. We then back-project the 2D morphing color-map and the depth-map to the point cloud using the canonical (fixed) view. Given that the generated 3D face morphing models will result in the holes due to a single canonical view, we have proposed a new algorithm for hole filling that will result in a high-quality 3D face morphing model. Extensive experiments are carried out on the newly generated 3D face dataset comprised of 675 3D scans corresponding to 41 unique data subjects. Experiments are performed to benchmark the vulnerability of automatic 2D and 3D FRS and human observer analysis. We also present the quantitative assessment of the quality of the generated 3D face morphing models using eight different quality metrics. Finally, we have proposed three different 3D face Morphing Attack Detection (3D-MAD) algorithms to benchmark the performance of the 3D MAD algorithms.



### Learning Population-level Shape Statistics and Anatomy Segmentation From Images: A Joint Deep Learning Model
- **Arxiv ID**: http://arxiv.org/abs/2201.03481v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.03481v1)
- **Published**: 2022-01-10 17:24:35+00:00
- **Updated**: 2022-01-10 17:24:35+00:00
- **Authors**: Wenzheng Tao, Riddhish Bhalodia, Shireen Elhabian
- **Comment**: None
- **Journal**: None
- **Summary**: Statistical shape modeling is an essential tool for the quantitative analysis of anatomical populations. Point distribution models (PDMs) represent the anatomical surface via a dense set of correspondences, an intuitive and easy-to-use shape representation for subsequent applications. These correspondences are exhibited in two coordinate spaces: the local coordinates describing the geometrical features of each individual anatomical surface and the world coordinates representing the population-level statistical shape information after removing global alignment differences across samples in the given cohort. We propose a deep-learning-based framework that simultaneously learns these two coordinate spaces directly from the volumetric images. The proposed joint model serves a dual purpose; the world correspondences can directly be used for shape analysis applications, circumventing the heavy pre-processing and segmentation involved in traditional PDM models. Additionally, the local correspondences can be used for anatomy segmentation. We demonstrate the efficacy of this joint model for both shape modeling applications on two datasets and its utility in inferring the anatomical surface.



### Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.03529v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.03529v2)
- **Published**: 2022-01-10 18:40:07+00:00
- **Updated**: 2022-07-25 18:58:24+00:00
- **Authors**: Utku Evci, Vincent Dumoulin, Hugo Larochelle, Michael C. Mozer
- **Comment**: presented at ICML 2022 (Oral)
- **Journal**: ICML 2022, Proceedings of the 39th International Conference on
  Machine Learning
- **Summary**: Transfer-learning methods aim to improve performance in a data-scarce target domain using a model pretrained on a data-rich source domain. A cost-efficient strategy, linear probing, involves freezing the source model and training a new classification head for the target domain. This strategy is outperformed by a more costly but state-of-the-art method -- fine-tuning all parameters of the source model to the target domain -- possibly because fine-tuning allows the model to leverage useful information from intermediate layers which is otherwise discarded by the later pretrained layers. We explore the hypothesis that these intermediate layers might be directly exploited. We propose a method, Head-to-Toe probing (Head2Toe), that selects features from all layers of the source model to train a classification head for the target-domain. In evaluations on the VTAB-1k, Head2Toe matches performance obtained with fine-tuning on average while reducing training and storage cost hundred folds or more, but critically, for out-of-distribution transfer, Head2Toe outperforms fine-tuning.



### A ConvNet for the 2020s
- **Arxiv ID**: http://arxiv.org/abs/2201.03545v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03545v2)
- **Published**: 2022-01-10 18:59:10+00:00
- **Updated**: 2022-03-02 15:08:16+00:00
- **Authors**: Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie
- **Comment**: CVPR 2022; Code: https://github.com/facebookresearch/ConvNeXt
- **Journal**: None
- **Summary**: The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.



### Language-driven Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.03546v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03546v2)
- **Published**: 2022-01-10 18:59:10+00:00
- **Updated**: 2022-04-03 03:33:43+00:00
- **Authors**: Boyi Li, Kilian Q. Weinberger, Serge Belongie, Vladlen Koltun, René Ranftl
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: We present LSeg, a novel model for language-driven semantic image segmentation. LSeg uses a text encoder to compute embeddings of descriptive input labels (e.g., "grass" or "building") together with a transformer-based image encoder that computes dense per-pixel embeddings of the input image. The image encoder is trained with a contrastive objective to align pixel embeddings to the text embedding of the corresponding semantic class. The text embeddings provide a flexible label representation in which semantically similar labels map to similar regions in the embedding space (e.g., "cat" and "furry"). This allows LSeg to generalize to previously unseen categories at test time, without retraining or even requiring a single additional training sample. We demonstrate that our approach achieves highly competitive zero-shot performance compared to existing zero- and few-shot semantic segmentation methods, and even matches the accuracy of traditional segmentation algorithms when a fixed label set is provided. Code and demo are available at https://github.com/isl-org/lang-seg.



### Cross-Modality Sub-Image Retrieval using Contrastive Multimodal Image Representations
- **Arxiv ID**: http://arxiv.org/abs/2201.03597v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03597v2)
- **Published**: 2022-01-10 19:04:28+00:00
- **Updated**: 2023-03-20 08:58:19+00:00
- **Authors**: Eva Breznik, Elisabeth Wetzer, Joakim Lindblad, Nataša Sladoje
- **Comment**: None
- **Journal**: None
- **Summary**: In tissue characterization and cancer diagnostics, multimodal imaging has emerged as a powerful technique. Thanks to computational advances, large datasets can be exploited to discover patterns in pathologies and improve diagnosis. However, this requires efficient and scalable image retrieval methods. Cross-modality image retrieval is particularly challenging, since images of similar (or even the same) content captured by different modalities might share few common structures. We propose a new application-independent content-based image retrieval (CBIR) system for reverse (sub-)image search across modalities, which combines deep learning to generate representations (embedding the different modalities in a common space) with classical feature extraction and bag-of-words models for efficient and reliable retrieval. We illustrate its advantages through a replacement study, exploring a number of feature extractors and learned representations, as well as through comparison to recent (cross-modality) CBIR methods. For the task of (sub-)image retrieval on a (publicly available) dataset of brightfield and second harmonic generation microscopy images, the results show that our approach is superior to all tested alternatives. We discuss the shortcomings of the compared methods and observe the importance of equivariance and invariance properties of the learned representations and feature extractors in the CBIR pipeline. Code is available at: \url{https://github.com/MIDA-group/CrossModal_ImgRetrieval}.



### Multi-Query Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2201.03639v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03639v2)
- **Published**: 2022-01-10 20:44:46+00:00
- **Updated**: 2022-07-20 18:18:18+00:00
- **Authors**: Zeyu Wang, Yu Wu, Karthik Narasimhan, Olga Russakovsky
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Retrieving target videos based on text descriptions is a task of great practical value and has received increasing attention over the past few years. Despite recent progress, imperfect annotations in existing video retrieval datasets have posed significant challenges on model evaluation and development. In this paper, we tackle this issue by focusing on the less-studied setting of multi-query video retrieval, where multiple descriptions are provided to the model for searching over the video archive. We first show that multi-query retrieval task effectively mitigates the dataset noise introduced by imperfect annotations and better correlates with human judgement on evaluating retrieval abilities of current models. We then investigate several methods which leverage multiple queries at training time, and demonstrate that the multi-query inspired training can lead to superior performance and better generalization. We hope further investigation in this direction can bring new insights on building systems that perform better in real-world video retrieval applications.



### 3D Segmentation with Fully Trainable Gabor Kernels and Pearson's Correlation Coefficient
- **Arxiv ID**: http://arxiv.org/abs/2201.03644v2
- **DOI**: 10.1007/978-3-031-21014-3_6
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.03644v2)
- **Published**: 2022-01-10 20:55:59+00:00
- **Updated**: 2022-12-15 16:24:29+00:00
- **Authors**: Ken C. L. Wong, Mehdi Moradi
- **Comment**: This paper was accepted by the International Workshop on Machine
  Learning in Medical Imaging (MLMI 2022)
- **Journal**: None
- **Summary**: The convolutional layer and loss function are two fundamental components in deep learning. Because of the success of conventional deep learning kernels, the less versatile Gabor kernels become less popular despite the fact that they can provide abundant features at different frequencies, orientations, and scales with much fewer parameters. For existing loss functions for multi-class image segmentation, there is usually a tradeoff among accuracy, robustness to hyperparameters, and manual weight selections for combining different losses. Therefore, to gain the benefits of using Gabor kernels while keeping the advantage of automatic feature generation in deep learning, we propose a fully trainable Gabor-based convolutional layer where all Gabor parameters are trainable through backpropagation. Furthermore, we propose a loss function based on the Pearson's correlation coefficient, which is accurate, robust to learning rates, and does not require manual weight selections. Experiments on 43 3D brain magnetic resonance images with 19 anatomical structures show that, using the proposed loss function with a proper combination of conventional and Gabor-based kernels, we can train a network with only 1.6 million parameters to achieve an average Dice coefficient of 83%. This size is 44 times smaller than the original V-Net which has 71 million parameters. This paper demonstrates the potentials of using learnable parametric kernels in deep learning for 3D segmentation.



### Towards Group Robustness in the presence of Partial Group Labels
- **Arxiv ID**: http://arxiv.org/abs/2201.03668v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2201.03668v1)
- **Published**: 2022-01-10 22:04:48+00:00
- **Updated**: 2022-01-10 22:04:48+00:00
- **Authors**: Vishnu Suresh Lokhande, Kihyuk Sohn, Jinsung Yoon, Madeleine Udell, Chen-Yu Lee, Tomas Pfister
- **Comment**: None
- **Journal**: None
- **Summary**: Learning invariant representations is an important requirement when training machine learning models that are driven by spurious correlations in the datasets. These spurious correlations, between input samples and the target labels, wrongly direct the neural network predictions resulting in poor performance on certain groups, especially the minority groups. Robust training against these spurious correlations requires the knowledge of group membership for every sample. Such a requirement is impractical in situations where the data labeling efforts for minority or rare groups are significantly laborious or where the individuals comprising the dataset choose to conceal sensitive information. On the other hand, the presence of such data collection efforts results in datasets that contain partially labeled group information. Recent works have tackled the fully unsupervised scenario where no labels for groups are available. Thus, we aim to fill the missing gap in the literature by tackling a more realistic setting that can leverage partially available sensitive or group information during training. First, we construct a constraint set and derive a high probability bound for the group assignment to belong to the set. Second, we propose an algorithm that optimizes for the worst-off group assignments from the constraint set. Through experiments on image and tabular datasets, we show improvements in the minority group's performance while preserving overall aggregate accuracy across groups.



### Neuroplastic graph attention networks for nuclei segmentation in histopathology images
- **Arxiv ID**: http://arxiv.org/abs/2201.03669v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2201.03669v1)
- **Published**: 2022-01-10 22:19:14+00:00
- **Updated**: 2022-01-10 22:19:14+00:00
- **Authors**: Yoav Alon, Huiyu Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Modern histopathological image analysis relies on the segmentation of cell structures to derive quantitative metrics required in biomedical research and clinical diagnostics. State-of-the-art deep learning approaches predominantly apply convolutional layers in segmentation and are typically highly customized for a specific experimental configuration; often unable to generalize to unknown data. As the model capacity of classical convolutional layers is limited by a finite set of learned kernels, our approach uses a graph representation of the image and focuses on the node transitions in multiple magnifications. We propose a novel architecture for semantic segmentation of cell nuclei robust to differences in experimental configuration such as staining and variation of cell types. The architecture is comprised of a novel neuroplastic graph attention network based on residual graph attention layers and concurrent optimization of the graph structure representing multiple magnification levels of the histopathological image. The modification of graph structure, which generates the node features by projection, is as important to the architecture as the graph neural network itself. It determines the possible message flow and critical properties to optimize attention, graph structure, and node updates in a balanced magnification loss. In experimental evaluation, our framework outperforms ensembles of state-of-the-art neural networks, with a fraction of the neurons typically required, and sets new standards for the segmentation of new nuclei datasets.



### PrintsGAN: Synthetic Fingerprint Generator
- **Arxiv ID**: http://arxiv.org/abs/2201.03674v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.03674v3)
- **Published**: 2022-01-10 22:25:10+00:00
- **Updated**: 2022-01-20 16:17:50+00:00
- **Authors**: Joshua J. Engelsma, Steven A. Grosz, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: A major impediment to researchers working in the area of fingerprint recognition is the lack of publicly available, large-scale, fingerprint datasets. The publicly available datasets that do exist contain very few identities and impressions per finger. This limits research on a number of topics, including e.g., using deep networks to learn fixed length fingerprint embeddings. Therefore, we propose PrintsGAN, a synthetic fingerprint generator capable of generating unique fingerprints along with multiple impressions for a given fingerprint. Using PrintsGAN, we synthesize a database of 525k fingerprints (35K distinct fingers, each with 15 impressions). Next, we show the utility of the PrintsGAN generated dataset by training a deep network to extract a fixed-length embedding from a fingerprint. In particular, an embedding model trained on our synthetic fingerprints and fine-tuned on a small number of publicly available real fingerprints (25K prints from NIST SD302) obtains a TAR of 87.03% @ FAR=0.01% on the NIST SD4 database (a boost from TAR=73.37% when only trained on NIST SD302). Prevailing synthetic fingerprint generation methods do not enable such performance gains due to i) lack of realism or ii) inability to generate multiple impressions per finger. We plan to release our database of synthetic fingerprints to the public.



### NFANet: A Novel Method for Weakly Supervised Water Extraction from High-Resolution Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/2201.03686v1
- **DOI**: 10.1109/TGRS.2022.3140323
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.03686v1)
- **Published**: 2022-01-10 23:05:48+00:00
- **Updated**: 2022-01-10 23:05:48+00:00
- **Authors**: Ming Lu, Leyuan Fang, Muxing Li, Bob Zhang, Yi Zhang, Pedram Ghamisi
- **Comment**: None
- **Journal**: None
- **Summary**: The use of deep learning for water extraction requires precise pixel-level labels. However, it is very difficult to label high-resolution remote sensing images at the pixel level. Therefore, we study how to utilize point labels to extract water bodies and propose a novel method called the neighbor feature aggregation network (NFANet). Compared with pixellevel labels, point labels are much easier to obtain, but they will lose much information. In this paper, we take advantage of the similarity between the adjacent pixels of a local water-body, and propose a neighbor sampler to resample remote sensing images. Then, the sampled images are sent to the network for feature aggregation. In addition, we use an improved recursive training algorithm to further improve the extraction accuracy, making the water boundary more natural. Furthermore, our method utilizes neighboring features instead of global or local features to learn more representative features. The experimental results show that the proposed NFANet method not only outperforms other studied weakly supervised approaches, but also obtains similar results as the state-of-the-art ones.



