# Arxiv Papers in cs.CV on 2022-01-18
### Unpaired Referring Expression Grounding via Bidirectional Cross-Modal Matching
- **Arxiv ID**: http://arxiv.org/abs/2201.06686v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06686v2)
- **Published**: 2022-01-18 01:13:19+00:00
- **Updated**: 2022-06-05 17:29:28+00:00
- **Authors**: Hengcan Shi, Munawar Hayat, Jianfei Cai
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Referring expression grounding is an important and challenging task in computer vision. To avoid the laborious annotation in conventional referring grounding, unpaired referring grounding is introduced, where the training data only contains a number of images and queries without correspondences. The few existing solutions to unpaired referring grounding are still preliminary, due to the challenges of learning image-text matching and lack of the top-down guidance with unpaired data. In this paper, we propose a novel bidirectional cross-modal matching (BiCM) framework to address these challenges. Particularly, we design a query-aware attention map (QAM) module that introduces top-down perspective via generating query-specific visual attention maps. A cross-modal object matching (COM) module is further introduced, which exploits the recently emerged image-text matching pretrained model, CLIP, to predict the target objects from a bottom-up perspective. The top-down and bottom-up predictions are then integrated via a similarity funsion (SF) module. We also propose a knowledge adaptation matching (KAM) module that leverages unpaired training data to adapt pretrained knowledge to the target dataset and task. Experiments show that our framework outperforms previous works by 6.55% and 9.94% on two popular grounding datasets.



### ProposalCLIP: Unsupervised Open-Category Object Proposal Generation via Exploiting CLIP Cues
- **Arxiv ID**: http://arxiv.org/abs/2201.06696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06696v1)
- **Published**: 2022-01-18 01:51:35+00:00
- **Updated**: 2022-01-18 01:51:35+00:00
- **Authors**: Hengcan Shi, Munawar Hayat, Yicheng Wu, Jianfei Cai
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Object proposal generation is an important and fundamental task in computer vision. In this paper, we propose ProposalCLIP, a method towards unsupervised open-category object proposal generation. Unlike previous works which require a large number of bounding box annotations and/or can only generate proposals for limited object categories, our ProposalCLIP is able to predict proposals for a large variety of object categories without annotations, by exploiting CLIP (contrastive language-image pre-training) cues. Firstly, we analyze CLIP for unsupervised open-category proposal generation and design an objectness score based on our empirical analysis on proposal selection. Secondly, a graph-based merging module is proposed to solve the limitations of CLIP cues and merge fragmented proposals. Finally, we present a proposal regression module that extracts pseudo labels based on CLIP cues and trains a lightweight network to further refine proposals. Extensive experiments on PASCAL VOC, COCO and Visual Genome datasets show that our ProposalCLIP can better generate proposals than previous state-of-the-art methods. Our ProposalCLIP also shows benefits for downstream tasks, such as unsupervised object detection.



### Cross-modal Contrastive Distillation for Instructional Activity Anticipation
- **Arxiv ID**: http://arxiv.org/abs/2201.06734v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06734v1)
- **Published**: 2022-01-18 04:20:33+00:00
- **Updated**: 2022-01-18 04:20:33+00:00
- **Authors**: Zhengyuan Yang, Jingen Liu, Jing Huang, Xiaodong He, Tao Mei, Chenliang Xu, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we aim to predict the plausible future action steps given an observation of the past and study the task of instructional activity anticipation. Unlike previous anticipation tasks that aim at action label prediction, our work targets at generating natural language outputs that provide interpretable and accurate descriptions of future action steps. It is a challenging task due to the lack of semantic information extracted from the instructional videos. To overcome this challenge, we propose a novel knowledge distillation framework to exploit the related external textual knowledge to assist the visual anticipation task. However, previous knowledge distillation techniques generally transfer information within the same modality. To bridge the gap between the visual and text modalities during the distillation process, we devise a novel cross-modal contrastive distillation (CCD) scheme, which facilitates knowledge distillation between teacher and student in heterogeneous modalities with the proposed cross-modal distillation loss. We evaluate our method on the Tasty Videos dataset. CCD improves the anticipation performance of the visual-alone student model by a large margin of 40.2% relatively in BLEU4. Our approach also outperforms the state-of-the-art approaches by a large margin.



### Convolutional Cobweb: A Model of Incremental Learning from 2D Images
- **Arxiv ID**: http://arxiv.org/abs/2201.06740v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.06740v1)
- **Published**: 2022-01-18 04:39:31+00:00
- **Updated**: 2022-01-18 04:39:31+00:00
- **Authors**: Christopher J. MacLellan, Harshil Thakur
- **Comment**: 14 pages, 6 figures, Presented at Advances in Cognitive Systems 2021
- **Journal**: None
- **Summary**: This paper presents a new concept formation approach that supports the ability to incrementally learn and predict labels for visual images. This work integrates the idea of convolutional image processing, from computer vision research, with a concept formation approach that is based on psychological studies of how humans incrementally form and use concepts. We experimentally evaluate this new approach by applying it to an incremental variation of the MNIST digit recognition task. We compare its performance to Cobweb, a concept formation approach that does not support convolutional processing, as well as two convolutional neural networks that vary in the complexity of their convolutional processing. This work represents a first step towards unifying modern computer vision ideas with classical concept formation research.



### DDU-Net: Dual-Decoder-U-Net for Road Extraction Using High-Resolution Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2201.06750v1
- **DOI**: 10.1109/TGRS.2022.3197546
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06750v1)
- **Published**: 2022-01-18 05:27:49+00:00
- **Updated**: 2022-01-18 05:27:49+00:00
- **Authors**: Ying Wang, Yuexing Peng, Xinran Liu, Wei Li, George C. Alexandropoulos, Junchuan Yu, Daqing Ge, Wei Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: Extracting roads from high-resolution remote sensing images (HRSIs) is vital in a wide variety of applications, such as autonomous driving, path planning, and road navigation. Due to the long and thin shape as well as the shades induced by vegetation and buildings, small-sized roads are more difficult to discern. In order to improve the reliability and accuracy of small-sized road extraction when roads of multiple sizes coexist in an HRSI, an enhanced deep neural network model termed Dual-Decoder-U-Net (DDU-Net) is proposed in this paper. Motivated by the U-Net model, a small decoder is added to form a dual-decoder structure for more detailed features. In addition, we introduce the dilated convolution attention module (DCAM) between the encoder and decoders to increase the receptive field as well as to distill multi-scale features through cascading dilated convolution and global average pooling. The convolutional block attention module (CBAM) is also embedded in the parallel dilated convolution and pooling branches to capture more attention-aware features. Extensive experiments are conducted on the Massachusetts Roads dataset with experimental results showing that the proposed model outperforms the state-of-the-art DenseUNet, DeepLabv3+ and D-LinkNet by 6.5%, 3.3%, and 2.1% in the mean Intersection over Union (mIoU), and by 4%, 4.8%, and 3.1% in the F1 score, respectively. Both ablation and heatmap analyses are presented to validate the effectiveness of the proposed model.



### Deformable One-Dimensional Object Detection for Routing and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2201.06775v1
- **DOI**: 10.1109/LRA.2022.3146920
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.06775v1)
- **Published**: 2022-01-18 07:19:17+00:00
- **Updated**: 2022-01-18 07:19:17+00:00
- **Authors**: Azarakhsh Keipour, Maryam Bandari, Stefan Schaal
- **Comment**: Accepted to IEEE Robotics and Automation Letters, January 2022. 8
  pages
- **Journal**: None
- **Summary**: Many methods exist to model and track deformable one-dimensional objects (e.g., cables, ropes, and threads) across a stream of video frames. However, these methods depend on the existence of some initial conditions. To the best of our knowledge, the topic of detection methods that can extract those initial conditions in non-trivial situations has hardly been addressed. The lack of detection methods limits the use of the tracking methods in real-world applications and is a bottleneck for fully autonomous applications that work with these objects.   This paper proposes an approach for detecting deformable one-dimensional objects which can handle crossings and occlusions. It can be used for tasks such as routing and manipulation and automatically provides the initialization required by the tracking methods. Our algorithm takes an image containing a deformable object and outputs a chain of fixed-length cylindrical segments connected with passive spherical joints. The chain follows the natural behavior of the deformable object and fills the gaps and occlusions in the original image. Our tests and experiments have shown that the method can correctly detect deformable one-dimensional objects in various complex conditions.



### Pruning-aware Sparse Regularization for Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/2201.06776v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.06776v1)
- **Published**: 2022-01-18 07:19:23+00:00
- **Updated**: 2022-01-18 07:19:23+00:00
- **Authors**: Nanfei Jiang, Xu Zhao, Chaoyang Zhao, Yongqi An, Ming Tang, Jinqiao Wang
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Structural neural network pruning aims to remove the redundant channels in the deep convolutional neural networks (CNNs) by pruning the filters of less importance to the final output accuracy. To reduce the degradation of performance after pruning, many methods utilize the loss with sparse regularization to produce structured sparsity. In this paper, we analyze these sparsity-training-based methods and find that the regularization of unpruned channels is unnecessary. Moreover, it restricts the network's capacity, which leads to under-fitting. To solve this problem, we propose a novel pruning method, named MaskSparsity, with pruning-aware sparse regularization. MaskSparsity imposes the fine-grained sparse regularization on the specific filters selected by a pruning mask, rather than all the filters of the model. Before the fine-grained sparse regularization of MaskSparity, we can use many methods to get the pruning mask, such as running the global sparse regularization. MaskSparsity achieves 63.03%-FLOPs reduction on ResNet-110 by removing 60.34% of the parameters, with no top-1 accuracy loss on CIFAR-10. On ILSVRC-2012, MaskSparsity reduces more than 51.07% FLOPs on ResNet-50, with only a loss of 0.76% in the top-1 accuracy.   The code is released at https://github.com/CASIA-IVA-Lab/MaskSparsity. Moreover, we have integrated the code of MaskSparity into a PyTorch pruning toolkit, EasyPruner, at https://gitee.com/casia_iva_engineer/easypruner.



### When Facial Expression Recognition Meets Few-Shot Learning: A Joint and Alternate Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/2201.06781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06781v1)
- **Published**: 2022-01-18 07:24:12+00:00
- **Updated**: 2022-01-18 07:24:12+00:00
- **Authors**: Xinyi Zou, Yan Yan, Jing-Hao Xue, Si Chen, Hanzi Wang
- **Comment**: 9 pages, 2 figures
- **Journal**: None
- **Summary**: Human emotions involve basic and compound facial expressions. However, current research on facial expression recognition (FER) mainly focuses on basic expressions, and thus fails to address the diversity of human emotions in practical scenarios. Meanwhile, existing work on compound FER relies heavily on abundant labeled compound expression training data, which are often laboriously collected under the professional instruction of psychology. In this paper, we study compound FER in the cross-domain few-shot learning setting, where only a few images of novel classes from the target domain are required as a reference. In particular, we aim to identify unseen compound expressions with the model trained on easily accessible basic expression datasets. To alleviate the problem of limited base classes in our FER task, we propose a novel Emotion Guided Similarity Network (EGS-Net), consisting of an emotion branch and a similarity branch, based on a two-stage learning framework. Specifically, in the first stage, the similarity branch is jointly trained with the emotion branch in a multi-task fashion. With the regularization of the emotion branch, we prevent the similarity branch from overfitting to sampled base classes that are highly overlapped across different episodes. In the second stage, the emotion branch and the similarity branch play a "two-student game" to alternately learn from each other, thereby further improving the inference ability of the similarity branch on unseen compound expressions. Experimental results on both in-the-lab and in-the-wild compound expression datasets demonstrate the superiority of our proposed method against several state-of-the-art methods.



### Resistance Training using Prior Bias: toward Unbiased Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2201.06794v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06794v2)
- **Published**: 2022-01-18 07:48:55+00:00
- **Updated**: 2022-04-21 02:53:16+00:00
- **Authors**: Chao Chen, Yibing Zhan, Baosheng Yu, Liu Liu, Yong Luo, Bo Du
- **Comment**: Accepted by AAAI 2022
- **Journal**: None
- **Summary**: Scene Graph Generation (SGG) aims to build a structured representation of a scene using objects and pairwise relationships, which benefits downstream tasks. However, current SGG methods usually suffer from sub-optimal scene graph generation because of the long-tailed distribution of training data. To address this problem, we propose Resistance Training using Prior Bias (RTPB) for the scene graph generation. Specifically, RTPB uses a distributed-based prior bias to improve models' detecting ability on less frequent relationships during training, thus improving the model generalizability on tail categories. In addition, to further explore the contextual information of objects and relationships, we design a contextual encoding backbone network, termed as Dual Transformer (DTrans). We perform extensive experiments on a very popular benchmark, VG150, to demonstrate the effectiveness of our method for the unbiased scene graph generation. In specific, our RTPB achieves an improvement of over 10% under the mean recall when applied to current SGG methods. Furthermore, DTrans with RTPB outperforms nearly all state-of-the-art methods with a large margin.



### Pistol: Pupil Invisible Supportive Tool to extract Pupil, Iris, Eye Opening, Eye Movements, Pupil and Iris Gaze Vector, and 2D as well as 3D Gaze
- **Arxiv ID**: http://arxiv.org/abs/2201.06799v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.06799v2)
- **Published**: 2022-01-18 07:54:55+00:00
- **Updated**: 2023-03-10 08:45:40+00:00
- **Authors**: Wolfgang Fuhl, Daniel Weber, Shahram Eivazi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes a feature extraction and gaze estimation software, named \textit{Pistol} that can be used with Pupil Invisible projects and other eye trackers in the future. In offline mode, our software extracts multiple features from the eye including, the pupil and iris ellipse, eye aperture, pupil vector, iris vector, eye movement types from pupil and iris velocities, marker detection, marker distance, 2D gaze estimation for the pupil center, iris center, pupil vector, and iris vector using Levenberg Marquart fitting and neural networks. The gaze signal is computed in 2D for each eye and each feature separately and for both eyes in 3D also for each feature separately. We hope this software helps other researchers to extract state-of-the-art features for their research out of their recordings.   Link: https://es-cloud.cs.uni-tuebingen.de/d/8e2ab8c3fdd444e1a135/?p=%2FPISTOL&mode=list



### Adaptive Weighted Guided Image Filtering for Depth Enhancement in Shape-From-Focus
- **Arxiv ID**: http://arxiv.org/abs/2201.06823v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06823v1)
- **Published**: 2022-01-18 08:52:26+00:00
- **Updated**: 2022-01-18 08:52:26+00:00
- **Authors**: Yuwen Li, Zhengguo Li, Chaobing Zheng, Shiqian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Existing shape from focus (SFF) techniques cannot preserve depth edges and fine structural details from a sequence of multi-focus images. Moreover, noise in the sequence of multi-focus images affects the accuracy of the depth map. In this paper, a novel depth enhancement algorithm for the SFF based on an adaptive weighted guided image filtering (AWGIF) is proposed to address the above issues. The AWGIF is applied to decompose an initial depth map which is estimated by the traditional SFF into a base layer and a detail layer. In order to preserve the edges accurately in the refined depth map, the guidance image is constructed from the multi-focus image sequence, and the coefficient of the AWGIF is utilized to suppress the noise while enhancing the fine depth details. Experiments on real and synthetic objects demonstrate the superiority of the proposed algorithm in terms of anti-noise, and the ability to preserve depth edges and fine structural details compared to existing methods.



### STURE: Spatial-Temporal Mutual Representation Learning for Robust Data Association in Online Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2201.06824v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06824v3)
- **Published**: 2022-01-18 08:52:40+00:00
- **Updated**: 2022-03-28 03:08:28+00:00
- **Authors**: Haidong Wang, Zhiyong Li, Yaping Li, Ke Nai, Ming Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Online multi-object tracking (MOT) is a longstanding task for computer vision and intelligent vehicle platform. At present, the main paradigm is tracking-by-detection, and the main difficulty of this paradigm is how to associate current candidate detections with historical tracklets. However, in the MOT scenarios, each historical tracklet is composed of an object sequence, while each candidate detection is just a flat image, which lacks temporal features of the object sequence. The feature difference between current candidate detections and historical tracklets makes the object association much harder. Therefore, we propose a Spatial-Temporal Mutual Representation Learning (STURE) approach which learns spatial-temporal representations between current candidate detections and historical sequences in a mutual representation space. For historical trackelets, the detection learning network is forced to match the representations of sequence learning network in a mutual representation space. The proposed approach is capable of extracting more distinguishing detection and sequence representations by using various designed losses in object association. As a result, spatial-temporal feature is learned mutually to reinforce the current detection features, and the feature difference can be relieved. To prove the robustness of the STURE, it is applied to the public MOT challenge benchmarks and performs well compared with various state-of-the-art online MOT trackers based on identity-preserving metrics.



### Deep Learning Based Framework for Iranian License Plate Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.06825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06825v1)
- **Published**: 2022-01-18 08:53:42+00:00
- **Updated**: 2022-01-18 08:53:42+00:00
- **Authors**: Mojtaba Shahidi Zandi, Roozbeh Rajabi
- **Comment**: 20 pages, journal
- **Journal**: None
- **Summary**: License plate recognition systems have a very important role in many applications such as toll management, parking control, and traffic management. In this paper, a framework of deep convolutional neural networks is proposed for Iranian license plate recognition. The first CNN is the YOLOv3 network that detects the Iranian license plate in the input image while the second CNN is a Faster R-CNN that recognizes and classifies the characters in the detected license plate. A dataset of Iranian license plates consisting of ill-conditioned images also developed in this paper. The YOLOv3 network achieved 99.6% mAP, 98.26% recall, 98.08% accuracy, and average detection speed is only 23ms. Also, the Faster R-CNN network trained and tested on the developed dataset and achieved 98.97% recall, 99.9% precision, and 98.8% accuracy. The proposed system can recognize the license plate in challenging situations like unwanted data on the license plate. Comparing this system with other Iranian license plate recognition systems shows that it is Faster, more accurate and also this system can work in an open environment.



### Taylor3DNet: Fast 3D Shape Inference With Landmark Points Based Taylor Series
- **Arxiv ID**: http://arxiv.org/abs/2201.06845v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06845v2)
- **Published**: 2022-01-18 09:47:40+00:00
- **Updated**: 2023-07-16 09:28:11+00:00
- **Authors**: Yuting Xiao, Jiale Xu, Shenghua Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Benefiting from the continuous representation ability, deep implicit functions can represent a shape at infinite resolution. However, extracting high-resolution iso-surface from an implicit function requires forward-propagating a network with a large number of parameters for numerous query points, thus preventing the generation speed. Inspired by the Taylor series, we propose Taylo3DNet to accelerate the inference of implicit shape representations. Taylor3DNet exploits a set of discrete landmark points and their corresponding Taylor series coefficients to represent the implicit field of a 3D shape, and the number of landmark points is independent of the resolution of the iso-surface extraction. Once the coefficients corresponding to the landmark points are predicted, the network evaluation for each query point can be simplified as a low-order Taylor series calculation with several nearest landmark points. Based on this efficient representation, our Taylor3DNet achieves a significantly faster inference speed than classical network-based implicit functions. We evaluate our approach on reconstruction tasks with various input types, and the results demonstrate that our approach can improve the inference speed by a large margin without sacrificing the performance compared with state-of-the-art baselines.



### RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2201.06857v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.06857v2)
- **Published**: 2022-01-18 10:24:58+00:00
- **Updated**: 2022-01-19 03:26:39+00:00
- **Authors**: Luya Wang, Feng Liang, Yangguang Li, Honggang Zhang, Wanli Ouyang, Jing Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, self-supervised vision transformers have attracted unprecedented attention for their impressive representation learning ability. However, the dominant method, contrastive learning, mainly relies on an instance discrimination pretext task, which learns a global understanding of the image. This paper incorporates local feature learning into self-supervised vision transformers via Reconstructive Pre-training (RePre). Our RePre extends contrastive frameworks by adding a branch for reconstructing raw image pixels in parallel with the existing contrastive objective. RePre is equipped with a lightweight convolution-based decoder that fuses the multi-hierarchy features from the transformer encoder. The multi-hierarchy features provide rich supervisions from low to high semantic information, which are crucial for our RePre. Our RePre brings decent improvements on various contrastive frameworks with different vision transformer architectures. Transfer performance in downstream tasks outperforms supervised pre-training and state-of-the-art (SOTA) self-supervised counterparts.



### Autoencoding Video Latents for Adversarial Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2201.06888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06888v1)
- **Published**: 2022-01-18 11:42:14+00:00
- **Updated**: 2022-01-18 11:42:14+00:00
- **Authors**: Sai Hemanth Kasaraneni
- **Comment**: preprint
- **Journal**: None
- **Summary**: Given the three dimensional complexity of a video signal, training a robust and diverse GAN based video generative model is onerous due to large stochasticity involved in data space. Learning disentangled representations of the data help to improve robustness and provide control in the sampling process. For video generation, there is a recent progress in this area by considering motion and appearance as orthogonal information and designing architectures that efficiently disentangle them. These approaches rely on handcrafting architectures that impose structural priors on the generator to decompose appearance and motion codes in the latent space. Inspired from the recent advancements in the autoencoder based image generation, we present AVLAE (Adversarial Video Latent AutoEncoder) which is a two stream latent autoencoder where the video distribution is learned by adversarial training. In particular, we propose to autoencode the motion and appearance latent vectors of the video generator in the adversarial setting. We demonstrate that our approach learns to disentangle motion and appearance codes even without the explicit structural composition in the generator. Several experiments with qualitative and quantitative results demonstrate the effectiveness of our method.



### Boosting Robustness of Image Matting with Context Assembling and Strong Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.06889v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06889v1)
- **Published**: 2022-01-18 11:45:17+00:00
- **Updated**: 2022-01-18 11:45:17+00:00
- **Authors**: Yutong Dai, Brian Price, He Zhang, Chunhua Shen
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: Deep image matting methods have achieved increasingly better results on benchmarks (e.g., Composition-1k/alphamatting.com). However, the robustness, including robustness to trimaps and generalization to images from different domains, is still under-explored. Although some works propose to either refine the trimaps or adapt the algorithms to real-world images via extra data augmentation, none of them has taken both into consideration, not to mention the significant performance deterioration on benchmarks while using those data augmentation. To fill this gap, we propose an image matting method which achieves higher robustness (RMat) via multilevel context assembling and strong data augmentation targeting matting. Specifically, we first build a strong matting framework by modeling ample global information with transformer blocks in the encoder, and focusing on details in combination with convolution layers as well as a low-level feature assembling attention block in the decoder. Then, based on this strong baseline, we analyze current data augmentation and explore simple but effective strong data augmentation to boost the baseline model and contribute a more generalizable matting method. Compared with previous methods, the proposed method not only achieves state-of-the-art results on the Composition-1k benchmark (11% improvement on SAD and 27% improvement on Grad) with smaller model size, but also shows more robust generalization results on other benchmarks, on real-world images, and also on varying coarse-to-fine trimaps with our extensive experiments.



### AI-based Carcinoma Detection and Classification Using Histopathological Images: A Systematic Review
- **Arxiv ID**: http://arxiv.org/abs/2201.07231v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2201.07231v1)
- **Published**: 2022-01-18 12:03:09+00:00
- **Updated**: 2022-01-18 12:03:09+00:00
- **Authors**: Swathi Prabhua, Keerthana Prasada, Antonio Robels-Kelly, Xuequan Lu
- **Comment**: accepted to Computers in Biology and Medicine
- **Journal**: None
- **Summary**: Histopathological image analysis is the gold standard to diagnose cancer. Carcinoma is a subtype of cancer that constitutes more than 80% of all cancer cases. Squamous cell carcinoma and adenocarcinoma are two major subtypes of carcinoma, diagnosed by microscopic study of biopsy slides. However, manual microscopic evaluation is a subjective and time-consuming process. Many researchers have reported methods to automate carcinoma detection and classification. The increasing use of artificial intelligence (AI) in the automation of carcinoma diagnosis also reveals a significant rise in the use of deep network models. In this systematic literature review, we present a comprehensive review of the state-of-the-art approaches reported in carcinoma diagnosis using histopathological images. Studies are selected from well-known databases with strict inclusion/exclusion criteria. We have categorized the articles and recapitulated their methods based on specific organs of carcinoma origin. Further, we have summarized pertinent literature on AI methods, highlighted critical challenges and limitations, and provided insights on future research direction in automated carcinoma diagnosis. Out of 101 articles selected, most of the studies experimented on private datasets with varied image sizes, obtaining accuracy between 63% and 100%. Overall, this review highlights the need for a generalized AI-based carcinoma diagnostic system. Additionally, it is desirable to have accountable approaches to extract microscopic features from images of multiple magnifications that should mimic pathologists' evaluations.



### Deep Equilibrium Models for Video Snapshot Compressive Imaging
- **Arxiv ID**: http://arxiv.org/abs/2201.06931v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.06931v4)
- **Published**: 2022-01-18 12:49:59+00:00
- **Updated**: 2023-02-28 13:09:19+00:00
- **Authors**: Yaping Zhao, Siming Zheng, Xin Yuan
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: The ability of snapshot compressive imaging (SCI) systems to efficiently capture high-dimensional (HD) data has led to an inverse problem, which consists of recovering the HD signal from the compressed and noisy measurement. While reconstruction algorithms grow fast to solve it with the recent advances of deep learning, the fundamental issue of accurate and stable recovery remains. To this end, we propose deep equilibrium models (DEQ) for video SCI, fusing data-driven regularization and stable convergence in a theoretically sound manner. Each equilibrium model implicitly learns a nonexpansive operator and analytically computes the fixed point, thus enabling unlimited iterative steps and infinite network depth with only a constant memory requirement in training and testing. Specifically, we demonstrate how DEQ can be applied to two existing models for video SCI reconstruction: recurrent neural networks (RNN) and Plug-and-Play (PnP) algorithms. On a variety of datasets and real data, both quantitative and qualitative evaluations of our results demonstrate the effectiveness and stability of our proposed method. The code and models are available at: https://github.com/IndigoPurple/DEQSCI .



### Context-Aware Scene Prediction Network (CASPNet)
- **Arxiv ID**: http://arxiv.org/abs/2201.06933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06933v1)
- **Published**: 2022-01-18 12:52:01+00:00
- **Updated**: 2022-01-18 12:52:01+00:00
- **Authors**: Maximilian Schäfer, Kun Zhao, Markus Bühren, Anton Kummert
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Predicting the future motion of surrounding road users is a crucial and challenging task for autonomous driving (AD) and various advanced driver-assistance systems (ADAS). Planning a safe future trajectory heavily depends on understanding the traffic scene and anticipating its dynamics. The challenges do not only lie in understanding the complex driving scenarios but also the numerous possible interactions among road users and environments, which are practically not feasible for explicit modeling. In this work, we tackle the above challenges by jointly learning and predicting the motion of all road users in a scene, using a novel convolutional neural network (CNN) and recurrent neural network (RNN) based architecture. Moreover, by exploiting grid-based input and output data structures, the computational cost is independent of the number of road users and multi-modal predictions become inherent properties of our proposed method. Evaluation on the nuScenes dataset shows that our approach reaches state-of-the-art results in the prediction benchmark.



### It's All in the Head: Representation Knowledge Distillation through Classifier Sharing
- **Arxiv ID**: http://arxiv.org/abs/2201.06945v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.06945v2)
- **Published**: 2022-01-18 13:10:36+00:00
- **Updated**: 2022-04-05 14:39:02+00:00
- **Authors**: Emanuel Ben-Baruch, Matan Karklinsky, Yossi Biton, Avi Ben-Cohen, Hussam Lawen, Nadav Zamir
- **Comment**: None
- **Journal**: None
- **Summary**: Representation knowledge distillation aims at transferring rich information from one model to another. Common approaches for representation distillation mainly focus on the direct minimization of distance metrics between the models' embedding vectors. Such direct methods may be limited in transferring high-order dependencies embedded in the representation vectors, or in handling the capacity gap between the teacher and student models. Moreover, in standard knowledge distillation, the teacher is trained without awareness of the student's characteristics and capacity. In this paper, we explore two mechanisms for enhancing representation distillation using classifier sharing between the teacher and student. We first investigate a simple scheme where the teacher's classifier is connected to the student backbone, acting as an additional classification head. Then, we propose a student-aware mechanism that asks to tailor the teacher model to a student with limited capacity by training the teacher with a temporary student's head. We analyze and compare these two mechanisms and show their effectiveness on various datasets and tasks, including image classification, fine-grained classification, and face verification. In particular, we achieve state-of-the-art results for face verification on the IJB-C dataset for a MobileFaceNet model: TAR@(FAR=1e-5)=93.7\%. Code is available at https://github.com/Alibaba-MIIL/HeadSharingKD.



### Continual Coarse-to-Fine Domain Adaptation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.06974v1
- **DOI**: 10.1016/j.imavis.2022.104426
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.06974v1)
- **Published**: 2022-01-18 13:31:19+00:00
- **Updated**: 2022-01-18 13:31:19+00:00
- **Authors**: Donald Shenaj, Francesco Barbato, Umberto Michieli, Pietro Zanuttigh
- **Comment**: 24 pages, 9 figures, 6 tables, under submission
- **Journal**: None
- **Summary**: Deep neural networks are typically trained in a single shot for a specific task and data distribution, but in real world settings both the task and the domain of application can change. The problem becomes even more challenging in dense predictive tasks, such as semantic segmentation, and furthermore most approaches tackle the two problems separately. In this paper we introduce the novel task of coarse-to-fine learning of semantic segmentation architectures in presence of domain shift. We consider subsequent learning stages progressively refining the task at the semantic level; i.e., the finer set of semantic labels at each learning step is hierarchically derived from the coarser set of the previous step. We propose a new approach (CCDA) to tackle this scenario. First, we employ the maximum squares loss to align source and target domains and, at the same time, to balance the gradients between well-classified and harder samples. Second, we introduce a novel coarse-to-fine knowledge distillation constraint to transfer network capabilities acquired on a coarser set of labels to a set of finer labels. Finally, we design a coarse-to-fine weight initialization rule to spread the importance from each coarse class to the respective finer classes. To evaluate our approach, we design two benchmarks where source knowledge is extracted from the GTA5 dataset and it is transferred to either the Cityscapes or the IDD datasets, and we show how it outperforms the main competitors.



### ASOCEM: Automatic Segmentation Of Contaminations in cryo-EM
- **Arxiv ID**: http://arxiv.org/abs/2201.06978v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2201.06978v1)
- **Published**: 2022-01-18 13:42:22+00:00
- **Updated**: 2022-01-18 13:42:22+00:00
- **Authors**: Amitay Eldar, Ido Amos, Yoel Shkolnisky
- **Comment**: None
- **Journal**: None
- **Summary**: Particle picking is currently a critical step in the cryo-electron microscopy single particle reconstruction pipeline. Contaminations in the acquired micrographs severely degrade the performance of particle pickers, resulting is many ``non-particles'' in the collected stack of particles. In this paper, we present ASOCEM (Automatic Segmentation Of Contaminations in cryo-EM), an automatic method to detect and segment contaminations, which requires as an input only the approximated particle size. In particular, it does not require any parameter tuning nor manual intervention. Our method is based on the observation that the statistical distribution of contaminated regions is different from that of the rest of the micrograph. This nonrestrictive assumption allows to automatically detect various types of contaminations, from the carbon edges of the supporting grid to high contrast blobs of different sizes. We demonstrate the efficiency of our algorithm using various experimental data sets containing various types of contaminations. ASOCEM is integrated as part of the KLT picker \cite{ELDAR2020107473} and is available at \url{https://github.com/ShkolniskyLab/kltpicker2}.



### MuSCLe: A Multi-Strategy Contrastive Learning Framework for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.07021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07021v1)
- **Published**: 2022-01-18 14:38:50+00:00
- **Updated**: 2022-01-18 14:38:50+00:00
- **Authors**: Kunhao Yuan, Gerald Schaefer, Yu-Kun Lai, Yifan Wang, Xiyao Liu, Lin Guan, Hui Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly supervised semantic segmentation (WSSS) has gained significant popularity since it relies only on weak labels such as image level annotations rather than pixel level annotations required by supervised semantic segmentation (SSS) methods. Despite drastically reduced annotation costs, typical feature representations learned from WSSS are only representative of some salient parts of objects and less reliable compared to SSS due to the weak guidance during training. In this paper, we propose a novel Multi-Strategy Contrastive Learning (MuSCLe) framework to obtain enhanced feature representations and improve WSSS performance by exploiting similarity and dissimilarity of contrastive sample pairs at image, region, pixel and object boundary levels. Extensive experiments demonstrate the effectiveness of our method and show that MuSCLe outperforms the current state-of-the-art on the widely used PASCAL VOC 2012 dataset.



### Joint denoising and HDR for RAW video sequences
- **Arxiv ID**: http://arxiv.org/abs/2201.07066v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.07066v1)
- **Published**: 2022-01-18 15:47:41+00:00
- **Updated**: 2022-01-18 15:47:41+00:00
- **Authors**: A. Buades, O. Martorell, M. Sánchez-Beeckman
- **Comment**: arXiv admin note: text overlap with arXiv:1812.11207
- **Journal**: None
- **Summary**: We propose a patch-based method for the simultaneous denoising and fusion of a sequence of RAW multi-exposed images. A spatio-temporal criterion is used to select similar patches along the sequence, and a weighted principal component analysis permits to both denoise and fuse the multi exposed data. The overall strategy permits to denoise and fuse the set of images without the need of recovering each denoised image in the multi-exposure set, leading to a very efficient procedure. Several experiments show that the proposed method permits to obtain state-of-the-art fusion results with real RAW data.



### Attention-based Proposals Refinement for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.07070v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.07070v3)
- **Published**: 2022-01-18 15:50:31+00:00
- **Updated**: 2022-05-25 17:28:42+00:00
- **Authors**: Minh-Quan Dao, Elwan Héry, Vincent Frémont
- **Comment**: Accepted for IV 2022
- **Journal**: None
- **Summary**: Recent advances in 3D object detection are made by developing the refinement stage for voxel-based Region Proposal Networks (RPN) to better strike the balance between accuracy and efficiency. A popular approach among state-of-the-art frameworks is to divide proposals, or Regions of Interest (ROI), into grids and extract features for each grid location before synthesizing them to form ROI features. While achieving impressive performances, such an approach involves several hand-crafted components (e.g. grid sampling, set abstraction) which requires expert knowledge to be tuned correctly. This paper proposes a data-driven approach to ROI feature computing named APRO3D-Net which consists of a voxel-based RPN and a refinement stage made of Vector Attention. Unlike the original multi-head attention, Vector Attention assigns different weights to different channels within a point feature, thus being able to capture a more sophisticated relation between pooled points and ROI. Our method achieves a competitive performance of 84.85 AP for class Car at moderate difficulty on the validation set of KITTI and 47.03 mAP (average over 10 classes) on NuScenes while having the least parameters compared to closely related methods and attaining an inference speed at 15 FPS on NVIDIA V100 GPU. The code is released at https://github.com/quan-dao/APRO3D-Net.



### Variational Inference for Quantifying Inter-observer Variability in Segmentation of Anatomical Structures
- **Arxiv ID**: http://arxiv.org/abs/2201.07106v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.07106v1)
- **Published**: 2022-01-18 16:33:33+00:00
- **Updated**: 2022-01-18 16:33:33+00:00
- **Authors**: Xiaofeng Liu, Fangxu Xing, Thibault Marin, Georges El Fakhri, Jonghye Woo
- **Comment**: SPIE Medical Imaging 2022 (Oral)
- **Journal**: None
- **Summary**: Lesions or organ boundaries visible through medical imaging data are often ambiguous, thus resulting in significant variations in multi-reader delineations, i.e., the source of aleatoric uncertainty. In particular, quantifying the inter-observer variability of manual annotations with Magnetic Resonance (MR) Imaging data plays a crucial role in establishing a reference standard for various diagnosis and treatment tasks. Most segmentation methods, however, simply model a mapping from an image to its single segmentation map and do not take the disagreement of annotators into consideration. In order to account for inter-observer variability, without sacrificing accuracy, we propose a novel variational inference framework to model the distribution of plausible segmentation maps, given a specific MR image, which explicitly represents the multi-reader variability. Specifically, we resort to a latent vector to encode the multi-reader variability and counteract the inherent information loss in the imaging data. Then, we apply a variational autoencoder network and optimize its evidence lower bound (ELBO) to efficiently approximate the distribution of the segmentation map, given an MR image. Experimental results, carried out with the QUBIQ brain growth MRI segmentation datasets with seven annotators, demonstrate the effectiveness of our approach.



### Contextual road lane and symbol generation for autonomous driving
- **Arxiv ID**: http://arxiv.org/abs/2201.07120v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.07120v1)
- **Published**: 2022-01-18 16:46:46+00:00
- **Updated**: 2022-01-18 16:46:46+00:00
- **Authors**: Ajay Soni, Pratik Padamwar, Krishna Reddy Konda
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a novel approach for lane detection and segmentation using generative models. Traditionally discriminative models have been employed to classify pixels semantically on a road. We model the probability distribution of lanes and road symbols by training a generative adversarial network. Based on the learned probability distribution, context-aware lanes and road signs are generated for a given image which are further quantized for nearest class label. Proposed method has been tested on BDD100K and Baidu ApolloScape datasets and performs better than state of the art and exhibits robustness to adverse conditions by generating lanes in faded out and occluded scenarios.



### Attentional Feature Refinement and Alignment Network for Aircraft Detection in SAR Imagery
- **Arxiv ID**: http://arxiv.org/abs/2201.07124v2
- **DOI**: 10.1109/TGRS.2021.3139994
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07124v2)
- **Published**: 2022-01-18 16:54:49+00:00
- **Updated**: 2022-01-19 04:37:26+00:00
- **Authors**: Yan Zhao, Lingjun Zhao, Zhong Liu, Dewen Hu, Gangyao Kuang, Li Liu
- **Comment**: A raw version as the same as the early access published in TGRS.
  Personal use of this material is permitted. Permission from IEEE must be
  obtained for all other uses
- **Journal**: None
- **Summary**: Aircraft detection in Synthetic Aperture Radar (SAR) imagery is a challenging task in SAR Automatic Target Recognition (SAR ATR) areas due to aircraft's extremely discrete appearance, obvious intraclass variation, small size and serious background's interference. In this paper, a single-shot detector namely Attentional Feature Refinement and Alignment Network (AFRAN) is proposed for detecting aircraft in SAR images with competitive accuracy and speed. Specifically, three significant components including Attention Feature Fusion Module (AFFM), Deformable Lateral Connection Module (DLCM) and Anchor-guided Detection Module (ADM), are carefully designed in our method for refining and aligning informative characteristics of aircraft. To represent characteristics of aircraft with less interference, low-level textural and high-level semantic features of aircraft are fused and refined in AFFM throughly. The alignment between aircraft's discrete back-scatting points and convolutional sampling spots is promoted in DLCM. Eventually, the locations of aircraft are predicted precisely in ADM based on aligned features revised by refined anchors. To evaluate the performance of our method, a self-built SAR aircraft sliced dataset and a large scene SAR image are collected. Extensive quantitative and qualitative experiments with detailed analysis illustrate the effectiveness of the three proposed components. Furthermore, the topmost detection accuracy and competitive speed are achieved by our method compared with other domain-specific,e.g., DAPN, PADN, and general CNN-based methods,e.g., FPN, Cascade R-CNN, SSD, RefineDet and RPDet.



### Leveraging Real Talking Faces via Self-Supervision for Robust Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.07131v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07131v3)
- **Published**: 2022-01-18 17:14:54+00:00
- **Updated**: 2022-10-21 11:36:32+00:00
- **Authors**: Alexandros Haliassos, Rodrigo Mira, Stavros Petridis, Maja Pantic
- **Comment**: CVPR 2022. Code: https://github.com/ahaliassos/RealForensics
- **Journal**: None
- **Summary**: One of the most pressing challenges for the detection of face-manipulated videos is generalising to forgery methods not seen during training while remaining effective under common corruptions such as compression. In this paper, we examine whether we can tackle this issue by harnessing videos of real talking faces, which contain rich information on natural facial appearance and behaviour and are readily available in large quantities online. Our method, termed RealForensics, consists of two stages. First, we exploit the natural correspondence between the visual and auditory modalities in real videos to learn, in a self-supervised cross-modal manner, temporally dense video representations that capture factors such as facial movements, expression, and identity. Second, we use these learned representations as targets to be predicted by our forgery detector along with the usual binary forgery classification task; this encourages it to base its real/fake decision on said factors. We show that our method achieves state-of-the-art performance on cross-manipulation generalisation and robustness experiments, and examine the factors that contribute to its performance. Our results suggest that leveraging natural and unlabelled videos is a promising direction for the development of more robust face forgery detectors.



### MUSE-VAE: Multi-Scale VAE for Environment-Aware Long Term Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2201.07189v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07189v1)
- **Published**: 2022-01-18 18:40:03+00:00
- **Updated**: 2022-01-18 18:40:03+00:00
- **Authors**: Mihee Lee, Samuel S. Sohn, Seonghyeon Moon, Sejong Yoon, Mubbasir Kapadia, Vladimir Pavlovic
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate long-term trajectory prediction in complex scenes, where multiple agents (e.g., pedestrians or vehicles) interact with each other and the environment while attempting to accomplish diverse and often unknown goals, is a challenging stochastic forecasting problem. In this work, we propose MUSE, a new probabilistic modeling framework based on a cascade of Conditional VAEs, which tackles the long-term, uncertain trajectory prediction task using a coarse-to-fine multi-factor forecasting architecture. In its Macro stage, the model learns a joint pixel-space representation of two key factors, the underlying environment and the agent movements, to predict the long and short-term motion goals. Conditioned on them, the Micro stage learns a fine-grained spatio-temporal representation for the prediction of individual agent trajectories. The VAE backbones across the two stages make it possible to naturally account for the joint uncertainty at both levels of granularity. As a result, MUSE offers diverse and simultaneously more accurate predictions compared to the current state-of-the-art. We demonstrate these assertions through a comprehensive set of experiments on nuScenes and SDD benchmarks as well as PFSD, a new synthetic dataset, which challenges the forecasting ability of models on complex agent-environment interaction scenarios.



### Optimizing Active Learning for Low Annotation Budgets
- **Arxiv ID**: http://arxiv.org/abs/2201.07200v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07200v1)
- **Published**: 2022-01-18 18:53:10+00:00
- **Updated**: 2022-01-18 18:53:10+00:00
- **Authors**: Umang Aggarwal, Adrian Popescu, Céline Hudelot
- **Comment**: None
- **Journal**: None
- **Summary**: When we can not assume a large amount of annotated data , active learning is a good strategy. It consists in learning a model on a small amount of annotated data (annotation budget) and in choosing the best set of points to annotate in order to improve the previous model and gain in generalization. In deep learning, active learning is usually implemented as an iterative process in which successive deep models are updated via fine tuning, but it still poses some issues. First, the initial batch of annotated images has to be sufficiently large to train a deep model. Such an assumption is strong, especially when the total annotation budget is reduced. We tackle this issue by using an approach inspired by transfer learning. A pre-trained model is used as a feature extractor and only shallow classifiers are learned during the active iterations. The second issue is the effectiveness of probability or feature estimates of early models for AL task. Samples are generally selected for annotation using acquisition functions based only on the last learned model. We introduce a novel acquisition function which exploits the iterative nature of AL process to select samples in a more robust fashion. Samples for which there is a maximum shift towards uncertainty between the last two learned models predictions are favored. A diversification step is added to select samples from different regions of the classification space and thus introduces a representativeness component in our approach. Evaluation is done against competitive methods with three balanced and imbalanced datasets and outperforms them.



### GANmouflage: 3D Object Nondetection with Texture Fields
- **Arxiv ID**: http://arxiv.org/abs/2201.07202v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.07202v2)
- **Published**: 2022-01-18 18:57:32+00:00
- **Updated**: 2023-04-23 06:02:44+00:00
- **Authors**: Rui Guo, Jasmine Collins, Oscar de Lima, Andrew Owens
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method that learns to camouflage 3D objects within scenes. Given an object's shape and a distribution of viewpoints from which it will be seen, we estimate a texture that will make it difficult to detect. Successfully solving this task requires a model that can accurately reproduce textures from the scene, while simultaneously dealing with the highly conflicting constraints imposed by each viewpoint. We address these challenges with a model based on texture fields and adversarial learning. Our model learns to camouflage a variety of object shapes from randomly sampled locations and viewpoints within the input scene, and is the first to address the problem of hiding complex object shapes. Using a human visual search study, we find that our estimated textures conceal objects significantly better than previous methods. Project site: https://rrrrrguo.github.io/ganmouflage/



### Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents
- **Arxiv ID**: http://arxiv.org/abs/2201.07207v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.07207v2)
- **Published**: 2022-01-18 18:59:45+00:00
- **Updated**: 2022-03-08 06:47:17+00:00
- **Authors**: Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch
- **Comment**: Project website at https://huangwl18.github.io/language-planner
- **Journal**: None
- **Summary**: Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. "make breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner



### OSSID: Online Self-Supervised Instance Detection by (and for) Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2201.07309v2
- **DOI**: 10.1109/LRA.2022.3145488
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.07309v2)
- **Published**: 2022-01-18 20:55:56+00:00
- **Updated**: 2022-04-27 01:00:05+00:00
- **Authors**: Qiao Gu, Brian Okorn, David Held
- **Comment**: 10 pages, 6 figures. RA-L and ICRA 2022
- **Journal**: IEEE Robotics and Automation Letters, vol. 7, no. 2, pp.
  3022-3029, April 2022
- **Summary**: Real-time object pose estimation is necessary for many robot manipulation algorithms. However, state-of-the-art methods for object pose estimation are trained for a specific set of objects; these methods thus need to be retrained to estimate the pose of each new object, often requiring tens of GPU-days of training for optimal performance. In this paper, we propose the OSSID framework, leveraging a slow zero-shot pose estimator to self-supervise the training of a fast detection algorithm. This fast detector can then be used to filter the input to the pose estimator, drastically improving its inference speed. We show that this self-supervised training exceeds the performance of existing zero-shot detection methods on two widely used object pose estimation and detection datasets, without requiring any human annotations. Further, we show that the resulting method for pose estimation has a significantly faster inference speed, due to the ability to filter out large parts of the image. Thus, our method for self-supervised online learning of a detector (trained using pseudo-labels from a slow pose estimator) leads to accurate pose estimation at real-time speeds, without requiring human annotations. Supplementary materials and code can be found at https://georgegu1997.github.io/OSSID/



### Lung Swapping Autoencoder: Learning a Disentangled Structure-texture Representation of Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2201.07344v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.07344v1)
- **Published**: 2022-01-18 22:55:24+00:00
- **Updated**: 2022-01-18 22:55:24+00:00
- **Authors**: Lei Zhou, Joseph Bae, Huidong Liu, Gagandeep Singh, Jeremy Green, Amit Gupta, Dimitris Samaras, Prateek Prasanna
- **Comment**: Extended version of the MICCAI 2021 paper
  https://link.springer.com/chapter/10.1007/978-3-030-87234-2_33 The code is
  available at https://github.com/cvlab-stonybrook/LSAE
- **Journal**: None
- **Summary**: Well-labeled datasets of chest radiographs (CXRs) are difficult to acquire due to the high cost of annotation. Thus, it is desirable to learn a robust and transferable representation in an unsupervised manner to benefit tasks that lack labeled data. Unlike natural images, medical images have their own domain prior; e.g., we observe that many pulmonary diseases, such as the COVID-19, manifest as changes in the lung tissue texture rather than the anatomical structure. Therefore, we hypothesize that studying only the texture without the influence of structure variations would be advantageous for downstream prognostic and predictive modeling tasks. In this paper, we propose a generative framework, the Lung Swapping Autoencoder (LSAE), that learns factorized representations of a CXR to disentangle the texture factor from the structure factor. Specifically, by adversarial training, the LSAE is optimized to generate a hybrid image that preserves the lung shape in one image but inherits the lung texture of another. To demonstrate the effectiveness of the disentangled texture representation, we evaluate the texture encoder $Enc^t$ in LSAE on ChestX-ray14 (N=112,120), and our own multi-institutional COVID-19 outcome prediction dataset, COVOC (N=340 (Subset-1) + 53 (Subset-2)). On both datasets, we reach or surpass the state-of-the-art by finetuning $Enc^t$ in LSAE that is 77% smaller than a baseline Inception v3. Additionally, in semi-and-self supervised settings with a similar model budget, $Enc^t$ in LSAE is also competitive with the state-of-the-art MoCo. By "re-mixing" the texture and shape factors, we generate meaningful hybrid images that can augment the training set. This data augmentation method can further improve COVOC prediction performance. The improvement is consistent even when we directly evaluate the Subset-1 trained model on Subset-2 without any fine-tuning.



### Weakly Supervised Contrastive Learning for Better Severity Scoring of Lung Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/2201.07357v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.07357v1)
- **Published**: 2022-01-18 23:45:18+00:00
- **Updated**: 2022-01-18 23:45:18+00:00
- **Authors**: Gautam Rajendrakumar Gare, Hai V. Tran, Bennett P deBoisblanc, Ricardo Luis Rodriguez, John Michael Galeotti
- **Comment**: Under Review for MIDL 2022 conference
- **Journal**: None
- **Summary**: With the onset of the COVID-19 pandemic, ultrasound has emerged as an effective tool for bedside monitoring of patients. Due to this, a large amount of lung ultrasound scans have been made available which can be used for AI based diagnosis and analysis. Several AI-based patient severity scoring models have been proposed that rely on scoring the appearance of the ultrasound scans. AI models are trained using ultrasound-appearance severity scores that are manually labeled based on standardized visual features. We address the challenge of labeling every ultrasound frame in the video clips. Our contrastive learning method treats the video clip severity labels as noisy weak severity labels for individual frames, thus requiring only video-level labels. We show that it performs better than the conventional cross-entropy loss based training. We combine frame severity predictions to come up with video severity predictions and show that the frame based model achieves comparable performance to a video based TSM model, on a large dataset combining public and private sources.



