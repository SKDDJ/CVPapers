# Arxiv Papers in cs.CV on 2022-01-24
### Improving Chest X-Ray Report Generation by Leveraging Warm Starting
- **Arxiv ID**: http://arxiv.org/abs/2201.09405v2
- **DOI**: 10.1016/j.artmed.2023.102633
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09405v2)
- **Published**: 2022-01-24 00:46:37+00:00
- **Updated**: 2023-07-12 23:14:28+00:00
- **Authors**: Aaron Nicolson, Jason Dowling, Bevan Koopman
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically generating a report from a patient's Chest X-Rays (CXRs) is a promising solution to reducing clinical workload and improving patient care. However, current CXR report generators -- which are predominantly encoder-to-decoder models -- lack the diagnostic accuracy to be deployed in a clinical setting. To improve CXR report generation, we investigate warm starting the encoder and decoder with recent open-source computer vision and natural language processing checkpoints, such as the Vision Transformer (ViT) and PubMedBERT. To this end, each checkpoint is evaluated on the MIMIC-CXR and IU X-Ray datasets. Our experimental investigation demonstrates that the Convolutional vision Transformer (CvT) ImageNet-21K and the Distilled Generative Pre-trained Transformer 2 (DistilGPT2) checkpoints are best for warm starting the encoder and decoder, respectively. Compared to the state-of-the-art ($\mathcal{M}^2$ Transformer Progressive), CvT2DistilGPT2 attained an improvement of 8.3\% for CE F-1, 1.8\% for BLEU-4, 1.6\% for ROUGE-L, and 1.0\% for METEOR. The reports generated by CvT2DistilGPT2 have a higher similarity to radiologist reports than previous approaches. This indicates that leveraging warm starting improves CXR report generation. Code and checkpoints for CvT2DistilGPT2 are available at https://github.com/aehrc/cvt2distilgpt2.



### Cross-Domain Document Layout Analysis via Unsupervised Document Style Guide
- **Arxiv ID**: http://arxiv.org/abs/2201.09407v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09407v1)
- **Published**: 2022-01-24 00:49:19+00:00
- **Updated**: 2022-01-24 00:49:19+00:00
- **Authors**: Xingjiao Wu, Luwei Xiao, Xiangcheng Du, Yingbin Zheng, Xin Li, Tianlong Ma, Liang He
- **Comment**: None
- **Journal**: None
- **Summary**: The document layout analysis (DLA) aims to decompose document images into high-level semantic areas (i.e., figures, tables, texts, and background). Creating a DLA framework with strong generalization capabilities is a challenge due to document objects are diversity in layout, size, aspect ratio, texture, etc. Many researchers devoted this challenge by synthesizing data to build large training sets. However, the synthetic training data has different styles and erratic quality. Besides, there is a large gap between the source data and the target data. In this paper, we propose an unsupervised cross-domain DLA framework based on document style guidance. We integrated the document quality assessment and the document cross-domain analysis into a unified framework. Our framework is composed of three components, Document Layout Generator (GLD), Document Elements Decorator(GED), and Document Style Discriminator(DSD). The GLD is used to document layout generates, the GED is used to document layout elements fill, and the DSD is used to document quality assessment and cross-domain guidance. First, we apply GLD to predict the positions of the generated document. Then, we design a novel algorithm based on aesthetic guidance to fill the document positions. Finally, we use contrastive learning to evaluate the quality assessment of the document. Besides, we design a new strategy to change the document quality assessment component into a document cross-domain style guide component. Our framework is an unsupervised document layout analysis framework. We have proved through numerous experiments that our proposed method has achieved remarkable performance.



### Mutual Attention-based Hybrid Dimensional Network for Multimodal Imaging Computer-aided Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2201.09421v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09421v1)
- **Published**: 2022-01-24 02:31:25+00:00
- **Updated**: 2022-01-24 02:31:25+00:00
- **Authors**: Yin Dai, Yifan Gao, Fayu Liu, Jun Fu
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Recent works on Multimodal 3D Computer-aided diagnosis have demonstrated that obtaining a competitive automatic diagnosis model when a 3D convolution neural network (CNN) brings more parameters and medical images are scarce remains nontrivial and challenging. Considering both consistencies of regions of interest in multimodal images and diagnostic accuracy, we propose a novel mutual attention-based hybrid dimensional network for MultiModal 3D medical image classification (MMNet). The hybrid dimensional network integrates 2D CNN with 3D convolution modules to generate deeper and more informative feature maps, and reduce the training complexity of 3D fusion. Besides, the pre-trained model of ImageNet can be used in 2D CNN, which improves the performance of the model. The stereoscopic attention is focused on building rich contextual interdependencies of the region in 3D medical images. To improve the regional correlation of pathological tissues in multimodal medical images, we further design a mutual attention framework in the network to build the region-wise consistency in similar stereoscopic regions of different image modalities, providing an implicit manner to instruct the network to focus on pathological tissues. MMNet outperforms many previous solutions and achieves results competitive to the state-of-the-art on three multimodal imaging datasets, i.e., Parotid Gland Tumor (PGT) dataset, the MRNet dataset, and the PROSTATEx dataset, and its advantages are validated by extensive experiments.



### AutoMC: Automated Model Compression based on Domain Knowledge and Progressive search strategy
- **Arxiv ID**: http://arxiv.org/abs/2201.09884v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09884v1)
- **Published**: 2022-01-24 04:24:31+00:00
- **Updated**: 2022-01-24 04:24:31+00:00
- **Authors**: Chunnan Wang, Hongzhi Wang, Xiangyu Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Model compression methods can reduce model complexity on the premise of maintaining acceptable performance, and thus promote the application of deep neural networks under resource constrained environments. Despite their great success, the selection of suitable compression methods and design of details of the compression scheme are difficult, requiring lots of domain knowledge as support, which is not friendly to non-expert users. To make more users easily access to the model compression scheme that best meet their needs, in this paper, we propose AutoMC, an effective automatic tool for model compression. AutoMC builds the domain knowledge on model compression to deeply understand the characteristics and advantages of each compression method under different settings. In addition, it presents a progressive search strategy to efficiently explore pareto optimal compression scheme according to the learned prior knowledge combined with the historical evaluation information. Extensive experimental results show that AutoMC can provide satisfying compression schemes within short time, demonstrating the effectiveness of AutoMC.



### UniFormer: Unifying Convolution and Self-attention for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.09450v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09450v3)
- **Published**: 2022-01-24 04:39:39+00:00
- **Updated**: 2023-05-31 09:19:23+00:00
- **Authors**: Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, Yu Qiao
- **Comment**: 18 pages, 10 figures, 23 tables. This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible
- **Journal**: None
- **Summary**: It is a challenging task to learn discriminative representation from images and videos, due to large local redundancy and complex global dependency in these visual data. Convolution neural networks (CNNs) and vision transformers (ViTs) have been two dominant frameworks in the past few years. Though CNNs can efficiently decrease local redundancy by convolution within a small neighborhood, the limited receptive field makes it hard to capture global dependency. Alternatively, ViTs can effectively capture long-range dependency via self-attention, while blind similarity comparisons among all the tokens lead to high redundancy. To resolve these problems, we propose a novel Unified transFormer (UniFormer), which can seamlessly integrate the merits of convolution and self-attention in a concise transformer format. Different from the typical transformer blocks, the relation aggregators in our UniFormer block are equipped with local and global token affinity respectively in shallow and deep layers, allowing to tackle both redundancy and dependency for efficient and effective representation learning. Finally, we flexibly stack our UniFormer blocks into a new powerful backbone, and adopt it for various vision tasks from image to video domain, from classification to dense prediction. Without any extra training data, our UniFormer achieves 86.3 top-1 accuracy on ImageNet-1K classification. With only ImageNet-1K pre-training, it can simply achieve state-of-the-art performance in a broad range of downstream tasks, e.g., it obtains 82.9/84.8 top-1 accuracy on Kinetics-400/600, 60.9/71.2 top-1 accuracy on Sth-Sth V1/V2 video classification, 53.8 box AP and 46.4 mask AP on COCO object detection, 50.8 mIoU on ADE20K semantic segmentation, and 77.4 AP on COCO pose estimation. We further build an efficient UniFormer with 2-4x higher throughput. Code is available at https://github.com/Sense-X/UniFormer.



### Cyber Mobility Mirror for Enabling Cooperative Driving Automation in Mixed Traffic: A Co-Simulation Platform
- **Arxiv ID**: http://arxiv.org/abs/2201.09463v2
- **DOI**: 10.1109/MITS.2022.3203662
- **Categories**: **cs.SE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09463v2)
- **Published**: 2022-01-24 05:27:20+00:00
- **Updated**: 2022-08-30 22:52:25+00:00
- **Authors**: Zhengwei Bai, Guoyuan Wu, Xuewei Qi, Yongkang Liu, Kentaro Oguchi, Matthew J. Barth
- **Comment**: Accepted by the IEEE Intelligent Transportation Systems Magazine
- **Journal**: IEEE Intelligent Transportation Systems Magazine 2022
- **Summary**: Endowed with automation and connectivity, Connected and Automated Vehicles are meant to be a revolutionary promoter for Cooperative Driving Automation. Nevertheless, CAVs need high-fidelity perception information on their surroundings, which is available but costly to collect from various onboard sensors as well as vehicle-to-everything (V2X) communications. Therefore, authentic perception information based on high-fidelity sensors via a cost-effective platform is crucial for enabling CDA-related research, e.g., cooperative decision-making or control. Most state-of-the-art traffic simulation studies for CAVs rely on situation-awareness information by directly calling on intrinsic attributes of the objects, which impedes the reliability and fidelity of the assessment of CDA algorithms. In this study, a \textit{Cyber Mobility Mirror (CMM)} Co-Simulation Platform is designed for enabling CDA by providing authentic perception information. The \textit{CMM} Co-Simulation Platform can emulate the real world with a high-fidelity sensor perception system and a cyber world with a real-time rebuilding system acting as a "\textit{Mirror}" of the real-world environment. Concretely, the real-world simulator is mainly in charge of simulating the traffic environment, sensors, as well as the authentic perception process. The mirror-world simulator is responsible for rebuilding objects and providing their information as intrinsic attributes of the simulator to support the development and evaluation of CDA algorithms. To illustrate the functionality of the proposed co-simulation platform, a roadside LiDAR-based vehicle perception system for enabling CDA is prototyped as a study case. Specific traffic environments and CDA tasks are designed for experiments whose results are demonstrated and analyzed to show the performance of the platform.



### Forgery Attack Detection in Surveillance Video Streams Using Wi-Fi Channel State Information
- **Arxiv ID**: http://arxiv.org/abs/2201.09487v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09487v1)
- **Published**: 2022-01-24 06:51:03+00:00
- **Updated**: 2022-01-24 06:51:03+00:00
- **Authors**: Yong Huang, Xiang Li, Wei Wang, Tao Jiang, Qian Zhang
- **Comment**: To appear in IEEE Transactions on Wireless Communications. arXiv
  admin note: text overlap with arXiv:2101.00848
- **Journal**: None
- **Summary**: The cybersecurity breaches expose surveillance video streams to forgery attacks, under which authentic streams are falsified to hide unauthorized activities. Traditional video forensics approaches can localize forgery traces using spatial-temporal analysis on relatively long video clips, while falling short in real-time forgery detection. The recent work correlates time-series camera and wireless signals to detect looped videos but cannot realize fine-grained forgery localization. To overcome these limitations, we propose Secure-Pose, which exploits the pervasive coexistence of surveillance and Wi-Fi infrastructures to defend against video forgery attacks in a real-time and fine-grained manner. We observe that coexisting camera and Wi-Fi signals convey common human semantic information and forgery attacks on video streams will decouple such information correspondence. Particularly, retrievable human pose features are first extracted from concurrent video and Wi-Fi channel state information (CSI) streams. Then, a lightweight detection network is developed to accurately discover forgery attacks and an efficient localization algorithm is devised to seamlessly track forgery traces in video streams. We implement Secure-Pose using one Logitech camera and two Intel 5300 NICs and evaluate it in different environments. Secure-Pose achieves a high detection accuracy of 98.7% and localizes abnormal objects under playback and tampering attacks.



### Accelerated Intravascular Ultrasound Imaging using Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.09522v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09522v1)
- **Published**: 2022-01-24 08:33:21+00:00
- **Updated**: 2022-01-24 08:33:21+00:00
- **Authors**: Tristan S. W. Stevens, Nishith Chennakeshava, Frederik J. de Bruijn, Martin Pekař, Ruud J. G. van Sloun
- **Comment**: 5 pages, 3 figures, conference
- **Journal**: ICASSP 2022 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)
- **Summary**: Intravascular ultrasound (IVUS) offers a unique perspective in the treatment of vascular diseases by creating a sequence of ultrasound-slices acquired from within the vessel. However, unlike conventional hand-held ultrasound, the thin catheter only provides room for a small number of physical channels for signal transfer from a transducer-array at the tip. For continued improvement of image quality and frame rate, we present the use of deep reinforcement learning to deal with the current physical information bottleneck. Valuable inspiration has come from the field of magnetic resonance imaging (MRI), where learned acquisition schemes have brought significant acceleration in image acquisition at competing image quality. To efficiently accelerate IVUS imaging, we propose a framework that utilizes deep reinforcement learning for an optimal adaptive acquisition policy on a per-frame basis enabled by actor-critic methods and Gumbel top-$K$ sampling.



### Image features of a splashing drop on a solid surface extracted using a feedforward neural network
- **Arxiv ID**: http://arxiv.org/abs/2201.09541v1
- **DOI**: 10.1063/5.0077050
- **Categories**: **physics.flu-dyn**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09541v1)
- **Published**: 2022-01-24 09:24:23+00:00
- **Updated**: 2022-01-24 09:24:23+00:00
- **Authors**: Jingzu Yee, Akinori Yamanaka, Yoshiyuki Tagawa
- **Comment**: 19 pages, 17 figures. Source code is available on GitHub:
  https://github.com/yeejingzuTUAT/Image-features-of-a-splashing-drop-on-a-solid-surface-extracted-using-a-feedforward-neural-network
- **Journal**: Phys. Fluids 34, 013317 (2022)
- **Summary**: This article reports nonintuitive characteristic of a splashing drop on a solid surface discovered through extracting image features using a feedforward neural network (FNN). Ethanol of area-equivalent radius about 1.29 mm was dropped from impact heights ranging from 4 cm to 60 cm (splashing threshold 20 cm) and impacted on a hydrophilic surface. The images captured when half of the drop impacted the surface were labeled according to their outcome, splashing or nonsplashing, and were used to train an FNN. A classification accuracy higher than 96% was achieved. To extract the image features identified by the FNN for classification, the weight matrix of the trained FNN for identifying splashing drops was visualized. Remarkably, the visualization showed that the trained FNN identified the contour height of the main body of the impacting drop as an important characteristic differentiating between splashing and nonsplashing drops, which has not been reported in previous studies. This feature was found throughout the impact, even when one and three-quarters of the drop impacted the surface. To confirm the importance of this image feature, the FNN was retrained to classify using only the main body without checking for the presence of ejected secondary droplets. The accuracy was still higher than 82%, confirming that the contour height is an important feature distinguishing splashing from nonsplashing drops. Several aspects of drop impact are analyzed and discussed with the aim of identifying the possible mechanism underlying the difference in contour height between splashing and nonsplashing drops.



### Consistent 3D Hand Reconstruction in Video via self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.09548v2
- **DOI**: 10.1109/TPAMI.2023.3247907
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.09548v2)
- **Published**: 2022-01-24 09:44:11+00:00
- **Updated**: 2023-03-20 04:28:10+00:00
- **Authors**: Zhigang Tu, Zhisheng Huang, Yujin Chen, Di Kang, Linchao Bao, Bisheng Yang, Junsong Yuan
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2103.11703
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence.
  2023
- **Summary**: We present a method for reconstructing accurate and consistent 3D hands from a monocular video. We observe that detected 2D hand keypoints and the image texture provide important cues about the geometry and texture of the 3D hand, which can reduce or even eliminate the requirement on 3D hand annotation. Thus we propose ${\rm {S}^{2}HAND}$, a self-supervised 3D hand reconstruction model, that can jointly estimate pose, shape, texture, and the camera viewpoint from a single RGB input through the supervision of easily accessible 2D detected keypoints. We leverage the continuous hand motion information contained in the unlabeled video data and propose ${\rm {S}^{2}HAND(V)}$, which uses a set of weights shared ${\rm {S}^{2}HAND}$ to process each frame and exploits additional motion, texture, and shape consistency constrains to promote more accurate hand poses and more consistent shapes and textures. Experiments on benchmark datasets demonstrate that our self-supervised approach produces comparable hand reconstruction performance compared with the recent full-supervised methods in single-frame as input setup, and notably improves the reconstruction accuracy and consistency when using video training data.



### Debiasing pipeline improves deep learning model generalization for X-ray based lung nodule detection
- **Arxiv ID**: http://arxiv.org/abs/2201.09563v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09563v1)
- **Published**: 2022-01-24 10:08:07+00:00
- **Updated**: 2022-01-24 10:08:07+00:00
- **Authors**: Michael Horry, Subrata Chakraborty, Biswajeet Pradhan, Manoranjan Paul, Jing Zhu, Hui Wen Loh, Prabal Datta Barua, U. Rajendra Arharya
- **Comment**: 32 pages, 17 figures, 4 tables
- **Journal**: None
- **Summary**: Lung cancer is the leading cause of cancer death worldwide and a good prognosis depends on early diagnosis. Unfortunately, screening programs for the early diagnosis of lung cancer are uncommon. This is in-part due to the at-risk groups being located in rural areas far from medical facilities. Reaching these populations would require a scaled approach that combines mobility, low cost, speed, accuracy, and privacy. We can resolve these issues by combining the chest X-ray imaging mode with a federated deep-learning approach, provided that the federated model is trained on homogenous data to ensure that no single data source can adversely bias the model at any point in time. In this study we show that an image pre-processing pipeline that homogenizes and debiases chest X-ray images can improve both internal classification and external generalization, paving the way for a low-cost and accessible deep learning-based clinical system for lung cancer screening. An evolutionary pruning mechanism is used to train a nodule detection deep learning model on the most informative images from a publicly available lung nodule X-ray dataset. Histogram equalization is used to remove systematic differences in image brightness and contrast. Model training is performed using all combinations of lung field segmentation, close cropping, and rib suppression operators. We show that this pre-processing pipeline results in deep learning models that successfully generalize an independent lung nodule dataset using ablation studies to assess the contribution of each operator in this pipeline. In stripping chest X-ray images of known confounding variables by lung field segmentation, along with suppression of signal noise from the bone structure we can train a highly accurate deep learning lung nodule detection algorithm with outstanding generalization accuracy of 89% to nodule samples in unseen data.



### Multi-Scale Iterative Refinement Network for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.09574v1
- **DOI**: 10.1016/j.engappai.2021.104473
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09574v1)
- **Published**: 2022-01-24 10:33:00+00:00
- **Updated**: 2022-01-24 10:33:00+00:00
- **Authors**: Ze-yu Liu, Jian-wei Liu, Xin Zuo, Ming-fei Hu
- **Comment**: 40 pages
- **Journal**: Engineering Applications of Artificial Intelligence(2021)
- **Summary**: The extensive research leveraging RGB-D information has been exploited in salient object detection. However, salient visual cues appear in various scales and resolutions of RGB images due to semantic gaps at different feature levels. Meanwhile, similar salient patterns are available in cross-modal depth images as well as multi-scale versions. Cross-modal fusion and multi-scale refinement are still an open problem in RGB-D salient object detection task. In this paper, we begin by introducing top-down and bottom-up iterative refinement architecture to leverage multi-scale features, and then devise attention based fusion module (ABF) to address on cross-modal correlation. We conduct extensive experiments on seven public datasets. The experimental results show the effectiveness of our devised method



### Importance of Textlines in Historical Document Classification
- **Arxiv ID**: http://arxiv.org/abs/2201.09575v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T07, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/2201.09575v2)
- **Published**: 2022-01-24 10:37:43+00:00
- **Updated**: 2022-03-30 10:50:31+00:00
- **Authors**: Martin Kišš, Jan Kohút, Karel Beneš, Michal Hradiš
- **Comment**: 13 pages, 7 figures, 5 tables
- **Journal**: None
- **Summary**: This paper describes a system prepared at Brno University of Technology for ICDAR 2021 Competition on Historical Document Classification, experiments leading to its design, and the main findings. The solved tasks include script and font classification, document origin localization, and dating. We combined patch-level and line-level approaches, where the line-level system utilizes an existing, publicly available page layout analysis engine. In both systems, neural networks provide local predictions which are combined into page-level decisions, and the results of both systems are fused using linear or log-linear interpolation. We propose loss functions suitable for weakly supervised classification problem where multiple possible labels are provided, and we propose loss functions suitable for interval regression in the dating task. The line-level system significantly improves results in script and font classification and in the dating task. The full system achieved 98.48 %, 88.84 %, and 79.69 % accuracy in the font, script, and location classification tasks respectively. In the dating task, our system achieved a mean absolute error of 21.91 years. Our system achieved the best results in all tasks and became the overall winner of the competition.



### AutoSeg -- Steering the Inductive Biases for Automatic Pathology Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.09579v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09579v1)
- **Published**: 2022-01-24 10:44:04+00:00
- **Updated**: 2022-01-24 10:44:04+00:00
- **Authors**: Felix Meissen, Georgios Kaissis, Daniel Rueckert
- **Comment**: 8 pages, 3 figures, part of the MICCAI MOOD Challenge 2021
- **Journal**: None
- **Summary**: In medical imaging, un-, semi-, or self-supervised pathology detection is often approached with anomaly- or out-of-distribution detection methods, whose inductive biases are not intentionally directed towards detecting pathologies, and are therefore sub-optimal for this task. To tackle this problem, we propose AutoSeg, an engine that can generate diverse artificial anomalies that resemble the properties of real-world pathologies. Our method can accurately segment unseen artificial anomalies and outperforms existing methods for pathology detection on a challenging real-world dataset of Chest X-ray images. We experimentally evaluate our method on the Medical Out-of-Distribution Analysis Challenge 2021.



### Describe me if you can! Characterized Instance-level Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/2201.09594v1
- **DOI**: 10.1109/ICIP42928.2021.9506509
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09594v1)
- **Published**: 2022-01-24 11:07:03+00:00
- **Updated**: 2022-01-24 11:07:03+00:00
- **Authors**: Angelique Loesch, Romaric Audigier
- **Comment**: 5 pages
- **Journal**: Published in: 2021 IEEE International Conference on Image
  Processing (ICIP)
- **Summary**: Several computer vision applications such as person search or online fashion rely on human description. The use of instance-level human parsing (HP) is therefore relevant since it localizes semantic attributes and body parts within a person. But how to characterize these attributes? To our knowledge, only some single-HP datasets describe attributes with some color, size and/or pattern characteristics. There is a lack of dataset for multi-HP in the wild with such characteristics. In this article, we propose the dataset CCIHP based on the multi-HP dataset CIHP, with 20 new labels covering these 3 kinds of characteristics. In addition, we propose HPTR, a new bottom-up multi-task method based on transformers as a fast and scalable baseline. It is the fastest method of multi-HP state of the art while having precision comparable to the most precise bottom-up method. We hope this will encourage research for fast and accurate methods of precise human descriptions.



### End-to-end Person Search Sequentially Trained on Aggregated Dataset
- **Arxiv ID**: http://arxiv.org/abs/2201.09604v1
- **DOI**: 10.1109/ICIP.2019.8803643
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09604v1)
- **Published**: 2022-01-24 11:22:15+00:00
- **Updated**: 2022-01-24 11:22:15+00:00
- **Authors**: Angelique Loesch, Jaonary Rabarisoa, Romaric Audigier
- **Comment**: 5 pages
- **Journal**: Published in: 2019 IEEE International Conference on Image
  Processing (ICIP)
- **Summary**: In video surveillance applications, person search is a challenging task consisting in detecting people and extracting features from their silhouette for re-identification (re-ID) purpose. We propose a new end-to-end model that jointly computes detection and feature extraction steps through a single deep Convolutional Neural Network architecture. Sharing feature maps between the two tasks for jointly describing people commonalities and specificities allows faster runtime, which is valuable in real-world applications. In addition to reaching state-of-the-art accuracy, this multi-task model can be sequentially trained task-by-task, which results in a broader acceptance of input dataset types. Indeed, we show that aggregating more pedestrian detection datasets without costly identity annotations makes the shared feature maps more generic, and improves re-ID precision. Moreover, these boosted shared feature maps result in re-ID features more robust to a cross-dataset scenario.



### SEN12MS-CR-TS: A Remote Sensing Data Set for Multi-modal Multi-temporal Cloud Removal
- **Arxiv ID**: http://arxiv.org/abs/2201.09613v1
- **DOI**: 10.1109/TGRS.2022.3146246
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09613v1)
- **Published**: 2022-01-24 11:38:49+00:00
- **Updated**: 2022-01-24 11:38:49+00:00
- **Authors**: Patrick Ebel, Yajin Xu, Michael Schmitt, Xiaoxiang Zhu
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, 2022
- **Summary**: About half of all optical observations collected via spaceborne satellites are affected by haze or clouds. Consequently, cloud coverage affects the remote sensing practitioner's capabilities of a continuous and seamless monitoring of our planet. This work addresses the challenge of optical satellite image reconstruction and cloud removal by proposing a novel multi-modal and multi-temporal data set called SEN12MS-CR-TS. We propose two models highlighting the benefits and use cases of SEN12MS-CR-TS: First, a multi-modal multi-temporal 3D-Convolution Neural Network that predicts a cloud-free image from a sequence of cloudy optical and radar images. Second, a sequence-to-sequence translation model that predicts a cloud-free time series from a cloud-covered time series. Both approaches are evaluated experimentally, with their respective models trained and tested on SEN12MS-CR-TS. The conducted experiments highlight the contribution of our data set to the remote sensing community as well as the benefits of multi-modal and multi-temporal information to reconstruct noisy information. Our data set is available at https://patrickTUM.github.io/cloud_removal



### Paired Image to Image Translation for Strikethrough Removal From Handwritten Words
- **Arxiv ID**: http://arxiv.org/abs/2201.09633v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09633v2)
- **Published**: 2022-01-24 12:28:27+00:00
- **Updated**: 2022-04-01 15:01:44+00:00
- **Authors**: Raphaela Heil, Ekta Vats, Anders Hast
- **Comment**: accepted at DAS2022
- **Journal**: None
- **Summary**: Transcribing struck-through, handwritten words, for example for the purpose of genetic criticism, can pose a challenge to both humans and machines, due to the obstructive properties of the superimposed strokes. This paper investigates the use of paired image to image translation approaches to remove strikethrough strokes from handwritten words. Four different neural network architectures are examined, ranging from a few simple convolutional layers to deeper ones, employing Dense blocks. Experimental results, obtained from one synthetic and one genuine paired strikethrough dataset, confirm that the proposed paired models outperform the CycleGAN-based state of the art, while using less than a sixth of the trainable parameters.



### Question Generation for Evaluating Cross-Dataset Shifts in Multi-modal Grounding
- **Arxiv ID**: http://arxiv.org/abs/2201.09639v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09639v1)
- **Published**: 2022-01-24 12:42:30+00:00
- **Updated**: 2022-01-24 12:42:30+00:00
- **Authors**: Arjun R. Akula
- **Comment**: None
- **Journal**: None
- **Summary**: Visual question answering (VQA) is the multi-modal task of answering natural language questions about an input image. Through cross-dataset adaptation methods, it is possible to transfer knowledge from a source dataset with larger train samples to a target dataset where training set is limited. Suppose a VQA model trained on one dataset train set fails in adapting to another, it is hard to identify the underlying cause of domain mismatch as there could exists a multitude of reasons such as image distribution mismatch and question distribution mismatch. At UCLA, we are working on a VQG module that facilitate in automatically generating OOD shifts that aid in systematically evaluating cross-dataset adaptation capabilities of VQA models.



### Which Style Makes Me Attractive? Interpretable Control Discovery and Counterfactual Explanation on StyleGAN
- **Arxiv ID**: http://arxiv.org/abs/2201.09689v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.09689v1)
- **Published**: 2022-01-24 13:42:47+00:00
- **Updated**: 2022-01-24 13:42:47+00:00
- **Authors**: Bo Li, Qiulin Wang, Jiquan Pei, Yu Yang, Xiangyang Ji
- **Comment**: None
- **Journal**: None
- **Summary**: The semantically disentangled latent subspace in GAN provides rich interpretable controls in image generation. This paper includes two contributions on semantic latent subspace analysis in the scenario of face generation using StyleGAN2. First, we propose a novel approach to disentangle latent subspace semantics by exploiting existing face analysis models, e.g., face parsers and face landmark detectors. These models provide the flexibility to construct various criterions with very concrete and interpretable semantic meanings (e.g., change face shape or change skin color) to restrict latent subspace disentanglement. Rich latent space controls unknown previously can be discovered using the constructed criterions. Second, we propose a new perspective to explain the behavior of a CNN classifier by generating counterfactuals in the interpretable latent subspaces we discovered. This explanation helps reveal whether the classifier learns semantics as intended. Experiments on various disentanglement criterions demonstrate the effectiveness of our approach. We believe this approach contributes to both areas of image manipulation and counterfactual explainability of CNNs. The code is available at \url{https://github.com/prclibo/ice}.



### Shape-consistent Generative Adversarial Networks for multi-modal Medical segmentation maps
- **Arxiv ID**: http://arxiv.org/abs/2201.09693v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09693v2)
- **Published**: 2022-01-24 13:57:31+00:00
- **Updated**: 2022-02-04 07:24:09+00:00
- **Authors**: Leo Segre, Or Hirschorn, Dvir Ginzburg, Dan Raviv
- **Comment**: None
- **Journal**: None
- **Summary**: Image translation across domains for unpaired datasets has gained interest and great improvement lately. In medical imaging, there are multiple imaging modalities, with very different characteristics. Our goal is to use cross-modality adaptation between CT and MRI whole cardiac scans for semantic segmentation. We present a segmentation network using synthesised cardiac volumes for extremely limited datasets. Our solution is based on a 3D cross-modality generative adversarial network to share information between modalities and generate synthesized data using unpaired datasets. Our network utilizes semantic segmentation to improve generator shape consistency, thus creating more realistic synthesised volumes to be used when re-training the segmentation network. We show that improved segmentation can be achieved on small datasets when using spatial augmentations to improve a generative adversarial network. These augmentations improve the generator capabilities, thus enhancing the performance of the Segmentor. Using only 16 CT and 16 MRI cardiovascular volumes, improved results are shown over other segmentation methods while using the suggested architecture.



### Feature transforms for image data augmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.09700v2
- **DOI**: 10.1007/s00521-022-07645-z
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.09700v2)
- **Published**: 2022-01-24 14:12:29+00:00
- **Updated**: 2022-08-23 03:44:03+00:00
- **Authors**: Loris Nanni, Michelangelo Paci, Sheryl Brahnam, Alessandra Lumini
- **Comment**: None
- **Journal**: None
- **Summary**: A problem with Convolutional Neural Networks (CNNs) is that they require large datasets to obtain adequate robustness; on small datasets, they are prone to overfitting. Many methods have been proposed to overcome this shortcoming with CNNs. In cases where additional samples cannot easily be collected, a common approach is to generate more data points from existing data using an augmentation technique. In image classification, many augmentation approaches utilize simple image manipulation algorithms. In this work, we build ensembles on the data level by adding images generated by combining fourteen augmentation approaches, three of which are proposed here for the first time. These novel methods are based on the Fourier Transform (FT), the Radon Transform (RT) and the Discrete Cosine Transform (DCT). Pretrained ResNet50 networks are finetuned on training sets that include images derived from each augmentation method. These networks and several fusions are evaluated and compared across eleven benchmarks. Results show that building ensembles on the data level by combining different data augmentation methods produce classifiers that not only compete competitively against the state-of-the-art but often surpass the best approaches reported in the literature.



### Learning Semantics for Visual Place Recognition through Multi-Scale Attention
- **Arxiv ID**: http://arxiv.org/abs/2201.09701v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09701v2)
- **Published**: 2022-01-24 14:13:12+00:00
- **Updated**: 2022-01-25 11:12:33+00:00
- **Authors**: Valerio Paolicelli, Antonio Tavera, Carlo Masone, Gabriele Berton, Barbara Caputo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we address the task of visual place recognition (VPR), where the goal is to retrieve the correct GPS coordinates of a given query image against a huge geotagged gallery. While recent works have shown that building descriptors incorporating semantic and appearance information is beneficial, current state-of-the-art methods opt for a top down definition of the significant semantic content. Here we present the first VPR algorithm that learns robust global embeddings from both visual appearance and semantic content of the data, with the segmentation process being dynamically guided by the recognition of places through a multi-scale attention module. Experiments on various scenarios validate this new approach and demonstrate its performance against state-of-the-art methods. Finally, we propose the first synthetic-world dataset suited for both place recognition and segmentation tasks.



### Keeping Deep Lithography Simulators Updated: Global-Local Shape-Based Novelty Detection and Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.09717v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09717v1)
- **Published**: 2022-01-24 14:50:30+00:00
- **Updated**: 2022-01-24 14:50:30+00:00
- **Authors**: Hao-Chiang Shao, Hsing-Lei Ping, Kuo-shiuan Chen, Weng-Tai Su, Chia-Wen Lin, Shao-Yun Fang, Pin-Yian Tsai, Yan-Hsiu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Learning-based pre-simulation (i.e., layout-to-fabrication) models have been proposed to predict the fabrication-induced shape deformation from an IC layout to its fabricated circuit. Such models are usually driven by pairwise learning, involving a training set of layout patterns and their reference shape images after fabrication. However, it is expensive and time-consuming to collect the reference shape images of all layout clips for model training and updating. To address the problem, we propose a deep learning-based layout novelty detection scheme to identify novel (unseen) layout patterns, which cannot be well predicted by a pre-trained pre-simulation model. We devise a global-local novelty scoring mechanism to assess the potential novelty of a layout by exploiting two subnetworks: an autoencoder and a pretrained pre-simulation model. The former characterizes the global structural dissimilarity between a given layout and training samples, whereas the latter extracts a latent code representing the fabrication-induced local deformation. By integrating the global dissimilarity with the local deformation boosted by a self-attention mechanism, our model can accurately detect novelties without the ground-truth circuit shapes of test samples. Based on the detected novelties, we further propose two active-learning strategies to sample a reduced amount of representative layouts most worthy to be fabricated for acquiring their ground-truth circuit shapes. Experimental results demonstrate i) our method's effectiveness in layout novelty detection, and ii) our active-learning strategies' ability in selecting representative novel layouts for keeping a learning-based pre-simulation model updated.



### Hot-Refresh Model Upgrades with Regression-Alleviating Compatible Training in Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2201.09724v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09724v1)
- **Published**: 2022-01-24 14:59:12+00:00
- **Updated**: 2022-01-24 14:59:12+00:00
- **Authors**: Binjie Zhang, Yixiao Ge, Yantao Shen, Yu Li, Chun Yuan, Xuyuan Xu, Yexin Wang, Ying Shan
- **Comment**: Accepted to ICLR 2022
- **Journal**: None
- **Summary**: The task of hot-refresh model upgrades of image retrieval systems plays an essential role in the industry but has never been investigated in academia before. Conventional cold-refresh model upgrades can only deploy new models after the gallery is overall backfilled, taking weeks or even months for massive data. In contrast, hot-refresh model upgrades deploy the new model immediately and then gradually improve the retrieval accuracy by backfilling the gallery on-the-fly. Compatible training has made it possible, however, the problem of model regression with negative flips poses a great challenge to the stable improvement of user experience. We argue that it is mainly due to the fact that new-to-old positive query-gallery pairs may show less similarity than new-to-new negative pairs. To solve the problem, we introduce a Regression-Alleviating Compatible Training (RACT) method to properly constrain the feature compatibility while reducing negative flips. The core is to encourage the new-to-old positive pairs to be more similar than both the new-to-old negative pairs and the new-to-new negative pairs. An efficient uncertainty-based backfilling strategy is further introduced to fasten accuracy improvements. Extensive experiments on large-scale retrieval benchmarks (e.g., Google Landmark) demonstrate that our RACT effectively alleviates the model regression for one more step towards seamless model upgrades. The code will be available at https://github.com/binjiezhang/RACT_ICLR2022.



### Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.09765v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09765v2)
- **Published**: 2022-01-24 15:53:32+00:00
- **Updated**: 2022-02-03 23:13:38+00:00
- **Authors**: Haichao Zhang, Wei Xu, Haonan Yu
- **Comment**: Spotlight paper at the 10th International Conference on Learning
  Representations (ICLR 2022)
- **Journal**: None
- **Summary**: Standard model-free reinforcement learning algorithms optimize a policy that generates the action to be taken in the current time step in order to maximize expected future return. While flexible, it faces difficulties arising from the inefficient exploration due to its single step nature. In this work, we present Generative Planning method (GPM), which can generate actions not only for the current step, but also for a number of future steps (thus termed as generative planning). This brings several benefits to GPM. Firstly, since GPM is trained by maximizing value, the plans generated from it can be regarded as intentional action sequences for reaching high value regions. GPM can therefore leverage its generated multi-step plans for temporally coordinated exploration towards high value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at single step level, whose consistent movement decays exponentially with the number of exploration steps. Secondly, starting from a crude initial plan generator, GPM can refine it to be adaptive to the task, which, in return, benefits future explorations. This is potentially more effective than commonly used action-repeat strategy, which is non-adaptive in its form of plans. Additionally, since the multi-step plan can be interpreted as the intent of the agent from now to a span of time period into the future, it offers a more informative and intuitive signal for interpretation. Experiments are conducted on several benchmark environments and the results demonstrated its effectiveness compared with several baseline methods.



### Patches Are All You Need?
- **Arxiv ID**: http://arxiv.org/abs/2201.09792v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09792v1)
- **Published**: 2022-01-24 16:42:56+00:00
- **Updated**: 2022-01-24 16:42:56+00:00
- **Authors**: Asher Trockman, J. Zico Kolter
- **Comment**: None
- **Journal**: None
- **Summary**: Although convolutional networks have been the dominant architecture for vision tasks for many years, recent experiments have shown that Transformer-based models, most notably the Vision Transformer (ViT), may exceed their performance in some settings. However, due to the quadratic runtime of the self-attention layers in Transformers, ViTs require the use of patch embeddings, which group together small regions of the image into single input features, in order to be applied to larger image sizes. This raises a question: Is the performance of ViTs due to the inherently-more-powerful Transformer architecture, or is it at least partly due to using patches as the input representation? In this paper, we present some evidence for the latter: specifically, we propose the ConvMixer, an extremely simple model that is similar in spirit to the ViT and the even-more-basic MLP-Mixer in that it operates directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network. In contrast, however, the ConvMixer uses only standard convolutions to achieve the mixing steps. Despite its simplicity, we show that the ConvMixer outperforms the ViT, MLP-Mixer, and some of their variants for similar parameter counts and data set sizes, in addition to outperforming classical vision models such as the ResNet. Our code is available at https://github.com/locuslab/convmixer.



### Interpretability in Convolutional Neural Networks for Building Damage Classification in Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2201.10523v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, physics.geo-ph, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2201.10523v1)
- **Published**: 2022-01-24 16:55:56+00:00
- **Updated**: 2022-01-24 16:55:56+00:00
- **Authors**: Thomas Y. Chen
- **Comment**: 8 pages; presented as Spotlight Talk at NeurIPS - Tackling Climate
  Change with Machine Learning workshop 2020
- **Journal**: NeurIPS 2020 Workshop on Tackling Climate Change with Machine
  Learning
- **Summary**: Natural disasters ravage the world's cities, valleys, and shores on a regular basis. Deploying precise and efficient computational mechanisms for assessing infrastructure damage is essential to channel resources and minimize the loss of life. Using a dataset that includes labeled pre- and post- disaster satellite imagery, we take a machine learning-based remote sensing approach and train multiple convolutional neural networks (CNNs) to assess building damage on a per-building basis. We present a novel methodology of interpretable deep learning that seeks to explicitly investigate the most useful modalities of information in the training data to create an accurate classification model. We also investigate which loss functions best optimize these models. Our findings include that ordinal-cross entropy loss is the most optimal criterion for optimization to use and that including the type of disaster that caused the damage in combination with pre- and post-disaster training data most accurately predicts the level of damage caused. Further, we make progress in the qualitative representation of which parts of the images that the model is using to predict damage levels, through gradient-weighted class activation mapping (Grad-CAM). Our research seeks to computationally contribute to aiding in this ongoing and growing humanitarian crisis, heightened by anthropogenic climate change.



### Neural Architecture Searching for Facial Attributes-based Depression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.09799v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.09799v1)
- **Published**: 2022-01-24 16:57:11+00:00
- **Updated**: 2022-01-24 16:57:11+00:00
- **Authors**: Mingzhe Chen, Xi Xiao, Bin Zhang, Xinyu Liu, Runiu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies show that depression can be partially reflected from human facial attributes. Since facial attributes have various data structure and carry different information, existing approaches fail to specifically consider the optimal way to extract depression-related features from each of them, as well as investigates the best fusion strategy. In this paper, we propose to extend Neural Architecture Search (NAS) technique for designing an optimal model for multiple facial attributes-based depression recognition, which can be efficiently and robustly implemented in a small dataset. Our approach first conducts a warmer up step to the feature extractor of each facial attribute, aiming to largely reduce the search space and providing customized architecture, where each feature extractor can be either a Convolution Neural Networks (CNN) or Graph Neural Networks (GNN). Then, we conduct an end-to-end architecture search for all feature extractors and the fusion network, allowing the complementary depression cues to be optimally combined with less redundancy. The experimental results on AVEC 2016 dataset show that the model explored by our approach achieves breakthrough performance with 27\% and 30\% RMSE and MAE improvements over the existing state-of-the-art. In light of these findings, this paper provides solid evidences and a strong baseline for applying NAS to time-series data-based mental health analysis.



### Spectral-PQ: A Novel Spectral Sensitivity-Orientated Perceptual Compression Technique for RGB 4:4:4 Video Data
- **Arxiv ID**: http://arxiv.org/abs/2201.09822v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09822v1)
- **Published**: 2022-01-24 17:39:05+00:00
- **Updated**: 2022-01-24 17:39:05+00:00
- **Authors**: Lee Prangnell, Victor Sanchez
- **Comment**: arXiv admin note: text overlap with arXiv:2005.07928
- **Journal**: None
- **Summary**: There exists an intrinsic relationship between the spectral sensitivity of the Human Visual System (HVS) and colour perception; these intertwined phenomena are often overlooked in perceptual compression research. In general, most previously proposed visually lossless compression techniques exploit luminance (luma) masking including luma spatiotemporal masking, luma contrast masking and luma texture/edge masking. The perceptual relevance of color in a picture is often overlooked, which constitutes a gap in the literature. With regard to the spectral sensitivity phenomenon of the HVS, the color channels of raw RGB 4:4:4 data contain significant color-based psychovisual redundancies. These perceptual redundancies can be quantized via color channel-level perceptual quantization. In this paper, we propose a novel spatiotemporal visually lossless coding method named Spectral Perceptual Quantization (Spectral-PQ). With application for RGB 4:4:4 video data, Spectral-PQ exploits HVS spectral sensitivity-related color masking in addition to spatial masking and temporal masking; the proposed method operates at the Coding Block (CB) level and the Prediction Unit (PU) level in the HEVC standard. Spectral-PQ perceptually adjusts the Quantization Step Size (QStep) at the CB level if high variance spatial data in G, B and R CBs is detected and also if high motion vector magnitudes in PUs are detected. Compared with anchor 1 (HEVC HM 16.17 RExt), Spectral-PQ considerably reduces bitrates with a maximum reduction of approximately 81%. The Mean Opinion Score (MOS) in the subjective evaluations show that Spectral-PQ successfully achieves perceptually lossless quality.



### MMLatch: Bottom-up Top-down Fusion for Multimodal Sentiment Analysis
- **Arxiv ID**: http://arxiv.org/abs/2201.09828v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09828v1)
- **Published**: 2022-01-24 17:48:04+00:00
- **Updated**: 2022-01-24 17:48:04+00:00
- **Authors**: Georgios Paraskevopoulos, Efthymios Georgiou, Alexandros Potamianos
- **Comment**: Accepted, ICASSP 2022
- **Journal**: None
- **Summary**: Current deep learning approaches for multimodal fusion rely on bottom-up fusion of high and mid-level latent modality representations (late/mid fusion) or low level sensory inputs (early fusion). Models of human perception highlight the importance of top-down fusion, where high-level representations affect the way sensory inputs are perceived, i.e. cognition affects perception. These top-down interactions are not captured in current deep learning models. In this work we propose a neural architecture that captures top-down cross-modal interactions, using a feedback mechanism in the forward pass during network training. The proposed mechanism extracts high-level representations for each modality and uses these representations to mask the sensory inputs, allowing the model to perform top-down feature masking. We apply the proposed model for multimodal sentiment recognition on CMU-MOSEI. Our method shows consistent improvements over the well established MulT and over our strong late fusion baseline, achieving state-of-the-art results.



### MonarchNet: Differentiating Monarch Butterflies from Butterflies Species with Similar Phenotypes
- **Arxiv ID**: http://arxiv.org/abs/2201.10526v1
- **DOI**: 10.1096/fasebj.2021.35.S1.05504
- **Categories**: **cs.CV**, cs.AI, q-bio.PE, stat.AP, I.4.9; I.2.1; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2201.10526v1)
- **Published**: 2022-01-24 17:51:42+00:00
- **Updated**: 2022-01-24 17:51:42+00:00
- **Authors**: Thomas Y. Chen
- **Comment**: 5 pages, 2 figures, Proceedings of NeurIPS 2020 - Learning Meaningful
  Representations of Life (LMRL) Workshop. The FASEB Journal
- **Journal**: CVPR 2021 Workshop on CV4Animals (Computer Vision for Animal
  Behavior Tracking and Modeling)
- **Summary**: In recent years, the monarch butterfly's iconic migration patterns have come under threat from a number of factors, from climate change to pesticide use. To track trends in their populations, scientists as well as citizen scientists must identify individuals accurately. This is uniquely key for the study of monarch butterflies because there exist other species of butterfly, such as viceroy butterflies, that are "look-alikes" (coined by the Convention on International Trade in Endangered Species of Wild Fauna and Flora), having similar phenotypes. To tackle this problem and to aid in more efficient identification, we present MonarchNet, the first comprehensive dataset consisting of butterfly imagery for monarchs and five look-alike species. We train a baseline deep-learning classification model to serve as a tool for differentiating monarch butterflies and its various look-alikes. We seek to contribute to the study of biodiversity and butterfly ecology by providing a novel method for computational classification of these particular butterfly species. The ultimate aim is to help scientists track monarch butterfly population and migration trends in the most precise and efficient manner possible.



### A Novel Mix-normalization Method for Generalizable Multi-source Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2201.09846v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09846v2)
- **Published**: 2022-01-24 18:09:38+00:00
- **Updated**: 2022-06-12 15:08:32+00:00
- **Authors**: Lei Qi, Lei Wang, Yinghuan Shi, Xin Geng
- **Comment**: Accepted by IEEE Transactions on Multimedia (TMM)
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) has achieved great success in the supervised scenario. However, it is difficult to directly transfer the supervised model to arbitrary unseen domains due to the model overfitting to the seen source domains. In this paper, we aim to tackle the generalizable multi-source person Re-ID task (i.e., there are multiple available source domains, and the testing domain is unseen during training) from the data augmentation perspective, thus we put forward a novel method, termed MixNorm, which consists of domain-aware mix-normalization (DMN) and domain-ware center regularization (DCR). Different from the conventional data augmentation, the proposed domain-aware mix-normalization to enhance the diversity of features during training from the normalization view of the neural network, which can effectively alleviate the model overfitting to the source domains, so as to boost the generalization capability of the model in the unseen domain. To better learn the domain-invariant model, we further develop the domain-aware center regularization to better map the produced diverse features into the same space. Extensive experiments on multiple benchmark datasets validate the effectiveness of the proposed method and show that the proposed method can outperform the state-of-the-art methods. Besides, further analysis also reveals the superiority of the proposed method.



### Hyperspectral Image Super-resolution with Deep Priors and Degradation Model Inversion
- **Arxiv ID**: http://arxiv.org/abs/2201.09851v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09851v1)
- **Published**: 2022-01-24 18:17:40+00:00
- **Updated**: 2022-01-24 18:17:40+00:00
- **Authors**: Xiuheng Wang, Jie Chen, Cédric Richard
- **Comment**: Proc. IEEE Int. Conf. on Acoust, Speech, Signal Process. (ICASSP), to
  be published. Manuscript submitted October 6th, 2021; revised January 8th,
  2022; accepted January 22nd, 2022
- **Journal**: None
- **Summary**: To overcome inherent hardware limitations of hyperspectral imaging systems with respect to their spatial resolution, fusion-based hyperspectral image (HSI) super-resolution is attracting increasing attention. This technique aims to fuse a low-resolution (LR) HSI and a conventional high-resolution (HR) RGB image in order to obtain an HR HSI. Recently, deep learning architectures have been used to address the HSI super-resolution problem and have achieved remarkable performance. However, they ignore the degradation model even though this model has a clear physical interpretation and may contribute to improve the performance. We address this problem by proposing a method that, on the one hand, makes use of the linear degradation model in the data-fidelity term of the objective function and, on the other hand, utilizes the output of a convolutional neural network for designing a deep prior regularizer in spectral and spatial gradient domains. Experiments show the performance improvement achieved with this strategy.



### RePaint: Inpainting using Denoising Diffusion Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/2201.09865v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09865v4)
- **Published**: 2022-01-24 18:40:15+00:00
- **Updated**: 2022-08-31 05:12:59+00:00
- **Authors**: Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, Luc Van Gool
- **Comment**: We missed out on other diffusion models that work on inpainting. We
  corrected that and apologize for this mistake
- **Journal**: None
- **Summary**: Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image information. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks.   RePaint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions.   Github Repository: git.io/RePaint



### Importance of Preprocessing in Histopathology Image Classification Using Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2201.09867v1
- **DOI**: 10.54569/aair.1016544
- **Categories**: **eess.IV**, cs.CV, 68T01 (Primary), 68T45 (Secondary), I.4.0; I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/2201.09867v1)
- **Published**: 2022-01-24 18:41:29+00:00
- **Updated**: 2022-01-24 18:41:29+00:00
- **Authors**: Nilgun Sengoz, Tuncay Yigit, Ozlem Ozmen, Ali Hakan Isik
- **Comment**: 6 Pages
- **Journal**: None
- **Summary**: The aim of this study is to propose an alternative and hybrid solution method for diagnosing the disease from histopathology images taken from animals with paratuberculosis and intact intestine. In detail, the hybrid method is based on using both image processing and deep learning for better results. Reliable disease detection from histo-pathology images is known as an open problem in medical image processing and alternative solutions need to be developed. In this context, 520 histopathology images were collected in a joint study with Burdur Mehmet Akif Ersoy University, Faculty of Veterinary Medicine, and Department of Pathology. Manually detecting and interpreting these images requires expertise and a lot of processing time. For this reason, veterinarians, especially newly recruited physicians, have a great need for imaging and computer vision systems in the development of detection and treatment methods for this disease. The proposed solution method in this study is to use the CLAHE method and image processing together. After this preprocessing, the diagnosis is made by classifying a convolutional neural network sup-ported by the VGG-16 architecture. This method uses completely original dataset images. Two types of systems were applied for the evaluation parameters. While the F1 Score was 93% in the method classified without data preprocessing, it was 98% in the method that was preprocessed with the CLAHE method.



### Transformers in Medical Imaging: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2201.09873v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.09873v1)
- **Published**: 2022-01-24 18:50:18+00:00
- **Updated**: 2022-01-24 18:50:18+00:00
- **Authors**: Fahad Shamshad, Salman Khan, Syed Waqas Zamir, Muhammad Haris Khan, Munawar Hayat, Fahad Shahbaz Khan, Huazhu Fu
- **Comment**: 41 pages,
  \url{https://github.com/fahadshamshad/awesome-transformers-in-medical-imaging}
- **Journal**: None
- **Summary**: Following unprecedented success on the natural language tasks, Transformers have been successfully applied to several computer vision problems, achieving state-of-the-art results and prompting researchers to reconsider the supremacy of convolutional neural networks (CNNs) as {de facto} operators. Capitalizing on these advances in computer vision, the medical imaging field has also witnessed growing interest for Transformers that can capture global context compared to CNNs with local receptive fields. Inspired from this transition, in this survey, we attempt to provide a comprehensive review of the applications of Transformers in medical imaging covering various aspects, ranging from recently proposed architectural designs to unsolved issues. Specifically, we survey the use of Transformers in medical image segmentation, detection, classification, reconstruction, synthesis, registration, clinical report generation, and other tasks. In particular, for each of these applications, we develop taxonomy, identify application-specific challenges as well as provide insights to solve them, and highlight recent trends. Further, we provide a critical discussion of the field's current state as a whole, including the identification of key challenges, open problems, and outlining promising future directions. We hope this survey will ignite further interest in the community and provide researchers with an up-to-date reference regarding applications of Transformer models in medical imaging. Finally, to cope with the rapid development in this field, we intend to regularly update the relevant latest papers and their open-source implementations at \url{https://github.com/fahadshamshad/awesome-transformers-in-medical-imaging}.



### Euclidean and Affine Curve Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2201.09929v2
- **DOI**: None
- **Categories**: **math.DG**, cs.CV, 53A04, 53A15, 53A55, 34A45, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2201.09929v2)
- **Published**: 2022-01-24 19:37:04+00:00
- **Updated**: 2023-02-06 20:57:03+00:00
- **Authors**: Jose Agudelo, Brooke Dippold, Ian Klein, Alex Kokot, Eric Geiger, Irina Kogan
- **Comment**: This paper is a result of an REU project conducted at the North
  Carolina State University in the Summer and Fall 2020. This version, with
  improved quality of presentation and figures, is accepted to "Involve"
  https://msp.org/involve/about/journal/about.html
- **Journal**: None
- **Summary**: We consider practical aspects of reconstructing planar curves with prescribed Euclidean or affine curvatures. These curvatures are invariant under the special Euclidean group and the equi-affine groups, respectively, and play an important role in computer vision and shape analysis. We discuss and implement algorithms for such reconstruction, and give estimates on how close reconstructed curves are relative to the closeness of their curvatures in appropriate metrics. Several illustrative examples are provided.



### Do Smart Glasses Dream of Sentimental Visions? Deep Emotionship Analysis for Eyewear Devices
- **Arxiv ID**: http://arxiv.org/abs/2201.09933v2
- **DOI**: 10.1145/3517250
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09933v2)
- **Published**: 2022-01-24 19:52:26+00:00
- **Updated**: 2022-04-19 13:01:23+00:00
- **Authors**: Yingying Zhao, Yuhu Chang, Yutian Lu, Yujiang Wang, Mingzhi Dong, Qin Lv, Robert P. Dick, Fan Yang, Tun Lu, Ning Gu, Li Shang
- **Comment**: The EMO-Film dataset is available at:
  https://github.com/MemX-Research/EMOShip
- **Journal**: Proceedings of the ACM on Interactive, Mobile, Wearable and
  Ubiquitous Technologies (IMWUT), Volume 6, Issue 1, Article 38. March 2022
- **Summary**: Emotion recognition in smart eyewear devices is highly valuable but challenging. One key limitation of previous works is that the expression-related information like facial or eye images is considered as the only emotional evidence. However, emotional status is not isolated; it is tightly associated with people's visual perceptions, especially those sentimental ones. However, little work has examined such associations to better illustrate the cause of different emotions. In this paper, we study the emotionship analysis problem in eyewear systems, an ambitious task that requires not only classifying the user's emotions but also semantically understanding the potential cause of such emotions. To this end, we devise EMOShip, a deep-learning-based eyewear system that can automatically detect the wearer's emotional status and simultaneously analyze its associations with semantic-level visual perceptions. Experimental studies with 20 participants demonstrate that, thanks to the emotionship awareness, EMOShip not only achieves superior emotion recognition accuracy over existing methods (80.2% vs. 69.4%), but also provides a valuable understanding of the cause of emotions. Pilot studies with 20 participants further motivate the potential use of EMOShip to empower emotion-aware applications, such as emotionship self-reflection and emotionship life-logging.



### What is the cost of adding a constraint in linear least squares?
- **Arxiv ID**: http://arxiv.org/abs/2201.09935v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2201.09935v1)
- **Published**: 2022-01-24 19:56:31+00:00
- **Updated**: 2022-01-24 19:56:31+00:00
- **Authors**: Ramakrishna Kakarala, Jun Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Although the theory of constrained least squares (CLS) estimation is well known, it is usually applied with the view that the constraints to be imposed are unavoidable. However, there are cases in which constraints are optional. For example, in camera color calibration, one of several possible color processing systems is obtained if a constraint on the row sums of a desired color correction matrix is imposed; in this example, it is not clear a priori whether imposing the constraint leads to better system performance. In this paper, we derive an exact expression connecting the constraint to the increase in fitting error obtained from imposing it. As another contribution, we show how to determine projection matrices that separate the measured data into two components: the first component drives up the fitting error due to imposing a constraint, and the second component is unaffected by the constraint. We demonstrate the use of these results in the color calibration problem.



### A Deep Learning Approach for the Detection of COVID-19 from Chest X-Ray Images using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.09952v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09952v1)
- **Published**: 2022-01-24 21:12:25+00:00
- **Updated**: 2022-01-24 21:12:25+00:00
- **Authors**: Aditya Saxena, Shamsheer Pal Singh
- **Comment**: None
- **Journal**: None
- **Summary**: The COVID-19 (coronavirus) is an ongoing pandemic caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The virus was first identified in mid-December 2019 in the Hubei province of Wuhan, China and by now has spread throughout the planet with more than 75.5 million confirmed cases and more than 1.67 million deaths. With limited number of COVID-19 test kits available in medical facilities, it is important to develop and implement an automatic detection system as an alternative diagnosis option for COVID-19 detection that can used on a commercial scale. Chest X-ray is the first imaging technique that plays an important role in the diagnosis of COVID-19 disease. Computer vision and deep learning techniques can help in determining COVID-19 virus with Chest X-ray Images. Due to the high availability of large-scale annotated image datasets, great success has been achieved using convolutional neural network for image analysis and classification. In this research, we have proposed a deep convolutional neural network trained on five open access datasets with binary output: Normal and Covid. The performance of the model is compared with four pre-trained convolutional neural network-based models (COVID-Net, ResNet18, ResNet and MobileNet-V2) and it has been seen that the proposed model provides better accuracy on the validation set as compared to the other four pre-trained models. This research work provides promising results which can be further improvise and implement on a commercial scale.



### Attacks and Defenses for Free-Riders in Multi-Discriminator GAN
- **Arxiv ID**: http://arxiv.org/abs/2201.09967v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2201.09967v1)
- **Published**: 2022-01-24 21:38:05+00:00
- **Updated**: 2022-01-24 21:38:05+00:00
- **Authors**: Zilong Zhao, Jiyue Huang, Stefanie Roos, Lydia Y. Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are increasingly adopted by the industry to synthesize realistic images. Due to data not being centrally available, Multi-Discriminator (MD)-GANs training framework employs multiple discriminators that have direct access to the real data. Distributedly training a joint GAN model entails the risk of free-riders, i.e., participants that aim to benefit from the common model while only pretending to participate in the training process. In this paper, we conduct the first characterization study of the impact of free-riders on MD-GAN. Based on two production prototypes of MD-GAN, we find that free-riders drastically reduce the ability of MD-GANs to produce images that are indistinguishable from real data, i.e., they increase the FID score -- the standard measure to assess the quality of generated images. To mitigate the model degradation, we propose a defense strategy against free-riders in MD-GAN, termed DFG. DFG distinguishes free-riders and benign participants through periodic probing and clustering of discriminators' responses based on a reference response of free-riders, which then allows the generator to exclude the detected free-riders from the training. Furthermore, we extend our defense, termed DFG+, to enable discriminators to filter out free-riders at the variant of MD-GAN that allows peer exchanges of discriminators networks. Extensive evaluation on various scenarios of free-riders, MD-GAN architecture, and three datasets show that our defenses effectively detect free-riders. With 1 to 5 free-riders, DFG and DFG+ averagely decreases FID by 5.22% to 11.53% for CIFAR10 and 5.79% to 13.22% for CIFAR100 in comparison to an attack without defense. In a shell, the proposed DFG(+) can effectively defend against free-riders without affecting benign clients at a negligible computation overhead.



### ImpliCity: City Modeling from Satellite Images with Deep Implicit Occupancy Fields
- **Arxiv ID**: http://arxiv.org/abs/2201.09968v3
- **DOI**: 10.5194/isprs-annals-V-2-2022-193-2022
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.09968v3)
- **Published**: 2022-01-24 21:40:16+00:00
- **Updated**: 2022-05-06 14:16:01+00:00
- **Authors**: Corinne Stucker, Bingxin Ke, Yuanwen Yue, Shengyu Huang, Iro Armeni, Konrad Schindler
- **Comment**: Accepted for publication in the International Annals of the
  Photogrammetry, Remote Sensing and Spatial Information Sciences (camera-ready
  version including keywords + supplementary material)
- **Journal**: ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., V-2-2022,
  193-201, 2022
- **Summary**: High-resolution optical satellite sensors, combined with dense stereo algorithms, have made it possible to reconstruct 3D city models from space. However, these models are, in practice, rather noisy and tend to miss small geometric features that are clearly visible in the images. We argue that one reason for the limited quality may be a too early, heuristic reduction of the triangulated 3D point cloud to an explicit height field or surface mesh. To make full use of the point cloud and the underlying images, we introduce ImpliCity, a neural representation of the 3D scene as an implicit, continuous occupancy field, driven by learned embeddings of the point cloud and a stereo pair of ortho-photos. We show that this representation enables the extraction of high-quality DSMs: with image resolution 0.5$\,$m, ImpliCity reaches a median height error of $\approx\,$0.7$\,$m and outperforms competing methods, especially w.r.t. building reconstruction, featuring intricate roof details, smooth surfaces, and straight, regular outlines.



### COVID-19 Detection Using CT Image Based On YOLOv5 Network
- **Arxiv ID**: http://arxiv.org/abs/2201.09972v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09972v1)
- **Published**: 2022-01-24 21:50:58+00:00
- **Updated**: 2022-01-24 21:50:58+00:00
- **Authors**: Ruyi Qu, Yi Yang, Yuwei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Computer aided diagnosis (CAD) increases diagnosis efficiency, helping doctors providing a quick and confident diagnosis, it has played an important role in the treatment of COVID19. In our task, we solve the problem about abnormality detection and classification. The dataset provided by Kaggle platform and we choose YOLOv5 as our model. We introduce some methods on objective detection in the related work section, the objection detection can be divided into two streams: onestage and two stage. The representational model are Faster RCNN and YOLO series. Then we describe the YOLOv5 model in the detail. Compared Experiments and results are shown in section IV. We choose mean average precision (mAP) as our experiments' metrics, and the higher (mean) mAP is, the better result the model will gain. mAP@0.5 of our YOLOv5s is 0.623 which is 0.157 and 0.101 higher than Faster RCNN and EfficientDet respectively.



### The Vehicle Trajectory Prediction Based on ResNet and EfficientNet Model
- **Arxiv ID**: http://arxiv.org/abs/2201.09973v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.09973v1)
- **Published**: 2022-01-24 22:00:32+00:00
- **Updated**: 2022-01-24 22:00:32+00:00
- **Authors**: Ruyi Qu, Shukai Huang, Jiexuan Zhou, ChenXi Fan, ZhiYuan Yan
- **Comment**: None
- **Journal**: None
- **Summary**: At present, a major challenge for the application of automatic driving technology is the accurate prediction of vehicle trajectory. With the vigorous development of computer technology and the emergence of convolution depth neural network, the accuracy of prediction results has been improved. But, the depth, width of the network and image resolution are still important reasons that restrict the accuracy of the model and the prediction results. The main innovation of this paper is the combination of RESNET network and efficient net network, which not only greatly increases the network depth, but also comprehensively changes the choice of network width and image resolution, so as to make the model performance better, but also save computing resources as much as possible. The experimental results also show that our proposed model obtains the optimal prediction results. Specifically, the loss value of our method is separately 4 less and 2.1 less than that of resnet and efficientnet method.



### Neural Manifold Clustering and Embedding
- **Arxiv ID**: http://arxiv.org/abs/2201.10000v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.10000v1)
- **Published**: 2022-01-24 23:13:37+00:00
- **Updated**: 2022-01-24 23:13:37+00:00
- **Authors**: Zengyi Li, Yubei Chen, Yann LeCun, Friedrich T. Sommer
- **Comment**: None
- **Journal**: None
- **Summary**: Given a union of non-linear manifolds, non-linear subspace clustering or manifold clustering aims to cluster data points based on manifold structures and also learn to parameterize each manifold as a linear subspace in a feature space. Deep neural networks have the potential to achieve this goal under highly non-linear settings given their large capacity and flexibility. We argue that achieving manifold clustering with neural networks requires two essential ingredients: a domain-specific constraint that ensures the identification of the manifolds, and a learning algorithm for embedding each manifold to a linear subspace in the feature space. This work shows that many constraints can be implemented by data augmentation. For subspace feature learning, Maximum Coding Rate Reduction (MCR$^2$) objective can be used. Putting them together yields {\em Neural Manifold Clustering and Embedding} (NMCE), a novel method for general purpose manifold clustering, which significantly outperforms autoencoder-based deep subspace clustering. Further, on more challenging natural image datasets, NMCE can also outperform other algorithms specifically designed for clustering. Qualitatively, we demonstrate that NMCE learns a meaningful and interpretable feature space. As the formulation of NMCE is closely related to several important Self-supervised learning (SSL) methods, we believe this work can help us build a deeper understanding on SSL representation learning.



