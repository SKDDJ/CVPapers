# Arxiv Papers in cs.CV on 2022-01-25
### Automatic Recognition and Digital Documentation of Cultural Heritage Hemispherical Domes using Images
- **Arxiv ID**: http://arxiv.org/abs/2201.10015v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10015v2)
- **Published**: 2022-01-25 00:14:04+00:00
- **Updated**: 2022-04-06 12:12:35+00:00
- **Authors**: Reza Maalek, Shahrokh Maalek
- **Comment**: None
- **Journal**: None
- **Summary**: Advancements in optical metrology has enabled documentation of dense 3D point clouds of cultural heritage sites. For large scale and continuous digital documentation, processing of dense 3D point clouds becomes computationally cumbersome, and often requires additional hardware for data management, increasing the time cost, and complexity of projects. To this end, this manuscript presents an original approach to generate fast and reliable semantic digital models of heritage hemispherical domes using only two images. New closed formulations were derived to establish the relationships between spheres and their projected ellipses onto images, which fostered the development of a new automatic framework for as-built generation of spheres. The effectiveness of the proposed method was evaluated under both laboratory and real-world datasets. The results revealed that the proposed method achieved as-built modeling accuracy of around 6mm, while improving the computation time by a factor of 7, when compared to established point cloud processing methods.



### PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.10029v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.10029v2)
- **Published**: 2022-01-25 01:07:32+00:00
- **Updated**: 2022-06-17 06:30:32+00:00
- **Authors**: Santhosh Kumar Ramakrishnan, Devendra Singh Chaplot, Ziad Al-Halah, Jitendra Malik, Kristen Grauman
- **Comment**: 8 pages + supplementary. Accepted in CVPR 2022
- **Journal**: None
- **Summary**: State-of-the-art approaches to ObjectGoal navigation rely on reinforcement learning and typically require significant computational resources and time for learning. We propose Potential functions for ObjectGoal Navigation with Interaction-free learning (PONI), a modular approach that disentangles the skills of `where to look?' for an object and `how to navigate to (x, y)?'. Our key insight is that `where to look?' can be treated purely as a perception problem, and learned without environment interactions. To address this, we propose a network that predicts two complementary potential functions conditioned on a semantic map and uses them to decide where to look for an unseen object. We train the potential function network using supervised learning on a passive dataset of top-down semantic maps, and integrate it into a modular framework to perform ObjectGoal navigation. Experiments on Gibson and Matterport3D demonstrate that our method achieves the state-of-the-art for ObjectGoal navigation while incurring up to 1,600x less computational cost for training. Code and pre-trained models are available: https://vision.cs.utexas.edu/projects/poni/



### Self-Supervised Point Cloud Registration with Deep Versatile Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2201.10034v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10034v2)
- **Published**: 2022-01-25 01:22:48+00:00
- **Updated**: 2023-04-13 01:27:14+00:00
- **Authors**: Dongrui Liu, Chuanchuan Chen, Changqing Xu, Robert Qiu, Lei Chu
- **Comment**: None
- **Journal**: None
- **Summary**: As a fundamental yet challenging problem in intelligent transportation systems, point cloud registration attracts vast attention and has been attained with various deep learning-based algorithms. The unsupervised registration algorithms take advantage of deep neural network-enabled novel representation learning while requiring no human annotations, making them applicable to industrial applications. However, unsupervised methods mainly depend on global descriptors, which ignore the high-level representations of local geometries. In this paper, we propose to jointly use both global and local descriptors to register point clouds in a self-supervised manner, which is motivated by a critical observation that all local geometries of point clouds are transformed consistently under the same transformation. Therefore, local geometries can be employed to enhance the representation ability of the feature extraction module. Moreover, the proposed local descriptor is flexible and can be integrated into most existing registration methods and improve their performance. Besides, we also utilize point cloud reconstruction and normal estimation to enhance the transformation awareness of global and local descriptors. Lastly, extensive experimental results on one synthetic and three real-world datasets demonstrate that our method outperforms existing state-of-art unsupervised registration methods and even surpasses supervised ones in some cases. Robustness and computational efficiency evaluations also indicate that the proposed method applies to intelligent vehicles.



### Event-based Video Reconstruction via Potential-assisted Spiking Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2201.10943v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10943v3)
- **Published**: 2022-01-25 02:05:20+00:00
- **Updated**: 2022-03-30 15:24:17+00:00
- **Authors**: Lin Zhu, Xiao Wang, Yi Chang, Jianing Li, Tiejun Huang, Yonghong Tian
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Neuromorphic vision sensor is a new bio-inspired imaging paradigm that reports asynchronous, continuously per-pixel brightness changes called `events' with high temporal resolution and high dynamic range. So far, the event-based image reconstruction methods are based on artificial neural networks (ANN) or hand-crafted spatiotemporal smoothing techniques. In this paper, we first implement the image reconstruction work via fully spiking neural network (SNN) architecture. As the bio-inspired neural networks, SNNs operating with asynchronous binary spikes distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. We propose a novel Event-based Video reconstruction framework based on a fully Spiking Neural Network (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and Membrane Potential (MP) neuron. We find that the spiking neurons have the potential to store useful temporal information (memory) to complete such time-dependent tasks. Furthermore, to better utilize the temporal information, we propose a hybrid potential-assisted framework (PA-EVSNN) using the membrane potential of spiking neuron. The proposed neuron is referred as Adaptive Membrane Potential (AMP) neuron, which adaptively updates the membrane potential according to the input spikes. The experimental results demonstrate that our models achieve comparable performance to ANN-based models on IJRR, MVSEC, and HQF datasets. The energy consumptions of EVSNN and PA-EVSNN are 19.36$\times$ and 7.75$\times$ more computationally efficient than their ANN architectures, respectively.



### Are Commercial Face Detection Models as Biased as Academic Models?
- **Arxiv ID**: http://arxiv.org/abs/2201.10047v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.10047v2)
- **Published**: 2022-01-25 02:21:42+00:00
- **Updated**: 2022-11-30 02:14:23+00:00
- **Authors**: Samuel Dooley, George Z. Wei, Tom Goldstein, John P. Dickerson
- **Comment**: This preprint and arXiv:2108.12508 were combined and a more rigorous
  analysis added to result in the NeurIPS Datasets & Benchmark 2022 paper
  arXiv:2211.15937
- **Journal**: None
- **Summary**: As facial recognition systems are deployed more widely, scholars and activists have studied their biases and harms. Audits are commonly used to accomplish this and compare the algorithmic facial recognition systems' performance against datasets with various metadata labels about the subjects of the images. Seminal works have found discrepancies in performance by gender expression, age, perceived race, skin type, etc. These studies and audits often examine algorithms which fall into two categories: academic models or commercial models. We present a detailed comparison between academic and commercial face detection systems, specifically examining robustness to noise. We find that state-of-the-art academic face detection models exhibit demographic disparities in their noise robustness, specifically by having statistically significant decreased performance on older individuals and those who present their gender in a masculine manner. When we compare the size of these disparities to that of commercial models, we conclude that commercial models - in contrast to their relatively larger development budget and industry-level fairness commitments - are always as biased or more biased than an academic model.



### ViT-HGR: Vision Transformer-based Hand Gesture Recognition from High Density Surface EMG Signals
- **Arxiv ID**: http://arxiv.org/abs/2201.10060v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2201.10060v1)
- **Published**: 2022-01-25 02:42:50+00:00
- **Updated**: 2022-01-25 02:42:50+00:00
- **Authors**: Mansooreh Montazerin, Soheil Zabihi, Elahe Rahimian, Arash Mohammadi, Farnoosh Naderkhani
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, there has been a surge of significant interest on application of Deep Learning (DL) models to autonomously perform hand gesture recognition using surface Electromyogram (sEMG) signals. DL models are, however, mainly designed to be applied on sparse sEMG signals. Furthermore, due to their complex structure, typically, we are faced with memory constraints; require large training times and a large number of training samples, and; there is the need to resort to data augmentation and/or transfer learning. In this paper, for the first time (to the best of our knowledge), we investigate and design a Vision Transformer (ViT) based architecture to perform hand gesture recognition from High Density (HD-sEMG) signals. Intuitively speaking, we capitalize on the recent breakthrough role of the transformer architecture in tackling different complex problems together with its potential for employing more input parallelization via its attention mechanism. The proposed Vision Transformer-based Hand Gesture Recognition (ViT-HGR) framework can overcome the aforementioned training time problems and can accurately classify a large number of hand gestures from scratch without any need for data augmentation and/or transfer learning. The efficiency of the proposed ViT-HGR framework is evaluated using a recently-released HD-sEMG dataset consisting of 65 isometric hand gestures. Our experiments with 64-sample (31.25 ms) window size yield average test accuracy of 84.62 +/- 3.07%, where only 78, 210 number of parameters is utilized. The compact structure of the proposed ViT-based ViT-HGR framework (i.e., having significantly reduced number of trainable parameters) shows great potentials for its practical application for prosthetic control.



### Splatting-based Synthesis for Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2201.10075v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10075v2)
- **Published**: 2022-01-25 03:31:15+00:00
- **Updated**: 2022-10-26 02:21:04+00:00
- **Authors**: Simon Niklaus, Ping Hu, Jiawen Chen
- **Comment**: WACV 2023, http://sniklaus.com/splatsyn
- **Journal**: None
- **Summary**: Frame interpolation is an essential video processing technique that adjusts the temporal resolution of an image sequence. While deep learning has brought great improvements to the area of video frame interpolation, techniques that make use of neural networks can typically not easily be deployed in practical applications like a video editor since they are either computationally too demanding or fail at high resolutions. In contrast, we propose a deep learning approach that solely relies on splatting to synthesize interpolated frames. This splatting-based synthesis for video frame interpolation is not only much faster than similar approaches, especially for multi-frame interpolation, but can also yield new state-of-the-art results at high resolutions.



### Real-time automatic polyp detection in colonoscopy using feature enhancement module and spatiotemporal similarity correlation unit
- **Arxiv ID**: http://arxiv.org/abs/2201.10079v1
- **DOI**: 10.1016/j.bspc.2021.102503
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10079v1)
- **Published**: 2022-01-25 03:40:30+00:00
- **Updated**: 2022-01-25 03:40:30+00:00
- **Authors**: Jianwei Xu, Ran Zhao, Yizhou Yu, Qingwei Zhang, Xianzhang Bian, Jun Wang, Zhizheng Ge, Dahong Qian
- **Comment**: This paper has been accepted by Biomedical Signal Processing and
  Control. Please cite the paper as Xu, J., Zhao, R., Yu, Y., Zhang, Q., Bian,
  X., Wang, J., Ge, Z., Qian, D., 2021. Real-time automatic polyp detection in
  colonoscopy using feature enhancement module and spatiotemporal similarity
  correlation unit. Biomedical Signal Processing and Control 66, 102503
- **Journal**: Biomedical Signal Processing and Control, vol. 66, p. 102503, Apr.
  2021
- **Summary**: Automatic detection of polyps is challenging because different polyps vary greatly, while the changes between polyps and their analogues are small. The state-of-the-art methods are based on convolutional neural networks (CNNs). However, they may fail due to lack of training data, resulting in high rates of missed detection and false positives (FPs). In order to solve these problems, our method combines the two-dimensional (2-D) CNN-based real-time object detector network with spatiotemporal information. Firstly, we use a 2-D detector network to detect static images and frames, and based on the detector network, we propose two feature enhancement modules-the FP Relearning Module (FPRM) to make the detector network learning more about the features of FPs for higher precision, and the Image Style Transfer Module (ISTM) to enhance the features of polyps for sensitivity improvement. In video detection, we integrate spatiotemporal information, which uses Structural Similarity (SSIM) to measure the similarity between video frames. Finally, we propose the Inter-frame Similarity Correlation Unit (ISCU) to combine the results obtained by the detector network and frame similarity to make the final decision. We verify our method on both private databases and publicly available databases. Experimental results show that these modules and units provide a performance improvement compared with the baseline method. Comparison with the state-of-the-art methods shows that the proposed method outperforms the existing ones which can meet real-time constraints. It's demonstrated that our method provides a performance improvement in sensitivity, precision and specificity, and has great potential to be applied in clinical colonoscopy.



### Revisiting L1 Loss in Super-Resolution: A Probabilistic View and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2201.10084v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10084v2)
- **Published**: 2022-01-25 04:04:44+00:00
- **Updated**: 2023-04-09 02:27:35+00:00
- **Authors**: Xiangyu He, Jian Cheng
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Super-resolution as an ill-posed problem has many high-resolution candidates for a low-resolution input. However, the popular $\ell_1$ loss used to best fit the given HR image fails to consider this fundamental property of non-uniqueness in image restoration. In this work, we fix the missing piece in $\ell_1$ loss by formulating super-resolution with neural networks as a probabilistic model. It shows that $\ell_1$ loss is equivalent to a degraded likelihood function that removes the randomness from the learning process. By introducing a data-adaptive random variable, we present a new objective function that aims at minimizing the expectation of the reconstruction error over all plausible solutions. The experimental results show consistent improvements on mainstream architectures, with no extra parameter or computing cost at inference time.



### A Classical Approach to Handcrafted Feature Extraction Techniques for Bangla Handwritten Digit Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.10102v1
- **DOI**: 10.1109/ICECIT54077.2021.9641406
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.10102v1)
- **Published**: 2022-01-25 05:27:57+00:00
- **Updated**: 2022-01-25 05:27:57+00:00
- **Authors**: Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj
- **Comment**: None
- **Journal**: None
- **Summary**: Bangla Handwritten Digit recognition is a significant step forward in the development of Bangla OCR. However, intricate shape, structural likeness and distinctive composition style of Bangla digits makes it relatively challenging to distinguish. Thus, in this paper, we benchmarked four rigorous classifiers to recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector Machine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT) based on three handcrafted feature extraction techniques: Histogram of Oriented Gradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly available Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW. Here, handcrafted feature extraction methods are used to extract features from the dataset image, which are then utilized to train machine learning classifiers to identify Bangla handwritten digits. We further fine-tuned the hyperparameters of the classification algorithms in order to acquire the finest Bangla handwritten digits recognition performance from these algorithms, and among all the models we employed, the HOG features combined with SVM model (HOG+SVM) attained the best performance metrics across all datasets. The recognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and BDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well as we compared the model performance with recent state-of-art methods.



### ARPD: Anchor-free Rotation-aware People Detection using Topview Fisheye Camera
- **Arxiv ID**: http://arxiv.org/abs/2201.10107v1
- **DOI**: 10.1109/AVSS52988.2021.9663768
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10107v1)
- **Published**: 2022-01-25 05:49:50+00:00
- **Updated**: 2022-01-25 05:49:50+00:00
- **Authors**: Quan Nguyen Minh, Bang Le Van, Can Nguyen, Anh Le, Viet Dung Nguyen
- **Comment**: 2021 17th IEEE International Conference on Advanced Video and Signal
  Based Surveillance (AVSS)
- **Journal**: None
- **Summary**: People detection in top-view, fish-eye images is challenging as people in fish-eye images often appear in arbitrary directions and are distorted differently. Due to this unique radial geometry, axis-aligned people detectors often work poorly on fish-eye frames. Recent works account for this variability by modifying existing anchor-based detectors or relying on complex pre/post-processing. Anchor-based methods spread a set of pre-defined bounding boxes on the input image, most of which are invalid. In addition to being inefficient, this approach could lead to a significant imbalance between the positive and negative anchor boxes. In this work, we propose ARPD, a single-stage anchor-free fully convolutional network to detect arbitrarily rotated people in fish-eye images. Our network uses keypoint estimation to find the center point of each object and regress the object's other properties directly. To capture the various orientation of people in fish-eye cameras, in addition to the center and size, ARPD also predicts the angle of each bounding box. We also propose a periodic loss function that accounts for angle periodicity and relieves the difficulty of learning small-angle oscillations. Experimental results show that our method competes favorably with state-of-the-art algorithms while running significantly faster.



### A Hybrid Quantum-Classical Algorithm for Robust Fitting
- **Arxiv ID**: http://arxiv.org/abs/2201.10110v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10110v4)
- **Published**: 2022-01-25 05:59:24+00:00
- **Updated**: 2022-06-27 06:13:11+00:00
- **Authors**: Anh-Dzung Doan, Michele Sasdelli, David Suter, Tat-Jun Chin
- **Comment**: IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR)
  2022
- **Journal**: None
- **Summary**: Fitting geometric models onto outlier contaminated data is provably intractable. Many computer vision systems rely on random sampling heuristics to solve robust fitting, which do not provide optimality guarantees and error bounds. It is therefore critical to develop novel approaches that can bridge the gap between exact solutions that are costly, and fast heuristics that offer no quality assurances. In this paper, we propose a hybrid quantum-classical algorithm for robust fitting. Our core contribution is a novel robust fitting formulation that solves a sequence of integer programs and terminates with a global solution or an error bound. The combinatorial subproblems are amenable to a quantum annealer, which helps to tighten the bound efficiently. While our usage of quantum computing does not surmount the fundamental intractability of robust fitting, by providing error bounds our algorithm is a practical improvement over randomised heuristics. Moreover, our work represents a concrete application of quantum computing in computer vision. We present results obtained using an actual quantum computer (D-Wave Advantage) and via simulation. Source code: https://github.com/dadung/HQC-robust-fitting



### SURDS: Self-Supervised Attention-guided Reconstruction and Dual Triplet Loss for Writer Independent Offline Signature Verification
- **Arxiv ID**: http://arxiv.org/abs/2201.10138v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10138v2)
- **Published**: 2022-01-25 07:26:55+00:00
- **Updated**: 2022-06-26 05:58:34+00:00
- **Authors**: Soumitri Chattopadhyay, Siladittya Manna, Saumik Bhattacharya, Umapada Pal
- **Comment**: Accepted at ICPR 2022
- **Journal**: None
- **Summary**: Offline Signature Verification (OSV) is a fundamental biometric task across various forensic, commercial and legal applications. The underlying task at hand is to carefully model fine-grained features of the signatures to distinguish between genuine and forged ones, which differ only in minute deformities. This makes OSV more challenging compared to other verification problems. In this work, we propose a two-stage deep learning framework that leverages self-supervised representation learning as well as metric learning for writer-independent OSV. First, we train an image reconstruction network using an encoder-decoder architecture that is augmented by a 2D spatial attention mechanism using signature image patches. Next, the trained encoder backbone is fine-tuned with a projector head using a supervised metric learning framework, whose objective is to optimize a novel dual triplet loss by sampling negative samples from both within the same writer class as well as from other writers. The intuition behind this is to ensure that a signature sample lies closer to its positive counterpart compared to negative samples from both intra-writer and cross-writer sets. This results in robust discriminative learning of the embedding space. To the best of our knowledge, this is the first work of using self-supervised learning frameworks for OSV. The proposed two-stage framework has been evaluated on two publicly available offline signature datasets and compared with various state-of-the-art methods. It is noted that the proposed method provided promising results outperforming several existing pieces of work. The code is publicly available at: https://github.com/soumitri2001/SURDS-SSL-OSV



### Riemannian Local Mechanism for SPD Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.10145v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10145v3)
- **Published**: 2022-01-25 07:39:25+00:00
- **Updated**: 2023-05-19 07:40:57+00:00
- **Authors**: Ziheng Chen, Tianyang Xu, Xiao-Jun Wu, Rui Wang, Zhiwu Huang, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: The Symmetric Positive Definite (SPD) matrices have received wide attention for data representation in many scientific areas. Although there are many different attempts to develop effective deep architectures for data processing on the Riemannian manifold of SPD matrices, very few solutions explicitly mine the local geometrical information in deep SPD feature representations. Given the great success of local mechanisms in Euclidean methods, we argue that it is of utmost importance to ensure the preservation of local geometric information in the SPD networks. We first analyse the convolution operator commonly used for capturing local information in Euclidean deep networks from the perspective of a higher level of abstraction afforded by category theory. Based on this analysis, we define the local information in the SPD manifold and design a multi-scale submanifold block for mining local geometry. Experiments involving multiple visual tasks validate the effectiveness of our approach. The supplement and source code can be found in https://github.com/GitZH-Chen/MSNet.git.



### TGFuse: An Infrared and Visible Image Fusion Approach Based on Transformer and Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2201.10147v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10147v2)
- **Published**: 2022-01-25 07:43:30+00:00
- **Updated**: 2022-02-04 03:09:59+00:00
- **Authors**: Dongyu Rao, Xiao-Jun Wu, Tianyang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: The end-to-end image fusion framework has achieved promising performance, with dedicated convolutional networks aggregating the multi-modal local appearance. However, long-range dependencies are directly neglected in existing CNN fusion approaches, impeding balancing the entire image-level perception for complex scenario fusion. In this paper, therefore, we propose an infrared and visible image fusion algorithm based on a lightweight transformer module and adversarial learning. Inspired by the global interaction power, we use the transformer technique to learn the effective global fusion relations. In particular, shallow features extracted by CNN are interacted in the proposed transformer fusion module to refine the fusion relationship within the spatial scope and across channels simultaneously. Besides, adversarial learning is designed in the training process to improve the output discrimination via imposing competitive consistency from the inputs, reflecting the specific characteristics in infrared and visible images. The experimental performance demonstrates the effectiveness of the proposed modules, with superior improvement against the state-of-the-art, generalising a novel paradigm via transformer and adversarial learning in the fusion task.



### Unsupervised Image Fusion Method based on Feature Mutual Mapping
- **Arxiv ID**: http://arxiv.org/abs/2201.10152v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10152v2)
- **Published**: 2022-01-25 07:50:14+00:00
- **Updated**: 2022-01-29 12:27:27+00:00
- **Authors**: Dongyu Rao, Xiao-Jun Wu, Tianyang Xu, Guoyang Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based image fusion approaches have obtained wide attention in recent years, achieving promising performance in terms of visual perception. However, the fusion module in the current deep learning-based methods suffers from two limitations, \textit{i.e.}, manually designed fusion function, and input-independent network learning. In this paper, we propose an unsupervised adaptive image fusion method to address the above issues. We propose a feature mutual mapping fusion module and dual-branch multi-scale autoencoder. More specifically, we construct a global map to measure the connections of pixels between the input source images. % The found mapping relationship guides the image fusion. Besides, we design a dual-branch multi-scale network through sampling transformation to extract discriminative image features. We further enrich feature representations of different scales through feature aggregation in the decoding process. Finally, we propose a modified loss function to train the network with efficient convergence property. Through sufficient training on infrared and visible image data sets, our method also shows excellent generalized performance in multi-focus and medical image fusion. Our method achieves superior performance in both visual perception and objective evaluation. Experiments prove that the performance of our proposed method on a variety of image fusion tasks surpasses other state-of-the-art methods, proving the effectiveness and versatility of our approach.



### Semantically Video Coding: Instill Static-Dynamic Clues into Structured Bitstream for AI Tasks
- **Arxiv ID**: http://arxiv.org/abs/2201.10162v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2201.10162v2)
- **Published**: 2022-01-25 08:06:32+00:00
- **Updated**: 2022-05-09 02:54:11+00:00
- **Authors**: Xin Jin, Ruoyu Feng, Simeng Sun, Runsen Feng, Tianyu He, Zhibo Chen
- **Comment**: 21 pages, 12 figures
- **Journal**: None
- **Summary**: Traditional media coding schemes typically encode image/video into a semantic-unknown binary stream, which fails to directly support downstream intelligent tasks at the bitstream level. Semantically Structured Image Coding (SSIC) framework makes the first attempt to enable decoding-free or partial-decoding image intelligent task analysis via a Semantically Structured Bitstream (SSB). However, the SSIC only considers image coding and its generated SSB only contains the static object information. In this paper, we extend the idea of semantically structured coding from video coding perspective and propose an advanced Semantically Structured Video Coding (SSVC) framework to support heterogeneous intelligent applications. Video signals contain more rich dynamic motion information and exist more redundancy due to the similarity between adjacent frames. Thus, we present a reformulation of semantically structured bitstream (SSB) in SSVC which contains both static object characteristics and dynamic motion clues. Specifically, we introduce optical flow to encode continuous motion information and reduce cross-frame redundancy via a predictive coding architecture, then the optical flow and residual information are reorganized into SSB, which enables the proposed SSVC could better adaptively support video-based downstream intelligent applications. Extensive experiments demonstrate that the proposed SSVC framework could directly support multiple intelligent tasks just depending on a partially decoded bitstream. This avoids the full bitstream decompression and thus significantly saves bitrate/bandwidth consumption for intelligent analytics. We verify this point on the tasks of image object detection, pose estimation, video action recognition, video object segmentation, etc.



### Dense Pixel-Labeling for Reverse-Transfer and Diagnostic Learning on Lung Ultrasound for COVID-19 and Pneumonia Detection
- **Arxiv ID**: http://arxiv.org/abs/2201.10166v1
- **DOI**: 10.1109/ISBI48211.2021.9433826
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.10166v1)
- **Published**: 2022-01-25 08:19:11+00:00
- **Updated**: 2022-01-25 08:19:11+00:00
- **Authors**: Gautam Rajendrakumar Gare, Andrew Schoenling, Vipin Philip, Hai V Tran, Bennett P deBoisblanc, Ricardo Luis Rodriguez, John Michael Galeotti
- **Comment**: Published in 2021 IEEE 18th International Symposium on Biomedical
  Imaging (ISBI) \copyright 2021 IEEE
- **Journal**: 2021 IEEE 18th International Symposium on Biomedical Imaging
  (ISBI), 2021, pp. 1406-1410
- **Summary**: We propose using a pre-trained segmentation model to perform diagnostic classification in order to achieve better generalization and interpretability, terming the technique reverse-transfer learning. We present an architecture to convert segmentation models to classification models. We compare and contrast dense vs sparse segmentation labeling and study its impact on diagnostic classification. We compare the performance of U-Net trained with dense and sparse labels to segment A-lines, B-lines, and Pleural lines on a custom dataset of lung ultrasound scans from 4 patients. Our experiments show that dense labels help reduce false positive detection. We study the classification capability of the dense and sparse trained U-Net and contrast it with a non-pretrained U-Net, to detect and differentiate COVID-19 and Pneumonia on a large ultrasound dataset of about 40k curvilinear and linear probe images. Our segmentation-based models perform better classification when using pretrained segmentation weights, with the dense-label pretrained U-Net performing the best.



### Explore-And-Match: Bridging Proposal-Based and Proposal-Free With Transformer for Sentence Grounding in Videos
- **Arxiv ID**: http://arxiv.org/abs/2201.10168v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10168v4)
- **Published**: 2022-01-25 08:26:48+00:00
- **Updated**: 2022-08-04 05:13:43+00:00
- **Authors**: Sangmin Woo, Jinyoung Park, Inyong Koo, Sumin Lee, Minki Jeong, Changick Kim
- **Comment**: Code: https://github.com/sangminwoo/Explore-And-Match
- **Journal**: None
- **Summary**: Natural Language Video Grounding (NLVG) aims to localize time segments in an untrimmed video according to sentence queries. In this work, we present a new paradigm named Explore-And-Match for NLVG that seamlessly unifies the strengths of two streams of NLVG methods: proposal-free and proposal-based; the former explores the search space to find time segments directly, and the latter matches the predefined time segments with ground truths. To achieve this, we formulate NLVG as a set prediction problem and design an end-to-end trainable Language Video Transformer (LVTR) that can enjoy two favorable properties, which are rich contextualization power and parallel decoding. We train LVTR with two losses. First, temporal localization loss allows time segments of all queries to regress targets (explore). Second, set guidance loss couples every query with their respective target (match). To our surprise, we found that training schedule shows divide-and-conquer-like pattern: time segments are first diversified regardless of the target, then coupled with each target, and fine-tuned to the target again. Moreover, LVTR is highly efficient and effective: it infers faster than previous baselines (by 2X or more) and sets competitive results on two NLVG benchmarks (ActivityCaptions and Charades-STA). Codes are available at https://github.com/sangminwoo/Explore-And-Match.



### RFMask: A Simple Baseline for Human Silhouette Segmentation with Radio Signals
- **Arxiv ID**: http://arxiv.org/abs/2201.10175v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.10175v1)
- **Published**: 2022-01-25 08:43:01+00:00
- **Updated**: 2022-01-25 08:43:01+00:00
- **Authors**: Zhi Wu, Dongheng Zhang, Chunyang Xie, Cong Yu, Jinbo Chen, Yang Hu, Yan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Human silhouette segmentation, which is originally defined in computer vision, has achieved promising results for understanding human activities. However, the physical limitation makes existing systems based on optical cameras suffer from severe performance degradation under low illumination, smoke, and/or opaque obstruction conditions. To overcome such limitations, in this paper, we propose to utilize the radio signals, which can traverse obstacles and are unaffected by the lighting conditions to achieve silhouette segmentation. The proposed RFMask framework is composed of three modules. It first transforms RF signals captured by millimeter wave radar on two planes into spatial domain and suppress interference with the signal processing module. Then, it locates human reflections on RF frames and extract features from surrounding signals with human detection module. Finally, the extracted features from RF frames are aggregated with an attention based mask generation module. To verify our proposed framework, we collect a dataset containing 804,760 radio frames and 402,380 camera frames with human activities under various scenes. Experimental results show that the proposed framework can achieve impressive human silhouette segmentation even under the challenging scenarios(such as low light and occlusion scenarios) where traditional optical-camera-based methods fail. To the best of our knowledge, this is the first investigation towards segmenting human silhouette based on millimeter wave signals. We hope that our work can serve as a baseline and inspire further research that perform vision tasks with radio signals. The dataset and codes will be made in public.



### Pre-Trained Language Transformers are Universal Image Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2201.10182v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.10182v1)
- **Published**: 2022-01-25 08:56:14+00:00
- **Updated**: 2022-01-25 08:56:14+00:00
- **Authors**: Rahul Goel, Modar Sulaiman, Kimia Noorbakhsh, Mahdi Sharifi, Rajesh Sharma, Pooyan Jamshidi, Kallol Roy
- **Comment**: None
- **Journal**: None
- **Summary**: Facial images disclose many hidden personal traits such as age, gender, race, health, emotion, and psychology. Understanding these traits will help to classify the people in different attributes. In this paper, we have presented a novel method for classifying images using a pretrained transformer model. We apply the pretrained transformer for the binary classification of facial images in criminal and non-criminal classes. The pretrained transformer of GPT-2 is trained to generate text and then fine-tuned to classify facial images. During the finetuning process with images, most of the layers of GT-2 are frozen during backpropagation and the model is frozen pretrained transformer (FPT). The FPT acts as a universal image classifier, and this paper shows the application of FPT on facial images. We also use our FPT on encrypted images for classification. Our FPT shows high accuracy on both raw facial images and encrypted images. We hypothesize the meta-learning capacity FPT gained because of its large size and trained on a large size with theory and experiments. The GPT-2 trained to generate a single word token at a time, through the autoregressive process, forced to heavy-tail distribution. Then the FPT uses the heavy-tail property as its meta-learning capacity for classifying images. Our work shows one way to avoid bias during the machine classification of images.The FPT encodes worldly knowledge because of the pretraining of one text, which it uses during the classification. The statistical error of classification is reduced because of the added context gained from the text.Our paper shows the ethical dimension of using encrypted data for classification.Criminal images are sensitive to share across the boundary but encrypted largely evades ethical concern.FPT showing good classification accuracy on encrypted images shows promise for further research on privacy-preserving machine learning.



### Estimating the Direction and Radius of Pipe from GPR Image by Ellipse Inversion Model
- **Arxiv ID**: http://arxiv.org/abs/2201.10184v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.10184v1)
- **Published**: 2022-01-25 09:01:51+00:00
- **Updated**: 2022-01-25 09:01:51+00:00
- **Authors**: Xiren Zhou, Qiuju Chen, Shengfei Lyu, Huanhuan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Ground Penetrating Radar (GPR) is widely used as a non-destructive approach to estimate buried utilities. When the GPR's detecting direction is perpendicular to a pipeline, a hyperbolic characteristic would be formed on the GPR B-scan image. However, in real-world applications, the direction of pipelines on the existing pipeline map could be inaccurate, and it is hard to ensure the moving direction of GPR to be actually perpendicular to underground pipelines. In this paper, a novel model is proposed to estimate the direction and radius of pipeline and revise the existing pipeline map from GPR B-scan images. The model consists of two parts: GPR B-scan image processing and Ellipse Iterative Inversion Algorithm (EIIA). Firstly, the GPR B-scan image is processed with downward-opening point set extracted. The obtained point set is then iteratively inverted to the elliptical cross section of the buried pipeline, which is caused by the angle between the GPR's detecting direction and the pipeline's direction. By minimizing the sum of the algebraic distances from the extracted point set to the inverted ellipse, the most likely pipeline's direction and radius are determined. Experiments on real-world datasets are conducted, and the results demonstrate the effectiveness of the method.



### Zero-Shot Sketch Based Image Retrieval using Graph Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.10185v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.10185v2)
- **Published**: 2022-01-25 09:02:39+00:00
- **Updated**: 2022-05-28 15:35:08+00:00
- **Authors**: Sumrit Gupta, Ushasi Chaudhuri, Biplab Banerjee
- **Comment**: Accepted at ICPR 2022
- **Journal**: None
- **Summary**: The performance of a zero-shot sketch-based image retrieval (ZS-SBIR) task is primarily affected by two challenges. The substantial domain gap between image and sketch features needs to be bridged, while at the same time the side information has to be chosen tactfully. Existing literature has shown that varying the semantic side information greatly affects the performance of ZS-SBIR. To this end, we propose a novel graph transformer based zero-shot sketch-based image retrieval (GTZSR) framework for solving ZS-SBIR tasks which uses a novel graph transformer to preserve the topology of the classes in the semantic space and propagates the context-graph of the classes within the embedding features of the visual space. To bridge the domain gap between the visual features, we propose minimizing the Wasserstein distance between images and sketches in a learned domain-shared space. We also propose a novel compatibility loss that further aligns the two visual domains by bridging the domain gap of one class with respect to the domain gap of all other classes in the training set. Experimental results obtained on the extended Sketchy, TU-Berlin, and QuickDraw datasets exhibit sharp improvements over the existing state-of-the-art methods in both ZS-SBIR and generalized ZS-SBIR.



### Universal Generative Modeling for Calibration-free Parallel Mr Imaging
- **Arxiv ID**: http://arxiv.org/abs/2201.10210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10210v1)
- **Published**: 2022-01-25 10:05:39+00:00
- **Updated**: 2022-01-25 10:05:39+00:00
- **Authors**: Wanqing Zhu, Bing Guan, Shanshan Wang, Minghui Zhang, Qiegen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The integration of compressed sensing and parallel imaging (CS-PI) provides a robust mechanism for accelerating MRI acquisitions. However, most such strategies require the explicit formation of either coil sensitivity profiles or a cross-coil correlation operator, and as a result reconstruction corresponds to solving a challenging bilinear optimization problem. In this work, we present an unsupervised deep learning framework for calibration-free parallel MRI, coined universal generative modeling for parallel imaging (UGM-PI). More precisely, we make use of the merits of both wavelet transform and the adaptive iteration strategy in a unified framework. We train a powerful noise conditional score network by forming wavelet tensor as the network input at the training phase. Experimental results on both physical phantom and in vivo datasets implied that the proposed method is comparable and even superior to state-of-the-art CS-PI reconstruction approaches.



### Feature Diversity Learning with Sample Dropout for Unsupervised Domain Adaptive Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2201.10212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10212v1)
- **Published**: 2022-01-25 10:10:48+00:00
- **Updated**: 2022-01-25 10:10:48+00:00
- **Authors**: Chunren Tang, Dingyu Xue, Dongyue Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Clustering-based approach has proved effective in dealing with unsupervised domain adaptive person re-identification (ReID) tasks. However, existing works along this approach still suffer from noisy pseudo labels and the unreliable generalization ability during the whole training process. To solve these problems, this paper proposes a new approach to learn the feature representation with better generalization ability through limiting noisy pseudo labels. At first, we propose a Sample Dropout (SD) method to prevent the training of the model from falling into the vicious circle caused by samples that are frequently assigned with noisy pseudo labels. In addition, we put forward a brand-new method referred as to Feature Diversity Learning (FDL) under the classic mutual-teaching architecture, which can significantly improve the generalization ability of the feature representation on the target domain. Experimental results show that our proposed FDL-SD achieves the state-of-the-art performance on multiple benchmark datasets.



### BERTHA: Video Captioning Evaluation Via Transfer-Learned Human Assessment
- **Arxiv ID**: http://arxiv.org/abs/2201.10243v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.10243v3)
- **Published**: 2022-01-25 11:29:58+00:00
- **Updated**: 2022-05-16 10:55:18+00:00
- **Authors**: Luis Lebron, Yvette Graham, Kevin McGuinness, Konstantinos Kouramas, Noel E. O'Connor
- **Comment**: In press in Language Resources and Evaluation Conference(LREC) 2022
- **Journal**: None
- **Summary**: Evaluating video captioning systems is a challenging task as there are multiple factors to consider; for instance: the fluency of the caption, multiple actions happening in a single scene, and the human bias of what is considered important. Most metrics try to measure how similar the system generated captions are to a single or a set of human-annotated captions. This paper presents a new method based on a deep learning model to evaluate these systems. The model is based on BERT, which is a language model that has been shown to work well in multiple NLP tasks. The aim is for the model to learn to perform an evaluation similar to that of a human. To do so, we use a dataset that contains human evaluations of system generated captions. The dataset consists of the human judgments of the captions produce by the system participating in various years of the TRECVid video to text task. These annotations will be made publicly available. BERTHA obtain favourable results, outperforming the commonly used metrics in some setups.



### DocEnTr: An End-to-End Document Image Enhancement Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.10252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10252v1)
- **Published**: 2022-01-25 11:45:35+00:00
- **Updated**: 2022-01-25 11:45:35+00:00
- **Authors**: Mohamed Ali Souibgui, Sanket Biswas, Sana Khamekhem Jemni, Yousri Kessentini, Alicia Fornés, Josep Lladós, Umapada Pal
- **Comment**: submitted to ICPR 2022
- **Journal**: None
- **Summary**: Document images can be affected by many degradation scenarios, which cause recognition and processing difficulties. In this age of digitization, it is important to denoise them for proper usage. To address this challenge, we present a new encoder-decoder architecture based on vision transformers to enhance both machine-printed and handwritten document images, in an end-to-end fashion. The encoder operates directly on the pixel patches with their positional information without the use of any convolutional layers, while the decoder reconstructs a clean image from the encoded patches. Conducted experiments show a superiority of the proposed model compared to the state-of the-art methods on several DIBCO benchmarks. Code and models will be publicly available at: \url{https://github.com/dali92002/DocEnTR}.



### Combining Commonsense Reasoning and Knowledge Acquisition to Guide Deep Learning in Robotics
- **Arxiv ID**: http://arxiv.org/abs/2201.10266v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.LO, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.10266v1)
- **Published**: 2022-01-25 12:24:22+00:00
- **Updated**: 2022-01-25 12:24:22+00:00
- **Authors**: Mohan Sridharan, Tiago Mota
- **Comment**: 37 pages, 17 figures, 5 tables
- **Journal**: None
- **Summary**: Algorithms based on deep network models are being used for many pattern recognition and decision-making tasks in robotics and AI. Training these models requires a large labeled dataset and considerable computational resources, which are not readily available in many domains. Also, it is difficult to explore the internal representations and reasoning mechanisms of these models. As a step towards addressing the underlying knowledge representation, reasoning, and learning challenges, the architecture described in this paper draws inspiration from research in cognitive systems. As a motivating example, we consider an assistive robot trying to reduce clutter in any given scene by reasoning about the occlusion of objects and stability of object configurations in an image of the scene. In this context, our architecture incrementally learns and revises a grounding of the spatial relations between objects and uses this grounding to extract spatial information from input images. Non-monotonic logical reasoning with this information and incomplete commonsense domain knowledge is used to make decisions about stability and occlusion. For images that cannot be processed by such reasoning, regions relevant to the tasks at hand are automatically identified and used to train deep network models to make the desired decisions. Image regions used to train the deep networks are also used to incrementally acquire previously unknown state constraints that are merged with the existing knowledge for subsequent reasoning. Experimental evaluation performed using simulated and real-world images indicates that in comparison with baselines based just on deep networks, our architecture improves reliability of decision making and reduces the effort involved in training data-driven deep network models.



### Convolutional Xformers for Vision
- **Arxiv ID**: http://arxiv.org/abs/2201.10271v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.4.0; I.4.1; I.4.7; I.4.8; I.4.9; I.4.10; I.2.10; I.5.1; I.5.2;
  I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2201.10271v1)
- **Published**: 2022-01-25 12:32:09+00:00
- **Updated**: 2022-01-25 12:32:09+00:00
- **Authors**: Pranav Jeevan, Amit sethi
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: Vision transformers (ViTs) have found only limited practical use in processing images, in spite of their state-of-the-art accuracy on certain benchmarks. The reason for their limited use include their need for larger training datasets and more computational resources compared to convolutional neural networks (CNNs), owing to the quadratic complexity of their self-attention mechanism. We propose a linear attention-convolution hybrid architecture -- Convolutional X-formers for Vision (CXV) -- to overcome these limitations. We replace the quadratic attention with linear attention mechanisms, such as Performer, Nystr\"omformer, and Linear Transformer, to reduce its GPU usage. Inductive prior for image data is provided by convolutional sub-layers, thereby eliminating the need for class token and positional embeddings used by the ViTs. We also propose a new training method where we use two different optimizers during different phases of training and show that it improves the top-1 image classification accuracy across different architectures. CXV outperforms other architectures, token mixers (e.g. ConvMixer, FNet and MLP Mixer), transformer models (e.g. ViT, CCT, CvT and hybrid Xformers), and ResNets for image classification in scenarios with limited data and GPU resources (cores, RAM, power).



### City3D: Large-Scale Building Reconstruction from Airborne LiDAR Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2201.10276v2
- **DOI**: 10.3390/rs14092254
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2201.10276v2)
- **Published**: 2022-01-25 12:41:11+00:00
- **Updated**: 2023-03-09 17:41:34+00:00
- **Authors**: Jin Huang, Jantien Stoter, Ravi Peters, Liangliang Nan
- **Comment**: None
- **Journal**: None
- **Summary**: We present a fully automatic approach for reconstructing compact 3D building models from large-scale airborne point clouds. A major challenge of urban reconstruction from airborne LiDAR point clouds lies in that the vertical walls are typically missing. Based on the observation that urban buildings typically consist of planar roofs connected with vertical walls to the ground, we propose an approach to infer the vertical walls directly from the data. With the planar segments of both roofs and walls, we hypothesize the faces of the building surface, and the final model is obtained by using an extended hypothesis-and-selection-based polygonal surface reconstruction framework. Specifically, we introduce a new energy term to encourage roof preferences and two additional hard constraints into the optimization step to ensure correct topology and enhance detail recovery. Experiments on various large-scale airborne LiDAR point clouds have demonstrated that the method is superior to the state-of-the-art methods in terms of reconstruction accuracy and robustness. In addition, we have generated a new dataset with our method consisting of the point clouds and 3D models of 20k real-world buildings. We believe this dataset can stimulate research in urban reconstruction from airborne LiDAR point clouds and the use of 3D city models in urban applications.



### S2MS: Self-Supervised Learning Driven Multi-Spectral CT Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2201.10294v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.10294v2)
- **Published**: 2022-01-25 13:11:06+00:00
- **Updated**: 2022-01-26 02:48:19+00:00
- **Authors**: Chaoyang Zhang, Shaojie Chang, Ti Bai, Xi Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Photon counting spectral CT (PCCT) can produce reconstructed attenuation maps in different energy channels, reflecting energy properties of the scanned object. Due to the limited photon numbers and the non-ideal detector response of each energy channel, the reconstructed images usually contain much noise. With the development of Deep Learning (DL) technique, different kinds of DL-based models have been proposed for noise reduction. However, most of the models require clean data set as the training labels, which are not always available in medical imaging field. Inspiring by the similarities of each channel's reconstructed image, we proposed a self-supervised learning based PCCT image enhancement framework via multi-spectral channels (S2MS). In S2MS framework, both the input and output labels are noisy images. Specifically, one single channel image was used as output while images of other single channels and channel-sum image were used as input to train the network, which can fully use the spectral data information without extra cost. The simulation results based on the AAPM Low-dose CT Challenge database showed that the proposed S2MS model can suppress the noise and preserve details more effectively in comparison with the traditional DL models, which has potential to improve the image quality of PCCT in clinical applications.



### Mutual information neural estimation for unsupervised multi-modal registration of brain images
- **Arxiv ID**: http://arxiv.org/abs/2201.10305v2
- **DOI**: 10.1109/EMBC48229.2022.9871220
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.10305v2)
- **Published**: 2022-01-25 13:22:34+00:00
- **Updated**: 2022-10-06 06:04:00+00:00
- **Authors**: Gerard Snaauw, Michele Sasdelli, Gabriel Maicas, Stephan Lau, Johan Verjans, Mark Jenkinson, Gustavo Carneiro
- **Comment**: 4 pages, 4 figures, 2022 44th Annual International Conference of the
  IEEE Engineering in Medicine & Biology Society (EMBC), oral presentation
- **Journal**: 2022 44th Annual International Conference of the IEEE Engineering
  in Medicine & Biology Society (EMBC), 2022, pp. 3510-3513
- **Summary**: Many applications in image-guided surgery and therapy require fast and reliable non-linear, multi-modal image registration. Recently proposed unsupervised deep learning-based registration methods have demonstrated superior performance compared to iterative methods in just a fraction of the time. Most of the learning-based methods have focused on mono-modal image registration. The extension to multi-modal registration depends on the use of an appropriate similarity function, such as the mutual information (MI). We propose guiding the training of a deep learning-based registration method with MI estimation between an image-pair in an end-to-end trainable network. Our results show that a small, 2-layer network produces competitive results in both mono- and multi-modal registration, with sub-second run-times. Comparisons to both iterative and deep learning-based methods show that our MI-based method produces topologically and qualitatively superior results with an extremely low rate of non-diffeomorphic transformations. Real-time clinical application will benefit from a better visual matching of anatomical structures and less registration failures/outliers.



### Addressing the Intra-class Mode Collapse Problem using Adaptive Input Image Normalization in GAN-based X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2201.10324v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.10324v2)
- **Published**: 2022-01-25 13:54:25+00:00
- **Updated**: 2022-04-12 15:47:56+00:00
- **Authors**: Muhammad Muneeb Saad, Mubashir Husain Rehmani, Ruairi O'Reilly
- **Comment**: Accepted to the IEEE EMBC22 Conference
- **Journal**: None
- **Summary**: Biomedical image datasets can be imbalanced due to the rarity of targeted diseases. Generative Adversarial Networks play a key role in addressing this imbalance by enabling the generation of synthetic images to augment datasets. It is important to generate synthetic images that incorporate a diverse range of features to accurately represent the distribution of features present in the training imagery. Furthermore, the absence of diverse features in synthetic images can degrade the performance of machine learning classifiers. The mode collapse problem can impact a Generative Adversarial Network's capacity to generate diversified images. Mode collapse comes in two varieties: intra-class and inter-class. In this paper, the intra-class mode collapse problem is investigated, and its subsequent impact on the diversity of synthetic X-ray images is evaluated. This work contributes an empirical demonstration of the benefits of integrating the adaptive input-image normalization for the Deep Convolutional GAN to alleviate the intra-class mode collapse problem. Results demonstrate that the DCGAN with adaptive input-image normalization outperforms DCGAN with un-normalized X-ray images as evident by the superior diversity scores.



### ShapeFormer: Transformer-based Shape Completion via Sparse Representation
- **Arxiv ID**: http://arxiv.org/abs/2201.10326v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.10326v3)
- **Published**: 2022-01-25 13:58:30+00:00
- **Updated**: 2022-05-22 08:03:09+00:00
- **Authors**: Xingguang Yan, Liqiang Lin, Niloy J. Mitra, Dani Lischinski, Daniel Cohen-Or, Hui Huang
- **Comment**: Project page: https://shapeformer.github.io/
- **Journal**: None
- **Summary**: We present ShapeFormer, a transformer-based network that produces a distribution of object completions, conditioned on incomplete, and possibly noisy, point clouds. The resultant distribution can then be sampled to generate likely completions, each exhibiting plausible shape details while being faithful to the input. To facilitate the use of transformers for 3D, we introduce a compact 3D representation, vector quantized deep implicit function, that utilizes spatial sparsity to represent a close approximation of a 3D shape by a short sequence of discrete variables. Experiments demonstrate that ShapeFormer outperforms prior art for shape completion from ambiguous partial inputs in terms of both completion quality and diversity. We also show that our approach effectively handles a variety of shape types, incomplete patterns, and real-world scans.



### Ultra Low-Parameter Denoising: Trainable Bilateral Filter Layers in Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2201.10345v1
- **DOI**: 10.1002/mp.15718
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.10345v1)
- **Published**: 2022-01-25 14:33:56+00:00
- **Updated**: 2022-01-25 14:33:56+00:00
- **Authors**: Fabian Wagner, Mareike Thies, Mingxuan Gu, Yixing Huang, Sabrina Pechmann, Mayank Patwari, Stefan Ploner, Oliver Aust, Stefan Uderhardt, Georg Schett, Silke Christiansen, Andreas Maier
- **Comment**: None
- **Journal**: Med.Phys. 49 (2022) 5107-5120
- **Summary**: Computed tomography is widely used as an imaging tool to visualize three-dimensional structures with expressive bone-soft tissue contrast. However, CT resolution and radiation dose are tightly entangled, highlighting the importance of low-dose CT combined with sophisticated denoising algorithms. Most data-driven denoising techniques are based on deep neural networks and, therefore, contain hundreds of thousands of trainable parameters, making them incomprehensible and prone to prediction failures. Developing understandable and robust denoising algorithms achieving state-of-the-art performance helps to minimize radiation dose while maintaining data integrity. This work presents an open-source CT denoising framework based on the idea of bilateral filtering. We propose a bilateral filter that can be incorporated into a deep learning pipeline and optimized in a purely data-driven way by calculating the gradient flow toward its hyperparameters and its input. Denoising in pure image-to-image pipelines and across different domains such as raw detector data and reconstructed volume, using a differentiable backprojection layer, is demonstrated. Although only using three spatial parameters and one range parameter per filter layer, the proposed denoising pipelines can compete with deep state-of-the-art denoising architectures with several hundred thousand parameters. Competitive denoising performance is achieved on x-ray microscope bone data (0.7053 and 33.10) and the 2016 Low Dose CT Grand Challenge dataset (0.9674 and 43.07) in terms of SSIM and PSNR. Due to the extremely low number of trainable parameters with well-defined effect, prediction reliance and data integrity is guaranteed at any time in the proposed pipelines, in contrast to most other deep learning-based denoising architectures.



### Resource-efficient Deep Neural Networks for Automotive Radar Interference Mitigation
- **Arxiv ID**: http://arxiv.org/abs/2201.10360v1
- **DOI**: 10.1109/JSTSP.2021.3062452
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.10360v1)
- **Published**: 2022-01-25 14:41:08+00:00
- **Updated**: 2022-01-25 14:41:08+00:00
- **Authors**: Johanna Rock, Wolfgang Roth, Mate Toth, Paul Meissner, Franz Pernkopf
- **Comment**: 15 pages; published in IEEE Journal of Selected Topics in Signal
  Processing, Special Issue on Recent Advances in Automotive Radar Signal
  Processing, Volume: 15, Issue: 4, June 2021. arXiv admin note: text overlap
  with arXiv:2011.12706
- **Journal**: IEEE Journal of Selected Topics in Signal Processing, vol. 15, no.
  4, pp. 927-940, June 2021
- **Summary**: Radar sensors are crucial for environment perception of driver assistance systems as well as autonomous vehicles. With a rising number of radar sensors and the so far unregulated automotive radar frequency band, mutual interference is inevitable and must be dealt with. Algorithms and models operating on radar data are required to run the early processing steps on specialized radar sensor hardware. This specialized hardware typically has strict resource-constraints, i.e. a low memory capacity and low computational power. Convolutional Neural Network (CNN)-based approaches for denoising and interference mitigation yield promising results for radar processing in terms of performance. Regarding resource-constraints, however, CNNs typically exceed the hardware's capacities by far.   In this paper we investigate quantization techniques for CNN-based denoising and interference mitigation of radar signals. We analyze the quantization of (i) weights and (ii) activations of different CNN-based model architectures. This quantization results in reduced memory requirements for model storage and during inference. We compare models with fixed and learned bit-widths and contrast two different methodologies for training quantized CNNs, i.e. the straight-through gradient estimator and training distributions over discrete weights. We illustrate the importance of structurally small real-valued base models for quantization and show that learned bit-widths yield the smallest models. We achieve a memory reduction of around 80\% compared to the real-valued baseline. Due to practical reasons, however, we recommend the use of 8 bits for weights and activations, which results in models that require only 0.2 megabytes of memory.



### ADAPT: An Open-Source sUAS Payload for Real-Time Disaster Prediction and Response with AI
- **Arxiv ID**: http://arxiv.org/abs/2201.10366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10366v1)
- **Published**: 2022-01-25 14:51:19+00:00
- **Updated**: 2022-01-25 14:51:19+00:00
- **Authors**: Daniel Davila, Joseph VanPelt, Alexander Lynch, Adam Romlein, Peter Webley, Matthew S. Brown
- **Comment**: To be published in Workshop on Practical Deep Learning in the Wild at
  AAAI Conference on Artificial Intelligence 2022, 9 pages, 5 figures
- **Journal**: None
- **Summary**: Small unmanned aircraft systems (sUAS) are becoming prominent components of many humanitarian assistance and disaster response (HADR) operations. Pairing sUAS with onboard artificial intelligence (AI) substantially extends their utility in covering larger areas with fewer support personnel. A variety of missions, such as search and rescue, assessing structural damage, and monitoring forest fires, floods, and chemical spills, can be supported simply by deploying the appropriate AI models. However, adoption by resource-constrained groups, such as local municipalities, regulatory agencies, and researchers, has been hampered by the lack of a cost-effective, readily-accessible baseline platform that can be adapted to their unique missions. To fill this gap, we have developed the free and open-source ADAPT multi-mission payload for deploying real-time AI and computer vision onboard a sUAS during local and beyond-line-of-site missions. We have emphasized a modular design with low-cost, readily-available components, open-source software, and thorough documentation (https://kitware.github.io/adapt/). The system integrates an inertial navigation system, high-resolution color camera, computer, and wireless downlink to process imagery and broadcast georegistered analytics back to a ground station. Our goal is to make it easy for the HADR community to build their own copies of the ADAPT payload and leverage the thousands of hours of engineering we have devoted to developing and testing. In this paper, we detail the development and testing of the ADAPT payload. We demonstrate the example mission of real-time, in-flight ice segmentation to monitor river ice state and provide timely predictions of catastrophic flooding events. We deploy a novel active learning workflow to annotate river ice imagery, train a real-time deep neural network for ice segmentation, and demonstrate operation in the field.



### Projective Urban Texturing
- **Arxiv ID**: http://arxiv.org/abs/2201.10938v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10938v2)
- **Published**: 2022-01-25 14:56:52+00:00
- **Updated**: 2022-02-04 09:41:10+00:00
- **Authors**: Yiangos Georgiou, Melinos Averkiou, Tom Kelly, Evangelos Kalogerakis
- **Comment**: None
- **Journal**: International Conference on 3D Vision 2021
- **Summary**: This paper proposes a method for automatic generation of textures for 3D city meshes in immersive urban environments. Many recent pipelines capture or synthesize large quantities of city geometry using scanners or procedural modeling pipelines. Such geometry is intricate and realistic, however the generation of photo-realistic textures for such large scenes remains a problem. We propose to generate textures for input target 3D meshes driven by the textural style present in readily available datasets of panoramic photos capturing urban environments. Re-targeting such 2D datasets to 3D geometry is challenging because the underlying shape, size, and layout of the urban structures in the photos do not correspond to the ones in the target meshes. Photos also often have objects (e.g., trees, vehicles) that may not even be present in the target geometry. To address these issues we present a method, called Projective Urban Texturing (PUT), which re-targets textural style from real-world panoramic images to unseen urban meshes. PUT relies on contrastive and adversarial training of a neural architecture designed for unpaired image-to-texture translation. The generated textures are stored in a texture atlas applied to the target 3D mesh geometry. To promote texture consistency, PUT employs an iterative procedure in which texture synthesis is conditioned on previously generated, adjacent textures. We demonstrate both quantitative and qualitative evaluation of the generated textures.



### Winograd Convolution for Deep Neural Networks: Efficient Point Selection
- **Arxiv ID**: http://arxiv.org/abs/2201.10369v1
- **DOI**: None
- **Categories**: **cs.CV**, C.3.2; G.0
- **Links**: [PDF](http://arxiv.org/pdf/2201.10369v1)
- **Published**: 2022-01-25 15:00:54+00:00
- **Updated**: 2022-01-25 15:00:54+00:00
- **Authors**: Syed Asad Alam, Andrew Anderson, Barbara Barabasz, David Gregg
- **Comment**: 19 pages, 3 figures, 9 tables and 32 equations
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have dramatically improved the accuracy of tasks such as object recognition, image segmentation and interactive speech systems. CNNs require large amounts of computing resources because ofcomputationally intensive convolution layers. Fast convolution algorithms such as Winograd convolution can greatly reduce the computational cost of these layers at a cost of poor numeric properties, such that greater savings in computation exponentially increase floating point errors.   A defining feature of each Winograd convolution algorithm is a set of real-value points where polynomials are sampled. The choice of points impacts the numeric accuracy of the algorithm, but the optimal set of points for small convolutions remains unknown. Existing work considers only small integers and simple fractions as candidate points. In this work, we propose a novel approach to point selection using points of the form {-1/c , -c, c, 1/c } using the full range of real-valued numbers for c. We show that groups of this form cause cancellations in the Winograd transform matrices that reduce numeric error. We find empirically that the error for different values of c forms a rough curve across the range of real-value numbers helping to localize the values of c that reduce error and that lower errors can be achieved with non-obvious real-valued evaluation points instead of integers or simple fractions. We study a range of sizes for small convolutions and achieve reduction in error ranging from 2% to around 59% for both 1D and 2D convolution. Furthermore, we identify patterns in cases when we select a subset of our proposed points which will always lead to a lower error. Finally we implement a complete Winograd convolution layer and use it to run deep convolution neural networks on real datasets and show that our proposed points reduce error, ranging from 22% to 63%.



### BLDNet: A Semi-supervised Change Detection Building Damage Framework using Graph Convolutional Networks and Urban Domain Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2201.10389v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.10389v1)
- **Published**: 2022-01-25 15:19:30+00:00
- **Updated**: 2022-01-25 15:19:30+00:00
- **Authors**: Ali Ismail, Mariette Awad
- **Comment**: 16 pages, 15 figures, submitted to IEEE Transactions on Geoscience
  and Remote Sensing
- **Journal**: None
- **Summary**: Change detection is instrumental to localize damage and understand destruction in disaster informatics. While convolutional neural networks are at the core of recent change detection solutions, we present in this work, BLDNet, a novel graph formulation for building damage change detection and enable learning relationships and representations from both local patterns and non-stationary neighborhoods. More specifically, we use graph convolutional networks to efficiently learn these features in a semi-supervised framework with few annotated data. Additionally, BLDNet formulation allows for the injection of additional contextual building meta-features. We train and benchmark on the xBD dataset to validate the effectiveness of our approach. We also demonstrate on urban data from the 2020 Beirut Port Explosion that performance is improved by incorporating domain knowledge building meta-features.



### Capturing Temporal Information in a Single Frame: Channel Sampling Strategies for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2201.10394v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10394v3)
- **Published**: 2022-01-25 15:24:37+00:00
- **Updated**: 2022-10-10 17:59:35+00:00
- **Authors**: Kiyoon Kim, Shreyank N Gowda, Oisin Mac Aodha, Laura Sevilla-Lara
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: We address the problem of capturing temporal information for video classification in 2D networks, without increasing their computational cost. Existing approaches focus on modifying the architecture of 2D networks (e.g. by including filters in the temporal dimension to turn them into 3D networks, or using optical flow, etc.), which increases computation cost. Instead, we propose a novel sampling strategy, where we re-order the channels of the input video, to capture short-term frame-to-frame changes. We observe that without bells and whistles, the proposed sampling strategy improves performance on multiple architectures (e.g. TSN, TRN, TSM, and MVFNet) and datasets (CATER, Something-Something-V1 and V2), up to 24% over the baseline of using the standard video input. In addition, our sampling strategies do not require training from scratch and do not increase the computational cost of training and testing. Given the generality of the results and the flexibility of the approach, we hope this can be widely useful to the video understanding community. Code is available on our website: https://github.com/kiyoon/channel_sampling.



### Towards Cross-Disaster Building Damage Assessment with Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/2201.10395v1
- **DOI**: 10.1109/IGARSS46834.2022.9883832
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.10395v1)
- **Published**: 2022-01-25 15:25:21+00:00
- **Updated**: 2022-01-25 15:25:21+00:00
- **Authors**: Ali Ismail, Mariette Awad
- **Comment**: 5 pages, 3 figures, submitted to IEEE IGARSS 2022
- **Journal**: None
- **Summary**: In the aftermath of disasters, building damage maps are obtained using change detection to plan rescue operations. Current convolutional neural network approaches do not consider the similarities between neighboring buildings for predicting the damage. We present a novel graph-based building damage detection solution to capture these relationships. Our proposed model architecture learns from both local and neighborhood features to predict building damage. Specifically, we adopt the sample and aggregate graph convolution strategy to learn aggregation functions that generalize to unseen graphs which is essential for alleviating the time needed to obtain predictions for new disasters. Our experiments on the xBD dataset and comparisons with a classical convolutional neural network reveal that while our approach is handicapped by class imbalance, it presents a promising and distinct advantage when it comes to cross-disaster generalization.



### Comparison of Evaluation Metrics for Landmark Detection in CMR Images
- **Arxiv ID**: http://arxiv.org/abs/2201.10410v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10410v2)
- **Published**: 2022-01-25 15:58:30+00:00
- **Updated**: 2022-01-28 15:18:30+00:00
- **Authors**: Sven Koehler, Lalith Sharan, Julian Kuhm, Arman Ghanaat, Jelizaveta Gordejeva, Nike K. Simon, Niko M. Grell, Florian André, Sandy Engelhardt
- **Comment**: Accepted at Bildverarbeitung f\"ur die Medizin (BVM), Informatik
  aktuell. Springer Vieweg, Wiesbaden 2022
- **Journal**: None
- **Summary**: Cardiac Magnetic Resonance (CMR) images are widely used for cardiac diagnosis and ventricular assessment. Extracting specific landmarks like the right ventricular insertion points is of importance for spatial alignment and 3D modeling. The automatic detection of such landmarks has been tackled by multiple groups using Deep Learning, but relatively little attention has been paid to the failure cases of evaluation metrics in this field. In this work, we extended the public ACDC dataset with additional labels of the right ventricular insertion points and compare different variants of a heatmap-based landmark detection pipeline. In this comparison, we demonstrate very likely pitfalls of apparently simple detection and localisation metrics which highlights the importance of a clear detection strategy and the definition of an upper limit for localisation-based metrics. Our preliminary results indicate that a combination of different metrics is necessary, as they yield different winners for method comparison. Additionally, they highlight the need of a comprehensive metric description and evaluation standardisation, especially for the error cases where no metrics could be computed or where no lower/upper boundary of a metric exists. Code and labels: https://github.com/Cardio-AI/rvip_landmark_detection



### Rayleigh EigenDirections (REDs): GAN latent space traversals for multidimensional features
- **Arxiv ID**: http://arxiv.org/abs/2201.10423v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.10423v1)
- **Published**: 2022-01-25 16:11:33+00:00
- **Updated**: 2022-01-25 16:11:33+00:00
- **Authors**: Guha Balakrishnan, Raghudeep Gadde, Aleix Martinez, Pietro Perona
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for finding paths in a deep generative model's latent space that can maximally vary one set of image features while holding others constant. Crucially, unlike past traversal approaches, ours can manipulate multidimensional features of an image such as facial identity and pixels within a specified region. Our method is principled and conceptually simple: optimal traversal directions are chosen by maximizing differential changes to one feature set such that changes to another set are negligible. We show that this problem is nearly equivalent to one of Rayleigh quotient maximization, and provide a closed-form solution to it based on solving a generalized eigenvalue equation. We use repeated computations of the corresponding optimal directions, which we call Rayleigh EigenDirections (REDs), to generate appropriately curved paths in latent space. We empirically evaluate our method using StyleGAN2 on two image domains: faces and living rooms. We show that our method is capable of controlling various multidimensional features out of the scope of previous latent space traversal methods: face identity, spatial frequency bands, pixels within a region, and the appearance and position of an object. Our work suggests that a wealth of opportunities lies in the local analysis of the geometry and semantics of latent spaces.



### Improving segmentation of calcified and non-calcified plaques on CCTA-CPR scans via masking of the artery wall
- **Arxiv ID**: http://arxiv.org/abs/2201.10424v4
- **DOI**: 10.1117/12.2652895
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.10424v4)
- **Published**: 2022-01-25 16:12:04+00:00
- **Updated**: 2023-04-10 05:40:30+00:00
- **Authors**: Antonio Tejero-de-Pablos, Hiroaki Yamane, Yusuke Kurose, Junichi Iho, Youji Tokunaga, Makoto Horie, Keisuke Nishizawa, Yusaku Hayashi, Yasushi Koyama, Tatsuya Harada
- **Comment**: Extended abstract (see SPIE for final published version)
- **Journal**: SPIE 12465, Medical Imaging 2023: Computer-Aided Diagnosis
- **Summary**: The presence of plaques in the coronary arteries is a major risk to the patients' life. In particular, non-calcified plaques pose a great challenge, as they are harder to detect and more likely to rupture than calcified plaques. While current deep learning techniques allow precise segmentation of real-life images, the performance in medical images is still low. This is caused mostly by blurriness and ambiguous voxel intensities of unrelated parts that fall on the same value range. In this paper, we propose a novel methodology for segmenting calcified and non-calcified plaques in CCTA-CPR scans of coronary arteries. The input slices are masked so only the voxels within the wall vessel are considered for segmentation, thus, reducing ambiguity. This mask can be automatically generated via a deep learning-based vessel detector, that provides not only the contour of the outer artery wall, but also the inner contour. For evaluation, we utilized a dataset in which each voxel is carefully annotated as one of five classes: background, lumen, artery wall, calcified plaque, or non-calcified plaque. We also provide an exhaustive evaluation by applying different types of masks, in order to validate the potential of vessel masking for plaque segmentation. Our methodology results in a prominent boost in segmentation performance, in both quantitative and qualitative evaluation, achieving accurate plaque shapes even for the challenging non-calcified plaques. Furthermore, when using highly accurate masks, difficult cases such as stenosis become segmentable. We believe our findings can lead the future research for high-performance plaque segmentation.



### Main Product Detection with Graph Networks for Fashion
- **Arxiv ID**: http://arxiv.org/abs/2201.10431v1
- **DOI**: 10.1007/s11042-022-13572-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10431v1)
- **Published**: 2022-01-25 16:26:04+00:00
- **Updated**: 2022-01-25 16:26:04+00:00
- **Authors**: Vacit Oguz Yazici, Longlong Yu, Arnau Ramisa, Luis Herranz, Joost van de Weijer
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision has established a foothold in the online fashion retail industry. Main product detection is a crucial step of vision-based fashion product feed parsing pipelines, focused in identifying the bounding boxes that contain the product being sold in the gallery of images of the product page. The current state-of-the-art approach does not leverage the relations between regions in the image, and treats images of the same product independently, therefore not fully exploiting visual and product contextual information. In this paper we propose a model that incorporates Graph Convolutional Networks (GCN) that jointly represent all detected bounding boxes in the gallery as nodes. We show that the proposed method is better than the state-of-the-art, especially, when we consider the scenario where title-input is missing at inference time and for cross-dataset evaluation, our method outperforms previous approaches by a large margin.



### Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition for Single and Multi-Person Video
- **Arxiv ID**: http://arxiv.org/abs/2201.10439v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2201.10439v3)
- **Published**: 2022-01-25 16:35:54+00:00
- **Updated**: 2022-10-31 18:37:22+00:00
- **Authors**: Dmitriy Serdyuk, Otavio Braga, Olivier Siohan
- **Comment**: 5 pages, 3 figures, published at Interspeech 2022
- **Journal**: None
- **Summary**: Audio-visual automatic speech recognition (AV-ASR) extends speech recognition by introducing the video modality as an additional source of information. In this work, the information contained in the motion of the speaker's mouth is used to augment the audio features. The video modality is traditionally processed with a 3D convolutional neural network (e.g. 3D version of VGG). Recently, image transformer networks arXiv:2010.11929 demonstrated the ability to extract rich visual features for image classification tasks. Here, we propose to replace the 3D convolution with a video transformer to extract visual features. We train our baselines and the proposed model on a large scale corpus of YouTube videos. The performance of our approach is evaluated on a labeled subset of YouTube videos as well as on the LRS3-TED public corpus. Our best video-only model obtains 31.4% WER on YTDEV18 and 17.0% on LRS3-TED, a 10% and 15% relative improvements over our convolutional baseline. We achieve the state of the art performance of the audio-visual recognition on the LRS3-TED after fine-tuning our model (1.6% WER). In addition, in a series of experiments on multi-person AV-ASR, we obtained an average relative reduction of 2% over our convolutional video frontend.



### AggMatch: Aggregating Pseudo Labels for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.10444v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.10444v1)
- **Published**: 2022-01-25 16:41:54+00:00
- **Updated**: 2022-01-25 16:41:54+00:00
- **Authors**: Jiwon Kim, Kwangrok Ryoo, Gyuseong Lee, Seokju Cho, Junyoung Seo, Daehwan Kim, Hansang Cho, Seungryong Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL) has recently proven to be an effective paradigm for leveraging a huge amount of unlabeled data while mitigating the reliance on large labeled data. Conventional methods focused on extracting a pseudo label from individual unlabeled data sample and thus they mostly struggled to handle inaccurate or noisy pseudo labels, which degenerate performance.   In this paper, we address this limitation with a novel SSL framework for aggregating pseudo labels, called AggMatch, which refines initial pseudo labels by using different confident instances. Specifically, we introduce an aggregation module for consistency regularization framework that aggregates the initial pseudo labels based on the similarity between the instances. To enlarge the aggregation candidates beyond the mini-batch, we present a class-balanced confidence-aware queue built with the momentum model, encouraging to provide more stable and consistent aggregation. We also propose a novel uncertainty-based confidence measure for the pseudo label by considering the consensus among multiple hypotheses with different subsets of the queue. We conduct experiments to demonstrate the effectiveness of AggMatch over the latest methods on standard benchmarks and provide extensive analyses.



### How Low Can We Go? Pixel Annotation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.10448v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10448v2)
- **Published**: 2022-01-25 16:50:24+00:00
- **Updated**: 2022-02-13 19:41:52+00:00
- **Authors**: Daniel Kigli, Ariel Shamir, Shai Avidan
- **Comment**: Paper and Supplementary
- **Journal**: None
- **Summary**: How many labeled pixels are needed to segment an image, without any prior knowledge? We conduct an experiment to answer this question.   In our experiment, an Oracle is using Active Learning to train a network from scratch. The Oracle has access to the entire label map of the image, but the goal is to reveal as little pixel labels to the network as possible. We find that, on average, the Oracle needs to reveal (i.e., annotate) less than 0.1% of the pixels in order to train a network. The network can then label all pixels in the image at an accuracy of more than 98%.   Based on this single-image-annotation experiment, we design an experiment to quickly annotate an entire data set. In the data set level experiment the Oracle trains a new network for each image from scratch. The network can then be used to create pseudo-labels, which are the network predicted labels of the unlabeled pixels, for the entire image. Only then, a data set level network is trained from scratch on all the pseudo-labeled images at once.   We repeat both image level and data set level experiments on two, very different, real-world data sets, and find that it is possible to reach the performance of a fully annotated data set using a fraction of the annotation cost.



### Sphere2Vec: Multi-Scale Representation Learning over a Spherical Surface for Geospatial Predictions
- **Arxiv ID**: http://arxiv.org/abs/2201.10489v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2.10; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/2201.10489v1)
- **Published**: 2022-01-25 17:34:29+00:00
- **Updated**: 2022-01-25 17:34:29+00:00
- **Authors**: Gengchen Mai, Yao Xuan, Wenyun Zuo, Krzysztof Janowicz, Ni Lao
- **Comment**: None
- **Journal**: None
- **Summary**: Generating learning-friendly representations for points in a 2D space is a fundamental and long-standing problem in machine learning. Recently, multi-scale encoding schemes (such as Space2Vec) were proposed to directly encode any point in 2D space as a high-dimensional vector, and has been successfully applied to various (geo)spatial prediction tasks. However, a map projection distortion problem rises when applying location encoding models to large-scale real-world GPS coordinate datasets (e.g., species images taken all over the world) - all current location encoding models are designed for encoding points in a 2D (Euclidean) space but not on a spherical surface, e.g., earth surface. To solve this problem, we propose a multi-scale location encoding model called Sphere2V ec which directly encodes point coordinates on a spherical surface while avoiding the mapprojection distortion problem. We provide theoretical proof that the Sphere2Vec encoding preserves the spherical surface distance between any two points. We also developed a unified view of distance-reserving encoding on spheres based on the Double Fourier Sphere (DFS). We apply Sphere2V ec to the geo-aware image classification task. Our analysis shows that Sphere2V ec outperforms other 2D space location encoder models especially on the polar regions and data-sparse areas for image classification tasks because of its nature for spherical surface distance preservation.



### Initial Investigations Towards Non-invasive Monitoring of Chronic Wound Healing Using Deep Learning and Ultrasound Imaging
- **Arxiv ID**: http://arxiv.org/abs/2201.10511v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.10511v1)
- **Published**: 2022-01-25 18:12:54+00:00
- **Updated**: 2022-01-25 18:12:54+00:00
- **Authors**: Maja Schlereth, Daniel Stromer, Yash Mantri, Jason Tsujimoto, Katharina Breininger, Andreas Maier, Caesar Anderson, Pranav S. Garimella, Jesse V. Jokerst
- **Comment**: 6 pages, 2 figures, accepted by BVM conference proceedings 2022
- **Journal**: None
- **Summary**: Chronic wounds including diabetic and arterial/venous insufficiency injuries have become a major burden for healthcare systems worldwide. Demographic changes suggest that wound care will play an even bigger role in the coming decades. Predicting and monitoring response to therapy in wound care is currently largely based on visual inspection with little information on the underlying tissue. Thus, there is an urgent unmet need for innovative approaches that facilitate personalized diagnostics and treatments at the point-of-care. It has been recently shown that ultrasound imaging can monitor response to therapy in wound care, but this work required onerous manual image annotations. In this study, we present initial results of a deep learning-based automatic segmentation of cross-sectional wound size in ultrasound images and identify requirements and challenges for future research on this application. Evaluation of the segmentation results underscores the potential of the proposed deep learning approach to complement non-invasive imaging with Dice scores of 0.34 (U-Net, FCN) and 0.27 (ResNet-U-Net) but also highlights the need for improving robustness further. We conclude that deep learning-supported analysis of non-invasive ultrasound images is a promising area of research to automatically extract cross-sectional wound size and depth information with potential value in monitoring response to therapy.



### Jacobian Computation for Cumulative B-Splines on SE(3) and Application to Continuous-Time Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2201.10602v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2201.10602v2)
- **Published**: 2022-01-25 19:53:33+00:00
- **Updated**: 2022-05-24 14:00:24+00:00
- **Authors**: Javier Tirado, Javier Civera
- **Comment**: Accepted at IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: In this paper we propose a method that estimates the $SE(3)$ continuous trajectories (orientation and translation) of the dynamic rigid objects present in a scene, from multiple RGB-D views. Specifically, we fit the object trajectories to cumulative B-Splines curves, which allow us to interpolate, at any intermediate time stamp, not only their poses but also their linear and angular velocities and accelerations. Additionally, we derive in this work the analytical $SE(3)$ Jacobians needed by the optimization, being applicable to any other approach that uses this type of curves. To the best of our knowledge this is the first work that proposes 6-DoF continuous-time object tracking, which we endorse with significant computational cost reduction thanks to our analytical derivations. We evaluate our proposal in synthetic data and in a public benchmark, showing competitive results in localization and significant improvements in velocity estimation in comparison to discrete-time approaches.



### Unsupervised Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation via Semi-supervised Learning and Label Fusion
- **Arxiv ID**: http://arxiv.org/abs/2201.10647v1
- **DOI**: 10.1007/978-3-031-09002-8_46
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10647v1)
- **Published**: 2022-01-25 22:01:04+00:00
- **Updated**: 2022-01-25 22:01:04+00:00
- **Authors**: Han Liu, Yubo Fan, Can Cui, Dingjie Su, Andrew McNeil, Benoit M. Dawant
- **Comment**: Accepted by MICCAI 2021 BrainLes Workshop. arXiv admin note:
  substantial text overlap with arXiv:2109.06274
- **Journal**: None
- **Summary**: Automatic methods to segment the vestibular schwannoma (VS) tumors and the cochlea from magnetic resonance imaging (MRI) are critical to VS treatment planning. Although supervised methods have achieved satisfactory performance in VS segmentation, they require full annotations by experts, which is laborious and time-consuming. In this work, we aim to tackle the VS and cochlea segmentation problem in an unsupervised domain adaptation setting. Our proposed method leverages both the image-level domain alignment to minimize the domain divergence and semi-supervised training to further boost the performance. Furthermore, we propose to fuse the labels predicted from multiple models via noisy label correction. In the MICCAI 2021 crossMoDA challenge, our results on the final evaluation leaderboard showed that our proposed method has achieved promising segmentation performance with mean dice score of 79.9% and 82.5% and ASSD of 1.29 mm and 0.18 mm for VS tumor and cochlea, respectively. The cochlea ASSD achieved by our method has outperformed all other competing methods as well as the supervised nnU-Net.



### Attentive Task Interaction Network for Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/2201.10649v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.10649v1)
- **Published**: 2022-01-25 22:03:20+00:00
- **Updated**: 2022-01-25 22:03:20+00:00
- **Authors**: Dimitrios Sinodinos, Narges Armanfard
- **Comment**: None
- **Journal**: None
- **Summary**: Multitask learning (MTL) has recently gained a lot of popularity as a learning paradigm that can lead to improved per-task performance while also using fewer per-task model parameters compared to single task learning. One of the biggest challenges regarding MTL networks involves how to share features across tasks. To address this challenge, we propose the Attentive Task Interaction Network (ATI-Net). ATI-Net employs knowledge distillation of the latent features for each task, then combines the feature maps to provide improved contextualized information to the decoder. This novel approach to introducing knowledge distillation into an attention based multitask network outperforms state of the art MTL baselines such as the standalone MTAN and PAD-Net, with roughly the same number of model parameters.



### Beyond Visual Image: Automated Diagnosis of Pigmented Skin Lesions Combining Clinical Image Features with Patient Data
- **Arxiv ID**: http://arxiv.org/abs/2201.10650v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.10650v1)
- **Published**: 2022-01-25 22:09:43+00:00
- **Updated**: 2022-01-25 22:09:43+00:00
- **Authors**: José G. M. Esgario, Renato A. Krohling
- **Comment**: 33 pages, 11 figures
- **Journal**: None
- **Summary**: kin cancer is considered one of the most common type of cancer in several countries. Due to the difficulty and subjectivity in the clinical diagnosis of skin lesions, Computer-Aided Diagnosis systems are being developed for assist experts to perform more reliable diagnosis. The clinical analysis and diagnosis of skin lesions relies not only on the visual information but also on the context information provided by the patient. This work addresses the problem of pigmented skin lesions detection from smartphones captured images. In addition to the features extracted from images, patient context information was collected to provide a more accurate diagnosis. The experiments showed that the combination of visual features with context information improved final results. Experimental results are very promising and comparable to experts.



### SA-VQA: Structured Alignment of Visual and Semantic Representations for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2201.10654v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10654v1)
- **Published**: 2022-01-25 22:26:09+00:00
- **Updated**: 2022-01-25 22:26:09+00:00
- **Authors**: Peixi Xiong, Quanzeng You, Pei Yu, Zicheng Liu, Ying Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) attracts much attention from both industry and academia. As a multi-modality task, it is challenging since it requires not only visual and textual understanding, but also the ability to align cross-modality representations. Previous approaches extensively employ entity-level alignments, such as the correlations between the visual regions and their semantic labels, or the interactions across question words and object features. These attempts aim to improve the cross-modality representations, while ignoring their internal relations. Instead, we propose to apply structured alignments, which work with graph representation of visual and textual content, aiming to capture the deep connections between the visual and textual modalities. Nevertheless, it is nontrivial to represent and integrate graphs for structured alignments. In this work, we attempt to solve this issue by first converting different modality entities into sequential nodes and the adjacency graph, then incorporating them for structured alignments. As demonstrated in our experimental results, such a structured alignment improves reasoning performance. In addition, our model also exhibits better interpretability for each generated answer. The proposed model, without any pretraining, outperforms the state-of-the-art methods on GQA dataset, and beats the non-pretrained state-of-the-art methods on VQA-v2 dataset.



### MGA-VQA: Multi-Granularity Alignment for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2201.10656v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10656v1)
- **Published**: 2022-01-25 22:30:54+00:00
- **Updated**: 2022-01-25 22:30:54+00:00
- **Authors**: Peixi Xiong, Yilin Shen, Hongxia Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Learning to answer visual questions is a challenging task since the multi-modal inputs are within two feature spaces. Moreover, reasoning in visual question answering requires the model to understand both image and question, and align them in the same space, rather than simply memorize statistics about the question-answer pairs. Thus, it is essential to find component connections between different modalities and within each modality to achieve better attention. Previous works learned attention weights directly on the features. However, the improvement is limited since these two modality features are in two domains: image features are highly diverse, lacking structure and grammatical rules as language, and natural language features have a higher probability of missing detailed information. To better learn the attention between visual and text, we focus on how to construct input stratification and embed structural information to improve the alignment between different level components. We propose Multi-Granularity Alignment architecture for Visual Question Answering task (MGA-VQA), which learns intra- and inter-modality correlations by multi-granularity alignment, and outputs the final result by the decision fusion module. In contrast to previous works, our model splits alignment into different levels to achieve learning better correlations without needing additional data and annotations. The experiments on the VQA-v2 and GQA datasets demonstrate that our model significantly outperforms non-pretrained state-of-the-art methods on both datasets without extra pretraining data and annotations. Moreover, it even achieves better results over the pre-trained methods on GQA.



### Do Neural Networks for Segmentation Understand Insideness?
- **Arxiv ID**: http://arxiv.org/abs/2201.10664v1
- **DOI**: 10.1162/neco_a_01413
- **Categories**: **cs.CV**, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2201.10664v1)
- **Published**: 2022-01-25 22:54:41+00:00
- **Updated**: 2022-01-25 22:54:41+00:00
- **Authors**: Kimberly Villalobos, Vilim Štih, Amineh Ahmadinejad, Shobhita Sundaram, Jamell Dozier, Andrew Francl, Frederico Azevedo, Tomotake Sasaki, Xavier Boix
- **Comment**: None
- **Journal**: Neural Computation 33 (2021) 2511-2549
- **Summary**: The insideness problem is an aspect of image segmentation that consists of determining which pixels are inside and outside a region. Deep Neural Networks (DNNs) excel in segmentation benchmarks, but it is unclear if they have the ability to solve the insideness problem as it requires evaluating long-range spatial dependencies. In this paper, the insideness problem is analysed in isolation, without texture or semantic cues, such that other aspects of segmentation do not interfere in the analysis. We demonstrate that DNNs for segmentation with few units have sufficient complexity to solve insideness for any curve. Yet, such DNNs have severe problems with learning general solutions. Only recurrent networks trained with small images learn solutions that generalize well to almost any curve. Recurrent networks can decompose the evaluation of long-range dependencies into a sequence of local operations, and learning with small images alleviates the common difficulties of training recurrent networks with a large number of unrolling steps.



### Writer Recognition Using Off-line Handwritten Single Block Characters
- **Arxiv ID**: http://arxiv.org/abs/2201.10665v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.10665v2)
- **Published**: 2022-01-25 23:04:10+00:00
- **Updated**: 2022-03-08 00:06:51+00:00
- **Authors**: Adrian Leo Hagström, Rustam Stanikzai, Josef Bigun, Fernando Alonso-Fernandez
- **Comment**: Accepted for publication at IEEE International Workshop on Biometrics
  and Forensics IWBF 2022
- **Journal**: None
- **Summary**: Block characters are often used when filling paper forms for a variety of purposes. We investigate if there is biometric information contained within individual digits of handwritten text. In particular, we use personal identity numbers consisting of the six digits of the date of birth, DoB. We evaluate two recognition approaches, one based on handcrafted features that compute contour directional measurements, and another based on deep features from a ResNet50 model. We use a self-captured database of 317 individuals and 4920 written DoBs in total. Results show the presence of identity-related information in a piece of handwritten information as small as six digits with the DoB. We also analyze the impact of the amount of enrolment samples, varying its number between one and ten. Results with such small amount of data are promising. With ten enrolment samples, the Top-1 accuracy with deep features is around 94%, and reaches nearly 100% by Top-10. The verification accuracy is more modest, with EER>20%with any given feature and enrolment set size, showing that there is still room for improvement.



### Virtual Adversarial Training for Semi-supervised Breast Mass Classification
- **Arxiv ID**: http://arxiv.org/abs/2201.10675v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2201.10675v1)
- **Published**: 2022-01-25 23:39:39+00:00
- **Updated**: 2022-01-25 23:39:39+00:00
- **Authors**: Xuxin Chen, Ximin Wang, Ke Zhang, Kar-Ming Fung, Theresa C. Thai, Kathleen Moore, Robert S. Mannel, Hong Liu, Bin Zheng, Yuchen Qiu
- **Comment**: To appear in the conference Biophotonics and Immune Responses of SPIE
- **Journal**: None
- **Summary**: This study aims to develop a novel computer-aided diagnosis (CAD) scheme for mammographic breast mass classification using semi-supervised learning. Although supervised deep learning has achieved huge success across various medical image analysis tasks, its success relies on large amounts of high-quality annotations, which can be challenging to acquire in practice. To overcome this limitation, we propose employing a semi-supervised method, i.e., virtual adversarial training (VAT), to leverage and learn useful information underlying in unlabeled data for better classification of breast masses. Accordingly, our VAT-based models have two types of losses, namely supervised and virtual adversarial losses. The former loss acts as in supervised classification, while the latter loss aims at enhancing model robustness against virtual adversarial perturbation, thus improving model generalizability. To evaluate the performance of our VAT-based CAD scheme, we retrospectively assembled a total of 1024 breast mass images, with equal number of benign and malignant masses. A large CNN and a small CNN were used in this investigation, and both were trained with and without the adversarial loss. When the labeled ratios were 40% and 80%, VAT-based CNNs delivered the highest classification accuracy of 0.740 and 0.760, respectively. The experimental results suggest that the VAT-based CAD scheme can effectively utilize meaningful knowledge from unlabeled data to better classify mammographic breast mass images.



