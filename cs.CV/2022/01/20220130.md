# Arxiv Papers in cs.CV on 2022-01-30
### Extracting Built Environment Features for Planning Research with Computer Vision: A Review and Discussion of State-of-the-Art Approaches
- **Arxiv ID**: http://arxiv.org/abs/2201.12693v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2201.12693v2)
- **Published**: 2022-01-30 01:02:18+00:00
- **Updated**: 2022-03-21 17:20:02+00:00
- **Authors**: Meiqing Li, Hao Sheng
- **Comment**: CUPUM 2021 (The 17th International Conference on Computational Urban
  Planning and Urban Management)
- **Journal**: None
- **Summary**: This is an extended abstract for a presentation at The 17th International Conference on CUPUM - Computational Urban Planning and Urban Management in June 2021. This study presents an interdisciplinary synthesis of the state-of-the-art approaches in computer vision technologies to extract built environment features that could improve the robustness of empirical research in planning. We discussed the findings from the review of studies in both planning and computer science.



### A Robust Framework for Deep Learning Approaches to Facial Emotion Recognition and Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2201.12705v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2201.12705v1)
- **Published**: 2022-01-30 02:10:01+00:00
- **Updated**: 2022-01-30 02:10:01+00:00
- **Authors**: Nyle Siddiqui, Rushit Dave, Tyler Bauer, Thomas Reither, Dylan Black, Mitchell Hanson
- **Comment**: None
- **Journal**: None
- **Summary**: Facial emotion recognition is a vast and complex problem space within the domain of computer vision and thus requires a universally accepted baseline method with which to evaluate proposed models. While test datasets have served this purpose in the academic sphere real world application and testing of such models lacks any real comparison. Therefore we propose a framework in which models developed for FER can be compared and contrasted against one another in a constant standardized fashion. A lightweight convolutional neural network is trained on the AffectNet dataset a large variable dataset for facial emotion recognition and a web application is developed and deployed with our proposed framework as a proof of concept. The CNN is embedded into our application and is capable of instant real time facial emotion recognition. When tested on the AffectNet test set this model achieves high accuracy for emotion classification of eight different emotions. Using our framework the validity of this model and others can be properly tested by evaluating a model efficacy not only based on its accuracy on a sample test dataset, but also on in the wild experiments. Additionally, our application is built with the ability to save and store any image captured or uploaded to it for emotion recognition, allowing for the curation of more quality and diverse facial emotion recognition datasets.



### Low-Rank Tensor Completion Based on Bivariate Equivalent Minimax-Concave Penalty
- **Arxiv ID**: http://arxiv.org/abs/2201.12709v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.12709v3)
- **Published**: 2022-01-30 03:28:01+00:00
- **Updated**: 2022-09-17 10:43:19+00:00
- **Authors**: Hongbing Zhang, Xinyi Liu, Hongtao Fan, Yajing Li, Yinlin Ye
- **Comment**: arXiv admin note: text overlap with arXiv:2109.12257
- **Journal**: None
- **Summary**: Low-rank tensor completion (LRTC) is an important problem in computer vision and machine learning. The minimax-concave penalty (MCP) function as a non-convex relaxation has achieved good results in the LRTC problem. To makes all the constant parameters of the MCP function as variables so that futherly improving the adaptability to the change of singular values in the LRTC problem, we propose the bivariate equivalent minimax-concave penalty (BEMCP) theorem. Applying the BEMCP theorem to tensor singular values leads to the bivariate equivalent weighted tensor $\Gamma$-norm (BEWTGN) theorem, and we analyze and discuss its corresponding properties. Besides, to facilitate the solution of the LRTC problem, we give the proximal operators of the BEMCP theorem and BEWTGN. Meanwhile, we propose a BEMCP model for the LRTC problem, which is optimally solved based on alternating direction multiplier (ADMM). Finally, the proposed method is applied to the data restorations of multispectral image (MSI), magnetic resonance imaging (MRI) and color video (CV) in real-world, and the experimental results demonstrate that it outperforms the state-of-arts methods.



### Win the Lottery Ticket via Fourier Analysis: Frequencies Guided Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/2201.12712v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.12712v1)
- **Published**: 2022-01-30 03:42:36+00:00
- **Updated**: 2022-01-30 03:42:36+00:00
- **Authors**: Yuzhang Shang, Bin Duan, Ziliang Zong, Liqiang Nie, Yan Yan
- **Comment**: accepted to ICASSP 2022
- **Journal**: None
- **Summary**: With the remarkable success of deep learning recently, efficient network compression algorithms are urgently demanded for releasing the potential computational power of edge devices, such as smartphones or tablets. However, optimal network pruning is a non-trivial task which mathematically is an NP-hard problem. Previous researchers explain training a pruned network as buying a lottery ticket. In this paper, we investigate the Magnitude-Based Pruning (MBP) scheme and analyze it from a novel perspective through Fourier analysis on the deep learning model to guide model designation. Besides explaining the generalization ability of MBP using Fourier transform, we also propose a novel two-stage pruning approach, where one stage is to obtain the topological structure of the pruned network and the other stage is to retrain the pruned network to recover the capacity using knowledge distillation from lower to higher on the frequency domain. Extensive experiments on CIFAR-10 and CIFAR-100 demonstrate the superiority of our novel Fourier analysis based MBP compared to other traditional MBP algorithms.



### You Only Demonstrate Once: Category-Level Manipulation from Single Visual Demonstration
- **Arxiv ID**: http://arxiv.org/abs/2201.12716v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2201.12716v2)
- **Published**: 2022-01-30 03:59:14+00:00
- **Updated**: 2022-05-06 14:46:46+00:00
- **Authors**: Bowen Wen, Wenzhao Lian, Kostas Bekris, Stefan Schaal
- **Comment**: None
- **Journal**: Robotics: Science and Systems (RSS) 2022
- **Summary**: Promising results have been achieved recently in category-level manipulation that generalizes across object instances. Nevertheless, it often requires expensive real-world data collection and manual specification of semantic keypoints for each object category and task. Additionally, coarse keypoint predictions and ignoring intermediate action sequences hinder adoption in complex manipulation tasks beyond pick-and-place. This work proposes a novel, category-level manipulation framework that leverages an object-centric, category-level representation and model-free 6 DoF motion tracking. The canonical object representation is learned solely in simulation and then used to parse a category-level, task trajectory from a single demonstration video. The demonstration is reprojected to a target trajectory tailored to a novel object via the canonical representation. During execution, the manipulation horizon is decomposed into longrange, collision-free motion and last-inch manipulation. For the latter part, a category-level behavior cloning (CatBC) method leverages motion tracking to perform closed-loop control. CatBC follows the target trajectory, projected from the demonstration and anchored to a dynamically selected category-level coordinate frame. The frame is automatically selected along the manipulation horizon by a local attention mechanism. This framework allows to teach different manipulation strategies by solely providing a single demonstration, without complicated manual programming. Extensive experiments demonstrate its efficacy in a range of challenging industrial tasks in highprecision assembly, which involve learning complex, long-horizon policies. The process exhibits robustness against uncertainty due to dynamics as well as generalization across object instances and scene configurations. The supplementary video is available at https://www.youtube.com/watch?v=WAr8ZY3mYyw



### A Frustratingly Simple Approach for End-to-End Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2201.12723v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2201.12723v3)
- **Published**: 2022-01-30 04:44:54+00:00
- **Updated**: 2022-04-14 11:03:49+00:00
- **Authors**: Ziyang Luo, Yadong Xi, Rongsheng Zhang, Jing Ma
- **Comment**: Work in progress
- **Journal**: None
- **Summary**: Image Captioning is a fundamental task to join vision and language, concerning about cross-modal understanding and text generation. Recent years witness the emerging attention on image captioning. Most of existing works follow a traditional two-stage training paradigm. Before training the captioning models, an extra object detector is utilized to recognize the objects in the image at first. However, they require sizeable datasets with fine-grained object annotation for training the object detector, which is a daunting task. In addition, the errors of the object detectors are easy to propagate to the following captioning models, degenerating models' performance. To alleviate such defects, we propose a frustratingly simple but highly effective end-to-end image captioning framework, Visual Conditioned GPT (VC-GPT), by connecting the pre-trained visual encoder (CLIP-ViT) and language decoder (GPT2). Different from the vanilla connection method that directly inserts the cross-attention modules into GPT2, we come up with a self-ensemble cross-modal fusion mechanism that comprehensively considers both the single- and cross-modal knowledge. As a result, we do not need extra object detectors for model training. Experimental results conducted on three popular image captioning benchmarks (MSCOCO, Flickr30k and NoCaps) demonstrate that our VC-GPT achieves either the best or the second-best performance across all evaluation metrics over extensive baseline systems.



### Generalized Global Ranking-Aware Neural Architecture Ranker for Efficient Image Classifier Search
- **Arxiv ID**: http://arxiv.org/abs/2201.12725v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12725v2)
- **Published**: 2022-01-30 04:54:59+00:00
- **Updated**: 2022-07-12 07:46:53+00:00
- **Authors**: Bicheng Guo, Tao Chen, Shibo He, Haoyu Liu, Lilin Xu, Peng Ye, Jiming Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) is a powerful tool for automating effective image processing DNN designing. The ranking has been advocated to design an efficient performance predictor for NAS. The previous contrastive method solves the ranking problem by comparing pairs of architectures and predicting their relative performance. However, it only focuses on the rankings between two involved architectures and neglects the overall quality distributions of the search space, which may suffer generalization issues. A predictor, namely Neural Architecture Ranker (NAR) which concentrates on the global quality tier of specific architecture, is proposed to tackle such problems caused by the local perspective. The NAR explores the quality tiers of the search space globally and classifies each individual to the tier they belong to according to its global ranking. Thus, the predictor gains the knowledge of the performance distributions of the search space which helps to generalize its ranking ability to the datasets more easily. Meanwhile, the global quality distribution facilitates the search phase by directly sampling candidates according to the statistics of quality tiers, which is free of training a search algorithm, e.g., Reinforcement Learning (RL) or Evolutionary Algorithm (EA), thus it simplifies the NAS pipeline and saves the computational overheads. The proposed NAR achieves better performance than the state-of-the-art methods on two widely used datasets for NAS research. On the vast search space of NAS-Bench-101, the NAR easily finds the architecture with top 0.01$\unicode{x2030}$ performance only by sampling. It also generalizes well to different image datasets of NAS-Bench-201, i.e., CIFAR-10, CIFAR-100, and ImageNet-16-120 by identifying the optimal architectures for each of them.



### Video-based Facial Micro-Expression Analysis: A Survey of Datasets, Features and Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2201.12728v2
- **DOI**: 10.1109/TPAMI.2021.3067464
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12728v2)
- **Published**: 2022-01-30 05:14:13+00:00
- **Updated**: 2022-02-16 03:56:14+00:00
- **Authors**: Xianye Ben, Yi Ren, Junping Zhang, Su-Jing Wang, Kidiyo Kpalma, Weixiao Meng, Yong-Jin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Unlike the conventional facial expressions, micro-expressions are involuntary and transient facial expressions capable of revealing the genuine emotions that people attempt to hide. Therefore, they can provide important information in a broad range of applications such as lie detection, criminal detection, etc. Since micro-expressions are transient and of low intensity, however, their detection and recognition is difficult and relies heavily on expert experiences. Due to its intrinsic particularity and complexity, video-based micro-expression analysis is attractive but challenging, and has recently become an active area of research. Although there have been numerous developments in this area, thus far there has been no comprehensive survey that provides researchers with a systematic overview of these developments with a unified evaluation. Accordingly, in this survey paper, we first highlight the key differences between macro- and micro-expressions, then use these differences to guide our research survey of video-based micro-expression analysis in a cascaded structure, encompassing the neuropsychological basis, datasets, features, spotting algorithms, recognition algorithms, applications and evaluation of state-of-the-art approaches. For each aspect, the basic techniques, advanced developments and major challenges are addressed and discussed. Furthermore, after considering the limitations of existing micro-expression datasets, we present and release a new dataset - called micro-and-macro expression warehouse (MMEW) - containing more video samples and more labeled emotion types. We then perform a unified comparison of representative methods on CAS(ME)2 for spotting, and on MMEW and SAMM for recognition, respectively. Finally, some potential future research directions are explored and outlined.



### TPC: Transformation-Specific Smoothing for Point Cloud Models
- **Arxiv ID**: http://arxiv.org/abs/2201.12733v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.12733v5)
- **Published**: 2022-01-30 05:41:50+00:00
- **Updated**: 2023-05-06 09:43:47+00:00
- **Authors**: Wenda Chu, Linyi Li, Bo Li
- **Comment**: Accepted as a conference paper at ICML 2022
- **Journal**: None
- **Summary**: Point cloud models with neural network architectures have achieved great success and have been widely used in safety-critical applications, such as Lidar-based recognition systems in autonomous vehicles. However, such models are shown vulnerable to adversarial attacks which aim to apply stealthy semantic transformations such as rotation and tapering to mislead model predictions. In this paper, we propose a transformation-specific smoothing framework TPC, which provides tight and scalable robustness guarantees for point cloud models against semantic transformation attacks. We first categorize common 3D transformations into three categories: additive (e.g., shearing), composable (e.g., rotation), and indirectly composable (e.g., tapering), and we present generic robustness certification strategies for all categories respectively. We then specify unique certification protocols for a range of specific semantic transformations and their compositions. Extensive experiments on several common 3D transformations show that TPC significantly outperforms the state of the art. For example, our framework boosts the certified accuracy against twisting transformation along z-axis (within 20$^\circ$) from 20.3$\%$ to 83.8$\%$. Codes and models are available at https://github.com/chuwd19/Point-Cloud-Smoothing.



### RIM-Net: Recursive Implicit Fields for Unsupervised Learning of Hierarchical Shape Structures
- **Arxiv ID**: http://arxiv.org/abs/2201.12763v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12763v2)
- **Published**: 2022-01-30 09:31:24+00:00
- **Updated**: 2022-03-28 07:51:50+00:00
- **Authors**: Chengjie Niu, Manyi Li, Kai Xu, Hao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce RIM-Net, a neural network which learns recursive implicit fields for unsupervised inference of hierarchical shape structures. Our network recursively decomposes an input 3D shape into two parts, resulting in a binary tree hierarchy. Each level of the tree corresponds to an assembly of shape parts, represented as implicit functions, to reconstruct the input shape. At each node of the tree, simultaneous feature decoding and shape decomposition are carried out by their respective feature and part decoders, with weight sharing across the same hierarchy level. As an implicit field decoder, the part decoder is designed to decompose a sub-shape, via a two-way branched reconstruction, where each branch predicts a set of parameters defining a Gaussian to serve as a local point distribution for shape reconstruction. With reconstruction losses accounted for at each hierarchy level and a decomposition loss at each node, our network training does not require any ground-truth segmentations, let alone hierarchies. Through extensive experiments and comparisons to state-of-the-art alternatives, we demonstrate the quality, consistency, and interpretability of hierarchical structural inference by RIM-Net.



### Improving Robustness by Enhancing Weak Subnets
- **Arxiv ID**: http://arxiv.org/abs/2201.12765v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12765v2)
- **Published**: 2022-01-30 09:36:19+00:00
- **Updated**: 2022-07-20 18:04:39+00:00
- **Authors**: Yong Guo, David Stutz, Bernt Schiele
- **Comment**: To appear in ECCV 2022
- **Journal**: None
- **Summary**: Despite their success, deep networks have been shown to be highly susceptible to perturbations, often causing significant drops in accuracy. In this paper, we investigate model robustness on perturbed inputs by studying the performance of internal sub-networks (subnets). Interestingly, we observe that most subnets show particularly poor robustness against perturbations. More importantly, these weak subnets are correlated with the overall lack of robustness. Tackling this phenomenon, we propose a new training procedure that identifies and enhances weak subnets (EWS) to improve robustness. Specifically, we develop a search algorithm to find particularly weak subnets and explicitly strengthen them via knowledge distillation from the full network. We show that EWS greatly improves both robustness against corrupted images as well as accuracy on clean data. Being complementary to popular data augmentation methods, EWS consistently improves robustness when combined with these approaches. To highlight the flexibility of our approach, we combine EWS also with popular adversarial training methods resulting in improved adversarial robustness.



### MVP-Net: Multiple View Pointwise Semantic Segmentation of Large-Scale Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2201.12769v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.12769v4)
- **Published**: 2022-01-30 09:43:00+00:00
- **Updated**: 2022-05-31 08:39:23+00:00
- **Authors**: Chuanyu Luo, Xiaohan Li, Nuo Cheng, Han Li, Shengguang Lei, Pu Li
- **Comment**: None
- **Journal**: 30. International Conference in Central Europe on Computer
  Graphics, Visualization and Computer Vision(WSCG), 2022
- **Summary**: Semantic segmentation of 3D point cloud is an essential task for autonomous driving environment perception. The pipeline of most pointwise point cloud semantic segmentation methods includes points sampling, neighbor searching, feature aggregation, and classification. Neighbor searching method like K-nearest neighbors algorithm, KNN, has been widely applied. However, the complexity of KNN is always a bottleneck of efficiency. In this paper, we propose an end-to-end neural architecture, Multiple View Pointwise Net, MVP-Net, to efficiently and directly infer large-scale outdoor point cloud without KNN or any complex pre/postprocessing. Instead, assumption-based space filling curves and multi-rotation of point cloud methods are introduced to point feature aggregation and receptive field expanding. Numerical experiments show that the proposed MVP-Net is 11 times faster than the most efficient pointwise semantic segmentation method RandLA-Net and achieves the same accuracy on the large-scale benchmark SemanticKITTI dataset.



### Self-Supervised Moving Vehicle Detection from Audio-Visual Cues
- **Arxiv ID**: http://arxiv.org/abs/2201.12771v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12771v2)
- **Published**: 2022-01-30 09:52:14+00:00
- **Updated**: 2022-06-13 06:12:31+00:00
- **Authors**: Jannik Zürn, Wolfram Burgard
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: Robust detection of moving vehicles is a critical task for any autonomously operating outdoor robot or self-driving vehicle. Most modern approaches for solving this task rely on training image-based detectors using large-scale vehicle detection datasets such as nuScenes or the Waymo Open Dataset. Providing manual annotations is an expensive and laborious exercise that does not scale well in practice. To tackle this problem, we propose a self-supervised approach that leverages audio-visual cues to detect moving vehicles in videos. Our approach employs contrastive learning for localizing vehicles in images from corresponding pairs of images and recorded audio. In extensive experiments carried out with a real-world dataset, we demonstrate that our approach provides accurate detections of moving vehicles and does not require manual annotations. We furthermore show that our model can be used as a teacher to supervise an audio-only detection model. This student model is invariant to illumination changes and thus effectively bridges the domain gap inherent to models leveraging exclusively vision as the predominant modality.



### Practical Noise Simulation for RGB Images
- **Arxiv ID**: http://arxiv.org/abs/2201.12773v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12773v1)
- **Published**: 2022-01-30 09:58:59+00:00
- **Updated**: 2022-01-30 09:58:59+00:00
- **Authors**: Saeed Ranjbar Alvar, Ivan V. Bajić
- **Comment**: Reference paper for the code
- **Journal**: None
- **Summary**: This document describes a noise generator that simulates realistic noise found in smartphone cameras. The generator simulates Poissonian-Gaussian noise whose parameters have been estimated on the Smartphone Image Denoising Dataset (SIDD). The generator is available online, and is currently being used in compressed-domain denoising exploration experiments in JPEG AI.



### TransBTSV2: Towards Better and More Efficient Volumetric Segmentation of Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2201.12785v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12785v3)
- **Published**: 2022-01-30 11:00:34+00:00
- **Updated**: 2022-05-17 12:28:08+00:00
- **Authors**: Jiangyun Li, Wenxuan Wang, Chen Chen, Tianxiang Zhang, Sen Zha, Jing Wang, Hong Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Transformer, benefiting from global (long-range) information modeling using self-attention mechanism, has been successful in natural language processing and computer vision recently. Convolutional Neural Networks, capable of capturing local features, are difficult to model explicit long-distance dependencies from global feature space. However, both local and global features are crucial for dense prediction tasks, especially for 3D medical image segmentation. In this paper, we present the further attempt to exploit Transformer in 3D CNN for 3D medical image volumetric segmentation and propose a novel network named TransBTSV2 based on the encoder-decoder structure. Different from TransBTS, the proposed TransBTSV2 is not limited to brain tumor segmentation (BTS) but focuses on general medical image segmentation, providing a stronger and more efficient 3D baseline for volumetric segmentation of medical images. As a hybrid CNN-Transformer architecture, TransBTSV2 can achieve accurate segmentation of medical images without any pre-training, possessing the strong inductive bias as CNNs and powerful global context modeling ability as Transformer. With the proposed insight to redesign the internal structure of Transformer block and the introduced Deformable Bottleneck Module to capture shape-aware local details, a highly efficient architecture is achieved with superior performance. Extensive experimental results on four medical image datasets (BraTS 2019, BraTS 2020, LiTS 2017 and KiTS 2019) demonstrate that TransBTSV2 achieves comparable or better results compared to the state-of-the-art methods for the segmentation of brain tumor, liver tumor as well as kidney tumor. Code will be publicly available at https://github.com/Wenxuan-1119/TransBTS.



### SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2201.12792v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2201.12792v2)
- **Published**: 2022-01-30 11:49:29+00:00
- **Updated**: 2022-04-05 13:47:11+00:00
- **Authors**: Boyi Jiang, Yang Hong, Hujun Bao, Juyong Zhang
- **Comment**: CVPR 2022, Oral. Project page: https://jby1993.github.io/SelfRecon/
- **Journal**: None
- **Summary**: We propose SelfRecon, a clothed human body reconstruction method that combines implicit and explicit representations to recover space-time coherent geometries from a monocular self-rotating human video. Explicit methods require a predefined template mesh for a given sequence, while the template is hard to acquire for a specific subject. Meanwhile, the fixed topology limits the reconstruction accuracy and clothing types. Implicit representation supports arbitrary topology and can represent high-fidelity geometry shapes due to its continuous nature. However, it is difficult to integrate multi-frame information to produce a consistent registration sequence for downstream applications. We propose to combine the advantages of both representations. We utilize differential mask loss of the explicit mesh to obtain the coherent overall shape, while the details on the implicit surface are refined with the differentiable neural rendering. Meanwhile, the explicit mesh is updated periodically to adjust its topology changes, and a consistency loss is designed to match both representations. Compared with existing methods, SelfRecon can produce high-fidelity surfaces for arbitrary clothed humans with self-supervised optimization. Extensive experimental results demonstrate its effectiveness on real captured monocular videos. The source code is available at https://github.com/jby1993/SelfReconCode.



### Generalizing similarity in noisy setups: the DIBS phenomenon
- **Arxiv ID**: http://arxiv.org/abs/2201.12803v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2201.12803v3)
- **Published**: 2022-01-30 12:53:51+00:00
- **Updated**: 2023-07-24 15:27:16+00:00
- **Authors**: Nayara Fonseca, Veronica Guidetti
- **Comment**: v3: version accepted at ECAI 2023 + Supplementary Material
- **Journal**: None
- **Summary**: This work uncovers an interplay among data density, noise, and the generalization ability in similarity learning. We consider Siamese Neural Networks (SNNs), which are the basic form of contrastive learning, and explore two types of noise that can impact SNNs, Pair Label Noise (PLN) and Single Label Noise (SLN). Our investigation reveals that SNNs exhibit double descent behaviour regardless of the training setup and that it is further exacerbated by noise. We demonstrate that the density of data pairs is crucial for generalization. When SNNs are trained on sparse datasets with the same amount of PLN or SLN, they exhibit comparable generalization properties. However, when using dense datasets, PLN cases generalize worse than SLN ones in the overparametrized region, leading to a phenomenon we call Density-Induced Break of Similarity (DIBS). In this regime, PLN similarity violation becomes macroscopical, corrupting the dataset to the point where complete interpolation cannot be achieved, regardless of the number of model parameters. Our analysis also delves into the correspondence between online optimization and offline generalization in similarity learning. The results show that this equivalence fails in the presence of label noise in all the scenarios considered.



### Automatic Segmentation of Left Ventricle in Cardiac Magnetic Resonance Images
- **Arxiv ID**: http://arxiv.org/abs/2201.12805v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12805v1)
- **Published**: 2022-01-30 13:05:35+00:00
- **Updated**: 2022-01-30 13:05:35+00:00
- **Authors**: Garvit Chhabra, J. H. Gagan, J. R. Harish Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation of the left ventricle in cardiac magnetic resonance imaging MRI scans enables cardiologists to calculate the volume of the left ventricle and subsequently its ejection fraction. The ejection fraction is a measurement that expresses the percentage of blood leaving the heart with each contraction. Cardiologists often use ejection fraction to determine one's cardiac function. We propose multiscale template matching technique for detection and an elliptical active disc for automated segmentation of the left ventricle in MR images. The elliptical active disc optimizes the local energy function with respect to its five free parameters which define the disc. Gradient descent is used to minimize the energy function along with Green's theorem to optimize the computation expenses. We report validations on 320 scans containing 5,273 annotated slices which are publicly available through the Multi-Centre, Multi-Vendor, and Multi-Disease Cardiac Segmentation (M&Ms) Challenge. We achieved successful localization of the left ventricle in 89.63% of the cases and a Dice coefficient of 0.873 on diastole slices and 0.770 on systole slices. The proposed technique is based on traditional image processing techniques with a performance on par with the deep learning techniques.



### Contrastive Learning from Demonstrations
- **Arxiv ID**: http://arxiv.org/abs/2201.12813v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2201.12813v2)
- **Published**: 2022-01-30 13:36:07+00:00
- **Updated**: 2022-11-07 16:03:27+00:00
- **Authors**: André Correia, Luís A. Alexandre
- **Comment**: None
- **Journal**: IEEE Robotic Computing, Naples, Italy, December 5-7, 2022
- **Summary**: This paper presents a framework for learning visual representations from unlabeled video demonstrations captured from multiple viewpoints. We show that these representations are applicable for imitating several robotic tasks, including pick and place. We optimize a recently proposed self-supervised learning algorithm by applying contrastive learning to enhance task-relevant information while suppressing irrelevant information in the feature embeddings. We validate the proposed method on the publicly available Multi-View Pouring and a custom Pick and Place data sets and compare it with the TCN triplet baseline. We evaluate the learned representations using three metrics: viewpoint alignment, stage classification and reinforcement learning, and in all cases the results improve when compared to state-of-the-art approaches, with the added benefit of reduced number of training iterations.



### OptG: Optimizing Gradient-driven Criteria in Network Sparsity
- **Arxiv ID**: http://arxiv.org/abs/2201.12826v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12826v4)
- **Published**: 2022-01-30 14:15:49+00:00
- **Updated**: 2022-12-01 01:13:49+00:00
- **Authors**: Yuxin Zhang, Mingbao Lin, Mengzhao Chen, Fei Chao, Rongrong Ji
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Network sparsity receives popularity mostly due to its capability to reduce the network complexity. Extensive studies excavate gradient-driven sparsity. Typically, these methods are constructed upon premise of weight independence, which however, is contrary to the fact that weights are mutually influenced. Thus, their performance remains to be improved. In this paper, we propose to optimize gradient-driven sparsity (OptG) by solving this independence paradox. Our motive comes from the recent advances in supermask training which shows that high-performing sparse subnetworks can be located by simply updating mask values without modifying any weight. We prove that supermask training is to accumulate the criteria of gradient-driven sparsity for both removed and preserved weights, and it can partly solve the independence paradox. Consequently, OptG integrates supermask training into gradient-driven sparsity, and a novel supermask optimizer is further proposed to comprehensively mitigate the independence paradox. Experiments show that OptG can well surpass many existing state-of-the-art competitors, especially at ultra-high sparsity levels. Our code is available at \url{https://github.com/zyxxmu/OptG}.



### Comprehensive Saliency Fusion for Object Co-segmentation
- **Arxiv ID**: http://arxiv.org/abs/2201.12828v1
- **DOI**: 10.1109/ISM52913.2021.00026
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12828v1)
- **Published**: 2022-01-30 14:22:58+00:00
- **Updated**: 2022-01-30 14:22:58+00:00
- **Authors**: Harshit Singh Chhabra, Koteswar Rao Jerripothula
- **Comment**: Published in IEEE ISM 2021. Please cite this paper in the following
  manner. H. S. Chhabra and K. Rao Jerripothula, "Comprehensive Saliency Fusion
  for Object Co-segmentation," 2021 IEEE International Symposium on Multimedia
  (ISM), 2021, pp. 107-110, doi: 10.1109/ISM52913.2021.00026
- **Journal**: 2021 IEEE International Symposium on Multimedia (ISM), 2021, pp.
  107-110
- **Summary**: Object co-segmentation has drawn significant attention in recent years, thanks to its clarity on the expected foreground, the shared object in a group of images. Saliency fusion has been one of the promising ways to carry it out. However, prior works either fuse saliency maps of the same image or saliency maps of different images to extract the expected foregrounds. Also, they rely on hand-crafted saliency extraction and correspondence processes in most cases. This paper revisits the problem and proposes fusing saliency maps of both the same image and different images. It also leverages advances in deep learning for the saliency extraction and correspondence processes. Hence, we call it comprehensive saliency fusion. Our experiments reveal that our approach achieves much-improved object co-segmentation results compared to prior works on important benchmark datasets such as iCoseg, MSRC, and Internet Images.



### A Dataset for Medical Instructional Video Classification and Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2201.12888v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2201.12888v1)
- **Published**: 2022-01-30 18:06:31+00:00
- **Updated**: 2022-01-30 18:06:31+00:00
- **Authors**: Deepak Gupta, Kush Attal, Dina Demner-Fushman
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a new challenge and datasets to foster research toward designing systems that can understand medical videos and provide visual answers to natural language questions. We believe medical videos may provide the best possible answers to many first aids, medical emergency, and medical education questions. Toward this, we created the MedVidCL and MedVidQA datasets and introduce the tasks of Medical Video Classification (MVC) and Medical Visual Answer Localization (MVAL), two tasks that focus on cross-modal (medical language and medical video) understanding. The proposed tasks and datasets have the potential to support the development of sophisticated downstream applications that can benefit the public and medical practitioners. Our datasets consist of 6,117 annotated videos for the MVC task and 3,010 annotated questions and answers timestamps from 899 videos for the MVAL task. These datasets have been verified and corrected by medical informatics experts. We have also benchmarked each task with the created MedVidCL and MedVidQA datasets and proposed the multimodal learning methods that set competitive baselines for future research.



### Augmenting Novelty Search with a Surrogate Model to Engineer Meta-Diversity in Ensembles of Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2201.12896v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2201.12896v3)
- **Published**: 2022-01-30 19:13:32+00:00
- **Updated**: 2022-02-08 02:34:43+00:00
- **Authors**: Rui P. Cardoso, Emma Hart, David Burth Kurka, Jeremy V. Pitt
- **Comment**: 16 pages, 4 figures, 3 tables, EvoStar 2022
- **Journal**: None
- **Summary**: Using Neuroevolution combined with Novelty Search to promote behavioural diversity is capable of constructing high-performing ensembles for classification. However, using gradient descent to train evolved architectures during the search can be computationally prohibitive. Here we propose a method to overcome this limitation by using a surrogate model which estimates the behavioural distance between two neural network architectures required to calculate the sparseness term in Novelty Search. We demonstrate a speedup of 10 times over previous work and significantly improve on previous reported results on three benchmark datasets from Computer Vision -- CIFAR-10, CIFAR-100, and SVHN. This results from the expanded architecture search space facilitated by using a surrogate. Our method represents an improved paradigm for implementing horizontal scaling of learning algorithms by making an explicit search for diversity considerably more tractable for the same bounded resources.



### Aggregating Global Features into Local Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2201.12903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2201.12903v1)
- **Published**: 2022-01-30 19:57:35+00:00
- **Updated**: 2022-01-30 19:57:35+00:00
- **Authors**: Krushi Patel, Andres M. Bur, Fengjun Li, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Local Transformer-based classification models have recently achieved promising results with relatively low computational costs. However, the effect of aggregating spatial global information of local Transformer-based architecture is not clear. This work investigates the outcome of applying a global attention-based module named multi-resolution overlapped attention (MOA) in the local window-based transformer after each stage. The proposed MOA employs slightly larger and overlapped patches in the key to enable neighborhood pixel information transmission, which leads to significant performance gain. In addition, we thoroughly investigate the effect of the dimension of essential architecture components through extensive experiments and discover an optimum architecture design. Extensive experimental results CIFAR-10, CIFAR-100, and ImageNet-1K datasets demonstrate that the proposed approach outperforms previous vision Transformers with a comparatively fewer number of parameters.



### COIN++: Neural Compression Across Modalities
- **Arxiv ID**: http://arxiv.org/abs/2201.12904v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2201.12904v3)
- **Published**: 2022-01-30 20:12:04+00:00
- **Updated**: 2022-12-08 11:07:51+00:00
- **Authors**: Emilien Dupont, Hrushikesh Loya, Milad Alizadeh, Adam Goliński, Yee Whye Teh, Arnaud Doucet
- **Comment**: TMLR camera ready
- **Journal**: None
- **Summary**: Neural compression algorithms are typically based on autoencoders that require specialized encoder and decoder architectures for different data modalities. In this paper, we propose COIN++, a neural compression framework that seamlessly handles a wide range of data modalities. Our approach is based on converting data to implicit neural representations, i.e. neural functions that map coordinates (such as pixel locations) to features (such as RGB values). Then, instead of storing the weights of the implicit neural representation directly, we store modulations applied to a meta-learned base network as a compressed code for the data. We further quantize and entropy code these modulations, leading to large compression gains while reducing encoding time by two orders of magnitude compared to baselines. We empirically demonstrate the feasibility of our method by compressing various data modalities, from images and audio to medical and climate data.



### Sparse Centroid-Encoder: A Nonlinear Model for Feature Selection
- **Arxiv ID**: http://arxiv.org/abs/2201.12910v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2201.12910v2)
- **Published**: 2022-01-30 20:46:24+00:00
- **Updated**: 2022-06-28 21:54:04+00:00
- **Authors**: Tomojit Ghosh, Michael Kirby
- **Comment**: 13 pages,56 figures, 5 tables. Used 12 data sets and 5
  state-of-the-art models for comparison
- **Journal**: None
- **Summary**: Autoencoders have been widely used as a nonlinear tool for data dimensionality reduction. While autoencoders don't utilize the label information, Centroid-Encoders (CE)\cite{ghosh2022supervised} use the class label in their learning process. In this study, we propose a sparse optimization using the Centroid-Encoder architecture to determine a minimal set of features that discriminate between two or more classes. The resulting algorithm, Sparse Centroid-Encoder (SCE), extracts discriminatory features in groups using a sparsity inducing $\ell_1$-norm while mapping a point to its class centroid. One key attribute of SCE is that it can extract informative features from a multi-modal data set, i.e., data sets whose classes appear to have multiple clusters. The algorithm is applied to a wide variety of real world data sets, including single-cell data, high dimensional biological data, image data, speech data, and accelerometer sensor data. We compared our method to various state-of-the-art feature selection techniques, including supervised Concrete Autoencoders (SCAE), Feature Selection Network (FsNet), deep feature selection (DFS), Stochastic Gate (STG), and LassoNet. We empirically showed that SCE features often produced better classification accuracy than other methods on sequester test set.



### Compositionality as Lexical Symmetry
- **Arxiv ID**: http://arxiv.org/abs/2201.12926v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2201.12926v2)
- **Published**: 2022-01-30 21:44:46+00:00
- **Updated**: 2023-07-05 17:59:33+00:00
- **Authors**: Ekin Akyürek, Jacob Andreas
- **Comment**: ACL2023 Final Version
- **Journal**: None
- **Summary**: In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general and model-agnostic formulation of compositionality as a constraint on symmetries of data distributions rather than models. Informally, we prove that whenever a task can be solved by a compositional model, there is a corresponding data augmentation scheme -- a procedure for transforming examples into other well formed examples -- that imparts compositional inductive bias on any model trained to solve the same task. We describe a procedure called LEXSYM that discovers these transformations automatically, then applies them to training data for ordinary neural sequence models. Unlike existing compositional data augmentation procedures, LEXSYM can be deployed agnostically across text, structured data, and even images. It matches or surpasses state-of-the-art, task-specific models on COGS semantic parsing, SCAN and ALCHEMY instruction following, and CLEVR-COGENT visual question answering datasets.



