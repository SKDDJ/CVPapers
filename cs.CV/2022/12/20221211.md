# Arxiv Papers in cs.CV on 2022-12-11
### General Adversarial Defense Against Black-box Attacks via Pixel Level and Feature Level Distribution Alignments
- **Arxiv ID**: http://arxiv.org/abs/2212.05387v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05387v1)
- **Published**: 2022-12-11 01:51:31+00:00
- **Updated**: 2022-12-11 01:51:31+00:00
- **Authors**: Xiaogang Xu, Hengshuang Zhao, Philip Torr, Jiaya Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are vulnerable to the black-box adversarial attack that is highly transferable. This threat comes from the distribution gap between adversarial and clean samples in feature space of the target DNNs. In this paper, we use Deep Generative Networks (DGNs) with a novel training mechanism to eliminate the distribution gap. The trained DGNs align the distribution of adversarial samples with clean ones for the target DNNs by translating pixel values. Different from previous work, we propose a more effective pixel level training constraint to make this achievable, thus enhancing robustness on adversarial samples. Further, a class-aware feature-level constraint is formulated for integrated distribution alignment. Our approach is general and applicable to multiple tasks, including image classification, semantic segmentation, and object detection. We conduct extensive experiments on different datasets. Our strategy demonstrates its unique effectiveness and generality against black-box attacks.



### How to Backdoor Diffusion Models?
- **Arxiv ID**: http://arxiv.org/abs/2212.05400v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.05400v3)
- **Published**: 2022-12-11 03:44:38+00:00
- **Updated**: 2023-06-09 01:20:27+00:00
- **Authors**: Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho
- **Comment**: Accepted by CVPR 2023
- **Journal**: None
- **Summary**: Diffusion models are state-of-the-art deep learning empowered generative models that are trained based on the principle of learning forward and reverse diffusion processes via progressive noise-addition and denoising. To gain a better understanding of the limitations and potential risks, this paper presents the first study on the robustness of diffusion models against backdoor attacks. Specifically, we propose BadDiffusion, a novel attack framework that engineers compromised diffusion processes during model training for backdoor implantation. At the inference stage, the backdoored diffusion model will behave just like an untampered generator for regular data inputs, while falsely generating some targeted outcome designed by the bad actor upon receiving the implanted trigger signal. Such a critical risk can be dreadful for downstream tasks and applications built upon the problematic model. Our extensive experiments on various backdoor attack settings show that BadDiffusion can consistently lead to compromised diffusion models with high utility and target specificity. Even worse, BadDiffusion can be made cost-effective by simply finetuning a clean pre-trained diffusion model to implant backdoors. We also explore some possible countermeasures for risk mitigation. Our results call attention to potential risks and possible misuse of diffusion models. Our code is available on https://github.com/IBM/BadDiffusion.



### DiffAlign : Few-shot learning using diffusion based synthesis and alignment
- **Arxiv ID**: http://arxiv.org/abs/2212.05404v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05404v1)
- **Published**: 2022-12-11 04:37:43+00:00
- **Updated**: 2022-12-11 04:37:43+00:00
- **Authors**: Aniket Roy, Anshul Shah, Ketul Shah, Anirban Roy, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of few-shot classification where the goal is to learn a classifier from a limited set of samples. While data-driven learning is shown to be effective in various applications, learning from less data still remains challenging. To address this challenge, existing approaches consider various data augmentation techniques for increasing the number of training samples. Pseudo-labeling is commonly used in a few-shot setup, where approximate labels are estimated for a large set of unlabeled images. We propose DiffAlign which focuses on generating images from class labels. Specifically, we leverage the recent success of the generative models (e.g., DALL-E and diffusion models) that can generate realistic images from texts. However, naive learning on synthetic images is not adequate due to the domain gap between real and synthetic images. Thus, we employ a maximum mean discrepancy (MMD) loss to align the synthetic images to the real images minimizing the domain gap. We evaluate our method on the standard few-shot classification benchmarks: CIFAR-FS, FC100, miniImageNet, tieredImageNet and a cross-domain few-shot classification benchmark: miniImageNet to CUB. The proposed approach significantly outperforms the stateof-the-art in both 5-shot and 1-shot setups on these benchmarks. Our approach is also shown to be effective in the zero-shot classification setup



### Teaching What You Should Teach: A Data-Based Distillation Method
- **Arxiv ID**: http://arxiv.org/abs/2212.05422v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05422v6)
- **Published**: 2022-12-11 06:22:14+00:00
- **Updated**: 2023-05-20 11:55:10+00:00
- **Authors**: Shitong Shao, Huanran Chen, Zhen Huang, Linrui Gong, Shuai Wang, Xinxiao Wu
- **Comment**: 13 pages, 4 figures Accepted by IJCAI2023
- **Journal**: None
- **Summary**: In real teaching scenarios, an excellent teacher always teaches what he (or she) is good at but the student is not. This gives the student the best assistance in making up for his (or her) weaknesses and becoming a good one overall. Enlightened by this, we introduce the "Teaching what you Should Teach" strategy into a knowledge distillation framework, and propose a data-based distillation method named "TST" that searches for desirable augmented samples to assist in distilling more efficiently and rationally. To be specific, we design a neural network-based data augmentation module with priori bias, which assists in finding what meets the teacher's strengths but the student's weaknesses, by learning magnitudes and probabilities to generate suitable data samples. By training the data augmentation module and the generalized distillation paradigm in turn, a student model is learned with excellent generalization ability. To verify the effectiveness of our method, we conducted extensive comparative experiments on object recognition, detection, and segmentation tasks. The results on the CIFAR-10, ImageNet-1k, MS-COCO, and Cityscapes datasets demonstrate that our method achieves state-of-the-art performance on almost all teacher-student pairs. Furthermore, we conduct visualization studies to explore what magnitudes and probabilities are needed for the distillation process.



### Ego Vehicle Speed Estimation using 3D Convolution with Masked Attention
- **Arxiv ID**: http://arxiv.org/abs/2212.05432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05432v1)
- **Published**: 2022-12-11 07:22:25+00:00
- **Updated**: 2022-12-11 07:22:25+00:00
- **Authors**: Athul M. Mathew, Thariq Khalid
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Speed estimation of an ego vehicle is crucial to enable autonomous driving and advanced driver assistance technologies. Due to functional and legacy issues, conventional methods depend on in-car sensors to extract vehicle speed through the Controller Area Network bus. However, it is desirable to have modular systems that are not susceptible to external sensors to execute perception tasks. In this paper, we propose a novel 3D-CNN with masked-attention architecture to estimate ego vehicle speed using a single front-facing monocular camera. To demonstrate the effectiveness of our method, we conduct experiments on two publicly available datasets, nuImages and KITTI. We also demonstrate the efficacy of masked-attention by comparing our method with a traditional 3D-CNN.



### 2D/3D Deep Image Registration by Learning 3D Displacement Fields for Abdominal Organs
- **Arxiv ID**: http://arxiv.org/abs/2212.05445v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.05445v1)
- **Published**: 2022-12-11 08:36:23+00:00
- **Updated**: 2022-12-11 08:36:23+00:00
- **Authors**: Ryuto Miura, Megumi Nakao, Mitsuhiro Nakamura, Tetsuya Matsuda
- **Comment**: None
- **Journal**: None
- **Summary**: Deformable registration of two-dimensional/three-dimensional (2D/3D) images of abdominal organs is a complicated task because the abdominal organs deform significantly and their contours are not detected in two-dimensional X-ray images. We propose a supervised deep learning framework that achieves 2D/3D deformable image registration between 3D volumes and single-viewpoint 2D projected images. The proposed method learns the translation from the target 2D projection images and the initial 3D volume to 3D displacement fields. In experiments, we registered 3D-computed tomography (CT) volumes to digitally reconstructed radiographs generated from abdominal 4D-CT volumes. For validation, we used 4D-CT volumes of 35 cases and confirmed that the 3D-CT volumes reflecting the nonlinear and local respiratory organ displacement were reconstructed. The proposed method demonstrate the compatible performance to the conventional methods with a dice similarity coefficient of 91.6 \% for the liver region and 85.9 \% for the stomach region, while estimating a significantly more accurate CT values.



### Vision Transformer with Attentive Pooling for Robust Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2212.05463v1
- **DOI**: 10.1109/TAFFC.2022.3226473
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05463v1)
- **Published**: 2022-12-11 10:33:19+00:00
- **Updated**: 2022-12-11 10:33:19+00:00
- **Authors**: Fanglei Xue, Qiangchang Wang, Zichang Tan, Zhongsong Ma, Guodong Guo
- **Comment**: Codes will be public on https://github.com/youqingxiaozhua/APViT
- **Journal**: None
- **Summary**: Facial Expression Recognition (FER) in the wild is an extremely challenging task. Recently, some Vision Transformers (ViT) have been explored for FER, but most of them perform inferiorly compared to Convolutional Neural Networks (CNN). This is mainly because the new proposed modules are difficult to converge well from scratch due to lacking inductive bias and easy to focus on the occlusion and noisy areas. TransFER, a representative transformer-based method for FER, alleviates this with multi-branch attention dropping but brings excessive computations. On the contrary, we present two attentive pooling (AP) modules to pool noisy features directly. The AP modules include Attentive Patch Pooling (APP) and Attentive Token Pooling (ATP). They aim to guide the model to emphasize the most discriminative features while reducing the impacts of less relevant features. The proposed APP is employed to select the most informative patches on CNN features, and ATP discards unimportant tokens in ViT. Being simple to implement and without learnable parameters, the APP and ATP intuitively reduce the computational cost while boosting the performance by ONLY pursuing the most discriminative features. Qualitative results demonstrate the motivations and effectiveness of our attentive poolings. Besides, quantitative results on six in-the-wild datasets outperform other state-of-the-art methods.



### SEPT: Towards Scalable and Efficient Visual Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2212.05473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05473v1)
- **Published**: 2022-12-11 11:02:11+00:00
- **Updated**: 2022-12-11 11:02:11+00:00
- **Authors**: Yiqi Lin, Huabin Zheng, Huaping Zhong, Jinjing Zhu, Weijia Li, Conghui He, Lin Wang
- **Comment**: Accepted by AAAI 2023
- **Journal**: None
- **Summary**: Recently, the self-supervised pre-training paradigm has shown great potential in leveraging large-scale unlabeled data to improve downstream task performance. However, increasing the scale of unlabeled pre-training data in real-world scenarios requires prohibitive computational costs and faces the challenge of uncurated samples. To address these issues, we build a task-specific self-supervised pre-training framework from a data selection perspective based on a simple hypothesis that pre-training on the unlabeled samples with similar distribution to the target task can bring substantial performance gains. Buttressed by the hypothesis, we propose the first yet novel framework for Scalable and Efficient visual Pre-Training (SEPT) by introducing a retrieval pipeline for data selection. SEPT first leverage a self-supervised pre-trained model to extract the features of the entire unlabeled dataset for retrieval pipeline initialization. Then, for a specific target task, SEPT retrievals the most similar samples from the unlabeled dataset based on feature similarity for each target instance for pre-training. Finally, SEPT pre-trains the target model with the selected unlabeled samples in a self-supervised manner for target data finetuning. By decoupling the scale of pre-training and available upstream data for a target task, SEPT achieves high scalability of the upstream dataset and high efficiency of pre-training, resulting in high model architecture flexibility. Results on various downstream tasks demonstrate that SEPT can achieve competitive or even better performance compared with ImageNet pre-training while reducing the size of training samples by one magnitude without resorting to any extra annotations.



### Mul-GAD: a semi-supervised graph anomaly detection framework via aggregating multi-view information
- **Arxiv ID**: http://arxiv.org/abs/2212.05478v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.05478v1)
- **Published**: 2022-12-11 11:34:34+00:00
- **Updated**: 2022-12-11 11:34:34+00:00
- **Authors**: Zhiyuan Liu, Chunjie Cao, Jingzhang Sun
- **Comment**: Graph anomaly detection on attribute network
- **Journal**: None
- **Summary**: Anomaly detection is defined as discovering patterns that do not conform to the expected behavior. Previously, anomaly detection was mostly conducted using traditional shallow learning techniques, but with little improvement. As the emergence of graph neural networks (GNN), graph anomaly detection has been greatly developed. However, recent studies have shown that GNN-based methods encounter challenge, in that no graph anomaly detection algorithm can perform generalization on most datasets. To bridge the tap, we propose a multi-view fusion approach for graph anomaly detection (Mul-GAD). The view-level fusion captures the extent of significance between different views, while the feature-level fusion makes full use of complementary information. We theoretically and experimentally elaborate the effectiveness of the fusion strategies. For a more comprehensive conclusion, we further investigate the effect of the objective function and the number of fused views on detection performance. Exploiting these findings, our Mul-GAD is proposed equipped with fusion strategies and the well-performed objective function. Compared with other state-of-the-art detection methods, we achieve a better detection performance and generalization in most scenarios via a series of experiments conducted on Pubmed, Amazon Computer, Amazon Photo, Weibo and Books. Our code is available at https://github.com/liuyishoua/Mul-Graph-Fusion.



### Target Detection Framework for Lobster Eye X-Ray Telescopes with Machine Learning Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2212.05497v1
- **DOI**: 10.3847/1538-4365/acab02
- **Categories**: **astro-ph.IM**, astro-ph.HE, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.05497v1)
- **Published**: 2022-12-11 13:10:44+00:00
- **Updated**: 2022-12-11 13:10:44+00:00
- **Authors**: Peng Jia, Wenbo Liu, Yuan Liu, Haiwu Pan
- **Comment**: Accepted by the APJS Journal. Full source code could be downloaded
  from the China VO with DOI of https://doi.org/10.12149/101175. Docker version
  of the code could be obtained under request to the corresponding author
- **Journal**: None
- **Summary**: Lobster eye telescopes are ideal monitors to detect X-ray transients, because they could observe celestial objects over a wide field of view in X-ray band. However, images obtained by lobster eye telescopes are modified by their unique point spread functions, making it hard to design a high efficiency target detection algorithm. In this paper, we integrate several machine learning algorithms to build a target detection framework for data obtained by lobster eye telescopes. Our framework would firstly generate two 2D images with different pixel scales according to positions of photons on the detector. Then an algorithm based on morphological operations and two neural networks would be used to detect candidates of celestial objects with different flux from these 2D images. At last, a random forest algorithm will be used to pick up final detection results from candidates obtained by previous steps. Tested with simulated data of the Wide-field X-ray Telescope onboard the Einstein Probe, our detection framework could achieve over 94% purity and over 90% completeness for targets with flux more than 3 mCrab (9.6 * 10-11 erg/cm2/s) and more than 94% purity and moderate completeness for targets with lower flux at acceptable time cost. The framework proposed in this paper could be used as references for data processing methods developed for other lobster eye X-ray telescopes.



### Applicability limitations of differentiable full-reference image-quality
- **Arxiv ID**: http://arxiv.org/abs/2212.05499v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM, eess.IV, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2212.05499v2)
- **Published**: 2022-12-11 13:15:06+00:00
- **Updated**: 2023-01-04 01:50:17+00:00
- **Authors**: Maksim Siniukov, Dmitriy Kulikov, Dmitriy Vatolin
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Subjective image-quality measurement plays a critical role in the development of image-processing applications. The purpose of a visual-quality metric is to approximate the results of subjective assessment. In this regard, more and more metrics are under development, but little research has considered their limitations. This paper addresses that deficiency: we show how image preprocessing before compression can artificially increase the quality scores provided by the popular metrics DISTS, LPIPS, HaarPSI, and VIF as well as how these scores are inconsistent with subjective-quality scores. We propose a series of neural-network preprocessing models that increase DISTS by up to 34.5%, LPIPS by up to 36.8%, VIF by up to 98.0%, and HaarPSI by up to 22.6% in the case of JPEG-compressed images. A subjective comparison of preprocessed images showed that for most of the metrics we examined, visual quality drops or stays unchanged, limiting the applicability of these metrics.



### Low-rank Tensor Assisted K-space Generative Model for Parallel Imaging Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2212.05503v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.05503v1)
- **Published**: 2022-12-11 13:34:43+00:00
- **Updated**: 2022-12-11 13:34:43+00:00
- **Authors**: Wei Zhang, Zengwei Xiao, Hui Tao, Minghui Zhang, Xiaoling Xu, Qiegen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Although recent deep learning methods, especially generative models, have shown good performance in fast magnetic resonance imaging, there is still much room for improvement in high-dimensional generation. Considering that internal dimensions in score-based generative models have a critical impact on estimating the gradient of the data distribution, we present a new idea, low-rank tensor assisted k-space generative model (LR-KGM), for parallel imaging reconstruction. This means that we transform original prior information into high-dimensional prior information for learning. More specifically, the multi-channel data is constructed into a large Hankel matrix and the matrix is subsequently folded into tensor for prior learning. In the testing phase, the low-rank rotation strategy is utilized to impose low-rank constraints on tensor output of the generative network. Furthermore, we alternately use traditional generative iterations and low-rank high-dimensional tensor iterations for reconstruction. Experimental comparisons with the state-of-the-arts demonstrated that the proposed LR-KGM method achieved better performance.



### Focal-PETR: Embracing Foreground for Efficient Multi-Camera 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2212.05505v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05505v2)
- **Published**: 2022-12-11 13:38:54+00:00
- **Updated**: 2022-12-13 09:28:01+00:00
- **Authors**: Shihao Wang, Xiaohui Jiang, Ying Li
- **Comment**: Tech Report
- **Journal**: None
- **Summary**: The dominant multi-camera 3D detection paradigm is based on explicit 3D feature construction, which requires complicated indexing of local image-view features via 3D-to-2D projection. Other methods implicitly introduce geometric positional encoding and perform global attention (e.g., PETR) to build the relationship between image tokens and 3D objects. The 3D-to-2D perspective inconsistency and global attention lead to a weak correlation between foreground tokens and queries, resulting in slow convergence. We propose Focal-PETR with instance-guided supervision and spatial alignment module to adaptively focus object queries on discriminative foreground regions. Focal-PETR additionally introduces a down-sampling strategy to reduce the consumption of global attention. Due to the highly parallelized implementation and down-sampling strategy, our model, without depth supervision, achieves leading performance on the large-scale nuScenes benchmark and a superior speed of 30 FPS on a single RTX3090 GPU. Extensive experiments show that our method outperforms PETR while consuming 3x fewer training hours. The code will be made publicly available.



### Mutimodal Ranking Optimization for Heterogeneous Face Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2212.05510v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05510v1)
- **Published**: 2022-12-11 14:04:24+00:00
- **Updated**: 2022-12-11 14:04:24+00:00
- **Authors**: Hui Hu, Jiawei Zhang, Zhen Han
- **Comment**: None
- **Journal**: None
- **Summary**: Heterogeneous face re-identification, namely matching heterogeneous faces across disjoint visible light (VIS) and near-infrared (NIR) cameras, has become an important problem in video surveillance application. However, the large domain discrepancy between heterogeneous NIR-VIS faces makes the performance of face re-identification degraded dramatically. To solve this problem, a multimodal fusion ranking optimization algorithm for heterogeneous face re-identification is proposed in this paper. Firstly, we design a heterogeneous face translation network to obtain multimodal face pairs, including NIR-VIS/NIR-NIR/VIS-VIS face pairs, through mutual transformation between NIR-VIS faces. Secondly, we propose linear and non-linear fusion strategies to aggregate initial ranking lists of multimodal face pairs and acquire the optimized re-ranked list based on modal complementarity. The experimental results show that the proposed multimodal fusion ranking optimization algorithm can effectively utilize the complementarity and outperforms some relative methods on the SCface dataset.



### Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned Receipt Images
- **Arxiv ID**: http://arxiv.org/abs/2212.05525v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.05525v2)
- **Published**: 2022-12-11 15:45:26+00:00
- **Updated**: 2022-12-13 03:19:37+00:00
- **Authors**: Hongkuan Zhang, Edward Whittaker, Ikuo Kitagishi
- **Comment**: None
- **Journal**: None
- **Summary**: Digitization of scanned receipts aims to extract text from receipt images and save it into structured documents. This is usually split into two sub-tasks: text localization and optical character recognition (OCR). Most existing OCR models only focus on the cropped text instance images, which require the bounding box information provided by a text region detection model. Introducing an additional detector to identify the text instance images in advance is inefficient, however instance-level OCR models have very low accuracy when processing the whole image for the document-level OCR, such as receipt images containing multiple text lines arranged in various layouts. To this end, we propose a localization-free document-level OCR model for transcribing all the characters in a receipt image into an ordered sequence end-to-end. Specifically, we finetune the pretrained Transformer-based instance-level model TrOCR with randomly cropped image chunks, and gradually increase the image chunk size to generalize the recognition ability from instance images to full-page images. In our experiments on the SROIE receipt OCR dataset, the model finetuned with our strategy achieved 64.4 F1-score and a 22.8% character error rates (CER) on the word-level and character-level metrics, respectively, which outperforms the baseline results with 48.5 F1-score and 50.6% CER. The best model, which splits the full image into 15 equally sized chunks, gives 87.8 F1-score and 4.98% CER with minimal additional pre or post-processing of the output. Moreover, the characters in the generated document-level sequences are arranged in the reading order, which is practical for real-world applications.



### Context-aware 6D Pose Estimation of Known Objects using RGB-D data
- **Arxiv ID**: http://arxiv.org/abs/2212.05560v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2212.05560v1)
- **Published**: 2022-12-11 18:01:01+00:00
- **Updated**: 2022-12-11 18:01:01+00:00
- **Authors**: Ankit Kumar, Priya Shukla, Vandana Kushwaha, G. C. Nandi
- **Comment**: None
- **Journal**: None
- **Summary**: 6D object pose estimation has been a research topic in the field of computer vision and robotics. Many modern world applications like robot grasping, manipulation, autonomous navigation etc, require the correct pose of objects present in a scene to perform their specific task. It becomes even harder when the objects are placed in a cluttered scene and the level of occlusion is high. Prior works have tried to overcome this problem but could not achieve accuracy that can be considered reliable in real-world applications. In this paper, we present an architecture that, unlike prior work, is context-aware. It utilizes the context information available to us about the objects. Our proposed architecture treats the objects separately according to their types i.e; symmetric and non-symmetric. A deeper estimator and refiner network pair is used for non-symmetric objects as compared to symmetric due to their intrinsic differences. Our experiments show an enhancement in the accuracy of about 3.2% over the LineMOD dataset, which is considered a benchmark for pose estimation in the occluded and cluttered scenes, against the prior state-of-the-art DenseFusion. Our results also show that the inference time we got is sufficient for real-time usage.



### Using Multiple Instance Learning to Build Multimodal Representations
- **Arxiv ID**: http://arxiv.org/abs/2212.05561v2
- **DOI**: 10.1007/978-3-031-34048-2_35
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05561v2)
- **Published**: 2022-12-11 18:01:11+00:00
- **Updated**: 2023-03-09 17:08:24+00:00
- **Authors**: Peiqi Wang, William M. Wells, Seth Berkowitz, Steven Horng, Polina Golland
- **Comment**: None
- **Journal**: None
- **Summary**: Image-text multimodal representation learning aligns data across modalities and enables important medical applications, e.g., image classification, visual grounding, and cross-modal retrieval. In this work, we establish a connection between multimodal representation learning and multiple instance learning. Based on this connection, we propose a generic framework for constructing permutation-invariant score functions with many existing multimodal representation learning approaches as special cases. Furthermore, we use the framework to derive a novel contrastive learning approach and demonstrate that our method achieves state-of-the-art results in several downstream tasks.



### YoloCurvSeg: You Only Label One Noisy Skeleton for Vessel-style Curvilinear Structure Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2212.05566v5
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.05566v5)
- **Published**: 2022-12-11 18:15:40+00:00
- **Updated**: 2023-08-18 15:43:37+00:00
- **Authors**: Li Lin, Linkai Peng, Huaqing He, Pujin Cheng, Jiewei Wu, Kenneth K. Y. Wong, Xiaoying Tang
- **Comment**: 20 pages, 15 figures, MEDIA accepted
- **Journal**: None
- **Summary**: Weakly-supervised learning (WSL) has been proposed to alleviate the conflict between data annotation cost and model performance through employing sparsely-grained (i.e., point-, box-, scribble-wise) supervision and has shown promising performance, particularly in the image segmentation field. However, it is still a very challenging task due to the limited supervision, especially when only a small number of labeled samples are available. Additionally, almost all existing WSL segmentation methods are designed for star-convex structures which are very different from curvilinear structures such as vessels and nerves. In this paper, we propose a novel sparsely annotated segmentation framework for curvilinear structures, named YoloCurvSeg. A very essential component of YoloCurvSeg is image synthesis. Specifically, a background generator delivers image backgrounds that closely match the real distributions through inpainting dilated skeletons. The extracted backgrounds are then combined with randomly emulated curves generated by a Space Colonization Algorithm-based foreground generator and through a multilayer patch-wise contrastive learning synthesizer. In this way, a synthetic dataset with both images and curve segmentation labels is obtained, at the cost of only one or a few noisy skeleton annotations. Finally, a segmenter is trained with the generated dataset and possibly an unlabeled dataset. The proposed YoloCurvSeg is evaluated on four publicly available datasets (OCTA500, CORN, DRIVE and CHASEDB1) and the results show that YoloCurvSeg outperforms state-of-the-art WSL segmentation methods by large margins. With only one noisy skeleton annotation (respectively 0.14\%, 0.03\%, 1.40\%, and 0.65\% of the full annotation), YoloCurvSeg achieves more than 97\% of the fully-supervised performance on each dataset. Code and datasets will be released at https://github.com/llmir/YoloCurvSeg.



### MAViC: Multimodal Active Learning for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2212.11109v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.11109v1)
- **Published**: 2022-12-11 18:51:57+00:00
- **Updated**: 2022-12-11 18:51:57+00:00
- **Authors**: Gyanendra Das, Xavier Thomas, Anant Raj, Vikram Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: A large number of annotated video-caption pairs are required for training video captioning models, resulting in high annotation costs. Active learning can be instrumental in reducing these annotation requirements. However, active learning for video captioning is challenging because multiple semantically similar captions are valid for a video, resulting in high entropy outputs even for less-informative samples. Moreover, video captioning algorithms are multimodal in nature with a visual encoder and language decoder. Further, the sequential and combinatorial nature of the output makes the problem even more challenging. In this paper, we introduce MAViC which leverages our proposed Multimodal Semantics Aware Sequential Entropy (M-SASE) based acquisition function to address the challenges of active learning approaches for video captioning. Our approach integrates semantic similarity and uncertainty of both visual and language dimensions in the acquisition function. Our detailed experiments empirically demonstrate the efficacy of M-SASE for active learning for video captioning and improve on the baselines by a large margin.



### Learning Neural Volumetric Field for Point Cloud Geometry Compression
- **Arxiv ID**: http://arxiv.org/abs/2212.05589v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.05589v1)
- **Published**: 2022-12-11 19:55:24+00:00
- **Updated**: 2022-12-11 19:55:24+00:00
- **Authors**: Yueyu Hu, Yao Wang
- **Comment**: In Proceedings of 2022 Picture Coding Symposium (PCS)
- **Journal**: None
- **Summary**: Due to the diverse sparsity, high dimensionality, and large temporal variation of dynamic point clouds, it remains a challenge to design an efficient point cloud compression method. We propose to code the geometry of a given point cloud by learning a neural volumetric field. Instead of representing the entire point cloud using a single overfit network, we divide the entire space into small cubes and represent each non-empty cube by a neural network and an input latent code. The network is shared among all the cubes in a single frame or multiple frames, to exploit the spatial and temporal redundancy. The neural field representation of the point cloud includes the network parameters and all the latent codes, which are generated by using back-propagation over the network parameters and its input. By considering the entropy of the network parameters and the latent codes as well as the distortion between the original and reconstructed cubes in the loss function, we derive a rate-distortion (R-D) optimal representation. Experimental results show that the proposed coding scheme achieves superior R-D performances compared to the octree-based G-PCC, especially when applied to multiple frames of a point cloud video. The code is available at https://github.com/huzi96/NVFPCC/.



### PromptCAL: Contrastive Affinity Learning via Auxiliary Prompts for Generalized Novel Category Discovery
- **Arxiv ID**: http://arxiv.org/abs/2212.05590v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05590v2)
- **Published**: 2022-12-11 20:06:14+00:00
- **Updated**: 2023-03-26 10:30:22+00:00
- **Authors**: Sheng Zhang, Salman Khan, Zhiqiang Shen, Muzammal Naseer, Guangyi Chen, Fahad Khan
- **Comment**: Accepted by CVPR2023
- **Journal**: None
- **Summary**: Although existing semi-supervised learning models achieve remarkable success in learning with unannotated in-distribution data, they mostly fail to learn on unlabeled data sampled from novel semantic classes due to their closed-set assumption. In this work, we target a pragmatic but under-explored Generalized Novel Category Discovery (GNCD) setting. The GNCD setting aims to categorize unlabeled training data coming from known and novel classes by leveraging the information of partially labeled known classes. We propose a two-stage Contrastive Affinity Learning method with auxiliary visual Prompts, dubbed PromptCAL, to address this challenging problem. Our approach discovers reliable pairwise sample affinities to learn better semantic clustering of both known and novel classes for the class token and visual prompts. First, we propose a discriminative prompt regularization loss to reinforce semantic discriminativeness of prompt-adapted pre-trained vision transformer for refined affinity relationships.Besides, we propose contrastive affinity learning to calibrate semantic representations based on our iterative semi-supervised affinity graph generation method for semantically-enhanced supervision. Extensive experimental evaluation demonstrates that our PromptCAL method is more effective in discovering novel classes even with limited annotations and surpasses the current state-of-the-art on generic and fine-grained benchmarks (e.g., with nearly 11% gain on CUB-200, and 9% on ImageNet-100) on overall accuracy. Our code is available at https://github.com/sheng-eatamath/PromptCAL.



### Diamond Abrasive Electroplated Surface Anomaly Detection using Convolutional Neural Networks for Industrial Quality Inspection
- **Arxiv ID**: http://arxiv.org/abs/2212.11122v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.11122v1)
- **Published**: 2022-12-11 20:14:18+00:00
- **Updated**: 2022-12-11 20:14:18+00:00
- **Authors**: Parviz Ali
- **Comment**: None
- **Journal**: None
- **Summary**: Electroplated diamond abrasive tools require nickel coating on a metal surface for abrasive bonding and part functionality. The electroplated nickel-coated abrasive tool is expected to have a high-quality part performance by having a nickel coating thickness of between 50% to 60% of the abrasive median diameter, uniformity of the nickel layer, abrasive distribution over the electroplated surface, and bright gloss. Electroplating parameters are set accordingly for this purpose. Industrial quality inspection for defects of these abrasive electroplated parts with optical inspection instruments is extremely challenging due to the diamond's light refraction, dispersion nature, and reflective bright nickel surface. The difficulty posed by this challenge requires parts to be quality inspected manually with an eye loupe that is subjective and costly. In this study, we use a Convolutional Neural Network (CNN) model in the production line to detect abrasive electroplated part anomalies allowing us to fix or eliminate those parts or elements that are in bad condition from the production chain and ultimately reduce manual quality inspection cost. We used 744 samples to train our model. Our model successfully identified over 99% of the parts with an anomaly. Keywords: Artificial Intelligence, Anomaly Detection, Industrial Quality Inspection, Electroplating, Diamond Abrasive Tool



### Recurrent Vision Transformers for Object Detection with Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/2212.05598v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05598v3)
- **Published**: 2022-12-11 20:28:59+00:00
- **Updated**: 2023-05-25 09:17:11+00:00
- **Authors**: Mathias Gehrig, Davide Scaramuzza
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  Vancouver, 2023
- **Summary**: We present Recurrent Vision Transformers (RVTs), a novel backbone for object detection with event cameras. Event cameras provide visual information with sub-millisecond latency at a high-dynamic range and with strong robustness against motion blur. These unique properties offer great potential for low-latency object detection and tracking in time-critical scenarios. Prior work in event-based vision has achieved outstanding detection performance but at the cost of substantial inference time, typically beyond 40 milliseconds. By revisiting the high-level design of recurrent vision backbones, we reduce inference time by a factor of 6 while retaining similar performance. To achieve this, we explore a multi-stage design that utilizes three key concepts in each stage: First, a convolutional prior that can be regarded as a conditional positional embedding. Second, local and dilated global self-attention for spatial feature interaction. Third, recurrent temporal feature aggregation to minimize latency while retaining temporal information. RVTs can be trained from scratch to reach state-of-the-art performance on event-based object detection - achieving an mAP of 47.2% on the Gen1 automotive dataset. At the same time, RVTs offer fast inference (<12 ms on a T4 GPU) and favorable parameter efficiency (5 times fewer than prior art). Our study brings new insights into effective design choices that can be fruitful for research beyond event-based vision.



### Orthogonal SVD Covariance Conditioning and Latent Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2212.05599v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.05599v1)
- **Published**: 2022-12-11 20:31:31+00:00
- **Updated**: 2022-12-11 20:31:31+00:00
- **Authors**: Yue Song, Nicu Sebe, Wei Wang
- **Comment**: Accepted by IEEE T-PAMI. arXiv admin note: substantial text overlap
  with arXiv:2207.02119
- **Journal**: None
- **Summary**: Inserting an SVD meta-layer into neural networks is prone to make the covariance ill-conditioned, which could harm the model in the training stability and generalization abilities. In this paper, we systematically study how to improve the covariance conditioning by enforcing orthogonality to the Pre-SVD layer. Existing orthogonal treatments on the weights are first investigated. However, these techniques can improve the conditioning but would hurt the performance. To avoid such a side effect, we propose the Nearest Orthogonal Gradient (NOG) and Optimal Learning Rate (OLR). The effectiveness of our methods is validated in two applications: decorrelated Batch Normalization (BN) and Global Covariance Pooling (GCP). Extensive experiments on visual recognition demonstrate that our methods can simultaneously improve covariance conditioning and generalization. The combinations with orthogonal weight can further boost the performance. Moreover, we show that our orthogonality techniques can benefit generative models for better latent disentanglement through a series of experiments on various benchmarks. Code is available at: \href{https://github.com/KingJamesSong/OrthoImproveCond}{https://github.com/KingJamesSong/OrthoImproveCond}.



### Accelerating Self-Supervised Learning via Efficient Training Strategies
- **Arxiv ID**: http://arxiv.org/abs/2212.05611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05611v1)
- **Published**: 2022-12-11 21:49:39+00:00
- **Updated**: 2022-12-11 21:49:39+00:00
- **Authors**: Mustafa Taha Koçyiğit, Timothy M. Hospedales, Hakan Bilen
- **Comment**: None
- **Journal**: None
- **Summary**: Recently the focus of the computer vision community has shifted from expensive supervised learning towards self-supervised learning of visual representations. While the performance gap between supervised and self-supervised has been narrowing, the time for training self-supervised deep networks remains an order of magnitude larger than its supervised counterparts, which hinders progress, imposes carbon cost, and limits societal benefits to institutions with substantial resources. Motivated by these issues, this paper investigates reducing the training time of recent self-supervised methods by various model-agnostic strategies that have not been used for this problem. In particular, we study three strategies: an extendable cyclic learning rate schedule, a matching progressive augmentation magnitude and image resolutions schedule, and a hard positive mining strategy based on augmentation difficulty. We show that all three methods combined lead up to 2.7 times speed-up in the training time of several self-supervised methods while retaining comparable performance to the standard self-supervised learning setting.



### DISCO: Adversarial Defense with Local Implicit Functions
- **Arxiv ID**: http://arxiv.org/abs/2212.05630v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05630v1)
- **Published**: 2022-12-11 23:54:26+00:00
- **Updated**: 2022-12-11 23:54:26+00:00
- **Authors**: Chih-Hui Ho, Nuno Vasconcelos
- **Comment**: Accepted to Neurips 2022
- **Journal**: None
- **Summary**: The problem of adversarial defenses for image classification, where the goal is to robustify a classifier against adversarial examples, is considered. Inspired by the hypothesis that these examples lie beyond the natural image manifold, a novel aDversarIal defenSe with local impliCit functiOns (DISCO) is proposed to remove adversarial perturbations by localized manifold projections. DISCO consumes an adversarial image and a query pixel location and outputs a clean RGB value at the location. It is implemented with an encoder and a local implicit module, where the former produces per-pixel deep features and the latter uses the features in the neighborhood of query pixel for predicting the clean RGB value. Extensive experiments demonstrate that both DISCO and its cascade version outperform prior defenses, regardless of whether the defense is known to the attacker. DISCO is also shown to be data and parameter efficient and to mount defenses that transfers across datasets, classifiers and attacks.



