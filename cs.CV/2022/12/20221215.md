# Arxiv Papers in cs.CV on 2022-12-15
### SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory
- **Arxiv ID**: http://arxiv.org/abs/2212.08476v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2212.08476v1)
- **Published**: 2022-12-15 00:02:36+00:00
- **Updated**: 2022-12-15 00:02:36+00:00
- **Authors**: Sicheng Li, Hao Li, Yue Wang, Yiyi Liao, Lu Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Radiance Fields (NeRF) have demonstrated superior novel view synthesis performance but are slow at rendering. To speed up the volume rendering process, many acceleration methods have been proposed at the cost of large memory consumption. To push the frontier of the efficiency-memory trade-off, we explore a new perspective to accelerate NeRF rendering, leveraging a key fact that the viewpoint change is usually smooth and continuous in interactive viewpoint control. This allows us to leverage the information of preceding viewpoints to reduce the number of rendered pixels as well as the number of sampled points along the ray of the remaining pixels. In our pipeline, a low-resolution feature map is rendered first by volume rendering, then a lightweight 2D neural renderer is applied to generate the output image at target resolution leveraging the features of preceding and current frames. We show that the proposed method can achieve competitive rendering quality while reducing the rendering time with little memory overhead, enabling 30FPS at 1080P image resolution with a low memory footprint.



### Multi-level and multi-modal feature fusion for accurate 3D object detection in Connected and Automated Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2212.07560v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.07560v2)
- **Published**: 2022-12-15 00:25:05+00:00
- **Updated**: 2022-12-19 14:11:41+00:00
- **Authors**: Yiming Hou, Mahdi Rezaei, Richard Romano
- **Comment**: None
- **Journal**: None
- **Summary**: Aiming at highly accurate object detection for connected and automated vehicles (CAVs), this paper presents a Deep Neural Network based 3D object detection model that leverages a three-stage feature extractor by developing a novel LIDAR-Camera fusion scheme. The proposed feature extractor extracts high-level features from two input sensory modalities and recovers the important features discarded during the convolutional process. The novel fusion scheme effectively fuses features across sensory modalities and convolutional layers to find the best representative global features. The fused features are shared by a two-stage network: the region proposal network (RPN) and the detection head (DH). The RPN generates high-recall proposals, and the DH produces final detection results. The experimental results show the proposed model outperforms more recent research on the KITTI 2D and 3D detection benchmark, particularly for distant and highly occluded instances.



### AirfRANS: High Fidelity Computational Fluid Dynamics Dataset for Approximating Reynolds-Averaged Navier-Stokes Solutions
- **Arxiv ID**: http://arxiv.org/abs/2212.07564v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.comp-ph, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/2212.07564v3)
- **Published**: 2022-12-15 00:41:09+00:00
- **Updated**: 2023-06-01 14:52:42+00:00
- **Authors**: Florent Bonnet, Ahmed Jocelyn Mazari, Paola Cinnella, Patrick Gallinari
- **Comment**: None
- **Journal**: 36th Conference on Neural Information Processing Systems (NeurIPS
  2022) Track on Datasets and Benchmarks
- **Summary**: Surrogate models are necessary to optimize meaningful quantities in physical dynamics as their recursive numerical resolutions are often prohibitively expensive. It is mainly the case for fluid dynamics and the resolution of Navier-Stokes equations. However, despite the fast-growing field of data-driven models for physical systems, reference datasets representing real-world phenomena are lacking. In this work, we develop AirfRANS, a dataset for studying the two-dimensional incompressible steady-state Reynolds-Averaged Navier-Stokes equations over airfoils at a subsonic regime and for different angles of attacks. We also introduce metrics on the stress forces at the surface of geometries and visualization of boundary layers to assess the capabilities of models to accurately predict the meaningful information of the problem. Finally, we propose deep learning baselines on four machine learning tasks to study AirfRANS under different constraints for generalization considerations: big and scarce data regime, Reynolds number, and angle of attack extrapolation.



### Learning Markerless Robot-Depth Camera Calibration and End-Effector Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2212.07567v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2212.07567v1)
- **Published**: 2022-12-15 00:53:42+00:00
- **Updated**: 2022-12-15 00:53:42+00:00
- **Authors**: Bugra C. Sefercik, Baris Akgun
- **Comment**: 8 pages, 6 figures, Conference on Robot Learning
- **Journal**: None
- **Summary**: Traditional approaches to extrinsic calibration use fiducial markers and learning-based approaches rely heavily on simulation data. In this work, we present a learning-based markerless extrinsic calibration system that uses a depth camera and does not rely on simulation data. We learn models for end-effector (EE) segmentation, single-frame rotation prediction and keypoint detection, from automatically generated real-world data. We use a transformation trick to get EE pose estimates from rotation predictions and a matching algorithm to get EE pose estimates from keypoint predictions. We further utilize the iterative closest point algorithm, multiple-frames, filtering and outlier detection to increase calibration robustness. Our evaluations with training data from multiple camera poses and test data from previously unseen poses give sub-centimeter and sub-deciradian average calibration and pose estimation errors. We also show that a carefully selected single training pose gives comparable results.



### Evaluation of direct attacks to fingerprint verification systems
- **Arxiv ID**: http://arxiv.org/abs/2212.07575v1
- **DOI**: 10.1007/s11235-010-9316-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07575v1)
- **Published**: 2022-12-15 01:26:09+00:00
- **Updated**: 2022-12-15 01:26:09+00:00
- **Authors**: J. Galbally, J. Fierrez, F. Alonso-Fernandez, M. Martinez-Diaz
- **Comment**: Published at Springer Journal of Telecommunication Systems, Special
  Issue of Biometrics Systems & Applications
- **Journal**: None
- **Summary**: The vulnerabilities of fingerprint-based recognition systems to direct attacks with and without the cooperation of the user are studied. Two different systems, one minutiae-based and one ridge feature-based, are evaluated on a database of real and fake fingerprints. Based on the fingerprint images quality and on the results achieved on different operational scenarios, we obtain a number of statistically significant observations regarding the robustness of the systems.



### Learning to Detect Semantic Boundaries with Image-level Class Labels
- **Arxiv ID**: http://arxiv.org/abs/2212.07579v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.07579v1)
- **Published**: 2022-12-15 01:56:22+00:00
- **Updated**: 2022-12-15 01:56:22+00:00
- **Authors**: Namyup Kim, Sehyun Hwang, Suha Kwak
- **Comment**: International Journal of Computer Vision (IJCV), 2022
- **Journal**: None
- **Summary**: This paper presents the first attempt to learn semantic boundary detection using image-level class labels as supervision. Our method starts by estimating coarse areas of object classes through attentions drawn by an image classification network. Since boundaries will locate somewhere between such areas of different classes, our task is formulated as a multiple instance learning (MIL) problem, where pixels on a line segment connecting areas of two different classes are regarded as a bag of boundary candidates. Moreover, we design a new neural network architecture that can learn to estimate semantic boundaries reliably even with uncertain supervision given by the MIL strategy. Our network is used to generate pseudo semantic boundary labels of training images, which are in turn used to train fully supervised models. The final model trained with our pseudo labels achieves an outstanding performance on the SBD dataset, where it is as competitive as some of previous arts trained with stronger supervision.



### Edema Estimation From Facial Images Taken Before and After Dialysis via Contrastive Multi-Patient Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2212.07582v1
- **DOI**: 10.1109/JBHI.2022.3227517
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.07582v1)
- **Published**: 2022-12-15 02:05:12+00:00
- **Updated**: 2022-12-15 02:05:12+00:00
- **Authors**: Yusuke Akamatsu, Yoshifumi Onishi, Hitoshi Imaoka, Junko Kameyama, Hideo Tsurushima
- **Comment**: Published in IEEE Journal of Biomedical and Health Informatics
  (J-BHI)
- **Journal**: IEEE.J.Biomed.Health.Inf. (2022)
- **Summary**: Edema is a common symptom of kidney disease, and quantitative measurement of edema is desired. This paper presents a method to estimate the degree of edema from facial images taken before and after dialysis of renal failure patients. As tasks to estimate the degree of edema, we perform pre- and post-dialysis classification and body weight prediction. We develop a multi-patient pre-training framework for acquiring knowledge of edema and transfer the pre-trained model to a model for each patient. For effective pre-training, we propose a novel contrastive representation learning, called weight-aware supervised momentum contrast (WeightSupMoCo). WeightSupMoCo aims to make feature representations of facial images closer in similarity of patient weight when the pre- and post-dialysis labels are the same. Experimental results show that our pre-training approach improves the accuracy of pre- and post-dialysis classification by 15.1% and reduces the mean absolute error of weight prediction by 0.243 kg compared with training from scratch. The proposed method accurately estimate the degree of edema from facial images; our edema estimation system could thus be beneficial to dialysis patients.



### Rethinking the Role of Pre-Trained Networks in Source-Free Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2212.07585v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.07585v2)
- **Published**: 2022-12-15 02:25:22+00:00
- **Updated**: 2023-08-25 10:39:23+00:00
- **Authors**: Wenyu Zhang, Li Shen, Chuan-Sheng Foo
- **Comment**: Accepted to ICCV 2023
- **Journal**: None
- **Summary**: Source-free domain adaptation (SFDA) aims to adapt a source model trained on a fully-labeled source domain to an unlabeled target domain. Large-data pre-trained networks are used to initialize source models during source training, and subsequently discarded. However, source training can cause the model to overfit to source data distribution and lose applicable target domain knowledge. We propose to integrate the pre-trained network into the target adaptation process as it has diversified features important for generalization and provides an alternate view of features and classification decisions different from the source model. We propose to distil useful target domain information through a co-learning strategy to improve target pseudolabel quality for finetuning the source model. Evaluation on 4 benchmark datasets show that our proposed strategy improves adaptation performance and can be successfully integrated with existing SFDA methods. Leveraging modern pre-trained networks that have stronger representation learning ability in the co-learning strategy further boosts performance.



### Solve the Puzzle of Instance Segmentation in Videos: A Weakly Supervised Framework with Spatio-Temporal Collaboration
- **Arxiv ID**: http://arxiv.org/abs/2212.07592v1
- **DOI**: 10.1109/TCSVT.2022.3202574
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07592v1)
- **Published**: 2022-12-15 02:44:13+00:00
- **Updated**: 2022-12-15 02:44:13+00:00
- **Authors**: Liqi Yan, Qifan Wang, Siqi Ma, Jingang Wang, Changbin Yu
- **Comment**: None
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology
  (2022)
- **Summary**: Instance segmentation in videos, which aims to segment and track multiple objects in video frames, has garnered a flurry of research attention in recent years. In this paper, we present a novel weakly supervised framework with \textbf{S}patio-\textbf{T}emporal \textbf{C}ollaboration for instance \textbf{Seg}mentation in videos, namely \textbf{STC-Seg}. Concretely, STC-Seg demonstrates four contributions. First, we leverage the complementary representations from unsupervised depth estimation and optical flow to produce effective pseudo-labels for training deep networks and predicting high-quality instance masks. Second, to enhance the mask generation, we devise a puzzle loss, which enables end-to-end training using box-level annotations. Third, our tracking module jointly utilizes bounding-box diagonal points with spatio-temporal discrepancy to model movements, which largely improves the robustness to different object appearances. Finally, our framework is flexible and enables image-level instance segmentation methods to operate the video-level task. We conduct an extensive set of experiments on the KITTI MOTS and YT-VIS datasets. Experimental results demonstrate that our method achieves strong performance and even outperforms fully supervised TrackR-CNN and MaskTrack R-CNN. We believe that STC-Seg can be a valuable addition to the community, as it reflects the tip of an iceberg about the innovative opportunities in the weakly supervised paradigm for instance segmentation in videos.



### Enhanced Training of Query-Based Object Detection via Selective Query Recollection
- **Arxiv ID**: http://arxiv.org/abs/2212.07593v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07593v3)
- **Published**: 2022-12-15 02:45:57+00:00
- **Updated**: 2023-03-22 00:23:39+00:00
- **Authors**: Fangyi Chen, Han Zhang, Kai Hu, Yu-kai Huang, Chenchen Zhu, Marios Savvides
- **Comment**: CVPR2023
- **Journal**: None
- **Summary**: This paper investigates a phenomenon where query-based object detectors mispredict at the last decoding stage while predicting correctly at an intermediate stage. We review the training process and attribute the overlooked phenomenon to two limitations: lack of training emphasis and cascading errors from decoding sequence. We design and present Selective Query Recollection (SQR), a simple and effective training strategy for query-based object detectors. It cumulatively collects intermediate queries as decoding stages go deeper and selectively forwards the queries to the downstream stages aside from the sequential structure. Such-wise, SQR places training emphasis on later stages and allows later stages to work with intermediate queries from earlier stages directly. SQR can be easily plugged into various query-based object detectors and significantly enhances their performance while leaving the inference pipeline unchanged. As a result, we apply SQR on Adamixer, DAB-DETR, and Deformable-DETR across various settings (backbone, number of queries, schedule) and consistently brings 1.4-2.8 AP improvement.



### Universal Generative Modeling in Dual-domain for Dynamic MR Imaging
- **Arxiv ID**: http://arxiv.org/abs/2212.07599v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.07599v1)
- **Published**: 2022-12-15 03:04:48+00:00
- **Updated**: 2022-12-15 03:04:48+00:00
- **Authors**: Chuanming Yu, Yu Guan, Ziwen Ke, Dong Liang, Qiegen Liu
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: Dynamic magnetic resonance image reconstruction from incomplete k-space data has generated great research interest due to its capability to reduce scan time. Never-theless, the reconstruction problem is still challenging due to its ill-posed nature. Recently, diffusion models espe-cially score-based generative models have exhibited great potential in algorithm robustness and usage flexi-bility. Moreover, the unified framework through the variance exploding stochastic differential equation (VE-SDE) is proposed to enable new sampling methods and further extend the capabilities of score-based gener-ative models. Therefore, by taking advantage of the uni-fied framework, we proposed a k-space and image Du-al-Domain collaborative Universal Generative Model (DD-UGM) which combines the score-based prior with low-rank regularization penalty to reconstruct highly under-sampled measurements. More precisely, we extract prior components from both image and k-space domains via a universal generative model and adaptively handle these prior components for faster processing while maintaining good generation quality. Experimental comparisons demonstrated the noise reduction and detail preservation abilities of the proposed method. Much more than that, DD-UGM can reconstruct data of differ-ent frames by only training a single frame image, which reflects the flexibility of the proposed model.



### Text-Guided Mask-free Local Image Retouching
- **Arxiv ID**: http://arxiv.org/abs/2212.07603v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07603v2)
- **Published**: 2022-12-15 03:26:53+00:00
- **Updated**: 2023-02-24 05:46:02+00:00
- **Authors**: Zerun Liu, Fan Zhang, Jingxuan He, Jin Wang, Zhangye Wang, Lechao Cheng
- **Comment**: 7 pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: In the realm of multi-modality, text-guided image retouching techniques emerged with the advent of deep learning. Most currently available text-guided methods, however, rely on object-level supervision to constrain the region that may be modified. This not only makes it more challenging to develop these algorithms, but it also limits how widely deep learning can be used for image retouching. In this paper, we offer a text-guided mask-free image retouching approach that yields consistent results to address this concern. In order to perform image retouching without mask supervision, our technique can construct plausible and edge-sharp masks based on the text for each object in the image. Extensive experiments have shown that our method can produce high-quality, accurate images based on spoken language. The source code will be released soon.



### DCS-RISR: Dynamic Channel Splitting for Efficient Real-world Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2212.07613v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.07613v4)
- **Published**: 2022-12-15 04:34:57+00:00
- **Updated**: 2023-01-02 04:58:30+00:00
- **Authors**: Junbo Qiao, Shaohui Lin, Yunlun Zhang, Wei Li, Jie Hu, Gaoqi He, Changbo Wang, Lizhuang Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world image super-resolution (RISR) has received increased focus for improving the quality of SR images under unknown complex degradation. Existing methods rely on the heavy SR models to enhance low-resolution (LR) images of different degradation levels, which significantly restricts their practical deployments on resource-limited devices. In this paper, we propose a novel Dynamic Channel Splitting scheme for efficient Real-world Image Super-Resolution, termed DCS-RISR. Specifically, we first introduce the light degradation prediction network to regress the degradation vector to simulate the real-world degradations, upon which the channel splitting vector is generated as the input for an efficient SR model. Then, a learnable octave convolution block is proposed to adaptively decide the channel splitting scale for low- and high-frequency features at each block, reducing computation overhead and memory cost by offering the large scale to low-frequency features and the small scale to the high ones. To further improve the RISR performance, Non-local regularization is employed to supplement the knowledge of patches from LR and HR subspace with free-computation inference. Extensive experiments demonstrate the effectiveness of DCS-RISR on different benchmark datasets. Our DCS-RISR not only achieves the best trade-off between computation/parameter and PSNR/SSIM metric, and also effectively handles real-world images with different degradation levels.



### Proposal Distribution Calibration for Few-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2212.07618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07618v1)
- **Published**: 2022-12-15 05:09:11+00:00
- **Updated**: 2022-12-15 05:09:11+00:00
- **Authors**: Bohao Li, Chang Liu, Mengnan Shi, Xiaozhong Chen, Xiangyang Ji, Qixiang Ye
- **Comment**: This paper is under review in IEEE TNNLS
- **Journal**: None
- **Summary**: Adapting object detectors learned with sufficient supervision to novel classes under low data regimes is charming yet challenging. In few-shot object detection (FSOD), the two-step training paradigm is widely adopted to mitigate the severe sample imbalance, i.e., holistic pre-training on base classes, then partial fine-tuning in a balanced setting with all classes. Since unlabeled instances are suppressed as backgrounds in the base training phase, the learned RPN is prone to produce biased proposals for novel instances, resulting in dramatic performance degradation. Unfortunately, the extreme data scarcity aggravates the proposal distribution bias, hindering the RoI head from evolving toward novel classes. In this paper, we introduce a simple yet effective proposal distribution calibration (PDC) approach to neatly enhance the localization and classification abilities of the RoI head by recycling its localization ability endowed in base training and enriching high-quality positive samples for semantic fine-tuning. Specifically, we sample proposals based on the base proposal statistics to calibrate the distribution bias and impose additional localization and classification losses upon the sampled proposals for fast expanding the base detector to novel classes. Experiments on the commonly used Pascal VOC and MS COCO datasets with explicit state-of-the-art performances justify the efficacy of our PDC for FSOD. Code is available at github.com/Bohao-Lee/PDC.



### SBSS: Stacking-Based Semantic Segmentation Framework for Very High Resolution Remote Sensing Image
- **Arxiv ID**: http://arxiv.org/abs/2212.07623v1
- **DOI**: 10.1109/TGRS.2023.3234549
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07623v1)
- **Published**: 2022-12-15 05:43:21+00:00
- **Updated**: 2022-12-15 05:43:21+00:00
- **Authors**: Yuanzhi Cai, Lei Fan, Yuan Fang
- **Comment**: 14 pages
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, 2023
- **Summary**: Semantic segmentation of Very High Resolution (VHR) remote sensing images is a fundamental task for many applications. However, large variations in the scales of objects in those VHR images pose a challenge for performing accurate semantic segmentation. Existing semantic segmentation networks are able to analyse an input image at up to four resizing scales, but this may be insufficient given the diversity of object scales. Therefore, Multi Scale (MS) test-time data augmentation is often used in practice to obtain more accurate segmentation results, which makes equal use of the segmentation results obtained at the different resizing scales. However, it was found in this study that different classes of objects had their preferred resizing scale for more accurate semantic segmentation. Based on this behaviour, a Stacking-Based Semantic Segmentation (SBSS) framework is proposed to improve the segmentation results by learning this behaviour, which contains a learnable Error Correction Module (ECM) for segmentation result fusion and an Error Correction Scheme (ECS) for computational complexity control. Two ECS, i.e., ECS-MS and ECS-SS, are proposed and investigated in this study. The Floating-point operations (Flops) required for ECS-MS and ECS-SS are similar to the commonly used MS test and the Single-Scale (SS) test, respectively. Extensive experiments on four datasets (i.e., Cityscapes, UAVid, LoveDA and Potsdam) show that SBSS is an effective and flexible framework. It achieved higher accuracy than MS when using ECS-MS, and similar accuracy as SS with a quarter of the memory footprint when using ECS-SS.



### NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object Interactions
- **Arxiv ID**: http://arxiv.org/abs/2212.07626v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07626v1)
- **Published**: 2022-12-15 05:58:45+00:00
- **Updated**: 2022-12-15 05:58:45+00:00
- **Authors**: Juze Zhang, Haimin Luo, Hongdi Yang, Xinru Xu, Qianyang Wu, Ye Shi, Jingyi Yu, Lan Xu, Jingya Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Humans constantly interact with objects in daily life tasks. Capturing such processes and subsequently conducting visual inferences from a fixed viewpoint suffers from occlusions, shape and texture ambiguities, motions, etc. To mitigate the problem, it is essential to build a training dataset that captures free-viewpoint interactions. We construct a dense multi-view dome to acquire a complex human object interaction dataset, named HODome, that consists of $\sim$75M frames on 10 subjects interacting with 23 objects. To process the HODome dataset, we develop NeuralDome, a layer-wise neural processing pipeline tailored for multi-view video inputs to conduct accurate tracking, geometry reconstruction and free-view rendering, for both human subjects and objects. Extensive experiments on the HODome dataset demonstrate the effectiveness of NeuralDome on a variety of inference, modeling, and rendering tasks. Both the dataset and the NeuralDome tools will be disseminated to the community for further development.



### EM-Paste: EM-guided Cut-Paste with DALL-E Augmentation for Image-level Weakly Supervised Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2212.07629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07629v1)
- **Published**: 2022-12-15 06:30:11+00:00
- **Updated**: 2022-12-15 06:30:11+00:00
- **Authors**: Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Laurent Itti, Vibhav Vineet
- **Comment**: 15 pages (including appendix), 7 figures
- **Journal**: None
- **Summary**: We propose EM-PASTE: an Expectation Maximization(EM) guided Cut-Paste compositional dataset augmentation approach for weakly-supervised instance segmentation using only image-level supervision. The proposed method consists of three main components. The first component generates high-quality foreground object masks. To this end, an EM-like approach is proposed that iteratively refines an initial set of object mask proposals generated by a generic region proposal method. Next, in the second component, high-quality context-aware background images are generated using a text-to-image compositional synthesis method like DALL-E. Finally, the third component creates a large-scale pseudo-labeled instance segmentation training dataset by compositing the foreground object masks onto the original and generated background images. The proposed approach achieves state-of-the-art weakly-supervised instance segmentation results on both the PASCAL VOC 2012 and MS COCO datasets by using only image-level, weak label information. In particular, it outperforms the best baseline by +7.4 and +2.8 mAP0.50 on PASCAL and COCO, respectively. Further, the method provides a new solution to the long-tail weakly-supervised instance segmentation problem (when many classes may only have few training samples), by selectively augmenting under-represented classes.



### Adaptive Multi-Agent Continuous Learning System
- **Arxiv ID**: http://arxiv.org/abs/2212.07646v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2212.07646v2)
- **Published**: 2022-12-15 07:39:50+00:00
- **Updated**: 2023-04-04 16:27:18+00:00
- **Authors**: Xingyu Qian, Aximu Yuemaier, Longfei Liang, Wen-Chi Yang, Xiaogang Chen, Shunfen Li, Weibang Dai, Zhitang Song
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an adaptive multi-agent clustering recognition system that can be self-supervised driven, based on a temporal sequences continuous learning mechanism with adaptability. The system is designed to use some different functional agents to build up a connection structure to improve adaptability to cope with environmental diverse demands, by predicting the input of the agent to drive the agent to achieve the act of clustering recognition of sequences using the traditional algorithmic approach. Finally, the feasibility experiments of video behavior clustering demonstrate the feasibility of the system to cope with dynamic situations. Our work is placed here\footnote{https://github.com/qian-git/MAMMALS}.



### Relightable Neural Human Assets from Multi-view Gradient Illuminations
- **Arxiv ID**: http://arxiv.org/abs/2212.07648v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.07648v3)
- **Published**: 2022-12-15 08:06:03+00:00
- **Updated**: 2023-06-23 07:50:16+00:00
- **Authors**: Taotao Zhou, Kai He, Di Wu, Teng Xu, Qixuan Zhang, Kuixiang Shao, Wenzheng Chen, Lan Xu, Jingyi Yu
- **Comment**: Project page: https://miaoing.github.io/RNHA
- **Journal**: None
- **Summary**: Human modeling and relighting are two fundamental problems in computer vision and graphics, where high-quality datasets can largely facilitate related research. However, most existing human datasets only provide multi-view human images captured under the same illumination. Although valuable for modeling tasks, they are not readily used in relighting problems. To promote research in both fields, in this paper, we present UltraStage, a new 3D human dataset that contains more than 2,000 high-quality human assets captured under both multi-view and multi-illumination settings. Specifically, for each example, we provide 32 surrounding views illuminated with one white light and two gradient illuminations. In addition to regular multi-view images, gradient illuminations help recover detailed surface normal and spatially-varying material maps, enabling various relighting applications. Inspired by recent advances in neural representation, we further interpret each example into a neural human asset which allows novel view synthesis under arbitrary lighting conditions. We show our neural human assets can achieve extremely high capture performance and are capable of representing fine details such as facial wrinkles and cloth folds. We also validate UltraStage in single image relighting tasks, training neural networks with virtual relighted data from neural assets and demonstrating realistic rendering improvements over prior arts. UltraStage will be publicly available to the community to stimulate significant future developments in various human modeling and rendering tasks. The dataset is available at https://miaoing.github.io/RNHA.



### Two-stage Contextual Transformer-based Convolutional Neural Network for Airway Extraction from CT Images
- **Arxiv ID**: http://arxiv.org/abs/2212.07651v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.07651v1)
- **Published**: 2022-12-15 08:18:37+00:00
- **Updated**: 2022-12-15 08:18:37+00:00
- **Authors**: Yanan Wu, Shuiqing Zhao, Shouliang Qi, Jie Feng, Haowen Pang, Runsheng Chang, Long Bai, Mengqi Li, Shuyue Xia, Wei Qian, Hongliang Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate airway extraction from computed tomography (CT) images is a critical step for planning navigation bronchoscopy and quantitative assessment of airway-related chronic obstructive pulmonary disease (COPD). The existing methods are challenging to sufficiently segment the airway, especially the high-generation airway, with the constraint of the limited label and cannot meet the clinical use in COPD. We propose a novel two-stage 3D contextual transformer-based U-Net for airway segmentation using CT images. The method consists of two stages, performing initial and refined airway segmentation. The two-stage model shares the same subnetwork with different airway masks as input. Contextual transformer block is performed both in the encoder and decoder path of the subnetwork to finish high-quality airway segmentation effectively. In the first stage, the total airway mask and CT images are provided to the subnetwork, and the intrapulmonary airway mask and corresponding CT scans to the subnetwork in the second stage. Then the predictions of the two-stage method are merged as the final prediction. Extensive experiments were performed on in-house and multiple public datasets. Quantitative and qualitative analysis demonstrate that our proposed method extracted much more branches and lengths of the tree while accomplishing state-of-the-art airway segmentation performance. The code is available at https://github.com/zhaozsq/airway_segmentation.



### Body-Part Joint Detection and Association via Extended Object Representation
- **Arxiv ID**: http://arxiv.org/abs/2212.07652v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07652v2)
- **Published**: 2022-12-15 08:19:02+00:00
- **Updated**: 2023-03-13 14:06:14+00:00
- **Authors**: Huayi Zhou, Fei Jiang, Hongtao Lu
- **Comment**: accepted by ICME2023
- **Journal**: None
- **Summary**: The detection of human body and its related parts (e.g., face, head or hands) have been intensively studied and greatly improved since the breakthrough of deep CNNs. However, most of these detectors are trained independently, making it a challenging task to associate detected body parts with people. This paper focuses on the problem of joint detection of human body and its corresponding parts. Specifically, we propose a novel extended object representation that integrates the center location offsets of body or its parts, and construct a dense single-stage anchor-based Body-Part Joint Detector (BPJDet). Body-part associations in BPJDet are embedded into the unified representation which contains both the semantic and geometric information. Therefore, BPJDet does not suffer from error-prone association post-matching, and has a better accuracy-speed trade-off. Furthermore, BPJDet can be seamlessly generalized to jointly detect any body part. To verify the effectiveness and superiority of our method, we conduct extensive experiments on the CityPersons, CrowdHuman and BodyHands datasets. The proposed BPJDet detector achieves state-of-the-art association performance on these three benchmarks while maintains high accuracy of detection. Code is in https://github.com/hnuzhy/BPJDet.



### Writer Retrieval and Writer Identification in Greek Papyri
- **Arxiv ID**: http://arxiv.org/abs/2212.07664v1
- **DOI**: 10.1007/978-3-031-19745-1_6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07664v1)
- **Published**: 2022-12-15 08:42:25+00:00
- **Updated**: 2022-12-15 08:42:25+00:00
- **Authors**: Vincent Christlein, Isabelle Marthot-Santaniello, Martin Mayr, Anguelos Nicolaou, Mathias Seuret
- **Comment**: None
- **Journal**: IGS 2022. Lecture Notes in Computer Science, vol 13424. Springer,
  Cham
- **Summary**: The analysis of digitized historical manuscripts is typically addressed by paleographic experts. Writer identification refers to the classification of known writers while writer retrieval seeks to find the writer by means of image similarity in a dataset of images. While automatic writer identification/retrieval methods already provide promising results for many historical document types, papyri data is very challenging due to the fiber structures and severe artifacts. Thus, an important step for an improved writer identification is the preprocessing and feature sampling process. We investigate several methods and show that a good binarization is key to an improved writer identification in papyri writings. We focus mainly on writer retrieval using unsupervised feature methods based on traditional or self-supervised-based methods. It is, however, also comparable to the state of the art supervised deep learning-based method in the case of writer classification/re-identification.



### Multi-task Fusion for Efficient Panoptic-Part Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2212.07671v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07671v2)
- **Published**: 2022-12-15 09:04:45+00:00
- **Updated**: 2022-12-19 13:19:21+00:00
- **Authors**: Sravan Kumar Jagadeesh, René Schuster, Didier Stricker
- **Comment**: Accepted in ICPRAM 2023
- **Journal**: None
- **Summary**: In this paper, we introduce a novel network that generates semantic, instance, and part segmentation using a shared encoder and effectively fuses them to achieve panoptic-part segmentation. Unifying these three segmentation problems allows for mutually improved and consistent representation learning. To fuse the predictions of all three heads efficiently, we introduce a parameter-free joint fusion module that dynamically balances the logits and fuses them to create panoptic-part segmentation. Our method is evaluated on the Cityscapes Panoptic Parts (CPP) and Pascal Panoptic Parts (PPP) datasets. For CPP, the PartPQ of our proposed model with joint fusion surpasses the previous state-of-the-art by 1.6 and 4.7 percentage points for all areas and segments with parts, respectively. On PPP, our joint fusion outperforms a model using the previous top-down merging strategy by 3.3 percentage points in PartPQ and 10.5 percentage points in PartPQ for partitionable classes.



### Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization
- **Arxiv ID**: http://arxiv.org/abs/2212.07672v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2212.07672v2)
- **Published**: 2022-12-15 09:05:26+00:00
- **Updated**: 2023-05-04 10:16:30+00:00
- **Authors**: Yunlong Liang, Fandong Meng, Jinan Xu, Jiaan Wang, Yufeng Chen, Jie Zhou
- **Comment**: Accepted at ACL 2023 as a long paper of the main conference. Data and
  Code: https://github.com/XL2248/SOV-MAS
- **Journal**: None
- **Summary**: Multimodal abstractive summarization (MAS) aims to produce a concise summary given the multimodal data (text and vision). Existing studies mainly focus on how to effectively use the visual features from the perspective of an article, having achieved impressive success on the high-resource English dataset. However, less attention has been paid to the visual features from the perspective of the summary, which may limit the model performance, especially in the low- and zero-resource scenarios. In this paper, we propose to improve the summary quality through summary-oriented visual features. To this end, we devise two auxiliary tasks including vision to summary task and masked image modeling task. Together with the main summarization task, we optimize the MAS model via the training objectives of all these tasks. By these means, the MAS model can be enhanced by capturing the summary-oriented visual features, thereby yielding more accurate summaries. Experiments on 44 languages, covering mid-high-, low-, and zero-resource scenarios, verify the effectiveness and superiority of the proposed approach, which achieves state-of-the-art performance under all scenarios. Additionally, we will contribute a large-scale multilingual multimodal abstractive summarization (MM-Sum) dataset.



### CNN-based real-time 2D-3D deformable registration from a single X-ray projection
- **Arxiv ID**: http://arxiv.org/abs/2212.07692v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.07692v2)
- **Published**: 2022-12-15 09:57:19+00:00
- **Updated**: 2023-03-27 14:22:54+00:00
- **Authors**: François Lecomte, Jean-Louis Dillenseger, Stéphane Cotin
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: The purpose of this paper is to present a method for real-time 2D-3D non-rigid registration using a single fluoroscopic image. Such a method can find applications in surgery, interventional radiology and radiotherapy. By estimating a three-dimensional displacement field from a 2D X-ray image, anatomical structures segmented in the preoperative scan can be projected onto the 2D image, thus providing a mixed reality view. Methods: A dataset composed of displacement fields and 2D projections of the anatomy is generated from the preoperative scan. From this dataset, a neural network is trained to recover the unknown 3D displacement field from a single projection image. Results: Our method is validated on lung 4D CT data at different stages of the lung deformation. The training is performed on a 3D CT using random (non domain-specific) diffeomorphic deformations, to which perturbations mimicking the pose uncertainty are added. The model achieves a mean TRE over a series of landmarks ranging from 2.3 to 5.5 mm depending on the amplitude of deformation. Conclusion: In this paper, a CNN-based method for real-time 2D-3D non-rigid registration is presented. This method is able to cope with pose estimation uncertainties, making it applicable to actual clinical scenarios, such as lung surgery, where the C-arm pose is planned before the intervention.



### Retrieval-based Disentanglement with Distant Supervision
- **Arxiv ID**: http://arxiv.org/abs/2212.07699v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.07699v1)
- **Published**: 2022-12-15 10:20:42+00:00
- **Updated**: 2022-12-15 10:20:42+00:00
- **Authors**: Jiawei Zhou, Xiaoguang Li, Lifeng Shang, Xin Jiang, Qun Liu, Lei Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Disentangled representation learning remains challenging as ground truth factors of variation do not naturally exist. To address this, we present Vocabulary Disentanglement Retrieval~(VDR), a simple yet effective retrieval-based disentanglement framework that leverages nature language as distant supervision. Our approach is built upon the widely-used bi-encoder architecture with disentanglement heads and is trained on data-text pairs that are readily available on the web or in existing datasets. This makes our approach task- and modality-agnostic with potential for a wide range of downstream applications. We conduct experiments on 16 datasets in both text-to-text and cross-modal scenarios and evaluate VDR in a zero-shot setting. With the incorporation of disentanglement heads and a minor increase in parameters, VDR achieves significant improvements over the base retriever it is built upon, with a 9% higher on NDCG@10 scores in zero-shot text-to-text retrieval and an average of 13% higher recall in cross-modal retrieval. In comparison to other baselines, VDR outperforms them in most tasks, while also improving explainability and efficiency.



### Colab NAS: Obtaining lightweight task-specific convolutional neural networks following Occam's razor
- **Arxiv ID**: http://arxiv.org/abs/2212.07700v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07700v2)
- **Published**: 2022-12-15 10:23:32+00:00
- **Updated**: 2023-08-29 09:49:34+00:00
- **Authors**: Andrea Mattia Garavagno, Daniele Leonardis, Antonio Frisoli
- **Comment**: None
- **Journal**: None
- **Summary**: The current trend of applying transfer learning from convolutional neural networks (CNNs) trained on large datasets can be an overkill when the target application is a custom and delimited problem, with enough data to train a network from scratch. On the other hand, the training of custom and lighter CNNs requires expertise, in the from-scratch case, and or high-end resources, as in the case of hardware-aware neural architecture search (HW NAS), limiting access to the technology by non-habitual NN developers.   For this reason, we present ColabNAS, an affordable HW NAS technique for producing lightweight task-specific CNNs. Its novel derivative-free search strategy, inspired by Occam's razor, allows to obtain state-of-the-art results on the Visual Wake Word dataset, a standard TinyML benchmark, in just 3.1 GPU hours using free online GPU services such as Google Colaboratory and Kaggle Kernel.



### Deep Learning-Based Automatic Assessment of AgNOR-scores in Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/2212.07721v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.07721v1)
- **Published**: 2022-12-15 10:56:47+00:00
- **Updated**: 2022-12-15 10:56:47+00:00
- **Authors**: Jonathan Ganz, Karoline Lipnik, Jonas Ammeling, Barbara Richter, Chloé Puget, Eda Parlak, Laura Diehl, Robert Klopfleisch, Taryn A. Donovan, Matti Kiupel, Christof A. Bertram, Katharina Breininger, Marc Aubreville
- **Comment**: 6 pages, 2 figures, 1 table
- **Journal**: None
- **Summary**: Nucleolar organizer regions (NORs) are parts of the DNA that are involved in RNA transcription. Due to the silver affinity of associated proteins, argyrophilic NORs (AgNORs) can be visualized using silver-based staining. The average number of AgNORs per nucleus has been shown to be a prognostic factor for predicting the outcome of many tumors. Since manual detection of AgNORs is laborious, automation is of high interest. We present a deep learning-based pipeline for automatically determining the AgNOR-score from histopathological sections. An additional annotation experiment was conducted with six pathologists to provide an independent performance evaluation of our approach. Across all raters and images, we found a mean squared error of 0.054 between the AgNOR- scores of the experts and those of the model, indicating that our approach offers performance comparable to humans.



### Attention-based Multiple Instance Learning for Survival Prediction on Lung Cancer Tissue Microarrays
- **Arxiv ID**: http://arxiv.org/abs/2212.07724v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07724v2)
- **Published**: 2022-12-15 11:07:23+00:00
- **Updated**: 2023-02-22 11:24:18+00:00
- **Authors**: Jonas Ammeling, Lars-Henning Schmidt, Jonathan Ganz, Tanja Niedermair, Christoph Brochhausen-Delius, Christian Schulz, Katharina Breininger, Marc Aubreville
- **Comment**: Final version for the BVM 2023 Workshop
- **Journal**: None
- **Summary**: Attention-based multiple instance learning (AMIL) algorithms have proven to be successful in utilizing gigapixel whole-slide images (WSIs) for a variety of different computational pathology tasks such as outcome prediction and cancer subtyping problems. We extended an AMIL approach to the task of survival prediction by utilizing the classical Cox partial likelihood as a loss function, converting the AMIL model into a nonlinear proportional hazards model. We applied the model to tissue microarray (TMA) slides of 330 lung cancer patients. The results show that AMIL approaches can handle very small amounts of tissue from a TMA and reach similar C-index performance compared to established survival prediction methods trained with highly discriminative clinical factors such as age, cancer grade, and cancer stage



### HUM3DIL: Semi-supervised Multi-modal 3D Human Pose Estimation for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2212.07729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07729v1)
- **Published**: 2022-12-15 11:15:14+00:00
- **Updated**: 2022-12-15 11:15:14+00:00
- **Authors**: Andrei Zanfir, Mihai Zanfir, Alexander Gorban, Jingwei Ji, Yin Zhou, Dragomir Anguelov, Cristian Sminchisescu
- **Comment**: Published at the 6th Conference on Robot Learning (CoRL 2022),
  Auckland, New Zealand
- **Journal**: None
- **Summary**: Autonomous driving is an exciting new industry, posing important research questions. Within the perception module, 3D human pose estimation is an emerging technology, which can enable the autonomous vehicle to perceive and understand the subtle and complex behaviors of pedestrians. While hardware systems and sensors have dramatically improved over the decades -- with cars potentially boasting complex LiDAR and vision systems and with a growing expansion of the available body of dedicated datasets for this newly available information -- not much work has been done to harness these novel signals for the core problem of 3D human pose estimation. Our method, which we coin HUM3DIL (HUMan 3D from Images and LiDAR), efficiently makes use of these complementary signals, in a semi-supervised fashion and outperforms existing methods with a large margin. It is a fast and compact model for onboard deployment. Specifically, we embed LiDAR points into pixel-aligned multi-modal features, which we pass through a sequence of Transformer refinement stages. Quantitative experiments on the Waymo Open Dataset support these claims, where we achieve state-of-the-art results on the task of 3D pose estimation.



### Combating Uncertainty and Class Imbalance in Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2212.07751v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07751v1)
- **Published**: 2022-12-15 12:09:02+00:00
- **Updated**: 2022-12-15 12:09:02+00:00
- **Authors**: Jiaxiang Fan, Jian Zhou, Xiaoyu Deng, Huabin Wang, Liang Tao, Hon Keung Kwan
- **Comment**: None
- **Journal**: None
- **Summary**: Recognition of facial expression is a challenge when it comes to computer vision. The primary reasons are class imbalance due to data collection and uncertainty due to inherent noise such as fuzzy facial expressions and inconsistent labels. However, current research has focused either on the problem of class imbalance or on the problem of uncertainty, ignoring the intersection of how to address these two problems. Therefore, in this paper, we propose a framework based on Resnet and Attention to solve the above problems. We design weight for each class. Through the penalty mechanism, our model will pay more attention to the learning of small samples during training, and the resulting decrease in model accuracy can be improved by a Convolutional Block Attention Module (CBAM). Meanwhile, our backbone network will also learn an uncertain feature for each sample. By mixing uncertain features between samples, the model can better learn those features that can be used for classification, thus suppressing uncertainty. Experiments show that our method surpasses most basic methods in terms of accuracy on facial expression data sets (e.g., AffectNet, RAF-DB), and it also solves the problem of class imbalance well.



### Event-based Visual Tracking in Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/2212.07754v1
- **DOI**: 10.1007/978-3-031-21065-5_15
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07754v1)
- **Published**: 2022-12-15 12:18:13+00:00
- **Updated**: 2022-12-15 12:18:13+00:00
- **Authors**: Irene Perez-Salesa, Rodrigo Aldana-Lopez, Carlos Sagues
- **Comment**: This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in ROBOT2022: Fifth Iberian Robotics Conference
- **Journal**: In ROBOT2022: Fifth Iberian Robotics Conference. Lecture Notes in
  Networks and Systems, vol 589. Springer, Cham (2023)
- **Summary**: Visual object tracking under challenging conditions of motion and light can be hindered by the capabilities of conventional cameras, prone to producing images with motion blur. Event cameras are novel sensors suited to robustly perform vision tasks under these conditions. However, due to the nature of their output, applying them to object detection and tracking is non-trivial. In this work, we propose a framework to take advantage of both event cameras and off-the-shelf deep learning for object tracking. We show that reconstructing event data into intensity frames improves the tracking performance in conditions under which conventional cameras fail to provide acceptable results.



### DeepLSD: Line Segment Detection and Refinement with Deep Image Gradients
- **Arxiv ID**: http://arxiv.org/abs/2212.07766v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07766v3)
- **Published**: 2022-12-15 12:36:49+00:00
- **Updated**: 2023-03-28 13:59:47+00:00
- **Authors**: Rémi Pautrat, Daniel Barath, Viktor Larsson, Martin R. Oswald, Marc Pollefeys
- **Comment**: Accepted at CVPR 2023
- **Journal**: None
- **Summary**: Line segments are ubiquitous in our human-made world and are increasingly used in vision tasks. They are complementary to feature points thanks to their spatial extent and the structural information they provide. Traditional line detectors based on the image gradient are extremely fast and accurate, but lack robustness in noisy images and challenging conditions. Their learned counterparts are more repeatable and can handle challenging images, but at the cost of a lower accuracy and a bias towards wireframe lines. We propose to combine traditional and learned approaches to get the best of both worlds: an accurate and robust line detector that can be trained in the wild without ground truth lines. Our new line segment detector, DeepLSD, processes images with a deep network to generate a line attraction field, before converting it to a surrogate image gradient magnitude and angle, which is then fed to any existing handcrafted line detector. Additionally, we propose a new optimization tool to refine line segments based on the attraction field and vanishing points. This refinement improves the accuracy of current deep detectors by a large margin. We demonstrate the performance of our method on low-level line detection metrics, as well as on several downstream tasks using multiple challenging datasets. The source code and models are available at https://github.com/cvg/DeepLSD.



### A scalable framework for annotating photovoltaic cell defects in electroluminescence images
- **Arxiv ID**: http://arxiv.org/abs/2212.07768v1
- **DOI**: 10.1109/TII.2022.3228680
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.07768v1)
- **Published**: 2022-12-15 12:46:31+00:00
- **Updated**: 2022-12-15 12:46:31+00:00
- **Authors**: Urtzi Otamendi, Inigo Martinez, Igor G. Olaizola, Marco Quartulli
- **Comment**: 10 pages, 10 figures, 1 table, accepted at IEEE Transactions on
  Industrial Informatics
- **Journal**: None
- **Summary**: The correct functioning of photovoltaic (PV) cells is critical to ensuring the optimal performance of a solar plant. Anomaly detection techniques for PV cells can result in significant cost savings in operation and maintenance (O&M). Recent research has focused on deep learning techniques for automatically detecting anomalies in Electroluminescence (EL) images. Automated anomaly annotations can improve current O&M methodologies and help develop decision-making systems to extend the life-cycle of the PV cells and predict failures. This paper addresses the lack of anomaly segmentation annotations in the literature by proposing a combination of state-of-the-art data-driven techniques to create a Golden Standard benchmark. The proposed method stands out for (1) its adaptability to new PV cell types, (2) cost-efficient fine-tuning, and (3) leverage public datasets to generate advanced annotations. The methodology has been validated in the annotation of a widely used dataset, obtaining a reduction of the annotation cost by 60%.



### Enhancing Indic Handwritten Text Recognition Using Global Semantic Information
- **Arxiv ID**: http://arxiv.org/abs/2212.07776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07776v1)
- **Published**: 2022-12-15 12:53:26+00:00
- **Updated**: 2022-12-15 12:53:26+00:00
- **Authors**: Ajoy Mondal, C. V. Jawahar
- **Comment**: None
- **Journal**: None
- **Summary**: Handwritten Text Recognition (HTR) is more interesting and challenging than printed text due to uneven variations in the handwriting style of the writers, content, and time. HTR becomes more challenging for the Indic languages because of (i) multiple characters combined to form conjuncts which increase the number of characters of respective languages, and (ii) near to 100 unique basic Unicode characters in each Indic script. Recently, many recognition methods based on the encoder-decoder framework have been proposed to handle such problems. They still face many challenges, such as image blur and incomplete characters due to varying writing styles and ink density. We argue that most encoder-decoder methods are based on local visual features without explicit global semantic information.   In this work, we enhance the performance of Indic handwritten text recognizers using global semantic information. We use a semantic module in an encoder-decoder framework for extracting global semantic information to recognize the Indic handwritten texts. The semantic information is used in both the encoder for supervision and the decoder for initialization. The semantic information is predicted from the word embedding of a pre-trained language model. Extensive experiments demonstrate that the proposed framework achieves state-of-the-art results on handwritten texts of ten Indic languages.



### Efficient Visual Computing with Camera RAW Snapshots
- **Arxiv ID**: http://arxiv.org/abs/2212.07778v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.07778v1)
- **Published**: 2022-12-15 12:54:21+00:00
- **Updated**: 2022-12-15 12:54:21+00:00
- **Authors**: Zhihao Li, Ming Lu, Xu Zhang, Xin Feng, M. Salman Asif, Zhan Ma
- **Comment**: home page: https://njuvision.github.io/rho-vision
- **Journal**: None
- **Summary**: Conventional cameras capture image irradiance on a sensor and convert it to RGB images using an image signal processor (ISP). The images can then be used for photography or visual computing tasks in a variety of applications, such as public safety surveillance and autonomous driving. One can argue that since RAW images contain all the captured information, the conversion of RAW to RGB using an ISP is not necessary for visual computing. In this paper, we propose a novel $\rho$-Vision framework to perform high-level semantic understanding and low-level compression using RAW images without the ISP subsystem used for decades. Considering the scarcity of available RAW image datasets, we first develop an unpaired CycleR2R network based on unsupervised CycleGAN to train modular unrolled ISP and inverse ISP (invISP) models using unpaired RAW and RGB images. We can then flexibly generate simulated RAW images (simRAW) using any existing RGB image dataset and finetune different models originally trained for the RGB domain to process real-world camera RAW images. We demonstrate object detection and image compression capabilities in RAW-domain using RAW-domain YOLOv3 and RAW image compressor (RIC) on snapshots from various cameras. Quantitative results reveal that RAW-domain task inference provides better detection accuracy and compression compared to RGB-domain processing. Furthermore, the proposed \r{ho}-Vision generalizes across various camera sensors and different task-specific models. Additional advantages of the proposed $\rho$-Vision that eliminates the ISP are the potential reductions in computations and processing times.



### Unsupervised Object Localization: Observing the Background to Discover Objects
- **Arxiv ID**: http://arxiv.org/abs/2212.07834v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07834v2)
- **Published**: 2022-12-15 13:43:11+00:00
- **Updated**: 2023-03-29 14:03:20+00:00
- **Authors**: Oriane Siméoni, Chloé Sekkat, Gilles Puy, Antonin Vobecky, Éloi Zablocki, Patrick Pérez
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Recent advances in self-supervised visual representation learning have paved the way for unsupervised methods tackling tasks such as object discovery and instance segmentation. However, discovering objects in an image with no supervision is a very hard task; what are the desired objects, when to separate them into parts, how many are there, and of what classes? The answers to these questions depend on the tasks and datasets of evaluation. In this work, we take a different approach and propose to look for the background instead. This way, the salient objects emerge as a by-product without any strong assumption on what an object should be. We propose FOUND, a simple model made of a single $conv1\times1$ initialized with coarse background masks extracted from self-supervised patch-based representations. After fast training and refining these seed masks, the model reaches state-of-the-art results on unsupervised saliency detection and object discovery benchmarks. Moreover, we show that our approach yields good results in the unsupervised semantic segmentation retrieval task. The code to reproduce our results is available at https://github.com/valeoai/FOUND.



### TeTIm-Eval: a novel curated evaluation data set for comparing text-to-image models
- **Arxiv ID**: http://arxiv.org/abs/2212.07839v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.07839v1)
- **Published**: 2022-12-15 13:52:03+00:00
- **Updated**: 2022-12-15 13:52:03+00:00
- **Authors**: Federico A. Galatolo, Mario G. C. A. Cimino, Edoardo Cogotti
- **Comment**: None
- **Journal**: None
- **Summary**: Evaluating and comparing text-to-image models is a challenging problem. Significant advances in the field have recently been made, piquing interest of various industrial sectors. As a consequence, a gold standard in the field should cover a variety of tasks and application contexts. In this paper a novel evaluation approach is experimented, on the basis of: (i) a curated data set, made by high-quality royalty-free image-text pairs, divided into ten categories; (ii) a quantitative metric, the CLIP-score, (iii) a human evaluation task to distinguish, for a given text, the real and the generated images. The proposed method has been applied to the most recent models, i.e., DALLE2, Latent Diffusion, Stable Diffusion, GLIDE and Craiyon. Early experimental results show that the accuracy of the human judgement is fully coherent with the CLIP-score. The dataset has been made available to the public.



### DETR4D: Direct Multi-View 3D Object Detection with Sparse Attention
- **Arxiv ID**: http://arxiv.org/abs/2212.07849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07849v1)
- **Published**: 2022-12-15 14:18:47+00:00
- **Updated**: 2022-12-15 14:18:47+00:00
- **Authors**: Zhipeng Luo, Changqing Zhou, Gongjie Zhang, Shijian Lu
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection with surround-view images is an essential task for autonomous driving. In this work, we propose DETR4D, a Transformer-based framework that explores sparse attention and direct feature query for 3D object detection in multi-view images. We design a novel projective cross-attention mechanism for query-image interaction to address the limitations of existing methods in terms of geometric cue exploitation and information loss for cross-view objects. In addition, we introduce a heatmap generation technique that bridges 3D and 2D spaces efficiently via query initialization. Furthermore, unlike the common practice of fusing intermediate spatial features for temporal aggregation, we provide a new perspective by introducing a novel hybrid approach that performs cross-frame fusion over past object queries and image features, enabling efficient and robust modeling of temporal information. Extensive experiments on the nuScenes dataset demonstrate the effectiveness and efficiency of the proposed DETR4D.



### QueryPose: Sparse Multi-Person Pose Regression via Spatial-Aware Part-Level Query
- **Arxiv ID**: http://arxiv.org/abs/2212.07855v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07855v1)
- **Published**: 2022-12-15 14:22:49+00:00
- **Updated**: 2022-12-15 14:22:49+00:00
- **Authors**: Yabo Xiao, Kai Su, Xiaojuan Wang, Dongdong Yu, Lei Jin, Mingshu He, Zehuan Yuan
- **Comment**: Published on NeurIPS 2022
- **Journal**: None
- **Summary**: We propose a sparse end-to-end multi-person pose regression framework, termed QueryPose, which can directly predict multi-person keypoint sequences from the input image. The existing end-to-end methods rely on dense representations to preserve the spatial detail and structure for precise keypoint localization. However, the dense paradigm introduces complex and redundant post-processes during inference. In our framework, each human instance is encoded by several learnable spatial-aware part-level queries associated with an instance-level query. First, we propose the Spatial Part Embedding Generation Module (SPEGM) that considers the local spatial attention mechanism to generate several spatial-sensitive part embeddings, which contain spatial details and structural information for enhancing the part-level queries. Second, we introduce the Selective Iteration Module (SIM) to adaptively update the sparse part-level queries via the generated spatial-sensitive part embeddings stage-by-stage. Based on the two proposed modules, the part-level queries are able to fully encode the spatial details and structural information for precise keypoint regression. With the bipartite matching, QueryPose avoids the hand-designed post-processes and surpasses the existing dense end-to-end methods with 73.6 AP on MS COCO mini-val set and 72.7 AP on CrowdPose test set. Code is available at https://github.com/buptxyb666/QueryPose.



### Localizing Scan Targets from Human Pose for Autonomous Lung Ultrasound Imaging
- **Arxiv ID**: http://arxiv.org/abs/2212.07867v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.RO, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2212.07867v2)
- **Published**: 2022-12-15 14:34:12+00:00
- **Updated**: 2023-02-25 06:49:55+00:00
- **Authors**: Jianzhi Long, Jicang Cai, Abdullah Al-Battal, Shiwei Jin, Jing Zhang, Dacheng Tao, Truong Nguyen
- **Comment**: v2 2023/02/25
- **Journal**: None
- **Summary**: Ultrasound is progressing toward becoming an affordable and versatile solution to medical imaging. With the advent of COVID-19 global pandemic, there is a need to fully automate ultrasound imaging as it requires trained operators in close proximity to patients for a long period of time, therefore increasing risk of infection. In this work, we investigate the important yet seldom-studied problem of scan target localization, under the setting of lung ultrasound imaging. We propose a purely vision-based, data driven method that incorporates learning-based computer vision techniques. We combine a human pose estimation model with a specially designed regression model to predict the lung ultrasound scan targets, and deploy multiview stereo vision to enhance the consistency of 3D target localization. While related works mostly focus on phantom experiments, we collect data from 30 human subjects for testing. Our method attains an accuracy level of 16.00(9.79) mm for probe positioning and 4.44(3.75) degree for probe orientation, with a success rate above 80% under an error threshold of 25mm for all scan targets. Moreover, our approach can serve as a general solution to other types of ultrasound modalities. The code for implementation has been released.



### Meta-Learned Kernel For Blind Super-Resolution Kernel Estimation
- **Arxiv ID**: http://arxiv.org/abs/2212.07886v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07886v1)
- **Published**: 2022-12-15 15:11:38+00:00
- **Updated**: 2022-12-15 15:11:38+00:00
- **Authors**: Royson Lee, Rui Li, Stylianos I. Venieris, Timothy Hospedales, Ferenc Huszár, Nicholas D. Lane
- **Comment**: Preprint: under review
- **Journal**: None
- **Summary**: Recent image degradation estimation methods have enabled single-image super-resolution (SR) approaches to better upsample real-world images. Among these methods, explicit kernel estimation approaches have demonstrated unprecedented performance at handling unknown degradations. Nonetheless, a number of limitations constrain their efficacy when used by downstream SR models. Specifically, this family of methods yields i) excessive inference time due to long per-image adaptation times and ii) inferior image fidelity due to kernel mismatch. In this work, we introduce a learning-to-learn approach that meta-learns from the information contained in a distribution of images, thereby enabling significantly faster adaptation to new images with substantially improved performance in both kernel estimation and image fidelity. Specifically, we meta-train a kernel-generating GAN, named MetaKernelGAN, on a range of tasks, such that when a new image is presented, the generator starts from an informed kernel estimate and the discriminator starts with a strong capability to distinguish between patch distributions. Compared with state-of-the-art methods, our experiments show that MetaKernelGAN better estimates the magnitude and covariance of the kernel, leading to state-of-the-art blind SR results within a similar computational regime when combined with a non-blind SR model. Through supervised learning of an unsupervised learner, our method maintains the generalizability of the unsupervised learner, improves the optimization stability of kernel estimation, and hence image adaptation, and leads to a faster inference with a speedup between 14.24 to 102.1x over existing methods.



### Full Contextual Attention for Multi-resolution Transformers in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2212.07890v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2212.07890v1)
- **Published**: 2022-12-15 15:19:09+00:00
- **Updated**: 2022-12-15 15:19:09+00:00
- **Authors**: Loic Themyr, Clement Rambour, Nicolas Thome, Toby Collins, Alexandre Hostettler
- **Comment**: Winter Conference on Applications of Computer Vision (WACV 2023)
- **Journal**: None
- **Summary**: Transformers have proved to be very effective for visual recognition tasks. In particular, vision transformers construct compressed global representations through self-attention and learnable class tokens. Multi-resolution transformers have shown recent successes in semantic segmentation but can only capture local interactions in high-resolution feature maps. This paper extends the notion of global tokens to build GLobal Attention Multi-resolution (GLAM) transformers. GLAM is a generic module that can be integrated into most existing transformer backbones. GLAM includes learnable global tokens, which unlike previous methods can model interactions between all image regions, and extracts powerful representations during training. Extensive experiments show that GLAM-Swin or GLAM-Swin-UNet exhibit substantially better performances than their vanilla counterparts on ADE20K and Cityscapes. Moreover, GLAM can be used to segment large 3D medical images, and GLAM-nnFormer achieves new state-of-the-art performance on the BCV dataset.



### Emergent Behaviors in Multi-Agent Target Acquisition
- **Arxiv ID**: http://arxiv.org/abs/2212.07891v1
- **DOI**: 10.1117/12.2618646
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2212.07891v1)
- **Published**: 2022-12-15 15:20:58+00:00
- **Updated**: 2022-12-15 15:20:58+00:00
- **Authors**: Piyush K. Sharma, Erin Zaroukian, Derrik E. Asher, Bryson Howell
- **Comment**: This article appeared in the news at:
  https://www.army.mil/article/258408/u_s_army_scientists_invent_a_method_to_characterize_ai_behavior
- **Journal**: Published in:Proceedings Volume 12113, Artificial Intelligence and
  Machine Learning for Multi-Domain Operations Applications IV; 1211314 (6 June
  2022), SPIE Defense + Commercial Sensing, 2022, Orlando, Florida, United
  States
- **Summary**: Only limited studies and superficial evaluations are available on agents' behaviors and roles within a Multi-Agent System (MAS). We simulate a MAS using Reinforcement Learning (RL) in a pursuit-evasion (a.k.a predator-prey pursuit) game, which shares task goals with target acquisition, and we create different adversarial scenarios by replacing RL-trained pursuers' policies with two distinct (non-RL) analytical strategies. Using heatmaps of agents' positions (state-space variable) over time, we are able to categorize an RL-trained evader's behaviors. The novelty of our approach entails the creation of an influential feature set that reveals underlying data regularities, which allow us to classify an agent's behavior. This classification may aid in catching the (enemy) targets by enabling us to identify and predict their behaviors, and when extended to pursuers, this approach towards identifying teammates' behavior may allow agents to coordinate more effectively.



### EVAL: Explainable Video Anomaly Localization
- **Arxiv ID**: http://arxiv.org/abs/2212.07900v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07900v1)
- **Published**: 2022-12-15 15:35:25+00:00
- **Updated**: 2022-12-15 15:35:25+00:00
- **Authors**: Ashish Singh, Michael J. Jones, Erik Learned-Miller
- **Comment**: None
- **Journal**: None
- **Summary**: We develop a novel framework for single-scene video anomaly localization that allows for human-understandable reasons for the decisions the system makes. We first learn general representations of objects and their motions (using deep networks) and then use these representations to build a high-level, location-dependent model of any particular scene. This model can be used to detect anomalies in new videos of the same scene. Importantly, our approach is explainable - our high-level appearance and motion features can provide human-understandable reasons for why any part of a video is classified as normal or anomalous. We conduct experiments on standard video anomaly detection datasets (Street Scene, CUHK Avenue, ShanghaiTech and UCSD Ped1, Ped2) and show significant improvements over the previous state-of-the-art.



### Automatic vehicle trajectory data reconstruction at scale
- **Arxiv ID**: http://arxiv.org/abs/2212.07907v1
- **DOI**: None
- **Categories**: **cs.DS**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.07907v1)
- **Published**: 2022-12-15 15:39:55+00:00
- **Updated**: 2022-12-15 15:39:55+00:00
- **Authors**: Yanbing Wang, Derek Gloudemans, Zi Nean Teoh, Lisa Liu, Gergely Zachár, William Barbour, Daniel Work
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle trajectory data has received increasing research attention over the past decades. With the technological sensing improvements such as high-resolution video cameras, in-vehicle radars and lidars, abundant individual and contextual traffic data is now available. However, though the data quantity is massive, it is by itself of limited utility for traffic research because of noise and systematic sensing errors, thus necessitates proper processing to ensure data quality. We draw particular attention to extracting high-resolution vehicle trajectory data from video cameras as traffic monitoring cameras are becoming increasingly ubiquitous. We explore methods for automatic trajectory data reconciliation, given "raw" vehicle detection and tracking information from automatic video processing algorithms. We propose a pipeline including a) an online data association algorithm to match fragments that are associated to the same object (vehicle), which is formulated as a min-cost network flow problem of a graph, and b) a trajectory reconciliation method formulated as a quadratic program to enhance raw detection data. The pipeline leverages vehicle dynamics and physical constraints to associate tracked objects when they become fragmented, remove measurement noise on trajectories and impute missing data due to fragmentations. The accuracy is benchmarked on a sample of manually-labeled data, which shows that the reconciled trajectories improve the accuracy on all the tested input data for a wide range of measures. An online version of the reconciliation pipeline is implemented and will be applied in a continuous video processing system running on a camera network covering a 4-mile stretch of Interstate-24 near Nashville, Tennessee.



### Urban Scene Semantic Segmentation with Low-Cost Coarse Annotation
- **Arxiv ID**: http://arxiv.org/abs/2212.07911v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.07911v1)
- **Published**: 2022-12-15 15:43:42+00:00
- **Updated**: 2022-12-15 15:43:42+00:00
- **Authors**: Anurag Das, Yongqin Xian, Yang He, Zeynep Akata, Bernt Schiele
- **Comment**: Accepted at WACV 2023
- **Journal**: None
- **Summary**: For best performance, today's semantic segmentation methods use large and carefully labeled datasets, requiring expensive annotation budgets. In this work, we show that coarse annotation is a low-cost but highly effective alternative for training semantic segmentation models. Considering the urban scene segmentation scenario, we leverage cheap coarse annotations for real-world captured data, as well as synthetic data to train our model and show competitive performance compared with finely annotated real-world data. Specifically, we propose a coarse-to-fine self-training framework that generates pseudo labels for unlabeled regions of the coarsely annotated data, using synthetic data to improve predictions around the boundaries between semantic classes, and using cross-domain data augmentation to increase diversity. Our extensive experimental results on Cityscapes and BDD100k datasets demonstrate that our method achieves a significantly better performance vs annotation cost tradeoff, yielding a comparable performance to fully annotated data with only a small fraction of the annotation budget. Also, when used as pretraining, our framework performs better compared to the standard fully supervised setting.



### The Effects of Character-Level Data Augmentation on Style-Based Dating of Historical Manuscripts
- **Arxiv ID**: http://arxiv.org/abs/2212.07923v1
- **DOI**: 10.5220/0011699500003411
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.07923v1)
- **Published**: 2022-12-15 15:55:44+00:00
- **Updated**: 2022-12-15 15:55:44+00:00
- **Authors**: Lisa Koopmans, Maruf A. Dhali, Lambert Schomaker
- **Comment**: Accepted after the peer-review process for ICPRAM 2023; scheduled to
  be presented on 22 February 2023
- **Journal**: Proceedings of the 12th International Conference on Pattern
  Recognition Applications and Methods - ICPRAM, 124-135, 2023
- **Summary**: Identifying the production dates of historical manuscripts is one of the main goals for paleographers when studying ancient documents. Automatized methods can provide paleographers with objective tools to estimate dates more accurately. Previously, statistical features have been used to date digitized historical manuscripts based on the hypothesis that handwriting styles change over periods. However, the sparse availability of such documents poses a challenge in obtaining robust systems. Hence, the research of this article explores the influence of data augmentation on the dating of historical manuscripts. Linear Support Vector Machines were trained with k-fold cross-validation on textural and grapheme-based features extracted from historical manuscripts of different collections, including the Medieval Paleographical Scale, early Aramaic manuscripts, and the Dead Sea Scrolls. Results show that training models with augmented data improve the performance of historical manuscripts dating by 1% - 3% in cumulative scores. Additionally, this indicates further enhancement possibilities by considering models specific to the features and the documents' scripts.



### Ring That Bell: A Corpus and Method for Multimodal Metaphor Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/2301.01134v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.01134v1)
- **Published**: 2022-12-15 17:11:35+00:00
- **Updated**: 2022-12-15 17:11:35+00:00
- **Authors**: Khalid Alnajjar, Mika Hämäläinen, Shuo Zhang
- **Comment**: Figlang 2022
- **Journal**: None
- **Summary**: We present the first openly available multimodal metaphor annotated corpus. The corpus consists of videos including audio and subtitles that have been annotated by experts. Furthermore, we present a method for detecting metaphors in the new dataset based on the textual content of the videos. The method achieves a high F1-score (62\%) for metaphorical labels. We also experiment with other modalities and multimodal methods; however, these methods did not out-perform the text-based model. In our error analysis, we do identify that there are cases where video could help in disambiguating metaphors, however, the visual cues are too subtle for our model to capture. The data is available on Zenodo.



### Vision Transformers are Parameter-Efficient Audio-Visual Learners
- **Arxiv ID**: http://arxiv.org/abs/2212.07983v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2212.07983v2)
- **Published**: 2022-12-15 17:31:54+00:00
- **Updated**: 2023-04-05 17:41:12+00:00
- **Authors**: Yan-Bo Lin, Yi-Lin Sung, Jie Lei, Mohit Bansal, Gedas Bertasius
- **Comment**: CVPR 2023 Project Page: https://genjib.github.io/project_page/LAVISH/
- **Journal**: None
- **Summary**: Vision transformers (ViTs) have achieved impressive results on various computer vision tasks in the last several years. In this work, we study the capability of frozen ViTs, pretrained only on visual data, to generalize to audio-visual data without finetuning any of its original parameters. To do so, we propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained ViTs to audio-visual tasks by injecting a small number of trainable parameters into every layer of a frozen ViT. To efficiently fuse visual and audio cues, our LAVISH adapter uses a small set of latent tokens, which form an attention bottleneck, thus, eliminating the quadratic cost of standard cross-attention. Compared to the existing modality-specific audio-visual methods, our approach achieves competitive or even better performance on various audio-visual tasks while using fewer tunable parameters and without relying on costly audio pretraining or external audio encoders. Our code is available at https://genjib.github.io/project_page/LAVISH/



### Alternating Objectives Generates Stronger PGD-Based Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2212.07992v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.07992v1)
- **Published**: 2022-12-15 17:44:31+00:00
- **Updated**: 2022-12-15 17:44:31+00:00
- **Authors**: Nikolaos Antoniou, Efthymios Georgiou, Alexandros Potamianos
- **Comment**: None
- **Journal**: None
- **Summary**: Designing powerful adversarial attacks is of paramount importance for the evaluation of $\ell_p$-bounded adversarial defenses. Projected Gradient Descent (PGD) is one of the most effective and conceptually simple algorithms to generate such adversaries. The search space of PGD is dictated by the steepest ascent directions of an objective. Despite the plethora of objective function choices, there is no universally superior option and robustness overestimation may arise from ill-suited objective selection. Driven by this observation, we postulate that the combination of different objectives through a simple loss alternating scheme renders PGD more robust towards design choices. We experimentally verify this assertion on a synthetic-data example and by evaluating our proposed method across 25 different $\ell_{\infty}$-robust models and 3 datasets. The performance improvement is consistent, when compared to the single loss counterparts. In the CIFAR-10 dataset, our strongest adversarial attack outperforms all of the white-box components of AutoAttack (AA) ensemble, as well as the most powerful attacks existing on the literature, achieving state-of-the-art results in the computational budget of our study ($T=100$, no restarts).



### A New Deep Boosted CNN and Ensemble Learning based IoT Malware Detection
- **Arxiv ID**: http://arxiv.org/abs/2212.08008v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.08008v3)
- **Published**: 2022-12-15 18:14:51+00:00
- **Updated**: 2023-01-15 07:52:31+00:00
- **Authors**: Saddam Hussain Khan, Wasi Ullah
- **Comment**: 20 pages, 10 figures, 6 tables; Corresponding saddamhkhan@ueas.edu.pk
- **Journal**: None
- **Summary**: Security issues are threatened in various types of networks, especially in the Internet of Things (IoT) environment that requires early detection. IoT is the network of real-time devices like home automation systems and can be controlled by open-source android devices, which can be an open ground for attackers. Attackers can access the network credentials, initiate a different kind of security breach, and compromises network control. Therefore, timely detecting the increasing number of sophisticated malware attacks is the challenge to ensure the credibility of network protection. In this regard, we have developed a new malware detection framework, Deep Squeezed-Boosted and Ensemble Learning (DSBEL), comprised of novel Squeezed-Boosted Boundary-Region Split-Transform-Merge (SB-BR-STM) CNN and ensemble learning. The proposed STM block employs multi-path dilated convolutional, Boundary, and regional operations to capture the homogenous and heterogeneous global malicious patterns. Moreover, diverse feature maps are achieved using transfer learning and multi-path-based squeezing and boosting at initial and final levels to learn minute pattern variations. Finally, the boosted discriminative features are extracted from the developed deep SB-BR-STM CNN and provided to the ensemble classifiers (SVM, MLP, and AdabooSTM1) to improve the hybrid learning generalization. The performance analysis of the proposed DSBEL framework and SB-BR-STM CNN against the existing techniques have been evaluated by the IOT_Malware dataset on standard performance measures. Evaluation results show progressive performance as 98.50% accuracy, 97.12% F1-Score, 91.91% MCC, 95.97 % Recall, and 98.42 % Precision. The proposed malware analysis framework is robust and helpful for the timely detection of malicious activity and suggests future strategies



### FlexiViT: One Model for All Patch Sizes
- **Arxiv ID**: http://arxiv.org/abs/2212.08013v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.08013v2)
- **Published**: 2022-12-15 18:18:38+00:00
- **Updated**: 2023-03-23 21:38:16+00:00
- **Authors**: Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, Filip Pavetic
- **Comment**: Code and pre-trained models available at
  https://github.com/google-research/big_vision. All authors made significant
  technical contributions. CVPR 2023
- **Journal**: None
- **Summary**: Vision Transformers convert images to sequences by slicing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model. In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time. We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, open-world detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that makes it easy to add compute-adaptive capabilities to most models relying on a ViT backbone architecture. Code and pre-trained models are available at https://github.com/google-research/big_vision



### Are Multimodal Models Robust to Image and Text Perturbations?
- **Arxiv ID**: http://arxiv.org/abs/2212.08044v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.08044v1)
- **Published**: 2022-12-15 18:52:03+00:00
- **Updated**: 2022-12-15 18:52:03+00:00
- **Authors**: Jielin Qiu, Yi Zhu, Xingjian Shi, Florian Wenzel, Zhiqiang Tang, Ding Zhao, Bo Li, Mu Li
- **Comment**: The project webpage is at: https://mmrobustness.github.io/
- **Journal**: None
- **Summary**: Multimodal image-text models have shown remarkable performance in the past few years. However, evaluating their robustness against distribution shifts is crucial before adopting them in real-world applications. In this paper, we investigate the robustness of 9 popular open-sourced image-text models under common perturbations on five tasks (image-text retrieval, visual reasoning, visual entailment, image captioning, and text-to-image generation). In particular, we propose several new multimodal robustness benchmarks by applying 17 image perturbation and 16 text perturbation techniques on top of existing datasets. We observe that multimodal models are not robust to image and text perturbations, especially to image perturbations. Among the tested perturbation methods, character-level perturbations constitute the most severe distribution shift for text, and zoom blur is the most severe shift for image data. We also introduce two new robustness metrics (MMI and MOR) for proper evaluations of multimodal models. We hope our extensive study sheds light on new directions for the development of robust multimodal models.



### CLIPPO: Image-and-Language Understanding from Pixels Only
- **Arxiv ID**: http://arxiv.org/abs/2212.08045v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.08045v2)
- **Published**: 2022-12-15 18:52:08+00:00
- **Updated**: 2023-04-01 21:01:36+00:00
- **Authors**: Michael Tschannen, Basil Mustafa, Neil Houlsby
- **Comment**: CVPR 2023. Code and pretrained models are available at
  https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/clippo/README.md
- **Journal**: None
- **Summary**: Multimodal models are becoming increasingly effective, in part due to unified components, such as the Transformer architecture. However, multimodal models still often consist of many task- and modality-specific pieces and training procedures. For example, CLIP (Radford et al., 2021) trains independent text and image towers via a contrastive loss. We explore an additional unification: the use of a pure pixel-based model to perform image, text, and multimodal tasks. Our model is trained with contrastive loss alone, so we call it CLIP-Pixels Only (CLIPPO). CLIPPO uses a single encoder that processes both regular images and text rendered as images. CLIPPO performs image-based tasks such as retrieval and zero-shot image classification almost as well as CLIP-style models, with half the number of parameters and no text-specific tower or embedding. When trained jointly via image-text contrastive learning and next-sentence contrastive learning, CLIPPO can perform well on natural language understanding tasks, without any word-level loss (language modelling or masked language modelling), outperforming pixel-based prior work. Surprisingly, CLIPPO can obtain good accuracy in visual question answering, simply by rendering the question and image together. Finally, we exploit the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multimodal retrieval without modifications.



### Objaverse: A Universe of Annotated 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/2212.08051v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2212.08051v1)
- **Published**: 2022-12-15 18:56:53+00:00
- **Updated**: 2022-12-15 18:56:53+00:00
- **Authors**: Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi
- **Comment**: Website: objaverse.allenai.org
- **Journal**: None
- **Summary**: Massive data corpora like WebText, Wikipedia, Conceptual Captions, WebImageText, and LAION have propelled recent dramatic progress in AI. Large neural models trained on such datasets produce impressive results and top many of today's benchmarks. A notable omission within this family of large-scale datasets is 3D data. Despite considerable interest and potential applications in 3D vision, datasets of high-fidelity 3D models continue to be mid-sized with limited diversity of object categories. Addressing this gap, we present Objaverse 1.0, a large dataset of objects with 800K+ (and growing) 3D models with descriptive captions, tags, and animations. Objaverse improves upon present day 3D repositories in terms of scale, number of categories, and in the visual diversity of instances within a category. We demonstrate the large potential of Objaverse via four diverse applications: training generative 3D models, improving tail category segmentation on the LVIS benchmark, training open-vocabulary object-navigation models for Embodied AI, and creating a new benchmark for robustness analysis of vision models. Objaverse can open new directions for research and enable new applications across the field of AI.



### Real-Time Neural Light Field on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2212.08057v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.08057v2)
- **Published**: 2022-12-15 18:58:56+00:00
- **Updated**: 2023-06-24 20:48:05+00:00
- **Authors**: Junli Cao, Huan Wang, Pavlo Chemerys, Vladislav Shakhrai, Ju Hu, Yun Fu, Denys Makoviichuk, Sergey Tulyakov, Jian Ren
- **Comment**: CVPR 2023. Project page: https://snap-research.github.io/MobileR2L/
  Code: https://github.com/snap-research/MobileR2L/
- **Journal**: None
- **Summary**: Recent efforts in Neural Rendering Fields (NeRF) have shown impressive results on novel view synthesis by utilizing implicit neural representation to represent 3D scenes. Due to the process of volumetric rendering, the inference speed for NeRF is extremely slow, limiting the application scenarios of utilizing NeRF on resource-constrained hardware, such as mobile devices. Many works have been conducted to reduce the latency of running NeRF models. However, most of them still require high-end GPU for acceleration or extra storage memory, which is all unavailable on mobile devices. Another emerging direction utilizes the neural light field (NeLF) for speedup, as only one forward pass is performed on a ray to predict the pixel color. Nevertheless, to reach a similar rendering quality as NeRF, the network in NeLF is designed with intensive computation, which is not mobile-friendly. In this work, we propose an efficient network that runs in real-time on mobile devices for neural rendering. We follow the setting of NeLF to train our network. Unlike existing works, we introduce a novel network architecture that runs efficiently on mobile devices with low latency and small size, i.e., saving $15\times \sim 24\times$ storage compared with MobileNeRF. Our model achieves high-resolution generation while maintaining real-time inference for both synthetic and real-world scenes on mobile devices, e.g., $18.04$ms (iPhone 13) for rendering one $1008\times756$ image of real 3D scenes. Additionally, we achieve similar image quality as NeRF and better quality than MobileNeRF (PSNR $26.15$ vs. $25.91$ on the real-world forward-facing dataset).



### Learning a Fast 3D Spectral Approach to Object Segmentation and Tracking over Space and Time
- **Arxiv ID**: http://arxiv.org/abs/2212.08058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.08058v1)
- **Published**: 2022-12-15 18:59:07+00:00
- **Updated**: 2022-12-15 18:59:07+00:00
- **Authors**: Elena Burceanu, Marius Leordeanu
- **Comment**: None
- **Journal**: None
- **Summary**: We pose video object segmentation as spectral graph clustering in space and time, with one graph node for each pixel and edges forming local space-time neighborhoods. We claim that the strongest cluster in this video graph represents the salient object. We start by introducing a novel and efficient method based on 3D filtering for approximating the spectral solution, as the principal eigenvector of the graph's adjacency matrix, without explicitly building the matrix. This key property allows us to have a fast parallel implementation on GPU, orders of magnitude faster than classical approaches for computing the eigenvector. Our motivation for a spectral space-time clustering approach, unique in video semantic segmentation literature, is that such clustering is dedicated to preserving object consistency over time, which we evaluate using our novel segmentation consistency measure. Further on, we show how to efficiently learn the solution over multiple input feature channels. Finally, we extend the formulation of our approach beyond the segmentation task, into the realm of object tracking. In extensive experiments we show significant improvements over top methods, as well as over powerful ensembles that combine them, achieving state-of-the-art on multiple benchmarks, both for tracking and segmentation.



### Rethinking Vision Transformers for MobileNet Size and Speed
- **Arxiv ID**: http://arxiv.org/abs/2212.08059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.08059v1)
- **Published**: 2022-12-15 18:59:12+00:00
- **Updated**: 2022-12-15 18:59:12+00:00
- **Authors**: Yanyu Li, Ju Hu, Yang Wen, Georgios Evangelidis, Kamyar Salahi, Yanzhi Wang, Sergey Tulyakov, Jian Ren
- **Comment**: Code is available at:
  https://github.com/snap-research/EfficientFormer
- **Journal**: None
- **Summary**: With the success of Vision Transformers (ViTs) in computer vision tasks, recent arts try to optimize the performance and complexity of ViTs to enable efficient deployment on mobile devices. Multiple approaches are proposed to accelerate attention mechanism, improve inefficient designs, or incorporate mobile-friendly lightweight convolutions to form hybrid architectures. However, ViT and its variants still have higher latency or considerably more parameters than lightweight CNNs, even true for the years-old MobileNet. In practice, latency and size are both crucial for efficient deployment on resource-constraint hardware. In this work, we investigate a central question, can transformer models run as fast as MobileNet and maintain a similar size? We revisit the design choices of ViTs and propose an improved supernet with low latency and high parameter efficiency. We further introduce a fine-grained joint search strategy that can find efficient architectures by optimizing latency and number of parameters simultaneously. The proposed models, EfficientFormerV2, achieve about $4\%$ higher top-1 accuracy than MobileNetV2 and MobileNetV2$\times1.4$ on ImageNet-1K with similar latency and parameters. We demonstrate that properly designed and optimized vision transformers can achieve high performance with MobileNet-level size and speed.



### MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2212.08062v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.08062v3)
- **Published**: 2022-12-15 18:59:33+00:00
- **Updated**: 2023-03-27 02:16:13+00:00
- **Authors**: Bowen Zhang, Chenyang Qi, Pan Zhang, Bo Zhang, HsiangTao Wu, Dong Chen, Qifeng Chen, Yong Wang, Fang Wen
- **Comment**: CVPR 2023, project page: https://meta-portrait.github.io
- **Journal**: None
- **Summary**: In this work, we propose an ID-preserving talking head generation framework, which advances previous methods in two aspects. First, as opposed to interpolating from sparse flow, we claim that dense landmarks are crucial to achieving accurate geometry-aware flow fields. Second, inspired by face-swapping methods, we adaptively fuse the source identity during synthesis, so that the network better preserves the key characteristics of the image portrait. Although the proposed model surpasses prior generation fidelity on established benchmarks, to further make the talking head generation qualified for real usage, personalized fine-tuning is usually needed. However, this process is rather computationally demanding that is unaffordable to standard users. To solve this, we propose a fast adaptation model using a meta-learning approach. The learned model can be adapted to a high-quality personalized model as fast as 30 seconds. Last but not the least, a spatial-temporal enhancement module is proposed to improve the fine details while ensuring temporal coherency. Extensive experiments prove the significant superiority of our approach over the state of the arts in both one-shot and personalized settings.



### Mod-Squad: Designing Mixture of Experts As Modular Multi-Task Learners
- **Arxiv ID**: http://arxiv.org/abs/2212.08066v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.08066v1)
- **Published**: 2022-12-15 18:59:52+00:00
- **Updated**: 2022-12-15 18:59:52+00:00
- **Authors**: Zitian Chen, Yikang Shen, Mingyu Ding, Zhenfang Chen, Hengshuang Zhao, Erik Learned-Miller, Chuang Gan
- **Comment**: None
- **Journal**: None
- **Summary**: Optimization in multi-task learning (MTL) is more challenging than single-task learning (STL), as the gradient from different tasks can be contradictory. When tasks are related, it can be beneficial to share some parameters among them (cooperation). However, some tasks require additional parameters with expertise in a specific type of data or discrimination (specialization). To address the MTL challenge, we propose Mod-Squad, a new model that is Modularized into groups of experts (a 'Squad'). This structure allows us to formalize cooperation and specialization as the process of matching experts and tasks. We optimize this matching process during the training of a single model. Specifically, we incorporate mixture of experts (MoE) layers into a transformer model, with a new loss that incorporates the mutual dependence between tasks and experts. As a result, only a small set of experts are activated for each task. This prevents the sharing of the entire backbone model between all tasks, which strengthens the model, especially when the training set size and the number of tasks scale up. More interestingly, for each task, we can extract the small set of experts as a standalone model that maintains the same performance as the large model. Extensive experiments on the Taskonomy dataset with 13 vision tasks and the PASCAL-Context dataset with 5 vision tasks show the superiority of our approach.



### VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2212.08067v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.08067v2)
- **Published**: 2022-12-15 18:59:54+00:00
- **Updated**: 2023-04-03 06:54:50+00:00
- **Authors**: Yufan Ren, Fangjinhua Wang, Tong Zhang, Marc Pollefeys, Sabine Süsstrunk
- **Comment**: None
- **Journal**: None
- **Summary**: The success of the Neural Radiance Fields (NeRF) in novel view synthesis has inspired researchers to propose neural implicit scene reconstruction. However, most existing neural implicit reconstruction methods optimize per-scene parameters and therefore lack generalizability to new scenes. We introduce VolRecon, a novel generalizable implicit reconstruction method with Signed Ray Distance Function (SRDF). To reconstruct the scene with fine details and little noise, VolRecon combines projection features aggregated from multi-view features, and volume features interpolated from a coarse global feature volume. Using a ray transformer, we compute SRDF values of sampled points on a ray and then render color and depth. On DTU dataset, VolRecon outperforms SparseNeuS by about 30% in sparse view reconstruction and achieves comparable accuracy as MVSNet in full view reconstruction. Furthermore, our approach exhibits good generalization performance on the large-scale ETH3D benchmark.



### NeRF-Art: Text-Driven Neural Radiance Fields Stylization
- **Arxiv ID**: http://arxiv.org/abs/2212.08070v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2212.08070v1)
- **Published**: 2022-12-15 18:59:58+00:00
- **Updated**: 2022-12-15 18:59:58+00:00
- **Authors**: Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao
- **Comment**: Project page: https://cassiepython.github.io/nerfart/
- **Journal**: None
- **Summary**: As a powerful representation of 3D scenes, the neural radiance field (NeRF) enables high-quality novel view synthesis from multi-view images. Stylizing NeRF, however, remains challenging, especially on simulating a text-guided style with both the appearance and the geometry altered simultaneously. In this paper, we present NeRF-Art, a text-guided NeRF stylization approach that manipulates the style of a pre-trained NeRF model with a simple text prompt. Unlike previous approaches that either lack sufficient geometry deformations and texture details or require meshes to guide the stylization, our method can shift a 3D scene to the target style characterized by desired geometry and appearance variations without any mesh guidance. This is achieved by introducing a novel global-local contrastive learning strategy, combined with the directional constraint to simultaneously control both the trajectory and the strength of the target style. Moreover, we adopt a weight regularization method to effectively suppress cloudy artifacts and geometry noises which arise easily when the density field is transformed during geometry stylization. Through extensive experiments on various styles, we demonstrate that our method is effective and robust regarding both single-view stylization quality and cross-view consistency. The code and more results can be found in our project page: https://cassiepython.github.io/nerfart/.



### MAViL: Masked Audio-Video Learners
- **Arxiv ID**: http://arxiv.org/abs/2212.08071v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2212.08071v2)
- **Published**: 2022-12-15 18:59:59+00:00
- **Updated**: 2023-07-17 05:44:35+00:00
- **Authors**: Po-Yao Huang, Vasu Sharma, Hu Xu, Chaitanya Ryali, Haoqi Fan, Yanghao Li, Shang-Wen Li, Gargi Ghosh, Jitendra Malik, Christoph Feichtenhofer
- **Comment**: Technical report
- **Journal**: None
- **Summary**: We present Masked Audio-Video Learners (MAViL) to train audio-visual representations. Our approach learns with three complementary forms of self-supervision: (1) reconstruction of masked audio and video input data, (2) intra- and inter-modal contrastive learning with masking, and (3) self-training by reconstructing joint audio-video contextualized features learned from the first two objectives. Pre-training with MAViL not only enables the model to perform well in audio-visual classification and retrieval tasks but also improves representations of each modality in isolation, without using information from the other modality for fine-tuning or inference. Empirically, MAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1% accuracy). For the first time, a self-supervised audio-visual model outperforms ones that use external supervision on these benchmarks.



### Backdoor Attack Detection in Computer Vision by Applying Matrix Factorization on the Weights of Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2212.08121v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.08121v1)
- **Published**: 2022-12-15 20:20:18+00:00
- **Updated**: 2022-12-15 20:20:18+00:00
- **Authors**: Khondoker Murad Hossain, Tim Oates
- **Comment**: 7 pages, 4 figures, 5 tables, AAAI Workshop on Safe AI 2023
- **Journal**: None
- **Summary**: The increasing importance of both deep neural networks (DNNs) and cloud services for training them means that bad actors have more incentive and opportunity to insert backdoors to alter the behavior of trained models. In this paper, we introduce a novel method for backdoor detection that extracts features from pre-trained DNN's weights using independent vector analysis (IVA) followed by a machine learning classifier. In comparison to other detection techniques, this has a number of benefits, such as not requiring any training data, being applicable across domains, operating with a wide range of network architectures, not assuming the nature of the triggers used to change network behavior, and being highly scalable. We discuss the detection pipeline, and then demonstrate the results on two computer vision datasets regarding image classification and object detection. Our method outperforms the competing algorithms in terms of efficiency and is more accurate, helping to ensure the safe application of deep learning and AI.



### Bayesian posterior approximation with stochastic ensembles
- **Arxiv ID**: http://arxiv.org/abs/2212.08123v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2212.08123v2)
- **Published**: 2022-12-15 20:23:09+00:00
- **Updated**: 2023-04-04 13:26:22+00:00
- **Authors**: Oleksandr Balabanov, Bernhard Mehlig, Hampus Linander
- **Comment**: 19 pages, CVPR 2023
- **Journal**: None
- **Summary**: We introduce ensembles of stochastic neural networks to approximate the Bayesian posterior, combining stochastic methods such as dropout with deep ensembles. The stochastic ensembles are formulated as families of distributions and trained to approximate the Bayesian posterior with variational inference. We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and a novel non-parametric version of dropout and evaluate them on a toy problem and CIFAR image classification. For both tasks, we test the quality of the posteriors directly against Hamiltonian Monte Carlo simulations. Our results show that stochastic ensembles provide more accurate posterior estimates than other popular baselines for Bayesian inference.



### On Evaluating Adversarial Robustness of Chest X-ray Classification: Pitfalls and Best Practices
- **Arxiv ID**: http://arxiv.org/abs/2212.08130v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.08130v1)
- **Published**: 2022-12-15 20:35:48+00:00
- **Updated**: 2022-12-15 20:35:48+00:00
- **Authors**: Salah Ghamizi, Maxime Cordy, Michail Papadakis, Yves Le Traon
- **Comment**: None
- **Journal**: None
- **Summary**: Vulnerability to adversarial attacks is a well-known weakness of Deep Neural Networks. While most of the studies focus on natural images with standardized benchmarks like ImageNet and CIFAR, little research has considered real world applications, in particular in the medical domain. Our research shows that, contrary to previous claims, robustness of chest x-ray classification is much harder to evaluate and leads to very different assessments based on the dataset, the architecture and robustness metric. We argue that previous studies did not take into account the peculiarity of medical diagnosis, like the co-occurrence of diseases, the disagreement of labellers (domain experts), the threat model of the attacks and the risk implications for each successful attack.   In this paper, we discuss the methodological foundations, review the pitfalls and best practices, and suggest new methodological considerations for evaluating the robustness of chest xray classification models. Our evaluation on 3 datasets, 7 models, and 18 diseases is the largest evaluation of robustness of chest x-ray classification models.



### MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models & Tasks
- **Arxiv ID**: http://arxiv.org/abs/2212.08158v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, 68Txx, I.2.7; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2212.08158v2)
- **Published**: 2022-12-15 21:41:06+00:00
- **Updated**: 2023-05-23 12:36:12+00:00
- **Authors**: Letitia Parcalabescu, Anette Frank
- **Comment**: Paper accepted for publication at ACL 2023 Main (Toronto); 10 pages,
  14 appendix pages, 11 figures, 3 tables
- **Journal**: None
- **Summary**: Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality. That a unimodal model achieves similar accuracy on a VL task to a multimodal one, indicates that so-called unimodal collapse occurred. However, accuracy-based tests fail to detect e.g., when the model prediction is wrong, while the model used relevant information from a modality. Instead, we propose MM-SHAP, a performance-agnostic multimodality score based on Shapley values that reliably quantifies in which proportions a multimodal model uses individual modalities. We apply MM-SHAP in two ways: (1) to compare models for their average degree of multimodality, and (2) to measure for individual models the contribution of individual modalities for different tasks and datasets. Experiments with six VL models -- LXMERT, CLIP and four ALBEF variants -- on four VL tasks highlight that unimodal collapse can occur to different degrees and in different directions, contradicting the wide-spread assumption that unimodal collapse is one-sided. Based on our results, we recommend MM-SHAP for analysing multimodal tasks, to diagnose and guide progress towards multimodal integration. Code available at \url{https://github.com/Heidelberg-NLP/MM-SHAP}.



### Dual Moving Average Pseudo-Labeling for Source-Free Inductive Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2212.08187v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.08187v1)
- **Published**: 2022-12-15 23:20:13+00:00
- **Updated**: 2022-12-15 23:20:13+00:00
- **Authors**: Hao Yan, Yuhong Guo
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Unsupervised domain adaptation reduces the reliance on data annotation in deep learning by adapting knowledge from a source to a target domain. For privacy and efficiency concerns, source-free domain adaptation extends unsupervised domain adaptation by adapting a pre-trained source model to an unlabeled target domain without accessing the source data. However, most existing source-free domain adaptation methods to date focus on the transductive setting, where the target training set is also the testing set. In this paper, we address source-free domain adaptation in the more realistic inductive setting, where the target training and testing sets are mutually exclusive. We propose a new semi-supervised fine-tuning method named Dual Moving Average Pseudo-Labeling (DMAPL) for source-free inductive domain adaptation. We first split the unlabeled training set in the target domain into a pseudo-labeled confident subset and an unlabeled less-confident subset according to the prediction confidence scores from the pre-trained source model. Then we propose a soft-label moving-average updating strategy for the unlabeled subset based on a moving-average prototypical classifier, which gradually adapts the source model towards the target domain. Experiments show that our proposed method achieves state-of-the-art performance and outperforms previous methods by large margins.



### NAWQ-SR: A Hybrid-Precision NPU Engine for Efficient On-Device Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2212.09501v3
- **DOI**: 10.1109/TMC.2023.3255822
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.09501v3)
- **Published**: 2022-12-15 23:51:18+00:00
- **Updated**: 2023-03-14 11:48:37+00:00
- **Authors**: Stylianos I. Venieris, Mario Almeida, Royson Lee, Nicholas D. Lane
- **Comment**: Accepted for publication at the IEEE Transactions on Mobile Computing
  (TMC), 2023
- **Journal**: None
- **Summary**: In recent years, image and video delivery systems have begun integrating deep learning super-resolution (SR) approaches, leveraging their unprecedented visual enhancement capabilities while reducing reliance on networking conditions. Nevertheless, deploying these solutions on mobile devices still remains an active challenge as SR models are excessively demanding with respect to workload and memory footprint. Despite recent progress on on-device SR frameworks, existing systems either penalize visual quality, lead to excessive energy consumption or make inefficient use of the available resources. This work presents NAWQ-SR, a novel framework for the efficient on-device execution of SR models. Through a novel hybrid-precision quantization technique and a runtime neural image codec, NAWQ-SR exploits the multi-precision capabilities of modern mobile NPUs in order to minimize latency, while meeting user-specified quality constraints. Moreover, NAWQ-SR selectively adapts the arithmetic precision at run time to equip the SR DNN's layers with wider representational power, improving visual quality beyond what was previously possible on NPUs. Altogether, NAWQ-SR achieves an average speedup of 7.9x, 3x and 1.91x over the state-of-the-art on-device SR systems that use heterogeneous processors (MobiSR), CPU (SplitSR) and NPU (XLSR), respectively. Furthermore, NAWQ-SR delivers an average of 3.2x speedup and 0.39 dB higher PSNR over status-quo INT8 NPU designs, but most importantly mitigates the negative effects of quantization on visual quality, setting a new state-of-the-art in the attainable quality of NPU-based SR.



