# Arxiv Papers in cs.CV on 2022-12-28
### Large-scale single-photon imaging
- **Arxiv ID**: http://arxiv.org/abs/2212.13654v1
- **DOI**: None
- **Categories**: **physics.optics**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13654v1)
- **Published**: 2022-12-28 00:38:04+00:00
- **Updated**: 2022-12-28 00:38:04+00:00
- **Authors**: Liheng Bian, Haoze Song, Lintao Peng, Xuyang Chang, Xi Yang, Roarke Horstmeyer, Lin Ye, Tong Qin, Dezhi Zheng, Jun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Benefiting from its single-photon sensitivity, single-photon avalanche diode (SPAD) array has been widely applied in various fields such as fluorescence lifetime imaging and quantum computing. However, large-scale high-fidelity single-photon imaging remains a big challenge, due to the complex hardware manufacture craft and heavy noise disturbance of SPAD arrays. In this work, we introduce deep learning into SPAD, enabling super-resolution single-photon imaging over an order of magnitude, with significant enhancement of bit depth and imaging quality. We first studied the complex photon flow model of SPAD electronics to accurately characterize multiple physical noise sources, and collected a real SPAD image dataset (64 $\times$ 32 pixels, 90 scenes, 10 different bit depth, 3 different illumination flux, 2790 images in total) to calibrate noise model parameters. With this real-world physical noise model, we for the first time synthesized a large-scale realistic single-photon image dataset (image pairs of 5 different resolutions with maximum megapixels, 17250 scenes, 10 different bit depth, 3 different illumination flux, 2.6 million images in total) for subsequent network training. To tackle the severe super-resolution challenge of SPAD inputs with low bit depth, low resolution, and heavy noise, we further built a deep transformer network with a content-adaptive self-attention mechanism and gated fusion modules, which can dig global contextual features to remove multi-source noise and extract full-frequency details. We applied the technique on a series of experiments including macroscopic and microscopic imaging, microfluidic inspection, and Fourier ptychography. The experiments validate the technique's state-of-the-art super-resolution SPAD imaging performance, with more than 5 dB superiority on PSNR compared to the existing methods.



### NeMo: 3D Neural Motion Fields from Multiple Video Instances of the Same Action
- **Arxiv ID**: http://arxiv.org/abs/2212.13660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13660v1)
- **Published**: 2022-12-28 01:40:32+00:00
- **Updated**: 2022-12-28 01:40:32+00:00
- **Authors**: Kuan-Chieh Wang, Zhenzhen Weng, Maria Xenochristou, Joao Pedro Araujo, Jeffrey Gu, C. Karen Liu, Serena Yeung
- **Comment**: None
- **Journal**: None
- **Summary**: The task of reconstructing 3D human motion has wideranging applications. The gold standard Motion capture (MoCap) systems are accurate but inaccessible to the general public due to their cost, hardware and space constraints. In contrast, monocular human mesh recovery (HMR) methods are much more accessible than MoCap as they take single-view videos as inputs. Replacing the multi-view Mo- Cap systems with a monocular HMR method would break the current barriers to collecting accurate 3D motion thus making exciting applications like motion analysis and motiondriven animation accessible to the general public. However, performance of existing HMR methods degrade when the video contains challenging and dynamic motion that is not in existing MoCap datasets used for training. This reduces its appeal as dynamic motion is frequently the target in 3D motion recovery in the aforementioned applications. Our study aims to bridge the gap between monocular HMR and multi-view MoCap systems by leveraging information shared across multiple video instances of the same action. We introduce the Neural Motion (NeMo) field. It is optimized to represent the underlying 3D motions across a set of videos of the same action. Empirically, we show that NeMo can recover 3D motion in sports using videos from the Penn Action dataset, where NeMo outperforms existing HMR methods in terms of 2D keypoint detection. To further validate NeMo using 3D metrics, we collected a small MoCap dataset mimicking actions in Penn Action,and show that NeMo achieves better 3D reconstruction compared to various baselines.



### Learning When to Use Adaptive Adversarial Image Perturbations against Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2212.13667v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13667v2)
- **Published**: 2022-12-28 02:36:58+00:00
- **Updated**: 2023-03-15 20:30:35+00:00
- **Authors**: Hyung-Jin Yoon, Hamidreza Jafarnejadsani, Petros Voulgaris
- **Comment**: None
- **Journal**: None
- **Summary**: The deep neural network (DNN) models for object detection using camera images are widely adopted in autonomous vehicles. However, DNN models are shown to be susceptible to adversarial image perturbations. In the existing methods of generating the adversarial image perturbations, optimizations take each incoming image frame as the decision variable to generate an image perturbation. Therefore, given a new image, the typically computationally-expensive optimization needs to start over as there is no learning between the independent optimizations. Very few approaches have been developed for attacking online image streams while considering the underlying physical dynamics of autonomous vehicles, their mission, and the environment. We propose a multi-level stochastic optimization framework that monitors an attacker's capability of generating the adversarial perturbations. Based on this capability level, a binary decision attack/not attack is introduced to enhance the effectiveness of the attacker. We evaluate our proposed multi-level image attack framework using simulations for vision-guided autonomous vehicles and actual tests with a small indoor drone in an office environment. The results show our method's capability to generate the image attack in real-time while monitoring when the attacker is proficient given state estimates.



### Circular Accessible Depth: A Robust Traversability Representation for UGV Navigation
- **Arxiv ID**: http://arxiv.org/abs/2212.13676v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13676v1)
- **Published**: 2022-12-28 03:13:32+00:00
- **Updated**: 2022-12-28 03:13:32+00:00
- **Authors**: Shikuan Xie, Ran Song, Yuenan Zhao, Xueqin Huang, Yibin Li, Wei Zhang
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: In this paper, we present the Circular Accessible Depth (CAD), a robust traversability representation for an unmanned ground vehicle (UGV) to learn traversability in various scenarios containing irregular obstacles. To predict CAD, we propose a neural network, namely CADNet, with an attention-based multi-frame point cloud fusion module, Stability-Attention Module (SAM), to encode the spatial features from point clouds captured by LiDAR. CAD is designed based on the polar coordinate system and focuses on predicting the border of traversable area. Since it encodes the spatial information of the surrounding environment, which enables a semi-supervised learning for the CADNet, and thus desirably avoids annotating a large amount of data. Extensive experiments demonstrate that CAD outperforms baselines in terms of robustness and precision. We also implement our method on a real UGV and show that it performs well in real-world scenarios.



### Part-guided Relational Transformers for Fine-grained Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2212.13685v1
- **DOI**: 10.1109/TIP.2021.3126490
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13685v1)
- **Published**: 2022-12-28 03:45:56+00:00
- **Updated**: 2022-12-28 03:45:56+00:00
- **Authors**: Yifan Zhao, Jia Li, Xiaowu Chen, Yonghong Tian
- **Comment**: Published in IEEE TIP 2021
- **Journal**: None
- **Summary**: Fine-grained visual recognition is to classify objects with visually similar appearances into subcategories, which has made great progress with the development of deep CNNs. However, handling subtle differences between different subcategories still remains a challenge. In this paper, we propose to solve this issue in one unified framework from two aspects, i.e., constructing feature-level interrelationships, and capturing part-level discriminative features. This framework, namely PArt-guided Relational Transformers (PART), is proposed to learn the discriminative part features with an automatic part discovery module, and to explore the intrinsic correlations with a feature transformation module by adapting the Transformer models from the field of natural language processing. The part discovery module efficiently discovers the discriminative regions which are highly-corresponded to the gradient descent procedure. Then the second feature transformation module builds correlations within the global embedding and multiple part embedding, enhancing spatial interactions among semantic pixels. Moreover, our proposed approach does not rely on additional part branches in the inference time and reaches state-of-the-art performance on 3 widely-used fine-grained object recognition benchmarks. Experimental results and explainable visualizations demonstrate the effectiveness of our proposed approach. The code can be found at https://github.com/iCVTEAM/PART.



### Efficient Semantic Segmentation on Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/2212.13691v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.13691v2)
- **Published**: 2022-12-28 04:13:11+00:00
- **Updated**: 2023-01-15 05:27:02+00:00
- **Authors**: Farshad Safavi, Irfan Ali, Venkatesh Dasari, Guanqun Song, Ting Zhu, Maryam Rahnemoonfar
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation works on the computer vision algorithm for assigning each pixel of an image into a class. The task of semantic segmentation should be performed with both accuracy and efficiency. Most of the existing deep FCNs yield to heavy computations and these networks are very power hungry, unsuitable for real-time applications on portable devices. This project analyzes current semantic segmentation models to explore the feasibility of applying these models for emergency response during catastrophic events. We compare the performance of real-time semantic segmentation models with non-real-time counterparts constrained by aerial images under oppositional settings. Furthermore, we train several models on the Flood-Net dataset, containing UAV images captured after Hurricane Harvey, and benchmark their execution on special classes such as flooded buildings vs. non-flooded buildings or flooded roads vs. non-flooded roads. In this project, we developed a real-time UNet based model and deployed that network on Jetson AGX Xavier module.



### Parsing Objects at a Finer Granularity: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2212.13693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13693v1)
- **Published**: 2022-12-28 04:20:10+00:00
- **Updated**: 2022-12-28 04:20:10+00:00
- **Authors**: Yifan Zhao, Jia Li, Yonghong Tian
- **Comment**: Survey for fine-grained part segmentation and object recognition;
  Accepted by Machine Intelligence Research (MIR)
- **Journal**: None
- **Summary**: Fine-grained visual parsing, including fine-grained part segmentation and fine-grained object recognition, has attracted considerable critical attention due to its importance in many real-world applications, e.g., agriculture, remote sensing, and space technologies. Predominant research efforts tackle these fine-grained sub-tasks following different paradigms, while the inherent relations between these tasks are neglected. Moreover, given most of the research remains fragmented, we conduct an in-depth study of the advanced work from a new perspective of learning the part relationship. In this perspective, we first consolidate recent research and benchmark syntheses with new taxonomies. Based on this consolidation, we revisit the universal challenges in fine-grained part segmentation and recognition tasks and propose new solutions by part relationship learning for these important challenges. Furthermore, we conclude several promising lines of research in fine-grained visual parsing for future research.



### Shape-Aware Fine-Grained Classification of Erythroid Cells
- **Arxiv ID**: http://arxiv.org/abs/2212.13695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13695v1)
- **Published**: 2022-12-28 04:43:25+00:00
- **Updated**: 2022-12-28 04:43:25+00:00
- **Authors**: Ye Wang, Rui Ma, Xiaoqing Ma, Honghua Cui, Yubin Xiao, Xuan Wu, You Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained classification and counting of bone marrow erythroid cells are vital for evaluating the health status and formulating therapeutic schedules for leukemia or hematopathy. Due to the subtle visual differences between different types of erythroid cells, it is challenging to apply existing image-based deep learning models for fine-grained erythroid cell classification. Moreover, there is no large open-source datasets on erythroid cells to support the model training. In this paper, we introduce BMEC (Bone Morrow Erythroid Cells), the first large fine-grained image dataset of erythroid cells, to facilitate more deep learning research on erythroid cells. BMEC contains 5,666 images of individual erythroid cells, each of which is extracted from the bone marrow erythroid cell smears and professionally annotated to one of the four types of erythroid cells. To distinguish the erythroid cells, one key indicator is the cell shape which is closely related to the cell growth and maturation. Therefore, we design a novel shape-aware image classification network for fine-grained erythroid cell classification. The shape feature is extracted from the shape mask image and aggregated to the raw image feature with a shape attention module. With the shape-attended image feature, our network achieved superior classification performance (81.12\% top-1 accuracy) on the BMEC dataset comparing to the baseline methods. Ablation studies also demonstrate the effectiveness of incorporating the shape information for the fine-grained cell classification. To further verify the generalizability of our method, we tested our network on two additional public white blood cells (WBC) datasets and the results show our shape-aware method can generally outperform recent state-of-the-art works on classifying the WBC. The code and BMEC dataset can be found on https://github.com/wangye8899/BMEC.



### Detection of Active Emergency Vehicles using Per-Frame CNNs and Output Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2212.13696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13696v1)
- **Published**: 2022-12-28 04:45:51+00:00
- **Updated**: 2022-12-28 04:45:51+00:00
- **Authors**: Meng Fan, Craig Bidstrup, Zhaoen Su, Jason Owens, Gary Yang, Nemanja Djuric
- **Comment**: None
- **Journal**: None
- **Summary**: While inferring common actor states (such as position or velocity) is an important and well-explored task of the perception system aboard a self-driving vehicle (SDV), it may not always provide sufficient information to the SDV. This is especially true in the case of active emergency vehicles (EVs), where light-based signals also need to be captured to provide a full context. We consider this problem and propose a sequential methodology for the detection of active EVs, using an off-the-shelf CNN model operating at a frame level and a downstream smoother that accounts for the temporal aspect of flashing EV lights. We also explore model improvements through data augmentation and training with additional hard samples.



### Building Segmentation on Satellite Images and Performance of Post-Processing Methods
- **Arxiv ID**: http://arxiv.org/abs/2212.13712v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13712v1)
- **Published**: 2022-12-28 06:16:39+00:00
- **Updated**: 2022-12-28 06:16:39+00:00
- **Authors**: Metehan Yalçın, Ahmet Alp Kindiroglu, Furkan Burak Bağcı, Ufuk Uyan, Mahiye Uluyağmur Öztürk
- **Comment**: in Turkish language
- **Journal**: None
- **Summary**: Researchers are doing intensive work on satellite images due to the information it contains with the development of computer vision algorithms and the ease of accessibility to satellite images. Building segmentation of satellite images can be used for many potential applications such as city, agricultural, and communication network planning. However, since no dataset exists for every region, the model trained in a region must gain generality. In this study, we trained several models in China and post-processing work was done on the best model selected among them. These models are evaluated in the Chicago region of the INRIA dataset. As can be seen from the results, although state-of-art results in this area have not been achieved, the results are promising. We aim to present our initial experimental results of a building segmentation from satellite images in this study.



### MyI-Net: Fully Automatic Detection and Quantification of Myocardial Infarction from Cardiovascular MRI Images
- **Arxiv ID**: http://arxiv.org/abs/2212.13715v1
- **DOI**: 10.3390/e25030431
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68T07, 68T05
- **Links**: [PDF](http://arxiv.org/pdf/2212.13715v1)
- **Published**: 2022-12-28 06:34:38+00:00
- **Updated**: 2022-12-28 06:34:38+00:00
- **Authors**: Shuihua Wang, Ahmed M. S. E. K Abdelaty, Kelly Parke, J Ranjit Arnold, Gerry P McCann, Ivan Y Tyukin
- **Comment**: None
- **Journal**: None
- **Summary**: A "heart attack" or myocardial infarction (MI), occurs when an artery supplying blood to the heart is abruptly occluded. The "gold standard" method for imaging MI is Cardiovascular Magnetic Resonance Imaging (MRI), with intravenously administered gadolinium-based contrast (late gadolinium enhancement). However, no "gold standard" fully automated method for the quantification of MI exists. In this work, we propose an end-to-end fully automatic system (MyI-Net) for the detection and quantification of MI in MRI images. This has the potential to reduce the uncertainty due to the technical variability across labs and inherent problems of the data and labels. Our system consists of four processing stages designed to maintain the flow of information across scales. First, features from raw MRI images are generated using feature extractors built on ResNet and MoblieNet architectures. This is followed by the Atrous Spatial Pyramid Pooling (ASPP) to produce spatial information at different scales to preserve more image context. High-level features from ASPP and initial low-level features are concatenated at the third stage and then passed to the fourth stage where spatial information is recovered via up-sampling to produce final image segmentation output into: i) background, ii) heart muscle, iii) blood and iv) scar areas. New models were compared with state-of-art models and manual quantification. Our models showed favorable performance in global segmentation and scar tissue detection relative to state-of-the-art work, including a four-fold better performance in matching scar pixels to contours produced by clinicians.



### VertMatch: A Semi-supervised Framework for Vertebral Structure Detection in 3D Ultrasound Volume
- **Arxiv ID**: http://arxiv.org/abs/2212.14747v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.14747v1)
- **Published**: 2022-12-28 06:50:35+00:00
- **Updated**: 2022-12-28 06:50:35+00:00
- **Authors**: Hongye Zeng, kang Zhou, Songhan Ge, Yuchong Gao, Jianhao Zhao, Shenghua Gao, Rui Zheng
- **Comment**: 15 pages, 8 figures
- **Journal**: None
- **Summary**: Three-dimensional (3D) ultrasound imaging technique has been applied for scoliosis assessment, but current assessment method only uses coronal projection image and cannot illustrate the 3D deformity and vertebra rotation. The vertebra detection is essential to reveal 3D spine information, but the detection task is challenging due to complex data and limited annotations. We propose VertMatch, a two-step framework to detect vertebral structures in 3D ultrasound volume by utilizing unlabeled data in semi-supervised manner. The first step is to detect the possible positions of structures on transverse slice globally, and then the local patches are cropped based on detected positions. The second step is to distinguish whether the patches contain real vertebral structures and screen the predicted positions from the first step. VertMatch develops three novel components for semi-supervised learning: for position detection in the first step, (1) anatomical prior is used to screen pseudo labels generated from confidence threshold method; (2) multi-slice consistency is used to utilize more unlabeled data by inputting multiple adjacent slices; (3) for patch identification in the second step, the categories are rebalanced in each batch to solve imbalance problem. Experimental results demonstrate that VertMatch can detect vertebra accurately in ultrasound volume and outperforms state-of-the-art methods. VertMatch is also validated in clinical application on forty ultrasound scans, and it can be a promising approach for 3D assessment of scoliosis.



### A Clustering-guided Contrastive Fusion for Multi-view Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2212.13726v4
- **DOI**: 10.1109/TCSVT.2023.3300319
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13726v4)
- **Published**: 2022-12-28 07:21:05+00:00
- **Updated**: 2023-08-04 13:20:43+00:00
- **Authors**: Guanzhou Ke, Guoqing Chao, Xiaoli Wang, Chenyang Xu, Yongqi Zhu, Yang Yu
- **Comment**: 13 pages, 9 figures
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology,
  2023
- **Summary**: The past two decades have seen increasingly rapid advances in the field of multi-view representation learning due to it extracting useful information from diverse domains to facilitate the development of multi-view applications. However, the community faces two challenges: i) how to learn robust representations from a large amount of unlabeled data to against noise or incomplete views setting, and ii) how to balance view consistency and complementary for various downstream tasks. To this end, we utilize a deep fusion network to fuse view-specific representations into the view-common representation, extracting high-level semantics for obtaining robust representation. In addition, we employ a clustering task to guide the fusion network to prevent it from leading to trivial solutions. For balancing consistency and complementary, then, we design an asymmetrical contrastive strategy that aligns the view-common representation and each view-specific representation. These modules are incorporated into a unified method known as CLustering-guided cOntrastiVE fusioN (CLOVEN). We quantitatively and qualitatively evaluate the proposed method on five datasets, demonstrating that CLOVEN outperforms 11 competitive multi-view learning methods in clustering and classification. In the incomplete view scenario, our proposed method resists noise interference better than those of our competitors. Furthermore, the visualization analysis shows that CLOVEN can preserve the intrinsic structure of view-specific representation while also improving the compactness of view-commom representation. Our source code will be available soon at https://github.com/guanzhou-ke/cloven.



### Single-Image Super-Resolution Reconstruction based on the Differences of Neighboring Pixels
- **Arxiv ID**: http://arxiv.org/abs/2212.13730v1
- **DOI**: 10.1007/978-3-030-92307-5_61
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13730v1)
- **Published**: 2022-12-28 07:30:07+00:00
- **Updated**: 2022-12-28 07:30:07+00:00
- **Authors**: Huipeng Zheng, Lukman Hakim, Takio Kurita, Junichi Miyao
- **Comment**: None
- **Journal**: None
- **Summary**: The deep learning technique was used to increase the performance of single image super-resolution (SISR). However, most existing CNN-based SISR approaches primarily focus on establishing deeper or larger networks to extract more significant high-level features. Usually, the pixel-level loss between the target high-resolution image and the estimated image is used, but the neighbor relations between pixels in the image are seldom used. On the other hand, according to observations, a pixel's neighbor relationship contains rich information about the spatial structure, local context, and structural knowledge. Based on this fact, in this paper, we utilize pixel's neighbor relationships in a different perspective, and we propose the differences of neighboring pixels to regularize the CNN by constructing a graph from the estimated image and the ground-truth image. The proposed method outperforms the state-of-the-art methods in terms of quantitative and qualitative evaluation of the benchmark datasets.   Keywords: Super-resolution, Convolutional Neural Networks, Deep Learning



### Pixel Relationships-based Regularizer for Retinal Vessel Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2212.13731v1
- **DOI**: 10.23919/IPEC-Himeji2022-ECCE53331.2022.9806976
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.13731v1)
- **Published**: 2022-12-28 07:35:20+00:00
- **Updated**: 2022-12-28 07:35:20+00:00
- **Authors**: Lukman Hakim, Takio Kurita
- **Comment**: None
- **Journal**: None
- **Summary**: The task of image segmentation is to classify each pixel in the image based on the appropriate label. Various deep learning approaches have been proposed for image segmentation that offers high accuracy and deep architecture. However, the deep learning technique uses a pixel-wise loss function for the training process. Using pixel-wise loss neglected the pixel neighbor relationships in the network learning process. The neighboring relationship of the pixels is essential information in the image. Utilizing neighboring pixel information provides an advantage over using only pixel-to-pixel information. This study presents regularizers to give the pixel neighbor relationship information to the learning process. The regularizers are constructed by the graph theory approach and topology approach: By graph theory approach, graph Laplacian is used to utilize the smoothness of segmented images based on output images and ground-truth images. By topology approach, Euler characteristic is used to identify and minimize the number of isolated objects on segmented images. Experiments show that our scheme successfully captures pixel neighbor relations and improves the performance of the convolutional neural network better than the baseline without a regularization term.



### TempCLR: Temporal Alignment Representation with Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2212.13738v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2212.13738v2)
- **Published**: 2022-12-28 08:10:31+00:00
- **Updated**: 2023-03-30 01:42:53+00:00
- **Authors**: Yuncong Yang, Jiawei Ma, Shiyuan Huang, Long Chen, Xudong Lin, Guangxing Han, Shih-Fu Chang
- **Comment**: ICLR 2023 Camera Ready. Code Link:
  https://github.com/yyuncong/TempCLR
- **Journal**: None
- **Summary**: Video representation learning has been successful in video-text pre-training for zero-shot transfer, where each sentence is trained to be close to the paired video clips in a common feature space. For long videos, given a paragraph of description where the sentences describe different segments of the video, by matching all sentence-clip pairs, the paragraph and the full video are aligned implicitly. However, such unit-level comparison may ignore global temporal context, which inevitably limits the generalization ability. In this paper, we propose a contrastive learning framework TempCLR to compare the full video and the paragraph explicitly. As the video/paragraph is formulated as a sequence of clips/sentences, under the constraint of their temporal order, we use dynamic time warping to compute the minimum cumulative cost over sentence-clip pairs as the sequence-level distance. To explore the temporal dynamics, we break the consistency of temporal succession by shuffling video clips w.r.t. temporal granularity. Then, we obtain the representations for clips/sentences, which perceive the temporal information and thus facilitate the sequence alignment. In addition to pre-training on the video and paragraph, our approach can also generalize on the matching between video instances. We evaluate our approach on video retrieval, action step localization, and few-shot action recognition, and achieve consistent performance gain over all three tasks. Detailed ablation studies are provided to justify the approach design.



### Representation Separation for Semantic Segmentation with Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2212.13764v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.13764v1)
- **Published**: 2022-12-28 09:54:52+00:00
- **Updated**: 2022-12-28 09:54:52+00:00
- **Authors**: Yuanduo Hong, Huihui Pan, Weichao Sun, Xinghu Yu, Huijun Gao
- **Comment**: 17 pages, 13 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
- **Journal**: None
- **Summary**: Vision transformers (ViTs) encoding an image as a sequence of patches bring new paradigms for semantic segmentation.We present an efficient framework of representation separation in local-patch level and global-region level for semantic segmentation with ViTs. It is targeted for the peculiar over-smoothness of ViTs in semantic segmentation, and therefore differs from current popular paradigms of context modeling and most existing related methods reinforcing the advantage of attention. We first deliver the decoupled two-pathway network in which another pathway enhances and passes down local-patch discrepancy complementary to global representations of transformers. We then propose the spatially adaptive separation module to obtain more separate deep representations and the discriminative cross-attention which yields more discriminative region representations through novel auxiliary supervisions. The proposed methods achieve some impressive results: 1) incorporated with large-scale plain ViTs, our methods achieve new state-of-the-art performances on five widely used benchmarks; 2) using masked pre-trained plain ViTs, we achieve 68.9% mIoU on Pascal Context, setting a new record; 3) pyramid ViTs integrated with the decoupled two-pathway network even surpass the well-designed high-resolution ViTs on Cityscapes; 4) the improved representations by our framework have favorable transferability in images with natural corruptions. The codes will be released publicly.



### Comparative Study of Parameter Selection for Enhanced Edge Inference for a Multi-Output Regression model for Head Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2302.00592v1
- **DOI**: 10.1109/TENCON55691.2022.9977637
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2302.00592v1)
- **Published**: 2022-12-28 09:58:04+00:00
- **Updated**: 2022-12-28 09:58:04+00:00
- **Authors**: Asiri Lindamulage, Nuwan Kodagoda, Shyam Reyal, Pradeepa Samarasinghe, Pratheepan Yogarajah
- **Comment**: Conference:- in TENCON 2022 - 2022 IEEE Region 10 Conference (TENCON)
- **Journal**: TENCON 2022 - 2022 IEEE Region 10 Conference (TENCON), Nov. 2022
- **Summary**: Magnitude-based pruning is a technique used to optimise deep learning models for edge inference. We have achieved over 75% model size reduction with a higher accuracy than the original multi-output regression model for head-pose estimation.



### OVO: One-shot Vision Transformer Search with Online distillation
- **Arxiv ID**: http://arxiv.org/abs/2212.13766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13766v1)
- **Published**: 2022-12-28 10:08:55+00:00
- **Updated**: 2022-12-28 10:08:55+00:00
- **Authors**: Zimian Wei, Hengyue Pan, Xin Niu, Dongsheng Li
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2107.00651 by
  other authors
- **Journal**: None
- **Summary**: Pure transformers have shown great potential for vision tasks recently. However, their accuracy in small or medium datasets is not satisfactory. Although some existing methods introduce a CNN as a teacher to guide the training process by distillation, the gap between teacher and student networks would lead to sub-optimal performance. In this work, we propose a new One-shot Vision transformer search framework with Online distillation, namely OVO. OVO samples sub-nets for both teacher and student networks for better distillation results. Benefiting from the online distillation, thousands of subnets in the supernet are well-trained without extra finetuning or retraining. In experiments, OVO-Ti achieves 73.32% top-1 accuracy on ImageNet and 75.2% on CIFAR-100, respectively.



### Exploring Vision Transformers as Diffusion Learners
- **Arxiv ID**: http://arxiv.org/abs/2212.13771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13771v1)
- **Published**: 2022-12-28 10:32:59+00:00
- **Updated**: 2022-12-28 10:32:59+00:00
- **Authors**: He Cao, Jianan Wang, Tianhe Ren, Xianbiao Qi, Yihao Chen, Yuan Yao, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Score-based diffusion models have captured widespread attention and funded fast progress of recent vision generative tasks. In this paper, we focus on diffusion model backbone which has been much neglected before. We systematically explore vision Transformers as diffusion learners for various generative tasks. With our improvements the performance of vanilla ViT-based backbone (IU-ViT) is boosted to be on par with traditional U-Net-based methods. We further provide a hypothesis on the implication of disentangling the generative backbone as an encoder-decoder structure and show proof-of-concept experiments verifying the effectiveness of a stronger encoder for generative tasks with ASymmetriC ENcoder Decoder (ASCEND). Our improvements achieve competitive results on CIFAR-10, CelebA, LSUN, CUB Bird and large-resolution text-to-image tasks. To the best of our knowledge, we are the first to successfully train a single diffusion model on text-to-image task beyond 64x64 resolution. We hope this will motivate people to rethink the modeling choices and the training pipelines for diffusion-based generative models.



### SynCLay: Interactive Synthesis of Histology Images from Bespoke Cellular Layouts
- **Arxiv ID**: http://arxiv.org/abs/2212.13780v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.13780v1)
- **Published**: 2022-12-28 11:07:00+00:00
- **Updated**: 2022-12-28 11:07:00+00:00
- **Authors**: Srijay Deshpande, Muhammad Dawood, Fayyaz Minhas, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Automated synthesis of histology images has several potential applications in computational pathology. However, no existing method can generate realistic tissue images with a bespoke cellular layout or user-defined histology parameters. In this work, we propose a novel framework called SynCLay (Synthesis from Cellular Layouts) that can construct realistic and high-quality histology images from user-defined cellular layouts along with annotated cellular boundaries. Tissue image generation based on bespoke cellular layouts through the proposed framework allows users to generate different histological patterns from arbitrary topological arrangement of different types of cells. SynCLay generated synthetic images can be helpful in studying the role of different types of cells present in the tumor microenvironmet. Additionally, they can assist in balancing the distribution of cellular counts in tissue images for designing accurate cellular composition predictors by minimizing the effects of data imbalance. We train SynCLay in an adversarial manner and integrate a nuclear segmentation and classification model in its training to refine nuclear structures and generate nuclear masks in conjunction with synthetic images. During inference, we combine the model with another parametric model for generating colon images and associated cellular counts as annotations given the grade of differentiation and cell densities of different cells. We assess the generated images quantitatively and report on feedback from trained pathologists who assigned realism scores to a set of images generated by the framework. The average realism score across all pathologists for synthetic images was as high as that for the real images. We also show that augmenting limited real data with the synthetic data generated by our framework can significantly boost prediction performance of the cellular composition prediction task.



### Explainable and Lightweight Model for COVID-19 Detection Using Chest Radiology Images
- **Arxiv ID**: http://arxiv.org/abs/2212.13788v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.13788v1)
- **Published**: 2022-12-28 11:48:29+00:00
- **Updated**: 2022-12-28 11:48:29+00:00
- **Authors**: Suba S, Nita Parekh
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning (DL) analysis of Chest X-ray (CXR) and Computed tomography (CT) images has garnered a lot of attention in recent times due to the COVID-19 pandemic. Convolutional Neural Networks (CNNs) are well suited for the image analysis tasks when trained on humongous amounts of data. Applications developed for medical image analysis require high sensitivity and precision compared to any other fields. Most of the tools proposed for detection of COVID-19 claims to have high sensitivity and recalls but have failed to generalize and perform when tested on unseen datasets. This encouraged us to develop a CNN model, analyze and understand the performance of it by visualizing the predictions of the model using class activation maps generated using (Gradient-weighted Class Activation Mapping) Grad-CAM technique. This study provides a detailed discussion of the success and failure of the proposed model at an image level. Performance of the model is compared with state-of-the-art DL models and shown to be comparable. The data and code used are available at https://github.com/aleesuss/c19.



### StyleID: Identity Disentanglement for Anonymizing Faces
- **Arxiv ID**: http://arxiv.org/abs/2212.13791v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG, cs.MM, 68T45, 68T07, 68P27, I.2; I.4; K.4
- **Links**: [PDF](http://arxiv.org/pdf/2212.13791v1)
- **Published**: 2022-12-28 12:04:24+00:00
- **Updated**: 2022-12-28 12:04:24+00:00
- **Authors**: Minh-Ha Le, Niklas Carlsson
- **Comment**: Accepted to Privacy Enhancing Technologies Symposium (PETS), July
  2023. Will appear in Proceedings on Privacy Enhancing Technologies (PoPETs),
  volume 1, 2023. 15 pages including references and appendix, 16 figures, 5
  tables
- **Journal**: None
- **Summary**: Privacy of machine learning models is one of the remaining challenges that hinder the broad adoption of Artificial Intelligent (AI). This paper considers this problem in the context of image datasets containing faces. Anonymization of such datasets is becoming increasingly important due to their central role in the training of autonomous cars, for example, and the vast amount of data generated by surveillance systems. While most prior work de-identifies facial images by modifying identity features in pixel space, we instead project the image onto the latent space of a Generative Adversarial Network (GAN) model, find the features that provide the biggest identity disentanglement, and then manipulate these features in latent space, pixel space, or both. The main contribution of the paper is the design of a feature-preserving anonymization framework, StyleID, which protects the individuals' identity, while preserving as many characteristics of the original faces in the image dataset as possible. As part of the contribution, we present a novel disentanglement metric, three complementing disentanglement methods, and new insights into identity disentanglement. StyleID provides tunable privacy, has low computational complexity, and is shown to outperform current state-of-the-art solutions.



### Periocular Biometrics: A Modality for Unconstrained Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2212.13792v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13792v2)
- **Published**: 2022-12-28 12:08:27+00:00
- **Updated**: 2023-07-20 12:37:06+00:00
- **Authors**: Fernando Alonso-Fernandez, Josef Bigun, Julian Fierrez, Naser Damer, Hugo Proença, Arun Ross
- **Comment**: Published at IEEE Computer journal
- **Journal**: None
- **Summary**: Periocular refers to the externally visible region of the face that surrounds the eye socket. This feature-rich area can provide accurate identification in unconstrained or uncooperative scenarios, where the iris or face modalities may not offer sufficient biometric cues due to factors such as partial occlusion or high subject-to-camera distance. The COVID-19 pandemic has further highlighted its importance, as the ocular region remained the only visible facial area even in controlled settings due to the widespread use of masks. This paper discusses the state of the art in periocular biometrics, presenting an overall framework encompassing its most significant research aspects, which include: (a) ocular definition, acquisition, and detection; (b) identity recognition, including combination with other modalities and use of various spectra; and (c) ocular soft-biometric analysis. Finally, we conclude by addressing current challenges and proposing future directions.



### Swin MAE: Masked Autoencoders for Small Datasets
- **Arxiv ID**: http://arxiv.org/abs/2212.13805v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.13805v2)
- **Published**: 2022-12-28 12:53:44+00:00
- **Updated**: 2023-01-05 10:07:41+00:00
- **Authors**: Zi'an Xu, Yin Dai, Fayu Liu, Weibing Chen, Yue Liu, Lifu Shi, Sheng Liu, Yuhang Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: The development of deep learning models in medical image analysis is majorly limited by the lack of large-sized and well-annotated datasets. Unsupervised learning does not require labels and is more suitable for solving medical image analysis problems. However, most of the current unsupervised learning methods need to be applied to large datasets. To make unsupervised learning applicable to small datasets, we proposed Swin MAE, which is a masked autoencoder with Swin Transformer as its backbone. Even on a dataset of only a few thousand medical images and without using any pre-trained models, Swin MAE is still able to learn useful semantic features purely from images. It can equal or even slightly outperform the supervised model obtained by Swin Transformer trained on ImageNet in terms of the transfer learning results of downstream tasks. The code is publicly available at https://github.com/Zian-Xu/Swin-MAE.



### Sparse Coding in a Dual Memory System for Lifelong Learning
- **Arxiv ID**: http://arxiv.org/abs/2301.05058v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.05058v1)
- **Published**: 2022-12-28 12:56:15+00:00
- **Updated**: 2022-12-28 12:56:15+00:00
- **Authors**: Fahad Sarfraz, Elahe Arani, Bahram Zonooz
- **Comment**: Camera ready version - "Thirty-Seventh AAAI Conference on Artificial
  Intelligence" (AAAI-2023)
- **Journal**: None
- **Summary**: Efficient continual learning in humans is enabled by a rich set of neurophysiological mechanisms and interactions between multiple memory systems. The brain efficiently encodes information in non-overlapping sparse codes, which facilitates the learning of new associations faster with controlled interference with previous associations. To mimic sparse coding in DNNs, we enforce activation sparsity along with a dropout mechanism which encourages the model to activate similar units for semantically similar inputs and have less overlap with activation patterns of semantically dissimilar inputs. This provides us with an efficient mechanism for balancing the reusability and interference of features, depending on the similarity of classes across tasks. Furthermore, we employ sparse coding in a multiple-memory replay mechanism. Our method maintains an additional long-term semantic memory that aggregates and consolidates information encoded in the synaptic weights of the working model. Our extensive evaluation and characteristics analysis show that equipped with these biologically inspired mechanisms, the model can further mitigate forgetting.



### All's well that FID's well? Result quality and metric scores in GAN models for lip-sychronization tasks
- **Arxiv ID**: http://arxiv.org/abs/2212.13810v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2212.13810v1)
- **Published**: 2022-12-28 13:06:30+00:00
- **Updated**: 2022-12-28 13:06:30+00:00
- **Authors**: Carina Geldhauser, Johan Liljegren, Pontus Nordqvist
- **Comment**: None
- **Journal**: None
- **Summary**: We test the performance of GAN models for lip-synchronization. For this, we reimplement LipGAN in Pytorch, train it on the dataset GRID and compare it to our own variation, L1WGAN-GP, adapted to the LipGAN architecture and also trained on GRID.



### Multi-Realism Image Compression with a Conditional Generator
- **Arxiv ID**: http://arxiv.org/abs/2212.13824v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13824v2)
- **Published**: 2022-12-28 13:56:54+00:00
- **Updated**: 2023-03-30 11:26:56+00:00
- **Authors**: Eirikur Agustsson, David Minnen, George Toderici, Fabian Mentzer
- **Comment**: CVPR'23 Camera Ready
- **Journal**: None
- **Summary**: By optimizing the rate-distortion-realism trade-off, generative compression approaches produce detailed, realistic images, even at low bit rates, instead of the blurry reconstructions produced by rate-distortion optimized models. However, previous methods do not explicitly control how much detail is synthesized, which results in a common criticism of these methods: users might be worried that a misleading reconstruction far from the input image is generated. In this work, we alleviate these concerns by training a decoder that can bridge the two regimes and navigate the distortion-realism trade-off. From a single compressed representation, the receiver can decide to either reconstruct a low mean squared error reconstruction that is close to the input, a realistic reconstruction with high perceptual quality, or anything in between. With our method, we set a new state-of-the-art in distortion-realism, pushing the frontier of achievable distortion-realism pairs, i.e., our method achieves better distortions at high realism and better realism at low distortion than ever before.



### Escaping Saddle Points for Effective Generalization on Class-Imbalanced Data
- **Arxiv ID**: http://arxiv.org/abs/2212.13827v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13827v1)
- **Published**: 2022-12-28 14:00:44+00:00
- **Updated**: 2022-12-28 14:00:44+00:00
- **Authors**: Harsh Rangwani, Sumukh K Aithal, Mayank Mishra, R. Venkatesh Babu
- **Comment**: NeurIPS 2022. Code: https://github.com/val-iisc/Saddle-LongTail
- **Journal**: None
- **Summary**: Real-world datasets exhibit imbalances of varying types and degrees. Several techniques based on re-weighting and margin adjustment of loss are often used to enhance the performance of neural networks, particularly on minority classes. In this work, we analyze the class-imbalanced learning problem by examining the loss landscape of neural networks trained with re-weighting and margin-based techniques. Specifically, we examine the spectral density of Hessian of class-wise loss, through which we observe that the network weights converge to a saddle point in the loss landscapes of minority classes. Following this observation, we also find that optimization methods designed to escape from saddle points can be effectively used to improve generalization on minority classes. We further theoretically and empirically demonstrate that Sharpness-Aware Minimization (SAM), a recent technique that encourages convergence to a flat minima, can be effectively used to escape saddle points for minority classes. Using SAM results in a 6.2\% increase in accuracy on the minority classes over the state-of-the-art Vector Scaling Loss, leading to an overall average increase of 4\% across imbalanced datasets. The code is available at: https://github.com/val-iisc/Saddle-LongTail.



### Evaluating Generalizability of Deep Learning Models Using Indian-COVID-19 CT Dataset
- **Arxiv ID**: http://arxiv.org/abs/2212.13929v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13929v1)
- **Published**: 2022-12-28 16:23:18+00:00
- **Updated**: 2022-12-28 16:23:18+00:00
- **Authors**: Suba S, Nita Parekh, Ramesh Loganathan, Vikram Pudi, Chinnababu Sunkavalli
- **Comment**: None
- **Journal**: None
- **Summary**: Computer tomography (CT) have been routinely used for the diagnosis of lung diseases and recently, during the pandemic, for detecting the infectivity and severity of COVID-19 disease. One of the major concerns in using ma-chine learning (ML) approaches for automatic processing of CT scan images in clinical setting is that these methods are trained on limited and biased sub-sets of publicly available COVID-19 data. This has raised concerns regarding the generalizability of these models on external datasets, not seen by the model during training. To address some of these issues, in this work CT scan images from confirmed COVID-19 data obtained from one of the largest public repositories, COVIDx CT 2A were used for training and internal vali-dation of machine learning models. For the external validation we generated Indian-COVID-19 CT dataset, an open-source repository containing 3D CT volumes and 12096 chest CT images from 288 COVID-19 patients from In-dia. Comparative performance evaluation of four state-of-the-art machine learning models, viz., a lightweight convolutional neural network (CNN), and three other CNN based deep learning (DL) models such as VGG-16, ResNet-50 and Inception-v3 in classifying CT images into three classes, viz., normal, non-covid pneumonia, and COVID-19 is carried out on these two datasets. Our analysis showed that the performance of all the models is comparable on the hold-out COVIDx CT 2A test set with 90% - 99% accuracies (96% for CNN), while on the external Indian-COVID-19 CT dataset a drop in the performance is observed for all the models (8% - 19%). The traditional ma-chine learning model, CNN performed the best on the external dataset (accu-racy 88%) in comparison to the deep learning models, indicating that a light-weight CNN is better generalizable on unseen data. The data and code are made available at https://github.com/aleesuss/c19.



### A Segmentation Method for fluorescence images without a machine learning approach
- **Arxiv ID**: http://arxiv.org/abs/2212.13945v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2212.13945v1)
- **Published**: 2022-12-28 16:47:05+00:00
- **Updated**: 2022-12-28 16:47:05+00:00
- **Authors**: Giuseppe Giacopelli, Michele Migliore, Domenico Tegolo
- **Comment**: 25 pages
- **Journal**: None
- **Summary**: Background: Image analysis applications in digital pathology include various methods for segmenting regions of interest. Their identification is one of the most complex steps, and therefore of great interest for the study of robust methods that do not necessarily rely on a machine learning (ML) approach. Method: A fully automatic and optimized segmentation process for different datasets is a prerequisite for classifying and diagnosing Indirect ImmunoFluorescence (IIF) raw data. This study describes a deterministic computational neuroscience approach for identifying cells and nuclei. It is far from the conventional neural network approach, but it is equivalent to their quantitative and qualitative performance, and it is also solid to adversative noise. The method is robust, based on formally correct functions, and does not suffer from tuning on specific data sets. Results: This work demonstrates the robustness of the method against the variability of parameters, such as image size, mode, and signal-to-noise ratio. We validated the method on two datasets (Neuroblastoma and NucleusSegData) using images annotated by independent medical doctors. Conclusions: The definition of deterministic and formally correct methods, from a functional to a structural point of view, guarantees the achievement of optimized and functionally correct results. The excellent performance of our deterministic method (NeuronalAlg) to segment cells and nuclei from fluorescence images was measured with quantitative indicators and compared with those achieved by three published ML approaches.



### CT-LungNet: A Deep Learning Framework for Precise Lung Tissue Segmentation in 3D Thoracic CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2212.13971v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2212.13971v4)
- **Published**: 2022-12-28 17:37:08+00:00
- **Updated**: 2023-04-26 14:57:11+00:00
- **Authors**: Niloufar Delfan, Hamid Abrishami Moghaddam, Mohammadreza Modaresi, Kimia Afshari, Kasra Nezamabadi, Neda Pak, Omid Ghaemi, Mohamad Forouzanfar
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation of lung tissue in computed tomography (CT) images is a precursor to most pulmonary image analysis applications. Semantic segmentation methods using deep learning have exhibited top-tier performance in recent years, however designing accurate and robust segmentation models for lung tissue is challenging due to the variations in shape, size, and orientation. Additionally, medical image artifacts and noise can affect lung tissue segmentation and degrade the accuracy of downstream analysis. The practicality of current deep learning methods for lung tissue segmentation is limited as they require significant computational resources and may not be easily deployable in clinical settings. This paper presents a fully automatic method that identifies the lungs in three-dimensional (3D) pulmonary CT images using deep networks and transfer learning. We introduce (1) a novel 2.5-dimensional image representation from consecutive CT slices that succinctly represents volumetric information and (2) a U-Net architecture equipped with pre-trained InceptionV3 blocks to segment 3D CT scans while maintaining the number of learnable parameters as low as possible. Our method was quantitatively assessed using one public dataset, LUNA16, for training and testing and two public datasets, namely, VESSEL12 and CRPF, only for testing. Due to the low number of learnable parameters, our method achieved high generalizability to the unseen VESSEL12 and CRPF datasets while obtaining superior performance over Luna16 compared to existing methods (Dice coefficients of 99.7, 99.1, and 98.8 over LUNA16, VESSEL12, and CRPF datasets, respectively). We made our method publicly accessible via a graphical user interface at medvispy.ee.kntu.ac.ir.



### Adversarial Virtual Exemplar Learning for Label-Frugal Satellite Image Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2212.13974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13974v1)
- **Published**: 2022-12-28 17:46:20+00:00
- **Updated**: 2022-12-28 17:46:20+00:00
- **Authors**: Hichem Sahbi, Sebastien Deschamps
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2203.11559
- **Journal**: None
- **Summary**: Satellite image change detection aims at finding occurrences of targeted changes in a given scene taken at different instants. This task is highly challenging due to the acquisition conditions and also to the subjectivity of changes. In this paper, we investigate satellite image change detection using active learning. Our method is interactive and relies on a question and answer model which asks the oracle (user) questions about the most informative display (dubbed as virtual exemplars), and according to the user's responses, updates change detections. The main contribution of our method consists in a novel adversarial model that allows frugally probing the oracle with only the most representative, diverse and uncertain virtual exemplars. The latter are learned to challenge the most the trained change decision criteria which ultimately leads to a better re-estimate of these criteria in the following iterations of active learning. Conducted experiments show the out-performance of our proposed adversarial display model against other display strategies as well as the related work.



### TiG-BEV: Multi-view BEV 3D Object Detection via Target Inner-Geometry Learning
- **Arxiv ID**: http://arxiv.org/abs/2212.13979v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.13979v1)
- **Published**: 2022-12-28 17:53:43+00:00
- **Updated**: 2022-12-28 17:53:43+00:00
- **Authors**: Peixiang Huang, Li Liu, Renrui Zhang, Song Zhang, Xinli Xu, Baichao Wang, Guoyi Liu
- **Comment**: Code link: https://github.com/ADLab3Ds/TiG-BEV
- **Journal**: None
- **Summary**: To achieve accurate and low-cost 3D object detection, existing methods propose to benefit camera-based multi-view detectors with spatial cues provided by the LiDAR modality, e.g., dense depth supervision and bird-eye-view (BEV) feature distillation. However, they directly conduct point-to-point mimicking from LiDAR to camera, which neglects the inner-geometry of foreground targets and suffers from the modal gap between 2D-3D features. In this paper, we propose the learning scheme of Target Inner-Geometry from the LiDAR modality into camera-based BEV detectors for both dense depth and BEV features, termed as TiG-BEV. First, we introduce an inner-depth supervision module to learn the low-level relative depth relations between different foreground pixels. This enables the camera-based detector to better understand the object-wise spatial structures. Second, we design an inner-feature BEV distillation module to imitate the high-level semantics of different keypoints within foreground targets. To further alleviate the BEV feature gap between two modalities, we adopt both inter-channel and inter-keypoint distillation for feature-similarity modeling. With our target inner-geometry distillation, TiG-BEV can effectively boost BEVDepth by +2.3% NDS and +2.4% mAP, along with BEVDet by +9.1% NDS and +10.3% mAP on nuScenes val set. Code will be available at https://github.com/ADLab3Ds/TiG-BEV.



### Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2212.14704v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.14704v2)
- **Published**: 2022-12-28 18:23:47+00:00
- **Updated**: 2023-04-03 15:55:40+00:00
- **Authors**: Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, Shenghua Gao
- **Comment**: Accepted by CVPR 2023. Project page:
  https://bluestyle97.github.io/dream3d/
- **Journal**: None
- **Summary**: Recent CLIP-guided 3D optimization methods, such as DreamFields and PureCLIPNeRF, have achieved impressive results in zero-shot text-to-3D synthesis. However, due to scratch training and random initialization without prior knowledge, these methods often fail to generate accurate and faithful 3D structures that conform to the input text. In this paper, we make the first attempt to introduce explicit 3D shape priors into the CLIP-guided 3D optimization process. Specifically, we first generate a high-quality 3D shape from the input text in the text-to-shape stage as a 3D shape prior. We then use it as the initialization of a neural radiance field and optimize it with the full prompt. To address the challenging text-to-shape generation task, we present a simple yet effective approach that directly bridges the text and image modalities with a powerful text-to-image diffusion model. To narrow the style domain gap between the images synthesized by the text-to-image diffusion model and shape renderings used to train the image-to-shape generator, we further propose to jointly optimize a learnable text prompt and fine-tune the text-to-image diffusion model for rendering-style image generation. Our method, Dream3D, is capable of generating imaginative 3D content with superior visual quality and shape accuracy compared to state-of-the-art methods.



### How Do Deepfakes Move? Motion Magnification for Deepfake Source Detection
- **Arxiv ID**: http://arxiv.org/abs/2212.14033v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.14033v1)
- **Published**: 2022-12-28 18:59:21+00:00
- **Updated**: 2022-12-28 18:59:21+00:00
- **Authors**: Umur Aybars Ciftci, Ilke Demir
- **Comment**: None
- **Journal**: None
- **Summary**: With the proliferation of deep generative models, deepfakes are improving in quality and quantity everyday. However, there are subtle authenticity signals in pristine videos, not replicated by SOTA GANs. We contrast the movement in deepfakes and authentic videos by motion magnification towards building a generalized deepfake source detector. The sub-muscular motion in faces has different interpretations per different generative models which is reflected in their generative residue. Our approach exploits the difference between real motion and the amplified GAN fingerprints, by combining deep and traditional motion magnification, to detect whether a video is fake and its source generator if so. Evaluating our approach on two multi-source datasets, we obtain 97.17% and 94.03% for video source detection. We compare against the prior deepfake source detector and other complex architectures. We also analyze the importance of magnification amount, phase extraction window, backbone network architecture, sample counts, and sample lengths. Finally, we report our results for different skin tones to assess the bias.



### Curator: Creating Large-Scale Curated Labelled Datasets using Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2212.14099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.14099v1)
- **Published**: 2022-12-28 21:22:57+00:00
- **Updated**: 2022-12-28 21:22:57+00:00
- **Authors**: Tarun Narayanan, Ajay Krishnan, Anirudh Koul, Siddha Ganju
- **Comment**: AAAI Fall Symposium 2022
- **Journal**: None
- **Summary**: Applying Machine learning to domains like Earth Sciences is impeded by the lack of labeled data, despite a large corpus of raw data available in such domains. For instance, training a wildfire classifier on satellite imagery requires curating a massive and diverse dataset, which is an expensive and time-consuming process that can span from weeks to months. Searching for relevant examples in over 40 petabytes of unlabelled data requires researchers to manually hunt for such images, much like finding a needle in a haystack. We present a no-code end-to-end pipeline, Curator, which dramatically minimizes the time taken to curate an exhaustive labeled dataset. Curator is able to search massive amounts of unlabelled data by combining self-supervision, scalable nearest neighbor search, and active learning to learn and differentiate image representations. The pipeline can also be readily applied to solve problems across different domains. Overall, the pipeline makes it practical for researchers to go from just one reference image to a comprehensive dataset in a diminutive span of time.



### Joint Discriminative and Metric Embedding Learning for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2212.14107v1
- **DOI**: 10.1007/978-3-031-20716-7_13
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.14107v1)
- **Published**: 2022-12-28 22:08:42+00:00
- **Updated**: 2022-12-28 22:08:42+00:00
- **Authors**: Sinan Sabri, Zaigham Randhawa, Gianfranco Doretto
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification is a challenging task because of the high intra-class variance induced by the unrestricted nuisance factors of variations such as pose, illumination, viewpoint, background, and sensor noise. Recent approaches postulate that powerful architectures have the capacity to learn feature representations invariant to nuisance factors, by training them with losses that minimize intra-class variance and maximize inter-class separation, without modeling nuisance factors explicitly. The dominant approaches use either a discriminative loss with margin, like the softmax loss with the additive angular margin, or a metric learning loss, like the triplet loss with batch hard mining of triplets. Since the softmax imposes feature normalization, it limits the gradient flow supervising the feature embedding. We address this by joining the losses and leveraging the triplet loss as a proxy for the missing gradients. We further improve invariance to nuisance factors by adding the discriminative task of predicting attributes. Our extensive evaluation highlights that when only a holistic representation is learned, we consistently outperform the state-of-the-art on the three most challenging datasets. Such representations are easier to deploy in practical systems. Finally, we found that joining the losses removes the requirement for having a margin in the softmax loss while increasing performance.



### Learning Representations for Masked Facial Recovery
- **Arxiv ID**: http://arxiv.org/abs/2212.14110v1
- **DOI**: 10.1007/978-3-031-20713-6_2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.14110v1)
- **Published**: 2022-12-28 22:22:15+00:00
- **Updated**: 2022-12-28 22:22:15+00:00
- **Authors**: Zaigham Randhawa, Shivang Patel, Donald Adjeroh, Gianfranco Doretto
- **Comment**: None
- **Journal**: None
- **Summary**: The pandemic of these very recent years has led to a dramatic increase in people wearing protective masks in public venues. This poses obvious challenges to the pervasive use of face recognition technology that now is suffering a decline in performance. One way to address the problem is to revert to face recovery methods as a preprocessing step. Current approaches to face reconstruction and manipulation leverage the ability to model the face manifold, but tend to be generic. We introduce a method that is specific for the recovery of the face image from an image of the same individual wearing a mask. We do so by designing a specialized GAN inversion method, based on an appropriate set of losses for learning an unmasking encoder. With extensive experiments, we show that the approach is effective at unmasking face images. In addition, we also show that the identity information is preserved sufficiently well to improve face verification performance based on several face recognition benchmark datasets.



### CellTranspose: Few-shot Domain Adaptation for Cellular Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2212.14121v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.14121v1)
- **Published**: 2022-12-28 23:00:50+00:00
- **Updated**: 2022-12-28 23:00:50+00:00
- **Authors**: Matthew Keaton, Ram Zaveri, Gianfranco Doretto
- **Comment**: Accepted in WACV 2023
- **Journal**: None
- **Summary**: Automated cellular instance segmentation is a process utilized for accelerating biological research for the past two decades, and recent advancements have produced higher quality results with less effort from the biologist. Most current endeavors focus on completely cutting the researcher out of the picture by generating highly generalized models. However, these models invariably fail when faced with novel data, distributed differently than the ones used for training. Rather than approaching the problem with methods that presume the availability of large amounts of target data and computing power for retraining, in this work we address the even greater challenge of designing an approach that requires minimal amounts of new annotated data as well as training time. We do so by designing specialized contrastive losses that leverage the few annotated samples very efficiently. A large set of results show that 3 to 5 annotations lead to models with accuracy that: 1) significantly mitigate the covariate shift effects; 2) matches or surpasses other adaptation methods; 3) even approaches methods that have been fully retrained on the target distribution. The adaptation training is only a few minutes, paving a path towards a balance between model performance, computing requirements and expert-level annotation needs.



### Joint Engagement Classification using Video Augmentation Techniques for Multi-person Human-robot Interaction
- **Arxiv ID**: http://arxiv.org/abs/2212.14128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.14128v1)
- **Published**: 2022-12-28 23:52:55+00:00
- **Updated**: 2022-12-28 23:52:55+00:00
- **Authors**: Yubin Kim, Huili Chen, Sharifa Alghowinem, Cynthia Breazeal, Hae Won Park
- **Comment**: None
- **Journal**: None
- **Summary**: Affect understanding capability is essential for social robots to autonomously interact with a group of users in an intuitive and reciprocal way. However, the challenge of multi-person affect understanding comes from not only the accurate perception of each user's affective state (e.g., engagement) but also the recognition of the affect interplay between the members (e.g., joint engagement) that presents as complex, but subtle, nonverbal exchanges between them. Here we present a novel hybrid framework for identifying a parent-child dyad's joint engagement by combining a deep learning framework with various video augmentation techniques. Using a dataset of parent-child dyads reading storybooks together with a social robot at home, we first train RGB frame- and skeleton-based joint engagement recognition models with four video augmentation techniques (General Aug, DeepFake, CutOut, and Mixed) applied datasets to improve joint engagement classification performance. Second, we demonstrate experimental results on the use of trained models in the robot-parent-child interaction context. Third, we introduce a behavior-based metric for evaluating the learned representation of the models to investigate the model interpretability when recognizing joint engagement. This work serves as the first step toward fully unlocking the potential of end-to-end video understanding models pre-trained on large public datasets and augmented with data augmentation and visualization techniques for affect recognition in the multi-person human-robot interaction in the wild.



