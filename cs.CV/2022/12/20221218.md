# Arxiv Papers in cs.CV on 2022-12-18
### Adaptive Uncertainty Distribution in Deep Learning for Unsupervised Underwater Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2212.08983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.08983v1)
- **Published**: 2022-12-18 01:07:20+00:00
- **Updated**: 2022-12-18 01:07:20+00:00
- **Authors**: Alzayat Saleh, Marcus Sheaves, Dean Jerry, Mostafa Rahimi Azghadi
- **Comment**: 18 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: One of the main challenges in deep learning-based underwater image enhancement is the limited availability of high-quality training data. Underwater images are difficult to capture and are often of poor quality due to the distortion and loss of colour and contrast in water. This makes it difficult to train supervised deep learning models on large and diverse datasets, which can limit the model's performance. In this paper, we explore an alternative approach to supervised underwater image enhancement. Specifically, we propose a novel unsupervised underwater image enhancement framework that employs a conditional variational autoencoder (cVAE) to train a deep learning model with probabilistic adaptive instance normalization (PAdaIN) and statistically guided multi-colour space stretch that produces realistic underwater images. The resulting framework is composed of a U-Net as a feature extractor and a PAdaIN to encode the uncertainty, which we call UDnet. To improve the visual quality of the images generated by UDnet, we use a statistically guided multi-colour space stretch module that ensures visual consistency with the input image and provides an alternative to training using a ground truth image. The proposed model does not need manual human annotation and can learn with a limited amount of data and achieves state-of-the-art results on underwater images. We evaluated our proposed framework on eight publicly-available datasets. The results show that our proposed framework yields competitive performance compared to other state-of-the-art approaches in quantitative as well as qualitative metrics. Code available at https://github.com/alzayats/UDnet .



### Efficient Image Captioning for Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/2212.08985v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.08985v1)
- **Published**: 2022-12-18 01:56:33+00:00
- **Updated**: 2022-12-18 01:56:33+00:00
- **Authors**: Ning Wang, Jiangrong Xie, Hang Luo, Qinglin Cheng, Jihao Wu, Mingbo Jia, Linlin Li
- **Comment**: To appear in AAAI 2023
- **Journal**: None
- **Summary**: Recent years have witnessed the rapid progress of image captioning. However, the demands for large memory storage and heavy computational burden prevent these captioning models from being deployed on mobile devices. The main obstacles lie in the heavyweight visual feature extractors (i.e., object detectors) and complicated cross-modal fusion networks. To this end, we propose LightCap, a lightweight image captioner for resource-limited devices. The core design is built on the recent CLIP model for efficient image captioning. To be specific, on the one hand, we leverage the CLIP model to extract the compact grid features without relying on the time-consuming object detectors. On the other hand, we transfer the image-text retrieval design of CLIP to image captioning scenarios by devising a novel visual concept extractor and a cross-modal modulator. We further optimize the cross-modal fusion model and parallel prediction heads via sequential and ensemble distillations. With the carefully designed architecture, our model merely contains 40M parameters, saving the model size by more than 75% and the FLOPs by more than 98% in comparison with the current state-of-the-art methods. In spite of the low capacity, our model still exhibits state-of-the-art performance on prevalent datasets, e.g., 136.6 CIDEr on COCO Karpathy test split. Testing on the smartphone with only a single CPU, the proposed LightCap exhibits a fast inference speed of 188ms per image, which is ready for practical applications.



### Plankton-FL: Exploration of Federated Learning for Privacy-Preserving Training of Deep Neural Networks for Phytoplankton Classification
- **Arxiv ID**: http://arxiv.org/abs/2212.08990v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.08990v1)
- **Published**: 2022-12-18 02:11:03+00:00
- **Updated**: 2022-12-18 02:11:03+00:00
- **Authors**: Daniel Zhang, Vikram Voleti, Alexander Wong, Jason Deglint
- **Comment**: None
- **Journal**: None
- **Summary**: Creating high-performance generalizable deep neural networks for phytoplankton monitoring requires utilizing large-scale data coming from diverse global water sources. A major challenge to training such networks lies in data privacy, where data collected at different facilities are often restricted from being transferred to a centralized location. A promising approach to overcome this challenge is federated learning, where training is done at site level on local data, and only the model parameters are exchanged over the network to generate a global model. In this study, we explore the feasibility of leveraging federated learning for privacy-preserving training of deep neural networks for phytoplankton classification. More specifically, we simulate two different federated learning frameworks, federated learning (FL) and mutually exclusive FL (ME-FL), and compare their performance to a traditional centralized learning (CL) framework. Experimental results from this study demonstrate the feasibility and potential of federated learning for phytoplankton monitoring.



### Smart Face Shield: A Sensor-Based Wearable Face Shield Utilizing Computer Vision Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2212.08996v1
- **DOI**: 10.25147/ijcsr.2017.001.1.118
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.08996v1)
- **Published**: 2022-12-18 03:23:38+00:00
- **Updated**: 2022-12-18 03:23:38+00:00
- **Authors**: Manuel Luis C. Delos Santos, Ronaldo S. Tinio, Darwin B. Diaz, Karlene Emily I. Tolosa
- **Comment**: None
- **Journal**: IJCSR Volume 6, October 2022, ISSN 2546-115X, pages 1-15
- **Summary**: The study aims the development of a wearable device to combat the onslaught of covid-19. Likewise, to enhance the regular face shield available in the market. Furthermore, to raise awareness of the health and safety protocols initiated by the government and its affiliates in the enforcement of social distancing with the integration of computer vision algorithms. The wearable device was composed of various hardware and software components such as a transparent polycarbonate face shield, microprocessor, sensors, camera, thin-film transistor on-screen display, jumper wires, power bank, and python programming language. The algorithm incorporated in the study was object detection under computer vision machine learning. The front camera with OpenCV technology determines the distance of a person in front of the user. Utilizing TensorFlow, the target object identifies and detects the image or live feed to get its bounding boxes. The focal length lens requires the determination of the distance from the camera to the target object. To get the focal length, multiply the pixel width by the known distance and divide it by the known width (Rosebrock, 2020). The deployment of unit testing ensures that the parameters are valid in terms of design and specifications.



### Graph Neural Network based Child Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2212.09013v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.09013v1)
- **Published**: 2022-12-18 05:07:11+00:00
- **Updated**: 2022-12-18 05:07:11+00:00
- **Authors**: Sanka Mohottala, Pradeepa Samarasinghe, Dharshana Kasthurirathna, Charith Abhayaratne
- **Comment**: Accepted to 23rd IEEE ICIT Conference (2022), 8 pages, 4 figures
- **Journal**: None
- **Summary**: This paper presents an implementation on child activity recognition (CAR) with a graph convolution network (GCN) based deep learning model since prior implementations in this domain have been dominated by CNN, LSTM and other methods despite the superior performance of GCN. To the best of our knowledge, we are the first to use a GCN model in child activity recognition domain. In overcoming the challenges of having small size publicly available child action datasets, several learning methods such as feature extraction, fine-tuning and curriculum learning were implemented to improve the model performance. Inspired by the contradicting claims made on the use of transfer learning in CAR, we conducted a detailed implementation and analysis on transfer learning together with a study on negative transfer learning effect on CAR as it hasn't been addressed previously. As the principal contribution, we were able to develop a ST-GCN based CAR model which, despite the small size of the dataset, obtained around 50% accuracy on vanilla implementations. With feature extraction and fine-tuning methods, accuracy was improved by 20%-30% with the highest accuracy being 82.24%. Furthermore, the results provided on activity datasets empirically demonstrate that with careful selection of pre-train model datasets through methods such as curriculum learning could enhance the accuracy levels. Finally, we provide preliminary evidence on possible frame rate effect on the accuracy of CAR models, a direction future research can explore.



### 2D Pose Estimation based Child Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2212.09027v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.09027v1)
- **Published**: 2022-12-18 07:36:32+00:00
- **Updated**: 2022-12-18 07:36:32+00:00
- **Authors**: Sanka Mohottala, Sandun Abeygunawardana, Pradeepa Samarasinghe, Dharshana Kasthurirathna, Charith Abhayaratne
- **Comment**: Paper Accepted for the IEEE TENCON Conference (2022). 7 pages, 5
  figures
- **Journal**: None
- **Summary**: We present a graph convolutional network with 2D pose estimation for the first time on child action recognition task achieving on par results with an RGB modality based model on a novel benchmark dataset containing unconstrained environment based videos.



### Speed up the inference of diffusion models via shortcut MCMC sampling
- **Arxiv ID**: http://arxiv.org/abs/2301.01206v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T10, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2301.01206v1)
- **Published**: 2022-12-18 07:37:26+00:00
- **Updated**: 2022-12-18 07:37:26+00:00
- **Authors**: Gang Chen
- **Comment**: 9
- **Journal**: None
- **Summary**: Diffusion probabilistic models have generated high quality image synthesis recently. However, one pain point is the notorious inference to gradually obtain clear images with thousands of steps, which is time consuming compared to other generative models. In this paper, we present a shortcut MCMC sampling algorithm, which balances training and inference, while keeping the generated data's quality. In particular, we add the global fidelity constraint with shortcut MCMC sampling to combat the local fitting from diffusion models. We do some initial experiments and show very promising results. Our implementation is available at https://github.com//vividitytech/diffusion-mcmc.git.



### Minimizing Maximum Model Discrepancy for Transferable Black-box Targeted Attacks
- **Arxiv ID**: http://arxiv.org/abs/2212.09035v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.09035v1)
- **Published**: 2022-12-18 08:19:08+00:00
- **Updated**: 2022-12-18 08:19:08+00:00
- **Authors**: Anqi Zhao, Tong Chu, Yahao Liu, Wen Li, Jingjing Li, Lixin Duan
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we study the black-box targeted attack problem from the model discrepancy perspective. On the theoretical side, we present a generalization error bound for black-box targeted attacks, which gives a rigorous theoretical analysis for guaranteeing the success of the attack. We reveal that the attack error on a target model mainly depends on empirical attack error on the substitute model and the maximum model discrepancy among substitute models. On the algorithmic side, we derive a new algorithm for black-box targeted attacks based on our theoretical analysis, in which we additionally minimize the maximum model discrepancy(M3D) of the substitute models when training the generator to generate adversarial examples. In this way, our model is capable of crafting highly transferable adversarial examples that are robust to the model variation, thus improving the success rate for attacking the black-box model. We conduct extensive experiments on the ImageNet dataset with different classification models, and our proposed approach outperforms existing state-of-the-art methods by a significant margin. Our codes will be released.



### Automated Optical Inspection of FAST's Reflector Surface using Drones and Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2212.09039v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.09039v1)
- **Published**: 2022-12-18 08:34:05+00:00
- **Updated**: 2022-12-18 08:34:05+00:00
- **Authors**: Jianan Li, Shenwang Jiang, Liqiang Song, Peiran Peng, Feng Mu, Hui Li, Peng Jiang, Tingfa Xu
- **Comment**: None
- **Journal**: None
- **Summary**: The Five-hundred-meter Aperture Spherical radio Telescope (FAST) is the world's largest single-dish radio telescope. Its large reflecting surface achieves unprecedented sensitivity but is prone to damage, such as dents and holes, caused by naturally-occurring falling objects. Hence, the timely and accurate detection of surface defects is crucial for FAST's stable operation. Conventional manual inspection involves human inspectors climbing up and examining the large surface visually, a time-consuming and potentially unreliable process. To accelerate the inspection process and increase its accuracy, this work makes the first step towards automating the inspection of FAST by integrating deep-learning techniques with drone technology. First, a drone flies over the surface along a predetermined route. Since surface defects significantly vary in scale and show high inter-class similarity, directly applying existing deep detectors to detect defects on the drone imagery is highly prone to missing and misidentifying defects. As a remedy, we introduce cross-fusion, a dedicated plug-in operation for deep detectors that enables the adaptive fusion of multi-level features in a point-wise selective fashion, depending on local defect patterns. Consequently, strong semantics and fine-grained details are dynamically fused at different positions to support the accurate detection of defects of various scales and types. Our AI-powered drone-based automated inspection is time-efficient, reliable, and has good accessibility, which guarantees the long-term and stable operation of FAST.



### Gait Recognition Using 3-D Human Body Shape Inference
- **Arxiv ID**: http://arxiv.org/abs/2212.09042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.09042v1)
- **Published**: 2022-12-18 09:27:00+00:00
- **Updated**: 2022-12-18 09:27:00+00:00
- **Authors**: Haidong Zhu, Zhaoheng Zheng, Ram Nevatia
- **Comment**: Accepted to WACV 2023
- **Journal**: None
- **Summary**: Gait recognition, which identifies individuals based on their walking patterns, is an important biometric technique since it can be observed from a distance and does not require the subject's cooperation. Recognizing a person's gait is difficult because of the appearance variants in human silhouette sequences produced by varying viewing angles, carrying objects, and clothing. Recent research has produced a number of ways for coping with these variants. In this paper, we present the usage of inferring 3-D body shapes distilled from limited images, which are, in principle, invariant to the specified variants. Inference of 3-D shape is a difficult task, especially when only silhouettes are provided in a dataset. We provide a method for learning 3-D body inference from silhouettes by transferring knowledge from 3-D shape prior from RGB photos. We use our method on multiple existing state-of-the-art gait baselines and obtain consistent improvements for gait identification on two public datasets, CASIA-B and OUMVLP, on several variants and settings, including a new setting of novel views not seen during training.



### Internal Diverse Image Completion
- **Arxiv ID**: http://arxiv.org/abs/2212.10280v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.10280v1)
- **Published**: 2022-12-18 10:02:53+00:00
- **Updated**: 2022-12-18 10:02:53+00:00
- **Authors**: Noa Alkobi, Tamar Rott Shaham, Tomer Michaeli
- **Comment**: None
- **Journal**: None
- **Summary**: Image completion is widely used in photo restoration and editing applications, e.g. for object removal. Recently, there has been a surge of research on generating diverse completions for missing regions. However, existing methods require large training sets from a specific domain of interest, and often fail on general-content images. In this paper, we propose a diverse completion method that does not require a training set and can thus treat arbitrary images from any domain. Our internal diverse completion (IDC) approach draws inspiration from recent single-image generative models that are trained on multiple scales of a single image, adapting them to the extreme setting in which only a small portion of the image is available for training. We illustrate the strength of IDC on several datasets, using both user studies and quantitative comparisons.



### Bort: Towards Explainable Neural Networks with Bounded Orthogonal Constraint
- **Arxiv ID**: http://arxiv.org/abs/2212.09062v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.09062v2)
- **Published**: 2022-12-18 11:02:50+00:00
- **Updated**: 2023-03-08 01:45:53+00:00
- **Authors**: Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu
- **Comment**: ICLR 2023 accepted
- **Journal**: None
- **Summary**: Deep learning has revolutionized human society, yet the black-box nature of deep neural networks hinders further application to reliability-demanded industries. In the attempt to unpack them, many works observe or impact internal variables to improve the comprehensibility and invertibility of the black-box models. However, existing methods rely on intuitive assumptions and lack mathematical guarantees. To bridge this gap, we introduce Bort, an optimizer for improving model explainability with boundedness and orthogonality constraints on model parameters, derived from the sufficient conditions of model comprehensibility and invertibility. We perform reconstruction and backtracking on the model representations optimized by Bort and observe a clear improvement in model explainability. Based on Bort, we are able to synthesize explainable adversarial samples without additional parameters and training. Surprisingly, we find Bort constantly improves the classification accuracy of various architectures including ResNet and DeiT on MNIST, CIFAR-10, and ImageNet. Code: https://github.com/zbr17/Bort.



### Fine-Tuning Is All You Need to Mitigate Backdoor Attacks
- **Arxiv ID**: http://arxiv.org/abs/2212.09067v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.09067v1)
- **Published**: 2022-12-18 11:30:59+00:00
- **Updated**: 2022-12-18 11:30:59+00:00
- **Authors**: Zeyang Sha, Xinlei He, Pascal Berrang, Mathias Humbert, Yang Zhang
- **Comment**: 17 pages, 17 figures
- **Journal**: None
- **Summary**: Backdoor attacks represent one of the major threats to machine learning models. Various efforts have been made to mitigate backdoors. However, existing defenses have become increasingly complex and often require high computational resources or may also jeopardize models' utility. In this work, we show that fine-tuning, one of the most common and easy-to-adopt machine learning training operations, can effectively remove backdoors from machine learning models while maintaining high model utility. Extensive experiments over three machine learning paradigms show that fine-tuning and our newly proposed super-fine-tuning achieve strong defense performance. Furthermore, we coin a new term, namely backdoor sequela, to measure the changes in model vulnerabilities to other attacks before and after the backdoor has been removed. Empirical evaluation shows that, compared to other defense methods, super-fine-tuning leaves limited backdoor sequela. We hope our results can help machine learning model owners better protect their models from backdoor threats. Also, it calls for the design of more advanced attacks in order to comprehensively assess machine learning models' backdoor vulnerabilities.



### Style-Hallucinated Dual Consistency Learning: A Unified Framework for Visual Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2212.09068v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.09068v1)
- **Published**: 2022-12-18 11:42:51+00:00
- **Updated**: 2022-12-18 11:42:51+00:00
- **Authors**: Yuyang Zhao, Zhun Zhong, Na Zhao, Nicu Sebe, Gim Hee Lee
- **Comment**: Journal extension of arXiv:2204.02548. Code is available at
  https://github.com/HeliosZhao/SHADE-VisualDG
- **Journal**: None
- **Summary**: Domain shift widely exists in the visual world, while modern deep neural networks commonly suffer from severe performance degradation under domain shift due to the poor generalization ability, which limits the real-world applications. The domain shift mainly lies in the limited source environmental variations and the large distribution gap between source and unseen target data. To this end, we propose a unified framework, Style-HAllucinated Dual consistEncy learning (SHADE), to handle such domain shift in various visual tasks. Specifically, SHADE is constructed based on two consistency constraints, Style Consistency (SC) and Retrospection Consistency (RC). SC enriches the source situations and encourages the model to learn consistent representation across style-diversified samples. RC leverages general visual knowledge to prevent the model from overfitting to source data and thus largely keeps the representation consistent between the source and general visual models. Furthermore, we present a novel style hallucination module (SHM) to generate style-diversified samples that are essential to consistency learning. SHM selects basis styles from the source distribution, enabling the model to dynamically generate diverse and realistic samples during training. Extensive experiments demonstrate that our versatile SHADE can significantly enhance the generalization in various visual recognition tasks, including image classification, semantic segmentation and object detection, with different models, i.e., ConvNets and Transformer.



### Masked Wavelet Representation for Compact Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2212.09069v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2212.09069v2)
- **Published**: 2022-12-18 11:43:32+00:00
- **Updated**: 2023-03-21 10:23:40+00:00
- **Authors**: Daniel Rho, Byeonghyeon Lee, Seungtae Nam, Joo Chan Lee, Jong Hwan Ko, Eunbyung Park
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: Neural radiance fields (NeRF) have demonstrated the potential of coordinate-based neural representation (neural fields or implicit neural representation) in neural rendering. However, using a multi-layer perceptron (MLP) to represent a 3D scene or object requires enormous computational resources and time. There have been recent studies on how to reduce these computational inefficiencies by using additional data structures, such as grids or trees. Despite the promising performance, the explicit data structure necessitates a substantial amount of memory. In this work, we present a method to reduce the size without compromising the advantages of having additional data structures. In detail, we propose using the wavelet transform on grid-based neural fields. Grid-based neural fields are for fast convergence, and the wavelet transform, whose efficiency has been demonstrated in high-performance standard codecs, is to improve the parameter efficiency of grids. Furthermore, in order to achieve a higher sparsity of grid coefficients while maintaining reconstruction quality, we present a novel trainable masking approach. Experimental results demonstrate that non-spatial grid coefficients, such as wavelet coefficients, are capable of attaining a higher level of sparsity than spatial grid coefficients, resulting in a more compact representation. With our proposed mask and compression pipeline, we achieved state-of-the-art performance within a memory budget of 2 MB. Our code is available at https://github.com/daniel03c1/masked_wavelet_nerf.



### LR-CSNet: Low-Rank Deep Unfolding Network for Image Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/2212.09088v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.09088v1)
- **Published**: 2022-12-18 13:54:11+00:00
- **Updated**: 2022-12-18 13:54:11+00:00
- **Authors**: Tianfang Zhang, Lei Li, Christian Igel, Stefan Oehmcke, Fabian Gieseke, Zhenming Peng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep unfolding networks (DUNs) have proven to be a viable approach to compressive sensing (CS). In this work, we propose a DUN called low-rank CS network (LR-CSNet) for natural image CS. Real-world image patches are often well-represented by low-rank approximations. LR-CSNet exploits this property by adding a low-rank prior to the CS optimization task. We derive a corresponding iterative optimization procedure using variable splitting, which is then translated to a new DUN architecture. The architecture uses low-rank generation modules (LRGMs), which learn low-rank matrix factorizations, as well as gradient descent and proximal mappings (GDPMs), which are proposed to extract high-frequency features to refine image details. In addition, the deep features generated at each reconstruction stage in the DUN are transferred between stages to boost the performance. Our extensive experiments on three widely considered datasets demonstrate the promising performance of LR-CSNet compared to state-of-the-art methods in natural image CS.



### Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion and UV GAN
- **Arxiv ID**: http://arxiv.org/abs/2212.09098v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.09098v5)
- **Published**: 2022-12-18 14:51:46+00:00
- **Updated**: 2023-05-30 17:07:58+00:00
- **Authors**: Lei Li, Tianfang Zhang, Zhongfeng Kang, Xikun Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-grained semantic segmentation of a person's face and head, including facial parts and head components, has progressed a great deal in recent years. However, it remains a challenging task, whereby considering ambiguous occlusions and large pose variations are particularly difficult. To overcome these difficulties, we propose a novel framework termed Mask-FPAN. It uses a de-occlusion module that learns to parse occluded faces in a semi-supervised way. In particular, face landmark localization, face occlusionstimations, and detected head poses are taken into account. A 3D morphable face model combined with the UV GAN improves the robustness of 2D face parsing. In addition, we introduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for face paring work. The proposed Mask-FPAN framework addresses the face parsing problem in the wild and shows significant performance improvements with MIOU from 0.7353 to 0.9013 compared to the state-of-the-art on challenging face datasets.



### SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input Images
- **Arxiv ID**: http://arxiv.org/abs/2212.09100v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2212.09100v3)
- **Published**: 2022-12-18 14:56:22+00:00
- **Updated**: 2023-08-21 12:53:09+00:00
- **Authors**: Abdullah Hamdi, Bernard Ghanem, Matthias Nießner
- **Comment**: published at ICCV 2023 workshop proceedings
- **Journal**: None
- **Summary**: Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel view synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels for efficient and fast rendering (plenoxels,InstantNGP). In order to leverage machine learning and adoption of SRFs as a 3D representation, we present SPARF, a large-scale ShapeNet-based synthetic dataset for novel view synthesis consisting of $\sim$ 17 million images rendered from nearly 40,000 shapes at high resolution (400 X 400 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis and includes more than one million 3D-optimized radiance fields with multiple voxel resolutions. Furthermore, we propose a novel pipeline (SuRFNet) that learns to generate sparse voxel radiance fields from only few views. This is done by using the densely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs partial SRFs from few/one images and a specialized SRF loss to learn to generate high-quality sparse voxel radiance fields that can be rendered from novel views. Our approach achieves state-of-the-art results in the task of unconstrained novel view synthesis based on few views on ShapeNet as compared to recent baselines. The SPARF dataset is made public with the code and models on the project website https://abdullahamdi.com/sparf/ .



### Face Generation and Editing with StyleGAN: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2212.09102v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.09102v2)
- **Published**: 2022-12-18 15:04:31+00:00
- **Updated**: 2023-04-13 11:03:47+00:00
- **Authors**: Andrew Melnik, Maksim Miasayedzenkau, Dzianis Makarovets, Dzianis Pirshtuk, Eren Akbulut, Dennis Holzmann, Tarek Renusch, Gustav Reichert, Helge Ritter
- **Comment**: None
- **Journal**: None
- **Summary**: Our goal with this survey is to provide an overview of the state of the art deep learning technologies for face generation and editing. We will cover popular latest architectures and discuss key ideas that make them work, such as inversion, latent representation, loss functions, training procedures, editing methods, and cross domain style transfer. We particularly focus on GAN-based architectures that have culminated in the StyleGAN approaches, which allow generation of high-quality face images and offer rich interfaces for controllable semantics editing and preserving photo quality. We aim to provide an entry point into the field for readers that have basic knowledge about the field of deep learning and are looking for an accessible introduction and overview.



### A Framework for Generalizing Critical Heat Flux Detection Models Using Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2212.09107v3
- **DOI**: 10.1016/j.eswa.2023.120265
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.09107v3)
- **Published**: 2022-12-18 15:26:08+00:00
- **Updated**: 2023-03-18 02:46:38+00:00
- **Authors**: Firas Al-Hindawi, Tejaswi Soori, Han Hu, Md Mahfuzur Rahman Siddiquee, Hyunsoo Yoon, Teresa Wu, Ying Sun
- **Comment**: This work has been submitted to the Expert Systems With Applications
  Journal on Sep 25, 2022
- **Journal**: None
- **Summary**: The detection of critical heat flux (CHF) is crucial in heat boiling applications as failure to do so can cause rapid temperature ramp leading to device failures. Many machine learning models exist to detect CHF, but their performance reduces significantly when tested on data from different domains. To deal with datasets from new domains a model needs to be trained from scratch. Moreover, the dataset needs to be annotated by a domain expert. To address this issue, we propose a new framework to support the generalizability and adaptability of trained CHF detection models in an unsupervised manner. This approach uses an unsupervised Image-to-Image (UI2I) translation model to transform images in the target dataset to look like they were obtained from the same domain the model previously trained on. Unlike other frameworks dealing with domain shift, our framework does not require retraining or fine-tuning of the trained classification model nor does it require synthesized datasets in the training process of either the classification model or the UI2I model. The framework was tested on three boiling datasets from different domains, and we show that the CHF detection model trained on one dataset was able to generalize to the other two previously unseen datasets with high accuracy. Overall, the framework enables CHF detection models to adapt to data generated from different domains without requiring additional annotation effort or retraining of the model.



### SUCRe: Leveraging Scene Structure for Underwater Color Restoration
- **Arxiv ID**: http://arxiv.org/abs/2212.09129v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.09129v2)
- **Published**: 2022-12-18 16:53:13+00:00
- **Updated**: 2023-03-16 14:57:16+00:00
- **Authors**: Clémentin Boittiaux, Ricard Marxer, Claire Dune, Aurélien Arnaubec, Maxime Ferrera, Vincent Hugel
- **Comment**: None
- **Journal**: None
- **Summary**: Underwater images are altered by the physical characteristics of the medium through which light rays pass before reaching the optical sensor. Scattering and wavelength-dependent absorption significantly modify the captured colors depending on the distance of observed elements to the image plane. In this paper, we aim to recover an image of the scene as if the water had no effect on light propagation. We introduce SUCRe, a new method that exploits the scene's 3D structure for underwater color restoration. By following points in multiple images and tracking their intensities at different distances to the sensor, we constrain the optimization of the parameters in an underwater image formation model and retrieve unattenuated pixel intensities. We conduct extensive quantitative and qualitative analyses of our approach in a variety of scenarios ranging from natural light to deep-sea environments using three underwater datasets acquired from real-world scenarios and one synthetic dataset. We also compare the performance of the proposed approach with that of a wide range of existing state-of-the-art methods. The results demonstrate a consistent benefit of exploiting multiple views across a spectrum of objective metrics. Our code is publicly available at https://github.com/clementinboittiaux/sucre.



### Performance Analysis of YOLO-based Architectures for Vehicle Detection from Traffic Images in Bangladesh
- **Arxiv ID**: http://arxiv.org/abs/2212.09144v2
- **DOI**: 10.1109/ICCIT57492.2022.10055758
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.09144v2)
- **Published**: 2022-12-18 18:53:35+00:00
- **Updated**: 2022-12-24 06:56:33+00:00
- **Authors**: Refaat Mohammad Alamgir, Ali Abir Shuvro, Mueeze Al Mushabbir, Mohammed Ashfaq Raiyan, Nusrat Jahan Rani, Md. Mushfiqur Rahman, Md. Hasanul Kabir, Sabbir Ahmed
- **Comment**: Accepted in 25th ICCIT (6 pages, 5 figures, 1 table)
- **Journal**: 2022 25th International Conference on Computer and Information
  Technology (ICCIT)
- **Summary**: The task of locating and classifying different types of vehicles has become a vital element in numerous applications of automation and intelligent systems ranging from traffic surveillance to vehicle identification and many more. In recent times, Deep Learning models have been dominating the field of vehicle detection. Yet, Bangladeshi vehicle detection has remained a relatively unexplored area. One of the main goals of vehicle detection is its real-time application, where `You Only Look Once' (YOLO) models have proven to be the most effective architecture. In this work, intending to find the best-suited YOLO architecture for fast and accurate vehicle detection from traffic images in Bangladesh, we have conducted a performance analysis of different variants of the YOLO-based architectures such as YOLOV3, YOLOV5s, and YOLOV5x. The models were trained on a dataset containing 7390 images belonging to 21 types of vehicles comprising samples from the DhakaAI dataset, the Poribohon-BD dataset, and our self-collected images. After thorough quantitative and qualitative analysis, we found the YOLOV5x variant to be the best-suited model, performing better than YOLOv3 and YOLOv5s models respectively by 7 & 4 percent in mAP, and 12 & 8.5 percent in terms of Accuracy.



