# Arxiv Papers in cs.CV on 2022-12-09
### Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2212.04613v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.04613v1)
- **Published**: 2022-12-09 00:34:50+00:00
- **Updated**: 2022-12-09 00:34:50+00:00
- **Authors**: Kyle Buettner, Adriana Kovashka
- **Comment**: To appear, 2nd International Workshop on Practical Deep Learning in
  the Wild at AAAI Conference on Artificial Intelligence 2023
- **Journal**: None
- **Summary**: Contrastive learning has emerged as a competitive pretraining method for object detection. Despite this progress, there has been minimal investigation into the robustness of contrastively pretrained detectors when faced with domain shifts. To address this gap, we conduct an empirical study of contrastive learning and out-of-domain object detection, studying how contrastive view design affects robustness. In particular, we perform a case study of the detection-focused pretext task Instance Localization (InsLoc) and propose strategies to augment views and enhance robustness in appearance-shifted and context-shifted scenarios. Amongst these strategies, we propose changes to cropping such as altering the percentage used, adding IoU constraints, and integrating saliency based object priors. We also explore the addition of shortcut-reducing augmentations such as Poisson blending, texture flattening, and elastic deformation. We benchmark these strategies on abstract, weather, and context domain shifts and illustrate robust ways to combine them, in both pretraining on single-object and multi-object image datasets. Overall, our results and insights show how to ensure robustness through the choice of views in contrastive learning.



### UNet Based Pipeline for Lung Segmentation from Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/2212.04617v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.04617v1)
- **Published**: 2022-12-09 01:00:10+00:00
- **Updated**: 2022-12-09 01:00:10+00:00
- **Authors**: Shashank Shekhar, Ritika Nandi, H Srikanth Kamath
- **Comment**: 6 Pages
- **Journal**: None
- **Summary**: Biomedical image segmentation is one of the fastest growing fields which has seen extensive automation through the use of Artificial Intelligence. This has enabled widespread adoption of accurate techniques to expedite the screening and diagnostic processes which would otherwise take several days to finalize. In this paper, we present an end-to-end pipeline to segment lungs from chest X-ray images, training the neural network model on the Japanese Society of Radiological Technology (JSRT) dataset, using UNet to enable faster processing of initial screening for various lung disorders. The pipeline developed can be readily used by medical centers with just the provision of X-Ray images as input. The model will perform the preprocessing, and provide a segmented image as the final output. It is expected that this will drastically reduce the manual effort involved and lead to greater accessibility in resource-constrained locations.



### Category-Level 6D Object Pose Estimation with Flexible Vector-Based Rotation Representation
- **Arxiv ID**: http://arxiv.org/abs/2212.04632v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04632v2)
- **Published**: 2022-12-09 02:13:43+00:00
- **Updated**: 2023-01-30 00:40:05+00:00
- **Authors**: Wei Chen, Xi Jia, Zhongqun Zhang, Hyung Jin Chang, Linlin Shen, Jinming Duan, Ales Leonardis
- **Comment**: revised from CVPR2021 paper FS-NET. arXiv admin note: substantial
  text overlap with arXiv:2103.07054
- **Journal**: None
- **Summary**: In this paper, we propose a novel 3D graph convolution based pipeline for category-level 6D pose and size estimation from monocular RGB-D images. The proposed method leverages an efficient 3D data augmentation and a novel vector-based decoupled rotation representation. Specifically, we first design an orientation-aware autoencoder with 3D graph convolution for latent feature learning. The learned latent feature is insensitive to point shift and size thanks to the shift and scale-invariance properties of the 3D graph convolution. Then, to efficiently decode the rotation information from the latent feature, we design a novel flexible vector-based decomposable rotation representation that employs two decoders to complementarily access the rotation information. The proposed rotation representation has two major advantages: 1) decoupled characteristic that makes the rotation estimation easier; 2) flexible length and rotated angle of the vectors allow us to find a more suitable vector representation for specific pose estimation task. Finally, we propose a 3D deformation mechanism to increase the generalization ability of the pipeline. Extensive experiments show that the proposed pipeline achieves state-of-the-art performance on category-level tasks. Further, the experiments demonstrate that the proposed rotation representation is more suitable for the pose estimation tasks than other rotation representations.



### Ego-Body Pose Estimation via Ego-Head Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2212.04636v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2212.04636v3)
- **Published**: 2022-12-09 02:25:20+00:00
- **Updated**: 2023-08-28 02:51:25+00:00
- **Authors**: Jiaman Li, C. Karen Liu, Jiajun Wu
- **Comment**: CVPR 2023 (Award Candidate)
- **Journal**: None
- **Summary**: Estimating 3D human motion from an egocentric video sequence plays a critical role in human behavior understanding and has various applications in VR/AR. However, naively learning a mapping between egocentric videos and human motions is challenging, because the user's body is often unobserved by the front-facing camera placed on the head of the user. In addition, collecting large-scale, high-quality datasets with paired egocentric videos and 3D human motions requires accurate motion capture devices, which often limit the variety of scenes in the videos to lab-like environments. To eliminate the need for paired egocentric video and human motions, we propose a new method, Ego-Body Pose Estimation via Ego-Head Pose Estimation (EgoEgo), which decomposes the problem into two stages, connected by the head motion as an intermediate representation. EgoEgo first integrates SLAM and a learning approach to estimate accurate head motion. Subsequently, leveraging the estimated head pose as input, EgoEgo utilizes conditional diffusion to generate multiple plausible full-body motions. This disentanglement of head and body pose eliminates the need for training datasets with paired egocentric videos and 3D human motion, enabling us to leverage large-scale egocentric video datasets and motion capture datasets separately. Moreover, for systematic benchmarking, we develop a synthetic dataset, AMASS-Replica-Ego-Syn (ARES), with paired egocentric videos and human motion. On both ARES and real data, our EgoEgo model performs significantly better than the current state-of-the-art methods.



### FLAG3D: A 3D Fitness Activity Dataset with Language Instruction
- **Arxiv ID**: http://arxiv.org/abs/2212.04638v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04638v2)
- **Published**: 2022-12-09 02:33:33+00:00
- **Updated**: 2023-04-19 13:31:03+00:00
- **Authors**: Yansong Tang, Jinpeng Liu, Aoyang Liu, Bin Yang, Wenxun Dai, Yongming Rao, Jiwen Lu, Jie Zhou, Xiu Li
- **Comment**: Accepted to CVPR2023
- **Journal**: None
- **Summary**: With the continuously thriving popularity around the world, fitness activity analytic has become an emerging research topic in computer vision. While a variety of new tasks and algorithms have been proposed recently, there are growing hunger for data resources involved in high-quality data, fine-grained labels, and diverse environments. In this paper, we present FLAG3D, a large-scale 3D fitness activity dataset with language instruction containing 180K sequences of 60 categories. FLAG3D features the following three aspects: 1) accurate and dense 3D human pose captured from advanced MoCap system to handle the complex activity and large movement, 2) detailed and professional language instruction to describe how to perform a specific activity, 3) versatile video resources from a high-tech MoCap system, rendering software, and cost-effective smartphones in natural environments. Extensive experiments and in-depth analysis show that FLAG3D contributes great research value for various challenges, such as cross-domain human action recognition, dynamic human mesh recovery, and language-guided human action generation. Our dataset and source code are publicly available at https://andytang15.github.io/FLAG3D.



### A Hyper-weight Network for Hyperspectral Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2301.06081v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.06081v1)
- **Published**: 2022-12-09 03:28:07+00:00
- **Updated**: 2022-12-09 03:28:07+00:00
- **Authors**: Xiangyu Rui, Xiangyong Cao, Jun Shu, Qian Zhao, Deyu Meng
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: In the hyperspectral image (HSI) denoising task, the real noise embedded in the HSI is always complex and diverse so that many model-based HSI denoising methods only perform well on some specific noisy HSIs. To enhance the noise adaptation capability of current methods, we first resort to the weighted HSI denoising model since its weight is capable of characterizing the noise in different positions of the image. However, the weight in these weighted models is always determined by an empirical updating formula, which does not fully utilize the noise information contained in noisy images and thus limits their performance improvement. In this work, we propose an automatic weighting scheme to alleviate this issue. Specifically, the weight in the weighted model is predicted by a hyper-weight network (i.e., HWnet), which can be learned in a bi-level optimization framework based on the data-driven methodology. The learned HWnet can be explicitly plugged into other weighted denoising models, and help adjust weights for different noisy HSIs and different weighted models. Extensive experiments verify that the proposed HWnet can help improve the generalization ability of a weighted model to adapt to more complex noise, and can also strengthen the weighted model by transferring the knowledge from another weighted model. Additionally, to explain the experimental results, we also theoretically prove the training error and generalization error upper bound of the proposed HWnet, which should be the first generalization error analysis in the low-level vision field as far as we know.



### MIMO Is All You Need : A Strong Multi-In-Multi-Out Baseline for Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2212.04655v3
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2212.04655v3)
- **Published**: 2022-12-09 03:57:13+00:00
- **Updated**: 2023-05-30 04:55:09+00:00
- **Authors**: Shuliang Ning, Mengcheng Lan, Yanran Li, Chaofeng Chen, Qian Chen, Xunlai Chen, Xiaoguang Han, Shuguang Cui
- **Comment**: None
- **Journal**: AAAI 2023
- **Summary**: The mainstream of the existing approaches for video prediction builds up their models based on a Single-In-Single-Out (SISO) architecture, which takes the current frame as input to predict the next frame in a recursive manner. This way often leads to severe performance degradation when they try to extrapolate a longer period of future, thus limiting the practical use of the prediction model. Alternatively, a Multi-In-Multi-Out (MIMO) architecture that outputs all the future frames at one shot naturally breaks the recursive manner and therefore prevents error accumulation. However, only a few MIMO models for video prediction are proposed and they only achieve inferior performance due to the date. The real strength of the MIMO model in this area is not well noticed and is largely under-explored. Motivated by that, we conduct a comprehensive investigation in this paper to thoroughly exploit how far a simple MIMO architecture can go. Surprisingly, our empirical studies reveal that a simple MIMO model can outperform the state-of-the-art work with a large margin much more than expected, especially in dealing with longterm error accumulation. After exploring a number of ways and designs, we propose a new MIMO architecture based on extending the pure Transformer with local spatio-temporal blocks and a new multi-output decoder, namely MIMO-VP, to establish a new standard in video prediction. We evaluate our model in four highly competitive benchmarks (Moving MNIST, Human3.6M, Weather, KITTI). Extensive experiments show that our model wins 1st place on all the benchmarks with remarkable performance gains and surpasses the best SISO model in all aspects including efficiency, quantity, and quality. We believe our model can serve as a new baseline to facilitate the future research of video prediction tasks. The code will be released.



### An Attention-based Multi-Scale Feature Learning Network for Multimodal Medical Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/2212.04661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04661v1)
- **Published**: 2022-12-09 04:19:43+00:00
- **Updated**: 2022-12-09 04:19:43+00:00
- **Authors**: Meng Zhou, Xiaolan Xu, Yuxuan Zhang
- **Comment**: 8 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: Medical images play an important role in clinical applications. Multimodal medical images could provide rich information about patients for physicians to diagnose. The image fusion technique is able to synthesize complementary information from multimodal images into a single image. This technique will prevent radiologists switch back and forth between different images and save lots of time in the diagnostic process. In this paper, we introduce a novel Dilated Residual Attention Network for the medical image fusion task. Our network is capable to extract multi-scale deep semantic features. Furthermore, we propose a novel fixed fusion strategy termed Softmax-based weighted strategy based on the Softmax weights and matrix nuclear norm. Extensive experiments show our proposed network and fusion strategy exceed the state-of-the-art performance compared with reference image fusion methods on four commonly used fusion metrics.



### A Computer Vision Method for Estimating Velocity from Jumps
- **Arxiv ID**: http://arxiv.org/abs/2212.04665v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04665v1)
- **Published**: 2022-12-09 04:40:36+00:00
- **Updated**: 2022-12-09 04:40:36+00:00
- **Authors**: Soumyadip Roy, Chaitanya Roygaga, Nathaniel Blanchard, Aparna Bharati
- **Comment**: None
- **Journal**: 2nd Workshop on Computer Vision for Winter Sports 2023
- **Summary**: Athletes routinely undergo fitness evaluations to evaluate their training progress. Typically, these evaluations require a trained professional who utilizes specialized equipment like force plates. For the assessment, athletes perform drop and squat jumps, and key variables are measured, e.g. velocity, flight time, and time to stabilization, to name a few. However, amateur athletes may not have access to professionals or equipment that can provide these assessments. Here, we investigate the feasibility of estimating key variables using video recordings. We focus on jump velocity as a starting point because it is highly correlated with other key variables and is important for determining posture and lower-limb capacity. We find that velocity can be estimated with a high degree of precision across a range of athletes, with an average R-value of 0.71 (SD = 0.06).



### Neural Volume Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2212.04666v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04666v2)
- **Published**: 2022-12-09 04:54:13+00:00
- **Updated**: 2023-05-05 20:32:30+00:00
- **Authors**: Yuval Bahat, Yuxuan Zhang, Hendrik Sommerhoff, Andreas Kolb, Felix Heide
- **Comment**: None
- **Journal**: None
- **Summary**: Neural volumetric representations have become a widely adopted model for radiance fields in 3D scenes. These representations are fully implicit or hybrid function approximators of the instantaneous volumetric radiance in a scene, which are typically learned from multi-view captures of the scene. We investigate the new task of neural volume super-resolution - rendering high-resolution views corresponding to a scene captured at low resolution. To this end, we propose a neural super-resolution network that operates directly on the volumetric representation of the scene. This approach allows us to exploit an advantage of operating in the volumetric domain, namely the ability to guarantee consistent super-resolution across different viewing directions. To realize our method, we devise a novel 3D representation that hinges on multiple 2D feature planes. This allows us to super-resolve the 3D scene representation by applying 2D convolutional networks on the 2D feature planes. We validate the proposed method by super-resolving multi-view consistent views on a diverse set of unseen 3D scenes, confirming qualitative and quantitatively favorable quality over existing approaches.



### Synthetic-to-Real Domain Generalized Semantic Segmentation for 3D Indoor Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2212.04668v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04668v1)
- **Published**: 2022-12-09 05:07:43+00:00
- **Updated**: 2022-12-09 05:07:43+00:00
- **Authors**: Yuyang Zhao, Na Zhao, Gim Hee Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation in 3D indoor scenes has achieved remarkable performance under the supervision of large-scale annotated data. However, previous works rely on the assumption that the training and testing data are of the same distribution, which may suffer from performance degradation when evaluated on the out-of-distribution scenes. To alleviate the annotation cost and the performance degradation, this paper introduces the synthetic-to-real domain generalization setting to this task. Specifically, the domain gap between synthetic and real-world point cloud data mainly lies in the different layouts and point patterns. To address these problems, we first propose a clustering instance mix (CINMix) augmentation technique to diversify the layouts of the source data. In addition, we augment the point patterns of the source data and introduce non-parametric multi-prototypes to ameliorate the intra-class variance enlarged by the augmented point patterns. The multi-prototypes can model the intra-class variance and rectify the global classifier in both training and inference stages. Experiments on the synthetic-to-real benchmark demonstrate that both CINMix and multi-prototypes can narrow the distribution gap and thus improve the generalization ability on real-world datasets.



### MSI: Maximize Support-Set Information for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2212.04673v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04673v2)
- **Published**: 2022-12-09 05:38:07+00:00
- **Updated**: 2023-03-30 05:04:25+00:00
- **Authors**: Seonghyeon Moon, Samuel S. Sohn, Honglu Zhou, Sejong Yoon, Vladimir Pavlovic, Muhammad Haris Khan, Mubbasir Kapadia
- **Comment**: None
- **Journal**: None
- **Summary**: FSS(Few-shot segmentation) aims to segment a target class using a small number of labeled images (support set). To extract the information relevant to target class, a dominant approach in best performing FSS methods removes background features using a support mask. We observe that this feature excision through a limiting support mask introduces an information bottleneck in several challenging FSS cases, e.g., for small targets and/or inaccurate target boundaries. To this end, we present a novel method (MSI), which maximizes the support-set information by exploiting two complementary sources of features to generate super correlation maps. We validate the effectiveness of our approach by instantiating it into three recent and strong FSS methods. Experimental results on several publicly available FSS benchmarks show that our proposed method consistently improves the performance by visible margins and leads to faster convergence. Our code and models will be publicly released.



### SemanticBEVFusion: Rethink LiDAR-Camera Fusion in Unified Bird's-Eye View Representation for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2212.04675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04675v1)
- **Published**: 2022-12-09 05:48:58+00:00
- **Updated**: 2022-12-09 05:48:58+00:00
- **Authors**: Qi Jiang, Hao Sun, Xi Zhang
- **Comment**: The first two authors contributed equally to this work
- **Journal**: The 2023 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS 2023)
- **Summary**: LiDAR and camera are two essential sensors for 3D object detection in autonomous driving. LiDAR provides accurate and reliable 3D geometry information while the camera provides rich texture with color. Despite the increasing popularity of fusing these two complementary sensors, the challenge remains in how to effectively fuse 3D LiDAR point cloud with 2D camera images. Recent methods focus on point-level fusion which paints the LiDAR point cloud with camera features in the perspective view or bird's-eye view (BEV)-level fusion which unifies multi-modality features in the BEV representation. In this paper, we rethink these previous fusion strategies and analyze their information loss and influences on geometric and semantic features. We present SemanticBEVFusion to deeply fuse camera features with LiDAR features in a unified BEV representation while maintaining per-modality strengths for 3D object detection. Our method achieves state-of-the-art performance on the large-scale nuScenes dataset, especially for challenging distant objects. The code will be made publicly available.



### Motion and Context-Aware Audio-Visual Conditioned Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2212.04679v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04679v2)
- **Published**: 2022-12-09 05:57:46+00:00
- **Updated**: 2023-04-23 03:58:57+00:00
- **Authors**: Yating Xu, Gim Hee Lee
- **Comment**: Under refinement
- **Journal**: None
- **Summary**: Existing state-of-the-art method for audio-visual conditioned video prediction uses the latent codes of the audio-visual frames from a multimodal stochastic network and a frame encoder to predict the next visual frame. However, a direct inference of per-pixel intensity for the next visual frame from the latent codes is extremely challenging because of the high-dimensional image space. To this end, we propose to decouple the audio-visual conditioned video prediction into motion and appearance modeling. The first part is the multimodal motion estimation module that learns motion information as optical flow from the given audio-visual clip. The second part is the context-aware refinement module that uses the predicted optical flow to warp the current visual frame into the next visual frame and refines it base on the given audio-visual context. Experimental results show that our method achieves competitive results on existing benchmarks.



### Dynamic Test-Time Augmentation via Differentiable Functions
- **Arxiv ID**: http://arxiv.org/abs/2212.04681v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04681v2)
- **Published**: 2022-12-09 06:06:47+00:00
- **Updated**: 2023-03-15 04:06:46+00:00
- **Authors**: Shohei Enomoto, Monikka Roslianna Busto, Takeharu Eda
- **Comment**: None
- **Journal**: None
- **Summary**: Distribution shifts, which often occur in the real world, degrade the accuracy of deep learning systems, and thus improving robustness is essential for practical applications. To improve robustness, we study an image enhancement method that generates recognition-friendly images without retraining the recognition model. We propose a novel image enhancement method, DynTTA, which is based on differentiable data augmentation techniques and generates a blended image from many augmented images to improve the recognition accuracy under distribution shifts. In addition to standard data augmentations, DynTTA also incorporates deep neural network-based image transformation, which further improves the robustness. Because DynTTA is composed of differentiable functions, it is directly trained with the classification loss of the recognition model. We experiment with widely used image recognition datasets using various classification models, including Vision Transformer and MLP-Mixer. DynTTA improves the robustness with almost no reduction in classification accuracy for clean images, which is a better result than the existing methods. Furthermore, we show that estimating the training time augmentation for distribution-shifted datasets using DynTTA and retraining the recognition model with the estimated augmentation significantly improves robustness.



### Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2212.04687v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.04687v1)
- **Published**: 2022-12-09 06:29:43+00:00
- **Updated**: 2022-12-09 06:29:43+00:00
- **Authors**: Rui Zhu, Di Tang, Siyuan Tang, XiaoFeng Wang, Haixu Tang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a simple yet surprisingly effective technique to induce "selective amnesia" on a backdoored model. Our approach, called SEAM, has been inspired by the problem of catastrophic forgetting (CF), a long standing issue in continual learning. Our idea is to retrain a given DNN model on randomly labeled clean data, to induce a CF on the model, leading to a sudden forget on both primary and backdoor tasks; then we recover the primary task by retraining the randomized model on correctly labeled clean data. We analyzed SEAM by modeling the unlearning process as continual learning and further approximating a DNN using Neural Tangent Kernel for measuring CF. Our analysis shows that our random-labeling approach actually maximizes the CF on an unknown backdoor in the absence of triggered inputs, and also preserves some feature extraction in the network to enable a fast revival of the primary task. We further evaluated SEAM on both image processing and Natural Language Processing tasks, under both data contamination and training manipulation attacks, over thousands of models either trained on popular image datasets or provided by the TrojAI competition. Our experiments show that SEAM vastly outperforms the state-of-the-art unlearning techniques, achieving a high Fidelity (measuring the gap between the accuracy of the primary task and that of the backdoor) within a few minutes (about 30 times faster than training a model from scratch using the MNIST dataset), with only a small amount of clean data (0.1% of training data for TrojAI models).



### Benchmarking Self-Supervised Learning on Diverse Pathology Datasets
- **Arxiv ID**: http://arxiv.org/abs/2212.04690v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.04690v2)
- **Published**: 2022-12-09 06:38:34+00:00
- **Updated**: 2023-04-18 15:07:46+00:00
- **Authors**: Mingu Kang, Heon Song, Seonwook Park, Donggeun Yoo, Sérgio Pereira
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: Computational pathology can lead to saving human lives, but models are annotation hungry and pathology images are notoriously expensive to annotate. Self-supervised learning has shown to be an effective method for utilizing unlabeled data, and its application to pathology could greatly benefit its downstream tasks. Yet, there are no principled studies that compare SSL methods and discuss how to adapt them for pathology. To address this need, we execute the largest-scale study of SSL pre-training on pathology image data, to date. Our study is conducted using 4 representative SSL methods on diverse downstream tasks. We establish that large-scale domain-aligned pre-training in pathology consistently out-performs ImageNet pre-training in standard SSL settings such as linear and fine-tuning evaluations, as well as in low-label regimes. Moreover, we propose a set of domain-specific techniques that we experimentally show leads to a performance boost. Lastly, for the first time, we apply SSL to the challenging task of nuclei instance segmentation and show large and consistent performance improvements under diverse settings.



### Tencent AVS: A Holistic Ads Video Dataset for Multi-modal Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2212.04700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04700v1)
- **Published**: 2022-12-09 07:26:20+00:00
- **Updated**: 2022-12-09 07:26:20+00:00
- **Authors**: Jie Jiang, Zhimin Li, Jiangfeng Xiong, Rongwei Quan, Qinglin Lu, Wei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal video segmentation and classification have been advanced greatly by public benchmarks in recent years. However, such research still mainly focuses on human actions, failing to describe videos in a holistic view. In addition, previous research tends to pay much attention to visual information yet ignores the multi-modal nature of videos. To fill this gap, we construct the Tencent `Ads Video Segmentation'~(TAVS) dataset in the ads domain to escalate multi-modal video analysis to a new level. TAVS describes videos from three independent perspectives as `presentation form', `place', and `style', and contains rich multi-modal information such as video, audio, and text. TAVS is organized hierarchically in semantic aspects for comprehensive temporal video segmentation with three levels of categories for multi-label classification, e.g., `place' - `working place' - `office'. Therefore, TAVS is distinguished from previous temporal segmentation datasets due to its multi-modal information, holistic view of categories, and hierarchical granularities. It includes 12,000 videos, 82 classes, 33,900 segments, 121,100 shots, and 168,500 labels. Accompanied with TAVS, we also present a strong multi-modal video segmentation baseline coupled with multi-label class prediction. Extensive experiments are conducted to evaluate our proposed method as well as existing representative methods to reveal key challenges of our dataset TAVS.



### 4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions
- **Arxiv ID**: http://arxiv.org/abs/2212.04701v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04701v2)
- **Published**: 2022-12-09 07:26:49+00:00
- **Updated**: 2023-04-04 02:42:35+00:00
- **Authors**: Zhongshu Wang, Lingzhi Li, Zhen Shen, Li Shen, Liefeng Bo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel and effective framework, named 4K-NeRF, to pursue high fidelity view synthesis on the challenging scenarios of ultra high resolutions, building on the methodology of neural radiance fields (NeRF). The rendering procedure of NeRF-based methods typically relies on a pixel-wise manner in which rays (or pixels) are treated independently on both training and inference phases, limiting its representational ability on describing subtle details, especially when lifting to a extremely high resolution. We address the issue by exploring ray correlation to enhance high-frequency details recovery. Particularly, we use the 3D-aware encoder to model geometric information effectively in a lower resolution space and recover fine details through the 3D-aware decoder, conditioned on ray features and depths estimated by the encoder. Joint training with patch-based sampling further facilitates our method incorporating the supervision from perception oriented regularization beyond pixel-wise loss. Benefiting from the use of geometry-aware local context, our method can significantly boost rendering quality on high-frequency details compared with modern NeRF methods, and achieve the state-of-the-art visual quality on 4K ultra-high-resolution scenarios. Code Available at \url{https://github.com/frozoul/4K-NeRF}



### DIP: Differentiable Interreflection-aware Physics-based Inverse Rendering
- **Arxiv ID**: http://arxiv.org/abs/2212.04705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04705v1)
- **Published**: 2022-12-09 07:33:49+00:00
- **Updated**: 2022-12-09 07:33:49+00:00
- **Authors**: Youming Deng, Xueting Li, Sifei Liu, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: We present a physics-based inverse rendering method that learns the illumination, geometry, and materials of a scene from posed multi-view RGB images. To model the illumination of a scene, existing inverse rendering works either completely ignore the indirect illumination or model it by coarse approximations, leading to sub-optimal illumination, geometry, and material prediction of the scene. In this work, we propose a physics-based illumination model that explicitly traces the incoming indirect lights at each surface point based on interreflection, followed by estimating each identified indirect light through an efficient neural network. Furthermore, we utilize the Leibniz's integral rule to resolve non-differentiability in the proposed illumination model caused by one type of environment light -- the tangent lights. As a result, the proposed interreflection-aware illumination model can be learned end-to-end together with geometry and materials estimation. As a side product, our physics-based inverse rendering model also facilitates flexible and realistic material editing as well as relighting. Extensive experiments on both synthetic and real-world datasets demonstrate that the proposed method performs favorably against existing inverse rendering methods on novel view synthesis and inverse rendering.



### ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow Removal
- **Arxiv ID**: http://arxiv.org/abs/2212.04711v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2212.04711v2)
- **Published**: 2022-12-09 07:48:30+00:00
- **Updated**: 2022-12-13 08:56:31+00:00
- **Authors**: Lanqing Guo, Chong Wang, Wenhan Yang, Siyu Huang, Yufei Wang, Hanspeter Pfister, Bihan Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Recent deep learning methods have achieved promising results in image shadow removal. However, their restored images still suffer from unsatisfactory boundary artifacts, due to the lack of degradation prior embedding and the deficiency in modeling capacity. Our work addresses these issues by proposing a unified diffusion framework that integrates both the image and degradation priors for highly effective shadow removal. In detail, we first propose a shadow degradation model, which inspires us to build a novel unrolling diffusion model, dubbed ShandowDiffusion. It remarkably improves the model's capacity in shadow removal via progressively refining the desired output with both degradation prior and diffusive generative prior, which by nature can serve as a new strong baseline for image restoration. Furthermore, ShadowDiffusion progressively refines the estimated shadow mask as an auxiliary task of the diffusion generator, which leads to more accurate and robust shadow-free image generation. We conduct extensive experiments on three popular public datasets, including ISTD, ISTD+, and SRD, to validate our method's effectiveness. Compared to the state-of-the-art methods, our model achieves a significant improvement in terms of PSNR, increasing from 31.69dB to 34.73dB over SRD dataset.



### Occluded Person Re-Identification via Relational Adaptive Feature Correction Learning
- **Arxiv ID**: http://arxiv.org/abs/2212.04712v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04712v1)
- **Published**: 2022-12-09 07:48:47+00:00
- **Updated**: 2022-12-09 07:48:47+00:00
- **Authors**: Minjung Kim, MyeongAh Cho, Heansung Lee, Suhwan Cho, Sangyoun Lee
- **Comment**: ICASSP 2022
- **Journal**: None
- **Summary**: Occluded person re-identification (Re-ID) in images captured by multiple cameras is challenging because the target person is occluded by pedestrians or objects, especially in crowded scenes. In addition to the processes performed during holistic person Re-ID, occluded person Re-ID involves the removal of obstacles and the detection of partially visible body parts. Most existing methods utilize the off-the-shelf pose or parsing networks as pseudo labels, which are prone to error. To address these issues, we propose a novel Occlusion Correction Network (OCNet) that corrects features through relational-weight learning and obtains diverse and representative features without using external networks. In addition, we present a simple concept of a center feature in order to provide an intuitive solution to pedestrian occlusion scenarios. Furthermore, we suggest the idea of Separation Loss (SL) for focusing on different parts between global features and part features. We conduct extensive experiments on five challenging benchmark datasets for occluded and holistic Re-ID tasks to demonstrate that our method achieves superior performance to state-of-the-art methods especially on occluded scene.



### Predicting Shape Development: a Riemannian Method
- **Arxiv ID**: http://arxiv.org/abs/2212.04740v3
- **DOI**: None
- **Categories**: **cs.CV**, math.DG, q-bio.TO, 92-05 (Primary), 53Z10, 62J99 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/2212.04740v3)
- **Published**: 2022-12-09 09:36:59+00:00
- **Updated**: 2023-08-28 09:59:28+00:00
- **Authors**: Doğa Türkseven, Islem Rekik, Christoph von Tycowicz, Martin Hanik
- **Comment**: new experiment with human motion data; fixed vertex-assignment bug in
  the prediction of the varifold-based method
- **Journal**: None
- **Summary**: Predicting the future development of an anatomical shape from a single baseline observation is a challenging task. But it can be essential for clinical decision-making. Research has shown that it should be tackled in curved shape spaces, as (e.g., disease-related) shape changes frequently expose nonlinear characteristics. We thus propose a novel prediction method that encodes the whole shape in a Riemannian shape space. It then learns a simple prediction technique founded on hierarchical statistical modeling of longitudinal training data. When applied to predict the future development of the shape of the right hippocampus under Alzheimer's disease and to human body motion, it outperforms deep learning-supported variants as well as state-of-the-art.



### Physically Plausible Animation of Human Upper Body from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2212.04741v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2212.04741v1)
- **Published**: 2022-12-09 09:36:59+00:00
- **Updated**: 2022-12-09 09:36:59+00:00
- **Authors**: Ziyuan Huang, Zhengping Zhou, Yung-Yu Chuang, Jiajun Wu, C. Karen Liu
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: We present a new method for generating controllable, dynamically responsive, and photorealistic human animations. Given an image of a person, our system allows the user to generate Physically plausible Upper Body Animation (PUBA) using interaction in the image space, such as dragging their hand to various locations. We formulate a reinforcement learning problem to train a dynamic model that predicts the person's next 2D state (i.e., keypoints on the image) conditioned on a 3D action (i.e., joint torque), and a policy that outputs optimal actions to control the person to achieve desired goals. The dynamic model leverages the expressiveness of 3D simulation and the visual realism of 2D videos. PUBA generates 2D keypoint sequences that achieve task goals while being responsive to forceful perturbation. The sequences of keypoints are then translated by a pose-to-image generator to produce the final photorealistic video.



### Weakly Supervised Semantic Segmentation for Large-Scale Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2212.04744v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.04744v1)
- **Published**: 2022-12-09 09:42:26+00:00
- **Updated**: 2022-12-09 09:42:26+00:00
- **Authors**: Yachao Zhang, Zonghao Li, Yuan Xie, Yanyun Qu, Cuihua Li, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Existing methods for large-scale point cloud semantic segmentation require expensive, tedious and error-prone manual point-wise annotations. Intuitively, weakly supervised training is a direct solution to reduce the cost of labeling. However, for weakly supervised large-scale point cloud semantic segmentation, too few annotations will inevitably lead to ineffective learning of network. We propose an effective weakly supervised method containing two components to solve the above problem. Firstly, we construct a pretext task, \textit{i.e.,} point cloud colorization, with a self-supervised learning to transfer the learned prior knowledge from a large amount of unlabeled point cloud to a weakly supervised network. In this way, the representation capability of the weakly supervised network can be improved by the guidance from a heterogeneous task. Besides, to generate pseudo label for unlabeled data, a sparse label propagation mechanism is proposed with the help of generated class prototypes, which is used to measure the classification confidence of unlabeled point. Our method is evaluated on large-scale point cloud datasets with different scenarios including indoor and outdoor. The experimental results show the large gain against existing weakly supervised and comparable results to fully supervised methods\footnote{Code based on mindspore: https://github.com/dmcv-ecnu/MindSpore\_ModelZoo/tree/main/WS3\_MindSpore}.



### SLAM for Visually Impaired People: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2212.04745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04745v1)
- **Published**: 2022-12-09 09:45:43+00:00
- **Updated**: 2022-12-09 09:45:43+00:00
- **Authors**: Marziyeh Bamdad, Davide Scaramuzza, Alireza Darvishy
- **Comment**: 26 pages, 5 tables, 3 figures
- **Journal**: None
- **Summary**: In recent decades, several assistive technologies for visually impaired and blind (VIB) people have been developed to improve their ability to navigate independently and safely. At the same time, simultaneous localization and mapping (SLAM) techniques have become sufficiently robust and efficient to be adopted in the development of assistive technologies. In this paper, we first report the results of an anonymous survey conducted with VIB people to understand their experience and needs; we focus on digital assistive technologies that help them with indoor and outdoor navigation. Then, we present a literature review of assistive technologies based on SLAM. We discuss proposed approaches and indicate their pros and cons. We conclude by presenting future opportunities and challenges in this domain.



### Estimating Chicago's tree cover and canopy height using multi-spectral satellite imagery
- **Arxiv ID**: http://arxiv.org/abs/2212.05061v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.05061v1)
- **Published**: 2022-12-09 10:12:34+00:00
- **Updated**: 2022-12-09 10:12:34+00:00
- **Authors**: John Francis, Stephen Law
- **Comment**: 4 pages, 4 figures, Submitted to Tackling Climate Change with Machine
  Learning: workshop at NeurIPS 2022
- **Journal**: None
- **Summary**: Information on urban tree canopies is fundamental to mitigating climate change [1] as well as improving quality of life [2]. Urban tree planting initiatives face a lack of up-to-date data about the horizontal and vertical dimensions of the tree canopy in cities. We present a pipeline that utilizes LiDAR data as ground-truth and then trains a multi-task machine learning model to generate reliable estimates of tree cover and canopy height in urban areas using multi-source multi-spectral satellite imagery for the case study of Chicago.



### Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2212.04761v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04761v2)
- **Published**: 2022-12-09 10:37:22+00:00
- **Updated**: 2023-07-19 02:20:18+00:00
- **Authors**: Jungho Lee, Minhyeok Lee, Suhwan Cho, Sungmin Woo, Sungjun Jang, Sangyoun Lee
- **Comment**: Accepted by ICCV 2023
- **Journal**: None
- **Summary**: Skeleton-based action recognition has attracted considerable attention due to its compact representation of the human body's skeletal sructure. Many recent methods have achieved remarkable performance using graph convolutional networks (GCNs) and convolutional neural networks (CNNs), which extract spatial and temporal features, respectively. Although spatial and temporal dependencies in the human skeleton have been explored separately, spatio-temporal dependency is rarely considered. In this paper, we propose the Spatio-Temporal Curve Network (STC-Net) to effectively leverage the spatio-temporal dependency of the human skeleton. Our proposed network consists of two novel elements: 1) The Spatio-Temporal Curve (STC) module; and 2) Dilated Kernels for Graph Convolution (DK-GC). The STC module dynamically adjusts the receptive field by identifying meaningful node connections between every adjacent frame and generating spatio-temporal curves based on the identified node connections, providing an adaptive spatio-temporal coverage. In addition, we propose DK-GC to consider long-range dependencies, which results in a large receptive field without any additional parameters by applying an extended kernel to the given adjacency matrices of the graph. Our STC-Net combines these two modules and achieves state-of-the-art performance on four skeleton-based action recognition benchmarks.



### Genie: Show Me the Data for Quantization
- **Arxiv ID**: http://arxiv.org/abs/2212.04780v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.04780v3)
- **Published**: 2022-12-09 11:18:40+00:00
- **Updated**: 2023-08-08 14:30:05+00:00
- **Authors**: Yongkweon Jeon, Chungman Lee, Ho-young Kim
- **Comment**: Accepted by CVPR 2023, https://github.com/SamsungLabs/Genie
- **Journal**: None
- **Summary**: Zero-shot quantization is a promising approach for developing lightweight deep neural networks when data is inaccessible owing to various reasons, including cost and issues related to privacy. By exploiting the learned parameters ($\mu$ and $\sigma$) of batch normalization layers in an FP32-pre-trained model, zero-shot quantization schemes focus on generating synthetic data. Subsequently, they distill knowledge from the pre-trained model (teacher) to the quantized model (student) such that the quantized model can be optimized with the synthetic dataset. However, thus far, zero-shot quantization has primarily been discussed in the context of quantization-aware training methods, which require task-specific losses and long-term optimization as much as retraining. We thus introduce a post-training quantization scheme for zero-shot quantization that produces high-quality quantized networks within a few hours. Furthermore, we propose a framework called Genie~that generates data suited for quantization. With the data synthesized by Genie, we can produce robust quantized models without real datasets, which is comparable to few-shot quantization. We also propose a post-training quantization algorithm to enhance the performance of quantized models. By combining them, we can bridge the gap between zero-shot and few-shot quantization while significantly improving the quantization performance compared to that of existing approaches. In other words, we can obtain a unique state-of-the-art zero-shot quantization approach. The code is available at \url{https://github.com/SamsungLabs/Genie}.



### Music Recommendation System based on Emotion, Age and Ethnicity
- **Arxiv ID**: http://arxiv.org/abs/2212.04782v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.04782v1)
- **Published**: 2022-12-09 11:23:39+00:00
- **Updated**: 2022-12-09 11:23:39+00:00
- **Authors**: Ramiz Mammadli, Huma Bilgin, Ali Can Karaca
- **Comment**: 14 Pages, 10 Figures and 3 Tables
- **Journal**: None
- **Summary**: A Music Recommendation System based on Emotion, Age, and Ethnicity is developed in this study, using FER-2013 and ``Age, Gender, and Ethnicity (Face Data) CSV'' datasets. The CNN architecture, which is extensively used for this kind of purpose has been applied to the training of the models. After adding several appropriate layers to the training end of the project, in total, 3 separate models are trained in the Deep Learning side of the project: Emotion, Ethnicity, and Age. After the training step of these models, they are used as classifiers on the web application side. The snapshot of the user taken through the interface is sent to the models to predict their mood, age, and ethnic origin. According to these classifiers, various kinds of playlists pulled from Spotify API are proposed to the user in order to establish a functional and user-friendly atmosphere for the music selection. Afterward, the user can choose the playlist they want and listen to it by following the given link.



### Image-Based Fire Detection in Industrial Environments with YOLOv4
- **Arxiv ID**: http://arxiv.org/abs/2212.04786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04786v1)
- **Published**: 2022-12-09 11:32:36+00:00
- **Updated**: 2022-12-09 11:32:36+00:00
- **Authors**: Otto Zell, Joel Pålsson, Kevin Hernandez-Diaz, Fernando Alonso-Fernandez, Felix Nilsson
- **Comment**: Accepted for publication at ICPRAM
- **Journal**: None
- **Summary**: Fires have destructive power when they break out and affect their surroundings on a devastatingly large scale. The best way to minimize their damage is to detect the fire as quickly as possible before it has a chance to grow. Accordingly, this work looks into the potential of AI to detect and recognize fires and reduce detection time using object detection on an image stream. Object detection has made giant leaps in speed and accuracy over the last six years, making real-time detection feasible. To our end, we collected and labeled appropriate data from several public sources, which have been used to train and evaluate several models based on the popular YOLOv4 object detector. Our focus, driven by a collaborating industrial partner, is to implement our system in an industrial warehouse setting, which is characterized by high ceilings. A drawback of traditional smoke detectors in this setup is that the smoke has to rise to a sufficient height. The AI models brought forward in this research managed to outperform these detectors by a significant amount of time, providing precious anticipation that could help to minimize the effects of fires further.



### Synthetic Data for Object Classification in Industrial Applications
- **Arxiv ID**: http://arxiv.org/abs/2212.04790v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04790v1)
- **Published**: 2022-12-09 11:43:04+00:00
- **Updated**: 2022-12-09 11:43:04+00:00
- **Authors**: August Baaz, Yonan Yonan, Kevin Hernandez-Diaz, Fernando Alonso-Fernandez, Felix Nilsson
- **Comment**: Accepted for publication at ICPRAM
- **Journal**: None
- **Summary**: One of the biggest challenges in machine learning is data collection. Training data is an important part since it determines how the model will behave. In object classification, capturing a large number of images per object and in different conditions is not always possible and can be very time-consuming and tedious. Accordingly, this work explores the creation of artificial images using a game engine to cope with limited data in the training dataset. We combine real and synthetic data to train the object classification engine, a strategy that has shown to be beneficial to increase confidence in the decisions made by the classifier, which is often critical in industrial setups. To combine real and synthetic data, we first train the classifier on a massive amount of synthetic data, and then we fine-tune it on real images. Another important result is that the amount of real images needed for fine-tuning is not very high, reaching top accuracy with just 12 or 24 images per class. This substantially reduces the requirements of capturing a great amount of real data.



### Visual Detection of Personal Protective Equipment and Safety Gear on Industry Workers
- **Arxiv ID**: http://arxiv.org/abs/2212.04794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04794v1)
- **Published**: 2022-12-09 11:50:03+00:00
- **Updated**: 2022-12-09 11:50:03+00:00
- **Authors**: Jonathan Karlsson, Fredrik Strand, Josef Bigun, Fernando Alonso-Fernandez, Kevin Hernandez-Diaz, Felix Nilsson
- **Comment**: Accepted for publication at ICPRAM
- **Journal**: None
- **Summary**: Workplace injuries are common in today's society due to a lack of adequately worn safety equipment. A system that only admits appropriately equipped personnel can be created to improve working conditions. The goal is thus to develop a system that will improve workers' safety using a camera that will detect the usage of Personal Protective Equipment (PPE). To this end, we collected and labeled appropriate data from several public sources, which have been used to train and evaluate several models based on the popular YOLOv4 object detector. Our focus, driven by a collaborating industrial partner, is to implement our system into an entry control point where workers must present themselves to obtain access to a restricted area. Combined with facial identity recognition, the system would ensure that only authorized people wearing appropriate equipment are granted access. A novelty of this work is that we increase the number of classes to five objects (hardhat, safety vest, safety gloves, safety glasses, and hearing protection), whereas most existing works only focus on one or two classes, usually hardhats or vests. The AI model developed provides good detection accuracy at a distance of 3 and 5 meters in the collaborative environment where we aim at operating (mAP of 99/89%, respectively). The small size of some objects or the potential occlusion by body parts have been identified as potential factors that are detrimental to accuracy, which we have counteracted via data augmentation and cropping of the body before applying PPE detection.



### CEPHA29: Automatic Cephalometric Landmark Detection Challenge 2023
- **Arxiv ID**: http://arxiv.org/abs/2212.04808v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04808v2)
- **Published**: 2022-12-09 12:25:58+00:00
- **Updated**: 2023-04-03 10:27:21+00:00
- **Authors**: Muhammad Anwaar Khalid, Kanwal Zulfiqar, Ulfat Bashir, Areeba Shaheen, Rida Iqbal, Zarnab Rizwan, Ghina Rizwan, Muhammad Moazam Fraz
- **Comment**: None
- **Journal**: None
- **Summary**: Quantitative cephalometric analysis is the most widely used clinical and research tool in modern orthodontics. Accurate localization of cephalometric landmarks enables the quantification and classification of anatomical abnormalities, however, the traditional manual way of marking these landmarks is a very tedious job. Endeavours have constantly been made to develop automated cephalometric landmark detection systems but they are inadequate for orthodontic applications. The fundamental reason for this is that the amount of publicly available datasets as well as the images provided for training in these datasets are insufficient for an AI model to perform well. To facilitate the development of robust AI solutions for morphometric analysis, we organise the CEPHA29 Automatic Cephalometric Landmark Detection Challenge in conjunction with IEEE International Symposium on Biomedical Imaging (ISBI 2023). In this context, we provide the largest known publicly available dataset, consisting of 1000 cephalometric X-ray images. We hope that our challenge will not only derive forward research and innovation in automatic cephalometric landmark identification but will also signal the beginning of a new era in the discipline.



### Reliable Multimodal Trajectory Prediction via Error Aligned Uncertainty Optimization
- **Arxiv ID**: http://arxiv.org/abs/2212.04812v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.04812v1)
- **Published**: 2022-12-09 12:33:26+00:00
- **Updated**: 2022-12-09 12:33:26+00:00
- **Authors**: Neslihan Kose, Ranganath Krishnan, Akash Dhamasia, Omesh Tickoo, Michael Paulitsch
- **Comment**: Accepted to ECCV 2022 workshop - Safe Artificial Intelligence for
  Automated Driving
- **Journal**: None
- **Summary**: Reliable uncertainty quantification in deep neural networks is very crucial in safety-critical applications such as automated driving for trustworthy and informed decision-making. Assessing the quality of uncertainty estimates is challenging as ground truth for uncertainty estimates is not available. Ideally, in a well-calibrated model, uncertainty estimates should perfectly correlate with model error. We propose a novel error aligned uncertainty optimization method and introduce a trainable loss function to guide the models to yield good quality uncertainty estimates aligning with the model error. Our approach targets continuous structured prediction and regression tasks, and is evaluated on multiple datasets including a large-scale vehicle motion prediction task involving real-world distributional shifts. We demonstrate that our method improves average displacement error by 1.69% and 4.69%, and the uncertainty correlation with model error by 17.22% and 19.13% as quantified by Pearson correlation coefficient on two state-of-the-art baselines.



### Noise2Contrast: Multi-Contrast Fusion Enables Self-Supervised Tomographic Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2212.04832v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.04832v1)
- **Published**: 2022-12-09 13:03:24+00:00
- **Updated**: 2022-12-09 13:03:24+00:00
- **Authors**: Fabian Wagner, Mareike Thies, Laura Pfaff, Noah Maul, Sabrina Pechmann, Mingxuan Gu, Jonas Utz, Oliver Aust, Daniela Weidner, Georgiana Neag, Stefan Uderhardt, Jang-Hwan Choi, Andreas Maier
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised image denoising techniques emerged as convenient methods that allow training denoising models without requiring ground-truth noise-free data. Existing methods usually optimize loss metrics that are calculated from multiple noisy realizations of similar images, e.g., from neighboring tomographic slices. However, those approaches fail to utilize the multiple contrasts that are routinely acquired in medical imaging modalities like MRI or dual-energy CT. In this work, we propose the new self-supervised training scheme Noise2Contrast that combines information from multiple measured image contrasts to train a denoising model. We stack denoising with domain-transfer operators to utilize the independent noise realizations of different image contrasts to derive a self-supervised loss. The trained denoising operator achieves convincing quantitative and qualitative results, outperforming state-of-the-art self-supervised methods by 4.7-11.0%/4.8-7.3% (PSNR/SSIM) on brain MRI data and by 43.6-50.5%/57.1-77.1% (PSNR/SSIM) on dual-energy CT X-ray microscopy data with respect to the noisy baseline. Our experiments on different real measured data sets indicate that Noise2Contrast training generalizes to other multi-contrast imaging modalities.



### PIVOT: Prompting for Video Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2212.04842v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.04842v2)
- **Published**: 2022-12-09 13:22:27+00:00
- **Updated**: 2023-04-04 22:28:05+00:00
- **Authors**: Andrés Villa, Juan León Alcázar, Motasem Alfarra, Kumail Alhamoud, Julio Hurtado, Fabian Caba Heilbron, Alvaro Soto, Bernard Ghanem
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Modern machine learning pipelines are limited due to data availability, storage quotas, privacy regulations, and expensive annotation processes. These constraints make it difficult or impossible to train and update large-scale models on such dynamic annotated sets. Continual learning directly approaches this problem, with the ultimate goal of devising methods where a deep neural network effectively learns relevant patterns for new (unseen) classes, without significantly altering its performance on previously learned ones. In this paper, we address the problem of continual learning for video data. We introduce PIVOT, a novel method that leverages extensive knowledge in pre-trained models from the image domain, thereby reducing the number of trainable parameters and the associated forgetting. Unlike previous methods, ours is the first approach that effectively uses prompting mechanisms for continual learning without any in-domain pre-training. Our experiments show that PIVOT improves state-of-the-art methods by a significant 27% on the 20-task ActivityNet setup.



### Album cover art image generation with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2212.04844v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.04844v1)
- **Published**: 2022-12-09 13:27:46+00:00
- **Updated**: 2022-12-09 13:27:46+00:00
- **Authors**: Felipe Perez Stoppa, Ester Vidaña-Vila, Joan Navarro
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) were introduced by Goodfellow in 2014, and since then have become popular for constructing generative artificial intelligence models. However, the drawbacks of such networks are numerous, like their longer training times, their sensitivity to hyperparameter tuning, several types of loss and optimization functions and other difficulties like mode collapse. Current applications of GANs include generating photo-realistic human faces, animals and objects. However, I wanted to explore the artistic ability of GANs in more detail, by using existing models and learning from them. This dissertation covers the basics of neural networks and works its way up to the particular aspects of GANs, together with experimentation and modification of existing available models, from least complex to most. The intention is to see if state of the art GANs (specifically StyleGAN2) can generate album art covers and if it is possible to tailor them by genre. This was attempted by first familiarizing myself with 3 existing GANs architectures, including the state of the art StyleGAN2. The StyleGAN2 code was used to train a model with a dataset containing 80K album cover images, then used to style images by picking curated images and mixing their styles.



### Frugal Reinforcement-based Active Learning
- **Arxiv ID**: http://arxiv.org/abs/2212.04868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04868v1)
- **Published**: 2022-12-09 14:17:45+00:00
- **Updated**: 2022-12-09 14:17:45+00:00
- **Authors**: Sebastien Deschamps, Hichem Sahbi
- **Comment**: arXiv admin note: text overlap with arXiv:2203.11564
- **Journal**: None
- **Summary**: Most of the existing learning models, particularly deep neural networks, are reliant on large datasets whose hand-labeling is expensive and time demanding. A current trend is to make the learning of these models frugal and less dependent on large collections of labeled data. Among the existing solutions, deep active learning is currently witnessing a major interest and its purpose is to train deep networks using as few labeled samples as possible. However, the success of active learning is highly dependent on how critical are these samples when training models. In this paper, we devise a novel active learning approach for label-efficient training. The proposed method is iterative and aims at minimizing a constrained objective function that mixes diversity, representativity and uncertainty criteria. The proposed approach is probabilistic and unifies all these criteria in a single objective function whose solution models the probability of relevance of samples (i.e., how critical) when learning a decision function. We also introduce a novel weighting mechanism based on reinforcement learning, which adaptively balances these criteria at each training iteration, using a particular stateless Q-learning model. Extensive experiments conducted on staple image classification data, including Object-DOTA, show the effectiveness of our proposed model w.r.t. several baselines including random, uncertainty and flat as well as other work.



### RCDT: Relational Remote Sensing Change Detection with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2212.04869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04869v1)
- **Published**: 2022-12-09 14:21:42+00:00
- **Updated**: 2022-12-09 14:21:42+00:00
- **Authors**: Kaixuan Lu, Xiao Huang
- **Comment**: 18 pages, 11 figures,
- **Journal**: None
- **Summary**: Deep learning based change detection methods have received wide attentoion, thanks to their strong capability in obtaining rich features from images. However, existing AI-based CD methods largely rely on three functionality-enhancing modules, i.e., semantic enhancement, attention mechanisms, and correspondence enhancement. The stacking of these modules leads to great model complexity. To unify these three modules into a simple pipeline, we introduce Relational Change Detection Transformer (RCDT), a novel and simple framework for remote sensing change detection tasks. The proposed RCDT consists of three major components, a weight-sharing Siamese Backbone to obtain bi-temporal features, a Relational Cross Attention Module (RCAM) that implements offset cross attention to obtain bi-temporal relation-aware features, and a Features Constrain Module (FCM) to achieve the final refined predictions with high-resolution constraints. Extensive experiments on four different publically available datasets suggest that our proposed RCDT exhibits superior change detection performance compared with other competing methods. The therotical, methodogical, and experimental knowledge of this study is expected to benefit future change detection efforts that involve the cross attention mechanism.



### Spurious Features Everywhere -- Large-Scale Detection of Harmful Spurious Features in ImageNet
- **Arxiv ID**: http://arxiv.org/abs/2212.04871v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.04871v2)
- **Published**: 2022-12-09 14:23:25+00:00
- **Updated**: 2023-08-22 15:00:06+00:00
- **Authors**: Yannic Neuhaus, Maximilian Augustin, Valentyn Boreiko, Matthias Hein
- **Comment**: accepted to ICCV 2023
- **Journal**: None
- **Summary**: Benchmark performance of deep learning classifiers alone is not a reliable predictor for the performance of a deployed model. In particular, if the image classifier has picked up spurious features in the training data, its predictions can fail in unexpected ways. In this paper, we develop a framework that allows us to systematically identify spurious features in large datasets like ImageNet. It is based on our neural PCA components and their visualization. Previous work on spurious features often operates in toy settings or requires costly pixel-wise annotations. In contrast, we work with ImageNet and validate our results by showing that presence of the harmful spurious feature of a class alone is sufficient to trigger the prediction of that class. We introduce the novel dataset "Spurious ImageNet" which allows to measure the reliance of any ImageNet classifier on harmful spurious features. Moreover, we introduce SpuFix as a simple mitigation method to reduce the dependence of any ImageNet classifier on previously identified harmful spurious features without requiring additional labels or retraining of the model. We provide code and data at https://github.com/YanNeu/spurious_imagenet .



### Multimodal Prototype-Enhanced Network for Few-Shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2212.04873v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.04873v1)
- **Published**: 2022-12-09 14:24:39+00:00
- **Updated**: 2022-12-09 14:24:39+00:00
- **Authors**: Xinzhe Ni, Hao Wen, Yong Liu, Yatai Ji, Yujiu Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Current methods for few-shot action recognition mainly fall into the metric learning framework following ProtoNet. However, they either ignore the effect of representative prototypes or fail to enhance the prototypes with multimodal information adequately. In this work, we propose a novel Multimodal Prototype-Enhanced Network (MORN) to use the semantic information of label texts as multimodal information to enhance prototypes, including two modality flows. A CLIP visual encoder is introduced in the visual flow, and visual prototypes are computed by the Temporal-Relational CrossTransformer (TRX) module. A frozen CLIP text encoder is introduced in the text flow, and a semantic-enhanced module is used to enhance text features. After inflating, text prototypes are obtained. The final multimodal prototypes are then computed by a multimodal prototype-enhanced module. Besides, there exist no evaluation metrics to evaluate the quality of prototypes. To the best of our knowledge, we are the first to propose a prototype evaluation metric called Prototype Similarity Difference (PRIDE), which is used to evaluate the performance of prototypes in discriminating different categories. We conduct extensive experiments on four popular datasets. MORN achieves state-of-the-art results on HMDB51, UCF101, Kinetics and SSv2. MORN also performs well on PRIDE, and we explore the correlation between PRIDE and accuracy.



### Expeditious Saliency-guided Mix-up through Random Gradient Thresholding
- **Arxiv ID**: http://arxiv.org/abs/2212.04875v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.04875v3)
- **Published**: 2022-12-09 14:29:57+00:00
- **Updated**: 2023-08-10 21:05:54+00:00
- **Authors**: Minh-Long Luu, Zeyi Huang, Eric P. Xing, Yong Jae Lee, Haohan Wang
- **Comment**: Accepted Long paper at 2nd Practical-DL Workshop at AAAI 2023
- **Journal**: None
- **Summary**: Mix-up training approaches have proven to be effective in improving the generalization ability of Deep Neural Networks. Over the years, the research community expands mix-up methods into two directions, with extensive efforts to improve saliency-guided procedures but minimal focus on the arbitrary path, leaving the randomization domain unexplored. In this paper, inspired by the superior qualities of each direction over one another, we introduce a novel method that lies at the junction of the two routes. By combining the best elements of randomness and saliency utilization, our method balances speed, simplicity, and accuracy. We name our method R-Mix following the concept of "Random Mix-up". We demonstrate its effectiveness in generalization, weakly supervised object localization, calibration, and robustness to adversarial attacks. Finally, in order to address the question of whether there exists a better decision protocol, we train a Reinforcement Learning agent that decides the mix-up policies based on the classifier's performance, reducing dependency on human-designed objectives and hyperparameter tuning. Extensive experiments further show that the agent is capable of performing at the cutting-edge level, laying the foundation for a fully automatic mix-up. Our code is released at [https://github.com/minhlong94/Random-Mixup].



### Co-training $2^L$ Submodels for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2212.04884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04884v1)
- **Published**: 2022-12-09 14:38:09+00:00
- **Updated**: 2022-12-09 14:38:09+00:00
- **Authors**: Hugo Touvron, Matthieu Cord, Maxime Oquab, Piotr Bojanowski, Jakob Verbeek, Hervé Jégou
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce submodel co-training, a regularization method related to co-training, self-distillation and stochastic depth. Given a neural network to be trained, for each sample we implicitly instantiate two altered networks, ``submodels'', with stochastic depth: we activate only a subset of the layers. Each network serves as a soft teacher to the other, by providing a loss that complements the regular loss provided by the one-hot label. Our approach, dubbed cosub, uses a single set of weights, and does not involve a pre-trained external model or temporal averaging.   Experimentally, we show that submodel co-training is effective to train backbones for recognition tasks such as image classification and semantic segmentation. Our approach is compatible with multiple architectures, including RegNet, ViT, PiT, XCiT, Swin and ConvNext. Our training strategy improves their results in comparable settings. For instance, a ViT-B pretrained with cosub on ImageNet-21k obtains 87.4% top-1 acc. @448 on ImageNet-val.



### AP: Selective Activation for De-sparsifying Pruned Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2212.06145v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.06145v1)
- **Published**: 2022-12-09 14:49:15+00:00
- **Updated**: 2022-12-09 14:49:15+00:00
- **Authors**: Shiyu Liu, Rohan Ghosh, Dylan Tan, Mehul Motani
- **Comment**: 16 Pages
- **Journal**: None
- **Summary**: The rectified linear unit (ReLU) is a highly successful activation function in neural networks as it allows networks to easily obtain sparse representations, which reduces overfitting in overparameterized networks. However, in network pruning, we find that the sparsity introduced by ReLU, which we quantify by a term called dynamic dead neuron rate (DNR), is not beneficial for the pruned network. Interestingly, the more the network is pruned, the smaller the dynamic DNR becomes during optimization. This motivates us to propose a method to explicitly reduce the dynamic DNR for the pruned network, i.e., de-sparsify the network. We refer to our method as Activating-while-Pruning (AP). We note that AP does not function as a stand-alone method, as it does not evaluate the importance of weights. Instead, it works in tandem with existing pruning methods and aims to improve their performance by selective activation of nodes to reduce the dynamic DNR. We conduct extensive experiments using popular networks (e.g., ResNet, VGG) via two classical and three state-of-the-art pruning methods. The experimental results on public datasets (e.g., CIFAR-10/100) suggest that AP works well with existing pruning methods and improves the performance by 3% - 4%. For larger scale datasets (e.g., ImageNet) and state-of-the-art networks (e.g., vision transformer), we observe an improvement of 2% - 3% with AP as opposed to without. Lastly, we conduct an ablation study to examine the effectiveness of the components comprising AP.



### An AI-Powered VVPAT Counter for Elections in India
- **Arxiv ID**: http://arxiv.org/abs/2212.11124v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.11124v1)
- **Published**: 2022-12-09 14:59:40+00:00
- **Updated**: 2022-12-09 14:59:40+00:00
- **Authors**: Prasath Murugesan, Shamshu Dharwez Saganvali
- **Comment**: 4 pages, 4 figures
- **Journal**: None
- **Summary**: The Election Commission of India has introduced Voter Verified Paper Audit Trail since 2019. This mechanism has increased voter confidence at the time of casting the votes. However, physical verification of the VVPATs against the party level counts from the EVMs is done only in 5 (randomly selected) machines per constituency. The time required to conduct physical verification becomes a bottleneck in scaling this activity for 100% of machines in all constituencies. We proposed an automated counter powered by image processing and machine learning algorithms to speed up the process and address this issue.



### PACMAN: a framework for pulse oximeter digit detection and reading in a low-resource setting
- **Arxiv ID**: http://arxiv.org/abs/2212.04964v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.04964v1)
- **Published**: 2022-12-09 16:22:28+00:00
- **Updated**: 2022-12-09 16:22:28+00:00
- **Authors**: Chiraphat Boonnag, Wanumaidah Saengmolee, Narongrid Seesawad, Amrest Chinkamol, Saendee Rattanasomrerk, Kanyakorn Veerakanjana, Kamonwan Thanontip, Warissara Limpornchitwilai, Piyalitt Ittichaiwong, Theerawit Wilaiprasitporn
- **Comment**: None
- **Journal**: None
- **Summary**: In light of the COVID-19 pandemic, patients were required to manually input their daily oxygen saturation (SpO2) and pulse rate (PR) values into a health monitoring system-unfortunately, such a process trend to be an error in typing. Several studies attempted to detect the physiological value from the captured image using optical character recognition (OCR). However, the technology has limited availability with high cost. Thus, this study aimed to propose a novel framework called PACMAN (Pandemic Accelerated Human-Machine Collaboration) with a low-resource deep learning-based computer vision. We compared state-of-the-art object detection algorithms (scaled YOLOv4, YOLOv5, and YOLOR), including the commercial OCR tools for digit recognition on the captured images from pulse oximeter display. All images were derived from crowdsourced data collection with varying quality and alignment. YOLOv5 was the best-performing model against the given model comparison across all datasets, notably the correctly orientated image dataset. We further improved the model performance with the digits auto-orientation algorithm and applied a clustering algorithm to extract SpO2 and PR values. The accuracy performance of YOLOv5 with the implementations was approximately 81.0-89.5%, which was enhanced compared to without any additional implementation. Accordingly, this study highlighted the completion of PACMAN framework to detect and read digits in real-world datasets. The proposed framework has been currently integrated into the patient monitoring system utilized by hospitals nationwide.



### Seeing a Rose in Five Thousand Ways
- **Arxiv ID**: http://arxiv.org/abs/2212.04965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04965v1)
- **Published**: 2022-12-09 16:24:37+00:00
- **Updated**: 2022-12-09 16:24:37+00:00
- **Authors**: Yunzhi Zhang, Shangzhe Wu, Noah Snavely, Jiajun Wu
- **Comment**: Project page: https://cs.stanford.edu/~yzzhang/projects/rose/
- **Journal**: None
- **Summary**: What is a rose, visually? A rose comprises its intrinsics, including the distribution of geometry, texture, and material specific to its object category. With knowledge of these intrinsic properties, we may render roses of different sizes and shapes, in different poses, and under different lighting conditions. In this work, we build a generative model that learns to capture such object intrinsics from a single image, such as a photo of a bouquet. Such an image includes multiple instances of an object type. These instances all share the same intrinsics, but appear different due to a combination of variance within these intrinsics and differences in extrinsic factors, such as pose and illumination. Experiments show that our model successfully learns object intrinsics (distribution of geometry, texture, and material) for a wide range of objects, each from a single Internet image. Our method achieves superior results on multiple downstream tasks, including intrinsic image decomposition, shape and image generation, view synthesis, and relighting.



### SupeRVol: Super-Resolution Shape and Reflectance Estimation in Inverse Volume Rendering
- **Arxiv ID**: http://arxiv.org/abs/2212.04968v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04968v2)
- **Published**: 2022-12-09 16:30:17+00:00
- **Updated**: 2023-03-16 17:35:55+00:00
- **Authors**: Mohammed Brahimi, Bjoern Haefner, Tarun Yenamandra, Bastian Goldluecke, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an end-to-end inverse rendering pipeline called SupeRVol that allows us to recover 3D shape and material parameters from a set of color images in a super-resolution manner. To this end, we represent both the bidirectional reflectance distribution function (BRDF) and the signed distance function (SDF) by multi-layer perceptrons. In order to obtain both the surface shape and its reflectance properties, we revert to a differentiable volume renderer with a physically based illumination model that allows us to decouple reflectance and lighting. This physical model takes into account the effect of the camera's point spread function thereby enabling a reconstruction of shape and material in a super-resolution quality. Experimental validation confirms that SupeRVol achieves state of the art performance in terms of inverse rendering quality. It generates reconstructions that are sharper than the individual input images, making this method ideally suited for 3D modeling from low-resolution imagery.



### Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers
- **Arxiv ID**: http://arxiv.org/abs/2212.04970v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2212.04970v1)
- **Published**: 2022-12-09 16:32:46+00:00
- **Updated**: 2022-12-09 16:32:46+00:00
- **Authors**: Yasheng Sun, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Zhibin Hong, Jingtuo Liu, Errui Ding, Jingdong Wang, Ziwei Liu, Hideki Koike
- **Comment**: Accepted to SIGGRAPH Asia 2022 (Conference Proceedings). Project
  page: https://hangz-nju-cuhk.github.io/projects/AV-CAT
- **Journal**: None
- **Summary**: Previous studies have explored generating accurately lip-synced talking faces for arbitrary targets given audio conditions. However, most of them deform or generate the whole facial area, leading to non-realistic results. In this work, we delve into the formulation of altering only the mouth shapes of the target person. This requires masking a large percentage of the original image and seamlessly inpainting it with the aid of audio and reference frames. To this end, we propose the Audio-Visual Context-Aware Transformer (AV-CAT) framework, which produces accurate lip-sync with photo-realistic quality by predicting the masked mouth shapes. Our key insight is to exploit desired contextual information provided in audio and visual modalities thoroughly with delicately designed Transformers. Specifically, we propose a convolution-Transformer hybrid backbone and design an attention-based fusion strategy for filling the masked parts. It uniformly attends to the textural information on the unmasked regions and the reference frame. Then the semantic audio information is involved in enhancing the self-attention computation. Additionally, a refinement network with audio injection improves both image and lip-sync quality. Extensive experiments validate that our model can generate high-fidelity lip-synced results for arbitrary subjects.



### Augmentation Matters: A Simple-yet-Effective Approach to Semi-supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2212.04976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04976v1)
- **Published**: 2022-12-09 16:36:52+00:00
- **Updated**: 2022-12-09 16:36:52+00:00
- **Authors**: Zhen Zhao, Lihe Yang, Sifan Long, Jimin Pi, Luping Zhou, Jingdong Wang
- **Comment**: 10 pages, 8 tables
- **Journal**: None
- **Summary**: Recent studies on semi-supervised semantic segmentation (SSS) have seen fast progress. Despite their promising performance, current state-of-the-art methods tend to increasingly complex designs at the cost of introducing more network components and additional training procedures. Differently, in this work, we follow a standard teacher-student framework and propose AugSeg, a simple and clean approach that focuses mainly on data perturbations to boost the SSS performance. We argue that various data augmentations should be adjusted to better adapt to the semi-supervised scenarios instead of directly applying these techniques from supervised learning. Specifically, we adopt a simplified intensity-based augmentation that selects a random number of data transformations with uniformly sampling distortion strengths from a continuous space. Based on the estimated confidence of the model on different unlabeled samples, we also randomly inject labelled information to augment the unlabeled samples in an adaptive manner. Without bells and whistles, our simple AugSeg can readily achieve new state-of-the-art performance on SSS benchmarks under different partition protocols.



### VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners
- **Arxiv ID**: http://arxiv.org/abs/2212.04979v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2212.04979v3)
- **Published**: 2022-12-09 16:39:09+00:00
- **Updated**: 2023-03-15 06:48:23+00:00
- **Authors**: Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, Jiahui Yu
- **Comment**: Tech report. arXiv v3: update text
- **Journal**: None
- **Summary**: We explore an efficient approach to establish a foundational video-text model. We present VideoCoCa that maximally reuses a pretrained image-text contrastive captioner (CoCa) model and adapt it to video-text tasks with minimal extra training. While previous works adapt image-text models with various cross-frame fusion modules, we find that the generative attentional pooling and contrastive attentional pooling layers in CoCa are instantly adaptable to flattened frame embeddings, yielding state-of-the-art results on zero-shot video classification and zero-shot text-to-video retrieval. Furthermore, we explore lightweight finetuning on top of VideoCoCa, and achieve strong results on video question-answering and video captioning.



### LoopDraw: a Loop-Based Autoregressive Model for Shape Synthesis and Editing
- **Arxiv ID**: http://arxiv.org/abs/2212.04981v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.04981v1)
- **Published**: 2022-12-09 16:41:15+00:00
- **Updated**: 2022-12-09 16:41:15+00:00
- **Authors**: Nam Anh Dinh, Haochen Wang, Greg Shakhnarovich, Rana Hanocka
- **Comment**: None
- **Journal**: None
- **Summary**: There is no settled universal 3D representation for geometry with many alternatives such as point clouds, meshes, implicit functions, and voxels to name a few. In this work, we present a new, compelling alternative for representing shapes using a sequence of cross-sectional closed loops. The loops across all planes form an organizational hierarchy which we leverage for autoregressive shape synthesis and editing. Loops are a non-local description of the underlying shape, as simple loop manipulations (such as shifts) result in significant structural changes to the geometry. This is in contrast to manipulating local primitives such as points in a point cloud or a triangle in a triangle mesh. We further demonstrate that loops are intuitive and natural primitive for analyzing and editing shapes, both computationally and for users.



### Finger-NestNet: Interpretable Fingerphoto Verification on Smartphone using Deep Nested Residual Network
- **Arxiv ID**: http://arxiv.org/abs/2212.05884v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2212.05884v1)
- **Published**: 2022-12-09 17:15:35+00:00
- **Updated**: 2022-12-09 17:15:35+00:00
- **Authors**: Raghavendra Ramachandra, Hailin Li
- **Comment**: a preprint paper accepted in wacv2023 workshop
- **Journal**: None
- **Summary**: Fingerphoto images captured using a smartphone are successfully used to verify the individuals that have enabled several applications. This work presents a novel algorithm for fingerphoto verification using a nested residual block: Finger-NestNet. The proposed Finger-NestNet architecture is designed with three consecutive convolution blocks followed by a series of nested residual blocks to achieve reliable fingerphoto verification. This paper also presents the interpretability of the proposed method using four different visualization techniques that can shed light on the critical regions in the fingerphoto biometrics that can contribute to the reliable verification performance of the proposed method. Extensive experiments are performed on the fingerphoto dataset comprised of 196 unique fingers collected from 52 unique data subjects using an iPhone6S. Experimental results indicate the improved verification of the proposed method compared to six different existing methods with EER = 1.15%.



### Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2212.04994v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04994v1)
- **Published**: 2022-12-09 17:23:00+00:00
- **Updated**: 2022-12-09 17:23:00+00:00
- **Authors**: Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang, Ashish Shah, Philip H. S. Torr, Ser-Nam Lim
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Patch Aligned Contrastive Learning (PACL), a modified compatibility function for CLIP's contrastive loss, intending to train an alignment between the patch tokens of the vision encoder and the CLS token of the text encoder. With such an alignment, a model can identify regions of an image corresponding to a given text input, and therefore transfer seamlessly to the task of open vocabulary semantic segmentation without requiring any segmentation annotations during training. Using pre-trained CLIP encoders with PACL, we are able to set the state-of-the-art on the task of open vocabulary zero-shot segmentation on 4 different segmentation benchmarks: Pascal VOC, Pascal Context, COCO Stuff and ADE20K. Furthermore, we show that PACL is also applicable to image-level predictions and when used with a CLIP backbone, provides a general improvement in zero-shot classification accuracy compared to CLIP, across a suite of 12 image classification datasets.



### Audiovisual Masked Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2212.05922v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2212.05922v2)
- **Published**: 2022-12-09 17:34:53+00:00
- **Updated**: 2023-07-28 12:22:59+00:00
- **Authors**: Mariana-Iuliana Georgescu, Eduardo Fonseca, Radu Tudor Ionescu, Mario Lucic, Cordelia Schmid, Anurag Arnab
- **Comment**: ICCV 2023
- **Journal**: None
- **Summary**: Can we leverage the audiovisual information already present in video to improve self-supervised representation learning? To answer this question, we study various pretraining architectures and objectives within the masked autoencoding framework, motivated by the success of similar methods in natural language and image understanding. We show that we can achieve significant improvements on audiovisual downstream classification tasks, surpassing the state-of-the-art on VGGSound and AudioSet. Furthermore, we can leverage our audiovisual pretraining scheme for multiple unimodal downstream tasks using a single audiovisual pretrained model. We additionally demonstrate the transferability of our representations, achieving state-of-the-art audiovisual results on Epic Kitchens without pretraining specifically for this dataset.



### Memories are One-to-Many Mapping Alleviators in Talking Face Generation
- **Arxiv ID**: http://arxiv.org/abs/2212.05005v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2212.05005v2)
- **Published**: 2022-12-09 17:45:36+00:00
- **Updated**: 2022-12-12 07:32:57+00:00
- **Authors**: Anni Tang, Tianyu He, Xu Tan, Jun Ling, Runnan Li, Sheng Zhao, Li Song, Jiang Bian
- **Comment**: Project page: see https://memoryface.github.io
- **Journal**: None
- **Summary**: Talking face generation aims at generating photo-realistic video portraits of a target person driven by input audio. Due to its nature of one-to-many mapping from the input audio to the output video (e.g., one speech content may have multiple feasible visual appearances), learning a deterministic mapping like previous works brings ambiguity during training, and thus causes inferior visual results. Although this one-to-many mapping could be alleviated in part by a two-stage framework (i.e., an audio-to-expression model followed by a neural-rendering model), it is still insufficient since the prediction is produced without enough information (e.g., emotions, wrinkles, etc.). In this paper, we propose MemFace to complement the missing information with an implicit memory and an explicit memory that follow the sense of the two stages respectively. More specifically, the implicit memory is employed in the audio-to-expression model to capture high-level semantics in the audio-expression shared space, while the explicit memory is employed in the neural-rendering model to help synthesize pixel-level details. Our experimental results show that our proposed MemFace surpasses all the state-of-the-art results across multiple scenarios consistently and significantly.



### LADIS: Language Disentanglement for 3D Shape Editing
- **Arxiv ID**: http://arxiv.org/abs/2212.05011v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2212.05011v1)
- **Published**: 2022-12-09 17:54:28+00:00
- **Updated**: 2022-12-09 17:54:28+00:00
- **Authors**: Ian Huang, Panos Achlioptas, Tianyi Zhang, Sergey Tulyakov, Minhyuk Sung, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: Natural language interaction is a promising direction for democratizing 3D shape design. However, existing methods for text-driven 3D shape editing face challenges in producing decoupled, local edits to 3D shapes. We address this problem by learning disentangled latent representations that ground language in 3D geometry. To this end, we propose a complementary tool set including a novel network architecture, a disentanglement loss, and a new editing procedure. Additionally, to measure edit locality, we define a new metric that we call part-wise edit precision. We show that our method outperforms existing SOTA methods by 20% in terms of edit locality, and up to 6.6% in terms of language reference resolution accuracy. Our work suggests that by solely disentangling language representations, downstream 3D shape editing can become more local to relevant parts, even if the model was never given explicit part-based supervision.



### Mesh Neural Networks for SE(3)-Equivariant Hemodynamics Estimation on the Artery Wall
- **Arxiv ID**: http://arxiv.org/abs/2212.05023v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.GR, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/2212.05023v1)
- **Published**: 2022-12-09 18:16:06+00:00
- **Updated**: 2022-12-09 18:16:06+00:00
- **Authors**: Julian Suk, Pim de Haan, Phillip Lippe, Christoph Brune, Jelmer M. Wolterink
- **Comment**: Preprint. Under Review
- **Journal**: None
- **Summary**: Computational fluid dynamics (CFD) is a valuable asset for patient-specific cardiovascular-disease diagnosis and prognosis, but its high computational demands hamper its adoption in practice. Machine-learning methods that estimate blood flow in individual patients could accelerate or replace CFD simulation to overcome these limitations. In this work, we consider the estimation of vector-valued quantities on the wall of three-dimensional geometric artery models. We employ group-equivariant graph convolution in an end-to-end SE(3)-equivariant neural network that operates directly on triangular surface meshes and makes efficient use of training data. We run experiments on a large dataset of synthetic coronary arteries and find that our method estimates directional wall shear stress (WSS) with an approximation error of 7.6% and normalised mean absolute error (NMAE) of 0.4% while up to two orders of magnitude faster than CFD. Furthermore, we show that our method is powerful enough to accurately predict transient, vector-valued WSS over the cardiac cycle while conditioned on a range of different inflow boundary conditions. These results demonstrate the potential of our proposed method as a plugin replacement for CFD in the personalised prediction of hemodynamic vector and scalar fields.



### Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2212.05032v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2212.05032v3)
- **Published**: 2022-12-09 18:30:24+00:00
- **Updated**: 2023-02-28 23:46:24+00:00
- **Authors**: Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, William Yang Wang
- **Comment**: ICLR 2023 Camera Ready version
- **Journal**: None
- **Summary**: Large-scale diffusion models have achieved state-of-the-art results on text-to-image synthesis (T2I) tasks. Despite their ability to generate high-quality yet creative images, we observe that attribution-binding and compositional capabilities are still considered major challenging issues, especially when involving multiple objects. In this work, we improve the compositional skills of T2I models, specifically more accurate attribute binding and better image compositions. To do this, we incorporate linguistic structures with the diffusion guidance process based on the controllable properties of manipulating cross-attention layers in diffusion-based T2I models. We observe that keys and values in cross-attention layers have strong semantic meanings associated with object layouts and content. Therefore, we can better preserve the compositional semantics in the generated image by manipulating the cross-attention representations based on linguistic insights. Built upon Stable Diffusion, a SOTA T2I model, our structured cross-attention design is efficient that requires no additional training samples. We achieve better compositional skills in qualitative and quantitative results, leading to a 5-8% advantage in head-to-head user comparison studies. Lastly, we conduct an in-depth analysis to reveal potential causes of incorrect image compositions and justify the properties of cross-attention layers in the generation process.



### SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model
- **Arxiv ID**: http://arxiv.org/abs/2212.05034v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05034v1)
- **Published**: 2022-12-09 18:36:13+00:00
- **Updated**: 2022-12-09 18:36:13+00:00
- **Authors**: Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, Kun Zhang
- **Comment**: None
- **Journal**: CVPR2023
- **Summary**: Generic image inpainting aims to complete a corrupted image by borrowing surrounding information, which barely generates novel content. By contrast, multi-modal inpainting provides more flexible and useful controls on the inpainted content, \eg, a text prompt can be used to describe an object with richer attributes, and a mask can be used to constrain the shape of the inpainted object rather than being only considered as a missing area. We propose a new diffusion-based model named SmartBrush for completing a missing region with an object using both text and shape-guidance. While previous work such as DALLE-2 and Stable Diffusion can do text-guided inapinting they do not support shape guidance and tend to modify background texture surrounding the generated object. Our model incorporates both text and shape guidance with precision control. To preserve the background better, we propose a novel training and sampling strategy by augmenting the diffusion U-net with object-mask prediction. Lastly, we introduce a multi-task training strategy by jointly training inpainting with text-to-image generation to leverage more training data. We conduct extensive experiments showing that our model outperforms all baselines in terms of visual quality, mask controllability, and background preservation.



### OmniHorizon: In-the-Wild Outdoors Depth and Normal Estimation from Synthetic Omnidirectional Dataset
- **Arxiv ID**: http://arxiv.org/abs/2212.05040v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.05040v2)
- **Published**: 2022-12-09 18:40:12+00:00
- **Updated**: 2023-01-09 11:48:19+00:00
- **Authors**: Jay Bhanushali, Praneeth Chakravarthula, Manivannan Muniyandi
- **Comment**: Fixed the overlapping text in caption for Figure 9 in supplementary
  section
- **Journal**: None
- **Summary**: Understanding the ambient scene is imperative for several applications such as autonomous driving and navigation. While obtaining real-world image data with per-pixel labels is challenging, existing accurate synthetic image datasets primarily focus on indoor spaces with fixed lighting and scene participants, thereby severely limiting their application to outdoor scenarios. In this work we introduce OmniHorizon, a synthetic dataset with 24,335 omnidirectional views comprising of a broad range of indoor and outdoor spaces consisting of buildings, streets, and diverse vegetation. Our dataset also accounts for dynamic scene components including lighting, different times of a day settings, pedestrians, and vehicles. Furthermore, we also demonstrate a learned synthetic-to-real cross-domain inference method for in-the-wild 3D scene depth and normal estimation method using our dataset. To this end, we propose UBotNet, an architecture based on a UNet and a Bottleneck Transformer, to estimate scene-consistent normals. We show that UBotNet achieves significantly improved depth accuracy (4.6%) and normal estimation (5.75%) compared to several existing networks such as U-Net with skip-connections. Finally, we demonstrate in-the-wild depth and normal estimation on real-world images with UBotNet trained purely on our OmniHorizon dataset, showing the promise of proposed dataset and network for scene understanding.



### VindLU: A Recipe for Effective Video-and-Language Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2212.05051v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05051v2)
- **Published**: 2022-12-09 18:54:05+00:00
- **Updated**: 2023-04-05 17:56:15+00:00
- **Authors**: Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit Bansal, Gedas Bertasius
- **Comment**: CVPR 2023. Project page: https://klauscc.github.io/vindlu.html
- **Journal**: None
- **Summary**: The last several years have witnessed remarkable progress in video-and-language (VidL) understanding. However, most modern VidL approaches use complex and specialized model architectures and sophisticated pretraining protocols, making the reproducibility, analysis and comparisons of these frameworks difficult. Hence, instead of proposing yet another new VidL model, this paper conducts a thorough empirical study demystifying the most important factors in the VidL model design. Among the factors that we investigate are (i) the spatiotemporal architecture design, (ii) the multimodal fusion schemes, (iii) the pretraining objectives, (iv) the choice of pretraining data, (v) pretraining and finetuning protocols, and (vi) dataset and model scaling. Our empirical study reveals that the most important design factors include: temporal modeling, video-to-text multimodal fusion, masked modeling objectives, and joint training on images and videos. Using these empirical insights, we then develop a step-by-step recipe, dubbed VindLU, for effective VidL pretraining. Our final model trained using our recipe achieves comparable or better than state-of-the-art results on several VidL tasks without relying on external CLIP pretraining. In particular, on the text-to-video retrieval task, our approach obtains 61.2% on DiDeMo, and 55.0% on ActivityNet, outperforming current SOTA by 7.8% and 6.1% respectively. Furthermore, our model also obtains state-of-the-art video question-answering results on ActivityNet-QA, MSRVTT-QA, MSRVTT-MC and TVQA. Our code and pretrained models are publicly available at: https://github.com/klauscc/VindLU.



### Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints
- **Arxiv ID**: http://arxiv.org/abs/2212.05055v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.05055v2)
- **Published**: 2022-12-09 18:57:37+00:00
- **Updated**: 2023-02-17 17:54:50+00:00
- **Authors**: Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, Neil Houlsby
- **Comment**: None
- **Journal**: None
- **Summary**: Training large, deep neural networks to convergence can be prohibitively expensive. As a result, often only a small selection of popular, dense models are reused across different contexts and tasks. Increasingly, sparsely activated models, which seek to decouple model size from computation costs, are becoming an attractive alternative to dense models. Although more efficient in terms of quality and computation cost, sparse models remain data-hungry and costly to train from scratch in the large scale regime. In this work, we propose sparse upcycling -- a simple way to reuse sunk training costs by initializing a sparsely activated Mixture-of-Experts model from a dense checkpoint. We show that sparsely upcycled T5 Base, Large, and XL language models and Vision Transformer Base and Large models, respectively, significantly outperform their dense counterparts on SuperGLUE and ImageNet, using only ~50% of the initial dense pretraining sunk cost. The upcycled models also outperform sparse models trained from scratch on 100% of the initial dense pretraining computation budget.



### A Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others
- **Arxiv ID**: http://arxiv.org/abs/2212.04825v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.04825v2)
- **Published**: 2022-12-09 18:59:57+00:00
- **Updated**: 2023-03-21 17:13:58+00:00
- **Authors**: Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner, Cristian Canton Ferrer, Chenliang Xu, Mark Ibrahim
- **Comment**: CVPR 2023. Code is available at
  https://github.com/facebookresearch/Whac-A-Mole
- **Journal**: None
- **Summary**: Machine learning models have been found to learn shortcuts -- unintended decision rules that are unable to generalize -- undermining models' reliability. Previous works address this problem under the tenuous assumption that only a single shortcut exists in the training data. Real-world images are rife with multiple visual cues from background to texture. Key to advancing the reliability of vision systems is understanding whether existing methods can overcome multiple shortcuts or struggle in a Whac-A-Mole game, i.e., where mitigating one shortcut amplifies reliance on others. To address this shortcoming, we propose two benchmarks: 1) UrbanCars, a dataset with precisely controlled spurious cues, and 2) ImageNet-W, an evaluation set based on ImageNet for watermark, a shortcut we discovered affects nearly every modern vision model. Along with texture and background, ImageNet-W allows us to study multiple shortcuts emerging from training on natural images. We find computer vision models, including large foundation models -- regardless of training set, architecture, and supervision -- struggle when multiple shortcuts are present. Even methods explicitly designed to combat shortcuts struggle in a Whac-A-Mole dilemma. To tackle this challenge, we propose Last Layer Ensemble, a simple-yet-effective method to mitigate multiple shortcuts without Whac-A-Mole behavior. Our results surface multi-shortcut mitigation as an overlooked challenge critical to advancing the reliability of vision systems. The datasets and code are released: https://github.com/facebookresearch/Whac-A-Mole.



### A soft nearest-neighbor framework for continual semi-supervised learning
- **Arxiv ID**: http://arxiv.org/abs/2212.05102v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.05102v2)
- **Published**: 2022-12-09 20:03:59+00:00
- **Updated**: 2023-04-05 13:29:13+00:00
- **Authors**: Zhiqi Kang, Enrico Fini, Moin Nabi, Elisa Ricci, Karteek Alahari
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Despite significant advances, the performance of state-of-the-art continual learning approaches hinges on the unrealistic scenario of fully labeled data. In this paper, we tackle this challenge and propose an approach for continual semi-supervised learning--a setting where not all the data samples are labeled. A primary issue in this scenario is the model forgetting representations of unlabeled data and overfitting the labeled samples. We leverage the power of nearest-neighbor classifiers to nonlinearly partition the feature space and flexibly model the underlying data distribution thanks to its non-parametric nature. This enables the model to learn a strong representation for the current task, and distill relevant information from previous tasks. We perform a thorough experimental evaluation and show that our method outperforms all the existing approaches by large margins, setting a solid state of the art on the continual semi-supervised learning paradigm. For example, on CIFAR-100 we surpass several others even when using at least 30 times less supervision (0.8% vs. 25% of annotations). Finally, our method works well on both low and high resolution images and scales seamlessly to more complex datasets such as ImageNet-100. The code is publicly available on https://github.com/kangzhiq/NNCSL



### Leveraging Contextual Data Augmentation for Generalizable Melanoma Detection
- **Arxiv ID**: http://arxiv.org/abs/2212.05116v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.05116v3)
- **Published**: 2022-12-09 20:45:09+00:00
- **Updated**: 2023-08-09 08:55:18+00:00
- **Authors**: Nick DiSanto, Gavin Harding, Ethan Martinez, Benjamin Sanders
- **Comment**: 6 pages, 3 figures, 4 tables
- **Journal**: None
- **Summary**: While skin cancer detection has been a valuable deep learning application for years, its evaluation has often neglected the context in which testing images are assessed. Traditional melanoma classifiers assume that their testing environments are comparable to the structured images they are trained on. This paper challenges this notion and argues that mole size, a critical attribute in professional dermatology, can be misleading in automated melanoma detection. While malignant melanomas tend to be larger than benign melanomas, relying solely on size can be unreliable and even harmful when contextual scaling of images is not possible. To address this issue, this implementation proposes a custom model that performs various data augmentation procedures to prevent overfitting to incorrect parameters and simulate real-world usage of melanoma detection applications. Multiple custom models employing different forms of data augmentation are implemented to highlight the most significant features of mole classifiers. These implementations emphasize the importance of considering user unpredictability when deploying such applications. The caution required when manually modifying data is acknowledged, as it can result in data loss and biased conclusions. Additionally, the significance of data augmentation in both the dermatology and deep learning communities is considered.



### All-in-One: A Highly Representative DNN Pruning Framework for Edge Devices with Dynamic Power Management
- **Arxiv ID**: http://arxiv.org/abs/2212.05122v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.05122v1)
- **Published**: 2022-12-09 21:30:52+00:00
- **Updated**: 2022-12-09 21:30:52+00:00
- **Authors**: Yifan Gong, Zheng Zhan, Pu Zhao, Yushu Wu, Chao Wu, Caiwen Ding, Weiwen Jiang, Minghai Qin, Yanzhi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: During the deployment of deep neural networks (DNNs) on edge devices, many research efforts are devoted to the limited hardware resource. However, little attention is paid to the influence of dynamic power management. As edge devices typically only have a budget of energy with batteries (rather than almost unlimited energy support on servers or workstations), their dynamic power management often changes the execution frequency as in the widely-used dynamic voltage and frequency scaling (DVFS) technique. This leads to highly unstable inference speed performance, especially for computation-intensive DNN models, which can harm user experience and waste hardware resources. We firstly identify this problem and then propose All-in-One, a highly representative pruning framework to work with dynamic power management using DVFS. The framework can use only one set of model weights and soft masks (together with other auxiliary parameters of negligible storage) to represent multiple models of various pruning ratios. By re-configuring the model to the corresponding pruning ratio for a specific execution frequency (and voltage), we are able to achieve stable inference speed, i.e., keeping the difference in speed performance under various execution frequencies as small as possible. Our experiments demonstrate that our method not only achieves high accuracy for multiple models of different pruning ratios, but also reduces their variance of inference latency for various frequencies, with minimal memory consumption of only one model and one soft mask.



### CLIP-TSA: CLIP-Assisted Temporal Self-Attention for Weakly-Supervised Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2212.05136v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.05136v3)
- **Published**: 2022-12-09 22:28:24+00:00
- **Updated**: 2023-07-03 23:03:22+00:00
- **Authors**: Hyekang Kevin Joo, Khoa Vo, Kashu Yamazaki, Ngan Le
- **Comment**: Published at the 30th IEEE International Conference on Image
  Processing (IEEE ICIP 2023)
- **Journal**: None
- **Summary**: Video anomaly detection (VAD) -- commonly formulated as a multiple-instance learning problem in a weakly-supervised manner due to its labor-intensive nature -- is a challenging problem in video surveillance where the frames of anomaly need to be localized in an untrimmed video. In this paper, we first propose to utilize the ViT-encoded visual features from CLIP, in contrast with the conventional C3D or I3D features in the domain, to efficiently extract discriminative representations in the novel technique. We then model temporal dependencies and nominate the snippets of interest by leveraging our proposed Temporal Self-Attention (TSA). The ablation study confirms the effectiveness of TSA and ViT feature. The extensive experiments show that our proposed CLIP-TSA outperforms the existing state-of-the-art (SOTA) methods by a large margin on three commonly-used benchmark datasets in the VAD problem (UCF-Crime, ShanghaiTech Campus, and XD-Violence). Our source code is available at https://github.com/joos2010kj/CLIP-TSA.



### Local Neighborhood Features for 3D Classification
- **Arxiv ID**: http://arxiv.org/abs/2212.05140v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2212.05140v1)
- **Published**: 2022-12-09 22:53:40+00:00
- **Updated**: 2022-12-09 22:53:40+00:00
- **Authors**: Shivanand Venkanna Sheshappanavar, Chandra Kambhamettu
- **Comment**: None
- **Journal**: None
- **Summary**: With advances in deep learning model training strategies, the training of Point cloud classification methods is significantly improving. For example, PointNeXt, which adopts prominent training techniques and InvResNet layers into PointNet++, achieves over 7% improvement on the real-world ScanObjectNN dataset. However, most of these models use point coordinates features of neighborhood points mapped to higher dimensional space while ignoring the neighborhood point features computed before feeding to the network layers. In this paper, we revisit the PointNeXt model to study the usage and benefit of such neighborhood point features. We train and evaluate PointNeXt on ModelNet40 (synthetic), ScanObjectNN (real-world), and a recent large-scale, real-world grocery dataset, i.e., 3DGrocery100. In addition, we provide an additional inference strategy of weight averaging the top two checkpoints of PointNeXt to improve classification accuracy. Together with the abovementioned ideas, we gain 0.5%, 1%, 4.8%, 3.4%, and 1.6% overall accuracy on the PointNeXt model with real-world datasets, ScanObjectNN (hardest variant), 3DGrocery100's Apple10, Fruits, Vegetables, and Packages subsets, respectively. We also achieve a comparable 0.2% accuracy gain on ModelNet40.



