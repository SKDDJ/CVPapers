# Arxiv Papers in cs.CV on 2022-12-26
### SMMix: Self-Motivated Image Mixing for Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2212.12977v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12977v2)
- **Published**: 2022-12-26 00:19:39+00:00
- **Updated**: 2023-03-17 02:52:25+00:00
- **Authors**: Mengzhao Chen, Mingbao Lin, ZhiHang Lin, Yuxin Zhang, Fei Chao, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: CutMix is a vital augmentation strategy that determines the performance and generalization ability of vision transformers (ViTs). However, the inconsistency between the mixed images and the corresponding labels harms its efficacy. Existing CutMix variants tackle this problem by generating more consistent mixed images or more precise mixed labels, but inevitably introduce heavy training overhead or require extra information, undermining ease of use. To this end, we propose an novel and effective Self-Motivated image Mixing method (SMMix), which motivates both image and label enhancement by the model under training itself. Specifically, we propose a max-min attention region mixing approach that enriches the attention-focused objects in the mixed images. Then, we introduce a fine-grained label assignment technique that co-trains the output tokens of mixed images with fine-grained supervision. Moreover, we devise a novel feature consistency constraint to align features from mixed and unmixed images. Due to the subtle designs of the self-motivated paradigm, our SMMix is significant in its smaller training overhead and better performance than other CutMix variants. In particular, SMMix improves the accuracy of DeiT-T/S/B, CaiT-XXS-24/36, and PVT-T/S/M/L by more than +1% on ImageNet-1k. The generalization capability of our method is also demonstrated on downstream tasks and out-of-distribution datasets. Our project is anonymously available at https://github.com/ChenMnZ/SMMix.



### Application of Unsupervised Domain Adaptation for Structural MRI Analysis
- **Arxiv ID**: http://arxiv.org/abs/2212.12986v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.12986v1)
- **Published**: 2022-12-26 01:59:56+00:00
- **Updated**: 2022-12-26 01:59:56+00:00
- **Authors**: Pranath Reddy
- **Comment**: None
- **Journal**: None
- **Summary**: The primary goal of this work is to study the effectiveness of an unsupervised domain adaptation approach for various applications such as binary classification and anomaly detection in the context of Alzheimer's disease (AD) detection for the OASIS datasets. We also explore image reconstruction and image synthesis for analyzing and generating 3D structural MRI data to establish performance benchmarks for anomaly detection. We successfully demonstrate that domain adaptation improves the performance of AD detection when implemented in both supervised and unsupervised settings. Additionally, the proposed methodology achieves state-of-the-art performance for binary classification on the OASIS-1 dataset.



### Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/2212.12990v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.12990v3)
- **Published**: 2022-12-26 02:37:38+00:00
- **Updated**: 2023-03-01 04:35:39+00:00
- **Authors**: Zijian Zhang, Zhou Zhao, Zhijie Lin
- **Comment**: Accepted by NeurIPS 2022 Conference
- **Journal**: None
- **Summary**: Diffusion Probabilistic Models (DPMs) have shown a powerful capacity of generating high-quality image samples. Recently, diffusion autoencoders (Diff-AE) have been proposed to explore DPMs for representation learning via autoencoding. Their key idea is to jointly train an encoder for discovering meaningful representations from images and a conditional DPM as the decoder for reconstructing images. Considering that training DPMs from scratch will take a long time and there have existed numerous pre-trained DPMs, we propose \textbf{P}re-trained \textbf{D}PM \textbf{A}uto\textbf{E}ncoding (\textbf{PDAE}), a general method to adapt existing pre-trained DPMs to the decoders for image reconstruction, with better training efficiency and performance than Diff-AE. Specifically, we find that the reason that pre-trained DPMs fail to reconstruct an image from its latent variables is due to the information loss of forward process, which causes a gap between their predicted posterior mean and the true one. From this perspective, the classifier-guided sampling method can be explained as computing an extra mean shift to fill the gap, reconstructing the lost class information in samples. These imply that the gap corresponds to the lost information of the image, and we can reconstruct the image by filling the gap. Drawing inspiration from this, we employ a trainable model to predict a mean shift according to encoded representation and train it to fill as much gap as possible, in this way, the encoder is forced to learn as much information as possible from images to help the filling. By reusing a part of network of pre-trained DPMs and redesigning the weighting scheme of diffusion loss, PDAE can learn meaningful representations from images efficiently. Extensive experiments demonstrate the effectiveness, efficiency and flexibility of PDAE.



### Simultaneously Optimizing Perturbations and Positions for Black-box Adversarial Patch Attacks
- **Arxiv ID**: http://arxiv.org/abs/2212.12995v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.12995v1)
- **Published**: 2022-12-26 02:48:37+00:00
- **Updated**: 2022-12-26 02:48:37+00:00
- **Authors**: Xingxing Wei, Ying Guo, Jie Yu, Bo Zhang
- **Comment**: Accepted by TPAMI 2022
- **Journal**: None
- **Summary**: Adversarial patch is an important form of real-world adversarial attack that brings serious risks to the robustness of deep neural networks. Previous methods generate adversarial patches by either optimizing their perturbation values while fixing the pasting position or manipulating the position while fixing the patch's content. This reveals that the positions and perturbations are both important to the adversarial attack. For that, in this paper, we propose a novel method to simultaneously optimize the position and perturbation for an adversarial patch, and thus obtain a high attack success rate in the black-box setting. Technically, we regard the patch's position, the pre-designed hyper-parameters to determine the patch's perturbations as the variables, and utilize the reinforcement learning framework to simultaneously solve for the optimal solution based on the rewards obtained from the target model with a small number of queries. Extensive experiments are conducted on the Face Recognition (FR) task, and results on four representative FR models show that our method can significantly improve the attack success rate and query efficiency. Besides, experiments on the commercial FR service and physical environments confirm its practical application value. We also extend our method to the traffic sign recognition task to verify its generalization ability.



### Improving Continuous Sign Language Recognition with Consistency Constraints and Signer Removal
- **Arxiv ID**: http://arxiv.org/abs/2212.13023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13023v1)
- **Published**: 2022-12-26 06:38:34+00:00
- **Updated**: 2022-12-26 06:38:34+00:00
- **Authors**: Ronglai Zuo, Brian Mak
- **Comment**: None
- **Journal**: None
- **Summary**: Most deep-learning-based continuous sign language recognition (CSLR) models share a similar backbone consisting of a visual module, a sequential module, and an alignment module. However, due to limited training samples, a connectionist temporal classification loss may not train such CSLR backbones sufficiently. In this work, we propose three auxiliary tasks to enhance the CSLR backbones. The first task enhances the visual module, which is sensitive to the insufficient training problem, from the perspective of consistency. Specifically, since the information of sign languages is mainly included in signers' facial expressions and hand movements, a keypoint-guided spatial attention module is developed to enforce the visual module to focus on informative regions, i.e., spatial attention consistency. Second, noticing that both the output features of the visual and sequential modules represent the same sentence, to better exploit the backbone's power, a sentence embedding consistency constraint is imposed between the visual and sequential modules to enhance the representation power of both features. We name the CSLR model trained with the above auxiliary tasks as consistency-enhanced CSLR, which performs well on signer-dependent datasets in which all signers appear during both training and testing. To make it more robust for the signer-independent setting, a signer removal module based on feature disentanglement is further proposed to remove signer information from the backbone. Extensive ablation studies are conducted to validate the effectiveness of these auxiliary tasks. More remarkably, with a transformer-based backbone, our model achieves state-of-the-art or competitive performance on five benchmarks, PHOENIX-2014, PHOENIX-2014-T, PHOENIX-2014-SI, CSL, and CSL-Daily.



### Key Feature Replacement of In-Distribution Samples for Out-of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2301.13012v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13012v1)
- **Published**: 2022-12-26 07:01:23+00:00
- **Updated**: 2022-12-26 07:01:23+00:00
- **Authors**: Jaeyoung Kim, Seo Taek Kong, Dongbin Na, Kyu-Hwan Jung
- **Comment**: Accepted to the 37th AAAI Conference on Artificial Intelligence (AAAI
  2023) Main Track
- **Journal**: None
- **Summary**: Out-of-distribution (OOD) detection can be used in deep learning-based applications to reject outlier samples from being unreliably classified by deep neural networks. Learning to classify between OOD and in-distribution samples is difficult because data comprising the former is extremely diverse. It has been observed that an auxiliary OOD dataset is most effective in training a "rejection" network when its samples are semantically similar to in-distribution images. We first deduce that OOD images are perceived by a deep neural network to be semantically similar to in-distribution samples when they share a common background, as deep networks are observed to incorrectly classify such images with high confidence. We then propose a simple yet effective Key In-distribution feature Replacement BY inpainting (KIRBY) procedure that constructs a surrogate OOD dataset by replacing class-discriminative features of in-distribution samples with marginal background features. The procedure can be implemented using off-the-shelf vision algorithms, where each step within the algorithm is shown to make the surrogate data increasingly similar to in-distribution data. Design choices in each step are studied extensively, and an exhaustive comparison with state-of-the-art algorithms demonstrates KIRBY's competitiveness on various benchmarks.



### RFPose-OT: RF-Based 3D Human Pose Estimation via Optimal Transport Theory
- **Arxiv ID**: http://arxiv.org/abs/2301.13013v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2301.13013v1)
- **Published**: 2022-12-26 07:09:09+00:00
- **Updated**: 2022-12-26 07:09:09+00:00
- **Authors**: Cong Yu, Dongheng Zhang, Zhi Wu, Zhi Lu, Chunyang Xie, Yang Hu, Yan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel framework, i.e., RFPose-OT, to enable the 3D human pose estimation from Radio Frequency (RF) signals. Different from existing methods that predict human poses from RF signals on the signal level directly, we consider the structure difference between the RF signals and the human poses, propose to transform the RF signals to the pose domain on the feature level based on Optimal Transport (OT) theory, and generate human poses from the transformed features. To evaluate RFPose-OT, we build a radio system and a multi-view camera system to acquire the RF signal data and the ground-truth human poses. The experimental results in basic indoor environment, occlusion indoor environment, and outdoor environment, all demonstrate that RFPose-OT can predict 3D human poses with higher precision than the state-of-the-art methods.



### Diagnosis of COVID-19 based on Chest Radiography
- **Arxiv ID**: http://arxiv.org/abs/2212.13032v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.13032v1)
- **Published**: 2022-12-26 08:05:56+00:00
- **Updated**: 2022-12-26 08:05:56+00:00
- **Authors**: Mei Gah Lim, Hoi Leong Lee
- **Comment**: None
- **Journal**: None
- **Summary**: The Coronavirus disease 2019 (COVID-19) was first identified in Wuhan, China, in early December 2019 and now becoming a pandemic. When COVID-19 patients undergo radiography examination, radiologists can observe the present of radiographic abnormalities from their chest X-ray (CXR) images. In this study, a deep convolutional neural network (CNN) model was proposed to aid radiologists in diagnosing COVID-19 patients. First, this work conducted a comparative study on the performance of modified VGG-16, ResNet-50 and DenseNet-121 to classify CXR images into normal, COVID-19 and viral pneumonia. Then, the impact of image augmentation on the classification results was evaluated. The publicly available COVID-19 Radiography Database was used throughout this study. After comparison, ResNet-50 achieved the highest accuracy with 95.88%. Next, after training ResNet-50 with rotation, translation, horizontal flip, intensity shift and zoom augmented dataset, the accuracy dropped to 80.95%. Furthermore, an ablation study on the effect of image augmentation on the classification results found that the combinations of rotation and intensity shift augmentation methods obtained an accuracy higher than baseline, which is 96.14%. Finally, ResNet-50 with rotation and intensity shift augmentations performed the best and was proposed as the final classification model in this work. These findings demonstrated that the proposed classification model can provide a promising result for COVID-19 diagnosis.



### Kidney and Kidney Tumour Segmentation in CT Images
- **Arxiv ID**: http://arxiv.org/abs/2212.13034v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13034v1)
- **Published**: 2022-12-26 08:08:44+00:00
- **Updated**: 2022-12-26 08:08:44+00:00
- **Authors**: Qi Ming How, Hoi Leong Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic segmentation of kidney and kidney tumour in Computed Tomography (CT) images is essential, as it uses less time as compared to the current gold standard of manual segmentation. However, many hospitals are still reliant on manual study and segmentation of CT images by medical practitioners because of its higher accuracy. Thus, this study focuses on the development of an approach for automatic kidney and kidney tumour segmentation in contrast-enhanced CT images. A method based on Convolutional Neural Network (CNN) was proposed, where a 3D U-Net segmentation model was developed and trained to delineate the kidney and kidney tumour from CT scans. Each CT image was pre-processed before inputting to the CNN, and the effect of down-sampled and patch-wise input images on the model performance was analysed. The proposed method was evaluated on the publicly available 2021 Kidney and Kidney Tumour Segmentation Challenge (KiTS21) dataset. The method with the best performing model recorded an average training Dice score of 0.6129, with the kidney and kidney tumour Dice scores of 0.7923 and 0.4344, respectively. For testing, the model obtained a kidney Dice score of 0.8034, and a kidney tumour Dice score of 0.4713, with an average Dice score of 0.6374.



### A Survey of Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2212.13038v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13038v1)
- **Published**: 2022-12-26 08:36:58+00:00
- **Updated**: 2022-12-26 08:36:58+00:00
- **Authors**: Xinyi Wang, Jianteng Peng, Sufang Zhang, Bihui Chen, Yi Wang, Yandong Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years witnessed the breakthrough of face recognition with deep convolutional neural networks. Dozens of papers in the field of FR are published every year. Some of them were applied in the industrial community and played an important role in human life such as device unlock, mobile payment, and so on. This paper provides an introduction to face recognition, including its history, pipeline, algorithms based on conventional manually designed features or deep learning, mainstream training, evaluation datasets, and related applications. We have analyzed and compared state-of-the-art works as many as possible, and also carefully designed a set of experiments to find the effect of backbone size and data distribution. This survey is a material of the tutorial named The Practical Face Recognition Technology in the Industrial World in the FG2023.



### MonoNeRF: Learning a Generalizable Dynamic Radiance Field from Monocular Videos
- **Arxiv ID**: http://arxiv.org/abs/2212.13056v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13056v3)
- **Published**: 2022-12-26 09:20:55+00:00
- **Updated**: 2023-08-14 17:20:43+00:00
- **Authors**: Fengrui Tian, Shaoyi Du, Yueqi Duan
- **Comment**: Accepted by ICCV 2023
- **Journal**: None
- **Summary**: In this paper, we target at the problem of learning a generalizable dynamic radiance field from monocular videos. Different from most existing NeRF methods that are based on multiple views, monocular videos only contain one view at each timestamp, thereby suffering from ambiguity along the view direction in estimating point features and scene flows. Previous studies such as DynNeRF disambiguate point features by positional encoding, which is not transferable and severely limits the generalization ability. As a result, these methods have to train one independent model for each scene and suffer from heavy computational costs when applying to increasing monocular videos in real-world applications. To address this, We propose MonoNeRF to simultaneously learn point features and scene flows with point trajectory and feature correspondence constraints across frames. More specifically, we learn an implicit velocity field to estimate point trajectory from temporal features with Neural ODE, which is followed by a flow-based feature aggregation module to obtain spatial features along the point trajectory. We jointly optimize temporal and spatial features in an end-to-end manner. Experiments show that our MonoNeRF is able to learn from multiple scenes and support new applications such as scene editing, unseen frame synthesis, and fast novel scene adaptation. Codes are available at https://github.com/tianfr/MonoNeRF.



### Crop mapping in the small sample/no sample case: an approach using a two-level cascade classifier and integrating domain knowledge
- **Arxiv ID**: http://arxiv.org/abs/2302.10270v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2302.10270v1)
- **Published**: 2022-12-26 09:27:12+00:00
- **Updated**: 2022-12-26 09:27:12+00:00
- **Authors**: Yunze Zang, Yifei Liu, Xuehong Chen, Anqi Li, Yichen Zhai, Shijie Li, Luling Liu, Chuanhai Zhu, Ruilin Chen, Shupeng Li, Na Jie
- **Comment**: in Chinese language
- **Journal**: None
- **Summary**: Mapping crops using remote sensing technology is important for food security and land management. Machine learning-based methods has become a popular approach for crop mapping in recent years. However, the key to machine learning, acquiring ample and accurate samples, is usually time-consuming and laborious. To solve this problem, a crop mapping method in the small sample/no sample case that integrating domain knowledge and using a cascaded classification framework that combine a weak classifier learned from samples with strong features and a strong classifier trained by samples with weak feature was proposed. First, based on the domain knowledge of various crops, a low-capacity classifier such as decision tree was applied to acquire those pixels with distinctive features and complete observation sequences as "strong feature" samples. Then, to improve the representativeness of these samples, sample augmentation strategy that artificially remove the observations of "strong feature" samples according to the average valid observation proportion in target area was applied. Finally, based on the original samples and augmented samples, a large-capacity classifier such as random forest was trained for crop mapping. The method achieved an overall accuracy of 82% in the MAP crop recognition competition held by Syngenta Group, China in 2021 (third prize, ranked fourth). This method integrates domain knowledge to overcome the difficulties of sample acquisition, providing a convenient, fast and accurate solution for crop mapping.



### OMSN and FAROS: OCTA Microstructure Segmentation Network and Fully Annotated Retinal OCTA Segmentation Dataset
- **Arxiv ID**: http://arxiv.org/abs/2212.13059v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13059v1)
- **Published**: 2022-12-26 09:32:52+00:00
- **Updated**: 2022-12-26 09:32:52+00:00
- **Authors**: Peng Xiao, Xiaodong Hu, Ke Ma, Gengyuan Wang, Ziqing Feng, Yuancong Huang, Jin Yuan
- **Comment**: 10 pages, 6 figures, submitted to IEEE Transactions on Medical
  Imaging (TMI)
- **Journal**: None
- **Summary**: The lack of efficient segmentation methods and fully-labeled datasets limits the comprehensive assessment of optical coherence tomography angiography (OCTA) microstructures like retinal vessel network (RVN) and foveal avascular zone (FAZ), which are of great value in ophthalmic and systematic diseases evaluation. Here, we introduce an innovative OCTA microstructure segmentation network (OMSN) by combining an encoder-decoder-based architecture with multi-scale skip connections and the split-attention-based residual network ResNeSt, paying specific attention to OCTA microstructural features while facilitating better model convergence and feature representations. The proposed OMSN achieves excellent single/multi-task performances for RVN or/and FAZ segmentation. Especially, the evaluation metrics on multi-task models outperform single-task models on the same dataset. On this basis, a fully annotated retinal OCTA segmentation (FAROS) dataset is constructed semi-automatically, filling the vacancy of a pixel-level fully-labeled OCTA dataset. OMSN multi-task segmentation model retrained with FAROS further certifies its outstanding accuracy for simultaneous RVN and FAZ segmentation.



### Transformer and GAN Based Super-Resolution Reconstruction Network for Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2212.13068v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2212.13068v1)
- **Published**: 2022-12-26 09:52:12+00:00
- **Updated**: 2022-12-26 09:52:12+00:00
- **Authors**: Weizhi Du, Harvery Tian
- **Comment**: 8 pages and 6 figures
- **Journal**: None
- **Summary**: Because of the necessity to obtain high-quality images with minimal radiation doses, such as in low-field magnetic resonance imaging, super-resolution reconstruction in medical imaging has become more popular (MRI). However, due to the complexity and high aesthetic requirements of medical imaging, image super-resolution reconstruction remains a difficult challenge. In this paper, we offer a deep learning-based strategy for reconstructing medical images from low resolutions utilizing Transformer and Generative Adversarial Networks (T-GAN). The integrated system can extract more precise texture information and focus more on important locations through global image matching after successfully inserting Transformer into the generative adversarial network for picture reconstruction. Furthermore, we weighted the combination of content loss, adversarial loss, and adversarial feature loss as the final multi-task loss function during the training of our proposed model T-GAN. In comparison to established measures like PSNR and SSIM, our suggested T-GAN achieves optimal performance and recovers more texture features in super-resolution reconstruction of MRI scanned images of the knees and belly.



### TypeFormer: Transformers for Mobile Keystroke Biometrics
- **Arxiv ID**: http://arxiv.org/abs/2212.13075v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2212.13075v2)
- **Published**: 2022-12-26 10:25:06+00:00
- **Updated**: 2023-05-31 11:38:22+00:00
- **Authors**: Giuseppe Stragapede, Paula Delgado-Santos, Ruben Tolosana, Ruben Vera-Rodriguez, Richard Guest, Aythami Morales
- **Comment**: None
- **Journal**: None
- **Summary**: The broad usage of mobile devices nowadays, the sensitiveness of the information contained in them, and the shortcomings of current mobile user authentication methods are calling for novel, secure, and unobtrusive solutions to verify the users' identity. In this article, we propose TypeFormer, a novel Transformer architecture to model free-text keystroke dynamics performed on mobile devices for the purpose of user authentication. The proposed model consists in Temporal and Channel Modules enclosing two Long Short-Term Memory (LSTM) recurrent layers, Gaussian Range Encoding (GRE), a multi-head Self-Attention mechanism, and a Block-Recurrent structure. Experimenting on one of the largest public databases to date, the Aalto mobile keystroke database, TypeFormer outperforms current state-of-the-art systems achieving Equal Error Rate (EER) values of 3.25% using only 5 enrolment sessions of 50 keystrokes each. In such way, we contribute to reducing the traditional performance gap of the challenging mobile free-text scenario with respect to its desktop and fixed-text counterparts. Additionally, we analyse the behaviour of the model with different experimental configurations such as the length of the keystroke sequences and the amount of enrolment sessions, showing margin for improvement with more enrolment data. Finally, a cross-database evaluation is carried out, demonstrating the robustness of the features extracted by TypeFormer in comparison with existing approaches.



### Semi-Supervised Domain Adaptation for Semantic Segmentation of Roads from Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2212.13079v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13079v1)
- **Published**: 2022-12-26 10:50:40+00:00
- **Updated**: 2022-12-26 10:50:40+00:00
- **Authors**: Ahmet Alp Kindiroglu, Metehan Yalçın, Furkan Burak Bağcı, Mahiye Uluyağmur Öztürk
- **Comment**: in Turkish language
- **Journal**: None
- **Summary**: This paper presents the preliminary findings of a semi-supervised segmentation method for extracting roads from sattelite images. Artificial Neural Networks and image segmentation methods are among the most successful methods for extracting road data from satellite images. However, these models require large amounts of training data from different regions to achieve high accuracy rates. In cases where this data needs to be of more quantity or quality, it is a standard method to train deep neural networks by transferring knowledge from annotated data obtained from different sources. This study proposes a method that performs path segmentation with semi-supervised learning methods. A semi-supervised field adaptation method based on pseudo-labeling and Minimum Class Confusion method has been proposed, and it has been observed to increase performance in targeted datasets.



### Fewer is More: Efficient Object Detection in Large Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2212.13136v2
- **DOI**: 10.1007/s11432-022-3718-5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13136v2)
- **Published**: 2022-12-26 12:49:47+00:00
- **Updated**: 2023-03-09 08:55:01+00:00
- **Authors**: Xingxing Xie, Gong Cheng, Qingyang Li, Shicheng Miao, Ke Li, Junwei Han
- **Comment**: This manuscript is the accepted version for SCIENCE CHINA Information
  Sciences
- **Journal**: SCIENCE CHINA Information Sciences, 2023
- **Summary**: Current mainstream object detection methods for large aerial images usually divide large images into patches and then exhaustively detect the objects of interest on all patches, no matter whether there exist objects or not. This paradigm, although effective, is inefficient because the detectors have to go through all patches, severely hindering the inference speed. This paper presents an Objectness Activation Network (OAN) to help detectors focus on fewer patches but achieve more efficient inference and more accurate results, enabling a simple and effective solution to object detection in large images. In brief, OAN is a light fully-convolutional network for judging whether each patch contains objects or not, which can be easily integrated into many object detectors and jointly trained with them end-to-end. We extensively evaluate our OAN with five advanced detectors. Using OAN, all five detectors acquire more than 30.0% speed-up on three large-scale aerial image datasets, meanwhile with consistent accuracy improvements. On extremely large Gaofen-2 images (29200$\times$27620 pixels), our OAN improves the detection speed by 70.5%. Moreover, we extend our OAN to driving-scene object detection and 4K video object detection, boosting the detection speed by 112.1% and 75.0%, respectively, without sacrificing the accuracy. Code is available at https://github.com/Ranchosky/OAN.



### Semantic Enhanced Knowledge Graph for Large-Scale Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2212.13151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13151v1)
- **Published**: 2022-12-26 13:18:36+00:00
- **Updated**: 2022-12-26 13:18:36+00:00
- **Authors**: Jiwei Wei, Yang Yang, Zeyu Ma, Jingjing Li, Xing Xu, Heng Tao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-Shot Learning has been a highlighted research topic in both vision and language areas. Recently, most existing methods adopt structured knowledge information to model explicit correlations among categories and use deep graph convolutional network to propagate information between different categories. However, it is difficult to add new categories to existing structured knowledge graph, and deep graph convolutional network suffers from over-smoothing problem. In this paper, we provide a new semantic enhanced knowledge graph that contains both expert knowledge and categories semantic correlation. Our semantic enhanced knowledge graph can further enhance the correlations among categories and make it easy to absorb new categories. To propagate information on the knowledge graph, we propose a novel Residual Graph Convolutional Network (ResGCN), which can effectively alleviate the problem of over-smoothing. Experiments conducted on the widely used large-scale ImageNet-21K dataset and AWA2 dataset show the effectiveness of our method, and establish a new state-of-the-art on zero-shot learning. Moreover, our results on the large-scale ImageNet-21K with various feature extraction networks show that our method has better generalization and robustness.



### Human Activity Recognition from Wi-Fi CSI Data Using Principal Component-Based Wavelet CNN
- **Arxiv ID**: http://arxiv.org/abs/2212.13161v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2212.13161v1)
- **Published**: 2022-12-26 13:45:19+00:00
- **Updated**: 2022-12-26 13:45:19+00:00
- **Authors**: Ishtiaque Ahmed Showmik, Tahsina Farah Sanam, Hafiz Imtiaz
- **Comment**: \c{opyright} 2022. This manuscript version is made available under
  the CC-BY-NC-ND 4.0 license
  https://creativecommons.org/licenses/by-nc-nd/4.0/
- **Journal**: None
- **Summary**: Human Activity Recognition (HAR) is an emerging technology with several applications in surveillance, security, and healthcare sectors. Noninvasive HAR systems based on Wi-Fi Channel State Information (CSI) signals can be developed leveraging the quick growth of ubiquitous Wi-Fi technologies, and the correlation between CSI dynamics and body motions. In this paper, we propose Principal Component-based Wavelet Convolutional Neural Network (or PCWCNN) -- a novel approach that offers robustness and efficiency for practical real-time applications. Our proposed method incorporates two efficient preprocessing algorithms -- the Principal Component Analysis (PCA) and the Discrete Wavelet Transform (DWT). We employ an adaptive activity segmentation algorithm that is accurate and computationally light. Additionally, we used the Wavelet CNN for classification, which is a deep convolutional network analogous to the well-studied ResNet and DenseNet networks. We empirically show that our proposed PCWCNN model performs very well on a real dataset, outperforming existing approaches.



### MRTNet: Multi-Resolution Temporal Network for Video Sentence Grounding
- **Arxiv ID**: http://arxiv.org/abs/2212.13163v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13163v2)
- **Published**: 2022-12-26 13:48:05+00:00
- **Updated**: 2022-12-27 05:14:51+00:00
- **Authors**: Wei Ji, Long Chen, Yinwei Wei, Yiming Wu, Tat-Seng Chua
- **Comment**: work in progress
- **Journal**: None
- **Summary**: Given an untrimmed video and natural language query, video sentence grounding aims to localize the target temporal moment in the video. Existing methods mainly tackle this task by matching and aligning semantics of the descriptive sentence and video segments on a single temporal resolution, while neglecting the temporal consistency of video content in different resolutions. In this work, we propose a novel multi-resolution temporal video sentence grounding network: MRTNet, which consists of a multi-modal feature encoder, a Multi-Resolution Temporal (MRT) module, and a predictor module. MRT module is an encoder-decoder network, and output features in the decoder part are in conjunction with Transformers to predict the final start and end timestamps. Particularly, our MRT module is hot-pluggable, which means it can be seamlessly incorporated into any anchor-free models. Besides, we utilize a hybrid loss to supervise cross-modal features in MRT module for more accurate grounding in three scales: frame-level, clip-level and sequence-level. Extensive experiments on three prevalent datasets have shown the effectiveness of MRTNet.



### Orientation-Shared Convolution Representation for CT Metal Artifact Learning
- **Arxiv ID**: http://arxiv.org/abs/2212.13166v1
- **DOI**: 10.1007/978-3-031-16446-0_63
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.13166v1)
- **Published**: 2022-12-26 13:56:12+00:00
- **Updated**: 2022-12-26 13:56:12+00:00
- **Authors**: Hong Wang, Qi Xie, Yuexiang Li, Yawen Huang, Deyu Meng, Yefeng Zheng
- **Comment**: None
- **Journal**: MICCAI 2022
- **Summary**: During X-ray computed tomography (CT) scanning, metallic implants carrying with patients often lead to adverse artifacts in the captured CT images and then impair the clinical treatment. Against this metal artifact reduction (MAR) task, the existing deep-learning-based methods have gained promising reconstruction performance. Nevertheless, there is still some room for further improvement of MAR performance and generalization ability, since some important prior knowledge underlying this specific task has not been fully exploited. Hereby, in this paper, we carefully analyze the characteristics of metal artifacts and propose an orientation-shared convolution representation strategy to adapt the physical prior structures of artifacts, i.e., rotationally symmetrical streaking patterns. The proposed method rationally adopts Fourier-series-expansion-based filter parametrization in artifact modeling, which can better separate artifacts from anatomical tissues and boost the model generalizability. Comprehensive experiments executed on synthesized and clinical datasets show the superiority of our method in detail preservation beyond the current representative MAR methods. Code will be available at \url{https://github.com/hongwang01/OSCNet}



### Weakly-Supervised Semantic Segmentation of Ships Using Thermal Imagery
- **Arxiv ID**: http://arxiv.org/abs/2212.13170v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2212.13170v1)
- **Published**: 2022-12-26 14:20:32+00:00
- **Updated**: 2022-12-26 14:20:32+00:00
- **Authors**: Rushil Joshi, Ethan Adams, Matthew Ziemann, Christopher A. Metzler
- **Comment**: None
- **Journal**: None
- **Summary**: The United States coastline spans 95,471 miles; a distance that cannot be effectively patrolled or secured by manual human effort alone. Unmanned Aerial Vehicles (UAVs) equipped with infrared cameras and deep-learning based algorithms represent a more efficient alternative for identifying and segmenting objects of interest - namely, ships. However, standard approaches to training these algorithms require large-scale datasets of densely labeled infrared maritime images. Such datasets are not publicly available and manually annotating every pixel in a large-scale dataset would have an extreme labor cost. In this work we demonstrate that, in the context of segmenting ships in infrared imagery, weakly-supervising an algorithm with sparsely labeled data can drastically reduce data labeling costs with minimal impact on system performance. We apply weakly-supervised learning to an unlabeled dataset of 7055 infrared images sourced from the Naval Air Warfare Center Aircraft Division (NAWCAD). We find that by sparsely labeling only 32 points per image, weakly-supervised segmentation models can still effectively detect and segment ships, with a Jaccard score of up to 0.756.



### Prototype-guided Cross-task Knowledge Distillation for Large-scale Models
- **Arxiv ID**: http://arxiv.org/abs/2212.13180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13180v1)
- **Published**: 2022-12-26 15:00:42+00:00
- **Updated**: 2022-12-26 15:00:42+00:00
- **Authors**: Deng Li, Aming Wu, Yahong Han, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, large-scale pre-trained models have shown their advantages in many tasks. However, due to the huge computational complexity and storage requirements, it is challenging to apply the large-scale model to real scenes. A common solution is knowledge distillation which regards the large-scale model as a teacher model and helps to train a small student model to obtain a competitive performance. Cross-task Knowledge distillation expands the application scenarios of the large-scale pre-trained model. Existing knowledge distillation works focus on directly mimicking the final prediction or the intermediate layers of the teacher model, which represent the global-level characteristics and are task-specific. To alleviate the constraint of different label spaces, capturing invariant intrinsic local object characteristics (such as the shape characteristics of the leg and tail of the cattle and horse) plays a key role. Considering the complexity and variability of real scene tasks, we propose a Prototype-guided Cross-task Knowledge Distillation (ProC-KD) approach to transfer the intrinsic local-level object knowledge of a large-scale teacher network to various task scenarios. First, to better transfer the generalized knowledge in the teacher model in cross-task scenarios, we propose a prototype learning module to learn from the essential feature representation of objects in the teacher model. Secondly, for diverse downstream tasks, we propose a task-adaptive feature augmentation module to enhance the features of the student model with the learned generalization prototype features and guide the training of the student model to improve its generalization ability. The experimental results on various visual tasks demonstrate the effectiveness of our approach for large-scale model cross-task knowledge distillation scenes.



### Generalized Differentiable RANSAC
- **Arxiv ID**: http://arxiv.org/abs/2212.13185v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13185v2)
- **Published**: 2022-12-26 15:13:13+00:00
- **Updated**: 2023-03-16 16:34:38+00:00
- **Authors**: Tong Wei, Yash Patel, Alexander Shekhovtsov, Jiri Matas, Daniel Barath
- **Comment**: None
- **Journal**: None
- **Summary**: We propose $\nabla$-RANSAC, a generalized differentiable RANSAC that allows learning the entire randomized robust estimation pipeline. The proposed approach enables the use of relaxation techniques for estimating the gradients in the sampling distribution, which are then propagated through a differentiable solver. The trainable quality function marginalizes over the scores from all the models estimated within $\nabla$-RANSAC to guide the network learning accurate and useful inlier probabilities or to train feature detection and matching networks. Our method directly maximizes the probability of drawing a good hypothesis, allowing us to learn better sampling distribution. We test $\nabla$-RANSAC on a number of real-world scenarios on fundamental and essential matrix estimation, both outdoors and indoors, with handcrafted and learning-based features. It is superior to the state-of-the-art in terms of accuracy while running at a similar speed to its less accurate alternatives. The code and trained models are available at https://github.com/weitong8591/differentiable_ransac.



### DSI2I: Dense Style for Unpaired Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2212.13253v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13253v2)
- **Published**: 2022-12-26 18:45:25+00:00
- **Updated**: 2022-12-29 13:06:04+00:00
- **Authors**: Baran Ozaydin, Tong Zhang, Sabine Süsstrunk, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: Unpaired exemplar-based image-to-image (UEI2I) translation aims to translate a source image to a target image domain with the style of a target image exemplar, without ground-truth input-translation pairs. Existing UEI2I methods represent style using either a global, image-level feature vector, or one vector per object instance/class but requiring knowledge of the scene semantics. Here, by contrast, we propose to represent style as a dense feature map, allowing for a finer-grained transfer to the source image without requiring any external semantic information. We then rely on perceptual and adversarial losses to disentangle our dense style and content representations, and exploit unsupervised cross-domain semantic correspondences to warp the exemplar style to the source content. We demonstrate the effectiveness of our method on two datasets using standard metrics together with a new localized style metric measuring style similarity in a class-wise manner. Our results evidence that the translations produced by our approach are more diverse and closer to the exemplars than those of the state-of-the-art methods while nonetheless preserving the source content.



### PMODE: Prototypical Mask based Object Dimension Estimation
- **Arxiv ID**: http://arxiv.org/abs/2212.13281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13281v1)
- **Published**: 2022-12-26 19:24:25+00:00
- **Updated**: 2022-12-26 19:24:25+00:00
- **Authors**: Thariq Khalid, Mohammed Yahya Hakami, Riad Souissi
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Can a neural network estimate an object's dimension in the wild? In this paper, we propose a method and deep learning architecture to estimate the dimensions of a quadrilateral object of interest in videos using a monocular camera. The proposed technique does not use camera calibration or handcrafted geometric features; however, features are learned with the help of coefficients of a segmentation neural network during the training process. A real-time instance segmentation-based Deep Neural Network with a ResNet50 backbone is employed, giving the object's prototype mask and thus provides a region of interest to regress its dimensions. The instance segmentation network is trained to look at only the nearest object of interest. The regression is performed using an MLP head which looks only at the mask coefficients of the bounding box detector head and the prototype segmentation mask. We trained the system with three different random cameras achieving 22% MAPE for the test dataset for the dimension estimation



### On the Level Sets and Invariance of Neural Tuning Landscapes
- **Arxiv ID**: http://arxiv.org/abs/2212.13285v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.AI, cs.CG, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2212.13285v1)
- **Published**: 2022-12-26 19:53:29+00:00
- **Updated**: 2022-12-26 19:53:29+00:00
- **Authors**: Binxu Wang, Carlos R. Ponce
- **Comment**: 24 pages, 13 figures. Published in NeurIPS 2022 Workshop on Symmetry
  and Geometry in Neural Representations, and PMLR volume 197
- **Journal**: None
- **Summary**: Visual representations can be defined as the activations of neuronal populations in response to images. The activation of a neuron as a function over all image space has been described as a "tuning landscape". As a function over a high-dimensional space, what is the structure of this landscape? In this study, we characterize tuning landscapes through the lens of level sets and Morse theory. A recent study measured the in vivo two-dimensional tuning maps of neurons in different brain regions. Here, we developed a statistically reliable signature for these maps based on the change of topology in level sets. We found this topological signature changed progressively throughout the cortical hierarchy, with similar trends found for units in convolutional neural networks (CNNs). Further, we analyzed the geometry of level sets on the tuning landscapes of CNN units. We advanced the hypothesis that higher-order units can be locally regarded as isotropic radial basis functions, but not globally. This shows the power of level sets as a conceptual tool to understand neuronal activations over image space.



### VQA and Visual Reasoning: An Overview of Recent Datasets, Methods and Challenges
- **Arxiv ID**: http://arxiv.org/abs/2212.13296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13296v1)
- **Published**: 2022-12-26 20:56:01+00:00
- **Updated**: 2022-12-26 20:56:01+00:00
- **Authors**: Rufai Yusuf Zakari, Jim Wilson Owusu, Hailin Wang, Ke Qin, Zaharaddeen Karami Lawal, Yuezhou Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial Intelligence (AI) and its applications have sparked extraordinary interest in recent years. This achievement can be ascribed in part to advances in AI subfields including Machine Learning (ML), Computer Vision (CV), and Natural Language Processing (NLP). Deep learning, a sub-field of machine learning that employs artificial neural network concepts, has enabled the most rapid growth in these domains. The integration of vision and language has sparked a lot of attention as a result of this. The tasks have been created in such a way that they properly exemplify the concepts of deep learning. In this review paper, we provide a thorough and an extensive review of the state of the arts approaches, key models design principles and discuss existing datasets, methods, their problem formulation and evaluation measures for VQA and Visual reasoning tasks to understand vision and language representation learning. We also present some potential future paths in this field of research, with the hope that our study may generate new ideas and novel approaches to handle existing difficulties and develop new applications.



