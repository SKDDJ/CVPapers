# Arxiv Papers in cs.CV on 2022-12-25
### TriPINet: Tripartite Progressive Integration Network for Image Manipulation Localization
- **Arxiv ID**: http://arxiv.org/abs/2212.12841v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12841v1)
- **Published**: 2022-12-25 02:27:58+00:00
- **Updated**: 2022-12-25 02:27:58+00:00
- **Authors**: Wei-Yun Liang, Jing Xu, Xiao Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Image manipulation localization aims at distinguishing forged regions from the whole test image. Although many outstanding prior arts have been proposed for this task, there are still two issues that need to be further studied: 1) how to fuse diverse types of features with forgery clues; 2) how to progressively integrate multistage features for better localization performance. In this paper, we propose a tripartite progressive integration network (TriPINet) for end-to-end image manipulation localization. First, we extract both visual perception information, e.g., RGB input images, and visual imperceptible features, e.g., frequency and noise traces for forensic feature learning. Second, we develop a guided cross-modality dual-attention (gCMDA) module to fuse different types of forged clues. Third, we design a set of progressive integration squeeze-and-excitation (PI-SE) modules to improve localization performance by appropriately incorporating multiscale features in the decoder. Extensive experiments are conducted to compare our method with state-of-the-art image forensics approaches. The proposed TriPINet obtains competitive results on several benchmark datasets.



### Weakly-Supervised Deep Learning Model for Prostate Cancer Diagnosis and Gleason Grading of Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/2212.12844v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.12844v1)
- **Published**: 2022-12-25 03:07:52+00:00
- **Updated**: 2022-12-25 03:07:52+00:00
- **Authors**: Mohammad Mahdi Behzadi, Mohammad Madani, Hanzhang Wang, Jun Bai, Ankit Bhardwaj, Anna Tarakanova, Harold Yamase, Ga Hie Nam, Sheida Nabavi
- **Comment**: None
- **Journal**: None
- **Summary**: Prostate cancer is the most common cancer in men worldwide and the second leading cause of cancer death in the United States. One of the prognostic features in prostate cancer is the Gleason grading of histopathology images. The Gleason grade is assigned based on tumor architecture on Hematoxylin and Eosin (H&E) stained whole slide images (WSI) by the pathologists. This process is time-consuming and has known interobserver variability. In the past few years, deep learning algorithms have been used to analyze histopathology images, delivering promising results for grading prostate cancer. However, most of the algorithms rely on the fully annotated datasets which are expensive to generate. In this work, we proposed a novel weakly-supervised algorithm to classify prostate cancer grades. The proposed algorithm consists of three steps: (1) extracting discriminative areas in a histopathology image by employing the Multiple Instance Learning (MIL) algorithm based on Transformers, (2) representing the image by constructing a graph using the discriminative patches, and (3) classifying the image into its Gleason grades by developing a Graph Convolutional Neural Network (GCN) based on the gated attention mechanism. We evaluated our algorithm using publicly available datasets, including TCGAPRAD, PANDA, and Gleason 2019 challenge datasets. We also cross validated the algorithm on an independent dataset. Results show that the proposed model achieved state-of-the-art performance in the Gleason grading task in terms of accuracy, F1 score, and cohen-kappa. The code is available at https://github.com/NabaviLab/Prostate-Cancer.



### Deep Cost-sensitive Learning for Wheat Frost Detection
- **Arxiv ID**: http://arxiv.org/abs/2212.12856v1
- **DOI**: 10.1109/ICBAIE56435.2022.9985932
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12856v1)
- **Published**: 2022-12-25 05:07:24+00:00
- **Updated**: 2022-12-25 05:07:24+00:00
- **Authors**: Shujian Cao, Lin Cui, Haipeng Liu
- **Comment**: 7 pages, 4 figures, accepted by ICBAIE 2022
- **Journal**: None
- **Summary**: Frost damage is one of the main factors leading to wheat yield reduction. Therefore, the detection of wheat frost accurately and efficiently is beneficial for growers to take corresponding measures in time to reduce economic loss. To detect the wheat frost, in this paper we create a hyperspectral wheat frost data set by collecting the data characterized by temperature, wheat yield, and hyperspectral information provided by the handheld hyperspectral spectrometer. However, due to the imbalance of data, that is, the number of healthy samples is much higher than the number of frost damage samples, a deep learning algorithm tends to predict biasedly towards the healthy samples resulting in model overfitting of the healthy samples. Therefore, we propose a method based on deep cost-sensitive learning, which uses a one-dimensional convolutional neural network as the basic framework and incorporates cost-sensitive learning with fixed factors and adjustment factors into the loss function to train the network. Meanwhile, the accuracy and score are used as evaluation metrics. Experimental results show that the detection accuracy and the score reached 0.943 and 0.623 respectively, this demonstration shows that this method not only ensures the overall accuracy but also effectively improves the detection rate of frost samples.



### StepNet: Spatial-temporal Part-aware Network for Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2212.12857v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12857v1)
- **Published**: 2022-12-25 05:24:08+00:00
- **Updated**: 2022-12-25 05:24:08+00:00
- **Authors**: Xiaolong Shen, Zhedong Zheng, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Sign language recognition (SLR) aims to overcome the communication barrier for the people with deafness or the people with hard hearing. Most existing approaches can be typically divided into two lines, i.e., Skeleton-based and RGB-based methods, but both the two lines of methods have their limitations. RGB-based approaches usually overlook the fine-grained hand structure, while Skeleton-based methods do not take the facial expression into account. In attempts to address both limitations, we propose a new framework named Spatial-temporal Part-aware network (StepNet), based on RGB parts. As the name implies, StepNet consists of two modules: Part-level Spatial Modeling and Part-level Temporal Modeling. Particularly, without using any keypoint-level annotations, Part-level Spatial Modeling implicitly captures the appearance-based properties, such as hands and faces, in the feature space. On the other hand, Part-level Temporal Modeling captures the pertinent properties over time by implicitly mining the long-short term context. Extensive experiments show that our StepNet, thanks to Spatial-temporal modules, achieves competitive Top-1 Per-instance accuracy on three widely-used SLR benchmarks, i.e., 56.89% on WLASL, 77.2% on NMFs-CSL, and 77.1% on BOBSL. Moreover, the proposed method is compatible with the optical flow input, and can yield higher performance if fused. We hope that this work can serve as a preliminary step for the people with deafness.



### Adaptive Blind Watermarking Using Psychovisual Image Features
- **Arxiv ID**: http://arxiv.org/abs/2212.12864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12864v1)
- **Published**: 2022-12-25 06:33:36+00:00
- **Updated**: 2022-12-25 06:33:36+00:00
- **Authors**: Arezoo PariZanganeh, Ghazaleh Ghorbanzadeh, Zahra Nabizadeh ShahreBabak, Nader Karimi, Shadrokh Samavi
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: With the growth of editing and sharing images through the internet, the importance of protecting the images' authorship has increased. Robust watermarking is a known approach to maintaining copyright protection. Robustness and imperceptibility are two factors that are tried to be maximized through watermarking. Usually, there is a trade-off between these two parameters. Increasing the robustness would lessen the imperceptibility of the watermarking. This paper proposes an adaptive method that determines the strength of the watermark embedding in different parts of the cover image regarding its texture and brightness. Adaptive embedding increases the robustness while preserving the quality of the watermarked image. Experimental results also show that the proposed method can effectively reconstruct the embedded payload in different kinds of common watermarking attacks. Our proposed method has shown good performance compared to a recent technique.



### PaletteNeRF: Palette-based Color Editing for NeRFs
- **Arxiv ID**: http://arxiv.org/abs/2212.12871v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2212.12871v1)
- **Published**: 2022-12-25 08:01:03+00:00
- **Updated**: 2022-12-25 08:01:03+00:00
- **Authors**: Qiling Wu, Jianchao Tan, Kun Xu
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: Neural Radiance Field (NeRF) is a powerful tool to faithfully generate novel views for scenes with only sparse captured images. Despite its strong capability for representing 3D scenes and their appearance, its editing ability is very limited. In this paper, we propose a simple but effective extension of vanilla NeRF, named PaletteNeRF, to enable efficient color editing on NeRF-represented scenes. Motivated by recent palette-based image decomposition works, we approximate each pixel color as a sum of palette colors modulated by additive weights. Instead of predicting pixel colors as in vanilla NeRFs, our method predicts additive weights. The underlying NeRF backbone could also be replaced with more recent NeRF models such as KiloNeRF to achieve real-time editing. Experimental results demonstrate that our method achieves efficient, view-consistent, and artifact-free color editing on a wide range of NeRF-represented scenes.



### Data class-specific all-optical transformations and encryption
- **Arxiv ID**: http://arxiv.org/abs/2212.12873v1
- **DOI**: 10.1002/adma.202212091
- **Categories**: **physics.optics**, cs.CV, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2212.12873v1)
- **Published**: 2022-12-25 08:31:03+00:00
- **Updated**: 2022-12-25 08:31:03+00:00
- **Authors**: Bijie Bai, Heming Wei, Xilin Yang, Deniz Mengu, Aydogan Ozcan
- **Comment**: 27 Pages, 9 Figures, 1 Table
- **Journal**: Advanced Materials (2023)
- **Summary**: Diffractive optical networks provide rich opportunities for visual computing tasks since the spatial information of a scene can be directly accessed by a diffractive processor without requiring any digital pre-processing steps. Here we present data class-specific transformations all-optically performed between the input and output fields-of-view (FOVs) of a diffractive network. The visual information of the objects is encoded into the amplitude (A), phase (P), or intensity (I) of the optical field at the input, which is all-optically processed by a data class-specific diffractive network. At the output, an image sensor-array directly measures the transformed patterns, all-optically encrypted using the transformation matrices pre-assigned to different data classes, i.e., a separate matrix for each data class. The original input images can be recovered by applying the correct decryption key (the inverse transformation) corresponding to the matching data class, while applying any other key will lead to loss of information. The class-specificity of these all-optical diffractive transformations creates opportunities where different keys can be distributed to different users; each user can only decode the acquired images of only one data class, serving multiple users in an all-optically encrypted manner. We numerically demonstrated all-optical class-specific transformations covering A-->A, I-->I, and P-->I transformations using various image datasets. We also experimentally validated the feasibility of this framework by fabricating a class-specific I-->I transformation diffractive network using two-photon polymerization and successfully tested it at 1550 nm wavelength. Data class-specific all-optical transformations provide a fast and energy-efficient method for image and data encryption, enhancing data security and privacy.



### A Lightweight Reconstruction Network for Surface Defect Inspection
- **Arxiv ID**: http://arxiv.org/abs/2212.12878v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.12878v1)
- **Published**: 2022-12-25 08:59:15+00:00
- **Updated**: 2022-12-25 08:59:15+00:00
- **Authors**: Chao Hu, Jian Yao, Weijie Wu, Weibin Qiu, Liqiang Zhu
- **Comment**: Journal of Mathematical Imaging and Vision(JMIV)
- **Journal**: 2023 Journal of Mathematical Imaging and Vision(2023 JMIV)
- **Summary**: Currently, most deep learning methods cannot solve the problem of scarcity of industrial product defect samples and significant differences in characteristics. This paper proposes an unsupervised defect detection algorithm based on a reconstruction network, which is realized using only a large number of easily obtained defect-free sample data. The network includes two parts: image reconstruction and surface defect area detection. The reconstruction network is designed through a fully convolutional autoencoder with a lightweight structure. Only a small number of normal samples are used for training so that the reconstruction network can be A defect-free reconstructed image is generated. A function combining structural loss and $\mathit{L}1$ loss is proposed as the loss function of the reconstruction network to solve the problem of poor detection of irregular texture surface defects. Further, the residual of the reconstructed image and the image to be tested is used as the possible region of the defect, and conventional image operations can realize the location of the fault. The unsupervised defect detection algorithm of the proposed reconstruction network is used on multiple defect image sample sets. Compared with other similar algorithms, the results show that the unsupervised defect detection algorithm of the reconstructed network has strong robustness and accuracy.



### TexPose: Neural Texture Learning for Self-Supervised 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2212.12902v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12902v2)
- **Published**: 2022-12-25 13:36:32+00:00
- **Updated**: 2023-03-14 10:39:16+00:00
- **Authors**: Hanzhi Chen, Fabian Manhardt, Nassir Navab, Benjamin Busam
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: In this paper, we introduce neural texture learning for 6D object pose estimation from synthetic data and a few unlabelled real images. Our major contribution is a novel learning scheme which removes the drawbacks of previous works, namely the strong dependency on co-modalities or additional refinement. These have been previously necessary to provide training signals for convergence. We formulate such a scheme as two sub-optimisation problems on texture learning and pose learning. We separately learn to predict realistic texture of objects from real image collections and learn pose estimation from pixel-perfect synthetic data. Combining these two capabilities allows then to synthesise photorealistic novel views to supervise the pose estimator with accurate geometry. To alleviate pose noise and segmentation imperfection present during the texture learning phase, we propose a surfel-based adversarial training loss together with texture regularisation from synthetic data. We demonstrate that the proposed approach significantly outperforms the recent state-of-the-art methods without ground-truth pose annotations and demonstrates substantial generalisation improvements towards unseen scenes. Remarkably, our scheme improves the adopted pose estimators substantially even when initialised with much inferior performance.



### Learning to Estimate 3D Human Pose from Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2212.12910v1
- **DOI**: 10.1109/JSEN.2020.2999849
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2212.12910v1)
- **Published**: 2022-12-25 14:22:01+00:00
- **Updated**: 2022-12-25 14:22:01+00:00
- **Authors**: Yufan Zhou, Haiwei Dong, Abdulmotaleb El Saddik
- **Comment**: None
- **Journal**: IEEE Sensors Journal, vol. 20, no. 20, pp. 12334-12342, 2020
- **Summary**: 3D pose estimation is a challenging problem in computer vision. Most of the existing neural-network-based approaches address color or depth images through convolution networks (CNNs). In this paper, we study the task of 3D human pose estimation from depth images. Different from the existing CNN-based human pose estimation method, we propose a deep human pose network for 3D pose estimation by taking the point cloud data as input data to model the surface of complex human structures. We first cast the 3D human pose estimation from 2D depth images to 3D point clouds and directly predict the 3D joint position. Our experiments on two public datasets show that our approach achieves higher accuracy than previous state-of-art methods. The reported results on both ITOP and EVAL datasets demonstrate the effectiveness of our method on the targeted tasks.



### Quality at the Tail
- **Arxiv ID**: http://arxiv.org/abs/2212.13925v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/2212.13925v2)
- **Published**: 2022-12-25 14:49:37+00:00
- **Updated**: 2023-08-15 10:49:26+00:00
- **Authors**: Zhengxin Yang, Wanling Gao, Chunjie Luo, Lei Wang, Fei Tang, Xu Wen, Jianfeng Zhan
- **Comment**: 11 pages, 4 figures, 5 tables
- **Journal**: None
- **Summary**: Benchmarking and evaluating deep learning models and systems necessitate a meticulous approach to ensure comprehensive assessment. In practical applications, it is paramount to consider both the inference quality and the inference time, particularly within critical contexts, where stringent requirements demand the simultaneous satisfaction of both metrics. Neglecting either aspect can result in severe and irreversible consequences, including loss of human life and property damage. Unfortunately, many studies lack a comprehensive consideration of these metrics, often conducted under ideal or permissive conditions, thereby leading to incomplete or non-intuitive evaluation methodologies.   This study reveals that deep learning inference quality exhibits fluctuations, which further introduces complications and challenges to the benchmarking and evaluation. To better characterize the phenomenon, the concept of "tail quality" is introduced, which indicates the quality at the tail of distributions. "Tail quality" can offer a more objective evaluation, overcoming the limitations of conventional inference quality and inference time metrics in capturing the quality fluctuation phenomenon. To capture the phenomenon, this paper also proposes a pioneering evaluation framework for comprehensive assessment and analysis of various factors affecting inference time and quality. Leveraging this framework enables the anticipation of the potential distribution of inference time and inference quality, thus capturing "tail quality" before practically applying deep learning. The effectiveness of the evaluation framework is validated through experiments conducted on deep learning models for three different tasks across four systems. Furthermore, employing this evaluation framework, the experiments conducted a preliminary analysis of several factors influencing inference quality and inference time.



### EVM-CNN: Real-Time Contactless Heart Rate Estimation from Facial Video
- **Arxiv ID**: http://arxiv.org/abs/2212.13843v1
- **DOI**: 10.1109/TMM.2018.2883866
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2212.13843v1)
- **Published**: 2022-12-25 15:25:15+00:00
- **Updated**: 2022-12-25 15:25:15+00:00
- **Authors**: Ying Qiu, Yang Liu, Juan Arteaga-Falconi, Haiwei Dong, Abdulmotaleb El Saddik
- **Comment**: None
- **Journal**: IEEE Transactions on Multimedia, vol. 21, no. 7, pp. 1778-1787,
  2019
- **Summary**: With the increase in health consciousness, noninvasive body monitoring has aroused interest among researchers. As one of the most important pieces of physiological information, researchers have remotely estimated the heart rate (HR) from facial videos in recent years. Although progress has been made over the past few years, there are still some limitations, like the processing time increasing with accuracy and the lack of comprehensive and challenging datasets for use and comparison. Recently, it was shown that HR information can be extracted from facial videos by spatial decomposition and temporal filtering. Inspired by this, a new framework is introduced in this paper to remotely estimate the HR under realistic conditions by combining spatial and temporal filtering and a convolutional neural network. Our proposed approach shows better performance compared with the benchmark on the MMSE-HR dataset in terms of both the average HR estimation and short-time HR estimation. High consistency in short-time HR estimation is observed between our method and the ground truth.



### Understanding Ethics, Privacy, and Regulations in Smart Video Surveillance for Public Safety
- **Arxiv ID**: http://arxiv.org/abs/2212.12936v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.12936v1)
- **Published**: 2022-12-25 17:14:18+00:00
- **Updated**: 2022-12-25 17:14:18+00:00
- **Authors**: Babak Rahimi Ardabili, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Christopher Neff, Arun Ravindran, Hamed Tabkhi
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Smart Video Surveillance (SVS) systems have been receiving more attention among scholars and developers as a substitute for the current passive surveillance systems. These systems are used to make the policing and monitoring systems more efficient and improve public safety. However, the nature of these systems in monitoring the public's daily activities brings different ethical challenges. There are different approaches for addressing privacy issues in implementing the SVS. In this paper, we are focusing on the role of design considering ethical and privacy challenges in SVS. Reviewing four policy protection regulations that generate an overview of best practices for privacy protection, we argue that ethical and privacy concerns could be addressed through four lenses: algorithm, system, model, and data. As an case study, we describe our proposed system and illustrate how our system can create a baseline for designing a privacy perseverance system to deliver safety to society. We used several Artificial Intelligence algorithms, such as object detection, single and multi camera re-identification, action recognition, and anomaly detection, to provide a basic functional system. We also use cloud-native services to implement a smartphone application in order to deliver the outputs to the end users.



### Exploiting the Generative Adversarial Network Approach to Create a Synthetic Topography Corneal Image
- **Arxiv ID**: http://arxiv.org/abs/2301.11871v1
- **DOI**: 10.3390/biom12121888
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2301.11871v1)
- **Published**: 2022-12-25 17:45:21+00:00
- **Updated**: 2022-12-25 17:45:21+00:00
- **Authors**: Samer Kais Jameel, Sezgin Aydin, Nebras H. Ghaeb, Jafar Majidpour, Tarik A. Rashid, Sinan Q. Salih, P. S. JosephNg
- **Comment**: 13 pages
- **Journal**: Biomolecules, 2022
- **Summary**: Corneal diseases are the most common eye disorders. Deep learning techniques are used to per-form automated diagnoses of cornea. Deep learning networks require large-scale annotated datasets, which is conceded as a weakness of deep learning. In this work, a method for synthesizing medical images using conditional generative adversarial networks (CGANs), is presented. It also illustrates how produced medical images may be utilized to enrich medical data, improve clinical decisions, and boost the performance of the conventional neural network (CNN) for medical image diagnosis. The study includes using corneal topography captured using a Pentacam device from patients with corneal diseases. The dataset contained 3448 different corneal images. Furthermore, it shows how an unbalanced dataset affects the performance of classifiers, where the data are balanced using the resampling approach. Finally, the results obtained from CNN networks trained on the balanced dataset are compared to those obtained from CNN networks trained on the imbalanced dataset. For performance, the system estimated the diagnosis accuracy, precision, and F1-score metrics. Lastly, some generated images were shown to an expert for evaluation and to see how well experts could identify the type of image and its condition. The expert recognized the image as useful for medical diagnosis and for determining the severity class according to the shape and values, by generating images based on real cases that could be used as new different stages of illness between healthy and unhealthy patients.



### Human Health Indicator Prediction from Gait Video
- **Arxiv ID**: http://arxiv.org/abs/2212.12948v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.12948v1)
- **Published**: 2022-12-25 19:10:37+00:00
- **Updated**: 2022-12-25 19:10:37+00:00
- **Authors**: Ziqing Li, Xuexin Yu, Xiaocong Lian, Yifeng Wang, Xiangyang Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Body Mass Index (BMI), age, height and weight are important indicators of human health conditions, which can provide useful information for plenty of practical purposes, such as health care, monitoring and re-identification. Most existing methods of health indicator prediction mainly use front-view body or face images. These inputs are hard to be obtained in daily life and often lead to the lack of robustness for the models, considering their strict requirements on view and pose. In this paper, we propose to employ gait videos to predict health indicators, which are more prevalent in surveillance and home monitoring scenarios. However, the study of health indicator prediction from gait videos using deep learning was hindered due to the small amount of open-sourced data. To address this issue, we analyse the similarity and relationship between pose estimation and health indicator prediction tasks, and then propose a paradigm enabling deep learning for small health indicator datasets by pre-training on the pose estimation task. Furthermore, to better suit the health indicator prediction task, we bring forward Global-Local Aware aNd Centrosymmetric Encoder (GLANCE) module. It first extracts local and global features by progressive convolutions and then fuses multi-level features by a centrosymmetric double-path hourglass structure in two different ways.   Experiments demonstrate that the proposed paradigm achieves state-of-the-art results for predicting health indicators on MoVi, and that the GLANCE module is also beneficial for pose estimation on 3DPW.



### Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program
- **Arxiv ID**: http://arxiv.org/abs/2212.12952v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.12952v2)
- **Published**: 2022-12-25 19:56:21+00:00
- **Updated**: 2023-04-06 20:39:16+00:00
- **Authors**: Tiange Luo, Honglak Lee, Justin Johnson
- **Comment**: TMLR; project page:
  https://tiangeluo.github.io/projectpages/shapecompiler.html
- **Journal**: None
- **Summary**: 3D shapes have complementary abstractions from low-level geometry to part-based hierarchies to languages, which convey different levels of information. This paper presents a unified framework to translate between pairs of shape abstractions: $\textit{Text}$ $\Longleftrightarrow$ $\textit{Point Cloud}$ $\Longleftrightarrow$ $\textit{Program}$. We propose $\textbf{Neural Shape Compiler}$ to model the abstraction transformation as a conditional generation process. It converts 3D shapes of three abstract types into unified discrete shape code, transforms each shape code into code of other abstract types through the proposed $\textit{ShapeCode Transformer}$, and decodes them to output the target shape abstraction. Point Cloud code is obtained in a class-agnostic way by the proposed $\textit{Point}$VQVAE. On Text2Shape, ShapeGlot, ABO, Genre, and Program Synthetic datasets, Neural Shape Compiler shows strengths in $\textit{Text}$ $\Longrightarrow$ $\textit{Point Cloud}$, $\textit{Point Cloud}$ $\Longrightarrow$ $\textit{Text}$, $\textit{Point Cloud}$ $\Longrightarrow$ $\textit{Program}$, and Point Cloud Completion tasks. Additionally, Neural Shape Compiler benefits from jointly training on all heterogeneous data and tasks.



### BD-KD: Balancing the Divergences for Online Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2212.12965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12965v1)
- **Published**: 2022-12-25 22:27:32+00:00
- **Updated**: 2022-12-25 22:27:32+00:00
- **Authors**: Ibtihel Amara, Nazanin Sepahvand, Brett H. Meyer, Warren J. Gross, James J. Clark
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation (KD) has gained a lot of attention in the field of model compression for edge devices thanks to its effectiveness in compressing large powerful networks into smaller lower-capacity models. Online distillation, in which both the teacher and the student are learning collaboratively, has also gained much interest due to its ability to improve on the performance of the networks involved. The Kullback-Leibler (KL) divergence ensures the proper knowledge transfer between the teacher and student. However, most online KD techniques present some bottlenecks under the network capacity gap. By cooperatively and simultaneously training, the models the KL distance becomes incapable of properly minimizing the teacher's and student's distributions. Alongside accuracy, critical edge device applications are in need of well-calibrated compact networks. Confidence calibration provides a sensible way of getting trustworthy predictions. We propose BD-KD: Balancing of Divergences for online Knowledge Distillation. We show that adaptively balancing between the reverse and forward divergences shifts the focus of the training strategy to the compact student network without limiting the teacher network's learning process. We demonstrate that, by performing this balancing design at the level of the student distillation loss, we improve upon both performance accuracy and calibration of the compact student network. We conducted extensive experiments using a variety of network architectures and show improvements on multiple datasets including CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet. We illustrate the effectiveness of our approach through comprehensive comparisons and ablations with current state-of-the-art online and offline KD techniques.



### A Combined Approach Toward Consistent Reconstructions of Indoor Spaces Based on 6D RGB-D Odometry and KinectFusion
- **Arxiv ID**: http://arxiv.org/abs/2212.14772v1
- **DOI**: 10.1145/2629673
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2212.14772v1)
- **Published**: 2022-12-25 22:52:25+00:00
- **Updated**: 2022-12-25 22:52:25+00:00
- **Authors**: Nadia Figueroa, Haiwei Dong, Abdulmotaleb El Saddik
- **Comment**: None
- **Journal**: ACM Trans. Intell. Syst., vol. 6, no. 2, pp. 14:1-10, 2015
- **Summary**: We propose a 6D RGB-D odometry approach that finds the relative camera pose between consecutive RGB-D frames by keypoint extraction and feature matching both on the RGB and depth image planes. Furthermore, we feed the estimated pose to the highly accurate KinectFusion algorithm, which uses a fast ICP (Iterative Closest Point) to fine-tune the frame-to-frame relative pose and fuse the depth data into a global implicit surface. We evaluate our method on a publicly available RGB-D SLAM benchmark dataset by Sturm et al. The experimental results show that our proposed reconstruction method solely based on visual odometry and KinectFusion outperforms the state-of-the-art RGB-D SLAM system accuracy. Moreover, our algorithm outputs a ready-to-use polygon mesh (highly suitable for creating 3D virtual worlds) without any postprocessing steps.



