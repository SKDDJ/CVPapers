# Arxiv Papers in cs.CV on 2022-12-24
### Out-of-Distribution Detection with Reconstruction Error and Typicality-based Penalty
- **Arxiv ID**: http://arxiv.org/abs/2212.12641v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.12641v1)
- **Published**: 2022-12-24 03:10:28+00:00
- **Updated**: 2022-12-24 03:10:28+00:00
- **Authors**: Genki Osada, Takahashi Tsubasa, Budrul Ahsan, Takashi Nishide
- **Comment**: Accepted at WACV 2023
- **Journal**: None
- **Summary**: The task of out-of-distribution (OOD) detection is vital to realize safe and reliable operation for real-world applications. After the failure of likelihood-based detection in high dimensions had been shown, approaches based on the \emph{typical set} have been attracting attention; however, they still have not achieved satisfactory performance. Beginning by presenting the failure case of the typicality-based approach, we propose a new reconstruction error-based approach that employs normalizing flow (NF). We further introduce a typicality-based penalty, and by incorporating it into the reconstruction error in NF, we propose a new OOD detection method, penalized reconstruction error (PRE). Because the PRE detects test inputs that lie off the in-distribution manifold, it effectively detects adversarial examples as well as OOD examples. We show the effectiveness of our method through the evaluation using natural image datasets, CIFAR-10, TinyImageNet, and ILSVRC2012.



### HandsOff: Labeled Dataset Generation With No Additional Human Annotations
- **Arxiv ID**: http://arxiv.org/abs/2212.12645v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2212.12645v2)
- **Published**: 2022-12-24 03:37:02+00:00
- **Updated**: 2023-03-30 19:38:05+00:00
- **Authors**: Austin Xu, Mariya I. Vasileva, Achal Dave, Arjun Seshadri
- **Comment**: 22 pages, 20 figures. CVPR 2023
- **Journal**: None
- **Summary**: Recent work leverages the expressive power of generative adversarial networks (GANs) to generate labeled synthetic datasets. These dataset generation methods often require new annotations of synthetic images, which forces practitioners to seek out annotators, curate a set of synthetic images, and ensure the quality of generated labels. We introduce the HandsOff framework, a technique capable of producing an unlimited number of synthetic images and corresponding labels after being trained on less than 50 pre-existing labeled images. Our framework avoids the practical drawbacks of prior work by unifying the field of GAN inversion with dataset generation. We generate datasets with rich pixel-wise labels in multiple challenging domains such as faces, cars, full-body human poses, and urban driving scenes. Our method achieves state-of-the-art performance in semantic segmentation, keypoint detection, and depth estimation compared to prior dataset generation approaches and transfer learning baselines. We additionally showcase its ability to address broad challenges in model development which stem from fixed, hand-annotated datasets, such as the long-tail problem in semantic segmentation. Project page: austinxu87.github.io/handsoff.



### Hyperspherical Loss-Aware Ternary Quantization
- **Arxiv ID**: http://arxiv.org/abs/2212.12649v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.12649v1)
- **Published**: 2022-12-24 04:27:01+00:00
- **Updated**: 2022-12-24 04:27:01+00:00
- **Authors**: Dan Liu, Xue Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the existing works use projection functions for ternary quantization in discrete space. Scaling factors and thresholds are used in some cases to improve the model accuracy. However, the gradients used for optimization are inaccurate and result in a notable accuracy gap between the full precision and ternary models. To get more accurate gradients, some works gradually increase the discrete portion of the full precision weights in the forward propagation pass, e.g., using temperature-based Sigmoid function. Instead of directly performing ternary quantization in discrete space, we push full precision weights close to ternary ones through regularization term prior to ternary quantization. In addition, inspired by the temperature-based method, we introduce a re-scaling factor to obtain more accurate gradients by simulating the derivatives of Sigmoid function. The experimental results show that our method can significantly improve the accuracy of ternary quantization in both image classification and object detection tasks.



### Pruning On-the-Fly: A Recoverable Pruning Method without Fine-tuning
- **Arxiv ID**: http://arxiv.org/abs/2212.12651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12651v1)
- **Published**: 2022-12-24 04:33:03+00:00
- **Updated**: 2022-12-24 04:33:03+00:00
- **Authors**: Dan Liu, Xue Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing pruning works are resource-intensive, requiring retraining or fine-tuning of the pruned models for accuracy. We propose a retraining-free pruning method based on hyperspherical learning and loss penalty terms. The proposed loss penalty term pushes some of the model weights far from zero, while the rest weight values are pushed near zero and can be safely pruned with no need for retraining and a negligible accuracy drop. In addition, our proposed method can instantly recover the accuracy of a pruned model by replacing the pruned values with their mean value. Our method obtains state-of-the-art results in retraining-free pruning and is evaluated on ResNet-18/50 and MobileNetV2 with ImageNet dataset. One can easily get a 50\% pruned ResNet18 model with a 0.47\% accuracy drop. With fine-tuning, the experiment results show that our method can significantly boost the accuracy of the pruned models compared with existing works. For example, the accuracy of a 70\% pruned (except the first convolutional layer) MobileNetV2 model only drops 3.5\%, much less than the 7\% $\sim$ 10\% accuracy drop with conventional methods.



### Hyperspherical Quantization: Toward Smaller and More Accurate Models
- **Arxiv ID**: http://arxiv.org/abs/2212.12653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12653v1)
- **Published**: 2022-12-24 04:42:15+00:00
- **Updated**: 2022-12-24 04:42:15+00:00
- **Authors**: Dan Liu, Xi Chen, Chen Ma, Xue Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Model quantization enables the deployment of deep neural networks under resource-constrained devices. Vector quantization aims at reducing the model size by indexing model weights with full-precision embeddings, i.e., codewords, while the index needs to be restored to 32-bit during computation. Binary and other low-precision quantization methods can reduce the model size up to 32$\times$, however, at the cost of a considerable accuracy drop. In this paper, we propose an efficient framework for ternary quantization to produce smaller and more accurate compressed models. By integrating hyperspherical learning, pruning and reinitialization, our proposed Hyperspherical Quantization (HQ) method reduces the cosine distance between the full-precision and ternary weights, thus reducing the bias of the straight-through gradient estimator during ternary quantization. Compared with existing work at similar compression levels ($\sim$30$\times$, $\sim$40$\times$), our method significantly improves the test accuracy and reduces the model size.



### Risk assessment and mitigation of e-scooter crashes with naturalistic driving data
- **Arxiv ID**: http://arxiv.org/abs/2212.12660v2
- **DOI**: None
- **Categories**: **eess.SY**, cs.CV, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/2212.12660v2)
- **Published**: 2022-12-24 05:28:31+00:00
- **Updated**: 2023-01-15 16:29:23+00:00
- **Authors**: Avinash Prabu, Zhengming Zhang, Renran Tian, Stanley Chien, Lingxi Li, Yaobin Chen, Rini Sherony
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, e-scooter-involved crashes have increased significantly but little information is available about the behaviors of on-road e-scooter riders. Most existing e-scooter crash research was based on retrospectively descriptive media reports, emergency room patient records, and crash reports. This paper presents a naturalistic driving study with a focus on e-scooter and vehicle encounters. The goal is to quantitatively measure the behaviors of e-scooter riders in different encounters to help facilitate crash scenario modeling, baseline behavior modeling, and the potential future development of in-vehicle mitigation algorithms. The data was collected using an instrumented vehicle and an e-scooter rider wearable system, respectively. A three-step data analysis process is developed. First, semi-automatic data labeling extracts e-scooter rider images and non-rider human images in similar environments to train an e-scooter-rider classifier. Then, a multi-step scene reconstruction pipeline generates vehicle and e-scooter trajectories in all encounters. The final step is to model e-scooter rider behaviors and e-scooter-vehicle encounter scenarios. A total of 500 vehicle to e-scooter interactions are analyzed. The variables pertaining to the same are also discussed in this paper.



### Differentiable Rendering for Pose Estimation in Proximity Operations
- **Arxiv ID**: http://arxiv.org/abs/2212.12668v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2212.12668v1)
- **Published**: 2022-12-24 06:12:16+00:00
- **Updated**: 2022-12-24 06:12:16+00:00
- **Authors**: Ramchander Rao Bhaskara, Roshan Thomas Eapen, Manoranjan Majji
- **Comment**: AIAA SciTech Forum 2023, 13 pages, 9 figures
- **Journal**: None
- **Summary**: Differentiable rendering aims to compute the derivative of the image rendering function with respect to the rendering parameters. This paper presents a novel algorithm for 6-DoF pose estimation through gradient-based optimization using a differentiable rendering pipeline. We emphasize two key contributions: (1) instead of solving the conventional 2D to 3D correspondence problem and computing reprojection errors, images (rendered using the 3D model) are compared only in the 2D feature space via sparse 2D feature correspondences. (2) Instead of an analytical image formation model, we compute an approximate local gradient of the rendering process through online learning. The learning data consists of image features extracted from multi-viewpoint renders at small perturbations in the pose neighborhood. The gradients are propagated through the rendering pipeline for the 6-DoF pose estimation using nonlinear least squares. This gradient-based optimization regresses directly upon the pose parameters by aligning the 3D model to reproduce a reference image shape. Using representative experiments, we demonstrate the application of our approach to pose estimation in proximity operations.



### Towards Blind Watermarking: Combining Invertible and Non-invertible Mechanisms
- **Arxiv ID**: http://arxiv.org/abs/2212.12678v1
- **DOI**: 10.1145/3503161.3547950
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.12678v1)
- **Published**: 2022-12-24 07:51:09+00:00
- **Updated**: 2022-12-24 07:51:09+00:00
- **Authors**: Rui Ma, Mengxi Guo, Yi Hou, Fan Yang, Yuan Li, Huizhu Jia, Xiaodong Xie
- **Comment**: 9 pages, 9 figures, 5 tables
- **Journal**: None
- **Summary**: Blind watermarking provides powerful evidence for copyright protection, image authentication, and tampering identification. However, it remains a challenge to design a watermarking model with high imperceptibility and robustness against strong noise attacks. To resolve this issue, we present a framework Combining the Invertible and Non-invertible (CIN) mechanisms. The CIN is composed of the invertible part to achieve high imperceptibility and the non-invertible part to strengthen the robustness against strong noise attacks. For the invertible part, we develop a diffusion and extraction module (DEM) and a fusion and split module (FSM) to embed and extract watermarks symmetrically in an invertible way. For the non-invertible part, we introduce a non-invertible attention-based module (NIAM) and the noise-specific selection module (NSM) to solve the asymmetric extraction under a strong noise attack. Extensive experiments demonstrate that our framework outperforms the current state-of-the-art methods of imperceptibility and robustness significantly. Our framework can achieve an average of 99.99% accuracy and 67.66 dB PSNR under noise-free conditions, while 96.64% and 39.28 dB combined strong noise attacks. The code will be available in https://github.com/rmpku/CIN.



### MURPHY: Relations Matter in Surgical Workflow Analysis
- **Arxiv ID**: http://arxiv.org/abs/2212.12719v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12719v1)
- **Published**: 2022-12-24 12:09:38+00:00
- **Updated**: 2022-12-24 12:09:38+00:00
- **Authors**: Shang Zhao, Yanzhe Liu, Qiyuan Wang, Dai Sun, Rong Liu, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous robotic surgery has advanced significantly based on analysis of visual and temporal cues in surgical workflow, but relational cues from domain knowledge remain under investigation. Complex relations in surgical annotations can be divided into intra- and inter-relations, both valuable to autonomous systems to comprehend surgical workflows. Intra- and inter-relations describe the relevance of various categories within a particular annotation type and the relevance of different annotation types, respectively. This paper aims to systematically investigate the importance of relational cues in surgery. First, we contribute the RLLS12M dataset, a large-scale collection of robotic left lateral sectionectomy (RLLS), by curating 50 videos of 50 patients operated by 5 surgeons and annotating a hierarchical workflow, which consists of 3 inter- and 6 intra-relations, 6 steps, 15 tasks, and 38 activities represented as the triplet of 11 instruments, 8 actions, and 16 objects, totaling 2,113,510 video frames and 12,681,060 annotation entities. Correspondingly, we propose a multi-relation purification hybrid network (MURPHY), which aptly incorporates novel relation modules to augment the feature representation by purifying relational features using the intra- and inter-relations embodied in annotations. The intra-relation module leverages a R-GCN to implant visual features in different graph relations, which are aggregated using a targeted relation purification with affinity information measuring label consistency and feature similarity. The inter-relation module is motivated by attention mechanisms to regularize the influence of relational features based on the hierarchy of annotation types from the domain knowledge. Extensive experimental results on the curated RLLS dataset confirm the effectiveness of our approach, demonstrating that relations matter in surgical workflow analysis.



### Boosting Out-of-Distribution Detection with Multiple Pre-trained Models
- **Arxiv ID**: http://arxiv.org/abs/2212.12720v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.12720v2)
- **Published**: 2022-12-24 12:11:38+00:00
- **Updated**: 2023-01-12 08:32:14+00:00
- **Authors**: Feng Xue, Zi He, Chuanlong Xie, Falong Tan, Zhenguo Li
- **Comment**: None
- **Journal**: None
- **Summary**: Out-of-Distribution (OOD) detection, i.e., identifying whether an input is sampled from a novel distribution other than the training distribution, is a critical task for safely deploying machine learning systems in the open world. Recently, post hoc detection utilizing pre-trained models has shown promising performance and can be scaled to large-scale problems. This advance raises a natural question: Can we leverage the diversity of multiple pre-trained models to improve the performance of post hoc detection methods? In this work, we propose a detection enhancement method by ensembling multiple detection decisions derived from a zoo of pre-trained models. Our approach uses the p-value instead of the commonly used hard threshold and leverages a fundamental framework of multiple hypothesis testing to control the true positive rate of In-Distribution (ID) data. We focus on the usage of model zoos and provide systematic empirical comparisons with current state-of-the-art methods on various OOD detection benchmarks. The proposed ensemble scheme shows consistent improvement compared to single-model detectors and significantly outperforms the current competitive methods. Our method substantially improves the relative performance by 65.40% and 26.96% on the CIFAR10 and ImageNet benchmarks.



### Polarimetric Multi-View Inverse Rendering
- **Arxiv ID**: http://arxiv.org/abs/2212.12721v1
- **DOI**: 10.1109/TPAMI.2022.3232211
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12721v1)
- **Published**: 2022-12-24 12:12:12+00:00
- **Updated**: 2022-12-24 12:12:12+00:00
- **Authors**: Jinyu Zhao, Yusuke Monno, Masatoshi Okutomi
- **Comment**: Paper accepted in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (2022). arXiv admin note: substantial text overlap with
  arXiv:2007.08830
- **Journal**: None
- **Summary**: A polarization camera has great potential for 3D reconstruction since the angle of polarization (AoP) and the degree of polarization (DoP) of reflected light are related to an object's surface normal. In this paper, we propose a novel 3D reconstruction method called Polarimetric Multi-View Inverse Rendering (Polarimetric MVIR) that effectively exploits geometric, photometric, and polarimetric cues extracted from input multi-view color-polarization images. We first estimate camera poses and an initial 3D model by geometric reconstruction with a standard structure-from-motion and multi-view stereo pipeline. We then refine the initial model by optimizing photometric rendering errors and polarimetric errors using multi-view RGB, AoP, and DoP images, where we propose a novel polarimetric cost function that enables an effective constraint on the estimated surface normal of each vertex, while considering four possible ambiguous azimuth angles revealed from the AoP measurement. The weight for the polarimetric cost is effectively determined based on the DoP measurement, which is regarded as the reliability of polarimetric information. Experimental results using both synthetic and real data demonstrate that our Polarimetric MVIR can reconstruct a detailed 3D shape without assuming a specific surface material and lighting condition.



### Frequency Regularization for Improving Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2212.12732v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12732v1)
- **Published**: 2022-12-24 13:14:45+00:00
- **Updated**: 2022-12-24 13:14:45+00:00
- **Authors**: Binxiao Huang, Chaofan Tao, Rui Lin, Ngai Wong
- **Comment**: accepted by AAAI 2023 workshop
- **Journal**: None
- **Summary**: Deep neural networks are incredibly vulnerable to crafted, human-imperceptible adversarial perturbations. Although adversarial training (AT) has proven to be an effective defense approach, we find that the AT-trained models heavily rely on the input low-frequency content for judgment, accounting for the low standard accuracy. To close the large gap between the standard and robust accuracies during AT, we investigate the frequency difference between clean and adversarial inputs, and propose a frequency regularization (FR) to align the output difference in the spectral domain. Besides, we find Stochastic Weight Averaging (SWA), by smoothing the kernels over epochs, further improves the robustness. Among various defense schemes, our method achieves the strongest robustness against attacks by PGD-20, C\&W and Autoattack, on a WideResNet trained on CIFAR-10 without any extra data.



### Artificial Pupil Dilation for Data Augmentation in Iris Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2212.12733v1
- **DOI**: 10.1109/ETCM56276.2022.9935749
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12733v1)
- **Published**: 2022-12-24 13:31:56+00:00
- **Updated**: 2022-12-24 13:31:56+00:00
- **Authors**: Daniel P. Benalcazar, David A. Benalcazar, Andres Valenzuela
- **Comment**: 6 pages, 7 figures, 2 tables
- **Journal**: 2022 IEEE Sixth Ecuador Technical Chapters Meeting (ETCM), 2022,
  pp. 1-6
- **Summary**: Biometrics is the science of identifying an individual based on their intrinsic anatomical or behavioural characteristics, such as fingerprints, face, iris, gait, and voice. Iris recognition is one of the most successful methods because it exploits the rich texture of the human iris, which is unique even for twins and does not degrade with age. Modern approaches to iris recognition utilize deep learning to segment the valid portion of the iris from the rest of the eye, so it can then be encoded, stored and compared. This paper aims to improve the accuracy of iris semantic segmentation systems by introducing a novel data augmentation technique. Our method can transform an iris image with a certain dilation level into any desired dilation level, thus augmenting the variability and number of training examples from a small dataset. The proposed method is fast and does not require training. The results indicate that our data augmentation method can improve segmentation accuracy up to 15% for images with high pupil dilation, which creates a more reliable iris recognition pipeline, even under extreme dilation.



### DDH-QA: A Dynamic Digital Humans Quality Assessment Database
- **Arxiv ID**: http://arxiv.org/abs/2212.12734v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12734v3)
- **Published**: 2022-12-24 13:35:31+00:00
- **Updated**: 2023-08-28 07:58:48+00:00
- **Authors**: Zicheng Zhang, Yingjie Zhou, Wei Sun, Wei Lu, Xiongkuo Min, Yu Wang, Guangtao Zhai
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, large amounts of effort have been put into pushing forward the real-world application of dynamic digital human (DDH). However, most current quality assessment research focuses on evaluating static 3D models and usually ignores motion distortions. Therefore, in this paper, we construct a large-scale dynamic digital human quality assessment (DDH-QA) database with diverse motion content as well as multiple distortions to comprehensively study the perceptual quality of DDHs. Both model-based distortion (noise, compression) and motion-based distortion (binding error, motion unnaturalness) are taken into consideration. Ten types of common motion are employed to drive the DDHs and a total of 800 DDHs are generated in the end. Afterward, we render the video sequences of the distorted DDHs as the evaluation media and carry out a well-controlled subjective experiment. Then a benchmark experiment is conducted with the state-of-the-art video quality assessment (VQA) methods and the experimental results show that existing VQA methods are limited in assessing the perceptual loss of DDHs.



### LMFLOSS: A Hybrid Loss For Imbalanced Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2212.12741v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2212.12741v1)
- **Published**: 2022-12-24 14:19:44+00:00
- **Updated**: 2022-12-24 14:19:44+00:00
- **Authors**: Abu Adnan Sadi, Labib Chowdhury, Nursrat Jahan, Mohammad Newaz Sharif Rafi, Radeya Chowdhury, Faisal Ahamed Khan, Nabeel Mohammed
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic medical image classification is a very important field where the use of AI has the potential to have a real social impact. However, there are still many challenges that act as obstacles to making practically effective solutions. One of those is the fact that most of the medical imaging datasets have a class imbalance problem. This leads to the fact that existing AI techniques, particularly neural network-based deep-learning methodologies, often perform poorly in such scenarios. Thus this makes this area an interesting and active research focus for researchers. In this study, we propose a novel loss function to train neural network models to mitigate this critical issue in this important field. Through rigorous experiments on three independently collected datasets of three different medical imaging domains, we empirically show that our proposed loss function consistently performs well with an improvement between 2%-10% macro f1 when compared to the baseline models. We hope that our work will precipitate new research toward a more generalized approach to medical image classification.



### GraffMatch: Global Matching of 3D Lines and Planes for Wide Baseline LiDAR Registration
- **Arxiv ID**: http://arxiv.org/abs/2212.12745v1
- **DOI**: 10.1109/LRA.2022.3229224
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.12745v1)
- **Published**: 2022-12-24 15:02:15+00:00
- **Updated**: 2022-12-24 15:02:15+00:00
- **Authors**: Parker C. Lusk, Devarth Parikh, Jonathan P. How
- **Comment**: accepted to RA-L; 8 pages. arXiv admin note: text overlap with
  arXiv:2205.08556
- **Journal**: IEEE Robotics and Automation Letters, vol. 8, no. 2, pp. 632-639,
  Feb. 2023
- **Summary**: Using geometric landmarks like lines and planes can increase navigation accuracy and decrease map storage requirements compared to commonly-used LiDAR point cloud maps. However, landmark-based registration for applications like loop closure detection is challenging because a reliable initial guess is not available. Global landmark matching has been investigated in the literature, but these methods typically use ad hoc representations of 3D line and plane landmarks that are not invariant to large viewpoint changes, resulting in incorrect matches and high registration error. To address this issue, we adopt the affine Grassmannian manifold to represent 3D lines and planes and prove that the distance between two landmarks is invariant to rotation and translation if a shift operation is performed before applying the Grassmannian metric. This invariance property enables the use of our graph-based data association framework for identifying landmark matches that can subsequently be used for registration in the least-squares sense. Evaluated on a challenging landmark matching and registration task using publicly-available LiDAR datasets, our approach yields a 1.7x and 3.5x improvement in successful registrations compared to methods that use viewpoint-dependent centroid and "closest point" representations, respectively.



### COLT: Cyclic Overlapping Lottery Tickets for Faster Pruning of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2212.12770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12770v1)
- **Published**: 2022-12-24 16:38:59+00:00
- **Updated**: 2022-12-24 16:38:59+00:00
- **Authors**: Md. Ismail Hossain, Mohammed Rakib, M. M. Lutfe Elahi, Nabeel Mohammed, Shafin Rahman
- **Comment**: None
- **Journal**: None
- **Summary**: Pruning refers to the elimination of trivial weights from neural networks. The sub-networks within an overparameterized model produced after pruning are often called Lottery tickets. This research aims to generate winning lottery tickets from a set of lottery tickets that can achieve similar accuracy to the original unpruned network. We introduce a novel winning ticket called Cyclic Overlapping Lottery Ticket (COLT) by data splitting and cyclic retraining of the pruned network from scratch. We apply a cyclic pruning algorithm that keeps only the overlapping weights of different pruned models trained on different data segments. Our results demonstrate that COLT can achieve similar accuracies (obtained by the unpruned model) while maintaining high sparsities. We show that the accuracy of COLT is on par with the winning tickets of Lottery Ticket Hypothesis (LTH) and, at times, is better. Moreover, COLTs can be generated using fewer iterations than tickets generated by the popular Iterative Magnitude Pruning (IMP) method. In addition, we also notice COLTs generated on large datasets can be transferred to small ones without compromising performance, demonstrating its generalizing capability. We conduct all our experiments on Cifar-10, Cifar-100 & TinyImageNet datasets and report superior performance than the state-of-the-art methods.



### DiP: Learning Discriminative Implicit Parts for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2212.13906v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.13906v2)
- **Published**: 2022-12-24 17:59:01+00:00
- **Updated**: 2023-03-13 03:21:43+00:00
- **Authors**: Dengjie Li, Siyu Chen, Yujie Zhong, Lin Ma
- **Comment**: None
- **Journal**: None
- **Summary**: In person re-identification (ReID) tasks, many works explore the learning of part features to improve the performance over global image features. Existing methods explicitly extract part features by either using a hand-designed image division or keypoints obtained with external visual systems. In this work, we propose to learn Discriminative implicit Parts (DiPs) which are decoupled from explicit body parts. Therefore, DiPs can learn to extract any discriminative features that can benefit in distinguishing identities, which is beyond predefined body parts (such as accessories). Moreover, we propose a novel implicit position to give a geometric interpretation for each DiP. The implicit position can also serve as a learning signal to encourage DiPs to be more position-equivariant with the identity in the image. Lastly, an additional DiP weighting is introduced to handle the invisible or occluded situation and further improve the feature representation of DiPs. Extensive experiments show that the proposed method achieves state-of-the-art performance on multiple person ReID benchmarks.



### Hybrid Representation Learning for Cognitive Diagnosis in Late-Life Depression Over 5 Years with Structural MRI
- **Arxiv ID**: http://arxiv.org/abs/2212.12810v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2212.12810v1)
- **Published**: 2022-12-24 19:53:11+00:00
- **Updated**: 2022-12-24 19:53:11+00:00
- **Authors**: Lintao Zhang, Lihong Wang, Minhui Yu, Rong Wu, David C. Steffens, Guy G. Potter, Mingxia Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Late-life depression (LLD) is a highly prevalent mood disorder occurring in older adults and is frequently accompanied by cognitive impairment (CI). Studies have shown that LLD may increase the risk of Alzheimer's disease (AD). However, the heterogeneity of presentation of geriatric depression suggests that multiple biological mechanisms may underlie it. Current biological research on LLD progression incorporates machine learning that combines neuroimaging data with clinical observations. There are few studies on incident cognitive diagnostic outcomes in LLD based on structural MRI (sMRI). In this paper, we describe the development of a hybrid representation learning (HRL) framework for predicting cognitive diagnosis over 5 years based on T1-weighted sMRI data. Specifically, we first extract prediction-oriented MRI features via a deep neural network, and then integrate them with handcrafted MRI features via a Transformer encoder for cognitive diagnosis prediction. Two tasks are investigated in this work, including (1) identifying cognitively normal subjects with LLD and never-depressed older healthy subjects, and (2) identifying LLD subjects who developed CI (or even AD) and those who stayed cognitively normal over five years. To the best of our knowledge, this is among the first attempts to study the complex heterogeneous progression of LLD based on task-oriented and handcrafted MRI features. We validate the proposed HRL on 294 subjects with T1-weighted MRIs from two clinically harmonized studies. Experimental results suggest that the HRL outperforms several classical machine learning and state-of-the-art deep learning methods in LLD identification and prediction tasks.



### Meta-Learning for Color-to-Infrared Cross-Modal Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2212.12824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2212.12824v1)
- **Published**: 2022-12-24 22:38:16+00:00
- **Updated**: 2022-12-24 22:38:16+00:00
- **Authors**: Evelyn A. Stump, Francesco Luzi, Leslie M. Collins, Jordan M. Malof
- **Comment**: None
- **Journal**: None
- **Summary**: Recent object detection models for infrared (IR) imagery are based upon deep neural networks (DNNs) and require large amounts of labeled training imagery. However, publicly-available datasets that can be used for such training are limited in their size and diversity. To address this problem, we explore cross-modal style transfer (CMST) to leverage large and diverse color imagery datasets so that they can be used to train DNN-based IR image based object detectors. We evaluate six contemporary stylization methods on four publicly-available IR datasets - the first comparison of its kind - and find that CMST is highly effective for DNN-based detectors. Surprisingly, we find that existing data-driven methods are outperformed by a simple grayscale stylization (an average of the color channels). Our analysis reveals that existing data-driven methods are either too simplistic or introduce significant artifacts into the imagery. To overcome these limitations, we propose meta-learning style transfer (MLST), which learns a stylization by composing and tuning well-behaved analytic functions. We find that MLST leads to more complex stylizations without introducing significant image artifacts and achieves the best overall detector performance on our benchmark datasets.



