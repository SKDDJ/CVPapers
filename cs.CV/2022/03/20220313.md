# Arxiv Papers in cs.CV on 2022-03-13
### Recursive 3D Segmentation of Shoulder Joint with Coarse-scanned MR Image
- **Arxiv ID**: http://arxiv.org/abs/2203.07846v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.07846v1)
- **Published**: 2022-03-13 00:24:11+00:00
- **Updated**: 2022-03-13 00:24:11+00:00
- **Authors**: Xiaoxiao He, Chaowei Tan, Virak Tan, Kang Li
- **Comment**: None
- **Journal**: None
- **Summary**: For diagnosis of shoulder illness, it is essential to look at the morphology deviation of scapula and humerus from the medical images that are acquired from Magnetic Resonance (MR) imaging. However, taking high-resolution MR images is time-consuming and costly because the reduction of the physical distance between image slices causes prolonged scanning time. Moreover, due to the lack of training images, images from various sources must be utilized, which creates the issue of high variance across the dataset. Also, there are human errors among the images due to the fact that it is hard to take the spatial relationship into consideration when labeling the 3D image in low resolution. In order to combat all obstacles stated above, we develop a fully automated algorithm for segmenting the humerus and scapula bone from coarsely scanned and low-resolution MR images and a recursive learning framework that iterative utilize the generated labels for reducing the errors among segmentations and increase our dataset set for training the next round network. In this study, 50 MR images are collected from several institutions and divided into five mutually exclusive sets for carrying five-fold cross-validation. Contours that are generated by the proposed method demonstrated a high level of accuracy when compared with ground truth and the traditional method. The proposed neural network and the recursive learning scheme improve the overall quality of the segmentation on humerus and scapula on the low-resolution dataset and reduced incorrect segmentation in the ground truth, which could have a positive impact on finding the cause of shoulder pain and patient's early relief.



### Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.06541v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06541v2)
- **Published**: 2022-03-13 01:15:23+00:00
- **Updated**: 2022-03-26 13:46:46+00:00
- **Authors**: Jiahao Xia, Weiwei qu, Wenjian Huang, Jianguo Zhang, Xi Wang, Min Xu
- **Comment**: Accepted to CVPR2022
- **Journal**: None
- **Summary**: Heatmap regression methods have dominated face alignment area in recent years while they ignore the inherent relation between different landmarks. In this paper, we propose a Sparse Local Patch Transformer (SLPT) for learning the inherent relation. The SLPT generates the representation of each single landmark from a local patch and aggregates them by an adaptive inherent relation based on the attention mechanism. The subpixel coordinate of each landmark is predicted independently based on the aggregated feature. Moreover, a coarse-to-fine framework is further introduced to incorporate with the SLPT, which enables the initial landmarks to gradually converge to the target facial landmarks using fine-grained features from dynamically resized local patches. Extensive experiments carried out on three popular benchmarks, including WFLW, 300W and COFW, demonstrate that the proposed method works at the state-of-the-art level with much less computational complexity by learning the inherent relation between facial landmarks. The code is available at the project website.



### Change Detection from Synthetic Aperture Radar Images via Dual Path Denoising Network
- **Arxiv ID**: http://arxiv.org/abs/2203.06543v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.06543v1)
- **Published**: 2022-03-13 01:51:51+00:00
- **Updated**: 2022-03-13 01:51:51+00:00
- **Authors**: Junjie Wang, Feng Gao, Junyu Dong, Qian Du, Heng-Chao Li
- **Comment**: Accepted by IEEE JSTARS
- **Journal**: None
- **Summary**: Benefited from the rapid and sustainable development of synthetic aperture radar (SAR) sensors, change detection from SAR images has received increasing attentions over the past few years. Existing unsupervised deep learning-based methods have made great efforts to exploit robust feature representations, but they consume much time to optimize parameters. Besides, these methods use clustering to obtain pseudo-labels for training, and the pseudo-labeled samples often involve errors, which can be considered as "label noise". To address these issues, we propose a Dual Path Denoising Network (DPDNet) for SAR image change detection. In particular, we introduce the random label propagation to clean the label noise involved in preclassification. We also propose the distinctive patch convolution for feature representation learning to reduce the time consumption. Specifically, the attention mechanism is used to select distinctive pixels in the feature maps, and patches around these pixels are selected as convolution kernels. Consequently, the DPDNet does not require a great number of training samples for parameter optimization, and its computational efficiency is greatly enhanced. Extensive experiments have been conducted on five SAR datasets to verify the proposed DPDNet. The experimental results demonstrate that our method outperforms several state-of-the-art methods in change detection results.



### CEKD:Cross Ensemble Knowledge Distillation for Augmented Fine-grained Data
- **Arxiv ID**: http://arxiv.org/abs/2203.06551v1
- **DOI**: 10.1007/s10489-022-03355-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06551v1)
- **Published**: 2022-03-13 02:57:25+00:00
- **Updated**: 2022-03-13 02:57:25+00:00
- **Authors**: Ke Zhang, Jin Fan, Shaoli Huang, Yongliang Qiao, Xiaofeng Yu, Feiwei Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation has been proved effective in training deep models. Existing data augmentation methods tackle the fine-grained problem by blending image pairs and fusing corresponding labels according to the statistics of mixed pixels, which produces additional noise harmful to the performance of networks. Motivated by this, we present a simple yet effective cross ensemble knowledge distillation (CEKD) model for fine-grained feature learning. We innovatively propose a cross distillation module to provide additional supervision to alleviate the noise problem, and propose a collaborative ensemble module to overcome the target conflict problem. The proposed model can be trained in an end-to-end manner, and only requires image-level label supervision. Extensive experiments on widely used fine-grained benchmarks demonstrate the effectiveness of our proposed model. Specifically, with the backbone of ResNet-101, CEKD obtains the accuracy of 89.59%, 95.96% and 94.56% in three datasets respectively, outperforming state-of-the-art API-Net by 0.99%, 1.06% and 1.16%.



### Contrastive Learning for Automotive mmWave Radar Detection Points Based Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.06553v3
- **DOI**: 10.1109/ITSC55140.2022.9922540
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06553v3)
- **Published**: 2022-03-13 03:00:34+00:00
- **Updated**: 2023-02-05 11:22:43+00:00
- **Authors**: Weiyi Xiong, Jianan Liu, Yuxuan Xia, Tao Huang, Bing Zhu, Wei Xiang
- **Comment**: Accepted by IEEE ITSC 2022
- **Journal**: None
- **Summary**: The automotive mmWave radar plays a key role in advanced driver assistance systems (ADAS) and autonomous driving. Deep learning-based instance segmentation enables real-time object identification from the radar detection points. In the conventional training process, accurate annotation is the key. However, high-quality annotations of radar detection points are challenging to achieve due to their ambiguity and sparsity. To address this issue, we propose a contrastive learning approach for implementing radar detection points-based instance segmentation. We define the positive and negative samples according to the ground-truth label, apply the contrastive loss to train the model first, and then perform fine-tuning for the following downstream task. In addition, these two steps can be merged into one, and pseudo labels can be generated for the unlabeled data to improve the performance further. Thus, there are four different training settings for our method. Experiments show that when the ground-truth information is only available for a small proportion of the training data, our method still achieves a comparable performance to the approach trained in a supervised manner with 100% ground-truth information.



### SATr: Slice Attention with Transformer for Universal Lesion Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.07373v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.07373v1)
- **Published**: 2022-03-13 03:37:27+00:00
- **Updated**: 2022-03-13 03:37:27+00:00
- **Authors**: Han Li, Long Chen, Hu Han, S. Kevin Zhou
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Universal Lesion Detection (ULD) in computed tomography plays an essential role in computer-aided diagnosis. Promising ULD results have been reported by multi-slice-input detection approaches which model 3D context from multiple adjacent CT slices, but such methods still experience difficulty in obtaining a global representation among different slices and within each individual slice since they only use convolution-based fusion operations. In this paper, we propose a novel Slice Attention Transformer (SATr) block which can be easily plugged into convolution-based ULD backbones to form hybrid network structures. Such newly formed hybrid backbones can better model long-distance feature dependency via the cascaded self-attention modules in the Transformer block while still holding a strong power of modeling local features with the convolutional operations in the original backbone. Experiments with five state-of-the-art methods show that the proposed SATr block can provide an almost free boost to lesion detection accuracy without extra hyperparameters or special network designs.



### AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.06558v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06558v4)
- **Published**: 2022-03-13 03:45:58+00:00
- **Updated**: 2022-04-15 05:05:38+00:00
- **Authors**: Xueyi Liu, Xiaomeng Xu, Anyi Rao, Chuang Gan, Li Yi
- **Comment**: 22 pages, CVPR 2022
- **Journal**: None
- **Summary**: Training a generalizable 3D part segmentation network is quite challenging but of great importance in real-world applications. To tackle this problem, some works design task-specific solutions by translating human understanding of the task to machine's learning process, which faces the risk of missing the optimal strategy since machines do not necessarily understand in the exact human way. Others try to use conventional task-agnostic approaches designed for domain generalization problems with no task prior knowledge considered. To solve the above issues, we propose AutoGPart, a generic method enabling training generalizable 3D part segmentation networks with the task prior considered. AutoGPart builds a supervision space with geometric prior knowledge encoded, and lets the machine to search for the optimal supervisions from the space for a specific segmentation task automatically. Extensive experiments on three generalizable 3D part segmentation tasks are conducted to demonstrate the effectiveness and versatility of AutoGPart. We demonstrate that the performance of segmentation networks using simple backbones can be significantly improved when trained with supervisions searched by our method.



### Query-Efficient Black-box Adversarial Attacks Guided by a Transfer-based Prior
- **Arxiv ID**: http://arxiv.org/abs/2203.06560v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2203.06560v1)
- **Published**: 2022-03-13 04:06:27+00:00
- **Updated**: 2022-03-13 04:06:27+00:00
- **Authors**: Yinpeng Dong, Shuyu Cheng, Tianyu Pang, Hang Su, Jun Zhu
- **Comment**: Accepted by IEEE Transactions on Pattern Recognition and Machine
  Intelligence (TPAMI). The official version is at
  https://ieeexplore.ieee.org/document/9609659
- **Journal**: None
- **Summary**: Adversarial attacks have been extensively studied in recent years since they can identify the vulnerability of deep learning models before deployed. In this paper, we consider the black-box adversarial setting, where the adversary needs to craft adversarial examples without access to the gradients of a target model. Previous methods attempted to approximate the true gradient either by using the transfer gradient of a surrogate white-box model or based on the feedback of model queries. However, the existing methods inevitably suffer from low attack success rates or poor query efficiency since it is difficult to estimate the gradient in a high-dimensional input space with limited information. To address these problems and improve black-box attacks, we propose two prior-guided random gradient-free (PRGF) algorithms based on biased sampling and gradient averaging, respectively. Our methods can take the advantage of a transfer-based prior given by the gradient of a surrogate model and the query information simultaneously. Through theoretical analyses, the transfer-based prior is appropriately integrated with model queries by an optimal coefficient in each method. Extensive experiments demonstrate that, in comparison with the alternative state-of-the-arts, both of our methods require much fewer queries to attack black-box models with higher success rates.



### Worst Case Matters for Few-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/2203.06574v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.06574v2)
- **Published**: 2022-03-13 05:39:40+00:00
- **Updated**: 2022-07-25 03:53:27+00:00
- **Authors**: Minghao Fu, Yun-Hao Cao, Jianxin Wu
- **Comment**: Accepted by ECCV2022
- **Journal**: None
- **Summary**: Few-shot recognition learns a recognition model with very few (e.g., 1 or 5) images per category, and current few-shot learning methods focus on improving the average accuracy over many episodes. We argue that in real-world applications we may often only try one episode instead of many, and hence maximizing the worst-case accuracy is more important than maximizing the average accuracy. We empirically show that a high average accuracy not necessarily means a high worst-case accuracy. Since this objective is not accessible, we propose to reduce the standard deviation and increase the average accuracy simultaneously. In turn, we devise two strategies from the bias-variance tradeoff perspective to implicitly reach this goal: a simple yet effective stability regularization (SR) loss together with model ensemble to reduce variance during fine-tuning, and an adaptability calibration mechanism to reduce the bias. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed strategies, which outperforms current state-of-the-art methods with a significant margin in terms of not only average, but also worst-case accuracy. Our code is available at https://github.com/heekhero/ACSR.



### CVFNet: Real-time 3D Object Detection by Learning Cross View Features
- **Arxiv ID**: http://arxiv.org/abs/2203.06585v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06585v2)
- **Published**: 2022-03-13 06:23:18+00:00
- **Updated**: 2022-07-15 03:10:58+00:00
- **Authors**: Jiaqi Gu, Zhiyu Xiang, Pan Zhao, Tingming Bai, Lingxuan Wang, Xijun Zhao, Zhiyuan Zhang
- **Comment**: 7 pages, 5 figures, accepted by IROS 2022
- **Journal**: None
- **Summary**: In recent years 3D object detection from LiDAR point clouds has made great progress thanks to the development of deep learning technologies. Although voxel or point based methods are popular in 3D object detection, they usually involve time-consuming operations such as 3D convolutions on voxels or ball query among points, making the resulting network inappropriate for time critical applications. On the other hand, 2D view-based methods feature high computing efficiency while usually obtaining inferior performance than the voxel or point based methods. In this work, we present a real-time view-based single stage 3D object detector, namely CVFNet to fulfill this task. To strengthen the cross-view feature learning under the condition of demanding efficiency, our framework extracts the features of different views and fuses them in an efficient progressive way. We first propose a novel Point-Range feature fusion module that deeply integrates point and range view features in multiple stages. Then, a special Slice Pillar is designed to well maintain the 3D geometry when transforming the obtained deep point-view features into bird's eye view. To better balance the ratio of samples, a sparse pillar detection head is presented to focus the detection on the nonempty grids. We conduct experiments on the popular KITTI and NuScenes benchmark, and state-of-the-art performances are achieved in terms of both accuracy and speed.



### AugShuffleNet: Communicate More, Compute Less
- **Arxiv ID**: http://arxiv.org/abs/2203.06589v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.06589v2)
- **Published**: 2022-03-13 07:01:23+00:00
- **Updated**: 2022-08-21 13:02:24+00:00
- **Authors**: Longqing Ye
- **Comment**: None
- **Journal**: None
- **Summary**: As a remarkable compact model, ShuffleNetV2 offers a good example to design efficient ConvNets but its limit is rarely noticed. In this paper, we rethink the design pattern of ShuffleNetV2 and find that the channel-wise redundancy problem still constrains the efficiency improvement of Shuffle block in the wider ShuffleNetV2. To resolve this issue, we propose another augmented variant of shuffle block in the form of bottleneck-like structure and more implicit short connections. To verify the effectiveness of this building block, we further build a more powerful and efficient model family, termed as AugShuffleNets. Evaluated on the CIFAR-10 and CIFAR-100 datasets, AugShuffleNet consistently outperforms ShuffleNetV2 in terms of accuracy with less computational cost and fewer parameter count.



### Masked Autoencoders for Point Cloud Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.06604v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06604v2)
- **Published**: 2022-03-13 09:23:39+00:00
- **Updated**: 2022-03-28 05:01:22+00:00
- **Authors**: Yatian Pang, Wenxiao Wang, Francis E. H. Tay, Wei Liu, Yonghong Tian, Li Yuan
- **Comment**: https://github.com/Pang-Yatian/Point-MAE
- **Journal**: None
- **Summary**: As a promising scheme of self-supervised learning, masked autoencoding has significantly advanced natural language processing and computer vision. Inspired by this, we propose a neat scheme of masked autoencoders for point cloud self-supervised learning, addressing the challenges posed by point cloud's properties, including leakage of location information and uneven information density. Concretely, we divide the input point cloud into irregular point patches and randomly mask them at a high ratio. Then, a standard Transformer based autoencoder, with an asymmetric design and a shifting mask tokens operation, learns high-level latent features from unmasked point patches, aiming to reconstruct the masked point patches. Extensive experiments show that our approach is efficient during pre-training and generalizes well on various downstream tasks. Specifically, our pre-trained models achieve 85.18% accuracy on ScanObjectNN and 94.04% accuracy on ModelNet40, outperforming all the other self-supervised learning methods. We show with our scheme, a simple architecture entirely based on standard Transformers can surpass dedicated Transformer models from supervised learning. Our approach also advances state-of-the-art accuracies by 1.5%-2.3% in the few-shot object classification. Furthermore, our work inspires the feasibility of applying unified architectures from languages and images to the point cloud.



### Depth-Aware Generative Adversarial Network for Talking Head Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2203.06605v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06605v2)
- **Published**: 2022-03-13 09:32:22+00:00
- **Updated**: 2022-03-15 01:34:02+00:00
- **Authors**: Fa-Ting Hong, Longhao Zhang, Li Shen, Dan Xu
- **Comment**: 15 Pages; Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Talking head video generation aims to produce a synthetic human face video that contains the identity and pose information respectively from a given source image and a driving video.Existing works for this task heavily rely on 2D representations (e.g. appearance and motion) learned from the input images. However, dense 3D facial geometry (e.g. pixel-wise depth) is extremely important for this task as it is particularly beneficial for us to essentially generate accurate 3D face structures and distinguish noisy information from the possibly cluttered background. Nevertheless, dense 3D geometry annotations are prohibitively costly for videos and are typically not available for this video generation task. In this paper, we first introduce a self-supervised geometry learning method to automatically recover the dense 3D geometry (i.e.depth) from the face videos without the requirement of any expensive 3D annotation data. Based on the learned dense depth maps, we further propose to leverage them to estimate sparse facial keypoints that capture the critical movement of the human head. In a more dense way, the depth is also utilized to learn 3D-aware cross-modal (i.e. appearance and depth) attention to guide the generation of motion fields for warping source image representations. All these contributions compose a novel depth-aware generative adversarial network (DaGAN) for talking head generation. Extensive experiments conducted demonstrate that our proposed method can generate highly realistic faces, and achieve significant results on the unseen human faces.



### Context-LSTM: a robust classifier for video detection on UCF101
- **Arxiv ID**: http://arxiv.org/abs/2203.06610v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.06610v1)
- **Published**: 2022-03-13 09:43:27+00:00
- **Updated**: 2022-03-13 09:43:27+00:00
- **Authors**: Dengshan Li, Rujing Wang
- **Comment**: 15 pages,6 figures
- **Journal**: None
- **Summary**: Video detection and human action recognition may be computationally expensive, and need a long time to train models. In this paper, we were intended to reduce the training time and the GPU memory usage of video detection, and achieved a competitive detection accuracy. Other research works such as Two-stream, C3D, TSN have shown excellent performance on UCF101. Here, we used a LSTM structure simply for video detection. We used a simple structure to perform a competitive top-1 accuracy on the entire validation dataset of UCF101. The LSTM structure is named Context-LSTM, since it may process the deep temporal features. The Context-LSTM may simulate the human recognition system. We cascaded the LSTM blocks in PyTorch and connected the cell state flow and hidden output flow. At the connection of the blocks, we used ReLU, Batch Normalization, and MaxPooling functions. The Context-LSTM could reduce the training time and the GPU memory usage, while keeping a state-of-the-art top-1 accuracy on UCF101 entire validation dataset, show a robust performance on video action detection.



### A Single Correspondence Is Enough: Robust Global Registration to Avoid Degeneracy in Urban Environments
- **Arxiv ID**: http://arxiv.org/abs/2203.06612v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.06612v1)
- **Published**: 2022-03-13 09:57:52+00:00
- **Updated**: 2022-03-13 09:57:52+00:00
- **Authors**: Hyungtae Lim, Suyong Yeon, Soohyun Ryu, Yonghan Lee, Youngji Kim, Jaeseong Yun, Euigon Jung, Donghwan Lee, Hyun Myung
- **Comment**: 8 pages. Acccepted by ICRA 2022
- **Journal**: None
- **Summary**: Global registration using 3D point clouds is a crucial technology for mobile platforms to achieve localization or manage loop-closing situations. In recent years, numerous researchers have proposed global registration methods to address a large number of outlier correspondences. Unfortunately, the degeneracy problem, which represents the phenomenon in which the number of estimated inliers becomes lower than three, is still potentially inevitable. To tackle the problem, a degeneracy-robust decoupling-based global registration method is proposed, called Quatro. In particular, our method employs quasi-SO(3) estimation by leveraging the Atlanta world assumption in urban environments to avoid degeneracy in rotation estimation. Thus, the minimum degree of freedom (DoF) of our method is reduced from three to one. As verified in indoor and outdoor 3D LiDAR datasets, our proposed method yields robust global registration performance compared with other global registration methods, even for distant point cloud pairs. Furthermore, the experimental results confirm the applicability of our method as a coarse alignment. Our code is available: https://github.com/url-kaist/quatro.



### LAS-AT: Adversarial Training with Learnable Attack Strategy
- **Arxiv ID**: http://arxiv.org/abs/2203.06616v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06616v1)
- **Published**: 2022-03-13 10:21:26+00:00
- **Updated**: 2022-03-13 10:21:26+00:00
- **Authors**: Xiaojun Jia, Yong Zhang, Baoyuan Wu, Ke Ma, Jue Wang, Xiaochun Cao
- **Comment**: None
- **Journal**: CVPR 2022
- **Summary**: Adversarial training (AT) is always formulated as a minimax problem, of which the performance depends on the inner optimization that involves the generation of adversarial examples (AEs). Most previous methods adopt Projected Gradient Decent (PGD) with manually specifying attack parameters for AE generation. A combination of the attack parameters can be referred to as an attack strategy. Several works have revealed that using a fixed attack strategy to generate AEs during the whole training phase limits the model robustness and propose to exploit different attack strategies at different training stages to improve robustness. But those multi-stage hand-crafted attack strategies need much domain expertise, and the robustness improvement is limited. In this paper, we propose a novel framework for adversarial training by introducing the concept of "learnable attack strategy", dubbed LAS-AT, which learns to automatically produce attack strategies to improve the model robustness. Our framework is composed of a target network that uses AEs for training to improve robustness and a strategy network that produces attack strategies to control the AE generation. Experimental evaluations on three benchmark databases demonstrate the superiority of the proposed method. The code is released at https://github.com/jiaxiaojunQAQ/LAS-AT.



### Multi-Bracket High Dynamic Range Imaging with Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/2203.06622v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.06622v3)
- **Published**: 2022-03-13 11:10:47+00:00
- **Updated**: 2022-04-28 08:18:37+00:00
- **Authors**: Nico Messikommer, Stamatios Georgoulis, Daniel Gehrig, Stepan Tulyakov, Julius Erbach, Alfredo Bochicchio, Yuanyou Li, Davide Scaramuzza
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition
  Workshop (CVPRW), New Orleans, 2022
- **Summary**: Modern high dynamic range (HDR) imaging pipelines align and fuse multiple low dynamic range (LDR) images captured at different exposure times. While these methods work well in static scenes, dynamic scenes remain a challenge since the LDR images still suffer from saturation and noise. In such scenarios, event cameras would be a valid complement, thanks to their higher temporal resolution and dynamic range. In this paper, we propose the first multi-bracket HDR pipeline combining a standard camera with an event camera. Our results show better overall robustness when using events, with improvements in PSNR by up to 5dB on synthetic data and up to 0.7dB on real-world data. We also introduce a new dataset containing bracketed LDR images with aligned events and HDR ground truth.



### Revisiting Deep Semi-supervised Learning: An Empirical Distribution Alignment Framework and Its Generalization Bound
- **Arxiv ID**: http://arxiv.org/abs/2203.06639v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06639v1)
- **Published**: 2022-03-13 11:59:52+00:00
- **Updated**: 2022-03-13 11:59:52+00:00
- **Authors**: Feiyu Wang, Qin Wang, Wen Li, Dong Xu, Luc Van Gool
- **Comment**: Submitted to T-PAMI on August 2021
- **Journal**: None
- **Summary**: In this work, we revisit the semi-supervised learning (SSL) problem from a new perspective of explicitly reducing empirical distribution mismatch between labeled and unlabeled samples. Benefited from this new perspective, we first propose a new deep semi-supervised learning framework called Semi-supervised Learning by Empirical Distribution Alignment (SLEDA), in which existing technologies from the domain adaptation community can be readily used to address the semi-supervised learning problem through reducing the empirical distribution distance between labeled and unlabeled data. Based on this framework, we also develop a new theoretical generalization bound for the research community to better understand the semi-supervised learning problem, in which we show the generalization error of semi-supervised learning can be effectively bounded by minimizing the training error on labeled data and the empirical distribution distance between labeled and unlabeled data. Building upon our new framework and the theoretical bound, we develop a simple and effective deep semi-supervised learning method called Augmented Distribution Alignment Network (ADA-Net) by simultaneously adopting the well-established adversarial training strategy from the domain adaptation community and a simple sample interpolation strategy for data augmentation. Additionally, we incorporate both strategies in our ADA-Net into two exiting SSL methods to further improve their generalization capability, which indicates that our new framework provides a complementary solution for solving the SSL problem. Our comprehensive experimental results on two benchmark datasets SVHN and CIFAR-10 for the semi-supervised image recognition task and another two benchmark datasets ModelNet40 and ShapeNet55 for the semi-supervised point cloud recognition task demonstrate the effectiveness of our proposed framework for SSL.



### Global2Local: A Joint-Hierarchical Attention for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/2203.06663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06663v1)
- **Published**: 2022-03-13 14:31:54+00:00
- **Updated**: 2022-03-13 14:31:54+00:00
- **Authors**: Chengpeng Dai, Fuhai Chen, Xiaoshuai Sun, Rongrong Ji, Qixiang Ye, Yongjian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, automatic video captioning has attracted increasing attention, where the core challenge lies in capturing the key semantic items, like objects and actions as well as their spatial-temporal correlations from the redundant frames and semantic content. To this end, existing works select either the key video clips in a global level~(across multi frames), or key regions within each frame, which, however, neglect the hierarchical order, i.e., key frames first and key regions latter. In this paper, we propose a novel joint-hierarchical attention model for video captioning, which embeds the key clips, the key frames and the key regions jointly into the captioning model in a hierarchical manner. Such a joint-hierarchical attention model first conducts a global selection to identify key frames, followed by a Gumbel sampling operation to identify further key regions based on the key frames, achieving an accurate global-to-local feature representation to guide the captioning. Extensive quantitative evaluations on two public benchmark datasets MSVD and MSR-VTT demonstrates the superiority of the proposed method over the state-of-the-art methods.



### Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video
- **Arxiv ID**: http://arxiv.org/abs/2203.06667v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2203.06667v6)
- **Published**: 2022-03-13 14:42:53+00:00
- **Updated**: 2022-03-29 15:37:35+00:00
- **Authors**: Bin Li, Yixuan Weng, Bin Sun, Shutao Li
- **Comment**: 8 pages, 6 figures, 3 tables
- **Journal**: None
- **Summary**: The temporal answering grounding in the video (TAGV) is a new task naturally derived from temporal sentence grounding in the video (TSGV). Given an untrimmed video and a text question, this task aims at locating the matching span from the video that can semantically answer the question. Existing methods tend to formulate the TAGV task with a visual span-based question answering (QA) approach by matching the visual frame span queried by the text question. However, due to the weak correlations and huge gaps of the semantic features between the textual question and visual answer, existing methods adopting visual span predictor perform poorly in the TAGV task. To bridge these gaps, we propose a visual-prompt text span localizing (VPTSL) method, which introduces the timestamped subtitles as a passage to perform the text span localization for the input text question, and prompts the visual highlight features into the pre-trained language model (PLM) for enhancing the joint semantic representations. Specifically, the context query attention is utilized to perform cross-modal interaction between the extracted textual and visual features. Then, the highlight features are obtained through the video-text highlighting for the visual prompt. To alleviate semantic differences between textual and visual features, we design the text span predictor by encoding the question, the subtitles, and the prompted visual highlight features with the PLM. As a result, the TAGV task is formulated to predict the span of subtitles matching the visual answer. Extensive experiments on the medical instructional dataset, namely MedVidQA, show that the proposed VPTSL outperforms the state-of-the-art (SOTA) method by 28.36% in terms of mIOU with a large margin, which demonstrates the effectiveness of the proposed visual prompt and the text span predictor.



### PNM: Pixel Null Model for General Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.06677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06677v1)
- **Published**: 2022-03-13 15:17:41+00:00
- **Updated**: 2022-03-13 15:17:41+00:00
- **Authors**: Han Zhang, Zihao Zhang, Wenhao Zheng, Wei Xu
- **Comment**: None
- **Journal**: None
- **Summary**: A major challenge in image segmentation is classifying object boundaries. Recent efforts propose to refine the segmentation result with boundary masks. However, models are still prone to misclassifying boundary pixels even when they correctly capture the object contours. In such cases, even a perfect boundary map is unhelpful for segmentation refinement. In this paper, we argue that assigning proper prior weights to error-prone pixels such as object boundaries can significantly improve the segmentation quality. Specifically, we present the \textit{pixel null model} (PNM), a prior model that weights each pixel according to its probability of being correctly classified by a random segmenter. Empirical analysis shows that PNM captures the misclassification distribution of different state-of-the-art (SOTA) segmenters. Extensive experiments on semantic, instance, and panoptic segmentation tasks over three datasets (Cityscapes, ADE20K, MS COCO) confirm that PNM consistently improves the segmentation quality of most SOTA methods (including the vision transformers) and outperforms boundary-based methods by a large margin. We also observe that the widely-used mean IoU (mIoU) metric is insensitive to boundaries of different sharpness. As a byproduct, we propose a new metric, \textit{PNM IoU}, which perceives the boundary sharpness and better reflects the model segmentation performance in error-prone regions.



### Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors
- **Arxiv ID**: http://arxiv.org/abs/2203.06691v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06691v3)
- **Published**: 2022-03-13 15:55:00+00:00
- **Updated**: 2022-04-19 19:49:51+00:00
- **Authors**: Naser Damer, César Augusto Fontanillo López, Meiling Fang, Noémie Spiller, Minh Vu Pham, Fadi Boutros
- **Comment**: Accepted at CVPR Workshops 2022
- **Journal**: None
- **Summary**: The main question this work aims at answering is: "can morphing attack detection (MAD) solutions be successfully developed based on synthetic data?". Towards that, this work introduces the first synthetic-based MAD development dataset, namely the Synthetic Morphing Attack Detection Development dataset (SMDD). This dataset is utilized successfully to train three MAD backbones where it proved to lead to high MAD performance, even on completely unknown attack types. Additionally, an essential aspect of this work is the detailed legal analyses of the challenges of using and sharing real biometric data, rendering our proposed SMDD dataset extremely essential. The SMDD dataset, consisting of 30,000 attack and 50,000 bona fide samples, is publicly available for research purposes.



### Training Protocol Matters: Towards Accurate Scene Text Recognition via Training Protocol Searching
- **Arxiv ID**: http://arxiv.org/abs/2203.06696v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06696v2)
- **Published**: 2022-03-13 16:11:58+00:00
- **Updated**: 2022-03-17 03:08:45+00:00
- **Authors**: Xiaojie Chu, Yongtao Wang, Chunhua Shen, Jingdong Chen, Wei Chu
- **Comment**: None
- **Journal**: None
- **Summary**: The development of scene text recognition (STR) in the era of deep learning has been mainly focused on novel architectures of STR models. However, training protocol (i.e., settings of the hyper-parameters involved in the training of STR models), which plays an equally important role in successfully training a good STR model, is under-explored for scene text recognition. In this work, we attempt to improve the accuracy of existing STR models by searching for optimal training protocol. Specifically, we develop a training protocol search algorithm, based on a newly designed search space and an efficient search algorithm using evolutionary optimization and proxy tasks. Experimental results show that our searched training protocol can improve the recognition accuracy of mainstream STR models by 2.7%~3.9%. In particular, with the searched training protocol, TRBA-Net achieves 2.1% higher accuracy than the state-of-the-art STR model (i.e., EFIFSTR), while the inference speed is 2.3x and 3.7x faster on CPU and GPU respectively. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method and the generalization ability of the training protocol found by our search method. Code is available at https://github.com/VDIGPKU/STR_TPSearch.



### Efficient Long-Range Attention Network for Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2203.06697v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06697v1)
- **Published**: 2022-03-13 16:17:48+00:00
- **Updated**: 2022-03-13 16:17:48+00:00
- **Authors**: Xindong Zhang, Hui Zeng, Shi Guo, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, transformer-based methods have demonstrated impressive results in various vision tasks, including image super-resolution (SR), by exploiting the self-attention (SA) for feature extraction. However, the computation of SA in most existing transformer based models is very expensive, while some employed operations may be redundant for the SR task. This limits the range of SA computation and consequently the SR performance. In this work, we propose an efficient long-range attention network (ELAN) for image SR. Specifically, we first employ shift convolution (shift-conv) to effectively extract the image local structural information while maintaining the same level of complexity as 1x1 convolution, then propose a group-wise multi-scale self-attention (GMSA) module, which calculates SA on non-overlapped groups of features using different window sizes to exploit the long-range image dependency. A highly efficient long-range attention block (ELAB) is then built by simply cascading two shift-conv with a GMSA module, which is further accelerated by using a shared attention mechanism. Without bells and whistles, our ELAN follows a fairly simple design by sequentially cascading the ELABs. Extensive experiments demonstrate that ELAN obtains even better results against the transformer-based SR models but with significantly less complexity. The source code can be found at https://github.com/xindongzhang/ELAN.



### Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs
- **Arxiv ID**: http://arxiv.org/abs/2203.06717v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.06717v4)
- **Published**: 2022-03-13 17:22:44+00:00
- **Updated**: 2022-04-02 09:52:27+00:00
- **Authors**: Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong Han, Guiguang Ding, Jian Sun
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large as 31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the performance gap between CNNs and ViTs, e.g., achieving comparable or superior results than Swin Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields and higher shape bias rather than texture bias. Code & models at https://github.com/megvii-research/RepLKNet.



### Food Recipe Recommendation Based on Ingredients Detection Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.06721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06721v1)
- **Published**: 2022-03-13 17:42:38+00:00
- **Updated**: 2022-03-13 17:42:38+00:00
- **Authors**: Md. Shafaat Jamil Rokon, Md Kishor Morol, Ishra Binte Hasan, A. M. Saif, Rafid Hussain Khan
- **Comment**: Accepted at ICCA 2022
- **Journal**: None
- **Summary**: Food is essential for human survival, and people always try to taste different types of delicious recipes. Frequently, people choose food ingredients without even knowing their names or pick up some food ingredients that are not obvious to them from a grocery store. Knowing which ingredients can be mixed to make a delicious food recipe is essential. Selecting the right recipe by choosing a list of ingredients is very difficult for a beginner cook. However, it can be a problem even for experts. One such example is recognising objects through image processing. Although this process is complex due to different food ingredients, traditional approaches will lead to an inaccuracy rate. These problems can be solved by machine learning and deep learning approaches. In this paper, we implemented a model for food ingredients recognition and designed an algorithm for recommending recipes based on recognised ingredients. We made a custom dataset consisting of 9856 images belonging to 32 different food ingredients classes. Convolution Neural Network (CNN) model was used to identify food ingredients, and for recipe recommendations, we have used machine learning. We achieved an accuracy of 94 percent, which is quite impressive.



### Feature space reduction as data preprocessing for the anomaly detection
- **Arxiv ID**: http://arxiv.org/abs/2203.06747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06747v1)
- **Published**: 2022-03-13 19:52:47+00:00
- **Updated**: 2022-03-13 19:52:47+00:00
- **Authors**: Simon Bilik, Karel Horak
- **Comment**: 27th Conference STUDENT EEICT 2020, Brno University of Technology
- **Journal**: In Proceedings I of the 27th Conference STUDENT EEICT 2021. Brno,
  2021. s. 415-419. ISBN: 978-80-214-5942-7
- **Summary**: In this paper, we present two pipelines in order to reduce the feature space for anomaly detection using the One Class SVM. As a first stage of both pipelines, we compare the performance of three convolutional autoencoders. We use the PCA method together with t-SNE as the first pipeline and the reconstruction errors based method as the second. Both methods have potential for the anomaly detection, but the reconstruction error metrics prove to be more robust for this task. We show that the convolutional autoencoder architecture doesn't have a significant effect for this task and we prove the potential of our approach on the real world dataset.



### Decontextualized I3D ConvNet for ultra-distance runners performance analysis at a glance
- **Arxiv ID**: http://arxiv.org/abs/2203.06749v3
- **DOI**: 10.1007/978-3-031-06433-3_21
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06749v3)
- **Published**: 2022-03-13 20:11:10+00:00
- **Updated**: 2022-05-26 10:24:49+00:00
- **Authors**: David Freire-Obregón, Javier Lorenzo-Navarro, Modesto Castrillón-Santana
- **Comment**: Accepted at 21st International Conference on Image Analysis and
  Processing (ICIAP 2021)
- **Journal**: None
- **Summary**: In May 2021, the site runnersworld.com published that participation in ultra-distance races has increased by 1,676% in the last 23 years. Moreover, nearly 41% of those runners participate in more than one race per year. The development of wearable devices has undoubtedly contributed to motivating participants by providing performance measures in real-time. However, we believe there is room for improvement, particularly from the organizers point of view. This work aims to determine how the runners performance can be quantified and predicted by considering a non-invasive technique focusing on the ultra-running scenario. In this sense, participants are captured when they pass through a set of locations placed along the race track. Each footage is considered an input to an I3D ConvNet to extract the participant's running gait in our work. Furthermore, weather and illumination capture conditions or occlusions may affect these footages due to the race staff and other runners. To address this challenging task, we have tracked and codified the participant's running gait at some RPs and removed the context intending to ensure a runner-of-interest proper evaluation. The evaluation suggests that the features extracted by an I3D ConvNet provide enough information to estimate the participant's performance along the different race tracks.



### TurbuGAN: An Adversarial Learning Approach to Spatially-Varying Multiframe Blind Deconvolution with Applications to Imaging Through Turbulence
- **Arxiv ID**: http://arxiv.org/abs/2203.06764v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.06764v3)
- **Published**: 2022-03-13 21:32:34+00:00
- **Updated**: 2023-01-02 23:00:11+00:00
- **Authors**: Brandon Yushan Feng, Mingyang Xie, Christopher A. Metzler
- **Comment**: None
- **Journal**: None
- **Summary**: We present a self-supervised and self-calibrating multi-shot approach to imaging through atmospheric turbulence, called TurbuGAN. Our approach requires no paired training data, adapts itself to the distribution of the turbulence, leverages domain-specific data priors, and can generalize from tens to thousands of measurements. We achieve such functionality through an adversarial sensing framework adapted from CryoGAN, which uses a discriminator network to match the distributions of captured and simulated measurements. Our framework builds on CryoGAN by (1) generalizing the forward measurement model to incorporate physically accurate and computationally efficient models for light propagation through anisoplanatic turbulence, (2) enabling adaptation to slightly misspecified forward models, and (3) leveraging domain-specific prior knowledge using pretrained generative networks, when available. We validate TurbuGAN on both computationally simulated and experimentally captured images distorted with anisoplanatic turbulence.



### Similarity Equivariant Linear Transformation of Joint Orientation-Scale Space Representations
- **Arxiv ID**: http://arxiv.org/abs/2203.06786v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06786v2)
- **Published**: 2022-03-13 23:53:51+00:00
- **Updated**: 2022-03-15 04:48:54+00:00
- **Authors**: Xinhua Zhang, Lance R. Williams
- **Comment**: 17 pages, 2 figures
- **Journal**: None
- **Summary**: Convolution is conventionally defined as a linear operation on functions of one or more variables which commutes with shifts. Group convolution generalizes the concept to linear operations on functions of group elements representing more general geometric transformations and which commute with those transformations. Since similarity transformation is the most general geometric transformation on images that preserves shape, the group convolution that is equivariant to similarity transformation is the most general shape preserving linear operator. Because similarity transformations have four free parameters, group convolutions are defined on four-dimensional, joint orientation-scale spaces. Although prior work on equivariant linear operators has been limited to discrete groups, the similarity group is continuous. In this paper, we describe linear operators on discrete representations that are equivariant to continuous similarity transformation. This is achieved by using a basis of functions that is it joint shiftable-twistable-scalable. These pinwheel functions use Fourier series in the orientation dimension and Laplace transform in the log-scale dimension to form a basis of spatially localized functions that can be continuously interpolated in position, orientation and scale. Although this result is potentially significant with respect to visual computation generally, we present an initial demonstration of its utility by using it to compute a shape equivariant distribution of closed contours traced by particles undergoing Brownian motion in velocity. The contours are constrained by sets of points and line endings representing well known bistable illusory contour inducing patterns.



### Euclidean Invariant Recognition of 2D Shapes Using Histograms of Magnitudes of Local Fourier-Mellin Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2203.06787v1
- **DOI**: 10.1109/WACV.2019.00038
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.06787v1)
- **Published**: 2022-03-13 23:54:56+00:00
- **Updated**: 2022-03-13 23:54:56+00:00
- **Authors**: Xinhua Zhang, Lance R. Williams
- **Comment**: 9 pages, 5 figures
- **Journal**: 2019 IEEE Winter Conference on Applications of Computer Vision
  (WACV), 2019, pp. 303-311
- **Summary**: Because the magnitude of inner products with its basis functions are invariant to rotation and scale change, the Fourier-Mellin transform has long been used as a component in Euclidean invariant 2D shape recognition systems. Yet Fourier-Mellin transform magnitudes are only invariant to rotation and scale changes about a known center point, and full Euclidean invariant shape recognition is not possible except when this center point can be consistently and accurately identified. In this paper, we describe a system where a Fourier-Mellin transform is computed at every point in the image. The spatial support of the Fourier-Mellin basis functions is made local by multiplying them with a polynomial envelope. Significantly, the magnitudes of convolutions with these complex filters at isolated points are not (by themselves) used as features for Euclidean invariant shape recognition because reliable discrimination would require filters with spatial support large enough to fully encompass the shapes. Instead, we rely on the fact that normalized histograms of magnitudes are fully Euclidean invariant. We demonstrate a system based on the VLAD machine learning method that performs Euclidean invariant recognition of 2D shapes and requires an order of magnitude less training data than comparable methods based on convolutional neural networks.



