# Arxiv Papers in cs.CV on 2022-03-26
### SolidGen: An Autoregressive Model for Direct B-rep Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2203.13944v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR, 68T07, I.3.5; J.6; I.2.m
- **Links**: [PDF](http://arxiv.org/pdf/2203.13944v2)
- **Published**: 2022-03-26 00:00:45+00:00
- **Updated**: 2023-02-21 04:51:08+00:00
- **Authors**: Pradeep Kumar Jayaraman, Joseph G. Lambourne, Nishkrit Desai, Karl D. D. Willis, Aditya Sanghi, Nigel J. W. Morris
- **Comment**: TMLR February 2023 paper
- **Journal**: None
- **Summary**: The Boundary representation (B-rep) format is the de-facto shape representation in computer-aided design (CAD) to model solid and sheet objects. Recent approaches to generating CAD models have focused on learning sketch-and-extrude modeling sequences that are executed by a solid modeling kernel in postprocess to recover a B-rep. In this paper we present a new approach that enables learning from and synthesizing B-reps without the need for supervision through CAD modeling sequence data. Our method SolidGen, is an autoregressive neural network that models the B-rep directly by predicting the vertices, edges, and faces using Transformer-based and pointer neural networks. Key to achieving this is our Indexed Boundary Representation that references B-rep vertices, edges and faces in a well-defined hierarchy to capture the geometric and topological relations suitable for use with machine learning. SolidGen can be easily conditioned on contexts e.g., class labels, images, and voxels thanks to its probabilistic modeling of the B-rep distribution. We demonstrate qualitatively, quantitatively, and through perceptual evaluation by human subjects that SolidGen can produce high quality, realistic CAD models.



### AI-augmented histopathologic review using image analysis to optimize DNA yield and tumor purity from FFPE slides
- **Arxiv ID**: http://arxiv.org/abs/2203.13948v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13948v2)
- **Published**: 2022-03-26 00:22:07+00:00
- **Updated**: 2022-04-07 15:49:16+00:00
- **Authors**: Bolesław L. Osinski, Aïcha BenTaieb, Irvin Ho, Ryan D. Jones, Rohan P. Joshi, Andrew Westley, Michael Carlson, Caleb Willis, Luke Schleicher, Brett M. Mahon, Martin C. Stumpe
- **Comment**: None
- **Journal**: None
- **Summary**: To achieve minimum DNA input and tumor purity requirements for next-generation sequencing (NGS), pathologists visually estimate macrodissection and slide count decisions. Misestimation may cause tissue waste and increased laboratory costs. We developed an AI-augmented smart pathology review system (SmartPath) to empower pathologists with quantitative metrics for determining tissue extraction parameters. Using digitized H&E-stained FFPE slides as inputs, SmartPath segments tumors, extracts cell-based features, and suggests macrodissection areas. To predict DNA yield per slide, the extracted features are correlated with known DNA yields. Then, a pathologist-defined target yield divided by the predicted DNA yield/slide gives the number of slides to scrape. Following model development, an internal validation trial was conducted within the Tempus Labs molecular sequencing laboratory. We evaluated our system on 501 clinical colorectal cancer slides, where half received SmartPath-augmented review and half traditional pathologist review. The SmartPath cohort had 25% more DNA yields within a desired target range of 100-2000ng. The SmartPath system recommended fewer slides to scrape for large tissue sections, saving tissue in these cases. Conversely, SmartPath recommended more slides to scrape for samples with scant tissue sections, helping prevent costly re-extraction due to insufficient extraction yield. A statistical analysis was performed to measure the impact of covariates on the results, offering insights on how to improve future applications of SmartPath. Overall, the study demonstrated that AI-augmented histopathologic review using SmartPath could decrease tissue waste, sequencing time, and laboratory costs by optimizing DNA yields and tumor purity.



### GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.13954v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.13954v2)
- **Published**: 2022-03-26 01:04:13+00:00
- **Updated**: 2022-04-14 13:07:54+00:00
- **Authors**: Yue Liao, Aixi Zhang, Miao Lu, Yongliang Wang, Xiaobo Li, Si Liu
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: The task of Human-Object Interaction~(HOI) detection could be divided into two core problems, i.e., human-object association and interaction understanding. In this paper, we reveal and address the disadvantages of the conventional query-driven HOI detectors from the two aspects. For the association, previous two-branch methods suffer from complex and costly post-matching, while single-branch methods ignore the features distinction in different tasks. We propose Guided-Embedding Network~(GEN) to attain a two-branch pipeline without post-matching. In GEN, we design an instance decoder to detect humans and objects with two independent query sets and a position Guided Embedding~(p-GE) to mark the human and object in the same position as a pair. Besides, we design an interaction decoder to classify interactions, where the interaction queries are made of instance Guided Embeddings (i-GE) generated from the outputs of each instance decoder layer. For the interaction understanding, previous methods suffer from long-tailed distribution and zero-shot discovery. This paper proposes a Visual-Linguistic Knowledge Transfer (VLKT) training strategy to enhance interaction understanding by transferring knowledge from a visual-linguistic pre-trained model CLIP. In specific, we extract text embeddings for all labels with CLIP to initialize the classifier and adopt a mimic loss to minimize the visual feature distance between GEN and CLIP. As a result, GEN-VLKT outperforms the state of the art by large margins on multiple datasets, e.g., +5.05 mAP on HICO-Det. The source codes are available at https://github.com/YueLiao/gen-vlkt.



### Transformer-empowered Multi-scale Contextual Matching and Aggregation for Multi-contrast MRI Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2203.13963v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.13963v1)
- **Published**: 2022-03-26 01:42:59+00:00
- **Updated**: 2022-03-26 01:42:59+00:00
- **Authors**: Guangyuan Li, Jun Lv, Yapeng Tian, Qi Dou, Chengyan Wang, Chenliang Xu, Jing Qin
- **Comment**: CVPR 2022 accepted
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) can present multi-contrast images of the same anatomical structures, enabling multi-contrast super-resolution (SR) techniques. Compared with SR reconstruction using a single-contrast, multi-contrast SR reconstruction is promising to yield SR images with higher quality by leveraging diverse yet complementary information embedded in different imaging modalities. However, existing methods still have two shortcomings: (1) they neglect that the multi-contrast features at different scales contain different anatomical details and hence lack effective mechanisms to match and fuse these features for better reconstruction; and (2) they are still deficient in capturing long-range dependencies, which are essential for the regions with complicated anatomical structures. We propose a novel network to comprehensively address these problems by developing a set of innovative Transformer-empowered multi-scale contextual matching and aggregation techniques; we call it McMRSR. Firstly, we tame transformers to model long-range dependencies in both reference and target images. Then, a new multi-scale contextual matching method is proposed to capture corresponding contexts from reference features at different scales. Furthermore, we introduce a multi-scale aggregation mechanism to gradually and interactively aggregate multi-scale matched features for reconstructing the target SR MR image. Extensive experiments demonstrate that our network outperforms state-of-the-art approaches and has great potential to be applied in clinical practice. Codes are available at https://github.com/XAIMI-Lab/McMRSR.



### Fusing Global and Local Features for Generalized AI-Synthesized Image Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.13964v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13964v2)
- **Published**: 2022-03-26 01:55:37+00:00
- **Updated**: 2022-11-22 23:24:10+00:00
- **Authors**: Yan Ju, Shan Jia, Lipeng Ke, Hongfei Xue, Koki Nagano, Siwei Lyu
- **Comment**: 5 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: With the development of the Generative Adversarial Networks (GANs) and DeepFakes, AI-synthesized images are now of such high quality that humans can hardly distinguish them from real images. It is imperative for media forensics to develop detectors to expose them accurately. Existing detection methods have shown high performance in generated images detection, but they tend to generalize poorly in the real-world scenarios, where the synthetic images are usually generated with unseen models using unknown source data. In this work, we emphasize the importance of combining information from the whole image and informative patches in improving the generalization ability of AI-synthesized image detection. Specifically, we design a two-branch model to combine global spatial information from the whole image and local informative features from multiple patches selected by a novel patch selection module. Multi-head attention mechanism is further utilized to fuse the global and local features. We collect a highly diverse dataset synthesized by 19 models with various objects and resolutions to evaluate our model. Experimental results demonstrate the high accuracy and good generalization ability of our method in detecting generated images. Our code is available at https://github.com/littlejuyan/FusingGlobalandLocal.



### Exploring Self-Attention for Visual Intersection Classification
- **Arxiv ID**: http://arxiv.org/abs/2203.13977v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13977v1)
- **Published**: 2022-03-26 03:12:38+00:00
- **Updated**: 2022-03-26 03:12:38+00:00
- **Authors**: Haruki Nakata, Kanji Tanaka, Koji Takeda
- **Comment**: 7 pages, 6 figures, technical report
- **Journal**: None
- **Summary**: In robot vision, self-attention has recently emerged as a technique for capturing non-local contexts. In this study, we introduced a self-attention mechanism into the intersection recognition system as a method to capture the non-local contexts behind the scenes. An intersection classification system comprises two distinctive modules: (a) a first-person vision (FPV) module, which uses a short egocentric view sequence as the intersection is passed, and (b) a third-person vision (TPV) module, which uses a single view immediately before entering the intersection. The self-attention mechanism is effective in the TPV module because most parts of the local pattern (e.g., road edges, buildings, and sky) are similar to each other, and thus the use of a non-local context (e.g., the angle between two diagonal corners around an intersection) would be effective. This study makes three major contributions. First, we proposed a self-attention-based approach for intersection classification using TPVs. Second, we presented a practical system in which a self-attention-based TPV module is combined with an FPV module to improve the overall recognition performance. Finally, experiments using the public KITTI dataset show that the above self-attention-based system outperforms conventional recognition based on local patterns and recognition based on convolution operations.



### Current Source Localization Using Deep Prior with Depth Weighting
- **Arxiv ID**: http://arxiv.org/abs/2203.13981v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2203.13981v1)
- **Published**: 2022-03-26 03:45:32+00:00
- **Updated**: 2022-03-26 03:45:32+00:00
- **Authors**: Rio Yamana, Hajime Yano, Ryoichi Takashima, Tetsuya Takiguchi, Seiji Nakagawa
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel neuronal current source localization method based on Deep Prior that represents a more complicated prior distribution of current source using convolutional networks. Deep Prior has been suggested as a means of an unsupervised learning approach that does not require learning using training data, and randomly-initialized neural networks are used to update a source location using a single observation. In our previous work, a Deep-Prior-based current source localization method in the brain has been proposed but the performance was not almost the same as those of conventional approaches, such as sLORETA. In order to improve the Deep-Prior-based approach, in this paper, a depth weight of the current source is introduced for Deep Prior, where depth weighting amounts to assigning more penalty to the superficial currents. Its effectiveness is confirmed by experiments of current source estimation on simulated MEG data.



### Depth Estimation vs Classification as Pre-training for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.13987v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13987v3)
- **Published**: 2022-03-26 04:27:28+00:00
- **Updated**: 2023-03-17 02:07:10+00:00
- **Authors**: Dong Lao, Alex Wong, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: Training a deep neural network for semantic segmentation is labor intensive, so it is common to pre-train on a different task for which data is abundant, typically image-level classification, and then fine-tune with a small annotated dataset. There is empirical evidence showing that incorporating depth information during training may improve semantic segmentation, but the extent of this effect has yet to be fully characterized. In this paper, we study whether monocular depth estimation can serve as pre-training for semantic segmentation, ideally eliminating the need for manually supervised pre-training. Using common benchmarks such as KITTI, Cityscapes, and NYU-V2, we evaluate pre-training using depth estimation vs. classification, measuring their effects on downstream semantic segmentation. The former edges out the latter by 5.8\% mIoU and 5.2\% pixel accuracy. We analyze the impact of different forms of supervision for depth estimation, training pipelines, and data resolution on semantic fine-tuning. Additionally, we find that other forms of self-supervision are less effective than depth pre-training, including optical flow, despite sharing the same loss, namely the photometric reprojection error.



### RSCFed: Random Sampling Consensus Federated Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.13993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13993v1)
- **Published**: 2022-03-26 05:10:44+00:00
- **Updated**: 2022-03-26 05:10:44+00:00
- **Authors**: Xiaoxiao Liang, Yiqun Lin, Huazhu Fu, Lei Zhu, Xiaomeng Li
- **Comment**: CVPR 2022, code: https://github.com/XMed-Lab/RSCFed
- **Journal**: None
- **Summary**: Federated semi-supervised learning (FSSL) aims to derive a global model by training fully-labeled and fully-unlabeled clients or training partially labeled clients. The existing approaches work well when local clients have independent and identically distributed (IID) data but fail to generalize to a more practical FSSL setting, i.e., Non-IID setting. In this paper, we present a Random Sampling Consensus Federated learning, namely RSCFed, by considering the uneven reliability among models from fully-labeled clients, fully-unlabeled clients or partially labeled clients. Our key motivation is that given models with large deviations from either labeled clients or unlabeled clients, the consensus could be reached by performing random sub-sampling over clients. To achieve it, instead of directly aggregating local models, we first distill several sub-consensus models by random sub-sampling over clients and then aggregating the sub-consensus models to the global model. To enhance the robustness of sub-consensus models, we also develop a novel distance-reweighted model aggregation method. Experimental results show that our method outperforms state-of-the-art methods on three benchmarked datasets, including both natural and medical images. The code is available at https://github.com/XMed-Lab/RSCFed.



### Learning to Predict RNA Sequence Expressions from Whole Slide Images with Applications for Search and Classification
- **Arxiv ID**: http://arxiv.org/abs/2203.13997v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.13997v1)
- **Published**: 2022-03-26 05:38:34+00:00
- **Updated**: 2022-03-26 05:38:34+00:00
- **Authors**: Amir Safarpoor, Jason D. Hipp, H. R. Tizhoosh
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods are widely applied in digital pathology to address clinical challenges such as prognosis and diagnosis. As one of the most recent applications, deep models have also been used to extract molecular features from whole slide images. Although molecular tests carry rich information, they are often expensive, time-consuming, and require additional tissue to sample. In this paper, we propose tRNAsfomer, an attention-based topology that can learn both to predict the bulk RNA-seq from an image and represent the whole slide image of a glass slide simultaneously. The tRNAsfomer uses multiple instance learning to solve a weakly supervised problem while the pixel-level annotation is not available for an image. We conducted several experiments and achieved better performance and faster convergence in comparison to the state-of-the-art algorithms. The proposed tRNAsfomer can assist as a computational pathology tool to facilitate a new generation of search and classification methods by combining the tissue morphology and the molecular fingerprint of the biopsy samples.



### Knowledge Distillation with the Reused Teacher Classifier
- **Arxiv ID**: http://arxiv.org/abs/2203.14001v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14001v1)
- **Published**: 2022-03-26 06:28:46+00:00
- **Updated**: 2022-03-26 06:28:46+00:00
- **Authors**: Defang Chen, Jian-Ping Mei, Hailin Zhang, Can Wang, Yan Feng, Chun Chen
- **Comment**: Accepted to CVPR-2022
- **Journal**: None
- **Summary**: Knowledge distillation aims to compress a powerful yet cumbersome teacher model into a lightweight student model without much sacrifice of performance. For this purpose, various approaches have been proposed over the past few years, generally with elaborately designed knowledge representations, which in turn increase the difficulty of model development and interpretation. In contrast, we empirically show that a simple knowledge distillation technique is enough to significantly narrow down the teacher-student performance gap. We directly reuse the discriminative classifier from the pre-trained teacher model for student inference and train a student encoder through feature alignment with a single $\ell_2$ loss. In this way, the student model is able to achieve exactly the same performance as the teacher model provided that their extracted features are perfectly aligned. An additional projector is developed to help the student encoder match with the teacher classifier, which renders our technique applicable to various teacher and student architectures. Extensive experiments demonstrate that our technique achieves state-of-the-art results at the modest cost of compression ratio due to the added projector.



### Learn to Adapt for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2203.14005v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.14005v1)
- **Published**: 2022-03-26 06:49:22+00:00
- **Updated**: 2022-03-26 06:49:22+00:00
- **Authors**: Qiyu Sun, Gary G. Yen, Yang Tang, Chaoqiang Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular depth estimation is one of the fundamental tasks in environmental perception and has achieved tremendous progress in virtue of deep learning. However, the performance of trained models tends to degrade or deteriorate when employed on other new datasets due to the gap between different datasets. Though some methods utilize domain adaptation technologies to jointly train different domains and narrow the gap between them, the trained models cannot generalize to new domains that are not involved in training. To boost the transferability of depth estimation models, we propose an adversarial depth estimation task and train the model in the pipeline of meta-learning. Our proposed adversarial task mitigates the issue of meta-overfitting, since the network is trained in an adversarial manner and aims to extract domain invariant representations. In addition, we propose a constraint to impose upon cross-task depth consistency to compel the depth estimation to be identical in different adversarial tasks, which improves the performance of our method and smoothens the training process. Experiments demonstrate that our method adapts well to new datasets after few training steps during the test procedure.



### EYNet: Extended YOLO for Airport Detection in Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2203.14007v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.14007v1)
- **Published**: 2022-03-26 07:07:53+00:00
- **Updated**: 2022-03-26 07:07:53+00:00
- **Authors**: Hengameh Mirhajianmoghadam, Behrouz Bolourian Haghighi
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, airport detection in remote sensing images has attracted considerable attention due to its strategic role in civilian and military scopes. In particular, uncrewed and operated aerial vehicles must immediately detect safe areas to land in emergencies. The previous schemes suffered from various aspects, including complicated backgrounds, scales, and shapes of the airport. Meanwhile, the rapid action and accuracy of the method are confronted with significant concerns. Hence, this study proposes an effective scheme by extending YOLOV3 and ShearLet transform. In this way, MobileNet and ResNet18, with fewer layers and parameters retrained on a similar dataset, are parallelly trained as base networks. According to airport geometrical characteristics, the ShearLet filters with different scales and directions are considered in the first convolution layers of ResNet18 as a visual attention mechanism. Besides, the major extended in YOLOV3 concerns the detection Sub-Networks with novel structures which boost object expression ability and training efficiency. In addition, novel augmentation and negative mining strategies are presented to significantly increase the localization phase's performance. The experimental results on the DIOR dataset reveal that the framework reliably detects different types of airports in a varied area and acquires robust results in complex scenes compared to traditional YOLOV3 and state-of-the-art schemes.



### Semantic-guided Disentangled Representation for Unsupervised Cross-modality Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.14025v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.14025v2)
- **Published**: 2022-03-26 08:31:00+00:00
- **Updated**: 2023-03-04 06:04:29+00:00
- **Authors**: Shuai Wang, Rui Li
- **Comment**: Tech Report. 10pages, 4 figures
- **Journal**: None
- **Summary**: Disentangled representation is a powerful technique to tackle domain shift problem in medical image analysis in unsupervised domain adaptation setting.However, previous methods only focus on exacting domain-invariant feature and ignore whether exacted feature is meaningful for downstream tasks.We propose a novel framework, called semantic-guided disentangled representation (SGDR), an effective method to exact semantically meaningful feature for segmentation task to improve performance of cross modality medical image segmentation in unsupervised domain adaptation setting. To exact the meaningful domain-invariant features of different modality, we introduce a content discriminator to force the content representation to be embedded to the same space and a feature discriminator to exact the meaningful representation.We also use pixel-level annotations to guide the encoder to learn features that are meaningful for segmentation task.We validated our method on two public datasets and experiment results show that our approach outperforms the state of the art methods on two evaluation metrics by a significant margin.



### Medicinal Boxes Recognition on a Deep Transfer Learning Augmented Reality Mobile Application
- **Arxiv ID**: http://arxiv.org/abs/2203.14031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14031v1)
- **Published**: 2022-03-26 09:21:56+00:00
- **Updated**: 2022-03-26 09:21:56+00:00
- **Authors**: Danilo Avola, Luigi Cinque, Alessio Fagioli, Gian Luca Foresti, Marco Raoul Marini, Alessio Mecca, Daniele Pannone
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Taking medicines is a fundamental aspect to cure illnesses. However, studies have shown that it can be hard for patients to remember the correct posology. More aggravating, a wrong dosage generally causes the disease to worsen. Although, all relevant instructions for a medicine are summarized in the corresponding patient information leaflet, the latter is generally difficult to navigate and understand. To address this problem and help patients with their medication, in this paper we introduce an augmented reality mobile application that can present to the user important details on the framed medicine. In particular, the app implements an inference engine based on a deep neural network, i.e., a densenet, fine-tuned to recognize a medicinal from its package. Subsequently, relevant information, such as posology or a simplified leaflet, is overlaid on the camera feed to help a patient when taking a medicine. Extensive experiments to select the best hyperparameters were performed on a dataset specifically collected to address this task; ultimately obtaining up to 91.30\% accuracy as well as real-time capabilities.



### Visual Abductive Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2203.14040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14040v1)
- **Published**: 2022-03-26 10:17:03+00:00
- **Updated**: 2022-03-26 10:17:03+00:00
- **Authors**: Chen Liang, Wenguan Wang, Tianfei Zhou, Yi Yang
- **Comment**: CVPR2022; Code, data: https://github.com/leonnnop/VAR
- **Journal**: None
- **Summary**: Abductive reasoning seeks the likeliest possible explanation for partial observations. Although abduction is frequently employed in human daily reasoning, it is rarely explored in computer vision literature. In this paper, we propose a new task and dataset, Visual Abductive Reasoning (VAR), for examining abductive reasoning ability of machine intelligence in everyday visual situations. Given an incomplete set of visual events, AI systems are required to not only describe what is observed, but also infer the hypothesis that can best explain the visual premise. Based on our large-scale VAR dataset, we devise a strong baseline model, Reasoner (causal-and-cascaded reasoning Transformer). First, to capture the causal structure of the observations, a contextualized directional position embedding strategy is adopted in the encoder, that yields discriminative representations for the premise and hypothesis. Then, multiple decoders are cascaded to generate and progressively refine the premise and hypothesis sentences. The prediction scores of the sentences are used to guide cross-sentence information flow in the cascaded reasoning procedure. Our VAR benchmarking results show that Reasoner surpasses many famous video-language models, while still being far behind human performance. This work is expected to foster future efforts in the reasoning-beyond-observation paradigm.



### Semantic Segmentation by Early Region Proxy
- **Arxiv ID**: http://arxiv.org/abs/2203.14043v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14043v1)
- **Published**: 2022-03-26 10:48:32+00:00
- **Updated**: 2022-03-26 10:48:32+00:00
- **Authors**: Yifan Zhang, Bo Pang, Cewu Lu
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Typical vision backbones manipulate structured features. As a compromise, semantic segmentation has long been modeled as per-point prediction on dense regular grids. In this work, we present a novel and efficient modeling that starts from interpreting the image as a tessellation of learnable regions, each of which has flexible geometrics and carries homogeneous semantics. To model region-wise context, we exploit Transformer to encode regions in a sequence-to-sequence manner by applying multi-layer self-attention on the region embeddings, which serve as proxies of specific regions. Semantic segmentation is now carried out as per-region prediction on top of the encoded region embeddings using a single linear classifier, where a decoder is no longer needed. The proposed RegProxy model discards the common Cartesian feature layout and operates purely at region level. Hence, it exhibits the most competitive performance-efficiency trade-off compared with the conventional dense prediction methods. For example, on ADE20K, the small-sized RegProxy-S/16 outperforms the best CNN model using 25% parameters and 4% computation, while the largest RegProxy-L/16 achieves 52.9mIoU which outperforms the state-of-the-art by 2.1% with fewer resources. Codes and models are available at https://github.com/YiF-Zhang/RegionProxy.



### Adaptively Lighting up Facial Expression Crucial Regions via Local Non-Local Joint Network
- **Arxiv ID**: http://arxiv.org/abs/2203.14045v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14045v1)
- **Published**: 2022-03-26 10:58:25+00:00
- **Updated**: 2022-03-26 10:58:25+00:00
- **Authors**: Shasha Mao, Guanghui Shi, Shuiping Gou, Dandan Yan, Licheng Jiao, Lin Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expression recognition (FER) is still one challenging research due to the small inter-class discrepancy in the facial expression data. In view of the significance of facial crucial regions for FER, many existing researches utilize the prior information from some annotated crucial points to improve the performance of FER. However, it is complicated and time-consuming to manually annotate facial crucial points, especially for vast wild expression images. Based on this, a local non-local joint network is proposed to adaptively light up the facial crucial regions in feature learning of FER in this paper. In the proposed method, two parts are constructed based on facial local and non-local information respectively, where an ensemble of multiple local networks are proposed to extract local features corresponding to multiple facial local regions and a non-local attention network is addressed to explore the significance of each local region. Especially, the attention weights obtained by the non-local network is fed into the local part to achieve the interactive feedback between the facial global and local information. Interestingly, the non-local weights corresponding to local regions are gradually updated and higher weights are given to more crucial regions. Moreover, U-Net is employed to extract the integrated features of deep semantic information and low hierarchical detail information of expression images. Finally, experimental results illustrate that the proposed method achieves more competitive performance compared with several state-of-the art methods on five benchmark datasets. Noticeably, the analyses of the non-local weights corresponding to local regions demonstrate that the proposed method can automatically enhance some crucial regions in the process of feature learning without any facial landmark information.



### A Survey of Robust Adversarial Training in Pattern Recognition: Fundamental, Theory, and Methodologies
- **Arxiv ID**: http://arxiv.org/abs/2203.14046v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.14046v1)
- **Published**: 2022-03-26 11:00:25+00:00
- **Updated**: 2022-03-26 11:00:25+00:00
- **Authors**: Zhuang Qian, Kaizhu Huang, Qiu-Feng Wang, Xu-Yao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In the last a few decades, deep neural networks have achieved remarkable success in machine learning, computer vision, and pattern recognition. Recent studies however show that neural networks (both shallow and deep) may be easily fooled by certain imperceptibly perturbed input samples called adversarial examples. Such security vulnerability has resulted in a large body of research in recent years because real-world threats could be introduced due to vast applications of neural networks. To address the robustness issue to adversarial examples particularly in pattern recognition, robust adversarial training has become one mainstream. Various ideas, methods, and applications have boomed in the field. Yet, a deep understanding of adversarial training including characteristics, interpretations, theories, and connections among different models has still remained elusive. In this paper, we present a comprehensive survey trying to offer a systematic and structured investigation on robust adversarial training in pattern recognition. We start with fundamentals including definition, notations, and properties of adversarial examples. We then introduce a unified theoretical framework for defending against adversarial samples - robust adversarial training with visualizations and interpretations on why adversarial training can lead to model robustness. Connections will be also established between adversarial training and other traditional learning theories. After that, we summarize, review, and discuss various methodologies with adversarial attack and defense/training algorithms in a structured way. Finally, we present analysis, outlook, and remarks of adversarial training.



### Learning Deep Implicit Functions for 3D Shapes with Dynamic Code Clouds
- **Arxiv ID**: http://arxiv.org/abs/2203.14048v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14048v2)
- **Published**: 2022-03-26 11:07:59+00:00
- **Updated**: 2022-03-30 03:15:32+00:00
- **Authors**: Tianyang Li, Xin Wen, Yu-Shen Liu, Hua Su, Zhizhong Han
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Deep Implicit Function (DIF) has gained popularity as an efficient 3D shape representation. To capture geometry details, current methods usually learn DIF using local latent codes, which discretize the space into a regular 3D grid (or octree) and store local codes in grid points (or octree nodes). Given a query point, the local feature is computed by interpolating its neighboring local codes with their positions. However, the local codes are constrained at discrete and regular positions like grid points, which makes the code positions difficult to be optimized and limits their representation ability. To solve this problem, we propose to learn DIF with Dynamic Code Cloud, named DCC-DIF. Our method explicitly associates local codes with learnable position vectors, and the position vectors are continuous and can be dynamically optimized, which improves the representation ability. In addition, we propose a novel code position loss to optimize the code positions, which heuristically guides more local codes to be distributed around complex geometric details. In contrast to previous methods, our DCC-DIF represents 3D shapes more efficiently with a small amount of local codes, and improves the reconstruction quality. Experiments demonstrate that DCC-DIF achieves better performance over previous methods. Code and data are available at https://github.com/lity20/DCCDIF.



### RGB-D Neural Radiance Fields: Local Sampling for Faster Training
- **Arxiv ID**: http://arxiv.org/abs/2203.15587v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.15587v2)
- **Published**: 2022-03-26 11:31:35+00:00
- **Updated**: 2022-11-07 13:51:16+00:00
- **Authors**: Arnab Dey, Andrew I. Comport
- **Comment**: None
- **Journal**: None
- **Summary**: Learning a 3D representation of a scene has been a challenging problem for decades in computer vision. Recent advances in implicit neural representation from images using neural radiance fields(NeRF) have shown promising results. Some of the limitations of previous NeRF based methods include longer training time, and inaccurate underlying geometry. The proposed method takes advantage of RGB-D data to reduce training time by leveraging depth sensing to improve local sampling. This paper proposes a depth-guided local sampling strategy and a smaller neural network architecture to achieve faster training time without compromising quality.



### FaceVerse: a Fine-grained and Detail-controllable 3D Face Morphable Model from a Hybrid Dataset
- **Arxiv ID**: http://arxiv.org/abs/2203.14057v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14057v3)
- **Published**: 2022-03-26 12:13:14+00:00
- **Updated**: 2022-05-27 13:39:22+00:00
- **Authors**: Lizhen Wang, Zhiyuan Chen, Tao Yu, Chenguang Ma, Liang Li, Yebin Liu
- **Comment**: https://github.com/LizhenWangT/FaceVerse
- **Journal**: None
- **Summary**: We present FaceVerse, a fine-grained 3D Neural Face Model, which is built from hybrid East Asian face datasets containing 60K fused RGB-D images and 2K high-fidelity 3D head scan models. A novel coarse-to-fine structure is proposed to take better advantage of our hybrid dataset. In the coarse module, we generate a base parametric model from large-scale RGB-D images, which is able to predict accurate rough 3D face models in different genders, ages, etc. Then in the fine module, a conditional StyleGAN architecture trained with high-fidelity scan models is introduced to enrich elaborate facial geometric and texture details. Note that different from previous methods, our base and detailed modules are both changeable, which enables an innovative application of adjusting both the basic attributes and the facial details of 3D face models. Furthermore, we propose a single-image fitting framework based on differentiable rendering. Rich experiments show that our method outperforms the state-of-the-art methods.



### Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture
- **Arxiv ID**: http://arxiv.org/abs/2203.14065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14065v1)
- **Published**: 2022-03-26 12:48:41+00:00
- **Updated**: 2022-03-26 12:48:41+00:00
- **Authors**: Buzhen Huang, Liang Pan, Yuan Yang, Jingyi Ju, Yangang Wang
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Due to the visual ambiguity, purely kinematic formulations on monocular human motion capture are often physically incorrect, biomechanically implausible, and can not reconstruct accurate interactions. In this work, we focus on exploiting the high-precision and non-differentiable physics simulator to incorporate dynamical constraints in motion capture. Our key-idea is to use real physical supervisions to train a target pose distribution prior for sampling-based motion control to capture physically plausible human motion. To obtain accurate reference motion with terrain interactions for the sampling, we first introduce an interaction constraint based on SDF (Signed Distance Field) to enforce appropriate ground contact modeling. We then design a novel two-branch decoder to avoid stochastic error from pseudo ground-truth and train a distribution prior with the non-differentiable physics simulator. Finally, we regress the sampling distribution from the current state of the physical character with the trained prior and sample satisfied target poses to track the estimated reference motion. Qualitative and quantitative results show that we can obtain physically plausible human motion with complex terrain interactions, human shape variations, and diverse behaviors. More information can be found at~\url{https://www.yangangwang.com/papers/HBZ-NM-2022-03.html}



### Learning to Answer Questions in Dynamic Audio-Visual Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2203.14072v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14072v2)
- **Published**: 2022-03-26 13:03:42+00:00
- **Updated**: 2022-04-05 12:04:15+00:00
- **Authors**: Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji-Rong Wen, Di Hu
- **Comment**: Accepted by CVPR2022 (Oral presentation)
- **Journal**: None
- **Summary**: In this paper, we focus on the Audio-Visual Question Answering (AVQA) task, which aims to answer questions regarding different visual objects, sounds, and their associations in videos. The problem requires comprehensive multimodal understanding and spatio-temporal reasoning over audio-visual scenes. To benchmark this task and facilitate our study, we introduce a large-scale MUSIC-AVQA dataset, which contains more than 45K question-answer pairs covering 33 different question templates spanning over different modalities and question types. We develop several baselines and introduce a spatio-temporal grounded audio-visual network for the AVQA problem. Our results demonstrate that AVQA benefits from multisensory perception and our model outperforms recent A-, V-, and AVQA approaches. We believe that our built dataset has the potential to serve as testbed for evaluating and promoting progress in audio-visual scene understanding and spatio-temporal reasoning. Code and dataset: http://gewu-lab.github.io/MUSIC-AVQA/



### V3GAN: Decomposing Background, Foreground and Motion for Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2203.14074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14074v1)
- **Published**: 2022-03-26 13:17:45+00:00
- **Updated**: 2022-03-26 13:17:45+00:00
- **Authors**: Arti Keshari, Sonam Gupta, Sukhendu Das
- **Comment**: None
- **Journal**: None
- **Summary**: Video generation is a challenging task that requires modeling plausible spatial and temporal dynamics in a video. Inspired by how humans perceive a video by grouping a scene into moving and stationary components, we propose a method that decomposes the task of video generation into the synthesis of foreground, background and motion. Foreground and background together describe the appearance, whereas motion specifies how the foreground moves in a video over time. We propose V3GAN, a novel three-branch generative adversarial network where two branches model foreground and background information, while the third branch models the temporal information without any supervision. The foreground branch is augmented with our novel feature-level masking layer that aids in learning an accurate mask for foreground and background separation. To encourage motion consistency, we further propose a shuffling loss for the video discriminator. Extensive quantitative and qualitative analysis on synthetic as well as real-world benchmark datasets demonstrates that V3GAN outperforms the state-of-the-art methods by a significant margin.



### 3D-OAE: Occlusion Auto-Encoders for Self-Supervised Learning on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2203.14084v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14084v3)
- **Published**: 2022-03-26 14:06:29+00:00
- **Updated**: 2022-10-28 04:44:00+00:00
- **Authors**: Junsheng Zhou, Xin Wen, Baorui Ma, Yu-Shen Liu, Yue Gao, Yi Fang, Zhizhong Han
- **Comment**: Project page: https://junshengzhou.github.io/3D-OAE; Code and data:
  https://github.com/junshengzhou/3D-OAE
- **Journal**: None
- **Summary**: The manual annotation for large-scale point clouds is still tedious and unavailable for many harsh real-world tasks. Self-supervised learning, which is used on raw and unlabeled data to pre-train deep neural networks, is a promising approach to address this issue. Existing works usually take the common aid from auto-encoders to establish the self-supervision by the self-reconstruction schema. However, the previous auto-encoders merely focus on the global shapes and do not distinguish the local and global geometric features apart. To address this problem, we present a novel and efficient self-supervised point cloud representation learning framework, named 3D Occlusion Auto-Encoder (3D-OAE), to facilitate the detailed supervision inherited in local regions and global shapes. We propose to randomly occlude some local patches of point clouds and establish the supervision via inpainting the occluded patches using the remaining ones. Specifically, we design an asymmetrical encoder-decoder architecture based on standard Transformer, where the encoder operates only on the visible subset of patches to learn local patterns, and a lightweight decoder is designed to leverage these visible patterns to infer the missing geometries via self-attention. We find that occluding a very high proportion of the input point cloud (e.g. 75%) will still yield a nontrivial self-supervisory performance, which enables us to achieve 3-4 times faster during training but also improve accuracy. Experimental results show that our approach outperforms the state-of-the-art on a diverse range of downstream discriminative and generative tasks.



### Near-Infrared Depth-Independent Image Dehazing using Haar Wavelets
- **Arxiv ID**: http://arxiv.org/abs/2203.14085v1
- **DOI**: 10.1109/ICPR48806.2021.9412589
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14085v1)
- **Published**: 2022-03-26 14:07:31+00:00
- **Updated**: 2022-03-26 14:07:31+00:00
- **Authors**: Sumit Laha, Ankit Sharma, Shengnan Hu, Hassan Foroosh
- **Comment**: Accepted in 25th International Conference on Pattern Recognition
  (ICPR 2020)
- **Journal**: 2020 25th International Conference on Pattern Recognition (ICPR)
  (2021) 5384-5390
- **Summary**: We propose a fusion algorithm for haze removal that combines color information from an RGB image and edge information extracted from its corresponding NIR image using Haar wavelets. The proposed algorithm is based on the key observation that NIR edge features are more prominent in the hazy regions of the image than the RGB edge features in those same regions. To combine the color and edge information, we introduce a haze-weight map which proportionately distributes the color and edge information during the fusion process. Because NIR images are, intrinsically, nearly haze-free, our work makes no assumptions like existing works that rely on a scattering model and essentially designing a depth-independent method. This helps in minimizing artifacts and gives a more realistic sense to the restored haze-free image. Extensive experiments show that the proposed algorithm is both qualitatively and quantitatively better on several key metrics when compared to existing state-of-the-art methods.



### Towards Visual Affordance Learning: A Benchmark for Affordance Segmentation and Recognition
- **Arxiv ID**: http://arxiv.org/abs/2203.14092v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.14092v2)
- **Published**: 2022-03-26 14:31:35+00:00
- **Updated**: 2023-07-05 13:48:43+00:00
- **Authors**: Zeyad Khalifa, Syed Afaq Ali Shah
- **Comment**: None
- **Journal**: None
- **Summary**: The physical and textural attributes of objects have been widely studied for recognition, detection and segmentation tasks in computer vision.~A number of datasets, such as large scale ImageNet, have been proposed for feature learning using data hungry deep neural networks and for hand-crafted feature extraction. To intelligently interact with objects, robots and intelligent machines need the ability to infer beyond the traditional physical/textural attributes, and understand/learn visual cues, called visual affordances, for affordance recognition, detection and segmentation. To date there is no publicly available large dataset for visual affordance understanding and learning. In this paper, we introduce a large scale multi-view RGBD visual affordance learning dataset, a benchmark of 47210 RGBD images from 37 object categories, annotated with 15 visual affordance categories. To the best of our knowledge, this is the first ever and the largest multi-view RGBD visual affordance learning dataset. We benchmark the proposed dataset for affordance segmentation and recognition tasks using popular Vision Transformer and Convolutional Neural Networks. Several state-of-the-art deep learning networks are evaluated each for affordance recognition and segmentation tasks. Our experimental results showcase the challenging nature of the dataset and present definite prospects for new and robust affordance learning algorithms. The dataset is publicly available at https://sites.google.com/view/afaqshah/dataset.



### Uncertainty-aware Contrastive Distillation for Incremental Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.14098v2
- **DOI**: 10.1109/TPAMI.2022.3163806
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14098v2)
- **Published**: 2022-03-26 15:32:12+00:00
- **Updated**: 2022-05-20 08:25:33+00:00
- **Authors**: Guanglei Yang, Enrico Fini, Dan Xu, Paolo Rota, Mingli Ding, Moin Nabi, Xavier Alameda-Pineda, Elisa Ricci
- **Comment**: TPAMI
- **Journal**: None
- **Summary**: A fundamental and challenging problem in deep learning is catastrophic forgetting, i.e. the tendency of neural networks to fail to preserve the knowledge acquired from old tasks when learning new tasks. This problem has been widely investigated in the research community and several Incremental Learning (IL) approaches have been proposed in the past years. While earlier works in computer vision have mostly focused on image classification and object detection, more recently some IL approaches for semantic segmentation have been introduced. These previous works showed that, despite its simplicity, knowledge distillation can be effectively employed to alleviate catastrophic forgetting. In this paper, we follow this research direction and, inspired by recent literature on contrastive learning, we propose a novel distillation framework, Uncertainty-aware Contrastive Distillation (\method). In a nutshell, \method~is operated by introducing a novel distillation loss that takes into account all the images in a mini-batch, enforcing similarity between features associated to all the pixels from the same classes, and pulling apart those corresponding to pixels from different classes. In order to mitigate catastrophic forgetting, we contrast features of the new model with features extracted by a frozen model learned at the previous incremental step. Our experimental results demonstrate the advantage of the proposed distillation technique, which can be used in synergy with previous IL approaches, and leads to state-of-art performance on three commonly adopted benchmarks for incremental semantic segmentation. The code is available at \url{https://github.com/ygjwd12345/UCD}.



### Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/2203.14104v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.14104v1)
- **Published**: 2022-03-26 15:52:27+00:00
- **Updated**: 2022-03-26 15:52:27+00:00
- **Authors**: Muheng Li, Lei Chen, Yueqi Duan, Zhilan Hu, Jianjiang Feng, Jie Zhou, Jiwen Lu
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Action recognition models have shown a promising capability to classify human actions in short video clips. In a real scenario, multiple correlated human actions commonly occur in particular orders, forming semantically meaningful human activities. Conventional action recognition approaches focus on analyzing single actions. However, they fail to fully reason about the contextual relations between adjacent actions, which provide potential temporal logic for understanding long videos. In this paper, we propose a prompt-based framework, Bridge-Prompt (Br-Prompt), to model the semantics across adjacent actions, so that it simultaneously exploits both out-of-context and contextual information from a series of ordinal actions in instructional videos. More specifically, we reformulate the individual action labels as integrated text prompts for supervision, which bridge the gap between individual action semantics. The generated text prompts are paired with corresponding video clips, and together co-train the text encoder and the video encoder via a contrastive approach. The learned vision encoder has a stronger capability for ordinal-action-related downstream tasks, e.g. action segmentation and human activity recognition. We evaluate the performances of our approach on several video datasets: Georgia Tech Egocentric Activities (GTEA), 50Salads, and the Breakfast dataset. Br-Prompt achieves state-of-the-art on multiple benchmarks. Code is available at https://github.com/ttlmh/Bridge-Prompt



### Probabilistic Registration for Gaussian Process 3D shape modelling in the presence of extensive missing data
- **Arxiv ID**: http://arxiv.org/abs/2203.14113v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2203.14113v2)
- **Published**: 2022-03-26 16:48:27+00:00
- **Updated**: 2023-04-24 09:30:43+00:00
- **Authors**: Filipa Valdeira, Ricardo Ferreira, Alessandra Micheletti, Cláudia Soares
- **Comment**: 18 pages, 6 figures. Accepted for publication in SIAM Journal on
  Mathematics of Data Science (SIMODS)
- **Journal**: None
- **Summary**: We propose a shape fitting/registration method based on a Gaussian Processes formulation, suitable for shapes with extensive regions of missing data. Gaussian Processes are a proven powerful tool, as they provide a unified setting for shape modelling and fitting. While the existing methods in this area prove to work well for the general case of the human head, when looking at more detailed and deformed data, with a high prevalence of missing data, such as the ears, the results are not satisfactory. In order to overcome this, we formulate the shape fitting problem as a multi-annotator Gaussian Process Regression and establish a parallel with the standard probabilistic registration. The achieved method SFGP shows better performance when dealing with extensive areas of missing data when compared to a state-of-the-art registration method and current approaches for registration with pre-existing shape models. Experiments are conducted both for a 2D small dataset with diverse transformations and a 3D dataset of ears.



### Feature Selective Transformer for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.14124v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14124v3)
- **Published**: 2022-03-26 17:58:18+00:00
- **Updated**: 2022-04-01 07:55:05+00:00
- **Authors**: Fangjian Lin, Tianyi Wu, Sitong Wu, Shengwei Tian, Guodong Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, it has attracted more and more attentions to fuse multi-scale features for semantic image segmentation. Various works were proposed to employ progressive local or global fusion, but the feature fusions are not rich enough for modeling multi-scale context features. In this work, we focus on fusing multi-scale features from Transformer-based backbones for semantic segmentation, and propose a Feature Selective Transformer (FeSeFormer), which aggregates features from all scales (or levels) for each query feature. Specifically, we first propose a Scale-level Feature Selection (SFS) module, which can choose an informative subset from the whole multi-scale feature set for each scale, where those features that are important for the current scale (or level) are selected and the redundant are discarded. Furthermore, we propose a Full-scale Feature Fusion (FFF) module, which can adaptively fuse features of all scales for queries. Based on the proposed SFS and FFF modules, we develop a Feature Selective Transformer (FeSeFormer), and evaluate our FeSeFormer on four challenging semantic segmentation benchmarks, including PASCAL Context, ADE20K, COCO-Stuff 10K, and Cityscapes, outperforming the state-of-the-art.



### Automated Thermal Screening for COVID-19 using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.14128v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.14128v2)
- **Published**: 2022-03-26 18:23:44+00:00
- **Updated**: 2022-03-30 10:23:06+00:00
- **Authors**: Pratik Katte, Siva Teja Kakileti, Himanshu J. Madhu, Geetha Manjunath
- **Comment**: Submitted to QIRT Journal
- **Journal**: None
- **Summary**: In the last two years, millions of lives have been lost due to COVID-19. Despite the vaccination programmes for a year, hospitalization rates and deaths are still high due to the new variants of COVID-19. Stringent guidelines and COVID-19 screening measures such as temperature check and mask check at all public places are helping reduce the spread of COVID-19. Visual inspections to ensure these screening measures can be taxing and erroneous. Automated inspection ensures an effective and accurate screening. Traditional approaches involve identification of faces and masks from visual camera images followed by extraction of temperature values from thermal imaging cameras. Use of visual imaging as a primary modality limits these applications only for good-lighting conditions. The use of thermal imaging alone for these screening measures makes the system invariant to illumination. However, lack of open source datasets is an issue to develop such systems. In this paper, we discuss our work on using machine learning over thermal video streams for face and mask detection and subsequent temperature screening in a passive non-invasive way that enables an effective automated COVID-19 screening method in public places. We open source our NTIC dataset that was used for training our models and was collected at 8 different locations. Our results show that the use of thermal imaging is as effective as visual imaging in the presence of high illumination. This performance stays the same for thermal images even under low-lighting conditions, whereas the performance with visual trained classifiers show more than 50% degradation.



### RGBD Object Tracking: An In-depth Review
- **Arxiv ID**: http://arxiv.org/abs/2203.14134v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14134v1)
- **Published**: 2022-03-26 18:53:51+00:00
- **Updated**: 2022-03-26 18:53:51+00:00
- **Authors**: Jinyu Yang, Zhe Li, Song Yan, Feng Zheng, Aleš Leonardis, Joni-Kristian Kämäräinen, Ling Shao
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: RGBD object tracking is gaining momentum in computer vision research thanks to the development of depth sensors. Although numerous RGBD trackers have been proposed with promising performance, an in-depth review for comprehensive understanding of this area is lacking. In this paper, we firstly review RGBD object trackers from different perspectives, including RGBD fusion, depth usage, and tracking framework. Then, we summarize the existing datasets and the evaluation metrics. We benchmark a representative set of RGBD trackers, and give detailed analyses based on their performances. Particularly, we are the first to provide depth quality evaluation and analysis of tracking results in depth-friendly scenarios in RGBD tracking. For long-term settings in most RGBD tracking videos, we give an analysis of trackers' performance on handling target disappearance. To enable better understanding of RGBD trackers, we propose robustness evaluation against input perturbations. Finally, we summarize the challenges and provide open directions for this community. All resources are publicly available at https://github.com/memoryunreal/RGBD-tracking-review.



### Reverse Engineering of Imperceptible Adversarial Image Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2203.14145v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.14145v2)
- **Published**: 2022-03-26 19:52:40+00:00
- **Updated**: 2022-04-01 00:14:01+00:00
- **Authors**: Yifan Gong, Yuguang Yao, Yize Li, Yimeng Zhang, Xiaoming Liu, Xue Lin, Sijia Liu
- **Comment**: None
- **Journal**: None
- **Summary**: It has been well recognized that neural network based image classifiers are easily fooled by images with tiny perturbations crafted by an adversary. There has been a vast volume of research to generate and defend such adversarial attacks. However, the following problem is left unexplored: How to reverse-engineer adversarial perturbations from an adversarial image? This leads to a new adversarial learning paradigm--Reverse Engineering of Deceptions (RED). If successful, RED allows us to estimate adversarial perturbations and recover the original images. However, carefully crafted, tiny adversarial perturbations are difficult to recover by optimizing a unilateral RED objective. For example, the pure image denoising method may overfit to minimizing the reconstruction error but hardly preserve the classification properties of the true adversarial perturbations. To tackle this challenge, we formalize the RED problem and identify a set of principles crucial to the RED approach design. Particularly, we find that prediction alignment and proper data augmentation (in terms of spatial transformations) are two criteria to achieve a generalizable RED approach. By integrating these RED principles with image denoising, we propose a new Class-Discriminative Denoising based RED framework, termed CDD-RED. Extensive experiments demonstrate the effectiveness of CDD-RED under different evaluation metrics (ranging from the pixel-level, prediction-level to the attribution-level alignment) and a variety of attack generation methods (e.g., FGSM, PGD, CW, AutoAttack, and adaptive attacks).



### Accurate 3-DoF Camera Geo-Localization via Ground-to-Satellite Image Matching
- **Arxiv ID**: http://arxiv.org/abs/2203.14148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14148v1)
- **Published**: 2022-03-26 20:10:38+00:00
- **Updated**: 2022-03-26 20:10:38+00:00
- **Authors**: Yujiao Shi, Xin Yu, Liu Liu, Dylan Campbell, Piotr Koniusz, Hongdong Li
- **Comment**: submitted to TPAMI in Jan 2021
- **Journal**: None
- **Summary**: We address the problem of ground-to-satellite image geo-localization, that is, estimating the camera latitude, longitude and orientation (azimuth angle) by matching a query image captured at the ground level against a large-scale database with geotagged satellite images. Our prior arts treat the above task as pure image retrieval by selecting the most similar satellite reference image matching the ground-level query image. However, such an approach often produces coarse location estimates because the geotag of the retrieved satellite image only corresponds to the image center while the ground camera can be located at any point within the image. To further consolidate our prior research findings, we present a novel geometry-aware geo-localization method. Our new method is able to achieve the fine-grained location of a query image, up to pixel size precision of the satellite image, once its coarse location and orientation have been determined. Moreover, we propose a new geometry-aware image retrieval pipeline to improve the coarse localization accuracy. Apart from a polar transform in our conference work, this new pipeline also maps satellite image pixels to the ground-level plane in the ground-view via a geometry-constrained projective transform to emphasize informative regions, such as road structures, for cross-view geo-localization. Extensive quantitative and qualitative experiments demonstrate the effectiveness of our newly proposed framework. We also significantly improve the performance of coarse localization results compared to the state-of-the-art in terms of location recalls.



### A Robust Optimization Method for Label Noisy Datasets Based on Adaptive Threshold: Adaptive-k
- **Arxiv ID**: http://arxiv.org/abs/2203.14165v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.14165v1)
- **Published**: 2022-03-26 21:48:12+00:00
- **Updated**: 2022-03-26 21:48:12+00:00
- **Authors**: Enes Dedeoglu, Himmet Toprak Kesgin, Mehmet Fatih Amasyali
- **Comment**: None
- **Journal**: None
- **Summary**: SGD does not produce robust results on datasets with label noise. Because the gradients calculated according to the losses of the noisy samples cause the optimization process to go in the wrong direction. In this paper, as an alternative to SGD, we recommend using samples with loss less than a threshold value determined during the optimization process, instead of using all samples in the mini-batch. Our proposed method, Adaptive-k, aims to exclude label noise samples from the optimization process and make the process robust. On noisy datasets, we found that using a threshold-based approach, such as Adaptive-k, produces better results than using all samples or a fixed number of low-loss samples in the mini-batch. Based on our theoretical analysis and experimental results, we show that the Adaptive-k method is closest to the performance of the oracle, in which noisy samples are entirely removed from the dataset. Adaptive-k is a simple but effective method. It does not require prior knowledge of the noise ratio of the dataset, does not require additional model training, and does not increase training time significantly. The code for Adaptive-k is available at https://github.com/enesdedeoglu-TR/Adaptive-k



