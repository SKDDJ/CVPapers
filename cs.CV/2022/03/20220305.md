# Arxiv Papers in cs.CV on 2022-03-05
### How to Train Unstable Looped Tensor Network
- **Arxiv ID**: http://arxiv.org/abs/2203.02617v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, 65K05, 49M27
- **Links**: [PDF](http://arxiv.org/pdf/2203.02617v1)
- **Published**: 2022-03-05 00:17:04+00:00
- **Updated**: 2022-03-05 00:17:04+00:00
- **Authors**: Anh-Huy Phan, Konstantin Sobolev, Dmitry Ermilov, Igor Vorona, Nikolay Kozyrskiy, Petr Tichavsky, Andrzej Cichocki
- **Comment**: None
- **Journal**: None
- **Summary**: A rising problem in the compression of Deep Neural Networks is how to reduce the number of parameters in convolutional kernels and the complexity of these layers by low-rank tensor approximation. Canonical polyadic tensor decomposition (CPD) and Tucker tensor decomposition (TKD) are two solutions to this problem and provide promising results. However, CPD often fails due to degeneracy, making the networks unstable and hard to fine-tune. TKD does not provide much compression if the core tensor is big. This motivates using a hybrid model of CPD and TKD, a decomposition with multiple Tucker models with small core tensor, known as block term decomposition (BTD). This paper proposes a more compact model that further compresses the BTD by enforcing core tensors in BTD identical. We establish a link between the BTD with shared parameters and a looped chain tensor network (TC). Unfortunately, such strongly constrained tensor networks (with loop) encounter severe numerical instability, as proved by y (Landsberg, 2012) and (Handschuh, 2015a). We study perturbation of chain tensor networks, provide interpretation of instability in TC, demonstrate the problem. We propose novel methods to gain the stability of the decomposition results, keep the network robust and attain better approximation. Experimental results will confirm the superiority of the proposed methods in compression of well-known CNNs, and TC decomposition under challenging scenarios



### Important Object Identification with Semi-Supervised Learning for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2203.02634v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.02634v1)
- **Published**: 2022-03-05 01:23:13+00:00
- **Updated**: 2022-03-05 01:23:13+00:00
- **Authors**: Jiachen Li, Haiming Gang, Hengbo Ma, Masayoshi Tomizuka, Chiho Choi
- **Comment**: ICRA 2022
- **Journal**: None
- **Summary**: Accurate identification of important objects in the scene is a prerequisite for safe and high-quality decision making and motion planning of intelligent agents (e.g., autonomous vehicles) that navigate in complex and dynamic environments. Most existing approaches attempt to employ attention mechanisms to learn importance weights associated with each object indirectly via various tasks (e.g., trajectory prediction), which do not enforce direct supervision on the importance estimation. In contrast, we tackle this task in an explicit way and formulate it as a binary classification ("important" or "unimportant") problem. We propose a novel approach for important object identification in egocentric driving scenarios with relational reasoning on the objects in the scene. Besides, since human annotations are limited and expensive to obtain, we present a semi-supervised learning pipeline to enable the model to learn from unlimited unlabeled data. Moreover, we propose to leverage the auxiliary tasks of ego vehicle behavior prediction to further improve the accuracy of importance estimation. The proposed approach is evaluated on a public egocentric driving dataset (H3D) collected in complex traffic scenarios. A detailed ablative study is conducted to demonstrate the effectiveness of each model component and the training strategy. Our approach also outperforms rule-based baselines by a large margin.



### Training privacy-preserving video analytics pipelines by suppressing features that reveal information about private attributes
- **Arxiv ID**: http://arxiv.org/abs/2203.02635v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.02635v2)
- **Published**: 2022-03-05 01:31:07+00:00
- **Updated**: 2022-06-01 21:02:04+00:00
- **Authors**: Chau Yi Li, Andrea Cavallaro
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are increasingly deployed for scene analytics, including to evaluate the attention and reaction of people exposed to out-of-home advertisements. However, the features extracted by a deep neural network that was trained to predict a specific, consensual attribute (e.g. emotion) may also encode and thus reveal information about private, protected attributes (e.g. age or gender). In this work, we focus on such leakage of private information at inference time. We consider an adversary with access to the features extracted by the layers of a deployed neural network and use these features to predict private attributes. To prevent the success of such an attack, we modify the training of the network using a confusion loss that encourages the extraction of features that make it difficult for the adversary to accurately predict private attributes. We validate this training approach on image-based tasks using a publicly available dataset. Results show that, compared to the original network, the proposed PrivateNet can reduce the leakage of private information of a state-of-the-art emotion recognition classifier by 2.88% for gender and by 13.06% for age group, with a minimal effect on task accuracy.



### Boosting Crowd Counting via Multifaceted Attention
- **Arxiv ID**: http://arxiv.org/abs/2203.02636v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.02636v1)
- **Published**: 2022-03-05 01:36:43+00:00
- **Updated**: 2022-03-05 01:36:43+00:00
- **Authors**: Hui Lin, Zhiheng Ma, Rongrong Ji, Yaowei Wang, Xiaopeng Hong
- **Comment**: Accepted by IEEE CVPR 2022. Codes available at:
  https://github.com/LoraLinH/Boosting-Crowd-Counting-via-Multifaceted-Attention
- **Journal**: None
- **Summary**: This paper focuses on the challenging crowd counting task. As large-scale variations often exist within crowd images, neither fixed-size convolution kernel of CNN nor fixed-size attention of recent vision transformers can well handle this kind of variation. To address this problem, we propose a Multifaceted Attention Network (MAN) to improve transformer models in local spatial relation encoding. MAN incorporates global attention from a vanilla transformer, learnable local attention, and instance attention into a counting model. Firstly, the local Learnable Region Attention (LRA) is proposed to assign attention exclusively for each feature location dynamically. Secondly, we design the Local Attention Regularization to supervise the training of LRA by minimizing the deviation among the attention for different feature locations. Finally, we provide an Instance Attention mechanism to focus on the most important instances dynamically during training. Extensive experiments on four challenging crowd counting datasets namely ShanghaiTech, UCF-QNRF, JHU++, and NWPU have validated the proposed method. Codes: https://github.com/LoraLinH/Boosting-Crowd-Counting-via-Multifaceted-Attention.



### Cluster-based Contrastive Disentangling for Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.02648v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.02648v1)
- **Published**: 2022-03-05 02:50:12+00:00
- **Updated**: 2022-03-05 02:50:12+00:00
- **Authors**: Yi Gao, Chenwei Tang, Jiancheng Lv
- **Comment**: None
- **Journal**: None
- **Summary**: Generalized Zero-Shot Learning (GZSL) aims to recognize both seen and unseen classes by training only the seen classes, in which the instances of unseen classes tend to be biased towards the seen class. In this paper, we propose a Cluster-based Contrastive Disentangling (CCD) method to improve GZSL by alleviating the semantic gap and domain shift problems. Specifically, we first cluster the batch data to form several sets containing similar classes. Then, we disentangle the visual features into semantic-unspecific and semantic-matched variables, and further disentangle the semantic-matched variables into class-shared and class-unique variables according to the clustering results. The disentangled learning module with random swapping and semantic-visual alignment bridges the semantic gap. Moreover, we introduce contrastive learning on semantic-matched and class-unique variables to learn high intra-set and intra-class similarity, as well as inter-set and inter-class discriminability. Then, the generated visual features conform to the underlying characteristics of general images and have strong discriminative information, which alleviates the domain shift problem well. We evaluate our proposed method on four datasets and achieve state-of-the-art results in both conventional and generalized settings.



### Ensemble Knowledge Guided Sub-network Search and Fine-tuning for Filter Pruning
- **Arxiv ID**: http://arxiv.org/abs/2203.02651v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.02651v3)
- **Published**: 2022-03-05 03:43:40+00:00
- **Updated**: 2022-07-08 20:03:23+00:00
- **Authors**: Seunghyun Lee, Byung Cheol Song
- **Comment**: accepted to ECCV2022
- **Journal**: None
- **Summary**: Conventional NAS-based pruning algorithms aim to find the sub-network with the best validation performance. However, validation performance does not successfully represent test performance, i.e., potential performance. Also, although fine-tuning the pruned network to restore the performance drop is an inevitable process, few studies have handled this issue. This paper provides a novel Ensemble Knowledge Guidance (EKG) to solve both problems at once. First, we experimentally prove that the fluctuation of loss landscape can be an effective metric to evaluate the potential performance. In order to search a sub-network with the smoothest loss landscape at a low cost, we employ EKG as a search reward. EKG utilized for the following search iteration is composed of the ensemble knowledge of interim sub-networks, i.e., the by-products of the sub-network evaluation. Next, we reuse EKG to provide a gentle and informative guidance to the pruned network while fine-tuning the pruned network. Since EKG is implemented as a memory bank in both phases, it requires a negligible cost. For example, when pruning and training ResNet-50, just 315 GPU hours are required to remove around 45.04% of FLOPS without any performance degradation, which can operate even on a low-spec workstation. the implemented code is available at https://github.com/sseung0703/EKG.



### A Large-scale Comprehensive Dataset and Copy-overlap Aware Evaluation Protocol for Segment-level Video Copy Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.02654v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.02654v2)
- **Published**: 2022-03-05 04:39:34+00:00
- **Updated**: 2022-06-16 08:55:27+00:00
- **Authors**: Sifeng He, Xudong Yang, Chen Jiang, Gang Liang, Wei Zhang, Tan Pan, Qing Wang, Furong Xu, Chunguang Li, Jingxiong Liu, Hui Xu, Kaiming Huang, Yuan Cheng, Feng Qian, Xiaobo Zhang, Lei Yang
- **Comment**: Accepted by CVPR 2022. Codes are all publicly available at
  https://github.com/alipay/VCSL
- **Journal**: None
- **Summary**: In this paper, we introduce VCSL (Video Copy Segment Localization), a new comprehensive segment-level annotated video copy dataset. Compared with existing copy detection datasets restricted by either video-level annotation or small-scale, VCSL not only has two orders of magnitude more segment-level labelled data, with 160k realistic video copy pairs containing more than 280k localized copied segment pairs, but also covers a variety of video categories and a wide range of video duration. All the copied segments inside each collected video pair are manually extracted and accompanied by precisely annotated starting and ending timestamps. Alongside the dataset, we also propose a novel evaluation protocol that better measures the prediction accuracy of copy overlapping segments between a video pair and shows improved adaptability in different scenarios. By benchmarking several baseline and state-of-the-art segment-level video copy detection methods with the proposed dataset and evaluation metric, we provide a comprehensive analysis that uncovers the strengths and weaknesses of current approaches, hoping to open up promising directions for future works. The VCSL dataset, metric and benchmark codes are all publicly available at https://github.com/alipay/VCSL.



### Triple Motion Estimation and Frame Interpolation based on Adaptive Threshold for Frame Rate Up-Conversion
- **Arxiv ID**: http://arxiv.org/abs/2203.03621v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2203.03621v1)
- **Published**: 2022-03-05 04:39:42+00:00
- **Updated**: 2022-03-05 04:39:42+00:00
- **Authors**: Hanieh Naderi, Mohammad Rahmati
- **Comment**: Frame rate up-conversion, frame interpolation, motion estimation,
  motion compensation
- **Journal**: None
- **Summary**: In this paper, we propose a novel motion-compensated frame rate up-conversion (MC-FRUC) algorithm. The proposed algorithm creates interpolated frames by first estimating motion vectors using unilateral (jointing forward and backward) and bilateral motion estimation. Then motion vectors are combined based on adaptive threshold, in order to creates high-quality interpolated frames and reduce block artifacts. Since motion-compensated frame interpolation along unilateral motion trajectories yields holes, a new algorithm is introduced to resolve this problem. The experimental results show that the quality of the interpolated frames using the proposed algorithm is much higher than the existing algorithms.



### Audio-visual speech separation based on joint feature representation with cross-modal attention
- **Arxiv ID**: http://arxiv.org/abs/2203.02655v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2203.02655v1)
- **Published**: 2022-03-05 04:39:46+00:00
- **Updated**: 2022-03-05 04:39:46+00:00
- **Authors**: Junwen Xiong, Peng Zhang, Lei Xie, Wei Huang, Yufei Zha, Yanning Zhang
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Multi-modal based speech separation has exhibited a specific advantage on isolating the target character in multi-talker noisy environments. Unfortunately, most of current separation strategies prefer a straightforward fusion based on feature learning of each single modality, which is far from sufficient consideration of inter-relationships between modalites. Inspired by learning joint feature representations from audio and visual streams with attention mechanism, in this study, a novel cross-modal fusion strategy is proposed to benefit the whole framework with semantic correlations between different modalities. To further improve audio-visual speech separation, the dense optical flow of lip motion is incorporated to strengthen the robustness of visual representation. The evaluation of the proposed work is performed on two public audio-visual speech separation benchmark datasets. The overall improvement of the performance has demonstrated that the additional motion network effectively enhances the visual representation of the combined lip images and audio signal, as well as outperforming the baseline in terms of all metrics with the proposed cross-modal fusion.



### Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic Segmentation with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2203.02664v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.02664v2)
- **Published**: 2022-03-05 06:07:17+00:00
- **Updated**: 2022-09-08 05:38:37+00:00
- **Authors**: Lixiang Ru, Yibing Zhan, Baosheng Yu, Bo Du
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Weakly-supervised semantic segmentation (WSSS) with image-level labels is an important and challenging task. Due to the high training efficiency, end-to-end solutions for WSSS have received increasing attention from the community. However, current methods are mainly based on convolutional neural networks and fail to explore the global information properly, thus usually resulting in incomplete object regions. In this paper, to address the aforementioned problem, we introduce Transformers, which naturally integrate global information, to generate more integral initial pseudo labels for end-to-end WSSS. Motivated by the inherent consistency between the self-attention in Transformers and the semantic affinity, we propose an Affinity from Attention (AFA) module to learn semantic affinity from the multi-head self-attention (MHSA) in Transformers. The learned affinity is then leveraged to refine the initial pseudo labels for segmentation. In addition, to efficiently derive reliable affinity labels for supervising AFA and ensure the local consistency of pseudo labels, we devise a Pixel-Adaptive Refinement module that incorporates low-level image appearance information to refine the pseudo labels. We perform extensive experiments and our method achieves 66.0% and 38.9% mIoU on the PASCAL VOC 2012 and MS COCO 2014 datasets, respectively, significantly outperforming recent end-to-end methods and several multi-stage competitors. Code is available at https://github.com/rulixiang/afa.



### Deep-ASPECTS: A Segmentation-Assisted Model for Stroke Severity Measurement
- **Arxiv ID**: http://arxiv.org/abs/2203.03622v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2203.03622v3)
- **Published**: 2022-03-05 06:12:49+00:00
- **Updated**: 2022-05-09 08:06:02+00:00
- **Authors**: Ujjwal Upadhyay, Mukul Ranjan, Satish Golla, Swetha Tanamala, Preetham Sreenivas, Sasank Chilamkurthy, Jeyaraj Pandian, Jason Tarpley
- **Comment**: None
- **Journal**: None
- **Summary**: A stroke occurs when an artery in the brain ruptures and bleeds or when the blood supply to the brain is cut off. Blood and oxygen cannot reach the brain's tissues due to the rupture or obstruction resulting in tissue death. The Middle cerebral artery (MCA) is the largest cerebral artery and the most commonly damaged vessel in stroke. The quick onset of a focused neurological deficit caused by interruption of blood flow in the territory supplied by the MCA is known as an MCA stroke. Alberta stroke programme early CT score (ASPECTS) is used to estimate the extent of early ischemic changes in patients with MCA stroke. This study proposes a deep learning-based method to score the CT scan for ASPECTS. Our work has three highlights. First, we propose a novel method for medical image segmentation for stroke detection. Second, we show the effectiveness of AI solution for fully-automated ASPECT scoring with reduced diagnosis time for a given non-contrast CT (NCCT) Scan. Our algorithms show a dice similarity coefficient of 0.64 for the MCA anatomy segmentation and 0.72 for the infarcts segmentation. Lastly, we show that our model's performance is inline with inter-reader variability between radiologists.



### Cross Language Image Matching for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.02668v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2203.02668v2)
- **Published**: 2022-03-05 06:39:48+00:00
- **Updated**: 2022-03-25 09:21:59+00:00
- **Authors**: Jinheng Xie, Xianxu Hou, Kai Ye, Linlin Shen
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: It has been widely known that CAM (Class Activation Map) usually only activates discriminative object regions and falsely includes lots of object-related backgrounds. As only a fixed set of image-level object labels are available to the WSSS (weakly supervised semantic segmentation) model, it could be very difficult to suppress those diverse background regions consisting of open set objects. In this paper, we propose a novel Cross Language Image Matching (CLIMS) framework, based on the recently introduced Contrastive Language-Image Pre-training (CLIP) model, for WSSS. The core idea of our framework is to introduce natural language supervision to activate more complete object regions and suppress closely-related open background regions. In particular, we design object, background region and text label matching losses to guide the model to excite more reasonable object regions for CAM of each category. In addition, we design a co-occurring background suppression loss to prevent the model from activating closely-related background regions, with a predefined set of class-related background text descriptions. These designs enable the proposed CLIMS to generate a more complete and compact activation map for the target objects. Extensive experiments on PASCAL VOC2012 dataset show that our CLIMS significantly outperforms the previous state-of-the-art methods.



### Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2203.03623v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.03623v1)
- **Published**: 2022-03-05 07:37:45+00:00
- **Updated**: 2022-03-05 07:37:45+00:00
- **Authors**: Yutong Xie, Quanzheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel and unified method, measurement-conditioned denoising diffusion probabilistic model (MC-DDPM), for under-sampled medical image reconstruction based on DDPM. Different from previous works, MC-DDPM is defined in measurement domain (e.g. k-space in MRI reconstruction) and conditioned on under-sampling mask. We apply this method to accelerate MRI reconstruction and the experimental results show excellent performance, outperforming full supervision baseline and the state-of-the-art score-based reconstruction method. Due to its generative nature, MC-DDPM can also quantify the uncertainty of reconstruction. Our code is available on github.



### Newton-PnP: Real-time Visual Navigation for Autonomous Toy-Drones
- **Arxiv ID**: http://arxiv.org/abs/2203.02686v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.02686v1)
- **Published**: 2022-03-05 09:00:50+00:00
- **Updated**: 2022-03-05 09:00:50+00:00
- **Authors**: Ibrahim Jubran, Fares Fares, Yuval Alfassi, Firas Ayoub, Dan Feldman
- **Comment**: None
- **Journal**: None
- **Summary**: The Perspective-n-Point problem aims to estimate the relative pose between a calibrated monocular camera and a known 3D model, by aligning pairs of 2D captured image points to their corresponding 3D points in the model. We suggest an algorithm that runs on weak IoT devices in real-time but still provides provable theoretical guarantees for both running time and correctness. Existing solvers provide only one of these requirements. Our main motivation was to turn the popular DJI's Tello Drone (<90gr, <\$100) into an autonomous drone that navigates in an indoor environment with no external human/laptop/sensor, by simply attaching a Raspberry PI Zero (<9gr, <\$25) to it. This tiny micro-processor takes as input a real-time video from a tiny RGB camera, and runs our PnP solver on-board. Extensive experimental results, open source code, and a demonstration video are included.



### FCNet: A Convolutional Neural Network for Arbitrary-Length Exposure Estimation
- **Arxiv ID**: http://arxiv.org/abs/2203.03624v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.03624v3)
- **Published**: 2022-03-05 09:06:29+00:00
- **Updated**: 2023-07-01 02:31:42+00:00
- **Authors**: Jin Liang, Yuchen Yang, Anran Zhang, Jun Xu, Hui Li, Xiantong Zhen
- **Comment**: None
- **Journal**: None
- **Summary**: The photographs captured by digital cameras usually suffer from over or under exposure problems. For image exposure enhancement, the tasks of Single-Exposure Correction (SEC) and Multi-Exposure Fusion (MEF) are widely studied in the image processing community. However, current SEC or MEF methods are developed under different motivations and thus ignore the internal correlation between SEC and MEF, making it difficult to process arbitrary-length sequences with improper exposures. Besides, the MEF methods usually fail at estimating the exposure of a sequence containing only under-exposed or over-exposed images. To alleviate these problems, in this paper, we develop a novel Fusion-Correction Network (FCNet) to tackle an arbitrary-length (including one) image sequence with improper exposures. This is achieved by fusing and correcting an image sequence by Laplacian Pyramid (LP) image decomposition. In each LP level, the low-frequency base component of the input image sequence is fed into a Fusion block and a Correction block sequentially for consecutive exposure estimation, implemented by alternative exposure fusion and correction. The exposure-corrected image in current LP level is upsampled and fused with the high-frequency detail components of the input image sequence in the next LP level, to output the base component for the Fusion and Correction blocks in next LP level. Experiments on the benchmark dataset demonstrate that our FCNet is effective on arbitrary-length exposure estimation, including both SEC and MEF.



### Zoom In and Out: A Mixed-scale Triplet Network for Camouflaged Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.02688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.02688v1)
- **Published**: 2022-03-05 09:13:52+00:00
- **Updated**: 2022-03-05 09:13:52+00:00
- **Authors**: Pang Youwei, Zhao Xiaoqi, Xiang Tian-Zhu, Zhang Lihe, Lu Huchuan
- **Comment**: Accepted by CVPR2022. This is the arxiv version that contains the
  appendix section
- **Journal**: None
- **Summary**: The recently proposed camouflaged object detection (COD) attempts to segment objects that are visually blended into their surroundings, which is extremely complex and difficult in real-world scenarios. Apart from high intrinsic similarity between the camouflaged objects and their background, the objects are usually diverse in scale, fuzzy in appearance, and even severely occluded. To deal with these problems, we propose a mixed-scale triplet network, \textbf{ZoomNet}, which mimics the behavior of humans when observing vague images, i.e., zooming in and out. Specifically, our ZoomNet employs the zoom strategy to learn the discriminative mixed-scale semantics by the designed scale integration unit and hierarchical mixed-scale unit, which fully explores imperceptible clues between the candidate objects and background surroundings. Moreover, considering the uncertainty and ambiguity derived from indistinguishable textures, we construct a simple yet effective regularization constraint, uncertainty-aware loss, to promote the model to accurately produce predictions with higher confidence in candidate regions. Without bells and whistles, our proposed highly task-friendly model consistently surpasses the existing 23 state-of-the-art methods on four public datasets. Besides, the superior performance over the recent cutting-edge models on the SOD task also verifies the effectiveness and generality of our model. The code will be available at \url{https://github.com/lartpang/ZoomNet}.



### Federated and Generalized Person Re-identification through Domain and Feature Hallucinating
- **Arxiv ID**: http://arxiv.org/abs/2203.02689v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.02689v2)
- **Published**: 2022-03-05 09:15:13+00:00
- **Updated**: 2022-03-08 12:04:03+00:00
- **Authors**: Fengxiang Yang, Zhun Zhong, Zhiming Luo, Shaozi Li, Nicu Sebe
- **Comment**: Under Review
- **Journal**: None
- **Summary**: In this paper, we study the problem of federated domain generalization (FedDG) for person re-identification (re-ID), which aims to learn a generalized model with multiple decentralized labeled source domains. An empirical method (FedAvg) trains local models individually and averages them to obtain the global model for further local fine-tuning or deploying in unseen target domains. One drawback of FedAvg is neglecting the data distributions of other clients during local training, making the local model overfit local data and producing a poorly-generalized global model. To solve this problem, we propose a novel method, called "Domain and Feature Hallucinating (DFH)", to produce diverse features for learning generalized local and global models. Specifically, after each model aggregation process, we share the Domain-level Feature Statistics (DFS) among different clients without violating data privacy. During local training, the DFS are used to synthesize novel domain statistics with the proposed domain hallucinating, which is achieved by re-weighting DFS with random weights. Then, we propose feature hallucinating to diversify local features by scaling and shifting them to the distribution of the obtained novel domain. The synthesized novel features retain the original pair-wise similarities, enabling us to utilize them to optimize the model in a supervised manner. Extensive experiments verify that the proposed DFH can effectively improve the generalization ability of the global model. Our method achieves the state-of-the-art performance for FedDG on four large-scale re-ID benchmarks.



### IDmUNet: A new image decomposition induced network for sparse feature segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.02690v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2203.02690v1)
- **Published**: 2022-03-05 09:17:32+00:00
- **Updated**: 2022-03-05 09:17:32+00:00
- **Authors**: Yumeng Ren, Yiming Gao, Chunlin Wu, Xue-cheng Tai
- **Comment**: 22 pages, 6 figures
- **Journal**: None
- **Summary**: UNet and its variants are among the most popular methods for medical image segmentation. Despite their successes in task generality, most of them consider little mathematical modeling behind specific applications. In this paper, we focus on the sparse feature segmentation task and make a task-oriented network design, in which the target objects are sparsely distributed and the background is hard to be mathematically modeled. We start from an image decomposition model with sparsity regularization, and propose a deep unfolding network, namely IDNet, based on an iterative solver, scaled alternating direction method of multipliers (scaled-ADMM). The IDNet splits raw inputs into double feature layers. Then a new task-oriented segmentation network is constructed, dubbed as IDmUNet, based on the proposed IDNets and a mini-UNet. Because of the sparsity prior and deep unfolding method in the structure design, this IDmUNet combines the advantages of mathematical modeling and data-driven approaches. Firstly, our approach has mathematical interpretability and can achieve favorable performance with far fewer learnable parameters. Secondly, our IDmUNet is robust in a simple end-to-end training with explainable behaviors. In the experiments of retinal vessel segmentation (RVS), IDmUNet produces the state-of-the-art results with only 0.07m parameters, whereas SA-UNet, one of the latest variants of UNet, contains 0.54m and the original UNet 31.04m. Moreover, the training procedure of our network converges faster without overfitting phenomenon. This decomposition-based network construction strategy can be generalized to other problems with mathematically clear targets and complicated unclear backgrounds.



### High-resolution Coastline Extraction in SAR Images via MISP-GGD Superpixel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.02708v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.02708v1)
- **Published**: 2022-03-05 11:23:42+00:00
- **Updated**: 2022-03-05 11:23:42+00:00
- **Authors**: Odysseas Pappas, Nantheera Anantrasirichai, Byron Adams, Alin Achim
- **Comment**: To appear in proceedings CIE RADAR 2021
- **Journal**: None
- **Summary**: High accuracy coastline/shoreline extraction from SAR imagery is a crucial step in a number of maritime and coastal monitoring applications. We present a method based on image segmentation using the Generalised Gamma Mixture Model superpixel algorithm (MISP-GGD). MISP-GGD produces superpixels adhering with great accuracy to object edges in the image, such as the coastline. Unsupervised clustering of the generated superpixels according to textural and radiometric features allows for generation of a land/water mask from which a highly accurate coastline can be extracted. We present results of our proposed method on a number of SAR images of varying characteristics.



### Towards Efficient and Scalable Sharpness-Aware Minimization
- **Arxiv ID**: http://arxiv.org/abs/2203.02714v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.02714v1)
- **Published**: 2022-03-05 11:53:37+00:00
- **Updated**: 2022-03-05 11:53:37+00:00
- **Authors**: Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, Yang You
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Recently, Sharpness-Aware Minimization (SAM), which connects the geometry of the loss landscape and generalization, has demonstrated significant performance boosts on training large-scale models such as vision transformers. However, the update rule of SAM requires two sequential (non-parallelizable) gradient computations at each step, which can double the computational overhead. In this paper, we propose a novel algorithm LookSAM - that only periodically calculates the inner gradient ascent, to significantly reduce the additional training cost of SAM. The empirical results illustrate that LookSAM achieves similar accuracy gains to SAM while being tremendously faster - it enjoys comparable computational complexity with first-order optimizers such as SGD or Adam. To further evaluate the performance and scalability of LookSAM, we incorporate a layer-wise modification and perform experiments in the large-batch training scenario, which is more prone to converge to sharp local minima. We are the first to successfully scale up the batch size when training Vision Transformers (ViTs). With a 64k batch size, we are able to train ViTs from scratch in minutes while maintaining competitive performance.



### A Novel Dual Dense Connection Network for Video Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2203.02723v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.02723v1)
- **Published**: 2022-03-05 12:21:29+00:00
- **Updated**: 2022-03-05 12:21:29+00:00
- **Authors**: Guofang Li, Yonggui Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Video super-resolution (VSR) refers to the reconstruction of high-resolution (HR) video from the corresponding low-resolution (LR) video. Recently, VSR has received increasing attention. In this paper, we propose a novel dual dense connection network that can generate high-quality super-resolution (SR) results. The input frames are creatively divided into reference frame, pre-temporal group and post-temporal group, representing information in different time periods. This grouping method provides accurate information of different time periods without causing time information disorder. Meanwhile, we produce a new loss function, which is beneficial to enhance the convergence ability of the model. Experiments show that our model is superior to other advanced models in Vid4 datasets and SPMCS-11 datasets.



### An End-to-End Approach for Seam Carving Detection using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2203.02728v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.02728v1)
- **Published**: 2022-03-05 12:53:55+00:00
- **Updated**: 2022-03-05 12:53:55+00:00
- **Authors**: Thierry P. Moreira, Marcos Cleison S. Santana, Leandro A. Passos João Paulo Papa, Kelton Augusto P. da Costa
- **Comment**: None
- **Journal**: None
- **Summary**: Seam carving is a computational method capable of resizing images for both reduction and expansion based on its content, instead of the image geometry. Although the technique is mostly employed to deal with redundant information, i.e., regions composed of pixels with similar intensity, it can also be used for tampering images by inserting or removing relevant objects. Therefore, detecting such a process is of extreme importance regarding the image security domain. However, recognizing seam-carved images does not represent a straightforward task even for human eyes, and robust computation tools capable of identifying such alterations are very desirable. In this paper, we propose an end-to-end approach to cope with the problem of automatic seam carving detection that can obtain state-of-the-art results. Experiments conducted over public and private datasets with several tampering configurations evidence the suitability of the proposed model.



### MaxDropoutV2: An Improved Method to Drop out Neurons in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2203.02740v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.02740v1)
- **Published**: 2022-03-05 13:41:56+00:00
- **Updated**: 2022-03-05 13:41:56+00:00
- **Authors**: Claudio Filipi Goncalves do Santos, Mateus Roder, Leandro A. Passos, João P. Papa
- **Comment**: None
- **Journal**: None
- **Summary**: In the last decade, exponential data growth supplied the machine learning-based algorithms' capacity and enabled their usage in daily life activities. Additionally, such an improvement is partially explained due to the advent of deep learning techniques, i.e., stacks of simple architectures that end up in more complex models. Although both factors produce outstanding results, they also pose drawbacks regarding the learning process since training complex models denotes an expensive task and results are prone to overfit the training data. A supervised regularization technique called MaxDropout was recently proposed to tackle the latter, providing several improvements concerning traditional regularization approaches. In this paper, we present its improved version called MaxDropoutV2. Results considering two public datasets show that the model performs faster than the standard version and, in most cases, provides more accurate results.



### MetaFormer: A Unified Meta Framework for Fine-Grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/2203.02751v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.02751v1)
- **Published**: 2022-03-05 14:12:25+00:00
- **Updated**: 2022-03-05 14:12:25+00:00
- **Authors**: Qishuai Diao, Yi Jiang, Bin Wen, Jia Sun, Zehuan Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Fine-Grained Visual Classification(FGVC) is the task that requires recognizing the objects belonging to multiple subordinate categories of a super-category. Recent state-of-the-art methods usually design sophisticated learning pipelines to tackle this task. However, visual information alone is often not sufficient to accurately differentiate between fine-grained visual categories. Nowadays, the meta-information (e.g., spatio-temporal prior, attribute, and text description) usually appears along with the images. This inspires us to ask the question: Is it possible to use a unified and simple framework to utilize various meta-information to assist in fine-grained identification? To answer this problem, we explore a unified and strong meta-framework(MetaFormer) for fine-grained visual classification. In practice, MetaFormer provides a simple yet effective approach to address the joint learning of vision and various meta-information. Moreover, MetaFormer also provides a strong baseline for FGVC without bells and whistles. Extensive experiments demonstrate that MetaFormer can effectively use various meta-information to improve the performance of fine-grained recognition. In a fair comparison, MetaFormer can outperform the current SotA approaches with only vision information on the iNaturalist2017 and iNaturalist2018 datasets. Adding meta-information, MetaFormer can exceed the current SotA approaches by 5.9% and 5.3%, respectively. Moreover, MetaFormer can achieve 92.3% and 92.7% on CUB-200-2011 and NABirds, which significantly outperforms the SotA approaches. The source code and pre-trained models are released athttps://github.com/dqshuai/MetaFormer.



### DrawingInStyles: Portrait Image Generation and Editing with Spatially Conditioned StyleGAN
- **Arxiv ID**: http://arxiv.org/abs/2203.02762v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.02762v3)
- **Published**: 2022-03-05 14:54:07+00:00
- **Updated**: 2022-05-31 07:17:58+00:00
- **Authors**: Wanchao Su, Hui Ye, Shu-Yu Chen, Lin Gao, Hongbo Fu
- **Comment**: 15 pages, 14 figures
- **Journal**: None
- **Summary**: The research topic of sketch-to-portrait generation has witnessed a boost of progress with deep learning techniques. The recently proposed StyleGAN architectures achieve state-of-the-art generation ability but the original StyleGAN is not friendly for sketch-based creation due to its unconditional generation nature. To address this issue, we propose a direct conditioning strategy to better preserve the spatial information under the StyleGAN framework. Specifically, we introduce Spatially Conditioned StyleGAN (SC-StyleGAN for short), which explicitly injects spatial constraints to the original StyleGAN generation process. We explore two input modalities, sketches and semantic maps, which together allow users to express desired generation results more precisely and easily. Based on SC-StyleGAN, we present DrawingInStyles, a novel drawing interface for non-professional users to easily produce high-quality, photo-realistic face images with precise control, either from scratch or editing existing ones. Qualitative and quantitative evaluations show the superior generation ability of our method to existing and alternative solutions. The usability and expressiveness of our system are confirmed by a user study.



### Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2203.02764v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.02764v1)
- **Published**: 2022-03-05 14:56:14+00:00
- **Updated**: 2022-03-05 14:56:14+00:00
- **Authors**: Yicong Hong, Zun Wang, Qi Wu, Stephen Gould
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing works in vision-and-language navigation (VLN) focus on either discrete or continuous environments, training agents that cannot generalize across the two. The fundamental difference between the two setups is that discrete navigation assumes prior knowledge of the connectivity graph of the environment, so that the agent can effectively transfer the problem of navigation with low-level controls to jumping from node to node with high-level actions by grounding to an image of a navigable direction. To bridge the discrete-to-continuous gap, we propose a predictor to generate a set of candidate waypoints during navigation, so that agents designed with high-level actions can be transferred to and trained in continuous environments. We refine the connectivity graph of Matterport3D to fit the continuous Habitat-Matterport3D, and train the waypoints predictor with the refined graphs to produce accessible waypoints at each time step. Moreover, we demonstrate that the predicted waypoints can be augmented during training to diversify the views and paths, and therefore enhance agent's generalization ability. Through extensive experiments we show that agents navigating in continuous environments with predicted waypoints perform significantly better than agents using low-level actions, which reduces the absolute discrete-to-continuous gap by 11.76% Success Weighted by Path Length (SPL) for the Cross-Modal Matching Agent and 18.24% SPL for the Recurrent VLN-BERT. Our agents, trained with a simple imitation learning objective, outperform previous methods by a large margin, achieving new state-of-the-art results on the testing environments of the R2R-CE and the RxR-CE datasets.



### Towards Robust Part-aware Instance Segmentation for Industrial Bin Picking
- **Arxiv ID**: http://arxiv.org/abs/2203.02767v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.02767v1)
- **Published**: 2022-03-05 14:58:05+00:00
- **Updated**: 2022-03-05 14:58:05+00:00
- **Authors**: Yidan Feng, Biqi Yang, Xianzhi Li, Chi-Wing Fu, Rui Cao, Kai Chen, Qi Dou, Mingqiang Wei, Yun-Hui Liu, Pheng-Ann Heng
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Industrial bin picking is a challenging task that requires accurate and robust segmentation of individual object instances. Particularly, industrial objects can have irregular shapes, that is, thin and concave, whereas in bin-picking scenarios, objects are often closely packed with strong occlusion. To address these challenges, we formulate a novel part-aware instance segmentation pipeline. The key idea is to decompose industrial objects into correlated approximate convex parts and enhance the object-level segmentation with part-level segmentation. We design a part-aware network to predict part masks and part-to-part offsets, followed by a part aggregation module to assemble the recognized parts into instances. To guide the network learning, we also propose an automatic label decoupling scheme to generate ground-truth part-level labels from instance-level labels. Finally, we contribute the first instance segmentation dataset, which contains a variety of industrial objects that are thin and have non-trivial shapes. Extensive experimental results on various industrial objects demonstrate that our method can achieve the best segmentation results compared with the state-of-the-art approaches.



### Don't Be So Dense: Sparse-to-Sparse GAN Training Without Sacrificing Performance
- **Arxiv ID**: http://arxiv.org/abs/2203.02770v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.02770v1)
- **Published**: 2022-03-05 15:18:03+00:00
- **Updated**: 2022-03-05 15:18:03+00:00
- **Authors**: Shiwei Liu, Yuesong Tian, Tianlong Chen, Li Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have received an upsurging interest since being proposed due to the high quality of the generated data. While achieving increasingly impressive results, the resource demands associated with the large model size hinders the usage of GANs in resource-limited scenarios. For inference, the existing model compression techniques can reduce the model complexity with comparable performance. However, the training efficiency of GANs has less been explored due to the fragile training process of GANs. In this paper, we, for the first time, explore the possibility of directly training sparse GAN from scratch without involving any dense or pre-training steps. Even more unconventionally, our proposed method enables directly training sparse unbalanced GANs with an extremely sparse generator from scratch. Instead of training full GANs, we start with sparse GANs and dynamically explore the parameter space spanned over the generator throughout training. Such a sparse-to-sparse training procedure enhances the capacity of the highly sparse generator progressively while sticking to a fixed small parameter budget with appealing training and inference efficiency gains. Extensive experiments with modern GAN architectures validate the effectiveness of our method. Our sparsified GANs, trained from scratch in one single run, are able to outperform the ones learned by expensive iterative pruning and re-training. Perhaps most importantly, we find instead of inheriting parameters from expensive pre-trained GANs, directly training sparse GANs from scratch can be a much more efficient solution. For example, only training with a 80% sparse generator and a 70% sparse discriminator, our method can achieve even better performance than the dense BigGAN.



### Rib Suppression in Digital Chest Tomosynthesis
- **Arxiv ID**: http://arxiv.org/abs/2203.02772v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.02772v1)
- **Published**: 2022-03-05 15:32:04+00:00
- **Updated**: 2022-03-05 15:32:04+00:00
- **Authors**: Yihua Sun, Qingsong Yao, Yuanyuan Lyu, Jianji Wang, Yi Xiao, Hongen Liao, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Digital chest tomosynthesis (DCT) is a technique to produce sectional 3D images of a human chest for pulmonary disease screening, with 2D X-ray projections taken within an extremely limited range of angles. However, under the limited angle scenario, DCT contains strong artifacts caused by the presence of ribs, jamming the imaging quality of the lung area. Recently, great progress has been achieved for rib suppression in a single X-ray image, to reveal a clearer lung texture. We firstly extend the rib suppression problem to the 3D case at the software level. We propose a $\textbf{T}$omosynthesis $\textbf{RI}$b Su$\textbf{P}$pression and $\textbf{L}$ung $\textbf{E}$nhancement $\textbf{Net}$work (TRIPLE-Net) to model the 3D rib component and provide a rib-free DCT. TRIPLE-Net takes the advantages from both 2D and 3D domains, which model the ribs in DCT with the exact FBP procedure and 3D depth information, respectively. The experiments on simulated datasets and clinical data have shown the effectiveness of TRIPLE-Net to preserve lung details as well as improve the imaging quality of pulmonary diseases. Finally, an expert user study confirms our findings.



### Adversarial Dual-Student with Differentiable Spatial Warping for Semi-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.02792v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.02792v3)
- **Published**: 2022-03-05 17:36:17+00:00
- **Updated**: 2022-09-27 09:36:56+00:00
- **Authors**: Cong Cao, Tianwei Lin, Dongliang He, Fu Li, Huanjing Yue, Jingyu Yang, Errui Ding
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT)
- **Journal**: None
- **Summary**: A common challenge posed to robust semantic segmentation is the expensive data annotation cost. Existing semi-supervised solutions show great potential for solving this problem. Their key idea is constructing consistency regularization with unsupervised data augmentation from unlabeled data for model training. The perturbations for unlabeled data enable the consistency training loss, which benefits semi-supervised semantic segmentation. However, these perturbations destroy image context and introduce unnatural boundaries, which is harmful for semantic segmentation. Besides, the widely adopted semi-supervised learning framework, i.e. mean-teacher, suffers performance limitation since the student model finally converges to the teacher model. In this paper, first of all, we propose a context friendly differentiable geometric warping to conduct unsupervised data augmentation; secondly, a novel adversarial dual-student framework is proposed to improve the Mean-Teacher from the following two aspects: (1) dual student models are learned independently except for a stabilization constraint to encourage exploiting model diversities; (2) adversarial training scheme is applied to both students and the discriminators are resorted to distinguish reliable pseudo-label of unlabeled data for self-training. Effectiveness is validated via extensive experiments on PASCAL VOC2012 and Cityscapes. Our solution significantly improves the performance and state-of-the-art results are achieved on both datasets. Remarkably, compared with fully supervision, our solution achieves comparable mIoU of 73.4% using only 12.5% annotated data on PASCAL VOC2012. Our codes and models are available at https://github.com/cao-cong/ADS-SemiSeg.



### Machine Learning Applications in Lung Cancer Diagnosis, Treatment and Prognosis
- **Arxiv ID**: http://arxiv.org/abs/2203.02794v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, q-bio.GN
- **Links**: [PDF](http://arxiv.org/pdf/2203.02794v3)
- **Published**: 2022-03-05 17:43:57+00:00
- **Updated**: 2022-03-26 01:28:46+00:00
- **Authors**: Yawei Li, Xin Wu, Ping Yang, Guoqian Jiang, Yuan Luo
- **Comment**: None
- **Journal**: None
- **Summary**: The recent development of imaging and sequencing technologies enables systematic advances in the clinical study of lung cancer. Meanwhile, the human mind is limited in effectively handling and fully utilizing the accumulation of such enormous amounts of data. Machine learning-based approaches play a critical role in integrating and analyzing these large and complex datasets, which have extensively characterized lung cancer through the use of different perspectives from these accrued data. In this article, we provide an overview of machine learning-based approaches that strengthen the varying aspects of lung cancer diagnosis and therapy, including early detection, auxiliary diagnosis, prognosis prediction and immunotherapy practice. Moreover, we highlight the challenges and opportunities for future applications of machine learning in lung cancer.



### WSSAMNet: Weakly Supervised Semantic Attentive Medical Image Registration Network
- **Arxiv ID**: http://arxiv.org/abs/2203.07114v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.07114v1)
- **Published**: 2022-03-05 18:55:09+00:00
- **Updated**: 2022-03-05 18:55:09+00:00
- **Authors**: Sahar Almahfouz Nasser, Nikhil Cherian Kurian, Saqib Shamsi, Mohit Meena, Amit Sethi
- **Comment**: None
- **Journal**: None
- **Summary**: We present WSSAMNet, a weakly supervised method for medical image registration. Ours is a two step method, with the first step being the computation of segmentation masks of the fixed and moving volumes. These masks are then used to attend to the input volume, which are then provided as inputs to a registration network in the second step. The registration network computes the deformation field to perform the alignment between the fixed and the moving volumes. We study the effectiveness of our technique on the BraTSReg challenge data against ANTs and VoxelMorph, where we demonstrate that our method performs competitively.



### Coordinate Translator for Learning Deformable Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2203.03626v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.03626v2)
- **Published**: 2022-03-05 21:23:03+00:00
- **Updated**: 2022-07-31 15:22:32+00:00
- **Authors**: Yihao Liu, Lianrui Zuo, Shuo Han, Yuan Xue, Jerry L. Prince, Aaron Carass
- **Comment**: None
- **Journal**: None
- **Summary**: The majority of deep learning (DL) based deformable image registration methods use convolutional neural networks (CNNs) to estimate displacement fields from pairs of moving and fixed images. This, however, requires the convolutional kernels in the CNN to not only extract intensity features from the inputs but also understand image coordinate systems. We argue that the latter task is challenging for traditional CNNs, limiting their performance in registration tasks. To tackle this problem, we first introduce Coordinate Translator, a differentiable module that identifies matched features between the fixed and moving image and outputs their coordinate correspondences without the need for training. It unloads the burden of understanding image coordinate systems for CNNs, allowing them to focus on feature extraction. We then propose a novel deformable registration network, im2grid, that uses multiple Coordinate Translator's with the hierarchical features extracted from a CNN encoder and outputs a deformation field in a coarse-to-fine fashion. We compared im2grid with the state-of-the-art DL and non-DL methods for unsupervised 3D magnetic resonance image registration. Our experiments show that im2grid outperforms these methods both qualitatively and quantitatively.



### Evaluation of Dirichlet Process Gaussian Mixtures for Segmentation on Noisy Hyperspectral Images
- **Arxiv ID**: http://arxiv.org/abs/2203.02820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.02820v1)
- **Published**: 2022-03-05 21:44:52+00:00
- **Updated**: 2022-03-05 21:44:52+00:00
- **Authors**: Kiran Mantripragada, Faisal Z. Qureshi
- **Comment**: None
- **Journal**: None
- **Summary**: Image segmentation is a fundamental step for the interpretation of Remote Sensing Images. Clustering or segmentation methods usually precede the classification task and are used as support tools for manual labeling. The most common algorithms, such as k-means, mean-shift, and MRS, require an extra manual step to find the scale parameter. The segmentation results are severely affected if the parameters are not correctly tuned and diverge from the optimal values. Additionally, the search for the optimal scale is a costly task, as it requires a comprehensive hyper-parameter search. This paper proposes and evaluates a method for segmentation of Hyperspectral Images using the Dirichlet Process Gaussian Mixture Model. Our model can self-regulate the parameters until it finds the optimal values of scale and the number of clusters in a given dataset. The results demonstrate the potential of our method to find objects in a Hyperspectral Image while bypassing the burden of manual search of the optimal parameters. In addition, our model also produces similar results on noisy datasets, while previous research usually required a pre-processing task for noise reduction and spectral smoothing.



