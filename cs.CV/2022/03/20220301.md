# Arxiv Papers in cs.CV on 2022-03-01
### Preemptive Motion Planning for Human-to-Robot Indirect Placement Handovers
- **Arxiv ID**: http://arxiv.org/abs/2203.00156v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.00156v2)
- **Published**: 2022-03-01 00:21:39+00:00
- **Updated**: 2022-04-16 21:07:54+00:00
- **Authors**: Andrew Choi, Mohammad Khalid Jawed, Jungseock Joo
- **Comment**: 6 pages, 6 figures, to appear in ICRA 2022
- **Journal**: None
- **Summary**: As technology advances, the need for safe, efficient, and collaborative human-robot-teams has become increasingly important. One of the most fundamental collaborative tasks in any setting is the object handover. Human-to-robot handovers can take either of two approaches: (1) direct hand-to-hand or (2) indirect hand-to-placement-to-pick-up. The latter approach ensures minimal contact between the human and robot but can also result in increased idle time due to having to wait for the object to first be placed down on a surface. To minimize such idle time, the robot must preemptively predict the human intent of where the object will be placed. Furthermore, for the robot to preemptively act in any sort of productive manner, predictions and motion planning must occur in real-time. We introduce a novel prediction-planning pipeline that allows the robot to preemptively move towards the human agent's intended placement location using gaze and gestures as model inputs. In this paper, we investigate the performance and drawbacks of our early intent predictor-planner as well as the practical benefits of using such a pipeline through a human-robot case study.



### Simultaneous Semantic and Instance Segmentation for Colon Nuclei Identification and Counting
- **Arxiv ID**: http://arxiv.org/abs/2203.00157v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00157v2)
- **Published**: 2022-03-01 00:25:06+00:00
- **Updated**: 2022-04-15 23:59:33+00:00
- **Authors**: Lihao Liu, Chenyang Hong, Angelica I. Aviles-Rivero, Carola-Bibiane Sch√∂nlieb
- **Comment**: 9 pages; 4 figures
- **Journal**: None
- **Summary**: We address the problem of automated nuclear segmentation, classification, and quantification from Haematoxylin and Eosin stained histology images, which is of great relevance for several downstream computational pathology applications. In this work, we present a solution framed as a simultaneous semantic and instance segmentation framework. Our solution is part of the Colon Nuclei Identification and Counting (CoNIC) Challenge. We first train a semantic and instance segmentation model separately. Our framework uses as backbone HoverNet and Cascade Mask-RCNN models. We then ensemble the results with a custom Non-Maximum Suppression embedding (NMS). In our framework, the semantic model computes a class prediction for the cells whilst the instance model provides a refined segmentation. We demonstrate, through our experimental results, that our model outperforms the provided baselines by a large margin.



### A Standardized Pipeline for Colon Nuclei Identification and Counting Challenge
- **Arxiv ID**: http://arxiv.org/abs/2203.00171v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2203.00171v2)
- **Published**: 2022-03-01 01:42:33+00:00
- **Updated**: 2022-03-20 15:01:32+00:00
- **Authors**: Jijun Cheng, Xipeng Pan, Feihu Hou, Bingchao Zhao, Jiatai Lin, Zhenbing Liu, Zaiyi Liu, Chu Han
- **Comment**: None
- **Journal**: None
- **Summary**: Nuclear segmentation and classification is an essential step for computational pathology. TIA lab from Warwick University organized a nuclear segmentation and classification challenge (CoNIC) for H&E stained histopathology images in colorectal cancer with two highly correlated tasks, nuclei segmentation and classification task and cellular composition task. There are a few obstacles we have to address in this challenge, 1) limited training samples, 2) color variation, 3) imbalanced annotations, 4) similar morphological appearance among classes. To deal with these challenges, we proposed a standardized pipeline for nuclear segmentation and classification by integrating several pluggable components. First, we built a GAN-based model to automatically generate pseudo images for data augmentation. Then we trained a self-supervised stain normalization model to solve the color variation problem. Next we constructed a baseline model HoVer-Net with cost-sensitive loss to encourage the model pay more attention on the minority classes. According to the results of the leaderboard, our proposed pipeline achieves 0.40665 mPQ+ (Rank 49th) and 0.62199 r2 (Rank 10th) in the preliminary test phase.



### Enhancing Local Feature Learning for 3D Point Cloud Processing using Unary-Pairwise Attention
- **Arxiv ID**: http://arxiv.org/abs/2203.00172v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00172v2)
- **Published**: 2022-03-01 01:43:24+00:00
- **Updated**: 2022-03-17 12:33:59+00:00
- **Authors**: Haoyi Xiu, Xin Liu, Weimin Wang, Kyoung-Sook Kim, Takayuki Shinohara, Qiong Chang, Masashi Matsuoka
- **Comment**: BMVC 2021
- **Journal**: None
- **Summary**: We present a simple but effective attention named the unary-pairwise attention (UPA) for modeling the relationship between 3D point clouds. Our idea is motivated by the analysis that the standard self-attention (SA) that operates globally tends to produce almost the same attention maps for different query positions, revealing difficulties for learning query-independent and query-dependent information jointly. Therefore, we reformulate the SA and propose query-independent (Unary) and query-dependent (Pairwise) components to facilitate the learning of both terms. In contrast to the SA, the UPA ensures query dependence via operating locally. Extensive experiments show that the UPA outperforms the SA consistently on various point cloud understanding tasks including shape classification, part segmentation, and scene segmentation. Moreover, simply equipping the popular PointNet++ method with the UPA even outperforms or is on par with the state-of-the-art attention-based approaches. In addition, the UPA systematically boosts the performance of both standard and modern networks when it is integrated into them as a compositional module.



### ACTIVE:Augmentation-Free Graph Contrastive Learning for Partial Multi-View Clustering
- **Arxiv ID**: http://arxiv.org/abs/2203.00186v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00186v1)
- **Published**: 2022-03-01 02:32:25+00:00
- **Updated**: 2022-03-01 02:32:25+00:00
- **Authors**: Yiming Wang, Dongxia Chang, Zhiqiang Fu, Jie Wen, Yao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose an augmentation-free graph contrastive learning framework, namely ACTIVE, to solve the problem of partial multi-view clustering. Notably, we suppose that the representations of similar samples (i.e., belonging to the same cluster) and their multiply views features should be similar. This is distinct from the general unsupervised contrastive learning that assumes an image and its augmentations share a similar representation. Specifically, relation graphs are constructed using the nearest neighbours to identify existing similar samples, then the constructed inter-instance relation graphs are transferred to the missing views to build graphs on the corresponding missing data. Subsequently, two main components, within-view graph contrastive learning (WGC) and cross-view graph consistency learning (CGC), are devised to maximize the mutual information of different views within a cluster. The proposed approach elevates instance-level contrastive learning and missing data inference to the cluster-level, effectively mitigating the impact of individual missing data on clustering. Experiments on several challenging datasets demonstrate the superiority of our proposed methods.



### Robots Autonomously Detecting People: A Multimodal Deep Contrastive Learning Method Robust to Intraclass Variations
- **Arxiv ID**: http://arxiv.org/abs/2203.00187v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00187v1)
- **Published**: 2022-03-01 02:36:17+00:00
- **Updated**: 2022-03-01 02:36:17+00:00
- **Authors**: Angus Fung, Beno Benhabib, Goldie Nejat
- **Comment**: None
- **Journal**: None
- **Summary**: Robotic detection of people in crowded and/or cluttered human-centered environments including hospitals, long-term care, stores and airports is challenging as people can become occluded by other people or objects, and deform due to variations in clothing or pose. There can also be loss of discriminative visual features due to poor lighting. In this paper, we present a novel multimodal person detection architecture to address the mobile robot problem of person detection under intraclass variations. We present a two-stage training approach using 1) a unique pretraining method we define as Temporal Invariant Multimodal Contrastive Learning (TimCLR), and 2) a Multimodal Faster R-CNN (MFRCNN) detector. TimCLR learns person representations that are invariant under intraclass variations through unsupervised learning. Our approach is unique in that it generates image pairs from natural variations within multimodal image sequences, in addition to synthetic data augmentation, and contrasts crossmodal features to transfer invariances between different modalities. These pretrained features are used by the MFRCNN detector for finetuning and person detection from RGB-D images. Extensive experiments validate the performance of our DL architecture in both human-centered crowded and cluttered environments. Results show that our method outperforms existing unimodal and multimodal person detection approaches in terms of detection accuracy in detecting people with body occlusions and pose deformations in different lighting conditions.



### Semi-supervised Deep Learning for Image Classification with Distribution Mismatch: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2203.00190v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.00190v3)
- **Published**: 2022-03-01 02:46:00+00:00
- **Updated**: 2022-03-10 15:54:29+00:00
- **Authors**: Saul Calderon-Ramirez, Shengxiang Yang, David Elizondo
- **Comment**: Submission to IEEE Transactions on AI
- **Journal**: None
- **Summary**: Deep learning methodologies have been employed in several different fields, with an outstanding success in image recognition applications, such as material quality control, medical imaging, autonomous driving, etc. Deep learning models rely on the abundance of labelled observations to train a prospective model. These models are composed of millions of parameters to estimate, increasing the need of more training observations. Frequently it is expensive to gather labelled observations of data, making the usage of deep learning models not ideal, as the model might over-fit data. In a semi-supervised setting, unlabelled data is used to improve the levels of accuracy and generalization of a model with small labelled datasets. Nevertheless, in many situations different unlabelled data sources might be available. This raises the risk of a significant distribution mismatch between the labelled and unlabelled datasets. Such phenomena can cause a considerable performance hit to typical semi-supervised deep learning frameworks, which often assume that both labelled and unlabelled datasets are drawn from similar distributions. Therefore, in this paper we study the latest approaches for semi-supervised deep learning for image recognition. Emphasis is made in semi-supervised deep learning models designed to deal with a distribution mismatch between the labelled and unlabelled datasets. We address open challenges with the aim to encourage the community to tackle them, and overcome the high data demand of traditional deep learning pipelines under real-world usage settings.



### Understanding the Challenges When 3D Semantic Segmentation Faces Class Imbalanced and OOD Data
- **Arxiv ID**: http://arxiv.org/abs/2203.00214v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.00214v1)
- **Published**: 2022-03-01 03:53:18+00:00
- **Updated**: 2022-03-01 03:53:18+00:00
- **Authors**: Yancheng Pan, Fan Xie, Huijing Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: 3D semantic segmentation (3DSS) is an essential process in the creation of a safe autonomous driving system. However, deep learning models for 3D semantic segmentation often suffer from the class imbalance problem and out-of-distribution (OOD) data. In this study, we explore how the class imbalance problem affects 3DSS performance and whether the model can detect the category prediction correctness, or whether data is ID (in-distribution) or OOD. For these purposes, we conduct two experiments using three representative 3DSS models and five trust scoring methods, and conduct both a confusion and feature analysis of each class. Furthermore, a data augmentation method for the 3D LiDAR dataset is proposed to create a new dataset based on SemanticKITTI and SemanticPOSS, called AugKITTI. We propose the wPre metric and TSD for a more in-depth analysis of the results, and follow are proposals with an insightful discussion. Based on the experimental results, we find that: (1) the classes are not only imbalanced in their data size but also in the basic properties of each semantic category. (2) The intraclass diversity and interclass ambiguity make class learning difficult and greatly limit the models' performance, creating the challenges of semantic and data gaps. (3) The trust scores are unreliable for classes whose features are confused with other classes. For 3DSS models, those misclassified ID classes and OODs may also be given high trust scores, making the 3DSS predictions unreliable, and leading to the challenges in judging 3DSS result trustworthiness. All of these outcomes point to several research directions for improving the performance and reliability of the 3DSS models used for real-world applications.



### Uncertainty categories in medical image segmentation: a study of source-related diversity
- **Arxiv ID**: http://arxiv.org/abs/2203.00238v2
- **DOI**: 10.1007/978-3-031-16749-2_3
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00238v2)
- **Published**: 2022-03-01 05:25:02+00:00
- **Updated**: 2022-09-17 03:27:40+00:00
- **Authors**: Luke Whitbread, Mark Jenkinson
- **Comment**: None
- **Journal**: Uncertainty for Safe Utilization of Machine Learning in Medical
  Imaging. UNSURE 2022. Lecture Notes in Computer Science, vol 13563. Springer,
  Cham
- **Summary**: Measuring uncertainties in the output of a deep learning method is useful in several ways, such as in assisting with interpretation of the outputs, helping build confidence with end users, and for improving the training and performance of the networks. Several different methods have been proposed to estimate uncertainties, including those from epistemic (relating to the model used) and aleatoric (relating to the data) sources using test-time dropout and augmentation, respectively. Not only are these uncertainty sources different, but they are governed by parameter settings (e.g., dropout rate or type and level of augmentation) that establish even more distinct uncertainty categories. This work investigates how different the uncertainties are from these categories, for magnitude and spatial pattern, to empirically address the question of whether they provide usefully distinct information that should be captured whenever uncertainties are used. We take the well characterised BraTS challenge dataset to demonstrate that there are substantial differences in both magnitude and spatial pattern of uncertainties from the different categories, and discuss the implications of these in various use cases.



### Unsupervised Vision-and-Language Pre-training via Retrieval-based Multi-Granular Alignment
- **Arxiv ID**: http://arxiv.org/abs/2203.00242v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2203.00242v1)
- **Published**: 2022-03-01 05:34:01+00:00
- **Updated**: 2022-03-01 05:34:01+00:00
- **Authors**: Mingyang Zhou, Licheng Yu, Amanpreet Singh, Mengjiao Wang, Zhou Yu, Ning Zhang
- **Comment**: First two authors contributed equally. 13 pages, 11 figures
- **Journal**: None
- **Summary**: Vision-and-Language (V+L) pre-training models have achieved tremendous success in recent years on various multi-modal benchmarks. However, the majority of existing models require pre-training on a large set of parallel image-text data, which is costly to collect, compared to image-only or text-only data. In this paper, we explore unsupervised Vision-and-Language pre-training (UVLP) to learn the cross-modal representation from non-parallel image and text datasets. We found two key factors that lead to good unsupervised V+L pre-training without parallel data: (i) joint image-and-text input (ii) overall image-text alignment (even for non-parallel data). Accordingly, we propose a novel unsupervised V+L pre-training curriculum for non-parallel texts and images. We first construct a weakly aligned image-text corpus via a retrieval-based approach, then apply a set of multi-granular alignment pre-training tasks, including region-to-tag, region-to-phrase, and image-to-sentence alignment, to bridge the gap between the two modalities. A comprehensive ablation study shows each granularity is helpful to learn a stronger pre-trained model. We adapt our pre-trained model to a set of V+L downstream tasks, including VQA, NLVR2, Visual Entailment, and RefCOCO+. Our model achieves the state-of-art performance in all these tasks under the unsupervised setting.



### When A Conventional Filter Meets Deep Learning: Basis Composition Learning on Image Filters
- **Arxiv ID**: http://arxiv.org/abs/2203.00258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00258v1)
- **Published**: 2022-03-01 06:34:54+00:00
- **Updated**: 2022-03-01 06:34:54+00:00
- **Authors**: Fu Lee Wang, Yidan Feng, Haoran Xie, Gary Cheng, Mingqiang Wei
- **Comment**: 14 pages, 10 figures
- **Journal**: None
- **Summary**: Image filters are fast, lightweight and effective, which make these conventional wisdoms preferable as basic tools in vision tasks. In practical scenarios, users have to tweak parameters multiple times to obtain satisfied results. This inconvenience heavily discounts the efficiency and user experience. We propose basis composition learning on single image filters to automatically determine their optimal formulas. The feasibility is based on a two-step strategy: first, we build a set of filtered basis (FB) consisting of approximations under selected parameter configurations; second, a dual-branch composition module is proposed to learn how the candidates in FB are combined to better approximate the target image. Our method is simple yet effective in practice; it renders filters to be user-friendly and benefits fundamental low-level vision problems including denoising, deraining and texture removal. Extensive experiments demonstrate that our method achieves an appropriate balance among the performance, time complexity and memory efficiency.



### Omni-frequency Channel-selection Representations for Unsupervised Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.00259v2
- **DOI**: 10.1109/TIP.2023.3293772
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00259v2)
- **Published**: 2022-03-01 06:35:15+00:00
- **Updated**: 2023-07-03 09:54:11+00:00
- **Authors**: Yufei Liang, Jiangning Zhang, Shiwei Zhao, Runze Wu, Yong Liu, Shuwen Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Density-based and classification-based methods have ruled unsupervised anomaly detection in recent years, while reconstruction-based methods are rarely mentioned for the poor reconstruction ability and low performance. However, the latter requires no costly extra training samples for the unsupervised training that is more practical, so this paper focuses on improving this kind of method and proposes a novel Omni-frequency Channel-selection Reconstruction (OCR-GAN) network to handle anomaly detection task in a perspective of frequency. Concretely, we propose a Frequency Decoupling (FD) module to decouple the input image into different frequency components and model the reconstruction process as a combination of parallel omni-frequency image restorations, as we observe a significant difference in the frequency distribution of normal and abnormal images. Given the correlation among multiple frequencies, we further propose a Channel Selection (CS) module that performs frequency interaction among different encoders by adaptively selecting different channels. Abundant experiments demonstrate the effectiveness and superiority of our approach over different kinds of methods, e.g., achieving a new state-of-the-art 98.3 detection AUC on the MVTec AD dataset without extra training data that markedly surpasses the reconstruction-based baseline by +38.1 and the current SOTA method by +0.3. Source code is available at https://github.com/zhangzjn/OCR-GAN.



### Separable-HoverNet and Instance-YOLO for Colon Nuclei Identification and Counting
- **Arxiv ID**: http://arxiv.org/abs/2203.00262v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00262v1)
- **Published**: 2022-03-01 06:48:23+00:00
- **Updated**: 2022-03-01 06:48:23+00:00
- **Authors**: Chunhui Lin, Liukun Zhang, Lijian Mao, Min Wu, Dong Hu
- **Comment**: arXiv admin note: text overlap with arXiv:2111.14485 by other authors
- **Journal**: None
- **Summary**: Nuclear segmentation, classification and quantification within Haematoxylin & Eosin stained histology images enables the extraction of interpretable cell-based features that can be used in downstream explainable models in computational pathology (CPath). However, automatic recognition of different nuclei is faced with a major challenge in that there are several different types of nuclei, some of them exhibiting large intraclass variability. In this work, we propose an approach that combine Separable-HoverNet and Instance-YOLOv5 to indentify colon nuclei small and unbalanced. Our approach can achieve mPQ+ 0.389 on the Segmentation and Classification-Preliminary Test Dataset and r2 0.599 on the Cellular Composition-Preliminary Test Dataset on ISBI 2022 CoNIC Challenge.



### ProgressLabeller: Visual Data Stream Annotation for Training Object-Centric 3D Perception
- **Arxiv ID**: http://arxiv.org/abs/2203.00283v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00283v2)
- **Published**: 2022-03-01 08:04:17+00:00
- **Updated**: 2022-08-01 20:09:03+00:00
- **Authors**: Xiaotong Chen, Huijie Zhang, Zeren Yu, Stanley Lewis, Odest Chadwicke Jenkins
- **Comment**: IROS 2022 accepted paper; project page:
  https://progress.eecs.umich.edu/projects/progress-labeller/
- **Journal**: None
- **Summary**: Visual perception tasks often require vast amounts of labelled data, including 3D poses and image space segmentation masks. The process of creating such training data sets can prove difficult or time-intensive to scale up to efficacy for general use. Consider the task of pose estimation for rigid objects. Deep neural network based approaches have shown good performance when trained on large, public datasets. However, adapting these networks for other novel objects, or fine-tuning existing models for different environments, requires significant time investment to generate newly labelled instances. Towards this end, we propose ProgressLabeller as a method for more efficiently generating large amounts of 6D pose training data from color images sequences for custom scenes in a scalable manner. ProgressLabeller is intended to also support transparent or translucent objects, for which the previous methods based on depth dense reconstruction will fail. We demonstrate the effectiveness of ProgressLabeller by rapidly create a dataset of over 1M samples with which we fine-tune a state-of-the-art pose estimation network in order to markedly improve the downstream robotic grasp success rates. ProgressLabeller is open-source at https://github.com/huijieZH/ProgressLabeller.



### Efficient Globally-Optimal Correspondence-Less Visual Odometry for Planar Ground Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2203.00291v1
- **DOI**: 10.1109/ICRA40945.2020.9196595
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00291v1)
- **Published**: 2022-03-01 08:49:21+00:00
- **Updated**: 2022-03-01 08:49:21+00:00
- **Authors**: Ling Gao, Junyan Su, Jiadi Cui, Xiangchen Zeng, Xin Peng, Laurent Kneip
- **Comment**: None
- **Journal**: IEEE International Conference on Robotics and Automation (ICRA),
  2020
- **Summary**: The motion of planar ground vehicles is often non-holonomic, and as a result may be modelled by the 2 DoF Ackermann steering model. We analyse the feasibility of estimating such motion with a downward facing camera that exerts fronto-parallel motion with respect to the ground plane. This turns the motion estimation into a simple image registration problem in which we only have to identify a 2-parameter planar homography. However, one difficulty that arises from this setup is that ground-plane features are indistinctive and thus hard to match between successive views. We encountered this difficulty by introducing the first globally-optimal, correspondence-less solution to plane-based Ackermann motion estimation. The solution relies on the branch-and-bound optimisation technique. Through the low-dimensional parametrisation, a derivation of tight bounds, and an efficient implementation, we demonstrate how this technique is eventually amenable to accurate real-time motion estimation. We prove its property of global optimality and analyse the impact of assuming a locally constant centre of rotation. Our results on real data finally demonstrate a significant advantage over the more traditional, correspondence-based hypothesise-and-test schemes.



### FP-Loc: Lightweight and Drift-free Floor Plan-assisted LiDAR Localization
- **Arxiv ID**: http://arxiv.org/abs/2203.00292v1
- **DOI**: 10.1109/ICRA46639.2022.9812361
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00292v1)
- **Published**: 2022-03-01 08:49:37+00:00
- **Updated**: 2022-03-01 08:49:37+00:00
- **Authors**: Ling Gao, Laurent Kneip
- **Comment**: None
- **Journal**: IEEE International Conference on Robotics and Automation (ICRA),
  2022
- **Summary**: We present a novel framework for floor plan-based, full six degree-of-freedom LiDAR localization. Our approach relies on robust ceiling and ground plane detection, which solves part of the pose and supports the segmentation of vertical structure elements such as walls and pillars. Our core contribution is a novel nearest neighbour data structure for an efficient look-up of nearest vertical structure elements from the floor plan. The registration is realized as a pair-wise regularized windowed pose graph optimization. Highly efficient, accurate and drift-free long-term localization is demonstrated on multiple scenes.



### Adversarial samples for deep monocular 6D object pose estimation
- **Arxiv ID**: http://arxiv.org/abs/2203.00302v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00302v2)
- **Published**: 2022-03-01 09:16:37+00:00
- **Updated**: 2022-03-05 01:37:57+00:00
- **Authors**: Jinlai Zhang, Weiming Li, Shuang Liang, Hao Wang, Jihong Zhu
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Estimating 6D object pose from an RGB image is important for many real-world applications such as autonomous driving and robotic grasping. Recent deep learning models have achieved significant progress on this task but their robustness received little research attention. In this work, for the first time, we study adversarial samples that can fool deep learning models with imperceptible perturbations to input image. In particular, we propose a Unified 6D pose estimation Attack, namely U6DA, which can successfully attack several state-of-the-art (SOTA) deep learning models for 6D pose estimation. The key idea of our U6DA is to fool the models to predict wrong results for object instance localization and shape that are essential for correct 6D pose estimation. Specifically, we explore a transfer-based black-box attack to 6D pose estimation. We design the U6DA loss to guide the generation of adversarial examples, the loss aims to shift the segmentation attention map away from its original position. We show that the generated adversarial samples are not only effective for direct 6D pose estimation models, but also are able to attack two-stage models regardless of their robust RANSAC modules. Extensive experiments were conducted to demonstrate the effectiveness, transferability, and anti-defense capability of our U6DA on large-scale public benchmarks. We also introduce a new U6DA-Linemod dataset for robustness study of the 6D pose estimation task. Our codes and dataset will be available at \url{https://github.com/cuge1995/U6DA}.



### Comprehensive Analysis of the Object Detection Pipeline on UAVs
- **Arxiv ID**: http://arxiv.org/abs/2203.00306v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00306v2)
- **Published**: 2022-03-01 09:30:01+00:00
- **Updated**: 2022-07-19 11:16:36+00:00
- **Authors**: Leon Amadeus Varga, Sebastian Koch, Andreas Zell
- **Comment**: Submitted WACV23
- **Journal**: None
- **Summary**: An object detection pipeline comprises a camera that captures the scene and an object detector that processes these images. The quality of the images directly affects the performance of the object detector. Many works nowadays focus either on improving the image quality or improving the object detection models independently, but neglect the importance of joint optimization of the two subsystems. The goal of this paper is to tune the detection throughput and accuracy of existing object detectors in the remote sensing scenario by focusing on optimizing the input images tailored to the object detector. To achieve this, we empirically analyze the influence of two selected camera calibration parameters (camera distortion correction and gamma correction) and five image parameters (quantization, compression, resolution, color model, additional channels) for these applications. For our experiments, we utilize three UAV data sets from different domains and a mixture of large and small state-of-the-art object detector models to provide an extensive evaluation of the influence of the pipeline parameters. Finally, we realize an object detection pipeline prototype on an embedded platform for an UAV and give a best practice recommendation for building object detection pipelines based on our findings. We show that not all parameters have an equal impact on detection accuracy and data throughput, and that by using a suitable compromise between parameters we are able to achieve higher detection accuracy for lightweight object detection models, while keeping the same data throughput.



### Temporal Perceiver: A General Architecture for Arbitrary Boundary Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.00307v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00307v2)
- **Published**: 2022-03-01 09:31:30+00:00
- **Updated**: 2022-10-05 08:27:53+00:00
- **Authors**: Jing Tan, Yuhong Wang, Gangshan Wu, Limin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Generic Boundary Detection (GBD) aims at locating the general boundaries that divide videos into semantically coherent and taxonomy-free units, and could serve as an important pre-processing step for long-form video understanding. Previous works often separately handle these different types of generic boundaries with specific designs of deep networks from simple CNN to LSTM. Instead, in this paper, we present Temporal Perceiver, a general architecture with Transformer, offering a unified solution to the detection of arbitrary generic boundaries, ranging from shot-level, event-level, to scene-level GBDs. The core design is to introduce a small set of latent feature queries as anchors to compress the redundant video input into a fixed dimension via cross-attention blocks. Thanks to this fixed number of latent units, it greatly reduces the quadratic complexity of attention operation to a linear form of input frames. Specifically, to explicitly leverage the temporal structure of videos, we construct two types of latent feature queries: boundary queries and context queries, which handle the semantic incoherence and coherence accordingly. Moreover, to guide the learning of latent feature queries, we propose an alignment loss on the cross-attention maps to explicitly encourage the boundary queries to attend on the top boundary candidates. Finally, we present a sparse detection head on the compressed representation, and directly output the final boundary detection results without any post-processing module. We test our Temporal Perceiver on a variety of GBD benchmarks. Our method obtains the state-of-the-art results on all benchmarks with RGB single-stream features: SoccerNet-v2 (81.9% avg-mAP), Kinetics-GEBD (86.0% avg-f1), TAPOS (73.2% avg-f1), MovieScenes (51.9% AP and 53.1% Miou) and MovieNet (53.3% AP and 53.2% Miou), demonstrating the generalization ability of our Temporal Perceiver.



### Towards IID representation learning and its application on biomedical data
- **Arxiv ID**: http://arxiv.org/abs/2203.00332v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00332v1)
- **Published**: 2022-03-01 10:15:14+00:00
- **Updated**: 2022-03-01 10:15:14+00:00
- **Authors**: Jiqing Wu, Inti Zlobec, Maxime Lafarge, Yukun He, Viktor H. Koelzer
- **Comment**: The paper is accepted by MIDL 2022--full paper track
- **Journal**: None
- **Summary**: Due to the heterogeneity of real-world data, the widely accepted independent and identically distributed (IID) assumption has been criticized in recent studies on causality. In this paper, we argue that instead of being a questionable assumption, IID is a fundamental task-relevant property that needs to be learned. Consider $k$ independent random vectors $\mathsf{X}^{i = 1, \ldots, k}$, we elaborate on how a variety of different causal questions can be reformulated to learning a task-relevant function $\phi$ that induces IID among $\mathsf{Z}^i := \phi \circ \mathsf{X}^i$, which we term IID representation learning.   For proof of concept, we examine the IID representation learning on Out-of-Distribution (OOD) generalization tasks. Concretely, by utilizing the representation obtained via the learned function that induces IID, we conduct prediction of molecular characteristics (molecular prediction) on two biomedical datasets with real-world distribution shifts introduced by a) preanalytical variation and b) sampling protocol. To enable reproducibility and for comparison to the state-of-the-art (SOTA) methods, this is done by following the OOD benchmarking guidelines recommended from WILDS. Compared to the SOTA baselines supported in WILDS, the results confirm the superior performance of IID representation learning on OOD tasks. The code is publicly accessible via https://github.com/CTPLab/IID_representation_learning.



### Affordance Learning from Play for Sample-Efficient Policy Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.00352v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.00352v1)
- **Published**: 2022-03-01 11:00:35+00:00
- **Updated**: 2022-03-01 11:00:35+00:00
- **Authors**: Jessica Borja-Diaz, Oier Mees, Gabriel Kalweit, Lukas Hermann, Joschka Boedecker, Wolfram Burgard
- **Comment**: Accepted at the 2022 IEEE International Conference on Robotics and
  Automation (ICRA). Videos at http://vapo.cs.uni-freiburg.de/
- **Journal**: None
- **Summary**: Robots operating in human-centered environments should have the ability to understand how objects function: what can be done with each object, where this interaction may occur, and how the object is used to achieve a goal. To this end, we propose a novel approach that extracts a self-supervised visual affordance model from human teleoperated play data and leverages it to enable efficient policy learning and motion planning. We combine model-based planning with model-free deep reinforcement learning (RL) to learn policies that favor the same object regions favored by people, while requiring minimal robot interactions with the environment. We evaluate our algorithm, Visual Affordance-guided Policy Optimization (VAPO), with both diverse simulation manipulation tasks and real world robot tidy-up experiments to demonstrate the effectiveness of our affordance-guided policies. We find that our policies train 4x faster than the baselines and generalize better to novel objects because our visual affordance model can anticipate their affordance regions.



### Tempera: Spatial Transformer Feature Pyramid Network for Cardiac MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.00355v1
- **DOI**: 10.1007/978-3-030-93722-5_29
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00355v1)
- **Published**: 2022-03-01 11:05:51+00:00
- **Updated**: 2022-03-01 11:05:51+00:00
- **Authors**: Christoforos Galazis, Huiyi Wu, Zhuoyu Li, Camille Petri, Anil A. Bharath, Marta Varela
- **Comment**: None
- **Journal**: Statistical Atlases and Computational Models of the Heart.
  Multi-Disease, Multi-View, and Multi-Center Right Ventricular Segmentation in
  Cardiac MRI Challenge. STACOM 2021. Lecture Notes in Computer Science, vol
  13131
- **Summary**: Assessing the structure and function of the right ventricle (RV) is important in the diagnosis of several cardiac pathologies. However, it remains more challenging to segment the RV than the left ventricle (LV). In this paper, we focus on segmenting the RV in both short (SA) and long-axis (LA) cardiac MR images simultaneously. For this task, we propose a new multi-input/output architecture, hybrid 2D/3D geometric spatial TransformEr Multi-Pass fEature pyRAmid (Tempera). Our feature pyramid extends current designs by allowing not only a multi-scale feature output but multi-scale SA and LA input images as well. Tempera transfers learned features between SA and LA images via layer weight sharing and incorporates a geometric target transformer to map the predicted SA segmentation to LA space. Our model achieves an average Dice score of 0.836 and 0.798 for the SA and LA, respectively, and 26.31 mm and 31.19 mm Hausdorff distances. This opens up the potential for the incorporation of RV segmentation models into clinical workflows.



### Deep Temporal Interpolation of Radar-based Precipitation
- **Arxiv ID**: http://arxiv.org/abs/2203.01277v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2.10; I.3.7; I.6.5; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2203.01277v1)
- **Published**: 2022-03-01 11:20:11+00:00
- **Updated**: 2022-03-01 11:20:11+00:00
- **Authors**: Michiaki Tatsubori, Takao Moriyama, Tatsuya Ishikawa, Paolo Fraccaro, Anne Jones, Blair Edwards, Julian Kuehnert, Sekou L. Remy
- **Comment**: 5 pagers, 4 figures, ICASSP-22. arXiv admin note: text overlap with
  arXiv:1712.00080 by other authors
- **Journal**: None
- **Summary**: When providing the boundary conditions for hydrological flood models and estimating the associated risk, interpolating precipitation at very high temporal resolutions (e.g. 5 minutes) is essential not to miss the cause of flooding in local regions. In this paper, we study optical flow-based interpolation of globally available weather radar images from satellites. The proposed approach uses deep neural networks for the interpolation of multiple video frames, while terrain information is combined with temporarily coarse-grained precipitation radar observation as inputs for self-supervised training. An experiment with the Meteonet radar precipitation dataset for the flood risk simulation in Aude, a department in Southern France (2018), demonstrated the advantage of the proposed method over a linear interpolation baseline, with up to 20% error reduction.



### Exploring Wilderness Characteristics Using Explainable Machine Learning in Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2203.00379v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00379v3)
- **Published**: 2022-03-01 11:51:49+00:00
- **Updated**: 2022-07-26 10:55:15+00:00
- **Authors**: Timo T. Stomberg, Taylor Stone, Johannes Leonhardt, Immanuel Weber, Ribana Roscher
- **Comment**: None
- **Journal**: None
- **Summary**: Wilderness areas offer important ecological and social benefits and there are urgent reasons to discover where their positive characteristics and ecological functions are present and able to flourish. We apply a novel explainable machine learning technique to satellite images which show wild and anthropogenic areas in Fennoscandia. Occluding certain activations in an interpretable artificial neural network we complete a comprehensive sensitivity analysis regarding wild and anthropogenic characteristics. This enables us to predict detailed and high-resolution sensitivity maps highlighting these characteristics. Our artificial neural network provides an interpretable activation space increasing confidence in our method. Within the activation space, regions are semantically arranged. Our approach advances explainable machine learning for remote sensing, offers opportunities for comprehensive analyses of existing wilderness, and has practical relevance for conservation efforts.



### CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP
- **Arxiv ID**: http://arxiv.org/abs/2203.00386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00386v1)
- **Published**: 2022-03-01 12:11:32+00:00
- **Updated**: 2022-03-01 12:11:32+00:00
- **Authors**: Zihao Wang, Wei Liu, Qian He, Xinglong Wu, Zili Yi
- **Comment**: None
- **Journal**: None
- **Summary**: Training a text-to-image generator in the general domain (e.g., Dall.e, CogView) requires huge amounts of paired text-image data, which is too expensive to collect. In this paper, we propose a self-supervised scheme named as CLIP-GEN for general text-to-image generation with the language-image priors extracted with a pre-trained CLIP model. In our approach, we only require a set of unlabeled images in the general domain to train a text-to-image generator. Specifically, given an image without text labels, we first extract the embedding of the image in the united language-vision embedding space with the image encoder of CLIP. Next, we convert the image into a sequence of discrete tokens in the VQGAN codebook space (the VQGAN model can be trained with the unlabeled image dataset in hand). Finally, we train an autoregressive transformer that maps the image tokens from its unified language-vision representation. Once trained, the transformer can generate coherent image tokens based on the text embedding extracted from the text encoder of CLIP upon an input text. Such a strategy enables us to train a strong and general text-to-image generator with large text-free image dataset such as ImageNet. Qualitative and quantitative evaluations verify that our method significantly outperforms optimization-based text-to-image methods in terms of image quality while not compromising the text-image matching. Our method can even achieve comparable performance as flagship supervised models like CogView.



### Motion-aware Dynamic Graph Neural Network for Video Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/2203.00387v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00387v1)
- **Published**: 2022-03-01 12:13:46+00:00
- **Updated**: 2022-03-01 12:13:46+00:00
- **Authors**: Ruiying Lu, Ziheng Cheng, Bo Chen, Xin Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Video snapshot compressive imaging (SCI) utilizes a 2D detector to capture sequential video frames and compresses them into a single measurement. Various reconstruction methods have been developed to recover the high-speed video frames from the snapshot measurement. However, most existing reconstruction methods are incapable of capturing long-range spatial and temporal dependencies, which are critical for video processing. In this paper, we propose a flexible and robust approach based on graph neural network (GNN) to efficiently model non-local interactions between pixels in space as well as time regardless of the distance. Specifically, we develop a motion-aware dynamic GNN for better video representation, i.e., represent each pixel as the aggregation of relative nodes under the guidance of frame-by-frame motions, which consists of motion-aware dynamic sampling, cross-scale node sampling and graph aggregation. Extensive results on both simulation and real data demonstrate both the effectiveness and efficiency of the proposed approach, and the visualization clearly illustrates the intrinsic dynamic sampling operations of our proposed model for boosting the video SCI reconstruction results. The code and models will be released to the public.



### Beam-Shape Effects and Noise Removal from THz Time-Domain Images in Reflection Geometry in the 0.25-6 THz Range
- **Arxiv ID**: http://arxiv.org/abs/2203.00417v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2203.00417v1)
- **Published**: 2022-03-01 13:15:50+00:00
- **Updated**: 2022-03-01 13:15:50+00:00
- **Authors**: Marina Ljubenovic, Alessia Artesani, Stefano Bonetti, Arianna Traviglia
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: The increasing need of restoring high-resolution Hyper-Spectral (HS) images is determining a growing reliance on Computer Vision-based processing to enhance the clarity of the image content. HS images can, in fact, suffer from degradation effects or artefacts caused by instrument limitations. This paper focuses on a procedure aimed at reducing the degradation effects, frequency-dependent blur and noise, in Terahertz Time-Domain Spectroscopy (THz-TDS) images in reflection geometry. It describes the application of a joint deblurring and denoising approach that had been previously proved to be effective for the restoration of THz-TDS images in transmission geometry, but that had never been tested in reflection modality. This mode is often the only one that can be effectively used in most cases, for example when analyzing objects that are either opaque in the THz range, or that cannot be displaced from their location (e.g., museums), such as those of cultural interest. Compared to transmission mode, reflection geometry introduces, however, further distortion to THz data, neglected in existing literature. In this work, we successfully implement image deblurring and denoising of both uniform-shape samples (a contemporary 1 Euro cent coin and an inlaid pendant) and samples with the uneven reliefs and corrosion products on the surface which make the analysis of the object particularly complex (an ancient Roman silver coin). The study demonstrates the ability of image processing to restore data in the 0.25 - 6 THz range, spanning over more than four octaves, and providing the foundation for future analytical approaches of cultural heritage using the far-infrared spectrum still not sufficiently investigated in literature.



### Boundary Corrected Multi-scale Fusion Network for Real-time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.00436v1
- **DOI**: 10.1109/ICIP46576.2022.9897907
- **Categories**: **cs.CV**, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2203.00436v1)
- **Published**: 2022-03-01 13:31:01+00:00
- **Updated**: 2022-03-01 13:31:01+00:00
- **Authors**: Tianjiao Jiang, Yi Jin, Tengfei Liang, Xu Wang, Yidong Li
- **Comment**: 5 pages, 3 figures
- **Journal**: 2022 IEEE International Conference on Image Processing (ICIP)
- **Summary**: Image semantic segmentation aims at the pixel-level classification of images, which has requirements for both accuracy and speed in practical application. Existing semantic segmentation methods mainly rely on the high-resolution input to achieve high accuracy and do not meet the requirements of inference time. Although some methods focus on high-speed scene parsing with lightweight architectures, they can not fully mine semantic features under low computation with relatively low performance. To realize the real-time and high-precision segmentation, we propose a new method named Boundary Corrected Multi-scale Fusion Network, which uses the designed Low-resolution Multi-scale Fusion Module to extract semantic information. Moreover, to deal with boundary errors caused by low-resolution feature map fusion, we further design an additional Boundary Corrected Loss to constrain overly smooth features. Extensive experiments show that our method achieves a state-of-the-art balance of accuracy and speed for the real-time semantic segmentation.



### Bridge the Gap between Supervised and Unsupervised Learning for Fine-Grained Classification
- **Arxiv ID**: http://arxiv.org/abs/2203.00441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00441v1)
- **Published**: 2022-03-01 13:33:00+00:00
- **Updated**: 2022-03-01 13:33:00+00:00
- **Authors**: Jiabao Wang, Yang Li, Xiu-Shen Wei, Hang Li, Zhuang Miao, Rui Zhang
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Unsupervised learning technology has caught up with or even surpassed supervised learning technology in general object classification (GOC) and person re-identification (re-ID). However, it is found that the unsupervised learning of fine-grained visual classification (FGVC) is more challenging than GOC and person re-ID. In order to bridge the gap between unsupervised and supervised learning for FGVC, we investigate the essential factors (including feature extraction, clustering, and contrastive learning) for the performance gap between supervised and unsupervised FGVC. Furthermore, we propose a simple, effective, and practical method, termed as UFCL, to alleviate the gap. Three key issues are concerned and improved: First, we introduce a robust and powerful backbone, ResNet50-IBN, which has an ability of domain adaptation when we transfer ImageNet pre-trained models to FGVC tasks. Next, we propose to introduce HDBSCAN instead of DBSCAN to do clustering, which can generate better clusters for adjacent categories with fewer hyper-parameters. Finally, we propose a weighted feature agent and its updating mechanism to do contrastive learning by using the pseudo labels with inevitable noise, which can improve the optimization process of learning the parameters of the network. The effectiveness of our UFCL is verified on CUB-200-2011, Oxford-Flowers, Oxford-Pets, Stanford-Dogs, Stanford-Cars and FGVC-Aircraft datasets. Under the unsupervised FGVC setting, we achieve state-of-the-art results, and analyze the key factors and the important parameters to provide a practical guidance.



### JOINED : Prior Guided Multi-task Learning for Joint Optic Disc/Cup Segmentation and Fovea Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.00461v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00461v1)
- **Published**: 2022-03-01 13:47:48+00:00
- **Updated**: 2022-03-01 13:47:48+00:00
- **Authors**: Huaqing He, Li Lin, Zhiyuan Cai, Xiaoying Tang
- **Comment**: 16 pages, 3 figures, Published in Medical Imaging with Deep Learning
  (MIDL) 2022
- **Journal**: None
- **Summary**: Fundus photography has been routinely used to document the presence and severity of various retinal degenerative diseases such as age-related macula degeneration, glaucoma, and diabetic retinopathy, for which the fovea, optic disc (OD), and optic cup (OC) are important anatomical landmarks. Identification of those anatomical landmarks is of great clinical importance. However, the presence of lesions, drusen, and other abnormalities during retinal degeneration severely complicates automatic landmark detection and segmentation. Most existing works treat the identification of each landmark as a single task and typically do not make use of any clinical prior information. In this paper, we present a novel method, named JOINED, for prior guided multi-task learning for joint OD/OC segmentation and fovea detection. An auxiliary branch for distance prediction, in addition to a segmentation branch and a detection branch, is constructed to effectively utilize the distance information from each image pixel to landmarks of interest. Our proposed JOINED pipeline consists of a coarse stage and a fine stage. At the coarse stage, we obtain the OD/OC coarse segmentation and the heatmap localization of fovea through a joint segmentation and detection module. Afterwards, we crop the regions of interest for subsequent fine processing and use predictions obtained at the coarse stage as additional information for better performance and faster convergence. Experimental results reveal that our proposed JOINED outperforms existing state-of-the-art approaches on the publicly-available GAMMA, PALM, and REFUGE datasets of fundus images. Furthermore, JOINED ranked the 5th on the OD/OC segmentation and fovea detection tasks in the GAMMA challenge hosted by the MICCAI2021 workshop OMIA8.



### Compliance Challenges in Forensic Image Analysis Under the Artificial Intelligence Act
- **Arxiv ID**: http://arxiv.org/abs/2203.00469v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2203.00469v1)
- **Published**: 2022-03-01 14:03:23+00:00
- **Updated**: 2022-03-01 14:03:23+00:00
- **Authors**: Benedikt Lorch, Nicole Scheler, Christian Riess
- **Comment**: None
- **Journal**: None
- **Summary**: In many applications of forensic image analysis, state-of-the-art results are nowadays achieved with machine learning methods. However, concerns about their reliability and opaqueness raise the question whether such methods can be used in criminal investigations. So far, this question of legal compliance has hardly been discussed, also because legal regulations for machine learning methods were not defined explicitly. To this end, the European Commission recently proposed the artificial intelligence (AI) act, a regulatory framework for the trustworthy use of AI. Under the draft AI act, high-risk AI systems for use in law enforcement are permitted but subject to compliance with mandatory requirements. In this paper, we review why the use of machine learning in forensic image analysis is classified as high-risk. We then summarize the mandatory requirements for high-risk AI systems and discuss these requirements in light of two forensic applications, license plate recognition and deep fake detection. The goal of this paper is to raise awareness of the upcoming legal requirements and to point out avenues for future research.



### Effect of Timing Error: A Case Study of Navigation Camera
- **Arxiv ID**: http://arxiv.org/abs/2203.01412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.01412v1)
- **Published**: 2022-03-01 14:42:07+00:00
- **Updated**: 2022-03-01 14:42:07+00:00
- **Authors**: Sandeep S. Kulkarni, Sanjay M. Joshi
- **Comment**: None
- **Journal**: None
- **Summary**: We focus on the problem of timing errors in navigation camera as a case study in a broader problem of the effect of a timing error in cyber-physical systems. These systems rely on the requirement that certain things happen at the same time or certain things happen periodically at some period $T$. However, as these systems get more complex, timing errors can occur between the components thereby violating the assumption about events being simultaneous (or periodic).   We consider the problem of a surgical navigation system where optical markers detected in the 2D pictures taken by two cameras are used to localize the markers in 3D space. A predefined array of such markers, known as a reference element, is used to navigate the corresponding CAD model of a surgical instrument on patient's images. The cameras rely on the assumption that the pictures from both cameras are taken exactly at the same time. If a timing error occurs then the instrument may have moved between the pictures. We find that, depending upon the location of the instrument, this can lead to a substantial error in the localization of the instrument. Specifically, we find that if the actual movement is $\delta$ then the observed movement may be as high as $5\delta$ in the operating range of the camera. Furthermore, we also identify potential issues that could affect the error in case there are changes to the camera system or to the operating range.



### Towards Creativity Characterization of Generative Models via Group-based Subset Scanning
- **Arxiv ID**: http://arxiv.org/abs/2203.00523v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00523v3)
- **Published**: 2022-03-01 15:07:14+00:00
- **Updated**: 2022-05-26 11:59:36+00:00
- **Authors**: Celia Cintas, Payel Das, Brian Quanz, Girmaw Abebe Tadesse, Skyler Speakman, Pin-Yu Chen
- **Comment**: Accepted to IJCAI 2022 - Creativity Track - Extended version from
  Synthetic Data Generation Workshop at ICLR'21 submission (arXiv:2104.00479).
  arXiv admin note: text overlap with arXiv:2105.12479
- **Journal**: None
- **Summary**: Deep generative models, such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), have been employed widely in computational creativity research. However, such models discourage out-of-distribution generation to avoid spurious sample generation, thereby limiting their creativity. Thus, incorporating research on human creativity into generative deep learning techniques presents an opportunity to make their outputs more compelling and human-like. As we see the emergence of generative models directed toward creativity research, a need for machine learning-based surrogate metrics to characterize creative output from these models is imperative. We propose group-based subset scanning to identify, quantify, and characterize creative processes by detecting a subset of anomalous node-activations in the hidden layers of the generative models. Our experiments on the standard image benchmarks, and their "creatively generated" variants, reveal that the proposed subset scores distribution is more useful for detecting creative processes in the activation space rather than the pixel space. Further, we found that creative samples generate larger subsets of anomalies than normal or non-creative samples across datasets. The node activations highlighted during the creative decoding process are different from those responsible for the normal sample generation. Lastly, we assess if the images from the subsets selected by our method were also found creative by human evaluators, presenting a link between creativity perception in humans and node activations within deep neural nets.



### Towards deep learning-powered IVF: A large public benchmark for morphokinetic parameter prediction
- **Arxiv ID**: http://arxiv.org/abs/2203.00531v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00531v2)
- **Published**: 2022-03-01 15:13:21+00:00
- **Updated**: 2022-05-13 16:25:53+00:00
- **Authors**: Tristan Gomez, Magalie Feyeux, Nicolas Normand, Laurent David, Perrine Paul-Gilloteaux, Thomas Fr√©our, Harold Mouch√®re
- **Comment**: None
- **Journal**: None
- **Summary**: An important limitation to the development of Artificial Intelligence (AI)-based solutions for In Vitro Fertilization (IVF) is the absence of a public reference benchmark to train and evaluate deep learning (DL) models. In this work, we describe a fully annotated dataset of 704 videos of developing embryos, for a total of 337k images. We applied ResNet, LSTM, and ResNet-3D architectures to our dataset and demonstrate that they overperform algorithmic approaches to automatically annotate stage development phases. Altogether, we propose the first public benchmark that will allow the community to evaluate morphokinetic models. This is the first step towards deep learning-powered IVF. Of note, we propose highly detailed annotations with 16 different development phases, including early cell division phases, but also late cell divisions, phases after morulation, and very early phases, which have never been used before. We postulate that this original approach will help improve the overall performance of deep learning approaches on time-lapse videos of embryo development, ultimately benefiting infertile patients with improved clinical success rates (Code and data are available at https://gitlab.univ-nantes.fr/E144069X/bench_mk_pred.git).



### Descriptellation: Deep Learned Constellation Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2203.00567v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00567v2)
- **Published**: 2022-03-01 15:43:01+00:00
- **Updated**: 2022-09-14 20:41:37+00:00
- **Authors**: Chunwei Xing, Xinyu Sun, Andrei Cramariuc, Samuel Gull, Jen Jen Chung, Cesar Cadena, Roland Siegwart, Florian Tschopp
- **Comment**: None
- **Journal**: None
- **Summary**: Current descriptors for global localization often struggle under vast viewpoint or appearance changes. One possible improvement is the addition of topological information on semantic objects. However, handcrafted topological descriptors are hard to tune and not robust to environmental noise, drastic perspective changes, object occlusion or misdetections. To solve this problem, we formulate a learning-based approach by modelling semantically meaningful object constellations as graphs and using Deep Graph Convolution Networks to map a constellation to a descriptor. We demonstrate the effectiveness of our Deep Learned Constellation Descriptor (Descriptellation) on two real-world datasets. Although Descriptellation is trained on randomly generated simulation datasets, it shows good generalization abilities on real-world datasets. Descriptellation also outperforms state-of-the-art and handcrafted constellation descriptors for global localization, and is robust to different types of noise. The code is publicly available at https://github.com/ethz-asl/Descriptellation.



### Towards a unified view of unsupervised non-local methods for image denoising: the NL-Ridge approach
- **Arxiv ID**: http://arxiv.org/abs/2203.00570v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00570v1)
- **Published**: 2022-03-01 15:45:50+00:00
- **Updated**: 2022-03-01 15:45:50+00:00
- **Authors**: S√©bastien Herbreteau, Charles Kervrann
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a unified view of unsupervised non-local methods for image denoising that linearily combine noisy image patches. The best methods, established in different modeling and estimation frameworks, are two-step algorithms. Leveraging Stein's unbiased risk estimate (SURE) for the first step and the "internal adaptation", a concept borrowed from deep learning theory, for the second one, we show that our NL-Ridge approach enables to reconcile several patch aggregation methods for image denoising. In the second step, our closed-form aggregation weights are computed through multivariate Ridge regressions. Experiments on artificially noisy images demonstrate that NL-Ridge may outperform well established state-of-the-art unsupervised denoisers such as BM3D and NL-Bayes, as well as recent unsupervised deep learning methods, while being simpler conceptually.



### Self-Supervised Vision Transformers Learn Visual Concepts in Histopathology
- **Arxiv ID**: http://arxiv.org/abs/2203.00585v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2203.00585v1)
- **Published**: 2022-03-01 16:14:41+00:00
- **Updated**: 2022-03-01 16:14:41+00:00
- **Authors**: Richard J. Chen, Rahul G. Krishnan
- **Comment**: Learning Meaningful Representations of Life (NeurIPS 2021)
- **Journal**: None
- **Summary**: Tissue phenotyping is a fundamental task in learning objective characterizations of histopathologic biomarkers within the tumor-immune microenvironment in cancer pathology. However, whole-slide imaging (WSI) is a complex computer vision in which: 1) WSIs have enormous image resolutions with precludes large-scale pixel-level efforts in data curation, and 2) diversity of morphological phenotypes results in inter- and intra-observer variability in tissue labeling. To address these limitations, current efforts have proposed using pretrained image encoders (transfer learning from ImageNet, self-supervised pretraining) in extracting morphological features from pathology, but have not been extensively validated. In this work, we conduct a search for good representations in pathology by training a variety of self-supervised models with validation on a variety of weakly-supervised and patch-level tasks. Our key finding is in discovering that Vision Transformers using DINO-based knowledge distillation are able to learn data-efficient and interpretable features in histology images wherein the different attention heads learn distinct morphological phenotypes. We make evaluation code and pretrained weights publicly-available at: https://github.com/Richarizardd/Self-Supervised-ViT-Path.



### SwitchHit: A Probabilistic, Complementarity-Based Switching System for Improved Visual Place Recognition in Changing Environments
- **Arxiv ID**: http://arxiv.org/abs/2203.00591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00591v1)
- **Published**: 2022-03-01 16:23:22+00:00
- **Updated**: 2022-03-01 16:23:22+00:00
- **Authors**: Maria Waheed, Michael Milford, Klaus McDonald-Maier, Shoaib Ehsan
- **Comment**: None
- **Journal**: None
- **Summary**: Visual place recognition (VPR), a fundamental task in computer vision and robotics, is the problem of identifying a place mainly based on visual information. Viewpoint and appearance changes, such as due to weather and seasonal variations, make this task challenging. Currently, there is no universal VPR technique that can work in all types of environments, on a variety of robotic platforms, and under a wide range of viewpoint and appearance changes. Recent work has shown the potential of combining different VPR methods intelligently by evaluating complementarity for some specific VPR datasets to achieve better performance. This, however, requires ground truth information (correct matches) which is not available when a robot is deployed in a real-world scenario. Moreover, running multiple VPR techniques in parallel may be prohibitive for resource-constrained embedded platforms. To overcome these limitations, this paper presents a probabilistic complementarity based switching VPR system, SwitchHit. Our proposed system consists of multiple VPR techniques, however, it does not simply run all techniques at once, rather predicts the probability of correct match for an incoming query image and dynamically switches to another complementary technique if the probability of correctly matching the query is below a certain threshold. This innovative use of multiple VPR techniques allow our system to be more efficient and robust than other combined VPR approaches employing brute force and running multiple VPR techniques at once. Thus making it more suitable for resource constrained embedded systems and achieving an overall superior performance from what any individual VPR method in the system could have by achieved running independently.



### A unified 3D framework for Organs at Risk Localization and Segmentation for Radiation Therapy Planning
- **Arxiv ID**: http://arxiv.org/abs/2203.00624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00624v1)
- **Published**: 2022-03-01 17:08:41+00:00
- **Updated**: 2022-03-01 17:08:41+00:00
- **Authors**: Fernando Navarro, Guido Sasahara, Suprosanna Shit, Ivan Ezhov, Jan C. Peeken, Stephanie E. Combs, Bjoern H. Menze
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic localization and segmentation of organs-at-risk (OAR) in CT are essential pre-processing steps in medical image analysis tasks, such as radiation therapy planning. For instance, the segmentation of OAR surrounding tumors enables the maximization of radiation to the tumor area without compromising the healthy tissues. However, the current medical workflow requires manual delineation of OAR, which is prone to errors and is annotator-dependent. In this work, we aim to introduce a unified 3D pipeline for OAR localization-segmentation rather than novel localization or segmentation architectures. To the best of our knowledge, our proposed framework fully enables the exploitation of 3D context information inherent in medical imaging. In the first step, a 3D multi-variate regression network predicts organs' centroids and bounding boxes. Secondly, 3D organ-specific segmentation networks are leveraged to generate a multi-organ segmentation map. Our method achieved an overall Dice score of $0.9260\pm 0.18 \%$ on the VISCERAL dataset containing CT scans with varying fields of view and multiple organs.



### Full RGB Just Noticeable Difference (JND) Modelling
- **Arxiv ID**: http://arxiv.org/abs/2203.00629v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00629v1)
- **Published**: 2022-03-01 17:16:57+00:00
- **Updated**: 2022-03-01 17:16:57+00:00
- **Authors**: Jian Jin, Dong Yu, Weisi Lin, Lili Meng, Hao Wang, Huaxiang Zhang
- **Comment**: 13 pages, 8 figures, 8 tables
- **Journal**: None
- **Summary**: Just Noticeable Difference (JND) has many applications in multimedia signal processing, especially for visual data processing up to date. It's generally defined as the minimum visual content changes that the human can perspective, which has been studied for decades. However, most of the existing methods only focus on the luminance component of JND modelling and simply regard chrominance components as scaled versions of luminance. In this paper, we propose a JND model to generate the JND by taking the characteristics of full RGB channels into account, termed as the RGB-JND. To this end, an RGB-JND-NET is proposed, where the visual content in full RGB channels is used to extract features for JND generation. To supervise the JND generation, an adaptive image quality assessment combination (AIC) is developed. Besides, the RDB-JND-NET also takes the visual attention into account by automatically mining the underlying relationship between visual attention and the JND, which is further used to constrain the JND spatial distribution. To the best of our knowledge, this is the first work on careful investigation of JND modelling for full-color space. Experimental results demonstrate that the RGB-JND-NET model outperforms the relevant state-of-the-art JND models. Besides, the JND of the red and blue channels are larger than that of the green one according to the experimental results of the proposed model, which demonstrates that more changes can be tolerated in the red and blue channels, in line with the well-known fact that the human visual system is more sensitive to the green channel in comparison with the red and blue ones.



### Multi-Task Multi-Scale Learning For Outcome Prediction in 3D PET Images
- **Arxiv ID**: http://arxiv.org/abs/2203.00641v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.00641v1)
- **Published**: 2022-03-01 17:30:28+00:00
- **Updated**: 2022-03-01 17:30:28+00:00
- **Authors**: Amine Amyar, Romain Modzelewski, Pierre Vera, Vincent Morard, Su Ruan
- **Comment**: None
- **Journal**: None
- **Summary**: Background and Objectives: Predicting patient response to treatment and survival in oncology is a prominent way towards precision medicine. To that end, radiomics was proposed as a field of study where images are used instead of invasive methods. The first step in radiomic analysis is the segmentation of the lesion. However, this task is time consuming and can be physician subjective. Automated tools based on supervised deep learning have made great progress to assist physicians. However, they are data hungry, and annotated data remains a major issue in the medical field where only a small subset of annotated images is available. Methods: In this work, we propose a multi-task learning framework to predict patient's survival and response. We show that the encoder can leverage multiple tasks to extract meaningful and powerful features that improve radiomics performance. We show also that subsidiary tasks serve as an inductive bias so that the model can better generalize. Results: Our model was tested and validated for treatment response and survival in lung and esophageal cancers, with an area under the ROC curve of 77% and 71% respectively, outperforming single task learning methods. Conclusions: We show that, by using a multi-task learning approach, we can boost the performance of radiomic analysis by extracting rich information of intratumoral and peritumoral regions.



### Variational Autoencoders Without the Variation
- **Arxiv ID**: http://arxiv.org/abs/2203.00645v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2203.00645v1)
- **Published**: 2022-03-01 17:39:02+00:00
- **Updated**: 2022-03-01 17:39:02+00:00
- **Authors**: Gregory A. Daly, Jonathan E. Fieldsend, Gavin Tabor
- **Comment**: 11 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: Variational autoencdoers (VAE) are a popular approach to generative modelling. However, exploiting the capabilities of VAEs in practice can be difficult. Recent work on regularised and entropic autoencoders have begun to explore the potential, for generative modelling, of removing the variational approach and returning to the classic deterministic autoencoder (DAE) with additional novel regularisation methods. In this paper we empirically explore the capability of DAEs for image generation without additional novel methods and the effect of the implicit regularisation and smoothness of large networks. We find that DAEs can be used successfully for image generation without additional loss terms, and that many of the useful properties of VAEs can arise implicitly from sufficiently large convolutional encoders and decoders when trained on CIFAR-10 and CelebA.



### Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2203.00667v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.00667v1)
- **Published**: 2022-03-01 18:37:48+00:00
- **Updated**: 2022-03-01 18:37:48+00:00
- **Authors**: Gilad Cohen, Raja Giryes
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are very popular frameworks for generating high-quality data, and are immensely used in both the academia and industry in many domains. Arguably, their most substantial impact has been in the area of computer vision, where they achieve state-of-the-art image generation. This chapter gives an introduction to GANs, by discussing their principle mechanism and presenting some of their inherent problems during training and evaluation. We focus on these three issues: (1) mode collapse, (2) vanishing gradients, and (3) generation of low-quality images. We then list some architecture-variant and loss-variant GANs that remedy the above challenges. Lastly, we present two utilization examples of GANs for real-world applications: Data augmentation and face images generation.



### Generalizable Person Re-Identification via Self-Supervised Batch Norm Test-Time Adaption
- **Arxiv ID**: http://arxiv.org/abs/2203.00672v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00672v2)
- **Published**: 2022-03-01 18:46:32+00:00
- **Updated**: 2022-03-28 14:04:56+00:00
- **Authors**: Ke Han, Chenyang Si, Yan Huang, Liang Wang, Tieniu Tan
- **Comment**: accepted by AAAI 2022
- **Journal**: None
- **Summary**: In this paper, we investigate the generalization problem of person re-identification (re-id), whose major challenge is the distribution shift on an unseen domain. As an important tool of regularizing the distribution, batch normalization (BN) has been widely used in existing methods. However, they neglect that BN is severely biased to the training domain and inevitably suffers the performance drop if directly generalized without being updated. To tackle this issue, we propose Batch Norm Test-time Adaption (BNTA), a novel re-id framework that applies the self-supervised strategy to update BN parameters adaptively. Specifically, BNTA quickly explores the domain-aware information within unlabeled target data before inference, and accordingly modulates the feature distribution normalized by BN to adapt to the target domain. This is accomplished by two designed self-supervised auxiliary tasks, namely part positioning and part nearest neighbor matching, which help the model mine the domain-aware information with respect to the structure and identity of body parts, respectively. To demonstrate the effectiveness of our method, we conduct extensive experiments on three re-id datasets and confirm the superior performance to the state-of-the-art methods.



### CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding
- **Arxiv ID**: http://arxiv.org/abs/2203.00680v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00680v3)
- **Published**: 2022-03-01 18:59:01+00:00
- **Updated**: 2022-03-24 07:59:50+00:00
- **Authors**: Mohamed Afham, Isuru Dissanayake, Dinithi Dissanayake, Amaya Dharmasiri, Kanchana Thilakarathna, Ranga Rodrigo
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Manual annotation of large-scale point cloud dataset for varying tasks such as 3D object classification, segmentation and detection is often laborious owing to the irregular structure of point clouds. Self-supervised learning, which operates without any human labeling, is a promising approach to address this issue. We observe in the real world that humans are capable of mapping the visual concepts learnt from 2D images to understand the 3D world. Encouraged by this insight, we propose CrossPoint, a simple cross-modal contrastive learning approach to learn transferable 3D point cloud representations. It enables a 3D-2D correspondence of objects by maximizing agreement between point clouds and the corresponding rendered 2D image in the invariant space, while encouraging invariance to transformations in the point cloud modality. Our joint training objective combines the feature correspondences within and across modalities, thus ensembles a rich learning signal from both 3D point cloud and 2D image modalities in a self-supervised fashion. Experimental results show that our approach outperforms the previous unsupervised learning methods on a diverse range of downstream tasks including 3D object classification and segmentation. Further, the ablation studies validate the potency of our approach for a better point cloud understanding. Code and pretrained models are available at http://github.com/MohamedAfham/CrossPoint.



### Colon Nuclei Instance Segmentation using a Probabilistic Two-Stage Detector
- **Arxiv ID**: http://arxiv.org/abs/2203.01321v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.01321v1)
- **Published**: 2022-03-01 20:42:38+00:00
- **Updated**: 2022-03-01 20:42:38+00:00
- **Authors**: Pedro Costa, Yongpan Fu, Jo√£o Nunes, Aur√©lio Campilho, Jaime S. Cardoso
- **Comment**: None
- **Journal**: None
- **Summary**: Cancer is one of the leading causes of death in the developed world. Cancer diagnosis is performed through the microscopic analysis of a sample of suspicious tissue. This process is time consuming and error prone, but Deep Learning models could be helpful for pathologists during cancer diagnosis. We propose to change the CenterNet2 object detection model to also perform instance segmentation, which we call SegCenterNet2. We train SegCenterNet2 in the CoNIC challenge dataset and show that it performs better than Mask R-CNN in the competition metrics.



### Knock, knock. Who's there? -- Identifying football player jersey numbers with synthetic data
- **Arxiv ID**: http://arxiv.org/abs/2203.00734v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.00734v2)
- **Published**: 2022-03-01 20:44:34+00:00
- **Updated**: 2022-04-04 16:55:15+00:00
- **Authors**: Divya Bhargavi, Erika Pelaez Coyotl, Sia Gholami
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic player identification is an essential and complex task in sports video analysis. Different strategies have been devised over the years, but identification based on jersey numbers is one of the most common approaches given its versatility and relative simplicity. However, automatic detection of jersey numbers is still challenging due to changing camera angles, low video resolution, small object size in wide-range shots and transient changes in the player's posture and movement. In this paper we present a novel approach for jersey number identification in a small, highly imbalanced dataset from the Seattle Seahawks practice videos. Our results indicate that simple models can achieve an acceptable performance on the jersey number detection task and that synthetic data can improve the performance dramatically (accuracy increase of ~9% overall, ~18% on low frequency numbers) making our approach achieve state of the art results.



### 3D Skeleton-based Human Motion Prediction with Manifold-Aware GAN
- **Arxiv ID**: http://arxiv.org/abs/2203.00736v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00736v1)
- **Published**: 2022-03-01 20:49:13+00:00
- **Updated**: 2022-03-01 20:49:13+00:00
- **Authors**: Baptiste Chopin, Naima Otberdout, Mohamed Daoudi, Angela Bartolo
- **Comment**: paper submitted to IEEE transactions on biometrics, behavior, and
  identity science, an extension of IEEE FG paper arXiv:2105.08715
- **Journal**: None
- **Summary**: In this work we propose a novel solution for 3D skeleton-based human motion prediction. The objective of this task consists in forecasting future human poses based on a prior skeleton pose sequence. This involves solving two main challenges still present in recent literature; (1) discontinuity of the predicted motion which results in unrealistic motions and (2) performance deterioration in long-term horizons resulting from error accumulation across time. We tackle these issues by using a compact manifold-valued representation of 3D human skeleton motion. Specifically, we model the temporal evolution of the 3D poses as trajectory, what allows us to map human motions to single points on a sphere manifold. Using such a compact representation avoids error accumulation and provides robust representation for long-term prediction while ensuring the smoothness and the coherence of the whole motion. To learn these non-Euclidean representations, we build a manifold-aware Wasserstein generative adversarial model that captures the temporal and spatial dependencies of human motion through different losses. Experiments have been conducted on CMU MoCap and Human 3.6M datasets and demonstrate the superiority of our approach over the state-of-the-art both in short and long term horizons. The smoothness of the generated motion is highlighted in the qualitative results.



### Runtime Detection of Executional Errors in Robot-Assisted Surgery
- **Arxiv ID**: http://arxiv.org/abs/2203.00737v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, I.2.6; I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2203.00737v1)
- **Published**: 2022-03-01 20:49:27+00:00
- **Updated**: 2022-03-01 20:49:27+00:00
- **Authors**: Zongyu Li, Kay Hutchinson, Homa Alemzadeh
- **Comment**: 7 pages, 6 figures, accepted for 2022 International Conference on
  Robotics and Automation (ICRA)
- **Journal**: None
- **Summary**: Despite significant developments in the design of surgical robots and automated techniques for objective evaluation of surgical skills, there are still challenges in ensuring safety in robot-assisted minimally-invasive surgery (RMIS). This paper presents a runtime monitoring system for the detection of executional errors during surgical tasks through the analysis of kinematic data. The proposed system incorporates dual Siamese neural networks and knowledge of surgical context, including surgical tasks and gestures, their distributional similarities, and common error modes, to learn the differences between normal and erroneous surgical trajectories from small training datasets. We evaluate the performance of the error detection using Siamese networks compared to single CNN and LSTM networks trained with different levels of contextual knowledge and training data, using the dry-lab demonstrations of the Suturing and Needle Passing tasks from the JIGSAWS dataset. Our results show that gesture specific task nonspecific Siamese networks obtain micro F1 scores of 0.94 (Siamese-CNN) and 0.95 (Siamese-LSTM), and perform better than single CNN (0.86) and LSTM (0.87) networks. These Siamese networks also outperform gesture nonspecific task specific Siamese-CNN and Siamese-LSTM models for Suturing and Needle Passing.



### There is a Time and Place for Reasoning Beyond the Image
- **Arxiv ID**: http://arxiv.org/abs/2203.00758v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.00758v2)
- **Published**: 2022-03-01 21:52:08+00:00
- **Updated**: 2022-03-28 04:47:22+00:00
- **Authors**: Xingyu Fu, Ben Zhou, Ishaan Preetam Chandratreya, Carl Vondrick, Dan Roth
- **Comment**: Article accepted to the ACL 2022 Main conference
- **Journal**: None
- **Summary**: Images are often more significant than only the pixels to human eyes, as we can infer, associate, and reason with contextual information from other sources to establish a more complete picture. For example, in Figure 1, we can find a way to identify the news articles related to the picture through segment-wise understandings of the signs, the buildings, the crowds, and more. This reasoning could provide the time and place the image was taken, which will help us in subsequent tasks, such as automatic storyline construction, correction of image source in intended effect photographs, and upper-stream processing such as image clustering for certain location or time.   In this work, we formulate this problem and introduce TARA: a dataset with 16k images with their associated news, time, and location, automatically extracted from New York Times, and an additional 61k examples as distant supervision from WIT. On top of the extractions, we present a crowdsourced subset in which we believe it is possible to find the images' spatio-temporal information for evaluation purpose. We show that there exists a $70\%$ gap between a state-of-the-art joint model and human performance, which is slightly filled by our proposed model that uses segment-wise reasoning, motivating higher-level vision-language joint models that can conduct open-ended reasoning with world knowledge. The data and code are publicly available at https://github.com/zeyofu/TARA.



### Tricks and Plugins to GBM on Images and Sequences
- **Arxiv ID**: http://arxiv.org/abs/2203.00761v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00761v1)
- **Published**: 2022-03-01 21:59:00+00:00
- **Updated**: 2022-03-01 21:59:00+00:00
- **Authors**: Biyi Fang, Jean Utke, Diego Klabjan
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) and transformers, which are composed of multiple processing layers and blocks to learn the representations of data with multiple abstract levels, are the most successful machine learning models in recent years. However, millions of parameters and many blocks make them difficult to be trained, and sometimes several days or weeks are required to find an ideal architecture or tune the parameters. Within this paper, we propose a new algorithm for boosting Deep Convolutional Neural Networks (BoostCNN) to combine the merits of dynamic feature selection and BoostCNN, and another new family of algorithms combining boosting and transformers. To learn these new models, we introduce subgrid selection and importance sampling strategies and propose a set of algorithms to incorporate boosting weights into a deep learning architecture based on a least squares objective function. These algorithms not only reduce the required manual effort for finding an appropriate network architecture but also result in superior performance and lower running time. Experiments show that the proposed methods outperform benchmarks on several fine-grained classification tasks.



### Low-Cost On-device Partial Domain Adaptation (LoCO-PDA): Enabling efficient CNN retraining on edge devices
- **Arxiv ID**: http://arxiv.org/abs/2203.00772v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.00772v1)
- **Published**: 2022-03-01 22:26:47+00:00
- **Updated**: 2022-03-01 22:26:47+00:00
- **Authors**: Aditya Rajagopal, Christos-Savvas Bouganis
- **Comment**: None
- **Journal**: None
- **Summary**: With the increased deployment of Convolutional Neural Networks (CNNs) on edge devices, the uncertainty of the observed data distribution upon deployment has led researchers to to utilise large and extensive datasets such as ILSVRC'12 to train CNNs. Consequently, it is likely that the observed data distribution upon deployment is a subset of the training data distribution. In such cases, not adapting a network to the observed data distribution can cause performance degradation due to negative transfer and alleviating this is the focus of Partial Domain Adaptation (PDA). Current works targeting PDA do not focus on performing the domain adaptation on an edge device, adapting to a changing target distribution or reducing the cost of deploying the adapted network. This work proposes a novel PDA methodology that targets all of these directions and opens avenues for on-device PDA. LoCO-PDA adapts a deployed network to the observed data distribution by enabling it to be retrained on an edge device. Across subsets of the ILSVRC12 dataset, LoCO-PDA improves classification accuracy by 3.04pp on average while achieving up to 15.1x reduction in retraining memory consumption and 2.07x improvement in inference latency on the NVIDIA Jetson TX2. The work is open-sourced at \emph{link removed for anonymity}.



### Image analysis for automatic measurement of crustose lichens
- **Arxiv ID**: http://arxiv.org/abs/2203.00787v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.1; I.4.9; J.3; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2203.00787v1)
- **Published**: 2022-03-01 23:11:59+00:00
- **Updated**: 2022-03-01 23:11:59+00:00
- **Authors**: Pedro Guedes, Maria Alexandra Oliveira, Cristina Branquinho, Jo√£o Nuno Silva
- **Comment**: None
- **Journal**: None
- **Summary**: Lichens, organisms resulting from a symbiosis between a fungus and an algae, are frequently used as age estimators, especially in recent geological deposits and archaeological structures, using the correlation between lichen size and age. Current non-automated manual lichen and measurement (with ruler, calipers or using digital image processing tools) is a time-consuming and laborious process, especially when the number of samples is high.   This work presents a workflow and set of image acquisition and processing tools developed to efficiently identify lichen thalli in flat rocky surfaces, and to produce relevant lichen size statistics (percentage cover, number of thalli, their area and perimeter).   The developed workflow uses a regular digital camera for image capture along with specially designed targets to allow for automatic image correction and scale assignment. After this step, lichen identification is done in a flow comprising assisted image segmentation and classification based on interactive foreground extraction tool (GrabCut) and automatic classification of images using Simple Linear Iterative Clustering (SLIC) for image segmentation and Support Vector Machines (SV) and Random Forest classifiers.   Initial evaluation shows promising results. The manual classification of images (for training) using GrabCut show an average speedup of 4 if compared with currently used techniques and presents an average precision of 95\%. The automatic classification using SLIC and SVM with default parameters produces results with average precision higher than 70\%. The developed system is flexible and allows a considerable reduction of processing time, the workflow allows it applicability to data sets of new lichen populations.



### Unified Physical Threat Monitoring System Aided by Virtual Building Simulation
- **Arxiv ID**: http://arxiv.org/abs/2203.00789v1
- **DOI**: 10.1109/ICVISP54630.2021.00045
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00789v1)
- **Published**: 2022-03-01 23:28:46+00:00
- **Updated**: 2022-03-01 23:28:46+00:00
- **Authors**: Zenjie Li, Barry Norton
- **Comment**: None
- **Journal**: 2021 5th International Conference on Vision, Image and Signal
  Processing (ICVISP), 2021, pp. 206-211
- **Summary**: With increasing physical threats in recent years targeted at critical infrastructures, it is crucial to establish a reliable threat monitoring system integrating video surveillance and digital sensors based on cutting-edge technologies. A physical threat monitoring solution unifying the floorplan, cameras, and sensors for smart buildings has been set up in our study. Computer vision and deep learning models are used for video streams analysis. When a threat is detected by a rule engine based on the real-time analysis results combining with feedback from related digital sensors, an alert is sent to the Video Management System so that human operators can take further action. A physical threat monitoring system typically needs to address complex and even destructive incidents, such as fire, which is unrealistic to simulate in real life. Restrictions imposed during the Covid-19 pandemic and privacy concerns have added to the challenges. Our study utilises the Unreal Engine to simulate some typical suspicious and intrusion scenes with photorealistic qualities in the context of a virtual building. Add-on programs are implemented to transfer the video stream from virtual PTZ cameras to the Milestone Video Management System and enable users to control those cameras from the graphic client application. Virtual sensors such as fire alarms, temperature sensors and door access controls are implemented similarly, fulfilling the same programmatic VMS interface as real-life sensors. Thanks to this simulation system's extensibility and repeatability, we have consolidated this unified physical threat monitoring system and verified its effectiveness and user-friendliness. Both the simulated Unreal scenes and the software add-ons developed during this study are highly modulated and thereby are ready for reuse in future projects in this area.



