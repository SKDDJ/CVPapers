# Arxiv Papers in cs.CV on 2022-03-25
### Point2Seq: Detecting 3D Objects as Sequences
- **Arxiv ID**: http://arxiv.org/abs/2203.13394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13394v1)
- **Published**: 2022-03-25 00:20:31+00:00
- **Updated**: 2022-03-25 00:20:31+00:00
- **Authors**: Yujing Xue, Jiageng Mao, Minzhe Niu, Hang Xu, Michael Bi Mi, Wei Zhang, Xiaogang Wang, Xinchao Wang
- **Comment**: To appear in CVPR2022
- **Journal**: None
- **Summary**: We present a simple and effective framework, named Point2Seq, for 3D object detection from point clouds. In contrast to previous methods that normally {predict attributes of 3D objects all at once}, we expressively model the interdependencies between attributes of 3D objects, which in turn enables a better detection accuracy. Specifically, we view each 3D object as a sequence of words and reformulate the 3D object detection task as decoding words from 3D scenes in an auto-regressive manner. We further propose a lightweight scene-to-sequence decoder that can auto-regressively generate words conditioned on features from a 3D scene as well as cues from the preceding words. The predicted words eventually constitute a set of sequences that completely describe the 3D objects in the scene, and all the predicted sequences are then automatically assigned to the respective ground truths through similarity-based sequence matching. Our approach is conceptually intuitive and can be readily plugged upon most existing 3D-detection backbones without adding too much computational overhead; the sequential decoding paradigm we proposed, on the other hand, can better exploit information from complex 3D scenes with the aid of preceding predicted words. Without bells and whistles, our method significantly outperforms previous anchor- and center-based 3D object detection frameworks, yielding the new state of the art on the challenging ONCE dataset as well as the Waymo Open Dataset. Code is available at \url{https://github.com/ocNflag/point2seq}.



### Multi-scale and Cross-scale Contrastive Learning for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.13409v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13409v2)
- **Published**: 2022-03-25 01:24:24+00:00
- **Updated**: 2022-07-19 21:51:22+00:00
- **Authors**: Theodoros Pissas, Claudio S. Ravasio, Lyndon Da Cruz, Christos Bergeles
- **Comment**: to appear at ECCV 2022
- **Journal**: None
- **Summary**: This work considers supervised contrastive learning for semantic segmentation. We apply contrastive learning to enhance the discriminative power of the multi-scale features extracted by semantic segmentation networks. Our key methodological insight is to leverage samples from the feature spaces emanating from multiple stages of a model's encoder itself requiring neither data augmentation nor online memory banks to obtain a diverse set of samples. To allow for such an extension we introduce an efficient and effective sampling process, that enables applying contrastive losses over the encoder's features at multiple scales. Furthermore, by first mapping the encoder's multi-scale representations to a common feature space, we instantiate a novel form of supervised local-global constraint by introducing cross-scale contrastive learning linking high-resolution local features to low-resolution global features. Combined, our multi-scale and cross-scale contrastive losses boost performance of various models (DeepLabV3, HRNet, OCRNet, UPerNet) with both CNN and Transformer backbones, when evaluated on 4 diverse datasets from natural (Cityscapes, PascalContext, ADE20K) but also surgical (CaDIS) domains. Our code is available at https://github.com/RViMLab/MS_CS_ContrSeg. datasets from natural (Cityscapes, PascalContext, ADE20K) but also surgical (CaDIS) domains.



### Self-Supervised Predictive Learning: A Negative-Free Method for Sound Source Localization in Visual Scenes
- **Arxiv ID**: http://arxiv.org/abs/2203.13412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13412v1)
- **Published**: 2022-03-25 01:42:42+00:00
- **Updated**: 2022-03-25 01:42:42+00:00
- **Authors**: Zengjie Song, Yuxi Wang, Junsong Fan, Tieniu Tan, Zhaoxiang Zhang
- **Comment**: Camera-ready, CVPR 2022. Code: https://github.com/zjsong/SSPL
- **Journal**: None
- **Summary**: Sound source localization in visual scenes aims to localize objects emitting the sound in a given image. Recent works showing impressive localization performance typically rely on the contrastive learning framework. However, the random sampling of negatives, as commonly adopted in these methods, can result in misalignment between audio and visual features and thus inducing ambiguity in localization. In this paper, instead of following previous literature, we propose Self-Supervised Predictive Learning (SSPL), a negative-free method for sound localization via explicit positive mining. Specifically, we first devise a three-stream network to elegantly associate sound source with two augmented views of one corresponding video frame, leading to semantically coherent similarities between audio and visual features. Second, we introduce a novel predictive coding module for audio-visual feature alignment. Such a module assists SSPL to focus on target objects in a progressive manner and effectively lowers the positive-pair learning difficulty. Experiments show surprising results that SSPL outperforms the state-of-the-art approach on two standard sound localization benchmarks. In particular, SSPL achieves significant improvements of 8.6% cIoU and 3.4% AUC on SoundNet-Flickr compared to the previous best. Code is available at: https://github.com/zjsong/SSPL.



### Noisy Boundaries: Lemon or Lemonade for Semi-supervised Instance Segmentation?
- **Arxiv ID**: http://arxiv.org/abs/2203.13427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13427v1)
- **Published**: 2022-03-25 03:06:24+00:00
- **Updated**: 2022-03-25 03:06:24+00:00
- **Authors**: Zhenyu Wang, Yali Li, Shengjin Wang
- **Comment**: Accepted by CVPR2022
- **Journal**: None
- **Summary**: Current instance segmentation methods rely heavily on pixel-level annotated images. The huge cost to obtain such fully-annotated images restricts the dataset scale and limits the performance. In this paper, we formally address semi-supervised instance segmentation, where unlabeled images are employed to boost the performance. We construct a framework for semi-supervised instance segmentation by assigning pixel-level pseudo labels. Under this framework, we point out that noisy boundaries associated with pseudo labels are double-edged. We propose to exploit and resist them in a unified manner simultaneously: 1) To combat the negative effects of noisy boundaries, we propose a noise-tolerant mask head by leveraging low-resolution features. 2) To enhance the positive impacts, we introduce a boundary-preserving map for learning detailed information within boundary-relevant regions. We evaluate our approach by extensive experiments. It behaves extraordinarily, outperforming the supervised baseline by a large margin, more than 6% on Cityscapes, 7% on COCO and 4.5% on BDD100k. On Cityscapes, our method achieves comparable performance by utilizing only 30% labeled images.



### Frame-level Prediction of Facial Expressions, Valence, Arousal and Action Units for Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2203.13436v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T10, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2203.13436v2)
- **Published**: 2022-03-25 03:53:27+00:00
- **Updated**: 2022-05-24 08:00:17+00:00
- **Authors**: Andrey V. Savchenko
- **Comment**: accepted at CVPR Workshop ABAW3, 8 pages, 2 figures, 6 tables
- **Journal**: None
- **Summary**: In this paper, we consider the problem of real-time video-based facial emotion analytics, namely, facial expression recognition, prediction of valence and arousal and detection of action unit points. We propose the novel frame-level emotion recognition algorithm by extracting facial features with the single EfficientNet model pre-trained on AffectNet. As a result, our approach may be implemented even for video analytics on mobile devices. Experimental results for the large scale Aff-Wild2 database from the third Affective Behavior Analysis in-the-wild (ABAW) Competition demonstrate that our simple model is significantly better when compared to the VggFace baseline. In particular, our method is characterized by 0.15-0.2 higher performance measures for validation sets in uni-task Expression Classification, Valence-Arousal Estimation and Expression Classification. Due to simplicity, our approach may be considered as a new baseline for all four sub-challenges.



### BCOT: A Markerless High-Precision 3D Object Tracking Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2203.13437v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13437v1)
- **Published**: 2022-03-25 03:55:03+00:00
- **Updated**: 2022-03-25 03:55:03+00:00
- **Authors**: Jiachen Li, Bin Wang, Shiqiang Zhu, Xin Cao, Fan Zhong, Wenxuan Chen, Te Li, Jason Gu, Xueying Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Template-based 3D object tracking still lacks a high-precision benchmark of real scenes due to the difficulty of annotating the accurate 3D poses of real moving video objects without using markers. In this paper, we present a multi-view approach to estimate the accurate 3D poses of real moving objects, and then use binocular data to construct a new benchmark for monocular textureless 3D object tracking. The proposed method requires no markers, and the cameras only need to be synchronous, relatively fixed as cross-view and calibrated. Based on our object-centered model, we jointly optimize the object pose by minimizing shape re-projection constraints in all views, which greatly improves the accuracy compared with the single-view approach, and is even more accurate than the depth-based method. Our new benchmark dataset contains 20 textureless objects, 22 scenes, 404 video sequences and 126K images captured in real scenes. The annotation error is guaranteed to be less than 2mm, according to both theoretical analysis and validation experiments. We re-evaluate the state-of-the-art 3D object tracking methods with our dataset, reporting their performance ranking in real scenes. Our BCOT benchmark and code can be found at https://ar3dv.github.io/BCOT-Benchmark/.



### Microstructure Surface Reconstruction from SEM Images: An Alternative to Digital Image Correlation (DIC)
- **Arxiv ID**: http://arxiv.org/abs/2203.13438v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.13438v1)
- **Published**: 2022-03-25 03:59:40+00:00
- **Updated**: 2022-03-25 03:59:40+00:00
- **Authors**: Khalid El-Awady
- **Comment**: 8 pages, 12 figures
- **Journal**: None
- **Summary**: We reconstruct a 3D model of the surface of a material undergoing fatigue testing and experiencing cracking. Specifically we reconstruct the surface depth (out of plane intrusions and extrusions) and lateral (in-plane) motion from multiple views of the sample at the end of the experiment, combined with a reverse optical flow propagation backwards in time that utilizes interim single view images. These measurements can be mapped to a material strain tensor which helps to understand material life and predict failure. This approach offers an alternative to the commonly used Digital Image Correlation (DIC) technique which relies on tracking a speckle pattern applied to the material surface. DIC only produces in-plane (2D) measurements whereas our approach is 3D and non-invasive (requires no pattern being applied to the material).



### Facial Action Unit Recognition Based on Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2203.14694v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.14694v1)
- **Published**: 2022-03-25 04:01:58+00:00
- **Updated**: 2022-03-25 04:01:58+00:00
- **Authors**: Shangfei Wang, Yanan Chang, Jiahe Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Facial action unit recognition is an important task for facial analysis. Owing to the complex collection environment, facial action unit recognition in the wild is still challenging. The 3rd competition on affective behavior analysis in-the-wild (ABAW) has provided large amount of facial images with facial action unit annotations. In this paper, we introduce a facial action unit recognition method based on transfer learning. We first use available facial images with expression labels to train the feature extraction network. Then we fine-tune the network for facial action unit recognition.



### 3D GAN Inversion for Controllable Portrait Image Animation
- **Arxiv ID**: http://arxiv.org/abs/2203.13441v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.13441v1)
- **Published**: 2022-03-25 04:06:06+00:00
- **Updated**: 2022-03-25 04:06:06+00:00
- **Authors**: Connor Z. Lin, David B. Lindell, Eric R. Chan, Gordon Wetzstein
- **Comment**: Project page:
  https://www.computationalimaging.org/publications/3dganinversion/
- **Journal**: None
- **Summary**: Millions of images of human faces are captured every single day; but these photographs portray the likeness of an individual with a fixed pose, expression, and appearance. Portrait image animation enables the post-capture adjustment of these attributes from a single image while maintaining a photorealistic reconstruction of the subject's likeness or identity. Still, current methods for portrait image animation are typically based on 2D warping operations or manipulations of a 2D generative adversarial network (GAN) and lack explicit mechanisms to enforce multi-view consistency. Thus these methods may significantly alter the identity of the subject, especially when the viewpoint relative to the camera is changed. In this work, we leverage newly developed 3D GANs, which allow explicit control over the pose of the image subject with multi-view consistency. We propose a supervision strategy to flexibly manipulate expressions with 3D morphable models, and we show that the proposed method also supports editing appearance attributes, such as age or hairstyle, by interpolating within the latent space of the GAN. The proposed technique for portrait image animation outperforms previous methods in terms of image quality, identity preservation, and pose transfer while also supporting attribute editing.



### MDAN: Multi-level Dependent Attention Network for Visual Emotion Analysis
- **Arxiv ID**: http://arxiv.org/abs/2203.13443v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13443v1)
- **Published**: 2022-03-25 04:15:17+00:00
- **Updated**: 2022-03-25 04:15:17+00:00
- **Authors**: Liwen Xu, Zhengtao Wang, Bin Wu, Simon Lui
- **Comment**: Published in CVPR 2022
- **Journal**: None
- **Summary**: Visual Emotion Analysis (VEA) is attracting increasing attention. One of the biggest challenges of VEA is to bridge the affective gap between visual clues in a picture and the emotion expressed by the picture. As the granularity of emotions increases, the affective gap increases as well. Existing deep approaches try to bridge the gap by directly learning discrimination among emotions globally in one shot without considering the hierarchical relationship among emotions at different affective levels and the affective level of emotions to be classified. In this paper, we present the Multi-level Dependent Attention Network (MDAN) with two branches, to leverage the emotion hierarchy and the correlation between different affective levels and semantic levels. The bottom-up branch directly learns emotions at the highest affective level and strictly follows the emotion hierarchy while predicting emotions at lower affective levels. In contrast, the top-down branch attempt to disentangle the affective gap by one-to-one mapping between semantic levels and affective levels, namely, Affective Semantic Mapping. At each semantic level, a local classifier learns discrimination among emotions at the corresponding affective level. Finally, We integrate global learning and local learning into a unified deep framework and optimize the network simultaneously. Moreover, to properly extract and leverage channel dependencies and spatial attention while disentangling the affective gap, we carefully designed two attention modules: the Multi-head Cross Channel Attention module and the Level-dependent Class Activation Map module. Finally, the proposed deep framework obtains new state-of-the-art performance on six VEA benchmarks, where it outperforms existing state-of-the-art methods by a large margin, e.g., +3.85% on the WEBEmo dataset at 25 classes classification accuracy.



### Vision Transformer Compression with Structured Pruning and Low Rank Approximation
- **Arxiv ID**: http://arxiv.org/abs/2203.13444v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13444v1)
- **Published**: 2022-03-25 04:18:07+00:00
- **Updated**: 2022-03-25 04:18:07+00:00
- **Authors**: Ankur Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Transformer architecture has gained popularity due to its ability to scale with large dataset. Consequently, there is a need to reduce the model size and latency, especially for on-device deployment. We focus on vision transformer proposed for image recognition task (Dosovitskiy et al., 2021), and explore the application of different compression techniques such as low rank approximation and pruning for this purpose. Specifically, we investigate a structured pruning method proposed recently in Zhu et al. (2021) and find that mostly feedforward blocks are pruned with this approach, that too, with severe degradation in accuracy. We propose a hybrid compression approach to mitigate this where we compress the attention blocks using low rank approximation and use the previously mentioned pruning with a lower rate for feedforward blocks in each transformer layer. Our technique results in 50% compression with 14% relative increase in classification error whereas we obtain 44% compression with 20% relative increase in error when only pruning is applied. We propose further enhancements to bridge the accuracy gap but leave it as a future work.



### PCA-Based Knowledge Distillation Towards Lightweight and Content-Style Balanced Photorealistic Style Transfer Models
- **Arxiv ID**: http://arxiv.org/abs/2203.13452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13452v1)
- **Published**: 2022-03-25 05:23:42+00:00
- **Updated**: 2022-03-25 05:23:42+00:00
- **Authors**: Tai-Yin Chiu, Danna Gurari
- **Comment**: None
- **Journal**: None
- **Summary**: Photorealistic style transfer entails transferring the style of a reference image to another image so the result seems like a plausible photo. Our work is inspired by the observation that existing models are slow due to their large sizes. We introduce PCA-based knowledge distillation to distill lightweight models and show it is motivated by theory. To our knowledge, this is the first knowledge distillation method for photorealistic style transfer. Our experiments demonstrate its versatility for use with different backbone architectures, VGG and MobileNet, across six image resolutions. Compared to existing models, our top-performing model runs at speeds 5-20x faster using at most 1\% of the parameters. Additionally, our distilled models achieve a better balance between stylization strength and content preservation than existing models. To support reproducing our method and models, we share the code at \textit{https://github.com/chiutaiyin/PCA-Knowledge-Distillation}.



### CNN LEGO: Disassembling and Assembling Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2203.13453v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13453v1)
- **Published**: 2022-03-25 05:27:28+00:00
- **Updated**: 2022-03-25 05:27:28+00:00
- **Authors**: Jiacong Hu, Jing Gao, Zunlei Feng, Lechao Cheng, Jie Lei, Hujun Bao, Mingli Song
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN), which mimics human visual perception mechanism, has been successfully used in many computer vision areas. Some psychophysical studies show that the visual perception mechanism synchronously processes the form, color, movement, depth, etc., in the initial stage [7,20] and then integrates all information for final recognition [38]. What's more, the human visual system [20] contains different subdivisions or different tasks. Inspired by the above visual perception mechanism, we investigate a new task, termed as Model Disassembling and Assembling (MDA-Task), which can disassemble the deep models into independent parts and assemble those parts into a new deep model without performance cost like playing LEGO toys. To this end, we propose a feature route attribution technique (FRAT) for disassembling CNN classifiers in this paper. In FRAT, the positive derivatives of predicted class probability w.r.t. the feature maps are adopted to locate the critical features in each layer. Then, relevance analysis between the critical features and preceding/subsequent parameter layers is adopted to bridge the route between two adjacent parameter layers. In the assembling phase, class-wise components of each layer are assembled into a new deep model for a specific task. Extensive experiments demonstrate that the assembled CNN classifier can achieve close accuracy with the original classifier without any fine-tune, and excess original performance with one-epoch fine-tune. What's more, we also conduct massive experiments to verify the broad application of MDA-Task on model decision route visualization, model compression, knowledge distillation, transfer learning, incremental learning, and so on.



### A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2203.13455v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2203.13455v1)
- **Published**: 2022-03-25 05:33:34+00:00
- **Updated**: 2022-03-25 05:33:34+00:00
- **Authors**: Yifei Wang, Yisen Wang, Jiansheng Yang, Zhouchen Lin
- **Comment**: Accepted by ICLR 2022
- **Journal**: None
- **Summary**: Adversarial Training (AT) is known as an effective approach to enhance the robustness of deep neural networks. Recently researchers notice that robust models with AT have good generative ability and can synthesize realistic images, while the reason behind it is yet under-explored. In this paper, we demystify this phenomenon by developing a unified probabilistic framework, called Contrastive Energy-based Models (CEM). On the one hand, we provide the first probabilistic characterization of AT through a unified understanding of robustness and generative ability. On the other hand, our unified framework can be extended to the unsupervised scenario, which interprets unsupervised contrastive learning as an important sampling of CEM. Based on these, we propose a principled method to develop adversarial learning and sampling methods. Experiments show that the sampling methods derived from our framework improve the sample quality in both supervised and unsupervised learning. Notably, our unsupervised adversarial sampling method achieves an Inception score of 9.61 on CIFAR-10, which is superior to previous energy-based models and comparable to state-of-the-art generative models.



### Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap
- **Arxiv ID**: http://arxiv.org/abs/2203.13457v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2203.13457v2)
- **Published**: 2022-03-25 05:36:26+00:00
- **Updated**: 2022-05-27 09:29:45+00:00
- **Authors**: Yifei Wang, Qi Zhang, Yisen Wang, Jiansheng Yang, Zhouchen Lin
- **Comment**: Accepeted by ICLR 2022
- **Journal**: None
- **Summary**: Recently, contrastive learning has risen to be a promising approach for large-scale self-supervised learning. However, theoretical understanding of how it works is still unclear. In this paper, we propose a new guarantee on the downstream performance without resorting to the conditional independence assumption that is widely adopted in previous work but hardly holds in practice. Our new theory hinges on the insight that the support of different intra-class samples will become more overlapped under aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make contrastive learning cluster intra-class samples together. Based on this augmentation overlap perspective, theoretically, we obtain asymptotically closed bounds for downstream performance under weaker assumptions, and empirically, we propose an unsupervised model selection metric ARC that aligns well with downstream accuracy. Our theory suggests an alternative understanding of contrastive learning: the role of aligning positive samples is more like a surrogate task than an ultimate goal, and the overlapped augmented views (i.e., the chaos) create a ladder for contrastive learning to gradually learn class-separated representations. The code for computing ARC is available at https://github.com/zhangq327/ARC.



### PANDORA: Polarization-Aided Neural Decomposition Of Radiance
- **Arxiv ID**: http://arxiv.org/abs/2203.13458v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2203.13458v1)
- **Published**: 2022-03-25 05:41:52+00:00
- **Updated**: 2022-03-25 05:41:52+00:00
- **Authors**: Akshat Dave, Yongyi Zhao, Ashok Veeraraghavan
- **Comment**: Project webpage: https://akshatdave.github.io/pandora
- **Journal**: None
- **Summary**: Reconstructing an object's geometry and appearance from multiple images, also known as inverse rendering, is a fundamental problem in computer graphics and vision. Inverse rendering is inherently ill-posed because the captured image is an intricate function of unknown lighting conditions, material properties and scene geometry. Recent progress in representing scene properties as coordinate-based neural networks have facilitated neural inverse rendering resulting in impressive geometry reconstruction and novel-view synthesis. Our key insight is that polarization is a useful cue for neural inverse rendering as polarization strongly depends on surface normals and is distinct for diffuse and specular reflectance. With the advent of commodity, on-chip, polarization sensors, capturing polarization has become practical. Thus, we propose PANDORA, a polarimetric inverse rendering approach based on implicit neural representations. From multi-view polarization images of an object, PANDORA jointly extracts the object's 3D geometry, separates the outgoing radiance into diffuse and specular and estimates the illumination incident on the object. We show that PANDORA outperforms state-of-the-art radiance decomposition techniques. PANDORA outputs clean surface reconstructions free from texture artefacts, models strong specularities accurately and estimates illumination under practical unstructured scenarios.



### Semi-supervised and Deep learning Frameworks for Video Classification and Key-frame Identification
- **Arxiv ID**: http://arxiv.org/abs/2203.13459v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.13459v1)
- **Published**: 2022-03-25 05:45:18+00:00
- **Updated**: 2022-03-25 05:45:18+00:00
- **Authors**: Sohini Roychowdhury
- **Comment**: 9 pages, 7 images, 3 tables
- **Journal**: None
- **Summary**: Automating video-based data and machine learning pipelines poses several challenges including metadata generation for efficient storage and retrieval and isolation of key-frames for scene understanding tasks. In this work, we present two semi-supervised approaches that automate this process of manual frame sifting in video streams by automatically classifying scenes for content and filtering frames for fine-tuning scene understanding tasks. The first rule-based method starts from a pre-trained object detector and it assigns scene type, uncertainty and lighting categories to each frame based on probability distributions of foreground objects. Next, frames with the highest uncertainty and structural dissimilarity are isolated as key-frames. The second method relies on the simCLR model for frame encoding followed by label-spreading from 20% of frame samples to label the remaining frames for scene and lighting categories. Also, clustering the video frames in the encoded feature space further isolates key-frames at cluster boundaries. The proposed methods achieve 64-93% accuracy for automated scene categorization for outdoor image videos from public domain datasets of JAAD and KITTI. Also, less than 10% of all input frames can be filtered as key-frames that can then be sent for annotation and fine tuning of machine vision algorithms. Thus, the proposed framework can be scaled to additional video data streams for automated training of perception-driven systems with minimal training images.



### Interpretation of Chest x-rays affected by bullets using deep transfer learning
- **Arxiv ID**: http://arxiv.org/abs/2203.13461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13461v1)
- **Published**: 2022-03-25 05:53:45+00:00
- **Updated**: 2022-03-25 05:53:45+00:00
- **Authors**: Shaheer Khan, Azib Farooq, Israr Khan, Muhammad Gulraiz Khan, Abdul Razzaq
- **Comment**: 10 pages, 8 figures, 1 table, Paper is intended to publish in journal
  and first author reserves all copyrights of reproduction
- **Journal**: None
- **Summary**: The potential of deep learning, especially in medical imaging, initiated astonishing results and improved the methodologies after every passing day. Deep learning in radiology provides the opportunity to classify, detect and segment different diseases automatically. In the proposed study, we worked on a non-trivial aspect of medical imaging where we classified and localized the X-Rays affected by bullets. We tested Images on different classification and localization models to get considerable accuracy. The replicated data set used in the study was replicated on different images of chest X-Rays. The proposed model worked not only on chest radiographs but other body organs X-rays like leg, abdomen, head, even the training dataset based on chest radiographs. Custom models have been used for classification and localization purposes after tuning parameters. Finally, the results of our findings manifested using different frameworks. This might assist the research enlightening towards this field. To the best of our knowledge, this is the first study on the detection and classification of radiographs affected by bullets using deep learning.



### CAD: Co-Adapting Discriminative Features for Improved Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2203.13465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13465v1)
- **Published**: 2022-03-25 06:14:51+00:00
- **Updated**: 2022-03-25 06:14:51+00:00
- **Authors**: Philip Chikontwe, Soopil Kim, Sang Hyun Park
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: Few-shot classification is a challenging problem that aims to learn a model that can adapt to unseen classes given a few labeled samples. Recent approaches pre-train a feature extractor, and then fine-tune for episodic meta-learning. Other methods leverage spatial features to learn pixel-level correspondence while jointly training a classifier. However, results using such approaches show marginal improvements. In this paper, inspired by the transformer style self-attention mechanism, we propose a strategy to cross-attend and re-weight discriminative features for few-shot classification. Given a base representation of support and query images after global pooling, we introduce a single shared module that projects features and cross-attends in two aspects: (i) query to support, and (ii) support to query. The module computes attention scores between features to produce an attention pooled representation of features in the same class that is later added to the original representation followed by a projection head. This effectively re-weights features in both aspects (i & ii) to produce features that better facilitate improved metric-based meta-learning. Extensive experiments on public benchmarks show our approach outperforms state-of-the-art methods by 3%~5%.



### RD-Optimized Trit-Plane Coding of Deep Compressed Image Latent Tensors
- **Arxiv ID**: http://arxiv.org/abs/2203.13467v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.13467v2)
- **Published**: 2022-03-25 06:33:16+00:00
- **Updated**: 2022-05-08 07:53:19+00:00
- **Authors**: Seungmin Jeon, Jae-Han Lee, Chang-Su Kim
- **Comment**: None
- **Journal**: None
- **Summary**: DPICT is the first learning-based image codec supporting fine granular scalability. In this paper, we describe how to implement two key components of DPICT efficiently: trit-plane slicing and rate-distortion-optimized (RD-optimized) coding. In DPICT, we transform an image into a latent tensor, represent the tensor in ternary digits (trits), and encode the trits in the decreasing order of significance. For entropy encoding, it is necessary to compute the probability of each trit, which demands high time complexity in both the encoder and the decoder. To reduce the complexity, we develop a parallel computing scheme for the probabilities, which is described in detail with pseudo-codes. Moreover, we compare the trit-plane slicing in DPICT with the alternative bit-plane slicing. Experimental results show that the time complexity is reduced significantly by the parallel computing and that the trit-plane slicing provides better RD performances than the bit-plane slicing.



### Interactive Style Transfer: All is Your Palette
- **Arxiv ID**: http://arxiv.org/abs/2203.13470v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13470v1)
- **Published**: 2022-03-25 06:38:46+00:00
- **Updated**: 2022-03-25 06:38:46+00:00
- **Authors**: Zheng Lin, Zhao Zhang, Kang-Rui Zhang, Bo Ren, Ming-Ming Cheng
- **Comment**: 8 pages, 11 figures
- **Journal**: None
- **Summary**: Neural style transfer (NST) can create impressive artworks by transferring reference style to content image. Current image-to-image NST methods are short of fine-grained controls, which are often demanded by artistic editing. To mitigate this limitation, we propose a drawing-like interactive style transfer (IST) method, by which users can interactively create a harmonious-style image. Our IST method can serve as a brush, dip style from anywhere, and then paint to any region of the target content image. To determine the action scope, we formulate a fluid simulation algorithm, which takes styles as pigments around the position of brush interaction, and diffusion in style or content images according to the similarity maps. Our IST method expands the creative dimension of NST. By dipping and painting, even employing one style image can produce thousands of eye-catching works. The demo video is available in supplementary files or in http://mmcheng.net/ist.



### Non-Probability Sampling Network for Stochastic Human Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2203.13471v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.13471v2)
- **Published**: 2022-03-25 06:41:47+00:00
- **Updated**: 2022-05-14 16:40:30+00:00
- **Authors**: Inhwan Bae, Jin-Hwi Park, Hae-Gon Jeon
- **Comment**: Accepted at CVPR 2022
- **Journal**: None
- **Summary**: Capturing multimodal natures is essential for stochastic pedestrian trajectory prediction, to infer a finite set of future trajectories. The inferred trajectories are based on observation paths and the latent vectors of potential decisions of pedestrians in the inference step. However, stochastic approaches provide varying results for the same data and parameter settings, due to the random sampling of the latent vector. In this paper, we analyze the problem by reconstructing and comparing probabilistic distributions from prediction samples and socially-acceptable paths, respectively. Through this analysis, we observe that the inferences of all stochastic models are biased toward the random sampling, and fail to generate a set of realistic paths from finite samples. The problem cannot be resolved unless an infinite number of samples is available, which is infeasible in practice. We introduce that the Quasi-Monte Carlo (QMC) method, ensuring uniform coverage on the sampling space, as an alternative to the conventional random sampling. With the same finite number of samples, the QMC improves all the multimodal prediction results. We take an additional step ahead by incorporating a learnable sampling network into the existing networks for trajectory prediction. For this purpose, we propose the Non-Probability Sampling Network (NPSN), a very small network (~5K parameters) that generates purposive sample sequences using the past paths of pedestrians and their social interactions. Extensive experiments confirm that NPSN can significantly improve both the prediction accuracy (up to 60%) and reliability of the public pedestrian trajectory prediction benchmark. Code is publicly available at https://github.com/inhwanbae/NPSN .



### Facial Expression Recognition with Swin Transformer
- **Arxiv ID**: http://arxiv.org/abs/2203.13472v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.13472v1)
- **Published**: 2022-03-25 06:42:31+00:00
- **Updated**: 2022-03-25 06:42:31+00:00
- **Authors**: Jun-Hwa Kim, Namho Kim, Chee Sun Won
- **Comment**: None
- **Journal**: None
- **Summary**: The task of recognizing human facial expressions plays a vital role in various human-related systems, including health care and medical fields. With the recent success of deep learning and the accessibility of a large amount of annotated data, facial expression recognition research has been mature enough to be utilized in real-world scenarios with audio-visual datasets. In this paper, we introduce Swin transformer-based facial expression approach for an in-the-wild audio-visual dataset of the Aff-Wild2 Expression dataset. Specifically, we employ a three-stream network (i.e., Visual stream, Temporal stream, and Audio stream) for the audio-visual videos to fuse the multi-modal information into facial expression recognition. Experimental results on the Aff-Wild2 dataset show the effectiveness of our proposed multi-modal approaches.



### Enhancing Transferability of Adversarial Examples with Spatial Momentum
- **Arxiv ID**: http://arxiv.org/abs/2203.13479v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13479v2)
- **Published**: 2022-03-25 07:03:17+00:00
- **Updated**: 2022-11-03 07:52:51+00:00
- **Authors**: Guoqiu Wang, Huanqian Yan, Xingxing Wei
- **Comment**: Accepted as Oral by 5-th Chinese Conference on Pattern Recognition
  and Computer Vision, PRCV 2022
- **Journal**: None
- **Summary**: Many adversarial attack methods achieve satisfactory attack success rates under the white-box setting, but they usually show poor transferability when attacking other DNN models. Momentum-based attack is one effective method to improve transferability. It integrates the momentum term into the iterative process, which can stabilize the update directions by adding the gradients' temporal correlation for each pixel. We argue that only this temporal momentum is not enough, the gradients from the spatial domain within an image, i.e. gradients from the context pixels centered on the target pixel are also important to the stabilization. For that, we propose a novel method named Spatial Momentum Iterative FGSM attack (SMI-FGSM), which introduces the mechanism of momentum accumulation from temporal domain to spatial domain by considering the context information from different regions within the image. SMI-FGSM is then integrated with temporal momentum to simultaneously stabilize the gradients' update direction from both the temporal and spatial domains. Extensive experiments show that our method indeed further enhances adversarial transferability. It achieves the best transferability success rate for multiple mainstream undefended and defended models, which outperforms the state-of-the-art attack methods by a large margin of 10\% on average.



### Polarization Multiplexed Diffractive Computing: All-Optical Implementation of a Group of Linear Transformations Through a Polarization-Encoded Diffractive Network
- **Arxiv ID**: http://arxiv.org/abs/2203.13482v1
- **DOI**: 10.1038/s41377-022-00849-x
- **Categories**: **physics.optics**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2203.13482v1)
- **Published**: 2022-03-25 07:10:47+00:00
- **Updated**: 2022-03-25 07:10:47+00:00
- **Authors**: Jingxi Li, Yi-Chun Hung, Onur Kulce, Deniz Mengu, Aydogan Ozcan
- **Comment**: 31 pages, 7 figures
- **Journal**: Light: Science & Applications (2022)
- **Summary**: Research on optical computing has recently attracted significant attention due to the transformative advances in machine learning. Among different approaches, diffractive optical networks composed of spatially-engineered transmissive surfaces have been demonstrated for all-optical statistical inference and performing arbitrary linear transformations using passive, free-space optical layers. Here, we introduce a polarization multiplexed diffractive processor to all-optically perform multiple, arbitrarily-selected linear transformations through a single diffractive network trained using deep learning. In this framework, an array of pre-selected linear polarizers is positioned between trainable transmissive diffractive materials that are isotropic, and different target linear transformations (complex-valued) are uniquely assigned to different combinations of input/output polarization states. The transmission layers of this polarization multiplexed diffractive network are trained and optimized via deep learning and error-backpropagation by using thousands of examples of the input/output fields corresponding to each one of the complex-valued linear transformations assigned to different input/output polarization combinations. Our results and analysis reveal that a single diffractive network can successfully approximate and all-optically implement a group of arbitrarily-selected target transformations with a negligible error when the number of trainable diffractive features/neurons (N) approaches N_p x N_i x N_o, where N_i and N_o represent the number of pixels at the input and output fields-of-view, respectively, and N_p refers to the number of unique linear transformations assigned to different input/output polarization combinations. This polarization-multiplexed all-optical diffractive processor can find various applications in optical computing and polarization-based machine vision tasks.



### Compare learning: bi-attention network for few-shot learning
- **Arxiv ID**: http://arxiv.org/abs/2203.13487v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13487v1)
- **Published**: 2022-03-25 07:39:10+00:00
- **Updated**: 2022-03-25 07:39:10+00:00
- **Authors**: Li Ke, Meng Pan, Weigao Wen, Dong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Learning with few labeled data is a key challenge for visual recognition, as deep neural networks tend to overfit using a few samples only. One of the Few-shot learning methods called metric learning addresses this challenge by first learning a deep distance metric to determine whether a pair of images belong to the same category, then applying the trained metric to instances from other test set with limited labels. This method makes the most of the few samples and limits the overfitting effectively. However, extant metric networks usually employ Linear classifiers or Convolutional neural networks (CNN) that are not precise enough to globally capture the subtle differences between vectors. In this paper, we propose a novel approach named Bi-attention network to compare the instances, which can measure the similarity between embeddings of instances precisely, globally and efficiently. We verify the effectiveness of our model on two benchmarks. Experiments show that our approach achieved improved accuracy and convergence speed over baseline models.



### Contrastive learning of Class-agnostic Activation Map for Weakly Supervised Object Localization and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.13505v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13505v2)
- **Published**: 2022-03-25 08:46:24+00:00
- **Updated**: 2022-12-22 13:16:09+00:00
- **Authors**: Jinheng Xie, Jianfeng Xiang, Junliang Chen, Xianxu Hou, Xiaodong Zhao, Linlin Shen
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: While class activation map (CAM) generated by image classification network has been widely used for weakly supervised object localization (WSOL) and semantic segmentation (WSSS), such classifiers usually focus on discriminative object regions. In this paper, we propose Contrastive learning for Class-agnostic Activation Map (C$^2$AM) generation only using unlabeled image data, without the involvement of image-level supervision. The core idea comes from the observation that i) semantic information of foreground objects usually differs from their backgrounds; ii) foreground objects with similar appearance or background with similar color/texture have similar representations in the feature space. We form the positive and negative pairs based on the above relations and force the network to disentangle foreground and background with a class-agnostic activation map using a novel contrastive loss. As the network is guided to discriminate cross-image foreground-background, the class-agnostic activation maps learned by our approach generate more complete object regions. We successfully extracted from C$^2$AM class-agnostic object bounding boxes for object localization and background cues to refine CAM generated by classification network for semantic segmentation. Extensive experiments on CUB-200-2011, ImageNet-1K, and PASCAL VOC2012 datasets show that both WSOL and WSSS can benefit from the proposed C$^2$AM.



### Analysis of the Production Strategy of Mask Types in the COVID-19 Environment
- **Arxiv ID**: http://arxiv.org/abs/2203.13506v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.13506v1)
- **Published**: 2022-03-25 08:49:58+00:00
- **Updated**: 2022-03-25 08:49:58+00:00
- **Authors**: Xiangri Lu, Zhanqing Wang, Hongbin Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Since the outbreak of the COVID-19 in December 2019, medical protective equipment such as disposable medical masks and KN95 masks have become essential resources for the public. Enterprises in all sectors of society have also transformed the production of medical masks. After the outbreak, how to choose the right time to produce medical protective masks, and what type of medical masks to produce will play a positive role in preventing and controlling the epidemic in a short time. In this regard, the evolutionary game competition analysis will be conducted through the relevant data of disposable medical masks and KN95 masks to determine the appropriate nodes for the production of corresponding mask types. After the research and analysis of the production strategy of mask types, it has a positive effect on how to guide the resumption of work and production.



### Analysis of OODA Loop based on Adversarial for Complex Game Environments
- **Arxiv ID**: http://arxiv.org/abs/2203.15502v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.15502v1)
- **Published**: 2022-03-25 09:17:07+00:00
- **Updated**: 2022-03-25 09:17:07+00:00
- **Authors**: Xiangri Lu, Hongbin Ma, Zhanqing Wang
- **Comment**: None
- **Journal**: None
- **Summary**: To address the problem of imperfect confrontation strategy caused by the lack of information of game environment in the simulation of non-complete information dynamic countermeasure modeling for intelligent game, the hierarchical analysis game strategy of confrontation model based on OODA ring (Observation, Orientation, Decision, Action) theory is proposed. At the same time, taking into account the trend of unmanned future warfare, NetLogo software simulation is used to construct a dynamic derivation of the confrontation between two tanks. In the validation process, the OODA loop theory is used to describe the operation process of the complex system between red and blue sides, and the four-step cycle of observation, judgment, decision and execution is carried out according to the number of armor of both sides, and then the OODA loop system adjusts the judgment and decision time coefficients for the next confrontation cycle according to the results of the first cycle. Compared with traditional simulation methods that consider objective factors such as loss rate and support rate, the OODA-loop-based hierarchical game analysis can analyze the confrontation situation more comprehensively.



### Multimodal Pre-training Based on Graph Attention Network for Document Understanding
- **Arxiv ID**: http://arxiv.org/abs/2203.13530v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13530v2)
- **Published**: 2022-03-25 09:27:50+00:00
- **Updated**: 2022-10-23 16:12:10+00:00
- **Authors**: Zhenrong Zhang, Jiefeng Ma, Jun Du, Licheng Wang, Jianshu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Document intelligence as a relatively new research topic supports many business applications. Its main task is to automatically read, understand, and analyze documents. However, due to the diversity of formats (invoices, reports, forms, etc.) and layouts in documents, it is difficult to make machines understand documents. In this paper, we present the GraphDoc, a multimodal graph attention-based model for various document understanding tasks. GraphDoc is pre-trained in a multimodal framework by utilizing text, layout, and image information simultaneously. In a document, a text block relies heavily on its surrounding contexts, accordingly we inject the graph structure into the attention mechanism to form a graph attention layer so that each input node can only attend to its neighborhoods. The input nodes of each graph attention layer are composed of textual, visual, and positional features from semantically meaningful regions in a document image. We do the multimodal feature fusion of each node by the gate fusion layer. The contextualization between each node is modeled by the graph attention layer. GraphDoc learns a generic representation from only 320k unlabeled documents via the Masked Sentence Modeling task. Extensive experimental results on the publicly available datasets show that GraphDoc achieves state-of-the-art performance, which demonstrates the effectiveness of our proposed method. The code is available at https://github.com/ZZR8066/GraphDoc.



### High-Performance Transformer Tracking
- **Arxiv ID**: http://arxiv.org/abs/2203.13533v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13533v2)
- **Published**: 2022-03-25 09:33:29+00:00
- **Updated**: 2022-11-23 08:45:18+00:00
- **Authors**: Xin Chen, Bin Yan, Jiawen Zhu, Huchuan Lu, Xiang Ruan, Dong Wang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2103.15436
- **Journal**: None
- **Summary**: Correlation has a critical role in the tracking field, especially in recent popular Siamese-based trackers. The correlation operation is a simple fusion method that considers the similarity between the template and the search region. However, the correlation operation is a local linear matching process, losing semantic information and easily falling into a local optimum, which may be the bottleneck in designing high-accuracy tracking algorithms. In this work, to determine whether a better feature fusion method exists than correlation, a novel attention-based feature fusion network, inspired by the transformer, is presented. This network effectively combines the template and search region features using attention. Specifically, the proposed method includes an ego-context augment module based on self-attention and a cross-feature augment module based on cross-attention. First, we present a transformer tracking (named TransT) method based on the Siamese-like feature extraction backbone, the designed attention-based fusion mechanism, and the classification and regression head. Based on the TransT baseline, we further design a segmentation branch to generate an accurate mask. Finally, we propose a stronger version of TransT by extending TransT with a multi-template scheme and an IoU prediction head, named TransT-M. Experiments show that our TransT and TransT-M methods achieve promising results on seven popular datasets. Code and models are available at https://github.com/chenxin-dlut/TransT-M.



### SeCo: Separating Unknown Musical Visual Sounds with Consistency Guidance
- **Arxiv ID**: http://arxiv.org/abs/2203.13535v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2203.13535v1)
- **Published**: 2022-03-25 09:42:11+00:00
- **Updated**: 2022-03-25 09:42:11+00:00
- **Authors**: Xinchi Zhou, Dongzhan Zhou, Wanli Ouyang, Hang Zhou, Ziwei Liu, Di Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed the success of deep learning on the visual sound separation task. However, existing works follow similar settings where the training and testing datasets share the same musical instrument categories, which to some extent limits the versatility of this task. In this work, we focus on a more general and challenging scenario, namely the separation of unknown musical instruments, where the categories in training and testing phases have no overlap with each other. To tackle this new setting, we propose the Separation-with-Consistency (SeCo) framework, which can accomplish the separation on unknown categories by exploiting the consistency constraints. Furthermore, to capture richer characteristics of the novel melodies, we devise an online matching strategy, which can bring stable enhancements with no cost of extra parameters. Experiments demonstrate that our SeCo framework exhibits strong adaptation ability on the novel musical categories and outperforms the baseline methods by a significant margin.



### Efficient Visual Tracking via Hierarchical Cross-Attention Transformer
- **Arxiv ID**: http://arxiv.org/abs/2203.13537v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13537v2)
- **Published**: 2022-03-25 09:45:27+00:00
- **Updated**: 2022-10-30 02:24:40+00:00
- **Authors**: Xin Chen, Ben Kang, Dong Wang, Dongdong Li, Huchuan Lu
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, target tracking has made great progress in accuracy. This development is mainly attributed to powerful networks (such as transformers) and additional modules (such as online update and refinement modules). However, less attention has been paid to tracking speed. Most state-of-the-art trackers are satisfied with the real-time speed on powerful GPUs. However, practical applications necessitate higher requirements for tracking speed, especially when edge platforms with limited resources are used. In this work, we present an efficient tracking method via a hierarchical cross-attention transformer named HCAT. Our model runs about 195 fps on GPU, 45 fps on CPU, and 55 fps on the edge AI platform of NVidia Jetson AGX Xavier. Experiments show that our HCAT achieves promising results on LaSOT, GOT-10k, TrackingNet, NFS, OTB100, UAV123, and VOT2020. Code and models are available at https://github.com/chenxin-dlut/HCAT.



### Deformable Butterfly: A Highly Structured and Sparse Linear Transform
- **Arxiv ID**: http://arxiv.org/abs/2203.13556v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.13556v1)
- **Published**: 2022-03-25 10:20:50+00:00
- **Updated**: 2022-03-25 10:20:50+00:00
- **Authors**: Rui Lin, Jie Ran, King Hung Chiu, Graziano Chesi, Ngai Wong
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new kind of linear transform named Deformable Butterfly (DeBut) that generalizes the conventional butterfly matrices and can be adapted to various input-output dimensions. It inherits the fine-to-coarse-grained learnable hierarchy of traditional butterflies and when deployed to neural networks, the prominent structures and sparsity in a DeBut layer constitutes a new way for network compression. We apply DeBut as a drop-in replacement of standard fully connected and convolutional layers, and demonstrate its superiority in homogenizing a neural network and rendering it favorable properties such as light weight and low inference complexity, without compromising accuracy. The natural complexity-accuracy tradeoff arising from the myriad deformations of a DeBut layer also opens up new rooms for analytical and practical research. The codes and Appendix are publicly available at: https://github.com/ruilin0212/DeBut.



### Neural Networks with Divisive normalization for image segmentation with application in cityscapes dataset
- **Arxiv ID**: http://arxiv.org/abs/2203.13558v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.13558v2)
- **Published**: 2022-03-25 10:26:39+00:00
- **Updated**: 2022-11-09 13:40:58+00:00
- **Authors**: Pablo Hernández-Cámara, Valero Laparra, Jesús Malo
- **Comment**: None
- **Journal**: None
- **Summary**: One of the key problems in computer vision is adaptation: models are too rigid to follow the variability of the inputs. The canonical computation that explains adaptation in sensory neuroscience is divisive normalization, and it has appealing effects on image manifolds. In this work we show that including divisive normalization in current deep networks makes them more invariant to non-informative changes in the images. In particular, we focus on U-Net architectures for image segmentation. Experiments show that the inclusion of divisive normalization in the U-Net architecture leads to better segmentation results with respect to conventional U-Net. The gain increases steadily when dealing with images acquired in bad weather conditions. In addition to the results on the Cityscapes and Foggy Cityscapes datasets, we explain these advantages through visualization of the responses: the equalization induced by the divisive normalization leads to more invariant features to local changes in contrast and illumination.



### A Visual Navigation Perspective for Category-Level Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2203.13572v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.13572v2)
- **Published**: 2022-03-25 10:57:37+00:00
- **Updated**: 2022-07-23 10:44:57+00:00
- **Authors**: Jiaxin Guo, Fangxun Zhong, Rong Xiong, Yunhui Liu, Yue Wang, Yiyi Liao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies category-level object pose estimation based on a single monocular image. Recent advances in pose-aware generative models have paved the way for addressing this challenging task using analysis-by-synthesis. The idea is to sequentially update a set of latent variables, e.g., pose, shape, and appearance, of the generative model until the generated image best agrees with the observation. However, convergence and efficiency are two challenges of this inference procedure. In this paper, we take a deeper look at the inference of analysis-by-synthesis from the perspective of visual navigation, and investigate what is a good navigation policy for this specific task. We evaluate three different strategies, including gradient descent, reinforcement learning and imitation learning, via thorough comparisons in terms of convergence, robustness and efficiency. Moreover, we show that a simple hybrid approach leads to an effective and efficient solution. We further compare these strategies to state-of-the-art methods, and demonstrate superior performance on synthetic and real-world datasets leveraging off-the-shelf pose-aware generative models.



### Continual Test-Time Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2203.13591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13591v1)
- **Published**: 2022-03-25 11:42:02+00:00
- **Updated**: 2022-03-25 11:42:02+00:00
- **Authors**: Qin Wang, Olga Fink, Luc Van Gool, Dengxin Dai
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Test-time domain adaptation aims to adapt a source pre-trained model to a target domain without using any source data. Existing works mainly consider the case where the target domain is static. However, real-world machine perception systems are running in non-stationary and continually changing environments where the target domain distribution can change over time. Existing methods, which are mostly based on self-training and entropy regularization, can suffer from these non-stationary environments. Due to the distribution shift over time in the target domain, pseudo-labels become unreliable. The noisy pseudo-labels can further lead to error accumulation and catastrophic forgetting. To tackle these issues, we propose a continual test-time adaptation approach~(CoTTA) which comprises two parts. Firstly, we propose to reduce the error accumulation by using weight-averaged and augmentation-averaged predictions which are often more accurate. On the other hand, to avoid catastrophic forgetting, we propose to stochastically restore a small part of the neurons to the source pre-trained weights during each iteration to help preserve source knowledge in the long-term. The proposed method enables the long-term adaptation for all parameters in the network. CoTTA is easy to implement and can be readily incorporated in off-the-shelf pre-trained models. We demonstrate the effectiveness of our approach on four classification tasks and a segmentation task for continual test-time adaptation, on which we outperform existing methods. Our code is available at \url{https://qin.ee/cotta}.



### Fast Hybrid Image Retargeting
- **Arxiv ID**: http://arxiv.org/abs/2203.13595v1
- **DOI**: 10.1109/ICIP42928.2021.9506584
- **Categories**: **cs.CV**, cs.AI, I.2.10; I.4.0; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2203.13595v1)
- **Published**: 2022-03-25 11:46:06+00:00
- **Updated**: 2022-03-25 11:46:06+00:00
- **Authors**: Daniel Valdez-Balderas, Oleg Muraveynyk, Timothy Smith
- **Comment**: 5 pages
- **Journal**: 2021 IEEE International Conference on Image Processing (ICIP),
  2021, pp. 1849-1853
- **Summary**: Image retargeting changes the aspect ratio of images while aiming to preserve content and minimise noticeable distortion. Fast and high-quality methods are particularly relevant at present, due to the large variety of image and display aspect ratios. We propose a retargeting method that quantifies and limits warping distortions with the use of content-aware cropping. The pipeline of the proposed approach consists of the following steps. First, an importance map of a source image is generated using deep semantic segmentation and saliency detection models. Then, a preliminary warping mesh is computed using axis aligned deformations, enhanced with the use of a distortion measure to ensure low warping deformations. Finally, the retargeted image is produced using a content-aware cropping algorithm. In order to evaluate our method, we perform a user study based on the RetargetMe benchmark. Experimental analyses show that our method outperforms recent approaches, while running in a fraction of their execution time.



### Navigable Proximity Graph-Driven Native Hybrid Queries with Structured and Unstructured Constraints
- **Arxiv ID**: http://arxiv.org/abs/2203.13601v1
- **DOI**: None
- **Categories**: **cs.DB**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2203.13601v1)
- **Published**: 2022-03-25 12:02:37+00:00
- **Updated**: 2022-03-25 12:02:37+00:00
- **Authors**: Mengzhao Wang, Lingwei Lv, Xiaoliang Xu, Yuxiang Wang, Qiang Yue, Jiongkang Ni
- **Comment**: 16 pages, 10 figures
- **Journal**: None
- **Summary**: As research interest surges, vector similarity search is applied in multiple fields, including data mining, computer vision, and information retrieval. {Given a set of objects (e.g., a set of images) and a query object, we can easily transform each object into a feature vector and apply the vector similarity search to retrieve the most similar objects. However, the original vector similarity search cannot well support \textit{hybrid queries}, where users not only input unstructured query constraint (i.e., the feature vector of query object) but also structured query constraint (i.e., the desired attributes of interest). Hybrid query processing aims at identifying these objects with similar feature vectors to query object and satisfying the given attribute constraints. Recent efforts have attempted to answer a hybrid query by performing attribute filtering and vector similarity search separately and then merging the results later, which limits efficiency and accuracy because they are not purpose-built for hybrid queries.} In this paper, we propose a native hybrid query (NHQ) framework based on proximity graph (PG), which provides the specialized \textit{composite index and joint pruning} modules for hybrid queries. We easily deploy existing various PGs on this framework to process hybrid queries efficiently. Moreover, we present two novel navigable PGs (NPGs) with optimized edge selection and routing strategies, which obtain better overall performance than existing PGs. After that, we deploy the proposed NPGs in NHQ to form two hybrid query methods, which significantly outperform the state-of-the-art competitors on all experimental datasets (10$\times$ faster under the same \textit{Recall}), including eight public and one in-house real-world datasets. Our code and datasets have been released at \url{https://github.com/AshenOn3/NHQ}.



### Rope3D: TheRoadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection Task
- **Arxiv ID**: http://arxiv.org/abs/2203.13608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13608v1)
- **Published**: 2022-03-25 12:13:23+00:00
- **Updated**: 2022-03-25 12:13:23+00:00
- **Authors**: Xiaoqing Ye, Mao Shu, Hanyu Li, Yifeng Shi, Yingying Li, Guangjie Wang, Xiao Tan, Errui Ding
- **Comment**: To appear in CVPR2022
- **Journal**: None
- **Summary**: Concurrent perception datasets for autonomous driving are mainly limited to frontal view with sensors mounted on the vehicle. None of them is designed for the overlooked roadside perception tasks. On the other hand, the data captured from roadside cameras have strengths over frontal-view data, which is believed to facilitate a safer and more intelligent autonomous driving system. To accelerate the progress of roadside perception, we present the first high-diversity challenging Roadside Perception 3D dataset- Rope3D from a novel view. The dataset consists of 50k images and over 1.5M 3D objects in various scenes, which are captured under different settings including various cameras with ambiguous mounting positions, camera specifications, viewpoints, and different environmental conditions. We conduct strict 2D-3D joint annotation and comprehensive data analysis, as well as set up a new 3D roadside perception benchmark with metrics and evaluation devkit. Furthermore, we tailor the existing frontal-view monocular 3D object detection approaches and propose to leverage the geometry constraint to solve the inherent ambiguities caused by various sensors, viewpoints. Our dataset is available on https://thudair.baai.ac.cn/rope.



### Unsupervised Pre-training for Temporal Action Localization Tasks
- **Arxiv ID**: http://arxiv.org/abs/2203.13609v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13609v1)
- **Published**: 2022-03-25 12:13:43+00:00
- **Updated**: 2022-03-25 12:13:43+00:00
- **Authors**: Can Zhang, Tianyu Yang, Junwu Weng, Meng Cao, Jue Wang, Yuexian Zou
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Unsupervised video representation learning has made remarkable achievements in recent years. However, most existing methods are designed and optimized for video classification. These pre-trained models can be sub-optimal for temporal localization tasks due to the inherent discrepancy between video-level classification and clip-level localization. To bridge this gap, we make the first attempt to propose a self-supervised pretext task, coined as Pseudo Action Localization (PAL) to Unsupervisedly Pre-train feature encoders for Temporal Action Localization tasks (UP-TAL). Specifically, we first randomly select temporal regions, each of which contains multiple clips, from one video as pseudo actions and then paste them onto different temporal positions of the other two videos. The pretext task is to align the features of pasted pseudo action regions from two synthetic videos and maximize the agreement between them. Compared to the existing unsupervised video representation learning approaches, our PAL adapts better to downstream TAL tasks by introducing a temporal equivariant contrastive learning paradigm in a temporally dense and scale-aware manner. Extensive experiments show that PAL can utilize large-scale unlabeled video data to significantly boost the performance of existing TAL methods. Our codes and models will be made publicly available at https://github.com/zhang-can/UP-TAL.



### Learning to Adapt to Unseen Abnormal Activities under Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/2203.13610v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13610v1)
- **Published**: 2022-03-25 12:15:44+00:00
- **Updated**: 2022-03-25 12:15:44+00:00
- **Authors**: Jaeyoo Park, Junha Kim, Bohyung Han
- **Comment**: 20 pages, ACCV 2020
- **Journal**: None
- **Summary**: We present a meta-learning framework for weakly supervised anomaly detection in videos, where the detector learns to adapt to unseen types of abnormal activities effectively when only video-level annotations of binary labels are available. Our work is motivated by the fact that existing methods suffer from poor generalization to diverse unseen examples. We claim that an anomaly detector equipped with a meta-learning scheme alleviates the limitation by leading the model to an initialization point for better optimization. We evaluate the performance of our framework on two challenging datasets, UCF-Crime and ShanghaiTech. The experimental results demonstrate that our algorithm boosts the capability to localize unseen abnormal events in a weakly supervised setting. Besides the technical contributions, we perform the annotation of missing labels in the UCF-Crime dataset and make our task evaluated effectively.



### Class-Incremental Learning for Action Recognition in Videos
- **Arxiv ID**: http://arxiv.org/abs/2203.13611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13611v1)
- **Published**: 2022-03-25 12:15:49+00:00
- **Updated**: 2022-03-25 12:15:49+00:00
- **Authors**: Jaeyoo Park, Minsoo Kang, Bohyung Han
- **Comment**: 12 pages, ICCV 2021
- **Journal**: None
- **Summary**: We tackle catastrophic forgetting problem in the context of class-incremental learning for video recognition, which has not been explored actively despite the popularity of continual learning. Our framework addresses this challenging task by introducing time-channel importance maps and exploiting the importance maps for learning the representations of incoming examples via knowledge distillation. We also incorporate a regularization scheme in our objective function, which encourages individual features obtained from different time steps in a video to be uncorrelated and eventually improves accuracy by alleviating catastrophic forgetting. We evaluate the proposed approach on brand-new splits of class-incremental action recognition benchmarks constructed upon the UCF101, HMDB51, and Something-Something V2 datasets, and demonstrate the effectiveness of our algorithm in comparison to the existing continual learning methods that are originally designed for image data.



### Lightweight Graph Convolutional Networks with Topologically Consistent Magnitude Pruning
- **Arxiv ID**: http://arxiv.org/abs/2203.13616v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.13616v1)
- **Published**: 2022-03-25 12:34:11+00:00
- **Updated**: 2022-03-25 12:34:11+00:00
- **Authors**: Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: Graph convolution networks (GCNs) are currently mainstream in learning with irregular data. These models rely on message passing and attention mechanisms that capture context and node-to-node relationships. With multi-head attention, GCNs become highly accurate but oversized, and their deployment on cheap devices requires their pruning. However, pruning at high regimes usually leads to topologically inconsistent networks with weak generalization. In this paper, we devise a novel method for lightweight GCN design. Our proposed approach parses and selects subnetworks with the highest magnitudes while guaranteeing their topological consistency. The latter is obtained by selecting only accessible and co-accessible connections which actually contribute in the evaluation of the selected subnetworks. Experiments conducted on the challenging FPHA dataset show the substantial gain of our topologically consistent pruning method especially at very high pruning regimes.



### Give Me Your Attention: Dot-Product Attention Considered Harmful for Adversarial Patch Robustness
- **Arxiv ID**: http://arxiv.org/abs/2203.13639v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T07, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2203.13639v1)
- **Published**: 2022-03-25 13:26:27+00:00
- **Updated**: 2022-03-25 13:26:27+00:00
- **Authors**: Giulio Lovisotto, Nicole Finnie, Mauricio Munoz, Chaithanya Kumar Mummadi, Jan Hendrik Metzen
- **Comment**: to be published in IEEE/CVF Conference on Computer Vision and Pattern
  Recognition 2022, CVPR22
- **Journal**: None
- **Summary**: Neural architectures based on attention such as vision transformers are revolutionizing image recognition. Their main benefit is that attention allows reasoning about all parts of a scene jointly. In this paper, we show how the global reasoning of (scaled) dot-product attention can be the source of a major vulnerability when confronted with adversarial patch attacks. We provide a theoretical understanding of this vulnerability and relate it to an adversary's ability to misdirect the attention of all queries to a single key token under the control of the adversarial patch. We propose novel adversarial objectives for crafting adversarial patches which target this vulnerability explicitly. We show the effectiveness of the proposed patch attacks on popular image classification (ViTs and DeiTs) and object detection models (DETR). We find that adversarial patches occupying 0.5% of the input can lead to robust accuracies as low as 0% for ViT on ImageNet, and reduce the mAP of DETR on MS COCO to less than 3%.



### StretchBEV: Stretching Future Instance Prediction Spatially and Temporally
- **Arxiv ID**: http://arxiv.org/abs/2203.13641v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.13641v2)
- **Published**: 2022-03-25 13:28:44+00:00
- **Updated**: 2022-08-11 01:09:28+00:00
- **Authors**: Adil Kaan Akan, Fatma Güney
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: In self-driving, predicting future in terms of location and motion of all the agents around the vehicle is a crucial requirement for planning. Recently, a new joint formulation of perception and prediction has emerged by fusing rich sensory information perceived from multiple cameras into a compact bird's-eye view representation to perform prediction. However, the quality of future predictions degrades over time while extending to longer time horizons due to multiple plausible predictions. In this work, we address this inherent uncertainty in future predictions with a stochastic temporal model. Our model learns temporal dynamics in a latent space through stochastic residual updates at each time step. By sampling from a learned distribution at each time step, we obtain more diverse future predictions that are also more accurate compared to previous work, especially stretching both spatially further regions in the scene and temporally over longer time horizons. Despite separate processing of each time step, our model is still efficient through decoupling of the learning of dynamics and the generation of future predictions.



### MDsrv -- visual sharing and analysis of molecular dynamics simulations
- **Arxiv ID**: http://arxiv.org/abs/2203.13658v2
- **DOI**: 10.1093/nar/gkac398
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2203.13658v2)
- **Published**: 2022-03-25 14:08:24+00:00
- **Updated**: 2022-05-04 14:09:53+00:00
- **Authors**: Michelle Kampfrath, René Staritzbichler, Guillermo Pérez Hernández, Alexander S. Rose, Johanna K. S. Tiemann, Gerik Scheuermann, Daniel Wiegreffe, Peter W. Hildebrand
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: Molecular dynamics simulation is a proven technique for computing and visualizing the time-resolved motion of macromolecules at atomic resolution. The MDsrv is a tool that streams MD trajectories and displays them interactively in web browsers without requiring advanced skills, facilitating interactive exploration and collaborative visual analysis. We have now enhanced the MDsrv to further simplify the upload and sharing of MD trajectories and improve their online viewing and analysis. With the new instance, the MDsrv simplifies the creation of sessions, which allows the exchange of MD trajectories with preset representations and perspectives. An important innovation is that the MDsrv can now access and visualize trajectories from remote datasets, which greatly expands its applicability and use, as the data no longer needs to be accessible on a local server. In addition, initial analyses such as sequence or structure alignments, distance measurements, or RMSD calculations have been implemented, which optionally support visual analysis. Finally, the MDsrv now offers a faster and more efficient visualization of even large trajectories.



### Adjacent Context Coordination Network for Salient Object Detection in Optical Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2203.13664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13664v1)
- **Published**: 2022-03-25 14:14:55+00:00
- **Updated**: 2022-03-25 14:14:55+00:00
- **Authors**: Gongyang Li, Zhi Liu, Dan Zeng, Weisi Lin, Haibin Ling
- **Comment**: 13 pages, 7 figures, Accepted by IEEE Transactions on Cybernetics
  2022
- **Journal**: None
- **Summary**: Salient object detection (SOD) in optical remote sensing images (RSIs), or RSI-SOD, is an emerging topic in understanding optical RSIs. However, due to the difference between optical RSIs and natural scene images (NSIs), directly applying NSI-SOD methods to optical RSIs fails to achieve satisfactory results. In this paper, we propose a novel Adjacent Context Coordination Network (ACCoNet) to explore the coordination of adjacent features in an encoder-decoder architecture for RSI-SOD. Specifically, ACCoNet consists of three parts: an encoder, Adjacent Context Coordination Modules (ACCoMs), and a decoder. As the key component of ACCoNet, ACCoM activates the salient regions of output features of the encoder and transmits them to the decoder. ACCoM contains a local branch and two adjacent branches to coordinate the multi-level features simultaneously. The local branch highlights the salient regions in an adaptive way, while the adjacent branches introduce global information of adjacent levels to enhance salient regions. Additionally, to extend the capabilities of the classic decoder block (i.e., several cascaded convolutional layers), we extend it with two bifurcations and propose a Bifurcation-Aggregation Block to capture the contextual information in the decoder. Extensive experiments on two benchmark datasets demonstrate that the proposed ACCoNet outperforms 22 state-of-the-art methods under nine evaluation metrics, and runs up to 81 fps on a single NVIDIA Titan X GPU. The code and results of our method are available at https://github.com/MathLee/ACCoNet.



### Dense Continuous-Time Optical Flow from Events and Frames
- **Arxiv ID**: http://arxiv.org/abs/2203.13674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13674v1)
- **Published**: 2022-03-25 14:29:41+00:00
- **Updated**: 2022-03-25 14:29:41+00:00
- **Authors**: Mathias Gehrig, Manasi Muglikar, Davide Scaramuzza
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for estimating dense continuous-time optical flow. Traditional dense optical flow methods compute the pixel displacement between two images. Due to missing information, these approaches cannot recover the pixel trajectories in the blind time between two images. In this work, we show that it is possible to compute per-pixel, continuous-time optical flow by additionally using events from an event camera. Events provide temporally fine-grained information about movement in image space due to their asynchronous nature and microsecond response time. We leverage these benefits to predict pixel trajectories densely in continuous-time via parameterized B\'ezier curves. To achieve this, we introduce multiple innovations to build a neural network with strong inductive biases for this task: First, we build multiple sequential correlation volumes in time using event data. Second, we use B\'ezier curves to index these correlation volumes at multiple timestamps along the trajectory. Third, we use the retrieved correlation to update the B\'ezier curve representations iteratively. Our method can optionally include image pairs to boost performance further. The proposed approach outperforms existing image-based and event-based methods by 11.5 % lower EPE on DSEC-Flow. Finally, we introduce a novel synthetic dataset MultiFlow for pixel trajectory regression on which our method is currently the only successful approach.



### On the performance of preconditioned methods to solve \(L^p\)-norm phase unwrapping
- **Arxiv ID**: http://arxiv.org/abs/2203.13675v1
- **DOI**: 10.1007/978-3-030-98457-1_11
- **Categories**: **math.NA**, cs.CV, cs.NA, 65K10, 78-10
- **Links**: [PDF](http://arxiv.org/pdf/2203.13675v1)
- **Published**: 2022-03-25 14:29:46+00:00
- **Updated**: 2022-03-25 14:29:46+00:00
- **Authors**: Ricardo Legarda-Saenz, Carlos Brito-Loeza, Arturo Espinosa-Romero
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we analyze and evaluate suitable preconditioning techniques to improve the performance of the $L^p$-norm phase unwrapping method. We consider five preconditioning techniques commonly found in the literature, and analyze their performance with different sizes of wrapped-phase maps. Keywords.- Phase unwrapping, $L^p$-norm based method, Preconditioning techniques.



### ST-FL: Style Transfer Preprocessing in Federated Learning for COVID-19 Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.13680v1
- **DOI**: 10.1117/12.2611096
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.13680v1)
- **Published**: 2022-03-25 14:33:02+00:00
- **Updated**: 2022-03-25 14:33:02+00:00
- **Authors**: Antonios Georgiadis, Varun Babbar, Fran Silavong, Sean Moran, Rob Otter
- **Comment**: 5 pages, 1 figure, full version (15 pages, 13 figures) to be
  published in SPIE: Medical Imaging 2022 Proceedings
- **Journal**: None
- **Summary**: Chest Computational Tomography (CT) scans present low cost, speed and objectivity for COVID-19 diagnosis and deep learning methods have shown great promise in assisting the analysis and interpretation of these images. Most hospitals or countries can train their own models using in-house data, however empirical evidence shows that those models perform poorly when tested on new unseen cases, surfacing the need for coordinated global collaboration. Due to privacy regulations, medical data sharing between hospitals and nations is extremely difficult. We propose a GAN-augmented federated learning model, dubbed ST-FL (Style Transfer Federated Learning), for COVID-19 image segmentation. Federated learning (FL) permits a centralised model to be learned in a secure manner from heterogeneous datasets located in disparate private data silos. We demonstrate that the widely varying data quality on FL client nodes leads to a sub-optimal centralised FL model for COVID-19 chest CT image segmentation. ST-FL is a novel FL framework that is robust in the face of highly variable data quality at client nodes. The robustness is achieved by a denoising CycleGAN model at each client of the federation that maps arbitrary quality images into the same target quality, counteracting the severe data variability evident in real-world FL use-cases. Each client is provided with the target style, which is the same for all clients, and trains their own denoiser. Our qualitative and quantitative results suggest that this FL model performs comparably to, and in some cases better than, a model that has centralised access to all the training data.



### The TerraByte Client: providing access to terabytes of plant data
- **Arxiv ID**: http://arxiv.org/abs/2203.13691v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.DB, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.13691v1)
- **Published**: 2022-03-25 14:55:25+00:00
- **Updated**: 2022-03-25 14:55:25+00:00
- **Authors**: Michael A. Beck, Christopher P. Bidinosti, Christopher J. Henry, Manisha Ajmani
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we demonstrate the TerraByte Client, a software to download user-defined plant datasets from a data portal hosted at Compute Canada. To that end the client offers two key functionalities: (1) It allows the user to get an overview on what data is available and a quick way to visually check samples of that data. For this the client receives the results of queries to a database and displays the number of images that fulfill the search criteria. Furthermore, a sample can be downloaded within seconds to confirm that the data suits the user's needs. (2) The user can then download the specified data to their own drive. This data is prepared into chunks server-side and sent to the user's end-system, where it is automatically extracted into individual files. The first chunks of data are available for inspection after a brief waiting period of a minute or less depending on available bandwidth and type of data. The TerraByte Client has a full graphical user interface for easy usage and uses end-to-end encryption. The user interface is built on top of a low-level client. This architecture in combination of offering the client program open-source makes it possible for the user to develop their own user interface or use the client's functionality directly. An example for direct usage could be to download specific data on demand within a larger application, such as training machine learning models.



### Implicit Neural Representations for Variable Length Human Motion Generation
- **Arxiv ID**: http://arxiv.org/abs/2203.13694v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13694v2)
- **Published**: 2022-03-25 15:00:38+00:00
- **Updated**: 2022-07-15 10:26:37+00:00
- **Authors**: Pablo Cervantes, Yusuke Sekikawa, Ikuro Sato, Koichi Shinoda
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: We propose an action-conditional human motion generation method using variational implicit neural representations (INR). The variational formalism enables action-conditional distributions of INRs, from which one can easily sample representations to generate novel human motion sequences. Our method offers variable-length sequence generation by construction because a part of INR is optimized for a whole sequence of arbitrary length with temporal embeddings. In contrast, previous works reported difficulties with modeling variable-length sequences. We confirm that our method with a Transformer decoder outperforms all relevant methods on HumanAct12, NTU-RGBD, and UESTC datasets in terms of realism and diversity of generated motions. Surprisingly, even our method with an MLP decoder consistently outperforms the state-of-the-art Transformer-based auto-encoder. In particular, we show that variable-length motions generated by our method are better than fixed-length motions generated by the state-of-the-art method in terms of realism and diversity. Code at https://github.com/PACerv/ImplicitMotion.



### Unsupervised Image Deraining: Optimization Model Driven Deep CNN
- **Arxiv ID**: http://arxiv.org/abs/2203.13699v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.13699v1)
- **Published**: 2022-03-25 15:13:52+00:00
- **Updated**: 2022-03-25 15:13:52+00:00
- **Authors**: Changfeng Yu, Yi Chang, Yi Li, Xile Zhao, Luxin Yan
- **Comment**: Accept to 2021ACMMM
- **Journal**: None
- **Summary**: The deep convolutional neural network has achieved significant progress for single image rain streak removal. However, most of the data-driven learning methods are full-supervised or semi-supervised, unexpectedly suffering from significant performance drops when dealing with real rain. These data-driven learning methods are representative yet generalize poor for real rain. The opposite holds true for the model-driven unsupervised optimization methods. To overcome these problems, we propose a unified unsupervised learning framework which inherits the generalization and representation merits for real rain removal. Specifically, we first discover a simple yet important domain knowledge that directional rain streak is anisotropic while the natural clean image is isotropic, and formulate the structural discrepancy into the energy function of the optimization model. Consequently, we design an optimization model-driven deep CNN in which the unsupervised loss function of the optimization model is enforced on the proposed network for better generalization. In addition, the architecture of the network mimics the main role of the optimization models with better feature representation. On one hand, we take advantage of the deep network to improve the representation. On the other hand, we utilize the unsupervised loss of the optimization model for better generalization. Overall, the unsupervised learning framework achieves good generalization and representation: unsupervised training (loss) with only a few real rainy images (input) and physical meaning network (architecture). Extensive experiments on synthetic and real-world rain datasets show the superiority of the proposed method.



### Clustering Aided Weakly Supervised Training to Detect Anomalous Events in Surveillance Videos
- **Arxiv ID**: http://arxiv.org/abs/2203.13704v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13704v1)
- **Published**: 2022-03-25 15:18:19+00:00
- **Updated**: 2022-03-25 15:18:19+00:00
- **Authors**: Muhammad Zaigham Zaheer, Arif Mahmood, Marcella Astrid, Seung-Ik Lee
- **Comment**: This work has been submitted to the IEEE Transactions on Neural
  Networks and Learning Systems (TNNLS) for possible publication. Copyright may
  be transferred without notice, after which this version may no longer be
  accessible
- **Journal**: None
- **Summary**: Formulating learning systems for the detection of real-world anomalous events using only video-level labels is a challenging task mainly due to the presence of noisy labels as well as the rare occurrence of anomalous events in the training data. We propose a weakly supervised anomaly detection system which has multiple contributions including a random batch selection mechanism to reduce inter-batch correlation and a normalcy suppression block which learns to minimize anomaly scores over normal regions of a video by utilizing the overall information available in a training batch. In addition, a clustering loss block is proposed to mitigate the label noise and to improve the representation learning for the anomalous and normal regions. This block encourages the backbone network to produce two distinct feature clusters representing normal and anomalous events. Extensive analysis of the proposed approach is provided using three popular anomaly detection datasets including UCF-Crime, ShanghaiTech, and UCSD Ped2. The experiments demonstrate a superior anomaly detection capability of our approach.



### Searching for Network Width with Bilaterally Coupled Network
- **Arxiv ID**: http://arxiv.org/abs/2203.13714v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.13714v1)
- **Published**: 2022-03-25 15:32:46+00:00
- **Updated**: 2022-03-25 15:32:46+00:00
- **Authors**: Xiu Su, Shan You, Jiyang Xie, Fei Wang, Chen Qian, Changshui Zhang, Chang Xu
- **Comment**: Extended version of CVPR 2021 paper BCNet arXiv:2105.10533
- **Journal**: None
- **Summary**: Searching for a more compact network width recently serves as an effective way of channel pruning for the deployment of convolutional neural networks (CNNs) under hardware constraints. To fulfill the searching, a one-shot supernet is usually leveraged to efficiently evaluate the performance \wrt~different network widths. However, current methods mainly follow a \textit{unilaterally augmented} (UA) principle for the evaluation of each width, which induces the training unfairness of channels in supernet. In this paper, we introduce a new supernet called Bilaterally Coupled Network (BCNet) to address this issue. In BCNet, each channel is fairly trained and responsible for the same amount of network widths, thus each network width can be evaluated more accurately. Besides, we propose to reduce the redundant search space and present the BCNetV2 as the enhanced supernet to ensure rigorous training fairness over channels. Furthermore, we leverage a stochastic complementary strategy for training the BCNet, and propose a prior initial population sampling method to boost the performance of the evolutionary search. We also propose the first open-source width benchmark on macro structures named Channel-Bench-Macro for the better comparison of width search algorithms. Extensive experiments on benchmark CIFAR-10 and ImageNet datasets indicate that our method can achieve state-of-the-art or competing performance over other baseline methods. Moreover, our method turns out to further boost the performance of NAS models by refining their network widths. For example, with the same FLOPs budget, our obtained EfficientNet-B0 achieves 77.53\% Top-1 accuracy on ImageNet dataset, surpassing the performance of original setting by 0.65\%.



### Stabilizing Adversarially Learned One-Class Novelty Detection Using Pseudo Anomalies
- **Arxiv ID**: http://arxiv.org/abs/2203.13716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13716v1)
- **Published**: 2022-03-25 15:37:52+00:00
- **Updated**: 2022-03-25 15:37:52+00:00
- **Authors**: Muhammad Zaigham Zaheer, Jin Ha Lee, Arif Mahmood, Marcella Astrid, Seung-Ik Lee
- **Comment**: This work has been submitted to the IEEE Transactions on Image
  Processing for possible publication. Copyright may be transferred without
  notice, after which this version may no longer be accessible
- **Journal**: None
- **Summary**: Recently, anomaly scores have been formulated using reconstruction loss of the adversarially learned generators and/or classification loss of discriminators. Unavailability of anomaly examples in the training data makes optimization of such networks challenging. Attributed to the adversarial training, performance of such models fluctuates drastically with each training step, making it difficult to halt the training at an optimal point. In the current study, we propose a robust anomaly detection framework that overcomes such instability by transforming the fundamental role of the discriminator from identifying real vs. fake data to distinguishing good vs. bad quality reconstructions. For this purpose, we propose a method that utilizes the current state as well as an old state of the same generator to create good and bad quality reconstruction examples. The discriminator is trained on these examples to detect the subtle distortions that are often present in the reconstructions of anomalous data. In addition, we propose an efficient generic criterion to stop the training of our model, ensuring elevated performance. Extensive experiments performed on six datasets across multiple domains including image and video based anomaly detection, medical diagnosis, and network security, have demonstrated excellent performance of our approach.



### Digital Fingerprinting of Microstructures
- **Arxiv ID**: http://arxiv.org/abs/2203.13718v1
- **DOI**: None
- **Categories**: **cs.CV**, cond-mat.mtrl-sci, physics.comp-ph
- **Links**: [PDF](http://arxiv.org/pdf/2203.13718v1)
- **Published**: 2022-03-25 15:40:44+00:00
- **Updated**: 2022-03-25 15:40:44+00:00
- **Authors**: Michael D. White, Alexander Tarakanov, Christopher P. Race, Philip J. Withers, Kody J. H. Law
- **Comment**: None
- **Journal**: None
- **Summary**: Finding efficient means of fingerprinting microstructural information is a critical step towards harnessing data-centric machine learning approaches. A statistical framework is systematically developed for compressed characterisation of a population of images, which includes some classical computer vision methods as special cases. The focus is on materials microstructure. The ultimate purpose is to rapidly fingerprint sample images in the context of various high-throughput design/make/test scenarios. This includes, but is not limited to, quantification of the disparity between microstructures for quality control, classifying microstructures, predicting materials properties from image data and identifying potential processing routes to engineer new materials with specific properties. Here, we consider microstructure classification and utilise the resulting features over a range of related machine learning tasks, namely supervised, semi-supervised, and unsupervised learning.   The approach is applied to two distinct datasets to illustrate various aspects and some recommendations are made based on the findings. In particular, methods that leverage transfer learning with convolutional neural networks (CNNs), pretrained on the ImageNet dataset, are generally shown to outperform other methods. Additionally, dimensionality reduction of these CNN-based fingerprints is shown to have negligible impact on classification accuracy for the supervised learning approaches considered. In situations where there is a large dataset with only a handful of images labelled, graph-based label propagation to unlabelled data is shown to be favourable over discarding unlabelled data and performing supervised learning. In particular, label propagation by Poisson learning is shown to be highly effective at low label rates.



### Salt Detection Using Segmentation of Seismic Image
- **Arxiv ID**: http://arxiv.org/abs/2203.13721v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.13721v1)
- **Published**: 2022-03-25 15:45:05+00:00
- **Updated**: 2022-03-25 15:45:05+00:00
- **Authors**: Mrinmoy Sarkar
- **Comment**: None
- **Journal**: None
- **Summary**: In this project, a state-of-the-art deep convolution neural network (DCNN) is presented to segment seismic images for salt detection below the earth's surface. Detection of salt location is very important for starting mining. Hence, a seismic image is used to detect the exact salt location under the earth's surface. However, precisely detecting the exact location of salt deposits is difficult. Therefore, professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective, highly variable renderings. Hence, to create the most accurate seismic images and 3D renderings, we need a robust algorithm that automatically and accurately identifies if a surface target is a salt or not. Since the performance of DCNN is well-known and well-established for object recognition in images, DCNN is a very good choice for this particular problem and being successfully applied to a dataset of seismic images in which each pixel is labeled as salt or not. The result of this algorithm is promising.



### FReSCO: Flow Reconstruction and Segmentation for low latency Cardiac Output monitoring using deep artifact suppression and segmentation
- **Arxiv ID**: http://arxiv.org/abs/2203.13729v1
- **DOI**: 10.1002/mrm.29374
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13729v1)
- **Published**: 2022-03-25 16:02:36+00:00
- **Updated**: 2022-03-25 16:02:36+00:00
- **Authors**: Olivier Jaubert, Javier Montalt-Tordera, James Brown, Daniel Knight, Simon Arridge, Jennifer Steeden, Vivek Muthurangu
- **Comment**: 10 pages, 5 Figures, 4 Supporting Information Figures
- **Journal**: Magn Reson Med . 2022 Nov;88(5):2179-2189
- **Summary**: Purpose: Real-time monitoring of cardiac output (CO) requires low latency reconstruction and segmentation of real-time phase contrast MR (PCMR), which has previously been difficult to perform. Here we propose a deep learning framework for 'Flow Reconstruction and Segmentation for low latency Cardiac Output monitoring' (FReSCO).   Methods: Deep artifact suppression and segmentation U-Nets were independently trained. Breath hold spiral PCMR data (n=516) was synthetically undersampled using a variable density spiral sampling pattern and gridded to create aliased data for training of the artifact suppression U-net. A subset of the data (n=96) was segmented and used to train the segmentation U-net. Real-time spiral PCMR was prospectively acquired and then reconstructed and segmented using the trained models (FReSCO) at low latency at the scanner in 10 healthy subjects during rest, exercise and recovery periods. CO obtained via FReSCO was compared to a reference rest CO and rest and exercise Compressed Sensing (CS) CO.   Results: FReSCO was demonstrated prospectively at the scanner. Beat-to-beat heartrate, stroke volume and CO could be visualized with a mean latency of 622ms. No significant differences were noted when compared to reference at rest (Bias = -0.21+-0.50 L/min, p=0.246) or CS at peak exercise (Bias=0.12+-0.48 L/min, p=0.458).   Conclusion: FReSCO was successfully demonstrated for real-time monitoring of CO during exercise and could provide a convenient tool for assessment of the hemodynamic response to a range of stressors.



### Efficient-VDVAE: Less is more
- **Arxiv ID**: http://arxiv.org/abs/2203.13751v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.13751v2)
- **Published**: 2022-03-25 16:29:46+00:00
- **Updated**: 2022-04-28 15:26:12+00:00
- **Authors**: Louay Hazami, Rayhane Mama, Ragavan Thurairatnam
- **Comment**: Added more information about C1 model configuration, potential
  negative impact, and fixed some typos
- **Journal**: None
- **Summary**: Hierarchical VAEs have emerged in recent years as a reliable option for maximum likelihood estimation. However, instability issues and demanding computational requirements have hindered research progress in the area. We present simple modifications to the Very Deep VAE to make it converge up to $2.6\times$ faster, save up to $20\times$ in memory load and improve stability during training. Despite these changes, our models achieve comparable or better negative log-likelihood performance than current state-of-the-art models on all $7$ commonly used image datasets we evaluated on. We also make an argument against using 5-bit benchmarks as a way to measure hierarchical VAE's performance due to undesirable biases caused by the 5-bit quantization. Additionally, we empirically demonstrate that roughly $3\%$ of the hierarchical VAE's latent space dimensions is sufficient to encode most of the image information, without loss of performance, opening up the doors to efficiently leverage the hierarchical VAEs' latent space in downstream tasks. We release our source code and models at https://github.com/Rayhane-mamah/Efficient-VDVAE .



### Analysis of the use of color and its emotional relationship in visual creations based on experiences during the context of the COVID-19 pandemic
- **Arxiv ID**: http://arxiv.org/abs/2203.13770v1
- **DOI**: 10.3390/su142012989
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2203.13770v1)
- **Published**: 2022-03-25 16:51:43+00:00
- **Updated**: 2022-03-25 16:51:43+00:00
- **Authors**: César González-Martín, Miguel Carrasco, Germán Oviedo
- **Comment**: None
- **Journal**: None
- **Summary**: Color is a complex communicative element that helps us understand and evaluate our environment. At the level of artistic creation, this component influences both the formal aspects of the composition and the symbolic weight, directly affecting the construction and transmission of the message that you want to communicate, creating a specific emotional reaction. During the COVID-19 pandemic, people generated countless images transmitting this event's subjective experiences. Using the repository of images created in the Instagram account CAM (The COVID Art Museum), we propose a methodology to understand the use of color and its emotional relationship in this context. The process considers two stages in parallel that are then combined. First, emotions are extracted and classified from the CAM dataset images through a convolutional neural network. Second, we extract the colors and their harmonies through a clustering process. Once both processes are completed, we combine the results generating an expanded discussion on the usage of color, harmonies, and emotion. The results indicate that warm colors are prevalent in the sample, with a preference for analog compositions over complementary ones. The relationship between emotions and these compositions shows a trend in positive emotions, reinforced by the results of the algorithm a priori and the emotional relationship analysis of the attributes of color (hue, chroma, and lighting).



### Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2203.13777v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.13777v1)
- **Published**: 2022-03-25 16:59:08+00:00
- **Updated**: 2022-03-25 16:59:08+00:00
- **Authors**: Tianpei Gu, Guangyi Chen, Junlong Li, Chunze Lin, Yongming Rao, Jie Zhou, Jiwen Lu
- **Comment**: Accepted to CVPR2022
- **Journal**: None
- **Summary**: Human behavior has the nature of indeterminacy, which requires the pedestrian trajectory prediction system to model the multi-modality of future motion states. Unlike existing stochastic trajectory prediction methods which usually use a latent variable to represent multi-modality, we explicitly simulate the process of human motion variation from indeterminate to determinate. In this paper, we present a new framework to formulate the trajectory prediction task as a reverse process of motion indeterminacy diffusion (MID), in which we progressively discard indeterminacy from all the walkable areas until reaching the desired trajectory. This process is learned with a parameterized Markov chain conditioned by the observed trajectories. We can adjust the length of the chain to control the degree of indeterminacy and balance the diversity and determinacy of the predictions. Specifically, we encode the history behavior information and the social interactions as a state embedding and devise a Transformer-based diffusion model to capture the temporal dependencies of trajectories. Extensive experiments on the human trajectory prediction benchmarks including the Stanford Drone and ETH/UCY datasets demonstrate the superiority of our method. Code is available at https://github.com/gutianpei/MID.



### Visual-based Safe Landing for UAVs in Populated Areas: Real-time Validation in Virtual Environments
- **Arxiv ID**: http://arxiv.org/abs/2203.13792v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.13792v1)
- **Published**: 2022-03-25 17:22:24+00:00
- **Updated**: 2022-03-25 17:22:24+00:00
- **Authors**: Hector Tovanche-Picon, Javier Gonzalez-Trejo, Angel Flores-Abad, Diego Mercado-Ravell
- **Comment**: None
- **Journal**: None
- **Summary**: Safe autonomous landing for Unmanned Aerial Vehicles (UAVs) in populated areas is a crucial aspect for successful urban deployment, particularly in emergency landing situations. Nonetheless, validating autonomous landing in real scenarios is a challenging task involving a high risk of injuring people. In this work, we propose a framework for real-time safe and thorough evaluation of vision-based autonomous landing in populated scenarios, using photo-realistic virtual environments. We propose to use the Unreal graphics engine coupled with the AirSim plugin for drone's simulation, and evaluate autonomous landing strategies based on visual detection of Safe Landing Zones (SLZ) in populated scenarios. Then, we study two different criteria for selecting the "best" SLZ, and evaluate them during autonomous landing of a virtual drone in different scenarios and conditions, under different distributions of people in urban scenes, including moving people. We evaluate different metrics to quantify the performance of the landing strategies, establishing a baseline for comparison with future works in this challenging task, and analyze them through an important number of randomized iterations. The study suggests that the use of the autonomous landing algorithms considerably helps to prevent accidents involving humans, which may allow to unleash the full potential of drones in urban environments near to people.



### Continuous Dynamic-NeRF: Spline-NeRF
- **Arxiv ID**: http://arxiv.org/abs/2203.13800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13800v1)
- **Published**: 2022-03-25 17:39:49+00:00
- **Updated**: 2022-03-25 17:39:49+00:00
- **Authors**: Julian Knodt
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of reconstructing continuous functions over time is important for problems such as reconstructing moving scenes, and interpolating between time steps. Previous approaches that use deep-learning rely on regularization to ensure that reconstructions are approximately continuous, which works well on short sequences. As sequence length grows, though, it becomes more difficult to regularize, and it becomes less feasible to learn only through regularization. We propose a new architecture for function reconstruction based on classical Bezier splines, which ensures $C^0$ and $C^1$-continuity, where $C^0$ continuity is that $\forall c:\lim\limits_{x\to c} f(x)   = f(c)$, or more intuitively that there are no breaks at any point in the function. In order to demonstrate our architecture, we reconstruct dynamic scenes using Neural Radiance Fields, but hope it is clear that our approach is general and can be applied to a variety of problems. We recover a Bezier spline $B(\beta, t\in[0,1])$, parametrized by the control points $\beta$. Using Bezier splines ensures reconstructions have $C^0$ and $C^1$ continuity, allowing for guaranteed interpolation over time. We reconstruct $\beta$ with a multi-layer perceptron (MLP), blending machine learning with classical animation techniques. All code is available at https://github.com/JulianKnodt/nerf_atlas, and datasets are from prior work.



### Playing Lottery Tickets in Style Transfer Models
- **Arxiv ID**: http://arxiv.org/abs/2203.13802v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.13802v2)
- **Published**: 2022-03-25 17:43:18+00:00
- **Updated**: 2022-04-10 09:07:37+00:00
- **Authors**: Meihao Kong, Jing Huo, Wenbin Li, Jing Wu, Yu-Kun Lai, Yang Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Style transfer has achieved great success and attracted a wide range of attention from both academic and industrial communities due to its flexible application scenarios. However, the dependence on a pretty large VGG-based autoencoder leads to existing style transfer models having high parameter complexities, which limits their applications on resource-constrained devices. Compared with many other tasks, the compression of style transfer models has been less explored. Recently, the lottery ticket hypothesis (LTH) has shown great potential in finding extremely sparse matching subnetworks which can achieve on par or even better performance than the original full networks when trained in isolation. In this work, we for the first time perform an empirical study to verify whether such trainable matching subnetworks also exist in style transfer models. Specifically, we take two most popular style transfer models, i.e., AdaIN and SANet, as the main testbeds, which represent global and local transformation based style transfer methods respectively. We carry out extensive experiments and comprehensive analysis, and draw the following conclusions. (1) Compared with fixing the VGG encoder, style transfer models can benefit more from training the whole network together. (2) Using iterative magnitude pruning, we find the matching subnetworks at 89.2% sparsity in AdaIN and 73.7% sparsity in SANet, which demonstrates that style transfer models can play lottery tickets too. (3) The feature transformation module should also be pruned to obtain a much sparser model without affecting the existence and quality of the matching subnetworks. (4) Besides AdaIN and SANet, other models such as LST, MANet, AdaAttN and MCCNet can also play lottery tickets, which shows that LTH can be generalized to various style transfer models.



### hate-alert@DravidianLangTech-ACL2022: Ensembling Multi-Modalities for Tamil TrollMeme Classification
- **Arxiv ID**: http://arxiv.org/abs/2204.12587v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2204.12587v1)
- **Published**: 2022-03-25 17:53:39+00:00
- **Updated**: 2022-03-25 17:53:39+00:00
- **Authors**: Mithun Das, Somnath Banerjee, Animesh Mukherjee
- **Comment**: Accepted at ACL 2022 DravidianLangTech Workshop
- **Journal**: None
- **Summary**: Social media platforms often act as breeding grounds for various forms of trolling or malicious content targeting users or communities. One way of trolling users is by creating memes, which in most cases unites an image with a short piece of text embedded on top of it. The situation is more complex for multilingual(e.g., Tamil) memes due to the lack of benchmark datasets and models. We explore several models to detect Troll memes in Tamil based on the shared task, "Troll Meme Classification in DravidianLangTech2022" at ACL-2022. We observe while the text-based model MURIL performs better for Non-troll meme classification, the image-based model VGG16 performs better for Troll-meme classification. Further fusing these two modalities help us achieve stable outcomes in both classes. Our fusion model achieved a 0.561 weighted average F1 score and ranked second in this task.



### Spatially Multi-conditional Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2203.13812v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13812v2)
- **Published**: 2022-03-25 17:57:13+00:00
- **Updated**: 2022-07-14 09:54:52+00:00
- **Authors**: Ritika Chakraborty, Nikola Popovic, Danda Pani Paudel, Thomas Probst, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: In most scenarios, conditional image generation can be thought of as an inversion of the image understanding process. Since generic image understanding involves solving multiple tasks, it is natural to aim at generating images via multi-conditioning. However, multi-conditional image generation is a very challenging problem due to the heterogeneity and the sparsity of the (in practice) available conditioning labels. In this work, we propose a novel neural architecture to address the problem of heterogeneity and sparsity of the spatially multi-conditional labels. Our choice of spatial conditioning, such as by semantics and depth, is driven by the promise it holds for better control of the image generation process. The proposed method uses a transformer-like architecture operating pixel-wise, which receives the available labels as input tokens to merge them in a learned homogeneous space of labels. The merged labels are then used for image generation via conditional generative adversarial training. In this process, the sparsity of the labels is handled by simply dropping the input tokens corresponding to the missing labels at the desired locations, thanks to the proposed pixel-wise operating architecture. Our experiments on three benchmark datasets demonstrate the clear superiority of our method over the state-of-the-art and compared baselines. The source code will be made publicly available.



### Versatile Multi-Modal Pre-Training for Human-Centric Perception
- **Arxiv ID**: http://arxiv.org/abs/2203.13815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13815v1)
- **Published**: 2022-03-25 17:58:29+00:00
- **Updated**: 2022-03-25 17:58:29+00:00
- **Authors**: Fangzhou Hong, Liang Pan, Zhongang Cai, Ziwei Liu
- **Comment**: CVPR 2022; Project Page
  https://hongfz16.github.io/projects/HCMoCo.html; Codes available at
  https://github.com/hongfz16/HCMoCo
- **Journal**: None
- **Summary**: Human-centric perception plays a vital role in vision and graphics. But their data annotations are prohibitively expensive. Therefore, it is desirable to have a versatile pre-train model that serves as a foundation for data-efficient downstream tasks transfer. To this end, we propose the Human-Centric Multi-Modal Contrastive Learning framework HCMoCo that leverages the multi-modal nature of human data (e.g. RGB, depth, 2D keypoints) for effective representation learning. The objective comes with two main challenges: dense pre-train for multi-modality data, efficient usage of sparse human priors. To tackle the challenges, we design the novel Dense Intra-sample Contrastive Learning and Sparse Structure-aware Contrastive Learning targets by hierarchically learning a modal-invariant latent space featured with continuous and ordinal feature distribution and structure-aware semantic consistency. HCMoCo provides pre-train for different modalities by combining heterogeneous datasets, which allows efficient usage of existing task-specific human data. Extensive experiments on four downstream tasks of different modalities demonstrate the effectiveness of HCMoCo, especially under data-efficient settings (7.16% and 12% improvement on DensePose Estimation and Human Parsing). Moreover, we demonstrate the versatility of HCMoCo by exploring cross-modality supervision and missing-modality inference, validating its strong ability in cross-modal association and reasoning.



### AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling
- **Arxiv ID**: http://arxiv.org/abs/2203.13817v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2203.13817v1)
- **Published**: 2022-03-25 17:59:59+00:00
- **Updated**: 2022-03-25 17:59:59+00:00
- **Authors**: Ziqian Bai, Timur Bagautdinov, Javier Romero, Michael Zollhöfer, Ping Tan, Shunsuke Saito
- **Comment**: Project page: https://zqbai-jeremy.github.io/autoavatar
- **Journal**: None
- **Summary**: Neural fields such as implicit surfaces have recently enabled avatar modeling from raw scans without explicit temporal correspondences. In this work, we exploit autoregressive modeling to further extend this notion to capture dynamic effects, such as soft-tissue deformations. Although autoregressive models are naturally capable of handling dynamics, it is non-trivial to apply them to implicit representations, as explicit state decoding is infeasible due to prohibitive memory requirements. In this work, for the first time, we enable autoregressive modeling of implicit avatars. To reduce the memory bottleneck and efficiently model dynamic implicit surfaces, we introduce the notion of articulated observer points, which relate implicit states to the explicit surface of a parametric human body model. We demonstrate that encoding implicit surfaces as a set of height fields defined on articulated observer points leads to significantly better generalization compared to a latent representation. The experiments show that our approach outperforms the state of the art, achieving plausible dynamic deformations even for unseen motions. https://zqbai-jeremy.github.io/autoavatar



### A Stitch in Time Saves Nine: A Train-Time Regularizing Loss for Improved Neural Network Calibration
- **Arxiv ID**: http://arxiv.org/abs/2203.13834v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.13834v1)
- **Published**: 2022-03-25 18:02:13+00:00
- **Updated**: 2022-03-25 18:02:13+00:00
- **Authors**: Ramya Hebbalaguppe, Jatin Prakash, Neelabh Madan, Chetan Arora
- **Comment**: Accepted in IEEE Computer Vision and Pattern Recognition 2022
- **Journal**: None
- **Summary**: Deep Neural Networks ( DNN s) are known to make overconfident mistakes, which makes their use problematic in safety-critical applications. State-of-the-art ( SOTA ) calibration techniques improve on the confidence of predicted labels alone and leave the confidence of non-max classes (e.g. top-2, top-5) uncalibrated. Such calibration is not suitable for label refinement using post-processing. Further, most SOTA techniques learn a few hyper-parameters post-hoc, leaving out the scope for image, or pixel specific calibration. This makes them unsuitable for calibration under domain shift, or for dense prediction tasks like semantic segmentation. In this paper, we argue for intervening at the train time itself, so as to directly produce calibrated DNN models. We propose a novel auxiliary loss function: Multi-class Difference in Confidence and Accuracy ( MDCA ), to achieve the same MDCA can be used in conjunction with other application/task-specific loss functions. We show that training with MDCA leads to better-calibrated models in terms of Expected Calibration Error ( ECE ), and Static Calibration Error ( SCE ) on image classification, and segmentation tasks. We report ECE ( SCE ) score of 0.72 (1.60) on the CIFAR 100 dataset, in comparison to 1.90 (1.71) by the SOTA. Under domain shift, a ResNet-18 model trained on PACS dataset using MDCA gives an average ECE ( SCE ) score of 19.7 (9.7) across all domains, compared to 24.2 (11.8) by the SOTA. For the segmentation task, we report a 2X reduction in calibration error on PASCAL - VOC dataset in comparison to Focal Loss. Finally, MDCA training improves calibration even on imbalanced data, and for natural language classification tasks. We have released the code here: code is available at https://github.com/mdca-loss



### Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas
- **Arxiv ID**: http://arxiv.org/abs/2203.13838v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2203.13838v1)
- **Published**: 2022-03-25 18:06:14+00:00
- **Updated**: 2022-03-25 18:06:14+00:00
- **Authors**: Raphael Schumann, Stefan Riezler
- **Comment**: accepted at ACL 2022
- **Journal**: None
- **Summary**: Vision and language navigation (VLN) is a challenging visually-grounded language understanding task. Given a natural language navigation instruction, a visual agent interacts with a graph-based environment equipped with panorama images and tries to follow the described route. Most prior work has been conducted in indoor scenarios where best results were obtained for navigation on routes that are similar to the training routes, with sharp drops in performance when testing on unseen environments. We focus on VLN in outdoor scenarios and find that in contrast to indoor VLN, most of the gain in outdoor VLN on unseen data is due to features like junction type embedding or heading delta that are specific to the respective environment graph, while image information plays a very minor role in generalizing VLN to unseen outdoor areas. These findings show a bias to specifics of graph representations of urban environments, demanding that VLN tasks grow in scale and diversity of geographical environments.



### Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets
- **Arxiv ID**: http://arxiv.org/abs/2203.13856v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.13856v1)
- **Published**: 2022-03-25 18:42:20+00:00
- **Updated**: 2022-03-25 18:42:20+00:00
- **Authors**: Guilherme C. Oliveira, Gustavo H. Rosa, Daniel C. G. Pedronette, João P. Papa, Himeesh Kumar, Leandro A. Passos, Dinesh Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been proposed for the assessment and classification of medical images. However, many medical image databases with appropriately labeled and annotated images are small and imbalanced, and thus unsuitable to train and validate such models. The option is to generate synthetic images and one successful technique has been patented which limits its use for others. We have developed a free-access, alternate method for generating synthetic high-resolution images using Generative Adversarial Networks (GAN) for data augmentation and showed their effectiveness using eye-fundus images for Age-Related Macular Degeneration (AMD) identification. Ten different GAN architectures were compared to generate synthetic eye-fundus images with and without AMD. Data from three public databases were evaluated using the Fr\'echet Inception Distance (FID), two clinical experts and deep-learning classification. The results show that StyleGAN2 reached the lowest FID (166.17), and clinicians could not accurately differentiate between real and synthetic images. ResNet-18 architecture obtained the best performance with 85% accuracy and outperformed the two experts in detecting AMD fundus images, whose average accuracy was 77.5%. These results are similar to a recently patented method, and will provide an alternative to generating high-quality synthetic medical images. Free access has been provided to the entire method to facilitate the further development of this field.



### Deep Multi-modal Fusion of Image and Non-image Data in Disease Diagnosis and Prognosis: A Review
- **Arxiv ID**: http://arxiv.org/abs/2203.15588v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.15588v3)
- **Published**: 2022-03-25 18:50:03+00:00
- **Updated**: 2023-01-26 22:44:06+00:00
- **Authors**: Can Cui, Haichun Yang, Yaohong Wang, Shilin Zhao, Zuhayr Asad, Lori A. Coburn, Keith T. Wilson, Bennett A. Landman, Yuankai Huo
- **Comment**: None
- **Journal**: None
- **Summary**: The rapid development of diagnostic technologies in healthcare is leading to higher requirements for physicians to handle and integrate the heterogeneous, yet complementary data that are produced during routine practice. For instance, the personalized diagnosis and treatment planning for a single cancer patient relies on the various images (e.g., radiological, pathological, and camera images) and non-image data (e.g., clinical data and genomic data). However, such decision-making procedures can be subjective, qualitative, and have large inter-subject variabilities. With the recent advances in multi-modal deep learning technologies, an increasingly large number of efforts have been devoted to a key question: how do we extract and aggregate multi-modal information to ultimately provide more objective, quantitative computer-aided clinical decision making? This paper reviews the recent studies on dealing with such a question. Briefly, this review will include the (1) overview of current multi-modal learning workflows, (2) summarization of multi-modal fusion methods, (3) discussion of the performance, (4) applications in disease diagnosis and prognosis, and (5) challenges and future directions.



### TimeReplayer: Unlocking the Potential of Event Cameras for Video Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2203.13859v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.13859v1)
- **Published**: 2022-03-25 18:57:42+00:00
- **Updated**: 2022-03-25 18:57:42+00:00
- **Authors**: Weihua He, Kaichao You, Zhendong Qiao, Xu Jia, Ziyang Zhang, Wenhui Wang, Huchuan Lu, Yaoyuan Wang, Jianxing Liao
- **Comment**: Accepted to CVPR 2022, project page
  https://sites.google.com/view/timereplayer/
- **Journal**: None
- **Summary**: Recording fast motion in a high FPS (frame-per-second) requires expensive high-speed cameras. As an alternative, interpolating low-FPS videos from commodity cameras has attracted significant attention. If only low-FPS videos are available, motion assumptions (linear or quadratic) are necessary to infer intermediate frames, which fail to model complex motions. Event camera, a new camera with pixels producing events of brightness change at the temporal resolution of $\mu s$ $(10^{-6}$ second $)$, is a game-changing device to enable video interpolation at the presence of arbitrarily complex motion. Since event camera is a novel sensor, its potential has not been fulfilled due to the lack of processing algorithms. The pioneering work Time Lens introduced event cameras to video interpolation by designing optical devices to collect a large amount of paired training data of high-speed frames and events, which is too costly to scale. To fully unlock the potential of event cameras, this paper proposes a novel TimeReplayer algorithm to interpolate videos captured by commodity cameras with events. It is trained in an unsupervised cycle-consistent style, canceling the necessity of high-speed training data and bringing the additional ability of video extrapolation. Its state-of-the-art results and demo videos in supplementary reveal the promising future of event-based vision.



### FD-SLAM: 3-D Reconstruction Using Features and Dense Matching
- **Arxiv ID**: http://arxiv.org/abs/2203.13861v1
- **DOI**: 10.1109/ICRA46639.2022.9812049
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.13861v1)
- **Published**: 2022-03-25 18:58:46+00:00
- **Updated**: 2022-03-25 18:58:46+00:00
- **Authors**: Xingrui Yang, Yuhang Ming, Zhaopeng Cui, Andrew Calway
- **Comment**: None
- **Journal**: None
- **Summary**: It is well known that visual SLAM systems based on dense matching are locally accurate but are also susceptible to long-term drift and map corruption. In contrast, feature matching methods can achieve greater long-term consistency but can suffer from inaccurate local pose estimation when feature information is sparse. Based on these observations, we propose an RGB-D SLAM system that leverages the advantages of both approaches: using dense frame-to-model odometry to build accurate sub-maps and on-the-fly feature-based matching across sub-maps for global map optimisation. In addition, we incorporate a learning-based loop closure component based on 3-D features which further stabilises map building. We have evaluated the approach on indoor sequences from public datasets, and the results show that it performs on par or better than state-of-the-art systems in terms of map reconstruction quality and pose estimation. The approach can also scale to large scenes where other systems often fail.



### Intelligent Masking: Deep Q-Learning for Context Encoding in Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2203.13865v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.13865v2)
- **Published**: 2022-03-25 19:05:06+00:00
- **Updated**: 2022-04-04 20:46:27+00:00
- **Authors**: Mojtaba Bahrami, Mahsa Ghorbani, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: The need for a large amount of labeled data in the supervised setting has led recent studies to utilize self-supervised learning to pre-train deep neural networks using unlabeled data. Many self-supervised training strategies have been investigated especially for medical datasets to leverage the information available in the much fewer unlabeled data. One of the fundamental strategies in image-based self-supervision is context prediction. In this approach, a model is trained to reconstruct the contents of an arbitrary missing region of an image based on its surroundings. However, the existing methods adopt a random and blind masking approach by focusing uniformly on all regions of the images. This approach results in a lot of unnecessary network updates that cause the model to forget the rich extracted features. In this work, we develop a novel self-supervised approach that occludes targeted regions to improve the pre-training procedure. To this end, we propose a reinforcement learning-based agent which learns to intelligently mask input images through deep Q-learning. We show that training the agent against the prediction model can significantly improve the semantic features extracted for downstream classification tasks. We perform our experiments on two public datasets for diagnosing breast cancer in the ultrasound images and detecting lower-grade glioma with MR images. In our experiments, we show that our novel masking strategy advances the learned features according to the performance on the classification task in terms of accuracy, macro F1, and AUROC.



### Self-supervised Semantic Segmentation Grounded in Visual Concepts
- **Arxiv ID**: http://arxiv.org/abs/2203.13868v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.13868v2)
- **Published**: 2022-03-25 19:10:28+00:00
- **Updated**: 2022-07-26 12:56:22+00:00
- **Authors**: Wenbin He, William Surmeier, Arvind Kumar Shekar, Liang Gou, Liu Ren
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised semantic segmentation requires assigning a label to every pixel without any human annotations. Despite recent advances in self-supervised representation learning for individual images, unsupervised semantic segmentation with pixel-level representations is still a challenging task and remains underexplored. In this work, we propose a self-supervised pixel representation learning method for semantic segmentation by using visual concepts (i.e., groups of pixels with semantic meanings, such as parts, objects, and scenes) extracted from images. To guide self-supervised learning, we leverage three types of relationships between pixels and concepts, including the relationships between pixels and local concepts, local and global concepts, as well as the co-occurrence of concepts. We evaluate the learned pixel embeddings and visual concepts on three datasets, including PASCAL VOC 2012, COCO 2017, and DAVIS 2017. Our results show that the proposed method gains consistent and substantial improvements over recent unsupervised semantic segmentation approaches, and also demonstrate that visual concepts can reveal insights into image datasets.



### Semi-supervised machine learning model for analysis of nanowire morphologies from transmission electron microscopy images
- **Arxiv ID**: http://arxiv.org/abs/2203.13875v2
- **DOI**: 10.1039/D2DD00066K
- **Categories**: **cond-mat.mtrl-sci**, cond-mat.soft, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.13875v2)
- **Published**: 2022-03-25 19:32:03+00:00
- **Updated**: 2022-09-27 12:42:17+00:00
- **Authors**: Shizhao Lu, Brian Montz, Todd Emrick, Arthi Jayaraman
- **Comment**: 18 pages, 10 figures
- **Journal**: None
- **Summary**: In the field of materials science, microscopy is the first and often only accessible method for structural characterization. There is a growing interest in the development of machine learning methods that can automate the analysis and interpretation of microscopy images. Typically training of machine learning models requires large numbers of images with associated structural labels, however, manual labeling of images requires domain knowledge and is prone to human error and subjectivity. To overcome these limitations, we present a semi-supervised transfer learning approach that uses a small number of labeled microscopy images for training and performs as effectively as methods trained on significantly larger image datasets. Specifically, we train an image encoder with unlabeled images using self-supervised learning methods and use that encoder for transfer learning of different downstream image tasks (classification and segmentation) with a minimal number of labeled images for training. We test the transfer learning ability of two self-supervised learning methods: SimCLR and Barlow-Twins on transmission electron microscopy (TEM) images. We demonstrate in detail how this machine learning workflow applied to TEM images of protein nanowires enables automated classification of nanowire morphologies (e.g., single nanowires, nanowire bundles, phase separated) as well as segmentation tasks that can serve as groundwork for quantification of nanowire domain sizes and shape analysis. We also extend the application of the machine learning workflow to classification of nanoparticle morphologies and identification of different type of viruses from TEM images.



### Reinforcement Learning with Action-Free Pre-Training from Videos
- **Arxiv ID**: http://arxiv.org/abs/2203.13880v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.13880v2)
- **Published**: 2022-03-25 19:44:09+00:00
- **Updated**: 2022-06-16 22:01:25+00:00
- **Authors**: Younggyo Seo, Kimin Lee, Stephen James, Pieter Abbeel
- **Comment**: International Conference on Machine Learning (ICML 2022). Project
  page: https://sites.google.com/view/rl-apv
- **Journal**: None
- **Summary**: Recent unsupervised pre-training methods have shown to be effective on language and vision domains by learning useful representations for multiple downstream tasks. In this paper, we investigate if such unsupervised pre-training methods can also be effective for vision-based reinforcement learning (RL). To this end, we introduce a framework that learns representations useful for understanding the dynamics via generative pre-training on videos. Our framework consists of two phases: we pre-train an action-free latent video prediction model, and then utilize the pre-trained representations for efficiently learning action-conditional world models on unseen environments. To incorporate additional action inputs during fine-tuning, we introduce a new architecture that stacks an action-conditional latent prediction model on top of the pre-trained action-free prediction model. Moreover, for better exploration, we propose a video-based intrinsic bonus that leverages pre-trained representations. We demonstrate that our framework significantly improves both final performances and sample-efficiency of vision-based RL in a variety of manipulation and locomotion tasks. Code is available at https://github.com/younggyoseo/apv.



### Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities
- **Arxiv ID**: http://arxiv.org/abs/2203.13883v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.CY, cs.MM, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2203.13883v3)
- **Published**: 2022-03-25 19:45:33+00:00
- **Updated**: 2022-07-26 21:55:37+00:00
- **Authors**: Sara Abdali
- **Comment**: None
- **Journal**: None
- **Summary**: As social media platforms are evolving from text-based forums into multi-modal environments, the nature of misinformation in social media is also changing accordingly. Taking advantage of the fact that visual modalities such as images and videos are more favorable and attractive to the users, and textual contents are sometimes skimmed carelessly, misinformation spreaders have recently targeted contextual correlations between modalities e.g., text and image. Thus, many research efforts have been put into development of automatic techniques for detecting possible cross-modal discordances in web-based media. In this work, we aim to analyze, categorize and identify existing approaches in addition to challenges and shortcomings they face in order to unearth new opportunities in furthering the research in the field of multi-modal misinformation detection.



### Sylph: A Hypernetwork Framework for Incremental Few-shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2203.13903v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.13903v2)
- **Published**: 2022-03-25 20:39:00+00:00
- **Updated**: 2022-04-05 00:44:59+00:00
- **Authors**: Li Yin, Juan M Perez-Rua, Kevin J Liang
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: We study the challenging incremental few-shot object detection (iFSD) setting. Recently, hypernetwork-based approaches have been studied in the context of continuous and finetune-free iFSD with limited success. We take a closer look at important design choices of such methods, leading to several key improvements and resulting in a more accurate and flexible framework, which we call Sylph. In particular, we demonstrate the effectiveness of decoupling object classification from localization by leveraging a base detector that is pretrained for class-agnostic localization on a large-scale dataset. Contrary to what previous results have suggested, we show that with a carefully designed class-conditional hypernetwork, finetune-free iFSD can be highly effective, especially when a large number of base categories with abundant data are available for meta-training, almost approaching alternatives that undergo test-time-training. This result is even more significant considering its many practical advantages: (1) incrementally learning new classes in sequence without additional training, (2) detecting both novel and seen classes in a single pass, and (3) no forgetting of previously seen classes. We benchmark our model on both COCO and LVIS, reporting as high as 17% AP on the long-tail rare classes on LVIS, indicating the promise of hypernetwork-based iFSD.



### Concept Embedding Analysis: A Review
- **Arxiv ID**: http://arxiv.org/abs/2203.13909v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML, I.2.4; I.2.6; I.2.10; I.2.m
- **Links**: [PDF](http://arxiv.org/pdf/2203.13909v1)
- **Published**: 2022-03-25 20:57:16+00:00
- **Updated**: 2022-03-25 20:57:16+00:00
- **Authors**: Gesina Schwalbe
- **Comment**: 47 pages, 6 tables, 4 figures; submitted to Artificial Intelligence
  Review
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have found their way into many applications with potential impact on the safety, security, and fairness of human-machine-systems. Such require basic understanding and sufficient trust by the users. This motivated the research field of explainable artificial intelligence (XAI), i.e. finding methods for opening the "black-boxes" DNNs represent. For the computer vision domain in specific, practical assessment of DNNs requires a globally valid association of human interpretable concepts with internals of the model. The research field of concept (embedding) analysis (CA) tackles this problem: CA aims to find global, assessable associations of humanly interpretable semantic concepts (e.g., eye, bearded) with internal representations of a DNN. This work establishes a general definition of CA and a taxonomy for CA methods, uniting several ideas from literature. That allows to easily position and compare CA approaches. Guided by the defined notions, the current state-of-the-art research regarding CA methods and interesting applications are reviewed. More than thirty relevant methods are discussed, compared, and categorized. Finally, for practitioners, a survey of fifteen datasets is provided that have been used for supervised concept analysis. Open challenges and research directions are pointed out at the end.



### Learning to segment fetal brain tissue from noisy annotations
- **Arxiv ID**: http://arxiv.org/abs/2203.14962v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.14962v2)
- **Published**: 2022-03-25 21:22:24+00:00
- **Updated**: 2023-01-03 22:43:03+00:00
- **Authors**: Davood Karimi, Caitlin K. Rollins, Clemente Velasco-Annis, Abdelhakim Ouaalam, Ali Gholipour
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic fetal brain tissue segmentation can enhance the quantitative assessment of brain development at this critical stage. Deep learning methods represent the state of the art in medical image segmentation and have also achieved impressive results in brain segmentation. However, effective training of a deep learning model to perform this task requires a large number of training images to represent the rapid development of the transient fetal brain structures. On the other hand, manual multi-label segmentation of a large number of 3D images is prohibitive. To address this challenge, we segmented 272 training images, covering 19-39 gestational weeks, using an automatic multi-atlas segmentation strategy based on deformable registration and probabilistic atlas fusion, and manually corrected large errors in those segmentations. Since this process generated a large training dataset with noisy segmentations, we developed a novel label smoothing procedure and a loss function to train a deep learning model with smoothed noisy segmentations. Our proposed methods properly account for the uncertainty in tissue boundaries. We evaluated our method on 23 manually-segmented test images of a separate set of fetuses. Results show that our method achieves an average Dice similarity coefficient of 0.893 and 0.916 for the transient structures of younger and older fetuses, respectively. Our method generated results that were significantly more accurate than several state-of-the-art methods including nnU-Net that achieved the closest results to our method. Our trained model can serve as a valuable tool to enhance the accuracy and reproducibility of fetal brain analysis in MRI.



### A Cross-Domain Approach for Continuous Impression Recognition from Dyadic Audio-Visual-Physio Signals
- **Arxiv ID**: http://arxiv.org/abs/2203.13932v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2203.13932v1)
- **Published**: 2022-03-25 22:40:53+00:00
- **Updated**: 2022-03-25 22:40:53+00:00
- **Authors**: Yuanchao Li, Catherine Lai
- **Comment**: 5 pages, 2 figures, submitted to INTERSPEECH 2022
- **Journal**: None
- **Summary**: The impression we make on others depends not only on what we say, but also, to a large extent, on how we say it. As a sub-branch of affective computing and social signal processing, impression recognition has proven critical in both human-human conversations and spoken dialogue systems. However, most research has studied impressions only from the signals expressed by the emitter, ignoring the response from the receiver. In this paper, we perform impression recognition using a proposed cross-domain architecture on the dyadic IMPRESSION dataset. This improved architecture makes use of cross-domain attention and regularization. The cross-domain attention consists of intra- and inter-attention mechanisms, which capture intra- and inter-domain relatedness, respectively. The cross-domain regularization includes knowledge distillation and similarity enhancement losses, which strengthen the feature connections between the emitter and receiver. The experimental evaluation verified the effectiveness of our approach. Our approach achieved a concordance correlation coefficient of 0.770 in competence dimension and 0.748 in warmth dimension.



