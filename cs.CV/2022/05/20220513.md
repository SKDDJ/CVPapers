# Arxiv Papers in cs.CV on 2022-05-13
### PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.06401v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.06401v3)
- **Published**: 2022-05-13 00:15:44+00:00
- **Updated**: 2023-01-02 21:29:48+00:00
- **Authors**: Hongbin Liu, Jinyuan Jia, Neil Zhenqiang Gong
- **Comment**: USENIX Security Symposium, 2022
- **Journal**: None
- **Summary**: Contrastive learning pre-trains an image encoder using a large amount of unlabeled data such that the image encoder can be used as a general-purpose feature extractor for various downstream tasks. In this work, we propose PoisonedEncoder, a data poisoning attack to contrastive learning. In particular, an attacker injects carefully crafted poisoning inputs into the unlabeled pre-training data, such that the downstream classifiers built based on the poisoned encoder for multiple target downstream tasks simultaneously classify attacker-chosen, arbitrary clean inputs as attacker-chosen, arbitrary classes. We formulate our data poisoning attack as a bilevel optimization problem, whose solution is the set of poisoning inputs; and we propose a contrastive-learning-tailored method to approximately solve it. Our evaluation on multiple datasets shows that PoisonedEncoder achieves high attack success rates while maintaining the testing accuracy of the downstream classifiers built upon the poisoned encoder for non-attacker-chosen inputs. We also evaluate five defenses against PoisonedEncoder, including one pre-processing, three in-processing, and one post-processing defenses. Our results show that these defenses can decrease the attack success rate of PoisonedEncoder, but they also sacrifice the utility of the encoder or require a large clean pre-training dataset.



### Tensor Decompositions for Hyperspectral Data Processing in Remote Sensing: A Comprehensive Review
- **Arxiv ID**: http://arxiv.org/abs/2205.06407v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.06407v1)
- **Published**: 2022-05-13 00:39:23+00:00
- **Updated**: 2022-05-13 00:39:23+00:00
- **Authors**: Minghua Wang, Danfeng Hong, Zhu Han, Jiaxin Li, Jing Yao, Lianru Gao, Bing Zhang, Jocelyn Chanussot
- **Comment**: None
- **Journal**: None
- **Summary**: Owing to the rapid development of sensor technology, hyperspectral (HS) remote sensing (RS) imaging has provided a significant amount of spatial and spectral information for the observation and analysis of the Earth's surface at a distance of data acquisition devices, such as aircraft, spacecraft, and satellite. The recent advancement and even revolution of the HS RS technique offer opportunities to realize the full potential of various applications, while confronting new challenges for efficiently processing and analyzing the enormous HS acquisition data. Due to the maintenance of the 3-D HS inherent structure, tensor decomposition has aroused widespread concern and research in HS data processing tasks over the past decades. In this article, we aim at presenting a comprehensive overview of tensor decomposition, specifically contextualizing the five broad topics in HS data processing, and they are HS restoration, compressed sensing, anomaly detection, super-resolution, and spectral unmixing. For each topic, we elaborate on the remarkable achievements of tensor decomposition models for HS RS with a pivotal description of the existing methodologies and a representative exhibition on the experimental results. As a result, the remaining challenges of the follow-up research directions are outlined and discussed from the perspective of the real HS RS practices and tensor decomposition merged with advanced priors and even with deep neural networks. This article summarizes different tensor decomposition-based HS data processing methods and categorizes them into different classes from simple adoptions to complex combinations with other priors for the algorithm beginners. We also expect this survey can provide new investigations and development trends for the experienced researchers who understand tensor decomposition and HS RS to some extent.



### Video-based assessment of intraoperative surgical skill
- **Arxiv ID**: http://arxiv.org/abs/2205.06416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06416v1)
- **Published**: 2022-05-13 01:45:22+00:00
- **Updated**: 2022-05-13 01:45:22+00:00
- **Authors**: Sanchit Hira, Digvijay Singh, Tae Soo Kim, Shobhit Gupta, Gregory Hager, Shameema Sikder, S. Swaroop Vedula
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: The objective of this investigation is to provide a comprehensive analysis of state-of-the-art methods for video-based assessment of surgical skill in the operating room. Methods: Using a data set of 99 videos of capsulorhexis, a critical step in cataract surgery, we evaluate feature based methods previously developed for surgical skill assessment mostly under benchtop settings. In addition, we present and validate two deep learning methods that directly assess skill using RGB videos. In the first method, we predict instrument tips as keypoints, and learn surgical skill using temporal convolutional neural networks. In the second method, we propose a novel architecture for surgical skill assessment that includes a frame-wise encoder (2D convolutional neural network) followed by a temporal model (recurrent neural network), both of which are augmented by visual attention mechanisms. We report the area under the receiver operating characteristic curve, sensitivity, specificity, and predictive values with each method through 5-fold cross-validation. Results: For the task of binary skill classification (expert vs. novice), deep neural network based methods exhibit higher AUC than the classical spatiotemporal interest point based methods. The neural network approach using attention mechanisms also showed high sensitivity and specificity. Conclusion: Deep learning methods are necessary for video-based assessment of surgical skill in the operating room. Our findings of internal validity of a network using attention mechanisms to assess skill directly using RGB videos should be evaluated for external validity in other data sets.



### Talking Face Generation with Multilingual TTS
- **Arxiv ID**: http://arxiv.org/abs/2205.06421v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.06421v1)
- **Published**: 2022-05-13 02:08:35+00:00
- **Updated**: 2022-05-13 02:08:35+00:00
- **Authors**: Hyoung-Kyu Song, Sang Hoon Woo, Junhyeok Lee, Seungmin Yang, Hyunjae Cho, Youseong Lee, Dongho Choi, Kang-wook Kim
- **Comment**: Accepted to CVPR Demo Track (2022)
- **Journal**: None
- **Summary**: In this work, we propose a joint system combining a talking face generation system with a text-to-speech system that can generate multilingual talking face videos from only the text input. Our system can synthesize natural multilingual speeches while maintaining the vocal identity of the speaker, as well as lip movements synchronized to the synthesized speech. We demonstrate the generalization capabilities of our system by selecting four languages (Korean, English, Japanese, and Chinese) each from a different language family. We also compare the outputs of our talking face generation model to outputs of a prior work that claims multilingual support. For our demo, we add a translation API to the preprocessing stage and present it in the form of a neural dubber so that users can utilize the multilingual property of our system more easily.



### Test-time Fourier Style Calibration for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2205.06427v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.06427v2)
- **Published**: 2022-05-13 02:43:03+00:00
- **Updated**: 2022-05-18 20:01:59+00:00
- **Authors**: Xingchen Zhao, Chang Liu, Anthony Sicilia, Seong Jae Hwang, Yun Fu
- **Comment**: 31st International Joint Conference on Artificial Intelligence
  (IJCAI) 2022
- **Journal**: None
- **Summary**: The topic of generalizing machine learning models learned on a collection of source domains to unknown target domains is challenging. While many domain generalization (DG) methods have achieved promising results, they primarily rely on the source domains at train-time without manipulating the target domains at test-time. Thus, it is still possible that those methods can overfit to source domains and perform poorly on target domains. Driven by the observation that domains are strongly related to styles, we argue that reducing the gap between source and target styles can boost models' generalizability. To solve the dilemma of having no access to the target domain during training, we introduce Test-time Fourier Style Calibration (TF-Cal) for calibrating the target domain style on the fly during testing. To access styles, we utilize Fourier transformation to decompose features into amplitude (style) features and phase (semantic) features. Furthermore, we present an effective technique to Augment Amplitude Features (AAF) to complement TF-Cal. Extensive experiments on several popular DG benchmarks and a segmentation dataset for medical images demonstrate that our method outperforms state-of-the-art methods.



### FRIH: Fine-grained Region-aware Image Harmonization
- **Arxiv ID**: http://arxiv.org/abs/2205.06448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06448v1)
- **Published**: 2022-05-13 04:50:26+00:00
- **Updated**: 2022-05-13 04:50:26+00:00
- **Authors**: Jinlong Peng, Zekun Luo, Liang Liu, Boshen Zhang, Tao Wang, Yabiao Wang, Ying Tai, Chengjie Wang, Weiyao Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Image harmonization aims to generate a more realistic appearance of foreground and background for a composite image. Existing methods perform the same harmonization process for the whole foreground. However, the implanted foreground always contains different appearance patterns. All the existing solutions ignore the difference of each color block and losing some specific details. Therefore, we propose a novel global-local two stages framework for Fine-grained Region-aware Image Harmonization (FRIH), which is trained end-to-end. In the first stage, the whole input foreground mask is used to make a global coarse-grained harmonization. In the second stage, we adaptively cluster the input foreground mask into several submasks by the corresponding pixel RGB values in the composite image. Each submask and the coarsely adjusted image are concatenated respectively and fed into a lightweight cascaded module, adjusting the global harmonization performance according to the region-aware local feature. Moreover, we further designed a fusion prediction module by fusing features from all the cascaded decoder layers together to generate the final result, which could utilize the different degrees of harmonization results comprehensively. Without bells and whistles, our FRIH algorithm achieves the best performance on iHarmony4 dataset (PSNR is 38.19 dB) with a lightweight model. The parameters for our model are only 11.98 M, far below the existing methods.



### A microstructure estimation Transformer inspired by sparse representation for diffusion MRI
- **Arxiv ID**: http://arxiv.org/abs/2205.06450v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.06450v1)
- **Published**: 2022-05-13 05:14:22+00:00
- **Updated**: 2022-05-13 05:14:22+00:00
- **Authors**: Tianshu Zheng, Cong Sun, Weihao Zheng, Wen Shi, Haotian Li, Yi Sun, Yi Zhang, Guangbin Wang, Chuyang Ye, Dan Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion magnetic resonance imaging (dMRI) is an important tool in characterizing tissue microstructure based on biophysical models, which are complex and highly non-linear. Resolving microstructures with optimization techniques is prone to estimation errors and requires dense sampling in the q-space. Deep learning based approaches have been proposed to overcome these limitations. Motivated by the superior performance of the Transformer, in this work, we present a learning-based framework based on Transformer, namely, a Microstructure Estimation Transformer with Sparse Coding (METSC) for dMRI-based microstructure estimation with downsampled q-space data. To take advantage of the Transformer while addressing its limitation in large training data requirements, we explicitly introduce an inductive bias - model bias into the Transformer using a sparse coding technique to facilitate the training process. Thus, the METSC is composed with three stages, an embedding stage, a sparse representation stage, and a mapping stage. The embedding stage is a Transformer-based structure that encodes the signal to ensure the voxel is represented effectively. In the sparse representation stage, a dictionary is constructed by solving a sparse reconstruction problem that unfolds the Iterative Hard Thresholding (IHT) process. The mapping stage is essentially a decoder that computes the microstructural parameters from the output of the second stage, based on the weighted sum of normalized dictionary coefficients where the weights are also learned. We tested our framework on two dMRI models with downsampled q-space data, including the intravoxel incoherent motion (IVIM) model and the neurite orientation dispersion and density imaging (NODDI) model. The proposed method achieved up to 11.25 folds of acceleration in scan time and outperformed the other state-of-the-art learning-based methods.



### Monocular Human Digitization via Implicit Re-projection Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.06468v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06468v2)
- **Published**: 2022-05-13 06:53:05+00:00
- **Updated**: 2022-05-16 02:01:29+00:00
- **Authors**: Min-Gyu Park, Ju-Mi Kang, Je Woo Kim, Ju Hong Yoon
- **Comment**: Presented at CVRRW (AI for Content Creation workshop) 2022
- **Journal**: None
- **Summary**: We present an approach to generating 3D human models from images. The key to our framework is that we predict double-sided orthographic depth maps and color images from a single perspective projected image. Our framework consists of three networks. The first network predicts normal maps to recover geometric details such as wrinkles in the clothes and facial regions. The second network predicts shade-removed images for the front and back views by utilizing the predicted normal maps. The last multi-headed network takes both normal maps and shade-free images and predicts depth maps while selectively fusing photometric and geometric information through multi-headed attention gates. Experimental results demonstrate that our method shows visually plausible results and competitive performance in terms of various evaluation metrics over state-of-the-art methods.



### A Survey of Left Atrial Appendage Segmentation and Analysis in 3D and 4D Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2205.06486v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.06486v1)
- **Published**: 2022-05-13 07:35:18+00:00
- **Updated**: 2022-05-13 07:35:18+00:00
- **Authors**: Hrvoje Leventić, Marin Benčević, Danilo Babin, Marija Habijan, Irena Galić
- **Comment**: None
- **Journal**: The 28th International Conference on Systems, Signals and Image
  Processing, IWSSIP 2021, Bratislava, Slovakia, 2021
- **Summary**: Atrial fibrillation (AF) is a cardiovascular disease identified as one of the main risk factors for stroke. The majority of strokes due to AF are caused by clots originating in the left atrial appendage (LAA). LAA occlusion is an effective procedure for reducing stroke risk. Planning the procedure using pre-procedural imaging and analysis has shown benefits. The analysis is commonly done by manually segmenting the appendage on 2D slices. Automatic LAA segmentation methods could save an expert's time and provide insightful 3D visualizations and accurate automatic measurements to aid in medical procedures. Several semi- and fully-automatic methods for segmenting the appendage have been proposed. This paper provides a review of automatic LAA segmentation methods on 3D and 4D medical images, including CT, MRI, and echocardiogram images. We classify methods into heuristic and model-based methods, as well as into semi- and fully-automatic methods. We summarize and compare the proposed methods, evaluate their effectiveness, and present current challenges in the field and approaches to overcome them.



### RTMaps-based Local Dynamic Map for multi-ADAS data fusion
- **Arxiv ID**: http://arxiv.org/abs/2205.06497v1
- **DOI**: None
- **Categories**: **cs.DB**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.06497v1)
- **Published**: 2022-05-13 08:07:16+00:00
- **Updated**: 2022-05-13 08:07:16+00:00
- **Authors**: Marcos Nieto, Mikel Garcia, Itziar Urbieta, Oihana Otaegui
- **Comment**: 9 pages. To be published in 14th ITS European Congress 2022
- **Journal**: None
- **Summary**: Work on Local Dynamic Maps (LDM) implementation is still in its early stages, as the LDM standards only define how information shall be structured in databases, while the mechanism to fuse or link information across different layers is left undefined. A working LDM component, as a real-time database inside the vehicle is an attractive solution to multi-ADAS systems, which may feed a real-time LDM database that serves as a central point of information inside the vehicle, exposing fused and structured information to other components (e.g., decision-making systems). In this paper we describe our approach implementing a real-time LDM component using the RTMaps middleware, as a database deployed in a vehicle, but also at road-side units (RSU), making use of the three pillars that guide a successful fusion strategy: utilisation of standards (with conversions between domains), middlewares to unify multiple ADAS sources, and linkage of data via semantic concepts.



### FontNet: Closing the gap to font designer performance in font synthesis
- **Arxiv ID**: http://arxiv.org/abs/2205.06512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06512v1)
- **Published**: 2022-05-13 08:37:10+00:00
- **Updated**: 2022-05-13 08:37:10+00:00
- **Authors**: Ammar Ul Hassan Muhammad, Jaeyoung Choi
- **Comment**: 5 pages, 2 Figures, 3 Tables. Accepted paper for AI4CC 2022
  (https://ai4cc.net/)
- **Journal**: None
- **Summary**: Font synthesis has been a very active topic in recent years because manual font design requires domain expertise and is a labor-intensive and time-consuming job. While remarkably successful, existing methods for font synthesis have major shortcomings; they require finetuning for unobserved font style with large reference images, the recent few-shot font synthesis methods are either designed for specific language systems or they operate on low-resolution images which limits their use. In this paper, we tackle this font synthesis problem by learning the font style in the embedding space. To this end, we propose a model, called FontNet, that simultaneously learns to separate font styles in the embedding space where distances directly correspond to a measure of font similarity, and translates input images into the given observed or unobserved font style. Additionally, we design the network architecture and training procedure that can be adopted for any language system and can produce high-resolution font images. Thanks to this approach, our proposed method outperforms the existing state-of-the-art font generation methods on both qualitative and quantitative experiments.



### How to Fine-tune Models with Few Samples: Update, Data Augmentation, and Test-time Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.07874v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07874v3)
- **Published**: 2022-05-13 08:47:06+00:00
- **Updated**: 2022-08-19 08:23:51+00:00
- **Authors**: Yujin Kim, Jaehoon Oh, Sungnyun Kim, Se-Young Yun
- **Comment**: 18 pages, 25 figures, 11 tables; previous version was presented at
  ICML UpML workshop, 2022
- **Journal**: None
- **Summary**: Most of the recent few-shot learning (FSL) algorithms are based on transfer learning, where a model is pre-trained using a large amount of source data, and the pre-trained model is fine-tuned using a small amount of target data. In transfer learning-based FSL, sophisticated pre-training methods have been widely studied for universal representation. Therefore, it has become more important to utilize the universal representation for downstream tasks, but there are few studies on fine-tuning in FSL. In this paper, we focus on how to transfer pre-trained models to few-shot downstream tasks from the three perspectives: update, data augmentation, and test-time augmentation. First, we compare the two popular update methods, full fine-tuning (i.e., updating the entire network, FT) and linear probing (i.e., updating only a linear classifier, LP). We find that LP is better than FT with extremely few samples, whereas FT outperforms LP as training samples increase. Next, we show that data augmentation cannot guarantee few-shot performance improvement and investigate the effectiveness of data augmentation based on the intensity of augmentation. Finally, we adopt augmentation to both a support set for update (i.e., data augmentation) as well as a query set for prediction (i.e., test-time augmentation), considering support-query distribution shifts, and improve few-shot performance. The code is available at https://github.com/kimyuji/updating_FSL.



### Modeling Semantic Composition with Syntactic Hypergraph for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2205.06530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06530v1)
- **Published**: 2022-05-13 09:28:13+00:00
- **Updated**: 2022-05-13 09:28:13+00:00
- **Authors**: Zenan Xu, Wanjun Zhong, Qinliang Su, Zijing Ou, Fuwei Zhang
- **Comment**: 11pages, 7 figures
- **Journal**: None
- **Summary**: A key challenge in video question answering is how to realize the cross-modal semantic alignment between textual concepts and corresponding visual objects. Existing methods mostly seek to align the word representations with the video regions. However, word representations are often not able to convey a complete description of textual concepts, which are in general described by the compositions of certain words. To address this issue, we propose to first build a syntactic dependency tree for each question with an off-the-shelf tool and use it to guide the extraction of meaningful word compositions. Based on the extracted compositions, a hypergraph is further built by viewing the words as nodes and the compositions as hyperedges. Hypergraph convolutional networks (HCN) are then employed to learn the initial representations of word compositions. Afterwards, an optimal transport based method is proposed to perform cross-modal semantic alignment for the textual and visual semantic space. To reflect the cross-modal influences, the cross-modal information is incorporated into the initial representations, leading to a model named cross-modality-aware syntactic HCN. Experimental results on three benchmarks show that our method outperforms all strong baselines. Further analyses demonstrate the effectiveness of each component, and show that our model is good at modeling different levels of semantic compositions and filtering out irrelevant information.



### Comparison of attention models and post-hoc explanation methods for embryo stage identification: a case study
- **Arxiv ID**: http://arxiv.org/abs/2205.06546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06546v1)
- **Published**: 2022-05-13 10:19:35+00:00
- **Updated**: 2022-05-13 10:19:35+00:00
- **Authors**: Tristan Gomez, Thomas Fréour, Harold Mouchère
- **Comment**: None
- **Journal**: None
- **Summary**: An important limitation to the development of AI-based solutions for In Vitro Fertilization (IVF) is the black-box nature of most state-of-the-art models, due to the complexity of deep learning architectures, which raises potential bias and fairness issues. The need for interpretable AI has risen not only in the IVF field but also in the deep learning community in general. This has started a trend in literature where authors focus on designing objective metrics to evaluate generic explanation methods. In this paper, we study the behavior of recently proposed objective faithfulness metrics applied to the problem of embryo stage identification. We benchmark attention models and post-hoc methods using metrics and further show empirically that (1) the metrics produce low overall agreement on the model ranking and (2) depending on the metric approach, either post-hoc methods or attention models are favored. We conclude with general remarks about the difficulty of defining faithfulness and the necessity of understanding its relationship with the type of approach that is favored.



### Meta Balanced Network for Fair Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.06548v1
- **DOI**: 10.1109/TPAMI.2021.3103191
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06548v1)
- **Published**: 2022-05-13 10:25:44+00:00
- **Updated**: 2022-05-13 10:25:44+00:00
- **Authors**: Mei Wang, Yaobin Zhang, Weihong Deng
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)
- **Journal**: None
- **Summary**: Although deep face recognition has achieved impressive progress in recent years, controversy has arisen regarding discrimination based on skin tone, questioning their deployment into real-world scenarios. In this paper, we aim to systematically and scientifically study this bias from both data and algorithm aspects. First, using the dermatologist approved Fitzpatrick Skin Type classification system and Individual Typology Angle, we contribute a benchmark called Identity Shades (IDS) database, which effectively quantifies the degree of the bias with respect to skin tone in existing face recognition algorithms and commercial APIs. Further, we provide two skin-tone aware training datasets, called BUPT-Globalface dataset and BUPT-Balancedface dataset, to remove bias in training data. Finally, to mitigate the algorithmic bias, we propose a novel meta-learning algorithm, called Meta Balanced Network (MBN), which learns adaptive margins in large margin loss such that the model optimized by this loss can perform fairly across people with different skin tones. To determine the margins, our method optimizes a meta skewness loss on a clean and unbiased meta set and utilizes backward-on-backward automatic differentiation to perform a second order gradient descent step on the current margins. Extensive experiments show that MBN successfully mitigates bias and learns more balanced performance for people with different skin tones in face recognition. The proposed datasets are available at http://www.whdeng.cn/RFW/index.html.



### Unsupervised Structure-Texture Separation Network for Oracle Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.06549v1
- **DOI**: 10.1109/TIP.2022.3165989
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06549v1)
- **Published**: 2022-05-13 10:27:02+00:00
- **Updated**: 2022-05-13 10:27:02+00:00
- **Authors**: Mei Wang, Weihong Deng, Cheng-Lin Liu
- **Comment**: Accepted by IEEE Transactions on Image Processing (TIP)
- **Journal**: None
- **Summary**: Oracle bone script is the earliest-known Chinese writing system of the Shang dynasty and is precious to archeology and philology. However, real-world scanned oracle data are rare and few experts are available for annotation which make the automatic recognition of scanned oracle characters become a challenging task. Therefore, we aim to explore unsupervised domain adaptation to transfer knowledge from handprinted oracle data, which are easy to acquire, to scanned domain. We propose a structure-texture separation network (STSN), which is an end-to-end learning framework for joint disentanglement, transformation, adaptation and recognition. First, STSN disentangles features into structure (glyph) and texture (noise) components by generative models, and then aligns handprinted and scanned data in structure feature space such that the negative influence caused by serious noises can be avoided when adapting. Second, transformation is achieved via swapping the learned textures across domains and a classifier for final classification is trained to predict the labels of the transformed scanned characters. This not only guarantees the absolute separation, but also enhances the discriminative ability of the learned features. Extensive experiments on Oracle-241 dataset show that STSN outperforms other adaptation methods and successfully improves recognition performance on scanned data even when they are contaminated by long burial and careless excavation.



### Contrastive Domain Disentanglement for Generalizable Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.06551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06551v1)
- **Published**: 2022-05-13 10:32:41+00:00
- **Updated**: 2022-05-13 10:32:41+00:00
- **Authors**: Ran Gu, Jiangshan Lu, Jingyang Zhang, Wenhui Lei, Xiaofan Zhang, Guotai Wang, Shaoting Zhang
- **Comment**: 10 pages, 2 figures
- **Journal**: None
- **Summary**: Efficiently utilizing discriminative features is crucial for convolutional neural networks to achieve remarkable performance in medical image segmentation and is also important for model generalization across multiple domains, where letting model recognize domain-specific and domain-invariant information among multi-site datasets is a reasonable strategy for domain generalization. Unfortunately, most of the recent disentangle networks are not directly adaptable to unseen-domain datasets because of the limitations of offered data distribution. To tackle this deficiency, we propose Contrastive Domain Disentangle (CDD) network for generalizable medical image segmentation. We first introduce a disentangle network to decompose medical images into an anatomical representation factor and a modality representation factor. Then, a style contrastive loss is proposed to encourage the modality representations from the same domain to distribute as close as possible while different domains are estranged from each other. Finally, we propose a domain augmentation strategy that can randomly generate new domains for model generalization training. Experimental results on multi-site fundus image datasets for optic cup and disc segmentation show that the CDD has good model generalization. Our proposed CDD outperforms several state-of-the-art methods in domain generalizable segmentation.



### Virtual passengers for real car solutions: synthetic datasets
- **Arxiv ID**: http://arxiv.org/abs/2205.06556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06556v1)
- **Published**: 2022-05-13 10:54:39+00:00
- **Updated**: 2022-05-13 10:54:39+00:00
- **Authors**: Paola Natalia Canas, Juan Diego Ortega, Marcos Nieto, Oihana Otaegui
- **Comment**: 9 pages, 6 figures, 14th ITS European Congress
- **Journal**: None
- **Summary**: Strategies that include the generation of synthetic data are beginning to be viable as obtaining real data can be logistically complicated, very expensive or slow. Not only the capture of the data can lead to complications, but also its annotation. To achieve high-fidelity data for training intelligent systems, we have built a 3D scenario and set-up to resemble reality as closely as possible. With our approach, it is possible to configure and vary parameters to add randomness to the scene and, in this way, allow variation in data, which is so important in the construction of a dataset. Besides, the annotation task is already included in the data generation exercise, rather than being a post-capture task, which can save a lot of resources. We present the process and concept of synthetic data generation in an automotive context, specifically for driver and passenger monitoring purposes, as an alternative to real data capturing.



### Self-Supervised Masking for Unsupervised Anomaly Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/2205.06568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06568v1)
- **Published**: 2022-05-13 11:42:06+00:00
- **Updated**: 2022-05-13 11:42:06+00:00
- **Authors**: Chaoqin Huang, Qinwei Xu, Yanfeng Wang, Yu Wang, Ya Zhang
- **Comment**: Transaction on Multimedia 2022
- **Journal**: None
- **Summary**: Recently, anomaly detection and localization in multimedia data have received significant attention among the machine learning community. In real-world applications such as medical diagnosis and industrial defect detection, anomalies only present in a fraction of the images. To extend the reconstruction-based anomaly detection architecture to the localized anomalies, we propose a self-supervised learning approach through random masking and then restoring, named Self-Supervised Masking (SSM) for unsupervised anomaly detection and localization. SSM not only enhances the training of the inpainting network but also leads to great improvement in the efficiency of mask prediction at inference. Through random masking, each image is augmented into a diverse set of training triplets, thus enabling the autoencoder to learn to reconstruct with masks of various sizes and shapes during training. To improve the efficiency and effectiveness of anomaly detection and localization at inference, we propose a novel progressive mask refinement approach that progressively uncovers the normal regions and finally locates the anomalous regions. The proposed SSM method outperforms several state-of-the-arts for both anomaly detection and anomaly localization, achieving 98.3% AUC on Retinal-OCT and 93.9% AUC on MVTec AD, respectively.



### Blind Image Inpainting with Sparse Directional Filter Dictionaries for Lightweight CNNs
- **Arxiv ID**: http://arxiv.org/abs/2205.06597v1
- **DOI**: None
- **Categories**: **cs.CV**, math.RT
- **Links**: [PDF](http://arxiv.org/pdf/2205.06597v1)
- **Published**: 2022-05-13 12:44:44+00:00
- **Updated**: 2022-05-13 12:44:44+00:00
- **Authors**: Jenny Schmalfuss, Erik Scheurer, Heng Zhao, Nikolaos Karantzas, Andrés Bruhn, Demetrio Labate
- **Comment**: None
- **Journal**: None
- **Summary**: Blind inpainting algorithms based on deep learning architectures have shown a remarkable performance in recent years, typically outperforming model-based methods both in terms of image quality and run time. However, neural network strategies typically lack a theoretical explanation, which contrasts with the well-understood theory underlying model-based methods. In this work, we leverage the advantages of both approaches by integrating theoretically founded concepts from transform domain methods and sparse approximations into a CNN-based approach for blind image inpainting. To this end, we present a novel strategy to learn convolutional kernels that applies a specifically designed filter dictionary whose elements are linearly combined with trainable weights. Numerical experiments demonstrate the competitiveness of this approach. Our results show not only an improved inpainting quality compared to conventional CNNs but also significantly faster network convergence within a lightweight network design.



### StyLandGAN: A StyleGAN based Landscape Image Synthesis using Depth-map
- **Arxiv ID**: http://arxiv.org/abs/2205.06611v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.06611v1)
- **Published**: 2022-05-13 13:05:33+00:00
- **Updated**: 2022-05-13 13:05:33+00:00
- **Authors**: Gunhee Lee, Jonghwa Yim, Chanran Kim, Minjae Kim
- **Comment**: AI for Content Creation Workshop, CVPR 2022
- **Journal**: None
- **Summary**: Despite recent success in conditional image synthesis, prevalent input conditions such as semantics and edges are not clear enough to express `Linear (Ridges)' and `Planar (Scale)' representations. To address this problem, we propose a novel framework StyLandGAN, which synthesizes desired landscape images using a depth map which has higher expressive power. Our StyleLandGAN is extended from the unconditional generation model to accept input conditions. We also propose a '2-phase inference' pipeline which generates diverse depth maps and shifts local parts so that it can easily reflect user's intend. As a comparison, we modified the existing semantic image synthesis models to accept a depth map as well. Experimental results show that our method is superior to existing methods in quality, diversity, and depth-accuracy.



### Local Attention Graph-based Transformer for Multi-target Genetic Alteration Prediction
- **Arxiv ID**: http://arxiv.org/abs/2205.06672v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.06672v2)
- **Published**: 2022-05-13 14:24:24+00:00
- **Updated**: 2022-06-17 15:24:39+00:00
- **Authors**: Daniel Reisenbüchler, Sophia J. Wagner, Melanie Boxberg, Tingying Peng
- **Comment**: None
- **Journal**: None
- **Summary**: Classical multiple instance learning (MIL) methods are often based on the identical and independent distributed assumption between instances, hence neglecting the potentially rich contextual information beyond individual entities. On the other hand, Transformers with global self-attention modules have been proposed to model the interdependencies among all instances. However, in this paper we question: Is global relation modeling using self-attention necessary, or can we appropriately restrict self-attention calculations to local regimes in large-scale whole slide images (WSIs)? We propose a general-purpose local attention graph-based Transformer for MIL (LA-MIL), introducing an inductive bias by explicitly contextualizing instances in adaptive local regimes of arbitrary size. Additionally, an efficiently adapted loss function enables our approach to learn expressive WSI embeddings for the joint analysis of multiple biomarkers. We demonstrate that LA-MIL achieves state-of-the-art results in mutation prediction for gastrointestinal cancer, outperforming existing models on important biomarkers such as microsatellite instability for colorectal cancer. Our findings suggest that local self-attention sufficiently models dependencies on par with global modules. Our LA-MIL implementation is available at https://github.com/agentdr1/LA_MIL.



### Open-Eye: An Open Platform to Study Human Performance on Identifying AI-Synthesized Faces
- **Arxiv ID**: http://arxiv.org/abs/2205.06680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06680v1)
- **Published**: 2022-05-13 14:30:59+00:00
- **Updated**: 2022-05-13 14:30:59+00:00
- **Authors**: Hui Guo, Shu Hu, Xin Wang, Ming-Ching Chang, Siwei Lyu
- **Comment**: Accepted by IEEE 5th International Conference on Multimedia
  Information Processing and Retrieval (MIPR), 2022. arXiv admin note:
  substantial text overlap with arXiv:2109.00162
- **Journal**: None
- **Summary**: AI-synthesized faces are visually challenging to discern from real ones. They have been used as profile images for fake social media accounts, which leads to high negative social impacts. Although progress has been made in developing automatic methods to detect AI-synthesized faces, there is no open platform to study the human performance of AI-synthesized faces detection. In this work, we develop an online platform called Open-eye to study the human performance of AI-synthesized face detection. We describe the design and workflow of the Open-eye in this paper.



### The Effectiveness of Temporal Dependency in Deepfake Video Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.06684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06684v1)
- **Published**: 2022-05-13 14:39:25+00:00
- **Updated**: 2022-05-13 14:39:25+00:00
- **Authors**: Will Rowan, Nick Pears
- **Comment**: None
- **Journal**: None
- **Summary**: Deepfakes are a form of synthetic image generation used to generate fake videos of individuals for malicious purposes. The resulting videos may be used to spread misinformation, reduce trust in media, or as a form of blackmail. These threats necessitate automated methods of deepfake video detection. This paper investigates whether temporal information can improve the deepfake detection performance of deep learning models.   To investigate this, we propose a framework that classifies new and existing approaches by their defining characteristics. These are the types of feature extraction: automatic or manual, and the temporal relationship between frames: dependent or independent. We apply this framework to investigate the effect of temporal dependency on a model's deepfake detection performance.   We find that temporal dependency produces a statistically significant (p < 0.05) increase in performance in classifying real images for the model using automatic feature selection, demonstrating that spatio-temporal information can increase the performance of deepfake video detection models.



### A Unified Framework for Implicit Sinkhorn Differentiation
- **Arxiv ID**: http://arxiv.org/abs/2205.06688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06688v1)
- **Published**: 2022-05-13 14:45:31+00:00
- **Updated**: 2022-05-13 14:45:31+00:00
- **Authors**: Marvin Eisenberger, Aysim Toker, Laura Leal-Taixé, Florian Bernard, Daniel Cremers
- **Comment**: To appear at CVPR 2022
- **Journal**: None
- **Summary**: The Sinkhorn operator has recently experienced a surge of popularity in computer vision and related fields. One major reason is its ease of integration into deep learning frameworks. To allow for an efficient training of respective neural networks, we propose an algorithm that obtains analytical gradients of a Sinkhorn layer via implicit differentiation. In comparison to prior work, our framework is based on the most general formulation of the Sinkhorn operator. It allows for any type of loss function, while both the target capacities and cost matrices are differentiated jointly. We further construct error bounds of the resulting algorithm for approximate inputs. Finally, we demonstrate that for a number of applications, simply replacing automatic differentiation with our algorithm directly improves the stability and accuracy of the obtained gradients. Moreover, we show that it is computationally more efficient, particularly when resources like GPU memory are scarce.



### Knowledge Distillation Meets Open-Set Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.06701v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06701v1)
- **Published**: 2022-05-13 15:15:27+00:00
- **Updated**: 2022-05-13 15:15:27+00:00
- **Authors**: Jing Yang, Xiatian Zhu, Adrian Bulat, Brais Martinez, Georgios Tzimiropoulos
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Existing knowledge distillation methods mostly focus on distillation of teacher's prediction and intermediate activation. However, the structured representation, which arguably is one of the most critical ingredients of deep models, is largely overlooked. In this work, we propose a novel {\em \modelname{}} ({\bf\em \shortname{})} method dedicated for distilling representational knowledge semantically from a pretrained teacher to a target student. The key idea is that we leverage the teacher's classifier as a semantic critic for evaluating the representations of both teacher and student and distilling the semantic knowledge with high-order structured information over all feature dimensions. This is accomplished by introducing a notion of cross-network logit computed through passing student's representation into teacher's classifier. Further, considering the set of seen classes as a basis for the semantic space in a combinatorial perspective, we scale \shortname{} to unseen classes for enabling effective exploitation of largely available, arbitrary unlabeled training data. At the problem level, this establishes an interesting connection between knowledge distillation with open-set semi-supervised learning (SSL). Extensive experiments show that our \shortname{} outperforms significantly previous state-of-the-art knowledge distillation methods on both coarse object classification and fine face recognition tasks, as well as less studied yet practically crucial binary network distillation. Under more realistic open-set SSL settings we introduce, we reveal that knowledge distillation is generally more effective than existing Out-Of-Distribution (OOD) sample detection, and our proposed \shortname{} is superior over both previous distillation and SSL competitors. The source code is available at \url{https://github.com/jingyang2017/SRD\_ossl}.



### Multi-encoder Network for Parameter Reduction of a Kernel-based Interpolation Architecture
- **Arxiv ID**: http://arxiv.org/abs/2205.06723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06723v1)
- **Published**: 2022-05-13 16:02:55+00:00
- **Updated**: 2022-05-13 16:02:55+00:00
- **Authors**: Issa Khalifeh, Marc Gorriz Blanch, Ebroul Izquierdo, Marta Mrak
- **Comment**: Paper accepted in NTIRE: New Trends in Image Restoration and
  Enhancement CVPR 2022 Workshop
- **Journal**: None
- **Summary**: Video frame interpolation involves the synthesis of new frames from existing ones. Convolutional neural networks (CNNs) have been at the forefront of the recent advances in this field. One popular CNN-based approach involves the application of generated kernels to the input frames to obtain an interpolated frame. Despite all the benefits interpolation methods offer, many of these networks require a lot of parameters, with more parameters meaning a heavier computational burden. Reducing the size of the model typically impacts performance negatively. This paper presents a method for parameter reduction for a popular flow-less kernel-based network (Adaptive Collaboration of Flows). Through our technique of removing the layers that require the most parameters and replacing them with smaller encoders, we reduce the number of parameters of the network and even achieve better performance compared to the original method. This is achieved by deploying rotation to force each individual encoder to learn different features from the input images. Ablations are conducted to justify design choices and an evaluation on how our method performs on full-length videos is presented.



### An empirical study of CTC based models for OCR of Indian languages
- **Arxiv ID**: http://arxiv.org/abs/2205.06740v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2205.06740v1)
- **Published**: 2022-05-13 16:19:21+00:00
- **Updated**: 2022-05-13 16:19:21+00:00
- **Authors**: Minesh Mathew, CV Jawahar
- **Comment**: work in progress
- **Journal**: None
- **Summary**: Recognition of text on word or line images, without the need for sub-word segmentation has become the mainstream of research and development of text recognition for Indian languages. Modelling unsegmented sequences using Connectionist Temporal Classification (CTC) is the most commonly used approach for segmentation-free OCR. In this work we present a comprehensive empirical study of various neural network models that uses CTC for transcribing step-wise predictions in the neural network output to a Unicode sequence. The study is conducted for 13 Indian languages, using an internal dataset that has around 1000 pages per language. We study the choice of line vs word as the recognition unit, and use of synthetic data to train the models. We compare our models with popular publicly available OCR tools for end-to-end document image recognition. Our end-to-end pipeline that employ our recognition models and existing text segmentation tools outperform these public OCR tools for 8 out of the 13 languages. We also introduce a new public dataset called Mozhi for word and line recognition in Indian language. The dataset contains more than 1.2 million annotated word images (120 thousand text lines) across 13 Indian languages. Our code, trained models and the Mozhi dataset will be made available at http://cvit.iiit.ac.in/research/projects/cvit-projects/



### Slimmable Video Codec
- **Arxiv ID**: http://arxiv.org/abs/2205.06754v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.06754v1)
- **Published**: 2022-05-13 16:37:27+00:00
- **Updated**: 2022-05-13 16:37:27+00:00
- **Authors**: Zhaocheng Liu, Luis Herranz, Fei Yang, Saiping Zhang, Shuai Wan, Marta Mrak, Marc Górriz Blanch
- **Comment**: Computer Vision and Pattern Recognition Workshop(CLIC2022)
- **Journal**: None
- **Summary**: Neural video compression has emerged as a novel paradigm combining trainable multilayer neural networks and machine learning, achieving competitive rate-distortion (RD) performances, but still remaining impractical due to heavy neural architectures, with large memory and computational demands. In addition, models are usually optimized for a single RD tradeoff. Recent slimmable image codecs can dynamically adjust their model capacity to gracefully reduce the memory and computation requirements, without harming RD performance. In this paper we propose a slimmable video codec (SlimVC), by integrating a slimmable temporal entropy model in a slimmable autoencoder. Despite a significantly more complex architecture, we show that slimming remains a powerful mechanism to control rate, memory footprint, computational cost and latency, all being important requirements for practical video compression.



### Scribble2D5: Weakly-Supervised Volumetric Image Segmentation via Scribble Annotations
- **Arxiv ID**: http://arxiv.org/abs/2205.06779v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06779v2)
- **Published**: 2022-05-13 17:04:10+00:00
- **Updated**: 2022-07-01 04:54:54+00:00
- **Authors**: Qiuhui Chen, Yi Hong
- **Comment**: Accepted by MICCAI 2022
- **Journal**: None
- **Summary**: Recently, weakly-supervised image segmentation using weak annotations like scribbles has gained great attention, since such annotations are much easier to obtain compared to time-consuming and label-intensive labeling at the pixel/voxel level. However, because scribbles lack structure information of region of interest (ROI), existing scribble-based methods suffer from poor boundary localization. Furthermore, most current methods are designed for 2D image segmentation, which do not fully leverage the volumetric information if directly applied to image slices. In this paper, we propose a scribble-based volumetric image segmentation, Scribble2D5, which tackles 3D anisotropic image segmentation and improves boundary prediction. To achieve this, we augment a 2.5D attention UNet with a proposed label propagation module to extend semantic information from scribbles and a combination of static and active boundary prediction to learn ROI's boundary and regularize its shape. Extensive experiments on three public datasets demonstrate Scribble2D5 significantly outperforms current scribble-based methods and approaches the performance of fully-supervised ones. Our code is available online.



### KG-SP: Knowledge Guided Simple Primitives for Open World Compositional Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.06784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06784v1)
- **Published**: 2022-05-13 17:18:15+00:00
- **Updated**: 2022-05-13 17:18:15+00:00
- **Authors**: Shyamgopal Karthik, Massimiliano Mancini, Zeynep Akata
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: The goal of open-world compositional zero-shot learning (OW-CZSL) is to recognize compositions of state and objects in images, given only a subset of them during training and no prior on the unseen compositions. In this setting, models operate on a huge output space, containing all possible state-object compositions. While previous works tackle the problem by learning embeddings for the compositions jointly, here we revisit a simple CZSL baseline and predict the primitives, i.e. states and objects, independently. To ensure that the model develops primitive-specific features, we equip the state and object classifiers with separate, non-linear feature extractors. Moreover, we estimate the feasibility of each composition through external knowledge, using this prior to remove unfeasible compositions from the output space. Finally, we propose a new setting, i.e. CZSL under partial supervision (pCZSL), where either only objects or state labels are available during training, and we can use our prior to estimate the missing labels. Our model, Knowledge-Guided Simple Primitives (KG-SP), achieves state of the art in both OW-CZSL and pCZSL, surpassing most recent competitors even when coupled with semi-supervised learning techniques. Code available at: https://github.com/ExplainableML/KG-SP.



### VQFR: Blind Face Restoration with Vector-Quantized Dictionary and Parallel Decoder
- **Arxiv ID**: http://arxiv.org/abs/2205.06803v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06803v3)
- **Published**: 2022-05-13 17:54:40+00:00
- **Updated**: 2022-07-27 07:48:29+00:00
- **Authors**: Yuchao Gu, Xintao Wang, Liangbin Xie, Chao Dong, Gen Li, Ying Shan, Ming-Ming Cheng
- **Comment**: Accepted to ECCV 2022 (Oral). Project webpage is available at
  https://ycgu.site/projects/vqfr
- **Journal**: None
- **Summary**: Although generative facial prior and geometric prior have recently demonstrated high-quality results for blind face restoration, producing fine-grained facial details faithful to inputs remains a challenging problem. Motivated by the classical dictionary-based methods and the recent vector quantization (VQ) technique, we propose a VQ-based face restoration method - VQFR. VQFR takes advantage of high-quality low-level feature banks extracted from high-quality faces and can thus help recover realistic facial details. However, the simple application of the VQ codebook cannot achieve good results with faithful details and identity preservation. Therefore, we further introduce two special network designs. 1). We first investigate the compression patch size in the VQ codebook and find that the VQ codebook designed with a proper compression patch size is crucial to balance the quality and fidelity. 2). To further fuse low-level features from inputs while not "contaminating" the realistic details generated from the VQ codebook, we proposed a parallel decoder consisting of a texture decoder and a main decoder. Those two decoders then interact with a texture warping module with deformable convolution. Equipped with the VQ codebook as a facial detail dictionary and the parallel decoder design, the proposed VQFR can largely enhance the restored quality of facial details while keeping the fidelity to previous methods.



### A Framework for Event-based Computer Vision on a Mobile Device
- **Arxiv ID**: http://arxiv.org/abs/2205.06836v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.06836v1)
- **Published**: 2022-05-13 18:06:20+00:00
- **Updated**: 2022-05-13 18:06:20+00:00
- **Authors**: Gregor Lenz, Serge Picaud, Sio-Hoi Ieng
- **Comment**: None
- **Journal**: None
- **Summary**: We present the first publicly available Android framework to stream data from an event camera directly to a mobile phone. Today's mobile devices handle a wider range of workloads than ever before and they incorporate a growing gamut of sensors that make devices smarter, more user friendly and secure. Conventional cameras in particular play a central role in such tasks, but they cannot record continuously, as the amount of redundant information recorded is costly to process. Bio-inspired event cameras on the other hand only record changes in a visual scene and have shown promising low-power applications that specifically suit mobile tasks such as face detection, gesture recognition or gaze tracking. Our prototype device is the first step towards embedding such an event camera into a battery-powered handheld device. The mobile framework allows us to stream events in real-time and opens up the possibilities for always-on and on-demand sensing on mobile phones. To liaise the asynchronous event camera output with synchronous von Neumann hardware, we look at how buffering events and processing them in batches can benefit mobile applications. We evaluate our framework in terms of latency and throughput and show examples of computer vision tasks that involve both event-by-event and pre-trained neural network methods for gesture recognition, aperture robust optical flow and grey-level image reconstruction from events. The code is available at https://github.com/neuromorphic-paris/frog



### From Images to Probabilistic Anatomical Shapes: A Deep Variational Bottleneck Approach
- **Arxiv ID**: http://arxiv.org/abs/2205.06862v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.06862v1)
- **Published**: 2022-05-13 19:39:08+00:00
- **Updated**: 2022-05-13 19:39:08+00:00
- **Authors**: Jadie Adams, Shireen Elhabian
- **Comment**: Provisionally accepted to MICCAI 2022 on May 4, 2022
- **Journal**: None
- **Summary**: Statistical shape modeling (SSM) directly from 3D medical images is an underutilized tool for detecting pathology, diagnosing disease, and conducting population-level morphology analysis. Deep learning frameworks have increased the feasibility of adopting SSM in medical practice by reducing the expert-driven manual and computational overhead in traditional SSM workflows. However, translating such frameworks to clinical practice requires calibrated uncertainty measures as neural networks can produce over-confident predictions that cannot be trusted in sensitive clinical decision-making. Existing techniques for predicting shape with aleatoric (data-dependent) uncertainty utilize a principal component analysis (PCA) based shape representation computed in isolation from the model training. This constraint restricts the learning task to solely estimating pre-defined shape descriptors from 3D images and imposes a linear relationship between this shape representation and the output (i.e., shape) space. In this paper, we propose a principled framework based on the variational information bottleneck theory to relax these assumptions while predicting probabilistic shapes of anatomy directly from images without supervised encoding of shape descriptors. Here, the latent representation is learned in the context of the learning task, resulting in a more scalable, flexible model that better captures data non-linearity. Additionally, this model is self-regularized and generalizes better given limited training data. Our experiments demonstrate that the proposed method provides improved accuracy and better calibrated aleatoric uncertainty estimates than state-of-the-art methods.



### Using Augmented Face Images to Improve Facial Recognition Tasks
- **Arxiv ID**: http://arxiv.org/abs/2205.06873v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2205.06873v1)
- **Published**: 2022-05-13 20:12:12+00:00
- **Updated**: 2022-05-13 20:12:12+00:00
- **Authors**: Shuo Cheng, Guoxian Song, Wan-Chun Ma, Chao Wang, Linjie Luo
- **Comment**: CHI 2022 Workshop: AI-Generated Characters: Putting Deepfakes to Good
  Use
- **Journal**: None
- **Summary**: We present a framework that uses GAN-augmented images to complement certain specific attributes, usually underrepresented, for machine learning model training. This allows us to improve inference quality over those attributes for the facial recognition tasks.



### AVCAffe: A Large Scale Audio-Visual Dataset of Cognitive Load and Affect for Remote Work
- **Arxiv ID**: http://arxiv.org/abs/2205.06887v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.06887v2)
- **Published**: 2022-05-13 20:55:25+00:00
- **Updated**: 2022-11-25 04:37:15+00:00
- **Authors**: Pritam Sarkar, Aaron Posen, Ali Etemad
- **Comment**: Accepted in AAAI 2023
- **Journal**: None
- **Summary**: We introduce AVCAffe, the first Audio-Visual dataset consisting of Cognitive load and Affect attributes. We record AVCAffe by simulating remote work scenarios over a video-conferencing platform, where subjects collaborate to complete a number of cognitively engaging tasks. AVCAffe is the largest originally collected (not collected from the Internet) affective dataset in English language. We recruit 106 participants from 18 different countries of origin, spanning an age range of 18 to 57 years old, with a balanced male-female ratio. AVCAffe comprises a total of 108 hours of video, equivalent to more than 58,000 clips along with task-based self-reported ground truth labels for arousal, valence, and cognitive load attributes such as mental demand, temporal demand, effort, and a few others. We believe AVCAffe would be a challenging benchmark for the deep learning research community given the inherent difficulty of classifying affect and cognitive load in particular. Moreover, our dataset fills an existing timely gap by facilitating the creation of learning systems for better self-management of remote work meetings, and further study of hypotheses regarding the impact of remote work on cognitive load and affective states.



### Unsupervised Representation Learning for 3D MRI Super Resolution with Degradation Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2205.06891v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2205.06891v4)
- **Published**: 2022-05-13 21:07:26+00:00
- **Updated**: 2023-03-27 23:39:54+00:00
- **Authors**: Jianan Liu, Hao Li, Tao Huang, Euijoon Ahn, Kang Han, Adeel Razi, Wei Xiang, Jinman Kim, David Dagan Feng
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: High-resolution (HR) magnetic resonance imaging is critical in aiding doctors in their diagnoses and image-guided treatments. However, acquiring HR images can be time-consuming and costly. Consequently, deep learning-based super-resolution reconstruction (SRR) has emerged as a promising solution for generating super-resolution (SR) images from low-resolution (LR) images. Unfortunately, training such neural networks requires aligned authentic HR and LR image pairs, which are challenging to obtain due to patient movements during and between image acquisitions. While rigid movements of hard tissues can be corrected with image registration, aligning deformed soft tissues is complex, making it impractical to train neural networks with authentic HR and LR image pairs. Previous studies have focused on SRR using authentic HR images and down-sampled synthetic LR images. However, the difference in degradation representations between synthetic and authentic LR images suppresses the quality of SR images reconstructed from authentic LR images. To address this issue, we propose a novel Unsupervised Degradation Adaptation Network (UDEAN). Our network consists of a degradation learning network and an SRR network. The degradation learning network downsamples the HR images using the degradation representation learned from the misaligned or unpaired LR images. The SRR network then learns the mapping from the down-sampled HR images to the original ones. Experimental results show that our method outperforms state-of-the-art networks and is a promising solution to the challenges in clinical settings.



### ImageSig: A signature transform for ultra-lightweight image recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.06929v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.06929v1)
- **Published**: 2022-05-13 23:48:32+00:00
- **Updated**: 2022-05-13 23:48:32+00:00
- **Authors**: Mohamed R. Ibrahim, Terry Lyons
- **Comment**: None
- **Journal**: Proceedings of the IEEE conference on computer vision and pattern
  recognition (CVPR) workshops,2022
- **Summary**: This paper introduces a new lightweight method for image recognition. ImageSig is based on computing signatures and does not require a convolutional structure or an attention-based encoder. It is striking to the authors that it achieves: a) an accuracy for 64 X 64 RGB images that exceeds many of the state-of-the-art methods and simultaneously b) requires orders of magnitude less FLOPS, power and memory footprint. The pretrained model can be as small as 44.2 KB in size. ImageSig shows unprecedented performance on hardware such as Raspberry Pi and Jetson-nano. ImageSig treats images as streams with multiple channels. These streams are parameterized by spatial directions. We contribute to the functionality of signature and rough path theory to stream-like data and vision tasks on static images beyond temporal streams. With very few parameters and small size models, the key advantage is that one could have many of these "detectors" assembled on the same chip; moreover, the feature acquisition can be performed once and shared between different models of different tasks - further accelerating the process. This contributes to energy efficiency and the advancements of embedded AI at the edge.



