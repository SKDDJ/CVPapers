# Arxiv Papers in cs.CV on 2022-05-09
### A Nonlocal Graph-PDE and Higher-Order Geometric Integration for Image Labeling
- **Arxiv ID**: http://arxiv.org/abs/2205.03991v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2205.03991v2)
- **Published**: 2022-05-09 01:29:44+00:00
- **Updated**: 2022-10-04 12:09:47+00:00
- **Authors**: Dmitrij Sitenko, Bastian Boll, Christoph Schn√∂rr
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel nonlocal partial difference equation (G-PDE) for labeling metric data on graphs. The G-PDE is derived as nonlocal reparametrization of the assignment flow approach that was introduced in \textit{J.~Math.~Imaging \& Vision} 58(2), 2017. Due to this parameterization, solving the G-PDE numerically is shown to be equivalent to computing the Riemannian gradient flow with respect to a nonconvex potential. We devise an entropy-regularized difference-of-convex-functions (DC) decomposition of this potential and show that the basic geometric Euler scheme for integrating the assignment flow is equivalent to solving the G-PDE by an established DC programming scheme. Moreover, the viewpoint of geometric integration reveals a basic way to exploit higher-order information of the vector field that drives the assignment flow, in order to devise a novel accelerated DC programming scheme. A detailed convergence analysis of both numerical schemes is provided and illustrated by numerical experiments.



### Hardware-Robust In-RRAM-Computing for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.03996v1
- **DOI**: 10.1109/JETCAS.2022.3171522
- **Categories**: **cs.AR**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03996v1)
- **Published**: 2022-05-09 01:46:24+00:00
- **Updated**: 2022-05-09 01:46:24+00:00
- **Authors**: Yu-Hsiang Chiang, Cheng En Ni, Yun Sung, Tuo-Hung Hou, Tian-Sheuan Chang, Shyh Jye Jou
- **Comment**: 10 pages, 18 figures
- **Journal**: None
- **Summary**: In-memory computing is becoming a popular architecture for deep-learning hardware accelerators recently due to its highly parallel computing, low power, and low area cost. However, in-RRAM computing (IRC) suffered from large device variation and numerous nonideal effects in hardware. Although previous approaches including these effects in model training successfully improved variation tolerance, they only considered part of the nonideal effects and relatively simple classification tasks. This paper proposes a joint hardware and software optimization strategy to design a hardware-robust IRC macro for object detection. We lower the cell current by using a low word-line voltage to enable a complete convolution calculation in one operation that minimizes the impact of nonlinear addition. We also implement ternary weight mapping and remove batch normalization for better tolerance against device variation, sense amplifier variation, and IR drop problem. An extra bias is included to overcome the limitation of the current sensing range. The proposed approach has been successfully applied to a complex object detection task with only 3.85\% mAP drop, whereas a naive design suffers catastrophic failure under these nonideal effects.



### Row-wise Accelerator for Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2205.03998v1
- **DOI**: None
- **Categories**: **cs.AR**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.03998v1)
- **Published**: 2022-05-09 01:47:44+00:00
- **Updated**: 2022-05-09 01:47:44+00:00
- **Authors**: Hong-Yi Wang, Tian-Sheuan Chang
- **Comment**: 5 pages, 6 figures, published in IEEE AICAS 2022
- **Journal**: None
- **Summary**: Following the success of the natural language processing, the transformer for vision applications has attracted significant attention in recent years due to its excellent performance. However, existing deep learning hardware accelerators for vision cannot execute this structure efficiently due to significant model architecture differences. As a result, this paper proposes the hardware accelerator for vision transformers with row-wise scheduling, which decomposes major operations in vision transformers as a single dot product primitive for a unified and efficient execution. Furthermore, by sharing weights in columns, we can reuse the data and reduce the usage of memory. The implementation with TSMC 40nm CMOS technology only requires 262K gate count and 149KB SRAM buffer for 403.2 GOPS throughput at 600MHz clock frequency.



### Photo-to-Shape Material Transfer for Diverse Structures
- **Arxiv ID**: http://arxiv.org/abs/2205.04018v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04018v1)
- **Published**: 2022-05-09 03:37:01+00:00
- **Updated**: 2022-05-09 03:37:01+00:00
- **Authors**: Ruizhen Hu, Xiangyu Su, Xiangkai Chen, Oliver Van Kaick, Hui Huang
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a method for assigning photorealistic relightable materials to 3D shapes in an automatic manner. Our method takes as input a photo exemplar of a real object and a 3D object with segmentation, and uses the exemplar to guide the assignment of materials to the parts of the shape, so that the appearance of the resulting shape is as similar as possible to the exemplar. To accomplish this goal, our method combines an image translation neural network with a material assignment neural network. The image translation network translates the color from the exemplar to a projection of the 3D shape and the part segmentation from the projection to the exemplar. Then, the material prediction network assigns materials from a collection of realistic materials to the projected parts, based on the translated images and perceptual similarity of the materials. One key idea of our method is to use the translation network to establish a correspondence between the exemplar and shape projection, which allows us to transfer materials between objects with diverse structures. Another key idea of our method is to use the two pairs of (color, segmentation) images provided by the image translation to guide the material assignment, which enables us to ensure the consistency in the assignment. We demonstrate that our method allows us to assign materials to shapes so that their appearances better resemble the input exemplars, improving the quality of the results over the state-of-the-art method, and allowing us to automatically create thousands of shapes with high-quality photorealistic materials. Code and data for this paper are available at https://github.com/XiangyuSu611/TMT.



### I Know What You Draw: Learning Grasp Detection Conditioned on a Few Freehand Sketches
- **Arxiv ID**: http://arxiv.org/abs/2205.04026v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04026v1)
- **Published**: 2022-05-09 04:23:36+00:00
- **Updated**: 2022-05-09 04:23:36+00:00
- **Authors**: Haitao Lin, Chilam Cheang, Yanwei Fu, Xiangyang Xue
- **Comment**: accepted by ICRA2022
- **Journal**: None
- **Summary**: In this paper, we are interested in the problem of generating target grasps by understanding freehand sketches. The sketch is useful for the persons who cannot formulate language and the cases where a textual description is not available on the fly. However, very few works are aware of the usability of this novel interactive way between humans and robots. To this end, we propose a method to generate a potential grasp configuration relevant to the sketch-depicted objects. Due to the inherent ambiguity of sketches with abstract details, we take the advantage of the graph by incorporating the structure of the sketch to enhance the representation ability. This graph-represented sketch is further validated to improve the generalization of the network, capable of learning the sketch-queried grasp detection by using a small collection (around 100 samples) of hand-drawn sketches. Additionally, our model is trained and tested in an end-to-end manner which is easy to be implemented in real-world applications. Experiments on the multi-object VMRD and GraspNet-1Billion datasets demonstrate the good generalization of the proposed method. The physical robot experiments confirm the utility of our method in object-cluttered scenes.



### Learning 6-DoF Object Poses to Grasp Category-level Objects by Language Instructions
- **Arxiv ID**: http://arxiv.org/abs/2205.04028v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04028v1)
- **Published**: 2022-05-09 04:25:14+00:00
- **Updated**: 2022-05-09 04:25:14+00:00
- **Authors**: Chilam Cheang, Haitao Lin, Yanwei Fu, Xiangyang Xue
- **Comment**: accepted by ICRA2022
- **Journal**: None
- **Summary**: This paper studies the task of any objects grasping from the known categories by free-form language instructions. This task demands the technique in computer vision, natural language processing, and robotics. We bring these disciplines together on this open challenge, which is essential to human-robot interaction. Critically, the key challenge lies in inferring the category of objects from linguistic instructions and accurately estimating the 6-DoF information of unseen objects from the known classes. In contrast, previous works focus on inferring the pose of object candidates at the instance level. This significantly limits its applications in real-world scenarios.In this paper, we propose a language-guided 6-DoF category-level object localization model to achieve robotic grasping by comprehending human intention. To this end, we propose a novel two-stage method. Particularly, the first stage grounds the target in the RGB image through language description of names, attributes, and spatial relations of objects. The second stage extracts and segments point clouds from the cropped depth image and estimates the full 6-DoF object pose at category-level. Under such a manner, our approach can locate the specific object by following human instructions, and estimate the full 6-DoF pose of a category-known but unseen instance which is not utilized for training the model. Extensive experimental results show that our method is competitive with the state-of-the-art language-conditioned grasp method. Importantly, we deploy our approach on a physical robot to validate the usability of our framework in real-world applications. Please refer to the supplementary for the demo videos of our robot experiments.



### Incremental-DETR: Incremental Few-Shot Object Detection via Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.04042v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04042v3)
- **Published**: 2022-05-09 05:08:08+00:00
- **Updated**: 2023-02-27 13:23:37+00:00
- **Authors**: Na Dong, Yongqiang Zhang, Mingli Ding, Gim Hee Lee
- **Comment**: Accepted by AAAI2023
- **Journal**: None
- **Summary**: Incremental few-shot object detection aims at detecting novel classes without forgetting knowledge of the base classes with only a few labeled training data from the novel classes. Most related prior works are on incremental object detection that rely on the availability of abundant training samples per novel class that substantially limits the scalability to real-world setting where novel data can be scarce. In this paper, we propose the Incremental-DETR that does incremental few-shot object detection via fine-tuning and self-supervised learning on the DETR object detector. To alleviate severe over-fitting with few novel class data, we first fine-tune the class-specific components of DETR with self-supervision from additional object proposals generated using Selective Search as pseudo labels. We further introduce an incremental few-shot fine-tuning strategy with knowledge distillation on the class-specific components of DETR to encourage the network in detecting novel classes without forgetting the base classes. Extensive experiments conducted on standard incremental object detection and incremental few-shot object detection settings show that our approach significantly outperforms state-of-the-art methods by a large margin.



### Masked Co-attentional Transformer reconstructs 100x ultra-fast/low-dose whole-body PET from longitudinal images and anatomically guided MRI
- **Arxiv ID**: http://arxiv.org/abs/2205.04044v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04044v1)
- **Published**: 2022-05-09 05:12:29+00:00
- **Updated**: 2022-05-09 05:12:29+00:00
- **Authors**: Yan-Ran, Wang, Liangqiong Qu, Natasha Diba Sheybani, Xiaolong Luo, Jiangshan Wang, Kristina Elizabeth Hawk, Ashok Joseph Theruvath, Sergios Gatidis, Xuerong Xiao, Allison Pribnow, Daniel Rubin, Heike E. Daldrup-Link
- **Comment**: This submission has been removed by arXiv administrators because the
  submitter did not have the right to assign the license at the time of
  submission
- **Journal**: None
- **Summary**: Despite its tremendous value for the diagnosis, treatment monitoring and surveillance of children with cancer, whole body staging with positron emission tomography (PET) is time consuming and associated with considerable radiation exposure. 100x (1% of the standard clinical dosage) ultra-low-dose/ultra-fast whole-body PET reconstruction has the potential for cancer imaging with unprecedented speed and improved safety, but it cannot be achieved by the naive use of machine learning techniques. In this study, we utilize the global similarity between baseline and follow-up PET and magnetic resonance (MR) images to develop Masked-LMCTrans, a longitudinal multi-modality co-attentional CNN-Transformer that provides interaction and joint reasoning between serial PET/MRs of the same patient. We mask the tumor area in the referenced baseline PET and reconstruct the follow-up PET scans. In this manner, Masked-LMCTrans reconstructs 100x almost-zero radio-exposure whole-body PET that was not possible before. The technique also opens a new pathway for longitudinal radiology imaging reconstruction, a significantly under-explored area to date. Our model was trained and tested with Stanford PET/MRI scans of pediatric lymphoma patients and evaluated externally on PET/MRI images from T\"ubingen University. The high image quality of the reconstructed 100x whole-body PET images resulting from the application of Masked-LMCTrans will substantially advance the development of safer imaging approaches and shorter exam-durations for pediatric patients, as well as expand the possibilities for frequent longitudinal monitoring of these patients by PET.



### Exploiting Digital Surface Models for Inferring Super-Resolution for Remotely Sensed Images
- **Arxiv ID**: http://arxiv.org/abs/2205.04056v2
- **DOI**: 10.1109/TGRS.2022.3209340
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04056v2)
- **Published**: 2022-05-09 06:02:50+00:00
- **Updated**: 2022-09-13 12:48:26+00:00
- **Authors**: Savvas Karatsiolis, Chirag Padubidri, Andreas Kamilaris
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the plethora of successful Super-Resolution Reconstruction (SRR) models applied to natural images, their application to remote sensing imagery tends to produce poor results. Remote sensing imagery is often more complicated than natural images and has its peculiarities such as being of lower resolution, it contains noise, and often depicting large textured surfaces. As a result, applying non-specialized SRR models on remote sensing imagery results in artifacts and poor reconstructions. To address these problems, this paper proposes an architecture inspired by previous research work, introducing a novel approach for forcing an SRR model to output realistic remote sensing images: instead of relying on feature-space similarities as a perceptual loss, the model considers pixel-level information inferred from the normalized Digital Surface Model (nDSM) of the image. This strategy allows the application of better-informed updates during the training of the model which sources from a task (elevation map inference) that is closely related to remote sensing. Nonetheless, the nDSM auxiliary information is not required during production and thus the model infers a super-resolution image without any additional data besides its low-resolution pairs. We assess our model on two remotely sensed datasets of different spatial resolutions that also contain the DSM pairs of the images: the DFC2018 dataset and the dataset containing the national Lidar fly-by of Luxembourg. Based on visual inspection, the inferred super-resolution images exhibit particularly superior quality. In particular, the results for the high-resolution DFC2018 dataset are realistic and almost indistinguishable from the ground truth images.



### Multilevel Hierarchical Network with Multiscale Sampling for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2205.04061v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.04061v1)
- **Published**: 2022-05-09 06:28:56+00:00
- **Updated**: 2022-05-09 06:28:56+00:00
- **Authors**: Min Peng, Chongyang Wang, Yuan Gao, Yu Shi, Xiang-Dong Zhou
- **Comment**: Accepted by IJCAI 2022. arXiv admin note: text overlap with
  arXiv:2109.04735
- **Journal**: None
- **Summary**: Video question answering (VideoQA) is challenging given its multimodal combination of visual understanding and natural language processing. While most existing approaches ignore the visual appearance-motion information at different temporal scales, it is unknown how to incorporate the multilevel processing capacity of a deep learning model with such multiscale information. Targeting these issues, this paper proposes a novel Multilevel Hierarchical Network (MHN) with multiscale sampling for VideoQA. MHN comprises two modules, namely Recurrent Multimodal Interaction (RMI) and Parallel Visual Reasoning (PVR). With a multiscale sampling, RMI iterates the interaction of appearance-motion information at each scale and the question embeddings to build the multilevel question-guided visual representations. Thereon, with a shared transformer encoder, PVR infers the visual cues at each level in parallel to fit with answering different question types that may rely on the visual information at relevant levels. Through extensive experiments on three VideoQA datasets, we demonstrate improved performances than previous state-of-the-arts and justify the effectiveness of each part of our method.



### Augmentations: An Insight into their Effectiveness on Convolution Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.04064v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04064v2)
- **Published**: 2022-05-09 06:36:40+00:00
- **Updated**: 2022-05-31 11:31:38+00:00
- **Authors**: Sabeesh Ethiraj, Bharath Kumar Bolla
- **Comment**: Accepted at ICACDS-2022
- **Journal**: None
- **Summary**: Augmentations are the key factor in determining the performance of any neural network as they provide a model with a critical edge in boosting its performance. Their ability to boost a model's robustness depends on two factors, viz-a-viz, the model architecture, and the type of augmentations. Augmentations are very specific to a dataset, and it is not imperative that all kinds of augmentation would necessarily produce a positive effect on a model's performance. Hence there is a need to identify augmentations that perform consistently well across a variety of datasets and also remain invariant to the type of architecture, convolutions, and the number of parameters used. Hence there is a need to identify augmentations that perform consistently well across a variety of datasets and also remain invariant to the type of architecture, convolutions, and the number of parameters used. This paper evaluates the effect of parameters using 3x3 and depth-wise separable convolutions on different augmentation techniques on MNIST, FMNIST, and CIFAR10 datasets. Statistical Evidence shows that techniques such as Cutouts and Random horizontal flip were consistent on both parametrically low and high architectures. Depth-wise separable convolutions outperformed 3x3 convolutions at higher parameters due to their ability to create deeper networks. Augmentations resulted in bridging the accuracy gap between the 3x3 and depth-wise separable convolutions, thus establishing their role in model generalization. At higher number augmentations did not produce a significant change in performance. The synergistic effect of multiple augmentations at higher parameters, with antagonistic effect at lower parameters, was also evaluated. The work proves that a delicate balance between architectural supremacy and augmentations needs to be achieved to enhance a model's performance in any given deep learning task.



### Multi-level Consistency Learning for Semi-supervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2205.04066v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04066v3)
- **Published**: 2022-05-09 06:41:18+00:00
- **Updated**: 2022-06-28 10:25:30+00:00
- **Authors**: Zizheng Yan, Yushuang Wu, Guanbin Li, Yipeng Qin, Xiaoguang Han, Shuguang Cui
- **Comment**: IJCAI 2022
- **Journal**: None
- **Summary**: Semi-supervised domain adaptation (SSDA) aims to apply knowledge learned from a fully labeled source domain to a scarcely labeled target domain. In this paper, we propose a Multi-level Consistency Learning (MCL) framework for SSDA. Specifically, our MCL regularizes the consistency of different views of target domain samples at three levels: (i) at inter-domain level, we robustly and accurately align the source and target domains using a prototype-based optimal transport method that utilizes the pros and cons of different views of target samples; (ii) at intra-domain level, we facilitate the learning of both discriminative and compact target feature representations by proposing a novel class-wise contrastive clustering loss; (iii) at sample level, we follow standard practice and improve the prediction accuracy by conducting a consistency-based self-training. Empirically, we verified the effectiveness of our MCL framework on three popular SSDA benchmarks, i.e., VisDA2017, DomainNet, and Office-Home datasets, and the experimental results demonstrate that our MCL framework achieves the state-of-the-art performance.



### Beyond Bounding Box: Multimodal Knowledge Learning for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.04072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04072v1)
- **Published**: 2022-05-09 07:03:30+00:00
- **Updated**: 2022-05-09 07:03:30+00:00
- **Authors**: Weixin Feng, Xingyuan Bu, Chenchen Zhang, Xubin Li
- **Comment**: Submitted to CVPR2022
- **Journal**: None
- **Summary**: Multimodal supervision has achieved promising results in many visual language understanding tasks, where the language plays an essential role as a hint or context for recognizing and locating instances. However, due to the defects of the human-annotated language corpus, multimodal supervision remains unexplored in fully supervised object detection scenarios. In this paper, we take advantage of language prompt to introduce effective and unbiased linguistic supervision into object detection, and propose a new mechanism called multimodal knowledge learning (\textbf{MKL}), which is required to learn knowledge from language supervision. Specifically, we design prompts and fill them with the bounding box annotations to generate descriptions containing extensive hints and context for instances recognition and localization. The knowledge from language is then distilled into the detection model via maximizing cross-modal mutual information in both image- and object-level. Moreover, the generated descriptions are manipulated to produce hard negatives to further boost the detector performance. Extensive experiments demonstrate that the proposed method yields a consistent performance gain by 1.6\% $\sim$ 2.1\% and achieves state-of-the-art on MS-COCO and OpenImages datasets.



### PS-Net: Learned Partially Separable Model for Dynamic MR Imaging
- **Arxiv ID**: http://arxiv.org/abs/2205.04073v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04073v2)
- **Published**: 2022-05-09 07:06:02+00:00
- **Updated**: 2022-08-10 02:13:56+00:00
- **Authors**: Chentao Cao, Zhuo-Xu Cui, Qingyong Zhu, Congcong Liu, Dong Liang, Yanjie Zhu
- **Comment**: journal
- **Journal**: None
- **Summary**: Deep learning methods driven by the low-rank regularization have achieved attractive performance in dynamic magnetic resonance (MR) imaging. However, most of these methods represent low-rank prior by hand-crafted nuclear norm, which cannot accurately approximate the low-rank prior over the entire dataset through a fixed regularization parameter. In this paper, we propose a learned low-rank method for dynamic MR imaging. In particular, we unrolled the semi-quadratic splitting method (HQS) algorithm for the partially separable (PS) model to a network, in which the low-rank is adaptively characterized by a learnable null-space transform. Experiments on the cardiac cine dataset show that the proposed model outperforms the state-of-the-art compressed sensing (CS) methods and existing deep learning methods both quantitatively and qualitatively.



### Single-view 3D Body and Cloth Reconstruction under Complex Poses
- **Arxiv ID**: http://arxiv.org/abs/2205.04087v1
- **DOI**: 10.5220/0010896100003124
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04087v1)
- **Published**: 2022-05-09 07:34:06+00:00
- **Updated**: 2022-05-09 07:34:06+00:00
- **Authors**: Nicolas Ugrinovic, Albert Pumarola, Alberto Sanfeliu, Francesc Moreno-Noguer
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in 3D human shape reconstruction from single images have shown impressive results, leveraging on deep networks that model the so-called implicit function to learn the occupancy status of arbitrarily dense 3D points in space. However, while current algorithms based on this paradigm, like PiFuHD, are able to estimate accurate geometry of the human shape and clothes, they require high-resolution input images and are not able to capture complex body poses. Most training and evaluation is performed on 1k-resolution images of humans standing in front of the camera under neutral body poses. In this paper, we leverage publicly available data to extend existing implicit function-based models to deal with images of humans that can have arbitrary poses and self-occluded limbs. We argue that the representation power of the implicit function is not sufficient to simultaneously model details of the geometry and of the body pose. We, therefore, propose a coarse-to-fine approach in which we first learn an implicit function that maps the input image to a 3D body shape with a low level of detail, but which correctly fits the underlying human pose, despite its complexity. We then learn a displacement map, conditioned on the smoothed surface and on the input image, which encodes the high-frequency details of the clothes and body. In the experimental section, we show that this coarse-to-fine strategy represents a very good trade-off between shape detail and pose correctness, comparing favorably to the most recent state-of-the-art approaches. Our code will be made publicly available.



### SmoothNets: Optimizing CNN architecture design for differentially private deep learning
- **Arxiv ID**: http://arxiv.org/abs/2205.04095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04095v1)
- **Published**: 2022-05-09 07:51:54+00:00
- **Updated**: 2022-05-09 07:51:54+00:00
- **Authors**: Nicolas W. Remerscheid, Alexander Ziller, Daniel Rueckert, Georgios Kaissis
- **Comment**: None
- **Journal**: None
- **Summary**: The arguably most widely employed algorithm to train deep neural networks with Differential Privacy is DPSGD, which requires clipping and noising of per-sample gradients. This introduces a reduction in model utility compared to non-private training. Empirically, it can be observed that this accuracy degradation is strongly dependent on the model architecture. We investigated this phenomenon and, by combining components which exhibit good individual performance, distilled a new model architecture termed SmoothNet, which is characterised by increased robustness to the challenges of DP-SGD training. Experimentally, we benchmark SmoothNet against standard architectures on two benchmark datasets and observe that our architecture outperforms others, reaching an accuracy of 73.5\% on CIFAR-10 at $\varepsilon=7.0$ and 69.2\% at $\varepsilon=7.0$ on ImageNette, a state-of-the-art result compared to prior architectural modifications for DP.



### Identifying Fixation and Saccades in Virtual Reality
- **Arxiv ID**: http://arxiv.org/abs/2205.04121v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2205.04121v1)
- **Published**: 2022-05-09 08:40:20+00:00
- **Updated**: 2022-05-09 08:40:20+00:00
- **Authors**: Xiao-lin Chen, Wen-jun Hou
- **Comment**: None
- **Journal**: None
- **Summary**: Gaze recognition can significantly reduce the amount of eye movement data for a better understanding of cognitive and visual processing. Gaze recognition is an essential precondition for eye-based interaction applications in virtual reality. However, the three-dimensional characteristics of virtual reality environments also pose new challenges to existing recognition algorithms. Based on seven evaluation metrics and the Overall score (the mean of the seven normalized metric values), we obtain optimal parameters of three existing recognition algorithms (Velocity-Threshold Identification, Dispersion-Threshold Identification, and Velocity & Dispersion-Threshold Identification) and our modified Velocity & Dispersion-Threshold Identification algorithm. We compare the performance of these four algorithms with optimal parameters. The results show that our modified Velocity & Dispersion-Threshold Identification performs the best. The impact of interface complexity on classification results is also preliminarily explored. The results show that the algorithms are not sensitive to interface complexity.



### Towards 3D Face Reconstruction in Perspective Projection: Estimating 6DoF Face Pose from Monocular Image
- **Arxiv ID**: http://arxiv.org/abs/2205.04126v2
- **DOI**: 10.1109/TIP.2023.3275535
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04126v2)
- **Published**: 2022-05-09 08:49:41+00:00
- **Updated**: 2023-05-17 11:35:41+00:00
- **Authors**: Yueying Kao, Bowen Pan, Miao Xu, Jiangjing Lyu, Xiangyu Zhu, Yuanzhang Chang, Xiaobo Li, Zhen Lei
- **Comment**: Accepted by TIP
- **Journal**: None
- **Summary**: In 3D face reconstruction, orthogonal projection has been widely employed to substitute perspective projection to simplify the fitting process. This approximation performs well when the distance between camera and face is far enough. However, in some scenarios that the face is very close to camera or moving along the camera axis, the methods suffer from the inaccurate reconstruction and unstable temporal fitting due to the distortion under the perspective projection. In this paper, we aim to address the problem of single-image 3D face reconstruction under perspective projection. Specifically, a deep neural network, Perspective Network (PerspNet), is proposed to simultaneously reconstruct 3D face shape in canonical space and learn the correspondence between 2D pixels and 3D points, by which the 6DoF (6 Degrees of Freedom) face pose can be estimated to represent perspective projection. Besides, we contribute a large ARKitFace dataset to enable the training and evaluation of 3D face reconstruction solutions under the scenarios of perspective projection, which has 902,724 2D facial images with ground-truth 3D face mesh and annotated 6DoF pose parameters. Experimental results show that our approach outperforms current state-of-the-art methods by a significant margin. The code and data are available at https://github.com/cbsropenproject/6dof_face.



### Scaling up sign spotting through sign language dictionaries
- **Arxiv ID**: http://arxiv.org/abs/2205.04152v1
- **DOI**: 10.1007/s11263-022-01589-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04152v1)
- **Published**: 2022-05-09 10:00:03+00:00
- **Updated**: 2022-05-09 10:00:03+00:00
- **Authors**: G√ºl Varol, Liliane Momeni, Samuel Albanie, Triantafyllos Afouras, Andrew Zisserman
- **Comment**: Appears in: 2022 International Journal of Computer Vision (IJCV). 25
  pages. arXiv admin note: substantial text overlap with arXiv:2010.04002
- **Journal**: International Journal of Computer Vision (2022)
- **Summary**: The focus of this work is $\textit{sign spotting}$ - given a video of an isolated sign, our task is to identify $\textit{whether}$ and $\textit{where}$ it has been signed in a continuous, co-articulated sign language video. To achieve this sign spotting task, we train a model using multiple types of available supervision by: (1) $\textit{watching}$ existing footage which is sparsely labelled using mouthing cues; (2) $\textit{reading}$ associated subtitles (readily available translations of the signed content) which provide additional $\textit{weak-supervision}$; (3) $\textit{looking up}$ words (for which no co-articulated labelled examples are available) in visual sign language dictionaries to enable novel sign spotting. These three tasks are integrated into a unified learning framework using the principles of Noise Contrastive Estimation and Multiple Instance Learning. We validate the effectiveness of our approach on low-shot sign spotting benchmarks. In addition, we contribute a machine-readable British Sign Language (BSL) dictionary dataset of isolated signs, BSLDict, to facilitate study of this task. The dataset, models and code are available at our project page.



### Improved-Flow Warp Module for Remote Sensing Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.04160v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04160v1)
- **Published**: 2022-05-09 10:15:18+00:00
- **Updated**: 2022-05-09 10:15:18+00:00
- **Authors**: Yinjie Zhang, Yi Liu, Wei Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Remote sensing semantic segmentation aims to assign automatically each pixel on aerial images with specific label. In this letter, we proposed a new module, called improved-flow warp module (IFWM), to adjust semantic feature maps across different scales for remote sensing semantic segmentation. The improved-flow warp module is applied along with the feature extraction process in the convolutional neural network. First, IFWM computes the offsets of pixels by a learnable way, which can alleviate the misalignment of the multi-scale features. Second, the offsets help with the low-resolution deep feature up-sampling process to improve the feature accordance, which boosts the accuracy of semantic segmentation. We validate our method on several remote sensing datasets, and the results prove the effectiveness of our method..



### NeuralHDHair: Automatic High-fidelity Hair Modeling from a Single Image Using Implicit Neural Representations
- **Arxiv ID**: http://arxiv.org/abs/2205.04175v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04175v1)
- **Published**: 2022-05-09 10:39:39+00:00
- **Updated**: 2022-05-09 10:39:39+00:00
- **Authors**: Keyu Wu, Yifan Ye, Lingchen Yang, Hongbo Fu, Kun Zhou, Youyi Zheng
- **Comment**: Accepted by IEEE CVPR 2022
- **Journal**: None
- **Summary**: Undoubtedly, high-fidelity 3D hair plays an indispensable role in digital humans. However, existing monocular hair modeling methods are either tricky to deploy in digital systems (e.g., due to their dependence on complex user interactions or large databases) or can produce only a coarse geometry. In this paper, we introduce NeuralHDHair, a flexible, fully automatic system for modeling high-fidelity hair from a single image. The key enablers of our system are two carefully designed neural networks: an IRHairNet (Implicit representation for hair using neural network) for inferring high-fidelity 3D hair geometric features (3D orientation field and 3D occupancy field) hierarchically and a GrowingNet(Growing hair strands using neural network) to efficiently generate 3D hair strands in parallel. Specifically, we perform a coarse-to-fine manner and propose a novel voxel-aligned implicit function (VIFu) to represent the global hair feature, which is further enhanced by the local details extracted from a hair luminance map. To improve the efficiency of a traditional hair growth algorithm, we adopt a local neural implicit function to grow strands based on the estimated 3D hair geometric features. Extensive experiments show that our method is capable of constructing a high-fidelity 3D hair model from a single image, both efficiently and effectively, and achieves the-state-of-the-art performance.



### Attracting and Dispersing: A Simple Approach for Source-free Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2205.04183v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04183v3)
- **Published**: 2022-05-09 10:49:08+00:00
- **Updated**: 2022-10-03 20:26:42+00:00
- **Authors**: Shiqi Yang, Yaxing Wang, Kai Wang, Shangling Jui, Joost van de Weijer
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: We propose a simple but effective source-free domain adaptation (SFDA) method. Treating SFDA as an unsupervised clustering problem and following the intuition that local neighbors in feature space should have more similar predictions than other features, we propose to optimize an objective of prediction consistency. This objective encourages local neighborhood features in feature space to have similar predictions while features farther away in feature space have dissimilar predictions, leading to efficient feature clustering and cluster assignment simultaneously. For efficient training, we seek to optimize an upper-bound of the objective resulting in two simple terms. Furthermore, we relate popular existing methods in domain adaptation, source-free domain adaptation and contrastive learning via the perspective of discriminability and diversity. The experimental results prove the superiority of our method, and our method can be adopted as a simple but strong baseline for future research in SFDA. Our method can be also adapted to source-free open-set and partial-set DA which further shows the generalization ability of our method. Code is available in https://github.com/Albert0147/AaD_SFDA.



### Paired Image-to-Image Translation Quality Assessment Using Multi-Method Fusion
- **Arxiv ID**: http://arxiv.org/abs/2205.04186v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04186v1)
- **Published**: 2022-05-09 11:05:15+00:00
- **Updated**: 2022-05-09 11:05:15+00:00
- **Authors**: Stefan Borasinski, Esin Yavuz, S√©bastien B√©huret
- **Comment**: 17 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: How best to evaluate synthesized images has been a longstanding problem in image-to-image translation, and to date remains largely unresolved. This paper proposes a novel approach that combines signals of image quality between paired source and transformation to predict the latter's similarity with a hypothetical ground truth. We trained a Multi-Method Fusion (MMF) model via an ensemble of gradient-boosted regressors using Image Quality Assessment (IQA) metrics to predict Deep Image Structure and Texture Similarity (DISTS), enabling models to be ranked without the need for ground truth data. Analysis revealed the task to be feature-constrained, introducing a trade-off at inference between metric computation time and prediction accuracy. The MMF model we present offers an efficient way to automate the evaluation of synthesized images, and by extension the image-to-image translation models that generated them.



### Joint learning of object graph and relation graph for visual question answering
- **Arxiv ID**: http://arxiv.org/abs/2205.04188v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.04188v1)
- **Published**: 2022-05-09 11:08:43+00:00
- **Updated**: 2022-05-09 11:08:43+00:00
- **Authors**: Hao Li, Xu Li, Belhal Karimi, Jie Chen, Mingming Sun
- **Comment**: 6 pages, 4 figures, Accepted by ICME 2022
- **Journal**: None
- **Summary**: Modeling visual question answering(VQA) through scene graphs can significantly improve the reasoning accuracy and interpretability. However, existing models answer poorly for complex reasoning questions with attributes or relations, which causes false attribute selection or missing relation in Figure 1(a). It is because these models cannot balance all kinds of information in scene graphs, neglecting relation and attribute information. In this paper, we introduce a novel Dual Message-passing enhanced Graph Neural Network (DM-GNN), which can obtain a balanced representation by properly encoding multi-scale scene graph information. Specifically, we (i)transform the scene graph into two graphs with diversified focuses on objects and relations; Then we design a dual structure to encode them, which increases the weights from relations (ii)fuse the encoder output with attribute features, which increases the weights from attributes; (iii)propose a message-passing mechanism to enhance the information transfer between objects, relations and attributes. We conduct extensive experiments on datasets including GQA, VG, motif-VG and achieve new state of the art.



### Alternative Data Augmentation for Industrial Monitoring using Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.04222v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04222v1)
- **Published**: 2022-05-09 12:16:38+00:00
- **Updated**: 2022-05-09 12:16:38+00:00
- **Authors**: Silvan Mertes, Andreas Margraf, Steffen Geinitz, Elisabeth Andr√©
- **Comment**: None
- **Journal**: None
- **Summary**: Visual inspection software has become a key factor in the manufacturing industry for quality control and process monitoring. Semantic segmentation models have gained importance since they allow for more precise examination. These models, however, require large image datasets in order to achieve a fair accuracy level. In some cases, training data is sparse or lacks of sufficient annotation, a fact that especially applies to highly specialized production environments. Data augmentation represents a common strategy to extend the dataset. Still, it only varies the image within a narrow range. In this article, a novel strategy is proposed to augment small image datasets. The approach is applied to surface monitoring of carbon fibers, a specific industry use case. We apply two different methods to create binary labels: a problem-tailored trigonometric function and a WGAN model. Afterwards, the labels are translated into color images using pix2pix and used to train a U-Net. The results suggest that the trigonometric function is superior to the WGAN model. However, a precise examination of the resulting images indicate that WGAN and image-to-image translation achieve good segmentation results and only deviate to a small degree from traditional data augmentation. In summary, this study examines an industry application of data synthesization using generative adversarial networks and explores its potential for monitoring systems of production environments. \keywords{Image-to-Image Translation, Carbon Fiber, Data Augmentation, Computer Vision, Industrial Monitoring, Adversarial Learning.



### An Effective Scheme for Maize Disease Recognition based on Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.04234v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04234v1)
- **Published**: 2022-05-09 12:37:11+00:00
- **Updated**: 2022-05-09 12:37:11+00:00
- **Authors**: Saeedeh Osouli, Behrouz Bolourian Haghighi, Ehsan Sadrossadat
- **Comment**: None
- **Journal**: None
- **Summary**: In the last decades, the area under cultivation of maize products has increased because of its essential role in the food cycle for humans, livestock, and poultry. Moreover, the diseases of plants impact food safety and can significantly reduce both the quality and quantity of agricultural products. There are many challenges to accurate and timely diagnosis of the disease. This research presents a novel scheme based on a deep neural network to overcome the mentioned challenges. Due to the limited number of data, the transfer learning technique is employed with the help of two well-known architectures. In this way, a new effective model is adopted by a combination of pre-trained MobileNetV2 and Inception Networks due to their effective performance on object detection problems. The convolution layers of MoblieNetV2 and Inception modules are parallelly arranged as earlier layers to extract crucial features. In addition, the imbalance problem of classes has been solved by an augmentation strategy. The proposed scheme has a superior performance compared to other state-of-the-art models published in recent years. The accuracy of the model reaches 97%, approximately. In summary, experimental results prove the method's validity and significant performance in diagnosing disease in plant leaves.



### Improved Evaluation and Generation of Grid Layouts using Distance Preservation Quality and Linear Assignment Sorting
- **Arxiv ID**: http://arxiv.org/abs/2205.04255v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/2205.04255v2)
- **Published**: 2022-05-09 13:15:24+00:00
- **Updated**: 2022-05-11 08:28:53+00:00
- **Authors**: Kai Uwe Barthel, Nico Hezel, Klaus Jung, Konstantin Schall
- **Comment**: None
- **Journal**: None
- **Summary**: Images sorted by similarity enables more images to be viewed simultaneously, and can be very useful for stock photo agencies or e-commerce applications. Visually sorted grid layouts attempt to arrange images so that their proximity on the grid corresponds as closely as possible to their similarity. Various metrics exist for evaluating such arrangements, but there is low experimental evidence on correlation between human perceived quality and metric value. We propose Distance Preservation Quality (DPQ) as a new metric to evaluate the quality of an arrangement. Extensive user testing revealed stronger correlation of DPQ with user-perceived quality and performance in image retrieval tasks compared to other metrics. In addition, we introduce Fast Linear Assignment Sorting (FLAS) as a new algorithm for creating visually sorted grid layouts. FLAS achieves very good sorting qualities while improving run time and computational resources.



### CoCoLoT: Combining Complementary Trackers in Long-Term Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2205.04261v1
- **DOI**: 10.1109/ICPR56361.2022.9956082
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04261v1)
- **Published**: 2022-05-09 13:25:13+00:00
- **Updated**: 2022-05-09 13:25:13+00:00
- **Authors**: Matteo Dunnhofer, Christian Micheloni
- **Comment**: International Conference on Pattern Recognition (ICPR) 2022
- **Journal**: None
- **Summary**: How to combine the complementary capabilities of an ensemble of different algorithms has been of central interest in visual object tracking. A significant progress on such a problem has been achieved, but considering short-term tracking scenarios. Instead, long-term tracking settings have been substantially ignored by the solutions. In this paper, we explicitly consider long-term tracking scenarios and provide a framework, named CoCoLoT, that combines the characteristics of complementary visual trackers to achieve enhanced long-term tracking performance. CoCoLoT perceives whether the trackers are following the target object through an online learned deep verification model, and accordingly activates a decision policy which selects the best performing tracker as well as it corrects the performance of the failing one. The proposed methodology is evaluated extensively and the comparison with several other solutions reveals that it competes favourably with the state-of-the-art on the most popular long-term visual tracking benchmarks.



### SwinIQA: Learned Swin Distance for Compressed Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2205.04264v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04264v1)
- **Published**: 2022-05-09 13:31:27+00:00
- **Updated**: 2022-05-09 13:31:27+00:00
- **Authors**: Jianzhao Liu, Xin Li, Yanding Peng, Tao Yu, Zhibo Chen
- **Comment**: CVPR2022 Workshop (CLIC) accepted
- **Journal**: None
- **Summary**: Image compression has raised widespread interest recently due to its significant importance for multimedia storage and transmission. Meanwhile, a reliable image quality assessment (IQA) for compressed images can not only help to verify the performance of various compression algorithms but also help to guide the compression optimization in turn. In this paper, we design a full-reference image quality assessment metric SwinIQA to measure the perceptual quality of compressed images in a learned Swin distance space. It is known that the compression artifacts are usually non-uniformly distributed with diverse distortion types and degrees. To warp the compressed images into the shared representation space while maintaining the complex distortion information, we extract the hierarchical feature representations from each stage of the Swin Transformer. Besides, we utilize cross attention operation to map the extracted feature representations into a learned Swin distance space. Experimental results show that the proposed metric achieves higher consistency with human's perceptual judgment compared with both traditional methods and learning-based methods on CLIC datasets.



### Detecting and Understanding Harmful Memes: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2205.04274v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04274v2)
- **Published**: 2022-05-09 13:43:27+00:00
- **Updated**: 2022-05-29 21:45:00+00:00
- **Authors**: Shivam Sharma, Firoj Alam, Md. Shad Akhtar, Dimitar Dimitrov, Giovanni Da San Martino, Hamed Firooz, Alon Halevy, Fabrizio Silvestri, Preslav Nakov, Tanmoy Chakraborty
- **Comment**: Accepted at IJCAI-ECAI 2022 (Survey Track) - Editorial Feedback
  Revised, 9 pages (7 main + 2 reference pages)
- **Journal**: None
- **Summary**: The automatic identification of harmful content online is of major concern for social media platforms, policymakers, and society. Researchers have studied textual, visual, and audio content, but typically in isolation. Yet, harmful content often combines multiple modalities, as in the case of memes, which are of particular interest due to their viral nature. With this in mind, here we offer a comprehensive survey with a focus on harmful memes. Based on a systematic analysis of recent literature, we first propose a new typology of harmful memes, and then we highlight and summarize the relevant state of the art. One interesting finding is that many types of harmful memes are not really studied, e.g., such featuring self-harm and extremism, partly due to the lack of suitable datasets. We further find that existing datasets mostly capture multi-class scenarios, which are not inclusive of the affective spectrum that memes can represent. Another observation is that memes can propagate globally through repackaging in different languages and that they can also be multilingual, blending different cultures. We conclude by highlighting several challenges related to multimodal semiotics, technological constraints, and non-trivial social engagement, and we present several open-ended aspects such as delineating online harm and empirically examining related frameworks and assistive interventions, which we believe will motivate and drive future research.



### TGANet: Text-guided attention for improved polyp segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.04280v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04280v1)
- **Published**: 2022-05-09 13:53:26+00:00
- **Updated**: 2022-05-09 13:53:26+00:00
- **Authors**: Nikhil Kumar Tomar, Debesh Jha, Ulas Bagci, Sharib Ali
- **Comment**: Provisional acceptance at 25th International Conference on Medical
  Image Computing and Computer Assisted Intervention (MICCAI'22)
- **Journal**: None
- **Summary**: Colonoscopy is a gold standard procedure but is highly operator-dependent. Automated polyp segmentation, a precancerous precursor, can minimize missed rates and timely treatment of colon cancer at an early stage. Even though there are deep learning methods developed for this task, variability in polyp size can impact model training, thereby limiting it to the size attribute of the majority of samples in the training dataset that may provide sub-optimal results to differently sized polyps. In this work, we exploit size-related and polyp number-related features in the form of text attention during training. We introduce an auxiliary classification task to weight the text-based embedding that allows network to learn additional feature representations that can distinctly adapt to differently sized polyps and can adapt to cases with multiple polyps. Our experimental results demonstrate that these added text embeddings improve the overall performance of the model compared to state-of-the-art segmentation methods. We explore four different datasets and provide insights for size-specific improvements. Our proposed text-guided attention network (TGANet) can generalize well to variable-sized polyps in different datasets.



### Siamese Object Tracking for Unmanned Aerial Vehicle: A Review and Comprehensive Analysis
- **Arxiv ID**: http://arxiv.org/abs/2205.04281v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04281v2)
- **Published**: 2022-05-09 13:53:34+00:00
- **Updated**: 2022-08-03 10:23:58+00:00
- **Authors**: Changhong Fu, Kunhan Lu, Guangze Zheng, Junjie Ye, Ziang Cao, Bowen Li, Geng Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Unmanned aerial vehicle (UAV)-based visual object tracking has enabled a wide range of applications and attracted increasing attention in the field of intelligent transportation systems because of its versatility and effectiveness. As an emerging force in the revolutionary trend of deep learning, Siamese networks shine in UAV-based object tracking with their promising balance of accuracy, robustness, and speed. Thanks to the development of embedded processors and the gradual optimization of deep neural networks, Siamese trackers receive extensive research and realize preliminary combinations with UAVs. However, due to the UAV's limited onboard computational resources and the complex real-world circumstances, aerial tracking with Siamese networks still faces severe obstacles in many aspects. To further explore the deployment of Siamese networks in UAV-based tracking, this work presents a comprehensive review of leading-edge Siamese trackers, along with an exhaustive UAV-specific analysis based on the evaluation using a typical UAV onboard processor. Then, the onboard tests are conducted to validate the feasibility and efficacy of representative Siamese trackers in real-world UAV deployment. Furthermore, to better promote the development of the tracking community, this work analyzes the limitations of existing Siamese trackers and conducts additional experiments represented by low-illumination evaluations. In the end, prospects for the development of Siamese tracking for UAV-based intelligent transportation systems are deeply discussed. The unified framework of leading-edge Siamese trackers, i.e., code library, and the results of their experimental evaluations are available at https://github.com/vision4robotics/SiameseTracking4UAV .



### Anatomy-aware Self-supervised Learning for Anomaly Detection in Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2205.04282v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04282v2)
- **Published**: 2022-05-09 13:53:40+00:00
- **Updated**: 2022-06-11 08:31:42+00:00
- **Authors**: Junya Sato, Yuki Suzuki, Tomohiro Wataya, Daiki Nishigaki, Kosuke Kita, Kazuki Yamagata, Noriyuki Tomiyama, Shoji Kido
- **Comment**: None
- **Journal**: None
- **Summary**: Large numbers of labeled medical images are essential for the accurate detection of anomalies, but manual annotation is labor-intensive and time-consuming. Self-supervised learning (SSL) is a training method to learn data-specific features without manual annotation. Several SSL-based models have been employed in medical image anomaly detection. These SSL methods effectively learn representations in several field-specific images, such as natural and industrial product images. However, owing to the requirement of medical expertise, typical SSL-based models are inefficient in medical image anomaly detection. We present an SSL-based model that enables anatomical structure-based unsupervised anomaly detection (UAD). The model employs the anatomy-aware pasting (AnatPaste) augmentation tool. AnatPaste employs a threshold-based lung segmentation pretext task to create anomalies in normal chest radiographs, which are used for model pretraining. These anomalies are similar to real anomalies and help the model recognize them. We evaluate our model on three opensource chest radiograph datasets. Our model exhibit area under curves (AUC) of 92.1%, 78.7%, and 81.9%, which are the highest among existing UAD models. This is the first SSL model to employ anatomical information as a pretext task. AnatPaste can be applied in various deep learning models and downstream tasks. It can be employed for other modalities by fixing appropriate segmentation. Our code is publicly available at: https://github.com/jun-sato/AnatPaste.



### Deeply Supervised Skin Lesions Diagnosis with Stage and Branch Attention
- **Arxiv ID**: http://arxiv.org/abs/2205.04326v6
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04326v6)
- **Published**: 2022-05-09 14:30:34+00:00
- **Updated**: 2022-08-23 05:50:01+00:00
- **Authors**: Wei Dai, Rui Liu, Tianyi Wu, Min Wang, Jianqin Yin, Jun Liu
- **Comment**: 11 pages, 9 figures
- **Journal**: None
- **Summary**: Accurate and unbiased examinations of skin lesions are critical for the early diagnosis and treatment of skin diseases. Visual features of skin lesions vary significantly because the images are collected from patients with different lesion colours and morphologies by using dissimilar imaging equipment. Recent studies have reported that ensembled convolutional neural networks (CNNs) are practical to classify the images for early diagnosis of skin disorders. However, the practical use of these ensembled CNNs is limited as these networks are heavyweight and inadequate for processing contextual information. Although lightweight networks (e.g., MobileNetV3 and EfficientNet) were developed to achieve parameters reduction for implementing deep neural networks on mobile devices, insufficient depth of feature representation restricts the performance. To address the existing limitations, we develop a new lite and effective neural network, namely HierAttn. The HierAttn applies a novel deep supervision strategy to learn the local and global features by using multi-stage and multi-branch attention mechanisms with only one training loss. The efficacy of HierAttn was evaluated by using the dermoscopy images dataset ISIC2019 and smartphone photos dataset PAD-UFES-20 (PAD2020). The experimental results show that HierAttn achieves the best accuracy and area under the curve (AUC) among the state-of-the-art lightweight networks. The code is available at https://github.com/anthonyweidai/HierAttn.



### SAN-Net: Learning Generalization to Unseen Sites for Stroke Lesion Segmentation with Self-Adaptive Normalization
- **Arxiv ID**: http://arxiv.org/abs/2205.04329v2
- **DOI**: 10.1016/j.compbiomed.2023.106717
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2205.04329v2)
- **Published**: 2022-05-09 14:33:06+00:00
- **Updated**: 2023-02-24 05:55:06+00:00
- **Authors**: Weiyi Yu, Zhizhong Huang, Junping Zhang, Hongming Shan
- **Comment**: 18 pages, 9 figures
- **Journal**: Computers in Biology and Medicine, 156, 106717, 2023
- **Summary**: There are considerable interests in automatic stroke lesion segmentation on magnetic resonance (MR) images in the medical imaging field, as stroke is an important cerebrovascular disease. Although deep learning-based models have been proposed for this task, generalizing these models to unseen sites is difficult due to not only the large inter-site discrepancy among different scanners, imaging protocols, and populations, but also the variations in stroke lesion shape, size, and location. To tackle this issue, we introduce a self-adaptive normalization network, termed SAN-Net, to achieve adaptive generalization on unseen sites for stroke lesion segmentation. Motivated by traditional z-score normalization and dynamic network, we devise a masked adaptive instance normalization (MAIN) to minimize inter-site discrepancies, which standardizes input MR images from different sites into a site-unrelated style by dynamically learning affine parameters from the input; \ie, MAIN can affinely transform the intensity values. Then, we leverage a gradient reversal layer to force the U-net encoder to learn site-invariant representation with a site classifier, which further improves the model generalization in conjunction with MAIN. Finally, inspired by the ``pseudosymmetry'' of the human brain, we introduce a simple yet effective data augmentation technique, termed symmetry-inspired data augmentation (SIDA), that can be embedded within SAN-Net to double the sample size while halving memory consumption. Experimental results on the benchmark Anatomical Tracings of Lesions After Stroke (ATLAS) v1.2 dataset, which includes MR images from 9 different sites, demonstrate that under the ``leave-one-site-out'' setting, the proposed SAN-Net outperforms recently published methods in terms of quantitative metrics and qualitative comparisons.



### Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation
- **Arxiv ID**: http://arxiv.org/abs/2205.04334v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04334v1)
- **Published**: 2022-05-09 14:34:55+00:00
- **Updated**: 2022-05-09 14:34:55+00:00
- **Authors**: Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caroline Pantofaru, Leonidas Guibas, Andrea Tagliasacchi, Frank Dellaert, Thomas Funkhouser
- **Comment**: CVPR 2022 paper. See project page at
  https://abhijitkundu.info/projects/pnf
- **Journal**: None
- **Summary**: We present Panoptic Neural Fields (PNF), an object-aware neural scene representation that decomposes a scene into a set of objects (things) and background (stuff). Each object is represented by an oriented 3D bounding box and a multi-layer perceptron (MLP) that takes position, direction, and time and outputs density and radiance. The background stuff is represented by a similar MLP that additionally outputs semantic labels. Each object MLPs are instance-specific and thus can be smaller and faster than previous object-aware approaches, while still leveraging category-specific priors incorporated via meta-learned initialization. Our model builds a panoptic radiance field representation of any scene from just color images. We use off-the-shelf algorithms to predict camera poses, object tracks, and 2D image semantic segmentations. Then we jointly optimize the MLP weights and bounding box parameters using analysis-by-synthesis with self-supervision from color images and pseudo-supervision from predicted semantic segmentations. During experiments with real-world dynamic scenes, we find that our model can be used effectively for several tasks like novel view synthesis, 2D panoptic segmentation, 3D scene editing, and multiview depth prediction.



### Object Detection with Spiking Neural Networks on Automotive Event Data
- **Arxiv ID**: http://arxiv.org/abs/2205.04339v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04339v1)
- **Published**: 2022-05-09 14:39:47+00:00
- **Updated**: 2022-05-09 14:39:47+00:00
- **Authors**: Lo√Øc Cordone, Beno√Æt Miramond, Philippe Thierion
- **Comment**: Accepted to the International Joint Conference on Neural Networks
  (IJCNN) 2022
- **Journal**: None
- **Summary**: Automotive embedded algorithms have very high constraints in terms of latency, accuracy and power consumption. In this work, we propose to train spiking neural networks (SNNs) directly on data coming from event cameras to design fast and efficient automotive embedded applications. Indeed, SNNs are more biologically realistic neural networks where neurons communicate using discrete and asynchronous spikes, a naturally energy-efficient and hardware friendly operating mode. Event data, which are binary and sparse in space and time, are therefore the ideal input for spiking neural networks. But to date, their performance was insufficient for automotive real-world problems, such as detecting complex objects in an uncontrolled environment. To address this issue, we took advantage of the latest advancements in matter of spike backpropagation - surrogate gradient learning, parametric LIF, SpikingJelly framework - and of our new \textit{voxel cube} event encoding to train 4 different SNNs based on popular deep learning networks: SqueezeNet, VGG, MobileNet, and DenseNet. As a result, we managed to increase the size and the complexity of SNNs usually considered in the literature. In this paper, we conducted experiments on two automotive event datasets, establishing new state-of-the-art classification results for spiking neural networks. Based on these results, we combined our SNNs with SSD to propose the first spiking neural networks capable of performing object detection on the complex GEN1 Automotive Detection event dataset.



### Classification and mapping of low-statured 'shrubland' cover types in post-agricultural landscapes of the US Northeast
- **Arxiv ID**: http://arxiv.org/abs/2205.05047v2
- **DOI**: 10.1080/01431161.2022.2155086
- **Categories**: **cs.CV**, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2205.05047v2)
- **Published**: 2022-05-09 14:54:41+00:00
- **Updated**: 2022-12-21 14:28:28+00:00
- **Authors**: Michael J Mahoney, Lucas K Johnson, Abigail Z Guinan, Colin M Beier
- **Comment**: 43 pages (35 main text, 8 supplementary materials); 11 figures (10
  main text, 1 supplementary materials), 10 tables (4 main text, 6
  supplementary materials)
- **Journal**: The International Journal of Remote Sensing 43(19-24), (2022),
  7117-7138
- **Summary**: Novel plant communities reshape landscapes and pose challenges for land cover classification and mapping that can constrain research and stewardship efforts. In the US Northeast, emergence of low-statured woody vegetation, or shrublands, instead of secondary forests in post-agricultural landscapes is well-documented by field studies, but poorly understood from a landscape perspective, which limits the ability to systematically study and manage these lands. To address gaps in classification/mapping of low-statured cover types where they have been historically rare, we developed models to predict shrubland distributions at 30m resolution across New York State (NYS), using a stacked ensemble combining a random forest, gradient boosting machine, and artificial neural network to integrate remote sensing of structural (airborne LIDAR) and optical (satellite imagery) properties of vegetation cover. We first classified a 1m canopy height model (CHM), derived from a patchwork of available LIDAR coverages, to define shrubland presence/absence. Next, these non-contiguous maps were used to train a model ensemble based on temporally-segmented imagery to predict shrubland probability for the entire study landscape (NYS). Approximately 2.5% of the CHM coverage area was classified as shrubland. Models using Landsat predictors trained on the classified CHM were effective at identifying shrubland (test set AUC=0.893, real-world AUC=0.904), in discriminating between shrub/young forest and other cover classes, and produced qualitatively sensible maps, even when extending beyond the original training data. Our results suggest that incorporation of airborne LiDAR, even from a discontinuous patchwork of coverages, can improve land cover classification of historically rare but increasingly prevalent shrubland habitats across broader areas.



### A Novel Augmented Reality Ultrasound Framework Using an RGB-D Camera and a 3D-printed Marker
- **Arxiv ID**: http://arxiv.org/abs/2205.04350v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04350v1)
- **Published**: 2022-05-09 14:54:47+00:00
- **Updated**: 2022-05-09 14:54:47+00:00
- **Authors**: Yitian Zhou, Ga√©tan Lelu, Boris Labb√©, Guillaume Pasquier, Pierre Le Gargasson, Albert Murienne, Laurent Launay
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Purpose. Ability to locate and track ultrasound images in the 3D operating space is of great benefit for multiple clinical applications. This is often accomplished by tracking the probe using a precise but expensive optical or electromagnetic tracking system. Our goal is to develop a simple and low cost augmented reality echography framework using a standard RGB-D Camera.   Methods. A prototype system consisting of an Occipital Structure Core RGB-D camera, a specifically-designed 3D marker, and a fast point cloud registration algorithm FaVoR was developed and evaluated on an Ultrasonix ultrasound system. The probe was calibrated on a 3D-printed N-wire phantom using the software PLUS toolkit. The proposed calibration method is simplified, requiring no additional markers or sensors attached to the phantom. Also, a visualization software based on OpenGL was developed for the augmented reality application.   Results. The calibrated probe was used to augment a real-world video in a simulated needle insertion scenario. The ultrasound images were rendered on the video, and visually-coherent results were observed. We evaluated the end-to-end accuracy of our AR US framework on localizing a cube of 5 cm size. From our two experiments, the target pose localization error ranges from 5.6 to 5.9 mm and from -3.9 to 4.2 degrees.   Conclusion. We believe that with the potential democratization of RGB-D cameras integrated in mobile devices and AR glasses in the future, our prototype solution may facilitate the use of 3D freehand ultrasound in clinical routine. Future work should include a more rigorous and thorough evaluation, by comparing the calibration accuracy with those obtained by commercial tracking solutions in both simulated and real medical scenarios.



### Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2205.04363v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04363v2)
- **Published**: 2022-05-09 15:05:24+00:00
- **Updated**: 2022-06-08 02:20:39+00:00
- **Authors**: Chia-Wen Kuo, Zsolt Kira
- **Comment**: paper accepted in CVPR 2022
- **Journal**: None
- **Summary**: Significant progress has been made on visual captioning, largely relying on pre-trained features and later fixed object detectors that serve as rich inputs to auto-regressive models. A key limitation of such methods, however, is that the output of the model is conditioned only on the object detector's outputs. The assumption that such outputs can represent all necessary information is unrealistic, especially when the detector is transferred across datasets. In this work, we reason about the graphical model induced by this assumption, and propose to add an auxiliary input to represent missing information such as object relationships. We specifically propose to mine attributes and relationships from the Visual Genome dataset and condition the captioning model on them. Crucially, we propose (and show to be important) the use of a multi-modal pre-trained model (CLIP) to retrieve such contextual descriptions. Further, object detector models are frozen and do not have sufficient richness to allow the captioning model to properly ground them. As a result, we propose to condition both the detector and description outputs on the image, and show qualitatively and quantitatively that this can improve grounding. We validate our method on image captioning, perform thorough analyses of each component and importance of the pre-trained multi-modal model, and demonstrate significant improvements over the current state of the art, specifically +7.5% in CIDEr and +1.3% in BLEU-4 metrics.



### Towards Measuring Domain Shift in Histopathological Stain Translation in an Unsupervised Manner
- **Arxiv ID**: http://arxiv.org/abs/2205.04368v1
- **DOI**: 10.1109/ISBI52829.2022.9761411
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04368v1)
- **Published**: 2022-05-09 15:18:12+00:00
- **Updated**: 2022-05-09 15:18:12+00:00
- **Authors**: Zeeshan Nisar, Jelica Vasiljeviƒá, Pierre Gan√ßarski, Thomas Lampert
- **Comment**: 5 pages, 3 figures, 2 tables
- **Journal**: ISBI, 2022, pp. 1-5
- **Summary**: Domain shift in digital histopathology can occur when different stains or scanners are used, during stain translation, etc. A deep neural network trained on source data may not generalise well to data that has undergone some domain shift. An important step towards being robust to domain shift is the ability to detect and measure it. This article demonstrates that the PixelCNN and domain shift metric can be used to detect and quantify domain shift in digital histopathology, and they demonstrate a strong correlation with generalisation performance. These findings pave the way for a mechanism to infer the average performance of a model (trained on source data) on unseen and unlabelled target data.



### FlowBot3D: Learning 3D Articulation Flow to Manipulate Articulated Objects
- **Arxiv ID**: http://arxiv.org/abs/2205.04382v5
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04382v5)
- **Published**: 2022-05-09 15:35:33+00:00
- **Updated**: 2023-01-29 21:02:02+00:00
- **Authors**: Ben Eisner, Harry Zhang, David Held
- **Comment**: Accepted to Robotics Science and Systems (RSS) 2022, Best Paper
  Finalist
- **Journal**: None
- **Summary**: We explore a novel method to perceive and manipulate 3D articulated objects that generalizes to enable a robot to articulate unseen classes of objects. We propose a vision-based system that learns to predict the potential motions of the parts of a variety of articulated objects to guide downstream motion planning of the system to articulate the objects. To predict the object motions, we train a neural network to output a dense vector field representing the point-wise motion direction of the points in the point cloud under articulation. We then deploy an analytical motion planner based on this vector field to achieve a policy that yields maximum articulation. We train the vision system entirely in simulation, and we demonstrate the capability of our system to generalize to unseen object instances and novel categories in both simulation and the real world, deploying our policy on a Sawyer robot with no finetuning. Results show that our system achieves state-of-the-art performance in both simulated and real-world experiments.



### Online Unsupervised Domain Adaptation for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2205.04383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04383v1)
- **Published**: 2022-05-09 15:36:08+00:00
- **Updated**: 2022-05-09 15:36:08+00:00
- **Authors**: Hamza Rami, Matthieu Ospici, St√©phane Lathuili√®re
- **Comment**: To appear in the IEEE Conference on Computer Vision and Pattern
  Recognition Workshop (CVPR-W) on Continual Learning in Computer Vision
  (CLVision) 2022
- **Journal**: None
- **Summary**: Unsupervised domain adaptation for person re-identification (Person Re-ID) is the task of transferring the learned knowledge on the labeled source domain to the unlabeled target domain. Most of the recent papers that address this problem adopt an offline training setting. More precisely, the training of the Re-ID model is done assuming that we have access to the complete training target domain data set. In this paper, we argue that the target domain generally consists of a stream of data in a practical real-world application, where data is continuously increasing from the different network's cameras. The Re-ID solutions are also constrained by confidentiality regulations stating that the collected data can be stored for only a limited period, hence the model can no longer get access to previously seen target images. Therefore, we present a new yet practical online setting for Unsupervised Domain Adaptation for person Re-ID with two main constraints: Online Adaptation and Privacy Protection. We then adapt and evaluate the state-of-the-art UDA algorithms on this new online setting using the well-known Market-1501, Duke, and MSMT17 benchmarks.



### Detecting the Role of an Entity in Harmful Memes: Techniques and Their Limitations
- **Arxiv ID**: http://arxiv.org/abs/2205.04402v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM, cs.SI, 68T50, I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2205.04402v1)
- **Published**: 2022-05-09 16:11:04+00:00
- **Updated**: 2022-05-09 16:11:04+00:00
- **Authors**: Rabindra Nath Nandi, Firoj Alam, Preslav Nakov
- **Comment**: Accepted at CONSTRAINT 2022 (Colocated with ACL-2022),
  disinformation, misinformation, factuality, harmfulness, fake news,
  propaganda, multimodality, text, images, videos, network structure,
  temporality
- **Journal**: None
- **Summary**: Harmful or abusive online content has been increasing over time, raising concerns for social media platforms, government agencies, and policymakers. Such harmful or abusive content can have major negative impact on society, e.g., cyberbullying can lead to suicides, rumors about COVID-19 can cause vaccine hesitance, promotion of fake cures for COVID-19 can cause health harms and deaths. The content that is posted and shared online can be textual, visual, or a combination of both, e.g., in a meme. Here, we describe our experiments in detecting the roles of the entities (hero, villain, victim) in harmful memes, which is part of the CONSTRAINT-2022 shared task, as well as our system for the task. We further provide a comparative analysis of different experimental settings (i.e., unimodal, multimodal, attention, and augmentation). For reproducibility, we make our experimental code publicly available. \url{https://github.com/robi56/harmful_memes_block_fusion}



### TeamX@DravidianLangTech-ACL2022: A Comparative Analysis for Troll-Based Meme Classification
- **Arxiv ID**: http://arxiv.org/abs/2205.04404v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.MM, cs.SI, 68T50, I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2205.04404v1)
- **Published**: 2022-05-09 16:19:28+00:00
- **Updated**: 2022-05-09 16:19:28+00:00
- **Authors**: Rabindra Nath Nandi, Firoj Alam, Preslav Nakov
- **Comment**: Accepted at DravidianLangTech-ACL2022 (Colocated with ACL-2022).
  disinformation, misinformation, factuality, harmfulness, fake news,
  propaganda, multimodality, text, images, videos, network structure,
  temporality
- **Journal**: None
- **Summary**: The spread of fake news, propaganda, misinformation, disinformation, and harmful content online raised concerns among social media platforms, government agencies, policymakers, and society as a whole. This is because such harmful or abusive content leads to several consequences to people such as physical, emotional, relational, and financial. Among different harmful content \textit{trolling-based} online content is one of them, where the idea is to post a message that is provocative, offensive, or menacing with an intent to mislead the audience. The content can be textual, visual, a combination of both, or a meme. In this study, we provide a comparative analysis of troll-based memes classification using the textual, visual, and multimodal content. We report several interesting findings in terms of code-mixed text, multimodal setting, and combining an additional dataset, which shows improvements over the majority baseline.



### Robustness of Humans and Machines on Object Recognition with Extreme Image Transformations
- **Arxiv ID**: http://arxiv.org/abs/2205.05167v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2205.05167v2)
- **Published**: 2022-05-09 17:15:54+00:00
- **Updated**: 2022-05-27 05:04:35+00:00
- **Authors**: Dakarai Crowder, Girik Malik
- **Comment**: Accepted at CVPR NeuroVision Workshop
- **Journal**: None
- **Summary**: Recent neural network architectures have claimed to explain data from the human visual cortex. Their demonstrated performance is however still limited by the dependence on exploiting low-level features for solving visual tasks. This strategy limits their performance in case of out-of-distribution/adversarial data. Humans, meanwhile learn abstract concepts and are mostly unaffected by even extreme image distortions. Humans and networks employ strikingly different strategies to solve visual tasks. To probe this, we introduce a novel set of image transforms and evaluate humans and networks on an object recognition task. We found performance for a few common networks quickly decreases while humans are able to recognize objects with a high accuracy.



### Skin disease diagnosis using image analysis and natural language processing
- **Arxiv ID**: http://arxiv.org/abs/2205.04468v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04468v1)
- **Published**: 2022-05-09 17:23:31+00:00
- **Updated**: 2022-05-09 17:23:31+00:00
- **Authors**: Martin Chileshe, Mayumbo Nyirenda
- **Comment**: None
- **Journal**: None
- **Summary**: In Zambia, there is a serious shortage of medical staff where each practitioner attends to about 17000 patients in a given district while still, other patients travel over 10 km to access the basic medical services. In this research, we implement a deep learning model that can perform the clinical diagnosis process. The study will prove whether image analysis is capable of performing clinical diagnosis. It will also enable us to understand if we can use image analysis to lessen the workload on medical practitioners by delegating some tasks to an AI. The success of this study has the potential to increase the accessibility of medical services to Zambians, which is one of the national goals of Vision 2030.



### Activating More Pixels in Image Super-Resolution Transformer
- **Arxiv ID**: http://arxiv.org/abs/2205.04437v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04437v3)
- **Published**: 2022-05-09 17:36:58+00:00
- **Updated**: 2023-03-19 01:25:49+00:00
- **Authors**: Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, Chao Dong
- **Comment**: Accepted to CVPR2023
- **Journal**: None
- **Summary**: Transformer-based methods have shown impressive performance in low-level vision tasks, such as image super-resolution. However, we find that these networks can only utilize a limited spatial range of input information through attribution analysis. This implies that the potential of Transformer is still not fully exploited in existing networks. In order to activate more input pixels for better reconstruction, we propose a novel Hybrid Attention Transformer (HAT). It combines both channel attention and window-based self-attention schemes, thus making use of their complementary advantages of being able to utilize global statistics and strong local fitting capability. Moreover, to better aggregate the cross-window information, we introduce an overlapping cross-attention module to enhance the interaction between neighboring window features. In the training stage, we additionally adopt a same-task pre-training strategy to exploit the potential of the model for further improvement. Extensive experiments show the effectiveness of the proposed modules, and we further scale up the model to demonstrate that the performance of this task can be greatly improved. Our overall method significantly outperforms the state-of-the-art methods by more than 1dB. Codes and models are available at https://github.com/XPixelGroup/HAT.



### MixAugment & Mixup: Augmentation Methods for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.04442v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04442v1)
- **Published**: 2022-05-09 17:43:08+00:00
- **Updated**: 2022-05-09 17:43:08+00:00
- **Authors**: Andreas Psaroudakis, Dimitrios Kollias
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic Facial Expression Recognition (FER) has attracted increasing attention in the last 20 years since facial expressions play a central role in human communication. Most FER methodologies utilize Deep Neural Networks (DNNs) that are powerful tools when it comes to data analysis. However, despite their power, these networks are prone to overfitting, as they often tend to memorize the training data. What is more, there are not currently a lot of in-the-wild (i.e. in unconstrained environment) large databases for FER. To alleviate this issue, a number of data augmentation techniques have been proposed. Data augmentation is a way to increase the diversity of available data by applying constrained transformations on the original data. One such technique, which has positively contributed to various classification tasks, is Mixup. According to this, a DNN is trained on convex combinations of pairs of examples and their corresponding labels. In this paper, we examine the effectiveness of Mixup for in-the-wild FER in which data have large variations in head poses, illumination conditions, backgrounds and contexts. We then propose a new data augmentation strategy which is based on Mixup, called MixAugment. According to this, the network is trained concurrently on a combination of virtual examples and real examples; all these examples contribute to the overall loss function. We conduct an extensive experimental study that proves the effectiveness of MixAugment over Mixup and various state-of-the-art methods. We further investigate the combination of dropout with Mixup and MixAugment, as well as the combination of other data augmentation techniques with MixAugment.



### Introspective Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.04449v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04449v1)
- **Published**: 2022-05-09 17:51:44+00:00
- **Updated**: 2022-05-09 17:51:44+00:00
- **Authors**: Chengkun Wang, Wenzhao Zheng, Zheng Zhu, Jie Zhou, Jiwen Lu
- **Comment**: Source code available at https://github.com/wangck20/IDML
- **Journal**: None
- **Summary**: This paper proposes an introspective deep metric learning (IDML) framework for uncertainty-aware comparisons of images. Conventional deep metric learning methods produce confident semantic distances between images regardless of the uncertainty level. However, we argue that a good similarity model should consider the semantic discrepancies with caution to better deal with ambiguous images for more robust training. To achieve this, we propose to represent an image using not only a semantic embedding but also an accompanying uncertainty embedding, which describes the semantic characteristics and ambiguity of an image, respectively. We further propose an introspective similarity metric to make similarity judgments between images considering both their semantic differences and ambiguities. Our framework attains state-of-the-art performance on the widely used CUB-200-2011, Cars196, and Stanford Online Products datasets for image retrieval. We further evaluate our framework for image classification on the ImageNet-1K, CIFAR-10, and CIFAR-100 datasets, which shows that equipping existing data mixing methods with the proposed introspective metric consistently achieves better results (e.g., +0.44 for CutMix on ImageNet-1K). Code is available at: https://github.com/wangck20/IDML.



### OpenPodcar: an Open Source Vehicle for Self-Driving Car Research
- **Arxiv ID**: http://arxiv.org/abs/2205.04454v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04454v1)
- **Published**: 2022-05-09 17:55:56+00:00
- **Updated**: 2022-05-09 17:55:56+00:00
- **Authors**: Fanta Camara, Chris Waltham, Grey Churchill, Charles Fox
- **Comment**: Under review
- **Journal**: None
- **Summary**: OpenPodcar is a low-cost, open source hardware and software, autonomous vehicle research platform based on an off-the-shelf, hard-canopy, mobility scooter donor vehicle. Hardware and software build instructions are provided to convert the donor vehicle into a low-cost and fully autonomous platform. The open platform consists of (a) hardware components: CAD designs, bill of materials, and build instructions; (b) Arduino, ROS and Gazebo control and simulation software files which provide standard ROS interfaces and simulation of the vehicle; and (c) higher-level ROS software implementations and configurations of standard robot autonomous planning and control, including the move_base interface with Timed-Elastic-Band planner which enacts commands to drive the vehicle from a current to a desired pose around obstacles. The vehicle is large enough to transport a human passenger or similar load at speeds up to 15km/h, for example for use as a last-mile autonomous taxi service or to transport delivery containers similarly around a city center. It is small and safe enough to be parked in a standard research lab and be used for realistic human-vehicle interaction studies. System build cost from new components is around USD7,000 in total in 2022. OpenPodcar thus provides a good balance between real world utility, safety, cost and research convenience.



### Multiview Stereo with Cascaded Epipolar RAFT
- **Arxiv ID**: http://arxiv.org/abs/2205.04502v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04502v1)
- **Published**: 2022-05-09 18:17:05+00:00
- **Updated**: 2022-05-09 18:17:05+00:00
- **Authors**: Zeyu Ma, Zachary Teed, Jia Deng
- **Comment**: None
- **Journal**: None
- **Summary**: We address multiview stereo (MVS), an important 3D vision task that reconstructs a 3D model such as a dense point cloud from multiple calibrated images. We propose CER-MVS (Cascaded Epipolar RAFT Multiview Stereo), a new approach based on the RAFT (Recurrent All-Pairs Field Transforms) architecture developed for optical flow. CER-MVS introduces five new changes to RAFT: epipolar cost volumes, cost volume cascading, multiview fusion of cost volumes, dynamic supervision, and multiresolution fusion of depth maps. CER-MVS is significantly different from prior work in multiview stereo. Unlike prior work, which operates by updating a 3D cost volume, CER-MVS operates by updating a disparity field. Furthermore, we propose an adaptive thresholding method to balance the completeness and accuracy of the reconstructed point clouds. Experiments show that our approach achieves competitive performance on DTU (the second best among known results) and state-of-the-art performance on the Tanks-and-Temples benchmark (both the intermediate and advanced set). Code is available at https://github.com/princeton-vl/CER-MVS



### Image2Gif: Generating Continuous Realistic Animations with Warping NODEs
- **Arxiv ID**: http://arxiv.org/abs/2205.04519v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04519v1)
- **Published**: 2022-05-09 18:39:47+00:00
- **Updated**: 2022-05-09 18:39:47+00:00
- **Authors**: Jurijs Nazarovs, Zhichun Huang
- **Comment**: AI for Content Creation Workshop, CVPR 2022
- **Journal**: None
- **Summary**: Generating smooth animations from a limited number of sequential observations has a number of applications in vision. For example, it can be used to increase number of frames per second, or generating a new trajectory only based on first and last frames, e.g. a motion of face emotions. Despite the discrete observed data (frames), the problem of generating a new trajectory is a continues problem. In addition, to be perceptually realistic, the domain of an image should not alter drastically through the trajectory of changes. In this paper, we propose a new framework, Warping Neural ODE, for generating a smooth animation (video frame interpolation) in a continuous manner, given two ("farther apart") frames, denoting the start and the end of the animation. The key feature of our framework is utilizing the continuous spatial transformation of the image based on the vector field, derived from a system of differential equations. This allows us to achieve the smoothness and the realism of an animation with infinitely small time steps between the frames. We show the application of our work in generating an animation given two frames, in different training settings, including Generative Adversarial Network (GAN) and with $L_2$ loss.



### Surreal-GAN:Semi-Supervised Representation Learning via GAN for uncovering heterogeneous disease-related imaging patterns
- **Arxiv ID**: http://arxiv.org/abs/2205.04523v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04523v1)
- **Published**: 2022-05-09 19:09:28+00:00
- **Updated**: 2022-05-09 19:09:28+00:00
- **Authors**: Zhijian Yang, Junhao Wen, Christos Davatzikos
- **Comment**: None
- **Journal**: None
- **Summary**: A plethora of machine learning methods have been applied to imaging data, enabling the construction of clinically relevant imaging signatures of neurological and neuropsychiatric diseases. Oftentimes, such methods don't explicitly model the heterogeneity of disease effects, or approach it via nonlinear models that are not interpretable. Moreover, unsupervised methods may parse heterogeneity that is driven by nuisance confounding factors that affect brain structure or function, rather than heterogeneity relevant to a pathology of interest. On the other hand, semi-supervised clustering methods seek to derive a dichotomous subtype membership, ignoring the truth that disease heterogeneity spatially and temporally extends along a continuum. To address the aforementioned limitations, herein, we propose a novel method, termed Surreal-GAN (Semi-SUpeRvised ReprEsentAtion Learning via GAN). Using cross-sectional imaging data, Surreal-GAN dissects underlying disease-related heterogeneity under the principle of semi-supervised clustering (cluster mappings from normal control to patient), proposes a continuously dimensional representation, and infers the disease severity of patients at individual level along each dimension. The model first learns a transformation function from normal control (CN) domain to the patient (PT) domain with latent variables controlling transformation directions. An inverse mapping function together with regularization on function continuity, pattern orthogonality and monotonicity was also imposed to make sure that the transformation function captures necessarily meaningful imaging patterns with clinical significance. We first validated the model through extensive semi-synthetic experiments, and then demonstrate its potential in capturing biologically plausible imaging patterns in Alzheimer's disease (AD).



### How Does Frequency Bias Affect the Robustness of Neural Image Classifiers against Common Corruption and Adversarial Perturbations?
- **Arxiv ID**: http://arxiv.org/abs/2205.04533v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04533v1)
- **Published**: 2022-05-09 20:09:31+00:00
- **Updated**: 2022-05-09 20:09:31+00:00
- **Authors**: Alvin Chan, Yew-Soon Ong, Clement Tan
- **Comment**: IJCAI 2022 Long Oral, Camera-ready full version
- **Journal**: None
- **Summary**: Model robustness is vital for the reliable deployment of machine learning models in real-world applications. Recent studies have shown that data augmentation can result in model over-relying on features in the low-frequency domain, sacrificing performance against low-frequency corruptions, highlighting a connection between frequency and robustness. Here, we take one step further to more directly study the frequency bias of a model through the lens of its Jacobians and its implication to model robustness. To achieve this, we propose Jacobian frequency regularization for models' Jacobians to have a larger ratio of low-frequency components. Through experiments on four image datasets, we show that biasing classifiers towards low (high)-frequency components can bring performance gain against high (low)-frequency corruption and adversarial perturbation, albeit with a tradeoff in performance for low (high)-frequency corruption. Our approach elucidates a more direct connection between the frequency bias and robustness of deep learning models.



### Is my Depth Ground-Truth Good Enough? HAMMER -- Highly Accurate Multi-Modal Dataset for DEnse 3D Scene Regression
- **Arxiv ID**: http://arxiv.org/abs/2205.04565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04565v1)
- **Published**: 2022-05-09 21:25:09+00:00
- **Updated**: 2022-05-09 21:25:09+00:00
- **Authors**: HyunJun Jung, Patrick Ruhkamp, Guangyao Zhai, Nikolas Brasch, Yitong Li, Yannick Verdie, Jifei Song, Yiren Zhou, Anil Armagan, Slobodan Ilic, Ales Leonardis, Benjamin Busam
- **Comment**: None
- **Journal**: None
- **Summary**: Depth estimation is a core task in 3D computer vision. Recent methods investigate the task of monocular depth trained with various depth sensor modalities. Every sensor has its advantages and drawbacks caused by the nature of estimates. In the literature, mostly mean average error of the depth is investigated and sensor capabilities are typically not discussed. Especially indoor environments, however, pose challenges for some devices. Textureless regions pose challenges for structure from motion, reflective materials are problematic for active sensing, and distances for translucent material are intricate to measure with existing sensors. This paper proposes HAMMER, a dataset comprising depth estimates from multiple commonly used sensors for indoor depth estimation, namely ToF, stereo, structured light together with monocular RGB+P data. We construct highly reliable ground truth depth maps with the help of 3D scanners and aligned renderings. A popular depth estimators is trained on this data and typical depth senosors. The estimates are extensively analyze on different scene structures. We notice generalization issues arising from various sensor technologies in household environments with challenging but everyday scene content. HAMMER, which we make publicly available, provides a reliable base to pave the way to targeted depth improvements and sensor fusion approaches.



### When does dough become a bagel? Analyzing the remaining mistakes on ImageNet
- **Arxiv ID**: http://arxiv.org/abs/2205.04596v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04596v2)
- **Published**: 2022-05-09 23:25:45+00:00
- **Updated**: 2022-05-25 20:27:16+00:00
- **Authors**: Vijay Vasudevan, Benjamin Caine, Raphael Gontijo-Lopes, Sara Fridovich-Keil, Rebecca Roelofs
- **Comment**: Data and analysis available at
  https://github.com/google-research/imagenet-mistakes
- **Journal**: None
- **Summary**: Image classification accuracy on the ImageNet dataset has been a barometer for progress in computer vision over the last decade. Several recent papers have questioned the degree to which the benchmark remains useful to the community, yet innovations continue to contribute gains to performance, with today's largest models achieving 90%+ top-1 accuracy. To help contextualize progress on ImageNet and provide a more meaningful evaluation for today's state-of-the-art models, we manually review and categorize every remaining mistake that a few top models make in order to provide insight into the long-tail of errors on one of the most benchmarked datasets in computer vision. We focus on the multi-label subset evaluation of ImageNet, where today's best models achieve upwards of 97% top-1 accuracy. Our analysis reveals that nearly half of the supposed mistakes are not mistakes at all, and we uncover new valid multi-labels, demonstrating that, without careful review, we are significantly underestimating the performance of these models. On the other hand, we also find that today's best models still make a significant number of mistakes (40%) that are obviously wrong to human reviewers. To calibrate future progress on ImageNet, we provide an updated multi-label evaluation set, and we curate ImageNet-Major: a 68-example "major error" slice of the obvious mistakes made by today's top models -- a slice where models should achieve near perfection, but today are far from doing so.



