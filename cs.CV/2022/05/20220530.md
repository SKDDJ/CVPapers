# Arxiv Papers in cs.CV on 2022-05-30
### Your Contrastive Learning Is Secretly Doing Stochastic Neighbor Embedding
- **Arxiv ID**: http://arxiv.org/abs/2205.14814v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.14814v2)
- **Published**: 2022-05-30 02:39:29+00:00
- **Updated**: 2023-06-02 08:55:43+00:00
- **Authors**: Tianyang Hu, Zhili Liu, Fengwei Zhou, Wenjia Wang, Weiran Huang
- **Comment**: Accepted by ICLR 2023
- **Journal**: None
- **Summary**: Contrastive learning, especially self-supervised contrastive learning (SSCL), has achieved great success in extracting powerful features from unlabeled data. In this work, we contribute to the theoretical understanding of SSCL and uncover its connection to the classic data visualization method, stochastic neighbor embedding (SNE), whose goal is to preserve pairwise distances. From the perspective of preserving neighboring information, SSCL can be viewed as a special case of SNE with the input space pairwise similarities specified by data augmentation. The established correspondence facilitates deeper theoretical understanding of learned features of SSCL, as well as methodological guidelines for practical improvement. Specifically, through the lens of SNE, we provide novel analysis on domain-agnostic augmentations, implicit bias and robustness of learned features. To illustrate the practical advantage, we demonstrate that the modifications from SNE to $t$-SNE can also be adopted in the SSCL setting, achieving significant improvement in both in-distribution and out-of-distribution generalization.



### Exposing Fine-Grained Adversarial Vulnerability of Face Anti-Spoofing Models
- **Arxiv ID**: http://arxiv.org/abs/2205.14851v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.14851v3)
- **Published**: 2022-05-30 04:56:33+00:00
- **Updated**: 2023-05-02 03:03:29+00:00
- **Authors**: Songlin Yang, Wei Wang, Chenye Xu, Ziwen He, Bo Peng, Jing Dong
- **Comment**: Accepted by IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) Workshop, 2023
- **Journal**: None
- **Summary**: Face anti-spoofing aims to discriminate the spoofing face images (e.g., printed photos) from live ones. However, adversarial examples greatly challenge its credibility, where adding some perturbation noise can easily change the predictions. Previous works conducted adversarial attack methods to evaluate the face anti-spoofing performance without any fine-grained analysis that which model architecture or auxiliary feature is vulnerable to the adversary. To handle this problem, we propose a novel framework to expose the fine-grained adversarial vulnerability of the face anti-spoofing models, which consists of a multitask module and a semantic feature augmentation (SFA) module. The multitask module can obtain different semantic features for further evaluation, but only attacking these semantic features fails to reflect the discrimination-related vulnerability. We then design the SFA module to introduce the data distribution prior for more discrimination-related gradient directions for generating adversarial examples. Comprehensive experiments show that SFA module increases the attack success rate by nearly 40$\%$ on average. We conduct this fine-grained adversarial analysis on different annotations, geometric maps, and backbone networks (e.g., Resnet network). These fine-grained adversarial examples can be used for selecting robust backbone networks and auxiliary features. They also can be used for adversarial training, which makes it practical to further improve the accuracy and robustness of the face anti-spoofing models.



### Benchmarking Unsupervised Anomaly Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/2205.14852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.14852v1)
- **Published**: 2022-05-30 04:57:25+00:00
- **Updated**: 2022-05-30 04:57:25+00:00
- **Authors**: Ye Zheng, Xiang Wang, Yu Qi, Wei Li, Liwei Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised anomaly detection and localization, as of one the most practical and challenging problems in computer vision, has received great attention in recent years. From the time the MVTec AD dataset was proposed to the present, new research methods that are constantly being proposed push its precision to saturation. It is the time to conduct a comprehensive comparison of existing methods to inspire further research. This paper extensively compares 13 papers in terms of the performance in unsupervised anomaly detection and localization tasks, and adds a comparison of inference efficiency previously ignored by the community. Meanwhile, analysis of the MVTec AD dataset are also given, especially the label ambiguity that affects the model fails to achieve full marks. Moreover, considering the proposal of the new MVTec 3D-AD dataset, this paper also conducts experiments using the existing state-of-the-art 2D methods on this new dataset, and reports the corresponding results with analysis.



### Prompt-aligned Gradient for Prompt Tuning
- **Arxiv ID**: http://arxiv.org/abs/2205.14865v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.14865v1)
- **Published**: 2022-05-30 06:05:21+00:00
- **Updated**: 2022-05-30 06:05:21+00:00
- **Authors**: Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, Hanwang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Thanks to the large pre-trained vision-language models (VLMs) like CLIP, we can craft a zero-shot classifier by "prompt", e.g., the confidence score of an image being "[CLASS]" can be obtained by using the VLM provided similarity measure between the image and the prompt sentence "a photo of a [CLASS]". Therefore, prompt shows a great potential for fast adaptation of VLMs to downstream tasks if we fine-tune the prompt-based similarity measure. However, we find a common failure that improper fine-tuning may not only undermine the prompt's inherent prediction for the task-related classes, but also for other classes in the VLM vocabulary. Existing methods still address this problem by using traditional anti-overfitting techniques such as early stopping and data augmentation, which lack a principled solution specific to prompt. We present Prompt-aligned Gradient, dubbed ProGrad, to prevent prompt tuning from forgetting the the general knowledge learned from VLMs. In particular, ProGrad only updates the prompt whose gradient is aligned (or non-conflicting) to the "general direction", which is represented as the gradient of the KL loss of the pre-defined prompt prediction. Extensive experiments demonstrate the stronger few-shot generalization ability of ProGrad over state-of-the-art prompt tuning methods. Codes are available at https://github.com/BeierZhu/Prompt-align.



### Compressible-composable NeRF via Rank-residual Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2205.14870v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.14870v2)
- **Published**: 2022-05-30 06:18:59+00:00
- **Updated**: 2022-10-11 08:07:13+00:00
- **Authors**: Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, Gang Zeng
- **Comment**: NeurIPS 2022 camera-ready version
- **Journal**: None
- **Summary**: Neural Radiance Field (NeRF) has emerged as a compelling method to represent 3D objects and scenes for photo-realistic rendering. However, its implicit representation causes difficulty in manipulating the models like the explicit mesh representation. Several recent advances in NeRF manipulation are usually restricted by a shared renderer network, or suffer from large model size. To circumvent the hurdle, in this paper, we present an explicit neural field representation that enables efficient and convenient manipulation of models. To achieve this goal, we learn a hybrid tensor rank decomposition of the scene without neural networks. Motivated by the low-rank approximation property of the SVD algorithm, we propose a rank-residual learning strategy to encourage the preservation of primary information in lower ranks. The model size can then be dynamically adjusted by rank truncation to control the levels of detail, achieving near-optimal compression without extra optimization. Furthermore, different models can be arbitrarily transformed and composed into one scene by concatenating along the rank dimension. The growth of storage cost can also be mitigated by compressing the unimportant objects in the composed scene. We demonstrate that our method is able to achieve comparable rendering quality to state-of-the-art methods, while enabling extra capability of compression and composition. Code will be made available at https://github.com/ashawkey/CCNeRF.



### You Only Need 90K Parameters to Adapt Light: A Light Weight Transformer for Image Enhancement and Exposure Correction
- **Arxiv ID**: http://arxiv.org/abs/2205.14871v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.14871v4)
- **Published**: 2022-05-30 06:21:52+00:00
- **Updated**: 2022-10-08 11:17:26+00:00
- **Authors**: Ziteng Cui, Kunchang Li, Lin Gu, Shenghan Su, Peng Gao, Zhengkai Jiang, Yu Qiao, Tatsuya Harada
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Challenging illumination conditions (low-light, under-exposure and over-exposure) in the real world not only cast an unpleasant visual appearance but also taint the computer vision tasks. After camera captures the raw-RGB data, it renders standard sRGB images with image signal processor (ISP). By decomposing ISP pipeline into local and global image components, we propose a lightweight fast Illumination Adaptive Transformer (IAT) to restore the normal lit sRGB image from either low-light or under/over-exposure conditions. Specifically, IAT uses attention queries to represent and adjust the ISP-related parameters such as colour correction, gamma correction. With only ~90k parameters and ~0.004s processing speed, our IAT consistently achieves superior performance over SOTA on the current benchmark low-light enhancement and exposure correction datasets. Competitive experimental performance also demonstrates that our IAT significantly enhances object detection and semantic segmentation tasks under various light conditions. Training code and pretrained model is available at https://github.com/cuiziteng/Illumination-Adaptive-Transformer.



### Easter2.0: Improving convolutional models for handwritten text recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.14879v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.14879v1)
- **Published**: 2022-05-30 06:33:15+00:00
- **Updated**: 2022-05-30 06:33:15+00:00
- **Authors**: Kartik Chaudhary, Raghav Bali
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have shown promising results for the task of Handwritten Text Recognition (HTR) but they still fall behind Recurrent Neural Networks (RNNs)/Transformer based models in terms of performance. In this paper, we propose a CNN based architecture that bridges this gap. Our work, Easter2.0, is composed of multiple layers of 1D Convolution, Batch Normalization, ReLU, Dropout, Dense Residual connection, Squeeze-and-Excitation module and make use of Connectionist Temporal Classification (CTC) loss. In addition to the Easter2.0 architecture, we propose a simple and effective data augmentation technique 'Tiling and Corruption (TACO)' relevant for the task of HTR/OCR. Our work achieves state-of-the-art results on IAM handwriting database when trained using only publicly available training data. In our experiments, we also present the impact of TACO augmentations and Squeeze-and-Excitation (SE) on text recognition accuracy. We further show that Easter2.0 is suitable for few-shot learning tasks and outperforms current best methods including Transformers when trained on limited amount of annotated data. Code and model is available at: https://github.com/kartikgill/Easter2



### Time3D: End-to-End Joint Monocular 3D Object Detection and Tracking for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2205.14882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.14882v1)
- **Published**: 2022-05-30 06:41:10+00:00
- **Updated**: 2022-05-30 06:41:10+00:00
- **Authors**: Peixuan Li, Jieyu Jin
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: While separately leveraging monocular 3D object detection and 2D multi-object tracking can be straightforwardly applied to sequence images in a frame-by-frame fashion, stand-alone tracker cuts off the transmission of the uncertainty from the 3D detector to tracking while cannot pass tracking error differentials back to the 3D detector. In this work, we propose jointly training 3D detection and 3D tracking from only monocular videos in an end-to-end manner. The key component is a novel spatial-temporal information flow module that aggregates geometric and appearance features to predict robust similarity scores across all objects in current and past frames. Specifically, we leverage the attention mechanism of the transformer, in which self-attention aggregates the spatial information in a specific frame, and cross-attention exploits relation and affinities of all objects in the temporal domain of sequence frames. The affinities are then supervised to estimate the trajectory and guide the flow of information between corresponding 3D objects. In addition, we propose a temporal   -consistency loss that explicitly involves 3D target motion modeling into the learning, making the 3D trajectory smooth in the world coordinate system. Time3D achieves 21.4\% AMOTA, 13.6\% AMOTP on the nuScenes 3D tracking benchmark, surpassing all published competitors, and running at 38 FPS, while Time3D achieves 31.2\% mAP, 39.4\% NDS on the nuScenes 3D detection benchmark.



### Adversarial synthesis based data-augmentation for code-switched spoken language identification
- **Arxiv ID**: http://arxiv.org/abs/2205.15747v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.15747v2)
- **Published**: 2022-05-30 06:41:13+00:00
- **Updated**: 2022-06-01 18:17:51+00:00
- **Authors**: Parth Shastri, Chirag Patil, Poorval Wanere, Dr. Shrinivas Mahajan, Dr. Abhishek Bhatt, Dr. Hardik Sailor
- **Comment**: 9 pages, 8 figures, updated
- **Journal**: None
- **Summary**: Spoken Language Identification (LID) is an important sub-task of Automatic Speech Recognition(ASR) that is used to classify the language(s) in an audio segment. Automatic LID plays an useful role in multilingual countries. In various countries, identifying a language becomes hard, due to the multilingual scenario where two or more than two languages are mixed together during conversation. Such phenomenon of speech is called as code-mixing or code-switching. This nature is followed not only in India but also in many Asian countries. Such code-mixed data is hard to find, which further reduces the capabilities of the spoken LID. Hence, this work primarily addresses this problem using data augmentation as a solution on the on the data scarcity of the code-switched class. This study focuses on Indic language code-mixed with English. Spoken LID is performed on Hindi, code-mixed with English. This research proposes Generative Adversarial Network (GAN) based data augmentation technique performed using Mel spectrograms for audio data. GANs have already been proven to be accurate in representing the real data distribution in the image domain. Proposed research exploits these capabilities of GANs in speech domains such as speech classification, automatic speech recognition, etc. GANs are trained to generate Mel spectrograms of the minority code-mixed class which are then used to augment data for the classifier. Utilizing GANs give an overall improvement on Unweighted Average Recall by an amount of 3.5% as compared to a Convolutional Recurrent Neural Network (CRNN) classifier used as the baseline reference.



### Neural Shape Mating: Self-Supervised Object Assembly with Adversarial Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/2205.14886v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.14886v1)
- **Published**: 2022-05-30 06:58:01+00:00
- **Updated**: 2022-05-30 06:58:01+00:00
- **Authors**: Yun-Chun Chen, Haoda Li, Dylan Turpin, Alec Jacobson, Animesh Garg
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Learning to autonomously assemble shapes is a crucial skill for many robotic applications. While the majority of existing part assembly methods focus on correctly posing semantic parts to recreate a whole object, we interpret assembly more literally: as mating geometric parts together to achieve a snug fit. By focusing on shape alignment rather than semantic cues, we can achieve across-category generalization. In this paper, we introduce a novel task, pairwise 3D geometric shape mating, and propose Neural Shape Mating (NSM) to tackle this problem. Given the point clouds of two object parts of an unknown category, NSM learns to reason about the fit of the two parts and predict a pair of 3D poses that tightly mate them together. We couple the training of NSM with an implicit shape reconstruction task to make NSM more robust to imperfect point cloud observations. To train NSM, we present a self-supervised data collection pipeline that generates pairwise shape mating data with ground truth by randomly cutting an object mesh into two parts, resulting in a dataset that consists of 200K shape mating pairs from numerous object meshes with diverse cut types. We train NSM on the collected dataset and compare it with several point cloud registration methods and one part assembly baseline. Extensive experimental results and ablation studies under various settings demonstrate the effectiveness of the proposed algorithm. Additional material is available at: https://neural-shape-mating.github.io/



### Deep Posterior Distribution-based Embedding for Hyperspectral Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2205.14887v2
- **DOI**: 10.1109/TIP.2022.3201478
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.14887v2)
- **Published**: 2022-05-30 06:59:01+00:00
- **Updated**: 2022-08-23 04:57:49+00:00
- **Authors**: Jinhui Hou, Zhiyu Zhu, Junhui Hou, Huanqiang Zeng, Jinjian Wu, Jiantao Zhou
- **Comment**: Accepted by IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: In this paper, we investigate the problem of hyperspectral (HS) image spatial super-resolution via deep learning. Particularly, we focus on how to embed the high-dimensional spatial-spectral information of HS images efficiently and effectively. Specifically, in contrast to existing methods adopting empirically-designed network modules, we formulate HS embedding as an approximation of the posterior distribution of a set of carefully-defined HS embedding events, including layer-wise spatial-spectral feature extraction and network-level feature aggregation. Then, we incorporate the proposed feature embedding scheme into a source-consistent super-resolution framework that is physically-interpretable, producing lightweight PDE-Net, in which high-resolution (HR) HS images are iteratively refined from the residuals between input low-resolution (LR) HS images and pseudo-LR-HS images degenerated from reconstructed HR-HS images via probability-inspired HS embedding. Extensive experiments over three common benchmark datasets demonstrate that PDE-Net achieves superior performance over state-of-the-art methods. Besides, the probabilistic characteristic of this kind of networks can provide the epistemic uncertainty of the network outputs, which may bring additional benefits when used for other HS image-based applications. The code will be publicly available at https://github.com/jinnh/PDE-Net.



### Exploring the Open World Using Incremental Extreme Value Machines
- **Arxiv ID**: http://arxiv.org/abs/2205.14892v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.14892v1)
- **Published**: 2022-05-30 07:21:13+00:00
- **Updated**: 2022-05-30 07:21:13+00:00
- **Authors**: Tobias Koch, Felix Liebezeit, Christian Riess, Vincent Christlein, Thomas Köhler
- **Comment**: Accepted at ICPR 2022
- **Journal**: None
- **Summary**: Dynamic environments require adaptive applications. One particular machine learning problem in dynamic environments is open world recognition. It characterizes a continuously changing domain where only some classes are seen in one batch of the training data and such batches can only be learned incrementally. Open world recognition is a demanding task that is, to the best of our knowledge, addressed by only a few methods. This work introduces a modification of the widely known Extreme Value Machine (EVM) to enable open world recognition. Our proposed method extends the EVM with a partial model fitting function by neglecting unaffected space during an update. This reduces the training time by a factor of 28. In addition, we provide a modified model reduction using weighted maximum K-set cover to strictly bound the model complexity and reduce the computational effort by a factor of 3.5 from 2.1 s to 0.6 s. In our experiments, we rigorously evaluate openness with two novel evaluation protocols. The proposed method achieves superior accuracy of about 12 % and computational efficiency in the tasks of image classification and face recognition.



### From Representation to Reasoning: Towards both Evidence and Commonsense Reasoning for Video Question-Answering
- **Arxiv ID**: http://arxiv.org/abs/2205.14895v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.14895v1)
- **Published**: 2022-05-30 07:26:54+00:00
- **Updated**: 2022-05-30 07:26:54+00:00
- **Authors**: Jiangtong Li, Li Niu, Liqing Zhang
- **Comment**: To appear in CVPR 2022
- **Journal**: None
- **Summary**: Video understanding has achieved great success in representation learning, such as video caption, video object grounding, and video descriptive question-answer. However, current methods still struggle on video reasoning, including evidence reasoning and commonsense reasoning. To facilitate deeper video understanding towards video reasoning, we present the task of Causal-VidQA, which includes four types of questions ranging from scene description (description) to evidence reasoning (explanation) and commonsense reasoning (prediction and counterfactual). For commonsense reasoning, we set up a two-step solution by answering the question and providing a proper reason. Through extensive experiments on existing VideoQA methods, we find that the state-of-the-art methods are strong in descriptions but weak in reasoning. We hope that Causal-VidQA can guide the research of video understanding from representation learning to deeper reasoning. The dataset and related resources are available at \url{https://github.com/bcmi/Causal-VidQA.git}.



### Adaptive color transfer from images to terrain visualizations
- **Arxiv ID**: http://arxiv.org/abs/2205.14908v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2205.14908v1)
- **Published**: 2022-05-30 08:03:30+00:00
- **Updated**: 2022-05-30 08:03:30+00:00
- **Authors**: Mingguang Wu, Yanjie Sun, Shangjing Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Terrain mapping is not only dedicated to communicating how high or how steep a landscape is but can also help to narrate how we feel about a place. However, crafting effective and expressive hypsometric tints is challenging for both nonexperts and experts. In this paper, we present a two-step image-to-terrain color transfer method that can transfer color from arbitrary images to diverse terrain models. First, we present a new image color organization method that organizes discrete, irregular image colors into a continuous, regular color grid that facilitates a series of color operations, such as local and global searching, categorical color selection and sequential color interpolation. Second, we quantify a series of subjective concerns about elevation color crafting, such as "the lower, the higher" principle, color conventions, and aerial perspectives. We also define color similarity between image and terrain visualization with aesthetic quality. We then mathematically formulate image-to-terrain color transfer as a dual-objective optimization problem and offer a heuristic searching method to solve the problem. Finally, we compare elevation tints from our method with a standard color scheme on four test terrains. The evaluations show that the hypsometric tints from the proposed method can work as effectively as the standard scheme and that our tints are more visually favorable. We also showcase that our method can transfer emotion from image to terrain visualization.



### Uncertainty Quantification and Resource-Demanding Computer Vision Applications of Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.14917v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T45, 62-07
- **Links**: [PDF](http://arxiv.org/pdf/2205.14917v1)
- **Published**: 2022-05-30 08:31:03+00:00
- **Updated**: 2022-05-30 08:31:03+00:00
- **Authors**: Julian Burghoff, Robin Chan, Hanno Gottschalk, Annika Muetze, Tobias Riedlinger, Matthias Rottmann, Marius Schubert
- **Comment**: None
- **Journal**: None
- **Summary**: Bringing deep neural networks (DNNs) into safety critical applications such as automated driving, medical imaging and finance, requires a thorough treatment of the model's uncertainties. Training deep neural networks is already resource demanding and so is also their uncertainty quantification. In this overview article, we survey methods that we developed to teach DNNs to be uncertain when they encounter new object classes. Additionally, we present training methods to learn from only a few labels with help of uncertainty quantification. Note that this is typically paid with a massive overhead in computation of an order of magnitude and more compared to ordinary network training. Finally, we survey our work on neural architecture search which is also an order of magnitude more resource demanding then ordinary network training.



### ACIL: Analytic Class-Incremental Learning with Absolute Memorization and Privacy Protection
- **Arxiv ID**: http://arxiv.org/abs/2205.14922v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.14922v2)
- **Published**: 2022-05-30 08:41:56+00:00
- **Updated**: 2022-12-10 13:27:30+00:00
- **Authors**: Huiping Zhuang, Zhenyu Weng, Hongxin Wei, Renchunzi Xie, Kar-Ann Toh, Zhiping Lin
- **Comment**: published in NeurIPS 2022
- **Journal**: None
- **Summary**: Class-incremental learning (CIL) learns a classification model with training data of different classes arising progressively. Existing CIL either suffers from serious accuracy loss due to catastrophic forgetting, or invades data privacy by revisiting used exemplars. Inspired by linear learning formulations, we propose an analytic class-incremental learning (ACIL) with absolute memorization of past knowledge while avoiding breaching of data privacy (i.e., without storing historical data). The absolute memorization is demonstrated in the sense that class-incremental learning using ACIL given present data would give identical results to that from its joint-learning counterpart which consumes both present and historical samples. This equality is theoretically validated. Data privacy is ensured since no historical data are involved during the learning process. Empirical validations demonstrate ACIL's competitive accuracy performance with near-identical results for various incremental task settings (e.g., 5-50 phases). This also allows ACIL to outperform the state-of-the-art methods for large-phase scenarios (e.g., 25 and 50 phases).



### Neural Volumetric Object Selection
- **Arxiv ID**: http://arxiv.org/abs/2205.14929v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.14929v1)
- **Published**: 2022-05-30 08:55:20+00:00
- **Updated**: 2022-05-30 08:55:20+00:00
- **Authors**: Zhongzheng Ren, Aseem Agarwala, Bryan Russell, Alexander G. Schwing, Oliver Wang
- **Comment**: CVPR 2022 camera ready
- **Journal**: None
- **Summary**: We introduce an approach for selecting objects in neural volumetric 3D representations, such as multi-plane images (MPI) and neural radiance fields (NeRF). Our approach takes a set of foreground and background 2D user scribbles in one view and automatically estimates a 3D segmentation of the desired object, which can be rendered into novel views. To achieve this result, we propose a novel voxel feature embedding that incorporates the neural volumetric 3D representation and multi-view image features from all input views. To evaluate our approach, we introduce a new dataset of human-provided segmentation masks for depicted objects in real-world multi-view scene captures. We show that our approach out-performs strong baselines, including 2D segmentation and 3D segmentation approaches adapted to our task.



### Spectral Maps for Learning on Subgraphs
- **Arxiv ID**: http://arxiv.org/abs/2205.14938v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.14938v4)
- **Published**: 2022-05-30 09:03:28+00:00
- **Updated**: 2023-01-31 09:36:26+00:00
- **Authors**: Marco Pegoraro, Riccardo Marin, Arianna Rampini, Simone Melzi, Luca Cosmo, Emanuele Rodolà
- **Comment**: None
- **Journal**: None
- **Summary**: In graph learning, maps between graphs and their subgraphs frequently arise. For instance, when coarsening or rewiring operations are present along the pipeline, one needs to keep track of the corresponding nodes between the original and modified graphs. Classically, these maps are represented as binary node-to-node correspondence matrices and used as-is to transfer node-wise features between the graphs. In this paper, we argue that simply changing this map representation can bring notable benefits to graph learning tasks. Drawing inspiration from recent progress in geometry processing, we introduce a spectral representation for maps that is easy to integrate into existing graph learning models. This spectral representation is a compact and straightforward plug-in replacement and is robust to topological changes of the graphs. Remarkably, the representation exhibits structural properties that make it interpretable, drawing an analogy with recent results on smooth manifolds. We demonstrate the benefits of incorporating spectral maps in graph learning pipelines, addressing scenarios where a node-to-node map is not well defined, or in the absence of exact isomorphism. Our approach bears practical benefits in knowledge distillation and hierarchical learning, where we show comparable or improved performance at a fraction of the computational cost.



### Edge YOLO: Real-Time Intelligent Object Detection System Based on Edge-Cloud Cooperation in Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2205.14942v1
- **DOI**: 10.1109/TITS.2022.3158253
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2205.14942v1)
- **Published**: 2022-05-30 09:16:35+00:00
- **Updated**: 2022-05-30 09:16:35+00:00
- **Authors**: Siyuan Liang, Hao Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Driven by the ever-increasing requirements of autonomous vehicles, such as traffic monitoring and driving assistant, deep learning-based object detection (DL-OD) has been increasingly attractive in intelligent transportation systems. However, it is difficult for the existing DL-OD schemes to realize the responsible, cost-saving, and energy-efficient autonomous vehicle systems due to low their inherent defects of low timeliness and high energy consumption. In this paper, we propose an object detection (OD) system based on edge-cloud cooperation and reconstructive convolutional neural networks, which is called Edge YOLO. This system can effectively avoid the excessive dependence on computing power and uneven distribution of cloud computing resources. Specifically, it is a lightweight OD framework realized by combining pruning feature extraction network and compression feature fusion network to enhance the efficiency of multi-scale prediction to the largest extent. In addition, we developed an autonomous driving platform equipped with NVIDIA Jetson for system-level verification. We experimentally demonstrate the reliability and efficiency of Edge YOLO on COCO2017 and KITTI data sets, respectively. According to COCO2017 standard datasets with a speed of 26.6 frames per second (FPS), the results show that the number of parameters in the entire network is only 25.67 MB, while the accuracy (mAP) is up to 47.3%.



### HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling
- **Arxiv ID**: http://arxiv.org/abs/2205.14949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.14949v1)
- **Published**: 2022-05-30 09:34:44+00:00
- **Updated**: 2022-05-30 09:34:44+00:00
- **Authors**: Xiaosong Zhang, Yunjie Tian, Wei Huang, Qixiang Ye, Qi Dai, Lingxi Xie, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, masked image modeling (MIM) has offered a new methodology of self-supervised pre-training of vision transformers. A key idea of efficient implementation is to discard the masked image patches (or tokens) throughout the target network (encoder), which requires the encoder to be a plain vision transformer (e.g., ViT), albeit hierarchical vision transformers (e.g., Swin Transformer) have potentially better properties in formulating vision inputs. In this paper, we offer a new design of hierarchical vision transformers named HiViT (short for Hierarchical ViT) that enjoys both high efficiency and good performance in MIM. The key is to remove the unnecessary "local inter-unit operations", deriving structurally simple hierarchical vision transformers in which mask-units can be serialized like plain vision transformers. For this purpose, we start with Swin Transformer and (i) set the masking unit size to be the token size in the main stage of Swin Transformer, (ii) switch off inter-unit self-attentions before the main stage, and (iii) eliminate all operations after the main stage. Empirical studies demonstrate the advantageous performance of HiViT in terms of fully-supervised, self-supervised, and transfer learning. In particular, in running MAE on ImageNet-1K, HiViT-B reports a +0.6% accuracy gain over ViT-B and a 1.9$\times$ speed-up over Swin-B, and the performance gain generalizes to downstream tasks of detection and segmentation. Code will be made publicly available.



### Benchmarking the Robustness of LiDAR-Camera Fusion for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.14951v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.14951v1)
- **Published**: 2022-05-30 09:35:37+00:00
- **Updated**: 2022-05-30 09:35:37+00:00
- **Authors**: Kaicheng Yu, Tang Tao, Hongwei Xie, Zhiwei Lin, Zhongwei Wu, Zhongyu Xia, Tingting Liang, Haiyang Sun, Jiong Deng, Dayang Hao, Yongtao Wang, Xiaodan Liang, Bing Wang
- **Comment**: Technical report. The first three authors contribute equally
- **Journal**: None
- **Summary**: There are two critical sensors for 3D perception in autonomous driving, the camera and the LiDAR. The camera provides rich semantic information such as color, texture, and the LiDAR reflects the 3D shape and locations of surrounding objects. People discover that fusing these two modalities can significantly boost the performance of 3D perception models as each modality has complementary information to the other. However, we observe that current datasets are captured from expensive vehicles that are explicitly designed for data collection purposes, and cannot truly reflect the realistic data distribution due to various reasons. To this end, we collect a series of real-world cases with noisy data distribution, and systematically formulate a robustness benchmark toolkit, that simulates these cases on any clean autonomous driving datasets. We showcase the effectiveness of our toolkit by establishing the robustness benchmark on two widely-adopted autonomous driving datasets, nuScenes and Waymo, then, to the best of our knowledge, holistically benchmark the state-of-the-art fusion methods for the first time. We observe that: i) most fusion methods, when solely developed on these data, tend to fail inevitably when there is a disruption to the LiDAR input; ii) the improvement of the camera input is significantly inferior to the LiDAR one. We further propose an efficient robust training strategy to improve the robustness of the current fusion method. The benchmark and code are available at https://github.com/kcyu2014/lidar-camera-robust-benchmark



### White-box Membership Attack Against Machine Learning Based Retinopathy Classification
- **Arxiv ID**: http://arxiv.org/abs/2206.03584v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2206.03584v1)
- **Published**: 2022-05-30 09:56:47+00:00
- **Updated**: 2022-05-30 09:56:47+00:00
- **Authors**: Mounia Hamidouche, Reda Bellafqira, Gwenolé Quellec, Gouenou Coatrieux
- **Comment**: None
- **Journal**: None
- **Summary**: The advances in machine learning (ML) have greatly improved AI-based diagnosis aid systems in medical imaging. However, being based on collecting medical data specific to individuals induces several security issues, especially in terms of privacy. Even though the owner of the images like a hospital put in place strict privacy protection provisions at the level of its information system, the model trained over his images still holds disclosure potential. The trained model may be accessible to an attacker as: 1) White-box: accessing to the model architecture and parameters; 2) Black box: where he can only query the model with his own inputs through an appropriate interface. Existing attack methods include: feature estimation attacks (FEA), membership inference attack (MIA), model memorization attack (MMA) and identification attacks (IA). In this work we focus on MIA against a model that has been trained to detect diabetic retinopathy from retinal images. Diabetic retinopathy is a condition that can cause vision loss and blindness in the people who have diabetes. MIA is the process of determining whether a data sample comes from the training data set of a trained ML model or not. From a privacy perspective in our use case where a diabetic retinopathy classification model is given to partners that have at their disposal images along with patients' identifiers, inferring the membership status of a data sample can help to state if a patient has contributed or not to the training of the model.



### PSNet: Fast Data Structuring for Hierarchical Deep Learning on Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2205.14965v2
- **DOI**: 10.1109/TCSVT.2022.3171968
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.14965v2)
- **Published**: 2022-05-30 10:03:13+00:00
- **Updated**: 2022-05-31 06:58:08+00:00
- **Authors**: Luyang Li, Ligang He, Jinjin Gao, Xie Han
- **Comment**: None
- **Journal**: None
- **Summary**: In order to retain more feature information of local areas on a point cloud, local grouping and subsampling are the necessary data structuring steps in most hierarchical deep learning models. Due to the disorder nature of the points in a point cloud, the significant time cost may be consumed when grouping and subsampling the points, which consequently results in poor scalability. This paper proposes a fast data structuring method called PSNet (Point Structuring Net). PSNet transforms the spatial features of the points and matches them to the features of local areas in a point cloud. PSNet achieves grouping and sampling at the same time while the existing methods process sampling and grouping in two separate steps (such as using FPS plus kNN). PSNet performs feature transformation pointwise while the existing methods uses the spatial relationship among the points as the reference for grouping. Thanks to these features, PSNet has two important advantages: 1) the grouping and sampling results obtained by PSNet is stable and permutation invariant; and 2) PSNet can be easily parallelized. PSNet can replace the data structuring methods in the mainstream point cloud deep learning models in a plug-and-play manner. We have conducted extensive experiments. The results show that PSNet can improve the training and inference speed significantly while maintaining the model accuracy.



### Guided Diffusion Model for Adversarial Purification
- **Arxiv ID**: http://arxiv.org/abs/2205.14969v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.14969v3)
- **Published**: 2022-05-30 10:11:15+00:00
- **Updated**: 2022-06-29 02:42:05+00:00
- **Authors**: Jinyi Wang, Zhaoyang Lyu, Dahua Lin, Bo Dai, Hongfei Fu
- **Comment**: None
- **Journal**: None
- **Summary**: With wider application of deep neural networks (DNNs) in various algorithms and frameworks, security threats have become one of the concerns. Adversarial attacks disturb DNN-based image classifiers, in which attackers can intentionally add imperceptible adversarial perturbations on input images to fool the classifiers. In this paper, we propose a novel purification approach, referred to as guided diffusion model for purification (GDMP), to help protect classifiers from adversarial attacks. The core of our approach is to embed purification into the diffusion denoising process of a Denoised Diffusion Probabilistic Model (DDPM), so that its diffusion process could submerge the adversarial perturbations with gradually added Gaussian noises, and both of these noises can be simultaneously removed following a guided denoising process. On our comprehensive experiments across various datasets, the proposed GDMP is shown to reduce the perturbations raised by adversarial attacks to a shallow range, thereby significantly improving the correctness of classification. GDMP improves the robust accuracy by 5%, obtaining 90.1% under PGD attack on the CIFAR10 dataset. Moreover, GDMP achieves 70.94% robustness on the challenging ImageNet dataset.



### Knowledge Distillation for 6D Pose Estimation by Aligning Distributions of Local Predictions
- **Arxiv ID**: http://arxiv.org/abs/2205.14971v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.14971v2)
- **Published**: 2022-05-30 10:17:17+00:00
- **Updated**: 2022-11-28 11:36:11+00:00
- **Authors**: Shuxuan Guo, Yinlin Hu, Jose M. Alvarez, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation facilitates the training of a compact student network by using a deep teacher one. While this has achieved great success in many tasks, it remains completely unstudied for image-based 6D object pose estimation. In this work, we introduce the first knowledge distillation method driven by the 6D pose estimation task. To this end, we observe that most modern 6D pose estimation frameworks output local predictions, such as sparse 2D keypoints or dense representations, and that the compact student network typically struggles to predict such local quantities precisely. Therefore, instead of imposing prediction-to-prediction supervision from the teacher to the student, we propose to distill the teacher's \emph{distribution} of local predictions into the student network, facilitating its training. Our experiments on several benchmarks show that our distillation method yields state-of-the-art results with different compact student models and for both keypoint-based and dense prediction-based architectures.



### GMML is All you Need
- **Arxiv ID**: http://arxiv.org/abs/2205.14986v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.14986v1)
- **Published**: 2022-05-30 10:36:55+00:00
- **Updated**: 2022-05-30 10:36:55+00:00
- **Authors**: Sara Atito, Muhammad Awais, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: Vision transformers have generated significant interest in the computer vision community because of their flexibility in exploiting contextual information, whether it is sharply confined local, or long range global. However, they are known to be data hungry. This has motivated the research in self-supervised transformer pretraining, which does not need to decode the semantic information conveyed by labels to link it to the image properties, but rather focuses directly on extracting a concise representation of the image data that reflects the notion of similarity, and is invariant to nuisance factors. The key vehicle for the self-learning process used by the majority of self-learning methods is the generation of multiple views of the training data and the creation of pretext tasks which use these views to define the notion of image similarity, and data integrity. However, this approach lacks the natural propensity to extract contextual information. We propose group masked model learning (GMML), a self-supervised learning (SSL) mechanism for pretraining vision transformers with the ability to extract the contextual information present in all the concepts in an image. GMML achieves this by manipulating randomly groups of connected tokens, ensuingly covering a meaningful part of a semantic concept, and then recovering the hidden semantic information from the visible part of the concept. GMML implicitly introduces a novel data augmentation process. Unlike most of the existing SSL approaches, GMML does not require momentum encoder, nor rely on careful implementation details such as large batches and gradient stopping, which are all artefacts of most of the current self-supervised learning techniques. The source code is publicly available for the community to train on bigger corpora: https://github.com/Sara-Ahmed/GMML.



### CompleteDT: Point Cloud Completion with Dense Augment Inference Transformers
- **Arxiv ID**: http://arxiv.org/abs/2205.14999v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.14999v2)
- **Published**: 2022-05-30 11:17:31+00:00
- **Updated**: 2022-06-10 10:10:27+00:00
- **Authors**: Jun Li, Shangwei Guo, Shaokun Han
- **Comment**: Submitted to International Journal of Computer Vision (IJCV)
- **Journal**: None
- **Summary**: Point cloud completion task aims to predict the missing part of incomplete point clouds and generate complete point clouds with details. In this paper, we propose a novel point cloud completion network, namely CompleteDT. Specifically, features are learned from point clouds with different resolutions, which is sampled from the incomplete input, and are converted to a series of \textit{spots} based on the geometrical structure. Then, the Dense Relation Augment Module (DRA) based on the transformer is proposed to learn features within \textit{spots} and consider the correlation among these \textit{spots}. The DRA consists of Point Local-Attention Module (PLA) and Point Dense Multi-Scale Attention Module (PDMA), where the PLA captures the local information within the local \textit{spots} by adaptively measuring weights of neighbors and the PDMA exploits the global relationship between these \textit{spots} in a multi-scale densely connected manner. Lastly, the complete shape is predicted from \textit{spots} by the Multi-resolution Point Fusion Module (MPF), which gradually generates complete point clouds from \textit{spots}, and updates \textit{spots} based on these generated point clouds. Experimental results show that, because the DRA based on the transformer can learn the expressive features from the incomplete input and the MPF can fully explore these feature to predict the complete input, our method largely outperforms the state-of-the-art methods.



### Task-Prior Conditional Variational Auto-Encoder for Few-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2205.15014v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.15014v1)
- **Published**: 2022-05-30 11:57:57+00:00
- **Updated**: 2022-05-30 11:57:57+00:00
- **Authors**: Zaiyun Yang
- **Comment**: few-shot leanring, meta learning, transducitve learning, imgae
  classification
- **Journal**: None
- **Summary**: Transductive methods always outperform inductive methods in few-shot image classification scenarios. However, the existing few-shot methods contain a latent condition: the number of samples in each class is the same, which may be unrealistic. To cope with those cases where the query shots of each class are nonuniform (i.e. nonuniform few-shot learning), we propose a Task-Prior Conditional Variational Auto-Encoder model named TP-VAE, conditioned on support shots and constrained by a task-level prior regularization. Our method obtains high performance in the more challenging nonuniform few-shot scenarios. Moreover, our method outperforms the state-of-the-art in a wide range of standard few-shot image classification scenarios. Among them, the accuracy of 1-shot increased by about 3\%.



### An Efficient Modern Baseline for FloodNet VQA
- **Arxiv ID**: http://arxiv.org/abs/2205.15025v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2205.15025v1)
- **Published**: 2022-05-30 12:04:49+00:00
- **Updated**: 2022-05-30 12:04:49+00:00
- **Authors**: Aditya Kane, Sahil Khose
- **Comment**: Under review, 4 pages, 2 figures, 1 table
- **Journal**: None
- **Summary**: Designing efficient and reliable VQA systems remains a challenging problem, more so in the case of disaster management and response systems. In this work, we revisit fundamental combination methods like concatenation, addition and element-wise multiplication with modern image and text feature abstraction models. We design a simple and efficient system which outperforms pre-existing methods on the FloodNet dataset and achieves state-of-the-art performance. This simplified system requires significantly less training and inference time than modern VQA architectures. We also study the performance of various backbones and report their consolidated results. Code is available at https://github.com/sahilkhose/floodnet_vqa.



### SMUDLP: Self-Teaching Multi-Frame Unsupervised Endoscopic Depth Estimation with Learnable Patchmatch
- **Arxiv ID**: http://arxiv.org/abs/2205.15034v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15034v1)
- **Published**: 2022-05-30 12:11:03+00:00
- **Updated**: 2022-05-30 12:11:03+00:00
- **Authors**: Shuwei Shao, Zhongcai Pei, Weihai Chen, Xingming Wu, Zhong Liu, Zhengguo Li
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Unsupervised monocular trained depth estimation models make use of adjacent frames as a supervisory signal during the training phase. However, temporally correlated frames are also available at inference time for many clinical applications, e.g., surgical navigation. The vast majority of monocular systems do not exploit this valuable signal that could be deployed to enhance the depth estimates. Those that do, achieve only limited gains due to the unique challenges in endoscopic scenes, such as low and homogeneous textures and inter-frame brightness fluctuations. In this work, we present SMUDLP, a novel and unsupervised paradigm for multi-frame monocular endoscopic depth estimation. The SMUDLP integrates a learnable patchmatch module to adaptively increase the discriminative ability in low-texture and homogeneous-texture regions, and enforces cross-teaching and self-teaching consistencies to provide efficacious regularizations towards brightness fluctuations. Our detailed experiments on both SCARED and Hamlyn datasets indicate that the SMUDLP exceeds state-of-the-art competitors by a large margin, including those that use single or multiple frames at inference time. The source code and trained models will be publicly available upon the acceptance.



### Deblurring Photographs of Characters Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.15053v2
- **DOI**: None
- **Categories**: **cs.CV**, Primary: 68U10, Secondary: 78A46, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2205.15053v2)
- **Published**: 2022-05-30 12:32:26+00:00
- **Updated**: 2022-05-31 07:45:45+00:00
- **Authors**: Thomas Germer, Tobias Uelwer, Stefan Harmeling
- **Comment**: 15 pages, 13 figures
- **Journal**: None
- **Summary**: In this paper, we present our approach for the Helsinki Deblur Challenge (HDC2021). The task of this challenge is to deblur images of characters without knowing the point spread function (PSF). The organizers provided a dataset of pairs of sharp and blurred images. Our method consists of three steps: First, we estimate a warping transformation of the images to align the sharp images with the blurred ones. Next, we estimate the PSF using a quasi-Newton method. The estimated PSF allows to generate additional pairs of sharp and blurred images. Finally, we train a deep convolutional neural network to reconstruct the sharp images from the blurred images. Our method is able to successfully reconstruct images from the first 10 stages of the HDC 2021 data. Our code is available at https://github.com/hhu-machine-learning/hdc2021-psfnn.



### Median Pixel Difference Convolutional Network for Robust Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.15867v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.15867v1)
- **Published**: 2022-05-30 13:15:49+00:00
- **Updated**: 2022-05-30 13:15:49+00:00
- **Authors**: Jiehua Zhang, Zhuo Su, Li Liu
- **Comment**: Accepted by BMVC2021
- **Journal**: None
- **Summary**: Face recognition is one of the most active tasks in computer vision and has been widely used in the real world. With great advances made in convolutional neural networks (CNN), lots of face recognition algorithms have achieved high accuracy on various face datasets. However, existing face recognition algorithms based on CNNs are vulnerable to noise. Noise corrupted image patterns could lead to false activations, significantly decreasing face recognition accuracy in noisy situations. To equip CNNs with built-in robustness to noise of different levels, we proposed a Median Pixel Difference Convolutional Network (MeDiNet) by replacing some traditional convolutional layers with the proposed novel Median Pixel Difference Convolutional Layer (MeDiConv) layer. The proposed MeDiNet integrates the idea of traditional multiscale median filtering with deep CNNs. The MeDiNet is tested on the four face datasets (LFW, CA-LFW, CP-LFW, and YTF) with versatile settings on blur kernels, noise intensities, scales, and JPEG quality factors. Extensive experiments show that our MeDiNet can effectively remove noisy pixels in the feature map and suppress the negative impact of noise, leading to achieving limited accuracy loss under these practical noises compared with the standard CNN under clean conditions.



### Why Adversarial Training of ReLU Networks Is Difficult?
- **Arxiv ID**: http://arxiv.org/abs/2205.15130v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.15130v1)
- **Published**: 2022-05-30 14:22:10+00:00
- **Updated**: 2022-05-30 14:22:10+00:00
- **Authors**: Xu Cheng, Hao Zhang, Yue Xin, Wen Shen, Jie Ren, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper mathematically derives an analytic solution of the adversarial perturbation on a ReLU network, and theoretically explains the difficulty of adversarial training. Specifically, we formulate the dynamics of the adversarial perturbation generated by the multi-step attack, which shows that the adversarial perturbation tends to strengthen eigenvectors corresponding to a few top-ranked eigenvalues of the Hessian matrix of the loss w.r.t. the input. We also prove that adversarial training tends to strengthen the influence of unconfident input samples with large gradient norms in an exponential manner. Besides, we find that adversarial training strengthens the influence of the Hessian matrix of the loss w.r.t. network parameters, which makes the adversarial training more likely to oscillate along directions of a few samples, and boosts the difficulty of adversarial training. Crucially, our proofs provide a unified explanation for previous findings in understanding adversarial training.



### Braille Letter Reading: A Benchmark for Spatio-Temporal Pattern Recognition on Neuromorphic Hardware
- **Arxiv ID**: http://arxiv.org/abs/2205.15864v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.15864v3)
- **Published**: 2022-05-30 14:30:45+00:00
- **Updated**: 2022-10-19 20:39:47+00:00
- **Authors**: Simon F Muller-Cleve, Vittorio Fra, Lyes Khacef, Alejandro Pequeno-Zurro, Daniel Klepatsch, Evelina Forno, Diego G Ivanovich, Shavika Rastogi, Gianvito Urgese, Friedemann Zenke, Chiara Bartolozzi
- **Comment**: None
- **Journal**: None
- **Summary**: Spatio-temporal pattern recognition is a fundamental ability of the brain which is required for numerous real-world activities. Recent deep learning approaches have reached outstanding accuracies in such tasks, but their implementation on conventional embedded solutions is still very computationally and energy expensive. Tactile sensing in robotic applications is a representative example where real-time processing and energy efficiency are required. Following a brain-inspired computing approach, we propose a new benchmark for spatio-temporal tactile pattern recognition at the edge through Braille letter reading. We recorded a new Braille letters dataset based on the capacitive tactile sensors of the iCub robot's fingertip. We then investigated the importance of spatial and temporal information as well as the impact of event-based encoding on spike-based computation. Afterward, we trained and compared feedforward and recurrent Spiking Neural Networks (SNNs) offline using Backpropagation Through Time (BPTT) with surrogate gradients, then we deployed them on the Intel Loihi neuromorphic chip for fast and efficient inference. We compared our approach to standard classifiers, in particular to the Long Short-Term Memory (LSTM) deployed on the embedded NVIDIA Jetson GPU, in terms of classification accuracy, power, energy consumption, and delay. Our results show that the LSTM reaches ~97% of accuracy, outperforming the recurrent SNN by ~17% when using continuous frame-based data instead of event-based inputs. However, the recurrent SNN on Loihi with event-based inputs is ~500 times more energy-efficient than the LSTM on Jetson, requiring a total power of only ~30 mW. This work proposes a new benchmark for tactile sensing and highlights the challenges and opportunities of event-based encoding, neuromorphic hardware, and spike-based computing for spatio-temporal pattern recognition at the edge.



### Batch Normalization Is Blind to the First and Second Derivatives of the Loss
- **Arxiv ID**: http://arxiv.org/abs/2205.15146v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.15146v2)
- **Published**: 2022-05-30 14:43:51+00:00
- **Updated**: 2022-06-02 09:29:20+00:00
- **Authors**: Zhanpeng Zhou, Wen Shen, Huixin Chen, Ling Tang, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we prove the effects of the BN operation on the back-propagation of the first and second derivatives of the loss. When we do the Taylor series expansion of the loss function, we prove that the BN operation will block the influence of the first-order term and most influence of the second-order term of the loss. We also find that such a problem is caused by the standardization phase of the BN operation. Experimental results have verified our theoretical conclusions, and we have found that the BN operation significantly affects feature representations in specific tasks, where losses of different samples share similar analytic formulas.



### Towards Efficient 3D Object Detection with Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2205.15156v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.15156v3)
- **Published**: 2022-05-30 15:02:16+00:00
- **Updated**: 2022-10-14 02:06:11+00:00
- **Authors**: Jihan Yang, Shaoshuai Shi, Runyu Ding, Zhe Wang, Xiaojuan Qi
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: Despite substantial progress in 3D object detection, advanced 3D detectors often suffer from heavy computation overheads. To this end, we explore the potential of knowledge distillation (KD) for developing efficient 3D object detectors, focusing on popular pillar- and voxel-based detectors.In the absence of well-developed teacher-student pairs, we first study how to obtain student models with good trade offs between accuracy and efficiency from the perspectives of model compression and input resolution reduction. Then, we build a benchmark to assess existing KD methods developed in the 2D domain for 3D object detection upon six well-constructed teacher-student pairs. Further, we propose an improved KD pipeline incorporating an enhanced logit KD method that performs KD on only a few pivotal positions determined by teacher classification response, and a teacher-guided student model initialization to facilitate transferring teacher model's feature extraction ability to students through weight inheritance. Finally, we conduct extensive experiments on the Waymo dataset. Our best performing model achieves $65.75\%$ LEVEL 2 mAPH, surpassing its teacher model and requiring only $44\%$ of teacher flops. Our most efficient model runs 51 FPS on an NVIDIA A100, which is $2.2\times$ faster than PointPillar with even higher accuracy. Code is available at \url{https://github.com/CVMI-Lab/SparseKD}.



### GAN-based Medical Image Small Region Forgery Detection via a Two-Stage Cascade Framework
- **Arxiv ID**: http://arxiv.org/abs/2205.15170v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.15170v1)
- **Published**: 2022-05-30 15:21:09+00:00
- **Updated**: 2022-05-30 15:21:09+00:00
- **Authors**: Jianyi Zhang, Xuanxi Huang, Yaqi Liu, Yuyang Han, Zixiao Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: Using generative adversarial network (GAN)\cite{RN90} for data enhancement of medical images is significantly helpful for many computer-aided diagnosis (CAD) tasks. A new attack called CT-GAN has emerged. It can inject or remove lung cancer lesions to CT scans. Because the tampering region may even account for less than 1\% of the original image, even state-of-the-art methods are challenging to detect the traces of such tampering.   This paper proposes a cascade framework to detect GAN-based medical image small region forgery like CT-GAN. In the local detection stage, we train the detector network with small sub-images so that interference information in authentic regions will not affect the detector. We use depthwise separable convolution and residual to prevent the detector from over-fitting and enhance the ability to find forged regions through the attention mechanism. The detection results of all sub-images in the same image will be combined into a heatmap. In the global classification stage, using gray level co-occurrence matrix (GLCM) can better extract features of the heatmap. Because the shape and size of the tampered area are uncertain, we train PCA and SVM methods for classification. Our method can classify whether a CT image has been tampered and locate the tampered position. Sufficient experiments show that our method can achieve excellent performance.



### Self-Supervised Pre-training of Vision Transformers for Dense Prediction Tasks
- **Arxiv ID**: http://arxiv.org/abs/2205.15173v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15173v2)
- **Published**: 2022-05-30 15:25:37+00:00
- **Updated**: 2022-06-07 15:42:14+00:00
- **Authors**: Jaonary Rabarisoa, Valentin Belissen, Florian Chabot, Quoc-Cuong Pham
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new self-supervised pre-training of Vision Transformers for dense prediction tasks. It is based on a contrastive loss across views that compares pixel-level representations to global image representations. This strategy produces better local features suitable for dense prediction tasks as opposed to contrastive pre-training based on global image representation only. Furthermore, our approach does not suffer from a reduced batch size since the number of negative examples needed in the contrastive loss is in the order of the number of local features. We demonstrate the effectiveness of our pre-training strategy on two dense prediction tasks: semantic segmentation and monocular depth estimation.



### ShuffleMixer: An Efficient ConvNet for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2205.15175v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15175v1)
- **Published**: 2022-05-30 15:26:52+00:00
- **Updated**: 2022-05-30 15:26:52+00:00
- **Authors**: Long Sun, Jinshan Pan, Jinhui Tang
- **Comment**: Winner of the model complexity track in NTIRE2022 Efficient
  Super-Resolution Challenge, CVPR 2022. The code is available at
  https://github.com/sunny2109/MobileSR-NTIRE2022
- **Journal**: None
- **Summary**: Lightweight and efficiency are critical drivers for the practical application of image super-resolution (SR) algorithms. We propose a simple and effective approach, ShuffleMixer, for lightweight image super-resolution that explores large convolution and channel split-shuffle operation. In contrast to previous SR models that simply stack multiple small kernel convolutions or complex operators to learn representations, we explore a large kernel ConvNet for mobile-friendly SR design. Specifically, we develop a large depth-wise convolution and two projection layers based on channel splitting and shuffling as the basic component to mix features efficiently. Since the contexts of natural images are strongly locally correlated, using large depth-wise convolutions only is insufficient to reconstruct fine details. To overcome this problem while maintaining the efficiency of the proposed module, we introduce Fused-MBConvs into the proposed network to model the local connectivity of different features. Experimental results demonstrate that the proposed ShuffleMixer is about 6x smaller than the state-of-the-art methods in terms of model parameters and FLOPs while achieving competitive performance. In NTIRE 2022, our primary method won the model complexity track of the Efficient Super-Resolution Challenge [23]. The code is available at https://github.com/sunny2109/MobileSR-NTIRE2022.



### STN: Scalable Tensorizing Networks via Structure-Aware Training and Adaptive Compression
- **Arxiv ID**: http://arxiv.org/abs/2205.15198v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.15198v1)
- **Published**: 2022-05-30 15:50:48+00:00
- **Updated**: 2022-05-30 15:50:48+00:00
- **Authors**: Chang Nie, Huan Wang, Lu Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have delivered a remarkable performance in many tasks of computer vision. However, over-parameterized representations of popular architectures dramatically increase their computational complexity and storage costs, and hinder their availability in edge devices with constrained resources. Regardless of many tensor decomposition (TD) methods that have been well-studied for compressing DNNs to learn compact representations, they suffer from non-negligible performance degradation in practice. In this paper, we propose Scalable Tensorizing Networks (STN), which dynamically and adaptively adjust the model size and decomposition structure without retraining. First, we account for compression during training by adding a low-rank regularizer to guarantee networks' desired low-rank characteristics in full tensor format. Then, considering network layers exhibit various low-rank structures, STN is obtained by a data-driven adaptive TD approach, for which the topological structure of decomposition per layer is learned from the pre-trained model, and the ranks are selected appropriately under specified storage constraints. As a result, STN is compatible with arbitrary network architectures and achieves higher compression performance and flexibility over other tensorizing versions. Comprehensive experiments on several popular architectures and benchmarks substantiate the superiority of our model towards improving parameter efficiency.



### The Devil is in the Pose: Ambiguity-free 3D Rotation-invariant Learning via Pose-aware Convolution
- **Arxiv ID**: http://arxiv.org/abs/2205.15210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15210v1)
- **Published**: 2022-05-30 16:11:55+00:00
- **Updated**: 2022-05-30 16:11:55+00:00
- **Authors**: Ronghan Chen, Yang Cong
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Rotation-invariant (RI) 3D deep learning methods suffer performance degradation as they typically design RI representations as input that lose critical global information comparing to 3D coordinates. Most state-of-the-arts address it by incurring additional blocks or complex global representations in a heavy and ineffective manner. In this paper, we reveal that the global information loss stems from an unexplored pose information loss problem, which can be solved more efficiently and effectively as we only need to restore more lightweight local pose in each layer, and the global information can be hierarchically aggregated in the deep networks without extra efforts. To address this problem, we develop a Pose-aware Rotation Invariant Convolution (i.e., PaRI-Conv), which dynamically adapts its kernels based on the relative poses. To implement it, we propose an Augmented Point Pair Feature (APPF) to fully encode the RI relative pose information, and a factorized dynamic kernel for pose-aware kernel generation, which can further reduce the computational cost and memory burden by decomposing the kernel into a shared basis matrix and a pose-aware diagonal matrix. Extensive experiments on shape classification and part segmentation tasks show that our PaRI-Conv surpasses the state-of-the-art RI methods while being more compact and efficient.



### Radar Image Reconstruction from Raw ADC Data using Parametric Variational Autoencoder with Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2207.06379v1
- **DOI**: 10.1109/ICPR48806.2021.9412858
- **Categories**: **cs.CV**, cs.LG, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2207.06379v1)
- **Published**: 2022-05-30 16:17:36+00:00
- **Updated**: 2022-05-30 16:17:36+00:00
- **Authors**: Michael Stephan, Thomas Stadelmayer, Avik Santra, Georg Fischer, Robert Weigel, Fabian Lurz
- **Comment**: None
- **Journal**: 25th International Conference on Pattern Recognition (ICPR), 2020,
  9529-9536
- **Summary**: This paper presents a parametric variational autoencoder-based human target detection and localization framework working directly with the raw analog-to-digital converter data from the frequency modulated continous wave radar. We propose a parametrically constrained variational autoencoder, with residual and skip connections, capable of generating the clustered and localized target detections on the range-angle image. Furthermore, to circumvent the problem of training the proposed neural network on all possible scenarios using real radar data, we propose domain adaptation strategies whereby we first train the neural network using ray tracing based model data and then adapt the network to work on real sensor data. This strategy ensures better generalization and scalability of the proposed neural network even though it is trained with limited radar data. We demonstrate the superior detection and localization performance of our proposed solution compared to the conventional signal processing pipeline and earlier state-of-art deep U-Net architecture with range-doppler images as inputs



### GraphWalks: Efficient Shape Agnostic Geodesic Shortest Path Estimation
- **Arxiv ID**: http://arxiv.org/abs/2205.15217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15217v1)
- **Published**: 2022-05-30 16:22:53+00:00
- **Updated**: 2022-05-30 16:22:53+00:00
- **Authors**: Rolandos Alexandros Potamias, Alexandros Neofytou, Kyriaki-Margarita Bintsi, Stefanos Zafeiriou
- **Comment**: CVPRw 2022
- **Journal**: None
- **Summary**: Geodesic paths and distances are among the most popular intrinsic properties of 3D surfaces. Traditionally, geodesic paths on discrete polygon surfaces were computed using shortest path algorithms, such as Dijkstra. However, such algorithms have two major limitations. They are non-differentiable which limits their direct usage in learnable pipelines and they are considerably time demanding. To address such limitations and alleviate the computational burden, we propose a learnable network to approximate geodesic paths. The proposed method is comprised by three major components: a graph neural network that encodes node positions in a high dimensional space, a path embedding that describes previously visited nodes and a point classifier that selects the next point in the path. The proposed method provides efficient approximations of the shortest paths and geodesic distances estimations. Given that all of the components of our method are fully differentiable, it can be directly plugged into any learnable pipeline as well as customized under any differentiable constraint. We extensively evaluate the proposed method with several qualitative and quantitative experiments.



### Few-shot Class-incremental Learning for 3D Point Cloud Objects
- **Arxiv ID**: http://arxiv.org/abs/2205.15225v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15225v2)
- **Published**: 2022-05-30 16:33:53+00:00
- **Updated**: 2022-07-22 05:12:43+00:00
- **Authors**: Townim Chowdhury, Ali Cheraghian, Sameera Ramasinghe, Sahar Ahmadi, Morteza Saberi, Shafin Rahman
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot class-incremental learning (FSCIL) aims to incrementally fine-tune a model (trained on base classes) for a novel set of classes using a few examples without forgetting the previous training. Recent efforts address this problem primarily on 2D images. However, due to the advancement of camera technology, 3D point cloud data has become more available than ever, which warrants considering FSCIL on 3D data. This paper addresses FSCIL in the 3D domain. In addition to well-known issues of catastrophic forgetting of past knowledge and overfitting of few-shot data, 3D FSCIL can bring newer challenges. For example, base classes may contain many synthetic instances in a realistic scenario. In contrast, only a few real-scanned samples (from RGBD sensors) of novel classes are available in incremental steps. Due to the data variation from synthetic to real, FSCIL endures additional challenges, degrading performance in later incremental steps. We attempt to solve this problem using Microshapes (orthogonal basis vectors) by describing any 3D objects using a pre-defined set of rules. It supports incremental training with few-shot examples minimizing synthetic to real data variation. We propose new test protocols for 3D FSCIL using popular synthetic datasets (ModelNet and ShapeNet) and 3D real-scanned datasets (ScanObjectNN and CO3D). By comparing state-of-the-art methods, we establish the effectiveness of our approach in the 3D domain.



### Few-Shot Adaptation of Pre-Trained Networks for Domain Shift
- **Arxiv ID**: http://arxiv.org/abs/2205.15234v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.15234v3)
- **Published**: 2022-05-30 16:49:59+00:00
- **Updated**: 2022-10-22 04:27:59+00:00
- **Authors**: Wenyu Zhang, Li Shen, Wanyue Zhang, Chuan-Sheng Foo
- **Comment**: Accepted to IJCAI 2022
- **Journal**: None
- **Summary**: Deep networks are prone to performance degradation when there is a domain shift between the source (training) data and target (test) data. Recent test-time adaptation methods update batch normalization layers of pre-trained source models deployed in new target environments with streaming data to mitigate such performance degradation. Although such methods can adapt on-the-fly without first collecting a large target domain dataset, their performance is dependent on streaming conditions such as mini-batch size and class-distribution, which can be unpredictable in practice. In this work, we propose a framework for few-shot domain adaptation to address the practical challenges of data-efficient adaptation. Specifically, we propose a constrained optimization of feature normalization statistics in pre-trained source models supervised by a small support set from the target domain. Our method is easy to implement and improves source model performance with as few as one sample per class for classification tasks. Extensive experiments on 5 cross-domain classification and 4 semantic segmentation datasets show that our method achieves more accurate and reliable performance than test-time adaptation, while not being constrained by streaming conditions.



### RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression
- **Arxiv ID**: http://arxiv.org/abs/2205.15236v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.15236v2)
- **Published**: 2022-05-30 16:51:25+00:00
- **Updated**: 2022-06-24 16:43:23+00:00
- **Authors**: Yu Gong, Greg Mori, Frederick Tung
- **Comment**: Accepted to ICML 2022
- **Journal**: None
- **Summary**: Data imbalance, in which a plurality of the data samples come from a small proportion of labels, poses a challenge in training deep neural networks. Unlike classification, in regression the labels are continuous, potentially boundless, and form a natural ordering. These distinct features of regression call for new techniques that leverage the additional information encoded in label-space relationships. This paper presents the RankSim (ranking similarity) regularizer for deep imbalanced regression, which encodes an inductive bias that samples that are closer in label space should also be closer in feature space. In contrast to recent distribution smoothing based approaches, RankSim captures both nearby and distant relationships: for a given data sample, RankSim encourages the sorted list of its neighbors in label space to match the sorted list of its neighbors in feature space. RankSim is complementary to conventional imbalanced learning techniques, including re-weighting, two-stage training, and distribution smoothing, and lifts the state-of-the-art performance on three imbalanced regression benchmarks: IMDB-WIKI-DIR, AgeDB-DIR, and STS-B-DIR.



### VLUE: A Multi-Task Benchmark for Evaluating Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2205.15237v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.15237v1)
- **Published**: 2022-05-30 16:52:30+00:00
- **Updated**: 2022-05-30 16:52:30+00:00
- **Authors**: Wangchunshu Zhou, Yan Zeng, Shizhe Diao, Xinsong Zhang
- **Comment**: ICML 2022, Benchmark website at https://vlue-benchmark.github.io
- **Journal**: None
- **Summary**: Recent advances in vision-language pre-training (VLP) have demonstrated impressive performance in a range of vision-language (VL) tasks. However, there exist several challenges for measuring the community's progress in building general multi-modal intelligence. First, most of the downstream VL datasets are annotated using raw images that are already seen during pre-training, which may result in an overestimation of current VLP models' generalization ability. Second, recent VLP work mainly focuses on absolute performance but overlooks the efficiency-performance trade-off, which is also an important indicator for measuring progress.   To this end, we introduce the Vision-Language Understanding Evaluation (VLUE) benchmark, a multi-task multi-dimension benchmark for evaluating the generalization capabilities and the efficiency-performance trade-off (``Pareto SOTA'') of VLP models. We demonstrate that there is a sizable generalization gap for all VLP models when testing on out-of-distribution test sets annotated on images from a more diverse distribution that spreads across cultures. Moreover, we find that measuring the efficiency-performance trade-off of VLP models leads to complementary insights for several design choices of VLP. We release the VLUE benchmark to promote research on building vision-language models that generalize well to more diverse images and concepts unseen during pre-training, and are practical in terms of efficiency-performance trade-off.



### Conformal Credal Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.15239v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.15239v2)
- **Published**: 2022-05-30 16:53:16+00:00
- **Updated**: 2023-06-09 13:30:44+00:00
- **Authors**: Julian Lienen, Caglar Demir, Eyke Hüllermeier
- **Comment**: 26 pages, 5 figures, 10 tables, to be published at the 12th Symposium
  on Conformal and Probabilistic Prediction with Applications (COPA 2023)
- **Journal**: None
- **Summary**: In semi-supervised learning, the paradigm of self-training refers to the idea of learning from pseudo-labels suggested by the learner itself. Across various domains, corresponding methods have proven effective and achieve state-of-the-art performance. However, pseudo-labels typically stem from ad-hoc heuristics, relying on the quality of the predictions though without guaranteeing their validity. One such method, so-called credal self-supervised learning, maintains pseudo-supervision in the form of sets of (instead of single) probability distributions over labels, thereby allowing for a flexible yet uncertainty-aware labeling. Again, however, there is no justification beyond empirical effectiveness. To address this deficiency, we make use of conformal prediction, an approach that comes with guarantees on the validity of set-valued predictions. As a result, the construction of credal sets of labels is supported by a rigorous theoretical foundation, leading to better calibrated and less error-prone supervision for unlabeled data. Along with this, we present effective algorithms for learning from credal self-supervision. An empirical study demonstrates excellent calibration properties of the pseudo-supervision, as well as the competitiveness of our method on several benchmark datasets.



### Re-parameterizing Your Optimizers rather than Architectures
- **Arxiv ID**: http://arxiv.org/abs/2205.15242v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.15242v4)
- **Published**: 2022-05-30 16:55:59+00:00
- **Updated**: 2023-02-09 06:42:50+00:00
- **Authors**: Xiaohan Ding, Honghao Chen, Xiangyu Zhang, Kaiqi Huang, Jungong Han, Guiguang Ding
- **Comment**: ICLR 2023
- **Journal**: None
- **Summary**: The well-designed structures in neural networks reflect the prior knowledge incorporated into the models. However, though different models have various priors, we are used to training them with model-agnostic optimizers such as SGD. In this paper, we propose to incorporate model-specific prior knowledge into optimizers by modifying the gradients according to a set of model-specific hyper-parameters. Such a methodology is referred to as Gradient Re-parameterization, and the optimizers are named RepOptimizers. For the extreme simplicity of model structure, we focus on a VGG-style plain model and showcase that such a simple model trained with a RepOptimizer, which is referred to as RepOpt-VGG, performs on par with or better than the recent well-designed models. From a practical perspective, RepOpt-VGG is a favorable base model because of its simple structure, high inference speed and training efficiency. Compared to Structural Re-parameterization, which adds priors into models via constructing extra training-time structures, RepOptimizers require no extra forward/backward computations and solve the problem of quantization. We hope to spark further research beyond the realms of model structure design. Code and models \url{https://github.com/DingXiaoH/RepOptimizers}.



### Pooling Revisited: Your Receptive Field is Suboptimal
- **Arxiv ID**: http://arxiv.org/abs/2205.15254v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.15254v2)
- **Published**: 2022-05-30 17:03:40+00:00
- **Updated**: 2022-06-29 22:21:53+00:00
- **Authors**: Dong-Hwan Jang, Sanghyeok Chu, Joonhyuk Kim, Bohyung Han
- **Comment**: CVPR 2022; reference updated for section 2
- **Journal**: None
- **Summary**: The size and shape of the receptive field determine how the network aggregates local information and affect the overall performance of a model considerably. Many components in a neural network, such as kernel sizes and strides for convolution and pooling operations, influence the configuration of a receptive field. However, they still rely on hyperparameters, and the receptive fields of existing models result in suboptimal shapes and sizes. Hence, we propose a simple yet effective Dynamically Optimized Pooling operation, referred to as DynOPool, which optimizes the scale factors of feature maps end-to-end by learning the desirable size and shape of its receptive field in each layer. Any kind of resizing modules in a deep neural network can be replaced by the operations with DynOPool at a minimal cost. Also, DynOPool controls the complexity of a model by introducing an additional loss term that constrains computational cost. Our experiments show that the models equipped with the proposed learnable resizing module outperform the baseline networks on multiple datasets in image classification and semantic segmentation.



### Going Beyond One-Hot Encoding in Classification: Can Human Uncertainty Improve Model Performance?
- **Arxiv ID**: http://arxiv.org/abs/2205.15265v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.15265v1)
- **Published**: 2022-05-30 17:19:11+00:00
- **Updated**: 2022-05-30 17:19:11+00:00
- **Authors**: Christoph Koller, Göran Kauermann, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Technological and computational advances continuously drive forward the broad field of deep learning. In recent years, the derivation of quantities describing theuncertainty in the prediction - which naturally accompanies the modeling process - has sparked general interest in the deep learning community. Often neglected in the machine learning setting is the human uncertainty that influences numerous labeling processes. As the core of this work, label uncertainty is explicitly embedded into the training process via distributional labels. We demonstrate the effectiveness of our approach on image classification with a remote sensing data set that contains multiple label votes by domain experts for each image: The incorporation of label uncertainty helps the model to generalize better to unseen data and increases model performance. Similar to existing calibration methods, the distributional labels lead to better-calibrated probabilities, which in turn yield more certain and trustworthy predictions.



### EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model
- **Arxiv ID**: http://arxiv.org/abs/2205.15278v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15278v3)
- **Published**: 2022-05-30 17:39:45+00:00
- **Updated**: 2022-09-23 12:24:13+00:00
- **Authors**: Xinya Ji, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Wayne Wu, Feng Xu, Xun Cao
- **Comment**: Accepted by SIGGRAPH 2022 Conference Proceedings. For demo video and
  codes, see https://jixinya.github.io/projects/EAMM/
- **Journal**: None
- **Summary**: Although significant progress has been made to audio-driven talking face generation, existing methods either neglect facial emotion or cannot be applied to arbitrary subjects. In this paper, we propose the Emotion-Aware Motion Model (EAMM) to generate one-shot emotional talking faces by involving an emotion source video. Specifically, we first propose an Audio2Facial-Dynamics module, which renders talking faces from audio-driven unsupervised zero- and first-order key-points motion. Then through exploring the motion model's properties, we further propose an Implicit Emotion Displacement Learner to represent emotion-related facial dynamics as linearly additive displacements to the previously acquired motion representations. Comprehensive experiments demonstrate that by incorporating the results from both modules, our method can generate satisfactory talking face results on arbitrary subjects with realistic emotion patterns.



### Fast Dynamic Radiance Fields with Time-Aware Neural Voxels
- **Arxiv ID**: http://arxiv.org/abs/2205.15285v2
- **DOI**: 10.1145/3550469.3555383
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2205.15285v2)
- **Published**: 2022-05-30 17:47:31+00:00
- **Updated**: 2022-09-26 09:04:53+00:00
- **Authors**: Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias Nießner, Qi Tian
- **Comment**: SIGGRAPH Asia 2022. Project page: https://jaminfong.cn/tineuvox
- **Journal**: None
- **Summary**: Neural radiance fields (NeRF) have shown great success in modeling 3D scenes and synthesizing novel-view images. However, most previous NeRF methods take much time to optimize one single scene. Explicit data structures, e.g. voxel features, show great potential to accelerate the training process. However, voxel features face two big challenges to be applied to dynamic scenes, i.e. modeling temporal information and capturing different scales of point motions. We propose a radiance field framework by representing scenes with time-aware voxel features, named as TiNeuVox. A tiny coordinate deformation network is introduced to model coarse motion trajectories and temporal information is further enhanced in the radiance network. A multi-distance interpolation method is proposed and applied on voxel features to model both small and large motions. Our framework significantly accelerates the optimization of dynamic radiance fields while maintaining high rendering quality. Empirical evaluation is performed on both synthetic and real scenes. Our TiNeuVox completes training with only 8 minutes and 8-MB storage cost while showing similar or even better rendering performance than previous dynamic NeRF methods.



### Self-Supervised Visual Representation Learning with Semantic Grouping
- **Arxiv ID**: http://arxiv.org/abs/2205.15288v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.15288v2)
- **Published**: 2022-05-30 17:50:59+00:00
- **Updated**: 2022-10-10 15:35:26+00:00
- **Authors**: Xin Wen, Bingchen Zhao, Anlin Zheng, Xiangyu Zhang, Xiaojuan Qi
- **Comment**: Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of learning visual representations from unlabeled scene-centric data. Existing works have demonstrated the potential of utilizing the underlying complex structure within scene-centric data; still, they commonly rely on hand-crafted objectness priors or specialized pretext tasks to build a learning framework, which may harm generalizability. Instead, we propose contrastive learning from data-driven semantic slots, namely SlotCon, for joint semantic grouping and representation learning. The semantic grouping is performed by assigning pixels to a set of learnable prototypes, which can adapt to each sample by attentive pooling over the feature and form new slots. Based on the learned data-dependent slots, a contrastive objective is employed for representation learning, which enhances the discriminability of features, and conversely facilitates grouping semantically coherent pixels together. Compared with previous efforts, by simultaneously optimizing the two coupled objectives of semantic grouping and contrastive learning, our approach bypasses the disadvantages of hand-crafted priors and is able to learn object/group-level representations from scene-centric images. Experiments show our approach effectively decomposes complex scenes into semantic groups for feature learning and significantly benefits downstream tasks, including object detection, instance segmentation, and semantic segmentation. Code is available at: https://github.com/CVMI-Lab/SlotCon.



### Zero-Shot and Few-Shot Learning for Lung Cancer Multi-Label Classification using Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2205.15290v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.15290v2)
- **Published**: 2022-05-30 17:54:07+00:00
- **Updated**: 2022-05-31 17:11:03+00:00
- **Authors**: Fu-Ming Guo, Yingfang Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Lung cancer is the leading cause of cancer-related death worldwide. Lung adenocarcinoma (LUAD) and lung squamous cell carcinoma (LUSC) are the most common histologic subtypes of non-small-cell lung cancer (NSCLC). Histology is an essential tool for lung cancer diagnosis. Pathologists make classifications according to the dominant subtypes. Although morphology remains the standard for diagnosis, significant tool needs to be developed to elucidate the diagnosis. In our study, we utilize the pre-trained Vision Transformer (ViT) model to classify multiple label lung cancer on histologic slices (from dataset LC25000), in both Zero-Shot and Few-Shot settings. Then we compare the performance of Zero-Shot and Few-Shot ViT on accuracy, precision, recall, sensitivity and specificity. Our study show that the pre-trained ViT model has a good performance in Zero-Shot setting, a competitive accuracy ($99.87\%$) in Few-Shot setting ({epoch = 1}) and an optimal result ($100.00\%$ on both validation set and test set) in Few-Shot seeting ({epoch = 5}).



### Adapting Rapid Motor Adaptation for Bipedal Robots
- **Arxiv ID**: http://arxiv.org/abs/2205.15299v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2205.15299v2)
- **Published**: 2022-05-30 17:59:09+00:00
- **Updated**: 2022-09-06 23:17:01+00:00
- **Authors**: Ashish Kumar, Zhongyu Li, Jun Zeng, Deepak Pathak, Koushil Sreenath, Jitendra Malik
- **Comment**: First two authors contributed equally. Website at
  https://ashish-kmr.github.io/a-rma/
- **Journal**: None
- **Summary**: Recent advances in legged locomotion have enabled quadrupeds to walk on challenging terrains. However, bipedal robots are inherently more unstable and hence it's harder to design walking controllers for them. In this work, we leverage recent advances in rapid adaptation for locomotion control, and extend them to work on bipedal robots. Similar to existing works, we start with a base policy which produces actions while taking as input an estimated extrinsics vector from an adaptation module. This extrinsics vector contains information about the environment and enables the walking controller to rapidly adapt online. However, the extrinsics estimator could be imperfect, which might lead to poor performance of the base policy which expects a perfect estimator. In this paper, we propose A-RMA (Adapting RMA), which additionally adapts the base policy for the imperfect extrinsics estimator by finetuning it using model-free RL. We demonstrate that A-RMA outperforms a number of RL-based baseline controllers and model-based controllers in simulation, and show zero-shot deployment of a single A-RMA policy to enable a bipedal robot, Cassie, to walk in a variety of different scenarios in the real world beyond what it has seen during training. Videos and results at https://ashish-kmr.github.io/a-rma/



### Searching for the Essence of Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2205.15357v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2205.15357v3)
- **Published**: 2022-05-30 18:04:57+00:00
- **Updated**: 2023-02-03 10:38:51+00:00
- **Authors**: Dennis Y. Menn, Tzu-hsun Feng, Hung-yi Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks have demonstrated state-of-the-art performance in various machine learning fields. However, the introduction of malicious perturbations in input data, known as adversarial examples, has been shown to deceive neural network predictions. This poses potential risks for real-world applications such as autonomous driving and text identification. In order to mitigate these risks, a comprehensive understanding of the mechanisms underlying adversarial examples is essential. In this study, we demonstrate that adversarial perturbations contain human-recognizable information, which is the key conspirator responsible for a neural network's incorrect prediction, in contrast to the widely held belief that human-unidentifiable characteristics play a critical role in fooling a network. This concept of human-recognizable characteristics enables us to explain key features of adversarial perturbations, including their existence, transferability among different neural networks, and increased interpretability for adversarial training. We also uncover two unique properties of adversarial perturbations that deceive neural networks: masking and generation. Additionally, a special class, the complementary class, is identified when neural networks classify input images. The presence of human-recognizable information in adversarial perturbations allows researchers to gain insight into the working principles of neural networks and may lead to the development of techniques for detecting and defending against adversarial attacks.



### AI-enabled Sound Pattern Recognition on Asthma Medication Adherence: Evaluation with the RDA Benchmark Suite
- **Arxiv ID**: http://arxiv.org/abs/2205.15360v3
- **DOI**: 10.1109/ACCESS.2023.3243547
- **Categories**: **cs.SD**, cs.CV, cs.CY, cs.GL, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2205.15360v3)
- **Published**: 2022-05-30 18:08:28+00:00
- **Updated**: 2023-04-16 17:32:06+00:00
- **Authors**: Nikos D. Fakotakis, Stavros Nousias, Gerasimos Arvanitis, Evangelia I. Zacharaki, Konstantinos Moustakas
- **Comment**: None
- **Journal**: None
- **Summary**: Asthma is a common, usually long-term respiratory disease with negative impact on global society and economy. Treatment involves using medical devices (inhalers) that distribute medication to the airways and its efficiency depends on the precision of the inhalation technique. There is a clinical need for objective methods to assess the inhalation technique, during clinical consultation. Integrated health monitoring systems, equipped with sensors, enable the recognition of drug actuation, embedded with sound signal detection, analysis and identification, from intelligent structures, that could provide powerful tools for reliable content management. Health monitoring systems equipped with sensors, embedded with sound signal detection, enable the recognition of drug actuation and could be used for effective audio content analysis. This paper revisits sound pattern recognition with machine learning techniques for asthma medication adherence assessment and presents the Respiratory and Drug Actuation (RDA) Suite (https://gitlab.com/vvr/monitoring-medication-adherence/rda-benchmark) for benchmarking and further research. The RDA Suite includes a set of tools for audio processing, feature extraction and classification procedures and is provided along with a dataset, consisting of respiratory and drug actuation sounds. The classification models in RDA are implemented based on conventional and advanced machine learning and deep networks' architectures. This study provides a comparative evaluation of the implemented approaches, examines potential improvements and discusses on challenges and future tendencies.



### TubeFormer-DeepLab: Video Mask Transformer
- **Arxiv ID**: http://arxiv.org/abs/2205.15361v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15361v2)
- **Published**: 2022-05-30 18:10:33+00:00
- **Updated**: 2023-03-05 23:42:50+00:00
- **Authors**: Dahun Kim, Jun Xie, Huiyu Wang, Siyuan Qiao, Qihang Yu, Hong-Seok Kim, Hartwig Adam, In So Kweon, Liang-Chieh Chen
- **Comment**: CVPR 2022; arXiv v2: add results on VIPSeg val/test sets and VSPW new
  test set
- **Journal**: None
- **Summary**: We present TubeFormer-DeepLab, the first attempt to tackle multiple core video segmentation tasks in a unified manner. Different video segmentation tasks (e.g., video semantic/instance/panoptic segmentation) are usually considered as distinct problems. State-of-the-art models adopted in the separate communities have diverged, and radically different approaches dominate in each task. By contrast, we make a crucial observation that video segmentation tasks could be generally formulated as the problem of assigning different predicted labels to video tubes (where a tube is obtained by linking segmentation masks along the time axis) and the labels may encode different values depending on the target task. The observation motivates us to develop TubeFormer-DeepLab, a simple and effective video mask transformer model that is widely applicable to multiple video segmentation tasks. TubeFormer-DeepLab directly predicts video tubes with task-specific labels (either pure semantic categories, or both semantic categories and instance identities), which not only significantly simplifies video segmentation models, but also advances state-of-the-art results on multiple video segmentation benchmarks



### Dictionary Learning with Accumulator Neurons
- **Arxiv ID**: http://arxiv.org/abs/2205.15386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15386v1)
- **Published**: 2022-05-30 19:06:41+00:00
- **Updated**: 2022-05-30 19:06:41+00:00
- **Authors**: Gavin Parpart, Carlos Gonzalez, Terrence C. Stewart, Edward Kim, Jocelyn Rego, Andrew O'Brien, Steven Nesbit, Garrett T. Kenyon, Yijing Watkins
- **Comment**: None
- **Journal**: None
- **Summary**: The Locally Competitive Algorithm (LCA) uses local competition between non-spiking leaky integrator neurons to infer sparse representations, allowing for potentially real-time execution on massively parallel neuromorphic architectures such as Intel's Loihi processor. Here, we focus on the problem of inferring sparse representations from streaming video using dictionaries of spatiotemporal features optimized in an unsupervised manner for sparse reconstruction. Non-spiking LCA has previously been used to achieve unsupervised learning of spatiotemporal dictionaries composed of convolutional kernels from raw, unlabeled video. We demonstrate how unsupervised dictionary learning with spiking LCA (\hbox{S-LCA}) can be efficiently implemented using accumulator neurons, which combine a conventional leaky-integrate-and-fire (\hbox{LIF}) spike generator with an additional state variable that is used to minimize the difference between the integrated input and the spiking output. We demonstrate dictionary learning across a wide range of dynamical regimes, from graded to intermittent spiking, for inferring sparse representations of both static images drawn from the CIFAR database as well as video frames captured from a DVS camera. On a classification task that requires identification of the suite from a deck of cards being rapidly flipped through as viewed by a DVS camera, we find essentially no degradation in performance as the LCA model used to infer sparse spatiotemporal representations migrates from graded to spiking. We conclude that accumulator neurons are likely to provide a powerful enabling component of future neuromorphic hardware for implementing online unsupervised learning of spatiotemporal dictionaries optimized for sparse reconstruction of streaming video from event based DVS cameras.



### VoGE: A Differentiable Volume Renderer using Gaussian Ellipsoids for Analysis-by-Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2205.15401v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.15401v2)
- **Published**: 2022-05-30 19:52:11+00:00
- **Updated**: 2023-02-10 19:37:10+00:00
- **Authors**: Angtian Wang, Peng Wang, Jian Sun, Adam Kortylewski, Alan Yuille
- **Comment**: Accepted by ICLR2023
- **Journal**: None
- **Summary**: The Gaussian reconstruction kernels have been proposed by Westover (1990) and studied by the computer graphics community back in the 90s, which gives an alternative representation of object 3D geometry from meshes and point clouds. On the other hand, current state-of-the-art (SoTA) differentiable renderers, Liu et al. (2019), use rasterization to collect triangles or points on each image pixel and blend them based on the viewing distance. In this paper, we propose VoGE, which utilizes the volumetric Gaussian reconstruction kernels as geometric primitives. The VoGE rendering pipeline uses ray tracing to capture the nearest primitives and blends them as mixtures based on their volume density distributions along the rays. To efficiently render via VoGE, we propose an approximate closeform solution for the volume density aggregation and a coarse-to-fine rendering strategy. Finally, we provide a CUDA implementation of VoGE, which enables real-time level rendering with a competitive rendering speed in comparison to PyTorch3D. Quantitative and qualitative experiment results show VoGE outperforms SoTA counterparts when applied to various vision tasks, e.g., object pose estimation, shape/texture fitting, and occlusion reasoning. The VoGE library and demos are available at: https://github.com/Angtian/VoGE.



### Gator: Customizable Channel Pruning of Neural Networks with Gating
- **Arxiv ID**: http://arxiv.org/abs/2205.15404v2
- **DOI**: 10.1007/978-3-030-86380-7_5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15404v2)
- **Published**: 2022-05-30 20:07:25+00:00
- **Updated**: 2022-06-01 08:42:05+00:00
- **Authors**: Eli Passov, Eli David, Nathan S. Netanyahu
- **Comment**: 14 pages, 3 figures. The version that appeared in ICANN is an earlier
  version
- **Journal**: In International Conference on Artificial Neural Networks, Vol.
  12894 (pp. 46-58). Springer, Cham 2021
- **Summary**: The rise of neural network (NN) applications has prompted an increased interest in compression, with a particular focus on channel pruning, which does not require any additional hardware. Most pruning methods employ either single-layer operations or global schemes to determine which channels to remove followed by fine-tuning of the network. In this paper we present Gator, a channel-pruning method which temporarily adds learned gating mechanisms for pruning of individual channels, and which is trained with an additional auxiliary loss, aimed at reducing the computational cost due to memory, (theoretical) speedup (in terms of FLOPs), and practical, hardware-specific speedup. Gator introduces a new formulation of dependencies between NN layers which, in contrast to most previous methods, enables pruning of non-sequential parts, such as layers on ResNet's highway, and even removing entire ResNet blocks. Gator's pruning for ResNet-50 trained on ImageNet produces state-of-the-art (SOTA) results, such as 50% FLOPs reduction with only 0.4%-drop in top-5 accuracy. Also, Gator outperforms previous pruning models, in terms of GPU latency by running 1.4 times faster. Furthermore, Gator achieves improved top-5 accuracy results, compared to MobileNetV2 and SqueezeNet, for similar runtimes. The source code of this work is available at: https://github.com/EliPassov/gator.



### Grid HTM: Hierarchical Temporal Memory for Anomaly Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/2205.15407v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.15407v1)
- **Published**: 2022-05-30 20:10:23+00:00
- **Updated**: 2022-05-30 20:10:23+00:00
- **Authors**: Vladimir Monakhov, Vajira Thambawita, Pål Halvorsen, Michael A. Riegler
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: The interest for video anomaly detection systems has gained traction for the past few years. The current approaches use deep learning to perform anomaly detection in videos, but this approach has multiple problems. For starters, deep learning in general has issues with noise, concept drift, explainability, and training data volumes. Additionally, anomaly detection in itself is a complex task and faces challenges such as unknowness, heterogeneity, and class imbalance. Anomaly detection using deep learning is therefore mainly constrained to generative models such as generative adversarial networks and autoencoders due to their unsupervised nature, but even they suffer from general deep learning issues and are hard to train properly. In this paper, we explore the capabilities of the Hierarchical Temporal Memory (HTM) algorithm to perform anomaly detection in videos, as it has favorable properties such as noise tolerance and online learning which combats concept drift. We introduce a novel version of HTM, namely, Grid HTM, which is an HTM-based architecture specifically for anomaly detection in complex videos such as surveillance footage.



### LiDAR-aid Inertial Poser: Large-scale Human Motion Capture by Sparse Inertial and LiDAR Sensors
- **Arxiv ID**: http://arxiv.org/abs/2205.15410v2
- **DOI**: 10.1109/TVCG.2023.3247088
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15410v2)
- **Published**: 2022-05-30 20:15:11+00:00
- **Updated**: 2023-04-07 18:04:50+00:00
- **Authors**: Yiming Ren, Chengfeng Zhao, Yannan He, Peishan Cong, Han Liang, Jingyi Yu, Lan Xu, Yuexin Ma
- **Comment**: None
- **Journal**: IEEE Transactions on Visualization and Computer Graphics ( Volume:
  29, Issue: 5, May 2023)
- **Summary**: We propose a multi-sensor fusion method for capturing challenging 3D human motions with accurate consecutive local poses and global trajectories in large-scale scenarios, only using single LiDAR and 4 IMUs, which are set up conveniently and worn lightly. Specifically, to fully utilize the global geometry information captured by LiDAR and local dynamic motions captured by IMUs, we design a two-stage pose estimator in a coarse-to-fine manner, where point clouds provide the coarse body shape and IMU measurements optimize the local actions. Furthermore, considering the translation deviation caused by the view-dependent partial point cloud, we propose a pose-guided translation corrector. It predicts the offset between captured points and the real root locations, which makes the consecutive movements and trajectories more precise and natural. Moreover, we collect a LiDAR-IMU multi-modal mocap dataset, LIPD, with diverse human actions in long-range scenarios. Extensive quantitative and qualitative experiments on LIPD and other open datasets all demonstrate the capability of our approach for compelling motion capture in large-scale scenarios, which outperforms other methods by an obvious margin. We will release our code and captured dataset to stimulate future research.



### PolypConnect: Image inpainting for generating realistic gastrointestinal tract images with polyps
- **Arxiv ID**: http://arxiv.org/abs/2205.15413v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.15413v1)
- **Published**: 2022-05-30 20:20:19+00:00
- **Updated**: 2022-05-30 20:20:19+00:00
- **Authors**: Jan Andre Fagereng, Vajira Thambawita, Andrea M. Storås, Sravanthi Parasa, Thomas de Lange, Pål Halvorsen, Michael A. Riegler
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Early identification of a polyp in the lower gastrointestinal (GI) tract can lead to prevention of life-threatening colorectal cancer. Developing computer-aided diagnosis (CAD) systems to detect polyps can improve detection accuracy and efficiency and save the time of the domain experts called endoscopists. Lack of annotated data is a common challenge when building CAD systems. Generating synthetic medical data is an active research area to overcome the problem of having relatively few true positive cases in the medical domain. To be able to efficiently train machine learning (ML) models, which are the core of CAD systems, a considerable amount of data should be used. In this respect, we propose the PolypConnect pipeline, which can convert non-polyp images into polyp images to increase the size of training datasets for training. We present the whole pipeline with quantitative and qualitative evaluations involving endoscopists. The polyp segmentation model trained using synthetic data, and real data shows a 5.1% improvement of mean intersection over union (mIOU), compared to the model trained only using real data. The codes of all the experiments are available on GitHub to reproduce the results.



### Fitting and recognition of geometric primitives in segmented 3D point clouds using a localized voting procedure
- **Arxiv ID**: http://arxiv.org/abs/2205.15426v2
- **DOI**: 10.1016/j.cagd.2022.102123
- **Categories**: **cs.CV**, cs.CG, cs.NA, math.NA, 65D18, 65D17, 68U05, 62H30,, G.1.2; I.3.5; I.5; J.6
- **Links**: [PDF](http://arxiv.org/pdf/2205.15426v2)
- **Published**: 2022-05-30 20:47:43+00:00
- **Updated**: 2022-07-07 08:54:44+00:00
- **Authors**: Andrea Raffo, Chiara Romanengo, Bianca Falcidieno, Silvia Biasotti
- **Comment**: None
- **Journal**: Computer Aided Geometric Design 97 (2022) 102123
- **Summary**: The automatic creation of geometric models from point clouds has numerous applications in CAD (e.g., reverse engineering, manufacturing, assembling) and, more in general, in shape modelling and processing. Given a segmented point cloud representing a man-made object, we propose a method for recognizing simple geometric primitives and their interrelationships. Our approach is based on the Hough transform (HT) for its ability to deal with noise, missing parts and outliers. In our method we introduce a novel technique for processing segmented point clouds that, through a voting procedure, is able to provide an initial estimate of the geometric parameters characterizing each primitive type. By using these estimates, we localize the search of the optimal solution in a dimensionally-reduced parameter space thus making it efficient to extend the HT to more primitives than those that are generally found in the literature, i.e. planes and spheres. Then, we extract a number of geometric descriptors that uniquely characterize a segment, and, on the basis of these descriptors, we show how to aggregate parts of primitives (segments). Experiments on both synthetic and industrial scans reveal the robustness of the primitive fitting method and its effectiveness for inferring relations among segments.



### Segmentation Consistency Training: Out-of-Distribution Generalization for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.15428v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.15428v1)
- **Published**: 2022-05-30 20:57:15+00:00
- **Updated**: 2022-05-30 20:57:15+00:00
- **Authors**: Birk Torpmann-Hagen, Vajira Thambawita, Kyrre Glette, Pål Halvorsen, Michael A. Riegler
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Generalizability is seen as one of the major challenges in deep learning, in particular in the domain of medical imaging, where a change of hospital or in imaging routines can lead to a complete failure of a model. To tackle this, we introduce Consistency Training, a training procedure and alternative to data augmentation based on maximizing models' prediction consistency across augmented and unaugmented data in order to facilitate better out-of-distribution generalization. To this end, we develop a novel region-based segmentation loss function called Segmentation Inconsistency Loss (SIL), which considers the differences between pairs of augmented and unaugmented predictions and labels. We demonstrate that Consistency Training outperforms conventional data augmentation on several out-of-distribution datasets on polyp segmentation, a popular medical task.



### Exploring Advances in Transformers and CNN for Skin Lesion Diagnosis on Small Datasets
- **Arxiv ID**: http://arxiv.org/abs/2205.15442v1
- **DOI**: 10.1007/978-3-031-21689-3_21
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15442v1)
- **Published**: 2022-05-30 21:41:32+00:00
- **Updated**: 2022-05-30 21:41:32+00:00
- **Authors**: Leandro M. de Lima, Renato A. Krohling
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Skin cancer is one of the most common types of cancer in the world. Different computer-aided diagnosis systems have been proposed to tackle skin lesion diagnosis, most of them based in deep convolutional neural networks. However, recent advances in computer vision achieved state-of-art results in many tasks, notably Transformer-based networks. We explore and evaluate advances in computer vision architectures, training methods and multimodal feature fusion for skin lesion diagnosis task. Experiments show that PiT ($0.800 \pm 0.006$), CoaT ($0.780 \pm 0.024$) and ViT ($0.771 \pm 0.018$) backbone models with MetaBlock fusion achieved state-of-art results for balanced accuracy metric in PAD-UFES-20 dataset.



### Continual Object Detection: A review of definitions, strategies, and challenges
- **Arxiv ID**: http://arxiv.org/abs/2205.15445v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.15445v1)
- **Published**: 2022-05-30 21:57:48+00:00
- **Updated**: 2022-05-30 21:57:48+00:00
- **Authors**: Angelo G. Menezes, Gustavo de Moura, Cézanne Alves, André C. P. L. F. de Carvalho
- **Comment**: None
- **Journal**: None
- **Summary**: The field of Continual Learning investigates the ability to learn consecutive tasks without losing performance on those previously learned. Its focus has been mainly on incremental classification tasks. We believe that research in continual object detection deserves even more attention due to its vast range of applications in robotics and autonomous vehicles. This scenario is more complex than conventional classification given the occurrence of instances of classes that are unknown at the time, but can appear in subsequent tasks as a new class to be learned, resulting in missing annotations and conflicts with the background label. In this review, we analyze the current strategies proposed to tackle the problem of class-incremental object detection. Our main contributions are: (1) a short and systematic review of the methods that propose solutions to traditional incremental object detection scenarios; (2) A comprehensive evaluation of the existing approaches using a new metric to quantify the stability and plasticity of each technique in a standard way; (3) an overview of the current trends within continual object detection and a discussion of possible future research directions.



### FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER
- **Arxiv ID**: http://arxiv.org/abs/2205.15448v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2205.15448v3)
- **Published**: 2022-05-30 22:09:57+00:00
- **Updated**: 2023-03-23 15:48:05+00:00
- **Authors**: Ce Zheng, Matias Mendieta, Taojiannan Yang, Guo-Jun Qi, Chen Chen
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Recently, vision transformers have shown great success in a set of human reconstruction tasks such as 2D human pose estimation (2D HPE), 3D human pose estimation (3D HPE), and human mesh reconstruction (HMR) tasks. In these tasks, feature map representations of the human structural information are often extracted first from the image by a CNN (such as HRNet), and then further processed by transformer to predict the heatmaps (encodes each joint's location into a feature map with a Gaussian distribution) for HPE or HMR. However, existing transformer architectures are not able to process these feature map inputs directly, forcing an unnatural flattening of the location-sensitive human structural information. Furthermore, much of the performance benefit in recent HPE and HMR methods has come at the cost of ever-increasing computation and memory needs. Therefore, to simultaneously address these problems, we propose FeatER, a novel transformer design that preserves the inherent structure of feature map representations when modeling attention while reducing memory and computational costs. Taking advantage of FeatER, we build an efficient network for a set of human reconstruction tasks including 2D HPE, 3D HPE, and HMR. A feature map reconstruction module is applied to improve the performance of the estimated human pose and mesh. Extensive experiments demonstrate the effectiveness of FeatER on various human pose and mesh datasets. For instance, FeatER outperforms the SOTA method MeshGraphormer by requiring 5% of Params and 16% of MACs on Human3.6M and 3DPW datasets. The project webpage is https://zczcwh.github.io/feater_page/.



### MVMO: A Multi-Object Dataset for Wide Baseline Multi-View Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.15452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15452v1)
- **Published**: 2022-05-30 22:37:43+00:00
- **Updated**: 2022-05-30 22:37:43+00:00
- **Authors**: Aitor Alvarez-Gila, Joost van de Weijer, Yaxing Wang, Estibaliz Garrote
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: We present MVMO (Multi-View, Multi-Object dataset): a synthetic dataset of 116,000 scenes containing randomly placed objects of 10 distinct classes and captured from 25 camera locations in the upper hemisphere. MVMO comprises photorealistic, path-traced image renders, together with semantic segmentation ground truth for every view. Unlike existing multi-view datasets, MVMO features wide baselines between cameras and high density of objects, which lead to large disparities, heavy occlusions and view-dependent object appearance. Single view semantic segmentation is hindered by self and inter-object occlusions that could benefit from additional viewpoints. Therefore, we expect that MVMO will propel research in multi-view semantic segmentation and cross-view semantic transfer. We also provide baselines that show that new research is needed in such fields to exploit the complementary information of multi-view setups.



### Registering Image Volumes using 3D SIFT and Discrete SP-Symmetry
- **Arxiv ID**: http://arxiv.org/abs/2205.15456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15456v1)
- **Published**: 2022-05-30 22:57:55+00:00
- **Updated**: 2022-05-30 22:57:55+00:00
- **Authors**: Laurent Chauvin, William Wells III, Matthew Toews
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: This paper proposes to extend local image features in 3D to include invariance to discrete symmetry including inversion of spatial axes and image contrast. A binary feature sign $s \in \{-1,+1\}$ is defined as the sign of the Laplacian operator $\nabla^2$, and used to obtain a descriptor that is invariant to image sign inversion $s \rightarrow -s$ and 3D parity transforms $(x,y,z)\rightarrow(-x,-y,-z)$, i.e. SP-invariant or SP-symmetric. SP-symmetry applies to arbitrary scalar image fields $I: R^3 \rightarrow R^1$ mapping 3D coordinates $(x,y,z) \in R^3$ to scalar intensity $I(x,y,z) \in R^1$, generalizing the well-known charge conjugation and parity symmetry (CP-symmetry) applying to elementary charged particles. Feature orientation is modeled as a set of discrete states corresponding to potential axis reflections, independently of image contrast inversion. Two primary axis vectors are derived from image observations and potentially subject to reflection, and a third axis is an axial vector defined by the right-hand rule. Augmenting local feature properties with sign in addition to standard (location, scale, orientation) geometry leads to descriptors that are invariant to coordinate reflections and intensity contrast inversion. Feature properties are factored in to probabilistic point-based registration as symmetric kernels, based on a model of binary feature correspondence. Experiments using the well-known coherent point drift (CPD) algorithm demonstrate that SIFT-CPD kernels achieve the most accurate and rapid registration of the human brain and CT chest, including multiple MRI modalities of differing intensity contrast, and abnormal local variations such as tumors or occlusions. SIFT-CPD image registration is invariant to global scaling, rotation and translation and image intensity inversions of the input data.



### Few-Shot Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2205.15463v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2205.15463v1)
- **Published**: 2022-05-30 23:20:33+00:00
- **Updated**: 2022-05-30 23:20:33+00:00
- **Authors**: Giorgio Giannone, Didrik Nielsen, Ole Winther
- **Comment**: None
- **Journal**: None
- **Summary**: Denoising diffusion probabilistic models (DDPM) are powerful hierarchical latent variable models with remarkable sample generation quality and training stability. These properties can be attributed to parameter sharing in the generative hierarchy, as well as a parameter-free diffusion-based inference procedure. In this paper, we present Few-Shot Diffusion Models (FSDM), a framework for few-shot generation leveraging conditional DDPMs. FSDMs are trained to adapt the generative process conditioned on a small set of images from a given class by aggregating image patch information using a set-based Vision Transformer (ViT). At test time, the model is able to generate samples from previously unseen classes conditioned on as few as 5 samples from that class. We empirically show that FSDM can perform few-shot generation and transfer to new datasets. We benchmark variants of our method on complex vision datasets for few-shot learning and compare to unconditional and conditional DDPM baselines. Additionally, we show how conditioning the model on patch-based input set information improves training convergence.



### Towards retrieving dispersion profiles using quantum-mimic Optical Coherence Tomography and Machine Learnin
- **Arxiv ID**: http://arxiv.org/abs/2206.02547v1
- **DOI**: 10.1364/OE.460079
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2206.02547v1)
- **Published**: 2022-05-30 23:30:29+00:00
- **Updated**: 2022-05-30 23:30:29+00:00
- **Authors**: Krzysztof A. Maliszewski, Piotr Kolenderski, Varvara Vetrova, Sylwia M. Kolenderska
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Artefacts in quantum-mimic Optical Coherence Tomography are considered detrimental because they scramble the images even for the simplest objects. They are a side effect of autocorrelation which is used in the quantum entanglement mimicking algorithm behind this method. Interestingly, the autocorrelation imprints certain characteristics onto an artefact - it makes its shape and characteristics depend on the amount of dispersion exhibited by the layer that artefact corresponds to. This unique relationship between the artefact and the layer's dispersion can be used to determine Group Velocity Dispersion (GVD) values of object layers and, based on them, build a dispersion-contrasted depth profile. The retrieval of GVD profiles is achieved via Machine Learning. During training, a neural network learns the relationship between GVD and the artefacts' shape and characteristics, and consequently, it is able to provide a good qualitative representation of object's dispersion profile for never-seen-before data: computer-generated single dispersive layers and experimental pieces of glass.



### GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2205.15469v4
- **DOI**: 10.1109/TPAMI.2023.3264571
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.15469v4)
- **Published**: 2022-05-30 23:49:19+00:00
- **Updated**: 2023-04-10 14:24:31+00:00
- **Authors**: Peng Zheng, Huazhu Fu, Deng-Ping Fan, Qi Fan, Jie Qin, Yu-Wing Tai, Chi-Keung Tang, Luc Van Gool
- **Comment**: T-PAMI 2023
- **Journal**: None
- **Summary**: In this paper, we present a novel end-to-end group collaborative learning network, termed GCoNet+, which can effectively and efficiently (250 fps) identify co-salient objects in natural scenes. The proposed GCoNet+ achieves the new state-of-the-art performance for co-salient object detection (CoSOD) through mining consensus representations based on the following two essential criteria: 1) intra-group compactness to better formulate the consistency among co-salient objects by capturing their inherent shared attributes using our novel group affinity module (GAM); 2) inter-group separability to effectively suppress the influence of noisy objects on the output by introducing our new group collaborating module (GCM) conditioning on the inconsistent consensus. To further improve the accuracy, we design a series of simple yet effective components as follows: i) a recurrent auxiliary classification module (RACM) promoting model learning at the semantic level; ii) a confidence enhancement module (CEM) assisting the model in improving the quality of the final predictions; and iii) a group-based symmetric triplet (GST) loss guiding the model to learn more discriminative features. Extensive experiments on three challenging benchmarks, i.e., CoCA, CoSOD3k, and CoSal2015, demonstrate that our GCoNet+ outperforms the existing 12 cutting-edge models. Code has been released at https://github.com/ZhengPeng7/GCoNet_plus.



