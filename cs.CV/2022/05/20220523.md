# Arxiv Papers in cs.CV on 2022-05-23
### Boosting Multi-Label Image Classification with Complementary Parallel Self-Distillation
- **Arxiv ID**: http://arxiv.org/abs/2205.10986v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.10986v1)
- **Published**: 2022-05-23 01:28:38+00:00
- **Updated**: 2022-05-23 01:28:38+00:00
- **Authors**: Jiazhi Xu, Sheng Huang, Fengtao Zhou, Luwen Huangfu, Daniel Zeng, Bo Liu
- **Comment**: accepted by IJCAI2022
- **Journal**: None
- **Summary**: Multi-Label Image Classification (MLIC) approaches usually exploit label correlations to achieve good performance. However, emphasizing correlation like co-occurrence may overlook discriminative features of the target itself and lead to model overfitting, thus undermining the performance. In this study, we propose a generic framework named Parallel Self-Distillation (PSD) for boosting MLIC models. PSD decomposes the original MLIC task into several simpler MLIC sub-tasks via two elaborated complementary task decomposition strategies named Co-occurrence Graph Partition (CGP) and Dis-occurrence Graph Partition (DGP). Then, the MLIC models of fewer categories are trained with these sub-tasks in parallel for respectively learning the joint patterns and the category-specific patterns of labels. Finally, knowledge distillation is leveraged to learn a compact global ensemble of full categories with these learned patterns for reconciling the label correlation exploitation and model overfitting. Extensive results on MS-COCO and NUS-WIDE datasets demonstrate that our framework can be easily plugged into many MLIC approaches and improve performances of recent state-of-the-art approaches. The explainable visual study also further validates that our method is able to learn both the category-specific and co-occurring features. The source code is released at https://github.com/Robbie-Xu/CPSD.



### MolMiner: You only look once for chemical structure recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.11016v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2205.11016v1)
- **Published**: 2022-05-23 03:16:19+00:00
- **Updated**: 2022-05-23 03:16:19+00:00
- **Authors**: Youjun Xu, Jinchuan Xiao, Chia-Han Chou, Jianhang Zhang, Jintao Zhu, Qiwan Hu, Hemin Li, Ningsheng Han, Bingyu Liu, Shuaipeng Zhang, Jinyu Han, Zhen Zhang, Shuhao Zhang, Weilin Zhang, Luhua Lai, Jianfeng Pei
- **Comment**: 19 pages, 4 figures
- **Journal**: None
- **Summary**: Molecular structures are always depicted as 2D printed form in scientific documents like journal papers and patents. However, these 2D depictions are not machine-readable. Due to a backlog of decades and an increasing amount of these printed literature, there is a high demand for the translation of printed depictions into machine-readable formats, which is known as Optical Chemical Structure Recognition (OCSR). Most OCSR systems developed over the last three decades follow a rule-based approach where the key step of vectorization of the depiction is based on the interpretation of vectors and nodes as bonds and atoms. Here, we present a practical software MolMiner, which is primarily built up using deep neural networks originally developed for semantic segmentation and object detection to recognize atom and bond elements from documents. These recognized elements can be easily connected as a molecular graph with distance-based construction algorithm. We carefully evaluate our software on four benchmark datasets with the state-of-the-art performance. Various real application scenarios are also tested, yielding satisfactory outcomes. The free download links of Mac and Windows versions are available: Mac: https://molminer-cdn.iipharma.cn/pharma-mind/artifact/latest/mac/PharmaMind-mac-latest-setup.dmg and Windows: https://molminer-cdn.iipharma.cn/pharma-mind/artifact/latest/win/PharmaMind-win-latest-setup.exe



### A Comprehensive Handwritten Paragraph Text Recognition System: LexiconNet
- **Arxiv ID**: http://arxiv.org/abs/2205.11018v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11018v3)
- **Published**: 2022-05-23 03:35:45+00:00
- **Updated**: 2023-07-05 06:08:29+00:00
- **Authors**: Lalita Kumari, Sukhdeep Singh, Vaibhav Varish Singh Rathore, Anuj Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we have presented an efficient procedure using two state-of-the-art approaches from the literature of handwritten text recognition as Vertical Attention Network and Word Beam Search. The attention module is responsible for internal line segmentation that consequently processes a page in a line-by-line manner. At the decoding step, we have added a connectionist temporal classification-based word beam search decoder as a post-processing step. In this study, an end-to-end paragraph recognition system is presented with a lexicon decoder as a post-processing step. Our procedure reports state-of-the-art results on standard datasets. The reported character error rate is 3.24% on the IAM dataset with 27.19% improvement, 1.13% on RIMES with 40.83% improvement and 2.43% on the READ-16 dataset with 32.31% improvement from existing literature and the word error rate is 8.29% on IAM dataset with 43.02% improvement, 2.94% on RIMES dataset with 56.25% improvement and 7.35% on READ-2016 dataset with 47.27% improvement from the existing results. The character error rate and word error rate reported in this work surpass the results reported in the literature.



### Cardiomegaly Detection using Deep Convolutional Neural Network with U-Net
- **Arxiv ID**: http://arxiv.org/abs/2205.11515v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11515v2)
- **Published**: 2022-05-23 04:02:20+00:00
- **Updated**: 2022-07-07 05:57:17+00:00
- **Authors**: Soham S. Sarpotdar
- **Comment**: 8 pages, 7 figures, submitted in the IEEE Transactions on Neural
  Networks and Learning Systems(IEEE TNNLS)
- **Journal**: None
- **Summary**: Cardiomegaly is indeed a medical disease in which the heart is enlarged. Cardiomegaly is better to handle if caught early, so early detection is critical. The chest X-ray, being one of the most often used radiography examinations, has been used to detect and visualize abnormalities of human organs for decades. X-ray is also a significant medical diagnosis tool for cardiomegaly. Even for domain experts, distinguishing the many types of diseases from the X-ray is a difficult and time-consuming task. Deep learning models are also most effective when used on huge data sets, yet due to privacy concerns, large datasets are rarely available inside the medical industry. A Deep learning-based customized retrained U-Net model for detecting Cardiomegaly disease is presented in this research. In the training phase, chest X-ray images from the "ChestX-ray8" open source real dataset are used. To reduce computing time, this model performs data preprocessing, picture improvement, image compression, and classification before moving on to the training step. The work used a chest x-ray image dataset to simulate and produced a diagnostic accuracy of 94%, a sensitivity of 96.2 percent, and a specificity of 92.5 percent, which beats prior pre-trained model findings for identifying Cardiomegaly disease.



### RCP: Recurrent Closest Point for Scene Flow Estimation on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2205.11028v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11028v2)
- **Published**: 2022-05-23 04:04:30+00:00
- **Updated**: 2022-05-24 04:11:44+00:00
- **Authors**: Xiaodong Gu, Chengzhou Tang, Weihao Yuan, Zuozhuo Dai, Siyu Zhu, Ping Tan
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: 3D motion estimation including scene flow and point cloud registration has drawn increasing interest. Inspired by 2D flow estimation, recent methods employ deep neural networks to construct the cost volume for estimating accurate 3D flow. However, these methods are limited by the fact that it is difficult to define a search window on point clouds because of the irregular data structure. In this paper, we avoid this irregularity by a simple yet effective method.We decompose the problem into two interlaced stages, where the 3D flows are optimized point-wisely at the first stage and then globally regularized in a recurrent network at the second stage. Therefore, the recurrent network only receives the regular point-wise information as the input. In the experiments, we evaluate the proposed method on both the 3D scene flow estimation and the point cloud registration task. For 3D scene flow estimation, we make comparisons on the widely used FlyingThings3D and KITTIdatasets. For point cloud registration, we follow previous works and evaluate the data pairs with large pose and partially overlapping from ModelNet40. The results show that our method outperforms the previous method and achieves a new state-of-the-art performance on both 3D scene flow estimation and point cloud registration, which demonstrates the superiority of the proposed zero-order method on irregular point cloud data.



### Body Composition Estimation Based on Multimodal Multi-task Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2205.11031v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11031v1)
- **Published**: 2022-05-23 04:31:06+00:00
- **Updated**: 2022-05-23 04:31:06+00:00
- **Authors**: Subas Chhatkuli, Iris Jiang, Kyohei Kamiyama
- **Comment**: 11 pages, 8 figures, 1 table
- **Journal**: None
- **Summary**: In addition to body weight and Body Mass Index (BMI), body composition is an essential data point that allows people to understand their overall health and body fitness. However, body composition is largely made up of muscle, fat, bones, and water, which makes estimation not as easy and straightforward as measuring body weight. In this paper, we introduce a multimodal multi-task deep neural network to estimate body fat percentage and skeletal muscle mass by analyzing facial images in addition to a person's height, gender, age, and weight information. Using a dataset representative of demographics in Japan, we confirmed that the proposed approach performed better compared to the existing methods. Moreover, the multi-task approach implemented in this study is also able to grasp the negative correlation between body fat percentage and skeletal muscle mass gain/loss.



### Keypoint-Based Category-Level Object Pose Tracking from an RGB Sequence with Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/2205.11047v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.11047v1)
- **Published**: 2022-05-23 05:20:22+00:00
- **Updated**: 2022-05-23 05:20:22+00:00
- **Authors**: Yunzhi Lin, Jonathan Tremblay, Stephen Tyree, Patricio A. Vela, Stan Birchfield
- **Comment**: ICRA 2022. Project site is at
  https://sites.google.com/view/centerposetrack
- **Journal**: None
- **Summary**: We propose a single-stage, category-level 6-DoF pose estimation algorithm that simultaneously detects and tracks instances of objects within a known category. Our method takes as input the previous and current frame from a monocular RGB video, as well as predictions from the previous frame, to predict the bounding cuboid and 6-DoF pose (up to scale). Internally, a deep network predicts distributions over object keypoints (vertices of the bounding cuboid) in image coordinates, after which a novel probabilistic filtering process integrates across estimates before computing the final pose using PnP. Our framework allows the system to take previous uncertainties into consideration when predicting the current frame, resulting in predictions that are more accurate and stable than single frame methods. Extensive experiments show that our method outperforms existing approaches on the challenging Objectron benchmark of annotated object videos. We also demonstrate the usability of our work in an augmented reality setting.



### Vegetation Mapping by UAV Visible Imagery and Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.11061v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML, I.2; I.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2205.11061v1)
- **Published**: 2022-05-23 05:59:25+00:00
- **Updated**: 2022-05-23 05:59:25+00:00
- **Authors**: Giuliano Vitali
- **Comment**: 16 pages, numberedlines
- **Journal**: None
- **Summary**: An experimental field cropped with sugar-beet with a wide spreading of weeds has been used to test vegetation identification from drone visible imagery. Expert masked and hue-filtered pictures have been used to train several Machine Learning algorithms to develop a semi-automatic methodology for identification and mapping species at high resolution. Results show that 5m altitude allows for obtaining maps with an identification efficiency of more than 90%. Such a method can be easily integrated to present VRHA, as much as tools to obtain detailed maps of vegetation.



### Saliency-Driven Active Contour Model for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.11063v1
- **DOI**: 10.1109/ACCESS.2017.DOI
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11063v1)
- **Published**: 2022-05-23 06:02:52+00:00
- **Updated**: 2022-05-23 06:02:52+00:00
- **Authors**: Ehtesham Iqbal, Asim Niaz, Asif Aziz Memon, Usman Asim, Kwang Nam Choi
- **Comment**: None
- **Journal**: IEEE Access 2020
- **Summary**: Active contour models have achieved prominent success in the area of image segmentation, allowing complex objects to be segmented from the background for further analysis. Existing models can be divided into region-based active contour models and edge-based active contour models. However, both models use direct image data to achieve segmentation and face many challenging problems in terms of the initial contour position, noise sensitivity, local minima and inefficiency owing to the in-homogeneity of image intensities. The saliency map of an image changes the image representation, making it more visual and meaningful. In this study, we propose a novel model that uses the advantages of a saliency map with local image information (LIF) and overcomes the drawbacks of previous models. The proposed model is driven by a saliency map of an image and the local image information to enhance the progress of the active contour models. In this model, the saliency map of an image is first computed to find the saliency driven local fitting energy. Then, the saliency-driven local fitting energy is combined with the LIF model, resulting in a final novel energy functional. This final energy functional is formulated through a level set formulation, and regulation terms are added to evolve the contour more precisely across the object boundaries. The quality of the proposed method was verified on different synthetic images, real images and publicly available datasets, including medical images. The image segmentation results, and quantitative comparisons confirmed the contour initialization independence, noise insensitivity, and superior segmentation accuracy of the proposed model in comparison to the other segmentation models.



### Self-distilled Knowledge Delegator for Exemplar-free Class Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.11071v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11071v1)
- **Published**: 2022-05-23 06:31:13+00:00
- **Updated**: 2022-05-23 06:31:13+00:00
- **Authors**: Fanfan Ye, Liang Ma, Qiaoyong Zhong, Di Xie, Shiliang Pu
- **Comment**: Accepted by IJCNN 2022
- **Journal**: None
- **Summary**: Exemplar-free incremental learning is extremely challenging due to inaccessibility of data from old tasks. In this paper, we attempt to exploit the knowledge encoded in a previously trained classification model to handle the catastrophic forgetting problem in continual learning. Specifically, we introduce a so-called knowledge delegator, which is capable of transferring knowledge from the trained model to a randomly re-initialized new model by generating informative samples. Given the previous model only, the delegator is effectively learned using a self-distillation mechanism in a data-free manner. The knowledge extracted by the delegator is then utilized to maintain the performance of the model on old tasks in incremental learning. This simple incremental learning framework surpasses existing exemplar-free methods by a large margin on four widely used class incremental benchmarks, namely CIFAR-100, ImageNet-Subset, Caltech-101 and Flowers-102. Notably, we achieve comparable performance to some exemplar-based methods without accessing any exemplars.



### Deep Digging into the Generalization of Self-Supervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2205.11083v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.11083v3)
- **Published**: 2022-05-23 06:56:25+00:00
- **Updated**: 2023-03-20 03:52:42+00:00
- **Authors**: Jinwoo Bae, Sungho Moon, Sunghoon Im
- **Comment**: Accepted to AAAI 2023
- **Journal**: None
- **Summary**: Self-supervised monocular depth estimation has been widely studied recently. Most of the work has focused on improving performance on benchmark datasets, such as KITTI, but has offered a few experiments on generalization performance. In this paper, we investigate the backbone networks (e.g. CNNs, Transformers, and CNN-Transformer hybrid models) toward the generalization of monocular depth estimation. We first evaluate state-of-the-art models on diverse public datasets, which have never been seen during the network training. Next, we investigate the effects of texture-biased and shape-biased representations using the various texture-shifted datasets that we generated. We observe that Transformers exhibit a strong shape bias and CNNs do a strong texture-bias. We also find that shape-biased models show better generalization performance for monocular depth estimation compared to texture-biased models. Based on these observations, we newly design a CNN-Transformer hybrid network with a multi-level adaptive feature fusion module, called MonoFormer. The design intuition behind MonoFormer is to increase shape bias by employing Transformers while compensating for the weak locality bias of Transformers by adaptively fusing multi-level representations. Extensive experiments show that the proposed method achieves state-of-the-art performance with various public datasets. Our method also shows the best generalization ability among the competitive methods.



### FaceMAE: Privacy-Preserving Face Recognition via Masked Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2205.11090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11090v1)
- **Published**: 2022-05-23 07:19:42+00:00
- **Updated**: 2022-05-23 07:19:42+00:00
- **Authors**: Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Jiankang Deng, Xinchao Wang, Hakan Bilen, Yang You
- **Comment**: A new paradigm for privacy-preserving face recognition via MAE
- **Journal**: None
- **Summary**: Face recognition, as one of the most successful applications in artificial intelligence, has been widely used in security, administration, advertising, and healthcare. However, the privacy issues of public face datasets have attracted increasing attention in recent years. Previous works simply mask most areas of faces or synthesize samples using generative models to construct privacy-preserving face datasets, which overlooks the trade-off between privacy protection and data utility. In this paper, we propose a novel framework FaceMAE, where the face privacy and recognition performance are considered simultaneously. Firstly, randomly masked face images are used to train the reconstruction module in FaceMAE. We tailor the instance relation matching (IRM) module to minimize the distribution gap between real faces and FaceMAE reconstructed ones. During the deployment phase, we use trained FaceMAE to reconstruct images from masked faces of unseen identities without extra training. The risk of privacy leakage is measured based on face retrieval between reconstructed and original datasets. Experiments prove that the identities of reconstructed images are difficult to be retrieved. We also perform sufficient privacy-preserving face recognition on several public face datasets (i.e. CASIA-WebFace and WebFace260M). Compared to previous state of the arts, FaceMAE consistently \textbf{reduces at least 50\% error rate} on LFW, CFP-FP and AgeDB.



### FedNorm: Modality-Based Normalization in Federated Learning for Multi-Modal Liver Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.11096v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11096v1)
- **Published**: 2022-05-23 07:34:34+00:00
- **Updated**: 2022-05-23 07:34:34+00:00
- **Authors**: Tobias Bernecker, Annette Peters, Christopher L. Schlett, Fabian Bamberg, Fabian Theis, Daniel Rueckert, Jakob Weiß, Shadi Albarqouni
- **Comment**: Under Review
- **Journal**: None
- **Summary**: Given the high incidence and effective treatment options for liver diseases, they are of great socioeconomic importance. One of the most common methods for analyzing CT and MRI images for diagnosis and follow-up treatment is liver segmentation. Recent advances in deep learning have demonstrated encouraging results for automatic liver segmentation. Despite this, their success depends primarily on the availability of an annotated database, which is often not available because of privacy concerns. Federated Learning has been recently proposed as a solution to alleviate these challenges by training a shared global model on distributed clients without access to their local databases. Nevertheless, Federated Learning does not perform well when it is trained on a high degree of heterogeneity of image data due to multi-modal imaging, such as CT and MRI, and multiple scanner types. To this end, we propose Fednorm and its extension \fednormp, two Federated Learning algorithms that use a modality-based normalization technique. Specifically, Fednorm normalizes the features on a client-level, while Fednorm+ employs the modality information of single slices in the feature normalization. Our methods were validated using 428 patients from six publicly available databases and compared to state-of-the-art Federated Learning algorithms and baseline models in heterogeneous settings (multi-institutional, multi-modal data). The experimental results demonstrate that our methods show an overall acceptable performance, achieve Dice per patient scores up to 0.961, consistently outperform locally trained models, and are on par or slightly better than centralized models.



### PointDistiller: Structured Knowledge Distillation Towards Efficient and Compact 3D Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.11098v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11098v1)
- **Published**: 2022-05-23 07:40:07+00:00
- **Updated**: 2022-05-23 07:40:07+00:00
- **Authors**: Linfeng Zhang, Runpei Dong, Hung-Shuo Tai, Kaisheng Ma
- **Comment**: None
- **Journal**: None
- **Summary**: The remarkable breakthroughs in point cloud representation learning have boosted their usage in real-world applications such as self-driving cars and virtual reality. However, these applications usually have an urgent requirement for not only accurate but also efficient 3D object detection. Recently, knowledge distillation has been proposed as an effective model compression technique, which transfers the knowledge from an over-parameterized teacher to a lightweight student and achieves consistent effectiveness in 2D vision. However, due to point clouds' sparsity and irregularity, directly applying previous image-based knowledge distillation methods to point cloud detectors usually leads to unsatisfactory performance. To fill the gap, this paper proposes PointDistiller, a structured knowledge distillation framework for point clouds-based 3D detection. Concretely, PointDistiller includes local distillation which extracts and distills the local geometric structure of point clouds with dynamic graph convolution and reweighted learning strategy, which highlights student learning on the crucial points or voxels to improve knowledge distillation efficiency. Extensive experiments on both voxels-based and raw points-based detectors have demonstrated the effectiveness of our method over seven previous knowledge distillation methods. For instance, our 4X compressed PointPillars student achieves 2.8 and 3.4 mAP improvements on BEV and 3D object detection, outperforming its teacher by 0.9 and 1.8 mAP, respectively. Codes have been released at https://github.com/RunpeiDong/PointDistiller.



### Supporting Vision-Language Model Inference with Causality-pruning Knowledge Prompt
- **Arxiv ID**: http://arxiv.org/abs/2205.11100v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11100v1)
- **Published**: 2022-05-23 07:51:15+00:00
- **Updated**: 2022-05-23 07:51:15+00:00
- **Authors**: Jiangmeng Li, Wenyi Mo, Wenwen Qiang, Bing Su, Changwen Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-language models are pre-trained by aligning image-text pairs in a common space so that the models can deal with open-set visual concepts by learning semantic information from textual labels. To boost the transferability of these models on downstream tasks in a zero-shot manner, recent works explore generating fixed or learnable prompts, i.e., classification weights are synthesized from natural language describing task-relevant categories, to reduce the gap between tasks in the training and test phases. However, how and what prompts can improve inference performance remains unclear. In this paper, we explicitly provide exploration and clarify the importance of including semantic information in prompts, while existing prompt methods generate prompts without exploring the semantic information of textual labels. A challenging issue is that manually constructing prompts, with rich semantic information, requires domain expertise and is extremely time-consuming. To this end, we propose Causality-pruning Knowledge Prompt (CapKP) for adapting pre-trained vision-language models to downstream image recognition. CapKP retrieves an ontological knowledge graph by treating the textual label as a query to explore task-relevant semantic information. To further refine the derived semantic information, CapKP introduces causality-pruning by following the first principle of Granger causality. Empirically, we conduct extensive evaluations to demonstrate the effectiveness of CapKP, e.g., with 8 shots, CapKP outperforms the manual-prompt method by 12.51% and the learnable-prompt method by 1.39% on average, respectively. Experimental analyses prove the superiority of CapKP in domain generalization compared to benchmark approaches.



### Paddy Doctor: A Visual Image Dataset for Automated Paddy Disease Classification and Benchmarking
- **Arxiv ID**: http://arxiv.org/abs/2205.11108v2
- **DOI**: 10.1145/3570991.3570994
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11108v2)
- **Published**: 2022-05-23 07:57:40+00:00
- **Updated**: 2022-11-25 11:23:28+00:00
- **Authors**: Petchiammal A, Briskline Kiruba S, D. Murugan, Pandarasamy A
- **Comment**: None
- **Journal**: None
- **Summary**: One of the critical biotic stress factors paddy farmers face is diseases caused by bacteria, fungi, and other organisms. These diseases affect plants' health severely and lead to significant crop loss. Most of these diseases can be identified by regularly observing the leaves and stems under expert supervision. In a country with vast agricultural regions and limited crop protection experts, manual identification of paddy diseases is challenging. Thus, to add a solution to this problem, it is necessary to automate the disease identification process and provide easily accessible decision support tools to enable effective crop protection measures. However, the lack of availability of public datasets with detailed disease information limits the practical implementation of accurate disease detection systems. This paper presents \emph{Paddy Doctor}, a visual image dataset for identifying paddy diseases. Our dataset contains 16,225 annotated paddy leaf images across 13 classes (12 diseases and normal leaf). We benchmarked the \emph{Paddy Doctor} dataset using a Convolutional Neural Network (CNN) and four transfer learning based models (VGG16, MobileNet, Xception, and ResNet34). The experimental results showed that ResNet34 achieved the highest F1-score of 97.50%. We release our dataset and reproducible code in the open source for community use.



### Gradient Hedging for Intensively Exploring Salient Interpretation beyond Neuron Activation
- **Arxiv ID**: http://arxiv.org/abs/2205.11109v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.11109v1)
- **Published**: 2022-05-23 07:57:42+00:00
- **Updated**: 2022-05-23 07:57:42+00:00
- **Authors**: Woo-Jeoung Nam, Seong-Whan Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Hedging is a strategy for reducing the potential risks in various types of investments by adopting an opposite position in a related asset. Motivated by the equity technique, we introduce a method for decomposing output predictions into intensive salient attributions by hedging the evidence for a decision. We analyze the conventional approach applied to the evidence for a decision and discuss the paradox of the conservation rule. Subsequently, we define the viewpoint of evidence as a gap of positive and negative influence among the gradient-derived initial contribution maps and propagate the antagonistic elements to the evidence as suppressors, following the criterion of the degree of positive attribution defined by user preference. In addition, we reflect the severance or sparseness contribution of inactivated neurons, which are mostly irrelevant to a decision, resulting in increased robustness to interpretability. We conduct the following assessments in a verified experimental environment: pointing game, most relevant first region insertion, outside-inside relevance ratio, and mean average precision on the PASCAL VOC 2007, MS COCO 2014, and ImageNet datasets. The results demonstrate that our method outperforms existing attribution methods in distinctive, intensive, and intuitive visualization with robustness and applicability in general models.



### Meta-Learning Regrasping Strategies for Physical-Agnostic Objects
- **Arxiv ID**: http://arxiv.org/abs/2205.11110v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11110v1)
- **Published**: 2022-05-23 07:58:50+00:00
- **Updated**: 2022-05-23 07:58:50+00:00
- **Authors**: Ruijie Chen, Ning Gao, Ngo Anh Vien, Hanna Ziesche, Gerhard Neumann
- **Comment**: Accepted as spotlight in ICRA 2022 Workshop: Scaling Robot Learning
- **Journal**: None
- **Summary**: Grasping inhomogeneous objects, practical use in real-world applications, remains a challenging task due to the unknown physical properties such as mass distribution and coefficient of friction. In this study, we propose a vision-based meta-learning algorithm to learn physical properties in an agnostic way. In particular, we employ Conditional Neural Processes (CNPs) on top of DexNet-2.0. CNPs learn physical embeddings rapidly from a few observations where each observation is composed of i) the cropped depth image, ii) the grasping height between the gripper and estimated grasping point, and iii) the binary grasping result. Our modified conditional DexNet-2.0 (DexNet-CNP) updates the predicted grasping quality iteratively from new observations, which can be executed in an online fashion. We evaluate our method in the Pybullet simulator using various shape primitive objects with different physical parameters. The results show that our model outperforms the original DexNet-2.0 and is able to generalize on unseen objects with different shapes.



### DTU-Net: Learning Topological Similarity for Curvilinear Structure Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.11115v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.11115v2)
- **Published**: 2022-05-23 08:15:26+00:00
- **Updated**: 2023-03-04 15:48:48+00:00
- **Authors**: Manxi Lin, Zahra Bashir, Martin Grønnebæk Tolsgaard, Anders Nymark Christensen, Aasa Feragen
- **Comment**: 12 pages, 4 figures
- **Journal**: None
- **Summary**: Curvilinear structure segmentation is important in medical imaging, quantifying structures such as vessels, airways, neurons, or organ boundaries in 2D slices. Segmentation via pixel-wise classification often fails to capture the small and low-contrast curvilinear structures. Prior topological information is typically used to address this problem, often at an expensive computational cost, and sometimes requiring prior knowledge of the expected topology.   We present DTU-Net, a data-driven approach to topology-preserving curvilinear structure segmentation. DTU-Net consists of two sequential, lightweight U-Nets, dedicated to texture and topology, respectively. While the texture net makes a coarse prediction using image texture information, the topology net learns topological information from the coarse prediction by employing a triplet loss trained to recognize false and missed splits in the structure. We conduct experiments on a challenging multi-class ultrasound scan segmentation dataset as well as a well-known retinal imaging dataset. Results show that our model outperforms existing approaches in both pixel-wise segmentation accuracy and topological continuity, with no need for prior topological knowledge.



### ConvPoseCNN2: Prediction and Refinement of Dense 6D Object Poses
- **Arxiv ID**: http://arxiv.org/abs/2205.11124v1
- **DOI**: 10.1007/978-3-030-94893-1_16
  10.1007/978-3-030-94893-1_1610.1007/978-3-030-94893-1_16
  10.1007/978-3-030-94893-1_16
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11124v1)
- **Published**: 2022-05-23 08:32:09+00:00
- **Updated**: 2022-05-23 08:32:09+00:00
- **Authors**: Arul Selvam Periyasamy, Catherine Capellen, Max Schwarz, Sven Behnke
- **Comment**: None
- **Journal**: Communications in Computer and Information Science (CCIS), vol.
  1474, pp. 353-371, Springer, 2022
- **Summary**: Object pose estimation is a key perceptual capability in robotics. We propose a fully-convolutional extension of the PoseCNN method, which densely predicts object translations and orientations. This has several advantages such as improving the spatial resolution of the orientation predictions -- useful in highly-cluttered arrangements, significant reduction in parameters by avoiding full connectivity, and fast inference. We propose and discuss several aggregation methods for dense orientation predictions that can be applied as a post-processing step, such as averaging and clustering techniques. We demonstrate that our method achieves the same accuracy as PoseCNN on the challenging YCB-Video dataset and provide a detailed ablation study of several variants of our method. Finally, we demonstrate that the model can be further improved by inserting an iterative refinement module into the middle of the network, which enforces consistency of the prediction.



### KRNet: Towards Efficient Knowledge Replay
- **Arxiv ID**: http://arxiv.org/abs/2205.11126v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.11126v1)
- **Published**: 2022-05-23 08:34:17+00:00
- **Updated**: 2022-05-23 08:34:17+00:00
- **Authors**: Yingying Zhang, Qiaoyong Zhong, Di Xie, Shiliang Pu
- **Comment**: Accepted by ICPR 2022
- **Journal**: None
- **Summary**: The knowledge replay technique has been widely used in many tasks such as continual learning and continuous domain adaptation. The key lies in how to effectively encode the knowledge extracted from previous data and replay them during current training procedure. A simple yet effective model to achieve knowledge replay is autoencoder. However, the number of stored latent codes in autoencoder increases linearly with the scale of data and the trained encoder is redundant for the replaying stage. In this paper, we propose a novel and efficient knowledge recording network (KRNet) which directly maps an arbitrary sample identity number to the corresponding datum. Compared with autoencoder, our KRNet requires significantly ($400\times$) less storage cost for the latent codes and can be trained without the encoder sub-network. Extensive experiments validate the efficiency of KRNet, and as a showcase, it is successfully applied in the task of continual learning.



### Heterogeneous Semantic Transfer for Multi-label Recognition with Partial Labels
- **Arxiv ID**: http://arxiv.org/abs/2205.11131v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11131v2)
- **Published**: 2022-05-23 08:37:38+00:00
- **Updated**: 2023-05-17 11:02:55+00:00
- **Authors**: Tianshui Chen, Tao Pu, Lingbo Liu, Yukai Shi, Zhijing Yang, Liang Lin
- **Comment**: Technical Report. arXiv admin note: text overlap with
  arXiv:2112.10941
- **Journal**: None
- **Summary**: Multi-label image recognition with partial labels (MLR-PL), in which some labels are known while others are unknown for each image, may greatly reduce the cost of annotation and thus facilitate large-scale MLR. We find that strong semantic correlations exist within each image and across different images, and these correlations can help transfer the knowledge possessed by the known labels to retrieve the unknown labels and thus improve the performance of the MLR-PL task (see Figure 1). In this work, we propose a novel heterogeneous semantic transfer (HST) framework that consists of two complementary transfer modules that explore both within-image and cross-image semantic correlations to transfer the knowledge possessed by known labels to generate pseudo labels for the unknown labels. Specifically, an intra-image semantic transfer (IST) module learns an image-specific label co-occurrence matrix for each image and maps the known labels to complement the unknown labels based on these matrices. Additionally, a cross-image transfer (CST) module learns category-specific feature-prototype similarities and then helps complement the unknown labels that have high degrees of similarity with the corresponding prototypes. Finally, both the known and generated pseudo labels are used to train MLR models. Extensive experiments conducted on the Microsoft COCO, Visual Genome, and Pascal VOC 2007 datasets show that the proposed HST framework achieves superior performance to that of current state-of-the-art algorithms. Specifically, it obtains mean average precision (mAP) improvements of 1.4%, 3.3%, and 0.4% on the three datasets over the results of the best-performing previously developed algorithm.



### OPQ: Compressing Deep Neural Networks with One-shot Pruning-Quantization
- **Arxiv ID**: http://arxiv.org/abs/2205.11141v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11141v1)
- **Published**: 2022-05-23 09:05:25+00:00
- **Updated**: 2022-05-23 09:05:25+00:00
- **Authors**: Peng Hu, Xi Peng, Hongyuan Zhu, Mohamed M. Sabry Aly, Jie Lin
- **Comment**: Accepted in AAAI2021 and Just upload to retrieve Arxiv DOI for
  Project Record
- **Journal**: None
- **Summary**: As Deep Neural Networks (DNNs) usually are overparameterized and have millions of weight parameters, it is challenging to deploy these large DNN models on resource-constrained hardware platforms, e.g., smartphones. Numerous network compression methods such as pruning and quantization are proposed to reduce the model size significantly, of which the key is to find suitable compression allocation (e.g., pruning sparsity and quantization codebook) of each layer. Existing solutions obtain the compression allocation in an iterative/manual fashion while finetuning the compressed model, thus suffering from the efficiency issue. Different from the prior art, we propose a novel One-shot Pruning-Quantization (OPQ) in this paper, which analytically solves the compression allocation with pre-trained weight parameters only. During finetuning, the compression module is fixed and only weight parameters are updated. To our knowledge, OPQ is the first work that reveals pre-trained model is sufficient for solving pruning and quantization simultaneously, without any complex iterative/manual optimization at the finetuning stage. Furthermore, we propose a unified channel-wise quantization method that enforces all channels of each layer to share a common codebook, which leads to low bit-rate allocation without introducing extra overhead brought by traditional channel-wise quantization. Comprehensive experiments on ImageNet with AlexNet/MobileNet-V1/ResNet-50 show that our method improves accuracy and training efficiency while obtains significantly higher compression rates compared to the state-of-the-art.



### Stability of the scattering transform for deformations with minimal regularity
- **Arxiv ID**: http://arxiv.org/abs/2205.11142v1
- **DOI**: None
- **Categories**: **math.FA**, cs.CV, 94A12, 42C40, 42C15, 42B35, 68T07, 68T05
- **Links**: [PDF](http://arxiv.org/pdf/2205.11142v1)
- **Published**: 2022-05-23 09:08:21+00:00
- **Updated**: 2022-05-23 09:08:21+00:00
- **Authors**: Fabio Nicola, S. Ivan Trapasso
- **Comment**: 28 pages, 1 figure
- **Journal**: None
- **Summary**: Within the mathematical analysis of deep convolutional neural networks, the wavelet scattering transform introduced by St\'ephane Mallat is a unique example of how the ideas of multiscale analysis can be combined with a cascade of modulus nonlinearities to build a nonexpansive, translation invariant signal representation with provable geometric stability properties, namely Lipschitz continuity to the action of small $C^2$ diffeomorphisms - a remarkable result for both theoretical and practical purposes, inherently depending on the choice of the filters and their arrangement into a hierarchical architecture. In this note, we further investigate the intimate relationship between the scattering structure and the regularity of the deformation in the H\"older regularity scale $C^\alpha$, $\alpha >0$. We are able to precisely identify the stability threshold, proving that stability is still achievable for deformations of class $C^{\alpha}$, $\alpha>1$, whereas instability phenomena can occur at lower regularity levels modelled by $C^\alpha$, $0\le \alpha <1$. While the behaviour at the threshold given by Lipschitz (or even $C^1$) regularity remains beyond reach, we are able to prove a stability bound in that case, up to $\varepsilon$ losses.



### A Coupling Enhancement Algorithm for ZrO2 Ceramic Bearing Ball Surface Defect Detection Based on Cartoon-texture Decomposition Model and Multi-Scale Filtering Method
- **Arxiv ID**: http://arxiv.org/abs/2205.11145v2
- **DOI**: None
- **Categories**: **physics.ins-det**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.11145v2)
- **Published**: 2022-05-23 09:11:36+00:00
- **Updated**: 2023-04-26 12:21:59+00:00
- **Authors**: Wei Wang, Xin Zhang, Jiaqi Yi, Xianqi Liao, Wenjie Li, Zhenhong Li
- **Comment**: In the follow-up study, it was found that the research data needed to
  be improved
- **Journal**: None
- **Summary**: This study aimed to improve the surface defect detection accuracy of ZrO2 ceramic bearing balls. Combined with the noise damage of the image samples, a surface defect detection method for ZrO2 ceramic bearing balls based on cartoon-texture decomposition model was proposed. Building a ZrO2 ceramic bearing ball surface defect detection system. The ZrO2 ceramic bearing ball surface defect image was decomposed by using the Gaussian curvature model and the decomposed image layer was filtered by using Winner filter and wavelet value domain filter. Then they were fused into a clear and undamaged ZrO2 ceramic bearing ball surface defect image and detected. The experimental results show that the image denoising method of ZrO2 ceramic bearing ball surface defect based on cartoon-texture decomposition model can denoise while retaining the image details. The PSNR of image is 34.1 dB, the SSIM is 0.9476, the detection accuracy is 95.8%, and the detection speed of a single defect image is 191ms / img. This method can effectively improve the efficiency and accuracy of ZrO2 ceramic bearing ball surface defect detection.



### Squeeze Training for Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2205.11156v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.11156v2)
- **Published**: 2022-05-23 09:41:41+00:00
- **Updated**: 2023-02-10 08:25:31+00:00
- **Authors**: Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen
- **Comment**: Accepted by ICLR 2023
- **Journal**: None
- **Summary**: The vulnerability of deep neural networks (DNNs) to adversarial examples has attracted great attention in the machine learning community. The problem is related to non-flatness and non-smoothness of normally obtained loss landscapes. Training augmented with adversarial examples (a.k.a., adversarial training) is considered as an effective remedy. In this paper, we highlight that some collaborative examples, nearly perceptually indistinguishable from both adversarial and benign examples yet show extremely lower prediction loss, can be utilized to enhance adversarial training. A novel method is therefore proposed to achieve new state-of-the-arts in adversarial robustness. Code: https://github.com/qizhangli/ST-AT.



### PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models
- **Arxiv ID**: http://arxiv.org/abs/2205.11169v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2205.11169v2)
- **Published**: 2022-05-23 10:17:53+00:00
- **Updated**: 2022-11-22 06:59:30+00:00
- **Authors**: Yuan Yao, Qianyu Chen, Ao Zhang, Wei Ji, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun
- **Comment**: Accepted by EMNLP 2022
- **Journal**: None
- **Summary**: Vision-language pre-training (VLP) has shown impressive performance on a wide range of cross-modal tasks, where VLP models without reliance on object detectors are becoming the mainstream due to their superior computation efficiency and competitive performance. However, the removal of object detectors also deprives the capability of VLP models in explicit object modeling, which is essential to various position-sensitive vision-language (VL) tasks, such as referring expression comprehension and visual commonsense reasoning. To address the challenge, we introduce PEVL that enhances the pre-training and prompt tuning of VLP models with explicit object position modeling. Specifically, PEVL reformulates discretized object positions and language in a unified language modeling framework, which facilitates explicit VL alignment during pre-training, and also enables flexible prompt tuning for various downstream tasks. We show that PEVL enables state-of-the-art performance of detector-free VLP models on position-sensitive tasks such as referring expression comprehension and phrase grounding, and also improves the performance on position-insensitive tasks with grounded inputs. We make the data and code for this paper publicly available at https://github.com/thunlp/PEVL.



### Online Hybrid Lightweight Representations Learning: Its Application to Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2205.11179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11179v1)
- **Published**: 2022-05-23 10:31:14+00:00
- **Updated**: 2022-05-23 10:31:14+00:00
- **Authors**: Ilchae Jung, Minji Kim, Eunhyeok Park, Bohyung Han
- **Comment**: 7 pages, 1 figure, accepted at IJCAI2022
- **Journal**: None
- **Summary**: This paper presents a novel hybrid representation learning framework for streaming data, where an image frame in a video is modeled by an ensemble of two distinct deep neural networks; one is a low-bit quantized network and the other is a lightweight full-precision network. The former learns coarse primary information with low cost while the latter conveys residual information for high fidelity to original representations. The proposed parallel architecture is effective to maintain complementary information since fixed-point arithmetic can be utilized in the quantized network and the lightweight model provides precise representations given by a compact channel-pruned network. We incorporate the hybrid representation technique into an online visual tracking task, where deep neural networks need to handle temporal variations of target appearances in real-time. Compared to the state-of-the-art real-time trackers based on conventional deep neural networks, our tracking algorithm demonstrates competitive accuracy on the standard benchmarks with a small fraction of computational cost and memory footprint.



### NPU-BOLT: A Dataset for Bolt Object Detection in Natural Scene Images
- **Arxiv ID**: http://arxiv.org/abs/2205.11191v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.11191v2)
- **Published**: 2022-05-23 10:51:33+00:00
- **Updated**: 2022-05-25 15:08:51+00:00
- **Authors**: Yadian Zhao, Zhenglin Yang, Chao Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Bolt joints are very common and important in engineering structures. Due to extreme service environment and load factors, bolts often get loose or even disengaged. To real-time or timely detect the loosed or disengaged bolts is an urgent need in practical engineering, which is critical to keep structural safety and service life. In recent years, many bolt loosening detection methods using deep learning and machine learning techniques have been proposed and are attracting more and more attention. However, most of these studies use bolt images captured in laboratory for deep leaning model training. The images are obtained in a well-controlled light, distance, and view angle conditions. Also, the bolted structures are well designed experimental structures with brand new bolts and the bolts are exposed without any shelter nearby. It is noted that in practical engineering, the above well controlled lab conditions are not easy realized and the real bolt images often have blur edges, oblique perspective, partial occlusion and indistinguishable colors etc., which make the trained models obtained in laboratory conditions loss their accuracy or fails. Therefore, the aim of this study is to develop a dataset named NPU-BOLT for bolt object detection in natural scene images and open it to researchers for public use and further development. In the first version of the dataset, it contains 337 samples of bolt joints images mainly in the natural environment, with image data sizes ranging from 400*400 to 6000*4000, totaling approximately 1275 bolt targets. The bolt targets are annotated into four categories named blur bolt, bolt head, bolt nut and bolt side. The dataset is tested with advanced object detection models including yolov5, Faster-RCNN and CenterNet. The effectiveness of the dataset is validated.



### Active Domain Adaptation with Multi-level Contrastive Units for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.11192v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11192v2)
- **Published**: 2022-05-23 10:55:39+00:00
- **Updated**: 2022-05-25 15:11:28+00:00
- **Authors**: Hao Zhang, Ruimao Zhang, Zhanglin Peng, Junle Wang, Yanqing Jing
- **Comment**: None
- **Journal**: None
- **Summary**: To further reduce the cost of semi-supervised domain adaptation (SSDA) labeling, a more effective way is to use active learning (AL) to annotate a selected subset with specific properties. However, domain adaptation tasks are always addressed in two interactive aspects: domain transfer and the enhancement of discrimination, which requires the selected data to be both uncertain under the model and diverse in feature space. Contrary to active learning in classification tasks, it is usually challenging to select pixels that contain both the above properties in segmentation tasks, leading to the complex design of pixel selection strategy. To address such an issue, we propose a novel Active Domain Adaptation scheme with Multi-level Contrastive Units (ADA-MCU) for semantic image segmentation. A simple pixel selection strategy followed with the construction of multi-level contrastive units is introduced to optimize the model for both domain adaptation and active supervised learning. In practice, MCUs are constructed from intra-image, cross-image, and cross-domain levels by using both labeled and unlabeled pixels. At each level, we define contrastive losses from center-to-center and pixel-to-pixel manners, with the aim of jointly aligning the category centers and reducing outliers near the decision boundaries. In addition, we also introduce a categories correlation matrix to implicitly describe the relationship between categories, which are used to adjust the weights of the losses for MCUs. Extensive experimental results on standard benchmarks show that the proposed method achieves competitive performance against state-of-the-art SSDA methods with 50% fewer labeled pixels and significantly outperforms state-of-the-art with a large margin by using the same level of annotation cost.



### Deep Image Retrieval is not Robust to Label Noise
- **Arxiv ID**: http://arxiv.org/abs/2205.11195v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11195v1)
- **Published**: 2022-05-23 11:04:09+00:00
- **Updated**: 2022-05-23 11:04:09+00:00
- **Authors**: Stanislav Dereka, Ivan Karpukhin, Sergey Kolesnikov
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale datasets are essential for the success of deep learning in image retrieval. However, manual assessment errors and semi-supervised annotation techniques can lead to label noise even in popular datasets. As previous works primarily studied annotation quality in image classification tasks, it is still unclear how label noise affects deep learning approaches to image retrieval. In this work, we show that image retrieval methods are less robust to label noise than image classification ones. Furthermore, we, for the first time, investigate different types of label noise specific to image retrieval tasks and study their effect on model performance.



### Feature-Distribution Perturbation and Calibration for Generalized Person ReID
- **Arxiv ID**: http://arxiv.org/abs/2205.11197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11197v1)
- **Published**: 2022-05-23 11:06:12+00:00
- **Updated**: 2022-05-23 11:06:12+00:00
- **Authors**: Qilei Li, Jiabo Huang, Jian Hu, Shaogang Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Person Re-identification (ReID) has been advanced remarkably over the last 10 years along with the rapid development of deep learning for visual recognition. However, the i.i.d. (independent and identically distributed) assumption commonly held in most deep learning models is somewhat non-applicable to ReID considering its objective to identify images of the same pedestrian across cameras at different locations often of variable and independent domain characteristics that are also subject to view-biased data distribution. In this work, we propose a Feature-Distribution Perturbation and Calibration (PECA) method to derive generic feature representations for person ReID, which is not only discriminative across cameras but also agnostic and deployable to arbitrary unseen target domains. Specifically, we perform per-domain feature-distribution perturbation to refrain the model from overfitting to the domain-biased distribution of each source (seen) domain by enforcing feature invariance to distribution shifts caused by perturbation. Furthermore, we design a global calibration mechanism to align feature distributions across all the source domains to improve the model generalization capacity by eliminating domain bias. These local perturbation and global calibration are conducted simultaneously, which share the same principle to avoid models overfitting by regularization respectively on the perturbed and the original distributions. Extensive experiments were conducted on eight person ReID datasets and the proposed PECA model outperformed the state-of-the-art competitors by significant margins.



### Denoising-based image reconstruction from pixels located at non-integer positions
- **Arxiv ID**: http://arxiv.org/abs/2205.11202v1
- **DOI**: 10.1109/ICIP.2015.7351671
- **Categories**: **cs.CV**, eess.IV, I.4.3; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2205.11202v1)
- **Published**: 2022-05-23 11:12:20+00:00
- **Updated**: 2022-05-23 11:12:20+00:00
- **Authors**: Ján Koloda, Jürgen Seiler, André Kaup
- **Comment**: arXiv admin note: text overlap with arXiv:2205.10138
- **Journal**: 2015 IEEE International Conference on Image Processing (ICIP),
  2015, pp. 4565-4569
- **Summary**: Digital images are commonly represented as regular 2D arrays, so pixels are organized in form of a matrix addressed by integers. However, there are many image processing operations, such as rotation or motion compensation, that produce pixels at non-integer positions. Typically, image reconstruction techniques cannot handle samples at non-integer positions. In this paper, we propose to use triangulation-based reconstruction as initial estimate that is later refined by a novel adaptive denoising framework. Simulations reveal that improvements of up to more than 1.8 dB (in terms of PSNR) are achieved with respect to the initial estimate.



### Scalable Kernel-Based Minimum Mean Square Error Estimator for Accelerated Image Error Concealment
- **Arxiv ID**: http://arxiv.org/abs/2205.11226v1
- **DOI**: 10.1109/TBC.2016.2619581
- **Categories**: **cs.CV**, I.4.3; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2205.11226v1)
- **Published**: 2022-05-23 12:15:24+00:00
- **Updated**: 2022-05-23 12:15:24+00:00
- **Authors**: Ján Koloda, Jürgen Seiler, Antonio M. Peinado, André Kaup
- **Comment**: None
- **Journal**: IEEE Transactions on Broadcasting, vol. 63, no. 1, pp. 59-70,
  March 2017
- **Summary**: Error concealment is of great importance for block-based video systems, such as DVB or video streaming services. In this paper, we propose a novel scalable spatial error concealment algorithm that aims at obtaining high quality reconstructions with reduced computational burden. The proposed technique exploits the excellent reconstructing abilities of the kernel-based minimum mean square error K-MMSE estimator. We propose to decompose this approach into a set of hierarchically stacked layers. The first layer performs the basic reconstruction that the subsequent layers can eventually refine. In addition, we design a layer management mechanism, based on profiles, that dynamically adapts the use of higher layers to the visual complexity of the area being reconstructed. The proposed technique outperforms other state-of-the-art algorithms and produces high quality reconstructions, equivalent to K-MMSE, while requiring around one tenth of its computational time.



### What You See is What You Classify: Black Box Attributions
- **Arxiv ID**: http://arxiv.org/abs/2205.11266v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11266v2)
- **Published**: 2022-05-23 12:30:04+00:00
- **Updated**: 2022-10-07 13:04:24+00:00
- **Authors**: Steven Stalder, Nathanaël Perraudin, Radhakrishna Achanta, Fernando Perez-Cruz, Michele Volpi
- **Comment**: None
- **Journal**: None
- **Summary**: An important step towards explaining deep image classifiers lies in the identification of image regions that contribute to individual class scores in the model's output. However, doing this accurately is a difficult task due to the black-box nature of such networks. Most existing approaches find such attributions either using activations and gradients or by repeatedly perturbing the input. We instead address this challenge by training a second deep network, the Explainer, to predict attributions for a pre-trained black-box classifier, the Explanandum. These attributions are provided in the form of masks that only show the classifier-relevant parts of an image, masking out the rest. Our approach produces sharper and more boundary-precise masks when compared to the saliency maps generated by other methods. Moreover, unlike most existing approaches, ours is capable of directly generating very distinct class-specific masks in a single forward pass. This makes the proposed method very efficient during inference. We show that our attributions are superior to established methods both visually and quantitatively with respect to the PASCAL VOC-2007 and Microsoft COCO-2014 datasets.



### Dynamic Split Computing for Efficient Deep Edge Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2205.11269v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11269v2)
- **Published**: 2022-05-23 12:35:18+00:00
- **Updated**: 2022-06-17 13:51:56+00:00
- **Authors**: Arian Bakhtiarnia, Nemanja Milošević, Qi Zhang, Dragana Bajović, Alexandros Iosifidis
- **Comment**: Accepted by the 2022 International Conference on Machine Learning
  (ICML 2022) DyNN Workshop
- **Journal**: None
- **Summary**: Deploying deep neural networks (DNNs) on IoT and mobile devices is a challenging task due to their limited computational resources. Thus, demanding tasks are often entirely offloaded to edge servers which can accelerate inference, however, it also causes communication cost and evokes privacy concerns. In addition, this approach leaves the computational capacity of end devices unused. Split computing is a paradigm where a DNN is split into two sections; the first section is executed on the end device, and the output is transmitted to the edge server where the final section is executed. Here, we introduce dynamic split computing, where the optimal split location is dynamically selected based on the state of the communication channel. By using natural bottlenecks that already exist in modern DNN architectures, dynamic split computing avoids retraining and hyperparameter optimization, and does not have any negative impact on the final accuracy of DNNs. Through extensive experiments, we show that dynamic split computing achieves faster inference in edge computing environments where the data rate and server load vary over time.



### GR-GAN: Gradual Refinement Text-to-image Generation
- **Arxiv ID**: http://arxiv.org/abs/2205.11273v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.11273v2)
- **Published**: 2022-05-23 12:42:04+00:00
- **Updated**: 2022-06-21 16:31:57+00:00
- **Authors**: Bo Yang, Fangxiang Feng, Xiaojie Wang
- **Comment**: Accepted by ICME 2022
- **Journal**: None
- **Summary**: A good Text-to-Image model should not only generate high quality images, but also ensure the consistency between the text and the generated image. Previous models failed to simultaneously fix both sides well. This paper proposes a Gradual Refinement Generative Adversarial Network (GR-GAN) to alleviates the problem efficiently. A GRG module is designed to generate images from low resolution to high resolution with the corresponding text constraints from coarse granularity (sentence) to fine granularity (word) stage by stage, a ITM module is designed to provide image-text matching losses at both sentence-image level and word-region level for corresponding stages. We also introduce a new metric Cross-Model Distance (CMD) for simultaneously evaluating image quality and image-text consistency. Experimental results show GR-GAN significant outperform previous models, and achieve new state-of-the-art on both FID and CMD. A detailed analysis demonstrates the efficiency of different generation stages in GR-GAN.



### SelfReformer: Self-Refined Network with Transformer for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.11283v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11283v4)
- **Published**: 2022-05-23 13:10:10+00:00
- **Updated**: 2022-07-18 05:18:36+00:00
- **Authors**: Yi Ke Yun, Weisi Lin
- **Comment**: None
- **Journal**: None
- **Summary**: The global and local contexts significantly contribute to the integrity of predictions in Salient Object Detection (SOD). Unfortunately, existing methods still struggle to generate complete predictions with fine details. There are two major problems in conventional approaches: first, for global context, high-level CNN-based encoder features cannot effectively catch long-range dependencies, resulting in incomplete predictions. Second, downsampling the ground truth to fit the size of predictions will introduce inaccuracy as the ground truth details are lost during interpolation or pooling. Thus, in this work, we developed a Transformer-based network and framed a supervised task for a branch to learn the global context information explicitly. Besides, we adopt Pixel Shuffle from Super-Resolution (SR) to reshape the predictions back to the size of ground truth instead of the reverse. Thus details in the ground truth are untouched. In addition, we developed a two-stage Context Refinement Module (CRM) to fuse global context and automatically locate and refine the local details in the predictions. The proposed network can guide and correct itself based on the global and local context generated, thus is named, Self-Refined Transformer (SelfReformer). Extensive experiments and evaluation results on five benchmark datasets demonstrate the outstanding performance of the network, and we achieved the state-of-the-art.



### Training Efficient CNNS: Tweaking the Nuts and Bolts of Neural Networks for Lighter, Faster and Robust Models
- **Arxiv ID**: http://arxiv.org/abs/2205.12050v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.12050v1)
- **Published**: 2022-05-23 13:51:06+00:00
- **Updated**: 2022-05-23 13:51:06+00:00
- **Authors**: Sabeesh Ethiraj, Bharath Kumar Bolla
- **Comment**: Accepted at Machine Learning Developers Summit-2022, Bangalore, India
- **Journal**: None
- **Summary**: Deep Learning has revolutionized the fields of computer vision, natural language understanding, speech recognition, information retrieval and more. Many techniques have evolved over the past decade that made models lighter, faster, and robust with better generalization. However, many deep learning practitioners persist with pre-trained models and architectures trained mostly on standard datasets such as Imagenet, MS-COCO, IMDB-Wiki Dataset, and Kinetics-700 and are either hesitant or unaware of redesigning the architecture from scratch that will lead to better performance. This scenario leads to inefficient models that are not suitable on various devices such as mobile, edge, and fog. In addition, these conventional training methods are of concern as they consume a lot of computing power. In this paper, we revisit various SOTA techniques that deal with architecture efficiency (Global Average Pooling, depth-wise convolutions & squeeze and excitation, Blurpool), learning rate (Cyclical Learning Rate), data augmentation (Mixup, Cutout), label manipulation (label smoothing), weight space manipulation (stochastic weight averaging), and optimizer (sharpness aware minimization). We demonstrate how an efficient deep convolution network can be built in a phased manner by sequentially reducing the number of training parameters and using the techniques mentioned above. We achieved a SOTA accuracy of 99.2% on MNIST data with just 1500 parameters and an accuracy of 86.01% with just over 140K parameters on the CIFAR-10 dataset.



### Continual Barlow Twins: continual self-supervised learning for remote sensing semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.11319v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11319v2)
- **Published**: 2022-05-23 14:02:12+00:00
- **Updated**: 2023-01-09 15:16:50+00:00
- **Authors**: Valerio Marsocci, Simone Scardapane
- **Comment**: None
- **Journal**: None
- **Summary**: In the field of Earth Observation (EO), Continual Learning (CL) algorithms have been proposed to deal with large datasets by decomposing them into several subsets and processing them incrementally. The majority of these algorithms assume that data is (a) coming from a single source, and (b) fully labeled. Real-world EO datasets are instead characterized by a large heterogeneity (e.g., coming from aerial, satellite, or drone scenarios), and for the most part they are unlabeled, meaning they can be fully exploited only through the emerging Self-Supervised Learning (SSL) paradigm. For these reasons, in this paper we propose a new algorithm for merging SSL and CL for remote sensing applications, that we call Continual Barlow Twins (CBT). It combines the advantages of one of the simplest self-supervision techniques, i.e., Barlow Twins, with the Elastic Weight Consolidation method to avoid catastrophic forgetting. In addition, for the first time we evaluate SSL methods on a highly heterogeneous EO dataset, showing the effectiveness of these strategies on a novel combination of three almost non-overlapping domains datasets (airborne Potsdam dataset, satellite US3D dataset, and drone UAVid dataset), on a crucial downstream task in EO, i.e., semantic segmentation. Encouraging results show the superiority of SSL in this setting, and the effectiveness of creating an incremental effective pretrained feature extractor, based on ResNet50, without the need of relying on the complete availability of all the data, with a valuable saving of time and resources.



### Towards automatic detection of wildlife trade using machine vision models
- **Arxiv ID**: http://arxiv.org/abs/2205.11324v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11324v1)
- **Published**: 2022-05-23 14:11:16+00:00
- **Updated**: 2022-05-23 14:11:16+00:00
- **Authors**: Ritwik Kulkarni, Enrico Di Minin
- **Comment**: None
- **Journal**: None
- **Summary**: Unsustainable trade in wildlife is one of the major threats affecting the global biodiversity crisis. An important part of the trade now occurs on the internet, especially on digital marketplaces and social media. Automated methods to identify trade posts are needed as resources for conservation are limited. Here, we developed machine vision models based on Deep Neural Networks with the aim to automatically identify images of exotic pet animals for sale. A new training dataset representing exotic pet animals advertised for sale on the web was generated for this purpose. We trained 24 neural-net models spanning a combination of five different architectures, three methods of training and two types of datasets. Specifically, model generalisation improved after setting a portion of the training images to represent negative features. Models were evaluated on both within and out of distribution data to test wider model applicability. The top performing models achieved an f-score of over 0.95 on within distribution evaluation and between 0.75 to 0.87 on the two out of distribution datasets. Notably, feature visualisation indicated that models performed well in detecting the surrounding context (e.g. a cage) in which an animal was located, therefore helping to automatically detect images of animals in non-natural environments. The proposed methods can help investigate the online wildlife trade, but can also be adapted to study other types of people-nature interactions from digital platforms. Future studies can use these findings to build robust machine learning models and new data collection pipelines for more taxonomic groups.



### Towards Deeper Understanding of Camouflaged Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.11333v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11333v2)
- **Published**: 2022-05-23 14:26:18+00:00
- **Updated**: 2023-01-03 03:49:35+00:00
- **Authors**: Yunqiu Lv, Jing Zhang, Yuchao Dai, Aixuan Li, Nick Barnes, Deng-Ping Fan
- **Comment**: IEEE Transactions on Circuits and Systems for Video Technology 2023
- **Journal**: None
- **Summary**: Preys in the wild evolve to be camouflaged to avoid being recognized by predators. In this way, camouflage acts as a key defence mechanism across species that is critical to survival. To detect and segment the whole scope of a camouflaged object, camouflaged object detection (COD) is introduced as a binary segmentation task, with the binary ground truth camouflage map indicating the exact regions of the camouflaged objects. In this paper, we revisit this task and argue that the binary segmentation setting fails to fully understand the concept of camouflage. We find that explicitly modeling the conspicuousness of camouflaged objects against their particular backgrounds can not only lead to a better understanding about camouflage, but also provide guidance to designing more sophisticated camouflage techniques. Furthermore, we observe that it is some specific parts of camouflaged objects that make them detectable by predators. With the above understanding about camouflaged objects, we present the first triple-task learning framework to simultaneously localize, segment, and rank camouflaged objects, indicating the conspicuousness level of camouflage. As no corresponding datasets exist for either the localization model or the ranking model, we generate localization maps with an eye tracker, which are then processed according to the instance level labels to generate our ranking-based training and testing dataset. We also contribute the largest COD testing set to comprehensively analyse performance of the COD models. Experimental results show that our triple-task learning framework achieves new state-of-the-art, leading to a more explainable COD network. Our code, data, and results are available at: \url{https://github.com/JingZhang617/COD-Rank-Localize-and-Segment}.



### Spatial Attention-based Implicit Neural Representation for Arbitrary Reduction of MRI Slice Spacing
- **Arxiv ID**: http://arxiv.org/abs/2205.11346v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.11346v2)
- **Published**: 2022-05-23 14:36:59+00:00
- **Updated**: 2023-03-20 01:59:51+00:00
- **Authors**: Xin Wang, Sheng Wang, Honglin Xiong, Kai Xuan, Zixu Zhuang, Mengjun Liu, Zhenrong Shen, Xiangyu Zhao, Lichi Zhang, Qian Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic resonance (MR) images collected in 2D clinical protocols typically have large inter-slice spacing, resulting in high in-plane resolution and reduced through-plane resolution. Super-resolution technique can enhance the through-plane resolution of MR images to facilitate downstream visualization and computer-aided diagnosis. However, most existing works train the super-resolution network at a fixed scaling factor, which is not friendly to clinical scenes of varying inter-slice spacing in MR scanning. Inspired by the recent progress in implicit neural representation, we propose a Spatial Attention-based Implicit Neural Representation (SA-INR) network for arbitrary reduction of MR inter-slice spacing. The SA-INR aims to represent an MR image as a continuous implicit function of 3D coordinates. In this way, the SA-INR can reconstruct the MR image with arbitrary inter-slice spacing by continuously sampling the coordinates in 3D space. In particular, a local-aware spatial attention operation is introduced to model nearby voxels and their affinity more accurately in a larger receptive field. Meanwhile, to improve the computational efficiency, a gradient-guided gating mask is proposed for applying the local-aware spatial attention to selected areas only. We evaluate our method on the public HCP-1200 dataset and the clinical knee MR dataset to demonstrate its superiority over other existing methods.



### Markedness in Visual Semantic AI
- **Arxiv ID**: http://arxiv.org/abs/2205.11378v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11378v1)
- **Published**: 2022-05-23 15:14:41+00:00
- **Updated**: 2022-05-23 15:14:41+00:00
- **Authors**: Robert Wolfe, Aylin Caliskan
- **Comment**: To be published at ACM FAccT 2022
- **Journal**: None
- **Summary**: We evaluate the state-of-the-art multimodal "visual semantic" model CLIP ("Contrastive Language Image Pretraining") for biases related to the marking of age, gender, and race or ethnicity. Given the option to label an image as "a photo of a person" or to select a label denoting race or ethnicity, CLIP chooses the "person" label 47.9% of the time for White individuals, compared with 5.0% or less for individuals who are Black, East Asian, Southeast Asian, Indian, or Latino or Hispanic. The model is more likely to rank the unmarked "person" label higher than labels denoting gender for Male individuals (26.7% of the time) vs. Female individuals (15.2% of the time). Age affects whether an individual is marked by the model: Female individuals under the age of 20 are more likely than Male individuals to be marked with a gender label, but less likely to be marked with an age label, while Female individuals over the age of 40 are more likely to be marked based on age than Male individuals. We also examine the self-similarity (mean pairwise cosine similarity) for each social group, where higher self-similarity denotes greater attention directed by CLIP to the shared characteristics (age, race, or gender) of the social group. As age increases, the self-similarity of representations of Female individuals increases at a higher rate than for Male individuals, with the disparity most pronounced at the "more than 70" age range. All ten of the most self-similar social groups are individuals under the age of 10 or over the age of 70, and six of the ten are Female individuals. Existing biases of self-similarity and markedness between Male and Female gender groups are further exacerbated when the groups compared are individuals who are White and Male and individuals who are Black and Female. Results indicate that CLIP reflects the biases of the language and society which produced its training data.



### Detection of Fights in Videos: A Comparison Study of Anomaly Detection and Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.11394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11394v1)
- **Published**: 2022-05-23 15:41:02+00:00
- **Updated**: 2022-05-23 15:41:02+00:00
- **Authors**: Weijun Tan, Jingfeng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Detection of fights is an important surveillance application in videos. Most existing methods use supervised binary action recognition. Since frame-level annotations are very hard to get for anomaly detection, weakly supervised learning using multiple instance learning is widely used. This paper explores the detection of fights in videos as one special type of anomaly detection and as binary action recognition. We use the UBI-Fight and NTU-CCTV-Fight datasets for most of the study since they have frame-level annotations. We find that the anomaly detection has similar or even better performance than the action recognition. Furthermore, we study to use anomaly detection as a toolbox to generate training datasets for action recognition in an iterative way conditioned on the performance of the anomaly detection. Experiment results should show that we achieve state-of-the-art performance on three fight detection datasets.



### Multi-Temporal Spatial-Spectral Comparison Network for Hyperspectral Anomalous Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.11395v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11395v1)
- **Published**: 2022-05-23 15:41:27+00:00
- **Updated**: 2022-05-23 15:41:27+00:00
- **Authors**: Meiqi Hu, Chen Wu, Bo Du
- **Comment**: 4pages; 5 figure; IGARSS2022
- **Journal**: None
- **Summary**: Hyperspectral anomalous change detection has been a challenging task for its emphasis on the dynamics of small and rare objects against the prevalent changes. In this paper, we have proposed a Multi-Temporal spatial-spectral Comparison Network for hyperspectral anomalous change detection (MTC-NET). The whole model is a deep siamese network, aiming at learning the prevalent spectral difference resulting from the complex imaging conditions from the hyperspectral images by contrastive learning. A three-dimensional spatial spectral attention module is designed to effectively extract the spatial semantic information and the key spectral differences. Then the gaps between the multi-temporal features are minimized, boosting the alignment of the semantic and spectral features and the suppression of the multi-temporal background spectral difference. The experiments on the "Viareggio 2013" datasets demonstrate the effectiveness of proposed MTC-NET.



### Super Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2205.11397v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11397v5)
- **Published**: 2022-05-23 15:42:12+00:00
- **Updated**: 2023-07-19 08:25:37+00:00
- **Authors**: Mingbao Lin, Mengzhao Chen, Yuxin Zhang, Chunhua Shen, Rongrong Ji, Liujuan Cao
- **Comment**: Accepted by International Journal of Computer Vision (IJCV) in the
  year of 2023
- **Journal**: None
- **Summary**: We attempt to reduce the computational costs in vision transformers (ViTs), which increase quadratically in the token number. We present a novel training paradigm that trains only one ViT model at a time, but is capable of providing improved image recognition performance with various computational costs. Here, the trained ViT model, termed super vision transformer (SuperViT), is empowered with the versatile ability to solve incoming patches of multiple sizes as well as preserve informative tokens with multiple keeping rates (the ratio of keeping tokens) to achieve good hardware efficiency for inference, given that the available hardware resources often change from time to time. Experimental results on ImageNet demonstrate that our SuperViT can considerably reduce the computational costs of ViT models with even performance increase. For example, we reduce 2x FLOPs of DeiT-S while increasing the Top-1 accuracy by 0.2% and 0.7% for 1.5x reduction. Also, our SuperViT significantly outperforms existing studies on efficient vision transformers. For example, when consuming the same amount of FLOPs, our SuperViT surpasses the recent state-of-the-art (SOTA) EViT by 1.1% when using DeiT-S as their backbones. The project of this work is made publicly available at https://github.com/lmbxmu/SuperViT.



### Fine-Grained Counting with Crowd-Sourced Supervision
- **Arxiv ID**: http://arxiv.org/abs/2205.11398v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11398v2)
- **Published**: 2022-05-23 15:42:18+00:00
- **Updated**: 2022-05-30 02:10:06+00:00
- **Authors**: Justin Kay, Catherine M. Foley, Tom Hart
- **Comment**: In Computer Vision for Animal Behavior Tracking and Modeling Workshop
  at CVPR 2022. 4 pages, 3 figures
- **Journal**: None
- **Summary**: Crowd-sourcing is an increasingly popular tool for image analysis in animal ecology. Computer vision methods that can utilize crowd-sourced annotations can help scale up analysis further. In this work we study the potential to do so on the challenging task of fine-grained counting. As opposed to the standard crowd counting task, fine-grained counting also involves classifying attributes of individuals in dense crowds. We introduce a new dataset from animal ecology to enable this study that contains 1.7M crowd-sourced annotations of 8 fine-grained classes. It is the largest available dataset for fine-grained counting and the first to enable the study of the task with crowd-sourced annotations. We introduce methods for generating aggregate "ground truths" from the collected annotations, as well as a counting method that can utilize the aggregate information. Our method improves results by 8% over a comparable baseline, indicating the potential for algorithms to learn fine-grained counting using crowd-sourced supervision.



### Enhanced Prototypical Learning for Unsupervised Domain Adaptation in LiDAR Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.11419v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11419v1)
- **Published**: 2022-05-23 16:04:22+00:00
- **Updated**: 2022-05-23 16:04:22+00:00
- **Authors**: Eojindl Yi, Juyoung Yang, Junmo Kim
- **Comment**: accepted to IEEE International Conference on Robotics and Automation
  (ICRA2022) (7 pages, 1 figure, 4 tables)
- **Journal**: None
- **Summary**: Despite its importance, unsupervised domain adaptation (UDA) on LiDAR semantic segmentation is a task that has not received much attention from the research community. Only recently, a completion-based 3D method has been proposed to tackle the problem and formally set up the adaptive scenarios. However, the proposed pipeline is complex, voxel-based and requires multi-stage inference, which inhibits it for real-time inference. We propose a range image-based, effective and efficient method for solving UDA on LiDAR segmentation. The method exploits class prototypes from the source domain to pseudo label target domain pixels, which is a research direction showing good performance in UDA for natural image semantic segmentation. Applying such approaches to LiDAR scans has not been considered because of the severe domain shift and lack of pre-trained feature extractor that is unavailable in the LiDAR segmentation setup. However, we show that proper strategies, including reconstruction-based pre-training, enhanced prototypes, and selective pseudo labeling based on distance to prototypes, is sufficient enough to enable the use of prototypical approaches. We evaluate the performance of our method on the recently proposed LiDAR segmentation UDA scenarios. Our method achieves remarkable performance among contemporary methods.



### LILA-BOTI : Leveraging Isolated Letter Accumulations By Ordering Teacher Insights for Bangla Handwriting Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.11420v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.11420v1)
- **Published**: 2022-05-23 16:04:25+00:00
- **Updated**: 2022-05-23 16:04:25+00:00
- **Authors**: Md. Ismail Hossain, Mohammed Rakib, Sabbir Mollah, Fuad Rahman, Nabeel Mohammed
- **Comment**: Accepted in ICPR2022
- **Journal**: None
- **Summary**: Word-level handwritten optical character recognition (OCR) remains a challenge for morphologically rich languages like Bangla. The complexity arises from the existence of a large number of alphabets, the presence of several diacritic forms, and the appearance of complex conjuncts. The difficulty is exacerbated by the fact that some graphemes occur infrequently but remain indispensable, so addressing the class imbalance is required for satisfactory results. This paper addresses this issue by introducing two knowledge distillation methods: Leveraging Isolated Letter Accumulations By Ordering Teacher Insights (LILA-BOTI) and Super Teacher LILA-BOTI. In both cases, a Convolutional Recurrent Neural Network (CRNN) student model is trained with the dark knowledge gained from a printed isolated character recognition teacher model. We conducted inter-dataset testing on \emph{BN-HTRd} and \emph{BanglaWriting} as our evaluation protocol, thus setting up a challenging problem where the results would better reflect the performance on unseen data. Our evaluations achieved up to a 3.5% increase in the F1-Macro score for the minor classes and up to 4.5% increase in our overall word recognition rate when compared with the base model (No KD) and conventional KD.



### Decoder Denoising Pretraining for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.11423v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6; I.5.4; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2205.11423v1)
- **Published**: 2022-05-23 16:08:31+00:00
- **Updated**: 2022-05-23 16:08:31+00:00
- **Authors**: Emmanuel Brempong Asiedu, Simon Kornblith, Ting Chen, Niki Parmar, Matthias Minderer, Mohammad Norouzi
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation labels are expensive and time consuming to acquire. Hence, pretraining is commonly used to improve the label-efficiency of segmentation models. Typically, the encoder of a segmentation model is pretrained as a classifier and the decoder is randomly initialized. Here, we argue that random initialization of the decoder can be suboptimal, especially when few labeled examples are available. We propose a decoder pretraining approach based on denoising, which can be combined with supervised pretraining of the encoder. We find that decoder denoising pretraining on the ImageNet dataset strongly outperforms encoder-only supervised pretraining. Despite its simplicity, decoder denoising pretraining achieves state-of-the-art results on label-efficient semantic segmentation and offers considerable gains on the Cityscapes, Pascal Context, and ADE20K datasets.



### SiSPRNet: End-to-End Learning for Single-Shot Phase Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2205.11434v2
- **DOI**: 10.1364/OE.464086
- **Categories**: **cs.CV**, cs.LG, eess.IV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2205.11434v2)
- **Published**: 2022-05-23 16:24:52+00:00
- **Updated**: 2022-07-24 05:50:38+00:00
- **Authors**: Qiuliang Ye, Li-Wen Wang, Daniel P. K. Lun
- **Comment**: None
- **Journal**: None
- **Summary**: With the success of deep learning methods in many image processing tasks, deep learning approaches have also been introduced to the phase retrieval problem recently. These approaches are different from the traditional iterative optimization methods in that they usually require only one intensity measurement and can reconstruct phase images in real-time. However, because of tremendous domain discrepancy, the quality of the reconstructed images given by these approaches still has much room to improve to meet the general application requirements. In this paper, we design a novel deep neural network structure named SiSPRNet for phase retrieval based on a single Fourier intensity measurement. To effectively utilize the spectral information of the measurements, we propose a new feature extraction unit using the Multi-Layer Perceptron (MLP) as the front end. It allows all pixels of the input intensity image to be considered together for exploring their global representation. The size of the MLP is carefully designed to facilitate the extraction of the representative features while reducing noises and outliers. A dropout layer is also equipped to mitigate the possible overfitting problem in training the MLP. To promote the global correlation in the reconstructed images, a self-attention mechanism is introduced to the Up-sampling and Reconstruction (UR) blocks of the proposed SiSPRNet. These UR blocks are inserted into a residual learning structure to prevent the weak information flow and vanishing gradient problems due to their complex layer structure. Extensive evaluations of the proposed model are performed using different testing datasets of phase-only images and images with linearly related magnitude and phase. Experiments were conducted on an optical experimentation platform to understand the performance of different deep learning methods when working in a practical environment.



### Graph-theoretical approach to robust 3D normal extraction of LiDAR data
- **Arxiv ID**: http://arxiv.org/abs/2205.11460v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11460v1)
- **Published**: 2022-05-23 16:54:49+00:00
- **Updated**: 2022-05-23 16:54:49+00:00
- **Authors**: Arpan Kusari, Wenbo Sun
- **Comment**: Published at ISPRS Annals of Photogrammetry and Remote Sensing
- **Journal**: None
- **Summary**: Low dimensional primitive feature extraction from LiDAR point clouds (such as planes) forms the basis of majority of LiDAR data processing tasks. A major challenge in LiDAR data analysis arises from the irregular nature of LiDAR data that forces practitioners to either regularize the data using some form of gridding or utilize a triangular mesh such as triangulated irregular network (TIN). While there have been a handful applications using LiDAR data as a connected graph, a principled treatment of utilizing graph-theoretical approach for LiDAR data modelling is still lacking. In this paper, we try to bridge this gap by utilizing graphical approach for normal estimation from LiDAR point clouds. We formulate the normal estimation problem in an optimization framework, where we find the corresponding normal vector for each LiDAR point by utilizing its nearest neighbors and simultaneously enforcing a graph smoothness assumption based on point samples. This is a non-linear constrained convex optimization problem which can then be solved using projected conjugate gradient descent to yield an unique solution. As an enhancement to our optimization problem, we also provide different weighted solutions based on the dot product of the normals and Euclidean distance between the points. In order to assess the performance of our proposed normal extraction method and weighting strategies, we first provide a detailed analysis on repeated randomly generated datasets with four different noise levels and four different tuning parameters. Finally, we benchmark our proposed method against existing state-of-the-art approaches on a large scale synthetic plane extraction dataset. The code for the proposed approach along with the simulations and benchmarking is available at https://github.com/arpan-kusari/graph-plane-extraction-simulation.



### Exposing Outlier Exposure: What Can Be Learned From Few, One, and Zero Outlier Images
- **Arxiv ID**: http://arxiv.org/abs/2205.11474v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2205.11474v2)
- **Published**: 2022-05-23 17:23:15+00:00
- **Updated**: 2022-11-14 15:27:47+00:00
- **Authors**: Philipp Liznerski, Lukas Ruff, Robert A. Vandermeulen, Billy Joe Franks, Klaus-Robert Müller, Marius Kloft
- **Comment**: 47 pages; extended experiments; published in Transactions on Machine
  Learning Research. arXiv admin note: substantial text overlap with
  arXiv:2006.00339
- **Journal**: None
- **Summary**: Due to the intractability of characterizing everything that looks unlike the normal data, anomaly detection (AD) is traditionally treated as an unsupervised problem utilizing only normal samples. However, it has recently been found that unsupervised image AD can be drastically improved through the utilization of huge corpora of random images to represent anomalousness; a technique which is known as Outlier Exposure. In this paper we show that specialized AD learning methods seem unnecessary for state-of-the-art performance, and furthermore one can achieve strong performance with just a small collection of Outlier Exposure data, contradicting common assumptions in the field of AD. We find that standard classifiers and semi-supervised one-class methods trained to discern between normal samples and relatively few random natural images are able to outperform the current state of the art on an established AD benchmark with ImageNet. Further experiments reveal that even one well-chosen outlier sample is sufficient to achieve decent performance on this benchmark (79.3% AUC). We investigate this phenomenon and find that one-class methods are more robust to the choice of training outliers, indicating that there are scenarios where these are still more useful than standard classifiers. Additionally, we include experiments that delineate the scenarios where our results hold. Lastly, no training samples are necessary when one uses the representations learned by CLIP, a recent foundation model, which achieves state-of-the-art AD results on CIFAR-10 and ImageNet in a zero-shot setting.



### Novel Light Field Imaging Device with Enhanced Light Collection for Cold Atom Clouds
- **Arxiv ID**: http://arxiv.org/abs/2205.11480v1
- **DOI**: 10.1088/1748-0221/17/08/P08021
- **Categories**: **physics.ins-det**, cs.CV, physics.atom-ph, physics.optics, quant-ph
- **Links**: [PDF](http://arxiv.org/pdf/2205.11480v1)
- **Published**: 2022-05-23 17:32:44+00:00
- **Updated**: 2022-05-23 17:32:44+00:00
- **Authors**: Sanha Cheong, Josef C. Frisch, Sean Gasiorowski, Jason M. Hogan, Michael Kagan, Murtaza Safdari, Ariel Schwartzman, Maxime Vandegar
- **Comment**: None
- **Journal**: 2022 JINST 17 P08021
- **Summary**: We present a light field imaging system that captures multiple views of an object with a single shot. The system is designed to maximize the total light collection by accepting a larger solid angle of light than a conventional lens with equivalent depth of field. This is achieved by populating a plane of virtual objects using mirrors and fully utilizing the available field of view and depth of field. Simulation results demonstrate that this design is capable of single-shot tomography of objects of size $\mathcal{O}$(1 mm$^3$), reconstructing the 3-dimensional (3D) distribution and features not accessible from any single view angle in isolation. In particular, for atom clouds used in atom interferometry experiments, the system can reconstruct 3D fringe patterns with size $\mathcal{O}$(100 $\mu$m). We also demonstrate this system with a 3D-printed prototype. The prototype is used to take images of $\mathcal{O}$(1 mm$^{3}$) sized objects, and 3D reconstruction algorithms running on a single-shot image successfully reconstruct $\mathcal{O}$(100 $\mu$m) internal features. The prototype also shows that the system can be built with 3D printing technology and hence can be deployed quickly and cost-effectively in experiments with needs for enhanced light collection or 3D reconstruction. Imaging of cold atom clouds in atom interferometry is a key application of this new type of imaging device where enhanced light collection, high depth of field, and 3D tomographic reconstruction can provide new handles to characterize the atom clouds.



### Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding
- **Arxiv ID**: http://arxiv.org/abs/2205.11487v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11487v1)
- **Published**: 2022-05-23 17:42:53+00:00
- **Updated**: 2022-05-23 17:42:53+00:00
- **Authors**: Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi
- **Comment**: None
- **Journal**: None
- **Summary**: We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.



### Flexible Diffusion Modeling of Long Videos
- **Arxiv ID**: http://arxiv.org/abs/2205.11495v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11495v3)
- **Published**: 2022-05-23 17:51:48+00:00
- **Updated**: 2022-12-15 20:57:59+00:00
- **Authors**: William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, Frank Wood
- **Comment**: None
- **Journal**: None
- **Summary**: We present a framework for video modeling based on denoising diffusion probabilistic models that produces long-duration video completions in a variety of realistic environments. We introduce a generative model that can at test-time sample any arbitrary subset of video frames conditioned on any other subset and present an architecture adapted for this purpose. Doing so allows us to efficiently compare and optimize a variety of schedules for the order in which frames in a long video are sampled and use selective sparse and long-range conditioning on previously sampled frames. We demonstrate improved video modeling over prior work on a number of datasets and sample temporally coherent videos over 25 minutes in length. We additionally release a new video modeling dataset and semantically meaningful metrics based on videos generated in the CARLA autonomous driving simulator.



### VQA-GNN: Reasoning with Multimodal Semantic Graph for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2205.11501v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2205.11501v1)
- **Published**: 2022-05-23 17:55:34+00:00
- **Updated**: 2022-05-23 17:55:34+00:00
- **Authors**: Yanan Wang, Michihiro Yasunaga, Hongyu Ren, Shinya Wada, Jure Leskovec
- **Comment**: None
- **Journal**: None
- **Summary**: Visual understanding requires seamless integration between recognition and reasoning: beyond image-level recognition (e.g., detecting objects), systems must perform concept-level reasoning (e.g., inferring the context of objects and intents of people). However, existing methods only model the image-level features, and do not ground them and reason with background concepts such as knowledge graphs (KGs). In this work, we propose a novel visual question answering method, VQA-GNN, which unifies the image-level information and conceptual knowledge to perform joint reasoning of the scene. Specifically, given a question-image pair, we build a scene graph from the image, retrieve a relevant linguistic subgraph from ConceptNet and visual subgraph from VisualGenome, and unify these three graphs and the question into one joint graph, multimodal semantic graph. Our VQA-GNN then learns to aggregate messages and reason across different modalities captured by the multimodal semantic graph. In the evaluation on the VCR task, our method outperforms the previous scene graph-based Trans-VL models by over 4%, and VQA-GNN-Large, our model that fuses a Trans-VL further improves the state of the art by 2%, attaining the top of the VCR leaderboard at the time of submission. This result suggests the efficacy of our model in performing conceptual reasoning beyond image-level recognition for visual understanding. Finally, we demonstrate that our model is the first work to provide interpretability across visual and textual knowledge domains for the VQA task.



### Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering
- **Arxiv ID**: http://arxiv.org/abs/2205.11506v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.11506v2)
- **Published**: 2022-05-23 17:59:03+00:00
- **Updated**: 2022-06-11 16:46:23+00:00
- **Authors**: Ekdeep Singh Lubana, Chi Ian Tang, Fahim Kawsar, Robert P. Dick, Akhil Mathur
- **Comment**: Camera-ready ICML, 2022
- **Journal**: None
- **Summary**: Federated learning is generally used in tasks where labels are readily available (e.g., next word prediction). Relaxing this constraint requires design of unsupervised learning techniques that can support desirable properties for federated training: robustness to statistical/systems heterogeneity, scalability with number of participants, and communication efficiency. Prior work on this topic has focused on directly extending centralized self-supervised learning techniques, which are not designed to have the properties listed above. To address this situation, we propose Orchestra, a novel unsupervised federated learning technique that exploits the federation's hierarchy to orchestrate a distributed clustering task and enforce a globally consistent partitioning of clients' data into discriminable clusters. We show the algorithmic pipeline in Orchestra guarantees good generalization performance under a linear probe, allowing it to outperform alternative techniques in a broad range of conditions, including variation in heterogeneity, number of clients, participation ratio, and local epochs.



### Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods
- **Arxiv ID**: http://arxiv.org/abs/2205.11508v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2205.11508v3)
- **Published**: 2022-05-23 17:59:32+00:00
- **Updated**: 2022-06-10 17:26:36+00:00
- **Authors**: Randall Balestriero, Yann LeCun
- **Comment**: None
- **Journal**: None
- **Summary**: Self-Supervised Learning (SSL) surmises that inputs and pairwise positive relationships are enough to learn meaningful representations. Although SSL has recently reached a milestone: outperforming supervised methods in many modalities\dots the theoretical foundations are limited, method-specific, and fail to provide principled design guidelines to practitioners. In this paper, we propose a unifying framework under the helm of spectral manifold learning to address those limitations. Through the course of this study, we will rigorously demonstrate that VICReg, SimCLR, BarlowTwins et al. correspond to eponymous spectral methods such as Laplacian Eigenmaps, Multidimensional Scaling et al.   This unification will then allow us to obtain (i) the closed-form optimal representation for each method, (ii) the closed-form optimal network parameters in the linear regime for each method, (iii) the impact of the pairwise relations used during training on each of those quantities and on downstream task performances, and most importantly, (iv) the first theoretical bridge between contrastive and non-contrastive methods towards global and local spectral embedding methods respectively, hinting at the benefits and limitations of each. For example, (i) if the pairwise relation is aligned with the downstream task, any SSL method can be employed successfully and will recover the supervised method, but in the low data regime, VICReg's invariance hyper-parameter should be high; (ii) if the pairwise relation is misaligned with the downstream task, VICReg with small invariance hyper-parameter should be preferred over SimCLR or BarlowTwins.



### From Hours to Seconds: Towards 100x Faster Quantitative Phase Imaging via Differentiable Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2205.11521v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.comp-ph, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2205.11521v1)
- **Published**: 2022-05-23 17:59:58+00:00
- **Updated**: 2022-05-23 17:59:58+00:00
- **Authors**: Udith Haputhanthri, Kithmini Herath, Ramith Hettiarachchi, Hasindu Kariyawasam, Azeem Ahmad, Balpreet S. Ahluwalia, Chamira U. S. Edussooriya, Dushan N. Wadduwage
- **Comment**: None
- **Journal**: None
- **Summary**: With applications ranging from metabolomics to histopathology, quantitative phase microscopy (QPM) is a powerful label-free imaging modality. Despite significant advances in fast multiplexed imaging sensors and deep-learning-based inverse solvers, the throughput of QPM is currently limited by the speed of electronic hardware. Complementarily, to improve throughput further, here we propose to acquire images in a compressed form such that more information can be transferred beyond the existing electronic hardware bottleneck. To this end, we present a learnable optical compression-decompression framework that learns content-specific features. The proposed differentiable optical-electronic quantitative phase microscopy ($\partial \mu$) first uses learnable optical feature extractors as image compressors. The intensity representation produced by these networks is then captured by the imaging sensor. Finally, a reconstruction network running on electronic hardware decompresses the QPM images. The proposed system achieves compression of $\times$ 64 while maintaining the SSIM of $\sim 0.90$ and PSNR of $\sim 30$ dB. The promising results demonstrated by our experiments open up a new pathway for achieving end-to-end optimized (i.e., optics and electronic) compact QPM systems that provide unprecedented throughput improvements.



### Accelerating the creation of instance segmentation training sets through bounding box annotation
- **Arxiv ID**: http://arxiv.org/abs/2205.11563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11563v1)
- **Published**: 2022-05-23 18:37:03+00:00
- **Updated**: 2022-05-23 18:37:03+00:00
- **Authors**: Niels Sayez, Christophe De Vleeschouwer
- **Comment**: None
- **Journal**: None
- **Summary**: Collecting image annotations remains a significant burden when deploying CNN in a specific applicative context. This is especially the case when the annotation consists in binary masks covering object instances. Our work proposes to delineate instances in three steps, based on a semi-automatic approach: (1) the extreme points of an object (left-most, right-most, top, bottom pixels) are manually defined, thereby providing the object bounding-box, (2) a universal automatic segmentation tool like Deep Extreme Cut is used to turn the bounded object into a segmentation mask that matches the extreme points; and (3) the predicted mask is manually corrected. Various strategies are then investigated to balance the human manual annotation resources between bounding-box definition and mask correction, including when the correction of instance masks is prioritized based on their overlap with other instance bounding-boxes, or the outcome of an instance segmentation model trained on a partially annotated dataset. Our experimental study considers a teamsport player segmentation task, and measures how the accuracy of the Panoptic-Deeplab instance segmentation model depends on the human annotation resources allocation strategy. It reveals that the sole definition of extreme points results in a model accuracy that would require up to 10 times more resources if the masks were defined through fully manual delineation of instances. When targeting higher accuracies, prioritizing the mask correction among the training set instances is also shown to save up to 80\% of correction annotation resources compared to a systematic frame by frame correction of instances, for a same trained instance segmentation model accuracy.



### VPAIR -- Aerial Visual Place Recognition and Localization in Large-scale Outdoor Environments
- **Arxiv ID**: http://arxiv.org/abs/2205.11567v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11567v1)
- **Published**: 2022-05-23 18:50:08+00:00
- **Updated**: 2022-05-23 18:50:08+00:00
- **Authors**: Michael Schleiss, Fahmi Rouatbi, Daniel Cremers
- **Comment**: ICRA 2022 AERIAL ROBOTICS WORKSHOP
- **Journal**: None
- **Summary**: Visual Place Recognition and Visual Localization are essential components in navigation and mapping for autonomous vehicles especially in GNSS-denied navigation scenarios. Recent work has focused on ground or close to ground applications such as self-driving cars or indoor-scenarios and low-altitude drone flights. However, applications such as Urban Air Mobility require operations in large-scale outdoor environments at medium to high altitudes. We present a new dataset named VPAIR. The dataset was recorded on board a light aircraft flying at an altitude of more than 300 meters above ground capturing images with a downwardfacing camera. Each image is paired with a high resolution reference render including dense depth information and 6-DoF reference poses. The dataset covers a more than one hundred kilometers long trajectory over various types of challenging landscapes, e.g. urban, farmland and forests. Experiments on this dataset illustrate the challenges introduced by the change in perspective to a bird's eye view such as in-plane rotations.



### Discriminative Feature Learning through Feature Distance Loss
- **Arxiv ID**: http://arxiv.org/abs/2205.11606v3
- **DOI**: 10.1007/s00138-023-01379-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11606v3)
- **Published**: 2022-05-23 20:01:32+00:00
- **Updated**: 2023-01-19 16:08:46+00:00
- **Authors**: Tobias Schlagenhauf, Yiwen Lin, Benjamin Noack
- **Comment**: None
- **Journal**: None
- **Summary**: Ensembles of Convolutional neural networks have shown remarkable results in learning discriminative semantic features for image classification tasks. Though, the models in the ensemble often concentrate on similar regions in images. This work proposes a novel method that forces a set of base models to learn different features for a classification task. These models are combined in an ensemble to make a collective classification. The key finding is that by forcing the models to concentrate on different features, the classification accuracy is increased. To learn different feature concepts, a so-called feature distance loss is implemented on the feature maps. The experiments on benchmark convolutional neural networks (VGG16, ResNet, AlexNet), popular datasets (Cifar10, Cifar100, miniImageNet, NEU, BSD, TEX), and different training samples (3, 5, 10, 20, 50, 100 per class) show the effectiveness of the proposed feature loss. The proposed method outperforms classical ensemble versions of the base models. The Class Activation Maps explicitly prove the ability to learn different feature concepts. The code is available at: https://github.com/2Obe/Feature-Distance-Loss.git



### TransforMatcher: Match-to-Match Attention for Semantic Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2205.11634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11634v1)
- **Published**: 2022-05-23 21:02:01+00:00
- **Updated**: 2022-05-23 21:02:01+00:00
- **Authors**: Seungwook Kim, Juhong Min, Minsu Cho
- **Comment**: Accepted to CVPR 2022 (poster presentation)
- **Journal**: None
- **Summary**: Establishing correspondences between images remains a challenging task, especially under large appearance changes due to different viewpoints or intra-class variations. In this work, we introduce a strong semantic image matching learner, dubbed TransforMatcher, which builds on the success of transformer networks in vision domains. Unlike existing convolution- or attention-based schemes for correspondence, TransforMatcher performs global match-to-match attention for precise match localization and dynamic refinement. To handle a large number of matches in a dense correlation map, we develop a light-weight attention architecture to consider the global match-to-match interactions. We also propose to utilize a multi-channel correlation map for refinement, treating the multi-level scores as features instead of a single score to fully exploit the richer layer-wise semantics. In experiments, TransforMatcher sets a new state of the art on SPair-71k while performing on par with existing SOTA methods on the PF-PASCAL dataset.



### Towards Model Generalization for Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.11664v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11664v4)
- **Published**: 2022-05-23 23:05:07+00:00
- **Updated**: 2022-06-13 03:50:20+00:00
- **Authors**: Zhenyu Li, Zehui Chen, Ang Li, Liangji Fang, Qinhong Jiang, Xianming Liu, Junjun Jiang
- **Comment**: Fixed some mistakes
- **Journal**: None
- **Summary**: Monocular 3D object detection (Mono3D) has achieved tremendous improvements with emerging large-scale autonomous driving datasets and the rapid development of deep learning techniques. However, caused by severe domain gaps (e.g., the field of view (FOV), pixel size, and object size among datasets), Mono3D detectors have difficulty in generalization, leading to drastic performance degradation on unseen domains. To solve these issues, we combine the position-invariant transform and multi-scale training with the pixel-size depth strategy to construct an effective unified camera-generalized paradigm (CGP). It fully considers discrepancies in the FOV and pixel size of images captured by different cameras. Moreover, we further investigate the obstacle in quantitative metrics when cross-dataset inference through an exhaustive systematic study. We discern that the size bias of prediction leads to a colossal failure. Hence, we propose the 2D-3D geometry-consistent object scaling strategy (GCOS) to bridge the gap via an instance-level augment. Our method called DGMono3D achieves remarkable performance on all evaluated datasets and surpasses the SoTA unsupervised domain adaptation scheme even without utilizing data on the target domain.



### Algorithm Development for Controlling Movement of a Robotic Platform by Digital Image Processing
- **Arxiv ID**: http://arxiv.org/abs/2205.11666v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.11666v1)
- **Published**: 2022-05-23 23:14:24+00:00
- **Updated**: 2022-05-23 23:14:24+00:00
- **Authors**: Benjamin Andres Huerfano Zapata, Humberto Numpaque Lopez, Cindy Lorena Diaz Murillo
- **Comment**: 6 figures,5 pages, in Spanish language
- **Journal**: Journal Entre Ciencia e Ingenieria, vol 6, issue 12, pages 19-23
  (2012)
- **Summary**: The following work shows an algorithm that can process images digitally with the goal of control the movement of a mobile robotic platform in a certain environment. The platform is identified with a specific color, and displacement environment of the platform shift has identified obstacles with different colors, for both cases it worked with the RGB color scale. To obtain the control's movement of the robotic platform, the algorithm was developed in C programming language, and used the Open CV libraries for processing images captured by a video camera on the Dev-platform C + +. The video camera was previously calibrated using ZHANG technique where parameters were obtained focal length and tilt focal pixel. In the algorithm histogram analysis and segmentation of the image were developed, allowing to determine exactly the relative position of the platform with respect to the obstacles and movement strategy to follow.



