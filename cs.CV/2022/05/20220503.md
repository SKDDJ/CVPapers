# Arxiv Papers in cs.CV on 2022-05-03
### Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2205.01271v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01271v4)
- **Published**: 2022-05-03 02:08:04+00:00
- **Updated**: 2022-07-11 16:17:22+00:00
- **Authors**: Yihan Wang, Muyang Li, Han Cai, Wei-Ming Chen, Song Han
- **Comment**: 11 pages
- **Journal**: IEEE / CVF Computer Vision and Pattern Recognition Conference
  (CVPR) 2022
- **Summary**: Pose estimation plays a critical role in human-centered vision applications. However, it is difficult to deploy state-of-the-art HRNet-based pose estimation models on resource-constrained edge devices due to the high computational cost (more than 150 GMACs per frame). In this paper, we study efficient architecture design for real-time multi-person pose estimation on edge. We reveal that HRNet's high-resolution branches are redundant for models at the low-computation region via our gradual shrinking experiments. Removing them improves both efficiency and performance. Inspired by this finding, we design LitePose, an efficient single-branch architecture for pose estimation, and introduce two simple approaches to enhance the capacity of LitePose, including Fusion Deconv Head and Large Kernel Convs. Fusion Deconv Head removes the redundancy in high-resolution branches, allowing scale-aware feature fusion with low overhead. Large Kernel Convs significantly improve the model's capacity and receptive field while maintaining a low computational cost. With only 25% computation increment, 7x7 kernels achieve +14.0 mAP better than 3x3 kernels on the CrowdPose dataset. On mobile platforms, LitePose reduces the latency by up to 5.0x without sacrificing performance, compared with prior state-of-the-art efficient pose estimation models, pushing the frontier of real-time multi-person pose estimation on edge. Our code and pre-trained models are released at https://github.com/mit-han-lab/litepose.



### Physics to the Rescue: Deep Non-line-of-sight Reconstruction for High-speed Imaging
- **Arxiv ID**: http://arxiv.org/abs/2205.01679v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01679v2)
- **Published**: 2022-05-03 02:47:02+00:00
- **Updated**: 2022-08-06 02:15:09+00:00
- **Authors**: Fangzhou Mu, Sicheng Mo, Jiayong Peng, Xiaochun Liu, Ji Hyun Nam, Siddeshwar Raghavan, Andreas Velten, Yin Li
- **Comment**: ICCP 2022 (TPAMI Special Issue on Computational Photography). Project
  page: https://pages.cs.wisc.edu/~fmu/nlos3d/
- **Journal**: None
- **Summary**: Computational approach to imaging around the corner, or non-line-of-sight (NLOS) imaging, is becoming a reality thanks to major advances in imaging hardware and reconstruction algorithms. A recent development towards practical NLOS imaging, Nam et al. demonstrated a high-speed non-confocal imaging system that operates at 5Hz, 100x faster than the prior art. This enormous gain in acquisition rate, however, necessitates numerous approximations in light transport, breaking many existing NLOS reconstruction methods that assume an idealized image formation model. To bridge the gap, we present a novel deep model that incorporates the complementary physics priors of wave propagation and volume rendering into a neural network for high-quality and robust NLOS reconstruction. This orchestrated design regularizes the solution space by relaxing the image formation model, resulting in a deep model that generalizes well on real captures despite being exclusively trained on synthetic data. Further, we devise a unified learning framework that enables our model to be flexibly trained using diverse supervision signals, including target intensity images or even raw NLOS transient measurements. Once trained, our model renders both intensity and depth images at inference time in a single forward pass, capable of processing more than 5 captures per second on a high-end GPU. Through extensive qualitative and quantitative experiments, we show that our method outperforms prior physics and learning based approaches on both synthetic and real measurements. We anticipate that our method along with the fast capturing system will accelerate future development of NLOS imaging for real world applications that require high-speed imaging.



### Cross Domain Object Detection by Target-Perceived Dual Branch Distillation
- **Arxiv ID**: http://arxiv.org/abs/2205.01291v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01291v1)
- **Published**: 2022-05-03 03:51:32+00:00
- **Updated**: 2022-05-03 03:51:32+00:00
- **Authors**: Mengzhe He, Yali Wang, Jiaxi Wu, Yiru Wang, Hanqing Li, Bo Li, Weihao Gan, Wei Wu, Yu Qiao
- **Comment**: CVPR2022
- **Journal**: None
- **Summary**: Cross domain object detection is a realistic and challenging task in the wild. It suffers from performance degradation due to large shift of data distributions and lack of instance-level annotations in the target domain. Existing approaches mainly focus on either of these two difficulties, even though they are closely coupled in cross domain object detection. To solve this problem, we propose a novel Target-perceived Dual-branch Distillation (TDD) framework. By integrating detection branches of both source and target domains in a unified teacher-student learning scheme, it can reduce domain shift and generate reliable supervision effectively. In particular, we first introduce a distinct Target Proposal Perceiver between two domains. It can adaptively enhance source detector to perceive objects in a target image, by leveraging target proposal contexts from iterative cross-attention. Afterwards, we design a concise Dual Branch Self Distillation strategy for model training, which can progressively integrate complementary object knowledge from different domains via self-distillation in two branches. Finally, we conduct extensive experiments on a number of widely-used scenarios in cross domain object detection. The results show that our TDD significantly outperforms the state-of-the-art methods on all the benchmarks. Our code and model will be available at https://github.com/Feobi1999/TDD.



### RU-Net: Regularized Unrolling Network for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2205.01297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01297v1)
- **Published**: 2022-05-03 04:21:15+00:00
- **Updated**: 2022-05-03 04:21:15+00:00
- **Authors**: Xin Lin, Changxing Ding, Jing Zhang, Yibing Zhan, Dacheng Tao
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Scene graph generation (SGG) aims to detect objects and predict the relationships between each pair of objects. Existing SGG methods usually suffer from several issues, including 1) ambiguous object representations, as graph neural network-based message passing (GMP) modules are typically sensitive to spurious inter-node correlations, and 2) low diversity in relationship predictions due to severe class imbalance and a large number of missing annotations. To address both problems, in this paper, we propose a regularized unrolling network (RU-Net). We first study the relation between GMP and graph Laplacian denoising (GLD) from the perspective of the unrolling technique, determining that GMP can be formulated as a solver for GLD. Based on this observation, we propose an unrolled message passing module and introduce an $\ell_p$-based graph regularization to suppress spurious connections between nodes. Second, we propose a group diversity enhancement module that promotes the prediction diversity of relationships via rank maximization. Systematic experiments demonstrate that RU-Net is effective under a variety of settings and metrics. Furthermore, RU-Net achieves new state-of-the-arts on three popular databases: VG, VRD, and OI. Code is available at https://github.com/siml3/RU-Net.



### Distilling Governing Laws and Source Input for Dynamical Systems from Videos
- **Arxiv ID**: http://arxiv.org/abs/2205.01314v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2205.01314v1)
- **Published**: 2022-05-03 05:40:01+00:00
- **Updated**: 2022-05-03 05:40:01+00:00
- **Authors**: Lele Luan, Yang Liu, Hao Sun
- **Comment**: 7 pages, 6 figures, IJCAI-2022. arXiv admin note: text overlap with
  arXiv:2106.04776
- **Journal**: Proceedings of the 31st International Joint Conference on
  Artificial Intelligence (IJCAI), 2022
- **Summary**: Distilling interpretable physical laws from videos has led to expanded interest in the computer vision community recently thanks to the advances in deep learning, but still remains a great challenge. This paper introduces an end-to-end unsupervised deep learning framework to uncover the explicit governing equations of dynamics presented by moving object(s), based on recorded videos. Instead in the pixel (spatial) coordinate system of image space, the physical law is modeled in a regressed underlying physical coordinate system where the physical states follow potential explicit governing equations. A numerical integrator-based sparse regression module is designed and serves as a physical constraint to the autoencoder and coordinate system regression, and, in the meanwhile, uncover the parsimonious closed-form governing equations from the learned physical states. Experiments on simulated dynamical scenes show that the proposed method is able to distill closed-form governing equations and simultaneously identify unknown excitation input for several dynamical systems recorded by videos, which fills in the gap in literature where no existing methods are available and applicable for solving this type of problem.



### HL-Net: Heterophily Learning Network for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2205.01316v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01316v2)
- **Published**: 2022-05-03 06:00:29+00:00
- **Updated**: 2022-05-04 01:04:20+00:00
- **Authors**: Xin Lin, Changxing Ding, Yibing Zhan, Zijian Li, Dacheng Tao
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Scene graph generation (SGG) aims to detect objects and predict their pairwise relationships within an image. Current SGG methods typically utilize graph neural networks (GNNs) to acquire context information between objects/relationships. Despite their effectiveness, however, current SGG methods only assume scene graph homophily while ignoring heterophily. Accordingly, in this paper, we propose a novel Heterophily Learning Network (HL-Net) to comprehensively explore the homophily and heterophily between objects/relationships in scene graphs. More specifically, HL-Net comprises the following 1) an adaptive reweighting transformer module, which adaptively integrates the information from different layers to exploit both the heterophily and homophily in objects; 2) a relationship feature propagation module that efficiently explores the connections between relationships by considering heterophily in order to refine the relationship representation; 3) a heterophily-aware message-passing scheme to further distinguish the heterophily and homophily between objects/relationships, thereby facilitating improved message passing in graphs. We conducted extensive experiments on two public datasets: Visual Genome (VG) and Open Images (OI). The experimental results demonstrate the superiority of our proposed HL-Net over existing state-of-the-art approaches. In more detail, HL-Net outperforms the second-best competitors by 2.1$\%$ on the VG dataset for scene graph classification and 1.2$\%$ on the IO dataset for the final score. Code is available at https://github.com/siml3/HL-Net.



### Improved Orientation Estimation and Detection with Hybrid Object Detection Networks for Automotive Radar
- **Arxiv ID**: http://arxiv.org/abs/2205.02111v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.02111v2)
- **Published**: 2022-05-03 06:29:03+00:00
- **Updated**: 2022-08-01 14:59:42+00:00
- **Authors**: Michael Ulrich, Sascha Braun, Daniel Köhler, Daniel Niederlöhner, Florian Faion, Claudius Gläser, Holger Blume
- **Comment**: (c) 2022 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: This paper presents novel hybrid architectures that combine grid- and point-based processing to improve the detection performance and orientation estimation of radar-based object detection networks. Purely grid-based detection models operate on a bird's-eye-view (BEV) projection of the input point cloud. These approaches suffer from a loss of detailed information through the discrete grid resolution. This applies in particular to radar object detection, where relatively coarse grid resolutions are commonly used to account for the sparsity of radar point clouds. In contrast, point-based models are not affected by this problem as they process point clouds without discretization. However, they generally exhibit worse detection performances than grid-based methods.   We show that a point-based model can extract neighborhood features, leveraging the exact relative positions of points, before grid rendering. This has significant benefits for a subsequent grid-based convolutional detection backbone. In experiments on the public nuScenes dataset our hybrid architecture achieves improvements in terms of detection performance (19.7% higher mAP for car class than next-best radar-only submission) and orientation estimates (11.5% relative orientation improvement) over networks from previous literature.



### BioTouchPass: Handwritten Passwords for Touchscreen Biometrics
- **Arxiv ID**: http://arxiv.org/abs/2205.01353v1
- **DOI**: 10.1109/TMC.2019.2911506
- **Categories**: **cs.CR**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2205.01353v1)
- **Published**: 2022-05-03 07:42:47+00:00
- **Updated**: 2022-05-03 07:42:47+00:00
- **Authors**: Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez
- **Comment**: arXiv admin note: text overlap with arXiv:2001.10223
- **Journal**: IEEE Transactions on Mobile Computing, 2019
- **Summary**: This work enhances traditional authentication systems based on Personal Identification Numbers (PIN) and One-Time Passwords (OTP) through the incorporation of biometric information as a second level of user authentication. In our proposed approach, users draw each digit of the password on the touchscreen of the device instead of typing them as usual. A complete analysis of our proposed biometric system is carried out regarding the discriminative power of each handwritten digit and the robustness when increasing the length of the password and the number of enrolment samples. The new e-BioDigit database, which comprises on-line handwritten digits from 0 to 9, has been acquired using the finger as input on a mobile device. This database is used in the experiments reported in this work and it is available together with benchmark results in GitHub. Finally, we discuss specific details for the deployment of our proposed approach on current PIN and OTP systems, achieving results with Equal Error Rates (EERs) ca. 4.0% when the attacker knows the password. These results encourage the deployment of our proposed approach in comparison to traditional PIN and OTP systems where the attack would have 100% success rate under the same impostor scenario.



### Predicting Loose-Fitting Garment Deformations Using Bone-Driven Motion Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.01355v3
- **DOI**: 10.1145/3528233.3530709
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01355v3)
- **Published**: 2022-05-03 07:54:39+00:00
- **Updated**: 2022-05-28 02:47:39+00:00
- **Authors**: Xiaoyu Pan, Jiaming Mai, Xinwei Jiang, Dongxue Tang, Jingxiang Li, Tianjia Shao, Kun Zhou, Xiaogang Jin, Dinesh Manocha
- **Comment**: SIGGRAPH 22 Conference Paper
- **Journal**: None
- **Summary**: We present a learning algorithm that uses bone-driven motion networks to predict the deformation of loose-fitting garment meshes at interactive rates. Given a garment, we generate a simulation database and extract virtual bones from simulated mesh sequences using skin decomposition. At runtime, we separately compute low- and high-frequency deformations in a sequential manner. The low-frequency deformations are predicted by transferring body motions to virtual bones' motions, and the high-frequency deformations are estimated leveraging the global information of virtual bones' motions and local information extracted from low-frequency meshes. In addition, our method can estimate garment deformations caused by variations of the simulation parameters (e.g., fabric's bending stiffness) using an RBF kernel ensembling trained networks for different sets of simulation parameters. Through extensive comparisons, we show that our method outperforms state-of-the-art methods in terms of prediction accuracy of mesh deformations by about 20% in RMSE and 10% in Hausdorff distance and STED. The code and data are available at https://github.com/non-void/VirtualBones.



### Biometric Signature Verification Using Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.02934v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.02934v1)
- **Published**: 2022-05-03 07:55:45+00:00
- **Updated**: 2022-05-03 07:55:45+00:00
- **Authors**: Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Javier Ortega-Garcia
- **Comment**: None
- **Journal**: Proc. The 14th IAPR International Conference on Document Analysis
  and Recognition, 2017
- **Summary**: Architectures based on Recurrent Neural Networks (RNNs) have been successfully applied to many different tasks such as speech or handwriting recognition with state-of-the-art results. The main contribution of this work is to analyse the feasibility of RNNs for on-line signature verification in real practical scenarios. We have considered a system based on Long Short-Term Memory (LSTM) with a Siamese architecture whose goal is to learn a similarity metric from pairs of signatures. For the experimental work, the BiosecurID database comprised of 400 users and 4 separated acquisition sessions are considered. Our proposed LSTM RNN system has outperformed the results of recent published works on the BiosecurID benchmark in figures ranging from 17.76% to 28.00% relative verification performance improvement for skilled forgeries.



### Understanding Urban Water Consumption using Remotely Sensed Data
- **Arxiv ID**: http://arxiv.org/abs/2205.02932v2
- **DOI**: 10.1109/IGARSS46834.2022.9883890
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.02932v2)
- **Published**: 2022-05-03 08:04:12+00:00
- **Updated**: 2023-01-06 02:29:55+00:00
- **Authors**: Shaswat Mohanty, Anirudh Vijay, Shailesh Deshpande
- **Comment**: 4 pages, 2 figures, IEEE Conference Proceedings (IGARSS 2022)
- **Journal**: None
- **Summary**: Urban metabolism is an active field of research that deals with the estimation of emissions and resource consumption from urban regions. The analysis could be carried out through a manual surveyor by the implementation of elegant machine learning algorithms. In this exploratory work, we estimate the water consumption by the buildings in the region captured by satellite imagery. To this end, we break our analysis into three parts: i) Identification of building pixels, given a satellite image, followed by ii) identification of the building type (residential/non-residential) from the building pixels, and finally iii) using the building pixels along with their type to estimate the water consumption using the average per unit area consumption for different building types as obtained from municipal surveys.



### A hybrid multi-object segmentation framework with model-based B-splines for microbial single cell analysis
- **Arxiv ID**: http://arxiv.org/abs/2205.01367v1
- **DOI**: 10.1109/ISBI52829.2022.9761575
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2205.01367v1)
- **Published**: 2022-05-03 08:31:55+00:00
- **Updated**: 2022-05-03 08:31:55+00:00
- **Authors**: Karina Ruzaeva, Katharina Nöh, Benjamin Berkels
- **Comment**: None
- **Journal**: 2022 IEEE 19th International Symposium on Biomedical Imaging
  (ISBI)
- **Summary**: In this paper, we propose a hybrid approach for multi-object microbial cell segmentation. The approach combines an ML-based detection with a geometry-aware variational-based segmentation using B-splines that are parametrized based on a geometric model of the cell shape. The detection is done first using YOLOv5. In a second step, each detected cell is segmented individually. Thus, the segmentation only needs to be done on a per-cell basis, which makes it amenable to a variational approach that incorporates prior knowledge on the geometry. Here, the contour of the segmentation is modelled as closed uniform cubic B-spline, whose control points are parametrized using the known cell geometry. Compared to purely ML-based segmentation approaches, which need accurate segmentation maps as training data that are very laborious to produce, our method just needs bounding boxes as training data. Still, the proposed method performs on par with ML-based segmentation approaches usually used in this context. We study the performance of the proposed method on time-lapse microscopy data of Corynebacterium glutamicum.



### Copy Motion From One to Another: Fake Motion Video Generation
- **Arxiv ID**: http://arxiv.org/abs/2205.01373v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.01373v3)
- **Published**: 2022-05-03 08:45:22+00:00
- **Updated**: 2022-12-23 14:17:23+00:00
- **Authors**: Zhenguang Liu, Sifan Wu, Chejian Xu, Xiang Wang, Lei Zhu, Shuang Wu, Fuli Feng
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: One compelling application of artificial intelligence is to generate a video of a target person performing arbitrary desired motion (from a source person). While the state-of-the-art methods are able to synthesize a video demonstrating similar broad stroke motion details, they are generally lacking in texture details. A pertinent manifestation appears as distorted face, feet, and hands, and such flaws are very sensitively perceived by human observers. Furthermore, current methods typically employ GANs with a L2 loss to assess the authenticity of the generated videos, inherently requiring a large amount of training samples to learn the texture details for adequate video generation. In this work, we tackle these challenges from three aspects: 1) We disentangle each video frame into foreground (the person) and background, focusing on generating the foreground to reduce the underlying dimension of the network output. 2) We propose a theoretically motivated Gromov-Wasserstein loss that facilitates learning the mapping from a pose to a foreground image. 3) To enhance texture details, we encode facial features with geometric guidance and employ local GANs to refine the face, feet, and hands. Extensive experiments show that our method is able to generate realistic target person videos, faithfully copying complex motions from a source person.



### Deep Learning in Multimodal Remote Sensing Data Fusion: A Comprehensive Review
- **Arxiv ID**: http://arxiv.org/abs/2205.01380v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2205.01380v1)
- **Published**: 2022-05-03 09:08:16+00:00
- **Updated**: 2022-05-03 09:08:16+00:00
- **Authors**: Jiaxin Li, Danfeng Hong, Lianru Gao, Jing Yao, Ke Zheng, Bing Zhang, Jocelyn Chanussot
- **Comment**: None
- **Journal**: None
- **Summary**: With the extremely rapid advances in remote sensing (RS) technology, a great quantity of Earth observation (EO) data featuring considerable and complicated heterogeneity is readily available nowadays, which renders researchers an opportunity to tackle current geoscience applications in a fresh way. With the joint utilization of EO data, much research on multimodal RS data fusion has made tremendous progress in recent years, yet these developed traditional algorithms inevitably meet the performance bottleneck due to the lack of the ability to comprehensively analyse and interpret these strongly heterogeneous data. Hence, this non-negligible limitation further arouses an intense demand for an alternative tool with powerful processing competence. Deep learning (DL), as a cutting-edge technology, has witnessed remarkable breakthroughs in numerous computer vision tasks owing to its impressive ability in data representation and reconstruction. Naturally, it has been successfully applied to the field of multimodal RS data fusion, yielding great improvement compared with traditional methods. This survey aims to present a systematic overview in DL-based multimodal RS data fusion. More specifically, some essential knowledge about this topic is first given. Subsequently, a literature survey is conducted to analyse the trends of this field. Some prevalent sub-fields in the multimodal RS data fusion are then reviewed in terms of the to-be-fused data modalities, i.e., spatiospectral, spatiotemporal, light detection and ranging-optical, synthetic aperture radar-optical, and RS-Geospatial Big Data fusion. Furthermore, We collect and summarize some valuable resources for the sake of the development in multimodal RS data fusion. Finally, the remaining challenges and potential future directions are highlighted.



### Sampling-free obstacle gradients and reactive planning in Neural Radiance Fields (NeRF)
- **Arxiv ID**: http://arxiv.org/abs/2205.01389v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01389v1)
- **Published**: 2022-05-03 09:32:02+00:00
- **Updated**: 2022-05-03 09:32:02+00:00
- **Authors**: Michael Pantic, Cesar Cadena, Roland Siegwart, Lionel Ott
- **Comment**: Accepted to the "Motion Planning with Implicit Neural Representations
  of Geometry" Workshop at ICRA 2022
- **Journal**: None
- **Summary**: This work investigates the use of Neural implicit representations, specifically Neural Radiance Fields (NeRF), for geometrical queries and motion planning. We show that by adding the capacity to infer occupancy in a radius to a pre-trained NeRF, we are effectively learning an approximation to a Euclidean Signed Distance Field (ESDF). Using backward differentiation of the augmented network, we obtain an obstacle gradient that is integrated into an obstacle avoidance policy based on the Riemannian Motion Policies (RMP) framework. Thus, our findings allow for very fast sampling-free obstacle avoidance planning in the implicit representation.



### Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)
- **Arxiv ID**: http://arxiv.org/abs/2205.01397v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01397v2)
- **Published**: 2022-05-03 10:06:51+00:00
- **Updated**: 2022-08-22 23:59:30+00:00
- **Authors**: Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, Ludwig Schmidt
- **Comment**: None
- **Journal**: None
- **Summary**: Contrastively trained language-image models such as CLIP, ALIGN, and BASIC have demonstrated unprecedented robustness to multiple challenging natural distribution shifts. Since these language-image models differ from previous training approaches in several ways, an important question is what causes the large robustness gains. We answer this question via a systematic experimental investigation. Concretely, we study five different possible causes for the robustness gains: (i) the training set size, (ii) the training distribution, (iii) language supervision at training time, (iv) language supervision at test time, and (v) the contrastive loss function. Our experiments show that the more diverse training distribution is the main cause for the robustness gains, with the other factors contributing little to no robustness. Beyond our experimental results, we also introduce ImageNet-Captions, a version of ImageNet with original text annotations from Flickr, to enable further controlled experiments of language-image training.



### Outdoor Monocular Depth Estimation: A Research Review
- **Arxiv ID**: http://arxiv.org/abs/2205.01399v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01399v1)
- **Published**: 2022-05-03 10:10:08+00:00
- **Updated**: 2022-05-03 10:10:08+00:00
- **Authors**: Pulkit Vyas, Chirag Saxena, Anwesh Badapanda, Anurag Goswami
- **Comment**: None
- **Journal**: None
- **Summary**: Depth estimation is an important task, applied in various methods and applications of computer vision. While the traditional methods of estimating depth are based on depth cues and require specific equipment such as stereo cameras and configuring input according to the approach being used, the focus at the current time is on a single source, or monocular, depth estimation. The recent developments in Convolution Neural Networks along with the integration of classical methods in these deep learning approaches have led to a lot of advancements in the depth estimation problem. The problem of outdoor depth estimation, or depth estimation in wild, is a very scarcely researched field of study. In this paper, we give an overview of the available datasets, depth estimation methods, research work, trends, challenges, and opportunities that exist for open research. To our knowledge, no openly available survey work provides a comprehensive collection of outdoor depth estimation techniques and research scope, making our work an essential contribution for people looking to enter this field of study.



### Multimodal Detection of Unknown Objects on Roads for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2205.01414v3
- **DOI**: 10.1109/SMC53654.2022.9945211
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.01414v3)
- **Published**: 2022-05-03 10:58:41+00:00
- **Updated**: 2022-07-22 10:21:57+00:00
- **Authors**: Daniel Bogdoll, Enrico Eisen, Maximilian Nitsche, Christin Scheib, J. Marius Zöllner
- **Comment**: Daniel Bogdoll, Enrico Eisen, Maximilian Nitsche, and Christin Scheib
  contributed equally. Accepted for publication at SMC 2022
- **Journal**: None
- **Summary**: Tremendous progress in deep learning over the last years has led towards a future with autonomous vehicles on our roads. Nevertheless, the performance of their perception systems is strongly dependent on the quality of the utilized training data. As these usually only cover a fraction of all object classes an autonomous driving system will face, such systems struggle with handling the unexpected. In order to safely operate on public roads, the identification of objects from unknown classes remains a crucial task. In this paper, we propose a novel pipeline to detect unknown objects. Instead of focusing on a single sensor modality, we make use of lidar and camera data by combining state-of-the art detection models in a sequential manner. We evaluate our approach on the Waymo Open Perception Dataset and point out current research gaps in anomaly detection.



### An Empirical Analysis of the Use of Real-Time Reachability for the Safety Assurance of Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2205.01419v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.FL, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2205.01419v1)
- **Published**: 2022-05-03 11:12:29+00:00
- **Updated**: 2022-05-03 11:12:29+00:00
- **Authors**: Patrick Musau, Nathaniel Hamilton, Diego Manzanas Lopez, Preston Robinette, Taylor T. Johnson
- **Comment**: 30 pages, 12 Figures, Submitted to Artificial Intelligence's Special
  Issue on "Risk-Aware Autonomous Systems: Theory and Practice."
- **Journal**: None
- **Summary**: Recent advances in machine learning technologies and sensing have paved the way for the belief that safe, accessible, and convenient autonomous vehicles may be realized in the near future. Despite tremendous advances within this context, fundamental challenges around safety and reliability are limiting their arrival and comprehensive adoption. Autonomous vehicles are often tasked with operating in dynamic and uncertain environments. As a result, they often make use of highly complex components, such as machine learning approaches, to handle the nuances of sensing, actuation, and control. While these methods are highly effective, they are notoriously difficult to assure. Moreover, within uncertain and dynamic environments, design time assurance analyses may not be sufficient to guarantee safety. Thus, it is critical to monitor the correctness of these systems at runtime. One approach for providing runtime assurance of systems with components that may not be amenable to formal analysis is the simplex architecture, where an unverified component is wrapped with a safety controller and a switching logic designed to prevent dangerous behavior. In this paper, we propose using a real-time reachability algorithm for the implementation of the simplex architecture to assure the safety of a 1/10 scale open source autonomous vehicle platform known as F1/10. The reachability algorithm that we leverage (a) provides provable guarantees of safety, and (b) is used to detect potentially unsafe scenarios. In our approach, the need to analyze an underlying controller is abstracted away, instead focusing on the effects of the controller's decisions on the system's future states. We demonstrate the efficacy of our architecture through a vast set of experiments conducted both in simulation and on an embedded hardware platform.



### Frequency-Selective Geometry Upsampling of Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2205.01458v2
- **DOI**: 10.1109/ICIP46576.2022.9897920
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01458v2)
- **Published**: 2022-05-03 12:44:10+00:00
- **Updated**: 2022-10-27 14:17:29+00:00
- **Authors**: Viktoria Heimann, Andreas Spruck, André Kaup
- **Comment**: 5 pages, 3 figures, International Conference on Image Processing
  (ICIP) 2022
- **Journal**: None
- **Summary**: The demand for high-resolution point clouds has increased throughout the last years. However, capturing high-resolution point clouds is expensive and thus, frequently replaced by upsampling of low-resolution data. Most state-of-the-art methods are either restricted to a rastered grid, incorporate normal vectors, or are trained for a single use case. We propose to use the frequency selectivity principle, where a frequency model is estimated locally that approximates the surface of the point cloud. Then, additional points are inserted into the approximated surface. Our novel frequency-selective geometry upsampling shows superior results in terms of subjective as well as objective quality compared to state-of-the-art methods for scaling factors of 2 and 4. On average, our proposed method shows a 4.4 times smaller point-to-point error than the second best state-of-the-art PU-Net for a scale factor of 4.



### 3D Semantic Scene Perception using Distributed Smart Edge Sensors
- **Arxiv ID**: http://arxiv.org/abs/2205.01460v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.01460v1)
- **Published**: 2022-05-03 12:46:26+00:00
- **Updated**: 2022-05-03 12:46:26+00:00
- **Authors**: Simon Bultmann, Sven Behnke
- **Comment**: 17th International Conference on Intelligent Autonomous Systems
  (IAS), Zagreb, Croatia, June 2022
- **Journal**: None
- **Summary**: We present a system for 3D semantic scene perception consisting of a network of distributed smart edge sensors. The sensor nodes are based on an embedded CNN inference accelerator and RGB-D and thermal cameras. Efficient vision CNN models for object detection, semantic segmentation, and human pose estimation run on-device in real time. 2D human keypoint estimations, augmented with the RGB-D depth estimate, as well as semantically annotated point clouds are streamed from the sensors to a central backend, where multiple viewpoints are fused into an allocentric 3D semantic scene model. As the image interpretation is computed locally, only semantic information is sent over the network. The raw images remain on the sensor boards, significantly reducing the required bandwidth, and mitigating privacy risks for the observed persons. We evaluate the proposed system in challenging real-world multi-person scenes in our lab. The proposed perception system provides a complete scene view containing semantically annotated 3D geometry and estimates 3D poses of multiple persons in real time.



### Subspace Diffusion Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2205.01490v2
- **DOI**: 10.1007/978-3-031-20050-2_17
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01490v2)
- **Published**: 2022-05-03 13:43:47+00:00
- **Updated**: 2023-02-27 18:44:42+00:00
- **Authors**: Bowen Jing, Gabriele Corso, Renato Berlinghieri, Tommi Jaakkola
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Score-based models generate samples by mapping noise to data (and vice versa) via a high-dimensional diffusion process. We question whether it is necessary to run this entire process at high dimensionality and incur all the inconveniences thereof. Instead, we restrict the diffusion via projections onto subspaces as the data distribution evolves toward noise. When applied to state-of-the-art models, our framework simultaneously improves sample quality -- reaching an FID of 2.17 on unconditional CIFAR-10 -- and reduces the computational cost of inference for the same number of denoising steps. Our framework is fully compatible with continuous-time diffusion and retains its flexible capabilities, including exact log-likelihoods and controllable generation. Code is available at https://github.com/bjing2016/subspace-diffusion.



### A Comprehensive Survey of Image Augmentation Techniques for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.01491v2
- **DOI**: 10.1016/j.patcog.2023.109347
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01491v2)
- **Published**: 2022-05-03 13:45:04+00:00
- **Updated**: 2022-11-23 10:48:23+00:00
- **Authors**: Mingle Xu, Sook Yoon, Alvaro Fuentes, Dong Sun Park
- **Comment**: Revision
- **Journal**: None
- **Summary**: Deep learning has been achieving decent performance in computer vision requiring a large volume of images, however, collecting images is expensive and difficult in many scenarios. To alleviate this issue, many image augmentation algorithms have been proposed as effective and efficient strategies. Understanding current algorithms is essential to find suitable methods or develop novel techniques for given tasks. In this paper, we perform a comprehensive survey on image augmentation for deep learning with a novel informative taxonomy. To get the basic idea why we need image augmentation, we introduce the challenges in computer vision tasks and vicinity distribution. Then, the algorithms are split into three categories; model-free, model-based, and optimizing policy-based. The model-free category employs image processing methods while the model-based method leverages trainable image generation models. In contrast, the optimizing policy-based approach aims to find the optimal operations or their combinations. Furthermore, we discuss the current trend of common applications with two more active topics, leveraging different ways to understand image augmentation, such as group and kernel theory, and deploying image augmentation for unsupervised learning. Based on the analysis, we believe that our survey gives a better understanding helpful to choose suitable methods or design novel algorithms for practical applications.



### Compact Neural Networks via Stacking Designed Basic Units
- **Arxiv ID**: http://arxiv.org/abs/2205.01508v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01508v1)
- **Published**: 2022-05-03 14:04:49+00:00
- **Updated**: 2022-05-03 14:04:49+00:00
- **Authors**: Weichao Lan, Yiu-ming Cheung, Juyong Jiang
- **Comment**: 17 pages, 4 figures, 5 tables
- **Journal**: None
- **Summary**: Unstructured pruning has the limitation of dealing with the sparse and irregular weights. By contrast, structured pruning can help eliminate this drawback but it requires complex criterion to determine which components to be pruned. To this end, this paper presents a new method termed TissueNet, which directly constructs compact neural networks with fewer weight parameters by independently stacking designed basic units, without requiring additional judgement criteria anymore. Given the basic units of various architectures, they are combined and stacked in a certain form to build up compact neural networks. We formulate TissueNet in diverse popular backbones for comparison with the state-of-the-art pruning methods on different benchmark datasets. Moreover, two new metrics are proposed to evaluate compression performance. Experiment results show that TissueNet can achieve comparable classification accuracy while saving up to around 80% FLOPs and 89.7% parameters. That is, stacking basic units provides a new promising way for network compression.



### MS Lesion Segmentation: Revisiting Weighting Mechanisms for Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.01509v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01509v1)
- **Published**: 2022-05-03 14:06:03+00:00
- **Updated**: 2022-05-03 14:06:03+00:00
- **Authors**: Dongnan Liu, Mariano Cabezas, Dongang Wang, Zihao Tang, Lei Bai, Geng Zhan, Yuling Luo, Kain Kyle, Linda Ly, James Yu, Chun-Chien Shieh, Aria Nguyen, Ettikan Kandasamy Karuppiah, Ryan Sullivan, Fernando Calamante, Michael Barnett, Wanli Ouyang, Weidong Cai, Chenyu Wang
- **Comment**: 10 pages, 3 figures, and 7 tables
- **Journal**: None
- **Summary**: Federated learning (FL) has been widely employed for medical image analysis to facilitate multi-client collaborative learning without sharing raw data. Despite great success, FL's performance is limited for multiple sclerosis (MS) lesion segmentation tasks, due to variance in lesion characteristics imparted by different scanners and acquisition parameters. In this work, we propose the first FL MS lesion segmentation framework via two effective re-weighting mechanisms. Specifically, a learnable weight is assigned to each local node during the aggregation process, based on its segmentation performance. In addition, the segmentation loss function in each client is also re-weighted according to the lesion volume for the data during training. Comparison experiments on two FL MS segmentation scenarios using public and clinical datasets have demonstrated the effectiveness of the proposed method by outperforming other FL methods significantly. Furthermore, the segmentation performance of FL incorporating our proposed aggregation mechanism can exceed centralised training with all the raw data. The extensive evaluation also indicated the superiority of our method when estimating brain volume differences estimation after lesion inpainting.



### Multitask Network for Joint Object Detection, Semantic Segmentation and Human Pose Estimation in Vehicle Occupancy Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2205.01515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01515v1)
- **Published**: 2022-05-03 14:11:18+00:00
- **Updated**: 2022-05-03 14:11:18+00:00
- **Authors**: Nikolas Ebert, Patrick Mangat, Oliver Wasenmüller
- **Comment**: This paper has been accepted at IEEE Intelligent Vehicles Symposium
  (IV), 2022 (ORAL)
- **Journal**: None
- **Summary**: In order to ensure safe autonomous driving, precise information about the conditions in and around the vehicle must be available. Accordingly, the monitoring of occupants and objects inside the vehicle is crucial. In the state-of-the-art, single or multiple deep neural networks are used for either object recognition, semantic segmentation, or human pose estimation. In contrast, we propose our Multitask Detection, Segmentation and Pose Estimation Network (MDSP) -- the first multitask network solving all these three tasks jointly in the area of occupancy monitoring. Due to the shared architecture, memory and computing costs can be saved while achieving higher accuracy. Furthermore, our architecture allows a flexible combination of the three mentioned tasks during a simple end-to-end training. We perform comprehensive evaluations on the public datasets SVIRO and TiCaM in order to demonstrate the superior performance.



### Masked Generative Distillation
- **Arxiv ID**: http://arxiv.org/abs/2205.01529v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01529v2)
- **Published**: 2022-05-03 14:30:26+00:00
- **Updated**: 2022-07-05 06:35:10+00:00
- **Authors**: Zhendong Yang, Zhe Li, Mingqi Shao, Dachuan Shi, Zehuan Yuan, Chun Yuan
- **Comment**: Accept by ECCV 2022
- **Journal**: None
- **Summary**: Knowledge distillation has been applied to various tasks successfully. The current distillation algorithm usually improves students' performance by imitating the output of the teacher. This paper shows that teachers can also improve students' representation power by guiding students' feature recovery. From this point of view, we propose Masked Generative Distillation (MGD), which is simple: we mask random pixels of the student's feature and force it to generate the teacher's full feature through a simple block. MGD is a truly general feature-based distillation method, which can be utilized on various tasks, including image classification, object detection, semantic segmentation and instance segmentation. We experiment on different models with extensive datasets and the results show that all the students achieve excellent improvements. Notably, we boost ResNet-18 from 69.90% to 71.69% ImageNet top-1 accuracy, RetinaNet with ResNet-50 backbone from 37.4 to 41.0 Boundingbox mAP, SOLO based on ResNet-50 from 33.1 to 36.2 Mask mAP and DeepLabV3 based on ResNet-18 from 73.20 to 76.02 mIoU. Our codes are available at https://github.com/yzd-v/MGD.



### BiOcularGAN: Bimodal Synthesis and Annotation of Ocular Images
- **Arxiv ID**: http://arxiv.org/abs/2205.01536v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01536v3)
- **Published**: 2022-05-03 14:43:39+00:00
- **Updated**: 2022-12-08 16:06:13+00:00
- **Authors**: Darian Tomašević, Peter Peer, Vitomir Štruc
- **Comment**: 13 pages, 14 figures
- **Journal**: None
- **Summary**: Current state-of-the-art segmentation techniques for ocular images are critically dependent on large-scale annotated datasets, which are labor-intensive to gather and often raise privacy concerns. In this paper, we present a novel framework, called BiOcularGAN, capable of generating synthetic large-scale datasets of photorealistic (visible light and near-infrared) ocular images, together with corresponding segmentation labels to address these issues. At its core, the framework relies on a novel Dual-Branch StyleGAN2 (DB-StyleGAN2) model that facilitates bimodal image generation, and a Semantic Mask Generator (SMG) component that produces semantic annotations by exploiting latent features of the DB-StyleGAN2 model. We evaluate BiOcularGAN through extensive experiments across five diverse ocular datasets and analyze the effects of bimodal data generation on image quality and the produced annotations. Our experimental results show that BiOcularGAN is able to produce high-quality matching bimodal images and annotations (with minimal manual intervention) that can be used to train highly competitive (deep) segmentation models (in a privacy aware-manner) that perform well across multiple real-world datasets. The source code for the BiOcularGAN framework is publicly available at https://github.com/dariant/BiOcularGAN.



### Point Cloud Semantic Segmentation using Multi Scale Sparse Convolution Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2205.01550v6
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01550v6)
- **Published**: 2022-05-03 15:01:20+00:00
- **Updated**: 2022-06-30 03:39:39+00:00
- **Authors**: Yunzheng Su, Lei Jiang, Jie Cao
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, with the development of computing resources and LiDAR, point cloud semantic segmentation has attracted many researchers. For the sparsity of point clouds, although there is already a way to deal with sparse convolution, multi-scale features are not considered. In this letter, we propose a feature extraction module based on multi-scale sparse convolution and a feature selection module based on channel attention and build a point cloud segmentation network framework based on this. By introducing multi-scale sparse convolution, the network could capture richer feature information based on convolution kernels with different sizes, improving the segmentation result of point cloud segmentation. Experimental results on Stanford large-scale 3-D Indoor Spaces(S3DIS) dataset and outdoor dataset(SemanticKITTI), demonstrate effectiveness and superiority of the proposed mothod.



### Cross-View Cross-Scene Multi-View Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2205.01551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01551v1)
- **Published**: 2022-05-03 15:03:44+00:00
- **Updated**: 2022-05-03 15:03:44+00:00
- **Authors**: Qi Zhang, Wei Lin, Antoni B. Chan
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: Multi-view crowd counting has been previously proposed to utilize multi-cameras to extend the field-of-view of a single camera, capturing more people in the scene, and improve counting performance for occluded people or those in low resolution. However, the current multi-view paradigm trains and tests on the same single scene and camera-views, which limits its practical application. In this paper, we propose a cross-view cross-scene (CVCS) multi-view crowd counting paradigm, where the training and testing occur on different scenes with arbitrary camera layouts. To dynamically handle the challenge of optimal view fusion under scene and camera layout change and non-correspondence noise due to camera calibration errors or erroneous features, we propose a CVCS model that attentively selects and fuses multiple views together using camera layout geometry, and a noise view regularization method to train the model to handle non-correspondence errors. We also generate a large synthetic multi-camera crowd counting dataset with a large number of scenes and camera views to capture many possible variations, which avoids the difficulty of collecting and annotating such a large real dataset. We then test our trained CVCS model on real multi-view counting datasets, by using unsupervised domain transfer. The proposed CVCS model trained on synthetic data outperforms the same model trained only on real data, and achieves promising performance compared to fully supervised methods that train and test on the same single scene.



### SpineNetV2: Automated Detection, Labelling and Radiological Grading Of Clinical MR Scans
- **Arxiv ID**: http://arxiv.org/abs/2205.01683v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01683v1)
- **Published**: 2022-05-03 15:05:58+00:00
- **Updated**: 2022-05-03 15:05:58+00:00
- **Authors**: Rhydian Windsor, Amir Jamaludin, Timor Kadir, Andrew Zisserman
- **Comment**: Technical Report, 22 pages, 9 Figures
- **Journal**: None
- **Summary**: This technical report presents SpineNetV2, an automated tool which: (i) detects and labels vertebral bodies in clinical spinal magnetic resonance (MR) scans across a range of commonly used sequences; and (ii) performs radiological grading of lumbar intervertebral discs in T2-weighted scans for a range of common degenerative changes. SpineNetV2 improves over the original SpineNet software in two ways: (1) The vertebral body detection stage is significantly faster, more accurate and works across a range of fields-of-view (as opposed to just lumbar scans). (2) Radiological grading adopts a more powerful architecture, adding several new grading schemes without loss in performance. A demo of the software is available at the project website: http://zeus.robots.ox.ac.uk/spinenet2/.



### Effect of Random Histogram Equalization on Breast Calcification Analysis Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.01684v1
- **DOI**: 10.1007/978-3-658-36932-3_38
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01684v1)
- **Published**: 2022-05-03 15:26:41+00:00
- **Updated**: 2022-05-03 15:26:41+00:00
- **Authors**: Adarsh Bhandary Panambur, Prathmesh Madhu, Andreas Maier
- **Comment**: Accepted at Bildverarbeitung f\"ur die Medizin (BVM) Workshop 2022
- **Journal**: None
- **Summary**: Early detection and analysis of calcifications in mammogram images is crucial in a breast cancer diagnosis workflow. Management of calcifications that require immediate follow-up and further analyzing its benignancy or malignancy can result in a better prognosis. Recent studies have shown that deep learning-based algorithms can learn robust representations to analyze suspicious calcifications in mammography. In this work, we demonstrate that randomly equalizing the histograms of calcification patches as a data augmentation technique can significantly improve the classification performance for analyzing suspicious calcifications. We validate our approach by using the CBIS-DDSM dataset for two classification tasks. The results on both the tasks show that the proposed methodology gains more than 1% mean accuracy and F1-score when equalizing the data with a probability of 0.4 when compared to not using histogram equalization. This is further supported by the t-tests, where we obtain a p-value of p<0.0001, thus showing the statistical significance of our approach.



### RAFT-MSF: Self-Supervised Monocular Scene Flow using Recurrent Optimizer
- **Arxiv ID**: http://arxiv.org/abs/2205.01568v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.01568v1)
- **Published**: 2022-05-03 15:43:57+00:00
- **Updated**: 2022-05-03 15:43:57+00:00
- **Authors**: Bayram Bayramli, Junhwa Hur, Hongtao Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Learning scene flow from a monocular camera still remains a challenging task due to its ill-posedness as well as lack of annotated data. Self-supervised methods demonstrate learning scene flow estimation from unlabeled data, yet their accuracy lags behind (semi-)supervised methods. In this paper, we introduce a self-supervised monocular scene flow method that substantially improves the accuracy over the previous approaches. Based on RAFT, a state-of-the-art optical flow model, we design a new decoder to iteratively update 3D motion fields and disparity maps simultaneously. Furthermore, we propose an enhanced upsampling layer and a disparity initialization technique, which overall further improves accuracy up to 7.2%. Our method achieves state-of-the-art accuracy among all self-supervised monocular scene flow methods, improving accuracy by 34.2%. Our fine-tuned model outperforms the best previous semi-supervised method with 228 times faster runtime. Code will be publicly available.



### Better plain ViT baselines for ImageNet-1k
- **Arxiv ID**: http://arxiv.org/abs/2205.01580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01580v1)
- **Published**: 2022-05-03 15:54:44+00:00
- **Updated**: 2022-05-03 15:54:44+00:00
- **Authors**: Lucas Beyer, Xiaohua Zhai, Alexander Kolesnikov
- **Comment**: Code available at https://github.com/google-research/big_vision
- **Journal**: None
- **Summary**: It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we find this is not the case and standard data augmentation is sufficient. This note presents a few minor modifications to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80% in less than one day.



### Simpler is Better: off-the-shelf Continual Learning Through Pretrained Backbones
- **Arxiv ID**: http://arxiv.org/abs/2205.01586v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01586v2)
- **Published**: 2022-05-03 16:03:46+00:00
- **Updated**: 2022-05-23 14:44:34+00:00
- **Authors**: Francesco Pelosin
- **Comment**: Accepted at "T4V: Transformers for Vision Workshop" at CVPR2022
- **Journal**: None
- **Summary**: In this short paper, we propose a baseline (off-the-shelf) for Continual Learning of Computer Vision problems, by leveraging the power of pretrained models. By doing so, we devise a simple approach achieving strong performance for most of the common benchmarks. Our approach is fast since requires no parameters updates and has minimal memory requirements (order of KBytes). In particular, the "training" phase reorders data and exploit the power of pretrained models to compute a class prototype and fill a memory bank. At inference time we match the closest prototype through a knn-like approach, providing us the prediction. We will see how this naive solution can act as an off-the-shelf continual learning system. In order to better consolidate our results, we compare the devised pipeline with common CNN models and show the superiority of Vision Transformers, suggesting that such architectures have the ability to produce features of higher quality. Moreover, this simple pipeline, raises the same questions raised by previous works \cite{gdumb} on the effective progresses made by the CL community especially in the dataset considered and the usage of pretrained models. Code is live at https://github.com/francesco-p/off-the-shelf-cl



### A Bidirectional Conversion Network for Cross-Spectral Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.01595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01595v1)
- **Published**: 2022-05-03 16:20:10+00:00
- **Updated**: 2022-05-03 16:20:10+00:00
- **Authors**: Zhicheng Cao, Jiaxuan Zhang, Liaojun Pang
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition in the infrared (IR) band has become an important supplement to visible light face recognition due to its advantages of independent background light, strong penetration, ability of imaging under harsh environments such as nighttime, rain and fog. However, cross-spectral face recognition (i.e., VIS to IR) is very challenging due to the dramatic difference between the visible light and IR imageries as well as the lack of paired training data. This paper proposes a framework of bidirectional cross-spectral conversion (BCSC-GAN) between the heterogeneous face images, and designs an adaptive weighted fusion mechanism based on information fusion theory. The network reduces the cross-spectral recognition problem into an intra-spectral problem, and improves performance by fusing bidirectional information. Specifically, a face identity retaining module (IRM) is introduced with the ability to preserve identity features, and a new composite loss function is designed to overcome the modal differences caused by different spectral characteristics. Two datasets of TINDERS and CASIA were tested, where performance metrics of FID, recognition rate, equal error rate and normalized distance were compared. Results show that our proposed network is superior than other state-of-the-art methods. Additionally, the proposed rule of Self Adaptive Weighted Fusion (SAWF) is better than the recognition results of the unfused case and other traditional fusion rules that are commonly used, which further justifies the effectiveness and superiority of the proposed bidirectional conversion approach.



### Toward Modeling Creative Processes for Algorithmic Painting
- **Arxiv ID**: http://arxiv.org/abs/2205.01605v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2205.01605v2)
- **Published**: 2022-05-03 16:33:45+00:00
- **Updated**: 2022-05-23 15:08:54+00:00
- **Authors**: Aaron Hertzmann
- **Comment**: Proc. ICCC 2022
- **Journal**: None
- **Summary**: This paper proposes a framework for computational modeling of artistic painting algorithms, inspired by human creative practices. Based on examples from expert artists and from the author's own experience, the paper argues that creative processes often involve two important components: vague, high-level goals (e.g., "make a good painting"), and exploratory processes for discovering new ideas. This paper then sketches out possible computational mechanisms for imitating those elements of the painting process, including underspecified loss functions and iterative painting procedures with explicit task decompositions.



### Automatic Segmentation of Aircraft Dents in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2205.01614v3
- **DOI**: 10.4271/2022-01-0022
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01614v3)
- **Published**: 2022-05-03 16:48:31+00:00
- **Updated**: 2023-02-24 09:58:08+00:00
- **Authors**: Pasquale Lafiosca, Ip-Shing Fan, Nicolas P. Avdelidis
- **Comment**: None
- **Journal**: None
- **Summary**: Dents on the aircraft skin are frequent and may easily go undetected during airworthiness checks, as their inspection process is tedious and extremely subject to human factors and environmental conditions. Nowadays, 3D scanning technologies are being proposed for more reliable, human-independent measurements, yet the process of inspection and reporting remains laborious and time consuming because data acquisition and validation are still carried out by the engineer. For full automation of dent inspection, the acquired point cloud data must be analysed via a reliable segmentation algorithm, releasing humans from the search and evaluation of damage. This paper reports on two developments towards automated dent inspection. The first is a method to generate a synthetic dataset of dented surfaces to train a fully convolutional neural network. The training of machine learning algorithms needs a substantial volume of dent data, which is not readily available. Dents are thus simulated in random positions and shapes, within criteria and definitions of a Boeing 737 structural repair manual. The noise distribution from the scanning apparatus is then added to reflect the complete process of 3D point acquisition on the training. The second proposition is a surface fitting strategy to convert 3D point clouds to 2.5D. This allows higher resolution point clouds to be processed with a small amount of memory compared with state-of-the-art methods involving 3D sampling approaches. Simulations with available ground truth data show that the proposed technique reaches an intersection-over-union of over 80%. Experiments over dent samples prove an effective detection of dents with a speed of over 500 000 points per second.



### Multi-view Geometry: Correspondences Refinement Based on Algebraic Properties
- **Arxiv ID**: http://arxiv.org/abs/2205.01634v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01634v1)
- **Published**: 2022-05-03 17:08:07+00:00
- **Updated**: 2022-05-03 17:08:07+00:00
- **Authors**: Trung-Kien Le, Ping Li
- **Comment**: None
- **Journal**: None
- **Summary**: Correspondences estimation or feature matching is a key step in the image-based 3D reconstruction problem. In this paper, we propose two algebraic properties for correspondences. The first is a rank deficient matrix construct from the correspondences of at least nine key-points on two images (two-view correspondences) and the second is also another rank deficient matrix built from the other correspondences of six key-points on at least five images (multi-view correspondences). To our knowledge, there are no theoretical results for multi-view correspondences prior to this paper. To obtain accurate correspondences, multi-view correspondences seem to be more useful than two-view correspondences. From these two algebraic properties, we propose an refinement algorithm for correspondences. This algorithm is a combination of correspondences refinement, outliers recognition and missing key-points recovery. Real experiments from the project of reconstructing Buddha statue show that the proposed refinement algorithm can reduce the average error from 77 pixels to 55 pixels on the correspondences estimation. This drop is substantial and it validates our results.



### A Contrario multi-scale anomaly detection method for industrial quality inspection
- **Arxiv ID**: http://arxiv.org/abs/2205.11611v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11611v1)
- **Published**: 2022-05-03 17:08:36+00:00
- **Updated**: 2022-05-03 17:08:36+00:00
- **Authors**: Matías Tailanian, Pablo Musé, Álvaro Pardo
- **Comment**: 12 pages, 8 figures, 4 tables. arXiv admin note: substantial text
  overlap with arXiv:2110.02407
- **Journal**: None
- **Summary**: Anomalies can be defined as any non-random structure which deviates from normality. Anomaly detection methods reported in the literature are numerous and diverse, as what is considered anomalous usually varies depending on particular scenarios and applications. In this work we propose an a contrario framework to detect anomalies in images applying statistical analysis to feature maps obtained via convolutions. We evaluate filters learned from the image under analysis via patch PCA, Gabor filters and the feature maps obtained from a pre-trained deep neural network (Resnet). The proposed method is multi-scale and fully unsupervised and is able to detect anomalies in a wide variety of scenarios. While the end goal of this work is the detection of subtle defects in leather samples for the automotive industry, we show that the same algorithm achieves state-of-the-art results in public anomalies datasets.



### MTTrans: Cross-Domain Object Detection with Mean-Teacher Transformer
- **Arxiv ID**: http://arxiv.org/abs/2205.01643v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01643v2)
- **Published**: 2022-05-03 17:11:55+00:00
- **Updated**: 2022-08-16 09:55:23+00:00
- **Authors**: Jinze Yu, Jiaming Liu, Xiaobao Wei, Haoyi Zhou, Yohei Nakata, Denis Gudovskiy, Tomoyuki Okuno, Jianxin Li, Kurt Keutzer, Shanghang Zhang
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Recently, DEtection TRansformer (DETR), an end-to-end object detection pipeline, has achieved promising performance. However, it requires large-scale labeled data and suffers from domain shift, especially when no labeled data is available in the target domain. To solve this problem, we propose an end-to-end cross-domain detection Transformer based on the mean teacher framework, MTTrans, which can fully exploit unlabeled target domain data in object detection training and transfer knowledge between domains via pseudo labels. We further propose the comprehensive multi-level feature alignment to improve the pseudo labels generated by the mean teacher framework taking advantage of the cross-scale self-attention mechanism in Deformable DETR. Image and object features are aligned at the local, global, and instance levels with domain query-based feature alignment (DQFA), bi-level graph-based prototype alignment (BGPA), and token-wise image feature alignment (TIFA). On the other hand, the unlabeled target domain data pseudo-labeled and available for the object detection training by the mean teacher framework can lead to better feature extraction and alignment. Thus, the mean teacher framework and the comprehensive multi-level feature alignment can be optimized iteratively and mutually based on the architecture of Transformers. Extensive experiments demonstrate that our proposed method achieves state-of-the-art performance in three domain adaptation scenarios, especially the result of Sim10k to Cityscapes scenario is remarkably improved from 52.6 mAP to 57.9 mAP. Code will be released.



### Smart City Intersections: Intelligence Nodes for Future Metropolises
- **Arxiv ID**: http://arxiv.org/abs/2205.01686v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01686v2)
- **Published**: 2022-05-03 17:22:57+00:00
- **Updated**: 2022-05-13 12:25:06+00:00
- **Authors**: Zoran Kostić, Alex Angus, Zhengye Yang, Zhuoxu Duan, Ivan Seskar, Gil Zussman, Dipankar Raychaudhuri
- **Comment**: None
- **Journal**: None
- **Summary**: Traffic intersections are the most suitable locations for the deployment of computing, communications, and intelligence services for smart cities of the future. The abundance of data to be collected and processed, in combination with privacy and security concerns, motivates the use of the edge-computing paradigm which aligns well with physical intersections in metropolises. This paper focuses on high-bandwidth, low-latency applications, and in that context it describes: (i) system design considerations for smart city intersection intelligence nodes; (ii) key technological components including sensors, networking, edge computing, low latency design, and AI-based intelligence; and (iii) applications such as privacy preservation, cloud-connected vehicles, a real-time "radar-screen", traffic management, and monitoring of pedestrian behavior during pandemics. The results of the experimental studies performed on the COSMOS testbed located in New York City are illustrated. Future challenges in designing human-centered smart city intersections are summarized.



### Episodic Memory Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2205.01652v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.01652v1)
- **Published**: 2022-05-03 17:28:43+00:00
- **Updated**: 2022-05-03 17:28:43+00:00
- **Authors**: Samyak Datta, Sameer Dharur, Vincent Cartillier, Ruta Desai, Mukul Khanna, Dhruv Batra, Devi Parikh
- **Comment**: Published at CVPR 2022 (Oral presentation)
- **Journal**: None
- **Summary**: Egocentric augmented reality devices such as wearable glasses passively capture visual data as a human wearer tours a home environment. We envision a scenario wherein the human communicates with an AI agent powering such a device by asking questions (e.g., where did you last see my keys?). In order to succeed at this task, the egocentric AI assistant must (1) construct semantically rich and efficient scene memories that encode spatio-temporal information about objects seen during the tour and (2) possess the ability to understand the question and ground its answer into the semantic memory representation. Towards that end, we introduce (1) a new task - Episodic Memory Question Answering (EMQA) wherein an egocentric AI assistant is provided with a video sequence (the tour) and a question as an input and is asked to localize its answer to the question within the tour, (2) a dataset of grounded questions designed to probe the agent's spatio-temporal understanding of the tour, and (3) a model for the task that encodes the scene as an allocentric, top-down semantic feature map and grounds the question into the map to localize the answer. We show that our choice of episodic scene memory outperforms naive, off-the-shelf solutions for the task as well as a host of very competitive baselines and is robust to noise in depth, pose as well as camera jitter. The project page can be found at: https://samyak-268.github.io/emqa .



### GeoRefine: Self-Supervised Online Depth Refinement for Accurate Dense Mapping
- **Arxiv ID**: http://arxiv.org/abs/2205.01656v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01656v1)
- **Published**: 2022-05-03 17:39:06+00:00
- **Updated**: 2022-05-03 17:39:06+00:00
- **Authors**: Pan Ji, Qingan Yan, Yuxin Ma, Yi Xu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a robust and accurate depth refinement system, named GeoRefine, for geometrically-consistent dense mapping from monocular sequences. GeoRefine consists of three modules: a hybrid SLAM module using learning-based priors, an online depth refinement module leveraging self-supervision, and a global mapping module via TSDF fusion. The proposed system is online by design and achieves great robustness and accuracy via: (i) a robustified hybrid SLAM that incorporates learning-based optical flow and/or depth; (ii) self-supervised losses that leverage SLAM outputs and enforce long-term geometric consistency; (iii) careful system design that avoids degenerate cases in online depth refinement. We extensively evaluate GeoRefine on multiple public datasets and reach as low as $5\%$ absolute relative depth errors.



### Cross-modal Representation Learning for Zero-shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.01657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01657v1)
- **Published**: 2022-05-03 17:39:27+00:00
- **Updated**: 2022-05-03 17:39:27+00:00
- **Authors**: Chung-Ching Lin, Kevin Lin, Linjie Li, Lijuan Wang, Zicheng Liu
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: We present a cross-modal Transformer-based framework, which jointly encodes video data and text labels for zero-shot action recognition (ZSAR). Our model employs a conceptually new pipeline by which visual representations are learned in conjunction with visual-semantic associations in an end-to-end manner. The model design provides a natural mechanism for visual and semantic representations to be learned in a shared knowledge space, whereby it encourages the learned visual embedding to be discriminative and more semantically consistent. In zero-shot inference, we devise a simple semantic transfer scheme that embeds semantic relatedness information between seen and unseen classes to composite unseen visual prototypes. Accordingly, the discriminative features in the visual structure could be preserved and exploited to alleviate the typical zero-shot issues of information loss, semantic gap, and the hubness problem. Under a rigorous zero-shot setting of not pre-training on additional datasets, the experiment results show our model considerably improves upon the state of the arts in ZSAR, reaching encouraging top-1 accuracy on UCF101, HMDB51, and ActivityNet benchmark datasets. Code will be made available.



### DANBO: Disentangled Articulated Neural Body Representations via Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.01666v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01666v2)
- **Published**: 2022-05-03 17:56:46+00:00
- **Updated**: 2022-10-11 22:40:22+00:00
- **Authors**: Shih-Yang Su, Timur Bagautdinov, Helge Rhodin
- **Comment**: ECCV 2022. Project website: https://lemonatsu.github.io/danbo
- **Journal**: None
- **Summary**: Deep learning greatly improved the realism of animatable human models by learning geometry and appearance from collections of 3D scans, template meshes, and multi-view imagery. High-resolution models enable photo-realistic avatars but at the cost of requiring studio settings not available to end users. Our goal is to create avatars directly from raw images without relying on expensive studio setups and surface tracking. While a few such approaches exist, those have limited generalization capabilities and are prone to learning spurious (chance) correlations between irrelevant body parts, resulting in implausible deformations and missing body parts on unseen poses. We introduce a three-stage method that induces two inductive biases to better disentangled pose-dependent deformation. First, we model correlations of body parts explicitly with a graph neural network. Second, to further reduce the effect of chance correlations, we introduce localized per-bone features that use a factorized volumetric representation and a new aggregation function. We demonstrate that our model produces realistic body shapes under challenging unseen poses and shows high-quality image synthesis. Our proposed representation strikes a better trade-off between model capacity, expressiveness, and robustness than competing methods. Project website: https://lemonatsu.github.io/danbo.



### End-to-End Visual Editing with a Generatively Pre-Trained Artist
- **Arxiv ID**: http://arxiv.org/abs/2205.01668v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01668v1)
- **Published**: 2022-05-03 17:59:30+00:00
- **Updated**: 2022-05-03 17:59:30+00:00
- **Authors**: Andrew Brown, Cheng-Yang Fu, Omkar Parkhi, Tamara L. Berg, Andrea Vedaldi
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the targeted image editing problem: blending a region in a source image with a driver image that specifies the desired change. Differently from prior works, we solve this problem by learning a conditional probability distribution of the edits, end-to-end. Training such a model requires addressing a fundamental technical challenge: the lack of example edits for training. To this end, we propose a self-supervised approach that simulates edits by augmenting off-the-shelf images in a target domain. The benefits are remarkable: implemented as a state-of-the-art auto-regressive transformer, our approach is simple, sidesteps difficulties with previous methods based on GAN-like priors, obtains significantly better edits, and is efficient. Furthermore, we show that different blending effects can be learned by an intuitive control of the augmentation process, with no other changes required to the model architecture. We demonstrate the superiority of this approach across several datasets in extensive quantitative and qualitative experiments, including human studies, significantly outperforming prior work.



### End2End Multi-View Feature Matching using Differentiable Pose Optimization
- **Arxiv ID**: http://arxiv.org/abs/2205.01694v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01694v2)
- **Published**: 2022-05-03 18:00:01+00:00
- **Updated**: 2022-11-28 09:18:44+00:00
- **Authors**: Barbara Roessle, Matthias Nießner
- **Comment**: Video: https://youtu.be/5bFIIDOHRZY
- **Journal**: None
- **Summary**: Erroneous feature matches have severe impact on subsequent camera pose estimation and often require additional, time-costly measures, like RANSAC, for outlier rejection. Our method tackles this challenge by addressing feature matching and pose optimization jointly. To this end, we propose a graph attention network to predict image correspondences along with confidence weights. The resulting matches serve as weighted constraints in a differentiable pose estimation. Training feature matching with gradients from pose optimization naturally learns to down-weight outliers and boosts pose estimation on image pairs compared to SuperGlue by 6.7% on ScanNet. At the same time, it reduces the pose estimation time by over 50% and renders RANSAC iterations unnecessary. Moreover, we integrate information from multiple views by spanning the graph across multiple frames to predict the matches all at once. Multi-view matching combined with end-to-end training improves the pose estimation metrics on Matterport3D by 18.8% compared to SuperGlue.



### Object Class Aware Video Anomaly Detection through Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2205.01706v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.01706v1)
- **Published**: 2022-05-03 18:04:27+00:00
- **Updated**: 2022-05-03 18:04:27+00:00
- **Authors**: Mohammad Baradaran, Robert Bergevin
- **Comment**: Accepted to CRV2022
- **Journal**: None
- **Summary**: Semi-supervised video anomaly detection (VAD) methods formulate the task of anomaly detection as detection of deviations from the learned normal patterns. Previous works in the field (reconstruction or prediction-based methods) suffer from two drawbacks: 1) They focus on low-level features, and they (especially holistic approaches) do not effectively consider the object classes. 2) Object-centric approaches neglect some of the context information (such as location). To tackle these challenges, this paper proposes a novel two-stream object-aware VAD method that learns the normal appearance and motion patterns through image translation tasks. The appearance branch translates the input image to the target semantic segmentation map produced by Mask-RCNN, and the motion branch associates each frame with its expected optical flow magnitude. Any deviation from the expected appearance or motion in the inference stage shows the degree of potential abnormality. We evaluated our proposed method on the ShanghaiTech, UCSD-Ped1, and UCSD-Ped2 datasets and the results show competitive performance compared with state-of-the-art works. Most importantly, the results show that, as significant improvements to previous methods, detections by our method are completely explainable and anomalies are localized accurately in the frames.



### Detection of Propaganda Techniques in Visuo-Lingual Metaphor in Memes
- **Arxiv ID**: http://arxiv.org/abs/2205.02937v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.02937v1)
- **Published**: 2022-05-03 18:33:27+00:00
- **Updated**: 2022-05-03 18:33:27+00:00
- **Authors**: Sunil Gundapu, Radhika Mamidi
- **Comment**: Paper accepted at 2nd International Conference on Machine Learning
  Techniques and Data Science (MLDS 2021)
- **Journal**: None
- **Summary**: The exponential rise of social media networks has allowed the production, distribution, and consumption of data at a phenomenal rate. Moreover, the social media revolution has brought a unique phenomenon to social media platforms called Internet memes. Internet memes are one of the most popular contents used on social media, and they can be in the form of images with a witty, catchy, or satirical text description. In this paper, we are dealing with propaganda that is often seen in Internet memes in recent times. Propaganda is communication, which frequently includes psychological and rhetorical techniques to manipulate or influence an audience to act or respond as the propagandist wants. To detect propaganda in Internet memes, we propose a multimodal deep learning fusion system that fuses the text and image feature representations and outperforms individual models based solely on either text or image modalities.



### In Defense of Image Pre-Training for Spatiotemporal Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.01721v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01721v2)
- **Published**: 2022-05-03 18:45:44+00:00
- **Updated**: 2022-08-01 18:24:38+00:00
- **Authors**: Xianhang Li, Huiyu Wang, Chen Wei, Jieru Mei, Alan Yuille, Yuyin Zhou, Cihang Xie
- **Comment**: Published as a conference paper at ECCV 2022
- **Journal**: None
- **Summary**: Image pre-training, the current de-facto paradigm for a wide range of visual tasks, is generally less favored in the field of video recognition. By contrast, a common strategy is to directly train with spatiotemporal convolutional neural networks (CNNs) from scratch. Nonetheless, interestingly, by taking a closer look at these from-scratch learned CNNs, we note there exist certain 3D kernels that exhibit much stronger appearance modeling ability than others, arguably suggesting appearance information is already well disentangled in learning. Inspired by this observation, we hypothesize that the key to effectively leveraging image pre-training lies in the decomposition of learning spatial and temporal features, and revisiting image pre-training as the appearance prior to initializing 3D kernels. In addition, we propose Spatial-Temporal Separable (STS) convolution, which explicitly splits the feature channels into spatial and temporal groups, to further enable a more thorough decomposition of spatiotemporal features for fine-tuning 3D CNNs. Our experiments show that simply replacing 3D convolution with STS notably improves a wide range of 3D CNNs without increasing parameters and computation on both Kinetics-400 and Something-Something V2. Moreover, this new training pipeline consistently achieves better results on video recognition with significant speedup. For instance, we achieve +0.6% top-1 of Slowfast on Kinetics-400 over the strong 256-epoch 128-GPU baseline while fine-tuning for only 50 epochs with 4 GPUs. The code and models are available at https://github.com/UCSC-VLAA/Image-Pretraining-for-Video.



### License Plate Privacy in Collaborative Visual Analysis of Traffic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2205.01724v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01724v1)
- **Published**: 2022-05-03 18:47:27+00:00
- **Updated**: 2022-05-03 18:47:27+00:00
- **Authors**: Saeed Ranjbar Alvar, Korcan Uyanik, Ivan V. Bajić
- **Comment**: submitted to IEEE MIPR'22
- **Journal**: None
- **Summary**: Traffic scene analysis is important for emerging technologies such as smart traffic management and autonomous vehicles. However, such analysis also poses potential privacy threats. For example, a system that can recognize license plates may construct patterns of behavior of the corresponding vehicles' owners and use that for various illegal purposes. In this paper we present a system that enables traffic scene analysis while at the same time preserving license plate privacy. The system is based on a multi-task model whose latent space is selectively compressed depending on the amount of information the specific features carry about analysis tasks and private information. Effectiveness of the proposed method is illustrated by experiments on the Cityscapes dataset, for which we also provide license plate annotations.



### Application of belief functions to medical image segmentation: A review
- **Arxiv ID**: http://arxiv.org/abs/2205.01733v4
- **DOI**: 10.1016/j.inffus.2022.11.008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01733v4)
- **Published**: 2022-05-03 19:06:45+00:00
- **Updated**: 2022-12-05 21:28:45+00:00
- **Authors**: Ling Huang, Su Ruan, Thierry Denoeux
- **Comment**: Accepted by Information fusion
- **Journal**: Information Fusion, Volume 91, March 2023, Pages 737-756
- **Summary**: The investigation of uncertainty is of major importance in risk-critical applications, such as medical image segmentation. Belief function theory, a formal framework for uncertainty analysis and multiple evidence fusion, has made significant contributions to medical image segmentation, especially since the development of deep learning. In this paper, we provide an introduction to the topic of medical image segmentation methods using belief function theory. We classify the methods according to the fusion step and explain how information with uncertainty or imprecision is modeled and fused with belief function theory. In addition, we discuss the challenges and limitations of present belief function-based medical image segmentation and propose orientations for future research. Future research could investigate both belief function theory and deep learning to achieve more promising and reliable segmentation results.



### Comparison of CoModGANs, LaMa and GLIDE for Art Inpainting- Completing M.C Escher's Print Gallery
- **Arxiv ID**: http://arxiv.org/abs/2205.01741v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01741v1)
- **Published**: 2022-05-03 19:26:05+00:00
- **Updated**: 2022-05-03 19:26:05+00:00
- **Authors**: Lucia Cipolina-Kun, Simone Caenazzo, Gaston Mazzei
- **Comment**: CVPR-NITRE workshop 2022
- **Journal**: None
- **Summary**: Digital art restoration has benefited from inpainting models to correct the degradation or missing sections of a painting. This work compares three current state-of-the art models for inpainting of large missing regions. We provide qualitative and quantitative comparison of the performance by CoModGANs, LaMa and GLIDE in inpainting of blurry and missing sections of images. We use Escher's incomplete painting Print Gallery as our test study since it presents several of the challenges commonly present in restorative inpainting.



### Data-Consistent Non-Cartesian Deep Subspace Learning for Efficient Dynamic MR Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2205.01770v1
- **DOI**: 10.1109/ISBI52829.2022.9761497
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01770v1)
- **Published**: 2022-05-03 20:37:21+00:00
- **Updated**: 2022-05-03 20:37:21+00:00
- **Authors**: Zihao Chen, Yuhua Chen, Yibin Xie, Debiao Li, Anthony G. Christodoulou
- **Comment**: Accepted by IEEE ISBI 2022
- **Journal**: None
- **Summary**: Non-Cartesian sampling with subspace-constrained image reconstruction is a popular approach to dynamic MRI, but slow iterative reconstruction limits its clinical application. Data-consistent (DC) deep learning can accelerate reconstruction with good image quality, but has not been formulated for non-Cartesian subspace imaging. In this study, we propose a DC non-Cartesian deep subspace learning framework for fast, accurate dynamic MR image reconstruction. Four novel DC formulations are developed and evaluated: two gradient decent approaches, a directly solved approach, and a conjugate gradient approach. We applied a U-Net model with and without DC layers to reconstruct T1-weighted images for cardiac MR Multitasking (an advanced multidimensional imaging method), comparing our results to the iteratively reconstructed reference. Experimental results show that the proposed framework significantly improves reconstruction accuracy over the U-Net model without DC, while significantly accelerating the reconstruction over conventional iterative reconstruction.



### The Brazilian Data at Risk in the Age of AI?
- **Arxiv ID**: http://arxiv.org/abs/2205.01772v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2205.01772v3)
- **Published**: 2022-05-03 20:41:21+00:00
- **Updated**: 2022-12-14 12:28:07+00:00
- **Authors**: Raoni F. da S. Teixeira, Rafael B. Januzi, Fabio A. Faria
- **Comment**: 8 pages in Portuguese and 5 figures, Top 3 among the best papers at
  the ENIAC 2022
- **Journal**: None
- **Summary**: Advances in image processing and analysis as well as machine learning techniques have contributed to the use of biometric recognition systems in daily people tasks. These tasks range from simple access to mobile devices to tagging friends in photos shared on social networks and complex financial operations on self-service devices for banking transactions. In China, the use of these systems goes beyond personal use becoming a country's government policy with the objective of monitoring the behavior of its population. On July 05th 2021, the Brazilian government announced acquisition of a biometric recognition system to be used nationwide. In the opposite direction to China, Europe and some American cities have already started the discussion about the legality of using biometric systems in public places, even banning this practice in their territory. In order to open a deeper discussion about the risks and legality of using these systems, this work exposes the vulnerabilities of biometric recognition systems, focusing its efforts on the face modality. Furthermore, it shows how it is possible to fool a biometric system through a well-known presentation attack approach in the literature called morphing. Finally, a list of ten concerns was created to start the discussion about the security of citizen data and data privacy law in the Age of Artificial Intelligence (AI).



### Deep Multi-Scale U-Net Architecture and Label-Noise Robust Training Strategies for Histopathological Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.01777v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01777v2)
- **Published**: 2022-05-03 21:00:44+00:00
- **Updated**: 2022-08-13 21:31:08+00:00
- **Authors**: Nikhil Cherian Kurian, Amit Lohan, Gregory Verghese, Nimish Dharamshi, Swati Meena, Mengyuan Li, Fangfang Liu, Cheryl Gillet, Swapnil Rane, Anita Grigoriadis, Amit Sethi
- **Comment**: 12 pages, 4 figures , 2 tables ,Added Attention UNet Results, Added
  Sinus and Germinal Center overlay images, Modified paper format, Fixed Title
  typos
- **Journal**: None
- **Summary**: Although the U-Net architecture has been extensively used for segmentation of medical images, we address two of its shortcomings in this work. Firstly, the accuracy of vanilla U-Net degrades when the target regions for segmentation exhibit significant variations in shape and size. Even though the U-Net already possesses some capability to analyze features at various scales, we propose to explicitly add multi-scale feature maps in each convolutional module of the U-Net encoder to improve segmentation of histology images. Secondly, the accuracy of a U-Net model also suffers when the annotations for supervised learning are noisy or incomplete. This can happen due to the inherent difficulty for a human expert to identify and delineate all instances of specific pathology very precisely and accurately. We address this challenge by introducing auxiliary confidence maps that emphasize less on the boundaries of the given target regions. Further, we utilize the bootstrapping properties of the deep network to address the missing annotation problem intelligently. In our experiments on a private dataset of breast cancer lymph nodes, where the primary task was to segment germinal centres and sinus histiocytosis, we observed substantial improvement over a U-Net baseline based on the two proposed augmentations.



### Synthesized Speech Detection Using Convolutional Transformer-Based Spectrogram Analysis
- **Arxiv ID**: http://arxiv.org/abs/2205.01800v1
- **DOI**: 10.1109/IEEECONF53345.2021.9723142
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2205.01800v1)
- **Published**: 2022-05-03 22:05:35+00:00
- **Updated**: 2022-05-03 22:05:35+00:00
- **Authors**: Emily R. Bartusiak, Edward J. Delp
- **Comment**: Accepted to the 2021 IEEE Asilomar Conference on Signals, Systems,
  and Computers
- **Journal**: IEEE Asilomar Conference on Signals, Systems, and Computers, pp.
  1426-1430, October 2021, Asilomar, CA
- **Summary**: Synthesized speech is common today due to the prevalence of virtual assistants, easy-to-use tools for generating and modifying speech signals, and remote work practices. Synthesized speech can also be used for nefarious purposes, including creating a purported speech signal and attributing it to someone who did not speak the content of the signal. We need methods to detect if a speech signal is synthesized. In this paper, we analyze speech signals in the form of spectrograms with a Compact Convolutional Transformer (CCT) for synthesized speech detection. A CCT utilizes a convolutional layer that introduces inductive biases and shared weights into a network, allowing a transformer architecture to perform well with fewer data samples used for training. The CCT uses an attention mechanism to incorporate information from all parts of a signal under analysis. Trained on both genuine human voice signals and synthesized human voice signals, we demonstrate that our CCT approach successfully differentiates between genuine and synthesized speech signals.



### Splicing Detection and Localization In Satellite Imagery Using Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/2205.01805v1
- **DOI**: 10.1109/MIPR.2019.00024
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01805v1)
- **Published**: 2022-05-03 22:25:48+00:00
- **Updated**: 2022-05-03 22:25:48+00:00
- **Authors**: Emily R. Bartusiak, Sri Kalyan Yarlagadda, David Güera, Paolo Bestagini, Stefano Tubaro, Fengqing M. Zhu, Edward J. Delp
- **Comment**: Accepted to the 2019 IEEE Conference on Multimedia Information
  Processing and Retrieval (MIPR)
- **Journal**: IEEE Conference on Multimedia Information Processing and
  Retrieval, pp. 91-96, March 2019, San Jose, CA
- **Summary**: The widespread availability of image editing tools and improvements in image processing techniques allow image manipulation to be very easy. Oftentimes, easy-to-use yet sophisticated image manipulation tools yields distortions/changes imperceptible to the human observer. Distribution of forged images can have drastic ramifications, especially when coupled with the speed and vastness of the Internet. Therefore, verifying image integrity poses an immense and important challenge to the digital forensic community. Satellite images specifically can be modified in a number of ways, including the insertion of objects to hide existing scenes and structures. In this paper, we describe the use of a Conditional Generative Adversarial Network (cGAN) to identify the presence of such spliced forgeries within satellite images. Additionally, we identify their locations and shapes. Trained on pristine and falsified images, our method achieves high success on these detection and localization objectives.



### Frequency Domain-Based Detection of Generated Audio
- **Arxiv ID**: http://arxiv.org/abs/2205.01806v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2205.01806v1)
- **Published**: 2022-05-03 22:27:51+00:00
- **Updated**: 2022-05-03 22:27:51+00:00
- **Authors**: Emily R. Bartusiak, Edward J. Delp
- **Comment**: Accepted to the 2021 Media Watermarking, Security, and Forensics
  Conference, IS&T Electronic Imaging Symposium (EI)
- **Journal**: Proceedings of the Media Watermarking, Security, and Forensics
  Conference, IS&T Electronic Imaging Symposium, pp 273-1 - 273-7, January
  2021, Burlingame, CA
- **Summary**: Attackers may manipulate audio with the intent of presenting falsified reports, changing an opinion of a public figure, and winning influence and power. The prevalence of inauthentic multimedia continues to rise, so it is imperative to develop a set of tools that determines the legitimacy of media. We present a method that analyzes audio signals to determine whether they contain real human voices or fake human voices (i.e., voices generated by neural acoustic and waveform models). Instead of analyzing the audio signals directly, the proposed approach converts the audio signals into spectrogram images displaying frequency, intensity, and temporal content and evaluates them with a Convolutional Neural Network (CNN). Trained on both genuine human voice signals and synthesized voice signals, we show our approach achieves high accuracy on this classification task.



### Assessing Dataset Bias in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2205.01811v1
- **DOI**: 10.13140/RG.2.2.19950.89924
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01811v1)
- **Published**: 2022-05-03 22:45:49+00:00
- **Updated**: 2022-05-03 22:45:49+00:00
- **Authors**: Athiya Deviyani
- **Comment**: 51 pages
- **Journal**: None
- **Summary**: A biased dataset is a dataset that generally has attributes with an uneven class distribution. These biases have the tendency to propagate to the models that train on them, often leading to a poor performance in the minority class. In this project, we will explore the extent to which various data augmentation methods alleviate intrinsic biases within the dataset. We will apply several augmentation techniques on a sample of the UTKFace dataset, such as undersampling, geometric transformations, variational autoencoders (VAEs), and generative adversarial networks (GANs). We then trained a classifier for each of the augmented datasets and evaluated their performance on the native test set and on external facial recognition datasets. We have also compared their performance to the state-of-the-art attribute classifier trained on the FairFace dataset. Through experimentation, we were able to find that training the model on StarGAN-generated images led to the best overall performance. We also found that training on geometrically transformed images lead to a similar performance with a much quicker training time. Additionally, the best performing models also exhibit a uniform performance across the classes within each attribute. This signifies that the model was also able to mitigate the biases present in the baseline model that was trained on the original training set. Finally, we were able to show that our model has a better overall performance and consistency on age and ethnicity classification on multiple datasets when compared with the FairFace model. Our final model has an accuracy on the UTKFace test set of 91.75%, 91.30%, and 87.20% for the gender, age, and ethnicity attribute respectively, with a standard deviation of less than 0.1 between the accuracies of the classes of each attribute.



### Diverse Image Captioning with Grounded Style
- **Arxiv ID**: http://arxiv.org/abs/2205.01813v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01813v1)
- **Published**: 2022-05-03 22:57:59+00:00
- **Updated**: 2022-05-03 22:57:59+00:00
- **Authors**: Franz Klein, Shweta Mahajan, Stefan Roth
- **Comment**: In the 43rd DAGM German Conference on Pattern Recognition (GCPR) 2021
- **Journal**: In Proceedings of the German Conference on Pattern Recognition
  (GCPR), Ed. by C. Bauckhage, J. Gall, and A. G. Schwing, Vol. 13024, Lecture
  Notes in Computer Science, Springer, 2021, pp. 421-436
- **Summary**: Stylized image captioning as presented in prior work aims to generate captions that reflect characteristics beyond a factual description of the scene composition, such as sentiments. Such prior work relies on given sentiment identifiers, which are used to express a certain global style in the caption, e.g. positive or negative, however without taking into account the stylistic content of the visual scene. To address this shortcoming, we first analyze the limitations of current stylized captioning datasets and propose COCO attribute-based augmentations to obtain varied stylized captions from COCO annotations. Furthermore, we encode the stylized information in the latent space of a Variational Autoencoder; specifically, we leverage extracted image attributes to explicitly structure its sequential latent space according to different localized style characteristics. Our experiments on the Senticap and COCO datasets show the ability of our approach to generate accurate captions with diversity in styles that are grounded in the image.



### i-Code: An Integrative and Composable Multimodal Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/2205.01818v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2205.01818v2)
- **Published**: 2022-05-03 23:38:50+00:00
- **Updated**: 2022-05-05 06:35:23+00:00
- **Authors**: Ziyi Yang, Yuwei Fang, Chenguang Zhu, Reid Pryzant, Dongdong Chen, Yu Shi, Yichong Xu, Yao Qian, Mei Gao, Yi-Ling Chen, Liyang Lu, Yujia Xie, Robert Gmyr, Noel Codella, Naoyuki Kanda, Bin Xiao, Lu Yuan, Takuya Yoshioka, Michael Zeng, Xuedong Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Human intelligence is multimodal; we integrate visual, linguistic, and acoustic signals to maintain a holistic worldview. Most current pretraining methods, however, are limited to one or two modalities. We present i-Code, a self-supervised pretraining framework where users may flexibly combine the modalities of vision, speech, and language into unified and general-purpose vector representations. In this framework, data from each modality are first given to pretrained single-modality encoders. The encoder outputs are then integrated with a multimodal fusion network, which uses novel attention mechanisms and other architectural innovations to effectively combine information from the different modalities. The entire system is pretrained end-to-end with new objectives including masked modality unit modeling and cross-modality contrastive learning. Unlike previous research using only video for pretraining, the i-Code framework can dynamically process single, dual, and triple-modality data during training and inference, flexibly projecting different combinations of modalities into a single representation space. Experimental results demonstrate how i-Code can outperform state-of-the-art techniques on five video understanding tasks and the GLUE NLP benchmark, improving by as much as 11% and demonstrating the power of integrative multimodal pretraining.



