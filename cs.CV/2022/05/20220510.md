# Arxiv Papers in cs.CV on 2022-05-10
### CoDo: Contrastive Learning with Downstream Background Invariance for Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.04617v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.04617v1)
- **Published**: 2022-05-10 01:26:15+00:00
- **Updated**: 2022-05-10 01:26:15+00:00
- **Authors**: Bing Zhao, Jun Li, Hong Zhu
- **Comment**: CVPR2022 workshop
- **Journal**: None
- **Summary**: The prior self-supervised learning researches mainly select image-level instance discrimination as pretext task. It achieves a fantastic classification performance that is comparable to supervised learning methods. However, with degraded transfer performance on downstream tasks such as object detection. To bridge the performance gap, we propose a novel object-level self-supervised learning method, called Contrastive learning with Downstream background invariance (CoDo). The pretext task is converted to focus on instance location modeling for various backgrounds, especially for downstream datasets. The ability of background invariance is considered vital for object detection. Firstly, a data augmentation strategy is proposed to paste the instances onto background images, and then jitter the bounding box to involve background information. Secondly, we implement architecture alignment between our pretraining network and the mainstream detection pipelines. Thirdly, hierarchical and multi views contrastive learning is designed to improve performance of visual representation learning. Experiments on MSCOCO demonstrate that the proposed CoDo with common backbones, ResNet50-FPN, yields strong transfer learning results for object detection.



### KEMP: Keyframe-Based Hierarchical End-to-End Deep Model for Long-Term Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2205.04624v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2205.04624v1)
- **Published**: 2022-05-10 02:10:49+00:00
- **Updated**: 2022-05-10 02:10:49+00:00
- **Authors**: Qiujing Lu, Weiqiao Han, Jeffrey Ling, Minfa Wang, Haoyu Chen, Balakrishnan Varadarajan, Paul Covington
- **Comment**: Accepted at the 39th IEEE Conference on Robotics and Automation
  (ICRA), 2022
- **Journal**: None
- **Summary**: Predicting future trajectories of road agents is a critical task for autonomous driving. Recent goal-based trajectory prediction methods, such as DenseTNT and PECNet, have shown good performance on prediction tasks on public datasets. However, they usually require complicated goal-selection algorithms and optimization. In this work, we propose KEMP, a hierarchical end-to-end deep learning framework for trajectory prediction. At the core of our framework is keyframe-based trajectory prediction, where keyframes are representative states that trace out the general direction of the trajectory. KEMP first predicts keyframes conditioned on the road context, and then fills in intermediate states conditioned on the keyframes and the road context. Under our general framework, goal-conditioned methods are special cases in which the number of keyframes equal to one. Unlike goal-conditioned methods, our keyframe predictor is learned automatically and does not require hand-crafted goal-selection algorithms. We evaluate our model on public benchmarks and our model ranked 1st on Waymo Open Motion Dataset Leaderboard (as of September 1, 2021).



### Using Frequency Attention to Make Adversarial Patch Powerful Against Person Detector
- **Arxiv ID**: http://arxiv.org/abs/2205.04638v2
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2205.04638v2)
- **Published**: 2022-05-10 02:54:23+00:00
- **Updated**: 2022-05-11 13:41:29+00:00
- **Authors**: Xiaochun Lei, Chang Lu, Zetao Jiang, Zhaoting Gong, Xiang Cai, Linjun Lu
- **Comment**: 10pages, 4 figures
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are vulnerable to adversarial attacks. In particular, object detectors may be attacked by applying a particular adversarial patch to the image. However, because the patch shrinks during preprocessing, most existing approaches that employ adversarial patches to attack object detectors would diminish the attack success rate on small and medium targets. This paper proposes a Frequency Module(FRAN), a frequency-domain attention module for guiding patch generation. This is the first study to introduce frequency domain attention to optimize the attack capabilities of adversarial patches. Our method increases the attack success rates of small and medium targets by 4.18% and 3.89%, respectively, over the state-of-the-art attack method for fooling the human detector while assaulting YOLOv3 without reducing the attack success rate of big targets.



### STDC-MA Network for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.04639v2
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2205.04639v2)
- **Published**: 2022-05-10 02:59:39+00:00
- **Updated**: 2022-05-11 00:38:38+00:00
- **Authors**: Xiaochun Lei, Linjun Lu, Zetao Jiang, Zhaoting Gong, Chang Lu, Jiaming Liang
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Semantic segmentation is applied extensively in autonomous driving and intelligent transportation with methods that highly demand spatial and semantic information. Here, an STDC-MA network is proposed to meet these demands. First, the STDC-Seg structure is employed in STDC-MA to ensure a lightweight and efficient structure. Subsequently, the feature alignment module (FAM) is applied to understand the offset between high-level and low-level features, solving the problem of pixel offset related to upsampling on the high-level feature map. Our approach implements the effective fusion between high-level features and low-level features. A hierarchical multiscale attention mechanism is adopted to reveal the relationship among attention regions from two different input sizes of one image. Through this relationship, regions receiving much attention are integrated into the segmentation results, thereby reducing the unfocused regions of the input image and improving the effective utilization of multiscale features. STDC- MA maintains the segmentation speed as an STDC-Seg network while improving the segmentation accuracy of small objects. STDC-MA was verified on the verification set of Cityscapes. The segmentation result of STDC-MA attained 76.81% mIOU with the input of 0.5x scale, 3.61% higher than STDC-Seg.



### Spatial Monitoring and Insect Behavioural Analysis Using Computer Vision for Precision Pollination
- **Arxiv ID**: http://arxiv.org/abs/2205.04675v2
- **DOI**: 10.1007/s11263-022-01715-4
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2205.04675v2)
- **Published**: 2022-05-10 05:11:28+00:00
- **Updated**: 2022-11-11 13:17:11+00:00
- **Authors**: Malika Nisal Ratnayake, Don Chathurika Amarathunga, Asaduz Zaman, Adrian G. Dyer, Alan Dorin
- **Comment**: Accepted to be published in the International Journal of Computer
  Vision (Acceptance date: 7th November 2022) 21 pages, 8 figures
- **Journal**: International Journal of Computer Vision (2022)
- **Summary**: Insects are the most important global pollinator of crops and play a key role in maintaining the sustainability of natural ecosystems. Insect pollination monitoring and management are therefore essential for improving crop production and food security. Computer vision facilitated pollinator monitoring can intensify data collection over what is feasible using manual approaches. The new data it generates may provide a detailed understanding of insect distributions and facilitate fine-grained analysis sufficient to predict their pollination efficacy and underpin precision pollination. Current computer vision facilitated insect tracking in complex outdoor environments is restricted in spatial coverage and often constrained to a single insect species. This limits its relevance to agriculture. Therefore, in this article we introduce a novel system to facilitate markerless data capture for insect counting, insect motion tracking, behaviour analysis and pollination prediction across large agricultural areas. Our system is comprised of edge computing multi-point video recording, offline automated multispecies insect counting, tracking and behavioural analysis. We implement and test our system on a commercial berry farm to demonstrate its capabilities. Our system successfully tracked four insect varieties, at nine monitoring stations within polytunnels, obtaining an F-score above 0.8 for each variety. The system enabled calculation of key metrics to assess the relative pollination impact of each insect variety. With this technological advancement, detailed, ongoing data collection for precision pollination becomes achievable. This is important to inform growers and apiarists managing crop pollination, as it allows data-driven decisions to be made to improve food production and food security.



### UNITS: Unsupervised Intermediate Training Stage for Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.04683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04683v1)
- **Published**: 2022-05-10 05:34:58+00:00
- **Updated**: 2022-05-10 05:34:58+00:00
- **Authors**: Youhui Guo, Yu Zhou, Xugong Qin, Enze Xie, Weiping Wang
- **Comment**: Accepted by ICME 2022
- **Journal**: None
- **Summary**: Recent scene text detection methods are almost based on deep learning and data-driven. Synthetic data is commonly adopted for pre-training due to expensive annotation cost. However, there are obvious domain discrepancies between synthetic data and real-world data. It may lead to sub-optimal performance to directly adopt the model initialized by synthetic data in the fine-tuning stage. In this paper, we propose a new training paradigm for scene text detection, which introduces an \textbf{UN}supervised \textbf{I}ntermediate \textbf{T}raining \textbf{S}tage (UNITS) that builds a buffer path to real-world data and can alleviate the gap between the pre-training stage and fine-tuning stage. Three training strategies are further explored to perceive information from real-world data in an unsupervised way. With UNITS, scene text detectors are improved without introducing any parameters and computations during inference. Extensive experimental results show consistent performance improvements on three public datasets.



### OTFPF: Optimal Transport-Based Feature Pyramid Fusion Network for Brain Age Estimation with 3D Overlapped ConvNeXt
- **Arxiv ID**: http://arxiv.org/abs/2205.04684v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04684v2)
- **Published**: 2022-05-10 05:39:35+00:00
- **Updated**: 2022-05-11 04:30:32+00:00
- **Authors**: Yu Fu, Yanyan Huang, Yalin Wang, Shunjie Dong, Le Xue, Xunzhao Yin, Qianqian Yang, Yiyu Shi, Cheng Zhuo
- **Comment**: None
- **Journal**: None
- **Summary**: Chronological age of healthy brain is able to be predicted using deep neural networks from T1-weighted magnetic resonance images (T1 MRIs), and the predicted brain age could serve as an effective biomarker for detecting aging-related diseases or disorders. In this paper, we propose an end-to-end neural network architecture, referred to as optimal transport based feature pyramid fusion (OTFPF) network, for the brain age estimation with T1 MRIs. The OTFPF consists of three types of modules: Optimal Transport based Feature Pyramid Fusion (OTFPF) module, 3D overlapped ConvNeXt (3D OL-ConvNeXt) module and fusion module. These modules strengthen the OTFPF network's understanding of each brain's semi-multimodal and multi-level feature pyramid information, and significantly improve its estimation performances. Comparing with recent state-of-the-art models, the proposed OTFPF converges faster and performs better. The experiments with 11,728 MRIs aged 3-97 years show that OTFPF network could provide accurate brain age estimation, yielding mean absolute error (MAE) of 2.097, Pearson's correlation coefficient (PCC) of 0.993 and Spearman's rank correlation coefficient (SRCC) of 0.989, between the estimated and chronological ages. Widespread quantitative experiments and ablation experiments demonstrate the superiority and rationality of OTFPF network. The codes and implement details will be released on GitHub: https://github.com/ZJU-Brain/OTFPF after final decision.



### An Asynchronous Event-Based Algorithm for Periodic Signals
- **Arxiv ID**: http://arxiv.org/abs/2205.04691v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP, 68W40, 68Q87, 60C05, F.2.2; I.4.8; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/2205.04691v3)
- **Published**: 2022-05-10 06:24:09+00:00
- **Updated**: 2023-07-24 20:56:50+00:00
- **Authors**: David El-Chai Ben-Ezra, Ron Arad, Ayelet Padowicz, Israel Tugendhaft
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Let $0\leq\tau_{1}\leq\tau_{2}\leq\cdots\leq\tau_{m}\leq1$, originated from a uniform distribution. Let also $\epsilon,\delta\in\mathbb{R}$, and $d\in\mathbb{N}$. What is the probability of having more than $d$ adjacent $\tau_{i}$-s pairs that the distance between them is $\delta$, up to an error $\epsilon$ ? In this paper we are going to show how this untreated theoretical probabilistic problem arises naturally from the motivation of analyzing a simple asynchronous algorithm for detection of signals with a known frequency, using the novel technology of an event camera.



### Automatic Detection of Microaneurysms in OCT Images Using Bag of Features
- **Arxiv ID**: http://arxiv.org/abs/2205.04695v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04695v1)
- **Published**: 2022-05-10 06:43:01+00:00
- **Updated**: 2022-05-10 06:43:01+00:00
- **Authors**: Elahe Sadat Kazemi Nasab, Ramin Almasi, Bijan Shoushtarian, Ehsan Golkar, Hossein Rabbani
- **Comment**: None
- **Journal**: None
- **Summary**: Diabetic Retinopathy (DR) caused by diabetes occurs as a result of changes in the retinal vessels and causes visual impairment. Microaneurysms (MAs) are the early clinical signs of DR, whose timely diagnosis can help detecting DR in the early stages of its development. It has been observed that MAs are more common in the inner retinal layers compared to the outer retinal layers in eyes suffering from DR. Optical Coherence Tomography (OCT) is a noninvasive imaging technique that provides a cross-sectional view of the retina and it has been used in recent years to diagnose many eye diseases. As a result, in this paper has attempted to identify areas with MA from normal areas of the retina using OCT images. This work is done using the dataset collected from FA and OCT images of 20 patients with DR. In this regard, firstly Fluorescein Angiography (FA) and OCT images were registered. Then the MA and normal areas were separated and the features of each of these areas were extracted using the Bag of Features (BOF) approach with Speeded-Up Robust Feature (SURF) descriptor. Finally, the classification process was performed using a multilayer perceptron network. For each of the criteria of accuracy, sensitivity, specificity, and precision, the obtained results were 96.33%, 97.33%, 95.4%, and 95.28%, respectively. Utilizing OCT images to detect MAsautomatically is a new idea and the results obtained as preliminary research in this field are promising .



### Efficient Burst Raw Denoising with Variance Stabilization and Multi-frequency Denoising Network
- **Arxiv ID**: http://arxiv.org/abs/2205.04721v1
- **DOI**: 10.1007/s11263-022-01627-3
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04721v1)
- **Published**: 2022-05-10 07:46:45+00:00
- **Updated**: 2022-05-10 07:46:45+00:00
- **Authors**: Dasong Li, Yi Zhang, Ka Lung Law, Xiaogang Wang, Hongwei Qin, Hongsheng Li
- **Comment**: Accepted for publication in International Journal of Computer Vision
- **Journal**: IJCV 2022
- **Summary**: With the growing popularity of smartphones, capturing high-quality images is of vital importance to smartphones. The cameras of smartphones have small apertures and small sensor cells, which lead to the noisy images in low light environment. Denoising based on a burst of multiple frames generally outperforms single frame denoising but with the larger compututional cost. In this paper, we propose an efficient yet effective burst denoising system. We adopt a three-stage design: noise prior integration, multi-frame alignment and multi-frame denoising. First, we integrate noise prior by pre-processing raw signals into a variance-stabilization space, which allows using a small-scale network to achieve competitive performance. Second, we observe that it is essential to adopt an explicit alignment for burst denoising, but it is not necessary to integrate a learning-based method to perform multi-frame alignment. Instead, we resort to a conventional and efficient alignment method and combine it with our multi-frame denoising network. At last, we propose a denoising strategy that processes multiple frames sequentially. Sequential denoising avoids filtering a large number of frames by decomposing multiple frames denoising into several efficient sub-network denoising. As for each sub-network, we propose an efficient multi-frequency denoising network to remove noise of different frequencies. Our three-stage design is efficient and shows strong performance on burst denoising. Experiments on synthetic and real raw datasets demonstrate that our method outperforms state-of-the-art methods, with less computational cost. Furthermore, the low complexity and high-quality performance make deployment on smartphones possible.



### Robust Medical Image Classification from Noisy Labeled Data with Global and Local Representation Guided Co-training
- **Arxiv ID**: http://arxiv.org/abs/2205.04723v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04723v1)
- **Published**: 2022-05-10 07:50:08+00:00
- **Updated**: 2022-05-10 07:50:08+00:00
- **Authors**: Cheng Xue, Lequan Yu, Pengfei Chen, Qi Dou, Pheng-Ann Heng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have achieved remarkable success in a wide variety of natural image and medical image computing tasks. However, these achievements indispensably rely on accurately annotated training data. If encountering some noisy-labeled images, the network training procedure would suffer from difficulties, leading to a sub-optimal classifier. This problem is even more severe in the medical image analysis field, as the annotation quality of medical images heavily relies on the expertise and experience of annotators. In this paper, we propose a novel collaborative training paradigm with global and local representation learning for robust medical image classification from noisy-labeled data to combat the lack of high quality annotated medical data. Specifically, we employ the self-ensemble model with a noisy label filter to efficiently select the clean and noisy samples. Then, the clean samples are trained by a collaborative training strategy to eliminate the disturbance from imperfect labeled samples. Notably, we further design a novel global and local representation learning scheme to implicitly regularize the networks to utilize noisy samples in a self-supervised manner. We evaluated our proposed robust learning strategy on four public medical image classification datasets with three types of label noise,ie,random noise, computer-generated label noise, and inter-observer variability noise. Our method outperforms other learning from noisy label methods and we also conducted extensive experiments to analyze each component of our method.



### Weakly-supervised segmentation of referring expressions
- **Arxiv ID**: http://arxiv.org/abs/2205.04725v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04725v2)
- **Published**: 2022-05-10 07:52:24+00:00
- **Updated**: 2022-05-12 07:17:56+00:00
- **Authors**: Robin Strudel, Ivan Laptev, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: Visual grounding localizes regions (boxes or segments) in the image corresponding to given referring expressions. In this work we address image segmentation from referring expressions, a problem that has so far only been addressed in a fully-supervised setting. A fully-supervised setup, however, requires pixel-wise supervision and is hard to scale given the expense of manual annotation. We therefore introduce a new task of weakly-supervised image segmentation from referring expressions and propose Text grounded semantic SEGgmentation (TSEG) that learns segmentation masks directly from image-level referring expressions without pixel-level annotations. Our transformer-based method computes patch-text similarities and guides the classification objective during training with a new multi-label patch assignment mechanism. The resulting visual grounding model segments image regions corresponding to given natural language expressions. Our approach TSEG demonstrates promising results for weakly-supervised referring expression segmentation on the challenging PhraseCut and RefCOCO datasets. TSEG also shows competitive performance when evaluated in a zero-shot setting for semantic segmentation on Pascal VOC.



### Spatio-Temporal Transformer for Dynamic Facial Expression Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2205.04749v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.04749v1)
- **Published**: 2022-05-10 08:47:15+00:00
- **Updated**: 2022-05-10 08:47:15+00:00
- **Authors**: Fuyan Ma, Bin Sun, Shutao Li
- **Comment**: None
- **Journal**: None
- **Summary**: Previous methods for dynamic facial expression in the wild are mainly based on Convolutional Neural Networks (CNNs), whose local operations ignore the long-range dependencies in videos. To solve this problem, we propose the spatio-temporal Transformer (STT) to capture discriminative features within each frame and model contextual relationships among frames. Spatio-temporal dependencies are captured and integrated by our unified Transformer. Specifically, given an image sequence consisting of multiple frames as input, we utilize the CNN backbone to translate each frame into a visual feature sequence. Subsequently, the spatial attention and the temporal attention within each block are jointly applied for learning spatio-temporal representations at the sequence level. In addition, we propose the compact softmax cross entropy loss to further encourage the learned features have the minimum intra-class distance and the maximum inter-class distance. Experiments on two in-the-wild dynamic facial expression datasets (i.e., DFEW and AFEW) indicate that our method provides an effective way to make use of the spatial and temporal dependencies for dynamic facial expression recognition. The source code and the training logs will be made publicly available.



### WG-VITON: Wearing-Guide Virtual Try-On for Top and Bottom Clothes
- **Arxiv ID**: http://arxiv.org/abs/2205.04759v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04759v1)
- **Published**: 2022-05-10 09:09:02+00:00
- **Updated**: 2022-05-10 09:09:02+00:00
- **Authors**: Soonchan Park, Jinah Park
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Studies of virtual try-on (VITON) have been shown their effectiveness in utilizing the generative neural network for virtually exploring fashion products, and some of recent researches of VITON attempted to synthesize human image wearing given multiple types of garments (e.g., top and bottom clothes). However, when replacing the top and bottom clothes of the target human, numerous wearing styles are possible with a certain combination of the clothes. In this paper, we address the problem of variation in wearing style when simultaneously replacing the top and bottom clothes of the model. We introduce Wearing-Guide VITON (i.e., WG-VITON) which utilizes an additional input binary mask to control the wearing styles of the generated image. Our experiments show that WG-VITON effectively generates an image of the model wearing given top and bottom clothes, and create complicated wearing styles such as partly tucking in the top to the bottom



### Explainable Deep Learning Methods in Medical Imaging Diagnosis: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2205.04766v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04766v2)
- **Published**: 2022-05-10 09:28:14+00:00
- **Updated**: 2022-06-13 13:31:09+00:00
- **Authors**: Cristiano Patrício, João C. Neves, Luís F. Teixeira
- **Comment**: Pre-print submitted to ACM CSUR
- **Journal**: None
- **Summary**: The remarkable success of deep learning has prompted interest in its application to medical imaging diagnosis. Even though state-of-the-art deep learning models have achieved human-level accuracy on the classification of different types of medical data, these models are hardly adopted in clinical workflows, mainly due to their lack of interpretability. The black-box-ness of deep learning models has raised the need for devising strategies to explain the decision process of these models, leading to the creation of the topic of eXplainable Artificial Intelligence (XAI). In this context, we provide a thorough survey of XAI applied to medical imaging diagnosis, including visual, textual, example-based and concept-based explanation methods. Moreover, this work reviews the existing medical imaging datasets and the existing metrics for evaluating the quality of the explanations. In addition, we include a performance comparison among a set of report generation-based methods. Finally, the major challenges in applying XAI to medical imaging and the future research directions on the topic are also discussed.



### VesNet-RL: Simulation-based Reinforcement Learning for Real-World US Probe Navigation
- **Arxiv ID**: http://arxiv.org/abs/2205.06676v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.06676v1)
- **Published**: 2022-05-10 09:34:42+00:00
- **Updated**: 2022-05-10 09:34:42+00:00
- **Authors**: Yuan Bi, Zhongliang Jiang, Yuan Gao, Thomas Wendler, Angelos Karlas, Nassir Navab
- **Comment**: Directly accepted by IEEE RAL after the first round of review. Video:
  https://www.youtube.com/watch?v=bzCO07Hquj8 Codes:
  https://github.com/yuan-12138/VesNet-RL
- **Journal**: None
- **Summary**: Ultrasound (US) is one of the most common medical imaging modalities since it is radiation-free, low-cost, and real-time. In freehand US examinations, sonographers often navigate a US probe to visualize standard examination planes with rich diagnostic information. However, reproducibility and stability of the resulting images often suffer from intra- and inter-operator variation. Reinforcement learning (RL), as an interaction-based learning method, has demonstrated its effectiveness in visual navigating tasks; however, RL is limited in terms of generalization. To address this challenge, we propose a simulation-based RL framework for real-world navigation of US probes towards the standard longitudinal views of vessels. A UNet is used to provide binary masks from US images; thereby, the RL agent trained on simulated binary vessel images can be applied in real scenarios without further training. To accurately characterize actual states, a multi-modality state representation structure is introduced to facilitate the understanding of environments. Moreover, considering the characteristics of vessels, a novel standard view recognition approach based on the minimum bounding rectangle is proposed to terminate the searching process. To evaluate the effectiveness of the proposed method, the trained policy is validated virtually on 3D volumes of a volunteer's in-vivo carotid artery, and physically on custom-designed gel phantoms using robotic US. The results demonstrate that proposed approach can effectively and accurately navigate the probe towards the longitudinal view of vessels.



### Domain Invariant Masked Autoencoders for Self-supervised Learning from Multi-domains
- **Arxiv ID**: http://arxiv.org/abs/2205.04771v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04771v2)
- **Published**: 2022-05-10 09:49:40+00:00
- **Updated**: 2022-06-06 13:23:56+00:00
- **Authors**: Haiyang Yang, Meilin Chen, Yizhou Wang, Shixiang Tang, Feng Zhu, Lei Bai, Rui Zhao, Wanli Ouyang
- **Comment**: None
- **Journal**: None
- **Summary**: Generalizing learned representations across significantly different visual domains is a fundamental yet crucial ability of the human visual system. While recent self-supervised learning methods have achieved good performances with evaluation set on the same domain as the training set, they will have an undesirable performance decrease when tested on a different domain. Therefore, the self-supervised learning from multiple domains task is proposed to learn domain-invariant features that are not only suitable for evaluation on the same domain as the training set but also can be generalized to unseen domains. In this paper, we propose a Domain-invariant Masked AutoEncoder (DiMAE) for self-supervised learning from multi-domains, which designs a new pretext task, \emph{i.e.,} the cross-domain reconstruction task, to learn domain-invariant features. The core idea is to augment the input image with style noise from different domains and then reconstruct the image from the embedding of the augmented image, regularizing the encoder to learn domain-invariant features. To accomplish the idea, DiMAE contains two critical designs, 1) content-preserved style mix, which adds style information from other domains to input while persevering the content in a parameter-free manner, and 2) multiple domain-specific decoders, which recovers the corresponding domain style of input to the encoded domain-invariant features for reconstruction. Experiments on PACS and DomainNet illustrate that DiMAE achieves considerable gains compared with recent state-of-the-art methods.



### Non-Isometric Shape Matching via Functional Maps on Landmark-Adapted Bases
- **Arxiv ID**: http://arxiv.org/abs/2205.04800v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, I.3.5
- **Links**: [PDF](http://arxiv.org/pdf/2205.04800v2)
- **Published**: 2022-05-10 11:02:14+00:00
- **Updated**: 2022-06-22 11:56:48+00:00
- **Authors**: Mikhail Panine, Maxime Kirgo, Maks Ovsjanikov
- **Comment**: To appear in: Computer Graphics Forum // Main Manuscript: 15 pages
  (without references), 19 figures, 4 tables // Appendix: 8 pages, 12 figures,
  3 tables // Second version fixes typos, font inconsistencies and a minor sign
  error
- **Journal**: None
- **Summary**: We propose a principled approach for non-isometric landmark-preserving non-rigid shape matching. Our method is based on the functional maps framework, but rather than promoting isometries we focus instead on near-conformal maps that preserve landmarks exactly. We achieve this, first, by introducing a novel landmark-adapted basis using an intrinsic Dirichlet-Steklov eigenproblem. Second, we establish the functional decomposition of conformal maps expressed in this basis. Finally, we formulate a conformally-invariant energy that promotes high-quality landmark-preserving maps, and show how it can be solved via a variant of the recently proposed ZoomOut method that we extend to our setting. Our method is descriptor-free, efficient and robust to significant mesh variability. We evaluate our approach on a range of benchmark datasets and demonstrate state-of-the-art performance on non-isometric benchmarks and near state-of-the-art performance on isometric ones.



### The Impact of Partial Occlusion on Pedestrian Detectability
- **Arxiv ID**: http://arxiv.org/abs/2205.04812v6
- **DOI**: 10.1016/j.birob.2023.100115
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04812v6)
- **Published**: 2022-05-10 11:21:18+00:00
- **Updated**: 2023-07-27 09:57:21+00:00
- **Authors**: Shane Gilroy, Darragh Mullins, Edward Jones, Ashkan Parsi, Martin Glavin
- **Comment**: This research has been published under the title "Replacing the human
  driver: An objective benchmark for occluded pedestrian detection" in
  Biomimetic Intelligence and Robotics
  https://doi.org/10.1016/j.birob.2023.100115
- **Journal**: Biomimetic Intelligence and Robotics. 2023 Jul 18:100115
- **Summary**: Robust detection of vulnerable road users is a safety critical requirement for the deployment of autonomous vehicles in heterogeneous traffic. One of the most complex outstanding challenges is that of partial occlusion where a target object is only partially available to the sensor due to obstruction by another foreground object. A number of leading pedestrian detection benchmarks provide annotation for partial occlusion, however each benchmark varies greatly in their definition of the occurrence and severity of occlusion. Recent research demonstrates that a high degree of subjectivity is used to classify occlusion level in these cases and occlusion is typically categorized into 2 to 3 broad categories such as partially and heavily occluded. This can lead to inaccurate or inconsistent reporting of pedestrian detection model performance depending on which benchmark is used. This research introduces a novel, objective benchmark for partially occluded pedestrian detection to facilitate the objective characterization of pedestrian detection models. Characterization is carried out on seven popular pedestrian detection models for a range of occlusion levels from 0-99%, in order to demonstrate the efficacy and increased analysis capabilities of the proposed characterization method. Results demonstrate that pedestrian detection performance degrades, and the number of false negative detections increase as pedestrian occlusion level increases. Of the seven popular pedestrian detection routines characterized, CenterNet has the greatest overall performance, followed by SSDlite. RetinaNet has the lowest overall detection performance across the range of occlusion levels.



### Self-supervised regression learning using domain knowledge: Applications to improving self-supervised denoising in imaging
- **Arxiv ID**: http://arxiv.org/abs/2205.04821v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04821v1)
- **Published**: 2022-05-10 11:46:10+00:00
- **Updated**: 2022-05-10 11:46:10+00:00
- **Authors**: Il Yong Chun, Dongwon Park, Xuehang Zheng, Se Young Chun, Yong Long
- **Comment**: 17 pages, 16 figures, 2 tables, submitted to IEEE T-IP
- **Journal**: None
- **Summary**: Regression that predicts continuous quantity is a central part of applications using computational imaging and computer vision technologies. Yet, studying and understanding self-supervised learning for regression tasks - except for a particular regression task, image denoising - have lagged behind. This paper proposes a general self-supervised regression learning (SSRL) framework that enables learning regression neural networks with only input data (but without ground-truth target data), by using a designable pseudo-predictor that encapsulates domain knowledge of a specific application. The paper underlines the importance of using domain knowledge by showing that under different settings, the better pseudo-predictor can lead properties of SSRL closer to those of ordinary supervised learning. Numerical experiments for low-dose computational tomography denoising and camera image denoising demonstrate that proposed SSRL significantly improves the denoising quality over several existing self-supervised denoising methods.



### Object Detection in Indian Food Platters using Transfer Learning with YOLOv4
- **Arxiv ID**: http://arxiv.org/abs/2205.04841v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04841v1)
- **Published**: 2022-05-10 12:28:01+00:00
- **Updated**: 2022-05-10 12:28:01+00:00
- **Authors**: Deepanshu Pandey, Purva Parmar, Gauri Toshniwal, Mansi Goel, Vishesh Agrawal, Shivangi Dhiman, Lavanya Gupta, Ganesh Bagler
- **Comment**: 6 pages, 7 figures, 38th IEEE International Conference on Data
  Engineering, 2022, DECOR Workshop
- **Journal**: None
- **Summary**: Object detection is a well-known problem in computer vision. Despite this, its usage and pervasiveness in the traditional Indian food dishes has been limited. Particularly, recognizing Indian food dishes present in a single photo is challenging due to three reasons: 1. Lack of annotated Indian food datasets 2. Non-distinct boundaries between the dishes 3. High intra-class variation. We solve these issues by providing a comprehensively labelled Indian food dataset- IndianFood10, which contains 10 food classes that appear frequently in a staple Indian meal and using transfer learning with YOLOv4 object detector model. Our model is able to achieve an overall mAP score of 91.8% and f1-score of 0.90 for our 10 class dataset. We also provide an extension of our 10 class dataset- IndianFood20, which contains 10 more traditional Indian food classes.



### Assessing Streamline Plausibility Through Randomized Iterative Spherical-Deconvolution Informed Tractogram Filtering
- **Arxiv ID**: http://arxiv.org/abs/2205.04843v1
- **DOI**: 10.1016/j.neuroimage.2023.120248
- **Categories**: **cs.CV**, cs.AI, 68T07, 68U10, 92C55, 94A12, I.4.10; I.4.7; I.5; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2205.04843v1)
- **Published**: 2022-05-10 12:36:30+00:00
- **Updated**: 2022-05-10 12:36:30+00:00
- **Authors**: Antonia Hain, Daniel Jörgens, Rodrigo Moreno
- **Comment**: 38 pages, 18 figures
- **Journal**: Neuroimage. 2023 ;278:120248
- **Summary**: Tractography has become an indispensable part of brain connectivity studies. However, it is currently facing problems with reliability. In particular, a substantial amount of nerve fiber reconstructions (streamlines) in tractograms produced by state-of-the-art tractography methods are anatomically implausible. To address this problem, tractogram filtering methods have been developed to remove faulty connections in a postprocessing step. This study takes a closer look at one such method, \textit{Spherical-deconvolution Informed Filtering of Tractograms} (SIFT), which uses a global optimization approach to improve the agreement between the remaining streamlines after filtering and the underlying diffusion magnetic resonance imaging data. SIFT is not suitable to judge the plausibility of individual streamlines since its results depend on the size and composition of the surrounding tractogram. To tackle this problem, we propose applying SIFT to randomly selected tractogram subsets in order to retrieve multiple assessments for each streamline. This approach makes it possible to identify streamlines with very consistent filtering results, which were used as pseudo ground truths for training classifiers. The trained classifier is able to distinguish the obtained groups of plausible and implausible streamlines with accuracy above 80%. The software code used in the paper and pretrained weights of the classifier are distributed freely via the Github repository https://github.com/djoerch/randomised_filtering.



### MNet: Rethinking 2D/3D Networks for Anisotropic Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.04846v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04846v1)
- **Published**: 2022-05-10 12:39:08+00:00
- **Updated**: 2022-05-10 12:39:08+00:00
- **Authors**: Zhangfu Dong, Yuting He, Xiaoming Qi, Yang Chen, Huazhong Shu, Jean-Louis Coatrieux, Guanyu Yang, Shuo Li
- **Comment**: Accepted by IJCAI 2022
- **Journal**: None
- **Summary**: The nature of thick-slice scanning causes severe inter-slice discontinuities of 3D medical images, and the vanilla 2D/3D convolutional neural networks (CNNs) fail to represent sparse inter-slice information and dense intra-slice information in a balanced way, leading to severe underfitting to inter-slice features (for vanilla 2D CNNs) and overfitting to noise from long-range slices (for vanilla 3D CNNs). In this work, a novel mesh network (MNet) is proposed to balance the spatial representation inter axes via learning. 1) Our MNet latently fuses plenty of representation processes by embedding multi-dimensional convolutions deeply into basic modules, making the selections of representation processes flexible, thus balancing representation for sparse inter-slice information and dense intra-slice information adaptively. 2) Our MNet latently fuses multi-dimensional features inside each basic module, simultaneously taking the advantages of 2D (high segmentation accuracy of the easily recognized regions in 2D view) and 3D (high smoothness of 3D organ contour) representations, thus obtaining more accurate modeling for target regions. Comprehensive experiments are performed on four public datasets (CT\&MR), the results consistently demonstrate the proposed MNet outperforms the other methods. The code and datasets are available at: https://github.com/zfdong-code/MNet



### Hyperparameter optimization of hybrid quantum neural networks for car classification
- **Arxiv ID**: http://arxiv.org/abs/2205.04878v1
- **DOI**: None
- **Categories**: **quant-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04878v1)
- **Published**: 2022-05-10 13:25:36+00:00
- **Updated**: 2022-05-10 13:25:36+00:00
- **Authors**: Asel Sagingalieva, Andrii Kurkin, Artem Melnikov, Daniil Kuhmistrov, Michael Perelshtein, Alexey Melnikov, Andrea Skolik, David Von Dollen
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Image recognition is one of the primary applications of machine learning algorithms. Nevertheless, machine learning models used in modern image recognition systems consist of millions of parameters that usually require significant computational time to be adjusted. Moreover, adjustment of model hyperparameters leads to additional overhead. Because of this, new developments in machine learning models and hyperparameter optimization techniques are required. This paper presents a quantum-inspired hyperparameter optimization technique and a hybrid quantum-classical machine learning model for supervised learning. We benchmark our hyperparameter optimization method over standard black-box objective functions and observe performance improvements in the form of reduced expected run times and fitness in response to the growth in the size of the search space. We test our approaches in a car image classification task, and demonstrate a full-scale implementation of the hybrid quantum neural network model with the tensor train hyperparameter optimization. Our tests show a qualitative and quantitative advantage over the corresponding standard classical tabular grid search approach used with a deep neural network ResNet34. A classification accuracy of 0.97 was obtained by the hybrid model after 18 iterations, whereas the classical model achieved an accuracy of 0.92 after 75 iterations.



### Identical Image Retrieval using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.04883v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/2205.04883v2)
- **Published**: 2022-05-10 13:34:41+00:00
- **Updated**: 2022-05-18 11:16:46+00:00
- **Authors**: Sayan Nath, Nikhil Nayak
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, we know that the interaction with images has increased. Image similarity involves fetching similar-looking images abiding by a given reference image. The target is to find out whether the image searched as a query can result in similar pictures. We are using the BigTransfer Model, which is a state-of-art model itself. BigTransfer(BiT) is essentially a ResNet but pre-trained on a larger dataset like ImageNet and ImageNet-21k with additional modifications. Using the fine-tuned pre-trained Convolution Neural Network Model, we extract the key features and train on the K-Nearest Neighbor model to obtain the nearest neighbor. The application of our model is to find similar images, which are hard to achieve through text queries within a low inference time. We analyse the benchmark of our model based on this application.



### Learning Non-target Knowledge for Few-shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.04903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04903v1)
- **Published**: 2022-05-10 13:52:48+00:00
- **Updated**: 2022-05-10 13:52:48+00:00
- **Authors**: Yuanwei Liu, Nian Liu, Qinglong Cao, Xiwen Yao, Junwei Han, Ling Shao
- **Comment**: Accepted to CVPR2022
- **Journal**: None
- **Summary**: Existing studies in few-shot semantic segmentation only focus on mining the target object information, however, often are hard to tell ambiguous regions, especially in non-target regions, which include background (BG) and Distracting Objects (DOs). To alleviate this problem, we propose a novel framework, namely Non-Target Region Eliminating (NTRE) network, to explicitly mine and eliminate BG and DO regions in the query. First, a BG Mining Module (BGMM) is proposed to extract the BG region via learning a general BG prototype. To this end, we design a BG loss to supervise the learning of BGMM only using the known target object segmentation ground truth. Then, a BG Eliminating Module and a DO Eliminating Module are proposed to successively filter out the BG and DO information from the query feature, based on which we can obtain a BG and DO-free target object segmentation result. Furthermore, we propose a prototypical contrastive learning algorithm to improve the model ability of distinguishing the target object from DOs. Extensive experiments on both PASCAL-5i and COCO-20i datasets show that our approach is effective despite its simplicity.



### Shadow-Aware Dynamic Convolution for Shadow Removal
- **Arxiv ID**: http://arxiv.org/abs/2205.04908v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.04908v3)
- **Published**: 2022-05-10 14:00:48+00:00
- **Updated**: 2022-08-30 03:36:29+00:00
- **Authors**: Yimin Xu, Mingbao Lin, Hong Yang, Fei Chao, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: With a wide range of shadows in many collected images, shadow removal has aroused increasing attention since uncontaminated images are of vital importance for many downstream multimedia tasks. Current methods consider the same convolution operations for both shadow and non-shadow regions while ignoring the large gap between the color mappings for the shadow region and the non-shadow region, leading to poor quality of reconstructed images and a heavy computation burden. To solve this problem, this paper introduces a novel plug-and-play Shadow-Aware Dynamic Convolution (SADC) module to decouple the interdependence between the shadow region and the non-shadow region. Inspired by the fact that the color mapping of the non-shadow region is easier to learn, our SADC processes the non-shadow region with a lightweight convolution module in a computationally cheap manner and recovers the shadow region with a more complicated convolution module to ensure the quality of image reconstruction. Given that the non-shadow region often contains more background color information, we further develop a novel intra-convolution distillation loss to strengthen the information flow from the non-shadow region to the shadow region. Extensive experiments on the ISTD and SRD datasets show our method achieves better performance in shadow removal over many state-of-the-arts. Our code is available at https://github.com/xuyimin0926/SADC.



### A Closer Look at Blind Super-Resolution: Degradation Models, Baselines, and Performance Upper Bounds
- **Arxiv ID**: http://arxiv.org/abs/2205.04910v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04910v1)
- **Published**: 2022-05-10 14:02:49+00:00
- **Updated**: 2022-05-10 14:02:49+00:00
- **Authors**: Wenlong Zhang, Guangyuan Shi, Yihao Liu, Chao Dong, Xiao-Ming Wu
- **Comment**: Accepted by CVPR Workshop, NTIRE 2022
- **Journal**: None
- **Summary**: Degradation models play an important role in Blind super-resolution (SR). The classical degradation model, which mainly involves blur degradation, is too simple to simulate real-world scenarios. The recently proposed practical degradation model includes a full spectrum of degradation types, but only considers complex cases that use all degradation types in the degradation process, while ignoring many important corner cases that are common in the real world. To address this problem, we propose a unified gated degradation model to generate a broad set of degradation cases using a random gate controller. Based on the gated degradation model, we propose simple baseline networks that can effectively handle non-blind, classical, practical degradation cases as well as many other corner cases. To fairly evaluate the performance of our baseline networks against state-of-the-art methods and understand their limits, we introduce the performance upper bound of an SR network for every degradation type. Our empirical analysis shows that with the unified gated degradation model, the proposed baselines can achieve much better performance than existing methods in quantitative and qualitative results, which are close to the performance upper bounds.



### Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training
- **Arxiv ID**: http://arxiv.org/abs/2205.04948v2
- **DOI**: None
- **Categories**: **cs.CV**, 68U35, H.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2205.04948v2)
- **Published**: 2022-05-10 15:03:00+00:00
- **Updated**: 2022-12-16 07:22:56+00:00
- **Authors**: Jing Yang, Junwen Chen, Keiji Yanai
- **Comment**: Accepted at MMM2023
- **Journal**: None
- **Summary**: In this paper, we present a cross-modal recipe retrieval framework, Transformer-based Network for Large Batch Training (TNLBT), which is inspired by ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer). TNLBT aims to accomplish retrieval tasks while generating images from recipe embeddings. We apply the Hierarchical Transformer-based recipe text encoder, the Vision Transformer~(ViT)-based recipe image encoder, and an adversarial network architecture to enable better cross-modal embedding learning for recipe texts and images. In addition, we use self-supervised learning to exploit the rich information in the recipe texts having no corresponding images. Since contrastive learning could benefit from a larger batch size according to the recent literature on self-supervised learning, we adopt a large batch size during training and have validated its effectiveness. In the experiments, the proposed framework significantly outperformed the current state-of-the-art frameworks in both cross-modal recipe retrieval and image generation tasks on the benchmark Recipe1M. This is the first work which confirmed the effectiveness of large batch training on cross-modal recipe embeddings.



### NeRF-Editing: Geometry Editing of Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2205.04978v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.04978v1)
- **Published**: 2022-05-10 15:35:52+00:00
- **Updated**: 2022-05-10 15:35:52+00:00
- **Authors**: Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma, Rongfei Jia, Lin Gao
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Implicit neural rendering, especially Neural Radiance Field (NeRF), has shown great potential in novel view synthesis of a scene. However, current NeRF-based methods cannot enable users to perform user-controlled shape deformation in the scene. While existing works have proposed some approaches to modify the radiance field according to the user's constraints, the modification is limited to color editing or object translation and rotation. In this paper, we propose a method that allows users to perform controllable shape deformation on the implicit representation of the scene, and synthesizes the novel view images of the edited scene without re-training the network. Specifically, we establish a correspondence between the extracted explicit mesh representation and the implicit neural representation of the target scene. Users can first utilize well-developed mesh-based deformation methods to deform the mesh representation of the scene. Our method then utilizes user edits from the mesh representation to bend the camera rays by introducing a tetrahedra mesh as a proxy, obtaining the rendering results of the edited scene. Extensive experiments demonstrate that our framework can achieve ideal editing results not only on synthetic data, but also on real scenes captured by users.



### Disentangling A Single MR Modality
- **Arxiv ID**: http://arxiv.org/abs/2205.04982v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.04982v1)
- **Published**: 2022-05-10 15:40:12+00:00
- **Updated**: 2022-05-10 15:40:12+00:00
- **Authors**: Lianrui Zuo, Yihao Liu, Yuan Xue, Shuo Han, Murat Bilgel, Susan M. Resnick, Jerry L. Prince, Aaron Carass
- **Comment**: None
- **Journal**: None
- **Summary**: Disentangling anatomical and contrast information from medical images has gained attention recently, demonstrating benefits for various image analysis tasks. Current methods learn disentangled representations using either paired multi-modal images with the same underlying anatomy or auxiliary labels (e.g., manual delineations) to provide inductive bias for disentanglement. However, these requirements could significantly increase the time and cost in data collection and limit the applicability of these methods when such data are not available. Moreover, these methods generally do not guarantee disentanglement. In this paper, we present a novel framework that learns theoretically and practically superior disentanglement from single modality magnetic resonance images. Moreover, we propose a new information-based metric to quantitatively evaluate disentanglement. Comparisons over existing disentangling methods demonstrate that the proposed method achieves superior performance in both disentanglement and cross-domain image-to-image translation tasks.



### KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints
- **Arxiv ID**: http://arxiv.org/abs/2205.04992v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.04992v2)
- **Published**: 2022-05-10 15:57:03+00:00
- **Updated**: 2022-07-21 13:40:37+00:00
- **Authors**: Marko Mihajlovic, Aayush Bansal, Michael Zollhoefer, Siyu Tang, Shunsuke Saito
- **Comment**: To appear at ECCV 2022. The project page is available at
  https://markomih.github.io/KeypointNeRF
- **Journal**: None
- **Summary**: Image-based volumetric humans using pixel-aligned features promise generalization to unseen poses and identities. Prior work leverages global spatial encodings and multi-view geometric consistency to reduce spatial ambiguity. However, global encodings often suffer from overfitting to the distribution of the training data, and it is difficult to learn multi-view consistent reconstruction from sparse views. In this work, we investigate common issues with existing spatial encodings and propose a simple yet highly effective approach to modeling high-fidelity volumetric humans from sparse views. One of the key ideas is to encode relative spatial 3D information via sparse 3D keypoints. This approach is robust to the sparsity of viewpoints and cross-dataset domain gap. Our approach outperforms state-of-the-art methods for head reconstruction. On human body reconstruction for unseen subjects, we also achieve performance comparable to prior work that uses a parametric human body model and temporal feature aggregation. Our experiments show that a majority of errors in prior work stem from an inappropriate choice of spatial encoding and thus we suggest a new direction for high-fidelity image-based human modeling. https://markomih.github.io/KeypointNeRF



### Using Deep Learning-based Features Extracted from CT scans to Predict Outcomes in COVID-19 Patients
- **Arxiv ID**: http://arxiv.org/abs/2205.05009v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.05009v1)
- **Published**: 2022-05-10 16:22:16+00:00
- **Updated**: 2022-05-10 16:22:16+00:00
- **Authors**: Sai Vidyaranya Nuthalapati, Marcela Vizcaychipi, Pallav Shah, Piotr Chudzik, Chee Hau Leow, Paria Yousefi, Ahmed Selim, Keiran Tait, Ben Irving
- **Comment**: None
- **Journal**: None
- **Summary**: The COVID-19 pandemic has had a considerable impact on day-to-day life. Tackling the disease by providing the necessary resources to the affected is of paramount importance. However, estimation of the required resources is not a trivial task given the number of factors which determine the requirement. This issue can be addressed by predicting the probability that an infected patient requires Intensive Care Unit (ICU) support and the importance of each of the factors that influence it. Moreover, to assist the doctors in determining the patients at high risk of fatality, the probability of death is also calculated. For determining both the patient outcomes (ICU admission and death), a novel methodology is proposed by combining multi-modal features, extracted from Computed Tomography (CT) scans and Electronic Health Record (EHR) data. Deep learning models are leveraged to extract quantitative features from CT scans. These features combined with those directly read from the EHR database are fed into machine learning models to eventually output the probabilities of patient outcomes. This work demonstrates both the ability to apply a broad set of deep learning methods for general quantification of Chest CT scans and the ability to link these quantitative metrics to patient outcomes. The effectiveness of the proposed method is shown by testing it on an internally curated dataset, achieving a mean area under Receiver operating characteristic curve (AUC) of 0.77 on ICU admission prediction and a mean AUC of 0.73 on death prediction using the best performing classifiers.



### Learning to Answer Visual Questions from Web Videos
- **Arxiv ID**: http://arxiv.org/abs/2205.05019v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.05019v2)
- **Published**: 2022-05-10 16:34:26+00:00
- **Updated**: 2022-05-11 05:31:08+00:00
- **Authors**: Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid
- **Comment**: Accepted at the TPAMI Special Issue on the Best Papers of ICCV 2021.
  Journal extension of the conference paper arXiv:2012.00451. 16 pages, 13
  figures
- **Journal**: None
- **Summary**: Recent methods for visual question answering rely on large-scale annotated datasets. Manual annotation of questions and answers for videos, however, is tedious, expensive and prevents scalability. In this work, we propose to avoid manual annotation and generate a large-scale training dataset for video question answering making use of automatic cross-modal supervision. We leverage a question generation transformer trained on text data and use it to generate question-answer pairs from transcribed video narrations. Given narrated videos, we then automatically generate the HowToVQA69M dataset with 69M video-question-answer triplets. To handle the open vocabulary of diverse answers in this dataset, we propose a training procedure based on a contrastive loss between a video-question multi-modal transformer and an answer transformer. We introduce the zero-shot VideoQA task and the VideoQA feature probe evaluation setting and show excellent results, in particular for rare answers. Furthermore, our method achieves competitive results on MSRVTT-QA, ActivityNet-QA, MSVD-QA and How2QA datasets. We also show that our VideoQA dataset generation approach generalizes to another source of web video and text data. We use our method to generate the WebVidVQA3M dataset from the WebVid dataset, i.e., videos with alt-text annotations, and show its benefits for training VideoQA models. Finally, for a detailed evaluation we introduce iVQA, a new VideoQA dataset with reduced language bias and high-quality manual annotations. Code, datasets and trained models are available at https://antoyang.github.io/just-ask.html



### MM-RealSR: Metric Learning based Interactive Modulation for Real-World Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2205.05065v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.05065v2)
- **Published**: 2022-05-10 17:46:59+00:00
- **Updated**: 2022-07-27 08:55:08+00:00
- **Authors**: Chong Mou, Yanze Wu, Xintao Wang, Chao Dong, Jian Zhang, Ying Shan
- **Comment**: Accepted by ECCV 2022. Code is available at:
  https://github.com/TencentARC/MM-RealSR
- **Journal**: None
- **Summary**: Interactive image restoration aims to restore images by adjusting several controlling coefficients, which determine the restoration strength. Existing methods are restricted in learning the controllable functions under the supervision of known degradation types and levels. They usually suffer from a severe performance drop when the real degradation is different from their assumptions. Such a limitation is due to the complexity of real-world degradations, which can not provide explicit supervision to the interactive modulation during training. However, how to realize the interactive modulation in real-world super-resolution has not yet been studied. In this work, we present a Metric Learning based Interactive Modulation for Real-World Super-Resolution (MM-RealSR). Specifically, we propose an unsupervised degradation estimation strategy to estimate the degradation level in real-world scenarios. Instead of using known degradation levels as explicit supervision to the interactive mechanism, we propose a metric learning strategy to map the unquantifiable degradation levels in real-world scenarios to a metric space, which is trained in an unsupervised manner. Moreover, we introduce an anchor point strategy in the metric learning process to normalize the distribution of metric space. Extensive experiments demonstrate that the proposed MM-RealSR achieves excellent modulation and restoration performance in real-world super-resolution. Codes are available at https://github.com/TencentARC/MM-RealSR.



### Accelerating the Training of Video Super-Resolution Models
- **Arxiv ID**: http://arxiv.org/abs/2205.05069v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.05069v2)
- **Published**: 2022-05-10 17:55:24+00:00
- **Updated**: 2022-05-17 02:54:33+00:00
- **Authors**: Lijian Lin, Xintao Wang, Zhongang Qi, Ying Shan
- **Comment**: The code is available at
  https://github.com/TencentARC/Efficient-VSR-Training
- **Journal**: None
- **Summary**: Despite that convolution neural networks (CNN) have recently demonstrated high-quality reconstruction for video super-resolution (VSR), efficiently training competitive VSR models remains a challenging problem. It usually takes an order of magnitude more time than training their counterpart image models, leading to long research cycles. Existing VSR methods typically train models with fixed spatial and temporal sizes from beginning to end. The fixed sizes are usually set to large values for good performance, resulting to slow training. However, is such a rigid training strategy necessary for VSR? In this work, we show that it is possible to gradually train video models from small to large spatial/temporal sizes, i.e., in an easy-to-hard manner. In particular, the whole training is divided into several stages and the earlier stage has smaller training spatial shape. Inside each stage, the temporal size also varies from short to long while the spatial size remains unchanged. Training is accelerated by such a multigrid training strategy, as most of computation is performed on smaller spatial and shorter temporal shapes. For further acceleration with GPU parallelization, we also investigate the large minibatch training without the loss in accuracy. Extensive experiments demonstrate that our method is capable of largely speeding up training (up to $6.2\times$ speedup in wall-clock training time) without performance drop for various VSR models. The code is available at https://github.com/TencentARC/Efficient-VSR-Training.



### Learning Visual Styles from Audio-Visual Associations
- **Arxiv ID**: http://arxiv.org/abs/2205.05072v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2205.05072v1)
- **Published**: 2022-05-10 17:57:07+00:00
- **Updated**: 2022-05-10 17:57:07+00:00
- **Authors**: Tingle Li, Yichen Liu, Andrew Owens, Hang Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: From the patter of rain to the crunch of snow, the sounds we hear often convey the visual textures that appear within a scene. In this paper, we present a method for learning visual styles from unlabeled audio-visual data. Our model learns to manipulate the texture of a scene to match a sound, a problem we term audio-driven image stylization. Given a dataset of paired audio-visual data, we learn to modify input images such that, after manipulation, they are more likely to co-occur with a given input sound. In quantitative and qualitative evaluations, our sound-based model outperforms label-based approaches. We also show that audio can be an intuitive representation for manipulating images, as adjusting a sound's volume or mixing two sounds together results in predictable changes to visual style. Project webpage: https://tinglok.netlify.app/files/avstyle



### Reduce Information Loss in Transformers for Pluralistic Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2205.05076v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2205.05076v2)
- **Published**: 2022-05-10 17:59:58+00:00
- **Updated**: 2022-05-15 17:17:17+00:00
- **Authors**: Qiankun Liu, Zhentao Tan, Dongdong Chen, Qi Chu, Xiyang Dai, Yinpeng Chen, Mengchen Liu, Lu Yuan, Nenghai Yu
- **Comment**: CVPR 2022, code is available at https://github.com/liuqk3/PUT
- **Journal**: None
- **Summary**: Transformers have achieved great success in pluralistic image inpainting recently. However, we find existing transformer based solutions regard each pixel as a token, thus suffer from information loss issue from two aspects: 1) They downsample the input image into much lower resolutions for efficiency consideration, incurring information loss and extra misalignment for the boundaries of masked regions. 2) They quantize $256^3$ RGB pixels to a small number (such as 512) of quantized pixels. The indices of quantized pixels are used as tokens for the inputs and prediction targets of transformer. Although an extra CNN network is used to upsample and refine the low-resolution results, it is difficult to retrieve the lost information back.To keep input information as much as possible, we propose a new transformer based framework "PUT". Specifically, to avoid input downsampling while maintaining the computation efficiency, we design a patch-based auto-encoder P-VQVAE, where the encoder converts the masked image into non-overlapped patch tokens and the decoder recovers the masked regions from inpainted tokens while keeping the unmasked regions unchanged. To eliminate the information loss caused by quantization, an Un-Quantized Transformer (UQ-Transformer) is applied, which directly takes the features from P-VQVAE encoder as input without quantization and regards the quantized tokens only as prediction targets. Extensive experiments show that PUT greatly outperforms state-of-the-art methods on image fidelity, especially for large masked regions and complex large-scale datasets. Code is available at https://github.com/liuqk3/PUT



### An Efficient Calculation of Quaternion Correlation of Signals and Color Images
- **Arxiv ID**: http://arxiv.org/abs/2205.05113v1
- **DOI**: None
- **Categories**: **math.AC**, cs.CV, cs.IT, math.IT, 13-11, G.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2205.05113v1)
- **Published**: 2022-05-10 18:30:31+00:00
- **Updated**: 2022-05-10 18:30:31+00:00
- **Authors**: Artyom M. Grigoryan, Sos S. Agaian
- **Comment**: 14 pages, 7 figures, 1 table
- **Journal**: None
- **Summary**: Over the past century, a correlation has been an essential mathematical technique utilized in engineering sciences, including practically every signal/image processing field. This paper describes an effective method of calculating the correlation function of signals and color images in quaternion algebra. We propose using the quaternions with a commutative multiplication operation and defining the corresponding correlation function in this arithmetic. The correlation between quaternion signals and images can be calculated by multiplying two quaternion DFTs of signals and images. The complexity of the correlation of color images is three times higher than in complex algebra.



### Privacy Enhancement for Cloud-Based Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.07864v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07864v2)
- **Published**: 2022-05-10 18:48:13+00:00
- **Updated**: 2022-08-23 12:15:38+00:00
- **Authors**: Archit Parnami, Muhammad Usama, Liyue Fan, Minwoo Lee
- **Comment**: 14 pages, 13 figures, 3 tables. Preprint. Accepted in IEEE WCCI 2022
  International Joint Conference on Neural Networks (IJCNN)
- **Journal**: None
- **Summary**: Requiring less data for accurate models, few-shot learning has shown robustness and generality in many application domains. However, deploying few-shot models in untrusted environments may inflict privacy concerns, e.g., attacks or adversaries that may breach the privacy of user-supplied data. This paper studies the privacy enhancement for the few-shot learning in an untrusted environment, e.g., the cloud, by establishing a novel privacy-preserved embedding space that preserves the privacy of data and maintains the accuracy of the model. We examine the impact of various image privacy methods such as blurring, pixelization, Gaussian noise, and differentially private pixelization (DP-Pix) on few-shot image classification and propose a method that learns privacy-preserved representation through the joint loss. The empirical results show how privacy-performance trade-off can be negotiated for privacy-enhanced few-shot learning.



### Deep fusion of gray level co-occurrence matrices for lung nodule classification
- **Arxiv ID**: http://arxiv.org/abs/2205.05123v2
- **DOI**: 10.1371/journal.pone.0274516
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.05123v2)
- **Published**: 2022-05-10 19:00:52+00:00
- **Updated**: 2022-11-05 07:55:48+00:00
- **Authors**: Ahmed Saihood, Hossein Karshenas, AhmadReza Naghsh Nilchi
- **Comment**: 24 pages, 6 figures, 80 references
- **Journal**: None
- **Summary**: Lung cancer is a severe menace to human health, due to which millions of people die because of late diagnoses of cancer; thus, it is vital to detect the disease as early as possible. The Computerized chest analysis Tomography of scan is assumed to be one of the efficient solutions for detecting and classifying lung nodules. The necessity of high accuracy of analyzing C.T. scan images of the lung is considered as one of the crucial challenges in detecting and classifying lung cancer. A new long-short-term-memory (LSTM) based deep fusion structure, is introduced, where, the texture features computed from lung nodules through new volumetric grey-level-co-occurrence-matrices (GLCM) computations are applied to classify the nodules into: benign, malignant and ambiguous. An improved Otsu segmentation method combined with the water strider optimization algorithm (WSA) is proposed to detect the lung nodules. Otsu-WSA thresholding can overcome the restrictions present in previous thresholding methods. Extended experiments are run to assess this fusion structure by considering 2D-GLCM computations based 2D-slices fusion, and an approximation of this 3D-GLCM with volumetric 2.5D-GLCM computations-based LSTM fusion structure. The proposed methods are trained and assessed through the LIDC-IDRI dataset, where 94.4%, 91.6%, and 95.8% Accuracy, sensitivity, and specificity are obtained, respectively for 2D-GLCM fusion and 97.33%, 96%, and 98%, accuracy, sensitivity, and specificity, respectively, for 2.5D-GLCM fusion. The yield of the same are 98.7%, 98%, and 99%, for the 3D-GLCM fusion. The obtained results and analysis indicate that the WSA-Otsu method requires less execution time and yields a more accurate thresholding process. It is found that 3D-GLCM based LSTM outperforms its counterparts.



### Few-Shot Image Classification Benchmarks are Too Far From Reality: Build Back Better with Semantic Task Sampling
- **Arxiv ID**: http://arxiv.org/abs/2205.05155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.05155v1)
- **Published**: 2022-05-10 20:25:43+00:00
- **Updated**: 2022-05-10 20:25:43+00:00
- **Authors**: Etienne Bennequin, Myriam Tami, Antoine Toubhans, Celine Hudelot
- **Comment**: CVPR 2022 Workshop on Vision Datasets Understanding
- **Journal**: None
- **Summary**: Every day, a new method is published to tackle Few-Shot Image Classification, showing better and better performances on academic benchmarks. Nevertheless, we observe that these current benchmarks do not accurately represent the real industrial use cases that we encountered. In this work, through both qualitative and quantitative studies, we expose that the widely used benchmark tieredImageNet is strongly biased towards tasks composed of very semantically dissimilar classes e.g. bathtub, cabbage, pizza, schipperke, and cardoon. This makes tieredImageNet (and similar benchmarks) irrelevant to evaluate the ability of a model to solve real-life use cases usually involving more fine-grained classification. We mitigate this bias using semantic information about the classes of tieredImageNet and generate an improved, balanced benchmark. Going further, we also introduce a new benchmark for Few-Shot Image Classification using the Danish Fungi 2020 dataset. This benchmark proposes a wide variety of evaluation tasks with various fine-graininess. Moreover, this benchmark includes many-way tasks (e.g. composed of 100 classes), which is a challenging setting yet very common in industrial applications. Our experiments bring out the correlation between the difficulty of a task and the semantic similarity between its classes, as well as a heavy performance drop of state-of-the-art methods on many-way few-shot classification, raising questions about the scaling abilities of these methods. We hope that our work will encourage the community to further question the quality of standard evaluation processes and their relevance to real-life applications.



### 2-d signature of images and texture classification
- **Arxiv ID**: http://arxiv.org/abs/2205.11236v1
- **DOI**: 10.1098/rspa.2022.0346
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11236v1)
- **Published**: 2022-05-10 20:46:24+00:00
- **Updated**: 2022-05-10 20:46:24+00:00
- **Authors**: Sheng Zhang, Guang Lin, Samy Tindel
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a proper notion of 2-dimensional signature for images. This object is inspired by the so-called rough paths theory, and it captures many essential features of a 2-dimensional object such as an image. It thus serves as a low-dimensional feature for pattern classification. Here we implement a simple procedure for texture classification. In this context, we show that a low dimensional set of features based on signatures produces an excellent accuracy.



### On Scale Space Radon Transform, Properties and Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2205.05188v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.05188v2)
- **Published**: 2022-05-10 21:55:26+00:00
- **Updated**: 2023-02-27 17:24:45+00:00
- **Authors**: Nafaa Nacereddine, Djemel Ziou, Aicha Baya Goumeidane
- **Comment**: None
- **Journal**: None
- **Summary**: When developing a Filtered Backprojection (FBP) algorithm, considering the Radon transform (RT) as a line integral necessitates assuming that all elements of the Computed Tomography (CT) system, such as the detector cell, are dimensionless. It is generally the result of such inadequate CT modeling that analytical methods are sensitive to artifacts and noise. Then, to address this problem, several algebraic reconstruction techniques utilizing iterative models are suggested. The high computational cost of these methods restricts their application. In this paper, we propose the utilization of the Scale Space Radon Transform (SSRT), recognized for its good behavior in the scale space where, the detector width is already considered into the SSRT design and is controlled by the Gaussian kernel standard deviation. After depicting the basic properties and the inversion of SSRT, the FBP algorithm is used in two different ways to reconstruct the image from the SSRT sinogram: (1) Deconv-Rad-FBP: Deconvolve SSRT to estimate RT and apply FBP or (2) SSRT-FBP: Modify FBP such that RT spectrum used in FBP is replaced by SSRT, expressed in the frequency domain. Comparison of image reconstruction using SSRT and RT are performed on Shepp-Logan head and anthropomorphic abdominal phantoms by using, as quality measures, PSNR and SSIM. The first findings show that the SSRT-based image reconstruction quality is better than the one based on RT where, the SSRT-FBP method reveals to be the most accurate, especially, when the number of projections is reduced, making it more appropriate for applications requiring low-dose radiation such as medical X-ray CT. While SSRT-FBP and RT-FBP algorithm have utmost the same execution time, the former is much faster than Deconv-Rad-FBP. Furthermore, the experiments show that the SSRT-FBP method is more robust to CT data Poisson-Gaussian noise.



### Multiplexed Immunofluorescence Brain Image Analysis Using Self-Supervised Dual-Loss Adaptive Masked Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2205.05194v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.05194v3)
- **Published**: 2022-05-10 22:32:29+00:00
- **Updated**: 2022-12-30 22:37:18+00:00
- **Authors**: Son T. Ly, Bai Lin, Hung Q. Vo, Dragan Maric, Badri Roysam, Hien V. Nguyen
- **Comment**: Adding new results on multiplexed image data and data efficiency.
  Pytorch code: https://github.com/hula-ai/DAMA
- **Journal**: None
- **Summary**: Reliable large-scale cell detection and segmentation is the fundamental first step to understanding biological processes in the brain. The ability to phenotype cells at scale can accelerate preclinical drug evaluation and system-level brain histology studies. The impressive advances in deep learning offer a practical solution to cell image detection and segmentation. Unfortunately, categorizing cells and delineating their boundaries for training deep networks is an expensive process that requires skilled biologists. This paper presents a novel self-supervised Dual-Loss Adaptive Masked Autoencoder (DAMA) for learning rich features from multiplexed immunofluorescence brain images. DAMA's objective function minimizes the conditional entropy in pixel-level reconstruction and feature-level regression. Unlike existing self-supervised learning methods based on a random image masking strategy, DAMA employs a novel adaptive mask sampling strategy to maximize mutual information and effectively learn brain cell data. To the best of our knowledge, this is the first effort to develop a self-supervised learning method for multiplexed immunofluorescence brain images. Our extensive experiments demonstrate that DAMA features enable superior cell detection, segmentation, and classification performance without requiring many annotations.



### Best of Both Worlds: Multi-task Audio-Visual Automatic Speech Recognition and Active Speaker Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.05206v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2205.05206v1)
- **Published**: 2022-05-10 23:03:19+00:00
- **Updated**: 2022-05-10 23:03:19+00:00
- **Authors**: Otavio Braga, Olivier Siohan
- **Comment**: None
- **Journal**: None
- **Summary**: Under noisy conditions, automatic speech recognition (ASR) can greatly benefit from the addition of visual signals coming from a video of the speaker's face. However, when multiple candidate speakers are visible this traditionally requires solving a separate problem, namely active speaker detection (ASD), which entails selecting at each moment in time which of the visible faces corresponds to the audio. Recent work has shown that we can solve both problems simultaneously by employing an attention mechanism over the competing video tracks of the speakers' faces, at the cost of sacrificing some accuracy on active speaker detection. This work closes this gap in active speaker detection accuracy by presenting a single model that can be jointly trained with a multi-task loss. By combining the two tasks during training we reduce the ASD classification accuracy by approximately 25%, while simultaneously improving the ASR performance when compared to the multi-person baseline trained exclusively for ASR.



