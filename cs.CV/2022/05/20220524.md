# Arxiv Papers in cs.CV on 2022-05-24
### Learning multi-scale functional representations of proteins from single-cell microscopy data
- **Arxiv ID**: http://arxiv.org/abs/2205.11676v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, q-bio.MN
- **Links**: [PDF](http://arxiv.org/pdf/2205.11676v1)
- **Published**: 2022-05-24 00:00:07+00:00
- **Updated**: 2022-05-24 00:00:07+00:00
- **Authors**: Anastasia Razdaibiedina, Alexander Brechalov
- **Comment**: ICLR MLDD 2022
- **Journal**: None
- **Summary**: Protein function is inherently linked to its localization within the cell, and fluorescent microscopy data is an indispensable resource for learning representations of proteins. Despite major developments in molecular representation learning, extracting functional information from biological images remains a non-trivial computational task. Current state-of-the-art approaches use autoencoder models to learn high-quality features by reconstructing images. However, such methods are prone to capturing noise and imaging artifacts. In this work, we revisit deep learning models used for classifying major subcellular localizations, and evaluate representations extracted from their final layers. We show that simple convolutional networks trained on localization classification can learn protein representations that encapsulate diverse functional information, and significantly outperform autoencoder-based models. We also propose a robust evaluation strategy to assess quality of protein representations across different scales of biological function.



### On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization
- **Arxiv ID**: http://arxiv.org/abs/2205.11686v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.11686v2)
- **Published**: 2022-05-24 00:52:40+00:00
- **Updated**: 2022-10-22 19:54:28+00:00
- **Authors**: Shruti Palaskar, Akshita Bhagia, Yonatan Bisk, Florian Metze, Alan W Black, Ana MarasoviÄ‡
- **Comment**: v2: EMNLP Findings 2022 accepted paper camera-ready version. 9 pages
  main, 2 pages appendix
- **Journal**: None
- **Summary**: Combining the visual modality with pretrained language models has been surprisingly effective for simple descriptive tasks such as image captioning. More general text generation however remains elusive. We take a step back and ask: How do these models work for more complex generative tasks, i.e. conditioning on both text and images? Are multimodal models simply visually adapted language models, or do they combine they reason jointly over modalities?   We investigate these questions in the context of self-rationalization (jointly generating task labels/answers and free-text explanations) of three tasks: (i) visual question answering in VQA-X, (ii) visual commonsense reasoning in VCR, and (iii) visual-textual entailment in e-SNLI-VE. We show that recent unimodal advances, CLIP image representations and scaling of language models, do not consistently improve self-rationalization in multimodal tasks. We find that no single model type works universally best across tasks, datasets, and finetuning data sizes. Our findings motivate the need for novel general backbones approach that move text generation from images and text beyond image captioning.



### M6-Fashion: High-Fidelity Multi-modal Image Generation and Editing
- **Arxiv ID**: http://arxiv.org/abs/2205.11705v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.11705v1)
- **Published**: 2022-05-24 01:18:14+00:00
- **Updated**: 2022-05-24 01:18:14+00:00
- **Authors**: Zhikang Li, Huiling Zhou, Shuai Bai, Peike Li, Chang Zhou, Hongxia Yang
- **Comment**: arXiv admin note: text overlap with arXiv:2105.14211
- **Journal**: None
- **Summary**: The fashion industry has diverse applications in multi-modal image generation and editing. It aims to create a desired high-fidelity image with the multi-modal conditional signal as guidance. Most existing methods learn different condition guidance controls by introducing extra models or ignoring the style prior knowledge, which is difficult to handle multiple signal combinations and faces a low-fidelity problem. In this paper, we adapt both style prior knowledge and flexibility of multi-modal control into one unified two-stage framework, M6-Fashion, focusing on the practical AI-aided Fashion design. It decouples style codes in both spatial and semantic dimensions to guarantee high-fidelity image generation in the first stage. M6-Fashion utilizes self-correction for the non-autoregressive generation to improve inference speed, enhance holistic consistency, and support various signal controls. Extensive experiments on a large-scale clothing dataset M2C-Fashion demonstrate superior performances on various image generation and editing tasks. M6-Fashion model serves as a highly potential AI designer for the fashion industry.



### SCVRL: Shuffled Contrastive Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.11710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11710v1)
- **Published**: 2022-05-24 01:24:47+00:00
- **Updated**: 2022-05-24 01:24:47+00:00
- **Authors**: Michael Dorkenwald, Fanyi Xiao, Biagio Brattoli, Joseph Tighe, Davide Modolo
- **Comment**: CVPR 2022 - L3DIVU workshop
- **Journal**: None
- **Summary**: We propose SCVRL, a novel contrastive-based framework for self-supervised learning for videos. Differently from previous contrast learning based methods that mostly focus on learning visual semantics (e.g., CVRL), SCVRL is capable of learning both semantic and motion patterns. For that, we reformulate the popular shuffling pretext task within a modern contrastive learning paradigm. We show that our transformer-based network has a natural capacity to learn motion in self-supervised settings and achieves strong performance, outperforming CVRL on four benchmarks.



### Improving Shape Awareness and Interpretability in Deep Networks Using Geometric Moments
- **Arxiv ID**: http://arxiv.org/abs/2205.11722v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11722v2)
- **Published**: 2022-05-24 02:08:05+00:00
- **Updated**: 2023-05-22 22:02:03+00:00
- **Authors**: Rajhans Singh, Ankita Shukla, Pavan Turaga
- **Comment**: Accepted at CVPR 2023 Workshop: Deep Learning for Geometric Computing
- **Journal**: None
- **Summary**: Deep networks for image classification often rely more on texture information than object shape. While efforts have been made to make deep-models shape-aware, it is often difficult to make such models simple, interpretable, or rooted in known mathematical definitions of shape. This paper presents a deep-learning model inspired by geometric moments, a classically well understood approach to measure shape-related properties. The proposed method consists of a trainable network for generating coordinate bases and affine parameters for making the features geometrically invariant yet in a task-specific manner. The proposed model improves the final feature's interpretation. We demonstrate the effectiveness of our method on standard image classification datasets. The proposed model achieves higher classification performance compared to the baseline and standard ResNet models while substantially improving interpretability.



### Single-View View Synthesis in the Wild with Learned Adaptive Multiplane Images
- **Arxiv ID**: http://arxiv.org/abs/2205.11733v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2205.11733v1)
- **Published**: 2022-05-24 02:57:16+00:00
- **Updated**: 2022-05-24 02:57:16+00:00
- **Authors**: Yuxuan Han, Ruicheng Wang, Jiaolong Yang
- **Comment**: ACM SIGGRAPH 2022. Project page: https://yxuhan.github.io/AdaMPI/
- **Journal**: None
- **Summary**: This paper deals with the challenging task of synthesizing novel views for in-the-wild photographs. Existing methods have shown promising results leveraging monocular depth estimation and color inpainting with layered depth representations. However, these methods still have limited capability to handle scenes with complex 3D geometry. We propose a new method based on the multiplane image (MPI) representation. To accommodate diverse scene layouts in the wild and tackle the difficulty in producing high-dimensional MPI contents, we design a network structure that consists of two novel modules, one for plane depth adjustment and another for depth-aware color prediction. The former adjusts the initial plane positions using the RGBD context feature and an attention mechanism. Given adjusted depth values, the latter predicts the color and density for each plane separately with proper inter-plane interactions achieved via a feature masking strategy. To train our method, we construct large-scale stereo training data using only unconstrained single-view image collections by a simple yet effective warp-back strategy. The experiments on both synthetic and real datasets demonstrate that our trained model works remarkably well and achieves state-of-the-art results.



### Alleviating Robust Overfitting of Adversarial Training With Consistency Regularization
- **Arxiv ID**: http://arxiv.org/abs/2205.11744v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.11744v1)
- **Published**: 2022-05-24 03:18:43+00:00
- **Updated**: 2022-05-24 03:18:43+00:00
- **Authors**: Shudong Zhang, Haichang Gao, Tianwei Zhang, Yunyi Zhou, Zihui Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial training (AT) has proven to be one of the most effective ways to defend Deep Neural Networks (DNNs) against adversarial attacks. However, the phenomenon of robust overfitting, i.e., the robustness will drop sharply at a certain stage, always exists during AT. It is of great importance to decrease this robust generalization gap in order to obtain a robust model. In this paper, we present an in-depth study towards the robust overfitting from a new angle. We observe that consistency regularization, a popular technique in semi-supervised learning, has a similar goal as AT and can be used to alleviate robust overfitting. We empirically validate this observation, and find a majority of prior solutions have implicit connections to consistency regularization. Motivated by this, we introduce a new AT solution, which integrates the consistency regularization and Mean Teacher (MT) strategy into AT. Specifically, we introduce a teacher model, coming from the average weights of the student models over the training steps. Then we design a consistency loss function to make the prediction distribution of the student models over adversarial examples consistent with that of the teacher model over clean samples. Experiments show that our proposed method can effectively alleviate robust overfitting and improve the robustness of DNN models against common adversarial attacks.



### UMSNet: An Universal Multi-sensor Network for Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.11756v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.5.1; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2205.11756v1)
- **Published**: 2022-05-24 03:29:54+00:00
- **Updated**: 2022-05-24 03:29:54+00:00
- **Authors**: Jialiang Wang, Haotian Wei, Yi Wang, Shu Yang, Chi Li
- **Comment**: None
- **Journal**: None
- **Summary**: Human activity recognition (HAR) based on multimodal sensors has become a rapidly growing branch of biometric recognition and artificial intelligence. However, how to fully mine multimodal time series data and effectively learn accurate behavioral features has always been a hot topic in this field. Practical applications also require a well-generalized framework that can quickly process a variety of raw sensor data and learn better feature representations. This paper proposes a universal multi-sensor network (UMSNet) for human activity recognition. In particular, we propose a new lightweight sensor residual block (called LSR block), which improves the performance by reducing the number of activation function and normalization layers, and adding inverted bottleneck structure and grouping convolution. Then, the Transformer is used to extract the relationship of series features to realize the classification and recognition of human activities. Our framework has a clear structure and can be directly applied to various types of multi-modal Time Series Classification (TSC) tasks after simple specialization. Extensive experiments show that the proposed UMSNet outperforms other state-of-the-art methods on two popular multi-sensor human activity recognition datasets (i.e. HHAR dataset and MHEALTH dataset).



### UNet#: A UNet-like Redesigning Skip Connections for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.11759v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.11759v1)
- **Published**: 2022-05-24 03:40:48+00:00
- **Updated**: 2022-05-24 03:40:48+00:00
- **Authors**: Ledan Qian, Xiao Zhou, Yi Li, Zhongyi Hu
- **Comment**: None
- **Journal**: None
- **Summary**: As an essential prerequisite for developing a medical intelligent assistant system, medical image segmentation has received extensive research and concentration from the neural network community. A series of UNet-like networks with encoder-decoder architecture has achieved extraordinary success, in which UNet2+ and UNet3+ redesign skip connections, respectively proposing dense skip connection and full-scale skip connection and dramatically improving compared with UNet in medical image segmentation. However, UNet2+ lacks sufficient information explored from the full scale, which will affect the learning of organs' location and boundary. Although UNet3+ can obtain the full-scale aggregation feature map, owing to the small number of neurons in the structure, it does not satisfy the segmentation of tiny objects when the number of samples is small. This paper proposes a novel network structure combining dense skip connections and full-scale skip connections, named UNet-sharp (UNet\#) for its shape similar to symbol \#. The proposed UNet\# can aggregate feature maps of different scales in the decoder sub-network and capture fine-grained details and coarse-grained semantics from the full scale, which benefits learning the exact location and accurately segmenting the boundary of organs or lesions. We perform deep supervision for model pruning to speed up testing and make it possible for the model to run on mobile devices; furthermore, designing two classification-guided modules to reduce false positives achieves more accurate segmentation results. Various experiments of semantic segmentation and instance segmentation on different modalities (EM, CT, MRI) and dimensions (2D, 3D) datasets, including the nuclei, brain tumor, liver, and lung, demonstrate that the proposed method outperforms state-of-the-art models.



### Ranking-Based Siamese Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2205.11761v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11761v1)
- **Published**: 2022-05-24 03:46:40+00:00
- **Updated**: 2022-05-24 03:46:40+00:00
- **Authors**: Feng Tang, Qiang Ling
- **Comment**: To appear in CVPR2022
- **Journal**: None
- **Summary**: Current Siamese-based trackers mainly formulate the visual tracking into two independent subtasks, including classification and localization. They learn the classification subnetwork by processing each sample separately and neglect the relationship among positive and negative samples. Moreover, such tracking paradigm takes only the classification confidence of proposals for the final prediction, which may yield the misalignment between classification and localization. To resolve these issues, this paper proposes a ranking-based optimization algorithm to explore the relationship among different proposals. To this end, we introduce two ranking losses, including the classification one and the IoU-guided one, as optimization constraints. The classification ranking loss can ensure that positive samples rank higher than hard negative ones, i.e., distractors, so that the trackers can select the foreground samples successfully without being fooled by the distractors. The IoU-guided ranking loss aims to align classification confidence scores with the Intersection over Union(IoU) of the corresponding localization prediction for positive samples, enabling the well-localized prediction to be represented by high classification confidence. Specifically, the proposed two ranking losses are compatible with most Siamese trackers and incur no additional computation for inference. Extensive experiments on seven tracking benchmarks, including OTB100, UAV123, TC128, VOT2016, NFS30, GOT-10k and LaSOT, demonstrate the effectiveness of the proposed ranking-based optimization algorithm. The code and raw results are available at https://github.com/sansanfree/RBO.



### Multi-Augmentation for Efficient Visual Representation Learning for Self-supervised Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2205.11772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11772v1)
- **Published**: 2022-05-24 04:18:39+00:00
- **Updated**: 2022-05-24 04:18:39+00:00
- **Authors**: Van-Nhiem Tran, Chi-En Huang, Shen-Hsuan Liu, Kai-Lin Yang, Timothy Ko, Yung-Hui Li
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, self-supervised learning has been studied to deal with the limitation of available labeled-dataset. Among the major components of self-supervised learning, the data augmentation pipeline is one key factor in enhancing the resulting performance. However, most researchers manually designed the augmentation pipeline, and the limited collections of transformation may cause the lack of robustness of the learned feature representation. In this work, we proposed Multi-Augmentations for Self-Supervised Representation Learning (MA-SSRL), which fully searched for various augmentation policies to build the entire pipeline to improve the robustness of the learned feature representation. MA-SSRL successfully learns the invariant feature representation and presents an efficient, effective, and adaptable data augmentation pipeline for self-supervised pre-training on different distribution and domain datasets. MA-SSRL outperforms the previous state-of-the-art methods on transfer and semi-supervised benchmarks while requiring fewer training epochs.



### AFNet-M: Adaptive Fusion Network with Masks for 2D+3D Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.11785v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.11785v1)
- **Published**: 2022-05-24 04:56:55+00:00
- **Updated**: 2022-05-24 04:56:55+00:00
- **Authors**: Mingzhe Sui, Hanting Li, Zhaoqing Zhu, Feng Zhao
- **Comment**: 6 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: 2D+3D facial expression recognition (FER) can effectively cope with illumination changes and pose variations by simultaneously merging 2D texture and more robust 3D depth information. Most deep learning-based approaches employ the simple fusion strategy that concatenates the multimodal features directly after fully-connected layers, without considering the different degrees of significance for each modality. Meanwhile, how to focus on both 2D and 3D local features in salient regions is still a great challenge. In this letter, we propose the adaptive fusion network with masks (AFNet-M) for 2D+3D FER. To enhance 2D and 3D local features, we take the masks annotating salient regions of the face as prior knowledge and design the mask attention module (MA) which can automatically learn two modulation vectors to adjust the feature maps. Moreover, we introduce a novel fusion strategy that can perform adaptive fusion at convolutional layers through the designed importance weights computing module (IWC). Experimental results demonstrate that our AFNet-M achieves the state-of-the-art performance on BU-3DFE and Bosphorus datasets and requires fewer parameters in comparison with other models.



### G-Rep: Gaussian Representation for Arbitrary-Oriented Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.11796v2
- **DOI**: 10.3390/rs15030757
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11796v2)
- **Published**: 2022-05-24 05:28:08+00:00
- **Updated**: 2023-01-28 12:17:04+00:00
- **Authors**: Liping Hou, Ke Lu, Xue Yang, Yuqiu Li, Jian Xue
- **Comment**: 24 pages, 6 figures, 10 tables, the code has been open sourced at
  https://github.com/open-mmlab/mmrotate
- **Journal**: Remote Sens. 2023, 15(3), 757
- **Summary**: Typical representations for arbitrary-oriented object detection tasks include oriented bounding box (OBB), quadrilateral bounding box (QBB), and point set (PointSet). Each representation encounters problems that correspond to its characteristics, such as the boundary discontinuity, square-like problem, representation ambiguity, and isolated points, which lead to inaccurate detection. Although many effective strategies have been proposed for various representations, there is still no unified solution. Current detection methods based on Gaussian modeling have demonstrated the possibility of breaking this dilemma; however, they remain limited to OBB. To go further, in this paper, we propose a unified Gaussian representation called G-Rep to construct Gaussian distributions for OBB, QBB, and PointSet, which achieves a unified solution to various representations and problems. Specifically, PointSet or QBB-based object representations are converted into Gaussian distributions, and their parameters are optimized using the maximum likelihood estimation algorithm. Then, three optional Gaussian metrics are explored to optimize the regression loss of the detector because of their excellent parameter optimization mechanisms. Furthermore, we also use Gaussian metrics for sampling to align label assignment and regression loss. Experimental results on several public available datasets, such as DOTA, HRSC2016, UCAS-AOD, and ICDAR2015, show the excellent performance of the proposed method for arbitrary-oriented object detection.



### Symbolic Expression Transformer: A Computer Vision Approach for Symbolic Regression
- **Arxiv ID**: http://arxiv.org/abs/2205.11798v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.11798v2)
- **Published**: 2022-05-24 05:35:46+00:00
- **Updated**: 2022-06-15 07:58:03+00:00
- **Authors**: Jiachen Li, Ye Yuan, Hong-Bin Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Symbolic Regression (SR) is a type of regression analysis to automatically find the mathematical expression that best fits the data. Currently, SR still basically relies on various searching strategies so that a sample-specific model is required to be optimized for every expression, which significantly limits the model's generalization and efficiency. Inspired by the fact that human beings can infer a mathematical expression based on the curve of it, we propose Symbolic Expression Transformer (SET), a sample-agnostic model from the perspective of computer vision for SR. Specifically, the collected data is represented as images and an image caption model is employed for translating images to symbolic expressions. A large-scale dataset without overlap between training and testing sets in the image domain is released. Our results demonstrate the effectiveness of SET and suggest the promising direction of image-based model for solving the challenging SR problem.



### SALAD: Source-free Active Label-Agnostic Domain Adaptation for Classification, Segmentation and Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.12840v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.12840v3)
- **Published**: 2022-05-24 05:50:49+00:00
- **Updated**: 2022-10-22 11:57:13+00:00
- **Authors**: Divya Kothandaraman, Sumit Shekhar, Abhilasha Sancheti, Manoj Ghuhan, Tripti Shukla, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel method, SALAD, for the challenging vision task of adapting a pre-trained "source" domain network to a "target" domain, with a small budget for annotation in the "target" domain and a shift in the label space. Further, the task assumes that the source data is not available for adaptation, due to privacy concerns or otherwise. We postulate that such systems need to jointly optimize the dual task of (i) selecting fixed number of samples from the target domain for annotation and (ii) transfer of knowledge from the pre-trained network to the target domain. To do this, SALAD consists of a novel Guided Attention Transfer Network (GATN) and an active learning function, HAL. The GATN enables feature distillation from pre-trained network to the target network, complemented with the target samples mined by HAL using transfer-ability and uncertainty criteria. SALAD has three key benefits: (i) it is task-agnostic, and can be applied across various visual tasks such as classification, segmentation and detection; (ii) it can handle shifts in output label space from the pre-trained source network to the target domain; (iii) it does not require access to source data for adaptation. We conduct extensive experiments across 3 visual tasks, viz. digits classification (MNIST, SVHN, VISDA), synthetic (GTA5) to real (CityScapes) image segmentation, and document layout detection (PubLayNet to DSSE). We show that our source-free approach, SALAD, results in an improvement of 0.5%-31.3%(across datasets and tasks) over prior adaptation methods that assume access to large amounts of annotated source data for adaptation.



### Package Theft Detection from Smart Home Security Cameras
- **Arxiv ID**: http://arxiv.org/abs/2205.11804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11804v1)
- **Published**: 2022-05-24 05:54:19+00:00
- **Updated**: 2022-05-24 05:54:19+00:00
- **Authors**: Hung-Min Hsu, Xinyu Yuan, Baohua Zhu, Zhongwei Cheng, Lin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Package theft detection has been a challenging task mainly due to lack of training data and a wide variety of package theft cases in reality. In this paper, we propose a new Global and Local Fusion Package Theft Detection Embedding (GLF-PTDE) framework to generate package theft scores for each segment within a video to fulfill the real-world requirements on package theft detection. Moreover, we construct a novel Package Theft Detection dataset to facilitate the research on this task. Our method achieves 80% AUC performance on the newly proposed dataset, showing the effectiveness of the proposed GLF-PTDE framework and its robustness in different real scenes for package theft detection.



### Learning to Assemble Geometric Shapes
- **Arxiv ID**: http://arxiv.org/abs/2205.11809v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11809v2)
- **Published**: 2022-05-24 06:07:13+00:00
- **Updated**: 2022-06-20 06:06:06+00:00
- **Authors**: Jinhwi Lee, Jungtaek Kim, Hyunsoo Chung, Jaesik Park, Minsu Cho
- **Comment**: 11 pages, 9 figures, 9 tables. Accepted at the 31st International
  Joint Conference on Artificial Intelligence (IJCAI 2022). J. Lee and J. Kim
  equally contributed
- **Journal**: None
- **Summary**: Assembling parts into an object is a combinatorial problem that arises in a variety of contexts in the real world and involves numerous applications in science and engineering. Previous related work tackles limited cases with identical unit parts or jigsaw-style parts of textured shapes, which greatly mitigate combinatorial challenges of the problem. In this work, we introduce the more challenging problem of shape assembly, which involves textureless fragments of arbitrary shapes with indistinctive junctions, and then propose a learning-based approach to solving it. We demonstrate the effectiveness on shape assembly tasks with various scenarios, including the ones with abnormal fragments (e.g., missing and distorted), the different number of fragments, and different rotation discretization.



### Thunder: Thumbnail based Fast Lightweight Image Denoising Network
- **Arxiv ID**: http://arxiv.org/abs/2205.11823v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.11823v1)
- **Published**: 2022-05-24 06:38:46+00:00
- **Updated**: 2022-05-24 06:38:46+00:00
- **Authors**: Yifeng Zhou, Xing Xu, Shuaicheng Liu, Guoqing Wang, Huimin Lu, Heng Tao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: To achieve promising results on removing noise from real-world images, most of existing denoising networks are formulated with complex network structure, making them impractical for deployment. Some attempts focused on reducing the number of filters and feature channels but suffered from large performance loss, and a more practical and lightweight denoising network with fast inference speed is of high demand.   To this end, a \textbf{Thu}mb\textbf{n}ail based \textbf{D}\textbf{e}noising Netwo\textbf{r}k dubbed Thunder, is proposed and implemented as a lightweight structure for fast restoration without comprising the denoising capabilities. Specifically, the Thunder model contains two newly-established modules:   (1) a wavelet-based Thumbnail Subspace Encoder (TSE) which can leverage sub-bands correlation to provide an approximate thumbnail based on the low-frequent feature; (2) a Subspace Projection based Refine Module (SPR) which can restore the details for thumbnail progressively based on the subspace projection approach.   Extensive experiments have been carried out on two real-world denoising benchmarks, demonstrating that the proposed Thunder outperforms the existing lightweight models and achieves competitive performance on PSNR and SSIM when compared with the complex designs.



### Unsupervised Difference Learning for Noisy Rigid Image Alignment
- **Arxiv ID**: http://arxiv.org/abs/2205.11829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11829v1)
- **Published**: 2022-05-24 06:48:57+00:00
- **Updated**: 2022-05-24 06:48:57+00:00
- **Authors**: Yu-Xuan Chen, Dagan Feng, Hong-Bin Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Rigid image alignment is a fundamental task in computer vision, while the traditional algorithms are either too sensitive to noise or time-consuming. Recent unsupervised image alignment methods developed based on spatial transformer networks show an improved performance on clean images but will not achieve satisfactory performance on noisy images due to its heavy reliance on pixel value comparations. To handle such challenging applications, we report a new unsupervised difference learning (UDL) strategy and apply it to rigid image alignment. UDL exploits the quantitative properties of regression tasks and converts the original unsupervised problem to pseudo supervised problem. Under the new UDL-based image alignment pipeline, rotation can be accurately estimated on both clean and noisy images and translations can then be easily solved. Experimental results on both nature and cryo-EM images demonstrate the efficacy of our UDL-based unsupervised rigid image alignment method.



### TraCon: A novel dataset for real-time traffic cones detection using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2205.11830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11830v1)
- **Published**: 2022-05-24 06:51:58+00:00
- **Updated**: 2022-05-24 06:51:58+00:00
- **Authors**: Iason Katsamenis, Eleni Eirini Karolou, Agapi Davradou, Eftychios Protopapadakis, Anastasios Doulamis, Nikolaos Doulamis, Dimitris Kalogeras
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Substantial progress has been made in the field of object detection in road scenes. However, it is mainly focused on vehicles and pedestrians. To this end, we investigate traffic cone detection, an object category crucial for road effects and maintenance. In this work, the YOLOv5 algorithm is employed, in order to find a solution for the efficient and fast detection of traffic cones. The YOLOv5 can achieve a high detection accuracy with the score of IoU up to 91.31%. The proposed method is been applied to an RGB roadwork image dataset, collected from various sources.



### CDFKD-MFS: Collaborative Data-free Knowledge Distillation via Multi-level Feature Sharing
- **Arxiv ID**: http://arxiv.org/abs/2205.11845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11845v1)
- **Published**: 2022-05-24 07:11:03+00:00
- **Updated**: 2022-05-24 07:11:03+00:00
- **Authors**: Zhiwei Hao, Yong Luo, Zhi Wang, Han Hu, Jianping An
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the compression and deployment of powerful deep neural networks (DNNs) on resource-limited edge devices to provide intelligent services have become attractive tasks. Although knowledge distillation (KD) is a feasible solution for compression, its requirement on the original dataset raises privacy concerns. In addition, it is common to integrate multiple pretrained models to achieve satisfactory performance. How to compress multiple models into a tiny model is challenging, especially when the original data are unavailable. To tackle this challenge, we propose a framework termed collaborative data-free knowledge distillation via multi-level feature sharing (CDFKD-MFS), which consists of a multi-header student module, an asymmetric adversarial data-free KD module, and an attention-based aggregation module. In this framework, the student model equipped with a multi-level feature-sharing structure learns from multiple teacher models and is trained together with a generator in an asymmetric adversarial manner. When some real samples are available, the attention module adaptively aggregates predictions of the student headers, which can further improve performance. We conduct extensive experiments on three popular computer visual datasets. In particular, compared with the most competitive alternative, the accuracy of the proposed framework is 1.18\% higher on the CIFAR-100 dataset, 1.67\% higher on the Caltech-101 dataset, and 2.99\% higher on the mini-ImageNet dataset.



### Collaborative 3D Object Detection for Automatic Vehicle Systems via Learnable Communications
- **Arxiv ID**: http://arxiv.org/abs/2205.11849v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.11849v1)
- **Published**: 2022-05-24 07:17:32+00:00
- **Updated**: 2022-05-24 07:17:32+00:00
- **Authors**: Junyong Wang, Yuan Zeng, Yi Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate detection of objects in 3D point clouds is a key problem in autonomous driving systems. Collaborative perception can incorporate information from spatially diverse sensors and provide significant benefits for improving the perception accuracy of autonomous driving systems. In this work, we consider that the autonomous vehicle uses local point cloud data and combines information from neighboring infrastructures through wireless links for cooperative 3D object detection. However, information sharing among vehicle and infrastructures in predefined communication schemes may result in communication congestion and/or bring limited performance improvement. To this end, we propose a novel collaborative 3D object detection framework that consists of three components: feature learning networks that map point clouds into feature maps; an efficient communication block that propagates compact and fine-grained query feature maps from vehicle to support infrastructures and optimizes attention weights between query and key to refine support feature maps; a region proposal network that fuses local feature maps and weighted support feature maps for 3D object detection. We evaluate the performance of the proposed framework using a synthetic cooperative dataset created in two complex driving scenarios: a roundabout and a T-junction. Experiment results and bandwidth usage analysis demonstrate that our approach can save communication and computation costs and significantly improve detection performance under different detection difficulties in all scenarios.



### Unsupervised Misaligned Infrared and Visible Image Fusion via Cross-Modality Image Generation and Registration
- **Arxiv ID**: http://arxiv.org/abs/2205.11876v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11876v1)
- **Published**: 2022-05-24 07:51:57+00:00
- **Updated**: 2022-05-24 07:51:57+00:00
- **Authors**: Di Wang, Jinyuan Liu, Xin Fan, Risheng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent learning-based image fusion methods have marked numerous progress in pre-registered multi-modality data, but suffered serious ghosts dealing with misaligned multi-modality data, due to the spatial deformation and the difficulty narrowing cross-modality discrepancy. To overcome the obstacles, in this paper, we present a robust cross-modality generation-registration paradigm for unsupervised misaligned infrared and visible image fusion (IVIF). Specifically, we propose a Cross-modality Perceptual Style Transfer Network (CPSTN) to generate a pseudo infrared image taking a visible image as input. Benefiting from the favorable geometry preservation ability of the CPSTN, the generated pseudo infrared image embraces a sharp structure, which is more conducive to transforming cross-modality image alignment into mono-modality registration coupled with the structure-sensitive of the infrared image. In this case, we introduce a Multi-level Refinement Registration Network (MRRN) to predict the displacement vector field between distorted and pseudo infrared images and reconstruct registered infrared image under the mono-modality setting. Moreover, to better fuse the registered infrared images and visible images, we present a feature Interaction Fusion Module (IFM) to adaptively select more meaningful features for fusion in the Dual-path Interaction Fusion Network (DIFN). Extensive experimental results suggest that the proposed method performs superior capability on misaligned cross-modality image fusion.



### Hierarchical Vectorization for Portrait Images
- **Arxiv ID**: http://arxiv.org/abs/2205.11880v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2205.11880v1)
- **Published**: 2022-05-24 07:58:41+00:00
- **Updated**: 2022-05-24 07:58:41+00:00
- **Authors**: Qian Fu, Linlin Liu, Fei Hou, Ying He
- **Comment**: None
- **Journal**: None
- **Summary**: Aiming at developing intuitive and easy-to-use portrait editing tools, we propose a novel vectorization method that can automatically convert raster images into a 3-tier hierarchical representation. The base layer consists of a set of sparse diffusion curves (DC) which characterize salient geometric features and low-frequency colors and provide means for semantic color transfer and facial expression editing. The middle level encodes specular highlights and shadows to large and editable Poisson regions (PR) and allows the user to directly adjust illumination via tuning the strength and/or changing shape of PR. The top level contains two types of pixel-sized PRs for high-frequency residuals and fine details such as pimples and pigmentation. We also train a deep generative model that can produce high-frequency residuals automatically. Thanks to the meaningful organization of vector primitives, editing portraits becomes easy and intuitive. In particular, our method supports color transfer, facial expression editing, highlight and shadow editing and automatic retouching. Thanks to the linearity of the Laplace operator, we introduce alpha blending, linear dodge and linear burn to vector editing and show that they are effective in editing highlights and shadows. To quantitatively evaluate the results, we extend the commonly used FLIP metric (which measures differences between two images) by considering illumination. The new metric, called illumination-sensitive FLIP or IS-FLIP, can effectively capture the salient changes in color transfer results, and is more consistent with human perception than FLIP and other quality measures on portrait images. We evaluate our method on the FFHQR dataset and show that our method is effective for common portrait editing tasks, such as retouching, light editing, color transfer and expression editing. We will make the code and trained models publicly available.



### Mind The Gap: Alleviating Local Imbalance for Unsupervised Cross-Modality Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.11888v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11888v2)
- **Published**: 2022-05-24 08:16:58+00:00
- **Updated**: 2022-09-05 13:51:58+00:00
- **Authors**: Zixian Su, Kai Yao, Xi Yang, Qiufeng Wang, Yuyao Yan, Jie Sun, Kaizhu Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised cross-modality medical image adaptation aims to alleviate the severe domain gap between different imaging modalities without using the target domain label. A key in this campaign relies upon aligning the distributions of source and target domain. One common attempt is to enforce the global alignment between two domains, which, however, ignores the fatal local-imbalance domain gap problem, i.e., some local features with larger domain gap are harder to transfer. Recently, some methods conduct alignment focusing on local regions to improve the efficiency of model learning. While this operation may cause a deficiency of critical information from contexts. To tackle this limitation, we propose a novel strategy to alleviate the domain gap imbalance considering the characteristics of medical images, namely Global-Local Union Alignment. Specifically, a feature-disentanglement style-transfer module first synthesizes the target-like source-content images to reduce the global domain gap. Then, a local feature mask is integrated to reduce the 'inter-gap' for local features by prioritizing those discriminative features with larger domain gap. This combination of global and local alignment can precisely localize the crucial regions in segmentation target while preserving the overall semantic consistency. We conduct a series of experiments with two cross-modality adaptation tasks, i,e. cardiac substructure and abdominal multi-organ segmentation. Experimental results indicate that our method achieves state-of-the-art performance in both tasks.



### An interpretation of the final fully connected layer
- **Arxiv ID**: http://arxiv.org/abs/2205.11908v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.11908v1)
- **Published**: 2022-05-24 09:05:19+00:00
- **Updated**: 2022-05-24 09:05:19+00:00
- **Authors**: Siddhartha
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years neural networks have achieved state-of-the-art accuracy for various tasks but the the interpretation of the generated outputs still remains difficult. In this work we attempt to provide a method to understand the learnt weights in the final fully connected layer in image classification models. We motivate our method by drawing a connection between the policy gradient objective in RL and supervised learning objective. We suggest that the commonly used cross entropy based supervised learning objective can be regarded as a special case of the policy gradient objective. Using this insight we propose a method to find the most discriminative and confusing parts of an image. Our method does not make any prior assumption about neural network achitecture and has low computational cost. We apply our method on publicly available pre-trained models and report the generated results.



### Robust 3D Object Detection in Cold Weather Conditions
- **Arxiv ID**: http://arxiv.org/abs/2205.11925v2
- **DOI**: 10.1109/IV51971.2022.9827398
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.11925v2)
- **Published**: 2022-05-24 09:37:07+00:00
- **Updated**: 2022-07-25 14:18:03+00:00
- **Authors**: Aldi Piroli, Vinzenz Dallabetta, Marc Walessa, Daniel Meissner, Johannes Kopp, Klaus Dietmayer
- **Comment**: Oral
- **Journal**: 2022 IEEE Intelligent Vehicles Symposium (IV)
- **Summary**: Adverse weather conditions can negatively affect LiDAR-based object detectors. In this work, we focus on the phenomenon of vehicle gas exhaust condensation in cold weather conditions. This everyday effect can influence the estimation of object sizes, orientations and introduce ghost object detections, compromising the reliability of the state of the art object detectors. We propose to solve this problem by using data augmentation and a novel training loss term. To effectively train deep neural networks, a large set of labeled data is needed. In case of adverse weather conditions, this process can be extremely laborious and expensive. We address this issue in two steps: First, we present a gas exhaust data generation method based on 3D surface reconstruction and sampling which allows us to generate large sets of gas exhaust clouds from a small pool of labeled data. Second, we introduce a point cloud augmentation process that can be used to add gas exhaust to datasets recorded in good weather conditions. Finally, we formulate a new training loss term that leverages the augmented point cloud to increase object detection robustness by penalizing predictions that include noise. In contrast to other works, our method can be used with both grid-based and point-based detectors. Moreover, since our approach does not require any network architecture changes, inference times remain unchanged. Experimental results on real data show that our proposed method greatly increases robustness to gas exhaust and noisy data.



### Image Trinarization Using a Partial Differential Equations: A Novel Approach to Automatic Sperm Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2205.11927v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, 35K55, 65M12, 92C55
- **Links**: [PDF](http://arxiv.org/pdf/2205.11927v1)
- **Published**: 2022-05-24 09:38:45+00:00
- **Updated**: 2022-05-24 09:38:45+00:00
- **Authors**: B. A. Jacobs
- **Comment**: None
- **Journal**: None
- **Summary**: Partial differential equations have recently garnered substantial attention as an image processing framework due to their extensibility, the ability to rigorously engineer and analyse the governing dynamics as well as the ease of implementation using numerical methods. This paper explores a novel approach to image trinarization with a concrete real-world application of classifying regions of sperm images used in the automatic analysis of sperm morphology. The proposed methodology engineers a diffusion equation with non-linear source term, exhibiting three steady-states. The model is implemented as an image processor using a standard finite difference method to illustrate the efficacy of the proposed approach. The performance of the proposed approach is benchmarked against standard image clustering/segmentation methods and shown to be highly effective.



### GraSens: A Gabor Residual Anti-aliasing Sensing Framework for Action Recognition using WiFi
- **Arxiv ID**: http://arxiv.org/abs/2205.11945v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2205.11945v1)
- **Published**: 2022-05-24 10:20:16+00:00
- **Updated**: 2022-05-24 10:20:16+00:00
- **Authors**: Yanling Hao, Zhiyuan Shi, Xidong Mu, Yuanwei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: WiFi-based human action recognition (HAR) has been regarded as a promising solution in applications such as smart living and remote monitoring due to the pervasive and unobtrusive nature of WiFi signals. However, the efficacy of WiFi signals is prone to be influenced by the change in the ambient environment and varies over different sub-carriers. To remedy this issue, we propose an end-to-end Gabor residual anti-aliasing sensing network (GraSens) to directly recognize the actions using the WiFi signals from the wireless devices in diverse scenarios. In particular, a new Gabor residual block is designed to address the impact of the changing surrounding environment with a focus on learning reliable and robust temporal-frequency representations of WiFi signals. In each block, the Gabor layer is integrated with the anti-aliasing layer in a residual manner to gain the shift-invariant features. Furthermore, fractal temporal and frequency self-attention are proposed in a joint effort to explicitly concentrate on the efficacy of WiFi signals and thus enhance the quality of output features scattered in different subcarriers. Experimental results throughout our wireless-vision action recognition dataset (WVAR) and three public datasets demonstrate that our proposed GraSens scheme outperforms state-of-the-art methods with respect to recognition accuracy.



### SHARP: Shape-Aware Reconstruction of People in Loose Clothing
- **Arxiv ID**: http://arxiv.org/abs/2205.11948v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11948v1)
- **Published**: 2022-05-24 10:26:42+00:00
- **Updated**: 2022-05-24 10:26:42+00:00
- **Authors**: Sai Sagar Jinka, Astitva Srivastava, Chandradeep Pokhariya, Avinash Sharma, P. J. Narayanan
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: Recent advancements in deep learning have enabled 3D human body reconstruction from a monocular image, which has broad applications in multiple domains. In this paper, we propose SHARP (SHape Aware Reconstruction of People in loose clothing), a novel end-to-end trainable network that accurately recovers the 3D geometry and appearance of humans in loose clothing from a monocular image. SHARP uses a sparse and efficient fusion strategy to combine parametric body prior with a non-parametric 2D representation of clothed humans. The parametric body prior enforces geometrical consistency on the body shape and pose, while the non-parametric representation models loose clothing and handle self-occlusions as well. We also leverage the sparseness of the non-parametric representation for faster training of our network while using losses on 2D maps. Another key contribution is 3DHumans, our new life-like dataset of 3D human body scans with rich geometrical and textural details. We evaluate SHARP on 3DHumans and other publicly available datasets and show superior qualitative and quantitative performance than existing state-of-the-art methods.



### Diffuse Map Guiding Unsupervised Generative Adversarial Network for SVBRDF Estimation
- **Arxiv ID**: http://arxiv.org/abs/2205.11951v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2205.11951v2)
- **Published**: 2022-05-24 10:32:27+00:00
- **Updated**: 2022-05-25 11:58:26+00:00
- **Authors**: Zhiyao Luo, Hongnan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstructing materials in the real world has always been a difficult problem in computer graphics. Accurately reconstructing the material in the real world is critical in the field of realistic rendering. Traditionally, materials in computer graphics are mapped by an artist, then mapped onto a geometric model by coordinate transformation, and finally rendered with a rendering engine to get realistic materials. For opaque objects, the industry commonly uses physical-based bidirectional reflectance distribution function (BRDF) rendering models for material modeling. The commonly used physical-based rendering models are Cook-Torrance BRDF, Disney BRDF. In this paper, we use the Cook-Torrance model to reconstruct the materials. The SVBRDF material parameters include Normal, Diffuse, Specular and Roughness. This paper presents a Diffuse map guiding material estimation method based on the Generative Adversarial Network(GAN). This method can predict plausible SVBRDF maps with global features using only a few pictures taken by the mobile phone. The main contributions of this paper are: 1) We preprocess a small number of input pictures to produce a large number of non-repeating pictures for training to reduce over-fitting. 2) We use a novel method to directly obtain the guessed diffuse map with global characteristics, which provides more prior information for the training process. 3) We improve the network architecture of the generator so that it can generate fine details of normal maps and reduce the possibility to generate over-flat normal maps. The method used in this paper can obtain prior knowledge without using dataset training, which greatly reduces the difficulty of material reconstruction and saves a lot of time to generate and calibrate datasets.



### 3D helical CT reconstruction with memory efficient invertible Learned Primal-Dual method
- **Arxiv ID**: http://arxiv.org/abs/2205.11952v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11952v1)
- **Published**: 2022-05-24 10:32:32+00:00
- **Updated**: 2022-05-24 10:32:32+00:00
- **Authors**: Buda BajiÄ‡, Ozan Ã–ktem, Jevgenija Rudzusika
- **Comment**: None
- **Journal**: None
- **Summary**: Helical acquisition geometry is the most common geometry used in computed tomography (CT) scanners for medical imaging. We adapt the invertible Learned Primal-Dual (iLPD) deep neural network architecture so that it can be applied to helical 3D CT reconstruction. We achieve this by splitting the geometry and the data in parts that fit the memory and by splitting images into corresponding sub-volumes. The architecture can be applied to images different in size along the rotation axis. We perform the experiments on tomographic data simulated from realistic helical geometries.



### A Wireless-Vision Dataset for Privacy Preserving Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.11962v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2205.11962v1)
- **Published**: 2022-05-24 10:49:11+00:00
- **Updated**: 2022-05-24 10:49:11+00:00
- **Authors**: Yanling Hao, Zhiyuan Shi, Yuanwei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Human Activity Recognition (HAR) has recently received remarkable attention in numerous applications such as assisted living and remote monitoring. Existing solutions based on sensors and vision technologies have obtained achievements but still suffering from considerable limitations in the environmental requirement. Wireless signals like WiFi-based sensing have emerged as a new paradigm since it is convenient and not restricted in the environment. In this paper, a new WiFi-based and video-based neural network (WiNN) is proposed to improve the robustness of activity recognition where the synchronized video serves as the supplement for the wireless data. Moreover, a wireless-vision benchmark (WiVi) is collected for 9 class actions recognition in three different visual conditions, including the scenes without occlusion, with partial occlusion, and with full occlusion. Both machine learning methods - support vector machine (SVM) as well as deep learning methods are used for the accuracy verification of the data set. Our results show that WiVi data set satisfies the primary demand and all three branches in the proposed pipeline keep more than $80\%$ of activity recognition accuracy over multiple action segmentation from 1s to 3s. In particular, WiNN is the most robust method in terms of all the actions on three action segmentation compared to the others.



### Generative Models for Reproducible Coronary Calcium Scoring
- **Arxiv ID**: http://arxiv.org/abs/2205.11967v1
- **DOI**: 10.1117/1.JMI.9.5.052406
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.11967v1)
- **Published**: 2022-05-24 10:59:32+00:00
- **Updated**: 2022-05-24 10:59:32+00:00
- **Authors**: Sanne G. M. van Velzen, Bob D. de Vos, Julia M. H. Noothout, Helena M. Verkooijen, Max A. Viergever, Ivana IÅ¡gum
- **Comment**: In press
- **Journal**: Journal of Medical Imaging 9(5) 052406 (31 May 2022)
- **Summary**: Purpose: Coronary artery calcium (CAC) score, i.e. the amount of CAC quantified in CT, is a strong and independent predictor of coronary heart disease (CHD) events. However, CAC scoring suffers from limited interscan reproducibility, which is mainly due to the clinical definition requiring application of a fixed intensity level threshold for segmentation of calcifications. This limitation is especially pronounced in non-ECG-synchronized CT where lesions are more impacted by cardiac motion and partial volume effects. Therefore, we propose a CAC quantification method that does not require a threshold for segmentation of CAC. Approach: Our method utilizes a generative adversarial network where a CT with CAC is decomposed into an image without CAC and an image showing only CAC. The method, using a CycleGAN, was trained using 626 low-dose chest CTs and 514 radiotherapy treatment planning CTs. Interscan reproducibility was compared to clinical calcium scoring in radiotherapy treatment planning CTs of 1,662 patients, each having two scans. Results: A lower relative interscan difference in CAC mass was achieved by the proposed method: 47% compared to 89% manual clinical calcium scoring. The intraclass correlation coefficient of Agatston scores was 0.96 for the proposed method compared to 0.91 for automatic clinical calcium scoring. Conclusions: The increased interscan reproducibility achieved by our method may lead to increased reliability of CHD risk categorization and improved accuracy of CHD event prediction.



### OPOM: Customized Invisible Cloak towards Face Privacy Protection
- **Arxiv ID**: http://arxiv.org/abs/2205.11981v1
- **DOI**: 10.1109/TPAMI.2022.3175602
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11981v1)
- **Published**: 2022-05-24 11:29:37+00:00
- **Updated**: 2022-05-24 11:29:37+00:00
- **Authors**: Yaoyao Zhong, Weihong Deng
- **Comment**: This article has been accepted by IEEE Transactions on Pattern
  Analysis & Machine Intelligence. Datasets and code are available at
  https://github.com/zhongyy/OPOM
- **Journal**: None
- **Summary**: While convenient in daily life, face recognition technologies also raise privacy concerns for regular users on the social media since they could be used to analyze face images and videos, efficiently and surreptitiously without any security restrictions. In this paper, we investigate the face privacy protection from a technology standpoint based on a new type of customized cloak, which can be applied to all the images of a regular user, to prevent malicious face recognition systems from uncovering their identity. Specifically, we propose a new method, named one person one mask (OPOM), to generate person-specific (class-wise) universal masks by optimizing each training sample in the direction away from the feature subspace of the source identity. To make full use of the limited training images, we investigate several modeling methods, including affine hulls, class centers, and convex hulls, to obtain a better description of the feature subspace of source identities. The effectiveness of the proposed method is evaluated on both common and celebrity datasets against black-box face recognition models with different loss functions and network architectures. In addition, we discuss the advantages and potential problems of the proposed method. In particular, we conduct an application study on the privacy protection of a video dataset, Sherlock, to demonstrate the potential practical usage of the proposed method. Datasets and code are available at https://github.com/zhongyy/OPOM.



### mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections
- **Arxiv ID**: http://arxiv.org/abs/2205.12005v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.12005v2)
- **Published**: 2022-05-24 11:52:06+00:00
- **Updated**: 2022-05-25 05:49:30+00:00
- **Authors**: Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng Cao, Ji Zhang, Songfang Huang, Fei Huang, Jingren Zhou, Luo Si
- **Comment**: None
- **Journal**: EMNLP2022
- **Summary**: Large-scale pretrained foundation models have been an emerging paradigm for building artificial intelligence (AI) systems, which can be quickly adapted to a wide range of downstream tasks. This paper presents mPLUG, a new vision-language foundation model for both cross-modal understanding and generation. Most existing pre-trained models suffer from the problems of low computational efficiency and information asymmetry brought by the long visual sequence in cross-modal alignment. To address these problems, mPLUG introduces an effective and efficient vision-language architecture with novel cross-modal skip-connections, which creates inter-layer shortcuts that skip a certain number of layers for time-consuming full self-attention on the vision side. mPLUG is pre-trained end-to-end on large-scale image-text pairs with both discriminative and generative objectives. It achieves state-of-the-art results on a wide range of vision-language downstream tasks, such as image captioning, image-text retrieval, visual grounding and visual question answering. mPLUG also demonstrates strong zero-shot transferability when directly transferred to multiple video-language tasks.



### SFace: Sigmoid-Constrained Hypersphere Loss for Robust Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.12010v1
- **DOI**: 10.1109/TIP.2020.3048632
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.12010v1)
- **Published**: 2022-05-24 11:54:15+00:00
- **Updated**: 2022-05-24 11:54:15+00:00
- **Authors**: Yaoyao Zhong, Weihong Deng, Jiani Hu, Dongyue Zhao, Xian Li, Dongchao Wen
- **Comment**: 12 pages, 9 figures
- **Journal**: IEEE Transactions on Image Processing, 2021
- **Summary**: Deep face recognition has achieved great success due to large-scale training databases and rapidly developing loss functions. The existing algorithms devote to realizing an ideal idea: minimizing the intra-class distance and maximizing the inter-class distance. However, they may neglect that there are also low quality training images which should not be optimized in this strict way. Considering the imperfection of training databases, we propose that intra-class and inter-class objectives can be optimized in a moderate way to mitigate overfitting problem, and further propose a novel loss function, named sigmoid-constrained hypersphere loss (SFace). Specifically, SFace imposes intra-class and inter-class constraints on a hypersphere manifold, which are controlled by two sigmoid gradient re-scale functions respectively. The sigmoid curves precisely re-scale the intra-class and inter-class gradients so that training samples can be optimized to some degree. Therefore, SFace can make a better balance between decreasing the intra-class distances for clean examples and preventing overfitting to the label noise, and contributes more robust deep face recognition models. Extensive experiments of models trained on CASIA-WebFace, VGGFace2, and MS-Celeb-1M databases, and evaluated on several face recognition benchmarks, such as LFW, MegaFace and IJB-C databases, have demonstrated the superiority of SFace.



### Naive Few-Shot Learning: Uncovering the fluid intelligence of machines
- **Arxiv ID**: http://arxiv.org/abs/2205.12013v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2205.12013v3)
- **Published**: 2022-05-24 12:00:39+00:00
- **Updated**: 2023-01-26 19:36:45+00:00
- **Authors**: Tomer Barak, Yonatan Loewenstein
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we aimed to help bridge the gap between human fluid intelligence - the ability to solve novel tasks without prior training - and the performance of deep neural networks, which typically require extensive prior training. An essential cognitive component for solving intelligence tests, which in humans are used to measure fluid intelligence, is the ability to identify regularities in sequences. This motivated us to construct a benchmark task, which we term \textit{sequence consistency evaluation} (SCE), whose solution requires the ability to identify regularities in sequences. Given the proven capabilities of deep networks, their ability to solve such tasks after extensive training is expected. Surprisingly, however, we show that naive (randomly initialized) deep learning models that are trained on a \textit{single} SCE with a \textit{single} optimization step can still solve non-trivial versions of the task relatively well. We extend our findings to solve, without any prior training, real-world anomaly detection tasks in the visual and auditory modalities. These results demonstrate the fluid-intelligent computational capabilities of deep networks. We discuss the implications of our work for constructing fluid-intelligent machines.



### Improving Human Image Synthesis with Residual Fast Fourier Transformation and Wasserstein Distance
- **Arxiv ID**: http://arxiv.org/abs/2205.12022v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.12022v2)
- **Published**: 2022-05-24 12:15:33+00:00
- **Updated**: 2022-05-26 06:16:01+00:00
- **Authors**: Jianhan Wu, Shijing Si, Jianzong Wang, Jing Xiao
- **Comment**: This paper is accepted by IJCNN2022
- **Journal**: None
- **Summary**: With the rapid development of the Metaverse, virtual humans have emerged, and human image synthesis and editing techniques, such as pose transfer, have recently become popular. Most of the existing techniques rely on GANs, which can generate good human images even with large variants and occlusions. But from our best knowledge, the existing state-of-the-art method still has the following problems: the first is that the rendering effect of the synthetic image is not realistic, such as poor rendering of some regions. And the second is that the training of GAN is unstable and slow to converge, such as model collapse. Based on the above two problems, we propose several methods to solve them. To improve the rendering effect, we use the Residual Fast Fourier Transform Block to replace the traditional Residual Block. Then, spectral normalization and Wasserstein distance are used to improve the speed and stability of GAN training. Experiments demonstrate that the methods we offer are effective at solving the problems listed above, and we get state-of-the-art scores in LPIPS and PSNR.



### Effect of Gender, Pose and Camera Distance on Human Body Dimensions Estimation
- **Arxiv ID**: http://arxiv.org/abs/2205.12028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.12028v1)
- **Published**: 2022-05-24 12:26:25+00:00
- **Updated**: 2022-05-24 12:26:25+00:00
- **Authors**: Yansel GÃ³nzalez Tejeda, Helmut A. Mayer
- **Comment**: Accepted to the workshop "Towards a Complete Analysis of People: From
  Face and Body to Clothes" (TCAP 2021) at the 21st International Conference on
  Image Analysis and Processing, May 23-27, 2022, Lecce, Italy
- **Journal**: None
- **Summary**: Human Body Dimensions Estimation (HBDE) is a task that an intelligent agent can perform to attempt to determine human body information from images (2D) or point clouds or meshes (3D). More specifically, if we define the HBDE problem as inferring human body measurements from images, then HBDE is a difficult, inverse, multi-task regression problem that can be tackled with machine learning techniques, particularly convolutional neural networks (CNN). Despite the community's tremendous effort to advance human shape analysis, there is a lack of systematic experiments to assess CNNs estimation of human body dimensions from images. Our contribution lies in assessing a CNN estimation performance in a series of controlled experiments. To that end, we augment our recently published neural anthropometer dataset by rendering images with different camera distance. We evaluate the network inference absolute and relative mean error between the estimated and actual HBDs. We train and evaluate the CNN in four scenarios: (1) training with subjects of a specific gender, (2) in a specific pose, (3) sparse camera distance and (4) dense camera distance. Not only our experiments demonstrate that the network can perform the task successfully, but also reveal a number of relevant facts that contribute to better understand the task of HBDE.



### VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification
- **Arxiv ID**: http://arxiv.org/abs/2205.12029v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.12029v3)
- **Published**: 2022-05-24 12:28:12+00:00
- **Updated**: 2023-05-11 15:31:06+00:00
- **Authors**: Souhail Bakkali, Zuheng Ming, Mickael Coustaty, MarÃ§al RusiÃ±ol, Oriol Ramos Terrades
- **Comment**: Accepted at PR
- **Journal**: None
- **Summary**: Multimodal learning from document data has achieved great success lately as it allows to pre-train semantically meaningful features as a prior into a learnable downstream task. In this paper, we approach the document classification problem by learning cross-modal representations through language and vision cues, considering intra- and inter-modality relationships. Instead of merging features from different modalities into a joint representation space, the proposed method exploits high-level interactions and learns relevant semantic information from effective attention flows within and across modalities. The proposed learning objective is devised between intra- and inter-modality alignment tasks, where the similarity distribution per task is computed by contracting positive sample pairs while simultaneously contrasting negative ones in the joint representation space}. Extensive experiments on public document classification datasets demonstrate the effectiveness and the generality of our model on low-scale and large-scale datasets.



### Privacy-Preserving Image Classification Using Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2205.12041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.12041v1)
- **Published**: 2022-05-24 12:51:48+00:00
- **Updated**: 2022-05-24 12:51:48+00:00
- **Authors**: Zheng Qi, AprilPyone MaungMaung, Yuma Kinoshita, Hitoshi Kiya
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a privacy-preserving image classification method that is based on the combined use of encrypted images and the vision transformer (ViT). The proposed method allows us not only to apply images without visual information to ViT models for both training and testing but to also maintain a high classification accuracy. ViT utilizes patch embedding and position embedding for image patches, so this architecture is shown to reduce the influence of block-wise image transformation. In an experiment, the proposed method for privacy-preserving image classification is demonstrated to outperform state-of-the-art methods in terms of classification accuracy and robustness against various attacks.



### Context Attention Network for Skeleton Extraction
- **Arxiv ID**: http://arxiv.org/abs/2205.12066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.12066v1)
- **Published**: 2022-05-24 13:34:13+00:00
- **Updated**: 2022-05-24 13:34:13+00:00
- **Authors**: Zixuan Huang, Yunfeng Wang, Zhiwen Chen, Xin Gao, Ruili Feng, Xiaobo Li
- **Comment**: Accepted at the Deep Learning for Geometric Computing (DLGC) workshop
  at CVPR 2022
- **Journal**: None
- **Summary**: Skeleton extraction is a task focused on providing a simple representation of an object by extracting the skeleton from the given binary or RGB image. In recent years many attractive works in skeleton extraction have been made. But as far as we know, there is little research on how to utilize the context information in the binary shape of objects. In this paper, we propose an attention-based model called Context Attention Network (CANet), which integrates the context extraction module in a UNet architecture and can effectively improve the ability of network to extract the skeleton pixels. Meanwhile, we also use some novel techniques including distance transform, weight focal loss to achieve good results on the given dataset. Finally, without model ensemble and with only 80% of the training images, our method achieves 0.822 F1 score during the development phase and 0.8507 F1 score during the final phase of the Pixel SkelNetOn Competition, ranking 1st place on the leaderboard.



### Classification of Phonological Parameters in Sign Languages
- **Arxiv ID**: http://arxiv.org/abs/2205.12072v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.12072v1)
- **Published**: 2022-05-24 13:40:45+00:00
- **Updated**: 2022-05-24 13:40:45+00:00
- **Authors**: Boris Mocialov, Graham Turner, Helen Hastie
- **Comment**: 18 pages, 27 figures
- **Journal**: None
- **Summary**: Signers compose sign language phonemes that enable communication by combining phonological parameters such as handshape, orientation, location, movement, and non-manual features. Linguistic research often breaks down signs into their constituent parts to study sign languages and often a lot of effort is invested into the annotation of the videos. In this work we show how a single model can be used to recognise the individual phonological parameters within sign languages with the aim of either to assist linguistic annotations or to describe the signs for the sign recognition models. We use Danish Sign Language data set `Ordbog over Dansk Tegnsprog' to generate multiple data sets using pose estimation model, which are then used for training the multi-label Fast R-CNN model to support multi-label modelling. Moreover, we show that there is a significant co-dependence between the orientation and location phonological parameters in the generated data and we incorporate this co-dependence in the model to achieve better performance.



### Sim-To-Real Transfer of Visual Grounding for Human-Aided Ambiguity Resolution
- **Arxiv ID**: http://arxiv.org/abs/2205.12089v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.12089v2)
- **Published**: 2022-05-24 14:12:32+00:00
- **Updated**: 2022-07-10 23:49:06+00:00
- **Authors**: Georgios Tziafas, Hamidreza Kasaei
- **Comment**: Accepted CoLLAs 2022
- **Journal**: None
- **Summary**: Service robots should be able to interact naturally with non-expert human users, not only to help them in various tasks but also to receive guidance in order to resolve ambiguities that might be present in the instruction. We consider the task of visual grounding, where the agent segments an object from a crowded scene given a natural language description. Modern holistic approaches to visual grounding usually ignore language structure and struggle to cover generic domains, therefore relying heavily on large datasets. Additionally, their transfer performance in RGB-D datasets suffers due to high visual discrepancy between the benchmark and the target domains. Modular approaches marry learning with domain modeling and exploit the compositional nature of language to decouple visual representation from language parsing, but either rely on external parsers or are trained in an end-to-end fashion due to the lack of strong supervision. In this work, we seek to tackle these limitations by introducing a fully decoupled modular framework for compositional visual grounding of entities, attributes, and spatial relations. We exploit rich scene graph annotations generated in a synthetic domain and train each module independently. Our approach is evaluated both in simulation and in two real RGB-D scene datasets. Experimental results show that the decoupled nature of our framework allows for easy integration with domain adaptation approaches for Sim-To-Real visual recognition, offering a data-efficient, robust, and interpretable solution to visual grounding in robotic applications.



### HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2205.12105v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2205.12105v2)
- **Published**: 2022-05-24 14:32:57+00:00
- **Updated**: 2022-05-31 08:14:53+00:00
- **Authors**: Feilong Chen, Xiuyi Chen, Jiaxin Shi, Duzhen Zhang, Jianlong Chang, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: In the past few years, the emergence of vision-language pre-training (VLP) has brought cross-modal retrieval to a new era. However, due to the latency and computation demand, it is commonly challenging to apply VLP in a real-time online retrieval system. To alleviate the defect, this paper proposes a \textbf{Hi}erarchical \textbf{V}ision-\textbf{}Language \textbf{P}re-Training (\textbf{HiVLP}) for fast Image-Text Retrieval (ITR). Specifically, we design a novel hierarchical retrieval objective, which uses the representation of different dimensions for coarse-to-fine ITR, i.e., using low-dimensional representation for large-scale coarse retrieval and high-dimensional representation for small-scale fine retrieval. We evaluate our proposed HiVLP on two popular image-text retrieval benchmarks, i.e., Flickr30k and COCO. Extensive experiments demonstrate that our HiVLP not only has fast inference speed but also can be easily scaled to large-scale ITR scenarios. The detailed results show that HiVLP is $1,427$$\sim$$120,649\times$ faster than the fusion-based model UNITER and 2$\sim$5 faster than the fastest embedding-based model LightingDot in different candidate scenarios. It also achieves about +4.9 AR on COCO and +3.8 AR on Flickr30K than LightingDot and achieves comparable performance with the state-of-the-art (SOTA) fusion-based model METER.



### Learning to Drive Using Sparse Imitation Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.12128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.12128v1)
- **Published**: 2022-05-24 15:03:11+00:00
- **Updated**: 2022-05-24 15:03:11+00:00
- **Authors**: Yuci Han, Alper Yilmaz
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose Sparse Imitation Reinforcement Learning (SIRL), a hybrid end-to-end control policy that combines the sparse expert driving knowledge with reinforcement learning (RL) policy for autonomous driving (AD) task in CARLA simulation environment. The sparse expert is designed based on hand-crafted rules which is suboptimal but provides a risk-averse strategy by enforcing experience for critical scenarios such as pedestrian and vehicle avoidance, and traffic light detection. As it has been demonstrated, training a RL agent from scratch is data-inefficient and time consuming particularly for the urban driving task, due to the complexity of situations stemming from the vast size of state space. Our SIRL strategy provides a solution to solve these problems by fusing the output distribution of the sparse expert policy and the RL policy to generate a composite driving policy. With the guidance of the sparse expert during the early training stage, SIRL strategy accelerates the training process and keeps the RL exploration from causing a catastrophe outcome, and ensures safe exploration. To some extent, the SIRL agent is imitating the driving expert's behavior. At the same time, it continuously gains knowledge during training therefore it keeps making improvement beyond the sparse expert, and can surpass both the sparse expert and a traditional RL agent. We experimentally validate the efficacy of proposed SIRL approach in a complex urban scenario within the CARLA simulator. Besides, we compare the SIRL agent's performance for risk-averse exploration and high learning efficiency with the traditional RL approach. We additionally demonstrate the SIRL agent's generalization ability to transfer the driving skill to unseen environment.



### Full-Reference Calibration-Free Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2205.12129v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.12129v1)
- **Published**: 2022-05-24 15:06:35+00:00
- **Updated**: 2022-05-24 15:06:35+00:00
- **Authors**: Elio D. Di Claudio, Paolo Giannitrapani, Giovanni Jacovitti
- **Comment**: None
- **Journal**: None
- **Summary**: One major problem of objective Image Quality Assessment (IQA) methods is the lack of linearity of their quality estimates with respect to scores expressed by human subjects. For this reason, usually IQA metrics undergo a calibration process based on subjective quality examples. However, example-based training makes generalization problematic, hampering result comparison across different applications and operative conditions. In this paper, new Full Reference (FR) techniques, providing estimates linearly correlated with human scores without using calibration are introduced. To reach this objective, these techniques are deeply rooted on principles and theoretical constraints. Restricting the interest on the IQA of the set of natural images, it is first recognized that application of estimation theory and psycho physical principles to images degraded by Gaussian blur leads to a so-called canonical IQA method, whose estimates are not only highly linearly correlated to subjective scores, but are also straightforwardly related to the Viewing Distance (VD). Then, it is shown that mainstream IQA methods can be reconducted to the canonical method applying a preliminary metric conversion based on a unique specimen image. The application of this scheme is then extended to a significant class of degraded images other than Gaussian blur, including noisy and compressed images. The resulting calibration-free FR IQA methods are suited for applications where comparability and interoperability across different imaging systems and on different VDs is a major requirement. A comparison of their statistical performance with respect to some conventional calibration prone methods is finally provided.



### Detecting Deforestation from Sentinel-1 Data in the Absence of Reliable Reference Data
- **Arxiv ID**: http://arxiv.org/abs/2205.12131v1
- **DOI**: None
- **Categories**: **stat.ME**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.12131v1)
- **Published**: 2022-05-24 15:08:02+00:00
- **Updated**: 2022-05-24 15:08:02+00:00
- **Authors**: Johannes N. Hansen, Edward T. A. Mitchard, Stuart King
- **Comment**: None
- **Journal**: None
- **Summary**: Forests are vital for the wellbeing of our planet. Large and small scale deforestation across the globe is threatening the stability of our climate, forest biodiversity, and therefore the preservation of fragile ecosystems and our natural habitat as a whole. With increasing public interest in climate change issues and forest preservation, a large demand for carbon offsetting, carbon footprint ratings, and environmental impact assessments is emerging. Most often, deforestation maps are created from optical data such as Landsat and MODIS. These maps are not typically available at less than annual intervals due to persistent cloud cover in many parts of the world, especially the tropics where most of the world's forest biomass is concentrated. Synthetic Aperture Radar (SAR) can fill this gap as it penetrates clouds. We propose and evaluate a novel method for deforestation detection in the absence of reliable reference data which often constitutes the largest practical hurdle. This method achieves a change detection sensitivity (producer's accuracy) of 96.5% in the study area, although false positives lead to a lower user's accuracy of about 75.7%, with a total balanced accuracy of 90.4%. The change detection accuracy is maintained when adding up to 20% noise to the reference labels. While further work is required to reduce the false positive rate, improve detection delay, and validate this method in additional circumstances, the results show that Sentinel-1 data have the potential to advance the timeliness of global deforestation monitoring.



### Improving the Latent Space of Image Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2205.12135v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.12135v1)
- **Published**: 2022-05-24 15:13:01+00:00
- **Updated**: 2022-05-24 15:13:01+00:00
- **Authors**: Yunpeng Bai, Cairong Wang, Chun Yuan, Yanbo Fan, Jue Wang
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Existing neural style transfer researches have studied to match statistical information between the deep features of content and style images, which were extracted by a pre-trained VGG, and achieved significant improvement in synthesizing artistic images. However, in some cases, the feature statistics from the pre-trained encoder may not be consistent with the visual style we perceived. For example, the style distance between images of different styles is less than that of the same style. In such an inappropriate latent space, the objective function of the existing methods will be optimized in the wrong direction, resulting in bad stylization results. In addition, the lack of content details in the features extracted by the pre-trained encoder also leads to the content leak problem. In order to solve these issues in the latent space used by style transfer, we propose two contrastive training schemes to get a refined encoder that is more suitable for this task. The style contrastive loss pulls the stylized result closer to the same visual style image and pushes it away from the content image. The content contrastive loss enables the encoder to retain more available details. We can directly add our training scheme to some existing style transfer methods and significantly improve their results. Extensive experimental results demonstrate the effectiveness and superiority of our methods.



### Optimizing Performance of Federated Person Re-identification: Benchmarking and Analysis
- **Arxiv ID**: http://arxiv.org/abs/2205.12144v1
- **DOI**: 10.1145/3531013
- **Categories**: **cs.CV**, cs.AI, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2205.12144v1)
- **Published**: 2022-05-24 15:20:32+00:00
- **Updated**: 2022-05-24 15:20:32+00:00
- **Authors**: Weiming Zhuang, Xin Gan, Yonggang Wen, Shuai Zhang
- **Comment**: TOMM
- **Journal**: None
- **Summary**: The increasingly stringent data privacy regulations limit the development of person re-identification (ReID) because person ReID training requires centralizing an enormous amount of data that contains sensitive personal information. To address this problem, we introduce federated person re-identification (FedReID) -- implementing federated learning, an emerging distributed training method, to person ReID. FedReID preserves data privacy by aggregating model updates, instead of raw data, from clients to a central server. Furthermore, we optimize the performance of FedReID under statistical heterogeneity via benchmark analysis. We first construct a benchmark with an enhanced algorithm, two architectures, and nine person ReID datasets with large variances to simulate the real-world statistical heterogeneity. The benchmark results present insights and bottlenecks of FedReID under statistical heterogeneity, including challenges in convergence and poor performance on datasets with large volumes. Based on these insights, we propose three optimization approaches: (1) We adopt knowledge distillation to facilitate the convergence of FedReID by better transferring knowledge from clients to the server; (2) We introduce client clustering to improve the performance of large datasets by aggregating clients with similar data distributions; (3) We propose cosine distance weight to elevate performance by dynamically updating the weights for aggregation depending on how well models are trained in clients. Extensive experiments demonstrate that these approaches achieve satisfying convergence with much better performance on all datasets. We believe that FedReID will shed light on implementing and optimizing federated learning on more computer vision applications.



### StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D Mutual Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.12183v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.12183v2)
- **Published**: 2022-05-24 16:29:50+00:00
- **Updated**: 2022-05-25 05:19:33+00:00
- **Authors**: Yi-Hua Huang, Yue He, Yu-Jie Yuan, Yu-Kun Lai, Lin Gao
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: 3D scene stylization aims at generating stylized images of the scene from arbitrary novel views following a given set of style examples, while ensuring consistency when rendered from different views. Directly applying methods for image or video stylization to 3D scenes cannot achieve such consistency. Thanks to recently proposed neural radiance fields (NeRF), we are able to represent a 3D scene in a consistent way. Consistent 3D scene stylization can be effectively achieved by stylizing the corresponding NeRF. However, there is a significant domain gap between style examples which are 2D images and NeRF which is an implicit volumetric representation. To address this problem, we propose a novel mutual learning framework for 3D scene stylization that combines a 2D image stylization network and NeRF to fuse the stylization ability of 2D stylization network with the 3D consistency of NeRF. We first pre-train a standard NeRF of the 3D scene to be stylized and replace its color prediction module with a style network to obtain a stylized NeRF. It is followed by distilling the prior knowledge of spatial consistency from NeRF to the 2D stylization network through an introduced consistency loss. We also introduce a mimic loss to supervise the mutual learning of the NeRF style module and fine-tune the 2D stylization decoder. In order to further make our model handle ambiguities of 2D stylization results, we introduce learnable latent codes that obey the probability distributions conditioned on the style. They are attached to training samples as conditional inputs to better learn the style module in our novel stylized NeRF. Experimental results demonstrate that our method is superior to existing approaches in both visual quality and long-range consistency.



### Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization
- **Arxiv ID**: http://arxiv.org/abs/2205.12191v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.12191v2)
- **Published**: 2022-05-24 16:44:45+00:00
- **Updated**: 2023-04-01 07:07:44+00:00
- **Authors**: Aishwarya Agrawal, Ivana KajiÄ‡, Emanuele Bugliarello, Elnaz Davoodi, Anita Gergely, Phil Blunsom, Aida Nematzadeh
- **Comment**: Findings of EACL 2023. Aishwarya, Ivana, Emanuele and Aida had equal
  first author contributions. Elnaz and Anita had equal contributions. Aida and
  Aishwarya had equal senior contributions
- **Journal**: None
- **Summary**: Vision-and-language (V&L) models pretrained on large-scale multimodal data have demonstrated strong performance on various tasks such as image captioning and visual question answering (VQA). The quality of such models is commonly assessed by measuring their performance on unseen data that typically comes from the same distribution as the training data. However, when evaluated under out-of-distribution (out-of-dataset) settings for VQA, we observe that these models exhibit poor generalization. We comprehensively evaluate two pretrained V&L models under different settings (i.e. classification and open-ended text generation) by conducting cross-dataset evaluations. We find that these models tend to learn to solve the benchmark, rather than learning the high-level skills required by the VQA task. We also find that in most cases generative models are less susceptible to shifts in data distribution compared to discriminative ones, and that multimodal pretraining is generally helpful for OOD generalization. Finally, we revisit assumptions underlying the use of automatic VQA evaluation metrics, and empirically show that their stringent nature repeatedly penalizes models for correct responses.



### Absolute Triangulation Algorithms for Space Exploration
- **Arxiv ID**: http://arxiv.org/abs/2205.12197v2
- **DOI**: 10.2514/1.G006989
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.12197v2)
- **Published**: 2022-05-24 16:54:07+00:00
- **Updated**: 2022-08-04 16:58:31+00:00
- **Authors**: Sebastien Henry, John A. Christian
- **Comment**: None
- **Journal**: None
- **Summary**: Images are an important source of information for spacecraft navigation and for three-dimensional reconstruction of observed space objects. Both of these applications take the form of a triangulation problem when the camera has a known attitude and the measurements extracted from the image are line of sight (LOS) directions. This work provides a comprehensive review of the history and theoretical foundations of triangulation. A variety of classical triangulation algorithms are reviewed, including a number of suboptimal linear methods (many LOS measurements) and the optimal method of Hartley and Sturm (only two LOS measurements). It is shown that the optimal many-measurement case may be solved without iteration as a linear system using the new Linear Optimal Sine Triangulation (LOST) method. Both LOST and the polynomial method of Hartley and Sturm provide the same result in the case of only two measurements. The various triangulation algorithms are assessed with a few numerical examples, including planetary terrain relative navigation, angles-only optical navigation at Uranus, 3-D reconstruction of Notre-Dame de Paris, and angles-only relative navigation.



### Aerial Vision-and-Dialog Navigation
- **Arxiv ID**: http://arxiv.org/abs/2205.12219v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2205.12219v3)
- **Published**: 2022-05-24 17:28:14+00:00
- **Updated**: 2023-06-01 06:39:11+00:00
- **Authors**: Yue Fan, Winson Chen, Tongzhou Jiang, Chun Zhou, Yi Zhang, Xin Eric Wang
- **Comment**: Accepted by ACL 2023 Findings
- **Journal**: None
- **Summary**: The ability to converse with humans and follow natural language commands is crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can relieve people's burden of holding a controller all the time, allow multitasking, and make drone control more accessible for people with disabilities or with their hands occupied. To this end, we introduce Aerial Vision-and-Dialog Navigation (AVDN), to navigate a drone via natural language conversation. We build a drone simulator with a continuous photorealistic environment and collect a new AVDN dataset of over 3k recorded navigation trajectories with asynchronous human-human dialogs between commanders and followers. The commander provides initial navigation instruction and further guidance by request, while the follower navigates the drone in the simulator and asks questions when needed. During data collection, followers' attention on the drone's visual observation is also recorded. Based on the AVDN dataset, we study the tasks of aerial navigation from (full) dialog history and propose an effective Human Attention Aided Transformer model (HAA-Transformer), which learns to predict both navigation waypoints and human attention.



### GLOBUS: GLObal Building heights for Urban Studies
- **Arxiv ID**: http://arxiv.org/abs/2205.12224v1
- **DOI**: None
- **Categories**: **physics.ao-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.12224v1)
- **Published**: 2022-05-24 17:34:14+00:00
- **Updated**: 2022-05-24 17:34:14+00:00
- **Authors**: Harsh G. Kamath, Manmeet Singh, Lori A. Magruder, Zong-Liang Yang, Dev Niyogi
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Urban weather and climate studies continue to be important as extreme events cause economic loss and impact public health. Weather models seek to represent urban areas but are oversimplified due to data availability, especially building information. This paper introduces a novel Level of Detail-1 (LoD-1) building dataset derived from a Deep Neural Network (DNN) called GLObal Building heights for Urban Studies (GLOBUS). GLOBUS uses open-source datasets as predictors: Advanced Land Observation Satellite (ALOS) Digital Surface Model (DSM) normalized using Shuttle Radar Topography Mission (SRTM) Digital Elevation Model (DEM), Landscan population density, and building footprints. The building information from GLOBUS can be ingested in Numerical Weather Prediction (NWP) and urban energy-water balance models to study localized phenomena such as the Urban Heat Island (UHI) effect. GLOBUS has been trained and validated using the United States Geological Survey (USGS) 3DEP Light Detection and Ranging (LiDAR) data. We used data from 5 US cities for training and the model was validated over 6 cities. Performance metrics are computed at a spatial resolution of 300-meter. The Root Mean Squared Error (RMSE) and Mean Absolute Percentage Error (MAPE) were 5.15 meters and 28.8 %, respectively. The standard deviation and histogram of building heights over a 300-meter grid are well represented using GLOBUS.



### ASSET: Autoregressive Semantic Scene Editing with Transformers at High Resolutions
- **Arxiv ID**: http://arxiv.org/abs/2205.12231v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2205.12231v1)
- **Published**: 2022-05-24 17:39:53+00:00
- **Updated**: 2022-05-24 17:39:53+00:00
- **Authors**: Difan Liu, Sandesh Shetty, Tobias Hinz, Matthew Fisher, Richard Zhang, Taesung Park, Evangelos Kalogerakis
- **Comment**: SIGGRAPH 2022 - Journal Track
- **Journal**: None
- **Summary**: We present ASSET, a neural architecture for automatically modifying an input high-resolution image according to a user's edits on its semantic segmentation map. Our architecture is based on a transformer with a novel attention mechanism. Our key idea is to sparsify the transformer's attention matrix at high resolutions, guided by dense attention extracted at lower image resolutions. While previous attention mechanisms are computationally too expensive for handling high-resolution images or are overly constrained within specific image regions hampering long-range interactions, our novel attention mechanism is both computationally efficient and effective. Our sparsified attention mechanism is able to capture long-range interactions and context, leading to synthesizing interesting phenomena in scenes, such as reflections of landscapes onto water or flora consistent with the rest of the landscape, that were not possible to generate reliably with previous convnets and transformer approaches. We present qualitative and quantitative results, along with user studies, demonstrating the effectiveness of our method.



### Gacs-Korner Common Information Variational Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2205.12239v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2205.12239v1)
- **Published**: 2022-05-24 17:47:26+00:00
- **Updated**: 2022-05-24 17:47:26+00:00
- **Authors**: Michael Kleinman, Alessandro Achille, Stefano Soatto, Jonathan Kao
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a notion of common information that allows one to quantify and separate the information that is shared between two random variables from the information that is unique to each. Our notion of common information is a variational relaxation of the G\'acs-K\"orner common information, which we recover as a special case, but is more amenable to optimization and can be approximated empirically using samples from the underlying distribution. We then provide a method to partition and quantify the common and unique information using a simple modification of a traditional variational auto-encoder. Empirically, we demonstrate that our formulation allows us to learn semantically meaningful common and unique factors of variation even on high-dimensional data such as images and videos. Moreover, on datasets where ground-truth latent factors are known, we show that we can accurately quantify the common information between the random variables. Additionally, we show that the auto-encoder that we learn recovers semantically meaningful disentangled factors of variation, even though we do not explicitly optimize for it.



### Differentiable Dynamics for Articulated 3d Human Motion Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2205.12256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.12256v1)
- **Published**: 2022-05-24 17:58:37+00:00
- **Updated**: 2022-05-24 17:58:37+00:00
- **Authors**: Erik GÃ¤rtner, Mykhaylo Andriluka, Erwin Coumans, Cristian Sminchisescu
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: We introduce DiffPhy, a differentiable physics-based model for articulated 3d human motion reconstruction from video. Applications of physics-based reasoning in human motion analysis have so far been limited, both by the complexity of constructing adequate physical models of articulated human motion, and by the formidable challenges of performing stable and efficient inference with physics in the loop. We jointly address such modeling and inference challenges by proposing an approach that combines a physically plausible body representation with anatomical joint limits, a differentiable physics simulator, and optimization techniques that ensure good performance and robustness to suboptimal local optima. In contrast to several recent methods, our approach readily supports full-body contact including interactions with objects in the scene. Most importantly, our model connects end-to-end with images, thus supporting direct gradient-based physics optimization by means of image-based loss functions. We validate the model by demonstrating that it can accurately reconstruct physically plausible 3d human motion from monocular video, both on public benchmarks with available 3d ground-truth, and on videos from the internet.



### OnePose: One-Shot Object Pose Estimation without CAD Models
- **Arxiv ID**: http://arxiv.org/abs/2205.12257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.12257v1)
- **Published**: 2022-05-24 17:59:21+00:00
- **Updated**: 2022-05-24 17:59:21+00:00
- **Authors**: Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He, Hongcheng Zhao, Guofeng Zhang, Xiaowei Zhou
- **Comment**: Accepted to CVPR 2022. Project page:
  https://zju3dv.github.io/onepose/
- **Journal**: None
- **Summary**: We propose a new method named OnePose for object pose estimation. Unlike existing instance-level or category-level methods, OnePose does not rely on CAD models and can handle objects in arbitrary categories without instance- or category-specific network training. OnePose draws the idea from visual localization and only requires a simple RGB video scan of the object to build a sparse SfM model of the object. Then, this model is registered to new query images with a generic feature matching network. To mitigate the slow runtime of existing visual localization methods, we propose a new graph attention network that directly matches 2D interest points in the query image with the 3D points in the SfM model, resulting in efficient and robust pose estimation. Combined with a feature-based pose tracker, OnePose is able to stably detect and track 6D poses of everyday household objects in real-time. We also collected a large-scale dataset that consists of 450 sequences of 150 objects.



### Trajectory Optimization for Physics-Based Reconstruction of 3d Human Pose from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2205.12292v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.12292v1)
- **Published**: 2022-05-24 18:02:49+00:00
- **Updated**: 2022-05-24 18:02:49+00:00
- **Authors**: Erik GÃ¤rtner, Mykhaylo Andriluka, Hongyi Xu, Cristian Sminchisescu
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: We focus on the task of estimating a physically plausible articulated human motion from monocular video. Existing approaches that do not consider physics often produce temporally inconsistent output with motion artifacts, while state-of-the-art physics-based approaches have either been shown to work only in controlled laboratory conditions or consider simplified body-ground contact limited to feet. This paper explores how these shortcomings can be addressed by directly incorporating a fully-featured physics engine into the pose estimation process. Given an uncontrolled, real-world scene as input, our approach estimates the ground-plane location and the dimensions of the physical body model. It then recovers the physical motion by performing trajectory optimization. The advantage of our formulation is that it readily generalizes to a variety of scenes that might have diverse ground properties and supports any form of self-contact and contact between the articulated body and scene geometry. We show that our approach achieves competitive results with respect to existing physics-based methods on the Human3.6M benchmark, while being directly applicable without re-training to more complex dynamic motions from the AIST benchmark and to uncontrolled internet videos.



### Face2Text revisited: Improved data set and baseline results
- **Arxiv ID**: http://arxiv.org/abs/2205.12342v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2205.12342v1)
- **Published**: 2022-05-24 19:38:49+00:00
- **Updated**: 2022-05-24 19:38:49+00:00
- **Authors**: Marc Tanti, Shaun Abdilla, Adrian Muscat, Claudia Borg, Reuben A. Farrugia, Albert Gatt
- **Comment**: 7 pages, 5 figures, 4 tables, to appear in LREC 2022 (P-VLAM
  workshop)
- **Journal**: None
- **Summary**: Current image description generation models do not transfer well to the task of describing human faces. To encourage the development of more human-focused descriptions, we developed a new data set of facial descriptions based on the CelebA image data set. We describe the properties of this data set, and present results from a face description generator trained on it, which explores the feasibility of using transfer learning from VGGFace/ResNet CNNs. Comparisons are drawn through both automated metrics and human evaluation by 76 English-speaking participants. The descriptions generated by the VGGFace-LSTM + Attention model are closest to the ground truth according to human evaluation whilst the ResNet-LSTM + Attention model obtained the highest CIDEr and CIDEr-D results (1.252 and 0.686 respectively). Together, the new data set and these experimental results provide data and baselines for future work in this area.



### Wavelet Feature Maps Compression for Image-to-Image CNNs
- **Arxiv ID**: http://arxiv.org/abs/2205.12268v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.12268v4)
- **Published**: 2022-05-24 20:29:19+00:00
- **Updated**: 2022-10-16 12:27:10+00:00
- **Authors**: Shahaf E. Finder, Yair Zohav, Maor Ashkenazi, Eran Treister
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are known for requiring extensive computational resources, and quantization is among the best and most common methods for compressing them. While aggressive quantization (i.e., less than 4-bits) performs well for classification, it may cause severe performance degradation in image-to-image tasks such as semantic segmentation and depth estimation. In this paper, we propose Wavelet Compressed Convolution (WCC) -- a novel approach for high-resolution activation maps compression integrated with point-wise convolutions, which are the main computational cost of modern architectures. To this end, we use an efficient and hardware-friendly Haar-wavelet transform, known for its effectiveness in image compression, and define the convolution on the compressed activation map. We experiment with various tasks that benefit from high-resolution input. By combining WCC with light quantization, we achieve compression rates equivalent to 1-4bit activation quantization with relatively small and much more graceful degradation in performance. Our code is available at https://github.com/BGUCompSci/WaveletCompressedConvolution.



### A Benchmark and Asymmetrical-Similarity Learning for Practical Image Copy Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.12358v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.12358v3)
- **Published**: 2022-05-24 20:39:11+00:00
- **Updated**: 2022-12-01 21:18:37+00:00
- **Authors**: Wenhao Wang, Yifan Sun, Yi Yang
- **Comment**: Accepted to AAAI 2023
- **Journal**: None
- **Summary**: Image copy detection (ICD) aims to determine whether a query image is an edited copy of any image from a reference set. Currently, there are very limited public benchmarks for ICD, while all overlook a critical challenge in real-world applications, i.e., the distraction from hard negative queries. Specifically, some queries are not edited copies but are inherently similar to some reference images. These hard negative queries are easily false recognized as edited copies, significantly compromising the ICD accuracy. This observation motivates us to build the first ICD benchmark featuring this characteristic. Based on existing ICD datasets, this paper constructs a new dataset by additionally adding 100, 000 and 24, 252 hard negative pairs into the training and test set, respectively. Moreover, this paper further reveals a unique difficulty for solving the hard negative problem in ICD, i.e., there is a fundamental conflict between current metric learning and ICD. This conflict is: the metric learning adopts symmetric distance while the edited copy is an asymmetric (unidirectional) process, e.g., a partial crop is close to its holistic reference image and is an edited copy, while the latter cannot be the edited copy of the former (in spite the distance is equally small). This insight results in an Asymmetrical-Similarity Learning (ASL) method, which allows the similarity in two directions (the query <-> the reference image) to be different from each other. Experimental results show that ASL outperforms state-of-the-art methods by a clear margin, confirming that solving the symmetric-asymmetric conflict is critical for ICD. The NDEC dataset and code are available at https://github.com/WangWenhao0716/ASL.



### Jointly Optimizing Color Rendition and In-Camera Backgrounds in an RGB Virtual Production Stage
- **Arxiv ID**: http://arxiv.org/abs/2205.12403v2
- **DOI**: 10.1145/3543664.3543681
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2205.12403v2)
- **Published**: 2022-05-24 23:23:40+00:00
- **Updated**: 2022-07-01 18:01:19+00:00
- **Authors**: Chloe LeGendre, Lukas Lepicovsky, Paul Debevec
- **Comment**: DigiPro 2022
- **Journal**: None
- **Summary**: While the LED panels used in virtual production systems can display vibrant imagery with a wide color gamut, they produce problematic color shifts when used as lighting due to their peaky spectral output from narrow-band red, green, and blue LEDs. In this work, we present an improved color calibration process for virtual production stages which ameliorates this color rendition problem while also passing through accurate in-camera background colors. We do this by optimizing linear color correction transformations for 1) the LED panel pixels visible in the field of view of the camera, 2) the pixels outside the field of view of the camera illuminating the subjects, and, as a post-process, 3) the pixel values recorded by the camera. The result is that footage shot in an RGB LED panel virtual production stage can exhibit more accurate skin tones and costume colors while still reproducing the desired colors of the in-camera background.



### Convolutional Neural Processes for Inpainting Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2205.12407v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.12407v1)
- **Published**: 2022-05-24 23:29:04+00:00
- **Updated**: 2022-05-24 23:29:04+00:00
- **Authors**: Alexander Pondaven, MÃ¤rt Bakler, Donghu Guo, Hamzah Hashim, Martin Ignatov, Harrison Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: The widespread availability of satellite images has allowed researchers to model complex systems such as disease dynamics. However, many satellite images have missing values due to measurement defects, which render them unusable without data imputation. For example, the scanline corrector for the LANDSAT 7 satellite broke down in 2003, resulting in a loss of around 20\% of its data. Inpainting involves predicting what is missing based on the known pixels and is an old problem in image processing, classically based on PDEs or interpolation methods, but recent deep learning approaches have shown promise. However, many of these methods do not explicitly take into account the inherent spatiotemporal structure of satellite images. In this work, we cast satellite image inpainting as a natural meta-learning problem, and propose using convolutional neural processes (ConvNPs) where we frame each satellite image as its own task or 2D regression problem. We show ConvNPs can outperform classical methods and state-of-the-art deep learning inpainting models on a scanline inpainting problem for LANDSAT 7 satellite images, assessed on a variety of in and out-of-distribution images.



