# Arxiv Papers in cs.CV on 2022-05-02
### MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries
- **Arxiv ID**: http://arxiv.org/abs/2205.00613v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.00613v1)
- **Published**: 2022-05-02 01:45:41+00:00
- **Updated**: 2022-05-02 01:45:41+00:00
- **Authors**: Tianyuan Zhang, Xuanyao Chen, Yue Wang, Yilun Wang, Hang Zhao
- **Comment**: Appear on CVPR 2022 Workshop on Autonomous Driving
- **Journal**: None
- **Summary**: Accurate and consistent 3D tracking from multiple cameras is a key component in a vision-based autonomous driving system. It involves modeling 3D dynamic objects in complex scenes across multiple cameras. This problem is inherently challenging due to depth estimation, visual occlusions, appearance ambiguity, etc. Moreover, objects are not consistently associated across time and cameras. To address that, we propose an end-to-end \textbf{MU}lti-camera \textbf{TR}acking framework called MUTR3D. In contrast to prior works, MUTR3D does not explicitly rely on the spatial and appearance similarity of objects. Instead, our method introduces \textit{3D track query} to model spatial and appearance coherent track for each object that appears in multiple cameras and multiple frames. We use camera transformations to link 3D trackers with their observations in 2D images. Each tracker is further refined according to the features that are obtained from camera images. MUTR3D uses a set-to-set loss to measure the difference between the predicted tracking results and the ground truths. Therefore, it does not require any post-processing such as non-maximum suppression and/or bounding box association. MUTR3D outperforms state-of-the-art methods by 5.3 AMOTA on the nuScenes dataset. Code is available at: \url{https://github.com/a1600012888/MUTR3D}.



### Deep fiber clustering: Anatomically informed fiber clustering with self-supervised deep learning for fast and effective tractography parcellation
- **Arxiv ID**: http://arxiv.org/abs/2205.00627v3
- **DOI**: 10.1016/j.neuroimage.2023.120086
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00627v3)
- **Published**: 2022-05-02 02:41:02+00:00
- **Updated**: 2023-07-08 13:53:52+00:00
- **Authors**: Yuqian Chen, Chaoyi Zhang, Tengfei Xue, Yang Song, Nikos Makris, Yogesh Rathi, Weidong Cai, Fan Zhang, Lauren J. O'Donnell
- **Comment**: 14 pages, 7 figures
- **Journal**: NeuroImage 273 (2023): 120086
- **Summary**: White matter fiber clustering is an important strategy for white matter parcellation, which enables quantitative analysis of brain connections in health and disease. In combination with expert neuroanatomical labeling, data-driven white matter fiber clustering is a powerful tool for creating atlases that can model white matter anatomy across individuals. While widely used fiber clustering approaches have shown good performance using classical unsupervised machine learning techniques, recent advances in deep learning reveal a promising direction toward fast and effective fiber clustering. In this work, we propose a novel deep learning framework for white matter fiber clustering, Deep Fiber Clustering (DFC), which solves the unsupervised clustering problem as a self-supervised learning task with a domain-specific pretext task to predict pairwise fiber distances. This process learns a high-dimensional embedding feature representation for each fiber, regardless of the order of fiber points reconstructed during tractography. We design a novel network architecture that represents input fibers as point clouds and allows the incorporation of additional sources of input information from gray matter parcellation to improve anatomical coherence of clusters. In addition, DFC conducts outlier removal naturally by rejecting fibers with low cluster assignment probability. We evaluate DFC on three independently acquired cohorts, including data from 220 individuals across genders, ages (young and elderly adults), and different health conditions (healthy control and multiple neuropsychiatric disorders). We compare DFC to several state-of-the-art white matter fiber clustering algorithms. Experimental results demonstrate superior performance of DFC in terms of cluster compactness, generalization ability, anatomical coherence, and computational efficiency.



### Design equivariant neural networks for 3D point cloud
- **Arxiv ID**: http://arxiv.org/abs/2205.00630v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.00630v1)
- **Published**: 2022-05-02 02:57:13+00:00
- **Updated**: 2022-05-02 02:57:13+00:00
- **Authors**: Thuan N. A. Trang, Thieu N. Vo, Khuong D. Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: This work seeks to improve the generalization and robustness of existing neural networks for 3D point clouds by inducing group equivariance under general group transformations. The main challenge when designing equivariant models for point clouds is how to trade-off the performance of the model and the complexity. Existing equivariant models are either too complicate to implement or very high complexity. The main aim of this study is to build a general procedure to introduce group equivariant property to SOTA models for 3D point clouds. The group equivariant models built form our procedure are simple to implement, less complexity in comparison with the existing ones, and they preserve the strengths of the original SOTA backbone. From the results of the experiments on object classification, it is shown that our methods are superior to other group equivariant models in performance and complexity. Moreover, our method also helps to improve the mIoU of semantic segmentation models. Overall, by using a combination of only-finite-rotation equivariance and augmentation, our models can outperform existing full $SO(3)$-equivariance models with much cheaper complexity and GPU memory. The proposed procedure is general and forms a fundamental approach to group equivariant neural networks. We believe that it can be easily adapted to other SOTA models in the future.



### Learning Multi-dimensional Edge Feature-based AU Relation Graph for Facial Action Unit Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.01782v2
- **DOI**: 10.24963/ijcai.2022/173
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.01782v2)
- **Published**: 2022-05-02 03:38:00+00:00
- **Updated**: 2022-08-02 12:13:49+00:00
- **Authors**: Cheng Luo, Siyang Song, Weicheng Xie, Linlin Shen, Hatice Gunes
- **Comment**: IJCAI 2022 conference (accepted)
- **Journal**: None
- **Summary**: The activations of Facial Action Units (AUs) mutually influence one another. While the relationship between a pair of AUs can be complex and unique, existing approaches fail to specifically and explicitly represent such cues for each pair of AUs in each facial display. This paper proposes an AU relationship modelling approach that deep learns a unique graph to explicitly describe the relationship between each pair of AUs of the target facial display. Our approach first encodes each AU's activation status and its association with other AUs into a node feature. Then, it learns a pair of multi-dimensional edge features to describe multiple task-specific relationship cues between each pair of AUs. During both node and edge feature learning, our approach also considers the influence of the unique facial display on AUs' relationship by taking the full face representation as an input. Experimental results on BP4D and DISFA datasets show that both node and edge feature learning modules provide large performance improvements for CNN and transformer-based backbones, with our best systems achieving the state-of-the-art AU recognition results. Our approach not only has a strong capability in modelling relationship cues for AU recognition but also can be easily incorporated into various backbones. Our PyTorch code is made available.



### Enhancing Adversarial Training with Feature Separability
- **Arxiv ID**: http://arxiv.org/abs/2205.00637v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.00637v1)
- **Published**: 2022-05-02 04:04:23+00:00
- **Updated**: 2022-05-02 04:04:23+00:00
- **Authors**: Yaxin Li, Xiaorui Liu, Han Xu, Wentao Wang, Jiliang Tang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Deep Neural Network (DNN) are vulnerable to adversarial attacks. As a countermeasure, adversarial training aims to achieve robustness based on the min-max optimization problem and it has shown to be one of the most effective defense strategies. However, in this work, we found that compared with natural training, adversarial training fails to learn better feature representations for either clean or adversarial samples, which can be one reason why adversarial training tends to have severe overfitting issues and less satisfied generalize performance. Specifically, we observe two major shortcomings of the features learned by existing adversarial training methods:(1) low intra-class feature similarity; and (2) conservative inter-classes feature variance. To overcome these shortcomings, we introduce a new concept of adversarial training graph (ATG) with which the proposed adversarial training with feature separability (ATFS) enables to coherently boost the intra-class feature similarity and increase inter-class feature variance. Through comprehensive experiments, we demonstrate that the proposed ATFS framework significantly improves both clean and robust performance.



### An Application to Generate Style Guided Compatible Outfit
- **Arxiv ID**: http://arxiv.org/abs/2205.00663v1
- **DOI**: 10.1145/3493700.3493737
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.00663v1)
- **Published**: 2022-05-02 05:45:05+00:00
- **Updated**: 2022-05-02 05:45:05+00:00
- **Authors**: Debopriyo Banerjee, Harsh Maheshwari, Lucky Dhakad1, Arnab Bhattacharya1, Niloy Ganguly, Muthusamy Chelliah, Suyash Agarwal1
- **Comment**: None
- **Journal**: None
- **Summary**: Fashion recommendation has witnessed a phenomenal growth of research, particularly in the domains of shop-the-look, contextaware outfit creation, personalizing outfit creation etc. Majority of the work in this area focuses on better understanding of the notion of complimentary relationship between lifestyle items. Quite recently, some works have realised that style plays a vital role in fashion, especially in the understanding of compatibility learning and outfit creation. In this paper, we would like to present the end-to-end design of a methodology in which we aim to generate outfits guided by styles or themes using a novel style encoder network. We present an extensive analysis of different aspects of our method through various experiments. We also provide a demonstration api to showcase the ability of our work in generating outfits based on an anchor item and styles.



### Revisiting Classical Multiclass Linear Discriminant Analysis with a Novel Prototype-based Interpretable Solution
- **Arxiv ID**: http://arxiv.org/abs/2205.00668v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00668v2)
- **Published**: 2022-05-02 06:12:42+00:00
- **Updated**: 2022-09-30 07:45:25+00:00
- **Authors**: Sayed Kamaledin Ghiasi-Shirazi
- **Comment**: None
- **Journal**: None
- **Summary**: Linear discriminant analysis (LDA) is a fundamental method for feature extraction and dimensionality reduction. Despite having many variants, classical LDA has its own importance, as it is a keystone in human knowledge about statistical pattern recognition. For a dataset containing C clusters, the classical solution to LDA extracts at most C-1 features. Here, we introduce a novel solution to classical LDA, called LDA++, that yields C features, each interpretable as measuring similarity to one cluster. This novel solution bridges dimensionality reduction and multiclass classification. Specifically, we prove that, for homoscedastic Gaussian data and under some mild conditions, the optimal weights of a linear multiclass classifier also make an optimal solution to LDA. In addition, we show that LDA++ reveals some important new facts about LDA that remarkably changes our understanding of classical multiclass LDA after 75 years of its introduction. We provide a complete numerical solution for LDA++ for the cases 1) when the scatter matrices can be constructed explicitly, 2) when constructing the scatter matrices is infeasible, and 3) the kernel extension.



### Deep Video Harmonization with Color Mapping Consistency
- **Arxiv ID**: http://arxiv.org/abs/2205.00687v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.00687v1)
- **Published**: 2022-05-02 07:10:18+00:00
- **Updated**: 2022-05-02 07:10:18+00:00
- **Authors**: Xinyuan Lu, Shengyuan Huang, Li Niu, Wenyan Cong, Liqing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Video harmonization aims to adjust the foreground of a composite video to make it compatible with the background. So far, video harmonization has only received limited attention and there is no public dataset for video harmonization. In this work, we construct a new video harmonization dataset HYouTube by adjusting the foreground of real videos to create synthetic composite videos. Moreover, we consider the temporal consistency in video harmonization task. Unlike previous works which establish the spatial correspondence, we design a novel framework based on the assumption of color mapping consistency, which leverages the color mapping of neighboring frames to refine the current frame. Extensive experiments on our HYouTube dataset prove the effectiveness of our proposed framework. Our dataset and code are available at https://github.com/bcmi/Video-Harmonization-Dataset-HYouTube.



### From Noisy Prediction to True Label: Noisy Prediction Calibration via Generative Model
- **Arxiv ID**: http://arxiv.org/abs/2205.00690v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.00690v3)
- **Published**: 2022-05-02 07:15:45+00:00
- **Updated**: 2022-05-26 03:38:37+00:00
- **Authors**: HeeSun Bae, Seungjae Shin, Byeonghu Na, JoonHo Jang, Kyungwoo Song, Il-Chul Moon
- **Comment**: 21 pages, 9 figures. International Conference on Machine Learning
  (ICML 2022), Baltimore, Jul 17, 2022
- **Journal**: None
- **Summary**: Noisy labels are inevitable yet problematic in machine learning society. It ruins the generalization of a classifier by making the classifier over-fitted to noisy labels. Existing methods on noisy label have focused on modifying the classifier during the training procedure. It has two potential problems. First, these methods are not applicable to a pre-trained classifier without further access to training. Second, it is not easy to train a classifier and regularize all negative effects from noisy labels, simultaneously. We suggest a new branch of method, Noisy Prediction Calibration (NPC) in learning with noisy labels. Through the introduction and estimation of a new type of transition matrix via generative model, NPC corrects the noisy prediction from the pre-trained classifier to the true label as a post-processing scheme. We prove that NPC theoretically aligns with the transition matrix based methods. Yet, NPC empirically provides more accurate pathway to estimate true label, even without involvement in classifier learning. Also, NPC is applicable to any classifier trained with noisy label methods, if training instances and its predictions are available. Our method, NPC, boosts the classification performances of all baseline models on both synthetic and real-world datasets. The implemented code is available at https://github.com/BaeHeeSun/NPC.



### A Multi-stage deep architecture for summary generation of soccer videos
- **Arxiv ID**: http://arxiv.org/abs/2205.00694v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.00694v1)
- **Published**: 2022-05-02 07:26:35+00:00
- **Updated**: 2022-05-02 07:26:35+00:00
- **Authors**: Melissa Sanabria, Frédéric Precioso, Pierre-Alexandre Mattei, Thomas Menguy
- **Comment**: None
- **Journal**: None
- **Summary**: Video content is present in an ever-increasing number of fields, both scientific and commercial. Sports, particularly soccer, is one of the industries that has invested the most in the field of video analytics, due to the massive popularity of the game and the emergence of new markets. Previous state-of-the-art methods on soccer matches video summarization rely on handcrafted heuristics to generate summaries which are poorly generalizable, but these works have yet proven that multiple modalities help detect the best actions of the game. On the other hand, machine learning models with higher generalization potential have entered the field of summarization of general-purpose videos, offering several deep learning approaches. However, most of them exploit content specificities that are not appropriate for sport whole-match videos. Although video content has been for many years the main source for automatizing knowledge extraction in soccer, the data that records all the events happening on the field has become lately very important in sports analytics, since this event data provides richer context information and requires less processing. We propose a method to generate the summary of a soccer match exploiting both the audio and the event metadata. The results show that our method can detect the actions of the match, identify which of these actions should belong to the summary and then propose multiple candidate summaries which are similar enough but with relevant variability to provide different options to the final editor. Furthermore, we show the generalization capability of our work since it can transfer knowledge between datasets from different broadcasting companies, different competitions, acquired in different conditions, and corresponding to summaries of different lengths



### Unsupervised Denoising of Optical Coherence Tomography Images with Dual_Merged CycleWGAN
- **Arxiv ID**: http://arxiv.org/abs/2205.00698v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.00698v1)
- **Published**: 2022-05-02 07:38:19+00:00
- **Updated**: 2022-05-02 07:38:19+00:00
- **Authors**: Jie Du, Xujian Yang, Kecheng Jin, Xuanzheng Qi, Hu Chen
- **Comment**: Mr. Hu Chen is our corresponding author
- **Journal**: None
- **Summary**: Nosie is an important cause of low quality Optical coherence tomography (OCT) image. The neural network model based on Convolutional neural networks(CNNs) has demonstrated its excellent performance in image denoising. However, OCT image denoising still faces great challenges because many previous neural network algorithms required a large number of labeled data, which might cost much time or is expensive. Besides, these CNN-based algorithms need numerous parameters and good tuning techniques, which is hardware resources consuming. To solved above problems, We proposed a new Cycle-Consistent Generative Adversarial Nets called Dual-Merged Cycle-WGAN for retinal OCT image denoiseing, which has remarkable performance with less unlabeled traning data. Our model consists of two Cycle-GAN networks with imporved generator, descriminator and wasserstein loss to achieve good training stability and better performance. Using image merge technique between two Cycle-GAN networks, our model could obtain more detailed information and hence better training effect. The effectiveness and generality of our proposed network has been proved via ablation experiments and comparative experiments. Compared with other state-of-the-art methods, our unsupervised method obtains best subjective visual effect and higher evaluation objective indicators.



### DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational Lensing Data
- **Arxiv ID**: http://arxiv.org/abs/2205.00701v4
- **DOI**: 10.1007/s00521-023-08766-9
- **Categories**: **astro-ph.IM**, astro-ph.CO, cs.AI, cs.CV, cs.LG, gr-qc
- **Links**: [PDF](http://arxiv.org/pdf/2205.00701v4)
- **Published**: 2022-05-02 07:45:51+00:00
- **Updated**: 2023-06-23 17:13:32+00:00
- **Authors**: Nicolò Oreste Pinciroli Vago, Piero Fraternali
- **Comment**: This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this article is
  published in Neural Computing and Applications, and is available online at
  https://doi.org/10.1007/s00521-023-08766-9
- **Journal**: Neural Comput & Applic (2023)
- **Summary**: Gravitational lensing is the relativistic effect generated by massive bodies, which bend the space-time surrounding them. It is a deeply investigated topic in astrophysics and allows validating theoretical relativistic results and studying faint astrophysical objects that would not be visible otherwise. In recent years Machine Learning methods have been applied to support the analysis of the gravitational lensing phenomena by detecting lensing effects in data sets consisting of images associated with brightness variation time series. However, the state-of-art approaches either consider only images and neglect time-series data or achieve relatively low accuracy on the most difficult data sets. This paper introduces DeepGraviLens, a novel multi-modal network that classifies spatio-temporal data belonging to one non-lensed system type and three lensed system types. It surpasses the current state of the art accuracy results by $\approx 3\%$ to $\approx 11\%$, depending on the considered data set. Such an improvement will enable the acceleration of the analysis of lensed objects in upcoming astrophysical surveys, which will exploit the petabytes of data collected, e.g., from the Vera C. Rubin Observatory.



### 3D Object Detection with a Self-supervised Lidar Scene Flow Backbone
- **Arxiv ID**: http://arxiv.org/abs/2205.00705v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00705v2)
- **Published**: 2022-05-02 07:53:29+00:00
- **Updated**: 2022-07-19 08:22:38+00:00
- **Authors**: Ekim Yurtsever, Emeç Erçelik, Mingyu Liu, Zhijie Yang, Hanzhen Zhang, Pınar Topçam, Maximilian Listl, Yılmaz Kaan Çaylı, Alois Knoll
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art lidar-based 3D object detection methods rely on supervised learning and large labeled datasets. However, annotating lidar data is resource-consuming, and depending only on supervised learning limits the applicability of trained models. Self-supervised training strategies can alleviate these issues by learning a general point cloud backbone model for downstream 3D vision tasks. Against this backdrop, we show the relationship between self-supervised multi-frame flow representations and single-frame 3D detection hypotheses. Our main contribution leverages learned flow and motion representations and combines a self-supervised backbone with a supervised 3D detection head. First, a self-supervised scene flow estimation model is trained with cycle consistency. Then, the point cloud encoder of this model is used as the backbone of a single-frame 3D object detection head model. This second 3D object detection model learns to utilize motion representations to distinguish dynamic objects exhibiting different movement patterns. Experiments on KITTI and nuScenes benchmarks show that the proposed self-supervised pre-training increases 3D detection performance significantly. https://github.com/emecercelik/ssl-3d-detection.git



### FedDKD: Federated Learning with Decentralized Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2205.00706v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.00706v1)
- **Published**: 2022-05-02 07:54:07+00:00
- **Updated**: 2022-05-02 07:54:07+00:00
- **Authors**: Xinjia Li, Boyu Chen, Wenlian Lu
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: The performance of federated learning in neural networks is generally influenced by the heterogeneity of the data distribution. For a well-performing global model, taking a weighted average of the local models, as done by most existing federated learning algorithms, may not guarantee consistency with local models in the space of neural network maps. In this paper, we propose a novel framework of federated learning equipped with the process of decentralized knowledge distillation (FedDKD) (i.e., without data on the server). The FedDKD introduces a module of decentralized knowledge distillation (DKD) to distill the knowledge of the local models to train the global model by approaching the neural network map average based on the metric of divergence defined in the loss function, other than only averaging parameters as done in literature. Numeric experiments on various heterogeneous datasets reveal that FedDKD outperforms the state-of-the-art methods with more efficient communication and training in a few DKD steps, especially on some extremely heterogeneous datasets.



### Dual networks based 3D Multi-Person Pose Estimation from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2205.00748v3
- **DOI**: 10.1109/TPAMI.2022.3170353
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00748v3)
- **Published**: 2022-05-02 08:53:38+00:00
- **Updated**: 2022-05-06 01:19:57+00:00
- **Authors**: Yu Cheng, Bo Wang, Robby T. Tan
- **Comment**: Accepted by TPAMI 2022. arXiv admin note: substantial text overlap
  with arXiv:2104.01797
- **Journal**: None
- **Summary**: Monocular 3D human pose estimation has made progress in recent years. Most of the methods focus on single persons, which estimate the poses in the person-centric coordinates, i.e., the coordinates based on the center of the target person. Hence, these methods are inapplicable for multi-person 3D pose estimation, where the absolute coordinates (e.g., the camera coordinates) are required. Moreover, multi-person pose estimation is more challenging than single pose estimation, due to inter-person occlusion and close human interactions. Existing top-down multi-person methods rely on human detection (i.e., top-down approach), and thus suffer from the detection errors and cannot produce reliable pose estimation in multi-person scenes. Meanwhile, existing bottom-up methods that do not use human detection are not affected by detection errors, but since they process all persons in a scene at once, they are prone to errors, particularly for persons in small scales. To address all these challenges, we propose the integration of top-down and bottom-up approaches to exploit their strengths. Our top-down network estimates human joints from all persons instead of one in an image patch, making it robust to possible erroneous bounding boxes. Our bottom-up network incorporates human-detection based normalized heatmaps, allowing the network to be more robust in handling scale variations. Finally, the estimated 3D poses from the top-down and bottom-up networks are fed into our integration network for final 3D poses. To address the common gaps between training and testing data, we do optimization during the test time, by refining the estimated 3D human poses using high-order temporal constraint, re-projection loss, and bone length regularizations. Our evaluations demonstrate the effectiveness of the proposed method. Code and models are available: https://github.com/3dpose/3D-Multi-Person-Pose.



### Exposing Deepfake Face Forgeries with Guided Residuals
- **Arxiv ID**: http://arxiv.org/abs/2205.00753v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00753v1)
- **Published**: 2022-05-02 08:58:19+00:00
- **Updated**: 2022-05-02 08:58:19+00:00
- **Authors**: Zhiqing Guo, Gaobo Yang, Jiyou Chen, Xingming Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Residual-domain feature is very useful for Deepfake detection because it suppresses irrelevant content features and preserves key manipulation traces. However, inappropriate residual prediction will bring side effects on detection accuracy. In addition, residual-domain features are easily affected by image operations such as compression. Most existing works exploit either spatial-domain features or residual-domain features, while neglecting that two types of features are mutually correlated. In this paper, we propose a guided residuals network, namely GRnet, which fuses spatial-domain and residual-domain features in a mutually reinforcing way, to expose face images generated by Deepfake. Different from existing prediction based residual extraction methods, we propose a manipulation trace extractor (MTE) to directly remove the content features and preserve manipulation traces. MTE is a fine-grained method that can avoid the potential bias caused by inappropriate prediction. Moreover, an attention fusion mechanism (AFM) is designed to selectively emphasize feature channel maps and adaptively allocate the weights for two streams. The experimental results show that the proposed GRnet achieves better performances than the state-of-the-art works on four public fake face datasets including HFF, FaceForensics++, DFDC and Celeb-DF. Especially, GRnet achieves an average accuracy of 97.72% on the HFF dataset, which is at least 5.25% higher than the existing works.



### Point Cloud Compression with Sibling Context and Surface Priors
- **Arxiv ID**: http://arxiv.org/abs/2205.00760v1
- **DOI**: 10.1007/978-3-031-19839-7_43
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00760v1)
- **Published**: 2022-05-02 09:13:26+00:00
- **Updated**: 2022-05-02 09:13:26+00:00
- **Authors**: Zhili Chen, Zian Qian, Sukai Wang, Qifeng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel octree-based multi-level framework for large-scale point cloud compression, which can organize sparse and unstructured point clouds in a memory-efficient way. In this framework, we propose a new entropy model that explores the hierarchical dependency in an octree using the context of siblings' children, ancestors, and neighbors to encode the occupancy information of each non-leaf octree node into a bitstream. Moreover, we locally fit quadratic surfaces with a voxel-based geometry-aware module to provide geometric priors in entropy encoding. These strong priors empower our entropy framework to encode the octree into a more compact bitstream. In the decoding stage, we apply a two-step heuristic strategy to restore point clouds with better reconstruction quality. The quantitative evaluation shows that our method outperforms state-of-the-art baselines with a bitrate improvement of 11-16% and 12-14% on the KITTI Odometry and nuScenes datasets, respectively.



### Rethinking Gradient Operator for Exposing AI-enabled Face Forgeries
- **Arxiv ID**: http://arxiv.org/abs/2205.00767v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00767v1)
- **Published**: 2022-05-02 09:28:26+00:00
- **Updated**: 2022-05-02 09:28:26+00:00
- **Authors**: Zhiqing Guo, Gaobo Yang, Dengyong Zhang, Ming Xia
- **Comment**: None
- **Journal**: None
- **Summary**: For image forensics, convolutional neural networks (CNNs) tend to learn content features rather than subtle manipulation traces, which limits forensic performance. Existing methods predominantly solve the above challenges by following a general pipeline, that is, subtracting the original pixel value from the predicted pixel value to make CNNs pay attention to the manipulation traces. However, due to the complicated learning mechanism, these methods may bring some unnecessary performance losses. In this work, we rethink the advantages of gradient operator in exposing face forgery, and design two plug-and-play modules by combining gradient operator with CNNs, namely tensor pre-processing (TP) and manipulation trace attention (MTA) module. Specifically, TP module refines the feature tensor of each channel in the network by gradient operator to highlight the manipulation traces and improve the feature representation. Moreover, MTA module considers two dimensions, namely channel and manipulation traces, to force the network to learn the distribution of manipulation traces. These two modules can be seamlessly integrated into CNNs for end-to-end training. Experiments show that the proposed network achieves better results than prior works on five public datasets. Especially, TP module greatly improves the accuracy by at least 4.60% compared with the existing pre-processing module only via simple tensor refinement. The code is available at: https://github.com/EricGzq/GocNet-pytorch.



### BSRA: Block-based Super Resolution Accelerator with Hardware Efficient Pixel Attention
- **Arxiv ID**: http://arxiv.org/abs/2205.00777v1
- **DOI**: None
- **Categories**: **cs.AR**, cs.CV, cs.LG, eess.IV, B.7.m; B.5.m
- **Links**: [PDF](http://arxiv.org/pdf/2205.00777v1)
- **Published**: 2022-05-02 09:56:29+00:00
- **Updated**: 2022-05-02 09:56:29+00:00
- **Authors**: Dun-Hao Yang, Tian-Sheuan Chang
- **Comment**: 5 pages, 5 figures, published in IEEE ISCAS 2022
- **Journal**: None
- **Summary**: Increasingly, convolution neural network (CNN) based super resolution models have been proposed for better reconstruction results, but their large model size and complicated structure inhibit their real-time hardware implementation. Current hardware designs are limited to a plain network and suffer from lower quality and high memory bandwidth requirements. This paper proposes a super resolution hardware accelerator with hardware efficient pixel attention that just needs 25.9K parameters and simple structure but achieves 0.38dB better reconstruction images than the widely used FSRCNN. The accelerator adopts full model block wise convolution for full model layer fusion to reduce external memory access to model input and output only. In addition, CNN and pixel attention are well supported by PE arrays with distributed weights. The final implementation can support full HD image reconstruction at 30 frames per second with TSMC 40nm CMOS process.



### Sparse Compressed Spiking Neural Network Accelerator for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.00778v1
- **DOI**: 10.1109/TCSI.2022.3149006
- **Categories**: **cs.AR**, cs.CV, cs.LG, cs.NE, B.5.m
- **Links**: [PDF](http://arxiv.org/pdf/2205.00778v1)
- **Published**: 2022-05-02 09:56:55+00:00
- **Updated**: 2022-05-02 09:56:55+00:00
- **Authors**: Hong-Han Lien, Tian-Sheuan Chang
- **Comment**: 11 pages, 18 figures, to be published in IEEE Transactions on
  Circuits and Systems--I: Regular Papers
- **Journal**: None
- **Summary**: Spiking neural networks (SNNs), which are inspired by the human brain, have recently gained popularity due to their relatively simple and low-power hardware for transmitting binary spikes and highly sparse activation maps. However, because SNNs contain extra time dimension information, the SNN accelerator will require more buffers and take longer to infer, especially for the more difficult high-resolution object detection task. As a result, this paper proposes a sparse compressed spiking neural network accelerator that takes advantage of the high sparsity of activation maps and weights by utilizing the proposed gated one-to-all product for low power and highly parallel model execution. The experimental result of the neural network shows 71.5$\%$ mAP with mixed (1,3) time steps on the IVS 3cls dataset. The accelerator with the TSMC 28nm CMOS process can achieve 1024$\times$576@29 frames per second processing when running at 500MHz with 35.88TOPS/W energy efficiency and 1.05mJ energy consumption per frame.



### Zebra: Memory Bandwidth Reduction for CNN Accelerators With Zero Block Regularization of Activation Maps
- **Arxiv ID**: http://arxiv.org/abs/2205.00779v1
- **DOI**: None
- **Categories**: **cs.AR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.00779v1)
- **Published**: 2022-05-02 09:57:17+00:00
- **Updated**: 2022-05-02 09:57:17+00:00
- **Authors**: Hsu-Tung Shih, Tian-Sheuan Chang
- **Comment**: 5 pages, 5 figures, published in IEEE ISCAS 2021
- **Journal**: None
- **Summary**: The large amount of memory bandwidth between local buffer and external DRAM has become the speedup bottleneck of CNN hardware accelerators, especially for activation maps. To reduce memory bandwidth, we propose to learn pruning unimportant blocks dynamically with zero block regularization of activation maps (Zebra). This strategy has low computational overhead and could easily integrate with other pruning methods for better performance. The experimental results show that the proposed method can reduce 70\% of memory bandwidth for Resnet-18 on Tiny-Imagenet within 1\% accuracy drops and 2\% accuracy gain with the combination of Network Slimming.



### RangeSeg: Range-Aware Real Time Segmentation of 3D LiDAR Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2205.01570v1
- **DOI**: 10.1109/TIV.2021.3085827
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01570v1)
- **Published**: 2022-05-02 09:57:59+00:00
- **Updated**: 2022-05-02 09:57:59+00:00
- **Authors**: Tzu-Hsuan Chen, Tian Sheuan Chang
- **Comment**: 10 pages, 7 figures. IEEE Transactions on Intelligent Vehicles (2021)
- **Journal**: None
- **Summary**: Semantic outdoor scene understanding based on 3D LiDAR point clouds is a challenging task for autonomous driving due to the sparse and irregular data structure. This paper takes advantages of the uneven range distribution of different LiDAR laser beams to propose a range aware instance segmentation network, RangeSeg. RangeSeg uses a shared encoder backbone with two range dependent decoders. A heavy decoder only computes top of a range image where the far and small objects locate to improve small object detection accuracy, and a light decoder computes whole range image for low computational cost. The results are further clustered by the DBSCAN method with a resolution weighted distance function to get instance-level segmentation results. Experiments on the KITTI dataset show that RangeSeg outperforms the state-of-the-art semantic segmentation methods with enormous speedup and improves the instance-level segmentation performance on small and far objects. The whole RangeSeg pipeline meets the real time requirement on NVIDIA\textsuperscript{\textregistered} JETSON AGX Xavier with 19 frames per second in average.



### A Real Time 1280x720 Object Detection Chip With 585MB/s Memory Traffic
- **Arxiv ID**: http://arxiv.org/abs/2205.01571v1
- **DOI**: 10.1109/TVLSI.2022.3149768
- **Categories**: **cs.AR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01571v1)
- **Published**: 2022-05-02 09:58:39+00:00
- **Updated**: 2022-05-02 09:58:39+00:00
- **Authors**: Kuo-Wei Chang, Hsu-Tung Shih, Tian-Sheuan Chang, Shang-Hong Tsai, Chih-Chyau Yang, Chien-Ming Wu, Chun-Ming Huang
- **Comment**: 11 pages, 14 figures, to be published IEEE Transactions on Very Large
  Scale Integration (VLSI) Systems
- **Journal**: None
- **Summary**: Memory bandwidth has become the real-time bottleneck of current deep learning accelerators (DLA), particularly for high definition (HD) object detection. Under resource constraints, this paper proposes a low memory traffic DLA chip with joint hardware and software optimization. To maximize hardware utilization under memory bandwidth, we morph and fuse the object detection model into a group fusion-ready model to reduce intermediate data access. This reduces the YOLOv2's feature memory traffic from 2.9 GB/s to 0.15 GB/s. To support group fusion, our previous DLA based hardware employes a unified buffer with write-masking for simple layer-by-layer processing in a fusion group. When compared to our previous DLA with the same PE numbers, the chip implemented in a TSMC 40nm process supports 1280x720@30FPS object detection and consumes 7.9X less external DRAM access energy, from 2607 mJ to 327.6 mJ.



### CenterCLIP: Token Clustering for Efficient Text-Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2205.00823v1
- **DOI**: 10.1145/3477495.3531950
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2205.00823v1)
- **Published**: 2022-05-02 12:02:09+00:00
- **Updated**: 2022-05-02 12:02:09+00:00
- **Authors**: Shuai Zhao, Linchao Zhu, Xiaohan Wang, Yi Yang
- **Comment**: accepted by SIGIR 2022, code is at
  https://github.com/mzhaoshuai/CenterCLIP
- **Journal**: None
- **Summary**: Recently, large-scale pre-training methods like CLIP have made great progress in multi-modal research such as text-video retrieval. In CLIP, transformers are vital for modeling complex multi-modal relations. However, in the vision transformer of CLIP, the essential visual tokenization process, which produces discrete visual token sequences, generates many homogeneous tokens due to the redundancy nature of consecutive and similar frames in videos. This significantly increases computation costs and hinders the deployment of video retrieval models in web applications. In this paper, to reduce the number of redundant video tokens, we design a multi-segment token clustering algorithm to find the most representative tokens and drop the non-essential ones. As the frame redundancy occurs mostly in consecutive frames, we divide videos into multiple segments and conduct segment-level clustering. Center tokens from each segment are later concatenated into a new sequence, while their original spatial-temporal relations are well maintained. We instantiate two clustering algorithms to efficiently find deterministic medoids and iteratively partition groups in high dimensional space. Through this token clustering and center selection procedure, we successfully reduce computation costs by removing redundant visual tokens. This method further enhances segment-level semantic alignment between video and text representations, enforcing the spatio-temporal interactions of tokens from within-segment frames. Our method, coined as CenterCLIP, surpasses existing state-of-the-art by a large margin on typical text-video benchmarks, while reducing the training memory cost by 35\% and accelerating the inference speed by 14\% at the best case. The code is available at \href{{https://github.com/mzhaoshuai/CenterCLIP}}{{https://github.com/mzhaoshuai/CenterCLIP}}.



### APP-Net: Auxiliary-point-based Push and Pull Operations for Efficient Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/2205.00847v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00847v2)
- **Published**: 2022-05-02 12:21:11+00:00
- **Updated**: 2022-08-18 15:21:01+00:00
- **Authors**: Tao Lu, Chunxu Liu, Youxin Chen, Gangshan Wu, Limin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Aggregating neighbor features is essential for point cloud classification. In the existing work, each point in the cloud may inevitably be selected as the neighbors of multiple aggregation centers, as all centers will gather neighbor features from the whole point cloud independently. Thus each point has to participate in the calculation repeatedly and generates redundant duplicates in the memory, leading to intensive computation costs and memory consumption. Meanwhile, to pursue higher accuracy, previous methods often rely on a complex local aggregator to extract fine geometric representation, which further slows down the classification pipeline. To address these issues, we propose a new local aggregator of linear complexity for point cloud classification, coined as APP. Specifically, we introduce an auxiliary container as an anchor to exchange features between the source point and the aggregating center. Each source point pushes its feature to only one auxiliary container, and each center point pulls features from only one auxiliary container. This avoids the re-computation issue of each source point. To facilitate the learning of the local structure of cloud point, we use an online normal estimation module to provide the explainable geometric information to enhance our APP modeling capability. Our built network is more efficient than all the previous baselines with a clear margin while still consuming a lower memory. Experiments on both synthetic and real datasets demonstrate that APP-Net reaches comparable accuracies to other networks. It can process more than 10,000 samples per second with less than 10GB of memory on a single GPU. We will release the code in https://github.com/MCG-NJU/APP-Net.



### Stability-driven Contact Reconstruction From Monocular Color Images
- **Arxiv ID**: http://arxiv.org/abs/2205.00848v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00848v1)
- **Published**: 2022-05-02 12:23:06+00:00
- **Updated**: 2022-05-02 12:23:06+00:00
- **Authors**: Zimeng Zhao, Binghui Zuo, Wei Xie, Yangang Wang
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: Physical contact provides additional constraints for hand-object state reconstruction as well as a basis for further understanding of interaction affordances. Estimating these severely occluded regions from monocular images presents a considerable challenge. Existing methods optimize the hand-object contact driven by distance threshold or prior from contact-labeled datasets. However, due to the number of subjects and objects involved in these indoor datasets being limited, the learned contact patterns could not be generalized easily. Our key idea is to reconstruct the contact pattern directly from monocular images, and then utilize the physical stability criterion in the simulation to optimize it. This criterion is defined by the resultant forces and contact distribution computed by the physics engine.Compared to existing solutions, our framework can be adapted to more personalized hands and diverse object shapes. Furthermore, an interaction dataset with extra physical attributes is created to verify the sim-to-real consistency of our methods. Through comprehensive evaluations, hand-object contact can be reconstructed with both accuracy and stability by the proposed framework.



### Cross-Domain Correlation Distillation for Unsupervised Domain Adaptation in Nighttime Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.00858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00858v1)
- **Published**: 2022-05-02 12:42:04+00:00
- **Updated**: 2022-05-02 12:42:04+00:00
- **Authors**: Huan Gao, Jichang Guo, Guoli Wang, Qian Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The performance of nighttime semantic segmentation is restricted by the poor illumination and a lack of pixel-wise annotation, which severely limit its application in autonomous driving. Existing works, e.g., using the twilight as the intermediate target domain to perform the adaptation from daytime to nighttime, may fail to cope with the inherent difference between datasets caused by the camera equipment and the urban style. Faced with these two types of domain shifts, i.e., the illumination and the inherent difference of the datasets, we propose a novel domain adaptation framework via cross-domain correlation distillation, called CCDistill. The invariance of illumination or inherent difference between two images is fully explored so as to make up for the lack of labels for nighttime images. Specifically, we extract the content and style knowledge contained in features, calculate the degree of inherent or illumination difference between two images. The domain adaptation is achieved using the invariance of the same kind of difference. Extensive experiments on Dark Zurich and ACDC demonstrate that CCDistill achieves the state-of-the-art performance for nighttime semantic segmentation. Notably, our method is a one-stage domain adaptation network which can avoid affecting the inference time. Our implementation is available at https://github.com/ghuan99/CCDistill.



### On the generalization capabilities of FSL methods through domain adaptation: a case study in endoscopic kidney stone image classification
- **Arxiv ID**: http://arxiv.org/abs/2205.00895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00895v1)
- **Published**: 2022-05-02 13:08:31+00:00
- **Updated**: 2022-05-02 13:08:31+00:00
- **Authors**: Mauricio Mendez-Ruiz, Francisco Lopez-Tiro, Jonathan El-Beze, Vincent Estrade, Gilberto Ochoa-Ruiz1, Jacques Hubert, Andres Mendez-Vazquez, Christian Daul
- **Comment**: Paper submitted to MICAI 2022
- **Journal**: None
- **Summary**: Deep learning has shown great promise in diverse areas of computer vision, such as image classification, object detection and semantic segmentation, among many others. However, as it has been repeatedly demonstrated, deep learning methods trained on a dataset do not generalize well to datasets from other domains or even to similar datasets, due to data distribution shifts. In this work, we propose the use of a meta-learning based few-shot learning approach to alleviate these problems. In order to demonstrate its efficacy, we use two datasets of kidney stones samples acquired with different endoscopes and different acquisition conditions. The results show how such methods are indeed capable of handling domain-shifts by attaining an accuracy of 74.38% and 88.52% in the 5-way 5-shot and 5-way 20-shot settings respectively. Instead, in the same dataset, traditional Deep Learning (DL) methods attain only an accuracy of 45%.



### MemSeg: A semi-supervised method for image surface defect detection using differences and commonalities
- **Arxiv ID**: http://arxiv.org/abs/2205.00908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00908v1)
- **Published**: 2022-05-02 13:37:12+00:00
- **Updated**: 2022-05-02 13:37:12+00:00
- **Authors**: Minghui Yang, Peng Wu, Jing Liu, Hui Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Under the semi-supervised framework, we propose an end-to-end memory-based segmentation network (MemSeg) to detect surface defects on industrial products. Considering the small intra-class variance of products in the same production line, from the perspective of differences and commonalities, MemSeg introduces artificially simulated abnormal samples and memory samples to assist the learning of the network. In the training phase, MemSeg explicitly learns the potential differences between normal and simulated abnormal images to obtain a robust classification hyperplane. At the same time, inspired by the mechanism of human memory, MemSeg uses a memory pool to store the general patterns of normal samples. By comparing the similarities and differences between input samples and memory samples in the memory pool to give effective guesses about abnormal regions; In the inference phase, MemSeg directly determines the abnormal regions of the input image in an end-to-end manner. Through experimental validation, MemSeg achieves the state-of-the-art (SOTA) performance on MVTec AD datasets with AUC scores of 99.56% and 98.84% at the image-level and pixel-level, respectively. In addition, MemSeg also has a significant advantage in inference speed benefiting from the end-to-end and straightforward network structure, which better meets the real-time requirement in industrial scenarios.



### A Novel Speech-Driven Lip-Sync Model with CNN and LSTM
- **Arxiv ID**: http://arxiv.org/abs/2205.00916v1
- **DOI**: 10.1109/CISP-BMEI53629.2021.9624360
- **Categories**: **cs.SD**, cs.AI, cs.CV, cs.GR, eess.AS, 68T07, 68T45, I.2.10; I.3.7
- **Links**: [PDF](http://arxiv.org/pdf/2205.00916v1)
- **Published**: 2022-05-02 13:57:50+00:00
- **Updated**: 2022-05-02 13:57:50+00:00
- **Authors**: Xiaohong Li, Xiang Wang, Kai Wang, Shiguo Lian
- **Comment**: This paper has been published on CISP-BMEI 2021. See
  https://ieeexplore.ieee.org/document/9624360
- **Journal**: None
- **Summary**: Generating synchronized and natural lip movement with speech is one of the most important tasks in creating realistic virtual characters. In this paper, we present a combined deep neural network of one-dimensional convolutions and LSTM to generate vertex displacement of a 3D template face model from variable-length speech input. The motion of the lower part of the face, which is represented by the vertex movement of 3D lip shapes, is consistent with the input speech. In order to enhance the robustness of the network to different sound signals, we adapt a trained speech recognition model to extract speech feature, and a velocity loss term is adopted to reduce the jitter of generated facial animation. We recorded a series of videos of a Chinese adult speaking Mandarin and created a new speech-animation dataset to compensate the lack of such public data. Qualitative and quantitative evaluations indicate that our model is able to generate smooth and natural lip movements synchronized with speech.



### Revisiting Gaussian Neurons for Online Clustering with Unknown Number of Clusters
- **Arxiv ID**: http://arxiv.org/abs/2205.00920v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2205.00920v2)
- **Published**: 2022-05-02 14:01:40+00:00
- **Updated**: 2022-08-06 20:19:08+00:00
- **Authors**: Ole Christian Eidheim
- **Comment**: Reviewed at
  https://openreview.net/forum?id=h05RLBNweX&referrer=%5BTMLR%5D(%2Fgroup%3Fid%3DTMLR)
- **Journal**: None
- **Summary**: Despite the recent success of artificial neural networks, more biologically plausible learning methods may be needed to resolve the weaknesses of backpropagation trained models such as catastrophic forgetting and adversarial attacks. Although these weaknesses are not specifically addressed, a novel local learning rule is presented that performs online clustering with an upper limit on the number of clusters to be found rather than a fixed cluster count. Instead of using orthogonal weight or output activation constraints, activation sparsity is achieved by mutual repulsion of lateral Gaussian neurons ensuring that multiple neuron centers cannot occupy the same location in the input domain. An update method is also presented for adjusting the widths of the Gaussian neurons in cases where the data samples can be represented by means and variances. The algorithms were applied on the MNIST and CIFAR-10 datasets to create filters capturing the input patterns of pixel patches of various sizes. The experimental results demonstrate stability in the learned parameters across a large number of training samples.



### Understanding CNNs from excitations
- **Arxiv ID**: http://arxiv.org/abs/2205.00932v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.00932v2)
- **Published**: 2022-05-02 14:27:35+00:00
- **Updated**: 2022-05-04 13:16:01+00:00
- **Authors**: Zijian Ying, Qianmu Li, Zhichao Lian
- **Comment**: None
- **Journal**: None
- **Summary**: For instance-level explanation, in order to reveal the relations between high-level semantics and detailed spatial information, this paper proposes a novel cognitive approach to neural networks, which named PANE. Under the guidance of PANE, a novel saliency map representation method, named IOM, is proposed for CNN-like models. We make the comparison with eight state-of-the-art saliency map representation methods. The experimental results show that IOM far outperforms baselines. The work of this paper may bring a new perspective to understand deep neural networks.



### Assessing unconstrained surgical cuttings in VR using CNNs
- **Arxiv ID**: http://arxiv.org/abs/2205.00934v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2205.00934v1)
- **Published**: 2022-05-02 14:32:59+00:00
- **Updated**: 2022-05-02 14:32:59+00:00
- **Authors**: Ilias Chrysovergis, Manos Kamarianakis, Mike Kentros, Dimitris Angelis, Antonis Protopsaltis, George Papagiannakis
- **Comment**: 2 pages, 2 figures, Submitted to the Siggraph '22 Poster Session
  (Vancouver, 8-11 Aug 2022)
- **Journal**: None
- **Summary**: We present a Convolutional Neural Network (CNN) suitable to assess unconstrained surgical cuttings, trained on a dataset created with a data augmentation technique.



### CCLF: A Contrastive-Curiosity-Driven Learning Framework for Sample-Efficient Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.00943v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2205.00943v2)
- **Published**: 2022-05-02 14:42:05+00:00
- **Updated**: 2022-05-03 06:22:54+00:00
- **Authors**: Chenyu Sun, Hangwei Qian, Chunyan Miao
- **Comment**: Full paper with supplementary material, accepted by IJCAI 2022.
  Acknowledgements and affiliations are updated
- **Journal**: None
- **Summary**: In reinforcement learning (RL), it is challenging to learn directly from high-dimensional observations, where data augmentation has recently been shown to remedy this via encoding invariances from raw pixels. Nevertheless, we empirically find that not all samples are equally important and hence simply injecting more augmented inputs may instead cause instability in Q-learning. In this paper, we approach this problem systematically by developing a model-agnostic Contrastive-Curiosity-Driven Learning Framework (CCLF), which can fully exploit sample importance and improve learning efficiency in a self-supervised manner. Facilitated by the proposed contrastive curiosity, CCLF is capable of prioritizing the experience replay, selecting the most informative augmented inputs, and more importantly regularizing the Q-function as well as the encoder to concentrate more on under-learned data. Moreover, it encourages the agent to explore with a curiosity-based reward. As a result, the agent can focus on more informative samples and learn representation invariances more efficiently, with significantly reduced augmented inputs. We apply CCLF to several base RL algorithms and evaluate on the DeepMind Control Suite, Atari, and MiniGrid benchmarks, where our approach demonstrates superior sample efficiency and learning performances compared with other state-of-the-art methods.



### Answer-Me: Multi-Task Open-Vocabulary Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2205.00949v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00949v2)
- **Published**: 2022-05-02 14:53:13+00:00
- **Updated**: 2022-11-30 21:57:52+00:00
- **Authors**: AJ Piergiovanni, Wei Li, Weicheng Kuo, Mohammad Saffar, Fred Bertsch, Anelia Angelova
- **Comment**: None
- **Journal**: None
- **Summary**: We present Answer-Me, a task-aware multi-task framework which unifies a variety of question answering tasks, such as, visual question answering, visual entailment, visual reasoning. In contrast to previous works using contrastive or generative captioning training, we propose a novel and simple recipe to pre-train a vision-language joint model, which is multi-task as well. The pre-training uses only noisy image captioning data, and is formulated to use the entire architecture end-to-end with both a strong language encoder and decoder. Our results show state-of-the-art performance, zero-shot generalization, robustness to forgetting, and competitive single-task results across a variety of question answering tasks. Our multi-task mixture training learns from tasks of various question intents and thus generalizes better, including on zero-shot vision-language tasks. We conduct experiments in the challenging multi-task and open-vocabulary settings and across a variety of datasets and tasks, such as VQA2.0, SNLI-VE, NLVR2, GQA. We observe that the proposed approach is able to generalize to unseen tasks and that more diverse mixtures lead to higher accuracy in both known and novel tasks.



### Leaf Tar Spot Detection Using RGB Images
- **Arxiv ID**: http://arxiv.org/abs/2205.00952v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00952v1)
- **Published**: 2022-05-02 14:56:06+00:00
- **Updated**: 2022-05-02 14:56:06+00:00
- **Authors**: Sriram Baireddy, Da-Young Lee, Carlos Gongora-Canul, Christian D. Cruz, Edward J. Delp
- **Comment**: None
- **Journal**: None
- **Summary**: Tar spot disease is a fungal disease that appears as a series of black circular spots containing spores on corn leaves. Tar spot has proven to be an impactful disease in terms of reducing crop yield. To quantify disease progression, experts usually have to visually phenotype leaves from the plant. This process is very time-consuming and is difficult to incorporate in any high-throughput phenotyping system. Deep neural networks could provide quick, automated tar spot detection with sufficient ground truth. However, manually labeling tar spots in images to serve as ground truth is also tedious and time-consuming. In this paper we first describe an approach that uses automated image analysis tools to generate ground truth images that are then used for training a Mask R-CNN. We show that a Mask R-CNN can be used effectively to detect tar spots in close-up images of leaf surfaces. We additionally show that the Mask R-CNN can also be used for in-field images of whole leaves to capture the number of tar spots and area of the leaf infected by the disease.



### Monocular 3D Fingerprint Reconstruction and Unwarping
- **Arxiv ID**: http://arxiv.org/abs/2205.00967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00967v1)
- **Published**: 2022-05-02 15:09:05+00:00
- **Updated**: 2022-05-02 15:09:05+00:00
- **Authors**: Zhe Cui, Jianjiang Feng, Jie Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Compared with contact-based fingerprint acquisition techniques, contactless acquisition has the advantages of less skin distortion, larger fingerprint area, and hygienic acquisition. However, perspective distortion is a challenge in contactless fingerprint recognition, which changes ridge orientation, frequency, and minutiae location, and thus causes degraded recognition accuracy. We propose a learning based shape from texture algorithm to reconstruct a 3D finger shape from a single image and unwarp the raw image to suppress perspective distortion. Experimental results on contactless fingerprint databases show that the proposed method has high 3D reconstruction accuracy. Matching experiments on contactless-contact and contactless-contactless matching prove that the proposed method improves matching accuracy.



### Detection Recovery in Online Multi-Object Tracking with Sparse Graph Tracker
- **Arxiv ID**: http://arxiv.org/abs/2205.00968v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00968v2)
- **Published**: 2022-05-02 15:09:36+00:00
- **Updated**: 2022-10-22 13:39:30+00:00
- **Authors**: Jeongseok Hyun, Myunggu Kang, Dongyoon Wee, Dit-Yan Yeung
- **Comment**: Accepted to WACV 2023
- **Journal**: None
- **Summary**: In existing joint detection and tracking methods, pairwise relational features are used to match previous tracklets to current detections. However, the features may not be discriminative enough for a tracker to identify a target from a large number of detections. Selecting only high-scored detections for tracking may lead to missed detections whose confidence score is low. Consequently, in the online setting, this results in disconnections of tracklets which cannot be recovered. In this regard, we present Sparse Graph Tracker (SGT), a novel online graph tracker using higher-order relational features which are more discriminative by aggregating the features of neighboring detections and their relations. SGT converts video data into a graph where detections, their connections, and the relational features of two connected nodes are represented by nodes, edges, and edge features, respectively. The strong edge features allow SGT to track targets with tracking candidates selected by top-K scored detections with large K. As a result, even low-scored detections can be tracked, and the missed detections are also recovered. The robustness of K value is shown through the extensive experiments. In the MOT16/17/20 and HiEve Challenge, SGT outperforms the state-of-the-art trackers with real-time inference speed. Especially, a large improvement in MOTA is shown in the MOT20 and HiEve Challenge. Code is available at https://github.com/HYUNJS/SGT.



### Open-Set Semi-Supervised Learning for 3D Point Cloud Understanding
- **Arxiv ID**: http://arxiv.org/abs/2205.01006v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.01006v1)
- **Published**: 2022-05-02 16:09:17+00:00
- **Updated**: 2022-05-02 16:09:17+00:00
- **Authors**: Xian Shi, Xun Xu, Wanyue Zhang, Xiatian Zhu, Chuan Sheng Foo, Kui Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic understanding of 3D point cloud relies on learning models with massively annotated data, which, in many cases, are expensive or difficult to collect. This has led to an emerging research interest in semi-supervised learning (SSL) for 3D point cloud. It is commonly assumed in SSL that the unlabeled data are drawn from the same distribution as that of the labeled ones; This assumption, however, rarely holds true in realistic environments. Blindly using out-of-distribution (OOD) unlabeled data could harm SSL performance. In this work, we propose to selectively utilize unlabeled data through sample weighting, so that only conducive unlabeled data would be prioritized. To estimate the weights, we adopt a bi-level optimization framework which iteratively optimizes a metaobjective on a held-out validation set and a task-objective on a training set. Faced with the instability of efficient bi-level optimizers, we further propose three regularization techniques to enhance the training stability. Extensive experiments on 3D point cloud classification and segmentation tasks verify the effectiveness of our proposed method. We also demonstrate the feasibility of a more efficient training strategy.



### Incomplete Gamma Kernels: Generalizing Locally Optimal Projection Operators
- **Arxiv ID**: http://arxiv.org/abs/2205.01087v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2205.01087v1)
- **Published**: 2022-05-02 17:59:02+00:00
- **Updated**: 2022-05-02 17:59:02+00:00
- **Authors**: Patrick Stotko, Michael Weinmann, Reinhard Klein
- **Comment**: None
- **Journal**: None
- **Summary**: We present incomplete gamma kernels, a generalization of Locally Optimal Projection (LOP) operators. In particular, we reveal the relation of the classical localized $ L_1 $ estimator, used in the LOP operator for surface reconstruction from noisy point clouds, to the common Mean Shift framework via a novel kernel. Furthermore, we generalize this result to a whole family of kernels that are built upon the incomplete gamma function and each represents a localized $ L_p $ estimator. By deriving various properties of the kernel family concerning distributional, Mean Shift induced, and other aspects such as strict positive definiteness, we obtain a deeper understanding of the operator's projection behavior. From these theoretical insights, we illustrate several applications ranging from an improved Weighted LOP (WLOP) density weighting scheme and a more accurate Continuous LOP (CLOP) kernel approximation to the definition of a novel set of robust loss functions. These incomplete gamma losses include the Gaussian and LOP loss as special cases and can be applied for reconstruction tasks such as normal filtering. We demonstrate the effects of each application in a range of quantitative and qualitative experiments that highlight the benefits induced by our modifications.



### ComPhy: Compositional Physical Reasoning of Objects and Events from Videos
- **Arxiv ID**: http://arxiv.org/abs/2205.01089v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.01089v1)
- **Published**: 2022-05-02 17:59:13+00:00
- **Updated**: 2022-05-02 17:59:13+00:00
- **Authors**: Zhenfang Chen, Kexin Yi, Yunzhu Li, Mingyu Ding, Antonio Torralba, Joshua B. Tenenbaum, Chuang Gan
- **Comment**: ICLR 2022. Project page: https://comphyreasoning.github.io/
- **Journal**: None
- **Summary**: Objects' motions in nature are governed by complex interactions and their properties. While some properties, such as shape and material, can be identified via the object's visual appearances, others like mass and electric charge are not directly visible. The compositionality between the visible and hidden properties poses unique challenges for AI models to reason from the physical world, whereas humans can effortlessly infer them with limited observations. Existing studies on video reasoning mainly focus on visually observable elements such as object appearance, movement, and contact interaction. In this paper, we take an initial step to highlight the importance of inferring the hidden physical properties not directly observable from visual appearances, by introducing the Compositional Physical Reasoning (ComPhy) dataset. For a given set of objects, ComPhy includes few videos of them moving and interacting under different initial conditions. The model is evaluated based on its capability to unravel the compositional hidden properties, such as mass and charge, and use this knowledge to answer a set of questions posted on one of the videos. Evaluation results of several state-of-the-art video reasoning models on ComPhy show unsatisfactory performance as they fail to capture these hidden properties. We further propose an oracle neural-symbolic framework named Compositional Physics Learner (CPL), combining visual perception, physical property learning, dynamic prediction, and symbolic execution into a unified framework. CPL can effectively identify objects' physical properties from their interactions and predict their dynamics to answer questions.



### A Deep Learning-based Integrated Framework for Quality-aware Undersampled Cine Cardiac MRI Reconstruction and Analysis
- **Arxiv ID**: http://arxiv.org/abs/2205.01673v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01673v1)
- **Published**: 2022-05-02 18:02:22+00:00
- **Updated**: 2022-05-02 18:02:22+00:00
- **Authors**: Inês P. Machado, Esther Puyol-Antón, Kerstin Hammernik, Gastão Cruz, Devran Ugurlu, Ihsane Olakorede, Ilkay Oksuz, Bram Ruijsink, Miguel Castelo-Branco, Alistair A. Young, Claudia Prieto, Julia A. Schnabel, Andrew P. King
- **Comment**: None
- **Journal**: None
- **Summary**: Cine cardiac magnetic resonance (CMR) imaging is considered the gold standard for cardiac function evaluation. However, cine CMR acquisition is inherently slow and in recent decades considerable effort has been put into accelerating scan times without compromising image quality or the accuracy of derived results. In this paper, we present a fully-automated, quality-controlled integrated framework for reconstruction, segmentation and downstream analysis of undersampled cine CMR data. The framework enables active acquisition of radial k-space data, in which acquisition can be stopped as soon as acquired data are sufficient to produce high quality reconstructions and segmentations. This results in reduced scan times and automated analysis, enabling robust and accurate estimation of functional biomarkers. To demonstrate the feasibility of the proposed approach, we perform realistic simulations of radial k-space acquisitions on a dataset of subjects from the UK Biobank and present results on in-vivo cine CMR k-space data collected from healthy subjects. The results demonstrate that our method can produce quality-controlled images in a mean scan time reduced from 12 to 4 seconds per slice, and that image quality is sufficient to allow clinically relevant parameters to be automatically estimated to within 5% mean absolute difference.



### Hausa Visual Genome: A Dataset for Multi-Modal English to Hausa Machine Translation
- **Arxiv ID**: http://arxiv.org/abs/2205.01133v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01133v2)
- **Published**: 2022-05-02 18:05:35+00:00
- **Updated**: 2022-05-06 16:00:39+00:00
- **Authors**: Idris Abdulmumin, Satya Ranjan Dash, Musa Abdullahi Dawud, Shantipriya Parida, Shamsuddeen Hassan Muhammad, Ibrahim Sa'id Ahmad, Subhadarshi Panda, Ondřej Bojar, Bashir Shehu Galadanci, Bello Shehu Bello
- **Comment**: Accepted at Language Resources and Evaluation Conference 2022
  (LREC2022)
- **Journal**: None
- **Summary**: Multi-modal Machine Translation (MMT) enables the use of visual information to enhance the quality of translations. The visual information can serve as a valuable piece of context information to decrease the ambiguity of input sentences. Despite the increasing popularity of such a technique, good and sizeable datasets are scarce, limiting the full extent of their potential. Hausa, a Chadic language, is a member of the Afro-Asiatic language family. It is estimated that about 100 to 150 million people speak the language, with more than 80 million indigenous speakers. This is more than any of the other Chadic languages. Despite a large number of speakers, the Hausa language is considered low-resource in natural language processing (NLP). This is due to the absence of sufficient resources to implement most NLP tasks. While some datasets exist, they are either scarce, machine-generated, or in the religious domain. Therefore, there is a need to create training and evaluation data for implementing machine learning tasks and bridging the research gap in the language. This work presents the Hausa Visual Genome (HaVG), a dataset that contains the description of an image or a section within the image in Hausa and its equivalent in English. To prepare the dataset, we started by translating the English description of the images in the Hindi Visual Genome (HVG) into Hausa automatically. Afterward, the synthetic Hausa data was carefully post-edited considering the respective images. The dataset comprises 32,923 images and their descriptions that are divided into training, development, test, and challenge test set. The Hausa Visual Genome is the first dataset of its kind and can be used for Hausa-English machine translation, multi-modal research, and image description, among various other natural language processing and generation tasks.



### D-DPCC: Deep Dynamic Point Cloud Compression via 3D Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2205.01135v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01135v1)
- **Published**: 2022-05-02 18:10:45+00:00
- **Updated**: 2022-05-02 18:10:45+00:00
- **Authors**: Tingyu Fan, Linyao Gao, Yiling Xu, Zhu Li, Dong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The non-uniformly distributed nature of the 3D dynamic point cloud (DPC) brings significant challenges to its high-efficient inter-frame compression. This paper proposes a novel 3D sparse convolution-based Deep Dynamic Point Cloud Compression (D-DPCC) network to compensate and compress the DPC geometry with 3D motion estimation and motion compensation in the feature space. In the proposed D-DPCC network, we design a {\it Multi-scale Motion Fusion} (MMF) module to accurately estimate the 3D optical flow between the feature representations of adjacent point cloud frames. Specifically, we utilize a 3D sparse convolution-based encoder to obtain the latent representation for motion estimation in the feature space and introduce the proposed MMF module for fused 3D motion embedding. Besides, for motion compensation, we propose a 3D {\it Adaptively Weighted Interpolation} (3DAWI) algorithm with a penalty coefficient to adaptively decrease the impact of distant neighbors. We compress the motion embedding and the residual with a lossy autoencoder-based network. To our knowledge, this paper is the first work proposing an end-to-end deep dynamic point cloud compression framework. The experimental result shows that the proposed D-DPCC framework achieves an average 76\% BD-Rate (Bjontegaard Delta Rate) gains against state-of-the-art Video-based Point Cloud Compression (V-PCC) v13 in inter mode.



### Cost-Aware Evaluation and Model Scaling for LiDAR-Based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.01142v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01142v3)
- **Published**: 2022-05-02 18:16:17+00:00
- **Updated**: 2023-03-10 03:50:24+00:00
- **Authors**: Xiaofang Wang, Kris M. Kitani
- **Comment**: ICRA 2023
- **Journal**: None
- **Summary**: Considerable research effort has been devoted to LiDAR-based 3D object detection and empirical performance has been significantly improved. While progress has been encouraging, we observe an overlooked issue: it is not yet common practice to compare different 3D detectors under the same cost, e.g., inference latency. This makes it difficult to quantify the true performance gain brought by recently proposed architecture designs. The goal of this work is to conduct a cost-aware evaluation of LiDAR-based 3D object detectors. Specifically, we focus on SECOND, a simple grid-based one-stage detector, and analyze its performance under different costs by scaling its original architecture. Then we compare the family of scaled SECOND with recent 3D detection methods, such as Voxel R-CNN and PV-RCNN++. The results are surprising. We find that, if allowed to use the same latency, SECOND can match the performance of PV-RCNN++, the current state-of-the-art method on the Waymo Open Dataset. Scaled SECOND also easily outperforms many recent 3D detection methods published during the past year. We recommend future research control the inference cost in their empirical comparison and include the family of scaled SECOND as a strong baseline when presenting novel 3D detection methods.



### Emotion-Controllable Generalized Talking Face Generation
- **Arxiv ID**: http://arxiv.org/abs/2205.01155v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.01155v1)
- **Published**: 2022-05-02 18:41:36+00:00
- **Updated**: 2022-05-02 18:41:36+00:00
- **Authors**: Sanjana Sinha, Sandika Biswas, Ravindra Yadav, Brojeshwar Bhowmick
- **Comment**: Accepted at IJCAI 2022
- **Journal**: None
- **Summary**: Despite the significant progress in recent years, very few of the AI-based talking face generation methods attempt to render natural emotions. Moreover, the scope of the methods is majorly limited to the characteristics of the training dataset, hence they fail to generalize to arbitrary unseen faces. In this paper, we propose a one-shot facial geometry-aware emotional talking face generation method that can generalize to arbitrary faces. We propose a graph convolutional neural network that uses speech content feature, along with an independent emotion input to generate emotion and speech-induced motion on facial geometry-aware landmark representation. This representation is further used in our optical flow-guided texture generation network for producing the texture. We propose a two-branch texture generation network, with motion and texture branches designed to consider the motion and texture content independently. Compared to the previous emotion talking face methods, our method can adapt to arbitrary faces captured in-the-wild by fine-tuning with only a single image of the target identity in neutral emotion.



### SELC: Self-Ensemble Label Correction Improves Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2205.01156v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01156v1)
- **Published**: 2022-05-02 18:42:47+00:00
- **Updated**: 2022-05-02 18:42:47+00:00
- **Authors**: Yangdi Lu, Wenbo He
- **Comment**: Accepted to IJCAI 2022
- **Journal**: None
- **Summary**: Deep neural networks are prone to overfitting noisy labels, resulting in poor generalization performance. To overcome this problem, we present a simple and effective method self-ensemble label correction (SELC) to progressively correct noisy labels and refine the model. We look deeper into the memorization behavior in training with noisy labels and observe that the network outputs are reliable in the early stage. To retain this reliable knowledge, SELC uses ensemble predictions formed by an exponential moving average of network outputs to update the original noisy labels. We show that training with SELC refines the model by gradually reducing supervision from noisy labels and increasing supervision from ensemble predictions. Despite its simplicity, compared with many state-of-the-art methods, SELC obtains more promising and stable results in the presence of class-conditional, instance-dependent, and real-world label noise. The code is available at https://github.com/MacLLL/SELC.



### Saliency map using features derived from spiking neural networks of primate visual cortex
- **Arxiv ID**: http://arxiv.org/abs/2205.01159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01159v1)
- **Published**: 2022-05-02 18:52:39+00:00
- **Updated**: 2022-05-02 18:52:39+00:00
- **Authors**: Reza Hojjaty Saeedy, Richard A. Messner
- **Comment**: 19 pages, 8 figures, 1 table
- **Journal**: None
- **Summary**: We propose a framework inspired by biological vision systems to produce saliency maps of digital images. Well-known computational models for receptive fields of areas in the visual cortex that are specialized for color and orientation perception are used. To model the connectivity between these areas we use the CARLsim library which is a spiking neural network(SNN) simulator. The spikes generated by CARLsim, then serve as extracted features and input to our saliency detection algorithm. This new method of saliency detection is described and applied to benchmark images.



### 3D Convolutional Neural Networks for Dendrite Segmentation Using Fine-Tuning and Hyperparameter Optimization
- **Arxiv ID**: http://arxiv.org/abs/2205.01167v1
- **DOI**: None
- **Categories**: **cs.CV**, cond-mat.mtrl-sci, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01167v1)
- **Published**: 2022-05-02 19:20:05+00:00
- **Updated**: 2022-05-02 19:20:05+00:00
- **Authors**: Jim James, Nathan Pruyne, Tiberiu Stan, Marcus Schwarting, Jiwon Yeom, Seungbum Hong, Peter Voorhees, Ben Blaiszik, Ian Foster
- **Comment**: None
- **Journal**: None
- **Summary**: Dendritic microstructures are ubiquitous in nature and are the primary solidification morphologies in metallic materials. Techniques such as x-ray computed tomography (XCT) have provided new insights into dendritic phase transformation phenomena. However, manual identification of dendritic morphologies in microscopy data can be both labor intensive and potentially ambiguous. The analysis of 3D datasets is particularly challenging due to their large sizes (terabytes) and the presence of artifacts scattered within the imaged volumes. In this study, we trained 3D convolutional neural networks (CNNs) to segment 3D datasets. Three CNN architectures were investigated, including a new 3D version of FCDense. We show that using hyperparameter optimization (HPO) and fine-tuning techniques, both 2D and 3D CNN architectures can be trained to outperform the previous state of the art. The 3D U-Net architecture trained in this study produced the best segmentations according to quantitative metrics (pixel-wise accuracy of 99.84% and a boundary displacement error of 0.58 pixels), while 3D FCDense produced the smoothest boundaries and best segmentations according to visual inspection. The trained 3D CNNs are able to segment entire 852 x 852 x 250 voxel 3D volumes in only ~60 seconds, thus hastening the progress towards a deeper understanding of phase transformation phenomena such as dendritic solidification.



### Boosting Video Object Segmentation based on Scale Inconsistency
- **Arxiv ID**: http://arxiv.org/abs/2205.01197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01197v1)
- **Published**: 2022-05-02 20:22:29+00:00
- **Updated**: 2022-05-02 20:22:29+00:00
- **Authors**: Hengyi Wang, Changjae Oh
- **Comment**: None
- **Journal**: None
- **Summary**: We present a refinement framework to boost the performance of pre-trained semi-supervised video object segmentation (VOS) models. Our work is based on scale inconsistency, which is motivated by the observation that existing VOS models generate inconsistent predictions from input frames with different sizes. We use the scale inconsistency as a clue to devise a pixel-level attention module that aggregates the advantages of the predictions from different-size inputs. The scale inconsistency is also used to regularize the training based on a pixel-level variance measured by an uncertainty estimation. We further present a self-supervised online adaptation, tailored for test-time optimization, that bootstraps the predictions without ground-truth masks based on the scale inconsistency. Experiments on DAVIS 16 and DAVIS 17 datasets show that our framework can be generically applied to various VOS models and improve their performance.



### NHA12D: A New Pavement Crack Dataset and a Comparison Study Of Crack Detection Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2205.01198v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01198v1)
- **Published**: 2022-05-02 20:22:50+00:00
- **Updated**: 2022-05-02 20:22:50+00:00
- **Authors**: Zhening Huang, Weiwei Chen, Abir Al-Tabbaa, Ioannis Brilakis
- **Comment**: Accepted at EC3 2022
- **Journal**: None
- **Summary**: Crack detection plays a key role in automated pavement inspection. Although a large number of algorithms have been developed in recent years to further boost performance, there are still remaining challenges in practice, due to the complexity of pavement images. To further accelerate the development and identify the remaining challenges, this paper conducts a comparison study to evaluate the performance of the state of the art crack detection algorithms quantitatively and objectively. A more comprehensive annotated pavement crack dataset (NHA12D) that contains images with different viewpoints and pavements types is proposed. In the comparison study, crack detection algorithms were trained equally on the largest public crack dataset collected and evaluated on the proposed dataset (NHA12D). Overall, the U-Net model with VGG-16 as backbone has the best all-around performance, but models generally fail to distinguish cracks from concrete joints, leading to a high false-positive rate. It also found that detecting cracks from concrete pavement images still has huge room for improvement. Dataset for concrete pavement images is also missing in the literature. Future directions in this area include filling the gap for concrete pavement images and using domain adaptation techniques to enhance the detection results on unseen datasets.



### MIRST-DM: Multi-Instance RST with Drop-Max Layer for Robust Classification of Breast Cancer
- **Arxiv ID**: http://arxiv.org/abs/2205.01674v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01674v1)
- **Published**: 2022-05-02 20:25:26+00:00
- **Updated**: 2022-05-02 20:25:26+00:00
- **Authors**: Shoukun Sun, Min Xian, Aleksandar Vakanski, Hossny Ghanem
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Robust self-training (RST) can augment the adversarial robustness of image classification models without significantly sacrificing models' generalizability. However, RST and other state-of-the-art defense approaches failed to preserve the generalizability and reproduce their good adversarial robustness on small medical image sets. In this work, we propose the Multi-instance RST with a drop-max layer, namely MIRST-DM, which involves a sequence of iteratively generated adversarial instances during training to learn smoother decision boundaries on small datasets. The proposed drop-max layer eliminates unstable features and helps learn representations that are robust to image perturbations. The proposed approach was validated using a small breast ultrasound dataset with 1,190 images. The results demonstrate that the proposed approach achieves state-of-the-art adversarial robustness against three prevalent attacks.



### Deep Learning Framework for Real-time Fetal Brain Segmentation in MRI
- **Arxiv ID**: http://arxiv.org/abs/2205.01675v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01675v1)
- **Published**: 2022-05-02 20:43:14+00:00
- **Updated**: 2022-05-02 20:43:14+00:00
- **Authors**: Razieh Faghihpirayesh, Davood Karimi, Deniz Erdogmus, Ali Gholipour
- **Comment**: 11 pages, 5 figures, submitted to Medical Image Computing and
  Computer Assisted Intervention (MICCAI) Conference
- **Journal**: None
- **Summary**: Fetal brain segmentation is an important first step for slice-level motion correction and slice-to-volume reconstruction in fetal MRI. Fast and accurate segmentation of the fetal brain on fetal MRI is required to achieve real-time fetal head pose estimation and motion tracking for slice re-acquisition and steering. To address this critical unmet need, in this work we analyzed the speed-accuracy performance of a variety of deep neural network models, and devised a symbolically small convolutional neural network that combines spatial details at high resolution with context features extracted at lower resolutions. We used multiple branches with skip connections to maintain high accuracy while devising a parallel combination of convolution and pooling operations as an input downsampling module to further reduce inference time. We trained our model as well as eight alternative, state-of-the-art networks with manually-labeled fetal brain MRI slices and tested on two sets of normal and challenging test cases. Experimental results show that our network achieved the highest accuracy and lowest inference time among all of the compared state-of-the-art real-time segmentation methods. We achieved average Dice scores of 97.99\% and 84.04\% on the normal and challenging test sets, respectively, with an inference time of 3.36 milliseconds per image on an NVIDIA GeForce RTX 2080 Ti. Code, data, and the trained models are available at https://github.com/bchimagine/real_time_fetal_brain_segmentation.



### FundusQ-Net: a Regression Quality Assessment Deep Learning Algorithm for Fundus Images Quality Grading
- **Arxiv ID**: http://arxiv.org/abs/2205.01676v3
- **DOI**: 10.1016/j.cmpb.2023.107522
- **Categories**: **eess.IV**, cs.CV, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2205.01676v3)
- **Published**: 2022-05-02 21:01:34+00:00
- **Updated**: 2023-06-06 19:41:33+00:00
- **Authors**: Or Abramovich, Hadas Pizem, Jan Van Eijgen, Ilan Oren, Joshua Melamed, Ingeborg Stalmans, Eytan Z. Blumenthal, Joachim A. Behar
- **Comment**: 12 pages, 9 figures, published in Computer Methods and Programs in
  Biomedicine
- **Journal**: Computer Methods and Programs in Biomedicine, Volume 239,
  September 2023, 107522, ISSN 0169-2607
- **Summary**: Objective: Ophthalmological pathologies such as glaucoma, diabetic retinopathy and age-related macular degeneration are major causes of blindness and vision impairment. There is a need for novel decision support tools that can simplify and speed up the diagnosis of these pathologies. A key step in this process is to automatically estimate the quality of the fundus images to make sure these are interpretable by a human operator or a machine learning model. We present a novel fundus image quality scale and deep learning (DL) model that can estimate fundus image quality relative to this new scale.   Methods: A total of 1,245 images were graded for quality by two ophthalmologists within the range 1-10, with a resolution of 0.5. A DL regression model was trained for fundus image quality assessment. The architecture used was Inception-V3. The model was developed using a total of 89,947 images from 6 databases, of which 1,245 were labeled by the specialists and the remaining 88,702 images were used for pre-training and semi-supervised learning. The final DL model was evaluated on an internal test set (n=209) as well as an external test set (n=194).   Results: The final DL model, denoted FundusQ-Net, achieved a mean absolute error of 0.61 (0.54-0.68) on the internal test set. When evaluated as a binary classification model on the public DRIMDB database as an external test set the model obtained an accuracy of 99%.   Significance: the proposed algorithm provides a new robust tool for automated quality grading of fundus images.



### One Weird Trick to Improve Your Semi-Weakly Supervised Semantic Segmentation Model
- **Arxiv ID**: http://arxiv.org/abs/2205.01233v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01233v1)
- **Published**: 2022-05-02 21:46:41+00:00
- **Updated**: 2022-05-02 21:46:41+00:00
- **Authors**: Wonho Bae, Junhyug Noh, Milad Jalali Asadabadi, Danica J. Sutherland
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-weakly supervised semantic segmentation (SWSSS) aims to train a model to identify objects in images based on a small number of images with pixel-level labels, and many more images with only image-level labels. Most existing SWSSS algorithms extract pixel-level pseudo-labels from an image classifier - a very difficult task to do well, hence requiring complicated architectures and extensive hyperparameter tuning on fully-supervised validation sets. We propose a method called prediction filtering, which instead of extracting pseudo-labels, just uses the classifier as a classifier: it ignores any segmentation predictions from classes which the classifier is confident are not present. Adding this simple post-processing method to baselines gives results competitive with or better than prior SWSSS algorithms. Moreover, it is compatible with pseudo-label methods: adding prediction filtering to existing SWSSS algorithms further improves segmentation performance.



### A Performance-Consistent and Computation-Efficient CNN System for High-Quality Automated Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.01239v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01239v1)
- **Published**: 2022-05-02 22:10:36+00:00
- **Updated**: 2022-05-02 22:10:36+00:00
- **Authors**: Juncheng Tong, Chunyan Wang
- **Comment**: 10 pages, 4 figures, currently under review of IEEE transactions on
  medical imaging
- **Journal**: None
- **Summary**: The research on developing CNN-based fully-automated Brain-Tumor-Segmentation systems has been progressed rapidly. For the systems to be applicable in practice, a good The research on developing CNN-based fully-automated Brain-Tumor-Segmentation systems has been progressed rapidly. For the systems to be applicable in practice, a good processing quality and reliability are the must. Moreover, for wide applications of such systems, a minimization of computation complexity is desirable, which can also result in a minimization of randomness in computation and, consequently, a better performance consistency. To this end, the CNN in the proposed system has a unique structure with 2 distinguished characters. Firstly, the three paths of its feature extraction block are designed to extract, from the multi-modality input, comprehensive feature information of mono-modality, paired-modality and cross-modality data, respectively. Also, it has a particular three-branch classification block to identify the pixels of 4 classes. Each branch is trained separately so that the parameters are updated specifically with the corresponding ground truth data of a target tumor areas. The convolution layers of the system are custom-designed with specific purposes, resulting in a very simple config of 61,843 parameters in total. The proposed system is tested extensively with BraTS2018 and BraTS2019 datasets. The mean Dice scores, obtained from the ten experiments on BraTS2018 validation samples, are 0.787+0.003, 0.886+0.002, 0.801+0.007, for enhancing tumor, whole tumor and tumor core, respectively, and 0.751+0.007, 0.885+0.002, 0.776+0.004 on BraTS2019. The test results demonstrate that the proposed system is able to perform high-quality segmentation in a consistent manner. Furthermore, its extremely low computation complexity will facilitate its implementation/application in various environments.



