# Arxiv Papers in cs.CV on 2022-05-21
### Deep Learning for Omnidirectional Vision: A Survey and New Perspectives
- **Arxiv ID**: http://arxiv.org/abs/2205.10468v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2205.10468v2)
- **Published**: 2022-05-21 00:19:56+00:00
- **Updated**: 2022-05-24 08:49:08+00:00
- **Authors**: Hao Ai, Zidong Cao, Jinjing Zhu, Haotian Bai, Yucheng Chen, Lin Wang
- **Comment**: 31 pages
- **Journal**: None
- **Summary**: Omnidirectional image (ODI) data is captured with a 360x180 field-of-view, which is much wider than the pinhole cameras and contains richer spatial information than the conventional planar images. Accordingly, omnidirectional vision has attracted booming attention due to its more advantageous performance in numerous applications, such as autonomous driving and virtual reality. In recent years, the availability of customer-level 360 cameras has made omnidirectional vision more popular, and the advance of deep learning (DL) has significantly sparked its research and applications. This paper presents a systematic and comprehensive review and analysis of the recent progress in DL methods for omnidirectional vision. Our work covers four main contents: (i) An introduction to the principle of omnidirectional imaging, the convolution methods on the ODI, and datasets to highlight the differences and difficulties compared with the 2D planar image data; (ii) A structural and hierarchical taxonomy of the DL methods for omnidirectional vision; (iii) A summarization of the latest novel learning strategies and applications; (iv) An insightful discussion of the challenges and open problems by highlighting the potential research directions to trigger more research in the community.



### Masterful: A Training Platform for Computer Vision Models
- **Arxiv ID**: http://arxiv.org/abs/2205.10469v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.10469v1)
- **Published**: 2022-05-21 00:20:09+00:00
- **Updated**: 2022-05-21 00:20:09+00:00
- **Authors**: Samuel Wookey, Yaoshiang Ho, Tom Rikert, Juan David Gil Lopez, Juan Manuel Mu√±oz Beancur, Santiago Cortes, Ray Tawil, Aaron Sabin, Jack Lynch, Travis Harper, Nikhil Gajendrakumar
- **Comment**: None
- **Journal**: None
- **Summary**: Masterful is a software platform to train deep learning computer vision models. Data and model architecture are inputs to the platform, and the output is a trained model. The platform's primary goal is to maximize a trained model's accuracy, which it achieves through its regularization and semi-supervised learning implementations. The platform's secondary goal is to minimize the amount of manual experimentation typically required to tune training hyperparameters, which it achieves via multiple metalearning algorithms which are custom built to control the platform's regularization and semi-supervised learning implementations. The platform's tertiary goal is to minimize the computing resources required to train a model, which it achieves via another set of metalearning algorithms which are purpose built to control Tensorflow's optimization implementations. The platform builds on top of Tensorflow's data management, architecture, automatic differentiation, and optimization implementations.



### Semi-Supervised Subspace Clustering via Tensor Low-Rank Representation
- **Arxiv ID**: http://arxiv.org/abs/2205.10481v2
- **DOI**: 10.1109/TCSVT.2023.3234556
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.10481v2)
- **Published**: 2022-05-21 01:47:17+00:00
- **Updated**: 2023-01-04 08:48:28+00:00
- **Authors**: Yuheng Jia, Guanxing Lu, Hui Liu, Junhui Hou
- **Comment**: None
- **Journal**: None
- **Summary**: In this letter, we propose a novel semi-supervised subspace clustering method, which is able to simultaneously augment the initial supervisory information and construct a discriminative affinity matrix. By representing the limited amount of supervisory information as a pairwise constraint matrix, we observe that the ideal affinity matrix for clustering shares the same low-rank structure as the ideal pairwise constraint matrix. Thus, we stack the two matrices into a 3-D tensor, where a global low-rank constraint is imposed to promote the affinity matrix construction and augment the initial pairwise constraints synchronously. Besides, we use the local geometry structure of input samples to complement the global low-rank prior to achieve better affinity matrix learning. The proposed model is formulated as a Laplacian graph regularized convex low-rank tensor representation problem, which is further solved with an alternative iterative algorithm. In addition, we propose to refine the affinity matrix with the augmented pairwise constraints. Comprehensive experimental results on eight commonly-used benchmark datasets demonstrate the superiority of our method over state-of-the-art methods. The code is publicly available at https://github.com/GuanxingLu/Subspace-Clustering.



### Mapping Emulation for Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2205.10490v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2205.10490v1)
- **Published**: 2022-05-21 02:38:16+00:00
- **Updated**: 2022-05-21 02:38:16+00:00
- **Authors**: Jing Ma, Xiang Xiang, Zihan Zhang, Yuwen Tan, Yiming Wan, Zhigang Zeng, Dacheng Tao
- **Comment**: 11 pages, 5 figures, 3 lemmas, 3 corollaries, 1 algorithm
- **Journal**: None
- **Summary**: This paper formalizes the source-blind knowledge distillation problem that is essential to federated learning. A new geometric perspective is presented to view such a problem as aligning generated distributions between the teacher and student. With its guidance, a new architecture MEKD is proposed to emulate the inverse mapping through generative adversarial training. Unlike mimicking logits and aligning logit distributions, reconstructing the mapping from classifier-logits has a geometric intuition of decreasing empirical distances, and theoretical guarantees using the universal function approximation and optimal mass transportation theories. A new algorithm is also proposed to train the student model that reaches the teacher's performance source-blindly. On various benchmarks, MEKD outperforms existing source-blind KD methods, explainable with ablation studies and visualized results.



### Enriched Robust Multi-View Kernel Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/2205.10495v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.10495v1)
- **Published**: 2022-05-21 03:06:24+00:00
- **Updated**: 2022-05-21 03:06:24+00:00
- **Authors**: Mengyuan Zhang, Kai Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Subspace clustering is to find underlying low-dimensional subspaces and cluster the data points correctly. In this paper, we propose a novel multi-view subspace clustering method. Most existing methods suffer from two critical issues. First, they usually adopt a two-stage framework and isolate the processes of affinity learning, multi-view information fusion and clustering. Second, they assume the data lies in a linear subspace which may fail in practice as most real-world datasets may have non-linearity structures. To address the above issues, in this paper we propose a novel Enriched Robust Multi-View Kernel Subspace Clustering framework where the consensus affinity matrix is learned from both multi-view data and spectral clustering. Due to the objective and constraints which is difficult to optimize, we propose an iterative optimization method which is easy to implement and can yield closed solution in each step. Extensive experiments have validated the superiority of our method over state-of-the-art clustering methods.



### Reinforced Pedestrian Attribute Recognition with Group Optimization Reward
- **Arxiv ID**: http://arxiv.org/abs/2205.14042v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.14042v1)
- **Published**: 2022-05-21 03:38:03+00:00
- **Updated**: 2022-05-21 03:38:03+00:00
- **Authors**: Zhong Ji, Zhenfei Hu, Yaodong Wang, Shengjia Li
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian Attribute Recognition (PAR) is a challenging task in intelligent video surveillance. Two key challenges in PAR include complex alignment relations between images and attributes, and imbalanced data distribution. Existing approaches usually formulate PAR as a recognition task. Different from them, this paper addresses it as a decision-making task via a reinforcement learning framework. Specifically, PAR is formulated as a Markov decision process (MDP) by designing ingenious states, action space, reward function and state transition. To alleviate the inter-attribute imbalance problem, we apply an Attribute Grouping Strategy (AGS) by dividing all attributes into subgroups according to their region and category information. Then we employ an agent to recognize each group of attributes, which is trained with Deep Q-learning algorithm. We also propose a Group Optimization Reward (GOR) function to alleviate the intra-attribute imbalance problem. Experimental results on the three benchmark datasets of PETA, RAP and PA100K illustrate the effectiveness and competitiveness of the proposed approach and demonstrate that the application of reinforcement learning to PAR is a valuable research direction.



### Making Video Quality Assessment Models Sensitive to Frame Rate Distortions
- **Arxiv ID**: http://arxiv.org/abs/2205.10501v1
- **DOI**: 10.1109/LSP.2022.3162159
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.10501v1)
- **Published**: 2022-05-21 04:13:57+00:00
- **Updated**: 2022-05-21 04:13:57+00:00
- **Authors**: Pavan C. Madhusudana, Neil Birkbeck, Yilin Wang, Balu Adsumilli, Alan C. Bovik
- **Comment**: None
- **Journal**: IEEE Signal Processing Letters. 29 (2022) 897-901
- **Summary**: We consider the problem of capturing distortions arising from changes in frame rate as part of Video Quality Assessment (VQA). Variable frame rate (VFR) videos have become much more common, and streamed videos commonly range from 30 frames per second (fps) up to 120 fps. VFR-VQA offers unique challenges in terms of distortion types as well as in making non-uniform comparisons of reference and distorted videos having different frame rates. The majority of current VQA models require compared videos to be of the same frame rate, but are unable to adequately account for frame rate artifacts. The recently proposed Generalized Entropic Difference (GREED) VQA model succeeds at this task, using natural video statistics models of entropic differences of temporal band-pass coefficients, delivering superior performance on predicting video quality changes arising from frame rate distortions. Here we propose a simple fusion framework, whereby temporal features from GREED are combined with existing VQA models, towards improving model sensitivity towards frame rate distortions. We find through extensive experiments that this feature fusion significantly boosts model performance on both HFR/VFR datasets as well as fixed frame rate (FFR) VQA databases. Our results suggest that employing efficient temporal representations can result much more robust and accurate VQA models when frame rate variations can occur.



### A Study on Transformer Configuration and Training Objective
- **Arxiv ID**: http://arxiv.org/abs/2205.10505v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.10505v3)
- **Published**: 2022-05-21 05:17:11+00:00
- **Updated**: 2023-05-18 16:08:10+00:00
- **Authors**: Fuzhao Xue, Jianghai Chen, Aixin Sun, Xiaozhe Ren, Zangwei Zheng, Xiaoxin He, Yongming Chen, Xin Jiang, Yang You
- **Comment**: Accepted at ICML 2023
- **Journal**: None
- **Summary**: Transformer-based models have delivered impressive results on many tasks, particularly vision and language tasks. In many model training situations, conventional configurations are typically adopted. For example, we often set the base model with hidden dimensions (i.e. model width) to be 768 and the number of transformer layers (i.e. model depth) to be 12. In this paper, we revisit these conventional configurations. Through theoretical analysis and experimental evaluation, we show that the masked autoencoder is effective in alleviating the over-smoothing issue in deep transformer training. Based on this finding, we propose Bamboo, an idea of using deeper and narrower transformer configurations, for masked autoencoder training. On ImageNet, with such a simple change in configuration, re-designed model achieves 87.1% top-1 accuracy and outperforms SoTA models like MAE and BEiT. On language tasks, re-designed model outperforms BERT with default setting by 1.1 points on average, on GLUE datasets.



### Travel Time, Distance and Costs Optimization for Paratransit Operations using Graph Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2205.10507v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.10507v1)
- **Published**: 2022-05-21 05:27:45+00:00
- **Updated**: 2022-05-21 05:27:45+00:00
- **Authors**: Kelvin Kwakye, Younho Seong, Sun Yi
- **Comment**: None
- **Journal**: None
- **Summary**: The provision of paratransit services is one option to meet the transportation needs of Vulnerable Road Users (VRUs). Like any other means of transportation, paratransit has obstacles such as high operational costs and longer trip times. As a result, customers are dissatisfied, and paratransit operators have a low approval rating. Researchers have undertaken various studies over the years to better understand the travel behaviors of paratransit customers and how they are operated. According to the findings of these researches, paratransit operators confront the challenge of determining the optimal route for their trips in order to save travel time. Depending on the nature of the challenge, most research used different optimization techniques to solve these routing problems. As a result, the goal of this study is to use Graph Convolutional Neural Networks (GCNs) to assist paratransit operators in researching various operational scenarios in a strategic setting in order to optimize routing, minimize operating costs and minimize their users' travel time. The study was carried out by using a randomized simulated dataset to help determine the decision to make in terms of fleet composition and capacity under different situations. For the various scenarios investigated, the GCN assisted in determining the minimum optimal gap.



### Visualizing CoAtNet Predictions for Aiding Melanoma Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.10515v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.10515v1)
- **Published**: 2022-05-21 06:41:52+00:00
- **Updated**: 2022-05-21 06:41:52+00:00
- **Authors**: Daniel Kvak
- **Comment**: None
- **Journal**: None
- **Summary**: Melanoma is considered to be the most aggressive form of skin cancer. Due to the similar shape of malignant and benign cancerous lesions, doctors spend considerably more time when diagnosing these findings. At present, the evaluation of malignancy is performed primarily by invasive histological examination of the suspicious lesion. Developing an accurate classifier for early and efficient detection can minimize and monitor the harmful effects of skin cancer and increase patient survival rates. This paper proposes a multi-class classification task using the CoAtNet architecture, a hybrid model that combines the depthwise convolution matrix operation of traditional convolutional neural networks with the strengths of Transformer models and self-attention mechanics to achieve better generalization and capacity. The proposed multi-class classifier achieves an overall precision of 0.901, recall 0.895, and AP 0.923, indicating high performance compared to other state-of-the-art networks.



### PointVector: A Vector Representation In Point Cloud Analysis
- **Arxiv ID**: http://arxiv.org/abs/2205.10528v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.10528v3)
- **Published**: 2022-05-21 07:40:18+00:00
- **Updated**: 2023-03-28 13:18:22+00:00
- **Authors**: Xin Deng, WenYu Zhang, Qing Ding, XinMing Zhang
- **Comment**: Accepted by CVPR2023
- **Journal**: None
- **Summary**: In point cloud analysis, point-based methods have rapidly developed in recent years. These methods have recently focused on concise MLP structures, such as PointNeXt, which have demonstrated competitiveness with Convolutional and Transformer structures. However, standard MLPs are limited in their ability to extract local features effectively. To address this limitation, we propose a Vector-oriented Point Set Abstraction that can aggregate neighboring features through higher-dimensional vectors. To facilitate network optimization, we construct a transformation from scalar to vector using independent angles based on 3D vector rotations. Finally, we develop a PointVector model that follows the structure of PointNeXt. Our experimental results demonstrate that PointVector achieves state-of-the-art performance $\textbf{72.3\% mIOU}$ on the S3DIS Area 5 and $\textbf{78.4\% mIOU}$ on the S3DIS (6-fold cross-validation) with only $\textbf{58\%}$ model parameters of PointNeXt. We hope our work will help the exploration of concise and effective feature representations. The code will be released soon.



### Fine-Grained Visual Classification using Self Assessment Classifier
- **Arxiv ID**: http://arxiv.org/abs/2205.10529v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.10529v1)
- **Published**: 2022-05-21 07:41:27+00:00
- **Updated**: 2022-05-21 07:41:27+00:00
- **Authors**: Tuong Do, Huy Tran, Erman Tjiputra, Quang D. Tran, Anh Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: Extracting discriminative features plays a crucial role in the fine-grained visual classification task. Most of the existing methods focus on developing attention or augmentation mechanisms to achieve this goal. However, addressing the ambiguity in the top-k prediction classes is not fully investigated. In this paper, we introduce a Self Assessment Classifier, which simultaneously leverages the representation of the image and top-k prediction classes to reassess the classification results. Our method is inspired by continual learning with coarse-grained and fine-grained classifiers to increase the discrimination of features in the backbone and produce attention maps of informative areas on the image. In practice, our method works as an auxiliary branch and can be easily integrated into different architectures. We show that by effectively addressing the ambiguity in the top-k prediction classes, our method achieves new state-of-the-art results on CUB200-2011, Stanford Dog, and FGVC Aircraft datasets. Furthermore, our method also consistently improves the accuracy of different existing fine-grained classifiers with a unified setup.



### Knowledge Distillation from A Stronger Teacher
- **Arxiv ID**: http://arxiv.org/abs/2205.10536v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.10536v3)
- **Published**: 2022-05-21 08:30:58+00:00
- **Updated**: 2022-12-28 04:02:19+00:00
- **Authors**: Tao Huang, Shan You, Fei Wang, Chen Qian, Chang Xu
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: Unlike existing knowledge distillation methods focus on the baseline settings, where the teacher models and training strategies are not that strong and competing as state-of-the-art approaches, this paper presents a method dubbed DIST to distill better from a stronger teacher. We empirically find that the discrepancy of predictions between the student and a stronger teacher may tend to be fairly severer. As a result, the exact match of predictions in KL divergence would disturb the training and make existing methods perform poorly. In this paper, we show that simply preserving the relations between the predictions of teacher and student would suffice, and propose a correlation-based loss to capture the intrinsic inter-class relations from the teacher explicitly. Besides, considering that different instances have different semantic similarities to each class, we also extend this relational match to the intra-class level. Our method is simple yet practical, and extensive experiments demonstrate that it adapts well to various architectures, model sizes and training strategies, and can achieve state-of-the-art performance consistently on image classification, object detection, and semantic segmentation tasks. Code is available at: https://github.com/hunto/DIST_KD .



### On the Feasibility and Generality of Patch-based Adversarial Attacks on Semantic Segmentation Problems
- **Arxiv ID**: http://arxiv.org/abs/2205.10539v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.10539v1)
- **Published**: 2022-05-21 08:49:22+00:00
- **Updated**: 2022-05-21 08:49:22+00:00
- **Authors**: Soma Kontar, Andras Horvath
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks were applied with success in a myriad of applications, but in safety critical use cases adversarial attacks still pose a significant threat. These attacks were demonstrated on various classification and detection tasks and are usually considered general in a sense that arbitrary network outputs can be generated by them.   In this paper we will demonstrate through simple case studies both in simulation and in real-life, that patch based attacks can be utilised to alter the output of segmentation networks. Through a few examples and the investigation of network complexity, we will also demonstrate that the number of possible output maps which can be generated via patch-based attacks of a given size is typically smaller than the area they effect or areas which should be attacked in case of practical applications.   We will prove that based on these results most patch-based attacks cannot be general in practice, namely they can not generate arbitrary output maps or if they could, they are spatially limited and this limit is significantly smaller than the receptive field of the patches.



### Improvements to Self-Supervised Representation Learning for Masked Image Modeling
- **Arxiv ID**: http://arxiv.org/abs/2205.10546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.10546v1)
- **Published**: 2022-05-21 09:45:50+00:00
- **Updated**: 2022-05-21 09:45:50+00:00
- **Authors**: Jiawei Mao, Xuesong Yin, Yuanqi Chang, Honggu Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: This paper explores improvements to the masked image modeling (MIM) paradigm. The MIM paradigm enables the model to learn the main object features of the image by masking the input image and predicting the masked part by the unmasked part. We found the following three main directions for MIM to be improved. First, since both encoders and decoders contribute to representation learning, MIM uses only encoders for downstream tasks, which ignores the impact of decoders on representation learning. Although the MIM paradigm already employs small decoders with asymmetric structures, we believe that continued reduction of decoder parameters is beneficial to improve the representational learning capability of the encoder . Second, MIM solves the image prediction task by training the encoder and decoder together , and does not design a separate task for the encoder . To further enhance the performance of the encoder when performing downstream tasks, we designed the encoder for the tasks of comparative learning and token position prediction. Third, since the input image may contain background and other objects, and the proportion of each object in the image varies, reconstructing the tokens related to the background or to other objects is not meaningful for MIM to understand the main object representations. Therefore we use ContrastiveCrop to crop the input image so that the input image contains as much as possible only the main objects. Based on the above three improvements to MIM, we propose a new model, Contrastive Masked AutoEncoders (CMAE). We achieved a Top-1 accuracy of 65.84% on tinyimagenet using the ViT-B backbone, which is +2.89 outperforming the MAE of competing methods when all conditions are equal. Code will be made available.



### Three-Dimensional Segmentation of the Left Ventricle in Late Gadolinium Enhanced MR Images of Chronic Infarction Combining Long- and Short-Axis Information
- **Arxiv ID**: http://arxiv.org/abs/2205.10548v1
- **DOI**: 10.1016/j.media.2013.03.001
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2205.10548v1)
- **Published**: 2022-05-21 09:47:50+00:00
- **Updated**: 2022-05-21 09:47:50+00:00
- **Authors**: Dong Wei, Ying Sun, Sim-Heng Ong, Ping Chai, Lynette L. Teo, Adrian F. Low
- **Comment**: Medical Image Analysis, Volume 17, Issue 6, August 2013, Pages
  685-697
- **Journal**: None
- **Summary**: Automatic segmentation of the left ventricle (LV) in late gadolinium enhanced (LGE) cardiac MR (CMR) images is difficult due to the intensity heterogeneity arising from accumulation of contrast agent in infarcted myocardium. In this paper, we present a comprehensive framework for automatic 3D segmentation of the LV in LGE CMR images. Given myocardial contours in cine images as a priori knowledge, the framework initially propagates the a priori segmentation from cine to LGE images via 2D translational registration. Two meshes representing respectively endocardial and epicardial surfaces are then constructed with the propagated contours. After construction, the two meshes are deformed towards the myocardial edge points detected in both short-axis and long-axis LGE images in a unified 3D coordinate system. Taking into account the intensity characteristics of the LV in LGE images, we propose a novel parametric model of the LV for consistent myocardial edge points detection regardless of pathological status of the myocardium (infarcted or healthy) and of the type of the LGE images (short-axis or long-axis). We have evaluated the proposed framework with 21 sets of real patient and 4 sets of simulated phantom data. Both distance- and region-based performance metrics confirm the observation that the framework can generate accurate and reliable results for myocardial segmentation of LGE images. We have also tested the robustness of the framework with respect to varied a priori segmentation in both practical and simulated settings. Experimental results show that the proposed framework can greatly compensate variations in the given a priori knowledge and consistently produce accurate segmentations.



### Robot Person Following in Uniform Crowd Environment
- **Arxiv ID**: http://arxiv.org/abs/2205.10553v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.10553v1)
- **Published**: 2022-05-21 10:20:14+00:00
- **Updated**: 2022-05-21 10:20:14+00:00
- **Authors**: Adarsh Ghimire, Xiaoxiong Zhang, Sajid Javed, Jorge Dias, Naoufel Werghi
- **Comment**: None
- **Journal**: ICRA Workshop 2022: ROBOTIC PERCEPTION AND MAPPING: EMERGING
  TECHNIQUES
- **Summary**: Person-tracking robots have many applications, such as in security, elderly care, and socializing robots. Such a task is particularly challenging when the person is moving in a Uniform crowd. Also, despite significant progress of trackers reported in the literature, state-of-the-art trackers have hardly addressed person following in such scenarios. In this work, we focus on improving the perceptivity of a robot for a person following task by developing a robust and real-time applicable object tracker. We present a new robot person tracking system with a new RGB-D tracker, Deep Tracking with RGB-D (DTRD) that is resilient to tricky challenges introduced by the uniform crowd environment. Our tracker utilizes transformer encoder-decoder architecture with RGB and depth information to discriminate the target person from similar distractors. A substantial amount of comprehensive experiments and results demonstrate that our tracker has higher performance in two quantitative evaluation metrics and confirms its superiority over other SOTA trackers.



### Cycle-GAN for eye-tracking
- **Arxiv ID**: http://arxiv.org/abs/2205.10556v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.10556v1)
- **Published**: 2022-05-21 10:25:00+00:00
- **Updated**: 2022-05-21 10:25:00+00:00
- **Authors**: Ildar Rakhmatulin
- **Comment**: 9 pages, 11 figures
- **Journal**: None
- **Summary**: This manuscript presents a not typical implementation of the cycle generative adversarial networks (Cycle-GAN) method for eye-tracking tasks.



### Unsupervised Sign Language Phoneme Clustering using HamNoSys Notation
- **Arxiv ID**: http://arxiv.org/abs/2205.10560v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.10560v1)
- **Published**: 2022-05-21 10:49:45+00:00
- **Updated**: 2022-05-21 10:49:45+00:00
- **Authors**: Boris Mocialov, Graham Turner, Helen Hastie
- **Comment**: 11 pages, 10 figures, VarDial2020
- **Journal**: None
- **Summary**: Traditionally, sign language resources have been collected in controlled settings for specific tasks involving supervised sign classification or linguistic studies accompanied by specific annotation type. To date, very few who explored signing videos found online on social media platforms as well as the use of unsupervised methods applied to such resources. Due to the fact that the field is striving to achieve acceptable model performance on the data that differs from that seen during training calls for more diversity in sign language data, stepping away from the data obtained in controlled laboratory settings. Moreover, since the sign language data collection and annotation carries large overheads, it is desirable to accelerate the annotation process. Considering the aforementioned tendencies, this paper takes the side of harvesting online data in a pursuit for automatically generating and annotating sign language corpora through phoneme clustering.



### ADT-SSL: Adaptive Dual-Threshold for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.10571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.10571v1)
- **Published**: 2022-05-21 11:52:08+00:00
- **Updated**: 2022-05-21 11:52:08+00:00
- **Authors**: Zechen Liang, Yuan-Gen Wang, Wei Lu, Xiaochun Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-Supervised Learning (SSL) has advanced classification tasks by inputting both labeled and unlabeled data to train a model jointly. However, existing SSL methods only consider the unlabeled data whose predictions are beyond a fixed threshold (e.g., 0.95), ignoring the valuable information from those less than 0.95. We argue that these discarded data have a large proportion and are usually of hard samples, thereby benefiting the model training. This paper proposes an Adaptive Dual-Threshold method for Semi-Supervised Learning (ADT-SSL). Except for the fixed threshold, ADT extracts another class-adaptive threshold from the labeled data to take full advantage of the unlabeled data whose predictions are less than 0.95 but more than the extracted one. Accordingly, we engage CE and $L_2$ loss functions to learn from these two types of unlabeled data, respectively. For highly similar unlabeled data, we further design a novel similar loss to make the prediction of the model consistency. Extensive experiments are conducted on benchmark datasets, including CIFAR-10, CIFAR-100, and SVHN. Experimental results show that the proposed ADT-SSL achieves state-of-the-art classification accuracy.



### A Comprehensive 3-D Framework for Automatic Quantification of Late Gadolinium Enhanced Cardiac Magnetic Resonance Images
- **Arxiv ID**: http://arxiv.org/abs/2205.10572v1
- **DOI**: 10.1109/TBME.2013.2237907
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2205.10572v1)
- **Published**: 2022-05-21 11:54:39+00:00
- **Updated**: 2022-05-21 11:54:39+00:00
- **Authors**: Dong Wei, Ying Sun, Sim-Heng Ong, Ping Chai, Lynette L Teo, Adrian F Low
- **Comment**: IEEE Transactions on Biomedical Engineering ( Volume: 60, Issue: 6,
  June 2013)
- **Journal**: None
- **Summary**: Late gadolinium enhanced (LGE) cardiac magnetic resonance (CMR) can directly visualize nonviable myocardium with hyperenhanced intensities with respect to normal myocardium. For heart attack patients, it is crucial to facilitate the decision of appropriate therapy by analyzing and quantifying their LGE CMR images. To achieve accurate quantification, LGE CMR images need to be processed in two steps: segmentation of the myocardium followed by classification of infarcts within the segmented myocardium. However, automatic segmentation is difficult usually due to the intensity heterogeneity of the myocardium and intensity similarity between the infarcts and blood pool. Besides, the slices of an LGE CMR dataset often suffer from spatial and intensity distortions, causing further difficulties in segmentation and classification. In this paper, we present a comprehensive 3-D framework for automatic quantification of LGE CMR images. In this framework, myocardium is segmented with a novel method that deforms coupled endocardial and epicardial meshes and combines information in both short- and long-axis slices, while infarcts are classified with a graph-cut algorithm incorporating intensity and spatial information. Moreover, both spatial and intensity distortions are effectively corrected with specially designed countermeasures. Experiments with 20 sets of real patient data show visually good segmentation and classification results that are quantitatively in strong agreement with those manually obtained by experts.



### Multi-feature Co-learning for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2205.10578v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.10578v1)
- **Published**: 2022-05-21 12:15:26+00:00
- **Updated**: 2022-05-21 12:15:26+00:00
- **Authors**: Jiayu Lin, Yuan-Gen Wang, Wenzhi Tang, Aifeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Image inpainting has achieved great advances by simultaneously leveraging image structure and texture features. However, due to lack of effective multi-feature fusion techniques, existing image inpainting methods still show limited improvement. In this paper, we design a deep multi-feature co-learning network for image inpainting, which includes Soft-gating Dual Feature Fusion (SDFF) and Bilateral Propagation Feature Aggregation (BPFA) modules. To be specific, we first use two branches to learn structure features and texture features separately. Then the proposed SDFF module integrates structure features into texture features, and meanwhile uses texture features as an auxiliary in generating structure features. Such a co-learning strategy makes the structure and texture features more consistent. Next, the proposed BPFA module enhances the connection from local feature to overall consistency by co-learning contextual attention, channel-wise information and feature space, which can further refine the generated structures and textures. Finally, extensive experiments are performed on benchmark datasets, including CelebA, Places2, and Paris StreetView. Experimental results demonstrate the superiority of the proposed method over the state-of-the-art. The source codes are available at https://github.com/GZHU-DVL/MFCL-Inpainting.



### Boosting Camouflaged Object Detection with Dual-Task Interactive Transformer
- **Arxiv ID**: http://arxiv.org/abs/2205.10579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.10579v1)
- **Published**: 2022-05-21 12:15:53+00:00
- **Updated**: 2022-05-21 12:15:53+00:00
- **Authors**: Zhengyi Liu, Zhili Zhang, Wei Wu
- **Comment**: Accepted by ICPR2022
- **Journal**: ICPR2022
- **Summary**: Camouflaged object detection intends to discover the concealed objects hidden in the surroundings. Existing methods follow the bio-inspired framework, which first locates the object and second refines the boundary. We argue that the discovery of camouflaged objects depends on the recurrent search for the object and the boundary. The recurrent processing makes the human tired and helpless, but it is just the advantage of the transformer with global search ability. Therefore, a dual-task interactive transformer is proposed to detect both accurate position of the camouflaged object and its detailed boundary. The boundary feature is considered as Query to improve the camouflaged object detection, and meanwhile the object feature is considered as Query to improve the boundary detection. The camouflaged object detection and the boundary detection are fully interacted by multi-head self-attention. Besides, to obtain the initial object feature and boundary feature, transformer-based backbones are adopted to extract the foreground and background. The foreground is just object, while foreground minus background is considered as boundary. Here, the boundary feature can be obtained from blurry boundary region of the foreground and background. Supervised by the object, the background and the boundary ground truth, the proposed model achieves state-of-the-art performance in public datasets. https://github.com/liuzywen/COD



### A comprehensive survey on semantic facial attribute editing using generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/2205.10587v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.10587v1)
- **Published**: 2022-05-21 13:09:38+00:00
- **Updated**: 2022-05-21 13:09:38+00:00
- **Authors**: Ahmad Nickabadi, Maryam Saeedi Fard, Nastaran Moradzadeh Farid, Najmeh Mohammadbagheri
- **Comment**: None
- **Journal**: None
- **Summary**: Generating random photo-realistic images has experienced tremendous growth during the past few years due to the advances of the deep convolutional neural networks and generative models. Among different domains, face photos have received a great deal of attention and a large number of face generation and manipulation models have been proposed. Semantic facial attribute editing is the process of varying the values of one or more attributes of a face image while the other attributes of the image are not affected. The requested modifications are provided as an attribute vector or in the form of driving face image and the whole process is performed by the corresponding models. In this paper, we survey the recent works and advances in semantic facial attribute editing. We cover all related aspects of these models including the related definitions and concepts, architectures, loss functions, datasets, evaluation metrics, and applications. Based on their architectures, the state-of-the-art models are categorized and studied as encoder-decoder, image-to-image, and photo-guided models. The challenges and restrictions of the current state-of-the-art methods are discussed as well.



### Facing the Void: Overcoming Missing Data in Multi-View Imagery
- **Arxiv ID**: http://arxiv.org/abs/2205.10592v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.10592v1)
- **Published**: 2022-05-21 13:21:27+00:00
- **Updated**: 2022-05-21 13:21:27+00:00
- **Authors**: Gabriel Machado, Keiller Nogueira, Matheus Barros Pereira, Jefersson Alex dos Santos
- **Comment**: None
- **Journal**: None
- **Summary**: In some scenarios, a single input image may not be enough to allow the object classification. In those cases, it is crucial to explore the complementary information extracted from images presenting the same object from multiple perspectives (or views) in order to enhance the general scene understanding and, consequently, increase the performance. However, this task, commonly called multi-view image classification, has a major challenge: missing data. In this paper, we propose a novel technique for multi-view image classification robust to this problem. The proposed method, based on state-of-the-art deep learning-based approaches and metric learning, can be easily adapted and exploited in other applications and domains. A systematic evaluation of the proposed algorithm was conducted using two multi-view aerial-ground datasets with very distinct properties. Results show that the proposed algorithm provides improvements in multi-view image classification accuracy when compared to state-of-the-art methods. Code available at \url{https://github.com/Gabriellm2003/remote_sensing_missing_data}.



### Myocardial Segmentation of Late Gadolinium Enhanced MR Images by Propagation of Contours from Cine MR Images
- **Arxiv ID**: http://arxiv.org/abs/2205.10595v1
- **DOI**: 10.1007/978-3-642-23626-6_53
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.10595v1)
- **Published**: 2022-05-21 13:34:59+00:00
- **Updated**: 2022-05-21 13:34:59+00:00
- **Authors**: Dong Wei, Ying Sun, Ping Chai, Adrian Low, Sim Heng Ong
- **Comment**: MICCAI 2011
- **Journal**: None
- **Summary**: Automatic segmentation of myocardium in Late Gadolinium Enhanced (LGE) Cardiac MR (CMR) images is often difficult due to the intensity heterogeneity resulting from accumulation of contrast agent in infarcted areas. In this paper, we propose an automatic segmentation framework that fully utilizes shared information between corresponding cine and LGE images of a same patient. Given myocardial contours in cine CMR images, the proposed framework achieves accurate segmentation of LGE CMR images in a coarse-to-fine manner. Affine registration is first performed between the corresponding cine and LGE image pair, followed by nonrigid registration, and finally local deformation of myocardial contours driven by forces derived from local features of the LGE image. Experimental results on real patient data with expert outlined ground truth show that the proposed framework can generate accurate and reliable results for myocardial segmentation of LGE CMR images.



### Brain Cortical Functional Gradients Predict Cortical Folding Patterns via Attention Mesh Convolution
- **Arxiv ID**: http://arxiv.org/abs/2205.10605v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.10605v1)
- **Published**: 2022-05-21 14:08:53+00:00
- **Updated**: 2022-05-21 14:08:53+00:00
- **Authors**: Li Yang, Zhibin He, Changhe Li, Junwei Han, Dajiang Zhu, Tianming Liu, Tuo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Since gyri and sulci, two basic anatomical building blocks of cortical folding patterns, were suggested to bear different functional roles, a precise mapping from brain function to gyro-sulcal patterns can provide profound insights into both biological and artificial neural networks. However, there lacks a generic theory and effective computational model so far, due to the highly nonlinear relation between them, huge inter-individual variabilities and a sophisticated description of brain function regions/networks distribution as mosaics, such that spatial patterning of them has not been considered. we adopted brain functional gradients derived from resting-state fMRI to embed the "gradual" change of functional connectivity patterns, and developed a novel attention mesh convolution model to predict cortical gyro-sulcal segmentation maps on individual brains. The convolution on mesh considers the spatial organization of functional gradients and folding patterns on a cortical sheet and the newly designed channel attention block enhances the interpretability of the contribution of different functional gradients to cortical folding prediction. Experiments show that the prediction performance via our model outperforms other state-of-the-art models. In addition, we found that the dominant functional gradients contribute less to folding prediction. On the activation maps of the last layer, some well-studied cortical landmarks are found on the borders of, rather than within, the highly activated regions. These results and findings suggest that a specifically designed artificial neural network can improve the precision of the mapping between brain functions and cortical folding patterns, and can provide valuable insight of brain anatomy-function relation for neuroscience.



### Lightweight Human Pose Estimation Using Heatmap-Weighting Loss
- **Arxiv ID**: http://arxiv.org/abs/2205.10611v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.10611v1)
- **Published**: 2022-05-21 14:26:14+00:00
- **Updated**: 2022-05-21 14:26:14+00:00
- **Authors**: Shiqi Li, Xiang Xiang
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: Recent research on human pose estimation exploits complex structures to improve performance on benchmark datasets, ignoring the resource overhead and inference speed when the model is actually deployed. In this paper, we lighten the computation cost and parameters of the deconvolution head network in SimpleBaseline and introduce an attention mechanism that utilizes original, inter-level, and intra-level information to intensify the accuracy. Additionally, we propose a novel loss function called heatmap weighting loss, which generates weights for each pixel on the heatmap that makes the model more focused on keypoints. Experiments demonstrate our method achieves a balance between performance, resource volume, and inference speed. Specifically, our method can achieve 65.3 AP score on COCO test-dev, while the inference speed is 55 FPS and 18 FPS on the mobile GPU and CPU, respectively.



### Gradient Concealment: Free Lunch for Defending Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2205.10617v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.10617v1)
- **Published**: 2022-05-21 15:02:56+00:00
- **Updated**: 2022-05-21 15:02:56+00:00
- **Authors**: Sen Pei, Jiaxi Sun, Xiaopeng Zhang, Gaofeng Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies show that the deep neural networks (DNNs) have achieved great success in various tasks. However, even the \emph{state-of-the-art} deep learning based classifiers are extremely vulnerable to adversarial examples, resulting in sharp decay of discrimination accuracy in the presence of enormous unknown attacks. Given the fact that neural networks are widely used in the open world scenario which can be safety-critical situations, mitigating the adversarial effects of deep learning methods has become an urgent need. Generally, conventional DNNs can be attacked with a dramatically high success rate since their gradient is exposed thoroughly in the white-box scenario, making it effortless to ruin a well trained classifier with only imperceptible perturbations in the raw data space. For tackling this problem, we propose a plug-and-play layer that is training-free, termed as \textbf{G}radient \textbf{C}oncealment \textbf{M}odule (GCM), concealing the vulnerable direction of gradient while guaranteeing the classification accuracy during the inference time. GCM reports superior defense results on the ImageNet classification benchmark, improving up to 63.41\% top-1 attack robustness (AR) when faced with adversarial inputs compared to the vanilla DNNs. Moreover, we use GCM in the CVPR 2022 Robust Classification Challenge, currently achieving \textbf{2nd} place in Phase II with only a tiny version of ConvNext. The code will be made available.



### A Pilot Study of Relating MYCN-Gene Amplification with Neuroblastoma-Patient CT Scans
- **Arxiv ID**: http://arxiv.org/abs/2205.10619v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.10619v1)
- **Published**: 2022-05-21 15:14:24+00:00
- **Updated**: 2022-05-21 15:14:24+00:00
- **Authors**: Zihan Zhang, Xiang Xiang, Xuehua Peng, Jianbo Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Neuroblastoma is one of the most common cancers in infants, and the initial diagnosis of this disease is difficult. At present, the MYCN gene amplification (MNA) status is detected by invasive pathological examination of tumor samples. This is time-consuming and may have a hidden impact on children. To handle this problem, we adopt multiple machine learning (ML) algorithms to predict the presence or absence of MYCN gene amplification. The dataset is composed of retrospective CT images of 23 neuroblastoma patients. Different from previous work, we develop the algorithm without manually-segmented primary tumors which is time-consuming and not practical. Instead, we only need the coordinate of the center point and the number of tumor slices given by a subspecialty-trained pediatric radiologist. Specifically, CNN-based method uses pre-trained convolutional neural network, and radiomics-based method extracts radiomics features. Our results show that CNN-based method outperforms the radiomics-based method.



### Exploring Concept Contribution Spatially: Hidden Layer Interpretation with Spatial Activation Concept Vector
- **Arxiv ID**: http://arxiv.org/abs/2205.11511v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11511v1)
- **Published**: 2022-05-21 15:58:57+00:00
- **Updated**: 2022-05-21 15:58:57+00:00
- **Authors**: Andong Wang, Wei-Ning Lee
- **Comment**: Accepted by CVPR 2022 Workshop XAI4CV
- **Journal**: None
- **Summary**: To interpret deep learning models, one mainstream is to explore the learned concepts by networks. Testing with Concept Activation Vector (TCAV) presents a powerful tool to quantify the contribution of query concepts (represented by user-defined guidance images) to a target class. For example, we can quantitatively evaluate whether and to what extent concept striped contributes to model prediction zebra with TCAV. Therefore, TCAV whitens the reasoning process of deep networks. And it has been applied to solve practical problems such as diagnosis. However, for some images where the target object only occupies a small fraction of the region, TCAV evaluation may be interfered with by redundant background features because TCAV calculates concept contribution to a target class based on a whole hidden layer. To tackle this problem, based on TCAV, we propose Spatial Activation Concept Vector (SACV) which identifies the relevant spatial locations to the query concept while evaluating their contributions to the model prediction of the target class. Experiment shows that SACV generates a more fine-grained explanation map for a hidden layer and quantifies concepts' contributions spatially. Moreover, it avoids interference from background features. The code is available on https://github.com/AntonotnaWang/Spatial-Activation-Concept-Vector.



### AutoLink: Self-supervised Learning of Human Skeletons and Object Outlines by Linking Keypoints
- **Arxiv ID**: http://arxiv.org/abs/2205.10636v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.10636v6)
- **Published**: 2022-05-21 16:32:34+00:00
- **Updated**: 2023-03-23 18:31:48+00:00
- **Authors**: Xingzhe He, Bastian Wandt, Helge Rhodin
- **Comment**: NeurIPS 2022
- **Journal**: Advances in Neural Information Processing Systems 2022
- **Summary**: Structured representations such as keypoints are widely used in pose transfer, conditional image generation, animation, and 3D reconstruction. However, their supervised learning requires expensive annotation for each target domain. We propose a self-supervised method that learns to disentangle object structure from the appearance with a graph of 2D keypoints linked by straight edges. Both the keypoint location and their pairwise edge weights are learned, given only a collection of images depicting the same object class. The resulting graph is interpretable, for example, AutoLink recovers the human skeleton topology when applied to images showing people. Our key ingredients are i) an encoder that predicts keypoint locations in an input image, ii) a shared graph as a latent variable that links the same pairs of keypoints in every image, iii) an intermediate edge map that combines the latent graph edge weights and keypoint locations in a soft, differentiable manner, and iv) an inpainting objective on randomly masked images. Although simpler, AutoLink outperforms existing self-supervised methods on the established keypoint and pose estimation benchmarks and paves the way for structure-conditioned generative models on more diverse datasets. Project website: https://xingzhehe.github.io/autolink/.



### Transformer-based out-of-distribution detection for clinically safe segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.10650v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.10650v2)
- **Published**: 2022-05-21 17:55:09+00:00
- **Updated**: 2023-05-17 21:38:23+00:00
- **Authors**: Mark S Graham, Petru-Daniel Tudosiu, Paul Wright, Walter Hugo Lopez Pinaya, U Jean-Marie, Yee Mah, James Teo, Rolf H J√§ger, David Werring, Parashkev Nachev, Sebastien Ourselin, M Jorge Cardoso
- **Comment**: Accepted at MIDL 2022 (Oral)
- **Journal**: None
- **Summary**: In a clinical setting it is essential that deployed image processing systems are robust to the full range of inputs they might encounter and, in particular, do not make confidently wrong predictions. The most popular approach to safe processing is to train networks that can provide a measure of their uncertainty, but these tend to fail for inputs that are far outside the training data distribution. Recently, generative modelling approaches have been proposed as an alternative; these can quantify the likelihood of a data sample explicitly, filtering out any out-of-distribution (OOD) samples before further processing is performed. In this work, we focus on image segmentation and evaluate several approaches to network uncertainty in the far-OOD and near-OOD cases for the task of segmenting haemorrhages in head CTs. We find all of these approaches are unsuitable for safe segmentation as they provide confidently wrong predictions when operating OOD. We propose performing full 3D OOD detection using a VQ-GAN to provide a compressed latent representation of the image and a transformer to estimate the data likelihood. Our approach successfully identifies images in both the far- and near-OOD cases. We find a strong relationship between image likelihood and the quality of a model's segmentation, making this approach viable for filtering images unsuitable for segmentation. To our knowledge, this is the first time transformers have been applied to perform OOD detection on 3D image data. Code is available at github.com/marksgraham/transformer-ood.



### Towards real-time and energy efficient Siamese tracking -- a hardware-software approach
- **Arxiv ID**: http://arxiv.org/abs/2205.10653v1
- **DOI**: 10.1007/978-3-031-12748-9_13
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.10653v1)
- **Published**: 2022-05-21 18:31:07+00:00
- **Updated**: 2022-05-21 18:31:07+00:00
- **Authors**: Dominika Przewlocka-Rus, Tomasz Kryjak
- **Comment**: Accepted for DASIP 2022 workshop
- **Journal**: None
- **Summary**: Siamese trackers have been among the state-of-the-art solutions in each Visual Object Tracking (VOT) challenge over the past few years. However, with great accuracy comes great computational complexity: to achieve real-time processing, these trackers have to be massively parallelised and are usually run on high-end GPUs. Easy to implement, this approach is energy consuming, and thus cannot be used in many low-power applications. To overcome this, one can use energy-efficient embedded devices, such as heterogeneous platforms joining the ARM processor system with programmable logic (FPGA). In this work, we propose a hardware-software implementation of the well-known fully connected Siamese tracker (SiamFC). We have developed a quantised Siamese network for the FINN accelerator, using algorithm-accelerator co-design, and performed design space exploration to achieve the best efficiency-to-energy ratio (determined by FPS and used resources). For our network, running in the programmable logic part of the Zynq UltraScale+ MPSoC ZCU104, we achieved the processing of almost 50 frames-per-second with tracker accuracy on par with its floating point counterpart, as well as the original SiamFC network. The complete tracking system, implemented in ARM with the network accelerated on FPGA, achieves up to 17 fps. These results bring us towards bridging the gap between the highly accurate but energy-demanding algorithms and energy-efficient solutions ready to be used in low-power, edge systems.



### Swept-Angle Synthetic Wavelength Interferometry
- **Arxiv ID**: http://arxiv.org/abs/2205.10655v4
- **DOI**: None
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2205.10655v4)
- **Published**: 2022-05-21 18:38:05+00:00
- **Updated**: 2023-03-29 19:06:36+00:00
- **Authors**: Alankar Kotwal, Anat Levin, Ioannis Gkioulekas
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new imaging technique, swept-angle synthetic wavelength interferometry, for full-field micron-scale 3D sensing. As in conventional synthetic wavelength interferometry, our technique uses light consisting of two narrowly-separated optical wavelengths, resulting in per-pixel interferometric measurements whose phase encodes scene depth. Our technique additionally uses a new type of light source that, by emulating spatially-incoherent illumination, makes interferometric measurements insensitive to aberrations and (sub)surface scattering, effects that corrupt phase measurements. The resulting technique combines the robustness to such corruptions of scanning interferometric setups, with the speed of full-field interferometric setups. Overall, our technique can recover full-frame depth at a lateral and axial resolution of 5 microns, at frame rates of 5 Hz, even under strong ambient light. We build an experimental prototype, and use it to demonstrate these capabilities by scanning a variety of objects, including objects representative of applications in inspection and fabrication, and objects that contain challenging light scattering effects.



### Vision Transformers in 2022: An Update on Tiny ImageNet
- **Arxiv ID**: http://arxiv.org/abs/2205.10660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.10660v1)
- **Published**: 2022-05-21 19:48:28+00:00
- **Updated**: 2022-05-21 19:48:28+00:00
- **Authors**: Ethan Huynh
- **Comment**: None
- **Journal**: None
- **Summary**: The recent advances in image transformers have shown impressive results and have largely closed the gap between traditional CNN architectures. The standard procedure is to train on large datasets like ImageNet-21k and then finetune on ImageNet-1k. After finetuning, researches will often consider the transfer learning performance on smaller datasets such as CIFAR-10/100 but have left out Tiny ImageNet. This paper offers an update on vision transformers' performance on Tiny ImageNet. I include Vision Transformer (ViT) , Data Efficient Image Transformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin Transformers. In addition, Swin Transformers beats the current state-of-the-art result with a validation accuracy of 91.35%. Code is available here: https://github.com/ehuynh1106/TinyImageNet-Transformers



### Equivariant Mesh Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.10662v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2205.10662v2)
- **Published**: 2022-05-21 19:53:14+00:00
- **Updated**: 2022-08-27 16:43:35+00:00
- **Authors**: Sourya Basu, Jose Gallego-Posada, Francesco Vigan√≤, James Rowbottom, Taco Cohen
- **Comment**: Published in Transactions on Machine Learning Research (08/2022).
  Official code made available at https://github.com/gallego-posada/eman - For
  the OpenReview entry, see https://openreview.net/forum?id=3IqqJh2Ycy
- **Journal**: None
- **Summary**: Equivariance to symmetries has proven to be a powerful inductive bias in deep learning research. Recent works on mesh processing have concentrated on various kinds of natural symmetries, including translations, rotations, scaling, node permutations, and gauge transformations. To date, no existing architecture is equivariant to all of these transformations. In this paper, we present an attention-based architecture for mesh data that is provably equivariant to all transformations mentioned above. Our pipeline relies on the use of relative tangential features: a simple, effective, equivariance-friendly alternative to raw node positions as inputs. Experiments on the FAUST and TOSCA datasets confirm that our proposed architecture achieves improved performance on these benchmarks and is indeed equivariant, and therefore robust, to a wide variety of local/global transformations.



### Transformer based Generative Adversarial Network for Liver Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.10663v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.10663v2)
- **Published**: 2022-05-21 19:55:43+00:00
- **Updated**: 2022-05-28 15:22:14+00:00
- **Authors**: Ugur Demir, Zheyuan Zhang, Bin Wang, Matthew Antalek, Elif Keles, Debesh Jha, Amir Borhani, Daniela Ladner, Ulas Bagci
- **Comment**: None
- **Journal**: ICPAI 2021
- **Summary**: Automated liver segmentation from radiology scans (CT, MRI) can improve surgery and therapy planning and follow-up assessment in addition to conventional use for diagnosis and prognosis. Although convolutional neural networks (CNNs) have become the standard image segmentation tasks, more recently this has started to change towards Transformers based architectures because Transformers are taking advantage of capturing long range dependence modeling capability in signals, so called attention mechanism. In this study, we propose a new segmentation approach using a hybrid approach combining the Transformer(s) with the Generative Adversarial Network (GAN) approach. The premise behind this choice is that the self-attention mechanism of the Transformers allows the network to aggregate the high dimensional feature and provide global information modeling. This mechanism provides better segmentation performance compared with traditional methods. Furthermore, we encode this generator into the GAN based architecture so that the discriminator network in the GAN can classify the credibility of the generated segmentation masks compared with the real masks coming from human (expert) annotations. This allows us to extract the high dimensional topology information in the mask for biomedical image segmentation and provide more reliable segmentation results. Our model achieved a high dice coefficient of 0.9433, recall of 0.9515, and precision of 0.9376 and outperformed other Transformer based approaches.



### Individual Topology Structure of Eye Movement Trajectories
- **Arxiv ID**: http://arxiv.org/abs/2205.10667v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.10667v5)
- **Published**: 2022-05-21 20:30:45+00:00
- **Updated**: 2022-07-23 12:10:46+00:00
- **Authors**: Arsenii A. Onuchin, Oleg N. Kachan
- **Comment**: None
- **Journal**: None
- **Summary**: Traditionally, extracting patterns from eye movement data relies on statistics of different macro-events such as fixations and saccades. This requires an additional preprocessing step to separate the eye movement subtypes, often with a number of parameters on which the classification results depend. Besides that, definitions of such macro events are formulated in different ways by different researchers.   We propose an application of a new class of features to the quantitative analysis of personal eye movement trajectories structure. This new class of features based on algebraic topology allows extracting patterns from different modalities of gaze such as time series of coordinates and amplitudes, heatmaps, and point clouds in a unified way at all scales from micro to macro. We experimentally demonstrate the competitiveness of the new class of features with the traditional ones and their significant synergy while being used together for the person authentication task on the recently published eye movement trajectories dataset.



### Scalable and Efficient Training of Large Convolutional Neural Networks with Differential Privacy
- **Arxiv ID**: http://arxiv.org/abs/2205.10683v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CC, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.10683v4)
- **Published**: 2022-05-21 22:01:12+00:00
- **Updated**: 2022-11-29 21:57:10+00:00
- **Authors**: Zhiqi Bu, Jialin Mao, Shiyun Xu
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: Large convolutional neural networks (CNN) can be difficult to train in the differentially private (DP) regime, since the optimization algorithms require a computationally expensive operation, known as the per-sample gradient clipping. We propose an efficient and scalable implementation of this clipping on convolutional layers, termed as the mixed ghost clipping, that significantly eases the private training in terms of both time and space complexities, without affecting the accuracy. The improvement in efficiency is rigorously studied through the first complexity analysis for the mixed ghost clipping and existing DP training algorithms.   Extensive experiments on vision classification tasks, with large ResNet, VGG, and Vision Transformers, demonstrate that DP training with mixed ghost clipping adds $1\sim 10\%$ memory overhead and $<2\times$ slowdown to the standard non-private training. Specifically, when training VGG19 on CIFAR10, the mixed ghost clipping is $3\times$ faster than state-of-the-art Opacus library with $18\times$ larger maximum batch size. To emphasize the significance of efficient DP training on convolutional layers, we achieve 96.7\% accuracy on CIFAR10 and 83.0\% on CIFAR100 at $\epsilon=1$ using BEiT, while the previous best results are 94.8\% and 67.4\%, respectively. We open-source a privacy engine (\url{https://github.com/woodyx218/private_vision}) that implements DP training of CNN with a few lines of code.



### Producing Histopathology Phantom Images using Generative Adversarial Networks to improve Tumor Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.10691v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.10691v1)
- **Published**: 2022-05-21 23:04:20+00:00
- **Updated**: 2022-05-21 23:04:20+00:00
- **Authors**: Vidit Gautam
- **Comment**: None
- **Journal**: None
- **Summary**: Advance in medical imaging is an important part in deep learning research. One of the goals of computer vision is development of a holistic, comprehensive model which can identify tumors from histology slides obtained via biopsies. A major problem that stands in the way is lack of data for a few cancer-types. In this paper, we ascertain that data augmentation using GANs can be a viable solution to reduce the unevenness in the distribution of different cancer types in our dataset. Our demonstration showed that a dataset augmented to a 50% increase causes an increase in tumor detection from 80% to 87.5%



