# Arxiv Papers in cs.CV on 2022-05-18
### Learning Monocular Depth Estimation via Selective Distillation of Stereo Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2205.08668v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08668v1)
- **Published**: 2022-05-18 00:34:28+00:00
- **Updated**: 2022-05-18 00:34:28+00:00
- **Authors**: Kyeongseob Song, Kuk-Jin Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular depth estimation has been extensively explored based on deep learning, yet its accuracy and generalization ability still lag far behind the stereo-based methods. To tackle this, a few recent studies have proposed to supervise the monocular depth estimation network by distilling disparity maps as proxy ground-truths. However, these studies naively distill the stereo knowledge without considering the comparative advantages of stereo-based and monocular depth estimation methods. In this paper, we propose to selectively distill the disparity maps for more reliable proxy supervision. Specifically, we first design a decoder (MaskDecoder) that learns two binary masks which are trained to choose optimally between the proxy disparity maps and the estimated depth maps for each pixel. The learned masks are then fed to another decoder (DepthDecoder) to enforce the estimated depths to learn from only the masked area in the proxy disparity maps. Additionally, a Teacher-Student module is designed to transfer the geometric knowledge of the StereoNet to the MonoNet. Extensive experiments validate our methods achieve state-of-the-art performance for self- and proxy-supervised monocular depth estimation on the KITTI dataset, even surpassing some of the semi-supervised methods.



### K-textures, a self-supervised hard clustering deep learning algorithm for satellite image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.08671v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/2205.08671v2)
- **Published**: 2022-05-18 00:51:53+00:00
- **Updated**: 2022-05-27 20:38:36+00:00
- **Authors**: Fabien H. Wagner, Ricardo Dalagnol, Alber H. SÃ¡nchez, Mayumi C. M. Hirye, Samuel Favrichon, Jake H. Lee, Steffen Mauceri, Yan Yang, Sassan Saatchi
- **Comment**: 19 pages, 10 figures, submitted to Frontiers in Environmental
  Science, section Environmental Informatics and Remote Sensing, Research
  Topic: Advances in Machine Learning and Deep Learning for Monitoring
  Terrestrial Ecosystems
- **Journal**: None
- **Summary**: Deep learning self-supervised algorithms that can segment an image in a fixed number of hard labels such as the k-means algorithm and relying only on deep learning techniques are still lacking. Here, we introduce the k-textures algorithm which provides self-supervised segmentation of a 4-band image (RGB-NIR) for a $k$ number of classes. An example of its application on high resolution Planet satellite imagery is given. Our algorithm shows that discrete search is feasible using convolutional neural networks (CNN) and gradient descent. The model detects $k$ hard clustering classes represented in the model as $k$ discrete binary masks and their associated $k$ independently generated textures, that combined are a simulation of the original image. The similarity loss is the mean squared error between the features of the original and the simulated image, both extracted from the penultimate convolutional block of Keras 'imagenet' pretrained VGG-16 model and a custom feature extractor made with Planet data. The main advances of the k-textures model are: first, the $k$ discrete binary masks are obtained inside the model using gradient descent. The model allows for the generation of discrete binary masks using a novel method using a hard sigmoid activation function. Second, it provides hard clustering classes -- each pixels has only one class. Finally, in comparison to k-means, where each pixel is considered independently, here, contextual information is also considered and each class is not associated only to similar values in the color channels but also to a texture. Our approach is designed to ease the production of training samples for satellite image segmentation and the k-textures architecture could be adapted to support different number of bands and for more complex tasks, such as object self-segmentation. The model codes and weights are available at https://doi.org/10.5281/zenodo.6359859



### Deep learning on rail profiles matching
- **Arxiv ID**: http://arxiv.org/abs/2205.08687v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08687v2)
- **Published**: 2022-05-18 01:50:54+00:00
- **Updated**: 2022-07-29 14:52:18+00:00
- **Authors**: Kunqi Wang, Daolin Si, Pu Wang, Jing Ge, Peiyuan Ni, Shuguo Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Matching the rail cross-section profiles measured on site with the designed profile is a must to evaluate the wear of the rail, which is very important for track maintenance and rail safety. So far, the measured rail profiles to be matched usually have four features, that is, large amount of data, diverse section shapes, hardware made errors, and human experience needs to be introduced to solve the complex situation on site during matching process. However, traditional matching methods based on feature points or feature lines could no longer meet the requirements. To this end, we first establish the rail profiles matching dataset composed of 46386 pairs of professional manual matched data, then propose a general high-precision method for rail profiles matching using pre-trained convolutional neural network (CNN). This new method based on deep learning is promising to be the dominant approach for this issue. Source code is at https://github.com/Kunqi1994/Deep-learning-on-rail-profile-matching.



### Hyperparameter Optimization with Neural Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/2205.08695v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.08695v1)
- **Published**: 2022-05-18 02:51:47+00:00
- **Updated**: 2022-05-18 02:51:47+00:00
- **Authors**: Kangil Lee, Junho Yim
- **Comment**: 17 pages, 3 figures
- **Journal**: None
- **Summary**: Since the deep learning model is highly dependent on hyperparameters, hyperparameter optimization is essential in developing deep learning model-based applications, even if it takes a long time. As service development using deep learning models has gradually become competitive, many developers highly demand rapid hyperparameter optimization algorithms. In order to keep pace with the needs of faster hyperparameter optimization algorithms, researchers are focusing on improving the speed of hyperparameter optimization algorithm. However, the huge time consumption of hyperparameter optimization due to the high computational cost of the deep learning model itself has not been dealt with in-depth. Like using surrogate model in Bayesian optimization, to solve this problem, it is necessary to consider proxy model for a neural network (N_B) to be used for hyperparameter optimization. Inspired by the main goal of neural network pruning, i.e., high computational cost reduction and performance preservation, we presumed that the neural network (N_P) obtained through neural network pruning would be a good proxy model of N_B. In order to verify our idea, we performed extensive experiments by using CIFAR10, CFIAR100, and TinyImageNet datasets and three generally-used neural networks and three representative hyperparameter optmization methods. Through these experiments, we verified that N_P can be a good proxy model of N_B for rapid hyperparameter optimization. The proposed hyperparameter optimization framework can reduce the amount of time up to 37%.



### SemiCurv: Semi-Supervised Curvilinear Structure Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.08706v2
- **DOI**: 10.1109/TIP.2022.3189823
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08706v2)
- **Published**: 2022-05-18 03:52:17+00:00
- **Updated**: 2022-05-19 05:48:42+00:00
- **Authors**: Xun Xu, Manh Cuong Nguyen, Yasin Yazici, Kangkang Lu, Hlaing Min, Chuan-Sheng Foo
- **Comment**: IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Recent work on curvilinear structure segmentation has mostly focused on backbone network design and loss engineering. The challenge of collecting labelled data, an expensive and labor intensive process, has been overlooked. While labelled data is expensive to obtain, unlabelled data is often readily available. In this work, we propose SemiCurv, a semi-supervised learning (SSL) framework for curvilinear structure segmentation that is able to utilize such unlabelled data to reduce the labelling burden. Our framework addresses two key challenges in formulating curvilinear segmentation in a semi-supervised manner. First, to fully exploit the power of consistency based SSL, we introduce a geometric transformation as strong data augmentation and then align segmentation predictions via a differentiable inverse transformation to enable the computation of pixel-wise consistency. Second, the traditional mean square error (MSE) on unlabelled data is prone to collapsed predictions and this issue exacerbates with severe class imbalance (significantly more background pixels). We propose a N-pair consistency loss to avoid trivial predictions on unlabelled data. We evaluate SemiCurv on six curvilinear segmentation datasets, and find that with no more than 5% of the labelled data, it achieves close to 95% of the performance relative to its fully supervised counterpart.



### It Isn't Sh!tposting, It's My CAT Posting
- **Arxiv ID**: http://arxiv.org/abs/2205.08710v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.08710v1)
- **Published**: 2022-05-18 04:11:55+00:00
- **Updated**: 2022-05-18 04:11:55+00:00
- **Authors**: Parthsarthi Rawat, Sayan Das, Jorge Aguirre, Akhil Daphara
- **Comment**: 5 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: In this paper, we describe a novel architecture which can generate hilarious captions for a given input image. The architecture is split into two halves, i.e. image captioning and hilarious text conversion. The architecture starts with a pre-trained CNN model, VGG16 in this implementation, and applies attention LSTM on it to generate normal caption. These normal captions then are fed forward to our hilarious text conversion transformer which converts this text into something hilarious while maintaining the context of the input image. The architecture can also be split into two halves and only the seq2seq transformer can be used to generate hilarious caption by inputting a sentence.This paper aims to help everyday user to be more lazy and hilarious at the same time by generating captions using CATNet.



### End-to-End Multi-Object Detection with a Regularized Mixture Model
- **Arxiv ID**: http://arxiv.org/abs/2205.08714v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08714v3)
- **Published**: 2022-05-18 04:20:23+00:00
- **Updated**: 2023-04-28 06:08:12+00:00
- **Authors**: Jaeyoung Yoo, Hojun Lee, Seunghyeon Seo, Inseop Chung, Nojun Kwak
- **Comment**: Accepted at ICML 2023
- **Journal**: None
- **Summary**: Recent end-to-end multi-object detectors simplify the inference pipeline by removing hand-crafted processes such as non-maximum suppression (NMS). However, during training, they still heavily rely on heuristics and hand-crafted processes which deteriorate the reliability of the predicted confidence score. In this paper, we propose a novel framework to train an end-to-end multi-object detector consisting of only two terms: negative log-likelihood (NLL) and a regularization term. In doing so, the multi-object detection problem is treated as density estimation of the ground truth bounding boxes utilizing a regularized mixture density model. The proposed \textit{end-to-end multi-object Detection with a Regularized Mixture Model} (D-RMM) is trained by minimizing the NLL with the proposed regularization term, maximum component maximization (MCM) loss, preventing duplicate predictions. Our method reduces the heuristics of the training process and improves the reliability of the predicted confidence score. Moreover, our D-RMM outperforms the previous end-to-end detectors on MS COCO dataset.



### RandomMix: A mixed sample data augmentation method with multiple mixed modes
- **Arxiv ID**: http://arxiv.org/abs/2205.08728v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08728v1)
- **Published**: 2022-05-18 05:31:36+00:00
- **Updated**: 2022-05-18 05:31:36+00:00
- **Authors**: Xiaoliang Liu, Furao Shen, Jian Zhao, Changhai Nie
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation is a very practical technique that can be used to improve the generalization ability of neural networks and prevent overfitting. Recently, mixed sample data augmentation has received a lot of attention and achieved great success. In order to enhance the performance of mixed sample data augmentation, a series of recent works are devoted to obtaining and analyzing the salient regions of the image, and using the saliency area to guide the image mixing. However, obtaining the salient information of an image requires a lot of extra calculations. Different from improving performance through saliency analysis, our proposed method RandomMix mainly increases the diversity of the mixed sample to enhance the generalization ability and performance of neural networks. Moreover, RandomMix can improve the robustness of the model, does not require too much additional calculation, and is easy to insert into the training pipeline. Finally, experiments on the CIFAR-10/100, Tiny-ImageNet, ImageNet, and Google Speech Commands datasets demonstrate that RandomMix achieves better performance than other state-of-the-art mixed sample data augmentation methods.



### TTAPS: Test-Time Adaption by Aligning Prototypes using Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2205.08731v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.08731v1)
- **Published**: 2022-05-18 05:43:06+00:00
- **Updated**: 2022-05-18 05:43:06+00:00
- **Authors**: Alexander Bartler, Florian Bender, Felix Wiewel, Bin Yang
- **Comment**: Accepted at International Joint Conference on Neural Networks 2022
  (IJCNN)
- **Journal**: None
- **Summary**: Nowadays, deep neural networks outperform humans in many tasks. However, if the input distribution drifts away from the one used in training, their performance drops significantly. Recently published research has shown that adapting the model parameters to the test sample can mitigate this performance degradation. In this paper, we therefore propose a novel modification of the self-supervised training algorithm SwAV that adds the ability to adapt to single test samples. Using the provided prototypes of SwAV and our derived test-time loss, we align the representation of unseen test samples with the self-supervised learned prototypes. We show the success of our method on the common benchmark dataset CIFAR10-C.



### Deep-learned orthogonal basis patterns for fast, noise-robust single-pixel imaging
- **Arxiv ID**: http://arxiv.org/abs/2205.08736v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2205.08736v1)
- **Published**: 2022-05-18 06:12:33+00:00
- **Updated**: 2022-05-18 06:12:33+00:00
- **Authors**: Ritz Ann Aguilar, Damian Dailisan
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Single-pixel imaging (SPI) is a novel, unconventional method that goes beyond the notion of traditional cameras but can be computationally expensive and slow for real-time applications. Deep learning has been proposed as an alternative approach for solving the SPI reconstruction problem, but a detailed analysis of its performance and generated basis patterns when used for SPI is limited. We present a modified deep convolutional autoencoder network (DCAN) for SPI on 64x64 pixel images with up to 6.25% compression ratio and apply binary and orthogonality regularizers during training. Training a DCAN with these regularizers allows it to learn multiple measurement bases that have combinations of binary or non-binary, and orthogonal or non-orthogonal patterns. We compare the reconstruction quality, orthogonality of the patterns, and robustness to noise of the resulting DCAN models to traditional SPI reconstruction algorithms (such as Total Variation minimization and Fourier Transform). Our DCAN models can be trained to be robust to noise while still having fast enough reconstruction times (~3 ms per frame) to be viable for real-time imaging.



### 3D-VFD: A Victim-free Detector against 3D Adversarial Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2205.08738v3
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.08738v3)
- **Published**: 2022-05-18 06:19:15+00:00
- **Updated**: 2023-02-15 05:22:34+00:00
- **Authors**: Jiahao Zhu, Huajun Zhou, Zixuan Chen, Yi Zhou, Xiaohua Xie
- **Comment**: 6 pages, 13pages
- **Journal**: None
- **Summary**: 3D deep models consuming point clouds have achieved sound application effects in computer vision. However, recent studies have shown they are vulnerable to 3D adversarial point clouds. In this paper, we regard these malicious point clouds as 3D steganography examples and present a new perspective, 3D steganalysis, to counter such examples. Specifically, we propose 3D-VFD, a victim-free detector against 3D adversarial point clouds. Its core idea is to capture the discrepancies between residual geometric feature distributions of benign point clouds and adversarial point clouds and map these point clouds to a lower dimensional space where we can efficiently distinguish them. Unlike existing detection techniques against 3D adversarial point clouds, 3D-VFD does not rely on the victim 3D deep model's outputs for discrimination. Extensive experiments demonstrate that 3D-VFD achieves state-of-the-art detection and can effectively detect 3D adversarial attacks based on point adding and point perturbation while keeping fast detection speed.



### Validation of a photogrammetric approach for the objective study of ancient bowed instruments
- **Arxiv ID**: http://arxiv.org/abs/2205.08745v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08745v2)
- **Published**: 2022-05-18 06:38:18+00:00
- **Updated**: 2023-03-13 16:37:07+00:00
- **Authors**: PhilÃ©mon Beghin, Anne-Emmanuelle Ceulemans, Paul Fisette, FranÃ§ois Glineur
- **Comment**: None
- **Journal**: None
- **Summary**: Some early violins have been reduced during their history to fit imposed morphological standards, while more recent ones have been built directly to these standards. We propose an objective photogrammetric approach to differentiate between a reduced and an unreduced instrument, whereby a three-dimensional mesh is studied geometrically by examining 2D slices. Our contribution is twofold. First, we validate the quality of the photogrammetric mesh through a comparison with reference images obtained by medical imaging, and conclude that a sub-millimetre accuracy is achieved. Then, we show how quantitative and qualitative features such as contour lines, channel of minima and a measure of asymmetry between the upper and lower surfaces of a violin can be automatically extracted from the validated photogrammetric meshes, allowing to successfully highlight differences between instruments.



### [Re] Distilling Knowledge via Knowledge Review
- **Arxiv ID**: http://arxiv.org/abs/2205.11246v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.11246v1)
- **Published**: 2022-05-18 07:01:05+00:00
- **Updated**: 2022-05-18 07:01:05+00:00
- **Authors**: Apoorva Verma, Pranjal Gulati, Sarthak Gupta
- **Comment**: This is a reproducibility effort based on the CVPR '21 paper
  'Distilling Knowledge via Knowledge Review' by Chen et al
- **Journal**: None
- **Summary**: This effort aims to reproduce the results of experiments and analyze the robustness of the review framework for knowledge distillation introduced in the CVPR '21 paper 'Distilling Knowledge via Knowledge Review' by Chen et al. Previous works in knowledge distillation only studied connections paths between the same levels of the student and the teacher, and cross-level connection paths had not been considered. Chen et al. propose a new residual learning framework to train a single student layer using multiple teacher layers. They also design a novel fusion module to condense feature maps across levels and a loss function to compare feature information stored across different levels to improve performance. In this work, we consistently verify the improvements in test accuracy across student models as reported in the original paper and study the effectiveness of the novel modules introduced by conducting ablation studies and new experiments.



### Visual Attention-based Self-supervised Absolute Depth Estimation using Geometric Priors in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2205.08780v3
- **DOI**: 10.1109/LRA.2022.3210298
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08780v3)
- **Published**: 2022-05-18 08:01:38+00:00
- **Updated**: 2022-10-06 04:08:31+00:00
- **Authors**: Jie Xiang, Yun Wang, Lifeng An, Haiyang Liu, Zijun Wang, Jian Liu
- **Comment**: Published on IEEE Robotics and Automation Letters (RA-L)
- **Journal**: IEEE Robotics and Automation Letters, vol. 7, no. 4, pp.
  11998-12005, Oct. 2022
- **Summary**: Although existing monocular depth estimation methods have made great progress, predicting an accurate absolute depth map from a single image is still challenging due to the limited modeling capacity of networks and the scale ambiguity issue. In this paper, we introduce a fully Visual Attention-based Depth (VADepth) network, where spatial attention and channel attention are applied to all stages. By continuously extracting the dependencies of features along the spatial and channel dimensions over a long distance, VADepth network can effectively preserve important details and suppress interfering features to better perceive the scene structure for more accurate depth estimates. In addition, we utilize geometric priors to form scale constraints for scale-aware model training. Specifically, we construct a novel scale-aware loss using the distance between the camera and a plane fitted by the ground points corresponding to the pixels of the rectangular area in the bottom middle of the image. Experimental results on the KITTI dataset show that this architecture achieves the state-of-the-art performance and our method can directly output absolute depth without post-processing. Moreover, our experiments on the SeasonDepth dataset also demonstrate the robustness of our model to multiple unseen environments.



### Cross-subject Action Unit Detection with Meta Learning and Transformer-based Relation Modeling
- **Arxiv ID**: http://arxiv.org/abs/2205.08787v1
- **DOI**: 10.1109/IJCNN55064.2022.9891984
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08787v1)
- **Published**: 2022-05-18 08:17:59+00:00
- **Updated**: 2022-05-18 08:17:59+00:00
- **Authors**: Jiyuan Cao, Zhilei Liu, Yong Zhang
- **Comment**: Accepted by IJCNN 2022
- **Journal**: None
- **Summary**: Facial Action Unit (AU) detection is a crucial task for emotion analysis from facial movements. The apparent differences of different subjects sometimes mislead changes brought by AUs, resulting in inaccurate results. However, most of the existing AU detection methods based on deep learning didn't consider the identity information of different subjects. The paper proposes a meta-learning-based cross-subject AU detection model to eliminate the identity-caused differences. Besides, a transformer-based relation learning module is introduced to learn the latent relations of multiple AUs. To be specific, our proposed work is composed of two sub-tasks. The first sub-task is meta-learning-based AU local region representation learning, called MARL, which learns discriminative representation of local AU regions that incorporates the shared information of multiple subjects and eliminates identity-caused differences. The second sub-task uses the local region representation of AU of the first sub-task as input, then adds relationship learning based on the transformer encoder architecture to capture AU relationships. The entire training process is cascaded. Ablation study and visualization show that our MARL can eliminate identity-caused differences, thus obtaining a robust and generalized AU discriminative embedding representation. Our results prove that on the two public datasets BP4D and DISFA, our method is superior to the state-of-the-art technology, and the F1 score is improved by 1.3% and 1.4%, respectively.



### PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects
- **Arxiv ID**: http://arxiv.org/abs/2205.08811v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08811v1)
- **Published**: 2022-05-18 09:21:09+00:00
- **Updated**: 2022-05-18 09:21:09+00:00
- **Authors**: Pengyuan Wang, HyunJun Jung, Yitong Li, Siyuan Shen, Rahul Parthasarathy Srikanth, Lorenzo Garattoni, Sven Meier, Nassir Navab, Benjamin Busam
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Object pose estimation is crucial for robotic applications and augmented reality. Beyond instance level 6D object pose estimation methods, estimating category-level pose and shape has become a promising trend. As such, a new research field needs to be supported by well-designed datasets. To provide a benchmark with high-quality ground truth annotations to the community, we introduce a multimodal dataset for category-level object pose estimation with photometrically challenging objects termed PhoCaL. PhoCaL comprises 60 high quality 3D models of household objects over 8 categories including highly reflective, transparent and symmetric objects. We developed a novel robot-supported multi-modal (RGB, depth, polarisation) data acquisition and annotation process. It ensures sub-millimeter accuracy of the pose for opaque textured, shiny and transparent objects, no motion blur and perfect camera synchronisation. To set a benchmark for our dataset, state-of-the-art RGB-D and monocular RGB methods are evaluated on the challenging scenes of PhoCaL.



### Anomaly detection using prediction error with Spatio-Temporal Convolutional LSTM
- **Arxiv ID**: http://arxiv.org/abs/2205.08812v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08812v1)
- **Published**: 2022-05-18 09:25:53+00:00
- **Updated**: 2022-05-18 09:25:53+00:00
- **Authors**: Hanh Thi Minh Tran, David Hogg
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel method for video anomaly detection motivated by an existing architecture for sequence-to-sequence prediction and reconstruction using a spatio-temporal convolutional Long Short-Term Memory (convLSTM). As in previous work on anomaly detection, anomalies arise as spatially localised failures in reconstruction or prediction. In experiments with five benchmark datasets, we show that using prediction gives superior performance to using reconstruction. We also compare performance with different length input/output sequences. Overall, our results using prediction are comparable with the state of the art on the benchmark datasets.



### Speckle Image Restoration without Clean Data
- **Arxiv ID**: http://arxiv.org/abs/2205.08833v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.08833v1)
- **Published**: 2022-05-18 10:06:17+00:00
- **Updated**: 2022-05-18 10:06:17+00:00
- **Authors**: Tsung-Ming Tai, Yun-Jie Jhang, Wen-Jyi Hwang, Chau-Jern Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Speckle noise is an inherent disturbance in coherent imaging systems such as digital holography, synthetic aperture radar, optical coherence tomography, or ultrasound systems. These systems usually produce only single observation per view angle of the same interest object, imposing the difficulty to leverage the statistic among observations. We propose a novel image restoration algorithm that can perform speckle noise removal without clean data and does not require multiple noisy observations in the same view angle. Our proposed method can also be applied to the situation without knowing the noise distribution as prior. We demonstrate our method is especially well-suited for spectral images by first validating on the synthetic dataset, and also applied on real-world digital holography samples. The results are superior in both quantitative measurement and visual inspection compared to several widely applied baselines. Our method even shows promising results across different speckle noise strengths, without the clean data needed.



### Large Neural Networks Learning from Scratch with Very Few Data and without Explicit Regularization
- **Arxiv ID**: http://arxiv.org/abs/2205.08836v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2205.08836v2)
- **Published**: 2022-05-18 10:08:28+00:00
- **Updated**: 2022-10-21 07:37:37+00:00
- **Authors**: Christoph Linse, Thomas Martinetz
- **Comment**: 11 pages, 3 figures, 4 tables
- **Journal**: None
- **Summary**: Recent findings have shown that highly over-parameterized Neural Networks generalize without pretraining or explicit regularization. It is achieved with zero training error, i.e., complete over-fitting by memorizing the training data. This is surprising, since it is completely against traditional machine learning wisdom. In our empirical study we fortify these findings in the domain of fine-grained image classification. We show that very large Convolutional Neural Networks with millions of weights do learn with only a handful of training samples and without image augmentation, explicit regularization or pretraining. We train the architectures ResNet018, ResNet101 and VGG19 on subsets of the difficult benchmark datasets Caltech101, CUB_200_2011, FGVCAircraft, Flowers102 and StanfordCars with 100 classes and more, perform a comprehensive comparative study and draw implications for the practical application of CNNs. Finally, we show that a randomly initialized VGG19 with 140 million weights learns to distinguish airplanes and motorbikes with up to 95% accuracy using only 20 training samples per class.



### Positional Information is All You Need: A Novel Pipeline for Self-Supervised SVDE from Videos
- **Arxiv ID**: http://arxiv.org/abs/2205.08851v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.08851v1)
- **Published**: 2022-05-18 10:34:14+00:00
- **Updated**: 2022-05-18 10:34:14+00:00
- **Authors**: Juan Luis Gonzalez Bello, Jaeho Moon, Munchurl Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, much attention has been drawn to learning the underlying 3D structures of a scene from monocular videos in a fully self-supervised fashion. One of the most challenging aspects of this task is handling the independently moving objects as they break the rigid-scene assumption. For the first time, we show that pixel positional information can be exploited to learn SVDE (Single View Depth Estimation) from videos. Our proposed moving object (MO) masks, which are induced by shifted positional information (SPI) and referred to as `SPIMO' masks, are very robust and consistently remove the independently moving objects in the scenes, allowing for better learning of SVDE from videos. Additionally, we introduce a new adaptive quantization scheme that assigns the best per-pixel quantization curve for our depth discretization. Finally, we employ existing boosting techniques in a new way to further self-supervise the depth of the moving objects. With these features, our pipeline is robust against moving objects and generalizes well to high-resolution images, even when trained with small patches, yielding state-of-the-art (SOTA) results with almost 8.5x fewer parameters than the previous works that learn from videos. We present extensive experiments on KITTI and CityScapes that show the effectiveness of our method.



### Transformer based multiple instance learning for weakly supervised histopathology image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.08878v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08878v1)
- **Published**: 2022-05-18 12:04:26+00:00
- **Updated**: 2022-05-18 12:04:26+00:00
- **Authors**: Ziniu Qian, Kailu Li, Maode Lai, Eric I-Chao Chang, Bingzheng Wei, Yubo Fan, Yan Xu
- **Comment**: Provisional accepted for MICCAI 2022
- **Journal**: None
- **Summary**: Hispathological image segmentation algorithms play a critical role in computer aided diagnosis technology. The development of weakly supervised segmentation algorithm alleviates the problem of medical image annotation that it is time-consuming and labor-intensive. As a subset of weakly supervised learning, Multiple Instance Learning (MIL) has been proven to be effective in segmentation. However, there is a lack of related information between instances in MIL, which limits the further improvement of segmentation performance. In this paper, we propose a novel weakly supervised method for pixel-level segmentation in histopathology images, which introduces Transformer into the MIL framework to capture global or long-range dependencies. The multi-head self-attention in the Transformer establishes the relationship between instances, which solves the shortcoming that instances are independent of each other in MIL. In addition, deep supervision is introduced to overcome the limitation of annotations in weakly supervised methods and make the better utilization of hierarchical information. The state-of-the-art results on the colon cancer dataset demonstrate the superiority of the proposed method compared with other weakly supervised methods. It is worth believing that there is a potential of our approach for various applications in medical images.



### 3D Segmentation Guided Style-based Generative Adversarial Networks for PET Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2205.08887v1
- **DOI**: 10.1109/TMI.2022.3156614
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.08887v1)
- **Published**: 2022-05-18 12:19:17+00:00
- **Updated**: 2022-05-18 12:19:17+00:00
- **Authors**: Yang Zhou, Zhiwen Yang, Hui Zhang, Eric I-Chao Chang, Yubo Fan, Yan Xu
- **Comment**: This article has been accepted for publication in a future issue of
  this journal, but has not been fully edited. Content may change prior to
  final publication. Citation information: DOI 10.1109/TMI.2022.3156614, IEEE
  Transactions on Medical Imaging
- **Journal**: IEEE Transactions on Medical Imaging, 2022, 41(8): 2092-2104
- **Summary**: Potential radioactive hazards in full-dose positron emission tomography (PET) imaging remain a concern, whereas the quality of low-dose images is never desirable for clinical use. So it is of great interest to translate low-dose PET images into full-dose. Previous studies based on deep learning methods usually directly extract hierarchical features for reconstruction. We notice that the importance of each feature is different and they should be weighted dissimilarly so that tiny information can be captured by the neural network. Furthermore, the synthesis on some regions of interest is important in some applications. Here we propose a novel segmentation guided style-based generative adversarial network (SGSGAN) for PET synthesis. (1) We put forward a style-based generator employing style modulation, which specifically controls the hierarchical features in the translation process, to generate images with more realistic textures. (2) We adopt a task-driven strategy that couples a segmentation task with a generative adversarial network (GAN) framework to improve the translation performance. Extensive experiments show the superiority of our overall framework in PET synthesis, especially on those regions of interest.



### Remote Sensing Novel View Synthesis with Implicit Multiplane Representations
- **Arxiv ID**: http://arxiv.org/abs/2205.08908v1
- **DOI**: 10.1109/TGRS.2022.3197409
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08908v1)
- **Published**: 2022-05-18 13:03:55+00:00
- **Updated**: 2022-05-18 13:03:55+00:00
- **Authors**: Yongchang Wu, Zhengxia Zou, Zhenwei Shi
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Novel view synthesis of remote sensing scenes is of great significance for scene visualization, human-computer interaction, and various downstream applications. Despite the recent advances in computer graphics and photogrammetry technology, generating novel views is still challenging particularly for remote sensing images due to its high complexity, view sparsity and limited view-perspective variations. In this paper, we propose a novel remote sensing view synthesis method by leveraging the recent advances in implicit neural representations. Considering the overhead and far depth imaging of remote sensing images, we represent the 3D space by combining implicit multiplane images (MPI) representation and deep neural networks. The 3D scene is reconstructed under a self-supervised optimization paradigm through a differentiable multiplane renderer with multi-view input constraints. Images from any novel views thus can be freely rendered on the basis of the reconstructed model. As a by-product, the depth maps corresponding to the given viewpoint can be generated along with the rendering output. We refer to our method as Implicit Multiplane Images (ImMPI). To further improve the view synthesis under sparse-view inputs, we explore the learning-based initialization of remote sensing 3D scenes and proposed a neural network based Prior extractor to accelerate the optimization process. In addition, we propose a new dataset for remote sensing novel view synthesis with multi-view real-world google earth images. Extensive experiments demonstrate the superiority of the ImMPI over previous state-of-the-art methods in terms of reconstruction accuracy, visual fidelity, and time efficiency. Ablation experiments also suggest the effectiveness of our methodology design. Our dataset and code can be found at https://github.com/wyc-Chang/ImMPI



### Financial Time Series Data Augmentation with Generative Adversarial Networks and Extended Intertemporal Return Plots
- **Arxiv ID**: http://arxiv.org/abs/2205.08924v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08924v2)
- **Published**: 2022-05-18 13:39:27+00:00
- **Updated**: 2022-05-19 07:26:12+00:00
- **Authors**: Justin Hellermann, Qinzhuan Qian, Ankit Shah
- **Comment**: None
- **Journal**: None
- **Summary**: Data augmentation is a key regularization method to support the forecast and classification performance of highly parameterized models in computer vision. In the time series domain however, regularization in terms of augmentation is not equally common even though these methods have proven to mitigate effects from small sample size or non-stationarity. In this paper we apply state-of-the art image-based generative models for the task of data augmentation and introduce the extended intertemporal return plot (XIRP), a new image representation for time series. Multiple tests are conducted to assess the quality of the augmentation technique regarding its ability to synthesize time series effectively and improve forecast results on a subset of the M4 competition. We further investigate the relationship between data set characteristics and sampling results via Shapley values for feature attribution on the performance metrics and the optimal ratio of augmented data. Over all data sets, our approach proves to be effective in reducing the return forecast error by 7% on 79% of the financial data sets with varying statistical properties and frequencies.



### COVID-Net UV: An End-to-End Spatio-Temporal Deep Neural Network Architecture for Automated Diagnosis of COVID-19 Infection from Ultrasound Videos
- **Arxiv ID**: http://arxiv.org/abs/2205.08932v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.08932v1)
- **Published**: 2022-05-18 13:59:16+00:00
- **Updated**: 2022-05-18 13:59:16+00:00
- **Authors**: Hilda Azimi, Ashkan Ebadi, Jessy Song, Pengcheng Xi, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Besides vaccination, as an effective way to mitigate the further spread of COVID-19, fast and accurate screening of individuals to test for the disease is yet necessary to ensure public health safety. We propose COVID-Net UV, an end-to-end hybrid spatio-temporal deep neural network architecture, to detect COVID-19 infection from lung point-of-care ultrasound videos captured by convex transducers. COVID-Net UV comprises a convolutional neural network that extracts spatial features and a recurrent neural network that learns temporal dependence. After careful hyperparameter tuning, the network achieves an average accuracy of 94.44% with no false-negative cases for COVID-19 cases. The goal with COVID-Net UV is to assist front-line clinicians in the fight against COVID-19 via accelerating the screening of lung point-of-care ultrasound videos and automatic detection of COVID-19 positive cases.



### Deep Features for CBIR with Scarce Data using Hebbian Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.08935v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2205.08935v1)
- **Published**: 2022-05-18 14:00:54+00:00
- **Updated**: 2022-05-18 14:00:54+00:00
- **Authors**: Gabriele Lagani, Davide Bacciu, Claudio Gallicchio, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato
- **Comment**: 6 Pages, 1 Figure, 2 Tables, Submitted at CBMI 2022
- **Journal**: None
- **Summary**: Features extracted from Deep Neural Networks (DNNs) have proven to be very effective in the context of Content Based Image Retrieval (CBIR). In recent work, biologically inspired \textit{Hebbian} learning algorithms have shown promises for DNN training. In this contribution, we study the performance of such algorithms in the development of feature extractors for CBIR tasks. Specifically, we consider a semi-supervised learning strategy in two steps: first, an unsupervised pre-training stage is performed using Hebbian learning on the image dataset; second, the network is fine-tuned using supervised Stochastic Gradient Descent (SGD) training. For the unsupervised pre-training stage, we explore the nonlinear Hebbian Principal Component Analysis (HPCA) learning rule. For the supervised fine-tuning stage, we assume sample efficiency scenarios, in which the amount of labeled samples is just a small fraction of the whole dataset. Our experimental analysis, conducted on the CIFAR10 and CIFAR100 datasets shows that, when few labeled samples are available, our Hebbian approach provides relevant improvements compared to various alternative methods.



### A lightweight multi-scale context network for salient object detection in optical remote sensing images
- **Arxiv ID**: http://arxiv.org/abs/2205.08959v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08959v1)
- **Published**: 2022-05-18 14:32:47+00:00
- **Updated**: 2022-05-18 14:32:47+00:00
- **Authors**: Yuhan Lin, Han Sun, Ningzhong Liu, Yetong Bian, Jun Cen, Huiyu Zhou
- **Comment**: accepted by ICPR2022, source code, see
  https://github.com/NuaaYH/MSCNet
- **Journal**: None
- **Summary**: Due to the more dramatic multi-scale variations and more complicated foregrounds and backgrounds in optical remote sensing images (RSIs), the salient object detection (SOD) for optical RSIs becomes a huge challenge. However, different from natural scene images (NSIs), the discussion on the optical RSI SOD task still remains scarce. In this paper, we propose a multi-scale context network, namely MSCNet, for SOD in optical RSIs. Specifically, a multi-scale context extraction module is adopted to address the scale variation of salient objects by effectively learning multi-scale contextual information. Meanwhile, in order to accurately detect complete salient objects in complex backgrounds, we design an attention-based pyramid feature aggregation mechanism for gradually aggregating and refining the salient regions from the multi-scale context extraction module. Extensive experiments on two benchmarks demonstrate that MSCNet achieves competitive performance with only 3.26M parameters. The code will be available at https://github.com/NuaaYH/MSCNet.



### Trading Positional Complexity vs. Deepness in Coordinate Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.08987v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08987v1)
- **Published**: 2022-05-18 15:17:09+00:00
- **Updated**: 2022-05-18 15:17:09+00:00
- **Authors**: Jianqiao Zheng, Sameera Ramasinghe, Xueqian Li, Simon Lucey
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2107.02561
- **Journal**: None
- **Summary**: It is well noted that coordinate-based MLPs benefit -- in terms of preserving high-frequency information -- through the encoding of coordinate positions as an array of Fourier features. Hitherto, the rationale for the effectiveness of these positional encodings has been mainly studied through a Fourier lens. In this paper, we strive to broaden this understanding by showing that alternative non-Fourier embedding functions can indeed be used for positional encoding. Moreover, we show that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates. We further establish that the now ubiquitous Fourier feature mapping of position is a special case that fulfills these conditions. Consequently, we present a more general theory to analyze positional encoding in terms of shifted basis functions. In addition, we argue that employing a more complex positional encoding -- that scales exponentially with the number of modes -- requires only a linear (rather than deep) coordinate function to achieve comparable performance. Counter-intuitively, we demonstrate that trading positional embedding complexity for network deepness is orders of magnitude faster than current state-of-the-art; despite the additional embedding complexity. To this end, we develop the necessary theoretical formulae and empirically verify that our theoretical claims hold in practice.



### Constraining the Attack Space of Machine Learning Models with Distribution Clamping Preprocessing
- **Arxiv ID**: http://arxiv.org/abs/2205.08989v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.08989v1)
- **Published**: 2022-05-18 15:20:18+00:00
- **Updated**: 2022-05-18 15:20:18+00:00
- **Authors**: Ryan Feng, Somesh Jha, Atul Prakash
- **Comment**: None
- **Journal**: None
- **Summary**: Preprocessing and outlier detection techniques have both been applied to neural networks to increase robustness with varying degrees of success. In this paper, we formalize the ideal preprocessor function as one that would take any input and set it to the nearest in-distribution input. In other words, we detect any anomalous pixels and set them such that the new input is in-distribution. We then illustrate a relaxed solution to this problem in the context of patch attacks. Specifically, we demonstrate that we can model constraints on the patch attack that specify regions as out of distribution. With these constraints, we are able to preprocess inputs successfully, increasing robustness on CARLA object detection.



### Empirical Advocacy of Bio-inspired Models for Robust Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.09037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.09037v1)
- **Published**: 2022-05-18 16:19:26+00:00
- **Updated**: 2022-05-18 16:19:26+00:00
- **Authors**: Harshitha Machiraju, Oh-Hyeon Choung, Michael H. Herzog, Pascal Frossard
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) have revolutionized computer vision and are often advocated as good models of the human visual system. However, there are currently many shortcomings of DCNNs, which preclude them as a model of human vision. There are continuous attempts to use features of the human visual system to improve the robustness of neural networks to data perturbations. We provide a detailed analysis of such bio-inspired models and their properties. To this end, we benchmark the robustness of several bio-inspired models against their most comparable baseline DCNN models. We find that bio-inspired models tend to be adversarially robust without requiring any special data augmentation. Additionally, we find that bio-inspired models beat adversarially trained models in the presence of more real-world common corruptions. Interestingly, we also find that bio-inspired models tend to use both low and mid-frequency information, in contrast to other DCNN models. We find that this mix of frequency information makes them robust to both adversarial perturbations and common corruptions.



### Global Contrast Masked Autoencoders Are Powerful Pathological Representation Learners
- **Arxiv ID**: http://arxiv.org/abs/2205.09048v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.09048v2)
- **Published**: 2022-05-18 16:28:56+00:00
- **Updated**: 2022-05-21 13:53:43+00:00
- **Authors**: Hao Quan, Xingyu Li, Weixing Chen, Qun Bai, Mingchen Zou, Ruijie Yang, Tingting Zheng, Ruiqun Qi, Xinghua Gao, Xiaoyu Cui
- **Comment**: None
- **Journal**: None
- **Summary**: Based on digital whole slide scanning technique, artificial intelligence algorithms represented by deep learning have achieved remarkable results in the field of computational pathology. Compared with other medical images such as Computed Tomography (CT) or Magnetic Resonance Imaging (MRI), pathological images are more difficult to annotate, thus there is an extreme lack of data sets that can be used for supervised learning. In this study, a self-supervised learning (SSL) model, Global Contrast Masked Autoencoders (GCMAE), is proposed, which has the ability to represent both global and local domain-specific features of whole slide image (WSI), as well as excellent cross-data transfer ability. The Camelyon16 and NCTCRC datasets are used to evaluate the performance of our model. When dealing with transfer learning tasks with different data sets, the experimental results show that GCMAE has better linear classification accuracy than MAE, which can reach 81.10% and 89.22% respectively. Our method outperforms the previous state-of-the-art algorithm and even surpass supervised learning (improved by 3.86% on NCTCRC data sets). The source code of this paper is publicly available at https://github.com/StarUniversus/gcmae



### VRAG: Region Attention Graphs for Content-Based Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2205.09068v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.09068v1)
- **Published**: 2022-05-18 16:50:45+00:00
- **Updated**: 2022-05-18 16:50:45+00:00
- **Authors**: Kennard Ng, Ser-Nam Lim, Gim Hee Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Content-based Video Retrieval (CBVR) is used on media-sharing platforms for applications such as video recommendation and filtering. To manage databases that scale to billions of videos, video-level approaches that use fixed-size embeddings are preferred due to their efficiency. In this paper, we introduce Video Region Attention Graph Networks (VRAG) that improves the state-of-the-art of video-level methods. We represent videos at a finer granularity via region-level features and encode video spatio-temporal dynamics through region-level relations. Our VRAG captures the relationships between regions based on their semantic content via self-attention and the permutation invariant aggregation of Graph Convolution. In addition, we show that the performance gap between video-level and frame-level methods can be reduced by segmenting videos into shots and using shot embeddings for video retrieval. We evaluate our VRAG over several video retrieval tasks and achieve a new state-of-the-art for video-level retrieval. Furthermore, our shot-level VRAG shows higher retrieval precision than other existing video-level methods, and closer performance to frame-level methods at faster evaluation speeds. Finally, our code will be made publicly available.



### Counting Phases and Faces Using Bayesian Thermodynamic Integration
- **Arxiv ID**: http://arxiv.org/abs/2206.07494v1
- **DOI**: None
- **Categories**: **cond-mat.stat-mech**, cond-mat.dis-nn, cs.CV, physics.data-an, 68U10, 82B20, 82C23, I.4.0; J.2
- **Links**: [PDF](http://arxiv.org/pdf/2206.07494v1)
- **Published**: 2022-05-18 17:11:23+00:00
- **Updated**: 2022-05-18 17:11:23+00:00
- **Authors**: Alexander Lobashev, Mikhail V. Tamm
- **Comment**: 20 pages, 9 figures, plus appendix with additional figures
- **Journal**: None
- **Summary**: We introduce a new approach to reconstruction of the thermodynamic functions and phase boundaries in two-parametric statistical mechanics systems. Our method is based on expressing the Fisher metric in terms of the posterior distributions over a space of external parameters and approximating the metric field by a Hessian of a convex function. We use the proposed approach to accurately reconstruct the partition functions and phase diagrams of the Ising model and the exactly solvable non-equilibrium TASEP without any a priori knowledge about microscopic rules of the models. We also demonstrate how our approach can be used to visualize the latent space of StyleGAN models and evaluate the variability of the generated images.



### Pluralistic Image Completion with Probabilistic Mixture-of-Experts
- **Arxiv ID**: http://arxiv.org/abs/2205.09086v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.09086v1)
- **Published**: 2022-05-18 17:24:21+00:00
- **Updated**: 2022-05-18 17:24:21+00:00
- **Authors**: Xiaobo Xia, Wenhao Yang, Jie Ren, Yewen Li, Yibing Zhan, Bo Han, Tongliang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Pluralistic image completion focuses on generating both visually realistic and diverse results for image completion. Prior methods enjoy the empirical successes of this task. However, their used constraints for pluralistic image completion are argued to be not well interpretable and unsatisfactory from two aspects. First, the constraints for visual reality can be weakly correlated to the objective of image completion or even redundant. Second, the constraints for diversity are designed to be task-agnostic, which causes the constraints to not work well. In this paper, to address the issues, we propose an end-to-end probabilistic method. Specifically, we introduce a unified probabilistic graph model that represents the complex interactions in image completion. The entire procedure of image completion is then mathematically divided into several sub-procedures, which helps efficient enforcement of constraints. The sub-procedure directly related to pluralistic results is identified, where the interaction is established by a Gaussian mixture model (GMM). The inherent parameters of GMM are task-related, which are optimized adaptively during training, while the number of its primitives can control the diversity of results conveniently. We formally establish the effectiveness of our method and demonstrate it with comprehensive experiments.



### BodyMap: Learning Full-Body Dense Correspondence Map
- **Arxiv ID**: http://arxiv.org/abs/2205.09111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.09111v1)
- **Published**: 2022-05-18 17:58:11+00:00
- **Updated**: 2022-05-18 17:58:11+00:00
- **Authors**: Anastasia Ianina, Nikolaos Sarafianos, Yuanlu Xu, Ignacio Rocco, Tony Tung
- **Comment**: CVPR 2022 Project Page: https://nsarafianos.github.io/bodymap
- **Journal**: None
- **Summary**: Dense correspondence between humans carries powerful semantic information that can be utilized to solve fundamental problems for full-body understanding such as in-the-wild surface matching, tracking and reconstruction. In this paper we present BodyMap, a new framework for obtaining high-definition full-body and continuous dense correspondence between in-the-wild images of clothed humans and the surface of a 3D template model. The correspondences cover fine details such as hands and hair, while capturing regions far from the body surface, such as loose clothing. Prior methods for estimating such dense surface correspondence i) cut a 3D body into parts which are unwrapped to a 2D UV space, producing discontinuities along part seams, or ii) use a single surface for representing the whole body, but none handled body details. Here, we introduce a novel network architecture with Vision Transformers that learn fine-level features on a continuous body surface. BodyMap outperforms prior work on various metrics and datasets, including DensePose-COCO by a large margin. Furthermore, we show various applications ranging from multi-layer dense cloth correspondence, neural rendering with novel-view synthesis and appearance swapping.



### Masked Autoencoders As Spatiotemporal Learners
- **Arxiv ID**: http://arxiv.org/abs/2205.09113v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.09113v2)
- **Published**: 2022-05-18 17:59:59+00:00
- **Updated**: 2022-10-21 09:16:43+00:00
- **Authors**: Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, Kaiming He
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., > 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.



### LeRaC: Learning Rate Curriculum
- **Arxiv ID**: http://arxiv.org/abs/2205.09180v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.09180v2)
- **Published**: 2022-05-18 18:57:36+00:00
- **Updated**: 2022-11-19 10:41:32+00:00
- **Authors**: Florinel-Alin Croitoru, Nicolae-Catalin Ristea, Radu Tudor Ionescu, Nicu Sebe
- **Comment**: Main paper + supplementary
- **Journal**: None
- **Summary**: Most curriculum learning methods require an approach to sort the data samples by difficulty, which is often cumbersome to perform. In this work, we propose a novel curriculum learning approach termed Learning Rate Curriculum (LeRaC), which leverages the use of a different learning rate for each layer of a neural network to create a data-free curriculum during the initial training epochs. More specifically, LeRaC assigns higher learning rates to neural layers closer to the input, gradually decreasing the learning rates as the layers are placed farther away from the input. The learning rates increase at various paces during the first training iterations, until they all reach the same value. From this point on, the neural model is trained as usual. This creates a model-level curriculum learning strategy that does not require sorting the examples by difficulty and is compatible with any neural network, generating higher performance levels regardless of the architecture. We conduct comprehensive experiments on eight datasets from the computer vision (CIFAR-10, CIFAR-100, Tiny ImageNet), language (BoolQ, QNLI, RTE) and audio (ESC-50, CREMA-D) domains, considering various convolutional (ResNet-18, Wide-ResNet-50, DenseNet-121), recurrent (LSTM) and transformer (CvT, BERT, SepTr) architectures, comparing our approach with the conventional training regime. Moreover, we also compare with Curriculum by Smoothing (CBS), a state-of-the-art data-free curriculum learning approach. Unlike CBS, our performance improvements over the standard training regime are consistent across all datasets and models. Furthermore, we significantly surpass CBS in terms of training time (there is no additional cost over the standard training regime for LeRaC).



### Computing the ensemble spread from deterministic weather predictions using conditional generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/2205.09182v1
- **DOI**: 10.1029/2022GL101452
- **Categories**: **cs.LG**, cs.CV, physics.ao-ph, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/2205.09182v1)
- **Published**: 2022-05-18 19:10:38+00:00
- **Updated**: 2022-05-18 19:10:38+00:00
- **Authors**: RÃ¼diger Brecht, Alex Bihlo
- **Comment**: 9 pages, 4 figures, 3 tables; release version
- **Journal**: None
- **Summary**: Ensemble prediction systems are an invaluable tool for weather forecasting. Practically, ensemble predictions are obtained by running several perturbations of the deterministic control forecast. However, ensemble prediction is associated with a high computational cost and often involves statistical post-processing steps to improve its quality. Here we propose to use deep-learning-based algorithms to learn the statistical properties of an ensemble prediction system, the ensemble spread, given only the deterministic control forecast. Thus, once trained, the costly ensemble prediction system will not be needed anymore to obtain future ensemble forecasts, and the statistical properties of the ensemble can be derived from a single deterministic forecast. We adapt the classical pix2pix architecture to a three-dimensional model and also experiment with a shared latent space encoder-decoder model, and train them against several years of operational (ensemble) weather forecasts for the 500 hPa geopotential height. The results demonstrate that the trained models indeed allow obtaining a highly accurate ensemble spread from the control forecast only.



### Scalable Multi-view Clustering with Graph Filtering
- **Arxiv ID**: http://arxiv.org/abs/2205.09228v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2205.09228v1)
- **Published**: 2022-05-18 22:04:23+00:00
- **Updated**: 2022-05-18 22:04:23+00:00
- **Authors**: Liang Liu, Peng Chen, Guangchun Luo, Zhao Kang, Yonggang Luo, Sanchu Han
- **Comment**: None
- **Journal**: None
- **Summary**: With the explosive growth of multi-source data, multi-view clustering has attracted great attention in recent years. Most existing multi-view methods operate in raw feature space and heavily depend on the quality of original feature representation. Moreover, they are often designed for feature data and ignore the rich topology structure information. Accordingly, in this paper, we propose a generic framework to cluster both attribute and graph data with heterogeneous features. It is capable of exploring the interplay between feature and structure. Specifically, we first adopt graph filtering technique to eliminate high-frequency noise to achieve a clustering-friendly smooth representation. To handle the scalability challenge, we develop a novel sampling strategy to improve the quality of anchors. Extensive experiments on attribute and graph benchmarks demonstrate the superiority of our approach with respect to state-of-the-art approaches.



### MESH2IR: Neural Acoustic Impulse Response Generator for Complex 3D Scenes
- **Arxiv ID**: http://arxiv.org/abs/2205.09248v2
- **DOI**: 10.1145/3503161.3548253
- **Categories**: **cs.SD**, cs.CV, cs.GR, cs.LG, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2205.09248v2)
- **Published**: 2022-05-18 23:50:34+00:00
- **Updated**: 2022-07-11 22:23:18+00:00
- **Authors**: Anton Ratnarajah, Zhenyu Tang, Rohith Chandrashekar Aralikatti, Dinesh Manocha
- **Comment**: Accepted to ACM Multimedia 2022. More results and source code is
  available at https://anton-jeran.github.io/M2IR/
- **Journal**: None
- **Summary**: We propose a mesh-based neural network (MESH2IR) to generate acoustic impulse responses (IRs) for indoor 3D scenes represented using a mesh. The IRs are used to create a high-quality sound experience in interactive applications and audio processing. Our method can handle input triangular meshes with arbitrary topologies (2K - 3M triangles). We present a novel training technique to train MESH2IR using energy decay relief and highlight its benefits. We also show that training MESH2IR on IRs preprocessed using our proposed technique significantly improves the accuracy of IR generation. We reduce the non-linearity in the mesh space by transforming 3D scene meshes to latent space using a graph convolution network. Our MESH2IR is more than 200 times faster than a geometric acoustic algorithm on a CPU and can generate more than 10,000 IRs per second on an NVIDIA GeForce RTX 2080 Ti GPU for a given furnished indoor 3D scene. The acoustic metrics are used to characterize the acoustic environment. We show that the acoustic metrics of the IRs predicted from our MESH2IR match the ground truth with less than 10% error. We also highlight the benefits of MESH2IR on audio and speech processing applications such as speech dereverberation and speech separation. To the best of our knowledge, ours is the first neural-network-based approach to predict IRs from a given 3D scene mesh in real-time.



### On the Limits of Evaluating Embodied Agent Model Generalization Using Validation Sets
- **Arxiv ID**: http://arxiv.org/abs/2205.09249v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.09249v1)
- **Published**: 2022-05-18 23:52:21+00:00
- **Updated**: 2022-05-18 23:52:21+00:00
- **Authors**: Hyounghun Kim, Aishwarya Padmakumar, Di Jin, Mohit Bansal, Dilek Hakkani-Tur
- **Comment**: ACL 2022 Insights Workshop (6 pages)
- **Journal**: None
- **Summary**: Natural language guided embodied task completion is a challenging problem since it requires understanding natural language instructions, aligning them with egocentric visual observations, and choosing appropriate actions to execute in the environment to produce desired changes. We experiment with augmenting a transformer model for this task with modules that effectively utilize a wider field of view and learn to choose whether the next step requires a navigation or manipulation action. We observed that the proposed modules resulted in improved, and in fact state-of-the-art performance on an unseen validation set of a popular benchmark dataset, ALFRED. However, our best model selected using the unseen validation set underperforms on the unseen test split of ALFRED, indicating that performance on the unseen validation set may not in itself be a sufficient indicator of whether model improvements generalize to unseen test sets. We highlight this result as we believe it may be a wider phenomenon in machine learning tasks but primarily noticeable only in benchmarks that limit evaluations on test splits, and highlights the need to modify benchmark design to better account for variance in model performance.



