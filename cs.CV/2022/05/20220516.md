# Arxiv Papers in cs.CV on 2022-05-16
### PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.07403v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07403v5)
- **Published**: 2022-05-16 00:14:50+00:00
- **Updated**: 2022-08-26 03:21:15+00:00
- **Authors**: Guangsheng Shi, Ruifeng Li, Chao Ma
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Real-time and high-performance 3D object detection is of critical importance for autonomous driving. Recent top-performing 3D object detectors mainly rely on point-based or 3D voxel-based convolutions, which are both computationally inefficient for onboard deployment. In contrast, pillar-based methods use solely 2D convolutions, which consume less computation resources, but they lag far behind their voxel-based counterparts in detection accuracy. In this paper, by examining the primary performance gap between pillar- and voxel-based detectors, we develop a real-time and high-performance pillar-based detector, dubbed PillarNet.The proposed PillarNet consists of a powerful encoder network for effective pillar feature learning, a neck network for spatial-semantic feature fusion and the commonly used detect head. Using only 2D convolutions, PillarNet is flexible to an optional pillar size and compatible with classical 2D CNN backbones, such as VGGNet and ResNet. Additionally, PillarNet benefits from our designed orientation-decoupled IoU regression loss along with the IoU-aware prediction branch. Extensive experimental results on the large-scale nuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet performs well over state-of-the-art 3D detectors in terms of effectiveness and efficiency. Code is available at \url{https://github.com/agent-sgs/PillarNet}.



### A New Outlier Removal Strategy Based on Reliability of Correspondence Graph for Fast Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2205.07404v1
- **DOI**: 10.1109/TPAMI.2022.3226498
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07404v1)
- **Published**: 2022-05-16 00:37:57+00:00
- **Updated**: 2022-05-16 00:37:57+00:00
- **Authors**: Li Yan, Pengcheng Wei, Hong Xie, Jicheng Dai, Hao Wu, Ming Huang
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Registration is a basic yet crucial task in point cloud processing. In correspondence-based point cloud registration, matching correspondences by point feature techniques may lead to an extremely high outlier ratio. Current methods still suffer from low efficiency, accuracy, and recall rate. We use a simple and intuitive method to describe the 6-DOF (degree of freedom) curtailment process in point cloud registration and propose an outlier removal strategy based on the reliability of the correspondence graph. The method constructs the corresponding graph according to the given correspondences and designs the concept of the reliability degree of the graph node for optimal candidate selection and the reliability degree of the graph edge to obtain the global maximum consensus set. The presented method could achieve fast and accurate outliers removal along with gradual aligning parameters estimation. Extensive experiments on simulations and challenging real-world datasets demonstrate that the proposed method can still perform effective point cloud registration even the correspondence outlier ratio is over 99%, and the efficiency is better than the state-of-the-art. Code is available at https://github.com/WPC-WHU/GROR.



### Transformers in 3D Point Clouds: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2205.07417v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07417v2)
- **Published**: 2022-05-16 01:32:18+00:00
- **Updated**: 2022-09-21 15:10:21+00:00
- **Authors**: Dening Lu, Qian Xie, Mingqiang Wei, Kyle Gao, Linlin Xu, Jonathan Li
- **Comment**: 20 pages, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Transformers have been at the heart of the Natural Language Processing (NLP) and Computer Vision (CV) revolutions. The significant success in NLP and CV inspired exploring the use of Transformers in point cloud processing. However, how do Transformers cope with the irregularity and unordered nature of point clouds? How suitable are Transformers for different 3D representations (e.g., point- or voxel-based)? How competent are Transformers for various 3D processing tasks? As of now, there is still no systematic survey of the research on these issues. For the first time, we provided a comprehensive overview of increasingly popular Transformers for 3D point cloud analysis. We start by introducing the theory of the Transformer architecture and reviewing its applications in 2D/3D fields. Then, we present three different taxonomies (i.e., implementation-, data representation-, and task-based), which can classify current Transformer-based methods from multiple perspectives. Furthermore, we present the results of an investigation of the variants and improvements of the self-attention mechanism in 3D. To demonstrate the superiority of Transformers in point cloud analysis, we present comprehensive comparisons of various Transformer-based methods for classification, segmentation, and object detection. Finally, we suggest three potential research directions, providing benefit references for the development of 3D Transformers.



### Binarizing by Classification: Is soft function really necessary?
- **Arxiv ID**: http://arxiv.org/abs/2205.07433v3
- **DOI**: 10.1109/TCSVT.2023.3288572
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.07433v3)
- **Published**: 2022-05-16 02:47:41+00:00
- **Updated**: 2023-07-16 07:22:19+00:00
- **Authors**: Yefei He, Luoming Zhang, Weijia Wu, Hong Zhou
- **Comment**: in IEEE Transactions on Circuits and Systems for Video Technology
  (2023)
- **Journal**: None
- **Summary**: Binary neural networks leverage $\mathrm{Sign}$ function to binarize weights and activations, which require gradient estimators to overcome its non-differentiability and will inevitably bring gradient errors during backpropagation. Although many hand-designed soft functions have been proposed as gradient estimators to better approximate gradients, their mechanism is not clear and there are still huge performance gaps between binary models and their full-precision counterparts. To address these issues and reduce gradient error, we propose to tackle network binarization as a binary classification problem and use a multi-layer perceptron (MLP) as the classifier in the forward pass and gradient estimator in the backward pass. Benefiting from the MLP's theoretical capability to fit any continuous function, it can be adaptively learned to binarize networks and backpropagate gradients without any prior knowledge of soft functions. From this perspective, we further empirically justify that even a simple linear function can outperform previous complex soft functions. Extensive experiments demonstrate that the proposed method yields surprising performance both in image classification and human pose estimation tasks. Specifically, we achieve $65.7\%$ top-1 accuracy of ResNet-34 on ImageNet dataset, with an absolute improvement of $2.6\%$. Moreover, we take binarization as a lightweighting approach for pose estimation models and propose well-designed binary pose estimation networks SBPN and BHRNet. When evaluating on the challenging Microsoft COCO keypoint dataset, the proposed method enables binary networks to achieve a mAP of up to $60.6$ for the first time. Experiments conducted on real platforms demonstrate that BNN achieves a better balance between performance and computational complexity, especially when computational resources are extremely low.



### ReDFeat: Recoupling Detection and Description for Multimodal Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.07439v1
- **DOI**: 10.1109/TIP.2022.3231135
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.07439v1)
- **Published**: 2022-05-16 04:24:22+00:00
- **Updated**: 2022-05-16 04:24:22+00:00
- **Authors**: Yuxin Deng, Jiayi Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Deep-learning-based local feature extraction algorithms that combine detection and description have made significant progress in visible image matching. However, the end-to-end training of such frameworks is notoriously unstable due to the lack of strong supervision of detection and the inappropriate coupling between detection and description. The problem is magnified in cross-modal scenarios, in which most methods heavily rely on the pre-training. In this paper, we recouple independent constraints of detection and description of multimodal feature learning with a mutual weighting strategy, in which the detected probabilities of robust features are forced to peak and repeat, while features with high detection scores are emphasized during optimization. Different from previous works, those weights are detached from back propagation so that the detected probability of indistinct features would not be directly suppressed and the training would be more stable. Moreover, we propose the Super Detector, a detector that possesses a large receptive field and is equipped with learnable non-maximum suppression layers, to fulfill the harsh terms of detection. Finally, we build a benchmark that contains cross visible, infrared, near-infrared and synthetic aperture radar image pairs for evaluating the performance of features in feature matching and image registration tasks. Extensive experiments demonstrate that features trained with the recoulped detection and description, named ReDFeat, surpass previous state-of-the-arts in the benchmark, while the model can be readily trained from scratch.



### Diffusion Models for Adversarial Purification
- **Arxiv ID**: http://arxiv.org/abs/2205.07460v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07460v1)
- **Published**: 2022-05-16 06:03:00+00:00
- **Updated**: 2022-05-16 06:03:00+00:00
- **Authors**: Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, Anima Anandkumar
- **Comment**: ICML 2022
- **Journal**: None
- **Summary**: Adversarial purification refers to a class of defense methods that remove adversarial perturbations using a generative model. These methods do not make assumptions on the form of attack and the classification model, and thus can defend pre-existing classifiers against unseen threats. However, their performance currently falls behind adversarial training methods. In this work, we propose DiffPure that uses diffusion models for adversarial purification: Given an adversarial example, we first diffuse it with a small amount of noise following a forward diffusion process, and then recover the clean image through a reverse generative process. To evaluate our method against strong adaptive attacks in an efficient and scalable way, we propose to use the adjoint method to compute full gradients of the reverse generative process. Extensive experiments on three image datasets including CIFAR-10, ImageNet and CelebA-HQ with three classifier architectures including ResNet, WideResNet and ViT demonstrate that our method achieves the state-of-the-art results, outperforming current adversarial training and adversarial purification methods, often by a large margin. Project page: https://diffpure.github.io.



### Robust Representation via Dynamic Feature Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2205.07466v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.07466v1)
- **Published**: 2022-05-16 06:22:15+00:00
- **Updated**: 2022-05-16 06:22:15+00:00
- **Authors**: Haozhe Liu, Haoqin Ji, Yuexiang Li, Nanjun He, Haoqian Wu, Feng Liu, Linlin Shen, Yefeng Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural network (CNN) based models are vulnerable to the adversarial attacks. One of the possible reasons is that the embedding space of CNN based model is sparse, resulting in a large space for the generation of adversarial samples. In this study, we propose a method, denoted as Dynamic Feature Aggregation, to compress the embedding space with a novel regularization. Particularly, the convex combination between two samples are regarded as the pivot for aggregation. In the embedding space, the selected samples are guided to be similar to the representation of the pivot. On the other side, to mitigate the trivial solution of such regularization, the last fully-connected layer of the model is replaced by an orthogonal classifier, in which the embedding codes for different classes are processed orthogonally and separately. With the regularization and orthogonal classifier, a more compact embedding space can be obtained, which accordingly improves the model robustness against adversarial attacks. An averaging accuracy of 56.91% is achieved by our method on CIFAR-10 against various attack methods, which significantly surpasses a solid baseline (Mixup) by a margin of 37.31%. More surprisingly, empirical results show that, the proposed method can also achieve the state-of-the-art performance for out-of-distribution (OOD) detection, due to the learned compact feature space. An F1 score of 0.937 is achieved by the proposed method, when adopting CIFAR-10 as in-distribution (ID) dataset and LSUN as OOD dataset. Code is available at https://github.com/HaozheLiu-ST/DynamicFeatureAggregation.



### Fusing Multiscale Texture and Residual Descriptors for Multilevel 2D Barcode Rebroadcasting Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.11242v1
- **DOI**: 10.1109/WIFS53200.2021.9648391
- **Categories**: **cs.CV**, cs.AI, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2205.11242v1)
- **Published**: 2022-05-16 06:26:20+00:00
- **Updated**: 2022-05-16 06:26:20+00:00
- **Authors**: Anselmo Ferreira, Changcheng Chen, Mauro Barni
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, 2D barcodes have been widely used for advertisement, mobile payment, and product authentication. However, in applications related to product authentication, an authentic 2D barcode can be illegally copied and attached to a counterfeited product in such a way to bypass the authentication scheme. In this paper, we employ a proprietary 2D barcode pattern and use multimedia forensics methods to analyse the scanning and printing artefacts resulting from the copy (rebroadcasting) attack. A diverse and complementary feature set is proposed to quantify the barcode texture distortions introduced during the illegal copying process. The proposed features are composed of global and local descriptors, which characterize the multi-scale texture appearance and the points of interest distribution, respectively. The proposed descriptors are compared against some existing texture descriptors and deep learning-based approaches under various scenarios, such as cross-datasets and cross-size. Experimental results highlight the practicality of the proposed method in real-world settings.



### Adaptive Convolutional Dictionary Network for CT Metal Artifact Reduction
- **Arxiv ID**: http://arxiv.org/abs/2205.07471v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07471v2)
- **Published**: 2022-05-16 06:49:36+00:00
- **Updated**: 2022-06-16 09:06:55+00:00
- **Authors**: Hong Wang, Yuexiang Li, Deyu Meng, Yefeng Zheng
- **Comment**: https://github.com/hongwang01/ACDNet
- **Journal**: the 31st International Joint Conference on Artificial Intelligence
  2022
- **Summary**: Inspired by the great success of deep neural networks, learning-based methods have gained promising performances for metal artifact reduction (MAR) in computed tomography (CT) images. However, most of the existing approaches put less emphasis on modelling and embedding the intrinsic prior knowledge underlying this specific MAR task into their network designs. Against this issue, we propose an adaptive convolutional dictionary network (ACDNet), which leverages both model-based and learning-based methods. Specifically, we explore the prior structures of metal artifacts, e.g., non-local repetitive streaking patterns, and encode them as an explicit weighted convolutional dictionary model. Then, a simple-yet-effective algorithm is carefully designed to solve the model. By unfolding every iterative substep of the proposed algorithm into a network module, we explicitly embed the prior structure into a deep network, \emph{i.e.,} a clear interpretability for the MAR task. Furthermore, our ACDNet can automatically learn the prior for artifact-free CT images via training data and adaptively adjust the representation kernels for each input CT image based on its content. Hence, our method inherits the clear interpretability of model-based methods and maintains the powerful representation ability of learning-based methods. Comprehensive experiments executed on synthetic and clinical datasets show the superiority of our ACDNet in terms of effectiveness and model generalization. {\color{blue}{{\textit{Code is available at {\url{https://github.com/hongwang01/ACDNet}}.}}}}



### Frequency selective extrapolation with residual filtering for image error concealment
- **Arxiv ID**: http://arxiv.org/abs/2205.07476v1
- **DOI**: 10.1109/ICASSP.2014.6853944
- **Categories**: **cs.CV**, I.4.3; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2205.07476v1)
- **Published**: 2022-05-16 06:59:36+00:00
- **Updated**: 2022-05-16 06:59:36+00:00
- **Authors**: Ján Koloda, Jürgen Seiler, André Kaup, Victoria Sánchez, Antonio M. Peinado
- **Comment**: None
- **Journal**: 2014 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), 2014, pp. 1976-1980
- **Summary**: The purpose of signal extrapolation is to estimate unknown signal parts from known samples. This task is especially important for error concealment in image and video communication. For obtaining a high quality reconstruction, assumptions have to be made about the underlying signal in order to solve this underdetermined problem. Among existent reconstruction algorithms, frequency selective extrapolation (FSE) achieves high performance by assuming that image signals can be sparsely represented in the frequency domain. However, FSE does not take into account the low-pass behaviour of natural images. In this paper, we propose a modified FSE that takes this prior knowledge into account for the modelling, yielding significant PSNR gains.



### Manifold Characteristics That Predict Downstream Task Performance
- **Arxiv ID**: http://arxiv.org/abs/2205.07477v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07477v1)
- **Published**: 2022-05-16 06:59:51+00:00
- **Updated**: 2022-05-16 06:59:51+00:00
- **Authors**: Ruan van der Merwe, Gregory Newman, Etienne Barnard
- **Comment**: Currently under review
- **Journal**: None
- **Summary**: Pretraining methods are typically compared by evaluating the accuracy of linear classifiers, transfer learning performance, or visually inspecting the representation manifold's (RM) lower-dimensional projections. We show that the differences between methods can be understood more clearly by investigating the RM directly, which allows for a more detailed comparison. To this end, we propose a framework and new metric to measure and compare different RMs. We also investigate and report on the RM characteristics for various pretraining methods. These characteristics are measured by applying sequentially larger local alterations to the input data, using white noise injections and Projected Gradient Descent (PGD) adversarial attacks, and then tracking each datapoint. We calculate the total distance moved for each datapoint and the relative change in distance between successive alterations. We show that self-supervised methods learn an RM where alterations lead to large but constant size changes, indicating a smoother RM than fully supervised methods. We then combine these measurements into one metric, the Representation Manifold Quality Metric (RMQM), where larger values indicate larger and less variable step sizes, and show that RMQM correlates positively with performance on downstream tasks.



### Topologically Persistent Features-based Object Recognition in Cluttered Indoor Environments
- **Arxiv ID**: http://arxiv.org/abs/2205.07479v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.07479v1)
- **Published**: 2022-05-16 07:01:16+00:00
- **Updated**: 2022-05-16 07:01:16+00:00
- **Authors**: Ekta U. Samani, Ashis G. Banerjee
- **Comment**: Accepted for presentation in the IEEE International Conference on
  Robotics and Automation (ICRA) 2022 Workshop on Robotic Perception and
  Mapping: Emerging Techniques
- **Journal**: None
- **Summary**: Recognition of occluded objects in unseen indoor environments is a challenging problem for mobile robots. This work proposes a new slicing-based topological descriptor that captures the 3D shape of object point clouds to address this challenge. It yields similarities between the descriptors of the occluded and the corresponding unoccluded objects, enabling object unity-based recognition using a library of trained models. The descriptor is obtained by partitioning an object's point cloud into multiple 2D slices and constructing filtrations (nested sequences of simplicial complexes) on the slices to mimic further slicing of the slices, thereby capturing detailed shapes through persistent homology-generated features. We use nine different sequences of cluttered scenes from a benchmark dataset for performance evaluation. Our method outperforms two state-of-the-art deep learning-based point cloud classification methods, namely, DGCNN and SimpleView.



### Residual Local Feature Network for Efficient Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2205.07514v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07514v1)
- **Published**: 2022-05-16 08:46:34+00:00
- **Updated**: 2022-05-16 08:46:34+00:00
- **Authors**: Fangyuan Kong, Mingxi Li, Songwei Liu, Ding Liu, Jingwen He, Yang Bai, Fangmin Chen, Lean Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning based approaches has achieved great performance in single image super-resolution (SISR). However, recent advances in efficient super-resolution focus on reducing the number of parameters and FLOPs, and they aggregate more powerful features by improving feature utilization through complex layer connection strategies. These structures may not be necessary to achieve higher running speed, which makes them difficult to be deployed to resource-constrained devices. In this work, we propose a novel Residual Local Feature Network (RLFN). The main idea is using three convolutional layers for residual local feature learning to simplify feature aggregation, which achieves a good trade-off between model performance and inference time. Moreover, we revisit the popular contrastive loss and observe that the selection of intermediate features of its feature extractor has great influence on the performance. Besides, we propose a novel multi-stage warm-start training strategy. In each stage, the pre-trained weights from previous stages are utilized to improve the model performance. Combined with the improved contrastive loss and training strategy, the proposed RLFN outperforms all the state-of-the-art efficient image SR models in terms of runtime while maintaining both PSNR and SSIM for SR. In addition, we won the first place in the runtime track of the NTIRE 2022 efficient super-resolution challenge. Code will be available at https://github.com/fyan111/RLFN.



### SQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization
- **Arxiv ID**: http://arxiv.org/abs/2205.07547v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07547v2)
- **Published**: 2022-05-16 09:49:37+00:00
- **Updated**: 2022-06-09 12:46:05+00:00
- **Authors**: Yuhta Takida, Takashi Shibuya, WeiHsiang Liao, Chieh-Hsin Lai, Junki Ohmura, Toshimitsu Uesaka, Naoki Murata, Shusuke Takahashi, Toshiyuki Kumakura, Yuki Mitsufuji
- **Comment**: 25 pages with 10 figures, accepted for publication in ICML 2022 (Our
  code is available at https://github.com/sony/sqvae)
- **Journal**: None
- **Summary**: One noted issue of vector-quantized variational autoencoder (VQ-VAE) is that the learned discrete representation uses only a fraction of the full capacity of the codebook, also known as codebook collapse. We hypothesize that the training scheme of VQ-VAE, which involves some carefully designed heuristics, underlies this issue. In this paper, we propose a new training scheme that extends the standard VAE via novel stochastic dequantization and quantization, called stochastically quantized variational autoencoder (SQ-VAE). In SQ-VAE, we observe a trend that the quantization is stochastic at the initial stage of the training but gradually converges toward a deterministic quantization, which we call self-annealing. Our experiments show that SQ-VAE improves codebook utilization without using common heuristics. Furthermore, we empirically show that SQ-VAE is superior to VAE and VQ-VAE in vision- and speech-related tasks.



### A Neuro-Symbolic ASP Pipeline for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2205.07548v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07548v1)
- **Published**: 2022-05-16 09:50:37+00:00
- **Updated**: 2022-05-16 09:50:37+00:00
- **Authors**: Thomas Eiter, Nelson Higuera, Johannes Oetsch, Michael Pritz
- **Comment**: Paper presented at the 38th International Conference on Logic
  Programming (ICLP 2022), 15 pages
- **Journal**: None
- **Summary**: We present a neuro-symbolic visual question answering (VQA) pipeline for CLEVR, which is a well-known dataset that consists of pictures showing scenes with objects and questions related to them. Our pipeline covers (i) training neural networks for object classification and bounding-box prediction of the CLEVR scenes, (ii) statistical analysis on the distribution of prediction values of the neural networks to determine a threshold for high-confidence predictions, and (iii) a translation of CLEVR questions and network predictions that pass confidence thresholds into logic programs so that we can compute the answers using an ASP solver. By exploiting choice rules, we consider deterministic and non-deterministic scene encodings. Our experiments show that the non-deterministic scene encoding achieves good results even if the neural networks are trained rather poorly in comparison with the deterministic approach. This is important for building robust VQA systems if network predictions are less-than perfect. Furthermore, we show that restricting non-determinism to reasonable choices allows for more efficient implementations in comparison with related neuro-symbolic approaches without loosing much accuracy. This work is under consideration for acceptance in TPLP.



### An Effective Transformer-based Solution for RSNA Intracranial Hemorrhage Detection Competition
- **Arxiv ID**: http://arxiv.org/abs/2205.07556v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07556v3)
- **Published**: 2022-05-16 10:05:39+00:00
- **Updated**: 2022-06-07 03:54:32+00:00
- **Authors**: Fangxin Shang, Siqi Wang, Xiaorong Wang, Yehui Yang
- **Comment**: None
- **Journal**: None
- **Summary**: We present an effective method for Intracranial Hemorrhage Detection (IHD) which exceeds the performance of the winner solution in RSNA-IHD competition (2019). Meanwhile, our model only takes quarter parameters and ten percent FLOPs compared to the winner's solution. The IHD task needs to predict the hemorrhage category of each slice for the input brain CT. We review the top-5 solutions for the IHD competition held by the Radiological Society of North America(RSNA) in 2019. Nearly all the top solutions rely on 2D convolutional networks and sequential models (Bidirectional GRU or LSTM) to extract intra-slice and inter-slice features, respectively. All the top solutions enhance the performance by leveraging the model ensemble, and the model number varies from 7 to 31. In the past years, since much progress has been made in the computer vision regime especially Transformer-based models, we introduce the Transformer-based techniques to extract the features in both intra-slice and inter-slice views for IHD tasks. Additionally, a semi-supervised method is embedded into our workflow to further improve the performance. The code is available in the manuscript.



### Weakly-supervised Biomechanically-constrained CT/MRI Registration of the Spine
- **Arxiv ID**: http://arxiv.org/abs/2205.07568v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.07568v1)
- **Published**: 2022-05-16 10:59:55+00:00
- **Updated**: 2022-05-16 10:59:55+00:00
- **Authors**: Bailiang Jian, Mohammad Farid Azampour, Francesca De Benetti, Johannes Oberreuter, Christina Bukas, Alexandra S. Gersing, Sarah C. Foreman, Anna-Sophia Dietrich, Jon Rischewski, Jan S. Kirschke, Nassir Navab, Thomas Wendler
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: CT and MRI are two of the most informative modalities in spinal diagnostics and treatment planning. CT is useful when analysing bony structures, while MRI gives information about the soft tissue. Thus, fusing the information of both modalities can be very beneficial. Registration is the first step for this fusion. While the soft tissues around the vertebra are deformable, each vertebral body is constrained to move rigidly. We propose a weakly-supervised deep learning framework that preserves the rigidity and the volume of each vertebra while maximizing the accuracy of the registration. To achieve this goal, we introduce anatomy-aware losses for training the network. We specifically design these losses to depend only on the CT label maps since automatic vertebra segmentation in CT gives more accurate results contrary to MRI. We evaluate our method on an in-house dataset of 167 patients. Our results show that adding the anatomy-aware losses increases the plausibility of the inferred transformation while keeping the accuracy untouched.



### An automatic pipeline for atlas-based fetal and neonatal brain segmentation and analysis
- **Arxiv ID**: http://arxiv.org/abs/2205.07575v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2205.07575v1)
- **Published**: 2022-05-16 11:15:26+00:00
- **Updated**: 2022-05-16 11:15:26+00:00
- **Authors**: Urru, Andrea, Nakaki, Ayako, Benkarim, Oualid, Crovetto, Francesca, Segales, Laura, Comte, Valentin, Hahner, Nadine, Eixarch, Elisenda, Gratacós, Eduard, Crispi, Fàtima, Piella, Gemma, González Ballester, Miguel A
- **Comment**: None
- **Journal**: None
- **Summary**: The automatic segmentation of perinatal brain structures in magnetic resonance imaging (MRI) is of utmost importance for the study of brain growth and related complications. While different methods exist for adult and pediatric MRI data, there is a lack for automatic tools for the analysis of perinatal imaging. In this work, a new pipeline for fetal and neonatal segmentation has been developed. We also report the creation of two new fetal atlases, and their use within the pipeline for atlas-based segmentation, based on novel registration methods. The pipeline is also able to extract cortical and pial surfaces and compute features, such as curvature, thickness, sulcal depth, and local gyrification index. Results show that the introduction of the new templates together with our segmentation strategy leads to accurate results when compared to expert annotations, as well as better performances when compared to a reference pipeline (developing Human Connectome Project (dHCP)), for both early and late-onset fetal brains.



### Noise-Tolerant Learning for Audio-Visual Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.07611v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.07611v2)
- **Published**: 2022-05-16 12:14:03+00:00
- **Updated**: 2022-05-20 10:10:55+00:00
- **Authors**: Haochen Han, Qinghua Zheng, Minnan Luo, Kaiyao Miao, Feng Tian, Yan Chen
- **Comment**: This work is going to be submitted to the IEEE for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible
- **Journal**: None
- **Summary**: Recently, video recognition is emerging with the help of multi-modal learning, which focuses on integrating multiple modalities to improve the performance or robustness of a model. Although various multi-modal learning methods have been proposed and offer remarkable recognition results, almost all of these methods rely on high-quality manual annotations and assume that modalities among multi-modal data provide relevant semantic information. Unfortunately, most widely used video datasets are collected from the Internet and inevitably contain noisy labels and noisy correspondence. To solve this problem, we use the audio-visual action recognition task as a proxy and propose a noise-tolerant learning framework to find anti-interference model parameters to both noisy labels and noisy correspondence. Our method consists of two phases and aims to rectify noise by the inherent correlation between modalities. A noise-tolerant contrastive training phase is performed first to learn robust model parameters unaffected by the noisy labels. To reduce the influence of noisy correspondence, we propose a cross-modal noise estimation component to adjust the consistency between different modalities. Since the noisy correspondence existed at the instance level, a category-level contrastive loss is proposed to further alleviate the interference of noisy correspondence. Then in the hybrid supervised training phase, we calculate the distance metric among features to obtain corrected labels, which are used as complementary supervision. In addition, we investigate the noisy correspondence in real-world datasets and conduct comprehensive experiments with synthetic and real noise data. The results verify the advantageous performance of our method compared to state-of-the-art methods.



### Scalable Vehicle Re-Identification via Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2205.07613v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07613v1)
- **Published**: 2022-05-16 12:14:42+00:00
- **Updated**: 2022-05-16 12:14:42+00:00
- **Authors**: Pirazh Khorramshahi, Vineet Shenoy, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: As Computer Vision technologies become more mature for intelligent transportation applications, it is time to ask how efficient and scalable they are for large-scale and real-time deployment. Among these technologies is Vehicle Re-Identification which is one of the key elements in city-scale vehicle analytics systems. Many state-of-the-art solutions for vehicle re-id mostly focus on improving the accuracy on existing re-id benchmarks and often ignore computational complexity. To balance the demands of accuracy and computational efficiency, in this work we propose a simple yet effective hybrid solution empowered by self-supervised training which only uses a single network during inference time and is free of intricate and computation-demanding add-on modules often seen in state-of-the-art approaches. Through extensive experiments, we show our approach, termed Self-Supervised and Boosted VEhicle Re-Identification (SSBVER), is on par with state-of-the-art alternatives in terms of accuracy without introducing any additional overhead during deployment. Additionally we show that our approach, generalizes to different backbone architectures which facilitates various resource constraints and consistently results in a significant accuracy boost.



### PUCK: Parallel Surface and Convolution-kernel Tracking for Event-Based Cameras
- **Arxiv ID**: http://arxiv.org/abs/2205.07657v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07657v1)
- **Published**: 2022-05-16 13:23:52+00:00
- **Updated**: 2022-05-16 13:23:52+00:00
- **Authors**: Luna Gava, Marco Monforte, Massimiliano Iacono, Chiara Bartolozzi, Arren Glover
- **Comment**: submitted to IROS 2022
- **Journal**: None
- **Summary**: Low latency and accuracy are fundamental requirements when vision is integrated in robots for high-speed interaction with targets, since they affect system reliability and stability. In such a scenario, the choice of the sensor and algorithms is important for the entire control loop. The technology of event-cameras can guarantee fast visual sensing in dynamic environments, but requires a tracking algorithm that can keep up with the high data rate induced by the robot ego-motion while maintaining accuracy and robustness to distractors. In this paper, we introduce a novel tracking method that leverages the Exponential Reduced Ordinal Surface (EROS) data representation to decouple event-by-event processing and tracking computation. The latter is performed using convolution kernels to detect and follow a circular target moving on a plane. To benchmark state-of-the-art event-based tracking, we propose the task of tracking the air hockey puck sliding on a surface, with the future aim of controlling the iCub robot to reach the target precisely and on time. Experimental results demonstrate that our algorithm achieves the best compromise between low latency and tracking accuracy both when the robot is still and when moving.



### BBDM: Image-to-image Translation with Brownian Bridge Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2205.07680v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07680v2)
- **Published**: 2022-05-16 13:47:02+00:00
- **Updated**: 2023-03-23 08:29:12+00:00
- **Authors**: Bo Li, Kaitao Xue, Bin Liu, Yu-Kun Lai
- **Comment**: 18 pages, 13 figures
- **Journal**: None
- **Summary**: Image-to-image translation is an important and challenging problem in computer vision and image processing. Diffusion models (DM) have shown great potentials for high-quality image synthesis, and have gained competitive performance on the task of image-to-image translation. However, most of the existing diffusion models treat image-to-image translation as conditional generation processes, and suffer heavily from the gap between distinct domains. In this paper, a novel image-to-image translation method based on the Brownian Bridge Diffusion Model (BBDM) is proposed, which models image-to-image translation as a stochastic Brownian bridge process, and learns the translation between two domains directly through the bidirectional diffusion process rather than a conditional generation process. To the best of our knowledge, it is the first work that proposes Brownian Bridge diffusion process for image-to-image translation. Experimental results on various benchmarks demonstrate that the proposed BBDM model achieves competitive performance through both visual inspection and measurable metrics.



### CONSENT: Context Sensitive Transformer for Bold Words Classification
- **Arxiv ID**: http://arxiv.org/abs/2205.07683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07683v1)
- **Published**: 2022-05-16 13:50:33+00:00
- **Updated**: 2022-05-16 13:50:33+00:00
- **Authors**: Ionut-Catalin Sandu, Daniel Voinea, Alin-Ionut Popa
- **Comment**: None
- **Journal**: None
- **Summary**: We present CONSENT, a simple yet effective CONtext SENsitive Transformer framework for context-dependent object classification within a fully-trainable end-to-end deep learning pipeline. We exemplify the proposed framework on the task of bold words detection proving state-of-the-art results. Given an image containing text of unknown font-types (e.g. Arial, Calibri, Helvetica), unknown language, taken under various degrees of illumination, angle distortion and scale variation, we extract all the words and learn a context-dependent binary classification (i.e. bold versus non-bold) using an end-to-end transformer-based neural network ensemble. To prove the extensibility of our framework, we demonstrate competitive results against state-of-the-art for the game of rock-paper-scissors by training the model to determine the winner given a sequence with $2$ pictures depicting hand poses.



### Real-time semantic segmentation on FPGAs for autonomous vehicles with hls4ml
- **Arxiv ID**: http://arxiv.org/abs/2205.07690v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.LG, physics.ins-det, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2205.07690v1)
- **Published**: 2022-05-16 13:55:16+00:00
- **Updated**: 2022-05-16 13:55:16+00:00
- **Authors**: Nicolò Ghielmetti, Vladimir Loncar, Maurizio Pierini, Marcel Roed, Sioni Summers, Thea Aarrestad, Christoffer Petersson, Hampus Linander, Jennifer Ngadiuba, Kelvin Lin, Philip Harris
- **Comment**: 11 pages, 6 tables, 5 figures
- **Journal**: None
- **Summary**: In this paper, we investigate how field programmable gate arrays can serve as hardware accelerators for real-time semantic segmentation tasks relevant for autonomous driving. Considering compressed versions of the ENet convolutional neural network architecture, we demonstrate a fully-on-chip deployment with a latency of 4.9 ms per image, using less than 30% of the available resources on a Xilinx ZCU102 evaluation board. The latency is reduced to 3 ms per image when increasing the batch size to ten, corresponding to the use case where the autonomous vehicle receives inputs from multiple cameras simultaneously. We show, through aggressive filter reduction and heterogeneous quantization-aware training, and an optimized implementation of convolutional layers, that the power consumption and resource utilization can be significantly reduced while maintaining accuracy on the Cityscapes dataset.



### Exploring Diversity-based Active Learning for 3D Object Detection in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2205.07708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07708v1)
- **Published**: 2022-05-16 14:21:30+00:00
- **Updated**: 2022-05-16 14:21:30+00:00
- **Authors**: Zhihao Liang, Xun Xu, Shengheng Deng, Lile Cai, Tao Jiang, Kui Jia
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection has recently received much attention due to its great potential in autonomous vehicle (AV). The success of deep learning based object detectors relies on the availability of large-scale annotated datasets, which is time-consuming and expensive to compile, especially for 3D bounding box annotation. In this work, we investigate diversity-based active learning (AL) as a potential solution to alleviate the annotation burden. Given limited annotation budget, only the most informative frames and objects are automatically selected for human to annotate. Technically, we take the advantage of the multimodal information provided in an AV dataset, and propose a novel acquisition function that enforces spatial and temporal diversity in the selected samples. We benchmark the proposed method against other AL strategies under realistic annotation cost measurement, where the realistic costs for annotating a frame and a 3D bounding box are both taken into consideration. We demonstrate the effectiveness of the proposed method on the nuScenes dataset and show that it outperforms existing AL strategies significantly.



### Prediction of stent under-expansion in calcified coronary arteries using machine-learning on intravascular optical coherence tomography
- **Arxiv ID**: http://arxiv.org/abs/2205.10354v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.10354v1)
- **Published**: 2022-05-16 14:28:51+00:00
- **Updated**: 2022-05-16 14:28:51+00:00
- **Authors**: Yazan Gharaibeh, Juhwan Lee, Vladislav N. Zimin, Chaitanya Kolluru, Luis A. P. Dallan, Gabriel T. R. Pereira, Armando Vergara-Martel, Justin N. Kim, Ammar Hoori, Pengfei Dong, Peshala T. Gamage, Linxia Gu, Hiram G. Bezerra, Sadeer Al-Kindi, David L. Wilson
- **Comment**: 25 pages, 7 figures, 1 table, 6 supplemental figures, 3 supplemental
  tables
- **Journal**: None
- **Summary**: BACKGROUND Careful evaluation of the risk of stent under-expansions before the intervention will aid treatment planning, including the application of a pre-stent plaque modification strategy.   OBJECTIVES It remains challenging to achieve a proper stent expansion in the presence of severely calcified coronary lesions. Building on our work in deep learning segmentation, we created an automated machine learning approach that uses lesion attributes to predict stent under-expansion from pre-stent images, suggesting the need for plaque modification.   METHODS Pre- and post-stent intravascular optical coherence tomography image data were obtained from 110 coronary lesions. Lumen and calcifications in pre-stent images were segmented using deep learning, and numerous features per lesion were extracted. We analyzed stent expansion along the lesion, enabling frame, segmental, and whole-lesion analyses. We trained regression models to predict the poststent lumen area and then to compute the stent expansion index (SEI). Stents with an SEI < or >/= 80% were classified as "under-expanded" and "well-expanded," respectively.   RESULTS Best performance (root-mean-square-error = 0.04+/-0.02 mm2, r = 0.94+/-0.04, p < 0.0001) was achieved when we used features from both the lumen and calcification to train a Gaussian regression model for a segmental analysis over a segment length of 31 frames. Under-expansion classification results (AUC=0.85+/-0.02) were significantly improved over other approaches.   CONCLUSIONS We used calcifications and lumen features to identify lesions at risk of stent under-expansion. Results suggest that the use of pre-stent images can inform physicians of the need to apply plaque modification approaches.



### Towards Space-to-Ground Data Availability for Agriculture Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2205.07721v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.07721v1)
- **Published**: 2022-05-16 14:35:48+00:00
- **Updated**: 2022-05-16 14:35:48+00:00
- **Authors**: George Choumos, Alkiviadis Koukos, Vasileios Sitokonstantinou, Charalampos Kontoes
- **Comment**: Has been accepted for publication in IEEE IVMSP 2022:
  https://2022.ivmsp.org/ Specifically in the special session "Multimodal
  Analysis, Fusion and Retrieval of satellite images":
  https://2022.ivmsp.org/wp-content/uploads/2022/02/IVMSP2022-CFP-SpecialSession-4-MultiSat-rev.pdf
- **Journal**: None
- **Summary**: The recent advances in machine learning and the availability of free and open big Earth data (e.g., Sentinel missions), which cover large areas with high spatial and temporal resolution, have enabled many agriculture monitoring applications. One example is the control of subsidy allocations of the Common Agricultural Policy (CAP). Advanced remote sensing systems have been developed towards the large-scale evidence-based monitoring of the CAP. Nevertheless, the spatial resolution of satellite images is not always adequate to make accurate decisions for all fields. In this work, we introduce the notion of space-to-ground data availability, i.e., from the satellite to the field, in an attempt to make the best out of the complementary characteristics of the different sources. We present a space-to-ground dataset that contains Sentinel-1 radar and Sentinel-2 optical image time-series, as well as street-level images from the crowdsourcing platform Mapillary, for grassland fields in the area of Utrecht for 2017. The multifaceted utility of our dataset is showcased through the downstream task of grassland classification. We train machine and deep learning algorithms on these different data domains and highlight the potential of fusion techniques towards increasing the reliability of decisions.



### Pest presence prediction using interpretable machine learning
- **Arxiv ID**: http://arxiv.org/abs/2205.07723v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.07723v1)
- **Published**: 2022-05-16 14:40:03+00:00
- **Updated**: 2022-05-16 14:40:03+00:00
- **Authors**: Ornela Nanushi, Vasileios Sitokonstantinou, Ilias Tsoumas, Charalampos Kontoes
- **Comment**: This work has been accepted for publication in IEEE 14th Image,
  Video, and Multidimensional Signal Processing Workshop (IVMSP 2022)
- **Journal**: None
- **Summary**: Helicoverpa Armigera, or cotton bollworm, is a serious insect pest of cotton crops that threatens the yield and the quality of lint. The timely knowledge of the presence of the insects in the field is crucial for effective farm interventions. Meteo-climatic and vegetation conditions have been identified as key drivers of crop pest abundance. In this work, we applied an interpretable classifier, i.e., Explainable Boosting Machine, which uses earth observation vegetation indices, numerical weather predictions and insect trap catches to predict the onset of bollworm harmfulness in cotton fields in Greece. The glass-box nature of our approach provides significant insight on the main drivers of the model and the interactions among them. Model interpretability adds to the trustworthiness of our approach and therefore its potential for rapid uptake and context-based implementation in operational farm management scenarios. Our results are satisfactory and the importance of drivers, through our analysis on global and local explainability, is in accordance with the literature.



### A Data Cube of Big Satellite Image Time-Series for Agriculture Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2205.07752v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.07752v1)
- **Published**: 2022-05-16 15:26:23+00:00
- **Updated**: 2022-05-16 15:26:23+00:00
- **Authors**: Thanassis Drivas, Vasileios Sitokonstantinou, Iason Tsardanidis, Alkiviadis Koukos, Charalampos Kontoes, Vassilia Karathanassi
- **Comment**: This work has been accepted for publication in IEEE 14th Image,
  Video, and Multidimensional Signal Processing Workshop (IVMSP 2022)
- **Journal**: None
- **Summary**: The modernization of the Common Agricultural Policy (CAP) requires the large scale and frequent monitoring of agricultural land. Towards this direction, the free and open satellite data (i.e., Sentinel missions) have been extensively used as the sources for the required high spatial and temporal resolution Earth observations. Nevertheless, monitoring the CAP at large scales constitutes a big data problem and puts a strain on CAP paying agencies that need to adapt fast in terms of infrastructure and know-how. Hence, there is a need for efficient and easy-to-use tools for the acquisition, storage, processing and exploitation of big satellite data. In this work, we present the Agriculture monitoring Data Cube (ADC), which is an automated, modular, end-to-end framework for discovering, pre-processing and indexing optical and Synthetic Aperture Radar (SAR) images into a multidimensional cube. We also offer a set of powerful tools on top of the ADC, including i) the generation of analysis-ready feature spaces of big satellite data to feed downstream machine learning tasks and ii) the support of Satellite Image Time-Series (SITS) analysis via services pertinent to the monitoring of the CAP (e.g., detecting trends and events, monitoring the growth status etc.). The knowledge extracted from the SITS analyses and the machine learning tasks returns to the data cube, building scalable country-specific knowledge bases that can efficiently answer complex and multi-faceted geospatial queries.



### FvOR: Robust Joint Shape and Pose Optimization for Few-view Object Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2205.07763v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07763v1)
- **Published**: 2022-05-16 15:39:27+00:00
- **Updated**: 2022-05-16 15:39:27+00:00
- **Authors**: Zhenpei Yang, Zhile Ren, Miguel Angel Bautista, Zaiwei Zhang, Qi Shan, Qixing Huang
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Reconstructing an accurate 3D object model from a few image observations remains a challenging problem in computer vision. State-of-the-art approaches typically assume accurate camera poses as input, which could be difficult to obtain in realistic settings. In this paper, we present FvOR, a learning-based object reconstruction method that predicts accurate 3D models given a few images with noisy input poses. The core of our approach is a fast and robust multi-view reconstruction algorithm to jointly refine 3D geometry and camera pose estimation using learnable neural network modules. We provide a thorough benchmark of state-of-the-art approaches for this problem on ShapeNet. Our approach achieves best-in-class results. It is also two orders of magnitude faster than the recent optimization-based approach IDR. Our code is released at \url{https://github.com/zhenpeiyang/FvOR/}



### Data-Driven Interpolation for Super-Scarce X-Ray Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2205.07888v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2205.07888v1)
- **Published**: 2022-05-16 15:42:41+00:00
- **Updated**: 2022-05-16 15:42:41+00:00
- **Authors**: Emilien Valat, Katayoun Farrahi, Thomas Blumensath
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of reconstructing X-Ray tomographic images from scarce measurements by interpolating missing acquisitions using a self-supervised approach. To do so, we train shallow neural networks to combine two neighbouring acquisitions into an estimated measurement at an intermediate angle. This procedure yields an enhanced sequence of measurements that can be reconstructed using standard methods, or further enhanced using regularisation approaches.   Unlike methods that improve the sequence of acquisitions using an initial deterministic interpolation followed by machine-learning enhancement, we focus on inferring one measurement at once. This allows the method to scale to 3D, the computation to be faster and crucially, the interpolation to be significantly better than the current methods, when they exist. We also establish that a sequence of measurements must be processed as such, rather than as an image or a volume. We do so by comparing interpolation and up-sampling methods, and find that the latter significantly under-perform.   We compare the performance of the proposed method against deterministic interpolation and up-sampling procedures and find that it outperforms them, even when used jointly with a state-of-the-art projection-data enhancement approach using machine-learning. These results are obtained for 2D and 3D imaging, on large biomedical datasets, in both projection space and image space.



### Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised Semantic Segmentation and Localization
- **Arxiv ID**: http://arxiv.org/abs/2205.07839v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.07839v1)
- **Published**: 2022-05-16 17:47:44+00:00
- **Updated**: 2022-05-16 17:47:44+00:00
- **Authors**: Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, Andrea Vedaldi
- **Comment**: Published at CVPR 2022. Project Page:
  https://lukemelas.github.io/deep-spectral-segmentation
- **Journal**: None
- **Summary**: Unsupervised localization and segmentation are long-standing computer vision challenges that involve decomposing an image into semantically-meaningful segments without any labeled data. These tasks are particularly interesting in an unsupervised setting due to the difficulty and cost of obtaining dense image annotations, but existing unsupervised approaches struggle with complex scenes containing multiple objects. Differently from existing methods, which are purely based on deep learning, we take inspiration from traditional spectral segmentation methods by reframing image decomposition as a graph partitioning problem. Specifically, we examine the eigenvectors of the Laplacian of a feature affinity matrix from self-supervised networks. We find that these eigenvectors already decompose an image into meaningful segments, and can be readily used to localize objects in a scene. Furthermore, by clustering the features associated with these segments across a dataset, we can obtain well-delineated, nameable regions, i.e. semantic segmentations. Experiments on complex datasets (Pascal VOC, MS-COCO) demonstrate that our simple spectral method outperforms the state-of-the-art in unsupervised localization and segmentation by a significant margin. Furthermore, our method can be readily used for a variety of complex image editing tasks, such as background removal and compositing.



### Guess What Moves: Unsupervised Video and Image Segmentation by Anticipating Motion
- **Arxiv ID**: http://arxiv.org/abs/2205.07844v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07844v2)
- **Published**: 2022-05-16 17:55:34+00:00
- **Updated**: 2022-10-13 18:01:37+00:00
- **Authors**: Subhabrata Choudhury, Laurynas Karazija, Iro Laina, Andrea Vedaldi, Christian Rupprecht
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Motion, measured via optical flow, provides a powerful cue to discover and learn objects in images and videos. However, compared to using appearance, it has some blind spots, such as the fact that objects become invisible if they do not move. In this work, we propose an approach that combines the strengths of motion-based and appearance-based segmentation. We propose to supervise an image segmentation network with the pretext task of predicting regions that are likely to contain simple motion patterns, and thus likely to correspond to objects. As the model only uses a single image as input, we can apply it in two settings: unsupervised video segmentation, and unsupervised image segmentation. We achieve state-of-the-art results for videos, and demonstrate the viability of our approach on still images containing novel objects. Additionally we experiment with different motion models and optical flow backbones and find the method to be robust to these change. Project page and code available at https://www.robots.ox.ac.uk/~vgg/research/gwm.



### Deep Apprenticeship Learning for Playing Games
- **Arxiv ID**: http://arxiv.org/abs/2205.07959v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, I.2.8; I.2.10; I.2.6; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2205.07959v1)
- **Published**: 2022-05-16 19:52:45+00:00
- **Updated**: 2022-05-16 19:52:45+00:00
- **Authors**: Dejan Markovikj
- **Comment**: A dissertation submitted in partial fulfillment of the requirements
  for the degree of Master of Science in Computer Science at University of
  Oxford
- **Journal**: None
- **Summary**: In the last decade, deep learning has achieved great success in machine learning tasks where the input data is represented with different levels of abstractions. Driven by the recent research in reinforcement learning using deep neural networks, we explore the feasibility of designing a learning model based on expert behaviour for complex, multidimensional tasks where reward function is not available. We propose a novel method for apprenticeship learning based on the previous research on supervised learning techniques in reinforcement learning. Our method is applied to video frames from Atari games in order to teach an artificial agent to play those games. Even though the reported results are not comparable with the state-of-the-art results in reinforcement learning, we demonstrate that such an approach has the potential to achieve strong performance in the future and is worthwhile for further research.



### Sparse Visual Counterfactual Explanations in Image Space
- **Arxiv ID**: http://arxiv.org/abs/2205.07972v2
- **DOI**: 10.1007/978-3-031-16788-1_9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07972v2)
- **Published**: 2022-05-16 20:23:11+00:00
- **Updated**: 2022-09-29 10:36:18+00:00
- **Authors**: Valentyn Boreiko, Maximilian Augustin, Francesco Croce, Philipp Berens, Matthias Hein
- **Comment**: None
- **Journal**: GCPR 2022
- **Summary**: Visual counterfactual explanations (VCEs) in image space are an important tool to understand decisions of image classifiers as they show under which changes of the image the decision of the classifier would change. Their generation in image space is challenging and requires robust models due to the problem of adversarial examples. Existing techniques to generate VCEs in image space suffer from spurious changes in the background. Our novel perturbation model for VCEs together with its efficient optimization via our novel Auto-Frank-Wolfe scheme yields sparse VCEs which lead to subtle changes specific for the target class. Moreover, we show that VCEs can be used to detect undesired behavior of ImageNet classifiers due to spurious features in the ImageNet dataset.



### TOCH: Spatio-Temporal Object-to-Hand Correspondence for Motion Refinement
- **Arxiv ID**: http://arxiv.org/abs/2205.07982v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07982v2)
- **Published**: 2022-05-16 20:41:45+00:00
- **Updated**: 2022-07-21 10:12:23+00:00
- **Authors**: Keyang Zhou, Bharat Lal Bhatnagar, Jan Eric Lenssen, Gerard Pons-Moll
- **Comment**: None
- **Journal**: None
- **Summary**: We present TOCH, a method for refining incorrect 3D hand-object interaction sequences using a data prior. Existing hand trackers, especially those that rely on very few cameras, often produce visually unrealistic results with hand-object intersection or missing contacts. Although correcting such errors requires reasoning about temporal aspects of interaction, most previous works focus on static grasps and contacts. The core of our method are TOCH fields, a novel spatio-temporal representation for modeling correspondences between hands and objects during interaction. TOCH fields are a point-wise, object-centric representation, which encode the hand position relative to the object. Leveraging this novel representation, we learn a latent manifold of plausible TOCH fields with a temporal denoising auto-encoder. Experiments demonstrate that TOCH outperforms state-of-the-art 3D hand-object interaction models, which are limited to static grasps and contacts. More importantly, our method produces smooth interactions even before and after contact. Using a single trained TOCH model, we quantitatively and qualitatively demonstrate its usefulness for correcting erroneous sequences from off-the-shelf RGB/RGB-D hand-object reconstruction methods and transferring grasps across objects.



### Test-Time Adaptation with Shape Moments for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.07983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.07983v1)
- **Published**: 2022-05-16 20:47:13+00:00
- **Updated**: 2022-05-16 20:47:13+00:00
- **Authors**: Mathilde Bateson, Hervé Lombaert, Ismail Ben Ayed
- **Comment**: Early Accept at International Conference on Medical Image Computing
  and Computer Assisted Intervention (MICCAI) 2022
- **Journal**: None
- **Summary**: Supervised learning is well-known to fail at generalization under distribution shifts. In typical clinical settings, the source data is inaccessible and the target distribution is represented with a handful of samples: adaptation can only happen at test time on a few or even a single subject(s). We investigate test-time single-subject adaptation for segmentation, and propose a Shape-guided Entropy Minimization objective for tackling this task. During inference for a single testing subject, our loss is minimized with respect to the batch normalization's scale and bias parameters. We show the potential of integrating various shape priors to guide adaptation to plausible solutions, and validate our method in two challenging scenarios: MRI-to-CT adaptation of cardiac segmentation and cross-site adaptation of prostate segmentation. Our approach exhibits substantially better performances than the existing test-time adaptation methods. Even more surprisingly, it fares better than state-of-the-art domain adaptation methods, although it forgoes training on additional target data during adaptation. Our results question the usefulness of training on target data in segmentation adaptation, and points to the substantial effect of shape priors on test-time inference. Our framework can be readily used for integrating various priors and for adapting any segmentation network, and our code is available.



### Lost in Compression: the Impact of Lossy Image Compression on Variable Size Object Detection within Infrared Imagery
- **Arxiv ID**: http://arxiv.org/abs/2205.08002v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.08002v1)
- **Published**: 2022-05-16 21:54:32+00:00
- **Updated**: 2022-05-16 21:54:32+00:00
- **Authors**: Neelanjan Bhowmik, Jack W. Barker, Yona Falinie A. Gaus, Toby P. Breckon
- **Comment**: None
- **Journal**: None
- **Summary**: Lossy image compression strategies allow for more efficient storage and transmission of data by encoding data to a reduced form. This is essential enable training with larger datasets on less storage-equipped environments. However, such compression can cause severe decline in performance of deep Convolution Neural Network (CNN) architectures even when mild compression is applied and the resulting compressed imagery is visually identical. In this work, we apply the lossy JPEG compression method with six discrete levels of increasing compression {95, 75, 50, 15, 10, 5} to infrared band (thermal) imagery. Our study quantitatively evaluates the affect that increasing levels of lossy compression has upon the performance of characteristically diverse object detection architectures (Cascade-RCNN, FSAF and Deformable DETR) with respect to varying sizes of objects present in the dataset. When training and evaluating on uncompressed data as a baseline, we achieve maximal mean Average Precision (mAP) of 0.823 with Cascade R-CNN across the FLIR dataset, outperforming prior work. The impact of the lossy compression is more extreme at higher compression levels (15, 10, 5) across all three CNN architectures. However, re-training models on lossy compressed imagery notably ameliorated performances for all three CNN models with an average increment of ~76% (at higher compression level 5). Additionally, we demonstrate the relative sensitivity of differing object areas {tiny, small, medium, large} with respect to the compression level. We show that tiny and small objects are more sensitive to compression than medium and large objects. Overall, Cascade R-CNN attains the maximal mAP across most of the object area categories.



### Continual learning on 3D point clouds with random compressed rehearsal
- **Arxiv ID**: http://arxiv.org/abs/2205.08013v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.08013v2)
- **Published**: 2022-05-16 22:59:52+00:00
- **Updated**: 2022-05-20 12:09:47+00:00
- **Authors**: Maciej Zamorski, Michał Stypułkowski, Konrad Karanowski, Tomasz Trzciński, Maciej Zięba
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: Contemporary deep neural networks offer state-of-the-art results when applied to visual reasoning, e.g., in the context of 3D point cloud data. Point clouds are important datatype for precise modeling of three-dimensional environments, but effective processing of this type of data proves to be challenging. In the world of large, heavily-parameterized network architectures and continuously-streamed data, there is an increasing need for machine learning models that can be trained on additional data. Unfortunately, currently available models cannot fully leverage training on additional data without losing their past knowledge. Combating this phenomenon, called catastrophic forgetting, is one of the main objectives of continual learning. Continual learning for deep neural networks has been an active field of research, primarily in 2D computer vision, natural language processing, reinforcement learning, and robotics. However, in 3D computer vision, there are hardly any continual learning solutions specifically designed to take advantage of point cloud structure. This work proposes a novel neural network architecture capable of continual learning on 3D point cloud data. We utilize point cloud structure properties for preserving a heavily compressed set of past data. By using rehearsal and reconstruction as regularization methods of the learning process, our approach achieves a significant decrease of catastrophic forgetting compared to the existing solutions on several most popular point cloud datasets considering two continual learning settings: when a task is known beforehand, and in the challenging scenario of when task information is unknown to the model.



