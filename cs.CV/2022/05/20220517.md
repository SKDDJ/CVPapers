# Arxiv Papers in cs.CV on 2022-05-17
### Detection and Physical Interaction with Deformable Linear Objects
- **Arxiv ID**: http://arxiv.org/abs/2205.08041v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.08041v2)
- **Published**: 2022-05-17 01:17:21+00:00
- **Updated**: 2023-04-08 21:18:38+00:00
- **Authors**: Azarakhsh Keipour, Mohammadreza Mousaei, Maryam Bandari, Stefan Schaal, Sebastian Scherer
- **Comment**: Presented at ICRA 2022 2nd Workshop on Representing and Manipulating
  Deformable Objects (https://deformable-workshop.github.io/icra2022/)
- **Journal**: None
- **Summary**: Deformable linear objects (e.g., cables, ropes, and threads) commonly appear in our everyday lives. However, perception of these objects and the study of physical interaction with them is still a growing area. There have already been successful methods to model and track deformable linear objects. However, the number of methods that can automatically extract the initial conditions in non-trivial situations for these methods has been limited, and they have been introduced to the community only recently. On the other hand, while physical interaction with these objects has been done with ground manipulators, there have not been any studies on physical interaction and manipulation of the deformable linear object with aerial robots.   This workshop describes our recent work on detecting deformable linear objects, which uses the segmentation output of the existing methods to provide the initialization required by the tracking methods automatically. It works with crossings and can fill the gaps and occlusions in the segmentation and output the model desirable for physical interaction and simulation. Then we present our work on using the method for tasks such as routing and manipulation with the ground and aerial robots. We discuss our feasibility analysis on extending the physical interaction with these objects to aerial manipulation applications.



### Collaborative Attention Memory Network for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.08075v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08075v2)
- **Published**: 2022-05-17 03:40:11+00:00
- **Updated**: 2022-05-23 03:16:40+00:00
- **Authors**: Zhixing Huang, Junli Zha, Fei Xie, Yuwei Zheng, Yuandong Zhong, Jinpeng Tang
- **Comment**: Technical Report. Proposed systems attain 6th in YouTube-VOS
  challenge 2021
- **Journal**: None
- **Summary**: Semi-supervised video object segmentation is a fundamental yet Challenging task in computer vision. Embedding matching based CFBI series networks have achieved promising results by foreground-background integration approach. Despite its superior performance, these works exhibit distinct shortcomings, especially the false predictions caused by little appearance instances in first frame, even they could easily be recognized by previous frame. Moreover, they suffer from object's occlusion and error drifts. In order to overcome the shortcomings , we propose Collaborative Attention Memory Network with an enhanced segmentation head. We introduce a object context scheme that explicitly enhances the object information, which aims at only gathering the pixels that belong to the same category as a given pixel as its context. Additionally, a segmentation head with Feature Pyramid Attention(FPA) module is adopted to perform spatial pyramid attention structure on high-level output. Furthermore, we propose an ensemble network to combine STM network with all these new refined CFBI network. Finally, we evaluated our approach on the 2021 Youtube-VOS challenge where we obtain 6th place with an overall score of 83.5\%.



### Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2205.08078v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2205.08078v2)
- **Published**: 2022-05-17 04:01:15+00:00
- **Updated**: 2022-05-20 17:26:35+00:00
- **Authors**: Arda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, Mert Pilanci
- **Comment**: 38 pages, 2 figures. To appear in ICML 2022
- **Journal**: None
- **Summary**: Vision transformers using self-attention or its proposed alternatives have demonstrated promising results in many image related tasks. However, the underpinning inductive bias of attention is not well understood. To address this issue, this paper analyzes attention through the lens of convex duality. For the non-linear dot-product self-attention, and alternative mechanisms such as MLP-mixer and Fourier Neural Operator (FNO), we derive equivalent finite-dimensional convex problems that are interpretable and solvable to global optimality. The convex programs lead to {\it block nuclear-norm regularization} that promotes low rank in the latent feature and token dimensions. In particular, we show how self-attention networks implicitly clusters the tokens, based on their latent similarity. We conduct experiments for transferring a pre-trained transformer backbone for CIFAR-100 classification by fine-tuning a variety of convex attention heads. The results indicate the merits of the bias induced by attention compared with the existing MLP or linear heads.



### Region-Aware Metric Learning for Open World Semantic Segmentation via Meta-Channel Aggregation
- **Arxiv ID**: http://arxiv.org/abs/2205.08083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08083v1)
- **Published**: 2022-05-17 04:12:47+00:00
- **Updated**: 2022-05-17 04:12:47+00:00
- **Authors**: Hexin Dong, Zifan Chen, Mingze Yuan, Yutong Xie, Jie Zhao, Fei Yu, Bin Dong, Li Zhang
- **Comment**: Accepted at IJCAI 2022
- **Journal**: None
- **Summary**: As one of the most challenging and practical segmentation tasks, open-world semantic segmentation requires the model to segment the anomaly regions in the images and incrementally learn to segment out-of-distribution (OOD) objects, especially under a few-shot condition. The current state-of-the-art (SOTA) method, Deep Metric Learning Network (DMLNet), relies on pixel-level metric learning, with which the identification of similar regions having different semantics is difficult. Therefore, we propose a method called region-aware metric learning (RAML), which first separates the regions of the images and generates region-aware features for further metric learning. RAML improves the integrity of the segmented anomaly regions. Moreover, we propose a novel meta-channel aggregation (MCA) module to further separate anomaly regions, forming high-quality sub-region candidates and thereby improving the model performance for OOD objects. To evaluate the proposed RAML, we have conducted extensive experiments and ablation studies on Lost And Found and Road Anomaly datasets for anomaly segmentation and the CityScapes dataset for incremental few-shot learning. The results show that the proposed RAML achieves SOTA performance in both stages of open world segmentation. Our code and appendix are available at https://github.com/czifan/RAML.



### Efficient Stereo Depth Estimation for Pseudo LiDAR: A Self-Supervised Approach Based on Multi-Input ResNet Encoder
- **Arxiv ID**: http://arxiv.org/abs/2205.08089v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08089v1)
- **Published**: 2022-05-17 04:42:13+00:00
- **Updated**: 2022-05-17 04:42:13+00:00
- **Authors**: Sabir Hossain, Xianke Lin
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Perception and localization are essential for autonomous delivery vehicles, mostly estimated from 3D LiDAR sensors due to their precise distance measurement capability. This paper presents a strategy to obtain the real-time pseudo point cloud instead of the laser sensor from the image sensor. We propose an approach to use different depth estimators to obtain pseudo point clouds like LiDAR to obtain better performance. Moreover, the training and validating strategy of the depth estimator has adopted stereo imagery data to estimate more accurate depth estimation as well as point cloud results. Our approach to generating depth maps outperforms on KITTI benchmark while yielding point clouds significantly faster than other approaches.



### A Linear Comb Filter for Event Flicker Removal
- **Arxiv ID**: http://arxiv.org/abs/2205.08090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08090v1)
- **Published**: 2022-05-17 04:47:26+00:00
- **Updated**: 2022-05-17 04:47:26+00:00
- **Authors**: Ziwei Wang, Dingran Yuan, Yonhon Ng, Robert Mahony
- **Comment**: 10 pages, 7 figures, published in IEEE International Conference on
  Robotics and Automation (ICRA), 2022
- **Journal**: None
- **Summary**: Event cameras are bio-inspired sensors that capture per-pixel asynchronous intensity change rather than the synchronous absolute intensity frames captured by a classical camera sensor. Such cameras are ideal for robotics applications since they have high temporal resolution, high dynamic range and low latency. However, due to their high temporal resolution, event cameras are particularly sensitive to flicker such as from fluorescent or LED lights. During every cycle from bright to dark, pixels that image a flickering light source generate many events that provide little or no useful information for a robot, swamping the useful data in the scene. In this paper, we propose a novel linear filter to preprocess event data to remove unwanted flicker events from an event stream. The proposed algorithm achieves over 4.6 times relative improvement in the signal-to-noise ratio when compared to the raw event stream due to the effective removal of flicker from fluorescent lighting. Thus, it is ideally suited to robotics applications that operate in indoor settings or scenes illuminated by flickering light sources.



### MATrIX -- Modality-Aware Transformer for Information eXtraction
- **Arxiv ID**: http://arxiv.org/abs/2205.08094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08094v1)
- **Published**: 2022-05-17 05:06:59+00:00
- **Updated**: 2022-05-17 05:06:59+00:00
- **Authors**: Thomas Delteil, Edouard Belval, Lei Chen, Luis Goncalves, Vijay Mahadevan
- **Comment**: None
- **Journal**: None
- **Summary**: We present MATrIX - a Modality-Aware Transformer for Information eXtraction in the Visual Document Understanding (VDU) domain. VDU covers information extraction from visually rich documents such as forms, invoices, receipts, tables, graphs, presentations, or advertisements. In these, text semantics and visual information supplement each other to provide a global understanding of the document. MATrIX is pre-trained in an unsupervised way with specifically designed tasks that require the use of multi-modal information (spatial, visual, or textual). We consider the spatial and text modalities all at once in a single token set. To make the attention more flexible, we use a learned modality-aware relative bias in the attention mechanism to modulate the attention between the tokens of different modalities. We evaluate MATrIX on 3 different datasets each with strong baselines.



### Dimensionality Reduced Training by Pruning and Freezing Parts of a Deep Neural Network, a Survey
- **Arxiv ID**: http://arxiv.org/abs/2205.08099v2
- **DOI**: 10.1007/s10462-023-10489-1
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2205.08099v2)
- **Published**: 2022-05-17 05:37:08+00:00
- **Updated**: 2023-05-25 05:59:41+00:00
- **Authors**: Paul Wimmer, Jens Mehnert, Alexandru Paul Condurache
- **Comment**: This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this article is
  published in Artificial Intelligence Review (2023), and is available online
  at https://doi.org/10.1007/s10462-023-10489-1
- **Journal**: Artif Intell Rev 2023
- **Summary**: State-of-the-art deep learning models have a parameter count that reaches into the billions. Training, storing and transferring such models is energy and time consuming, thus costly. A big part of these costs is caused by training the network. Model compression lowers storage and transfer costs, and can further make training more efficient by decreasing the number of computations in the forward and/or backward pass. Thus, compressing networks also at training time while maintaining a high performance is an important research topic. This work is a survey on methods which reduce the number of trained weights in deep learning models throughout the training. Most of the introduced methods set network parameters to zero which is called pruning. The presented pruning approaches are categorized into pruning at initialization, lottery tickets and dynamic sparse training. Moreover, we discuss methods that freeze parts of a network at its random initialization. By freezing weights, the number of trainable parameters is shrunken which reduces gradient computations and the dimensionality of the model's optimization space. In this survey we first propose dimensionality reduced training as an underlying mathematical model that covers pruning and freezing during training. Afterwards, we present and discuss different dimensionality reduced training methods.



### Computerized Tomography Pulmonary Angiography Image Simulation using Cycle Generative Adversarial Network from Chest CT imaging in Pulmonary Embolism Patients
- **Arxiv ID**: http://arxiv.org/abs/2205.08106v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.08106v1)
- **Published**: 2022-05-17 06:02:33+00:00
- **Updated**: 2022-05-17 06:02:33+00:00
- **Authors**: Chia-Hung Yang, Yun-Chien Cheng, Chin Kuo
- **Comment**: 23 pages, 14 figures, 6 tables
- **Journal**: None
- **Summary**: The purpose of this research is to develop a system that generates simulated computed tomography pulmonary angiography (CTPA) images clinically for pulmonary embolism diagnoses. Nowadays, CTPA images are the gold standard computerized detection method to determine and identify the symptoms of pulmonary embolism (PE), although performing CTPA is harmful for patients and also expensive. Therefore, we aim to detect possible PE patients through CT images. The system will simulate CTPA images with deep learning models for the identification of PE patients' symptoms, providing physicians with another reference for determining PE patients. In this study, the simulated CTPA image generation system uses a generative antagonistic network to enhance the features of pulmonary vessels in the CT images to strengthen the reference value of the images and provide a basis for hospitals to judge PE patients. We used the CT images of 22 patients from National Cheng Kung University Hospital and the corresponding CTPA images as the training data for the task of simulating CTPA images and generated them using two sets of generative countermeasure networks. This study is expected to propose a new approach to the clinical diagnosis of pulmonary embolism, in which a deep learning network is used to assist in the complex screening process and to review the generated simulated CTPA images, allowing physicians to assess whether a patient needs to undergo detailed testing for CTPA, improving the speed of detection of pulmonary embolism and significantly reducing the number of undetected patients.



### Using artificial intelligence to detect chest X-rays with no significant findings in a primary health care setting in Oulu, Finland
- **Arxiv ID**: http://arxiv.org/abs/2205.08123v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2205.08123v1)
- **Published**: 2022-05-17 06:48:01+00:00
- **Updated**: 2022-05-17 06:48:01+00:00
- **Authors**: Tommi Keski-Filppula, Marko Nikki, Marianne Haapea, Naglis Ramanauskas, Osmo Tervonen
- **Comment**: Abstract #21318 - ECR 2022 oral presentations
- **Journal**: None
- **Summary**: Objectives: To assess the use of artificial intelligence-based software in ruling out chest X-ray cases, with no significant findings in a primary health care setting.   Methods: In this retrospective study, a commercially available artificial intelligence (AI) software was used to analyse 10 000 chest X-rays of Finnish primary health care patients. In studies with a mismatch between an AI normal report and the original radiologist report, a consensus read by two board-certified radiologists was conducted to make the final diagnosis.   Results: After the exclusion of cases not meeting the study criteria, 9579 cases were analysed by AI. Of these cases, 4451 were considered normal in the original radiologist report and 4644 after the consensus reading. The number of cases correctly found nonsignificant by AI was 1692 (17.7% of all studies and 36.4% of studies with no significant findings). After the consensus read, there were nine confirmed false-negative studies. These studies included four cases of slightly enlarged heart size, four cases of slightly increased pulmonary opacification and one case with a small unilateral pleural effusion. This gives the AI a sensitivity of 99.8% (95% CI= 99.65-99.92) and specificity of 36.4 % (95% CI= 35.05-37.84) for recognising significant pathology on a chest X-ray.   Conclusions: AI was able to correctly rule out 36.4% of chest X-rays with no significant findings of primary health care patients, with a minimal number of false negatives that would lead to effectively no compromise on patient safety. No critical findings were missed by the software.



### Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space
- **Arxiv ID**: http://arxiv.org/abs/2205.08129v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.08129v2)
- **Published**: 2022-05-17 06:58:17+00:00
- **Updated**: 2023-04-18 07:06:50+00:00
- **Authors**: Kuan Fang, Patrick Yin, Ashvin Nair, Sergey Levine
- **Comment**: None
- **Journal**: None
- **Summary**: General-purpose robots require diverse repertoires of behaviors to complete challenging tasks in real-world unstructured environments. To address this issue, goal-conditioned reinforcement learning aims to acquire policies that can reach configurable goals for a wide range of tasks on command. However, such goal-conditioned policies are notoriously difficult and time-consuming to train from scratch. In this paper, we propose Planning to Practice (PTP), a method that makes it practical to train goal-conditioned policies for long-horizon tasks that require multiple distinct types of interactions to solve. Our approach is based on two key ideas. First, we decompose the goal-reaching problem hierarchically, with a high-level planner that sets intermediate subgoals using conditional subgoal generators in the latent space for a low-level model-free policy. Second, we propose a hybrid approach which first pre-trains both the conditional subgoal generator and the policy on previously collected data through offline reinforcement learning, and then fine-tunes the policy via online exploration. This fine-tuning process is itself facilitated by the planned subgoals, which breaks down the original target task into short-horizon goal-reaching tasks that are significantly easier to learn. We conduct experiments in both the simulation and real world, in which the policy is pre-trained on demonstrations of short primitive behaviors and fine-tuned for temporally extended tasks that are unseen in the offline data. Our experimental results show that PTP can generate feasible sequences of subgoals that enable the policy to efficiently solve the target tasks.



### Brachial Plexus Nerve Trunk Segmentation Using Deep Learning: A Comparative Study with Doctors' Manual Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.08143v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.08143v1)
- **Published**: 2022-05-17 07:23:28+00:00
- **Updated**: 2022-05-17 07:23:28+00:00
- **Authors**: Yu Wang, Binbin Zhu, Lingsi Kong, Jianlin Wang, Bin Gao, Jianhua Wang, Dingcheng Tian, Yudong Yao
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Ultrasound-guided nerve block anesthesia (UGNB) is a high-tech visual nerve block anesthesia method that can observe the target nerve and its surrounding structures, the puncture needle's advancement, and local anesthetics spread in real-time. The key in UGNB is nerve identification. With the help of deep learning methods, the automatic identification or segmentation of nerves can be realized, assisting doctors in completing nerve block anesthesia accurately and efficiently. Here, we establish a public dataset containing 320 ultrasound images of brachial plexus (BP). Three experienced doctors jointly produce the BP segmentation ground truth and label brachial plexus trunks. We design a brachial plexus segmentation system (BPSegSys) based on deep learning. BPSegSys achieves experienced-doctor-level nerve identification performance in various experiments. We evaluate BPSegSys' performance in terms of intersection-over-union (IoU), a commonly used performance measure for segmentation experiments. Considering three dataset groups in our established public dataset, the IoU of BPSegSys are 0.5238, 0.4715, and 0.5029, respectively, which exceed the IoU 0.5205, 0.4704, and 0.4979 of experienced doctors. In addition, we show that BPSegSys can help doctors identify brachial plexus trunks more accurately, with IoU improvement up to 27%, which has significant clinical application value.



### Pairwise Comparison Network for Remote Sensing Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/2205.08147v2
- **DOI**: 10.1109/LGRS.2021.3139695
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.08147v2)
- **Published**: 2022-05-17 07:31:36+00:00
- **Updated**: 2022-05-21 07:33:29+00:00
- **Authors**: Zhang Yue, Zheng Xiangtao, Lu Xiaoqiang
- **Comment**: 6 pages, 4 figures, published to GRSL
- **Journal**: IEEE Geoscience and Remote Sensing Letters, vol. 19, pp. 1-5, 2022
- **Summary**: Remote sensing scene classification aims to assign a specific semantic label to a remote sensing image. Recently, convolutional neural networks have greatly improved the performance of remote sensing scene classification. However, some confused images may be easily recognized as the incorrect category, which generally degrade the performance. The differences between image pairs can be used to distinguish image categories. This paper proposed a pairwise comparison network, which contains two main steps: pairwise selection and pairwise representation. The proposed network first selects similar image pairs, and then represents the image pairs with pairwise representations. The self-representation is introduced to highlight the informative parts of each image itself, while the mutual-representation is proposed to capture the subtle differences between image pairs. Comprehensive experimental results on two challenging datasets (AID, NWPU-RESISC45) demonstrate the effectiveness of the proposed network. The codes are provided in https://github.com/spectralpublic/PCNet.git.



### Gender and Racial Bias in Visual Question Answering Datasets
- **Arxiv ID**: http://arxiv.org/abs/2205.08148v3
- **DOI**: 10.1145/3531146.3533184
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2205.08148v3)
- **Published**: 2022-05-17 07:33:24+00:00
- **Updated**: 2022-06-03 06:36:16+00:00
- **Authors**: Yusuke Hirota, Yuta Nakashima, Noa Garcia
- **Comment**: ACM Conference on Fairness, Accountability, and Transparency (FAccT
  2022)
- **Journal**: None
- **Summary**: Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models. A popular task in the field is visual question answering (VQA), which aims to answer questions about images. However, VQA models have been shown to exploit language bias by learning the statistical correlations between questions and answers without looking into the image content: e.g., questions about the color of a banana are answered with yellow, even if the banana in the image is green. If societal bias (e.g., sexism, racism, ableism, etc.) is present in the training data, this problem may be causing VQA models to learn harmful stereotypes. For this reason, we investigate gender and racial bias in five VQA datasets. In our analysis, we find that the distribution of answers is highly different between questions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that specific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets. Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset collection process.



### UnPWC-SVDLO: Multi-SVD on PointPWC for Unsupervised Lidar Odometry
- **Arxiv ID**: http://arxiv.org/abs/2205.08150v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.08150v1)
- **Published**: 2022-05-17 07:37:21+00:00
- **Updated**: 2022-05-17 07:37:21+00:00
- **Authors**: Yiming Tu
- **Comment**: None
- **Journal**: None
- **Summary**: High-precision lidar odomety is an essential part of autonomous driving. In recent years, deep learning methods have been widely used in lidar odomety tasks, but most of the current methods only extract the global features of the point clouds. It is impossible to obtain more detailed point-level features in this way. In addition, only the fully connected layer is used to estimate the pose. The fully connected layer has achieved obvious results in the classification task, but the changes in pose are a continuous rather than discrete process, high-precision pose estimation can not be obtained only by using the fully connected layer. Our method avoids problems mentioned above. We use PointPWC as our backbone network. PointPWC is originally used for scene flow estimation. The scene flow estimation task has a strong correlation with lidar odomety. Traget point clouds can be obtained by adding the scene flow and source point clouds. We can achieve the pose directly through ICP algorithm solved by SVD, and the fully connected layer is no longer used. PointPWC extracts point-level features from point clouds with different sampling levels, which solves the problem of too rough feature extraction. We conduct experiments on KITTI, Ford Campus Vision and Lidar DataSe and Apollo-SouthBay Dataset. Our result is comparable with the state-of-the-art unsupervised deep learing method SelfVoxeLO.



### Uncertainty-based Network for Few-shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2205.08157v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.08157v1)
- **Published**: 2022-05-17 07:49:32+00:00
- **Updated**: 2022-05-17 07:49:32+00:00
- **Authors**: Minglei Yuan, Qian Xu, Chunhao Cai, Yin-Dong Zheng, Tao Wang, Tong Lu
- **Comment**: Few-shot learning, Uncertainty, Mutual information
- **Journal**: None
- **Summary**: The transductive inference is an effective technique in the few-shot learning task, where query sets update prototypes to improve themselves. However, these methods optimize the model by considering only the classification scores of the query instances as confidence while ignoring the uncertainty of these classification scores. In this paper, we propose a novel method called Uncertainty-Based Network, which models the uncertainty of classification results with the help of mutual information. Specifically, we first data augment and classify the query instance and calculate the mutual information of these classification scores. Then, mutual information is used as uncertainty to assign weights to classification scores, and the iterative update strategy based on classification scores and uncertainties assigns the optimal weights to query instances in prototype optimization. Extensive results on four benchmarks show that Uncertainty-Based Network achieves comparable performance in classification accuracy compared to state-of-the-art method.



### CellTypeGraph: A New Geometric Computer Vision Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2205.08166v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.08166v1)
- **Published**: 2022-05-17 08:08:19+00:00
- **Updated**: 2022-05-17 08:08:19+00:00
- **Authors**: Lorenzo Cerrone, Athul Vijayan, Tejasvinee Mody, Kay Schneitz, Fred A. Hamprecht
- **Comment**: None
- **Journal**: None
- **Summary**: Classifying all cells in an organ is a relevant and difficult problem from plant developmental biology. We here abstract the problem into a new benchmark for node classification in a geo-referenced graph. Solving it requires learning the spatial layout of the organ including symmetries. To allow the convenient testing of new geometrical learning methods, the benchmark of Arabidopsis thaliana ovules is made available as a PyTorch data loader, along with a large number of precomputed features. Finally, we benchmark eight recent graph neural network architectures, finding that DeeperGCN currently works best on this problem.



### User Localization using RF Sensing: A Performance comparison between LIS and mmWave Radars
- **Arxiv ID**: http://arxiv.org/abs/2205.10321v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.10321v1)
- **Published**: 2022-05-17 09:44:56+00:00
- **Updated**: 2022-05-17 09:44:56+00:00
- **Authors**: Cristian J. Vaca-Rubio, Dariush Salami, Petar Popovski, Elisabeth de Carvalho, Zheng-Hua Tan, Stephan Sigg
- **Comment**: None
- **Journal**: None
- **Summary**: Since electromagnetic signals are omnipresent, Radio Frequency (RF)-sensing has the potential to become a universal sensing mechanism with applications in localization, smart-home, retail, gesture recognition, intrusion detection, etc. Two emerging technologies in RF-sensing, namely sensing through Large Intelligent Surfaces (LISs) and mmWave Frequency-Modulated Continuous-Wave (FMCW) radars, have been successfully applied to a wide range of applications. In this work, we compare LIS and mmWave radars for localization in real-world and simulated environments. In our experiments, the mmWave radar achieves 0.71 Intersection Over Union (IOU) and 3cm error for bounding boxes, while LIS has 0.56 IOU and 10cm distance error. Although the radar outperforms the LIS in terms of accuracy, LIS features additional applications in communication in addition to sensing scenarios.



### Dark solitons in Bose-Einstein condensates: a dataset for many-body physics research
- **Arxiv ID**: http://arxiv.org/abs/2205.09114v2
- **DOI**: 10.1088/2632-2153/ac9454
- **Categories**: **cond-mat.quant-gas**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2205.09114v2)
- **Published**: 2022-05-17 09:53:16+00:00
- **Updated**: 2023-02-11 21:36:56+00:00
- **Authors**: Amilson R. Fritsch, Shangjie Guo, Sophia M. Koh, I. B. Spielman, Justyna P. Zwolak
- **Comment**: 16 pages, 4 figures
- **Journal**: Mach. Learn.: Sci. Technol. 3, 047001 (2022)
- **Summary**: We establish a dataset of over $1.6\times10^4$ experimental images of Bose--Einstein condensates containing solitonic excitations to enable machine learning (ML) for many-body physics research. About $33~\%$ of this dataset has manually assigned and carefully curated labels. The remainder is automatically labeled using SolDet -- an implementation of a physics-informed ML data analysis framework -- consisting of a convolutional-neural-network-based classifier and OD as well as a statistically motivated physics-informed classifier and a quality metric. This technical note constitutes the definitive reference of the dataset, providing an opportunity for the data science community to develop more sophisticated analysis tools, to further understand nonlinear many-body physics, and even advance cold atom experiments.



### DynPL-SVO: A Robust Stereo Visual Odometry for Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2205.08207v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.08207v3)
- **Published**: 2022-05-17 10:08:03+00:00
- **Updated**: 2023-06-24 08:47:01+00:00
- **Authors**: Baosheng Zhang, Xiaoguang Ma, Hongjun Ma, Chunbo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Most feature-based stereo visual odometry (SVO) approaches estimate the motion of mobile robots by matching and tracking point features along a sequence of stereo images. However, in dynamic scenes mainly comprising moving pedestrians, vehicles, etc., there are insufficient robust static point features to enable accurate motion estimation, causing failures when reconstructing robotic motion. In this paper, we proposed DynPL-SVO, a complete dynamic SVO method that integrated united cost functions containing information between matched point features and re-projection errors perpendicular and parallel to the direction of the line features. Additionally, we introduced a \textit{dynamic} \textit{grid} algorithm to enhance its performance in dynamic scenes. The stereo camera motion was estimated through Levenberg-Marquard minimization of the re-projection errors of both point and line features. Comprehensive experimental results on KITTI and EuRoC MAV datasets showed that accuracy of the DynPL-SVO was improved by over 20\% on average compared to other state-of-the-art SVO systems, especially in dynamic scenes.



### blob loss: instance imbalance aware loss functions for semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.08209v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.08209v3)
- **Published**: 2022-05-17 10:13:27+00:00
- **Updated**: 2023-06-06 17:54:34+00:00
- **Authors**: Florian Kofler, Suprosanna Shit, Ivan Ezhov, Lucas Fidon, Izabela Horvath, Rami Al-Maskari, Hongwei Li, Harsharan Bhatia, Timo Loehr, Marie Piraud, Ali Erturk, Jan Kirschke, Jan C. Peeken, Tom Vercauteren, Claus Zimmer, Benedikt Wiestler, Bjoern Menze
- **Comment**: 23 pages, 7 figures // corrected one mistake where it said beta
  instead of alpha in the text
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNN) have proven to be remarkably effective in semantic segmentation tasks. Most popular loss functions were introduced targeting improved volumetric scores, such as the Dice coefficient (DSC). By design, DSC can tackle class imbalance, however, it does not recognize instance imbalance within a class. As a result, a large foreground instance can dominate minor instances and still produce a satisfactory DSC. Nevertheless, detecting tiny instances is crucial for many applications, such as disease monitoring. For example, it is imperative to locate and surveil small-scale lesions in the follow-up of multiple sclerosis patients. We propose a novel family of loss functions, \emph{blob loss}, primarily aimed at maximizing instance-level detection metrics, such as F1 score and sensitivity. \emph{Blob loss} is designed for semantic segmentation problems where detecting multiple instances matters. We extensively evaluate a DSC-based \emph{blob loss} in five complex 3D semantic segmentation tasks featuring pronounced instance heterogeneity in terms of texture and morphology. Compared to soft Dice loss, we achieve 5% improvement for MS lesions, 3% improvement for liver tumor, and an average 2% improvement for microscopy segmentation tasks considering F1 score.



### Deep Quality Estimation: Creating Surrogate Models for Human Quality Ratings
- **Arxiv ID**: http://arxiv.org/abs/2205.10355v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.10355v2)
- **Published**: 2022-05-17 10:32:27+00:00
- **Updated**: 2022-08-30 19:08:26+00:00
- **Authors**: Florian Kofler, Ivan Ezhov, Lucas Fidon, Izabela Horvath, Ezequiel de la Rosa, John LaMaster, Hongwei Li, Tom Finck, Suprosanna Shit, Johannes Paetzold, Spyridon Bakas, Marie Piraud, Jan Kirschke, Tom Vercauteren, Claus Zimmer, Benedikt Wiestler, Bjoern Menze
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Human ratings are abstract representations of segmentation quality. To approximate human quality ratings on scarce expert data, we train surrogate quality estimation models. We evaluate on a complex multi-class segmentation problem, specifically glioma segmentation, following the BraTS annotation protocol. The training data features quality ratings from 15 expert neuroradiologists on a scale ranging from 1 to 6 stars for various computer-generated and manual 3D annotations. Even though the networks operate on 2D images and with scarce training data, we can approximate segmentation quality within a margin of error comparable to human intra-rater reliability. Segmentation quality prediction has broad applications. While an understanding of segmentation quality is imperative for successful clinical translation of automatic segmentation quality algorithms, it can play an essential role in training new segmentation models. Due to the split-second inference times, it can be directly applied within a loss function or as a fully-automatic dataset curation mechanism in a federated learning setting.



### CAS-Net: Conditional Atlas Generation and Brain Segmentation for Fetal MRI
- **Arxiv ID**: http://arxiv.org/abs/2205.08239v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.08239v1)
- **Published**: 2022-05-17 11:23:02+00:00
- **Updated**: 2022-05-17 11:23:02+00:00
- **Authors**: Liu Li, Qiang Ma, Matthew Sinclair, Antonios Makropoulos, Joseph Hajnal, A. David Edwards, Bernhard Kainz, Daniel Rueckert, Amir Alansary
- **Comment**: None
- **Journal**: None
- **Summary**: Fetal Magnetic Resonance Imaging (MRI) is used in prenatal diagnosis and to assess early brain development. Accurate segmentation of the different brain tissues is a vital step in several brain analysis tasks, such as cortical surface reconstruction and tissue thickness measurements. Fetal MRI scans, however, are prone to motion artifacts that can affect the correctness of both manual and automatic segmentation techniques. In this paper, we propose a novel network structure that can simultaneously generate conditional atlases and predict brain tissue segmentation, called CAS-Net. The conditional atlases provide anatomical priors that can constrain the segmentation connectivity, despite the heterogeneity of intensity values caused by motion or partial volume effects. The proposed method is trained and evaluated on 253 subjects from the developing Human Connectome Project (dHCP). The results demonstrate that the proposed method can generate conditional age-specific atlas with sharp boundary and shape variance. It also segment multi-category brain tissues for fetal MRI with a high overall Dice similarity coefficient (DSC) of $85.2\%$ for the selected 9 tissue labels.



### Learnable Optimal Sequential Grouping for Video Scene Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.08249v1
- **DOI**: 10.1145/3394171.3413612
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08249v1)
- **Published**: 2022-05-17 11:45:03+00:00
- **Updated**: 2022-05-17 11:45:03+00:00
- **Authors**: Daniel Rotman, Yevgeny Yaroker, Elad Amrani, Udi Barzelay, Rami Ben-Ari
- **Comment**: None
- **Journal**: In Proceedings of the 28th ACM International Conference on
  Multimedia, pp. 1958-1966. 2020
- **Summary**: Video scene detection is the task of dividing videos into temporal semantic chapters. This is an important preliminary step before attempting to analyze heterogeneous video content. Recently, Optimal Sequential Grouping (OSG) was proposed as a powerful unsupervised solution to solve a formulation of the video scene detection problem. In this work, we extend the capabilities of OSG to the learning regime. By giving the capability to both learn from examples and leverage a robust optimization formulation, we can boost performance and enhance the versatility of the technology. We present a comprehensive analysis of incorporating OSG into deep learning neural networks under various configurations. These configurations include learning an embedding in a straight-forward manner, a tailored loss designed to guide the solution of OSG, and an integrated model where the learning is performed through the OSG pipeline. With thorough evaluation and analysis, we assess the benefits and behavior of the various configurations, and show that our learnable OSG approach exhibits desirable behavior and enhanced performance compared to the state of the art.



### Detection Masking for Improved OCR on Noisy Documents
- **Arxiv ID**: http://arxiv.org/abs/2205.08257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08257v1)
- **Published**: 2022-05-17 11:59:18+00:00
- **Updated**: 2022-05-17 11:59:18+00:00
- **Authors**: Daniel Rotman, Ophir Azulai, Inbar Shapira, Yevgeny Burshtein, Udi Barzelay
- **Comment**: None
- **Journal**: None
- **Summary**: Optical Character Recognition (OCR), the task of extracting textual information from scanned documents is a vital and broadly used technology for digitizing and indexing physical documents. Existing technologies perform well for clean documents, but when the document is visually degraded, or when there are non-textual elements, OCR quality can be greatly impacted, specifically due to erroneous detections. In this paper we present an improved detection network with a masking system to improve the quality of OCR performed on documents. By filtering non-textual elements from the image we can utilize document-level OCR to incorporate contextual information to improve OCR results. We perform a unified evaluation on a publicly available dataset demonstrating the usefulness and broad applicability of our method. Additionally, we present and make publicly available our synthetic dataset with a unique hard-negative component specifically tuned to improve detection results, and evaluate the benefits that can be gained from its usage



### MulT: An End-to-End Multitask Learning Transformer
- **Arxiv ID**: http://arxiv.org/abs/2205.08303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08303v1)
- **Published**: 2022-05-17 13:03:18+00:00
- **Updated**: 2022-05-17 13:03:18+00:00
- **Authors**: Deblina Bhattacharjee, Tong Zhang, Sabine Süsstrunk, Mathieu Salzmann
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: We propose an end-to-end Multitask Learning Transformer framework, named MulT, to simultaneously learn multiple high-level vision tasks, including depth estimation, semantic segmentation, reshading, surface normal estimation, 2D keypoint detection, and edge detection. Based on the Swin transformer model, our framework encodes the input image into a shared representation and makes predictions for each vision task using task-specific transformer-based decoder heads. At the heart of our approach is a shared attention mechanism modeling the dependencies across the tasks. We evaluate our model on several multitask benchmarks, showing that our MulT framework outperforms both the state-of-the art multitask convolutional neural network models and all the respective single task transformer models. Our experiments further highlight the benefits of sharing attention across all the tasks, and demonstrate that our MulT model is robust and generalizes well to new domains. Our project website is at https://ivrl.github.io/MulT/.



### Self-Supervised Learning of Multi-Object Keypoints for Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2205.08316v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.08316v2)
- **Published**: 2022-05-17 13:15:07+00:00
- **Updated**: 2022-10-11 09:06:57+00:00
- **Authors**: Jan Ole von Hartz, Eugenio Chisari, Tim Welschehold, Abhinav Valada
- **Comment**: Presented at IEEE ICRA 2022 Workshop 'Reinforcement Learning for
  Contact-Rich Manipulation'
- **Journal**: None
- **Summary**: In recent years, policy learning methods using either reinforcement or imitation have made significant progress. However, both techniques still suffer from being computationally expensive and requiring large amounts of training data. This problem is especially prevalent in real-world robotic manipulation tasks, where access to ground truth scene features is not available and policies are instead learned from raw camera observations. In this paper, we demonstrate the efficacy of learning image keypoints via the Dense Correspondence pretext task for downstream policy learning. Extending prior work to challenging multi-object scenes, we show that our model can be trained to deal with important problems in representation learning, primarily scale-invariance and occlusion. We evaluate our approach on diverse robot manipulation tasks, compare it to other visual representation learning approaches, and demonstrate its flexibility and effectiveness for sample-efficient policy learning.



### Unified Interactive Image Matting
- **Arxiv ID**: http://arxiv.org/abs/2205.08324v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.08324v2)
- **Published**: 2022-05-17 13:20:30+00:00
- **Updated**: 2022-05-23 02:20:05+00:00
- **Authors**: Stephen D. H. Yang, Bin Wang, Weijia Li, YiQi Lin, Conghui He
- **Comment**: None
- **Journal**: None
- **Summary**: Recent image matting studies are developing towards proposing trimap-free or interactive methods for complete complex image matting tasks. Although avoiding the extensive labors of trimap annotation, existing methods still suffer from two limitations: (1) For the single image with multiple objects, it is essential to provide extra interaction information to help determining the matting target; (2) For transparent objects, the accurate regression of alpha matte from RGB image is much more difficult compared with the opaque ones. In this work, we propose a Unified Interactive image Matting method, named UIM, which solves the limitations and achieves satisfying matting results for any scenario. Specifically, UIM leverages multiple types of user interaction to avoid the ambiguity of multiple matting targets, and we compare the pros and cons of different annotation types in detail. To unify the matting performance for transparent and opaque objects, we decouple image matting into two stages, i.e., foreground segmentation and transparency prediction. Moreover, we design a multi-scale attentive fusion module to alleviate the vagueness in the boundary region. Experimental results demonstrate that UIM achieves state-of-the-art performance on the Composition-1K test set and a synthetic unified dataset. Our code and models will be released soon.



### GraphMapper: Efficient Visual Navigation by Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2205.08325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08325v1)
- **Published**: 2022-05-17 13:21:20+00:00
- **Updated**: 2022-05-17 13:21:20+00:00
- **Authors**: Zachary Seymour, Niluthpol Chowdhury Mithun, Han-Pang Chiu, Supun Samarasekera, Rakesh Kumar
- **Comment**: ICPR 2022
- **Journal**: None
- **Summary**: Understanding the geometric relationships between objects in a scene is a core capability in enabling both humans and autonomous agents to navigate in new environments. A sparse, unified representation of the scene topology will allow agents to act efficiently to move through their environment, communicate the environment state with others, and utilize the representation for diverse downstream tasks. To this end, we propose a method to train an autonomous agent to learn to accumulate a 3D scene graph representation of its environment by simultaneously learning to navigate through said environment. We demonstrate that our approach, GraphMapper, enables the learning of effective navigation policies through fewer interactions with the environment than vision-based systems alone. Further, we show that GraphMapper can act as a modular scene encoder to operate alongside existing Learning-based solutions to not only increase navigational efficiency but also generate intermediate scene representations that are useful for other future tasks.



### Privacy Preserving Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2205.10120v6
- **DOI**: 10.1007/978-3-031-16446-0_13
- **Categories**: **cs.CV**, cs.AI, cs.CR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.10120v6)
- **Published**: 2022-05-17 14:00:58+00:00
- **Updated**: 2023-03-09 16:39:58+00:00
- **Authors**: Riccardo Taiello, Melek Önen, Francesco Capano, Olivier Humbert, Marco Lorenzi
- **Comment**: None
- **Journal**: Medical Image Computing and Computer Assisted Intervention (2022)
  130-140
- **Summary**: Image registration is a key task in medical imaging applications, allowing to represent medical images in a common spatial reference frame. Current approaches to image registration are generally based on the assumption that the content of the images is usually accessible in clear form, from which the spatial transformation is subsequently estimated. This common assumption may not be met in practical applications, since the sensitive nature of medical images may ultimately require their analysis under privacy constraints, preventing to openly share the image content.In this work, we formulate the problem of image registration under a privacy preserving regime, where images are assumed to be confidential and cannot be disclosed in clear. We derive our privacy preserving image registration framework by extending classical registration paradigms to account for advanced cryptographic tools, such as secure multi-party computation and homomorphic encryption, that enable the execution of operations without leaking the underlying data. To overcome the problem of performance and scalability of cryptographic tools in high dimensions, we propose several techniques to optimize the image registration operations by using gradient approximations, and by revisiting the use of homomorphic encryption trough packing, to allow the efficient encryption and multiplication of large matrices. We demonstrate our privacy preserving framework in linear and non-linear registration problems, evaluating its accuracy and scalability with respect to standard, non-private counterparts. Our results show that privacy preserving image registration is feasible and can be adopted in sensitive medical imaging applications.



### HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2205.08390v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.08390v2)
- **Published**: 2022-05-17 14:11:07+00:00
- **Updated**: 2022-07-15 05:09:37+00:00
- **Authors**: Yuhao Mo, Chu Han, Yu Liu, Min Liu, Zhenwei Shi, Jiatai Lin, Bingchao Zhao, Chunwang Huang, Bingjiang Qiu, Yanfen Cui, Lei Wu, Xipeng Pan, Zeyan Xu, Xiaomei Huang, Zaiyi Liu, Ying Wang, Changhong Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasonography is an important routine examination for breast cancer diagnosis, due to its non-invasive, radiation-free and low-cost properties. However, the diagnostic accuracy of breast cancer is still limited due to its inherent limitations. It would be a tremendous success if we can precisely diagnose breast cancer by breast ultrasound images (BUS). Many learning-based computer-aided diagnostic methods have been proposed to achieve breast cancer diagnosis/lesion classification. However, most of them require a pre-define ROI and then classify the lesion inside the ROI. Conventional classification backbones, such as VGG16 and ResNet50, can achieve promising classification results with no ROI requirement. But these models lack interpretability, thus restricting their use in clinical practice. In this study, we propose a novel ROI-free model for breast cancer diagnosis in ultrasound images with interpretable feature representations. We leverage the anatomical prior knowledge that malignant and benign tumors have different spatial relationships between different tissue layers, and propose a HoVer-Transformer to formulate this prior knowledge. The proposed HoVer-Trans block extracts the inter- and intra-layer spatial information horizontally and vertically. We conduct and release an open dataset GDPH&SYSUCC for breast cancer diagnosis in BUS. The proposed model is evaluated in three datasets by comparing with four CNN-based models and two vision transformer models via five-fold cross validation. It achieves state-of-the-art classification performance with the best model interpretability. In the meanwhile, our proposed model outperforms two senior sonographers on the breast cancer diagnosis when only one BUS image is given.



### Semi-Supervised Building Footprint Generation with Feature and Output Consistency Training
- **Arxiv ID**: http://arxiv.org/abs/2205.08416v1
- **DOI**: 10.1109/TGRS.2022.3174636
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08416v1)
- **Published**: 2022-05-17 14:55:13+00:00
- **Updated**: 2022-05-17 14:55:13+00:00
- **Authors**: Qingyu Li, Yilei Shi, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and reliable building footprint maps are vital to urban planning and monitoring, and most existing approaches fall back on convolutional neural networks (CNNs) for building footprint generation. However, one limitation of these methods is that they require strong supervisory information from massive annotated samples for network learning. State-of-the-art semi-supervised semantic segmentation networks with consistency training can help to deal with this issue by leveraging a large amount of unlabeled data, which encourages the consistency of model output on data perturbation. Considering that rich information is also encoded in feature maps, we propose to integrate the consistency of both features and outputs in the end-to-end network training of unlabeled samples, enabling to impose additional constraints. Prior semi-supervised semantic segmentation networks have established the cluster assumption, in which the decision boundary should lie in the vicinity of low sample density. In this work, we observe that for building footprint generation, the low-density regions are more apparent at the intermediate feature representations within the encoder than the encoder's input or output. Therefore, we propose an instruction to assign the perturbation to the intermediate feature representations within the encoder, which considers the spatial resolution of input remote sensing imagery and the mean size of individual buildings in the study area. The proposed method is evaluated on three datasets with different resolutions: Planet dataset (3 m/pixel), Massachusetts dataset (1 m/pixel), and Inria dataset (0.3 m/pixel). Experimental results show that the proposed approach can well extract more complete building structures and alleviate omission errors.



### Conditional Visual Servoing for Multi-Step Tasks
- **Arxiv ID**: http://arxiv.org/abs/2205.08441v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.08441v1)
- **Published**: 2022-05-17 15:34:54+00:00
- **Updated**: 2022-05-17 15:34:54+00:00
- **Authors**: Sergio Izquierdo, Max Argus, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Servoing has been effectively used to move a robot into specific target locations or to track a recorded demonstration. It does not require manual programming, but it is typically limited to settings where one demonstration maps to one environment state. We propose a modular approach to extend visual servoing to scenarios with multiple demonstration sequences. We call this conditional servoing, as we choose the next demonstration conditioned on the observation of the robot. This method presents an appealing strategy to tackle multi-step problems, as individual demonstrations can be combined flexibly into a control policy. We propose different selection functions and compare them on a shape-sorting task in simulation. With the reprojection error yielding the best overall results, we implement this selection function on a real robot and show the efficacy of the proposed conditional servoing. For videos of our experiments, please check out our project page: https://lmb.informatik.uni-freiburg.de/projects/conditional_servoing/



### Application of Graph Based Features in Computer Aided Diagnosis for Histopathological Image Classification of Gastric Cancer
- **Arxiv ID**: http://arxiv.org/abs/2205.08467v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.08467v1)
- **Published**: 2022-05-17 16:16:29+00:00
- **Updated**: 2022-05-17 16:16:29+00:00
- **Authors**: Haiqing Zhang, Chen Li, Shiliang Ai, Haoyuan Chen, Yuchao Zheng, Yixin Li, Xiaoyan Li, Hongzan Sun, Xinyu Huang, Marcin Grzegorzek
- **Comment**: None
- **Journal**: None
- **Summary**: The gold standard for gastric cancer detection is gastric histopathological image analysis, but there are certain drawbacks in the existing histopathological detection and diagnosis. In this paper, based on the study of computer aided diagnosis system, graph based features are applied to gastric cancer histopathology microscopic image analysis, and a classifier is used to classify gastric cancer cells from benign cells. Firstly, image segmentation is performed, and after finding the region, cell nuclei are extracted using the k-means method, the minimum spanning tree (MST) is drawn, and graph based features of the MST are extracted. The graph based features are then put into the classifier for classification. In this study, different segmentation methods are compared in the tissue segmentation stage, among which are Level-Set, Otsu thresholding, watershed, SegNet, U-Net and Trans-U-Net segmentation; Graph based features, Red, Green, Blue features, Grey-Level Co-occurrence Matrix features, Histograms of Oriented Gradient features and Local Binary Patterns features are compared in the feature extraction stage; Radial Basis Function (RBF) Support Vector Machine (SVM), Linear SVM, Artificial Neural Network, Random Forests, k-NearestNeighbor, VGG16, and Inception-V3 are compared in the classifier stage. It is found that using U-Net to segment tissue areas, then extracting graph based features, and finally using RBF SVM classifier gives the optimal results with 94.29%.



### ColonFormer: An Efficient Transformer based Method for Colon Polyp Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.08473v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08473v3)
- **Published**: 2022-05-17 16:34:04+00:00
- **Updated**: 2022-06-07 14:23:55+00:00
- **Authors**: Nguyen Thanh Duc, Nguyen Thi Oanh, Nguyen Thi Thuy, Tran Minh Triet, Dinh Viet Sang
- **Comment**: None
- **Journal**: None
- **Summary**: Identifying polyps is challenging for automatic analysis of endoscopic images in computer-aided clinical support systems. Models based on convolutional networks (CNN), transformers, and their combinations have been proposed to segment polyps with promising results. However, those approaches have limitations either in modeling the local appearance of the polyps only or lack of multi-level features for spatial dependency in the decoding process. This paper proposes a novel network, namely ColonFormer, to address these limitations. ColonFormer is an encoder-decoder architecture capable of modeling long-range semantic information at both encoder and decoder branches. The encoder is a lightweight architecture based on transformers for modeling global semantic relations at multi scales. The decoder is a hierarchical network structure designed for learning multi-level features to enrich feature representation. Besides, a refinement module is added with a new skip connection technique to refine the boundary of polyp objects in the global map for accurate segmentation. Extensive experiments have been conducted on five popular benchmark datasets for polyp segmentation, including Kvasir, CVC-Clinic DB, CVC-ColonDB, CVC-T, and ETIS-Larib. Experimental results show that our ColonFormer outperforms other state-of-the-art methods on all benchmark datasets.



### A CLIP-Hitchhiker's Guide to Long Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2205.08508v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08508v1)
- **Published**: 2022-05-17 17:26:23+00:00
- **Updated**: 2022-05-17 17:26:23+00:00
- **Authors**: Max Bain, Arsha Nagrani, Gül Varol, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: Our goal in this paper is the adaptation of image-text models for long video retrieval. Recent works have demonstrated state-of-the-art performance in video retrieval by adopting CLIP, effectively hitchhiking on the image-text representation for video tasks. However, there has been limited success in learning temporal aggregation that outperform mean-pooling the image-level representations extracted per frame by CLIP. We find that the simple yet effective baseline of weighted-mean of frame embeddings via query-scoring is a significant improvement above all prior temporal modelling attempts and mean-pooling. In doing so, we provide an improved baseline for others to compare to and demonstrate state-of-the-art performance of this simple baseline on a suite of long video retrieval benchmarks.



### Unsupervised Segmentation in Real-World Images via Spelke Object Inference
- **Arxiv ID**: http://arxiv.org/abs/2205.08515v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2205.08515v2)
- **Published**: 2022-05-17 17:39:24+00:00
- **Updated**: 2022-07-25 16:24:49+00:00
- **Authors**: Honglin Chen, Rahul Venkatesh, Yoni Friedman, Jiajun Wu, Joshua B. Tenenbaum, Daniel L. K. Yamins, Daniel M. Bear
- **Comment**: 25 pages, 10 figures
- **Journal**: None
- **Summary**: Self-supervised, category-agnostic segmentation of real-world images is a challenging open problem in computer vision. Here, we show how to learn static grouping priors from motion self-supervision by building on the cognitive science concept of a Spelke Object: a set of physical stuff that moves together. We introduce the Excitatory-Inhibitory Segment Extraction Network (EISEN), which learns to extract pairwise affinity graphs for static scenes from motion-based training signals. EISEN then produces segments from affinities using a novel graph propagation and competition network. During training, objects that undergo correlated motion (such as robot arms and the objects they move) are decoupled by a bootstrapping process: EISEN explains away the motion of objects it has already learned to segment. We show that EISEN achieves a substantial improvement in the state of the art for self-supervised image segmentation on challenging synthetic and real-world robotics datasets.



### Do Neural Networks Compress Manifolds Optimally?
- **Arxiv ID**: http://arxiv.org/abs/2205.08518v2
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2205.08518v2)
- **Published**: 2022-05-17 17:41:53+00:00
- **Updated**: 2022-09-10 01:26:14+00:00
- **Authors**: Sourbh Bhadane, Aaron B. Wagner, Johannes Ballé
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial Neural-Network-based (ANN-based) lossy compressors have recently obtained striking results on several sources. Their success may be ascribed to an ability to identify the structure of low-dimensional manifolds in high-dimensional ambient spaces. Indeed, prior work has shown that ANN-based compressors can achieve the optimal entropy-distortion curve for some such sources. In contrast, we determine the optimal entropy-distortion tradeoffs for two low-dimensional manifolds with circular structure and show that state-of-the-art ANN-based compressors fail to optimally compress them.



### Self-supervised Neural Articulated Shape and Appearance Models
- **Arxiv ID**: http://arxiv.org/abs/2205.08525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08525v1)
- **Published**: 2022-05-17 17:50:47+00:00
- **Updated**: 2022-05-17 17:50:47+00:00
- **Authors**: Fangyin Wei, Rohan Chabra, Lingni Ma, Christoph Lassner, Michael Zollhöfer, Szymon Rusinkiewicz, Chris Sweeney, Richard Newcombe, Mira Slavcheva
- **Comment**: 15 pages. CVPR 2022. Project page available at
  https://weify627.github.io/nasam/
- **Journal**: None
- **Summary**: Learning geometry, motion, and appearance priors of object classes is important for the solution of a large variety of computer vision problems. While the majority of approaches has focused on static objects, dynamic objects, especially with controllable articulation, are less explored. We propose a novel approach for learning a representation of the geometry, appearance, and motion of a class of articulated objects given only a set of color images as input. In a self-supervised manner, our novel representation learns shape, appearance, and articulation codes that enable independent control of these semantic dimensions. Our model is trained end-to-end without requiring any articulation annotations. Experiments show that our approach performs well for different joint types, such as revolute and prismatic joints, as well as different combinations of these joints. Compared to state of the art that uses direct 3D supervision and does not output appearance, we recover more faithful geometry and appearance from 2D observations only. In addition, our representation enables a large variety of applications, such as few-shot reconstruction, the generation of novel articulations, and novel view-synthesis.



### Vision Transformer Adapter for Dense Predictions
- **Arxiv ID**: http://arxiv.org/abs/2205.08534v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08534v4)
- **Published**: 2022-05-17 17:59:11+00:00
- **Updated**: 2023-02-13 15:50:22+00:00
- **Authors**: Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, Yu Qiao
- **Comment**: Accepted to ICLR 2023
- **Journal**: None
- **Summary**: This work investigates a simple yet powerful dense prediction task adapter for Vision Transformer (ViT). Unlike recently advanced variants that incorporate vision-specific inductive biases into their architectures, the plain ViT suffers inferior performance on dense predictions due to weak prior assumptions. To address this issue, we propose the ViT-Adapter, which allows plain ViT to achieve comparable performance to vision-specific transformers. Specifically, the backbone in our framework is a plain ViT that can learn powerful representations from large-scale multi-modal data. When transferring to downstream tasks, a pre-training-free adapter is used to introduce the image-related inductive biases into the model, making it suitable for these tasks. We verify ViT-Adapter on multiple dense prediction tasks, including object detection, instance segmentation, and semantic segmentation. Notably, without using extra detection data, our ViT-Adapter-L yields state-of-the-art 60.9 box AP and 53.0 mask AP on COCO test-dev. We hope that the ViT-Adapter could serve as an alternative for vision-specific transformers and facilitate future research. The code and models will be released at https://github.com/czczup/ViT-Adapter.



### AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars
- **Arxiv ID**: http://arxiv.org/abs/2205.08535v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08535v1)
- **Published**: 2022-05-17 17:59:19+00:00
- **Updated**: 2022-05-17 17:59:19+00:00
- **Authors**: Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, Ziwei Liu
- **Comment**: SIGGRAPH 2022; Project Page
  https://hongfz16.github.io/projects/AvatarCLIP.html Codes available at
  https://github.com/hongfz16/AvatarCLIP
- **Journal**: None
- **Summary**: 3D avatar creation plays a crucial role in the digital age. However, the whole production process is prohibitively time-consuming and labor-intensive. To democratize this technology to a larger audience, we propose AvatarCLIP, a zero-shot text-driven framework for 3D avatar generation and animation. Unlike professional software that requires expert knowledge, AvatarCLIP empowers layman users to customize a 3D avatar with the desired shape and texture, and drive the avatar with the described motions using solely natural languages. Our key insight is to take advantage of the powerful vision-language model CLIP for supervising neural human generation, in terms of 3D geometry, texture and animation. Specifically, driven by natural language descriptions, we initialize 3D human geometry generation with a shape VAE network. Based on the generated 3D human shapes, a volume rendering model is utilized to further facilitate geometry sculpting and texture generation. Moreover, by leveraging the priors learned in the motion VAE, a CLIP-guided reference-based motion synthesis method is proposed for the animation of the generated 3D avatar. Extensive qualitative and quantitative experiments validate the effectiveness and generalizability of AvatarCLIP on a wide range of avatars. Remarkably, AvatarCLIP can generate unseen 3D avatars with novel animations, achieving superior zero-shot capability.



### Disentangling Visual Embeddings for Attributes and Objects
- **Arxiv ID**: http://arxiv.org/abs/2205.08536v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.08536v1)
- **Published**: 2022-05-17 17:59:36+00:00
- **Updated**: 2022-05-17 17:59:36+00:00
- **Authors**: Nirat Saini, Khoi Pham, Abhinav Shrivastava
- **Comment**: To appear at CVPR 2022 (Oral)
- **Journal**: None
- **Summary**: We study the problem of compositional zero-shot learning for object-attribute recognition. Prior works use visual features extracted with a backbone network, pre-trained for object classification and thus do not capture the subtly distinct features associated with attributes. To overcome this challenge, these studies employ supervision from the linguistic space, and use pre-trained word embeddings to better separate and compose attribute-object pairs for recognition. Analogous to linguistic embedding space, which already has unique and agnostic embeddings for object and attribute, we shift the focus back to the visual space and propose a novel architecture that can disentangle attribute and object features in the visual space. We use visual decomposed features to hallucinate embeddings that are representative for the seen and novel compositions to better regularize the learning of our model. Extensive experiments show that our method outperforms existing work with significant margin on three datasets: MIT-States, UT-Zappos, and a new benchmark created based on VAW. The code, models, and dataset splits are publicly available at https://github.com/nirat1606/OADis.



### Text Detection & Recognition in the Wild for Robot Localization
- **Arxiv ID**: http://arxiv.org/abs/2205.08565v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.08565v2)
- **Published**: 2022-05-17 18:16:34+00:00
- **Updated**: 2022-05-19 14:04:10+00:00
- **Authors**: Zobeir Raisi, John Zelek
- **Comment**: 6 papged, VI section, typos corrected, revison changes, no result
  changes
- **Journal**: None
- **Summary**: Signage is everywhere and a robot should be able to take advantage of signs to help it localize (including Visual Place Recognition (VPR)) and map. Robust text detection & recognition in the wild is challenging due to such factors as pose, irregular text, illumination, and occlusion. We propose an end-to-end scene text spotting model that simultaneously outputs the text string and bounding boxes. This model is more suitable for VPR. Our central contribution is introducing utilizing an end-to-end scene text spotting framework to adequately capture the irregular and occluded text regions in different challenging places. To evaluate our proposed architecture's performance for VPR, we conducted several experiments on the challenging Self-Collected Text Place (SCTP) benchmark dataset. The initial experimental results show that the proposed method outperforms the SOTA methods in terms of precision and recall when tested on this benchmark.



### Label-Efficient Self-Supervised Federated Learning for Tackling Data Heterogeneity in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2205.08576v2
- **DOI**: 10.1109/TMI.2022.3233574
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.08576v2)
- **Published**: 2022-05-17 18:33:43+00:00
- **Updated**: 2023-01-11 07:30:24+00:00
- **Authors**: Rui Yan, Liangqiong Qu, Qingyue Wei, Shih-Cheng Huang, Liyue Shen, Daniel Rubin, Lei Xing, Yuyin Zhou
- **Comment**: Code and trained models are available at:
  https://github.com/rui-yan/SSL-FL
- **Journal**: None
- **Summary**: The collection and curation of large-scale medical datasets from multiple institutions is essential for training accurate deep learning models, but privacy concerns often hinder data sharing. Federated learning (FL) is a promising solution that enables privacy-preserving collaborative learning among different institutions, but it generally suffers from performance deterioration due to heterogeneous data distributions and a lack of quality labeled data. In this paper, we present a robust and label-efficient self-supervised FL framework for medical image analysis. Our method introduces a novel Transformer-based self-supervised pre-training paradigm that pre-trains models directly on decentralized target task datasets using masked image modeling, to facilitate more robust representation learning on heterogeneous data and effective knowledge transfer to downstream models. Extensive empirical results on simulated and real-world medical imaging non-IID federated datasets show that masked image modeling with Transformers significantly improves the robustness of models against various degrees of data heterogeneity. Notably, under severe data heterogeneity, our method, without relying on any additional pre-training data, achieves an improvement of 5.06%, 1.53% and 4.58% in test accuracy on retinal, dermatology and chest X-ray classification compared to the supervised baseline with ImageNet pre-training. In addition, we show that our federated self-supervised pre-training methods yield models that generalize better to out-of-distribution data and perform more effectively when fine-tuning with limited labeled data, compared to existing FL algorithms. The code is available at https://github.com/rui-yan/SSL-FL.



### RARITYNet: Rarity Guided Affective Emotion Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/2205.08595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08595v1)
- **Published**: 2022-05-17 19:27:46+00:00
- **Updated**: 2022-05-17 19:27:46+00:00
- **Authors**: Monu Verma, Santosh Kumar Vipparthi
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired from the assets of handcrafted and deep learning approaches, we proposed a RARITYNet: RARITY guided affective emotion learning framework to learn the appearance features and identify the emotion class of facial expressions. The RARITYNet framework is designed by combining the shallow (RARITY) and deep (AffEmoNet) features to recognize the facial expressions from challenging images as spontaneous expressions, pose variations, ethnicity changes, and illumination conditions. The RARITY is proposed to encode the inter-radial transitional patterns in the local neighbourhood. The AffEmoNet: affective emotion learning network is proposed by incorporating three feature streams: high boost edge filtering (HBSEF) stream, to extract the edge information of highly affected facial expressive regions, multi-scale sophisticated edge cumulative (MSSEC) stream is to learns the sophisticated edge information from multi-receptive fields and RARITY uplift complementary context feature (RUCCF) stream refines the RARITY-encoded features and aid the MSSEC stream features to enrich the learning ability of RARITYNet.



### Towards Robust Low Light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2205.08615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.08615v1)
- **Published**: 2022-05-17 20:14:18+00:00
- **Updated**: 2022-05-17 20:14:18+00:00
- **Authors**: Sara Aghajanzadeh, David Forsyth
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of making brighter images from dark images found in the wild. The images are dark because they are taken in dim environments. They suffer from color shifts caused by quantization and from sensor noise. We don't know the true camera reponse function for such images and they are not RAW. We use a supervised learning method, relying on a straightforward simulation of an imaging pipeline to generate usable dataset for training and testing. On a number of standard datasets, our approach outperforms the state of the art quantitatively. Qualitative comparisons suggest strong improvements in reconstruction accuracy.



### Semantically Accurate Super-Resolution Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2205.08659v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.08659v1)
- **Published**: 2022-05-17 23:05:27+00:00
- **Updated**: 2022-05-17 23:05:27+00:00
- **Authors**: Tristan Frizza, Donald G. Dansereau, Nagita Mehr Seresht, Michael Bewley
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: This work addresses the problems of semantic segmentation and image super-resolution by jointly considering the performance of both in training a Generative Adversarial Network (GAN). We propose a novel architecture and domain-specific feature loss, allowing super-resolution to operate as a pre-processing step to increase the performance of downstream computer vision tasks, specifically semantic segmentation. We demonstrate this approach using Nearmap's aerial imagery dataset which covers hundreds of urban areas at 5-7 cm per pixel resolution. We show the proposed approach improves perceived image quality as well as quantitative segmentation accuracy across all prediction classes, yielding an average accuracy improvement of 11.8% and 108% at 4x and 32x super-resolution, compared with state-of-the art single-network methods. This work demonstrates that jointly considering image-based and task-specific losses can improve the performance of both, and advances the state-of-the-art in semantic-aware super-resolution of aerial imagery.



### Exploring the Adjugate Matrix Approach to Quaternion Pose Extraction
- **Arxiv ID**: http://arxiv.org/abs/2205.09116v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2205.09116v1)
- **Published**: 2022-05-17 23:20:55+00:00
- **Updated**: 2022-05-17 23:20:55+00:00
- **Authors**: Andrew J. Hanson, Sonya M. Hanson
- **Comment**: 67 pages, 5 appendices, 9 figures
- **Journal**: None
- **Summary**: Quaternions are important for a wide variety of rotation-related problems in computer graphics, machine vision, and robotics. We study the nontrivial geometry of the relationship between quaternions and rotation matrices by exploiting the adjugate matrix of the characteristic equation of a related eigenvalue problem to obtain the manifold of the space of a quaternion eigenvector. We argue that quaternions parameterized by their corresponding rotation matrices cannot be expressed, for example, in machine learning tasks, as single-valued functions: the quaternion solution must instead be treated as a manifold, with different algebraic solutions for each of several single-valued sectors represented by the adjugate matrix. We conclude with novel constructions exploiting the quaternion adjugate variables to revisit several classic pose estimation applications: 2D point-cloud matching, 2D point-cloud-to-projection matching, 3D point-cloud matching, 3D orthographic point-cloud-to-projection matching, and 3D perspective point-cloud-to-projection matching. We find an exact solution to the 3D orthographic least squares pose extraction problem, and apply it successfully also to the perspective pose extraction problem with results that improve on existing methods.



