# Arxiv Papers in cs.CV on 2022-05-01
### Traffic Context Aware Data Augmentation for Rare Object Detection in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2205.00376v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00376v2)
- **Published**: 2022-05-01 01:45:00+00:00
- **Updated**: 2022-07-02 08:04:12+00:00
- **Authors**: Naifan Li, Fan Song, Ying Zhang, Pengpeng Liang, Erkang Cheng
- **Comment**: The IEEE Conference on Robotics and Automation, ICRA 2022
- **Journal**: None
- **Summary**: Detection of rare objects (e.g., traffic cones, traffic barrels and traffic warning triangles) is an important perception task to improve the safety of autonomous driving. Training of such models typically requires a large number of annotated data which is expensive and time consuming to obtain. To address the above problem, an emerging approach is to apply data augmentation to automatically generate cost-free training samples. In this work, we propose a systematic study on simple Copy-Paste data augmentation for rare object detection in autonomous driving. Specifically, local adaptive instance-level image transformation is introduced to generate realistic rare object masks from source domain to the target domain. Moreover, traffic scene context is utilized to guide the placement of masks of rare objects. To this end, our data augmentation generates training data with high quality and realistic characteristics by leveraging both local and global consistency. In addition, we build a new dataset, Rare Object Dataset (ROD), consisting 10k training images, 4k validation images and the corresponding labels with a diverse range of scenarios in autonomous driving. Experiments on ROD show that our method achieves promising results on rare object detection. We also present a thorough study to illustrate the effectiveness of our local-adaptive and global constraints based Copy-Paste data augmentation for rare object detection. The data, development kit and more information of ROD are available online at: \url{https://nullmax-vision.github.io}.



### Geometric Graph Representation with Learnable Graph Structure and Adaptive AU Constraint for Micro-Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.00380v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.00380v2)
- **Published**: 2022-05-01 02:20:43+00:00
- **Updated**: 2023-03-03 09:09:01+00:00
- **Authors**: Jinsheng Wei, Wei Peng, Guanming Lu, Yante Li, Jingjie Yan, Guoying Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Micro-expression recognition (MER) is valuable because micro-expressions (MEs) can reveal genuine emotions. Most works take image sequences as input and cannot effectively explore ME information because subtle ME-related motions are easily submerged in unrelated information. Instead, the facial landmark is a low-dimensional and compact modality, which achieves lower computational cost and potentially concentrates on ME-related movement features. However, the discriminability of facial landmarks for MER is unclear. Thus, this paper explores the contribution of facial landmarks and proposes a novel framework to efficiently recognize MEs. Firstly, a geometric two-stream graph network is constructed to aggregate the low-order and high-order geometric movement information from facial landmarks to obtain discriminative ME representation. Secondly, a self-learning fashion is introduced to automatically model the dynamic relationship between nodes even long-distance nodes. Furthermore, an adaptive action unit loss is proposed to reasonably build the strong correlation between landmarks, facial action units and MEs. Notably, this work provides a novel idea with much higher efficiency to promote MER, only utilizing graph-based geometric features. The experimental results demonstrate that the proposed method achieves competitive performance with a significantly reduced computational cost. Furthermore, facial landmarks significantly contribute to MER and are worth further study for high-efficient ME analysis.



### Convex Combination Consistency between Neighbors for Weakly-supervised Action Localization
- **Arxiv ID**: http://arxiv.org/abs/2205.00400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00400v1)
- **Published**: 2022-05-01 05:30:53+00:00
- **Updated**: 2022-05-01 05:30:53+00:00
- **Authors**: Qinying Liu, Zilei Wang, Ruoxi Chen, Zhilin Li
- **Comment**: None
- **Journal**: None
- **Summary**: In weakly-supervised temporal action localization (WS-TAL), the methods commonly follow the "localization by classification" procedure, which uses the snippet predictions to form video class scores and then optimizes a video classification loss. In this procedure, the snippet predictions (or snippet attention weights) are used to separate foreground and background. However, the snippet predictions are usually inaccurate due to absence of frame-wise labels, and then the overall performance is hindered. In this paper, we propose a novel C$^3$BN to achieve robust snippet predictions. C$^3$BN includes two key designs by exploring the inherent characteristics of video data. First, because of the natural continuity of adjacent snippets, we propose a micro data augmentation strategy to increase the diversity of snippets with convex combination of adjacent snippets. Second, we propose a macro-micro consistency regularization strategy to force the model to be invariant (or equivariant) to the transformations of snippets with respect to video semantics, snippet predictions and snippet features. Experimental results demonstrate the effectiveness of our proposed method on top of baselines for the WS-TAL tasks with video-level and point-level supervision.



### Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions
- **Arxiv ID**: http://arxiv.org/abs/2205.00415v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.00415v2)
- **Published**: 2022-05-01 07:51:22+00:00
- **Updated**: 2023-02-07 03:58:25+00:00
- **Authors**: Mihir Parmar, Swaroop Mishra, Mor Geva, Chitta Baral
- **Comment**: Accepted to EACL 2023
- **Journal**: None
- **Summary**: In recent years, progress in NLU has been driven by benchmarks. These benchmarks are typically collected by crowdsourcing, where annotators write examples based on annotation instructions crafted by dataset creators. In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write many similar examples that are then over-represented in the collected data. We study this form of bias, termed instruction bias, in 14 recent NLU benchmarks, showing that instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data. This extends previous work (Geva et al., 2019) and raises a new concern of whether we are modeling the dataset creator's instructions, rather than the task. Through a series of experiments, we show that, indeed, instruction bias can lead to overestimation of model performance, and that models struggle to generalize beyond biases originating in the crowdsourcing instructions. We further analyze the influence of instruction bias in terms of pattern frequency and model size, and derive concrete recommendations for creating future NLU benchmarks.



### UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/2205.00423v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00423v2)
- **Published**: 2022-05-01 08:36:18+00:00
- **Updated**: 2022-05-03 09:25:00+00:00
- **Authors**: Cheng Chen, Yudong Zhu, Zhenshan Tan, Qingrong Cheng, Xin Jiang, Qun Liu, Xiaodong Gu
- **Comment**: Accepted in CVPR 2022
- **Journal**: None
- **Summary**: Visual Dialog aims to answer multi-round, interactive questions based on the dialog history and image content. Existing methods either consider answer ranking and generating individually or only weakly capture the relation across the two tasks implicitly by two separate models. The research on a universal framework that jointly learns to rank and generate answers in a single model is seldom explored. In this paper, we propose a contrastive learning-based framework UTC to unify and facilitate both discriminative and generative tasks in visual dialog with a single model. Specifically, considering the inherent limitation of the previous learning paradigm, we devise two inter-task contrastive losses i.e., context contrastive loss and answer contrastive loss to make the discriminative and generative tasks mutually reinforce each other. These two complementary contrastive losses exploit dialog context and target answer as anchor points to provide representation learning signals from different perspectives. We evaluate our proposed UTC on the VisDial v1.0 dataset, where our method outperforms the state-of-the-art on both discriminative and generative tasks and surpasses previous state-of-the-art generative methods by more than 2 absolute points on Recall@1.



### Analysis of Diffractive Neural Networks for Seeing Through Random Diffusers
- **Arxiv ID**: http://arxiv.org/abs/2205.00428v1
- **DOI**: 10.1109/JSTQE.2022.3194574
- **Categories**: **physics.optics**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2205.00428v1)
- **Published**: 2022-05-01 09:12:24+00:00
- **Updated**: 2022-05-01 09:12:24+00:00
- **Authors**: Yuhang Li, Yi Luo, Bijie Bai, Aydogan Ozcan
- **Comment**: 42 Pages, 9 Figures
- **Journal**: IEEE Journal of Selected Topics in Quantum Electronics (2022)
- **Summary**: Imaging through diffusive media is a challenging problem, where the existing solutions heavily rely on digital computers to reconstruct distorted images. We provide a detailed analysis of a computer-free, all-optical imaging method for seeing through random, unknown phase diffusers using diffractive neural networks, covering different deep learning-based training strategies. By analyzing various diffractive networks designed to image through random diffusers with different correlation lengths, a trade-off between the image reconstruction fidelity and distortion reduction capability of the diffractive network was observed. During its training, random diffusers with a range of correlation lengths were used to improve the diffractive network's generalization performance. Increasing the number of random diffusers used in each epoch reduced the overfitting of the diffractive network's imaging performance to known diffusers. We also demonstrated that the use of additional diffractive layers improved the generalization capability to see through new, random diffusers. Finally, we introduced deliberate misalignments in training to 'vaccinate' the network against random layer-to-layer shifts that might arise due to the imperfect assembly of the diffractive networks. These analyses provide a comprehensive guide in designing diffractive networks to see through random diffusers, which might profoundly impact many fields, such as biomedical imaging, atmospheric physics, and autonomous driving.



### Reinforced Swin-Convs Transformer for Underwater Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2205.00434v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.00434v1)
- **Published**: 2022-05-01 09:46:33+00:00
- **Updated**: 2022-05-01 09:46:33+00:00
- **Authors**: Tingdi Ren, Haiyong Xu, Gangyi Jiang, Mei Yu, Ting Luo
- **Comment**: Submitted by NeurIPS 2022
- **Journal**: None
- **Summary**: Underwater Image Enhancement (UIE) technology aims to tackle the challenge of restoring the degraded underwater images due to light absorption and scattering. To address problems, a novel U-Net based Reinforced Swin-Convs Transformer for the Underwater Image Enhancement method (URSCT-UIE) is proposed. Specifically, with the deficiency of U-Net based on pure convolutions, we embedded the Swin Transformer into U-Net for improving the ability to capture the global dependency. Then, given the inadequacy of the Swin Transformer capturing the local attention, the reintroduction of convolutions may capture more local attention. Thus, we provide an ingenious manner for the fusion of convolutions and the core attention mechanism to build a Reinforced Swin-Convs Transformer Block (RSCTB) for capturing more local attention, which is reinforced in the channel and the spatial attention of the Swin Transformer. Finally, the experimental results on available datasets demonstrate that the proposed URSCT-UIE achieves state-of-the-art performance compared with other methods in terms of both subjective and objective evaluations. The code will be released on GitHub after acceptance.



### A Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2205.00463v2
- **DOI**: 10.1088/1361-6420/ac8ac6
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2205.00463v2)
- **Published**: 2022-05-01 13:05:04+00:00
- **Updated**: 2022-10-05 07:02:42+00:00
- **Authors**: Qiaoqiao Ding, Hui Ji, Yuhui Quan, Xiaoqun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Low-dose CT (LDCT) imaging attracted a considerable interest for the reduction of the object's exposure to X-ray radiation. In recent years, supervised deep learning (DL) has been extensively studied for LDCT image reconstruction, which trains a network over a dataset containing many pairs of normal-dose and low-dose images. However, the challenge on collecting many such pairs in the clinical setup limits the application of such supervised-learning-based methods for LDCT image reconstruction in practice. Aiming at addressing the challenges raised by the collection of training dataset, this paper proposed a unsupervised deep learning method for LDCT image reconstruction, which does not require any external training data. The proposed method is built on a re-parametrization technique for Bayesian inference via deep network with random weights, combined with additional total variational~(TV) regularization. The experiments show that the proposed method noticeably outperforms existing dataset-free image reconstruction methods on the test data.



### Preserve Pre-trained Knowledge: Transfer Learning With Self-Distillation For Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.00506v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.00506v1)
- **Published**: 2022-05-01 16:31:25+00:00
- **Updated**: 2022-05-01 16:31:25+00:00
- **Authors**: Yang Zhou, Zhanhao He, Keyu Lu, Guanhong Wang, Gaoang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Video-based action recognition is one of the most popular topics in computer vision. With recent advances of selfsupervised video representation learning approaches, action recognition usually follows a two-stage training framework, i.e., self-supervised pre-training on large-scale unlabeled sets and transfer learning on a downstream labeled set. However, catastrophic forgetting of the pre-trained knowledge becomes the main issue in the downstream transfer learning of action recognition, resulting in a sub-optimal solution. In this paper, to alleviate the above issue, we propose a novel transfer learning approach that combines self-distillation in fine-tuning to preserve knowledge from the pre-trained model learned from the large-scale dataset. Specifically, we fix the encoder from the last epoch as the teacher model to guide the training of the encoder from the current epoch in the transfer learning. With such a simple yet effective learning strategy, we outperform state-of-the-art methods on widely used UCF101 and HMDB51 datasets in action recognition task.



### The Best of Both Worlds: Combining Model-based and Nonparametric Approaches for 3D Human Body Estimation
- **Arxiv ID**: http://arxiv.org/abs/2205.00508v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00508v1)
- **Published**: 2022-05-01 16:39:09+00:00
- **Updated**: 2022-05-01 16:39:09+00:00
- **Authors**: Zhe Wang, Jimei Yang, Charless Fowlkes
- **Comment**: None
- **Journal**: CVPR ABAW 2022
- **Summary**: Nonparametric based methods have recently shown promising results in reconstructing human bodies from monocular images while model-based methods can help correct these estimates and improve prediction. However, estimating model parameters from global image features may lead to noticeable misalignment between the estimated meshes and image evidence. To address this issue and leverage the best of both worlds, we propose a framework of three consecutive modules. A dense map prediction module explicitly establishes the dense UV correspondence between the image evidence and each part of the body model. The inverse kinematics module refines the key point prediction and generates a posed template mesh. Finally, a UV inpainting module relies on the corresponding feature, prediction and the posed template, and completes the predictions of occluded body shape. Our framework leverages the best of non-parametric and model-based methods and is also robust to partial occlusion. Experiments demonstrate that our framework outperforms existing 3D human estimation methods on multiple public benchmarks.



### Deep vs. Shallow Learning: A Benchmark Study in Low Magnitude Earthquake Detection
- **Arxiv ID**: http://arxiv.org/abs/2205.00525v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/2205.00525v1)
- **Published**: 2022-05-01 17:59:18+00:00
- **Updated**: 2022-05-01 17:59:18+00:00
- **Authors**: Akshat Goel, Denise Gorse
- **Comment**: None
- **Journal**: None
- **Summary**: While deep learning models have seen recent high uptake in the geosciences, and are appealing in their ability to learn from minimally processed input data, as black box models they do not provide an easy means to understand how a decision is reached, which in safety-critical tasks especially can be problematical. An alternative route is to use simpler, more transparent white box models, in which task-specific feature construction replaces the more opaque feature discovery process performed automatically within deep learning models. Using data from the Groningen Gas Field in the Netherlands, we build on an existing logistic regression model by the addition of four further features discovered using elastic net driven data mining within the catch22 time series analysis package. We then evaluate the performance of the augmented logistic regression model relative to a deep (CNN) model, pre-trained on the Groningen data, on progressively increasing noise-to-signal ratios. We discover that, for each ratio, our logistic regression model correctly detects every earthquake, while the deep model fails to detect nearly 20 % of seismic events, thus justifying at least a degree of caution in the application of deep models, especially to data with higher noise-to-signal ratios.



### COUCH: Towards Controllable Human-Chair Interactions
- **Arxiv ID**: http://arxiv.org/abs/2205.00541v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.00541v1)
- **Published**: 2022-05-01 19:14:22+00:00
- **Updated**: 2022-05-01 19:14:22+00:00
- **Authors**: Xiaohan Zhang, Bharat Lal Bhatnagar, Vladimir Guzov, Sebastian Starke, Gerard Pons-Moll
- **Comment**: None
- **Journal**: None
- **Summary**: Humans interact with an object in many different ways by making contact at different locations, creating a highly complex motion space that can be difficult to learn, particularly when synthesizing such human interactions in a controllable manner. Existing works on synthesizing human scene interaction focus on the high-level control of action but do not consider the fine-grained control of motion. In this work, we study the problem of synthesizing scene interactions conditioned on different contact positions on the object. As a testbed to investigate this new problem, we focus on human-chair interaction as one of the most common actions which exhibit large variability in terms of contacts. We propose a novel synthesis framework COUCH that plans ahead the motion by predicting contact-aware control signals of the hands, which are then used to synthesize contact-conditioned interactions. Furthermore, we contribute a large human-chair interaction dataset with clean annotations, the COUCH Dataset. Our method shows significant quantitative and qualitative improvements over existing methods for human-object interactions. More importantly, our method enables control of the motion through user-specified or automatically predicted contacts.



### Engineering deep learning methods on automatic detection of damage in infrastructure due to extreme events
- **Arxiv ID**: http://arxiv.org/abs/2205.02125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.02125v1)
- **Published**: 2022-05-01 19:55:56+00:00
- **Updated**: 2022-05-01 19:55:56+00:00
- **Authors**: Yongsheng Bai, Bing Zha, Halil Sezen, Alper Yilmaz
- **Comment**: Thanks for the revivers' help for improving this paper. Structural
  Health Monitoring (2022)
- **Journal**: None
- **Summary**: This paper presents a few comprehensive experimental studies for automated Structural Damage Detection (SDD) in extreme events using deep learning methods for processing 2D images. In the first study, a 152-layer Residual network (ResNet) is utilized to classify multiple classes in eight SDD tasks, which include identification of scene levels, damage levels, material types, etc. The proposed ResNet achieved high accuracy for each task while the positions of the damage are not identifiable. In the second study, the existing ResNet and a segmentation network (U-Net) are combined into a new pipeline, cascaded networks, for categorizing and locating structural damage. The results show that the accuracy of damage detection is significantly improved compared to only using a segmentation network. In the third and fourth studies, end-to-end networks are developed and tested as a new solution to directly detect cracks and spalling in the image collections of recent large earthquakes. One of the proposed networks can achieve an accuracy above 67.6% for all tested images at various scales and resolutions, and shows its robustness for these human-free detection tasks. As a preliminary field study, we applied the proposed method to detect damage in a concrete structure that was tested to study its progressive collapse performance. The experiments indicate that these solutions for automatic detection of structural damage using deep learning methods are feasible and promising. The training datasets and codes will be made available for the public upon the publication of this paper.



### Using a novel fractional-order gradient method for CNN back-propagation
- **Arxiv ID**: http://arxiv.org/abs/2205.00581v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, D.1.2; F.3.1; F.4.1, F.2.2, I.2.7 K.5
- **Links**: [PDF](http://arxiv.org/pdf/2205.00581v1)
- **Published**: 2022-05-01 23:38:06+00:00
- **Updated**: 2022-05-01 23:38:06+00:00
- **Authors**: Mundher Mohammed Taresh, Ningbo Zhu, Talal Ahmed Ali Ali, Mohammed Alghaili, Weihua Guo
- **Comment**: 9 pages, 6 figuers
- **Journal**: None
- **Summary**: Computer-aided diagnosis tools have experienced rapid growth and development in recent years. Among all, deep learning is the most sophisticated and popular tool. In this paper, researchers propose a novel deep learning model and apply it to COVID-19 diagnosis. Our model uses the tool of fractional calculus, which has the potential to improve the performance of gradient methods. To this end, the researcher proposes a fractional-order gradient method for the back-propagation of convolutional neural networks based on the Caputo definition. However, if only the first term of the infinite series of the Caputo definition is used to approximate the fractional-order derivative, the length of the memory is truncated. Therefore, the fractional-order gradient (FGD) method with a fixed memory step and an adjustable number of terms is used to update the weights of the layers. Experiments were performed on the COVIDx dataset to demonstrate fast convergence, good accuracy, and the ability to bypass the local optimal point. We also compared the performance of the developed fractional-order neural networks and Integer-order neural networks. The results confirmed the effectiveness of our proposed model in the diagnosis of COVID-19.



