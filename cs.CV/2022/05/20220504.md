# Arxiv Papers in cs.CV on 2022-05-04
### FedMix: Mixed Supervised Federated Learning for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2205.01840v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01840v1)
- **Published**: 2022-05-04 01:17:53+00:00
- **Updated**: 2022-05-04 01:17:53+00:00
- **Authors**: Jeffry Wicaksana, Zengqiang Yan, Dong Zhang, Xijie Huang, Huimin Wu, Xin Yang, Kwang-Ting Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: The purpose of federated learning is to enable multiple clients to jointly train a machine learning model without sharing data. However, the existing methods for training an image segmentation model have been based on an unrealistic assumption that the training set for each local client is annotated in a similar fashion and thus follows the same image supervision level. To relax this assumption, in this work, we propose a label-agnostic unified federated learning framework, named FedMix, for medical image segmentation based on mixed image labels. In FedMix, each client updates the federated model by integrating and effectively making use of all available labeled data ranging from strong pixel-level labels, weak bounding box labels, to weakest image-level class labels. Based on these local models, we further propose an adaptive weight assignment procedure across local clients, where each client learns an aggregation weight during the global model update. Compared to the existing methods, FedMix not only breaks through the constraint of a single level of image supervision, but also can dynamically adjust the aggregation weight of each local client, achieving rich yet discriminative feature representations. To evaluate its effectiveness, experiments have been carried out on two challenging medical image segmentation tasks, i.e., breast tumor segmentation and skin lesion segmentation. The results validate that our proposed FedMix outperforms the state-of-the-art method by a large margin.



### Visual Commonsense in Pretrained Unimodal and Multimodal Models
- **Arxiv ID**: http://arxiv.org/abs/2205.01850v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01850v1)
- **Published**: 2022-05-04 02:07:55+00:00
- **Updated**: 2022-05-04 02:07:55+00:00
- **Authors**: Chenyu Zhang, Benjamin Van Durme, Zhuowan Li, Elias Stengel-Eskin
- **Comment**: To appear in NAACL 2022
- **Journal**: None
- **Summary**: Our commonsense knowledge about objects includes their typical visual attributes; we know that bananas are typically yellow or green, and not purple. Text and image corpora, being subject to reporting bias, represent this world-knowledge to varying degrees of faithfulness. In this paper, we investigate to what degree unimodal (language-only) and multimodal (image and language) models capture a broad range of visually salient attributes. To that end, we create the Visual Commonsense Tests (ViComTe) dataset covering 5 property types (color, shape, material, size, and visual co-occurrence) for over 5000 subjects. We validate this dataset by showing that our grounded color data correlates much better than ungrounded text-only data with crowdsourced color judgments provided by Paik et al. (2021). We then use our dataset to evaluate pretrained unimodal models and multimodal models. Our results indicate that multimodal models better reconstruct attribute distributions, but are still subject to reporting bias. Moreover, increasing model size does not enhance performance, suggesting that the key to visual commonsense lies in the data.



### UCL-Dehaze: Towards Real-world Image Dehazing via Unsupervised Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2205.01871v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01871v1)
- **Published**: 2022-05-04 03:25:13+00:00
- **Updated**: 2022-05-04 03:25:13+00:00
- **Authors**: Yongzhen Wang, Xuefeng Yan, Fu Lee Wang, Haoran Xie, Wenhan Yang, Mingqiang Wei, Jing Qin
- **Comment**: 14 pages, 9 figures, 9 tables
- **Journal**: None
- **Summary**: While the wisdom of training an image dehazing model on synthetic hazy data can alleviate the difficulty of collecting real-world hazy/clean image pairs, it brings the well-known domain shift problem. From a different yet new perspective, this paper explores contrastive learning with an adversarial training effort to leverage unpaired real-world hazy and clean images, thus bridging the gap between synthetic and real-world haze is avoided. We propose an effective unsupervised contrastive learning paradigm for image dehazing, dubbed UCL-Dehaze. Unpaired real-world clean and hazy images are easily captured, and will serve as the important positive and negative samples respectively when training our UCL-Dehaze network. To train the network more effectively, we formulate a new self-contrastive perceptual loss function, which encourages the restored images to approach the positive samples and keep away from the negative samples in the embedding space. Besides the overall network architecture of UCL-Dehaze, adversarial training is utilized to align the distributions between the positive samples and the dehazed images. Compared with recent image dehazing works, UCL-Dehaze does not require paired data during training and utilizes unpaired positive/negative data to better enhance the dehazing performance. We conduct comprehensive experiments to evaluate our UCL-Dehaze and demonstrate its superiority over the state-of-the-arts, even only 1,800 unpaired real-world images are used to train our network. Source code has been available at https://github.com/yz-wang/UCL-Dehaze.



### Joint Image Compression and Denoising via Latent-Space Scalability
- **Arxiv ID**: http://arxiv.org/abs/2205.01874v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.01874v2)
- **Published**: 2022-05-04 03:29:50+00:00
- **Updated**: 2022-09-05 01:23:52+00:00
- **Authors**: Saeed Ranjbar Alvar, Mateen Ulhaq, Hyomin Choi, Ivan V. BajiÄ‡
- **Comment**: None
- **Journal**: None
- **Summary**: When it comes to image compression in digital cameras, denoising is traditionally performed prior to compression. However, there are applications where image noise may be necessary to demonstrate the trustworthiness of the image, such as court evidence and image forensics. This means that noise itself needs to be coded, in addition to the clean image itself. In this paper, we present a learning-based image compression framework where image denoising and compression are performed jointly. The latent space of the image codec is organized in a scalable manner such that the clean image can be decoded from a subset of the latent space (the base layer), while the noisy image is decoded from the full latent space at a higher rate. Using a subset of the latent space for the denoised image allows denoising to be carried out at a lower rate. Besides providing a scalable representation of the noisy input image, performing denoising jointly with compression makes intuitive sense because noise is hard to compress; hence, compressibility is one of the criteria that may help distinguish noise from the signal. The proposed codec is compared against established compression and denoising benchmarks, and the experiments reveal considerable bitrate savings compared to a cascade combination of a state-of-the-art codec and a state-of-the-art denoiser.



### All You May Need for VQA are Image Captions
- **Arxiv ID**: http://arxiv.org/abs/2205.01883v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2205.01883v1)
- **Published**: 2022-05-04 04:09:23+00:00
- **Updated**: 2022-05-04 04:09:23+00:00
- **Authors**: Soravit Changpinyo, Doron Kukliansky, Idan Szpektor, Xi Chen, Nan Ding, Radu Soricut
- **Comment**: 2022 Annual Conference of the North American Chapter of the
  Association for Computational Linguistics (NAACL 2022)
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) has benefited from increasingly sophisticated models, but has not enjoyed the same level of engagement in terms of data creation. In this paper, we propose a method that automatically derives VQA examples at volume, by leveraging the abundance of existing image-caption annotations combined with neural models for textual question generation. We show that the resulting data is of high-quality. VQA models trained on our data improve state-of-the-art zero-shot accuracy by double digits and achieve a level of robustness that lacks in the same model trained on human-annotated VQA data.



### Unsupervised Domain Adaptation Learning for Hierarchical Infant Pose Recognition with Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2205.01892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01892v1)
- **Published**: 2022-05-04 04:59:26+00:00
- **Updated**: 2022-05-04 04:59:26+00:00
- **Authors**: Cheng-Yen Yang, Zhongyu Jiang, Shih-Yu Gu, Jenq-Neng Hwang, Jang-Hee Yoo
- **Comment**: Accepted as a conference paper at ICME 2022
- **Journal**: None
- **Summary**: The Alberta Infant Motor Scale (AIMS) is a well-known assessment scheme that evaluates the gross motor development of infants by recording the number of specific poses achieved. With the aid of the image-based pose recognition model, the AIMS evaluation procedure can be shortened and automated, providing early diagnosis or indicator of potential developmental disorder. Due to limited public infant-related datasets, many works use the SMIL-based method to generate synthetic infant images for training. However, this domain mismatch between real and synthetic training samples often leads to performance degradation during inference. In this paper, we present a CNN-based model which takes any infant image as input and predicts the coarse and fine-level pose labels. The model consists of an image branch and a pose branch, which respectively generates the coarse-level logits facilitated by the unsupervised domain adaptation and the 3D keypoints using the HRNet with SMPLify optimization. Then the outputs of these branches will be sent into the hierarchical pose recognition module to estimate the fine-level pose labels. We also collect and label a new AIMS dataset, which contains 750 real and 4000 synthetic infants images with AIMS pose labels. Our experimental results show that the proposed method can significantly align the distribution of synthetic and real-world datasets, thus achieving accurate performance on fine-grained infant pose recognition.



### Pik-Fix: Restoring and Colorizing Old Photos
- **Arxiv ID**: http://arxiv.org/abs/2205.01902v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.01902v3)
- **Published**: 2022-05-04 05:46:43+00:00
- **Updated**: 2022-10-06 07:00:40+00:00
- **Authors**: Runsheng Xu, Zhengzhong Tu, Yuanqi Du, Xiaoyu Dong, Jinlong Li, Zibo Meng, Jiaqi Ma, Alan Bovik, Hongkai Yu
- **Comment**: WACV 2022; code: https://github.com/DerrickXuNu/Pik-Fix. arXiv admin
  note: text overlap with arXiv:2202.02606
- **Journal**: None
- **Summary**: Restoring and inpainting the visual memories that are present, but often impaired, in old photos remains an intriguing but unsolved research topic. Decades-old photos often suffer from severe and commingled degradation such as cracks, defocus, and color-fading, which are difficult to treat individually and harder to repair when they interact. Deep learning presents a plausible avenue, but the lack of large-scale datasets of old photos makes addressing this restoration task very challenging. Here we present a novel reference-based end-to-end learning framework that is able to both repair and colorize old, degraded pictures. Our proposed framework consists of three modules: a restoration sub-network that conducts restoration from degradations, a similarity network that performs color histogram matching and color transfer, and a colorization subnet that learns to predict the chroma elements of images conditioned on chromatic reference signals. The overall system makes uses of color histogram priors from reference images, which greatly reduces the need for large-scale training data. We have also created a first-of-a-kind public dataset of real old photos that are paired with ground truth ''pristine'' photos that have been manually restored by PhotoShop experts. We conducted extensive experiments on this dataset and synthetic datasets, and found that our method significantly outperforms previous state-of-the-art models using both qualitative comparisons and quantitative measurements. The code is available at https://github.com/DerrickXuNu/Pik-Fix.



### Self-Taught Metric Learning without Labels
- **Arxiv ID**: http://arxiv.org/abs/2205.01903v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01903v1)
- **Published**: 2022-05-04 05:48:40+00:00
- **Updated**: 2022-05-04 05:48:40+00:00
- **Authors**: Sungyeon Kim, Dongwon Kim, Minsu Cho, Suha Kwak
- **Comment**: Accepted to CVPR 2022
- **Journal**: None
- **Summary**: We present a novel self-taught framework for unsupervised metric learning, which alternates between predicting class-equivalence relations between data through a moving average of an embedding model and learning the model with the predicted relations as pseudo labels. At the heart of our framework lies an algorithm that investigates contexts of data on the embedding space to predict their class-equivalence relations as pseudo labels. The algorithm enables efficient end-to-end training since it demands no off-the-shelf module for pseudo labeling. Also, the class-equivalence relations provide rich supervisory signals for learning an embedding space. On standard benchmarks for metric learning, it clearly outperforms existing unsupervised learning methods and sometimes even beats supervised learning models using the same backbone network. It is also applied to semi-supervised metric learning as a way of exploiting additional unlabeled data, and achieves the state of the art by boosting performance of supervised learning substantially.



### Generalized Knowledge Distillation via Relationship Matching
- **Arxiv ID**: http://arxiv.org/abs/2205.01915v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01915v1)
- **Published**: 2022-05-04 06:49:47+00:00
- **Updated**: 2022-05-04 06:49:47+00:00
- **Authors**: Han-Jia Ye, Su Lu, De-Chuan Zhan
- **Comment**: This manuscript has been accepted by TPAMI
- **Journal**: None
- **Summary**: The knowledge of a well-trained deep neural network (a.k.a. the "teacher") is valuable for learning similar tasks. Knowledge distillation extracts knowledge from the teacher and integrates it with the target model (a.k.a. the "student"), which expands the student's knowledge and improves its learning efficacy. Instead of enforcing the teacher to work on the same task as the student, we borrow the knowledge from a teacher trained from a general label space -- in this "Generalized Knowledge Distillation (GKD)", the classes of the teacher and the student may be the same, completely different, or partially overlapped. We claim that the comparison ability between instances acts as an essential factor threading knowledge across tasks, and propose the RElationship FacIlitated Local cLassifiEr Distillation (REFILLED) approach, which decouples the GKD flow of the embedding and the top-layer classifier. In particular, different from reconciling the instance-label confidence between models, REFILLED requires the teacher to reweight the hard tuples pushed forward by the student and then matches the similarity comparison levels between instances. An embedding-induced classifier based on the teacher model supervises the student's classification confidence and adaptively emphasizes the most related supervision from the teacher. REFILLED demonstrates strong discriminative ability when the classes of the teacher vary from the same to a fully non-overlapped set w.r.t. the student. It also achieves state-of-the-art performance on standard knowledge distillation, one-step incremental learning, and few-shot learning tasks.



### CoCa: Contrastive Captioners are Image-Text Foundation Models
- **Arxiv ID**: http://arxiv.org/abs/2205.01917v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.01917v2)
- **Published**: 2022-05-04 07:01:14+00:00
- **Updated**: 2022-06-14 00:48:04+00:00
- **Authors**: Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.



### Scene Clustering Based Pseudo-labeling Strategy for Multi-modal Aerial View Object Classification
- **Arxiv ID**: http://arxiv.org/abs/2205.01920v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01920v5)
- **Published**: 2022-05-04 07:13:01+00:00
- **Updated**: 2022-10-09 14:43:28+00:00
- **Authors**: Jun Yu, Hao Chang, Keda Lu, Liwen Zhang, Shenshen Du, Zhong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modal aerial view object classification (MAVOC) in Automatic target recognition (ATR), although an important and challenging problem, has been under studied. This paper firstly finds that fine-grained data, class imbalance and various shooting conditions preclude the representational ability of general image classification. Moreover, the MAVOC dataset has scene aggregation characteristics. By exploiting these properties, we propose Scene Clustering Based Pseudo-labeling Strategy (SCP-Label), a simple yet effective method to employ in post-processing. The SCP-Label brings greater accuracy by assigning the same label to objects within the same scene while also mitigating bias and confusion with model ensembles. Its performance surpasses the official baseline by a large margin of +20.57% Accuracy on Track 1 (SAR), and +31.86% Accuracy on Track 2 (SAR+EO), demonstrating the potential of SCP-Label as post-processing. Finally, we win the championship both on Track1 and Track2 in the CVPR 2022 Perception Beyond the Visible Spectrum (PBVS) Workshop MAVOC Challenge. Our code is available at https://github.com/HowieChangchn/SCP-Label.



### Zero-Episode Few-Shot Contrastive Predictive Coding: Solving intelligence tests without prior training
- **Arxiv ID**: http://arxiv.org/abs/2205.01924v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01924v1)
- **Published**: 2022-05-04 07:46:03+00:00
- **Updated**: 2022-05-04 07:46:03+00:00
- **Authors**: T. Barak, Y. Loewenstein
- **Comment**: None
- **Journal**: None
- **Summary**: Video prediction models often combine three components: an encoder from pixel space to a small latent space, a latent space prediction model, and a generative model back to pixel space. However, the large and unpredictable pixel space makes training such models difficult, requiring many training examples. We argue that finding a predictive latent variable and using it to evaluate the consistency of a future image enables data-efficient predictions because it precludes the necessity of a generative model training. To demonstrate it, we created sequence completion intelligence tests in which the task is to identify a predictably changing feature in a sequence of images and use this prediction to select the subsequent image. We show that a one-dimensional Markov Contrastive Predictive Coding (M-CPC_1D) model solves these tests efficiently, with only five examples. Finally, we demonstrate the usefulness of M-CPC_1D in solving two tasks without prior training: anomaly detection and stochastic movement video prediction.



### Self-supervised learning in non-small cell lung cancer discovers novel morphological clusters linked to patient outcome and molecular phenotypes
- **Arxiv ID**: http://arxiv.org/abs/2205.01931v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01931v2)
- **Published**: 2022-05-04 08:06:55+00:00
- **Updated**: 2022-07-15 16:23:53+00:00
- **Authors**: Adalberto Claudio Quiros, Nicolas Coudray, Anna Yeaton, Xinyu Yang, Luis Chiriboga, Afreen Karimkhan, Navneet Narula, Harvey Pass, Andre L. Moreira, John Le Quesne, Aristotelis Tsirigos, Ke Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Histopathological images provide the definitive source of cancer diagnosis, containing information used by pathologists to identify and subclassify malignant disease, and to guide therapeutic choices. These images contain vast amounts of information, much of which is currently unavailable to human interpretation. Supervised deep learning approaches have been powerful for classification tasks, but they are inherently limited by the cost and quality of annotations. Therefore, we developed Histomorphological Phenotype Learning, an unsupervised methodology, which requires no annotations and operates via the self-discovery of discriminatory image features in small image tiles. Tiles are grouped into morphologically similar clusters which appear to represent recurrent modes of tumor growth emerging under natural selection. These clusters have distinct features which can be identified using orthogonal methods. Applied to lung cancer tissues, we show that they align closely with patient outcomes, with histopathologically recognised tumor types and growth patterns, and with transcriptomic measures of immunophenotype.



### Homography-Based Loss Function for Camera Pose Regression
- **Arxiv ID**: http://arxiv.org/abs/2205.01937v1
- **DOI**: 10.1109/LRA.2022.3168329
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.01937v1)
- **Published**: 2022-05-04 08:15:34+00:00
- **Updated**: 2022-05-04 08:15:34+00:00
- **Authors**: ClÃ©mentin Boittiaux, Ricard Marxer, Claire Dune, AurÃ©lien Arnaubec, Vincent Hugel
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters 7 (3), pp.6242-6249 (2022)
- **Summary**: Some recent visual-based relocalization algorithms rely on deep learning methods to perform camera pose regression from image data. This paper focuses on the loss functions that embed the error between two poses to perform deep learning based camera pose regression. Existing loss functions are either difficult-to-tune multi-objective functions or present unstable reprojection errors that rely on ground truth 3D scene points and require a two-step training. To deal with these issues, we introduce a novel loss function which is based on a multiplane homography integration. This new function does not require prior initialization and only depends on physically interpretable hyperparameters. Furthermore, the experiments carried out on well established relocalization datasets show that it minimizes best the mean square reprojection error during training when compared with existing loss functions.



### A Deep Learning Ensemble Framework for Off-Nadir Geocentric Pose Prediction
- **Arxiv ID**: http://arxiv.org/abs/2205.11230v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.11230v3)
- **Published**: 2022-05-04 08:33:41+00:00
- **Updated**: 2022-08-06 08:16:21+00:00
- **Authors**: Christopher Sun, Jai Sharma, Milind Maiti
- **Comment**: 2022 5th International Conference on Pattern Recognition and
  Artificial Intelligence (PRAI)
- **Journal**: None
- **Summary**: Computational methods to accelerate natural disaster response include change detection, map alignment, and vision-aided navigation. Current software functions optimally only on near-nadir images, though off-nadir images are often the first sources of information following a natural disaster. The use of off-nadir images for the aforementioned tasks requires the computation of geocentric pose, which is an aerial vehicle's spatial orientation with respect to gravity. This study proposes a deep learning ensemble framework to predict geocentric pose using 5,923 near-nadir and off-nadir RGB satellite images of cities worldwide. First, a U-Net Fully Convolutional Neural Network predicts the pixel-wise above-ground elevation mask of the RGB images. Then, the elevation masks are concatenated with the RGB images to form four-channel inputs fed into a second convolutional model, which predicts orientation angle and magnification scale. A performance accuracy of R2=0.917 significantly outperforms previous methodologies. In addition, outlier removal is performed through supervised interpolation, and a sensitivity analysis of elevation masks is conducted to gauge the usefulness of data features, motivating future avenues of feature engineering. The high-accuracy software built in this study contributes to mapping and navigation procedures for effective disaster response to save lives.



### EllSeg-Gen, towards Domain Generalization for head-mounted eyetracking
- **Arxiv ID**: http://arxiv.org/abs/2205.01947v1
- **DOI**: 10.1145/3530880
- **Categories**: **cs.CV**, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.01947v1)
- **Published**: 2022-05-04 08:35:52+00:00
- **Updated**: 2022-05-04 08:35:52+00:00
- **Authors**: Rakshit S. Kothari, Reynold J. Bailey, Christopher Kanan, Jeff B. Pelz, Gabriel J. Diaz
- **Comment**: Code available at https://bitbucket.org/RSKothari/multiset_gaze/
- **Journal**: None
- **Summary**: The study of human gaze behavior in natural contexts requires algorithms for gaze estimation that are robust to a wide range of imaging conditions. However, algorithms often fail to identify features such as the iris and pupil centroid in the presence of reflective artifacts and occlusions. Previous work has shown that convolutional networks excel at extracting gaze features despite the presence of such artifacts. However, these networks often perform poorly on data unseen during training. This work follows the intuition that jointly training a convolutional network with multiple datasets learns a generalized representation of eye parts. We compare the performance of a single model trained with multiple datasets against a pool of models trained on individual datasets. Results indicate that models tested on datasets in which eye images exhibit higher appearance variability benefit from multiset training. In contrast, dataset-specific models generalize better onto eye images with lower appearance variability.



### Sequencer: Deep LSTM for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2205.01972v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.01972v4)
- **Published**: 2022-05-04 09:47:46+00:00
- **Updated**: 2023-01-12 14:18:08+00:00
- **Authors**: Yuki Tatsunami, Masato Taki
- **Comment**: Accepted in NeurIPS 2022; camera ready edition
- **Journal**: None
- **Summary**: In recent computer vision research, the advent of the Vision Transformer (ViT) has rapidly revolutionized various architectural design efforts: ViT achieved state-of-the-art image classification performance using self-attention found in natural language processing, and MLP-Mixer achieved competitive performance using simple multi-layer perceptrons. In contrast, several studies have also suggested that carefully redesigned convolutional neural networks (CNNs) can achieve advanced performance comparable to ViT without resorting to these new ideas. Against this background, there is growing interest in what inductive bias is suitable for computer vision. Here we propose Sequencer, a novel and competitive architecture alternative to ViT that provides a new perspective on these issues. Unlike ViTs, Sequencer models long-range dependencies using LSTMs rather than self-attention layers. We also propose a two-dimensional version of Sequencer module, where an LSTM is decomposed into vertical and horizontal LSTMs to enhance performance. Despite its simplicity, several experiments demonstrate that Sequencer performs impressively well: Sequencer2D-L, with 54M parameters, realizes 84.6% top-1 accuracy on only ImageNet-1K. Not only that, we show that it has good transferability and the robust resolution adaptability on double resolution-band.



### MM-Claims: A Dataset for Multimodal Claim Detection in Social Media
- **Arxiv ID**: http://arxiv.org/abs/2205.01989v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.MM, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2205.01989v1)
- **Published**: 2022-05-04 10:43:58+00:00
- **Updated**: 2022-05-04 10:43:58+00:00
- **Authors**: Gullal S. Cheema, Sherzod Hakimov, Abdul Sittar, Eric MÃ¼ller-Budack, Christian Otto, Ralph Ewerth
- **Comment**: Accepted to Findings of NAACL 2022
- **Journal**: None
- **Summary**: In recent years, the problem of misinformation on the web has become widespread across languages, countries, and various social media platforms. Although there has been much work on automated fake news detection, the role of images and their variety are not well explored. In this paper, we investigate the roles of image and text at an earlier stage of the fake news detection pipeline, called claim detection. For this purpose, we introduce a novel dataset, MM-Claims, which consists of tweets and corresponding images over three topics: COVID-19, Climate Change and broadly Technology. The dataset contains roughly 86000 tweets, out of which 3400 are labeled manually by multiple annotators for the training and evaluation of multimodal models. We describe the dataset in detail, evaluate strong unimodal and multimodal baselines, and analyze the potential and drawbacks of current models.



### Attention-based Knowledge Distillation in Multi-attention Tasks: The Impact of a DCT-driven Loss
- **Arxiv ID**: http://arxiv.org/abs/2205.01997v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.01997v2)
- **Published**: 2022-05-04 11:05:18+00:00
- **Updated**: 2022-06-06 08:28:37+00:00
- **Authors**: Alejandro LÃ³pez-Cifuentes, Marcos Escudero-ViÃ±olo, JesÃºs BescÃ³s, Juan C. SanMiguel
- **Comment**: Preprint under review in TCSVT Journal
- **Journal**: None
- **Summary**: Knowledge Distillation (KD) is a strategy for the definition of a set of transferability gangways to improve the efficiency of Convolutional Neural Networks. Feature-based Knowledge Distillation is a subfield of KD that relies on intermediate network representations, either unaltered or depth-reduced via maximum activation maps, as the source knowledge. In this paper, we propose and analyse the use of a 2D frequency transform of the activation maps before transferring them. We pose that\textemdash by using global image cues rather than pixel estimates, this strategy enhances knowledge transferability in tasks such as scene recognition, defined by strong spatial and contextual relationships between multiple and varied concepts. To validate the proposed method, an extensive evaluation of the state-of-the-art in scene recognition is presented. Experimental results provide strong evidences that the proposed strategy enables the student network to better focus on the relevant image areas learnt by the teacher network, hence leading to better descriptive features and higher transferred performance than every other state-of-the-art alternative. We publicly release the training and evaluation framework used along this paper at http://www-vpu.eps.uam.es/publications/DCTBasedKDForSceneRecognition.



### TransRank: Self-supervised Video Representation Learning via Ranking-based Transformation Recognition
- **Arxiv ID**: http://arxiv.org/abs/2205.02028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.02028v1)
- **Published**: 2022-05-04 12:39:25+00:00
- **Updated**: 2022-05-04 12:39:25+00:00
- **Authors**: Haodong Duan, Nanxuan Zhao, Kai Chen, Dahua Lin
- **Comment**: CVPR 2022 Oral
- **Journal**: None
- **Summary**: Recognizing transformation types applied to a video clip (RecogTrans) is a long-established paradigm for self-supervised video representation learning, which achieves much inferior performance compared to instance discrimination approaches (InstDisc) in recent works. However, based on a thorough comparison of representative RecogTrans and InstDisc methods, we observe the great potential of RecogTrans on both semantic-related and temporal-related downstream tasks. Based on hard-label classification, existing RecogTrans approaches suffer from noisy supervision signals in pre-training. To mitigate this problem, we developed TransRank, a unified framework for recognizing Transformations in a Ranking formulation. TransRank provides accurate supervision signals by recognizing transformations relatively, consistently outperforming the classification-based formulation. Meanwhile, the unified framework can be instantiated with an arbitrary set of temporal or spatial transformations, demonstrating good generality. With a ranking-based formulation and several empirical practices, we achieve competitive performance on video retrieval and action recognition. Under the same setting, TransRank surpasses the previous state-of-the-art method by 6.4% on UCF101 and 8.3% on HMDB51 for action recognition (Top1 Acc); improves video retrieval on UCF101 by 20.4% (R@1). The promising results validate that RecogTrans is still a worth exploring paradigm for video self-supervised learning. Codes will be released at https://github.com/kennymckormick/TransRank.



### Immiscible Color Flows in Optimal Transport Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2205.02938v2
- **DOI**: 10.3389/fphy.2023.1089114
- **Categories**: **cs.CV**, cs.LG, math.DS, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2205.02938v2)
- **Published**: 2022-05-04 12:41:36+00:00
- **Updated**: 2023-02-27 12:40:17+00:00
- **Authors**: Alessandro Lonardi, Diego Baptista, Caterina De Bacco
- **Comment**: 23 pages, 13 figures, 2 tables
- **Journal**: None
- **Summary**: In classification tasks, it is crucial to meaningfully exploit the information contained in data. While much of the work in addressing these tasks is devoted to building complex algorithmic infrastructures to process inputs in a black-box fashion, less is known about how to exploit the various facets of the data, before inputting this into an algorithm. Here, we focus on this latter perspective, by proposing a physics-inspired dynamical system that adapts Optimal Transport principles to effectively leverage color distributions of images. Our dynamics regulates immiscible fluxes of colors traveling on a network built from images. Instead of aggregating colors together, it treats them as different commodities that interact with a shared capacity on edges. The resulting optimal flows can then be fed into standard classifiers to distinguish images in different classes. We show how our method can outperform competing approaches on image classification tasks in datasets where color information matters.



### Self-Supervised Super-Resolution for Multi-Exposure Push-Frame Satellites
- **Arxiv ID**: http://arxiv.org/abs/2205.02031v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2205.02031v1)
- **Published**: 2022-05-04 12:42:57+00:00
- **Updated**: 2022-05-04 12:42:57+00:00
- **Authors**: Ngoc Long Nguyen, JÃ©rÃ©my Anger, Axel Davy, Pablo Arias, Gabriele Facciolo
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Modern Earth observation satellites capture multi-exposure bursts of push-frame images that can be super-resolved via computational means. In this work, we propose a super-resolution method for such multi-exposure sequences, a problem that has received very little attention in the literature. The proposed method can handle the signal-dependent noise in the inputs, process sequences of any length, and be robust to inaccuracies in the exposure times. Furthermore, it can be trained end-to-end with self-supervision, without requiring ground truth high resolution frames, which makes it especially suited to handle real data. Central to our method are three key contributions: i) a base-detail decomposition for handling errors in the exposure times, ii) a noise-level-aware feature encoding for improved fusion of frames with varying signal-to-noise ratio and iii) a permutation invariant fusion strategy by temporal pooling operators. We evaluate the proposed method on synthetic and real data and show that it outperforms by a significant margin existing single-exposure approaches that we adapted to the multi-exposure case.



### Self-Supervised Learning for Invariant Representations from Multi-Spectral and SAR Images
- **Arxiv ID**: http://arxiv.org/abs/2205.02049v2
- **DOI**: 10.1109/JSTARS.2022.3204888
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.02049v2)
- **Published**: 2022-05-04 13:16:48+00:00
- **Updated**: 2022-09-05 13:02:01+00:00
- **Authors**: Pallavi Jain, Bianca Schoen-Phelan, Robert Ross
- **Comment**: Manuscript Accepted at IEEE JSTAR
- **Journal**: IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing, 2022
- **Summary**: Self-Supervised learning (SSL) has become the new state-of-art in several domain classification and segmentation tasks. Of these, one popular category in SSL is distillation networks such as BYOL. This work proposes RSDnet, which applies the distillation network (BYOL) in the remote sensing (RS) domain where data is non-trivially different from natural RGB images. Since Multi-spectral (MS) and synthetic aperture radar (SAR) sensors provide varied spectral and spatial resolution information, we utilised them as an implicit augmentation to learn invariant feature embeddings. In order to learn RS based invariant features with SSL, we trained RSDnet in two ways, i.e., single channel feature learning and three channel feature learning. This work explores the usefulness of single channel feature learning from random MS and SAR bands compared to the common notion of using three or more bands. In our linear evaluation, these single channel features reached a 0.92 F1 score on the EuroSAT classification task and 59.6 mIoU on the DFC segmentation task for certain single bands. We also compared our results with ImageNet weights and showed that the RS based SSL model outperforms the supervised ImageNet based model. We further explored the usefulness of multi-modal data compared to single modality data, and it is shown that utilising MS and SAR data learn better invariant representations than utilising only MS data.



### SVTS: Scalable Video-to-Speech Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2205.02058v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2205.02058v2)
- **Published**: 2022-05-04 13:34:07+00:00
- **Updated**: 2022-08-15 18:38:37+00:00
- **Authors**: Rodrigo Mira, Alexandros Haliassos, Stavros Petridis, BjÃ¶rn W. Schuller, Maja Pantic
- **Comment**: accepted to INTERSPEECH 2022 (Oral Presentation)
- **Journal**: None
- **Summary**: Video-to-speech synthesis (also known as lip-to-speech) refers to the translation of silent lip movements into the corresponding audio. This task has received an increasing amount of attention due to its self-supervised nature (i.e., can be trained without manual labelling) combined with the ever-growing collection of audio-visual data available online. Despite these strong motivations, contemporary video-to-speech works focus mainly on small- to medium-sized corpora with substantial constraints in both vocabulary and setting. In this work, we introduce a scalable video-to-speech framework consisting of two components: a video-to-spectrogram predictor and a pre-trained neural vocoder, which converts the mel-frequency spectrograms into waveform audio. We achieve state-of-the art results for GRID and considerably outperform previous approaches on LRW. More importantly, by focusing on spectrogram prediction using a simple feedforward model, we can efficiently and effectively scale our method to very large and unconstrained datasets: To the best of our knowledge, we are the first to show intelligible results on the challenging LRS3 dataset.



### Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2205.02065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.02065v1)
- **Published**: 2022-05-04 13:54:34+00:00
- **Updated**: 2022-05-04 13:54:34+00:00
- **Authors**: Julien Posso, Guy Bois, Yvon Savaria
- **Comment**: None
- **Journal**: None
- **Summary**: Spacecraft pose estimation is an essential computer vision application that can improve the autonomy of in-orbit operations. An ESA/Stanford competition brought out solutions that seem hardly compatible with the constraints imposed on spacecraft onboard computers. URSONet is among the best in the competition for its generalization capabilities but at the cost of a tremendous number of parameters and high computational complexity. In this paper, we propose Mobile-URSONet: a spacecraft pose estimation convolutional neural network with 178 times fewer parameters while degrading accuracy by no more than four times compared to URSONet.



### Dual Branch Neural Network for Sea Fog Detection in Geostationary Ocean Color Imager
- **Arxiv ID**: http://arxiv.org/abs/2205.02069v1
- **DOI**: 10.1109/TGRS.2022.3196177
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.02069v1)
- **Published**: 2022-05-04 14:01:38+00:00
- **Updated**: 2022-05-04 14:01:38+00:00
- **Authors**: Yuan Zhou, Keran Chen, Xiaofeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Sea fog significantly threatens the safety of maritime activities. This paper develops a sea fog dataset (SFDD) and a dual branch sea fog detection network (DB-SFNet). We investigate all the observed sea fog events in the Yellow Sea and the Bohai Sea (118.1{\deg}E-128.1{\deg}E, 29.5{\deg}N-43.8{\deg}N) from 2010 to 2020, and collect the sea fog images for each event from the Geostationary Ocean Color Imager (GOCI) to comprise the dataset SFDD. The location of the sea fog in each image in SFDD is accurately marked. The proposed dataset is characterized by a long-time span, large number of samples, and accurate labeling, that can substantially improve the robustness of various sea fog detection models. Furthermore, this paper proposes a dual branch sea fog detection network to achieve accurate and holistic sea fog detection. The poporsed DB-SFNet is composed of a knowledge extraction module and a dual branch optional encoding decoding module. The two modules jointly extracts discriminative features from both visual and statistical domain. Experiments show promising sea fog detection results with an F1-score of 0.77 and a critical success index of 0.63. Compared with existing advanced deep learning networks, DB-SFNet is superior in detection performance and stability, particularly in the mixed cloud and fog areas.



### DeepPortraitDrawing: Generating Human Body Images from Freehand Sketches
- **Arxiv ID**: http://arxiv.org/abs/2205.02070v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.02070v2)
- **Published**: 2022-05-04 14:02:45+00:00
- **Updated**: 2022-05-20 17:00:19+00:00
- **Authors**: Xian Wu, Chen Wang, Hongbo Fu, Ariel Shamir, Song-Hai Zhang, Shi-Min Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Researchers have explored various ways to generate realistic images from freehand sketches, e.g., for objects and human faces. However, how to generate realistic human body images from sketches is still a challenging problem. It is, first because of the sensitivity to human shapes, second because of the complexity of human images caused by body shape and pose changes, and third because of the domain gap between realistic images and freehand sketches. In this work, we present DeepPortraitDrawing, a deep generative framework for converting roughly drawn sketches to realistic human body images. To encode complicated body shapes under various poses, we take a local-to-global approach. Locally, we employ semantic part auto-encoders to construct part-level shape spaces, which are useful for refining the geometry of an input pre-segmented hand-drawn sketch. Globally, we employ a cascaded spatial transformer network to refine the structure of body parts by adjusting their spatial locations and relative proportions. Finally, we use a global synthesis network for the sketch-to-image translation task, and a face refinement network to enhance facial details. Extensive experiments have shown that given roughly sketched human portraits, our method produces more realistic images than the state-of-the-art sketch-to-image synthesis techniques.



### ANUBIS: Skeleton Action Recognition Dataset, Review, and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2205.02071v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.02071v3)
- **Published**: 2022-05-04 14:03:43+00:00
- **Updated**: 2022-05-08 04:36:52+00:00
- **Authors**: Zhenyue Qin, Yang Liu, Madhawa Perera, Tom Gedeon, Pan Ji, Dongwoo Kim, Saeed Anwar
- **Comment**: None
- **Journal**: None
- **Summary**: Skeleton-based action recognition, as a subarea of action recognition, is swiftly accumulating attention and popularity. The task is to recognize actions performed by human articulation points. Compared with other data modalities, 3D human skeleton representations have extensive unique desirable characteristics, including succinctness, robustness, racial-impartiality, and many more. We aim to provide a roadmap for new and existing researchers a on the landscapes of skeleton-based action recognition for new and existing researchers. To this end, we present a review in the form of a taxonomy on existing works of skeleton-based action recognition. We partition them into four major categories: (1) datasets; (2) extracting spatial features; (3) capturing temporal patterns; (4) improving signal quality. For each method, we provide concise yet informatively-sufficient descriptions. To promote more fair and comprehensive evaluation on existing approaches of skeleton-based action recognition, we collect ANUBIS, a large-scale human skeleton dataset. Compared with previously collected dataset, ANUBIS are advantageous in the following four aspects: (1) employing more recently released sensors; (2) containing novel back view; (3) encouraging high enthusiasm of subjects; (4) including actions of the COVID pandemic era. Using ANUBIS, we comparably benchmark performance of current skeleton-based action recognizers. At the end of this paper, we outlook future development of skeleton-based action recognition by listing several new technical problems. We believe they are valuable to solve in order to commercialize skeleton-based action recognition in the near future. The dataset of ANUBIS is available at: http://hcc-workshop.anu.edu.au/webs/anu101/home.



### SDF-based RGB-D Camera Tracking in Neural Scene Representations
- **Arxiv ID**: http://arxiv.org/abs/2205.02079v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.02079v1)
- **Published**: 2022-05-04 14:18:39+00:00
- **Updated**: 2022-05-04 14:18:39+00:00
- **Authors**: Leonard Bruns, Fereidoon Zangeneh, Patric Jensfelt
- **Comment**: Accepted to the "Motion Planning with Implicit Neural Representations
  of Geometry" Workshop at ICRA 2022
- **Journal**: None
- **Summary**: We consider the problem of tracking the 6D pose of a moving RGB-D camera in a neural scene representation. Different such representations have recently emerged, and we investigate the suitability of them for the task of camera tracking. In particular, we propose to track an RGB-D camera using a signed distance field-based representation and show that compared to density-based representations, tracking can be sped up, which enables more robust and accurate pose estimates when computation time is limited.



### Video Extrapolation in Space and Time
- **Arxiv ID**: http://arxiv.org/abs/2205.02084v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2205.02084v3)
- **Published**: 2022-05-04 14:25:08+00:00
- **Updated**: 2022-07-26 05:15:07+00:00
- **Authors**: Yunzhi Zhang, Jiajun Wu
- **Comment**: ECCV 2022. Project page:
  https://cs.stanford.edu/~yzzhang/projects/vest/
- **Journal**: None
- **Summary**: Novel view synthesis (NVS) and video prediction (VP) are typically considered disjoint tasks in computer vision. However, they can both be seen as ways to observe the spatial-temporal world: NVS aims to synthesize a scene from a new point of view, while VP aims to see a scene from a new point of time. These two tasks provide complementary signals to obtain a scene representation, as viewpoint changes from spatial observations inform depth, and temporal observations inform the motion of cameras and individual objects. Inspired by these observations, we propose to study the problem of Video Extrapolation in Space and Time (VEST). We propose a model that leverages the self-supervision and the complementary cues from both tasks, while existing methods can only solve one of them. Experiments show that our method achieves performance better than or comparable to several state-of-the-art NVS and VP methods on indoor and outdoor real-world datasets.



### Hypercomplex Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2205.02087v1
- **DOI**: 10.1109/IJCNN55064.2022.9892119
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.02087v1)
- **Published**: 2022-05-04 14:28:50+00:00
- **Updated**: 2022-05-04 14:28:50+00:00
- **Authors**: Eleonora Grassucci, Luigi Sigillo, Aurelio Uncini, Danilo Comminiello
- **Comment**: None
- **Journal**: 2022 International Joint Conference on Neural Networks (IJCNN)
- **Summary**: Image-to-image translation (I2I) aims at transferring the content representation from an input domain to an output one, bouncing along different target domains. Recent I2I generative models, which gain outstanding results in this task, comprise a set of diverse deep networks each with tens of million parameters. Moreover, images are usually three-dimensional being composed of RGB channels and common neural models do not take dimensions correlation into account, losing beneficial information. In this paper, we propose to leverage hypercomplex algebra properties to define lightweight I2I generative models capable of preserving pre-existing relations among image dimensions, thus exploiting additional input information. On manifold I2I benchmarks, we show how the proposed Quaternion StarGANv2 and parameterized hypercomplex StarGANv2 (PHStarGANv2) reduce parameters and storage memory amount while ensuring high domain translation performance and good image quality as measured by FID and LPIPS scores. Full code is available at: https://github.com/ispamm/HI2I.



### Dynamic Sparse R-CNN
- **Arxiv ID**: http://arxiv.org/abs/2205.02101v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.02101v1)
- **Published**: 2022-05-04 14:56:25+00:00
- **Updated**: 2022-05-04 14:56:25+00:00
- **Authors**: Qinghang Hong, Fengming Liu, Dong Li, Ji Liu, Lu Tian, Yi Shan
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Sparse R-CNN is a recent strong object detection baseline by set prediction on sparse, learnable proposal boxes and proposal features. In this work, we propose to improve Sparse R-CNN with two dynamic designs. First, Sparse R-CNN adopts a one-to-one label assignment scheme, where the Hungarian algorithm is applied to match only one positive sample for each ground truth. Such one-to-one assignment may not be optimal for the matching between the learned proposal boxes and ground truths. To address this problem, we propose dynamic label assignment (DLA) based on the optimal transport algorithm to assign increasing positive samples in the iterative training stages of Sparse R-CNN. We constrain the matching to be gradually looser in the sequential stages as the later stage produces the refined proposals with improved precision. Second, the learned proposal boxes and features remain fixed for different images in the inference process of Sparse R-CNN. Motivated by dynamic convolution, we propose dynamic proposal generation (DPG) to assemble multiple proposal experts dynamically for providing better initial proposal boxes and features for the consecutive training stages. DPG thereby can derive sample-dependent proposal boxes and features for inference. Experiments demonstrate that our method, named Dynamic Sparse R-CNN, can boost the strong Sparse R-CNN baseline with different backbones for object detection. Particularly, Dynamic Sparse R-CNN reaches the state-of-the-art 47.2% AP on the COCO 2017 validation set, surpassing Sparse R-CNN by 2.2% AP with the same ResNet-50 backbone.



### Neuroevolutionary Multi-objective approaches to Trajectory Prediction in Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2205.02105v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.02105v3)
- **Published**: 2022-05-04 15:03:26+00:00
- **Updated**: 2022-05-06 16:36:05+00:00
- **Authors**: Fergal Stapleton, Edgar GalvÃ¡n, Ganesh Sistu, Senthil Yogamani
- **Comment**: Accepted in Genetic and Evolutionary Computation Conference Companion
  (GECCO '22 Companion), July 9--13, 2022, Boston, MA, USA, 4 pages, 1 figure,
  6 tables
- **Journal**: None
- **Summary**: The incentive for using Evolutionary Algorithms (EAs) for the automated optimization and training of deep neural networks (DNNs), a process referred to as neuroevolution, has gained momentum in recent years. The configuration and training of these networks can be posed as optimization problems. Indeed, most of the recent works on neuroevolution have focused their attention on single-objective optimization. Moreover, from the little research that has been done at the intersection of neuroevolution and evolutionary multi-objective optimization (EMO), all the research that has been carried out has focused predominantly on the use of one type of DNN: convolutional neural networks (CNNs), using well-established standard benchmark problems such as MNIST. In this work, we make a leap in the understanding of these two areas (neuroevolution and EMO), regarded in this work as neuroevolutionary multi-objective, by using and studying a rich DNN composed of a CNN and Long-short Term Memory network. Moreover, we use a robust and challenging vehicle trajectory prediction problem. By using the well-known Non-dominated Sorting Genetic Algorithm-II, we study the effects of five different objectives, tested in categories of three, allowing us to show how these objectives have either a positive or detrimental effect in neuroevolution for trajectory prediction in autonomous vehicles.



### Prediction of fish location by combining fisheries data and sea bottom temperature forecasting
- **Arxiv ID**: http://arxiv.org/abs/2205.02107v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.02107v1)
- **Published**: 2022-05-04 15:08:51+00:00
- **Updated**: 2022-05-04 15:08:51+00:00
- **Authors**: Matthieu Ospici, Klaas Sys, Sophie Guegan-Marat
- **Comment**: Accepted at 21st International Conference on Image Analysis and
  Processing (ICIAP 2021)
- **Journal**: None
- **Summary**: This paper combines fisheries dependent data and environmental data to be used in a machine learning pipeline to predict the spatio-temporal abundance of two species (plaice and sole) commonly caught by the Belgian fishery in the North Sea. By combining fisheries related features with environmental data, sea bottom temperature derived from remote sensing, a higher accuracy can be achieved. In a forecast setting, the predictive accuracy is further improved by predicting, using a recurrent deep neural network, the sea bottom temperature up to four days in advance instead of relying on the last previous temperature measurement.



### Domino Saliency Metrics: Improving Existing Channel Saliency Metrics with Structural Information
- **Arxiv ID**: http://arxiv.org/abs/2205.02131v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.02131v2)
- **Published**: 2022-05-04 15:37:51+00:00
- **Updated**: 2022-06-19 12:01:28+00:00
- **Authors**: Kaveena Persand, Andrew Anderson, David Gregg
- **Comment**: None
- **Journal**: None
- **Summary**: Channel pruning is used to reduce the number of weights in a Convolutional Neural Network (CNN). Channel pruning removes slices of the weight tensor so that the convolution layer remains dense. The removal of these weight slices from a single layer causes mismatching number of feature maps between layers of the network. A simple solution is to force the number of feature map between layers to match through the removal of weight slices from subsequent layers. This additional constraint becomes more apparent in DNNs with branches where multiple channels need to be pruned together to keep the network dense. Popular pruning saliency metrics do not factor in the structural dependencies that arise in DNNs with branches. We propose Domino metrics (built on existing channel saliency metrics) to reflect these structural constraints. We test Domino saliency metrics against the baseline channel saliency metrics on multiple networks with branches. Domino saliency metrics improved pruning rates in most tested networks and up to 25% in AlexNet on CIFAR-10.



### RecipeSnap -- a lightweight image-to-recipe model
- **Arxiv ID**: http://arxiv.org/abs/2205.02141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.02141v1)
- **Published**: 2022-05-04 15:49:52+00:00
- **Updated**: 2022-05-04 15:49:52+00:00
- **Authors**: Jianfa Chen, Yue Yin, Yifan Xu
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: In this paper we want to address the problem of automation for recognition of photographed cooking dishes and generating the corresponding food recipes. Current image-to-recipe models are computation expensive and require powerful GPUs for model training and implementation. High computational cost prevents those existing models from being deployed on portable devices, like smart phones. To solve this issue we introduce a lightweight image-to-recipe prediction model, RecipeSnap, that reduces memory cost and computational cost by more than 90% while still achieving 2.0 MedR, which is in line with the state-of-the-art model. A pre-trained recipe encoder was used to compute recipe embeddings. Recipes from Recipe1M dataset and corresponding recipe embeddings are collected as a recipe library, which are used for image encoder training and image query later. We use MobileNet-V2 as image encoder backbone, which makes our model suitable to portable devices. This model can be further developed into an application for smart phones with a few effort. A comparison of the performance between this lightweight model to other heavy models are presented in this paper. Code, data and models are publicly accessible on github.



### An Analysis of Generative Methods for Multiple Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2205.02146v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.4; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2205.02146v1)
- **Published**: 2022-05-04 15:54:08+00:00
- **Updated**: 2022-05-04 15:54:08+00:00
- **Authors**: Coloma Ballester, Aurelie Bugeau, Samuel Hurault, Simone Parisotto, Patricia Vitoria
- **Comment**: None
- **Journal**: None
- **Summary**: Image inpainting refers to the restoration of an image with missing regions in a way that is not detectable by the observer. The inpainting regions can be of any size and shape. This is an ill-posed inverse problem that does not have a unique solution. In this work, we focus on learning-based image completion methods for multiple and diverse inpainting which goal is to provide a set of distinct solutions for a given damaged image. These methods capitalize on the probabilistic nature of certain generative models to sample various solutions that coherently restore the missing content. Along the chapter, we will analyze the underlying theory and analyze the recent proposals for multiple inpainting. To investigate the pros and cons of each method, we present quantitative and qualitative comparisons, on common datasets, regarding both the quality and the diversity of the set of inpainted solutions. Our analysis allows us to identify the most successful generative strategies in both inpainting quality and inpainting diversity. This task is closely related to the learning of an accurate probability distribution of images. Depending on the dataset in use, the challenges that entail the training of such a model will be discussed through the analysis.



### Dual Cross-Attention Learning for Fine-Grained Visual Categorization and Object Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2205.02151v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.02151v1)
- **Published**: 2022-05-04 16:14:26+00:00
- **Updated**: 2022-05-04 16:14:26+00:00
- **Authors**: Haowei Zhu, Wenjing Ke, Dong Li, Ji Liu, Lu Tian, Yi Shan
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Recently, self-attention mechanisms have shown impressive performance in various NLP and CV tasks, which can help capture sequential characteristics and derive global information. In this work, we explore how to extend self-attention modules to better learn subtle feature embeddings for recognizing fine-grained objects, e.g., different bird species or person identities. To this end, we propose a dual cross-attention learning (DCAL) algorithm to coordinate with self-attention learning. First, we propose global-local cross-attention (GLCA) to enhance the interactions between global images and local high-response regions, which can help reinforce the spatial-wise discriminative clues for recognition. Second, we propose pair-wise cross-attention (PWCA) to establish the interactions between image pairs. PWCA can regularize the attention learning of an image by treating another image as distractor and will be removed during inference. We observe that DCAL can reduce misleading attentions and diffuse the attention response to discover more complementary parts for recognition. We conduct extensive evaluations on fine-grained visual categorization and object re-identification. Experiments demonstrate that DCAL performs on par with state-of-the-art methods and consistently improves multiple self-attention baselines, e.g., surpassing DeiT-Tiny and ViT-Base by 2.8% and 2.4% mAP on MSMT17, respectively.



### Evaluating Transferability for Covid 3D Localization Using CT SARS-CoV-2 segmentation models
- **Arxiv ID**: http://arxiv.org/abs/2205.02152v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.02152v4)
- **Published**: 2022-05-04 16:15:25+00:00
- **Updated**: 2022-05-20 10:52:27+00:00
- **Authors**: Constantine Maganaris, Eftychios Protopapadakis, Nikolaos Bakalos, Nikolaos Doulamis, Dimitris Kalogeras, Aikaterini Angeli
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies indicate that detecting radiographic patterns on CT scans can yield high sensitivity and specificity for Covid-19 localization. In this paper, we investigate the appropriateness of deep learning models transferability, for semantic segmentation of pneumonia-infected areas in CT images. Transfer learning allows for the fast initialization/reutilization of detection models, given that large volumes of training data are not available. Our work explores the efficacy of using pre-trained U-Net architectures, on a specific CT data set, for identifying Covid-19 side-effects over images from different datasets. Experimental results indicate improvement in the segmentation accuracy of identifying Covid-19 infected regions.



### UnrealNAS: Can We Search Neural Architectures with Unreal Data?
- **Arxiv ID**: http://arxiv.org/abs/2205.02162v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.02162v3)
- **Published**: 2022-05-04 16:30:26+00:00
- **Updated**: 2022-05-19 05:33:56+00:00
- **Authors**: Zhen Dong, Kaicheng Zhou, Guohao Li, Qiang Zhou, Mingfei Guo, Bernard Ghanem, Kurt Keutzer, Shanghang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Neural architecture search (NAS) has shown great success in the automatic design of deep neural networks (DNNs). However, the best way to use data to search network architectures is still unclear and under exploration. Previous work has analyzed the necessity of having ground-truth labels in NAS and inspired broad interest. In this work, we take a further step to question whether real data is necessary for NAS to be effective. The answer to this question is important for applications with limited amount of accessible data, and can help people improve NAS by leveraging the extra flexibility of data generation. To explore if NAS needs real data, we construct three types of unreal datasets using: 1) randomly labeled real images; 2) generated images and labels; and 3) generated Gaussian noise with random labels. These datasets facilitate to analyze the generalization and expressivity of the searched architectures. We study the performance of architectures searched on these constructed datasets using popular differentiable NAS methods. Extensive experiments on CIFAR, ImageNet and CheXpert show that the searched architectures can achieve promising results compared with those derived from the conventional NAS pipeline with real labeled data, suggesting the feasibility of performing NAS with unreal data.



### Compound virtual screening by learning-to-rank with gradient boosting decision tree and enrichment-based cumulative gain
- **Arxiv ID**: http://arxiv.org/abs/2205.02169v2
- **DOI**: 10.1109/CIBCB55180.2022.9863032
- **Categories**: **q-bio.BM**, cs.CV, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2205.02169v2)
- **Published**: 2022-05-04 16:36:24+00:00
- **Updated**: 2022-08-29 16:25:41+00:00
- **Authors**: Kairi Furui, Masahito Ohue
- **Comment**: {\copyright} 2022 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: In Proceedings of The 19th IEEE International Conference on
  Computational Intelligence in Bioinformatics and Computational Biology (CIBCB
  2022)
- **Summary**: Learning-to-rank, a machine learning technique widely used in information retrieval, has recently been applied to the problem of ligand-based virtual screening, to accelerate the early stages of new drug development. Ranking prediction models learn based on ordinal relationships, making them suitable for integrating assay data from various environments. Existing studies of rank prediction in compound screening have generally used a learning-to-rank method called RankSVM. However, they have not been compared with or validated against the gradient boosting decision tree (GBDT)-based learning-to-rank methods that have gained popularity recently. Furthermore, although the ranking metric called Normalized Discounted Cumulative Gain (NDCG) is widely used in information retrieval, it only determines whether the predictions are better than those of other models. In other words, NDCG is incapable of recognizing when a prediction model produces worse than random results. Nevertheless, NDCG is still used in the performance evaluation of compound screening using learning-to-rank. This study used the GBDT model with ranking loss functions, called lambdarank and lambdaloss, for ligand-based virtual screening; results were compared with existing RankSVM methods and GBDT models using regression. We also proposed a new ranking metric, Normalized Enrichment Discounted Cumulative Gain (NEDCG), which aims to properly evaluate the goodness of ranking predictions. Results showed that the GBDT model with learning-to-rank outperformed existing regression methods using GBDT and RankSVM on diverse datasets. Moreover, NEDCG showed that predictions by regression were comparable to random predictions in multi-assay, multi-family datasets, demonstrating its usefulness for a more direct assessment of compound screening performance.



### COOPERNAUT: End-to-End Driving with Cooperative Perception for Networked Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2205.02222v1
- **DOI**: 10.1109/CVPR52688.2022.01674
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.02222v1)
- **Published**: 2022-05-04 17:55:12+00:00
- **Updated**: 2022-05-04 17:55:12+00:00
- **Authors**: Jiaxun Cui, Hang Qiu, Dian Chen, Peter Stone, Yuke Zhu
- **Comment**: None
- **Journal**: CVPR 2022
- **Summary**: Optical sensors and learning algorithms for autonomous vehicles have dramatically advanced in the past few years. Nonetheless, the reliability of today's autonomous vehicles is hindered by the limited line-of-sight sensing capability and the brittleness of data-driven methods in handling extreme situations. With recent developments of telecommunication technologies, cooperative perception with vehicle-to-vehicle communications has become a promising paradigm to enhance autonomous driving in dangerous or emergency situations. We introduce COOPERNAUT, an end-to-end learning model that uses cross-vehicle perception for vision-based cooperative driving. Our model encodes LiDAR information into compact point-based representations that can be transmitted as messages between vehicles via realistic wireless channels. To evaluate our model, we develop AutoCastSim, a network-augmented driving simulation framework with example accident-prone scenarios. Our experiments on AutoCastSim suggest that our cooperative perception driving models lead to a 40% improvement in average success rate over egocentric driving models in these challenging driving situations and a 5 times smaller bandwidth requirement than prior work V2VNet. COOPERNAUT and AUTOCASTSIM are available at https://ut-austin-rpl.github.io/Coopernaut/.



### P3IV: Probabilistic Procedure Planning from Instructional Videos with Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/2205.02300v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.02300v1)
- **Published**: 2022-05-04 19:37:32+00:00
- **Updated**: 2022-05-04 19:37:32+00:00
- **Authors**: He Zhao, Isma Hadji, Nikita Dvornik, Konstantinos G. Derpanis, Richard P. Wildes, Allan D. Jepson
- **Comment**: Accepted as an oral paper at CVPR 2022
- **Journal**: None
- **Summary**: In this paper, we study the problem of procedure planning in instructional videos. Here, an agent must produce a plausible sequence of actions that can transform the environment from a given start to a desired goal state. When learning procedure planning from instructional videos, most recent work leverages intermediate visual observations as supervision, which requires expensive annotation efforts to localize precisely all the instructional steps in training videos. In contrast, we remove the need for expensive temporal video annotations and propose a weakly supervised approach by learning from natural language instructions. Our model is based on a transformer equipped with a memory module, which maps the start and goal observations to a sequence of plausible actions. Furthermore, we augment our model with a probabilistic generative module to capture the uncertainty inherent to procedure planning, an aspect largely overlooked by previous work. We evaluate our model on three datasets and show our weaklysupervised approach outperforms previous fully supervised state-of-the-art models on multiple metrics.



### BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking
- **Arxiv ID**: http://arxiv.org/abs/2205.02301v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2205.02301v3)
- **Published**: 2022-05-04 19:38:26+00:00
- **Updated**: 2022-07-24 20:52:48+00:00
- **Authors**: Dorian F. Henning, Tristan Laidlow, Stefan Leutenegger
- **Comment**: ECCV 2022. Video: https://youtu.be/0-SL3VeWEvU
- **Journal**: None
- **Summary**: Estimating human motion from video is an active research area due to its many potential applications. Most state-of-the-art methods predict human shape and posture estimates for individual images and do not leverage the temporal information available in video. Many "in the wild" sequences of human motion are captured by a moving camera, which adds the complication of conflated camera and human motion to the estimation. We therefore present BodySLAM, a monocular SLAM system that jointly estimates the position, shape, and posture of human bodies, as well as the camera trajectory. We also introduce a novel human motion model to constrain sequential body postures and observe the scale of the scene. Through a series of experiments on video sequences of human motion captured by a moving monocular camera, we demonstrate that BodySLAM improves estimates of all human body parameters and camera poses when compared to estimating these separately.



### GAN Inversion for Data Augmentation to Improve Colonoscopy Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/2205.02840v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2205.02840v1)
- **Published**: 2022-05-04 23:15:45+00:00
- **Updated**: 2022-05-04 23:15:45+00:00
- **Authors**: Mayank Golhar, Taylor L. Bobrow, Saowanee Ngamruengphong, Nicholas J. Durr
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: A major challenge in applying deep learning to medical imaging is the paucity of annotated data. This study demonstrates that synthetic colonoscopy images generated by Generative Adversarial Network (GAN) inversion can be used as training data to improve the lesion classification performance of deep learning models. This approach inverts pairs of images with the same label to a semantically rich & disentangled latent space and manipulates latent representations to produce new synthetic images with the same label. We perform image modality translation (style transfer) between white light and narrowband imaging (NBI). We also generate realistic-looking synthetic lesion images by interpolating between original training images to increase the variety of lesion shapes in the training dataset. We show that these approaches outperform comparative colonoscopy data augmentation techniques without the need to re-train multiple generative models. This approach also leverages information from datasets that may not have been designed for the specific colonoscopy downstream task. E.g. using a bowel prep grading dataset for a polyp classification task. Our experiments show this approach can perform multiple colonoscopy data augmentations, which improve the downstream polyp classification performance over baseline and comparison methods by up to 6%.



### Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion
- **Arxiv ID**: http://arxiv.org/abs/2205.02357v4
- **DOI**: 10.1145/3477495.3531992
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2205.02357v4)
- **Published**: 2022-05-04 23:40:04+00:00
- **Updated**: 2022-11-01 05:13:18+00:00
- **Authors**: Xiang Chen, Ningyu Zhang, Lei Li, Shumin Deng, Chuanqi Tan, Changliang Xu, Fei Huang, Luo Si, Huajun Chen
- **Comment**: Accepted by SIGIR 2022. Fix a severe bug
- **Journal**: None
- **Summary**: Multimodal Knowledge Graphs (MKGs), which organize visual-text factual knowledge, have recently been successfully applied to tasks such as information retrieval, question answering, and recommendation system. Since most MKGs are far from complete, extensive knowledge graph completion studies have been proposed focusing on the multimodal entity, relation extraction and link prediction. However, different tasks and modalities require changes to the model architecture, and not all images/objects are relevant to text input, which hinders the applicability to diverse real-world scenarios. In this paper, we propose a hybrid transformer with multi-level fusion to address those issues. Specifically, we leverage a hybrid transformer architecture with unified input-output for diverse multimodal knowledge graph completion tasks. Moreover, we propose multi-level fusion, which integrates visual and text representation via coarse-grained prefix-guided interaction and fine-grained correlation-aware fusion modules. We conduct extensive experiments to validate that our MKGformer can obtain SOTA performance on four datasets of multimodal link prediction, multimodal RE, and multimodal NER. Code is available in https://github.com/zjunlp/MKGformer.



### Creating a Forensic Database of Shoeprints from Online Shoe Tread Photos
- **Arxiv ID**: http://arxiv.org/abs/2205.02361v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2205.02361v2)
- **Published**: 2022-05-04 23:42:55+00:00
- **Updated**: 2022-10-20 10:35:29+00:00
- **Authors**: Samia Shafique, Bailey Kong, Shu Kong, Charless C. Fowlkes
- **Comment**: published in WACV 2023; 8 pages including 11 figures and 3 tables;
  contains reference and appendix
- **Journal**: None
- **Summary**: Shoe tread impressions are one of the most common types of evidence left at crime scenes. However, the utility of such evidence is limited by the lack of databases of footwear prints that cover the large and growing number of distinct shoe models. Moreover, the database is preferred to contain the 3D shape, or depth, of shoe-tread photos so as to allow for extracting shoeprints to match a query (crime-scene) print. We propose to address this gap by leveraging shoe-tread photos collected by online retailers. The core challenge is to predict depth maps for these photos. As they do not have ground-truth 3D shapes allowing for training depth predictors, we exploit synthetic data that does. We develop a method termed ShoeRinsics that learns to predict depth by leveraging a mix of fully supervised synthetic data and unsupervised retail image data. In particular, we find domain adaptation and intrinsic image decomposition techniques effectively mitigate the synthetic-real domain gap and yield significantly better depth prediction. To validate our method, we introduce 2 validation sets consisting of shoe-tread image and print pairs and define a benchmarking protocol to quantify the quality of predicted depth. On this benchmark, ShoeRinsics outperforms existing methods of depth prediction and synthetic-to-real domain adaptation.



