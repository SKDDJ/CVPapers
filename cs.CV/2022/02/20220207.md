# Arxiv Papers in cs.CV on 2022-02-07
### Performance Evaluation of Infrared Image Enhancement Techniques
- **Arxiv ID**: http://arxiv.org/abs/2202.03427v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2202.03427v1)
- **Published**: 2022-02-07 00:05:01+00:00
- **Updated**: 2022-02-07 00:05:01+00:00
- **Authors**: Rania Gaber, AbdElmgied Ali, Kareem Ahmed
- **Comment**: None
- **Journal**: None
- **Summary**: Infrared (IR) images are widely used in many fields such as medical imaging, object tracking, astronomy and military purposes for securing borders. Infrared images can be captured day or night based on the type of capturing device. The capturing devices use electromagnetic radiation with longer wavelengths. There are several types of IR radiation based on the range of wavelength and corresponding frequency. Due to noising and other artifacts, IR images are not clearly visible. In this paper, we present a complete up-todate survey on IR imaging enhancement techniques. The survey includes IR radiation types and devices and existing IR datasets. The survey covers spatial enhancement techniques, frequency-domain based enhancement techniques and Deep learning-based techniques.



### A Topology-Attention ConvLSTM Network and Its Application to EM Images
- **Arxiv ID**: http://arxiv.org/abs/2202.03430v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03430v1)
- **Published**: 2022-02-07 01:33:01+00:00
- **Updated**: 2022-02-07 01:33:01+00:00
- **Authors**: Jiaqi Yang, Xiaoling Hu, Chao Chen, Chialing Tsai
- **Comment**: 12 pages, 6 figures, Accepted by MICCAI'21
- **Journal**: None
- **Summary**: Structural accuracy of segmentation is important for finescale structures in biomedical images. We propose a novel TopologyAttention ConvLSTM Network (TACNet) for 3D image segmentation in order to achieve high structural accuracy for 3D segmentation tasks. Specifically, we propose a Spatial Topology-Attention (STA) module to process a 3D image as a stack of 2D image slices and adopt ConvLSTM to leverage contextual structure information from adjacent slices. In order to effectively transfer topology-critical information across slices, we propose an Iterative-Topology Attention (ITA) module that provides a more stable topology-critical map for segmentation. Quantitative and qualitative results show that our proposed method outperforms various baselines in terms of topology-aware evaluation metrics.



### Inter-subject Contrastive Learning for Subject Adaptive EEG-based Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.02901v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02901v1)
- **Published**: 2022-02-07 01:34:57+00:00
- **Updated**: 2022-02-07 01:34:57+00:00
- **Authors**: Pilhyeon Lee, Sunhee Hwang, Jewook Lee, Minjung Shin, Seogkyu Jeon, Hyeran Byun
- **Comment**: Accepted by the 10th IEEE International Winter Conference on
  Brain-Computer Interface (BCI 2022). Code is available at
  https://github.com/DeepBCI/Deep-BCI
- **Journal**: None
- **Summary**: This paper tackles the problem of subject adaptive EEG-based visual recognition. Its goal is to accurately predict the categories of visual stimuli based on EEG signals with only a handful of samples for the target subject during training. The key challenge is how to appropriately transfer the knowledge obtained from abundant data of source subjects to the subject of interest. To this end, we introduce a novel method that allows for learning subject-independent representation by increasing the similarity of features sharing the same class but coming from different subjects. With the dedicated sampling principle, our model effectively captures the common knowledge shared across different subjects, thereby achieving promising performance for the target subject even under harsh problem settings with limited data. Specifically, on the EEG-ImageNet40 benchmark, our model records the top-1 / top-3 test accuracy of 72.6% / 91.6% when using only five EEG samples per class for the target subject. Our code is available at https://github.com/DeepBCI/Deep-BCI/tree/master/1_Intelligent_BCI/Inter_Subject_Contrastive_Learning_for_EEG.



### Dataset Condensation with Contrastive Signals
- **Arxiv ID**: http://arxiv.org/abs/2202.02916v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02916v3)
- **Published**: 2022-02-07 03:05:32+00:00
- **Updated**: 2022-06-16 15:20:57+00:00
- **Authors**: Saehyung Lee, Sanghyuk Chun, Sangwon Jung, Sangdoo Yun, Sungroh Yoon
- **Comment**: Accepted at ICML 2022
- **Journal**: None
- **Summary**: Recent studies have demonstrated that gradient matching-based dataset synthesis, or dataset condensation (DC), methods can achieve state-of-the-art performance when applied to data-efficient learning tasks. However, in this study, we prove that the existing DC methods can perform worse than the random selection method when task-irrelevant information forms a significant part of the training dataset. We attribute this to the lack of participation of the contrastive signals between the classes resulting from the class-wise gradient matching strategy. To address this problem, we propose Dataset Condensation with Contrastive signals (DCC) by modifying the loss function to enable the DC methods to effectively capture the differences between classes. In addition, we analyze the new loss function in terms of training dynamics by tracking the kernel velocity. Furthermore, we introduce a bi-level warm-up strategy to stabilize the optimization. Our experimental results indicate that while the existing methods are ineffective for fine-grained image classification tasks, the proposed method can successfully generate informative synthetic datasets for the same tasks. Moreover, we demonstrate that the proposed method outperforms the baselines even on benchmark datasets such as SVHN, CIFAR-10, and CIFAR-100. Finally, we demonstrate the high applicability of the proposed method by applying it to continual learning tasks.



### Benchmarking Deep Models for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.02925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02925v1)
- **Published**: 2022-02-07 03:43:16+00:00
- **Updated**: 2022-02-07 03:43:16+00:00
- **Authors**: Huajun Zhou, Yang Lin, Lingxiao Yang, Jianhuang Lai, Xiaohua Xie
- **Comment**: 24 pages
- **Journal**: None
- **Summary**: In recent years, deep network-based methods have continuously refreshed state-of-the-art performance on Salient Object Detection (SOD) task. However, the performance discrepancy caused by different implementation details may conceal the real progress in this task. Making an impartial comparison is required for future researches. To meet this need, we construct a general SALient Object Detection (SALOD) benchmark to conduct a comprehensive comparison among several representative SOD methods. Specifically, we re-implement 14 representative SOD methods by using consistent settings for training. Moreover, two additional protocols are set up in our benchmark to investigate the robustness of existing methods in some limited conditions. In the first protocol, we enlarge the difference between objectness distributions of train and test sets to evaluate the robustness of these SOD methods. In the second protocol, we build multiple train subsets with different scales to validate whether these methods can extract discriminative features from only a few samples. In the above experiments, we find that existing loss functions usually specialized in some metrics but reported inferior results on the others. Therefore, we propose a novel Edge-Aware (EA) loss that promotes deep networks to learn more discriminative features by integrating both pixel- and image-level supervision signals. Experiments prove that our EA loss reports more robust performances compared to existing losses.



### Towards Micro-video Thumbnail Selection via a Multi-label Visual-semantic Embedding Model
- **Arxiv ID**: http://arxiv.org/abs/2202.02930v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02930v1)
- **Published**: 2022-02-07 04:15:26+00:00
- **Updated**: 2022-02-07 04:15:26+00:00
- **Authors**: Liu Bo
- **Comment**: None
- **Journal**: None
- **Summary**: The thumbnail, as the first sight of a micro-video, plays a pivotal role in attracting users to click and watch. While in the real scenario, the more the thumbnails satisfy the users, the more likely the micro-videos will be clicked. In this paper, we aim to select the thumbnail of a given micro-video that meets most users` interests. Towards this end, we present a multi-label visual-semantic embedding model to estimate the similarity between the pair of each frame and the popular topics that users are interested in. In this model, the visual and textual information is embedded into a shared semantic space, whereby the similarity can be measured directly, even the unseen words. Moreover, to compare the frame to all words from the popular topics, we devise an attention embedding space associated with the semantic-attention projection. With the help of these two embedding spaces, the popularity score of a frame, which is defined by the sum of similarity scores over the corresponding visual information and popular topic pairs, is achieved. Ultimately, we fuse the visual representation score and the popularity score of each frame to select the attractive thumbnail for the given micro-video. Extensive experiments conducted on a real-world dataset have well-verified that our model significantly outperforms several state-of-the-art baselines.



### Deep Deterministic Independent Component Analysis for Hyperspectral Unmixing
- **Arxiv ID**: http://arxiv.org/abs/2202.02951v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02951v2)
- **Published**: 2022-02-07 05:26:32+00:00
- **Updated**: 2022-02-15 00:11:37+00:00
- **Authors**: Hongming Li, Shujian Yu, Jose C. Principe
- **Comment**: Accepted by ICASSP 2022
- **Journal**: None
- **Summary**: We develop a new neural network based independent component analysis (ICA) method by directly minimizing the dependence amongst all extracted components. Using the matrix-based R{\'e}nyi's $\alpha$-order entropy functional, our network can be directly optimized by stochastic gradient descent (SGD), without any variational approximation or adversarial training. As a solid application, we evaluate our ICA in the problem of hyperspectral unmixing (HU) and refute a statement that "\emph{ICA does not play a role in unmixing hyperspectral data}", which was initially suggested by \cite{nascimento2005does}. Code and additional remarks of our DDICA is available at https://github.com/hongmingli1995/DDICA.



### Supervision by Denoising
- **Arxiv ID**: http://arxiv.org/abs/2202.02952v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02952v2)
- **Published**: 2022-02-07 05:29:16+00:00
- **Updated**: 2023-04-19 21:08:30+00:00
- **Authors**: Sean I. Young, Adrian V. Dalca, Enzo Ferrante, Polina Golland, Christopher A. Metzler, Bruce Fischl, Juan Eugenio Iglesias
- **Comment**: None
- **Journal**: None
- **Summary**: Learning-based image reconstruction models, such as those based on the U-Net, require a large set of labeled images if good generalization is to be guaranteed. In some imaging domains, however, labeled data with pixel- or voxel-level label accuracy are scarce due to the cost of acquiring them. This problem is exacerbated further in domains like medical imaging, where there is no single ground truth label, resulting in large amounts of repeat variability in the labels. Therefore, training reconstruction networks to generalize better by learning from both labeled and unlabeled examples (called semi-supervised learning) is problem of practical and theoretical interest. However, traditional semi-supervised learning methods for image reconstruction often necessitate handcrafting a differentiable regularizer specific to some given imaging problem, which can be extremely time-consuming. In this work, we propose "supervision by denoising" (SUD), a framework that enables us to supervise reconstruction models using their own denoised output as soft labels. SUD unifies stochastic averaging and spatial denoising techniques under a spatio-temporal denoising framework and alternates denoising and model weight update steps in an optimization framework for semi-supervision. As example applications, we apply SUD to two problems arising from biomedical imaging -- anatomical brain reconstruction (3D) and cortical parcellation (2D) -- to demonstrate a significant improvement in the image reconstructions over supervised-only and stochastic averaging baselines.



### 3D Object Detection from Images for Autonomous Driving: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2202.02980v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02980v2)
- **Published**: 2022-02-07 07:12:24+00:00
- **Updated**: 2022-02-12 19:43:31+00:00
- **Authors**: Xinzhu Ma, Wanli Ouyang, Andrea Simonelli, Elisa Ricci
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection from images, one of the fundamental and challenging problems in autonomous driving, has received increasing attention from both industry and academia in recent years. Benefiting from the rapid development of deep learning technologies, image-based 3D detection has achieved remarkable progress. Particularly, more than 200 works have studied this problem from 2015 to 2021, encompassing a broad spectrum of theories, algorithms, and applications. However, to date no recent survey exists to collect and organize this knowledge. In this paper, we fill this gap in the literature and provide the first comprehensive survey of this novel and continuously growing research field, summarizing the most commonly used pipelines for image-based 3D detection and deeply analyzing each of their components. Additionally, we also propose two new taxonomies to organize the state-of-the-art methods into different categories, with the intent of providing a more systematic review of existing methods and facilitating fair comparisons with future works. In retrospect of what has been achieved so far, we also analyze the current challenges in the field and discuss future directions for image-based 3D detection research.



### Automatic defect segmentation by unsupervised anomaly learning
- **Arxiv ID**: http://arxiv.org/abs/2202.02998v2
- **DOI**: 10.1109/ICIP46576.2022.9898035
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02998v2)
- **Published**: 2022-02-07 08:33:36+00:00
- **Updated**: 2022-06-21 12:49:07+00:00
- **Authors**: Nati Ofir, Ran Yacobi, Omer Granoviter, Boris Levant, Ore Shtalrid
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of defect segmentation in semiconductor manufacturing. The input of our segmentation is a scanning-electron-microscopy (SEM) image of the candidate defect region. We train a U-net shape network to segment defects using a dataset of clean background images. The samples of the training phase are produced automatically such that no manual labeling is required. To enrich the dataset of clean background samples, we apply defect implant augmentation. To that end, we apply a copy-and-paste of a random image patch in the clean specimen. To improve the robustness of the unlabeled data scenario, we train the features of the network with unsupervised learning methods and loss functions. Our experiments show that we succeed to segment real defects with high quality, even though our dataset contains no defect examples. Our approach performs accurately also on the problem of supervised and labeled defect segmentation.



### Learning Sound Localization Better From Semantically Similar Samples
- **Arxiv ID**: http://arxiv.org/abs/2202.03007v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03007v1)
- **Published**: 2022-02-07 08:53:55+00:00
- **Updated**: 2022-02-07 08:53:55+00:00
- **Authors**: Arda Senocak, Hyeonggon Ryu, Junsik Kim, In So Kweon
- **Comment**: Accepted to ICASSP 2022. SOTA performance in Audio-Visual Sound
  Localization. 5 Pages
- **Journal**: None
- **Summary**: The objective of this work is to localize the sound sources in visual scenes. Existing audio-visual works employ contrastive learning by assigning corresponding audio-visual pairs from the same source as positives while randomly mismatched pairs as negatives. However, these negative pairs may contain semantically matched audio-visual information. Thus, these semantically correlated pairs, "hard positives", are mistakenly grouped as negatives. Our key contribution is showing that hard positives can give similar response maps to the corresponding pairs. Our approach incorporates these hard positives by adding their response maps into a contrastive learning objective directly. We demonstrate the effectiveness of our approach on VGG-SS and SoundNet-Flickr test sets, showing favorable performance to the state-of-the-art methods.



### Context Autoencoder for Self-Supervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.03026v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03026v3)
- **Published**: 2022-02-07 09:33:45+00:00
- **Updated**: 2023-08-10 11:01:14+00:00
- **Authors**: Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, Jingdong Wang
- **Comment**: Accepted by International Journal of Computer Vision (IJCV)
- **Journal**: None
- **Summary**: We present a novel masked image modeling (MIM) approach, context autoencoder (CAE), for self-supervised representation pretraining. We pretrain an encoder by making predictions in the encoded representation space. The pretraining tasks include two tasks: masked representation prediction - predict the representations for the masked patches, and masked patch reconstruction - reconstruct the masked patches. The network is an encoder-regressor-decoder architecture: the encoder takes the visible patches as input; the regressor predicts the representations of the masked patches, which are expected to be aligned with the representations computed from the encoder, using the representations of visible patches and the positions of visible and masked patches; the decoder reconstructs the masked patches from the predicted encoded representations. The CAE design encourages the separation of learning the encoder (representation) from completing the pertaining tasks: masked representation prediction and masked patch reconstruction tasks, and making predictions in the encoded representation space empirically shows the benefit to representation learning. We demonstrate the effectiveness of our CAE through superior transfer performance in downstream tasks: semantic segmentation, object detection and instance segmentation, and classification. The code will be available at https://github.com/Atten4Vis/CAE.



### A comprehensive benchmark analysis for sand dust image reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2202.03031v2
- **DOI**: 10.1016/j.jvcir.2022.103638
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03031v2)
- **Published**: 2022-02-07 09:48:09+00:00
- **Updated**: 2022-04-19 07:20:17+00:00
- **Authors**: Yazhong Si, Fan Yang, Ya Guo, Wei Zhang, Yipu Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Numerous sand dust image enhancement algorithms have been proposed in recent years. To our best acknowledge, however, most methods evaluated their performance with no-reference way using few selected real-world images from internet. It is unclear how to quantitatively analysis the performance of the algorithms in a supervised way and how we could gauge the progress in the field. Moreover, due to the absence of large-scale benchmark datasets, there are no well-known reports of data-driven based method for sand dust image enhancement up till now. To advance the development of deep learning-based algorithms for sand dust image reconstruction, while enabling supervised objective evaluation of algorithm performance. In this paper, we presented a comprehensive perceptual study and analysis of real-world sand dust images, then constructed a Sand-dust Image Reconstruction Benchmark (SIRB) for training Convolutional Neural Networks (CNNs) and evaluating algorithms performance. In addition, we adopted the existing image transformation neural network trained on SIRB as baseline to illustrate the generalization of SIRB for training CNNs. Finally, we conducted the qualitative and quantitative evaluation to demonstrate the performance and limitations of the state-of-the-arts (SOTA), which shed light on future research in sand dust image reconstruction.



### Inference of captions from histopathological patches
- **Arxiv ID**: http://arxiv.org/abs/2202.03432v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03432v1)
- **Published**: 2022-02-07 10:02:38+00:00
- **Updated**: 2022-02-07 10:02:38+00:00
- **Authors**: Masayuki Tsuneki, Fahdi Kanavati
- **Comment**: None
- **Journal**: None
- **Summary**: Computational histopathology has made significant strides in the past few years, slowly getting closer to clinical adoption. One area of benefit would be the automatic generation of diagnostic reports from H\&E-stained whole slide images which would further increase the efficiency of the pathologists' routine diagnostic workflows. In this study, we compiled a dataset (PatchGastricADC22) of histopathological captions of stomach adenocarcinoma endoscopic biopsy specimens, which we extracted from diagnostic reports and paired with patches extracted from the associated whole slide images. The dataset contains a variety of gastric adenocarcinoma subtypes. We trained a baseline attention-based model to predict the captions from features extracted from the patches and obtained promising results. We make the captioned dataset of 262K patches publicly available.



### A new face swap method for image and video domains: a technical report
- **Arxiv ID**: http://arxiv.org/abs/2202.03046v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03046v1)
- **Published**: 2022-02-07 10:15:50+00:00
- **Updated**: 2022-02-07 10:15:50+00:00
- **Authors**: Daniil Chesakov, Anastasia Maltseva, Alexander Groshev, Andrey Kuznetsov, Denis Dimitrov
- **Comment**: None
- **Journal**: None
- **Summary**: Deep fake technology became a hot field of research in the last few years. Researchers investigate sophisticated Generative Adversarial Networks (GAN), autoencoders, and other approaches to establish precise and robust algorithms for face swapping. Achieved results show that the deep fake unsupervised synthesis task has problems in terms of the visual quality of generated data. These problems usually lead to high fake detection accuracy when an expert analyzes them. The first problem is that existing image-to-image approaches do not consider video domain specificity and frame-by-frame processing leads to face jittering and other clearly visible distortions. Another problem is the generated data resolution, which is low for many existing methods due to high computational complexity. The third problem appears when the source face has larger proportions (like bigger cheeks), and after replacement it becomes visible on the face border. Our main goal was to develop such an approach that could solve these problems and outperform existing solutions on a number of clue metrics. We introduce a new face swap pipeline that is based on FaceShifter architecture and fixes the problems stated above. With a new eye loss function, super-resolution block, and Gaussian-based face mask generation leads to improvements in quality which is confirmed during evaluation.



### OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/2202.03052v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2202.03052v2)
- **Published**: 2022-02-07 10:38:21+00:00
- **Updated**: 2022-06-01 09:24:35+00:00
- **Authors**: Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang
- **Comment**: Accepted at ICML2022
- **Journal**: None
- **Summary**: In this work, we pursue a unified paradigm for multimodal pretraining to break the scaffolds of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision & language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at https://github.com/OFA-Sys/OFA.



### Evaluation of Runtime Monitoring for UAV Emergency Landing
- **Arxiv ID**: http://arxiv.org/abs/2202.03059v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03059v1)
- **Published**: 2022-02-07 10:51:23+00:00
- **Updated**: 2022-02-07 10:51:23+00:00
- **Authors**: Joris Guerin, Kevin Delmas, Jérémie Guiochet
- **Comment**: 7 pages, 4 figures, 1 table. To appear in the proceedings of 2022
  IEEE International Conference on Robotics and Automation (ICRA)
- **Journal**: None
- **Summary**: To certify UAV operations in populated areas, risk mitigation strategies -- such as Emergency Landing (EL) -- must be in place to account for potential failures. EL aims at reducing ground risk by finding safe landing areas using on-board sensors. The first contribution of this paper is to present a new EL approach, in line with safety requirements introduced in recent research. In particular, the proposed EL pipeline includes mechanisms to monitor learning based components during execution. This way, another contribution is to study the behavior of Machine Learning Runtime Monitoring (MLRM) approaches within the context of a real-world critical system. A new evaluation methodology is introduced, and applied to assess the practical safety benefits of three MLRM mechanisms. The proposed approach is compared to a default mitigation strategy (open a parachute when a failure is detected), and appears to be much safer.



### Imposing Temporal Consistency on Deep Monocular Body Shape and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.03074v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03074v2)
- **Published**: 2022-02-07 11:11:55+00:00
- **Updated**: 2022-02-08 16:58:13+00:00
- **Authors**: Alexandra Zimmer, Anna Hilsmann, Wieland Morgenstern, Peter Eisert
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and temporally consistent modeling of human bodies is essential for a wide range of applications, including character animation, understanding human social behavior and AR/VR interfaces. Capturing human motion accurately from a monocular image sequence is still challenging and the modeling quality is strongly influenced by the temporal consistency of the captured body motion. Our work presents an elegant solution for the integration of temporal constraints in the fitting process. This does not only increase temporal consistency but also robustness during the optimization. In detail, we derive parameters of a sequence of body models, representing shape and motion of a person, including jaw poses, facial expressions, and finger poses. We optimize these parameters over the complete image sequence, fitting one consistent body shape while imposing temporal consistency on the body motion, assuming linear body joint trajectories over a short time. Our approach enables the derivation of realistic 3D body models from image sequences, including facial expression and articulated hands. In extensive experiments, we show that our approach results in accurately estimated body shape and motion, also for challenging movements and poses. Further, we apply it to the special application of sign language analysis, where accurate and temporal consistent motion modelling is essential, and show that the approach is well-suited for this kind of application.



### Temporal Point Cloud Completion with Pose Disturbance
- **Arxiv ID**: http://arxiv.org/abs/2202.03084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03084v1)
- **Published**: 2022-02-07 11:41:12+00:00
- **Updated**: 2022-02-07 11:41:12+00:00
- **Authors**: Jieqi Shi, Lingyun Xu, Peiliang Li, Xiaozhi Chen, Shaojie Shen
- **Comment**: 8 pages; Accepted by RAL with ICRA 2022
- **Journal**: None
- **Summary**: Point clouds collected by real-world sensors are always unaligned and sparse, which makes it hard to reconstruct the complete shape of object from a single frame of data. In this work, we manage to provide complete point clouds from sparse input with pose disturbance by limited translation and rotation. We also use temporal information to enhance the completion model, refining the output with a sequence of inputs. With the help of gated recovery units(GRU) and attention mechanisms as temporal units, we propose a point cloud completion framework that accepts a sequence of unaligned and sparse inputs, and outputs consistent and aligned point clouds. Our network performs in an online manner and presents a refined point cloud for each frame, which enables it to be integrated into any SLAM or reconstruction pipeline. As far as we know, our framework is the first to utilize temporal information and ensure temporal consistency with limited transformation. Through experiments in ShapeNet and KITTI, we prove that our framework is effective in both synthetic and real-world datasets.



### Unsupervised Long-Term Person Re-Identification with Clothes Change
- **Arxiv ID**: http://arxiv.org/abs/2202.03087v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03087v2)
- **Published**: 2022-02-07 11:55:23+00:00
- **Updated**: 2022-02-08 14:28:18+00:00
- **Authors**: Mingkun Li, Peng Xu, Xiatian Zhu, Jun Guo
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate unsupervised person re-identification (Re-ID) with clothes change, a new challenging problem with more practical usability and scalability to real-world deployment. Most existing re-id methods artificially assume the clothes of every single person to be stationary across space and time. This condition is mostly valid for short-term re-id scenarios since an average person would often change the clothes even within a single day. To alleviate this assumption, several recent works have introduced the clothes change facet to re-id, with a focus on supervised learning person identity discriminative representation with invariance to clothes changes. Taking a step further towards this long-term re-id direction, we further eliminate the requirement of person identity labels, as they are significantly more expensive and more tedious to annotate in comparison to short-term person re-id datasets. Compared to conventional unsupervised short-term re-id, this new problem is drastically more challenging as different people may have similar clothes whilst the same person can wear multiple suites of clothes over different locations and times with very distinct appearance. To overcome such obstacles, we introduce a novel Curriculum Person Clustering (CPC) method that can adaptively regulate the unsupervised clustering criterion according to the clustering confidence. Experiments on three long-term person re-id datasets show that our CPC outperforms SOTA unsupervised re-id methods and even closely matches the supervised re-id models.



### Auto-Lambda: Disentangling Dynamic Task Relationships
- **Arxiv ID**: http://arxiv.org/abs/2202.03091v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03091v2)
- **Published**: 2022-02-07 12:07:50+00:00
- **Updated**: 2022-06-02 11:40:03+00:00
- **Authors**: Shikun Liu, Stephen James, Andrew J. Davison, Edward Johns
- **Comment**: Published at TMLR 2022. Project Page:
  https://shikun.io/projects/auto-lambda Code:
  https://github.com/lorenmt/auto-lambda
- **Journal**: None
- **Summary**: Understanding the structure of multiple related tasks allows for multi-task learning to improve the generalisation ability of one or all of them. However, it usually requires training each pairwise combination of tasks together in order to capture task relationships, at an extremely high computational cost. In this work, we learn task relationships via an automated weighting framework, named Auto-Lambda. Unlike previous methods where task relationships are assumed to be fixed, Auto-Lambda is a gradient-based meta learning framework which explores continuous, dynamic task relationships via task-specific weightings, and can optimise any choice of combination of tasks through the formulation of a meta-loss; where the validation loss automatically influences task weightings throughout training. We apply the proposed framework to both multi-task and auxiliary learning problems in computer vision and robotics, and show that Auto-Lambda achieves state-of-the-art performance, even when compared to optimisation strategies designed specifically for each problem and data domain. Finally, we observe that Auto-Lambda can discover interesting learning behaviors, leading to new insights in multi-task learning. Code is available at https://github.com/lorenmt/auto-lambda.



### A Coarse-to-fine Morphological Approach With Knowledge-based Rules and Self-adapting Correction for Lung Nodules Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.03433v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03433v1)
- **Published**: 2022-02-07 12:10:37+00:00
- **Updated**: 2022-02-07 12:10:37+00:00
- **Authors**: Xinliang Fu, Jiayin Zheng, Juanyun Mai, Yanbo Shao, Minghao Wang, Linyu Li, Zhaoqi Diao, Yulong Chen, Jianyu Xiao, Jian You, Airu Yin, Yang Yang, Xiangcheng Qiu, Jinsheng Tao, Bo Wang, Hua Ji
- **Comment**: None
- **Journal**: None
- **Summary**: The segmentation module which precisely outlines the nodules is a crucial step in a computer-aided diagnosis(CAD) system. The most challenging part of such a module is how to achieve high accuracy of the segmentation, especially for the juxtapleural, non-solid and small nodules. In this research, we present a coarse-to-fine methodology that greatly improves the thresholding method performance with a novel self-adapting correction algorithm and effectively removes noisy pixels with well-defined knowledge-based principles. Compared with recent strong morphological baselines, our algorithm, by combining dataset features, achieves state-of-the-art performance on both the public LIDC-IDRI dataset (DSC 0.699) and our private LC015 dataset (DSC 0.760) which closely approaches the SOTA deep learning-based models' performances. Furthermore, unlike most available morphological methods that can only segment the isolated and well-circumscribed nodules accurately, the precision of our method is totally independent of the nodule type or diameter, proving its applicability and generality.



### Bubble identification from images with machine learning methods
- **Arxiv ID**: http://arxiv.org/abs/2202.03107v2
- **DOI**: 10.1016/j.ijmultiphaseflow.2022.104169
- **Categories**: **cs.CV**, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/2202.03107v2)
- **Published**: 2022-02-07 12:38:17+00:00
- **Updated**: 2022-08-05 06:32:36+00:00
- **Authors**: Hendrik Hessenkemper, Sebastian Starke, Yazan Atassi, Thomas Ziegenhein, Dirk Lucas
- **Comment**: None
- **Journal**: None
- **Summary**: An automated and reliable processing of bubbly flow images is highly needed to analyse large data sets of comprehensive experimental series. A particular difficulty arises due to overlapping bubble projections in recorded images, which highly complicates the identification of individual bubbles. Recent approaches focus on the use of deep learning algorithms for this task and have already proven the high potential of such techniques. The main difficulties are the capability to handle different image conditions, higher gas volume fractions and a proper reconstruction of the hidden segment of a partly occluded bubble. In the present work, we try to tackle these points by testing three different methods based on Convolutional Neural Networks (CNNs) for the two former and two individual approaches that can be used subsequently to address the latter. To validate our methodology, we created test data sets with synthetic images that further demonstrate the capabilities as well as limitations of our combined approach. The generated data, code and trained models are made accessible to facilitate the use as well as further developments in the research field of bubble recognition in experimental images.



### Leveraging Ensembles and Self-Supervised Learning for Fully-Unsupervised Person Re-Identification and Text Authorship Attribution
- **Arxiv ID**: http://arxiv.org/abs/2202.03126v4
- **DOI**: 10.1109/TIFS.2023.3289448
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03126v4)
- **Published**: 2022-02-07 13:08:11+00:00
- **Updated**: 2023-06-30 17:08:15+00:00
- **Authors**: Gabriel Bertocco, Antônio Theophilo, Fernanda Andaló, Anderson Rocha
- **Comment**: This work has been accepted for publication in the IEEE Transactions
  on Information Forensics and Security
- **Journal**: None
- **Summary**: Learning from fully-unlabeled data is challenging in Multimedia Forensics problems, such as Person Re-Identification and Text Authorship Attribution. Recent self-supervised learning methods have shown to be effective when dealing with fully-unlabeled data in cases where the underlying classes have significant semantic differences, as intra-class distances are substantially lower than inter-class distances. However, this is not the case for forensic applications in which classes have similar semantics and the training and test sets have disjoint identities. General self-supervised learning methods might fail to learn discriminative features in this scenario, thus requiring more robust strategies. We propose a strategy to tackle Person Re-Identification and Text Authorship Attribution by enabling learning from unlabeled data even when samples from different classes are not prominently diverse. We propose a novel ensemble-based clustering strategy whereby clusters derived from different configurations are combined to generate a better grouping for the data samples in a fully-unsupervised way. This strategy allows clusters with different densities and higher variability to emerge, reducing intra-class discrepancies without requiring the burden of finding an optimal configuration per dataset. We also consider different Convolutional Neural Networks for feature extraction and subsequent distance computations between samples. We refine these distances by incorporating context and grouping them to capture complementary information. Our method is robust across both tasks, with different data modalities, and outperforms state-of-the-art methods with a fully-unsupervised solution without any labeling or human intervention.



### Transformers in Self-Supervised Monocular Depth Estimation with Unknown Camera Intrinsics
- **Arxiv ID**: http://arxiv.org/abs/2202.03131v1
- **DOI**: 10.5220/0010884000003124
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.03131v1)
- **Published**: 2022-02-07 13:17:29+00:00
- **Updated**: 2022-02-07 13:17:29+00:00
- **Authors**: Arnav Varma, Hemang Chawla, Bahram Zonooz, Elahe Arani
- **Comment**: Published in 17th International Conference on Computer Vision Theory
  and Applications (VISAP, 2022)
- **Journal**: None
- **Summary**: The advent of autonomous driving and advanced driver assistance systems necessitates continuous developments in computer vision for 3D scene understanding. Self-supervised monocular depth estimation, a method for pixel-wise distance estimation of objects from a single camera without the use of ground truth labels, is an important task in 3D scene understanding. However, existing methods for this task are limited to convolutional neural network (CNN) architectures. In contrast with CNNs that use localized linear operations and lose feature resolution across the layers, vision transformers process at constant resolution with a global receptive field at every stage. While recent works have compared transformers against their CNN counterparts for tasks such as image classification, no study exists that investigates the impact of using transformers for self-supervised monocular depth estimation. Here, we first demonstrate how to adapt vision transformers for self-supervised monocular depth estimation. Thereafter, we compare the transformer and CNN-based architectures for their performance on KITTI depth prediction benchmarks, as well as their robustness to natural corruptions and adversarial attacks, including when the camera intrinsics are unknown. Our study demonstrates how transformer-based architecture, though lower in run-time efficiency, achieves comparable performance while being more robust and generalizable.



### Patch-Based Stochastic Attention for Image Editing
- **Arxiv ID**: http://arxiv.org/abs/2202.03163v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03163v3)
- **Published**: 2022-02-07 13:42:00+00:00
- **Updated**: 2022-09-30 15:47:13+00:00
- **Authors**: Nicolas Cherel, Andrés Almansa, Yann Gousseau, Alasdair Newson
- **Comment**: 17 pages, 11 figures
- **Journal**: None
- **Summary**: Attention mechanisms have become of crucial importance in deep learning in recent years. These non-local operations, which are similar to traditional patch-based methods in image processing, complement local convolutions. However, computing the full attention matrix is an expensive step with heavy memory and computational loads. These limitations curb network architectures and performances, in particular for the case of high resolution images. We propose an efficient attention layer based on the stochastic algorithm PatchMatch, which is used for determining approximate nearest neighbors. We refer to our proposed layer as a "Patch-based Stochastic Attention Layer" (PSAL). Furthermore, we propose different approaches, based on patch aggregation, to ensure the differentiability of PSAL, thus allowing end-to-end training of any network containing our layer. PSAL has a small memory footprint and can therefore scale to high resolution images. It maintains this footprint without sacrificing spatial precision and globality of the nearest neighbors, which means that it can be easily inserted in any level of a deep architecture, even in shallower levels. We demonstrate the usefulness of PSAL on several image editing tasks, such as image inpainting, guided image colorization, and single-image super-resolution. Our code is available at: https://github.com/ncherel/psal



### Field-of-View IoU for Object Detection in 360° Images
- **Arxiv ID**: http://arxiv.org/abs/2202.03176v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03176v2)
- **Published**: 2022-02-07 14:01:59+00:00
- **Updated**: 2022-09-23 02:05:17+00:00
- **Authors**: Miao Cao, Satoshi Ikehata, Kiyoharu Aizawa
- **Comment**: None
- **Journal**: None
- **Summary**: 360{\deg} cameras have gained popularity over the last few years. In this paper, we propose two fundamental techniques -- Field-of-View IoU (FoV-IoU) and 360Augmentation for object detection in 360{\deg} images. Although most object detection neural networks designed for the perspective images are applicable to 360{\deg} images in equirectangular projection (ERP) format, their performance deteriorates owing to the distortion in ERP images. Our method can be readily integrated with existing perspective object detectors and significantly improves the performance. The FoV-IoU computes the intersection-over-union of two Field-of-View bounding boxes in a spherical image which could be used for training, inference, and evaluation while 360Augmentation is a data augmentation technique specific to 360{\deg} object detection task which randomly rotates a spherical image and solves the bias due to the sphere-to-plane projection. We conduct extensive experiments on the 360indoor dataset with different types of perspective object detectors and show the consistent effectiveness of our method.



### Recent Trends in 2D Object Detection and Applications in Video Event Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.03206v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03206v1)
- **Published**: 2022-02-07 14:15:11+00:00
- **Updated**: 2022-02-07 14:15:11+00:00
- **Authors**: Prithwish Jana, Partha Pratim Mohanta
- **Comment**: Book chapter: P Jana and PP Mohanta, Recent Trends in 2D Object
  Detection and Applications in Video Event Recognition, published in
  Advancement of Deep Learning and its Applications in Object Detection and
  Recognition, edited by R N Mir et al, 2022, published by River Publishers
- **Journal**: None
- **Summary**: Object detection serves as a significant step in improving performance of complex downstream computer vision tasks. It has been extensively studied for many years now and current state-of-the-art 2D object detection techniques proffer superlative results even in complex images. In this chapter, we discuss the geometry-based pioneering works in object detection, followed by the recent breakthroughs that employ deep learning. Some of these use a monolithic architecture that takes a RGB image as input and passes it to a feed-forward ConvNet or vision Transformer. These methods, thereby predict class-probability and bounding-box coordinates, all in a single unified pipeline. Two-stage architectures on the other hand, first generate region proposals and then feed it to a CNN to extract features and predict object category and bounding-box. We also elaborate upon the applications of object detection in video event recognition, to achieve better fine-grained video classification performance. Further, we highlight recent datasets for 2D object detection both in images and videos, and present a comparative performance summary of various state-of-the-art object detection techniques.



### PSSNet: Planarity-sensible Semantic Segmentation of Large-scale Urban Meshes
- **Arxiv ID**: http://arxiv.org/abs/2202.03209v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03209v4)
- **Published**: 2022-02-07 14:16:10+00:00
- **Updated**: 2022-12-24 12:59:37+00:00
- **Authors**: Weixiao Gao, Liangliang Nan, Bas Boom, Hugo Ledoux
- **Comment**: 24 pages,11 figures
- **Journal**: None
- **Summary**: We introduce a novel deep learning-based framework to interpret 3D urban scenes represented as textured meshes. Based on the observation that object boundaries typically align with the boundaries of planar regions, our framework achieves semantic segmentation in two steps: planarity-sensible over-segmentation followed by semantic classification. The over-segmentation step generates an initial set of mesh segments that capture the planar and non-planar regions of urban scenes. In the subsequent classification step, we construct a graph that encodes the geometric and photometric features of the segments in its nodes and the multi-scale contextual features in its edges. The final semantic segmentation is obtained by classifying the segments using a graph convolutional network. Experiments and comparisons on two semantic urban mesh benchmarks demonstrate that our approach outperforms the state-of-the-art methods in terms of boundary quality, mean IoU (intersection over union), and generalization ability. We also introduce several new metrics for evaluating mesh over-segmentation methods dedicated to semantic segmentation, and our proposed over-segmentation approach outperforms state-of-the-art methods on all metrics. Our source code is available at \url{https://github.com/WeixiaoGao/PSSNet}.



### Towards an Analytical Definition of Sufficient Data
- **Arxiv ID**: http://arxiv.org/abs/2202.03238v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03238v1)
- **Published**: 2022-02-07 14:44:31+00:00
- **Updated**: 2022-02-07 14:44:31+00:00
- **Authors**: Adam Byerly, Tatiana Kalganova
- **Comment**: 17 pages, 36 figures, 7 tables
- **Journal**: None
- **Summary**: We show that, for each of five datasets of increasing complexity, certain training samples are more informative of class membership than others. These samples can be identified a priori to training by analyzing their position in reduced dimensional space relative to the classes' centroids. Specifically, we demonstrate that samples nearer the classes' centroids are less informative than those that are furthest from it. For all five datasets, we show that there is no statistically significant difference between training on the entire training set and when excluding up to 2% of the data nearest to each class's centroid.



### Confidence Guided Depth Completion Network
- **Arxiv ID**: http://arxiv.org/abs/2202.03257v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T07
- **Links**: [PDF](http://arxiv.org/pdf/2202.03257v1)
- **Published**: 2022-02-07 14:57:28+00:00
- **Updated**: 2022-02-07 14:57:28+00:00
- **Authors**: Yongjin Lee, Seokjun Park, Beomgu Kang, Hyunwook Park
- **Comment**: 6 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: The paper proposes an image-guided depth completion method to estimate accurate dense depth maps with fast computation time. The proposed network has two-stage structure. The first stage predicts a first depth map. Then, the second stage further refines the first depth map using the confidence maps. The second stage consists of two layers, each of which focuses on different regions and generates a refined depth map and a confidence map. The final depth map is obtained by combining two depth maps from the second stage using the corresponding confidence maps. Compared with the top-ranked models on the KITTI depth completion online leaderboard, the proposed model shows much faster computation time and competitive performance.



### Multi-modal data generation with a deep metric variational autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2202.03434v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03434v1)
- **Published**: 2022-02-07 15:00:02+00:00
- **Updated**: 2022-02-07 15:00:02+00:00
- **Authors**: Josefine Vilsbøll Sundgaard, Morten Rieger Hannemose, Søren Laugesen, Peter Bray, James Harte, Yosuke Kamide, Chiemi Tanaka, Rasmus R. Paulsen, Anders Nymark Christensen
- **Comment**: None
- **Journal**: None
- **Summary**: We present a deep metric variational autoencoder for multi-modal data generation. The variational autoencoder employs triplet loss in the latent space, which allows for conditional data generation by sampling in the latent space within each class cluster. The approach is evaluated on a multi-modal dataset consisting of otoscopy images of the tympanic membrane with corresponding wideband tympanometry measurements. The modalities in this dataset are correlated, as they represent different aspects of the state of the middle ear, but they do not present a direct pixel-to-pixel correlation. The approach shows promising results for the conditional generation of pairs of images and tympanograms, and will allow for efficient data augmentation of data from multi-modal sources.



### Crafting Better Contrastive Views for Siamese Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.03278v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03278v3)
- **Published**: 2022-02-07 15:09:00+00:00
- **Updated**: 2022-03-30 02:47:31+00:00
- **Authors**: Xiangyu Peng, Kai Wang, Zheng Zhu, Mang Wang, Yang You
- **Comment**: Accepted by CVPR 2022 as an oral paper
- **Journal**: None
- **Summary**: Recent self-supervised contrastive learning methods greatly benefit from the Siamese structure that aims at minimizing distances between positive pairs. For high performance Siamese representation learning, one of the keys is to design good contrastive pairs. Most previous works simply apply random sampling to make different crops of the same image, which overlooks the semantic information that may degrade the quality of views. In this work, we propose ContrastiveCrop, which could effectively generate better crops for Siamese representation learning. Firstly, a semantic-aware object localization strategy is proposed within the training process in a fully unsupervised manner. This guides us to generate contrastive views which could avoid most false positives (i.e., object vs. background). Moreover, we empirically find that views with similar appearances are trivial for the Siamese model training. Thus, a center-suppressed sampling is further designed to enlarge the variance of crops. Remarkably, our method takes a careful consideration of positive pairs for contrastive learning with negligible extra training overhead. As a plug-and-play and framework-agnostic module, ContrastiveCrop consistently improves SimCLR, MoCo, BYOL, SimSiam by 0.4% ~ 2.0% classification accuracy on CIFAR-10, CIFAR-100, Tiny ImageNet and STL-10. Superior results are also achieved on downstream detection and segmentation tasks when pre-trained on ImageNet-1K.



### CZU-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and 10 wearable inertial sensors
- **Arxiv ID**: http://arxiv.org/abs/2202.03283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03283v1)
- **Published**: 2022-02-07 15:17:08+00:00
- **Updated**: 2022-02-07 15:17:08+00:00
- **Authors**: Xin Chao, Zhenjie Hou, Yujian Mo
- **Comment**: None
- **Journal**: None
- **Summary**: Human action recognition has been widely used in many fields of life, and many human action datasets have been published at the same time. However, most of the multi-modal databases have some shortcomings in the layout and number of sensors, which cannot fully represent the action features. Regarding the problems, this paper proposes a freely available dataset, named CZU-MHAD (Changzhou University: a comprehensive multi-modal human action dataset). It consists of 22 actions and three modals temporal synchronized data. These modals include depth videos and skeleton positions from a kinect v2 camera, and inertial signals from 10 wearable sensors. Compared with single modal sensors, multi-modal sensors can collect different modal data, so the use of multi-modal sensors can describe actions more accurately. Moreover, CZU-MHAD obtains the 3-axis acceleration and 3-axis angular velocity of 10 main motion joints by binding inertial sensors to them, and these data were captured at the same time. Experimental results are provided to show that this dataset can be used to study structural relationships between different parts of the human body when performing actions and fusion approaches that involve multi-modal sensor data.



### Motion-Plane-Adaptive Inter Prediction in 360-Degree Video Coding
- **Arxiv ID**: http://arxiv.org/abs/2202.03323v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03323v2)
- **Published**: 2022-02-07 16:15:08+00:00
- **Updated**: 2022-07-11 16:22:37+00:00
- **Authors**: Andy Regensky, Christian Herglotz, André Kaup
- **Comment**: 14 pages, 12 figures, 6 tables; This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible
- **Journal**: None
- **Summary**: Inter prediction is one of the key technologies enabling the high compression efficiency of modern video coding standards. 360-degree video needs to be mapped to the 2D image plane prior to coding in order to allow compression using existing video coding standards. The distortions that inevitably occur when mapping spherical data onto the 2D image plane, however, impair the performance of classical inter prediction techniques. In this paper, we propose a motion-plane-adaptive inter prediction technique (MPA) for 360-degree video that takes the spherical characteristics of 360-degree video into account. Based on the known projection format of the video, MPA allows to perform inter prediction on different motion planes in 3D space instead of having to work on the - in theory arbitrarily mapped - 2D image representation directly. We furthermore derive a motion-plane-adaptive motion vector prediction technique (MPA-MVP) that allows to translate motion information between different motion planes and motion models. Our proposed integration of MPA together with MPA-MVP into the state-of-the-art H.266/VVC video coding standard shows significant Bjontegaard Delta rate savings of 1.72% with a peak of 3.97% based on PSNR and 1.56% with a peak of 3.40% based on WS-PSNR compared to the VTM-14.2 baseline on average.



### A Review of Landcover Classification with Very-High Resolution Remotely Sensed Optical Images-Analysis Unit,Model Scalability and Transferability
- **Arxiv ID**: http://arxiv.org/abs/2202.03342v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03342v1)
- **Published**: 2022-02-07 16:38:40+00:00
- **Updated**: 2022-02-07 16:38:40+00:00
- **Authors**: Rongjun Qin, Tao Liu
- **Comment**: 29 pages
- **Journal**: Remote Sensing, 14(3),646, 2022
- **Summary**: As an important application in remote sensing, landcover classification remains one of the most challenging tasks in very-high-resolution (VHR) image analysis. As the rapidly increasing number of Deep Learning (DL) based landcover methods and training strategies are claimed to be the state-of-the-art, the already fragmented technical landscape of landcover mapping methods has been further complicated. Although there exists a plethora of literature review work attempting to guide researchers in making an informed choice of landcover mapping methods, the articles either focus on the review of applications in a specific area or revolve around general deep learning models, which lack a systematic view of the ever advancing landcover mapping methods. In addition, issues related to training samples and model transferability have become more critical than ever in an era dominated by data-driven approaches, but these issues were addressed to a lesser extent in previous review articles regarding remote sensing classification. Therefore, in this paper, we present a systematic overview of existing methods by starting from learning methods and varying basic analysis units for landcover mapping tasks, to challenges and solutions on three aspects of scalability and transferability with a remote sensing classification focus including (1) sparsity and imbalance of data; (2) domain gaps across different geographical regions; and (3) multi-source and multi-view fusion. We discuss in detail each of these categorical methods and draw concluding remarks in these developments and recommend potential directions for the continued endeavor.



### FrePGAN: Robust Deepfake Detection Using Frequency-level Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2202.03347v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03347v1)
- **Published**: 2022-02-07 16:45:11+00:00
- **Updated**: 2022-02-07 16:45:11+00:00
- **Authors**: Yonghyun Jeong, Doyeon Kim, Youngmin Ro, Jongwon Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Various deepfake detectors have been proposed, but challenges still exist to detect images of unknown categories or GAN models outside of the training settings. Such issues arise from the overfitting issue, which we discover from our own analysis and the previous studies to originate from the frequency-level artifacts in generated images. We find that ignoring the frequency-level artifacts can improve the detector's generalization across various GAN models, but it can reduce the model's performance for the trained GAN models. Thus, we design a framework to generalize the deepfake detector for both the known and unseen GAN models. Our framework generates the frequency-level perturbation maps to make the generated images indistinguishable from the real images. By updating the deepfake detector along with the training of the perturbation generator, our model is trained to detect the frequency-level artifacts at the initial iterations and consider the image-level irregularities at the last iterations. For experiments, we design new test scenarios varying from the training settings in GAN models, color manipulations, and object categories. Numerous experiments validate the state-of-the-art performance of our deepfake detector.



### Simple Control Baselines for Evaluating Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.03365v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03365v1)
- **Published**: 2022-02-07 17:26:26+00:00
- **Updated**: 2022-02-07 17:26:26+00:00
- **Authors**: Andrei Atanov, Shijian Xu, Onur Beker, Andrei Filatov, Amir Zamir
- **Comment**: Project website: https://transfer-controls.epfl.ch
- **Journal**: None
- **Summary**: Transfer learning has witnessed remarkable progress in recent years, for example, with the introduction of augmentation-based contrastive self-supervised learning methods. While a number of large-scale empirical studies on the transfer performance of such models have been conducted, there is not yet an agreed-upon set of control baselines, evaluation practices, and metrics to report, which often hinders a nuanced and calibrated understanding of the real efficacy of the methods. We share an evaluation standard that aims to quantify and communicate transfer learning performance in an informative and accessible setup. This is done by baking a number of simple yet critical control baselines in the evaluation method, particularly the blind-guess (quantifying the dataset bias), scratch-model (quantifying the architectural contribution), and maximal-supervision (quantifying the upper-bound). To demonstrate how the evaluation standard can be employed, we provide an example empirical study investigating a few basic questions about self-supervised learning. For example, using this standard, the study shows the effectiveness of existing self-supervised pre-training methods is skewed towards image classification tasks versus dense pixel-wise predictions. In general, we encourage using/reporting the suggested control baselines in evaluating transfer learning in order to gain a more meaningful and informative understanding.



### LEDNet: Joint Low-light Enhancement and Deblurring in the Dark
- **Arxiv ID**: http://arxiv.org/abs/2202.03373v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03373v2)
- **Published**: 2022-02-07 17:44:05+00:00
- **Updated**: 2022-08-30 08:22:49+00:00
- **Authors**: Shangchen Zhou, Chongyi Li, Chen Change Loy
- **Comment**: ECCV2022
- **Journal**: None
- **Summary**: Night photography typically suffers from both low light and blurring issues due to the dim environment and the common use of long exposure. While existing light enhancement and deblurring methods could deal with each problem individually, a cascade of such methods cannot work harmoniously to cope well with joint degradation of visibility and textures. Training an end-to-end network is also infeasible as no paired data is available to characterize the coexistence of low light and blurs. We address the problem by introducing a novel data synthesis pipeline that models realistic low-light blurring degradations. With the pipeline, we present the first large-scale dataset for joint low-light enhancement and deblurring. The dataset, LOL-Blur, contains 12,000 low-blur/normal-sharp pairs with diverse darkness and motion blurs in different scenarios. We further present an effective network, named LEDNet, to perform joint low-light enhancement and deblurring. Our network is unique as it is specially designed to consider the synergy between the two inter-connected tasks. Both the proposed dataset and network provide a foundation for this challenging joint task. Extensive experiments demonstrate the effectiveness of our method on both synthetic and real-world datasets.



### Message Passing Neural PDE Solvers
- **Arxiv ID**: http://arxiv.org/abs/2202.03376v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2202.03376v3)
- **Published**: 2022-02-07 17:47:46+00:00
- **Updated**: 2023-03-20 07:52:57+00:00
- **Authors**: Johannes Brandstetter, Daniel Worrall, Max Welling
- **Comment**: Published at ICLR 2022 (Spotlight paper), Github:
  https://github.com/brandstetter-johannes/MP-Neural-PDE-Solvers
- **Journal**: None
- **Summary**: The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a method that is based on the principle of zero-stability, posing stability as a domain adaptation problem. We validate our method on various fluid-like flow problems, demonstrating fast, stable, and accurate performance across different domain topologies, equation parameters, discretizations, etc., in 1D and 2D.



### Benchmarking and Analyzing Point Cloud Classification under Corruptions
- **Arxiv ID**: http://arxiv.org/abs/2202.03377v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03377v3)
- **Published**: 2022-02-07 17:50:21+00:00
- **Updated**: 2022-06-14 16:03:22+00:00
- **Authors**: Jiawei Ren, Liang Pan, Ziwei Liu
- **Comment**: ICML 2022; Code and dataset are available at
  https://github.com/jiawei-ren/modelnetc
- **Journal**: None
- **Summary**: 3D perception, especially point cloud classification, has achieved substantial progress. However, in real-world deployment, point cloud corruptions are inevitable due to the scene complexity, sensor inaccuracy, and processing imprecision. In this work, we aim to rigorously benchmark and analyze point cloud classification under corruptions. To conduct a systematic investigation, we first provide a taxonomy of common 3D corruptions and identify the atomic corruptions. Then, we perform a comprehensive evaluation on a wide range of representative point cloud models to understand their robustness and generalizability. Our benchmark results show that although point cloud classification performance improves over time, the state-of-the-art methods are on the verge of being less robust. Based on the obtained observations, we propose several effective techniques to enhance point cloud classifier robustness. We hope our comprehensive benchmark, in-depth analysis, and proposed techniques could spark future research in robust 3D perception.



### Corrupted Image Modeling for Self-Supervised Visual Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2202.03382v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03382v2)
- **Published**: 2022-02-07 17:59:04+00:00
- **Updated**: 2023-03-16 05:58:10+00:00
- **Authors**: Yuxin Fang, Li Dong, Hangbo Bao, Xinggang Wang, Furu Wei
- **Comment**: ICLR 2023
- **Journal**: None
- **Summary**: We introduce Corrupted Image Modeling (CIM) for self-supervised visual pre-training. CIM uses an auxiliary generator with a small trainable BEiT to corrupt the input image instead of using artificial [MASK] tokens, where some patches are randomly selected and replaced with plausible alternatives sampled from the BEiT output distribution. Given this corrupted image, an enhancer network learns to either recover all the original image pixels, or predict whether each visual token is replaced by a generator sample or not. The generator and the enhancer are simultaneously trained and synergistically updated. After pre-training, the enhancer can be used as a high-capacity visual encoder for downstream tasks. CIM is a general and flexible visual pre-training framework that is suitable for various network architectures. For the first time, CIM demonstrates that both ViT and CNN can learn rich visual representations using a unified, non-Siamese framework. Experimental results show that our approach achieves compelling results in vision benchmarks, such as ImageNet classification and ADE20K semantic segmentation.



### Hybrid Contrastive Quantization for Efficient Cross-View Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2202.03384v2
- **DOI**: 10.1145/3485447.3512022
- **Categories**: **cs.IR**, cs.CV, cs.LG, cs.MM, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2202.03384v2)
- **Published**: 2022-02-07 18:04:10+00:00
- **Updated**: 2022-02-10 01:30:08+00:00
- **Authors**: Jinpeng Wang, Bin Chen, Dongliang Liao, Ziyun Zeng, Gongfu Li, Shu-Tao Xia, Jin Xu
- **Comment**: Accepted to The Web Conference 2022 (WWW'22). 11 pages, 5 tables, 6
  figures
- **Journal**: None
- **Summary**: With the recent boom of video-based social platforms (e.g., YouTube and TikTok), video retrieval using sentence queries has become an important demand and attracts increasing research attention. Despite the decent performance, existing text-video retrieval models in vision and language communities are impractical for large-scale Web search because they adopt brute-force search based on high-dimensional embeddings. To improve efficiency, Web search engines widely apply vector compression libraries (e.g., FAISS) to post-process the learned embeddings. Unfortunately, separate compression from feature encoding degrades the robustness of representations and incurs performance decay. To pursue a better balance between performance and efficiency, we propose the first quantized representation learning method for cross-view video retrieval, namely Hybrid Contrastive Quantization (HCQ). Specifically, HCQ learns both coarse-grained and fine-grained quantizations with transformers, which provide complementary understandings for texts and videos and preserve comprehensive semantic information. By performing Asymmetric-Quantized Contrastive Learning (AQ-CL) across views, HCQ aligns texts and videos at coarse-grained and multiple fine-grained levels. This hybrid-grained learning strategy serves as strong supervision on the cross-view video quantization model, where contrastive learning at different levels can be mutually promoted. Extensive experiments on three Web video benchmark datasets demonstrate that HCQ achieves competitive performance with state-of-the-art non-compressed retrieval methods while showing high efficiency in storage and computation. Code and configurations are available at https://github.com/gimpong/WWW22-HCQ.



### PatClArC: Using Pattern Concept Activation Vectors for Noise-Robust Model Debugging
- **Arxiv ID**: http://arxiv.org/abs/2202.03482v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03482v1)
- **Published**: 2022-02-07 19:40:20+00:00
- **Updated**: 2022-02-07 19:40:20+00:00
- **Authors**: Frederik Pahde, Leander Weber, Christopher J. Anders, Wojciech Samek, Sebastian Lapuschkin
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art machine learning models are commonly (pre-)trained on large benchmark datasets. These often contain biases, artifacts, or errors that have remained unnoticed in the data collection process and therefore fail in representing the real world truthfully. This can cause models trained on these datasets to learn undesired behavior based upon spurious correlations, e.g., the existence of a copyright tag in an image. Concept Activation Vectors (CAV) have been proposed as a tool to model known concepts in latent space and have been used for concept sensitivity testing and model correction. Specifically, class artifact compensation (ClArC) corrects models using CAVs to represent data artifacts in feature space linearly. Modeling CAVs with filters of linear models, however, causes a significant influence of the noise portion within the data, as recent work proposes the unsuitability of linear model filters to find the signal direction in the input, which can be avoided by instead using patterns. In this paper we propose Pattern Concept Activation Vectors (PCAV) for noise-robust concept representations in latent space. We demonstrate that pattern-based artifact modeling has beneficial effects on the application of CAVs as a means to remove influence of confounding features from models via the ClArC framework.



### Random Ferns for Semantic Segmentation of PolSAR Images
- **Arxiv ID**: http://arxiv.org/abs/2202.03498v1
- **DOI**: 10.1109/TGRS.2021.3131418
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03498v1)
- **Published**: 2022-02-07 20:22:57+00:00
- **Updated**: 2022-02-07 20:22:57+00:00
- **Authors**: Pengchao Wei, Ronny Hänsch
- **Comment**: This is the author's version of the article as accepted for
  publication in IEEE Transactions on Geoscience and Remote Sensing, 2021. Link
  to original: https://ieeexplore.ieee.org/document/9627989
- **Journal**: None
- **Summary**: Random Ferns -- as a less known example of Ensemble Learning -- have been successfully applied in many Computer Vision applications ranging from keypoint matching to object detection. This paper extends the Random Fern framework to the semantic segmentation of polarimetric synthetic aperture radar images. By using internal projections that are defined over the space of Hermitian matrices, the proposed classifier can be directly applied to the polarimetric covariance matrices without the need to explicitly compute predefined image features. Furthermore, two distinct optimization strategies are proposed: The first based on pre-selection and grouping of internal binary features before the creation of the classifier; and the second based on iteratively improving the properties of a given Random Fern. Both strategies are able to boost the performance by filtering features that are either redundant or have a low information content and by grouping correlated features to best fulfill the independence assumptions made by the Random Fern classifier. Experiments show that results can be achieved that are similar to a more complex Random Forest model and competitive to a deep learning baseline.



### Scribble-based Boundary-aware Network for Weakly Supervised Salient Object Detection in Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2202.03501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03501v1)
- **Published**: 2022-02-07 20:32:21+00:00
- **Updated**: 2022-02-07 20:32:21+00:00
- **Authors**: Zhou Huang, Tian-Zhu Xiang, Huai-Xin Chen, Hang Dai
- **Comment**: 33 pages, 10 figures
- **Journal**: None
- **Summary**: Existing CNNs-based salient object detection (SOD) heavily depends on the large-scale pixel-level annotations, which is labor-intensive, time-consuming, and expensive. By contrast, the sparse annotations become appealing to the salient object detection community. However, few efforts are devoted to learning salient object detection from sparse annotations, especially in the remote sensing field. In addition, the sparse annotation usually contains scanty information, which makes it challenging to train a well-performing model, resulting in its performance largely lagging behind the fully-supervised models. Although some SOD methods adopt some prior cues to improve the detection performance, they usually lack targeted discrimination of object boundaries and thus provide saliency maps with poor boundary localization. To this end, in this paper, we propose a novel weakly-supervised salient object detection framework to predict the saliency of remote sensing images from sparse scribble annotations. To implement it, we first construct the scribble-based remote sensing saliency dataset by relabelling an existing large-scale SOD dataset with scribbles, namely S-EOR dataset. After that, we present a novel scribble-based boundary-aware network (SBA-Net) for remote sensing salient object detection. Specifically, we design a boundary-aware module (BAM) to explore the object boundary semantics, which is explicitly supervised by the high-confidence object boundary (pseudo) labels generated by the boundary label generation (BLG) module, forcing the model to learn features that highlight the object structure and thus boosting the boundary localization of objects. Then, the boundary semantics are integrated with high-level features to guide the salient object detection under the supervision of scribble labels.



### Integrated Multiscale Domain Adaptive YOLO
- **Arxiv ID**: http://arxiv.org/abs/2202.03527v3
- **DOI**: 10.1109/TIP.2023.3255106
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03527v3)
- **Published**: 2022-02-07 21:30:53+00:00
- **Updated**: 2022-07-04 16:45:30+00:00
- **Authors**: Mazin Hnewa, Hayder Radha
- **Comment**: This paper is a significantly expanded version of our 2021 ICIP paper
  arXiv:2106.01483 and includes new tools and architectures for improving the
  original MS-DAYOLO framework
- **Journal**: None
- **Summary**: The area of domain adaptation has been instrumental in addressing the domain shift problem encountered by many applications. This problem arises due to the difference between the distributions of source data used for training in comparison with target data used during realistic testing scenarios. In this paper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO) framework that employs multiple domain adaptation paths and corresponding domain classifiers at different scales of the recently introduced YOLOv4 object detector. Building on our baseline multiscale DAYOLO framework, we introduce three novel deep learning architectures for a Domain Adaptation Network (DAN) that generates domain-invariant features. In particular, we propose a Progressive Feature Reduction (PFR), a Unified Classifier (UC), and an Integrated architecture. We train and test our proposed DAN architectures in conjunction with YOLOv4 using popular datasets. Our experiments show significant improvements in object detection performance when training YOLOv4 using the proposed MS-DAYOLO architectures and when tested on target data for autonomous driving applications. Moreover, MS-DAYOLO framework achieves an order of magnitude real-time speed improvement relative to Faster R-CNN solutions while providing comparable object detection performance.



### MINER: Multiscale Implicit Neural Representations
- **Arxiv ID**: http://arxiv.org/abs/2202.03532v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03532v2)
- **Published**: 2022-02-07 21:49:33+00:00
- **Updated**: 2022-07-18 00:28:05+00:00
- **Authors**: Vishwanath Saragadam, Jasper Tan, Guha Balakrishnan, Richard G. Baraniuk, Ashok Veeraraghavan
- **Comment**: 14 pages, accepted to ECCV 2022
- **Journal**: None
- **Summary**: We introduce a new neural signal model designed for efficient high-resolution representation of large-scale signals. The key innovation in our multiscale implicit neural representation (MINER) is an internal representation via a Laplacian pyramid, which provides a sparse multiscale decomposition of the signal that captures orthogonal parts of the signal across scales. We leverage the advantages of the Laplacian pyramid by representing small disjoint patches of the pyramid at each scale with a small MLP. This enables the capacity of the network to adaptively increase from coarse to fine scales, and only represent parts of the signal with strong signal energy. The parameters of each MLP are optimized from coarse-to-fine scale which results in faster approximations at coarser scales, thereby ultimately an extremely fast training process. We apply MINER to a range of large-scale signal representation tasks, including gigapixel images and very large point clouds, and demonstrate that it requires fewer than 25% of the parameters, 33% of the memory footprint, and 10% of the computation time of competing techniques such as ACORN to reach the same representation accuracy.



### SliTraNet: Automatic Detection of Slide Transitions in Lecture Videos using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.03540v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03540v1)
- **Published**: 2022-02-07 22:03:27+00:00
- **Updated**: 2022-02-07 22:03:27+00:00
- **Authors**: Aline Sindel, Abner Hernandez, Seung Hee Yang, Vincent Christlein, Andreas Maier
- **Comment**: 6 pages, 5 figures, 1 table, accepted to OAGM Workshop 2021
- **Journal**: None
- **Summary**: With the increasing number of online learning material in the web, search for specific content in lecture videos can be time consuming. Therefore, automatic slide extraction from the lecture videos can be helpful to give a brief overview of the main content and to support the students in their studies. For this task, we propose a deep learning method to detect slide transitions in lectures videos. We first process each frame of the video by a heuristic-based approach using a 2-D convolutional neural network to predict transition candidates. Then, we increase the complexity by employing two 3-D convolutional neural networks to refine the transition candidates. Evaluation results demonstrate the effectiveness of our method in finding slide transitions.



### Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling
- **Arxiv ID**: http://arxiv.org/abs/2202.03543v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CL, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2202.03543v2)
- **Published**: 2022-02-07 22:09:54+00:00
- **Updated**: 2022-03-02 18:02:11+00:00
- **Authors**: Puyuan Peng, David Harwath
- **Comment**: SAS workshop at AAAI2022, code and model weights available at
  https://github.com/jasonppy/FaST-VGS-Family
- **Journal**: None
- **Summary**: In this paper, we describe our submissions to the ZeroSpeech 2021 Challenge and SUPERB benchmark. Our submissions are based on the recently proposed FaST-VGS model, which is a Transformer-based model that learns to associate raw speech waveforms with semantically related images, all without the use of any transcriptions of the speech. Additionally, we introduce a novel extension of this model, FaST-VGS+, which is learned in a multi-task fashion with a masked language modeling objective in addition to the visual grounding objective. On ZeroSpeech 2021, we show that our models perform competitively on the ABX task, outperform all other concurrent submissions on the Syntactic and Semantic tasks, and nearly match the best system on the Lexical task. On the SUPERB benchmark, we show that our models also achieve strong performance, in some cases even outperforming the popular wav2vec2.0 model.



### LwPosr: Lightweight Efficient Fine-Grained Head Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.03544v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03544v1)
- **Published**: 2022-02-07 22:12:27+00:00
- **Updated**: 2022-02-07 22:12:27+00:00
- **Authors**: Naina Dhingra
- **Comment**: 11 pages, 3 figures
- **Journal**: In Proceedings of the IEEE/CVF Winter Conference on Applications
  of Computer Vision (pp. 1495-1505), 2022
- **Summary**: This paper presents a lightweight network for head pose estimation (HPE) task. While previous approaches rely on convolutional neural networks, the proposed network \textit{LwPosr} uses mixture of depthwise separable convolutional (DSC) and transformer encoder layers which are structured in two streams and three stages to provide fine-grained regression for predicting head poses. The quantitative and qualitative demonstration is provided to show that the proposed network is able to learn head poses efficiently while using less parameter space. Extensive ablations are conducted using three open-source datasets namely 300W-LP, AFLW2000, and BIWI datasets. To our knowledge, (1) \textit{LwPosr} is the lightest network proposed for estimating head poses compared to both keypoints-based and keypoints-free approaches; (2) it sets a benchmark for both overperforming the previous lightweight network on mean absolute error and on reducing number of parameters; (3) it is first of its kind to use mixture of DSCs and transformer encoders for HPE. This approach is suitable for mobile devices which require lightweight networks.



### HeadPosr: End-to-end Trainable Head Pose Estimation using Transformer Encoders
- **Arxiv ID**: http://arxiv.org/abs/2202.03548v1
- **DOI**: 10.1109/FG52635.2021.9667080
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03548v1)
- **Published**: 2022-02-07 22:29:21+00:00
- **Updated**: 2022-02-07 22:29:21+00:00
- **Authors**: Naina Dhingra
- **Comment**: 8 pages, 4 figures
- **Journal**: In 2021 16th IEEE International Conference on Automatic Face and
  Gesture Recognition (FG 2021) (pp. 1-8)
- **Summary**: In this paper, HeadPosr is proposed to predict the head poses using a single RGB image. \textit{HeadPosr} uses a novel architecture which includes a transformer encoder. In concrete, it consists of: (1) backbone; (2) connector; (3) transformer encoder; (4) prediction head. The significance of using a transformer encoder for HPE is studied. An extensive ablation study is performed on varying the (1) number of encoders; (2) number of heads; (3) different position embeddings; (4) different activations; (5) input channel size, in a transformer used in HeadPosr. Further studies on using: (1) different backbones, (2) using different learning rates are also shown. The elaborated experiments and ablations studies are conducted using three different open-source widely used datasets for HPE, i.e., 300W-LP, AFLW2000, and BIWI datasets. Experiments illustrate that \textit{HeadPosr} outperforms all the state-of-art methods including both the landmark-free and the others based on using landmark or depth estimation on the AFLW2000 dataset and BIWI datasets when trained with 300W-LP. It also outperforms when averaging the results from the compared datasets, hence setting a benchmark for the problem of HPE, also demonstrating the effectiveness of using transformers over the state-of-the-art.



### Aladdin: Joint Atlas Building and Diffeomorphic Registration Learning with Pairwise Alignment
- **Arxiv ID**: http://arxiv.org/abs/2202.03563v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03563v2)
- **Published**: 2022-02-07 23:52:21+00:00
- **Updated**: 2022-03-31 18:38:25+00:00
- **Authors**: Zhipeng Ding, Marc Niethammer
- **Comment**: Accepted by CVPR-2022
- **Journal**: None
- **Summary**: Atlas building and image registration are important tasks for medical image analysis. Once one or multiple atlases from an image population have been constructed, commonly (1) images are warped into an atlas space to study intra-subject or inter-subject variations or (2) a possibly probabilistic atlas is warped into image space to assign anatomical labels. Atlas estimation and nonparametric transformations are computationally expensive as they usually require numerical optimization. Additionally, previous approaches for atlas building often define similarity measures between a fuzzy atlas and each individual image, which may cause alignment difficulties because a fuzzy atlas does not exhibit clear anatomical structures in contrast to the individual images. This work explores using a convolutional neural network (CNN) to jointly predict the atlas and a stationary velocity field (SVF) parameterization for diffeomorphic image registration with respect to the atlas. Our approach does not require affine pre-registrations and utilizes pairwise image alignment losses to increase registration accuracy. We evaluate our model on 3D knee magnetic resonance images (MRI) from the OAI-ZIB dataset. Our results show that the proposed framework achieves better performance than other state-of-the-art image registration algorithms, allows for end-to-end training, and for fast inference at test time.



### DeepSSN: a deep convolutional neural network to assess spatial scene similarity
- **Arxiv ID**: http://arxiv.org/abs/2202.04755v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10; I.4.8; H.3.3; H.4
- **Links**: [PDF](http://arxiv.org/pdf/2202.04755v1)
- **Published**: 2022-02-07 23:53:20+00:00
- **Updated**: 2022-02-07 23:53:20+00:00
- **Authors**: Danhuai Guo, Shiyin Ge, Shu Zhang, Song Gao, Ran Tao, Yangang Wang
- **Comment**: 28 pages, 10 figures, 8 tables
- **Journal**: Transactions in GIS, 2022
- **Summary**: Spatial-query-by-sketch is an intuitive tool to explore human spatial knowledge about geographic environments and to support communication with scene database queries. However, traditional sketch-based spatial search methods perform insufficiently due to their inability to find hidden multi-scale map features from mental sketches. In this research, we propose a deep convolutional neural network, namely Deep Spatial Scene Network (DeepSSN), to better assess the spatial scene similarity. In DeepSSN, a triplet loss function is designed as a comprehensive distance metric to support the similarity assessment. A positive and negative example mining strategy using qualitative constraint networks in spatial reasoning is designed to ensure a consistently increasing distinction of triplets during the training process. Moreover, we develop a prototype spatial scene search system using the proposed DeepSSN, in which the users input spatial query via sketch maps and the system can automatically augment the sketch training data. The proposed model is validated using multi-source conflated map data including 131,300 labeled scene samples after data augmentation. The empirical results demonstrate that the DeepSSN outperforms baseline methods including k-nearest-neighbors, multilayer perceptron, AlexNet, DenseNet, and ResNet using mean reciprocal rank and precision metrics. This research advances geographic information retrieval studies by introducing a novel deep learning method tailored to spatial scene queries.



### Accurate super-resolution low-field brain MRI
- **Arxiv ID**: http://arxiv.org/abs/2202.03564v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03564v1)
- **Published**: 2022-02-07 23:57:28+00:00
- **Updated**: 2022-02-07 23:57:28+00:00
- **Authors**: Juan Eugenio Iglesias, Riana Schleicher, Sonia Laguna, Benjamin Billot, Pamela Schaefer, Brenna McKaig, Joshua N. Goldstein, Kevin N. Sheth, Matthew S. Rosen, W. Taylor Kimberly
- **Comment**: None
- **Journal**: None
- **Summary**: The recent introduction of portable, low-field MRI (LF-MRI) into the clinical setting has the potential to transform neuroimaging. However, LF-MRI is limited by lower resolution and signal-to-noise ratio, leading to incomplete characterization of brain regions. To address this challenge, recent advances in machine learning facilitate the synthesis of higher resolution images derived from one or multiple lower resolution scans. Here, we report the extension of a machine learning super-resolution (SR) algorithm to synthesize 1 mm isotropic MPRAGE-like scans from LF-MRI T1-weighted and T2-weighted sequences. Our initial results on a paired dataset of LF and high-field (HF, 1.5T-3T) clinical scans show that: (i) application of available automated segmentation tools directly to LF-MRI images falters; but (ii) segmentation tools succeed when applied to SR images with high correlation to gold standard measurements from HF-MRI (e.g., r = 0.85 for hippocampal volume, r = 0.84 for the thalamus, r = 0.92 for the whole cerebrum). This work demonstrates proof-of-principle post-processing image enhancement from lower resolution LF-MRI sequences. These results lay the foundation for future work to enhance the detection of normal and abnormal image findings at LF and ultimately improve the diagnostic performance of LF-MRI. Our tools are publicly available on FreeSurfer (surfer.nmr.mgh.harvard.edu/).



