# Arxiv Papers in cs.CV on 2022-02-21
### Generative Target Update for Adaptive Siamese Tracking
- **Arxiv ID**: http://arxiv.org/abs/2202.09938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09938v1)
- **Published**: 2022-02-21 00:22:49+00:00
- **Updated**: 2022-02-21 00:22:49+00:00
- **Authors**: Madhu Kiran, Le Thanh Nguyen-Meidine, Rajat Sahay, Rafael Menelau Oliveira E Cruz, Louis-Antoine Blais-Morin, Eric Granger
- **Comment**: None
- **Journal**: None
- **Summary**: Siamese trackers perform similarity matching with templates (i.e., target models) to recursively localize objects within a search region. Several strategies have been proposed in the literature to update a template based on the tracker output, typically extracted from the target search region in the current frame, and thereby mitigate the effects of target drift. However, this may lead to corrupted templates, limiting the potential benefits of a template update strategy.   This paper proposes a model adaptation method for Siamese trackers that uses a generative model to produce a synthetic template from the object search regions of several previous frames, rather than directly using the tracker output. Since the search region encompasses the target, attention from the search region is used for robust model adaptation. In particular, our approach relies on an auto-encoder trained through adversarial learning to detect changes in a target object's appearance and predict a future target template, using a set of target templates localized from tracker outputs at previous frames. To prevent template corruption during the update, the proposed tracker also performs change detection using the generative model to suspend updates until the tracker stabilizes, and robust matching can resume through dynamic template fusion.   Extensive experiments conducted on VOT-16, VOT-17, OTB-50, and OTB-100 datasets highlight the effectiveness of our method, along with the impact of its key components. Results indicate that our proposed approach can outperform state-of-art trackers, and its overall robustness allows tracking for a longer time before failure.



### Multiscale Crowd Counting and Localization By Multitask Point Supervision
- **Arxiv ID**: http://arxiv.org/abs/2202.09942v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.09942v1)
- **Published**: 2022-02-21 01:22:34+00:00
- **Updated**: 2022-02-21 01:22:34+00:00
- **Authors**: Mohsen Zand, Haleh Damirchi, Andrew Farley, Mahdiyar Molahasani, Michael Greenspan, Ali Etemad
- **Comment**: 4 pages + references, 3 figures, 2 tables, Accepted by ICASSP 2022
  Conference
- **Journal**: None
- **Summary**: We propose a multitask approach for crowd counting and person localization in a unified framework. As the detection and localization tasks are well-correlated and can be jointly tackled, our model benefits from a multitask solution by learning multiscale representations of encoded crowd images, and subsequently fusing them. In contrast to the relatively more popular density-based methods, our model uses point supervision to allow for crowd locations to be accurately identified. We test our model on two popular crowd counting datasets, ShanghaiTech A and B, and demonstrate that our method achieves strong results on both counting and localization tasks, with MSE measures of 110.7 and 15.0 for crowd counting and AP measures of 0.71 and 0.75 for localization, on ShanghaiTech A and B respectively. Our detailed ablation experiments show the impact of our multiscale approach as well as the effectiveness of the fusion module embedded in our network. Our code is available at: https://github.com/RCVLab-AiimLab/crowd_counting.



### LiDAR-guided Stereo Matching with a Spatial Consistency Constraint
- **Arxiv ID**: http://arxiv.org/abs/2202.09953v2
- **DOI**: 10.1016/j.isprsjprs.2021.11.003
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09953v2)
- **Published**: 2022-02-21 02:29:19+00:00
- **Updated**: 2022-02-24 13:00:26+00:00
- **Authors**: Yongjun Zhang, Siyuan Zou, Xinyi Liu, Xu Huang, Yi Wan, Yongxiang Yao
- **Comment**: we replace an article because of the addition of journal reference,
  DOI, and report number information
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing Volume
  183(2021) 164-177
- **Summary**: The complementary fusion of light detection and ranging (LiDAR) data and image data is a promising but challenging task for generating high-precision and high-density point clouds. This study proposes an innovative LiDAR-guided stereo matching approach called LiDAR-guided stereo matching (LGSM), which considers the spatial consistency represented by continuous disparity or depth changes in the homogeneous region of an image. The LGSM first detects the homogeneous pixels of each LiDAR projection point based on their color or intensity similarity. Next, we propose a riverbed enhancement function to optimize the cost volume of the LiDAR projection points and their homogeneous pixels to improve the matching robustness. Our formulation expands the constraint scopes of sparse LiDAR projection points with the guidance of image information to optimize the cost volume of pixels as much as possible. We applied LGSM to semi-global matching and AD-Census on both simulated and real datasets. When the percentage of LiDAR points in the simulated datasets was 0.16%, the matching accuracy of our method achieved a subpixel level, while that of the original stereo matching algorithm was 3.4 pixels. The experimental results show that LGSM is suitable for indoor, street, aerial, and satellite image datasets and provides good transferability across semi-global matching and AD-Census. Furthermore, the qualitative and quantitative evaluations demonstrate that LGSM is superior to two state-of-the-art optimizing cost volume methods, especially in reducing mismatches in difficult matching areas and refining the boundaries of objects.



### Deep Feature based Cross-slide Registration
- **Arxiv ID**: http://arxiv.org/abs/2202.09971v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09971v5)
- **Published**: 2022-02-21 03:25:12+00:00
- **Updated**: 2022-04-26 03:16:54+00:00
- **Authors**: Ruqayya Awan, Shan E Ahmed Raza, Johannes Lotz, Nick Weiss, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-slide image analysis provides additional information by analysing the expression of different biomarkers as compared to a single slide analysis. These biomarker stained slides are analysed side by side, revealing unknown relations between them. During the slide preparation, a tissue section may be placed at an arbitrary orientation as compared to other sections of the same tissue block. The problem is compounded by the fact that tissue contents are likely to change from one section to the next and there may be unique artefacts on some of the slides. This makes registration of each section to a reference section of the same tissue block an important pre-requisite task before any cross-slide analysis. We propose a deep feature based registration (DFBR) method which utilises data-driven features to estimate the rigid transformation. We adopted a multi-stage strategy for improving the quality of registration. We also developed a visualisation tool to view registered pairs of WSIs at different magnifications. With the help of this tool, one can apply a transformation on the fly without the need to generate transformed source WSI in a pyramidal form. We compared the performance of data-driven features with that of hand-crafted features on the COMET dataset. Our approach can align the images with low registration errors. Generally, the success of non-rigid registration is dependent on the quality of rigid registration. To evaluate the efficacy of the DFBR method, the first two steps of the ANHIR winner's framework are replaced with our DFBR to register challenge provided image pairs. The modified framework produces comparable results to that of challenge winning team.



### Feasibility Study of Multi-Site Split Learning for Privacy-Preserving Medical Systems under Data Imbalance Constraints in COVID-19, X-Ray, and Cholesterol Dataset
- **Arxiv ID**: http://arxiv.org/abs/2202.10456v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10456v1)
- **Published**: 2022-02-21 03:51:27+00:00
- **Updated**: 2022-02-21 03:51:27+00:00
- **Authors**: Yoo Jeong Ha, Gusang Lee, Minjae Yoo, Soyi Jung, Seehwan Yoo, Joongheon Kim
- **Comment**: None
- **Journal**: None
- **Summary**: It seems as though progressively more people are in the race to upload content, data, and information online; and hospitals haven't neglected this trend either. Hospitals are now at the forefront for multi-site medical data sharing to provide groundbreaking advancements in the way health records are shared and patients are diagnosed. Sharing of medical data is essential in modern medical research. Yet, as with all data sharing technology, the challenge is to balance improved treatment with protecting patient's personal information. This paper provides a novel split learning algorithm coined the term, "multi-site split learning", which enables a secure transfer of medical data between multiple hospitals without fear of exposing personal data contained in patient records. It also explores the effects of varying the number of end-systems and the ratio of data-imbalance on the deep learning performance. A guideline for the most optimal configuration of split learning that ensures privacy of patient data whilst achieving performance is empirically given. We argue the benefits of our multi-site split learning algorithm, especially regarding the privacy preserving factor, using CT scans of COVID-19 patients, X-ray bone scans, and cholesterol level medical data.



### Audio Visual Scene-Aware Dialog Generation with Transformer-based Video Representations
- **Arxiv ID**: http://arxiv.org/abs/2202.09979v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09979v1)
- **Published**: 2022-02-21 04:09:32+00:00
- **Updated**: 2022-02-21 04:09:32+00:00
- **Authors**: Yoshihiro Yamazaki, Shota Orihashi, Ryo Masumura, Mihiro Uchida, Akihiko Takashima
- **Comment**: Accepted at DSTC10 Workshop at AAAI 2022
- **Journal**: None
- **Summary**: There have been many attempts to build multimodal dialog systems that can respond to a question about given audio-visual information, and the representative task for such systems is the Audio Visual Scene-Aware Dialog (AVSD). Most conventional AVSD models adopt the Convolutional Neural Network (CNN)-based video feature extractor to understand visual information. While a CNN tends to obtain both temporally and spatially local information, global information is also crucial for boosting video understanding because AVSD requires long-term temporal visual dependency and whole visual information. In this study, we apply the Transformer-based video feature that can capture both temporally and spatially global representations more efficiently than the CNN-based feature. Our AVSD model with its Transformer-based feature attains higher objective performance scores for answer generation. In addition, our model achieves a subjective score close to that of human answers in DSTC10. We observed that the Transformer-based visual feature is beneficial for the AVSD task because our model tends to correctly answer the questions that need a temporally and spatially broad range of visual information.



### Don't Touch What Matters: Task-Aware Lipschitz Data Augmentation for Visual Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.09982v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.09982v2)
- **Published**: 2022-02-21 04:22:07+00:00
- **Updated**: 2022-02-22 15:04:35+00:00
- **Authors**: Zhecheng Yuan, Guozheng Ma, Yao Mu, Bo Xia, Bo Yuan, Xueqian Wang, Ping Luo, Huazhe Xu
- **Comment**: None
- **Journal**: None
- **Summary**: One of the key challenges in visual Reinforcement Learning (RL) is to learn policies that can generalize to unseen environments. Recently, data augmentation techniques aiming at enhancing data diversity have demonstrated proven performance in improving the generalization ability of learned policies. However, due to the sensitivity of RL training, naively applying data augmentation, which transforms each pixel in a task-agnostic manner, may suffer from instability and damage the sample efficiency, thus further exacerbating the generalization performance. At the heart of this phenomenon is the diverged action distribution and high-variance value estimation in the face of augmented images. To alleviate this issue, we propose Task-aware Lipschitz Data Augmentation (TLDA) for visual RL, which explicitly identifies the task-correlated pixels with large Lipschitz constants, and only augments the task-irrelevant pixels. To verify the effectiveness of TLDA, we conduct extensive experiments on DeepMind Control suite, CARLA and DeepMind Manipulation tasks, showing that TLDA improves both sample efficiency in training time and generalization in test time. It outperforms previous state-of-the-art methods across the 3 different visual control benchmarks.



### Outlier-based Autism Detection using Longitudinal Structural MRI
- **Arxiv ID**: http://arxiv.org/abs/2202.09988v2
- **DOI**: 10.1109/ACCESS.2022.3157613
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.09988v2)
- **Published**: 2022-02-21 04:37:25+00:00
- **Updated**: 2022-03-10 10:33:18+00:00
- **Authors**: Devika K, Venkata Ramana Murthy Oruganti, Dwarikanath Mahapatra, Ramanathan Subramanian
- **Comment**: None
- **Journal**: None
- **Summary**: Diagnosis of Autism Spectrum Disorder (ASD) using clinical evaluation (cognitive tests) is challenging due to wide variations amongst individuals. Since no effective treatment exists, prompt and reliable ASD diagnosis can enable the effective preparation of treatment regimens. This paper proposes structural Magnetic Resonance Imaging (sMRI)-based ASD diagnosis via an outlier detection approach. To learn Spatio-temporal patterns in structural brain connectivity, a Generative Adversarial Network (GAN) is trained exclusively with sMRI scans of healthy subjects. Given a stack of three adjacent slices as input, the GAN generator reconstructs the next three adjacent slices; the GAN discriminator then identifies ASD sMRI scan reconstructions as outliers. This model is compared against two other baselines -- a simpler UNet and a sophisticated Self-Attention GAN. Axial, Coronal, and Sagittal sMRI slices from the multi-site ABIDE II dataset are used for evaluation. Extensive experiments reveal that our ASD detection framework performs comparably with the state-of-the-art with far fewer training data. Furthermore, longitudinal data (two scans per subject over time) achieve 17-28% higher accuracy than cross-sectional data (one scan per subject). Among other findings, metrics employed for model training as well as reconstruction loss computation impact detection performance, and the coronal modality is found to best encode structural information for ASD detection.



### Transferring Adversarial Robustness Through Robust Representation Matching
- **Arxiv ID**: http://arxiv.org/abs/2202.09994v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09994v2)
- **Published**: 2022-02-21 05:15:40+00:00
- **Updated**: 2022-05-05 22:49:41+00:00
- **Authors**: Pratik Vaishnavi, Kevin Eykholt, Amir Rahmati
- **Comment**: To appear at USENIX Security '22. Updated version with artifact
  evaluation badges and appendix
- **Journal**: None
- **Summary**: With the widespread use of machine learning, concerns over its security and reliability have become prevalent. As such, many have developed defenses to harden neural networks against adversarial examples, imperceptibly perturbed inputs that are reliably misclassified. Adversarial training in which adversarial examples are generated and used during training is one of the few known defenses able to reliably withstand such attacks against neural networks. However, adversarial training imposes a significant training overhead and scales poorly with model complexity and input dimension. In this paper, we propose Robust Representation Matching (RRM), a low-cost method to transfer the robustness of an adversarially trained model to a new model being trained for the same task irrespective of architectural differences. Inspired by student-teacher learning, our method introduces a novel training loss that encourages the student to learn the teacher's robust representations. Compared to prior works, RRM is superior with respect to both model performance and adversarial training time. On CIFAR-10, RRM trains a robust model $\sim 1.8\times$ faster than the state-of-the-art. Furthermore, RRM remains effective on higher-dimensional datasets. On Restricted-ImageNet, RRM trains a ResNet50 model $\sim 18\times$ faster than standard adversarial training.



### Domain-Augmented Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2202.10000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2202.10000v1)
- **Published**: 2022-02-21 05:42:02+00:00
- **Updated**: 2022-02-21 05:42:02+00:00
- **Authors**: Qiuhao Zeng, Tianze Luo, Boyu Wang
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) enables knowledge transfer from the labelled source domain to the unlabeled target domain by reducing the cross-domain discrepancy. However, most of the studies were based on direct adaptation from the source domain to the target domain and have suffered from large domain discrepancies. To overcome this challenge, in this paper, we propose the domain-augmented domain adaptation (DADA) to generate pseudo domains that have smaller discrepancies with the target domain, to enhance the knowledge transfer process by minimizing the discrepancy between the target domain and pseudo domains. Furthermore, we design a pseudo-labeling method for DADA by projecting representations from the target domain to multiple pseudo domains and taking the averaged predictions on the classification from the pseudo domains as the pseudo labels. We conduct extensive experiments with the state-of-the-art domain adaptation methods on four benchmark datasets: Office Home, Office-31, VisDA2017, and Digital datasets. The results demonstrate the superiority of our model.



### DGAFF: Deep Genetic Algorithm Fitness Formation for EEG Bio-Signal Channel Selection
- **Arxiv ID**: http://arxiv.org/abs/2202.10034v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10034v1)
- **Published**: 2022-02-21 08:06:17+00:00
- **Updated**: 2022-02-21 08:06:17+00:00
- **Authors**: Ghazaleh Ghorbanzadeh, Zahra Nabizadeh, Nader Karimi, Pejman Khadivi, Ali Emami, Shadrokh Samavi
- **Comment**: 15 pages, 4 figures
- **Journal**: None
- **Summary**: Brain-computer interface systems aim to facilitate human-computer interactions in a great deal by direct translation of brain signals for computers. Recently, using many electrodes has caused better performance in these systems. However, increasing the number of recorded electrodes leads to additional time, hardware, and computational costs besides undesired complications of the recording process. Channel selection has been utilized to decrease data dimension and eliminate irrelevant channels while reducing the noise effects. Furthermore, the technique lowers the time and computational costs in real-time applications. We present a channel selection method, which combines a sequential search method with a genetic algorithm called Deep GA Fitness Formation (DGAFF). The proposed method accelerates the convergence of the genetic algorithm and increases the system's performance. The system evaluation is based on a lightweight deep neural network that automates the whole model training process. The proposed method outperforms other channel selection methods in classifying motor imagery on the utilized dataset.



### PCSCNet: Fast 3D Semantic Segmentation of LiDAR Point Cloud for Autonomous Car using Point Convolution and Sparse Convolution Network
- **Arxiv ID**: http://arxiv.org/abs/2202.10047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10047v1)
- **Published**: 2022-02-21 08:31:37+00:00
- **Updated**: 2022-02-21 08:31:37+00:00
- **Authors**: Jaehyun Park, Chansoo Kim, Kichun Jo
- **Comment**: None
- **Journal**: None
- **Summary**: The autonomous car must recognize the driving environment quickly for safe driving. As the Light Detection And Range (LiDAR) sensor is widely used in the autonomous car, fast semantic segmentation of LiDAR point cloud, which is the point-wise classification of the point cloud within the sensor framerate, has attracted attention in recognition of the driving environment. Although the voxel and fusion-based semantic segmentation models are the state-of-the-art model in point cloud semantic segmentation recently, their real-time performance suffer from high computational load due to high voxel resolution. In this paper, we propose the fast voxel-based semantic segmentation model using Point Convolution and 3D Sparse Convolution (PCSCNet). The proposed model is designed to outperform at both high and low voxel resolution using point convolution-based feature extraction. Moreover, the proposed model accelerates the feature propagation using 3D sparse convolution after the feature extraction. The experimental results demonstrate that the proposed model outperforms the state-of-the-art real-time models in semantic segmentation of SemanticKITTI and nuScenes, and achieves the real-time performance in LiDAR point cloud inference.



### Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution
- **Arxiv ID**: http://arxiv.org/abs/2202.10054v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10054v1)
- **Published**: 2022-02-21 09:03:34+00:00
- **Updated**: 2022-02-21 09:03:34+00:00
- **Authors**: Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, Percy Liang
- **Comment**: ICLR (Oral) 2022
- **Journal**: None
- **Summary**: When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer -- the "head"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR $\to$ STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head -- this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning).



### Point Cloud Denoising via Momentum Ascent in Gradient Fields
- **Arxiv ID**: http://arxiv.org/abs/2202.10094v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10094v3)
- **Published**: 2022-02-21 10:21:40+00:00
- **Updated**: 2023-06-25 05:27:57+00:00
- **Authors**: Yaping Zhao, Haitian Zheng, Zhongrui Wang, Jiebo Luo, Edmund Y. Lam
- **Comment**: 5 pages, 6 figures
- **Journal**: None
- **Summary**: To achieve point cloud denoising, traditional methods heavily rely on geometric priors, and most learning-based approaches suffer from outliers and loss of details. Recently, the gradient-based method was proposed to estimate the gradient fields from the noisy point clouds using neural networks, and refine the position of each point according to the estimated gradient. However, the predicted gradient could fluctuate, leading to perturbed and unstable solutions, as well as a long inference time. To address these issues, we develop the momentum gradient ascent method that leverages the information of previous iterations in determining the trajectories of the points, thus improving the stability of the solution and reducing the inference time. Experiments demonstrate that the proposed method outperforms state-of-the-art approaches with a variety of point clouds, noise types, and noise levels. Code is available at: https://github.com/IndigoPurple/MAG



### Simplified Learning of CAD Features Leveraging a Deep Residual Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2202.10099v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10099v1)
- **Published**: 2022-02-21 10:27:55+00:00
- **Updated**: 2022-02-21 10:27:55+00:00
- **Authors**: Raoul Schönhof, Jannes Elstner, Radu Manea, Steffen Tauber, Ramez Awad, Marco F. Huber
- **Comment**: Accepted/Peer-Revied Articel
- **Journal**: None
- **Summary**: In the domain of computer vision, deep residual neural networks like EfficientNet have set new standards in terms of robustness and accuracy. One key problem underlying the training of deep neural networks is the immanent lack of a sufficient amount of training data. The problem worsens especially if labels cannot be generated automatically, but have to be annotated manually. This challenge occurs for instance if expert knowledge related to 3D parts should be externalized based on example models. One way to reduce the necessary amount of labeled data may be the use of autoencoders, which can be learned in an unsupervised fashion without labeled data. In this work, we present a deep residual 3D autoencoder based on the EfficientNet architecture, intended for transfer learning tasks related to 3D CAD model assessment. For this purpose, we adopted EfficientNet to 3D problems like voxel models derived from a STEP file. Striving to reduce the amount of labeled 3D data required, the networks encoder can be utilized for transfer training.



### ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2202.10108v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10108v2)
- **Published**: 2022-02-21 10:40:05+00:00
- **Updated**: 2022-11-27 05:17:34+00:00
- **Authors**: Qiming Zhang, Yufei Xu, Jing Zhang, Dacheng Tao
- **Comment**: An extended version of the Neurips paper "ViTAE: Vision Transformer
  Advanced by Exploring Intrinsic Inductive Bias". arXiv admin note:
  substantial text overlap with arXiv:2106.03348
- **Journal**: None
- **Summary**: Vision transformers have shown great potential in various computer vision tasks owing to their strong capability to model long-range dependency using the self-attention mechanism. Nevertheless, they treat an image as a 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance, which is instead learned implicitly from large-scale training data with longer training schedules. In this paper, we propose a Vision Transformer Advanced by Exploring intrinsic IB from convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and can learn robust feature representation for objects at various scales. Moreover, in each transformer layer, ViTAE has a convolution block parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network. Consequently, it has the intrinsic locality IB and is able to learn local features and global dependencies collaboratively. The proposed two kinds of cells are stacked in both isotropic and multi-stage manners to formulate two families of ViTAE models, i.e., the vanilla ViTAE and ViTAEv2. Experiments on the ImageNet dataset as well as downstream tasks on the MS COCO, ADE20K, and AP10K datasets validate the superiority of our models over the baseline transformer models and concurrent works. Besides, we scale up our ViTAE model to 644M parameters and obtain the state-of-the-art classification performance, i.e., 88.5% Top-1 classification accuracy on ImageNet validation set and the best 91.2% Top-1 accuracy on ImageNet real validation set, without using extra private data.



### An Efficient Smoothing and Thresholding Image Segmentation Framework with Weighted Anisotropic-Isotropic Total Variation
- **Arxiv ID**: http://arxiv.org/abs/2202.10115v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10115v3)
- **Published**: 2022-02-21 10:57:16+00:00
- **Updated**: 2022-10-13 21:28:12+00:00
- **Authors**: Kevin Bui, Yifei Lou, Fredrick Park, Jack Xin
- **Comment**: expanded numerical experiments
- **Journal**: None
- **Summary**: In this paper, we design an efficient, multi-stage image segmentation framework that incorporates a weighted difference of anisotropic and isotropic total variation (AITV). The segmentation framework generally consists of two stages: smoothing and thresholding, thus referred to as SaT. In the first stage, a smoothed image is obtained by an AITV-regularized Mumford-Shah (MS) model, which can be solved efficiently by the alternating direction method of multipliers (ADMM) with a closed-form solution of a proximal operator of the $\ell_1 -\alpha \ell_2$ regularizer. Convergence of the ADMM algorithm is analyzed. In the second stage, we threshold the smoothed image by $k$-means clustering to obtain the final segmentation result. Numerical experiments demonstrate that the proposed segmentation framework is versatile for both grayscale and color images, efficient in producing high-quality segmentation results within a few seconds, and robust to input images that are corrupted with noise, blur, or both. We compare the AITV method with its original convex and nonconvex TV$^p (0<p<1)$ counterparts, showcasing the qualitative and quantitative advantages of our proposed method.



### Multi-Task Conditional Imitation Learning for Autonomous Navigation at Crowded Intersections
- **Arxiv ID**: http://arxiv.org/abs/2202.10124v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10124v1)
- **Published**: 2022-02-21 11:13:59+00:00
- **Updated**: 2022-02-21 11:13:59+00:00
- **Authors**: Zeyu Zhu, Huijing Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, great efforts have been devoted to deep imitation learning for autonomous driving control, where raw sensory inputs are directly mapped to control actions. However, navigating through densely populated intersections remains a challenging task due to uncertainty caused by uncertain traffic participants. We focus on autonomous navigation at crowded intersections that require interaction with pedestrians. A multi-task conditional imitation learning framework is proposed to adapt both lateral and longitudinal control tasks for safe and efficient interaction. A new benchmark called IntersectNav is developed and human demonstrations are provided. Empirical results show that the proposed method can achieve a success rate gain of up to 30% compared to the state-of-the-art.



### Synthetic CT Skull Generation for Transcranial MR Imaging-Guided Focused Ultrasound Interventions with Conditional Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.10136v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10136v2)
- **Published**: 2022-02-21 11:34:29+00:00
- **Updated**: 2022-02-22 17:38:49+00:00
- **Authors**: Han Liu, Michelle K. Sigona, Thomas J. Manuel, Li Min Chen, Charles F. Caskey, Benoit M. Dawant
- **Comment**: Accepted by SPIE Medical Imaging 2022
- **Journal**: None
- **Summary**: Transcranial MRI-guided focused ultrasound (TcMRgFUS) is a therapeutic ultrasound method that focuses sound through the skull to a small region noninvasively under MRI guidance. It is clinically approved to thermally ablate regions of the thalamus and is being explored for other therapies, such as blood brain barrier opening and neuromodulation. To accurately target ultrasound through the skull, the transmitted waves must constructively interfere at the target region. However, heterogeneity of the sound speed, density, and ultrasound attenuation in different individuals' skulls requires patient-specific estimates of these parameters for optimal treatment planning. CT imaging is currently the gold standard for estimating acoustic properties of an individual skull during clinical procedures, but CT imaging exposes patients to radiation and increases the overall number of imaging procedures required for therapy. A method to estimate acoustic parameters in the skull without the need for CT would be desirable. Here, we synthesized CT images from routinely acquired T1-weighted MRI by using a 3D patch-based conditional generative adversarial network and evaluated the performance of synthesized CT images for treatment planning with transcranial focused ultrasound. We compared the performance of synthetic CT to real CT images using Kranion and k-Wave acoustic simulation. Our work demonstrates the feasibility of replacing real CT with the MR-synthesized CT for TcMRgFUS planning.



### Diffusion Causal Models for Counterfactual Estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.10166v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10166v1)
- **Published**: 2022-02-21 12:23:01+00:00
- **Updated**: 2022-02-21 12:23:01+00:00
- **Authors**: Pedro Sanchez, Sotirios A. Tsaftaris
- **Comment**: Accepted at CLeaR (Causal Learning and Reasoning) 2022
- **Journal**: None
- **Summary**: We consider the task of counterfactual estimation from observational imaging data given a known causal structure. In particular, quantifying the causal effect of interventions for high-dimensional data with neural networks remains an open challenge. Herein we propose Diff-SCM, a deep structural causal model that builds on recent advances of generative energy-based models. In our setting, inference is performed by iteratively sampling gradients of the marginal and conditional distributions entailed by the causal model. Counterfactual estimation is achieved by firstly inferring latent variables with deterministic forward diffusion, then intervening on a reverse diffusion process using the gradients of an anti-causal predictor w.r.t the input. Furthermore, we propose a metric for evaluating the generated counterfactuals. We find that Diff-SCM produces more realistic and minimal counterfactuals than baselines on MNIST data and can also be applied to ImageNet data. Code is available https://github.com/vios-s/Diff-SCM.



### Cell nuclei classification in histopathological images using hybrid OLConvNet
- **Arxiv ID**: http://arxiv.org/abs/2202.10177v1
- **DOI**: 10.1145/3345318
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10177v1)
- **Published**: 2022-02-21 12:39:37+00:00
- **Updated**: 2022-02-21 12:39:37+00:00
- **Authors**: Suvidha Tripathi, Satish Kumar Singh
- **Comment**: None
- **Journal**: @article{10.1145/3345318, year = {2020},journal = {ACM Trans.
  Multimedia Comput. Commun. Appl.}, volume = {16}, number = {1s}, issn =
  {1551-6857}, articleno = {32}, numpages = {22}}
- **Summary**: Computer-aided histopathological image analysis for cancer detection is a major research challenge in the medical domain. Automatic detection and classification of nuclei for cancer diagnosis impose a lot of challenges in developing state of the art algorithms due to the heterogeneity of cell nuclei and data set variability. Recently, a multitude of classification algorithms has used complex deep learning models for their dataset. However, most of these methods are rigid and their architectural arrangement suffers from inflexibility and non-interpretability. In this research article, we have proposed a hybrid and flexible deep learning architecture OLConvNet that integrates the interpretability of traditional object-level features and generalization of deep learning features by using a shallower Convolutional Neural Network (CNN) named as $CNN_{3L}$. $CNN_{3L}$ reduces the training time by training fewer parameters and hence eliminating space constraints imposed by deeper algorithms. We used F1-score and multiclass Area Under the Curve (AUC) performance parameters to compare the results. To further strengthen the viability of our architectural approach, we tested our proposed methodology with state of the art deep learning architectures AlexNet, VGG16, VGG19, ResNet50, InceptionV3, and DenseNet121 as backbone networks. After a comprehensive analysis of classification results from all four architectures, we observed that our proposed model works well and perform better than contemporary complex algorithms.



### A Novel Architecture Slimming Method for Network Pruning and Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2202.10461v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10461v1)
- **Published**: 2022-02-21 12:45:51+00:00
- **Updated**: 2022-02-21 12:45:51+00:00
- **Authors**: Dongqi Wang, Shengyu Zhang, Zhipeng Di, Xin Lin, Weihua Zhou, Fei Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Network pruning and knowledge distillation are two widely-known model compression methods that efficiently reduce computation cost and model size. A common problem in both pruning and distillation is to determine compressed architecture, i.e., the exact number of filters per layer and layer configuration, in order to preserve most of the original model capacity. In spite of the great advances in existing works, the determination of an excellent architecture still requires human interference or tremendous experimentations. In this paper, we propose an architecture slimming method that automates the layer configuration process. We start from the perspective that the capacity of the over-parameterized model can be largely preserved by finding the minimum number of filters preserving the maximum parameter variance per layer, resulting in a thin architecture. We formulate the determination of compressed architecture as a one-step orthogonal linear transformation, and integrate principle component analysis (PCA), where the variances of filters in the first several projections are maximized. We demonstrate the rationality of our analysis and the effectiveness of the proposed method through extensive experiments. In particular, we show that under the same overall compression rate, the compressed architecture determined by our method shows significant performance gain over baselines after pruning and distillation. Surprisingly, we find that the resulting layer-wise compression rates correspond to the layer sensitivities found by existing works through tremendous experimentations.



### OSegNet: Operational Segmentation Network for COVID-19 Detection using Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2202.10185v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10185v2)
- **Published**: 2022-02-21 12:52:23+00:00
- **Updated**: 2022-05-30 10:15:16+00:00
- **Authors**: Aysen Degerli, Serkan Kiranyaz, Muhammad E. H. Chowdhury, Moncef Gabbouj
- **Comment**: None
- **Journal**: None
- **Summary**: Coronavirus disease 2019 (COVID-19) has been diagnosed automatically using Machine Learning algorithms over chest X-ray (CXR) images. However, most of the earlier studies used Deep Learning models over scarce datasets bearing the risk of overfitting. Additionally, previous studies have revealed the fact that deep networks are not reliable for classification since their decisions may originate from irrelevant areas on the CXRs. Therefore, in this study, we propose Operational Segmentation Network (OSegNet) that performs detection by segmenting COVID-19 pneumonia for a reliable diagnosis. To address the data scarcity encountered in training and especially in evaluation, this study extends the largest COVID-19 CXR dataset: QaTa-COV19 with 121,378 CXRs including 9258 COVID-19 samples with their corresponding ground-truth segmentation masks that are publicly shared with the research community. Consequently, OSegNet has achieved a detection performance with the highest accuracy of 99.65% among the state-of-the-art deep models with 98.09% precision.



### Learning Multiple Explainable and Generalizable Cues for Face Anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/2202.10187v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10187v1)
- **Published**: 2022-02-21 12:55:59+00:00
- **Updated**: 2022-02-21 12:55:59+00:00
- **Authors**: Ying Bian, Peng Zhang, Jingjing Wang, Chunmao Wang, Shiliang Pu
- **Comment**: Camera Ready, ICASSP 2022
- **Journal**: None
- **Summary**: Although previous CNN based face anti-spoofing methods have achieved promising performance under intra-dataset testing, they suffer from poor generalization under cross-dataset testing. The main reason is that they learn the network with only binary supervision, which may learn arbitrary cues overfitting on the training dataset. To make the learned feature explainable and more generalizable, some researchers introduce facial depth and reflection map as the auxiliary supervision. However, many other generalizable cues are unexplored for face anti-spoofing, which limits their performance under cross-dataset testing. To this end, we propose a novel framework to learn multiple explainable and generalizable cues (MEGC) for face anti-spoofing. Specifically, inspired by the process of human decision, four mainly used cues by humans are introduced as auxiliary supervision including the boundary of spoof medium, moir\'e pattern, reflection artifacts and facial depth in addition to the binary supervision. To avoid extra labelling cost, corresponding synthetic methods are proposed to generate these auxiliary supervision maps. Extensive experiments on public datasets validate the effectiveness of these cues, and state-of-the-art performances are achieved by our proposed method.



### OG-SGG: Ontology-Guided Scene Graph Generation. A Case Study in Transfer Learning for Telepresence Robotics
- **Arxiv ID**: http://arxiv.org/abs/2202.10201v3
- **DOI**: 10.1109/ACCESS.2022.3230590
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10201v3)
- **Published**: 2022-02-21 13:23:15+00:00
- **Updated**: 2022-12-20 08:36:49+00:00
- **Authors**: Fernando Amodeo, Fernando Caballero, Natalia Díaz-Rodríguez, Luis Merino
- **Comment**: 20 pages; version accepted and published in IEEE Access
- **Journal**: None
- **Summary**: Scene graph generation from images is a task of great interest to applications such as robotics, because graphs are the main way to represent knowledge about the world and regulate human-robot interactions in tasks such as Visual Question Answering (VQA). Unfortunately, its corresponding area of machine learning is still relatively in its infancy, and the solutions currently offered do not specialize well in concrete usage scenarios. Specifically, they do not take existing "expert" knowledge about the domain world into account; and that might indeed be necessary in order to provide the level of reliability demanded by the use case scenarios. In this paper, we propose an initial approximation to a framework called Ontology-Guided Scene Graph Generation (OG-SGG), that can improve the performance of an existing machine learning based scene graph generator using prior knowledge supplied in the form of an ontology (specifically, using the axioms defined within); and we present results evaluated on a specific scenario founded in telepresence robotics. These results show quantitative and qualitative improvements in the generated scene graphs.



### Learning Bayesian Sparse Networks with Full Experience Replay for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.10203v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10203v1)
- **Published**: 2022-02-21 13:25:03+00:00
- **Updated**: 2022-02-21 13:25:03+00:00
- **Authors**: Dong Gong, Qingsen Yan, Yuhang Liu, Anton van den Hengel, Javen Qinfeng Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Continual Learning (CL) methods aim to enable machine learning models to learn new tasks without catastrophic forgetting of those that have been previously mastered. Existing CL approaches often keep a buffer of previously-seen samples, perform knowledge distillation, or use regularization techniques towards this goal. Despite their performance, they still suffer from interference across tasks which leads to catastrophic forgetting. To ameliorate this problem, we propose to only activate and select sparse neurons for learning current and past tasks at any stage. More parameters space and model capacity can thus be reserved for the future tasks. This minimizes the interference between parameters for different tasks. To do so, we propose a Sparse neural Network for Continual Learning (SNCL), which employs variational Bayesian sparsity priors on the activations of the neurons in all layers. Full Experience Replay (FER) provides effective supervision in learning the sparse activations of the neurons in different layers. A loss-aware reservoir-sampling strategy is developed to maintain the memory buffer. The proposed method is agnostic as to the network structures and the task boundaries. Experiments on different datasets show that our approach achieves state-of-the-art performance for mitigating forgetting.



### Offline Text-Independent Writer Identification based on word level data
- **Arxiv ID**: http://arxiv.org/abs/2202.10207v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10207v2)
- **Published**: 2022-02-21 13:32:09+00:00
- **Updated**: 2022-07-05 11:03:38+00:00
- **Authors**: Vineet Kumar, Suresh Sundaram
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel scheme to identify the authorship of a document based on handwritten input word images of an individual. Our approach is text-independent and does not place any restrictions on the size of the input word images under consideration. To begin with, we employ the SIFT algorithm to extract multiple key points at various levels of abstraction (comprising allograph, character, or combination of characters). These key points are then passed through a trained CNN network to generate feature maps corresponding to a convolution layer. However, owing to the scale corresponding to the SIFT key points, the size of a generated feature map may differ. As an alleviation to this issue, the histogram of gradients is applied on the feature map to produce a fixed representation. Typically, in a CNN, the number of filters of each convolution block increase depending on the depth of the network. Thus, extracting histogram features for each of the convolution feature map increase the dimension as well as the computational load. To address this aspect, we use an entropy-based method to learn the weights of the feature maps of a particular CNN layer during the training phase of our algorithm. The efficacy of our proposed system has been demonstrated on two publicly available databases namely CVL and IAM. We empirically show that the results obtained are promising when compared with previous works.



### Localformer: a Locality-Preserving Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2202.10240v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10240v5)
- **Published**: 2022-02-21 13:53:04+00:00
- **Updated**: 2023-04-23 11:04:22+00:00
- **Authors**: Qingsong Zhao, Zhipeng Zhou, Yi Wang, Yu Qiao, Cairong Zhao
- **Comment**: Updating more experiments, and introducing more innovative content
- **Journal**: None
- **Summary**: Zigzag flattening (ZF) is commonly used in computer vision as a default option to unfold matrices, \eg in patch slicing for Vision Transformer (ViT). However, when decomposing multi-scale-object web images, ZF cannot preserve the smoothness of local information well. To address this, we draw inspiration from Space-Filling Curves (SFC) and investigate Hilbert flattening (HF) as an alternative for visual models. We provide a comprehensive theoretical discussion and practical analysis, demonstrating the superiority of HF over other SFC in locality and multi-scale robustness. We leverage HF to alleviate the problem of the lack of locality bias in the shallow layers of ViT, which formulates our Localformer. Extensive experiments demonstrate that Localformer consistently improves performance for several common visual tasks. Additionally, upon inspection, we find that Localformer enhances representation learning and length extrapolation abilities of ViT.



### PointSCNet: Point Cloud Structure and Correlation Learning Based on Space Filling Curve-Guided Sampling
- **Arxiv ID**: http://arxiv.org/abs/2202.10251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10251v1)
- **Published**: 2022-02-21 14:00:20+00:00
- **Updated**: 2022-02-21 14:00:20+00:00
- **Authors**: Xingye Chen, Yiqi Wu, Wenjie Xu, Jin Li, Huaiyi Dong, Yilin Chen
- **Comment**: None
- **Journal**: [J]. Symmetry, 2022, 14(1): 8
- **Summary**: Geometrical structures and the internal local region relationship, such as symmetry, regular array, junction, etc., are essential for understanding a 3D shape. This paper proposes a point cloud feature extraction network named PointSCNet, to capture the geometrical structure information and local region correlation information of a point cloud. The PointSCNet consists of three main modules: the space-filling curve-guided sampling module, the information fusion module, and the channel-spatial attention module. The space-filling curve-guided sampling module uses Z-order curve coding to sample points that contain geometrical correlation. The information fusion module uses a correlation tensor and a set of skip connections to fuse the structure and correlation information. The channel-spatial attention module enhances the representation of key points and crucial feature channels to refine the network. The proposed PointSCNet is evaluated on shape classification and part segmentation tasks. The experimental results demonstrate that the PointSCNet outperforms or is on par with state-of-the-art methods by learning the structure and correlation of point clouds effectively.



### Inflation of test accuracy due to data leakage in deep learning-based classification of OCT images
- **Arxiv ID**: http://arxiv.org/abs/2202.12267v2
- **DOI**: 10.1038/s41597-022-01618-6
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12267v2)
- **Published**: 2022-02-21 14:08:42+00:00
- **Updated**: 2022-09-27 16:38:47+00:00
- **Authors**: Iulian Emil Tampu, Anders Eklund, Neda Haj-Hosseini
- **Comment**: 8 pages, 2 figures
- **Journal**: Sci Data 9, 580 (2022)
- **Summary**: In the application of deep learning on optical coherence tomography (OCT) data, it is common to train classification networks using 2D images originating from volumetric data. Given the micrometer resolution of OCT systems, consecutive images are often very similar in both visible structures and noise. Thus, an inappropriate data split can result in overlap between the training and testing sets, with a large portion of the literature overlooking this aspect. In this study, the effect of improper dataset splitting on model evaluation is demonstrated for three classification tasks using three OCT open-access datasets extensively used, Kermany's and Srinivasan's ophthalmology datasets, and AIIMS breast tissue dataset. Results show that the classification performance is inflated by 0.07 up to 0.43 in terms of Matthews Correlation Coefficient (accuracy: 5% to 30%) for models tested on datasets with improper splitting, highlighting the considerable effect of dataset handling on model evaluation. This study intends to raise awareness on the importance of dataset splitting given the increased research interest in implementing deep learning on OCT data.



### A Self-Supervised Descriptor for Image Copy Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.10261v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10261v2)
- **Published**: 2022-02-21 14:25:32+00:00
- **Updated**: 2022-03-25 18:15:44+00:00
- **Authors**: Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, Matthijs Douze
- **Comment**: None
- **Journal**: None
- **Summary**: Image copy detection is an important task for content moderation. We introduce SSCD, a model that builds on a recent self-supervised contrastive training objective. We adapt this method to the copy detection task by changing the architecture and training objective, including a pooling operator from the instance matching literature, and adapting contrastive learning to augmentations that combine images.   Our approach relies on an entropy regularization term, promoting consistent separation between descriptor vectors, and we demonstrate that this significantly improves copy detection accuracy. Our method produces a compact descriptor vector, suitable for real-world web scale applications. Statistical information from a background image distribution can be incorporated into the descriptor.   On the recent DISC2021 benchmark, SSCD is shown to outperform both baseline copy detection models and self-supervised architectures designed for image classification by huge margins, in all settings. For example, SSCD out-performs SimCLR descriptors by 48% absolute. Code is available at https://github.com/facebookresearch/sscd-copy-detection.



### End-to-End High Accuracy License Plate Recognition Based on Depthwise Separable Convolution Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.10277v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10277v1)
- **Published**: 2022-02-21 14:45:03+00:00
- **Updated**: 2022-02-21 14:45:03+00:00
- **Authors**: Song-Ren Wang, Hong-Yang Shih, Zheng-Yi Shen, Wen-Kai Tai
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic license plate recognition plays a crucial role in modern transportation systems such as for traffic monitoring and vehicle violation detection. In real-world scenarios, license plate recognition still faces many challenges and is impaired by unpredictable interference such as weather or lighting conditions. Many machine learning based ALPR solutions have been proposed to solve such challenges in recent years. However, most are not convincing, either because their results are evaluated on small or simple datasets that lack diverse surroundings, or because they require powerful hardware to achieve a reasonable frames-per-second in real-world applications. In this paper, we propose a novel segmentation-free framework for license plate recognition and introduce NP-ALPR, a diverse and challenging dataset which resembles real-world scenarios. The proposed network model consists of the latest deep learning methods and state-of-the-art ideas, and benefits from a novel network architecture. It achieves higher accuracy with lower computational requirements than previous works. We evaluate the effectiveness of the proposed method on three different datasets and show a recognition accuracy of over 99% and over 70 fps, demonstrating that our method is not only robust but also computationally efficient.



### ALGAN: Anomaly Detection by Generating Pseudo Anomalous Data via Latent Variables
- **Arxiv ID**: http://arxiv.org/abs/2202.10281v2
- **DOI**: 10.1109/ACCESS.2022.3169594
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10281v2)
- **Published**: 2022-02-21 14:53:05+00:00
- **Updated**: 2022-05-09 14:48:36+00:00
- **Authors**: Hironori Murase, Kenji Fukumizu
- **Comment**: 13 pages, 8 figures
- **Journal**: IEEE Access, vol. 10, pp. 44259-44270, 2022
- **Summary**: In many anomaly detection tasks, where anomalous data rarely appear and are difficult to collect, training using only normal data is important. Although it is possible to manually create anomalous data using prior knowledge, they may be subject to user bias. In this paper, we propose an Anomalous Latent variable Generative Adversarial Network (ALGAN) in which the GAN generator produces pseudo-anomalous data as well as fake-normal data, whereas the discriminator is trained to distinguish between normal and pseudo-anomalous data. This differs from the standard GAN discriminator, which specializes in classifying two similar classes. The training dataset contains only normal data; the latent variables are introduced in anomalous states and are input into the generator to produce diverse pseudo-anomalous data. We compared the performance of ALGAN with other existing methods on the MVTec-AD, Magnetic Tile Defects, and COIL-100 datasets. The experimental results showed that ALGAN exhibited an AUROC comparable to those of state-of-the-art methods while achieving a much faster prediction time.



### A Review of Emerging Research Directions in Abstract Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2202.10284v2
- **DOI**: 10.1016/j.inffus.2022.11.011
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10284v2)
- **Published**: 2022-02-21 14:58:02+00:00
- **Updated**: 2022-03-07 09:56:09+00:00
- **Authors**: Mikołaj Małkiński, Jacek Mańdziuk
- **Comment**: None
- **Journal**: Information Fusion, Volume 91, March 2023, Pages 713-736
- **Summary**: Abstract Visual Reasoning (AVR) problems are commonly used to approximate human intelligence. They test the ability of applying previously gained knowledge, experience and skills in a completely new setting, which makes them particularly well-suited for this task. Recently, the AVR problems have become popular as a proxy to study machine intelligence, which has led to emergence of new distinct types of problems and multiple benchmark sets. In this work we review this emerging AVR research and propose a taxonomy to categorise the AVR tasks along 5 dimensions: input shapes, hidden rules, target task, cognitive function, and main challenge. The perspective taken in this survey allows to characterise AVR problems with respect to their shared and distinct properties, provides a unified view on the existing approaches for solving AVR tasks, shows how the AVR problems relate to practical applications, and outlines promising directions for future work. One of them refers to the observation that in the machine learning literature different tasks are considered in isolation, which is in the stark contrast with the way the AVR tasks are used to measure human intelligence, where multiple types of problems are combined within a single IQ test.



### A Comprehensive Evaluation on Multi-channel Biometric Face Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.10286v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10286v1)
- **Published**: 2022-02-21 15:04:39+00:00
- **Updated**: 2022-02-21 15:04:39+00:00
- **Authors**: Anjith George, David Geissbuhler, Sebastien Marcel
- **Comment**: 16 pages, 11 images
- **Journal**: None
- **Summary**: The vulnerability against presentation attacks is a crucial problem undermining the wide-deployment of face recognition systems. Though presentation attack detection (PAD) systems try to address this problem, the lack of generalization and robustness continues to be a major concern. Several works have shown that using multi-channel PAD systems could alleviate this vulnerability and result in more robust systems. However, there is a wide selection of channels available for a PAD system such as RGB, Near Infrared, Shortwave Infrared, Depth, and Thermal sensors. Having a lot of sensors increases the cost of the system, and therefore an understanding of the performance of different sensors against a wide variety of attacks is necessary while selecting the modalities. In this work, we perform a comprehensive study to understand the effectiveness of various imaging modalities for PAD. The studies are performed on a multi-channel PAD dataset, collected with 14 different sensing modalities considering a wide range of 2D, 3D, and partial attacks. We used the multi-channel convolutional network-based architecture, which uses pixel-wise binary supervision. The model has been evaluated with different combinations of channels, and different image qualities on a variety of challenging known and unknown attack protocols. The results reveal interesting trends and can act as pointers for sensor selection for safety-critical presentation attack detection systems. The source codes and protocols to reproduce the results are made available publicly making it possible to extend this work to other architectures.



### Seeing the advantage: visually grounding word embeddings to better capture human semantic knowledge
- **Arxiv ID**: http://arxiv.org/abs/2202.10292v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10292v1)
- **Published**: 2022-02-21 15:13:48+00:00
- **Updated**: 2022-02-21 15:13:48+00:00
- **Authors**: Danny Merkx, Stefan L. Frank, Mirjam Ernestus
- **Comment**: None
- **Journal**: Proceedings of the Workshop on Cognitive Modeling and
  Computational Linguistics (CMCL) 2022
- **Summary**: Distributional semantic models capture word-level meaning that is useful in many natural language processing tasks and have even been shown to capture cognitive aspects of word meaning. The majority of these models are purely text based, even though the human sensory experience is much richer. In this paper we create visually grounded word embeddings by combining English text and images and compare them to popular text-based methods, to see if visual information allows our model to better capture cognitive aspects of word meaning. Our analysis shows that visually grounded embedding similarities are more predictive of the human reaction times in a large priming experiment than the purely text-based embeddings. The visually grounded embeddings also correlate well with human word similarity ratings. Importantly, in both experiments we show that the grounded embeddings account for a unique portion of explained variance, even when we include text-based embeddings trained on huge corpora. This shows that visual grounding allows our model to capture information that cannot be extracted using text as the only source of information.



### VLAD-VSA: Cross-Domain Face Presentation Attack Detection with Vocabulary Separation and Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2202.10301v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.10301v1)
- **Published**: 2022-02-21 15:27:41+00:00
- **Updated**: 2022-02-21 15:27:41+00:00
- **Authors**: Jiong Wang, Zhou Zhao, Weike Jin, Xinyu Duan, Zhen Lei, Baoxing Huai, Yiling Wu, Xiaofei He
- **Comment**: ACM MM 2021
- **Journal**: None
- **Summary**: For face presentation attack detection (PAD), most of the spoofing cues are subtle, local image patterns (e.g., local image distortion, 3D mask edge and cut photo edges). The representations of existing PAD works with simple global pooling method, however, lose the local feature discriminability. In this paper, the VLAD aggregation method is adopted to quantize local features with visual vocabulary locally partitioning the feature space, and hence preserve the local discriminability. We further propose the vocabulary separation and adaptation method to modify VLAD for cross-domain PADtask. The proposed vocabulary separation method divides vocabulary into domain-shared and domain-specific visual words to cope with the diversity of live and attack faces under the cross-domain scenario. The proposed vocabulary adaptation method imitates the maximization step of the k-means algorithm in the end-to-end training, which guarantees the visual words be close to the center of assigned local features and thus brings robust similarity measurement. We give illustrations and extensive experiments to demonstrate the effectiveness of VLAD with the proposed vocabulary separation and adaptation method on standard cross-domain PAD benchmarks. The codes are available at https://github.com/Liubinggunzu/VLAD-VSA.



### Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion
- **Arxiv ID**: http://arxiv.org/abs/2202.10304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10304v1)
- **Published**: 2022-02-21 15:30:14+00:00
- **Updated**: 2022-02-21 15:30:14+00:00
- **Authors**: Minghui Liao, Zhisheng Zou, Zhaoyi Wan, Cong Yao, Xiang Bai
- **Comment**: Accepted by TPAMI. arXiv admin note: substantial text overlap with
  arXiv:1911.08947
- **Journal**: None
- **Summary**: Recently, segmentation-based scene text detection methods have drawn extensive attention in the scene text detection field, because of their superiority in detecting the text instances of arbitrary shapes and extreme aspect ratios, profiting from the pixel-level descriptions. However, the vast majority of the existing segmentation-based approaches are limited to their complex post-processing algorithms and the scale robustness of their segmentation models, where the post-processing algorithms are not only isolated to the model optimization but also time-consuming and the scale robustness is usually strengthened by fusing multi-scale feature maps directly. In this paper, we propose a Differentiable Binarization (DB) module that integrates the binarization process, one of the most important steps in the post-processing procedure, into a segmentation network. Optimized along with the proposed DB module, the segmentation network can produce more accurate results, which enhances the accuracy of text detection with a simple pipeline. Furthermore, an efficient Adaptive Scale Fusion (ASF) module is proposed to improve the scale robustness by fusing features of different scales adaptively. By incorporating the proposed DB and ASF with the segmentation network, our proposed scene text detector consistently achieves state-of-the-art results, in terms of both detection accuracy and speed, on five standard benchmarks.



### Reducing the Gibbs effect in multimodal medical imaging by the Fake Nodes Approach
- **Arxiv ID**: http://arxiv.org/abs/2202.10325v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, 68U10, 65D05, 41A15
- **Links**: [PDF](http://arxiv.org/pdf/2202.10325v1)
- **Published**: 2022-02-21 15:46:58+00:00
- **Updated**: 2022-02-21 15:46:58+00:00
- **Authors**: Davide Poggiali, Diego Cecchin, Stefano De Marchi
- **Comment**: None
- **Journal**: None
- **Summary**: It is a common practice in multimodal medical imaging to undersample the anatomically-derived segmentation images to measure the mean activity of a co-acquired functional image. This practice avoids the resampling-related Gibbs effect that would occur in oversampling the functional image. As sides effect, waste of time and efforts are produced since the anatomical segmentation at full resolution is performed in many hours of computations or manual work. In this work we explain the commonly-used resampling methods and give errors bound in the cases of continuous and discontinuous signals. Then we propose a Fake Nodes scheme for image resampling designed to reduce the Gibbs effect when oversampling the functional image. This new approach is compared to the traditional counterpart in two significant experiments, both showing that Fake Nodes resampling gives smaller errors.



### On the Evaluation of RGB-D-based Categorical Pose and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.10346v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.10346v1)
- **Published**: 2022-02-21 16:31:18+00:00
- **Updated**: 2022-02-21 16:31:18+00:00
- **Authors**: Leonard Bruns, Patric Jensfelt
- **Comment**: 17 pages, 8 figures, submitted to IAS-17
- **Journal**: None
- **Summary**: Recently, various methods for 6D pose and shape estimation of objects have been proposed. Typically, these methods evaluate their pose estimation in terms of average precision, and reconstruction quality with chamfer distance. In this work we take a critical look at this predominant evaluation protocol including metrics and datasets. We propose a new set of metrics, contribute new annotations for the Redwood dataset and evaluate state-of-the-art methods in a fair comparison. We find that existing methods do not generalize well to unconstrained orientations, and are actually heavily biased towards objects being upright. We contribute an easy-to-use evaluation toolbox with well-defined metrics, method and dataset interfaces, which readily allows evaluation and comparison with various state-of-the-art approaches (see https://github.com/roym899/pose_and_shape_evaluation ).



### Self-Supervised Bulk Motion Artifact Removal in Optical Coherence Tomography Angiography
- **Arxiv ID**: http://arxiv.org/abs/2202.10360v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10360v2)
- **Published**: 2022-02-21 16:58:22+00:00
- **Updated**: 2022-03-27 14:51:32+00:00
- **Authors**: Jiaxiang Ren, Kicheon Park, Yingtian Pan, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: Optical coherence tomography angiography (OCTA) is an important imaging modality in many bioengineering tasks. The image quality of OCTA, however, is often degraded by Bulk Motion Artifacts (BMA), which are due to micromotion of subjects and typically appear as bright stripes surrounded by blurred areas. State-of-the-art methods usually treat BMA removal as a learning-based image inpainting problem, but require numerous training samples with nontrivial annotation. In addition, these methods discard the rich structural and appearance information carried in the BMA stripe region. To address these issues, in this paper we propose a self-supervised content-aware BMA removal model. First, the gradient-based structural information and appearance feature are extracted from the BMA area and injected into the model to capture more connectivity. Second, with easily collected defective masks, the model is trained in a self-supervised manner, in which only the clear areas are used for training while the BMA areas for inference. With the structural information and appearance feature from noisy image as references, our model can remove larger BMA and produce better visualizing result. In addition, only 2D images with defective masks are involved, hence improving the efficiency of our method. Experiments on OCTA of mouse cortex demonstrate that our model can remove most BMA with extremely large sizes and inconsistent intensities while previous methods fail.



### MIST GAN: Modality Imputation Using Style Transfer for MRI
- **Arxiv ID**: http://arxiv.org/abs/2202.10396v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10396v1)
- **Published**: 2022-02-21 17:50:40+00:00
- **Updated**: 2022-02-21 17:50:40+00:00
- **Authors**: Jaya Chandra Raju, Kompella Subha Gayatri, Keerthi Ram, Rajeswaran Rangasami, Rajoo Ramachandran, Mohansankar Sivaprakasam
- **Comment**: None
- **Journal**: None
- **Summary**: MRI entails a great amount of cost, time and effort for the generation of all the modalities that are recommended for efficient diagnosis and treatment planning. Recent advancements in deep learning research show that generative models have achieved substantial improvement in the aspects of style transfer and image synthesis. In this work, we formulate generating the missing MR modality from existing MR modalities as an imputation problem using style transfer. With a multiple-to-one mapping, we model a network that accommodates domain specific styles in generating the target image. We analyse the style diversity both within and across MR modalities. Our model is tested on the BraTS'18 dataset and the results obtained are observed to be on par with the state-of-the-art in terms of visual metrics, SSIM and PSNR. After being evaluated by two expert radiologists, we show that our model is efficient, extendable, and suitable for clinical applications.



### Vision-Language Pre-Training with Triple Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.10401v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10401v4)
- **Published**: 2022-02-21 17:54:57+00:00
- **Updated**: 2022-03-28 14:44:39+00:00
- **Authors**: Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, Junzhou Huang
- **Comment**: CVPR 2022; code: https://github.com/uta-smile/TCL
- **Journal**: None
- **Summary**: Vision-language representation learning largely benefits from image-text alignment through contrastive losses (e.g., InfoNCE loss). The success of this alignment strategy is attributed to its capability in maximizing the mutual information (MI) between an image and its matched text. However, simply performing cross-modal alignment (CMA) ignores data potential within each modality, which may result in degraded representations. For instance, although CMA-based models are able to map image-text pairs close together in the embedding space, they fail to ensure that similar inputs from the same modality stay close by. This problem can get even worse when the pre-training data is noisy. In this paper, we propose triple contrastive learning (TCL) for vision-language pre-training by leveraging both cross-modal and intra-modal self-supervision. Besides CMA, TCL introduces an intra-modal contrastive objective to provide complementary benefits in representation learning. To take advantage of localized and structural information from image and text input, TCL further maximizes the average MI between local regions of image/text and their global summary. To the best of our knowledge, ours is the first work that takes into account local structure information for multi-modality representation learning. Experimental evaluations show that our approach is competitive and achieves the new state of the art on various common down-stream vision-language tasks such as image-text retrieval and visual question answering.



### Malaria detection in Segmented Blood Cell using Convolutional Neural Networks and Canny Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.10426v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10426v1)
- **Published**: 2022-02-21 18:34:35+00:00
- **Updated**: 2022-02-21 18:34:35+00:00
- **Authors**: Tahsinur Rahman Talukdar, Mohammad Jaber Hossain, Tahmid H. Talukdar
- **Comment**: None
- **Journal**: None
- **Summary**: We apply convolutional neural networks to identify between malaria infected and non-infected segmented cells from the thin blood smear slide images. We optimize our model to find over 95% accuracy in malaria cell detection. We also apply Canny image processing to reduce training file size while maintaining comparable accuracy (~ 94%).



### Robotic Telekinesis: Learning a Robotic Hand Imitator by Watching Humans on Youtube
- **Arxiv ID**: http://arxiv.org/abs/2202.10448v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10448v2)
- **Published**: 2022-02-21 18:59:59+00:00
- **Updated**: 2022-07-24 06:08:35+00:00
- **Authors**: Aravind Sivakumar, Kenneth Shaw, Deepak Pathak
- **Comment**: RSS 2022 final version. Website and demos at
  https://robotic-telekinesis.github.io/
- **Journal**: None
- **Summary**: We build a system that enables any human to control a robot hand and arm, simply by demonstrating motions with their own hand. The robot observes the human operator via a single RGB camera and imitates their actions in real-time. Human hands and robot hands differ in shape, size, and joint structure, and performing this translation from a single uncalibrated camera is a highly underconstrained problem. Moreover, the retargeted trajectories must effectively execute tasks on a physical robot, which requires them to be temporally smooth and free of self-collisions. Our key insight is that while paired human-robot correspondence data is expensive to collect, the internet contains a massive corpus of rich and diverse human hand videos. We leverage this data to train a system that understands human hands and retargets a human video stream into a robot hand-arm trajectory that is smooth, swift, safe, and semantically similar to the guiding demonstration. We demonstrate that it enables previously untrained people to teleoperate a robot on various dexterous manipulation tasks. Our low-cost, glove-free, marker-free remote teleoperation system makes robot teaching more accessible and we hope that it can aid robots in learning to act autonomously in the real world. Videos at https://robotic-telekinesis.github.io/



### CaMEL: Mean Teacher Learning for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2202.10492v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2202.10492v1)
- **Published**: 2022-02-21 19:04:46+00:00
- **Updated**: 2022-02-21 19:04:46+00:00
- **Authors**: Manuele Barraco, Matteo Stefanini, Marcella Cornia, Silvia Cascianelli, Lorenzo Baraldi, Rita Cucchiara
- **Comment**: None
- **Journal**: None
- **Summary**: Describing images in natural language is a fundamental step towards the automatic modeling of connections between the visual and textual modalities. In this paper we present CaMEL, a novel Transformer-based architecture for image captioning. Our proposed approach leverages the interaction of two interconnected language models that learn from each other during the training phase. The interplay between the two language models follows a mean teacher learning paradigm with knowledge distillation. Experimentally, we assess the effectiveness of the proposed solution on the COCO dataset and in conjunction with different visual feature extractors. When comparing with existing proposals, we demonstrate that our model provides state-of-the-art caption quality with a significantly reduced number of parameters. According to the CIDEr metric, we obtain a new state of the art on COCO when training without using external data. The source code and trained models are publicly available at: https://github.com/aimagelab/camel.



### Self-Evolutionary Clustering
- **Arxiv ID**: http://arxiv.org/abs/2202.10505v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.10505v1)
- **Published**: 2022-02-21 19:38:18+00:00
- **Updated**: 2022-02-21 19:38:18+00:00
- **Authors**: Hanxuan Wang, Na Lu, Qinyang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep clustering outperforms conventional clustering by mutually promoting representation learning and cluster assignment. However, most existing deep clustering methods suffer from two major drawbacks. First, most cluster assignment methods are based on simple distance comparison and highly dependent on the target distribution generated by a handcrafted nonlinear mapping. These facts largely limit the possible performance that deep clustering methods can reach. Second, the clustering results can be easily guided towards wrong direction by the misassigned samples in each cluster. The existing deep clustering methods are incapable of discriminating such samples. To address these issues, a novel modular Self-Evolutionary Clustering (Self-EvoC) framework is constructed, which boosts the clustering performance by classification in a self-supervised manner. Fuzzy theory is used to score the sample membership with probability which evaluates the intermediate clustering result certainty of each sample. Based on which, the most reliable samples can be selected and augmented. The augmented data are employed to fine-tune an off-the-shelf deep network classifier with the labels from the clustering, which results in a model to generate the target distribution. The proposed framework can efficiently discriminate sample outliers and generate better target distribution with the assistance of self-supervised classifier. Extensive experiments indicate that the Self-EvoC remarkably outperforms state-of-the-art deep clustering methods on three benchmark datasets.



### Semi-Implicit Hybrid Gradient Methods with Application to Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/2202.10523v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2202.10523v1)
- **Published**: 2022-02-21 20:27:10+00:00
- **Updated**: 2022-02-21 20:27:10+00:00
- **Authors**: Beomsu Kim, Junghoon Seo
- **Comment**: International Conference on Artificial Intelligence and Statistics
  (AISTATS) 2022
- **Journal**: None
- **Summary**: Adversarial examples, crafted by adding imperceptible perturbations to natural inputs, can easily fool deep neural networks (DNNs). One of the most successful methods for training adversarially robust DNNs is solving a nonconvex-nonconcave minimax problem with an adversarial training (AT) algorithm. However, among the many AT algorithms, only Dynamic AT (DAT) and You Only Propagate Once (YOPO) guarantee convergence to a stationary point. In this work, we generalize the stochastic primal-dual hybrid gradient algorithm to develop semi-implicit hybrid gradient methods (SI-HGs) for finding stationary points of nonconvex-nonconcave minimax problems. SI-HGs have the convergence rate $O(1/K)$, which improves upon the rate $O(1/K^{1/2})$ of DAT and YOPO. We devise a practical variant of SI-HGs, and show that it outperforms other AT algorithms in terms of convergence speed and robustness.



### Dynamic Sampling Rate: Harnessing Frame Coherence in Graphics Applications for Energy-Efficient GPUs
- **Arxiv ID**: http://arxiv.org/abs/2202.10533v1
- **DOI**: None
- **Categories**: **cs.AR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10533v1)
- **Published**: 2022-02-21 21:15:14+00:00
- **Updated**: 2022-02-21 21:15:14+00:00
- **Authors**: Martí Anglada, Enrique de Lucas, Joan-Manuel Parcerisa, Juan L. Aragón, Antonio González
- **Comment**: None
- **Journal**: None
- **Summary**: In real-time rendering, a 3D scene is modelled with meshes of triangles that the GPU projects to the screen. They are discretized by sampling each triangle at regular space intervals to generate fragments which are then added texture and lighting effects by a shader program. Realistic scenes require detailed geometric models, complex shaders, high-resolution displays and high screen refreshing rates, which all come at a great compute time and energy cost. This cost is often dominated by the fragment shader, which runs for each sampled fragment. Conventional GPUs sample the triangles once per pixel, however, there are many screen regions containing low variation that produce identical fragments and could be sampled at lower than pixel-rate with no loss in quality. Additionally, as temporal frame coherence makes consecutive frames very similar, such variations are usually maintained from frame to frame. This work proposes Dynamic Sampling Rate (DSR), a novel hardware mechanism to reduce redundancy and improve the energy efficiency in graphics applications. DSR analyzes the spatial frequencies of the scene once it has been rendered. Then, it leverages the temporal coherence in consecutive frames to decide, for each region of the screen, the lowest sampling rate to employ in the next frame that maintains image quality. We evaluate the performance of a state-of-the-art mobile GPU architecture extended with DSR for a wide variety of applications. Experimental results show that DSR is able to remove most of the redundancy inherent in the color computations at fragment granularity, which brings average speedups of 1.68x and energy savings of 40%.



### ReViVD: Exploration and Filtering of Trajectories in an Immersive Environment using 3D Shapes
- **Arxiv ID**: http://arxiv.org/abs/2202.10545v1
- **DOI**: 10.1109/VR46266.2020.00096
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10545v1)
- **Published**: 2022-02-21 21:58:41+00:00
- **Updated**: 2022-02-21 21:58:41+00:00
- **Authors**: François Homps, Yohan Beugin, Romain Vuillemot
- **Comment**: Accepted at IEEE Conference on Virtual Reality and 3D User Interfaces
  (VR) 2020
- **Journal**: 2020 IEEE Conference on Virtual Reality and 3D User Interfaces
  (VR)
- **Summary**: We present ReViVD, a tool for exploring and filtering large trajectory-based datasets using virtual reality. ReViVD's novelty lies in using simple 3D shapes -- such as cuboids, spheres and cylinders -- as queries for users to select and filter groups of trajectories. Building on this simple paradigm, more complex queries can be created by combining previously made selection groups through a system of user-created Boolean operations. We demonstrate the use of ReViVD in different application domains, from GPS position tracking to simulated data (e.g., turbulent particle flows and traffic simulation). Our results show the ease of use and expressiveness of the 3D geometric shapes in a broad range of exploratory tasks. ReViVD was found to be particularly useful for progressively refining selections to isolate outlying behaviors. It also acts as a powerful communication tool for conveying the structure of normally abstract datasets to an audience.



### Privacy Leakage of Adversarial Training Models in Federated Learning Systems
- **Arxiv ID**: http://arxiv.org/abs/2202.10546v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10546v1)
- **Published**: 2022-02-21 22:03:58+00:00
- **Updated**: 2022-02-21 22:03:58+00:00
- **Authors**: Jingyang Zhang, Yiran Chen, Hai Li
- **Comment**: 6 pages, 6 figures. Submitted to CVPR'22 workshop "The Art of
  Robustness"
- **Journal**: None
- **Summary**: Adversarial Training (AT) is crucial for obtaining deep neural networks that are robust to adversarial attacks, yet recent works found that it could also make models more vulnerable to privacy attacks. In this work, we further reveal this unsettling property of AT by designing a novel privacy attack that is practically applicable to the privacy-sensitive Federated Learning (FL) systems. Using our method, the attacker can exploit AT models in the FL system to accurately reconstruct users' private training images even when the training batch size is large. Code is available at https://github.com/zjysteven/PrivayAttack_AT_FL.



### Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.10571v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10571v1)
- **Published**: 2022-02-21 23:24:01+00:00
- **Updated**: 2022-02-21 23:24:01+00:00
- **Authors**: Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, Jinwoo Shin
- **Comment**: ICLR 2022. Project page with videos and code:
  https://sihyun-yu.github.io/digan/
- **Journal**: None
- **Summary**: In the deep learning era, long video generation of high-quality still remains challenging due to the spatio-temporal complexity and continuity of videos. Existing prior works have attempted to model video distribution by representing videos as 3D grids of RGB values, which impedes the scale of generated videos and neglects continuous dynamics. In this paper, we found that the recent emerging paradigm of implicit neural representations (INRs) that encodes a continuous signal into a parameterized neural network effectively mitigates the issue. By utilizing INRs of video, we propose dynamics-aware implicit generative adversarial network (DIGAN), a novel generative adversarial network for video generation. Specifically, we introduce (a) an INR-based video generator that improves the motion dynamics by manipulating the space and time coordinates differently and (b) a motion discriminator that efficiently identifies the unnatural motions without observing the entire long frame sequences. We demonstrate the superiority of DIGAN under various datasets, along with multiple intriguing properties, e.g., long video synthesis, video extrapolation, and non-autoregressive video generation. For example, DIGAN improves the previous state-of-the-art FVD score on UCF-101 by 30.7% and can be trained on 128 frame videos of 128x128 resolution, 80 frames longer than the 48 frames of the previous state-of-the-art method.



### Fast Semantic-Assisted Outlier Removal for Large-scale Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2202.10579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10579v1)
- **Published**: 2022-02-21 23:57:03+00:00
- **Updated**: 2022-02-21 23:57:03+00:00
- **Authors**: Giang Truong, Huu Le, Alvaro Parra, Syed Zulqarnain Gilani, Syed M. S. Islam, David Suter
- **Comment**: None
- **Journal**: None
- **Summary**: With current trends in sensors (cheaper, more volume of data) and applications (increasing affordability for new tasks, new ideas in what 3D data could be useful for); there is corresponding increasing interest in the ability to automatically, reliably, and cheaply, register together individual point clouds. The volume of data to handle, and still elusive need to have the registration occur fully reliably and fully automatically, mean there is a need to innovate further. One largely untapped area of innovation is that of exploiting the {\em semantic information} of the points in question. Points on a tree should match points on a tree, for example, and not points on car. Moreover, such a natural restriction is clearly human-like - a human would generally quickly eliminate candidate regions for matching based on semantics. Employing semantic information is not only efficient but natural. It is also timely - due to the recent advances in semantic classification capabilities. This paper advances this theme by demonstrating that state of the art registration techniques, in particular ones that rely on "preservation of length under rigid motion" as an underlying matching consistency constraint, can be augmented with semantic information. Semantic identity is of course also preserved under rigid-motion, but also under wider motions present in a scene. We demonstrate that not only the potential obstacle of cost of semantic segmentation, and the potential obstacle of the unreliability of semantic segmentation; are both no impediment to achieving both speed and accuracy in fully automatic registration of large scale point clouds.



