# Arxiv Papers in cs.CV on 2022-02-15
### Gaze-Guided Class Activation Mapping: Leveraging Human Attention for Network Attention in Chest X-rays Classification
- **Arxiv ID**: http://arxiv.org/abs/2202.07107v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07107v1)
- **Published**: 2022-02-15 00:33:23+00:00
- **Updated**: 2022-02-15 00:33:23+00:00
- **Authors**: Hongzhi Zhu, Septimiu Salcudean, Robert Rohling
- **Comment**: This manuscript was submitted to WACV 2022 on Aug. 18, 2021
- **Journal**: None
- **Summary**: The increased availability and accuracy of eye-gaze tracking technology has sparked attention-related research in psychology, neuroscience, and, more recently, computer vision and artificial intelligence. The attention mechanism in artificial neural networks is known to improve learning tasks. However, no previous research has combined the network attention and human attention. This paper describes a gaze-guided class activation mapping (GG-CAM) method to directly regulate the formation of network attention based on expert radiologists' visual attention for the chest X-ray pathology classification problem, which remains challenging due to the complex and often nuanced differences among images. GG-CAM is a lightweight ($3$ additional trainable parameters for regulating the learning process) and generic extension that can be easily applied to most classification convolutional neural networks (CNN). GG-CAM-modified CNNs do not require human attention as an input when fully trained. Comparative experiments suggest that two standard CNNs with the GG-CAM extension achieve significantly greater classification performance. The median area under the curve (AUC) metrics for ResNet50 increases from $0.721$ to $0.776$. For EfficientNetv2 (s), the median AUC increases from $0.723$ to $0.801$. The GG-CAM also brings better interpretability of the network that facilitates the weakly-supervised pathology localization and analysis.



### Multi-task UNet: Jointly Boosting Saliency Prediction and Disease Classification on Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/2202.07118v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07118v1)
- **Published**: 2022-02-15 01:12:42+00:00
- **Updated**: 2022-02-15 01:12:42+00:00
- **Authors**: Hongzhi Zhu, Robert Rohling, Septimiu Salcudean
- **Comment**: None
- **Journal**: None
- **Summary**: Human visual attention has recently shown its distinct capability in boosting machine learning models. However, studies that aim to facilitate medical tasks with human visual attention are still scarce. To support the use of visual attention, this paper describes a novel deep learning model for visual saliency prediction on chest X-ray (CXR) images. To cope with data deficiency, we exploit the multi-task learning method and tackles disease classification on CXR simultaneously. For a more robust training process, we propose a further optimized multi-task learning scheme to better handle model overfitting. Experiments show our proposed deep learning model with our new learning scheme can outperform existing methods dedicated either for saliency prediction or image classification. The code used in this paper is available at https://github.com/hz-zhu/MT-UNet.



### Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework
- **Arxiv ID**: http://arxiv.org/abs/2202.07123v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.07123v2)
- **Published**: 2022-02-15 01:39:07+00:00
- **Updated**: 2022-11-29 07:07:37+00:00
- **Authors**: Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, Yun Fu
- **Comment**: Accepted by ICLR 2022. Codes are made publically available at
  https://github.com/ma-xu/pointMLP-pytorch; updated some errors
- **Journal**: None
- **Summary**: Point cloud analysis is challenging due to irregularity and unordered data structure. To capture the 3D geometries, prior works mainly rely on exploring sophisticated local geometric extractors using convolution, graph, or attention mechanisms. These methods, however, incur unfavorable latency during inference, and the performance saturates over the past few years. In this paper, we present a novel perspective on this task. We notice that detailed local geometrical information probably is not the key to point cloud analysis -- we introduce a pure residual MLP network, called PointMLP, which integrates no sophisticated local geometrical extractors but still performs very competitively. Equipped with a proposed lightweight geometric affine module, PointMLP delivers the new state-of-the-art on multiple datasets. On the real-world ScanObjectNN dataset, our method even surpasses the prior best method by 3.3% accuracy. We emphasize that PointMLP achieves this strong performance without any sophisticated operations, hence leading to a superior inference speed. Compared to most recent CurveNet, PointMLP trains 2x faster, tests 7x faster, and is more accurate on ModelNet40 benchmark. We hope our PointMLP may help the community towards a better understanding of point cloud analysis. The code is available at https://github.com/ma-xu/pointMLP-pytorch.



### Sim-to-Real Domain Adaptation for Lane Detection and Classification in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2202.07133v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07133v2)
- **Published**: 2022-02-15 02:10:14+00:00
- **Updated**: 2022-05-30 12:12:24+00:00
- **Authors**: Chuqing Hu, Sinclair Hudson, Martin Ethier, Mohammad Al-Sharman, Derek Rayside, William Melek
- **Comment**: Accepted by IV 2022
- **Journal**: None
- **Summary**: While supervised detection and classification frameworks in autonomous driving require large labelled datasets to converge, Unsupervised Domain Adaptation (UDA) approaches, facilitated by synthetic data generated from photo-real simulated environments, are considered low-cost and less time-consuming solutions. In this paper, we propose UDA schemes using adversarial discriminative and generative methods for lane detection and classification applications in autonomous driving. We also present Simulanes dataset generator to create a synthetic dataset that is naturalistic utilizing CARLA's vast traffic scenarios and weather conditions. The proposed UDA frameworks take the synthesized dataset with labels as the source domain, whereas the target domain is the unlabelled real-world data. Using adversarial generative and feature discriminators, the learnt models are tuned to predict the lane location and class in the target domain. The proposed techniques are evaluated using both real-world and our synthetic datasets. The results manifest that the proposed methods have shown superiority over other baseline schemes in terms of detection and classification accuracy and consistency. The ablation study reveals that the size of the simulation dataset plays important roles in the classification performance of the proposed methods. Our UDA frameworks are available at https://github.com/anita-hu/sim2real-lane-detection and our dataset generator is released at https://github.com/anita-hu/simulanes



### Compositional Scene Representation Learning via Reconstruction: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2202.07135v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07135v4)
- **Published**: 2022-02-15 02:14:05+00:00
- **Updated**: 2023-06-14 16:25:03+00:00
- **Authors**: Jinyang Yuan, Tonglin Chen, Bin Li, Xiangyang Xue
- **Comment**: None
- **Journal**: None
- **Summary**: Visual scenes are composed of visual concepts and have the property of combinatorial explosion. An important reason for humans to efficiently learn from diverse visual scenes is the ability of compositional perception, and it is desirable for artificial intelligence to have similar abilities. Compositional scene representation learning is a task that enables such abilities. In recent years, various methods have been proposed to apply deep neural networks, which have been proven to be advantageous in representation learning, to learn compositional scene representations via reconstruction, advancing this research direction into the deep learning era. Learning via reconstruction is advantageous because it may utilize massive unlabeled data and avoid costly and laborious data annotation. In this survey, we first outline the current progress on reconstruction-based compositional scene representation learning with deep neural networks, including development history and categorizations of existing methods from the perspectives of the modeling of visual scenes and the inference of scene representations; then provide benchmarks, including an open source toolbox to reproduce the benchmark experiments, of representative methods that consider the most extensively studied problem setting and form the foundation for other methods; and finally discuss the limitations of existing methods and future directions of this research topic.



### Debiased Self-Training for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.07136v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, Machine Learning
- **Links**: [PDF](http://arxiv.org/pdf/2202.07136v5)
- **Published**: 2022-02-15 02:14:33+00:00
- **Updated**: 2022-11-09 12:28:22+00:00
- **Authors**: Baixu Chen, Junguang Jiang, Ximei Wang, Pengfei Wan, Jianmin Wang, Mingsheng Long
- **Comment**: NIPS 2022 Oral
- **Journal**: None
- **Summary**: Deep neural networks achieve remarkable performances on a wide range of tasks with the aid of large-scale labeled datasets. Yet these datasets are time-consuming and labor-exhaustive to obtain on realistic tasks. To mitigate the requirement for labeled data, self-training is widely used in semi-supervised learning by iteratively assigning pseudo labels to unlabeled samples. Despite its popularity, self-training is well-believed to be unreliable and often leads to training instability. Our experimental studies further reveal that the bias in semi-supervised learning arises from both the problem itself and the inappropriate training with potentially incorrect pseudo labels, which accumulates the error in the iterative self-training process. To reduce the above bias, we propose Debiased Self-Training (DST). First, the generation and utilization of pseudo labels are decoupled by two parameter-independent classifier heads to avoid direct error accumulation. Second, we estimate the worst case of self-training bias, where the pseudo labeling function is accurate on labeled samples, yet makes as many mistakes as possible on unlabeled samples. We then adversarially optimize the representations to improve the quality of pseudo labels by avoiding the worst case. Extensive experiments justify that DST achieves an average improvement of 6.3% against state-of-the-art methods on standard semi-supervised learning benchmark datasets and 18.9%$ against FixMatch on 13 diverse tasks. Furthermore, DST can be seamlessly adapted to other self-training methods and help stabilize their training and balance performance across classes in both cases of training from scratch and finetuning from pre-trained models.



### GAN-generated Faces Detection: A Survey and New Perspectives
- **Arxiv ID**: http://arxiv.org/abs/2202.07145v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07145v5)
- **Published**: 2022-02-15 02:36:56+00:00
- **Updated**: 2023-05-04 23:07:20+00:00
- **Authors**: Xin Wang, Hui Guo, Shu Hu, Ming-Ching Chang, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GAN) have led to the generation of very realistic face images, which have been used in fake social media accounts and other disinformation matters that can generate profound impacts. Therefore, the corresponding GAN-face detection techniques are under active development that can examine and expose such fake faces. In this work, we aim to provide a comprehensive review of recent progress in GAN-face detection. We focus on methods that can detect face images that are generated or synthesized from GAN models. We classify the existing detection works into four categories: (1) deep learning-based, (2) physical-based, (3) physiological-based methods, and (4) evaluation and comparison against human visual performance. For each category, we summarize the key ideas and connect them with method implementations. We also discuss open problems and suggest future research directions.



### To what extent can Plug-and-Play methods outperform neural networks alone in low-dose CT reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2202.07173v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07173v1)
- **Published**: 2022-02-15 03:52:07+00:00
- **Updated**: 2022-02-15 03:52:07+00:00
- **Authors**: Qifan Xu, Qihui Lyu, Dan Ruan, Ke Sheng
- **Comment**: Accepted to IEEE ISBI 2022
- **Journal**: None
- **Summary**: The Plug-and-Play (PnP) framework was recently introduced for low-dose CT reconstruction to leverage the interpretability and the flexibility of model-based methods to incorporate various plugins, such as trained deep learning (DL) neural networks. However, the benefits of PnP vs. state-of-the-art DL methods have not been clearly demonstrated. In this work, we proposed an improved PnP framework to address the previous limitations and develop clinical-relevant segmentation metrics for quantitative result assessment. Compared with the DL alone methods, our proposed PnP framework was slightly inferior in MSE and PSNR. However, the power spectrum of the resulting images better matched that of full-dose images than that of DL denoised images. The resulting images supported higher accuracy in airway segmentation than DL denoised images for all the ten patients in the test set, more substantially on the airways with a cross-section smaller than 0.61cm$^2$, and outperformed the DL denoised images for 45 out of 50 lung lobes in lobar segmentation. Our PnP method proved to be significantly better at preserving the image texture, which translated to task-specific benefits in automated structure segmentation and detection.



### A Survey of Neural Trojan Attacks and Defenses in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.07183v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07183v1)
- **Published**: 2022-02-15 04:26:44+00:00
- **Updated**: 2022-02-15 04:26:44+00:00
- **Authors**: Jie Wang, Ghulam Mubashar Hassan, Naveed Akhtar
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Artificial Intelligence (AI) relies heavily on deep learning - a technology that is becoming increasingly popular in real-life applications of AI, even in the safety-critical and high-risk domains. However, it is recently discovered that deep learning can be manipulated by embedding Trojans inside it. Unfortunately, pragmatic solutions to circumvent the computational requirements of deep learning, e.g. outsourcing model training or data annotation to third parties, further add to model susceptibility to the Trojan attacks. Due to the key importance of the topic in deep learning, recent literature has seen many contributions in this direction. We conduct a comprehensive review of the techniques that devise Trojan attacks for deep learning and explore their defenses. Our informative survey systematically organizes the recent literature and discusses the key concepts of the methods while assuming minimal knowledge of the domain on the readers part. It provides a comprehensible gateway to the broader community to understand the recent developments in Neural Trojans.



### Pruning Networks with Cross-Layer Ranking & k-Reciprocal Nearest Filters
- **Arxiv ID**: http://arxiv.org/abs/2202.07190v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07190v1)
- **Published**: 2022-02-15 04:53:24+00:00
- **Updated**: 2022-02-15 04:53:24+00:00
- **Authors**: Mingbao Lin, Liujuan Cao, Yuxin Zhang, Ling Shao, Chia-Wen Lin, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: This paper focuses on filter-level network pruning. A novel pruning method, termed CLR-RNF, is proposed. We first reveal a "long-tail" long-tail pruning problem in magnitude-based weight pruning methods, and then propose a computation-aware measurement for individual weight importance, followed by a Cross-Layer Ranking (CLR) of weights to identify and remove the bottom-ranked weights. Consequently, the per-layer sparsity makes up of the pruned network structure in our filter pruning. Then, we introduce a recommendation-based filter selection scheme where each filter recommends a group of its closest filters. To pick the preserved filters from these recommended groups, we further devise a k-Reciprocal Nearest Filter (RNF) selection scheme where the selected filters fall into the intersection of these recommended groups. Both our pruned network structure and the filter selection are non-learning processes, which thus significantly reduce the pruning complexity, and differentiate our method from existing works. We conduct image classification on CIFAR-10 and ImageNet to demonstrate the superiority of our CLR-RNF over the state-of-the-arts. For example, on CIFAR-10, CLR-RNF removes 74.1% FLOPs and 95.0% parameters from VGGNet-16 with even 0.3\% accuracy improvements. On ImageNet, it removes 70.2% FLOPs and 64.8% parameters from ResNet-50 with only 1.7% top-5 accuracy drops. Our project is at https://github.com/lmbxmu/CLR-RNF.



### Improving Human Sperm Head Morphology Classification with Unsupervised Anatomical Feature Distillation
- **Arxiv ID**: http://arxiv.org/abs/2202.07191v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.07191v3)
- **Published**: 2022-02-15 04:58:29+00:00
- **Updated**: 2022-03-16 20:59:11+00:00
- **Authors**: Yejia Zhang, Jingjing Zhang, Xiaomin Zha, Yiru Zhou, Yunxia Cao, Danny Z. Chen
- **Comment**: Accepted to ISBI 2022 proceedings
- **Journal**: None
- **Summary**: With rising male infertility, sperm head morphology classification becomes critical for accurate and timely clinical diagnosis. Recent deep learning (DL) morphology analysis methods achieve promising benchmark results, but leave performance and robustness on the table by relying on limited and possibly noisy class labels. To address this, we introduce a new DL training framework that leverages anatomical and image priors from human sperm microscopy crops to extract useful features without additional labeling cost. Our core idea is to distill sperm head information with reliably-generated pseudo-masks and unsupervised spatial prediction tasks. The predicted foreground masks from this distillation step are then leveraged to regularize and reduce image and label noise in the tuning stage. We evaluate our new approach on two public sperm datasets and achieve state-of-the-art performances (e.g. 65.9% SCIAN accuracy and 96.5% HuSHeM accuracy).



### Balancing Domain Experts for Long-Tailed Camera-Trap Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.07215v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07215v2)
- **Published**: 2022-02-15 06:08:13+00:00
- **Updated**: 2022-02-16 01:41:01+00:00
- **Authors**: Byeongjun Park, Jeongsoo Kim, Seungju Cho, Heeseon Kim, Changick Kim
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Label distributions in camera-trap images are highly imbalanced and long-tailed, resulting in neural networks tending to be biased towards head-classes that appear frequently. Although long-tail learning has been extremely explored to address data imbalances, few studies have been conducted to consider camera-trap characteristics, such as multi-domain and multi-frame setup. Here, we propose a unified framework and introduce two datasets for long-tailed camera-trap recognition. We first design domain experts, where each expert learns to balance imperfect decision boundaries caused by data imbalances and complement each other to generate domain-balanced decision boundaries. Also, we propose a flow consistency loss to focus on moving objects, expecting class activation maps of multi-frame matches the flow with optical flow maps for input images. Moreover, two long-tailed camera-trap datasets, WCS-LT and DMZ-LT, are introduced to validate our methods. Experimental results show the effectiveness of our framework, and proposed methods outperform previous methods on recessive domain samples.



### Leveraging the Learnable Vertex-Vertex Relationship to Generalize Human Pose and Mesh Reconstruction for In-the-Wild Scenes
- **Arxiv ID**: http://arxiv.org/abs/2202.07228v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07228v2)
- **Published**: 2022-02-15 07:06:24+00:00
- **Updated**: 2022-11-08 05:52:05+00:00
- **Authors**: Trung Tran-Quang, Cuong Than-Cao, Hai Nguyen-Thanh, Hong Hoang Si
- **Comment**: None
- **Journal**: None
- **Summary**: We present MeshLeTemp, a powerful method for 3D human pose and mesh reconstruction from a single image. In terms of human body priors encoding, we propose using a learnable template human mesh instead of a constant template as utilized by previous state-of-the-art methods. The proposed learnable template reflects not only vertex-vertex interactions but also the human pose and body shape, being able to adapt to diverse images. We conduct extensive experiments to show the generalizability of our method on unseen scenarios.



### Few-shot semantic segmentation via mask aggregation
- **Arxiv ID**: http://arxiv.org/abs/2202.07231v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.07231v1)
- **Published**: 2022-02-15 07:13:09+00:00
- **Updated**: 2022-02-15 07:13:09+00:00
- **Authors**: Wei Ao, Shunyi Zheng, Yan Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot semantic segmentation aims to recognize novel classes with only very few labelled data. This challenging task requires mining of the relevant relationships between the query image and the support images. Previous works have typically regarded it as a pixel-wise classification problem. Therefore, various models have been designed to explore the correlation of pixels between the query image and the support images. However, they focus only on pixel-wise correspondence and ignore the overall correlation of objects. In this paper, we introduce a mask-based classification method for addressing this problem. The mask aggregation network (MANet), which is a simple mask classification model, is proposed to simultaneously generate a fixed number of masks and their probabilities of being targets. Then, the final segmentation result is obtained by aggregating all the masks according to their locations. Experiments on both the PASCAL-5^i and COCO-20^i datasets show that our method performs comparably to the state-of-the-art pixel-based methods. This competitive performance demonstrates the potential of mask classification as an alternative baseline method in few-shot semantic segmentation. Our source code will be made available at https://github.com/TinyAway/MANet.



### Neural Architecture Search for Dense Prediction Tasks in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2202.07242v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.07242v1)
- **Published**: 2022-02-15 08:06:50+00:00
- **Updated**: 2022-02-15 08:06:50+00:00
- **Authors**: Thomas Elsken, Arber Zela, Jan Hendrik Metzen, Benedikt Staffler, Thomas Brox, Abhinav Valada, Frank Hutter
- **Comment**: None
- **Journal**: None
- **Summary**: The success of deep learning in recent years has lead to a rising demand for neural network architecture engineering. As a consequence, neural architecture search (NAS), which aims at automatically designing neural network architectures in a data-driven manner rather than manually, has evolved as a popular field of research. With the advent of weight sharing strategies across architectures, NAS has become applicable to a much wider range of problems. In particular, there are now many publications for dense prediction tasks in computer vision that require pixel-level predictions, such as semantic segmentation or object detection. These tasks come with novel challenges, such as higher memory footprints due to high-resolution data, learning multi-scale representations, longer training times, and more complex and larger neural architectures. In this manuscript, we provide an overview of NAS for dense prediction tasks by elaborating on these novel challenges and surveying ways to address them to ease future research and application of existing methods to novel problems.



### CommerceMM: Large-Scale Commerce MultiModal Representation Learning with Omni Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2202.07247v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/2202.07247v1)
- **Published**: 2022-02-15 08:23:59+00:00
- **Updated**: 2022-02-15 08:23:59+00:00
- **Authors**: Licheng Yu, Jun Chen, Animesh Sinha, Mengjiao MJ Wang, Hugo Chen, Tamara L. Berg, Ning Zhang
- **Comment**: 10 pages, 7 figures. Commerce Multimodal Model towards Real
  Applications at Facebook
- **Journal**: None
- **Summary**: We introduce CommerceMM - a multimodal model capable of providing a diverse and granular understanding of commerce topics associated to the given piece of content (image, text, image+text), and having the capability to generalize to a wide range of tasks, including Multimodal Categorization, Image-Text Retrieval, Query-to-Product Retrieval, Image-to-Product Retrieval, etc. We follow the pre-training + fine-tuning training regime and present 5 effective pre-training tasks on image-text pairs. To embrace more common and diverse commerce data with text-to-multimodal, image-to-multimodal, and multimodal-to-multimodal mapping, we propose another 9 novel cross-modal and cross-pair retrieval tasks, called Omni-Retrieval pre-training. The pre-training is conducted in an efficient manner with only two forward/backward updates for the combined 14 tasks. Extensive experiments and analysis show the effectiveness of each task. When combining all pre-training tasks, our model achieves state-of-the-art performance on 7 commerce-related downstream tasks after fine-tuning. Additionally, we propose a novel approach of modality randomization to dynamically adjust our model under different efficiency constraints.



### Review of the Fingerprint Liveness Detection (LivDet) competition series: from 2009 to 2021
- **Arxiv ID**: http://arxiv.org/abs/2202.07259v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07259v1)
- **Published**: 2022-02-15 09:14:08+00:00
- **Updated**: 2022-02-15 09:14:08+00:00
- **Authors**: Marco Micheletto, Giulia Orrù, Roberto Casula, David Yambay, Gian Luca Marcialis, Stephanie C. Schuckers
- **Comment**: Chapter of the Handbook of Biometric Anti-Spoofing (Third Edition)
- **Journal**: None
- **Summary**: Fingerprint authentication systems are highly vulnerable to artificial reproductions of fingerprint, called fingerprint presentation attacks. Detecting presentation attacks is not trivial because attackers refine their replication techniques from year to year. The International Fingerprint liveness Detection Competition (LivDet), an open and well-acknowledged meeting point of academies and private companies that deal with the problem of presentation attack detection, has the goal to assess the performance of fingerprint presentation attack detection (FPAD) algorithms by using standard experimental protocols and data sets. Each LivDet edition, held biannually since 2009, is characterized by a different set of challenges against which competitors must be dealt with. The continuous increase of competitors and the noticeable decrease in error rates across competitions demonstrate a growing interest in the topic. This paper reviews the LivDet editions from 2009 to 2021 and points out their evolution over the years.



### Exploring the Devil in Graph Spectral Domain for 3D Point Cloud Attacks
- **Arxiv ID**: http://arxiv.org/abs/2202.07261v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07261v4)
- **Published**: 2022-02-15 09:16:12+00:00
- **Updated**: 2022-12-26 14:53:58+00:00
- **Authors**: Qianjiang Hu, Daizong Liu, Wei Hu
- **Comment**: None
- **Journal**: None
- **Summary**: With the maturity of depth sensors, point clouds have received increasing attention in various applications such as autonomous driving, robotics, surveillance, etc., while deep point cloud learning models have shown to be vulnerable to adversarial attacks. Existing attack methods generally add/delete points or perform point-wise perturbation over point clouds to generate adversarial examples in the data space, which may neglect the geometric characteristics of point clouds. Instead, we propose point cloud attacks from a new perspective -- Graph Spectral Domain Attack (GSDA), aiming to perturb transform coefficients in the graph spectral domain that corresponds to varying certain geometric structure. In particular, we naturally represent a point cloud over a graph, and adaptively transform the coordinates of points into the graph spectral domain via graph Fourier transform (GFT) for compact representation. We then analyze the influence of different spectral bands on the geometric structure of the point cloud, based on which we propose to perturb the GFT coefficients in a learnable manner guided by an energy constraint loss function. Finally, the adversarial point cloud is generated by transforming the perturbed spectral representation back to the data domain via the inverse GFT (IGFT). Experimental results demonstrate the effectiveness of the proposed GSDA in terms of both imperceptibility and attack success rates under a variety of defense strategies. The code is available at https://github.com/WoodwindHu/GSDA.



### Hyper-relationship Learning Network for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2202.07271v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07271v2)
- **Published**: 2022-02-15 09:26:16+00:00
- **Updated**: 2022-03-06 10:35:22+00:00
- **Authors**: Yibing Zhan, Zhi Chen, Jun Yu, BaoSheng Yu, Dacheng Tao, Yong Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Generating informative scene graphs from images requires integrating and reasoning from various graph components, i.e., objects and relationships. However, current scene graph generation (SGG) methods, including the unbiased SGG methods, still struggle to predict informative relationships due to the lack of 1) high-level inference such as transitive inference between relationships and 2) efficient mechanisms that can incorporate all interactions of graph components. To address the issues mentioned above, we devise a hyper-relationship learning network, termed HLN, for SGG. Specifically, the proposed HLN stems from hypergraphs and two graph attention networks (GATs) are designed to infer relationships: 1) the object-relationship GAT or OR-GAT to explore interactions between objects and relationships, and 2) the hyper-relationship GAT or HR-GAT to integrate transitive inference of hyper-relationships, i.e., the sequential relationships between three objects for transitive reasoning. As a result, HLN significantly improves the performance of scene graph generation by integrating and reasoning from object interactions, relationship interactions, and transitive inference of hyper-relationships. We evaluate HLN on the most popular SGG dataset, i.e., the Visual Genome dataset, and the experimental results demonstrate its great superiority over recent state-of-the-art methods. For example, the proposed HLN improves the recall per relationship from 11.3\% to 13.1\%, and maintains the recall per image from 19.8\% to 34.9\%. We will release the source code and pretrained models on GitHub.



### Exploring Discontinuity for Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2202.07291v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07291v5)
- **Published**: 2022-02-15 10:17:02+00:00
- **Updated**: 2023-03-23 04:44:42+00:00
- **Authors**: Sangjin Lee, Hyeongmin Lee, Chajin Shin, Hanbin Son, Sangyoun Lee
- **Comment**: highlight at CVPR 2023 (10% of accepted papers)
- **Journal**: None
- **Summary**: Video frame interpolation (VFI) is the task that synthesizes the intermediate frame given two consecutive frames. Most of the previous studies have focused on appropriate frame warping operations and refinement modules for the warped frames. These studies have been conducted on natural videos containing only continuous motions. However, many practical videos contain various unnatural objects with discontinuous motions such as logos, user interfaces and subtitles. We propose three techniques to make the existing deep learning-based VFI architectures robust to these elements. First is a novel data augmentation strategy called figure-text mixing (FTM) which can make the models learn discontinuous motions during training stage without any extra dataset. Second, we propose a simple but effective module that predicts a map called discontinuity map (D-map), which densely distinguishes between areas of continuous and discontinuous motions. Lastly, we propose loss functions to give supervisions of the discontinuous motion areas which can be applied along with FTM and D-map. We additionally collect a special test benchmark called Graphical Discontinuous Motion (GDM) dataset consisting of some mobile games and chatting videos. Applied to the various state-of-the-art VFI networks, our method significantly improves the interpolation qualities on the videos from not only GDM dataset, but also the existing benchmarks containing only continuous motions such as Vimeo90K, UCF101, and DAVIS.



### ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer
- **Arxiv ID**: http://arxiv.org/abs/2202.07305v2
- **DOI**: 10.1145/3487553.3524649
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2202.07305v2)
- **Published**: 2022-02-15 10:53:08+00:00
- **Updated**: 2022-04-07 06:33:19+00:00
- **Authors**: Kohei Uehara, Yusuke Mori, Yusuke Mukuta, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: Image narrative generation is a task to create a story from an image with a subjective viewpoint. Given the importance of the subjective feelings of writers, readers, and characters in storytelling, an image narrative generation method should consider human emotion. In this study, we propose a novel method of image narrative generation called ViNTER (Visual Narrative Transformer with Emotion arc Representation), which takes "emotion arc" as input to capture a sequence of emotional changes. Since emotion arcs represent the trajectory of emotional change, it is expected that we can include detailed information about the emotional changes in the story to the model. We present experimental results of both automatic and manual evaluations on the Image Narrative dataset and demonstrate the effectiveness of the proposed approach.



### HAA4D: Few-Shot Human Atomic Action Recognition via 3D Spatio-Temporal Skeletal Alignment
- **Arxiv ID**: http://arxiv.org/abs/2202.07308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07308v1)
- **Published**: 2022-02-15 10:55:21+00:00
- **Updated**: 2022-02-15 10:55:21+00:00
- **Authors**: Mu-Ruei Tseng, Abhishek Gupta, Chi-Keung Tang, Yu-Wing Tai
- **Comment**: None
- **Journal**: None
- **Summary**: Human actions involve complex pose variations and their 2D projections can be highly ambiguous. Thus 3D spatio-temporal or 4D (i.e., 3D+T) human skeletons, which are photometric and viewpoint invariant, are an excellent alternative to 2D+T skeletons/pixels to improve action recognition accuracy. This paper proposes a new 4D dataset HAA4D which consists of more than 3,300 RGB videos in 300 human atomic action classes. HAA4D is clean, diverse, class-balanced where each class is viewpoint-balanced with the use of 4D skeletons, in which as few as one 4D skeleton per class is sufficient for training a deep recognition model. Further, the choice of atomic actions makes annotation even easier, because each video clip lasts for only a few seconds. All training and testing 3D skeletons in HAA4D are globally aligned, using a deep alignment model to the same global space, making each skeleton face the negative z-direction. Such alignment makes matching skeletons more stable by reducing intraclass variations and thus with fewer training samples per class needed for action recognition. Given the high diversity and skeletal alignment in HAA4D, we construct the first baseline few-shot 4D human atomic action recognition network without bells and whistles, which produces comparable or higher performance than relevant state-of-the-art techniques relying on embedded space encoding without explicit skeletal alignment, using the same small number of training samples of unseen classes.



### Using Social Media Images for Building Function Classification
- **Arxiv ID**: http://arxiv.org/abs/2202.07315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07315v1)
- **Published**: 2022-02-15 11:05:10+00:00
- **Updated**: 2022-02-15 11:05:10+00:00
- **Authors**: Eike Jens Hoffmann, Karam Abdulahhad, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Urban land use on a building instance level is crucial geo-information for many applications, yet difficult to obtain. An intuitive approach to close this gap is predicting building functions from ground level imagery. Social media image platforms contain billions of images, with a large variety of motifs including but not limited to street perspectives. To cope with this issue this study proposes a filtering pipeline to yield high quality, ground level imagery from large social media image datasets. The pipeline ensures that all resulting images have full and valid geotags with a compass direction to relate image content and spatial objects from maps.   We analyze our method on a culturally diverse social media dataset from Flickr with more than 28 million images from 42 cities around the world. The obtained dataset is then evaluated in a context of 3-classes building function classification task. The three building classes that are considered in this study are: commercial, residential, and other. Fine-tuned state-of-the-art architectures yield F1-scores of up to 0.51 on the filtered images. Our analysis shows that the performance is highly limited by the quality of the labels obtained from OpenStreetMap, as the metrics increase by 0.2 if only human validated labels are considered. Therefore, we consider these labels to be weak and publish the resulting images from our pipeline together with the buildings they are showing as a weakly labeled dataset.



### A Unified Framework for Masked and Mask-Free Face Recognition via Feature Rectification
- **Arxiv ID**: http://arxiv.org/abs/2202.07358v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07358v1)
- **Published**: 2022-02-15 12:37:59+00:00
- **Updated**: 2022-02-15 12:37:59+00:00
- **Authors**: Shaozhe Hao, Chaofeng Chen, Zhenfang Chen, Kwan-Yee K. Wong
- **Comment**: 5 pages, 4 figures, conference
- **Journal**: None
- **Summary**: Face recognition under ideal conditions is now considered a well-solved problem with advances in deep learning. Recognizing faces under occlusion, however, still remains a challenge. Existing techniques often fail to recognize faces with both the mouth and nose covered by a mask, which is now very common under the COVID-19 pandemic. Common approaches to tackle this problem include 1) discarding information from the masked regions during recognition and 2) restoring the masked regions before recognition. Very few works considered the consistency between features extracted from masked faces and from their mask-free counterparts. This resulted in models trained for recognizing masked faces often showing degraded performance on mask-free faces. In this paper, we propose a unified framework, named Face Feature Rectification Network (FFR-Net), for recognizing both masked and mask-free faces alike. We introduce rectification blocks to rectify features extracted by a state-of-the-art recognition model, in both spatial and channel dimensions, to minimize the distance between a masked face and its mask-free counterpart in the rectified feature space. Experiments show that our unified framework can learn a rectified feature space for recognizing both masked and mask-free faces effectively, achieving state-of-the-art results. Project code: https://github.com/haoosz/FFR-Net



### Multimodal Driver Referencing: A Comparison of Pointing to Objects Inside and Outside the Vehicle
- **Arxiv ID**: http://arxiv.org/abs/2202.07360v1
- **DOI**: 10.1145/3490099.3511142
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07360v1)
- **Published**: 2022-02-15 12:40:13+00:00
- **Updated**: 2022-02-15 12:40:13+00:00
- **Authors**: Abdul Rafey Aftab, Michael von der Beeck
- **Comment**: None
- **Journal**: 27th International Conference on Intelligent User Interfaces (IUI
  '22), March 22--25, 2022, Helsinki, Finland
- **Summary**: Advanced in-cabin sensing technologies, especially vision based approaches, have tremendously progressed user interaction inside the vehicle, paving the way for new applications of natural user interaction. Just as humans use multiple modes to communicate with each other, we follow an approach which is characterized by simultaneously using multiple modalities to achieve natural human-machine interaction for a specific task: pointing to or glancing towards objects inside as well as outside the vehicle for deictic references. By tracking the movements of eye-gaze, head and finger, we design a multimodal fusion architecture using a deep neural network to precisely identify the driver's referencing intent. Additionally, we use a speech command as a trigger to separate each referencing event. We observe differences in driver behavior in the two pointing use cases (i.e. for inside and outside objects), especially when analyzing the preciseness of the three modalities eye, head, and finger. We conclude that there is no single modality that is solely optimal for all cases as each modality reveals certain limitations. Fusion of multiple modalities exploits the relevant characteristics of each modality, hence overcoming the case dependent limitations of each individual modality. Ultimately, we propose a method to identity whether the driver's referenced object lies inside or outside the vehicle, based on the predicted pointing direction.



### Deep Learning-based Anomaly Detection on X-ray Images of Fuel Cell Electrodes
- **Arxiv ID**: http://arxiv.org/abs/2202.07361v1
- **DOI**: 10.5220/0010785400003124
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07361v1)
- **Published**: 2022-02-15 12:41:31+00:00
- **Updated**: 2022-02-15 12:41:31+00:00
- **Authors**: Simon B. Jensen, Thomas B. Moeslund, Søren J. Andreasen
- **Comment**: 10 pages, 9 figures, VISAPP2022
- **Journal**: Proceedings of the 17th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications - Volume 4:
  VISAPP 2022
- **Summary**: Anomaly detection in X-ray images has been an active and lasting research area in the last decades, especially in the domain of medical X-ray images. For this work, we created a real-world labeled anomaly dataset, consisting of 16-bit X-ray image data of fuel cell electrodes coated with a platinum catalyst solution and perform anomaly detection on the dataset using a deep learning approach. The dataset contains a diverse set of anomalies with 11 identified common anomalies where the electrodes contain e.g. scratches, bubbles, smudges etc. We experiment with 16-bit image to 8-bit image conversion methods to utilize pre-trained Convolutional Neural Networks as feature extractors (transfer learning) and find that we achieve the best performance by maximizing the contrasts globally across the dataset during the 16-bit to 8-bit conversion, through histogram equalization. We group the fuel cell electrodes with anomalies into a single class called abnormal and the normal fuel cell electrodes into a class called normal, thereby abstracting the anomaly detection problem into a binary classification problem. We achieve a balanced accuracy of 85.18\%. The anomaly detection is used by the company, Serenergy, for optimizing the time spend on the quality control of the fuel cell electrodes



### SODAR: Segmenting Objects by DynamicallyAggregating Neighboring Mask Representations
- **Arxiv ID**: http://arxiv.org/abs/2202.07402v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07402v2)
- **Published**: 2022-02-15 13:53:03+00:00
- **Updated**: 2022-12-23 13:58:40+00:00
- **Authors**: Tao Wang, Jun Hao Liew, Yu Li, Yunpeng Chen, Jiashi Feng
- **Comment**: accepted to IEEE Transactions on Image Processing (TIP), code:
  https://github.com/advdfacd/AggMask
- **Journal**: None
- **Summary**: Recent state-of-the-art one-stage instance segmentation model SOLO divides the input image into a grid and directly predicts per grid cell object masks with fully-convolutional networks, yielding comparably good performance as traditional two-stage Mask R-CNN yet enjoying much simpler architecture and higher efficiency. We observe SOLO generates similar masks for an object at nearby grid cells, and these neighboring predictions can complement each other as some may better segment certain object part, most of which are however directly discarded by non-maximum-suppression. Motivated by the observed gap, we develop a novel learning-based aggregation method that improves upon SOLO by leveraging the rich neighboring information while maintaining the architectural efficiency. The resulting model is named SODAR. Unlike the original per grid cell object masks, SODAR is implicitly supervised to learn mask representations that encode geometric structure of nearby objects and complement adjacent representations with context. The aggregation method further includes two novel designs: 1) a mask interpolation mechanism that enables the model to generate much fewer mask representations by sharing neighboring representations among nearby grid cells, and thus saves computation and memory; 2) a deformable neighbour sampling mechanism that allows the model to adaptively adjust neighbor sampling locations thus gathering mask representations with more relevant context and achieving higher performance. SODAR significantly improves the instance segmentation performance, e.g., it outperforms a SOLO model with ResNet-101 backbone by 2.2 AP on COCO \texttt{test} set, with only about 3\% additional computation. We further show consistent performance gain with the SOLOv2 model.



### A precortical module for robust CNNs to light variations
- **Arxiv ID**: http://arxiv.org/abs/2202.07432v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/2202.07432v2)
- **Published**: 2022-02-15 14:18:40+00:00
- **Updated**: 2022-02-21 17:57:02+00:00
- **Authors**: R. Fioresi, J. Petkovic
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple mathematical model for the mammalian low visual pathway, taking into account its key elements: retina, lateral geniculate nucleus (LGN), primary visual cortex (V1). The analogies between the cortical level of the visual system and the structure of popular CNNs, used in image classification tasks, suggests the introduction of an additional preliminary convolutional module inspired to precortical neuronal circuits to improve robustness with respect to global light intensity and contrast variations in the input images. We validate our hypothesis on the popular databases MNIST, FashionMNIST and SVHN, obtaining significantly more robust CNNs with respect to these variations, once such extra module is added.



### Random Walks for Adversarial Meshes
- **Arxiv ID**: http://arxiv.org/abs/2202.07453v2
- **DOI**: 10.1145/3528233.3530710
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07453v2)
- **Published**: 2022-02-15 14:31:17+00:00
- **Updated**: 2022-05-26 12:14:18+00:00
- **Authors**: Amir Belder, Gal Yefet, Ran Ben Izhak, Ayellet Tal
- **Comment**: None
- **Journal**: None
- **Summary**: A polygonal mesh is the most-commonly used representation of surfaces in computer graphics. Therefore, it is not surprising that a number of mesh classification networks have recently been proposed. However, while adversarial attacks are wildly researched in 2D, the field of adversarial meshes is under explored. This paper proposes a novel, unified, and general adversarial attack, which leads to misclassification of several state-of-the-art mesh classification neural networks. Our attack approach is black-box, i.e. it has access only to the network's predictions, but not to the network's full architecture or gradients. The key idea is to train a network to imitate a given classification network. This is done by utilizing random walks along the mesh surface, which gather geometric information. These walks provide insight onto the regions of the mesh that are important for the correct prediction of the given classification network. These mesh regions are then modified more than other regions in order to attack the network in a manner that is barely visible to the naked eye.



### DualConv: Dual Convolutional Kernels for Lightweight Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.07481v1
- **DOI**: 10.1109/TNNLS.2022.3151138
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.07481v1)
- **Published**: 2022-02-15 14:47:13+00:00
- **Updated**: 2022-02-15 14:47:13+00:00
- **Authors**: Jiachen Zhong, Junying Chen, Ajmal Mian
- **Comment**: Accepted for publication in IEEE TNNLS
- **Journal**: None
- **Summary**: CNN architectures are generally heavy on memory and computational requirements which makes them infeasible for embedded systems with limited hardware resources. We propose dual convolutional kernels (DualConv) for constructing lightweight deep neural networks. DualConv combines 3$\times$3 and 1$\times$1 convolutional kernels to process the same input feature map channels simultaneously and exploits the group convolution technique to efficiently arrange convolutional filters. DualConv can be employed in any CNN model such as VGG-16 and ResNet-50 for image classification, YOLO and R-CNN for object detection, or FCN for semantic segmentation. In this paper, we extensively test DualConv for classification since these network architectures form the backbones for many other tasks. We also test DualConv for image detection on YOLO-V3. Experimental results show that, combined with our structural innovations, DualConv significantly reduces the computational cost and number of parameters of deep neural networks while surprisingly achieving slightly higher accuracy than the original models in some cases. We use DualConv to further reduce the number of parameters of the lightweight MobileNetV2 by 54% with only 0.68% drop in accuracy on CIFAR-100 dataset. When the number of parameters is not an issue, DualConv increases the accuracy of MobileNetV1 by 4.11% on the same dataset. Furthermore, DualConv significantly improves the YOLO-V3 object detection speed and improves its accuracy by 4.4% on PASCAL VOC dataset.



### Texture Aware Autoencoder Pre-training And Pairwise Learning Refinement For Improved Iris Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.07499v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07499v1)
- **Published**: 2022-02-15 15:12:31+00:00
- **Updated**: 2022-02-15 15:12:31+00:00
- **Authors**: Manashi Chakraborty, Aritri Chakraborty, Prabir Kumar Biswas, Pabitra Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a texture aware end-to-end trainable iris recognition system, specifically designed for datasets like iris having limited training data. We build upon our previous stagewise learning framework with certain key optimization and architectural innovations. First, we pretrain a Stage-1 encoder network with an unsupervised autoencoder learning optimized with an additional data relation loss on top of usual reconstruction loss. The data relation loss enables learning better texture representation which is pivotal for a texture rich dataset such as iris. Robustness of Stage-1 feature representation is further enhanced with an auxiliary denoising task. Such pre-training proves beneficial for effectively training deep networks on data constrained iris datasets. Next, in Stage-2 supervised refinement, we design a pairwise learning architecture for an end-to-end trainable iris recognition system. The pairwise learning includes the task of iris matching inside the training pipeline itself and results in significant improvement in recognition performance compared to usual offline matching. We validate our model across three publicly available iris datasets and the proposed model consistently outperforms both traditional and deep learning baselines for both Within-Dataset and Cross-Dataset configurations



### Self-Supervised Class-Cognizant Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2202.08149v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.08149v1)
- **Published**: 2022-02-15 15:28:06+00:00
- **Updated**: 2022-02-15 15:28:06+00:00
- **Authors**: Ojas Kishore Shirekar, Hadi Jamali-Rad
- **Comment**: 7 pages, 1 figure
- **Journal**: None
- **Summary**: Unsupervised learning is argued to be the dark matter of human intelligence. To build in this direction, this paper focuses on unsupervised learning from an abundance of unlabeled data followed by few-shot fine-tuning on a downstream classification task. To this aim, we extend a recent study on adopting contrastive learning for self-supervised pre-training by incorporating class-level cognizance through iterative clustering and re-ranking and by expanding the contrastive optimization loss to account for it. To our knowledge, our experimentation both in standard and cross-domain scenarios demonstrate that we set a new state-of-the-art (SoTA) in (5-way, 1 and 5-shot) settings of standard mini-ImageNet benchmark as well as the (5-way, 5 and 20-shot) settings of cross-domain CDFSL benchmark. Our code and experimentation can be found in our GitHub repository: https://github.com/ojss/c3lr.



### Deep Constrained Least Squares for Blind Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2202.07508v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07508v3)
- **Published**: 2022-02-15 15:32:11+00:00
- **Updated**: 2022-03-25 14:24:10+00:00
- **Authors**: Ziwei Luo, Haibin Huang, Lei Yu, Youwei Li, Haoqiang Fan, Shuaicheng Liu
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of blind image super-resolution(SR) with a reformulated degradation model and two novel modules. Following the common practices of blind SR, our method proposes to improve both the kernel estimation as well as the kernel-based high-resolution image restoration. To be more specific, we first reformulate the degradation model such that the deblurring kernel estimation can be transferred into the low-resolution space. On top of this, we introduce a dynamic deep linear filter module. Instead of learning a fixed kernel for all images, it can adaptively generate deblurring kernel weights conditional on the input and yield a more robust kernel estimation. Subsequently, a deep constrained least square filtering module is applied to generate clean features based on the reformulation and estimated kernel. The deblurred feature and the low input image feature are then fed into a dual-path structured SR network and restore the final high-resolution result. To evaluate our method, we further conduct evaluations on several benchmarks, including Gaussian8 and DIV2KRK. Our experiments demonstrate that the proposed method achieves better accuracy and visual improvements against state-of-the-art methods.



### Post-Training Quantization for Cross-Platform Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2202.07513v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07513v2)
- **Published**: 2022-02-15 15:41:12+00:00
- **Updated**: 2022-11-30 08:24:47+00:00
- **Authors**: Dailan He, Ziming Yang, Yuan Chen, Qi Zhang, Hongwei Qin, Yan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: It has been witnessed that learned image compression has outperformed conventional image coding techniques and tends to be practical in industrial applications. One of the most critical issues that need to be considered is the non-deterministic calculation, which makes the probability prediction cross-platform inconsistent and frustrates successful decoding. We propose to solve this problem by introducing well-developed post-training quantization and making the model inference integer-arithmetic-only, which is much simpler than presently existing training and fine-tuning based approaches yet still keeps the superior rate-distortion performance of learned image compression. Based on that, we further improve the discretization of the entropy parameters and extend the deterministic inference to fit Gaussian mixture models. With our proposed methods, the current state-of-the-art image compression models can infer in a cross-platform consistent manner, which makes the further development and practice of learned image compression more promising.



### Label fusion and training methods for reliable representation of inter-rater uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2202.07550v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07550v3)
- **Published**: 2022-02-15 16:35:47+00:00
- **Updated**: 2023-01-12 02:39:42+00:00
- **Authors**: Andreanne Lemay, Charley Gros, Enamundram Naga Karthik, Julien Cohen-Adad
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://www.melba-journal.org/papers/2022:031.html
- **Journal**: Machine Learning for Biomedical Imaging. 1 (2022)
- **Summary**: Medical tasks are prone to inter-rater variability due to multiple factors such as image quality, professional experience and training, or guideline clarity. Training deep learning networks with annotations from multiple raters is a common practice that mitigates the model's bias towards a single expert. Reliable models generating calibrated outputs and reflecting the inter-rater disagreement are key to the integration of artificial intelligence in clinical practice. Various methods exist to take into account different expert labels. We focus on comparing three label fusion methods: STAPLE, average of the rater's segmentation, and random sampling of each rater's segmentation during training. Each label fusion method is studied using both the conventional training framework and the recently published SoftSeg framework that limits information loss by treating the segmentation task as a regression. Our results, across 10 data splittings on two public datasets, indicate that SoftSeg models, regardless of the ground truth fusion method, had better calibration and preservation of the inter-rater rater variability compared with their conventional counterparts without impacting the segmentation performance. Conventional models, i.e., trained with a Dice loss, with binary inputs, and sigmoid/softmax final activate, were overconfident and underestimated the uncertainty associated with inter-rater variability. Conversely, fusing labels by averaging with the SoftSeg framework led to underconfident outputs and overestimation of the rater disagreement. In terms of segmentation performance, the best label fusion method was different for the two datasets studied, indicating this parameter might be task-dependent. However, SoftSeg had segmentation performance systematically superior or equal to the conventionally trained models and had the best calibration and preservation of the inter-rater variability.



### Improving the repeatability of deep learning models with Monte Carlo dropout
- **Arxiv ID**: http://arxiv.org/abs/2202.07562v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07562v1)
- **Published**: 2022-02-15 16:46:44+00:00
- **Updated**: 2022-02-15 16:46:44+00:00
- **Authors**: Andreanne Lemay, Katharina Hoebel, Christopher P. Bridge, Brian Befano, Silvia De Sanjosé, Diden Egemen, Ana Cecilia Rodriguez, Mark Schiffman, John Peter Campbell, Jayashree Kalpathy-Cramer
- **Comment**: arXiv admin note: text overlap with arXiv:2111.06754
- **Journal**: None
- **Summary**: The integration of artificial intelligence into clinical workflows requires reliable and robust models. Repeatability is a key attribute of model robustness. Repeatable models output predictions with low variation during independent tests carried out under similar conditions. During model development and evaluation, much attention is given to classification performance while model repeatability is rarely assessed, leading to the development of models that are unusable in clinical practice. In this work, we evaluate the repeatability of four model types (binary classification, multi-class classification, ordinal classification, and regression) on images that were acquired from the same patient during the same visit. We study the performance of binary, multi-class, ordinal, and regression models on four medical image classification tasks from public and private datasets: knee osteoarthritis, cervical cancer screening, breast density estimation, and retinopathy of prematurity. Repeatability is measured and compared on ResNet and DenseNet architectures. Moreover, we assess the impact of sampling Monte Carlo dropout predictions at test time on classification performance and repeatability. Leveraging Monte Carlo predictions significantly increased repeatability for all tasks on the binary, multi-class, and ordinal models leading to an average reduction of the 95\% limits of agreement by 16% points and of the disagreement rate by 7% points. The classification accuracy improved in most settings along with the repeatability. Our results suggest that beyond about 20 Monte Carlo iterations, there is no further gain in repeatability. In addition to the higher test-retest agreement, Monte Carlo predictions were better calibrated which leads to output probabilities reflecting more accurately the true likelihood of being correctly classified.



### ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2202.07570v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07570v3)
- **Published**: 2022-02-15 16:55:09+00:00
- **Updated**: 2022-07-18 12:47:39+00:00
- **Authors**: Thomas Stegmüller, Behzad Bozorgtabar, Antoine Spahr, Jean-Philippe Thiran
- **Comment**: 19 pages, 7 figures
- **Journal**: None
- **Summary**: Progress in digital pathology is hindered by high-resolution images and the prohibitive cost of exhaustive localized annotations. The commonly used paradigm to categorize pathology images is patch-based processing, which often incorporates multiple instance learning (MIL) to aggregate local patch-level representations yielding image-level prediction. Nonetheless, diagnostically relevant regions may only take a small fraction of the whole tissue, and current MIL-based approaches often process images uniformly, discarding the inter-patches interactions. To alleviate these issues, we propose ScoreNet, a new efficient transformer that exploits a differentiable recommendation stage to extract discriminative image regions and dedicate computational resources accordingly. The proposed transformer leverages the local and global attention of a few dynamically recommended high-resolution regions at an efficient computational cost. We further introduce a novel mixing data-augmentation, namely ScoreMix, by leveraging the image's semantic distribution to guide the data mixing and produce coherent sample-label pairs. ScoreMix is embarrassingly simple and mitigates the pitfalls of previous augmentations, which assume a uniform semantic distribution and risk mislabeling the samples. Thorough experiments and ablation studies on three breast cancer histology datasets of Haematoxylin & Eosin (H&E) have validated the superiority of our approach over prior arts, including transformer-based models on tumour regions-of-interest (TRoIs) classification. ScoreNet equipped with proposed ScoreMix augmentation demonstrates better generalization capabilities and achieves new state-of-the-art (SOTA) results with only 50% of the data compared to other mixing augmentation variants. Finally, ScoreNet yields high efficacy and outperforms SOTA efficient transformers, namely TransPath and SwinTransformer.



### On Representation Learning with Feedback
- **Arxiv ID**: http://arxiv.org/abs/2202.07572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07572v1)
- **Published**: 2022-02-15 16:58:49+00:00
- **Updated**: 2022-02-15 16:58:49+00:00
- **Authors**: Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: This note complements the author's recent paper "Robust representation learning with feedback for single image deraining" by providing heuristically theoretical explanations on the mechanism of representation learning with feedback, namely an essential merit of the works presented in this recent article. This note facilitates understanding of key points in the mechanism of representation learning with feedback.



### Fairness Indicators for Systematic Assessments of Visual Feature Extractors
- **Arxiv ID**: http://arxiv.org/abs/2202.07603v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2202.07603v1)
- **Published**: 2022-02-15 17:45:33+00:00
- **Updated**: 2022-02-15 17:45:33+00:00
- **Authors**: Priya Goyal, Adriana Romero Soriano, Caner Hazirbas, Levent Sagun, Nicolas Usunier
- **Comment**: None
- **Journal**: None
- **Summary**: Does everyone equally benefit from computer vision systems? Answers to this question become more and more important as computer vision systems are deployed at large scale, and can spark major concerns when they exhibit vast performance discrepancies between people from various demographic and social backgrounds. Systematic diagnosis of fairness, harms, and biases of computer vision systems is an important step towards building socially responsible systems. To initiate an effort towards standardized fairness audits, we propose three fairness indicators, which aim at quantifying harms and biases of visual systems. Our indicators use existing publicly available datasets collected for fairness evaluations, and focus on three main types of harms and bias identified in the literature, namely harmful label associations, disparity in learned representations of social and demographic traits, and biased performance on geographically diverse images from across the world.We define precise experimental protocols applicable to a wide range of computer vision models. These indicators are part of an ever-evolving suite of fairness probes and are not intended to be a substitute for a thorough analysis of the broader impact of the new computer vision technologies. Yet, we believe it is a necessary first step towards (1) facilitating the widespread adoption and mandate of the fairness assessments in computer vision research, and (2) tracking progress towards building socially responsible models. To study the practical effectiveness and broad applicability of our proposed indicators to any visual system, we apply them to off-the-shelf models built using widely adopted model training paradigms which vary in their ability to whether they can predict labels on a given image or only produce the embeddings. We also systematically study the effect of data domain and model size.



### Energy-Efficient Parking Analytics System using Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.08973v2
- **DOI**: 10.1145/3486611.3486660
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.08973v2)
- **Published**: 2022-02-15 18:27:50+00:00
- **Updated**: 2022-04-20 20:53:27+00:00
- **Authors**: Yoones Rezaei, Stephen Lee, Daniel Mosse
- **Comment**: None
- **Journal**: Proceedings of the 8th ACM International Conference on Systems for
  Energy-Efficient Buildings, Cities, and Transportation November 2021 Pages
  81-90
- **Summary**: Advances in deep vision techniques and ubiquity of smart cameras will drive the next generation of video analytics. However, video analytics applications consume vast amounts of energy as both deep learning techniques and cameras are power-hungry. In this paper, we focus on a parking video analytics platform and propose RL-CamSleep, a deep reinforcement learning-based technique, to actuate the cameras to reduce the energy footprint while retaining the system's utility. Our key insight is that many video-analytics applications do not always need to be operational, and we can design policies to activate video analytics only when necessary. Moreover, our work is complementary to existing work that focuses on improving hardware and software efficiency. We evaluate our approach on a city-scale parking dataset having 76 streets spread across the city. Our analysis demonstrates how streets have various parking patterns, highlighting the importance of an adaptive policy. Our approach can learn such an adaptive policy that can reduce the average energy consumption by 76.38% and achieve an average accuracy of more than 98% in performing video analytics.



### Lie Point Symmetry Data Augmentation for Neural PDE Solvers
- **Arxiv ID**: http://arxiv.org/abs/2202.07643v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07643v2)
- **Published**: 2022-02-15 18:43:17+00:00
- **Updated**: 2022-05-29 13:02:02+00:00
- **Authors**: Johannes Brandstetter, Max Welling, Daniel E. Worrall
- **Comment**: Published at ICML 2022, Github:
  https://github.com/brandstetter-johannes/LPSDA
- **Journal**: None
- **Summary**: Neural networks are increasingly being used to solve partial differential equations (PDEs), replacing slower numerical solvers. However, a critical issue is that neural PDE solvers require high-quality ground truth data, which usually must come from the very solvers they are designed to replace. Thus, we are presented with a proverbial chicken-and-egg problem. In this paper, we present a method, which can partially alleviate this problem, by improving neural PDE solver sample complexity -- Lie point symmetry data augmentation (LPSDA). In the context of PDEs, it turns out that we are able to quantitatively derive an exhaustive list of data transformations, based on the Lie point symmetry group of the PDEs in question, something not possible in other application areas. We present this framework and demonstrate how it can easily be deployed to improve neural PDE solver sample complexity by an order of magnitude.



### Misinformation Detection in Social Media Video Posts
- **Arxiv ID**: http://arxiv.org/abs/2202.07706v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07706v2)
- **Published**: 2022-02-15 20:14:54+00:00
- **Updated**: 2022-07-31 00:50:37+00:00
- **Authors**: Kehan Wang, David Chan, Seth Z. Zhao, John Canny, Avideh Zakhor
- **Comment**: We discovered an error in our dataset construction where retweets
  were not properly filtered. This resulted in test data leakage in training
  data, and the results reported are affected
- **Journal**: None
- **Summary**: With the growing adoption of short-form video by social media platforms, reducing the spread of misinformation through video posts has become a critical challenge for social media providers. In this paper, we develop methods to detect misinformation in social media posts, exploiting modalities such as video and text. Due to the lack of large-scale public data for misinformation detection in multi-modal datasets, we collect 160,000 video posts from Twitter, and leverage self-supervised learning to learn expressive representations of joint visual and textual data. In this work, we propose two new methods for detecting semantic inconsistencies within short-form social media video posts, based on contrastive learning and masked language modeling. We demonstrate that our new approaches outperform current state-of-the-art methods on both artificial data generated by random-swapping of positive samples and in the wild on a new manually-labeled test set for semantic misinformation.



### Privacy Preserving Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2202.07712v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07712v1)
- **Published**: 2022-02-15 20:22:50+00:00
- **Updated**: 2022-02-15 20:22:50+00:00
- **Authors**: Cristian-Paul Bara, Qing Ping, Abhinav Mathur, Govind Thattai, Rohith MV, Gaurav S. Sukhatme
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel privacy-preserving methodology for performing Visual Question Answering on the edge. Our method constructs a symbolic representation of the visual scene, using a low-complexity computer vision model that jointly predicts classes, attributes and predicates. This symbolic representation is non-differentiable, which means it cannot be used to recover the original image, thereby keeping the original image private. Our proposed hybrid solution uses a vision model which is more than 25 times smaller than the current state-of-the-art (SOTA) vision models, and 100 times smaller than end-to-end SOTA VQA models. We report detailed error analysis and discuss the trade-offs of using a distilled vision model and a symbolic representation of the visual scene.



### A Subjective Quality Study for Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2202.07727v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07727v2)
- **Published**: 2022-02-15 21:13:23+00:00
- **Updated**: 2023-06-22 12:56:35+00:00
- **Authors**: Duolikun Danier, Fan Zhang, David Bull
- **Comment**: None
- **Journal**: None
- **Summary**: Video frame interpolation (VFI) is one of the fundamental research areas in video processing and there has been extensive research on novel and enhanced interpolation algorithms. The same is not true for quality assessment of the interpolated content. In this paper, we describe a subjective quality study for VFI based on a newly developed video database, BVI-VFI. BVI-VFI contains 36 reference sequences at three different frame rates and 180 distorted videos generated using five conventional and learning based VFI algorithms. Subjective opinion scores have been collected from 60 human participants, and then employed to evaluate eight popular quality metrics, including PSNR, SSIM and LPIPS which are all commonly used for assessing VFI methods. The results indicate that none of these metrics provide acceptable correlation with the perceived quality on interpolated content, with the best-performing metric, LPIPS, offering a SROCC value below 0.6. Our findings show that there is an urgent need to develop a bespoke perceptual quality metric for VFI. The BVI-VFI dataset is publicly available and can be accessed at https://danier97.github.io/BVI-VFI/.



### Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis
- **Arxiv ID**: http://arxiv.org/abs/2202.07728v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07728v2)
- **Published**: 2022-02-15 21:13:55+00:00
- **Updated**: 2023-03-18 10:45:10+00:00
- **Authors**: Thomas Fel, Melanie Ducoffe, David Vigouroux, Remi Cadene, Mikael Capelle, Claire Nicodeme, Thomas Serre
- **Comment**: None
- **Journal**: Proceedings of the IEEE / CVF Computer Vision and Pattern
  Recognition Conference (CVPR), 2023
- **Summary**: A variety of methods have been proposed to try to explain how deep neural networks make their decisions. Key to those approaches is the need to sample the pixel space efficiently in order to derive importance maps. However, it has been shown that the sampling methods used to date introduce biases and other artifacts, leading to inaccurate estimates of the importance of individual pixels and severely limit the reliability of current explainability methods. Unfortunately, the alternative -- to exhaustively sample the image space is computationally prohibitive. In this paper, we introduce EVA (Explaining using Verified perturbation Analysis) -- the first explainability method guarantee to have an exhaustive exploration of a perturbation space. Specifically, we leverage the beneficial properties of verified perturbation analysis -- time efficiency, tractability and guaranteed complete coverage of a manifold -- to efficiently characterize the input variables that are most likely to drive the model decision. We evaluate the approach systematically and demonstrate state-of-the-art results on multiple benchmarks.



### Enhancing Deformable Convolution based Video Frame Interpolation with Coarse-to-fine 3D CNN
- **Arxiv ID**: http://arxiv.org/abs/2202.07731v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07731v2)
- **Published**: 2022-02-15 21:20:18+00:00
- **Updated**: 2023-06-22 12:58:40+00:00
- **Authors**: Duolikun Danier, Fan Zhang, David Bull
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new deformable convolution-based video frame interpolation (VFI) method, using a coarse to fine 3D CNN to enhance the multi-flow prediction. This model first extracts spatio-temporal features at multiple scales using a 3D CNN, and estimates multi-flows using these features in a coarse-to-fine manner. The estimated multi-flows are then used to warp the original input frames as well as context maps, and the warped results are fused by a synthesis network to produce the final output. This VFI approach has been fully evaluated against 12 state-of-the-art VFI methods on three commonly used test databases. The results evidently show the effectiveness of the proposed method, which offers superior interpolation performance over other state of the art algorithms, with PSNR gains up to 0.19dB.



### Ab-initio Contrast Estimation and Denoising of Cryo-EM Images
- **Arxiv ID**: http://arxiv.org/abs/2202.07737v2
- **DOI**: None
- **Categories**: **cs.CV**, 15A29, 65D18, 62H35, 94A08
- **Links**: [PDF](http://arxiv.org/pdf/2202.07737v2)
- **Published**: 2022-02-15 21:31:34+00:00
- **Updated**: 2022-06-30 17:17:13+00:00
- **Authors**: Yunpeng Shi, Amit Singer
- **Comment**: None
- **Journal**: None
- **Summary**: Background and Objective: The contrast of cryo-EM images varies from one to another, primarily due to the uneven thickness of the ice layer. This contrast variation can affect the quality of 2-D class averaging, 3-D ab-initio modeling, and 3-D heterogeneity analysis. Contrast estimation is currently performed during 3-D iterative refinement. As a result, the estimates are not available at the earlier computational stages of class averaging and ab-initio modeling. This paper aims to solve the contrast estimation problem directly from the picked particle images in the ab-initio stage, without estimating the 3-D volume, image rotations, or class averages.   Methods: The key observation underlying our analysis is that the 2-D covariance matrix of the raw images is related to the covariance of the underlying clean images, the noise variance, and the contrast variability between images. We show that the contrast variability can be derived from the 2-D covariance matrix and we apply the existing Covariance Wiener Filtering (CWF) framework to estimate it. We also demonstrate a modification of CWF to estimate the contrast of individual images.   Results: Our method improves the contrast estimation by a large margin, compared to the previous CWF method. Its estimation accuracy is often comparable to that of an oracle that knows the ground truth covariance of the clean images. The more accurate contrast estimation also improves the quality of image restoration as demonstrated in both synthetic and experimental datasets.   Conclusions: This paper proposes an effective method for contrast estimation directly from noisy images without using any 3-D volume information. It enables contrast correction in the earlier stage of single particle analysis, and may improve the accuracy of downstream processing.



### Efficient Cross-Modal Retrieval via Deep Binary Hashing and Quantization
- **Arxiv ID**: http://arxiv.org/abs/2202.10232v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10232v1)
- **Published**: 2022-02-15 22:00:04+00:00
- **Updated**: 2022-02-15 22:00:04+00:00
- **Authors**: Yang Shi, Young-joo Chung
- **Comment**: Accepted at BMVC 2021
- **Journal**: BMVC 2021
- **Summary**: Cross-modal retrieval aims to search for data with similar semantic meanings across different content modalities. However, cross-modal retrieval requires huge amounts of storage and retrieval time since it needs to process data in multiple modalities. Existing works focused on learning single-source compact features such as binary hash codes that preserve similarities between different modalities. In this work, we propose a jointly learned deep hashing and quantization network (HQ) for cross-modal retrieval. We simultaneously learn binary hash codes and quantization codes to preserve semantic information in multiple modalities by an end-to-end deep learning architecture. At the retrieval step, binary hashing is used to retrieve a subset of items from the search space, then quantization is used to re-rank the retrieved items. We theoretically and empirically show that this two-stage retrieval approach provides faster retrieval results while preserving accuracy. Experimental results on the NUS-WIDE, MIR-Flickr, and Amazon datasets demonstrate that HQ achieves boosts of more than 7% in precision compared to supervised neural network-based compact coding models.



### K-Means for Noise-Insensitive Multi-Dimensional Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.07754v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP, 68T10, I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/2202.07754v3)
- **Published**: 2022-02-15 22:08:29+00:00
- **Updated**: 2022-08-08 20:10:15+00:00
- **Authors**: Nicholas Pellegrino, Paul Fieguth, Parsin Haji Reza
- **Comment**: Under consideration at Pattern Recognition Letters. 6 pages
  (excluding references), 5 figures
- **Journal**: None
- **Summary**: Many measurement modalities which perform imaging by probing an object pixel-by-pixel, such as via Photoacoustic Microscopy, produce a multi-dimensional feature (typically a time-domain signal) at each pixel. In principle, the many degrees of freedom in the time-domain signal would admit the possibility of significant multi-modal information being implicitly present, much more than a single scalar "brightness", regarding the underlying targets being observed. However, the measured signal is neither a weighted-sum of basis functions (such as principal components) nor one of a set of prototypes (K-means), which has motivated the novel clustering method proposed here. Signals are clustered based on their shape, but not amplitude, via angular distance and centroids are calculated as the direction of maximal intra-cluster variance, resulting in a clustering algorithm capable of learning centroids (signal shapes) that are related to the underlying, albeit unknown, target characteristics in a scalable and noise-robust manner.



### Deep Learning-Assisted Co-registration of Full-Spectral Autofluorescence Lifetime Microscopic Images with H&E-Stained Histology Images
- **Arxiv ID**: http://arxiv.org/abs/2202.07755v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2202.07755v1)
- **Published**: 2022-02-15 22:09:06+00:00
- **Updated**: 2022-02-15 22:09:06+00:00
- **Authors**: Qiang Wang, Susan Fernandes, Gareth O. S. Williams, Neil Finlayson, Ahsan R. Akram, Kevin Dhaliwal, James R. Hopgood, Marta Vallejo
- **Comment**: 21 pages, 9 figures, 5 equations, 1 table
- **Journal**: None
- **Summary**: Autofluorescence lifetime images reveal unique characteristics of endogenous fluorescence in biological samples. Comprehensive understanding and clinical diagnosis rely on co-registration with the gold standard, histology images, which is extremely challenging due to the difference of both images. Here, we show an unsupervised image-to-image translation network that significantly improves the success of the co-registration using a conventional optimisation-based regression network, applicable to autofluorescence lifetime images at different emission wavelengths. A preliminary blind comparison by experienced researchers shows the superiority of our method on co-registration. The results also indicate that the approach is applicable to various image formats, like fluorescence intensity images. With the registration, stitching outcomes illustrate the distinct differences of the spectral lifetime across an unstained tissue, enabling macro-level rapid visual identification of lung cancer and cellular-level characterisation of cell variants and common types. The approach could be effortlessly extended to lifetime images beyond this range and other staining technologies.



### General-purpose, long-context autoregressive modeling with Perceiver AR
- **Arxiv ID**: http://arxiv.org/abs/2202.07765v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2202.07765v2)
- **Published**: 2022-02-15 22:31:42+00:00
- **Updated**: 2022-06-14 16:55:16+00:00
- **Authors**: Curtis Hawthorne, Andrew Jaegle, Cătălina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, Hannah Sheahan, Neil Zeghidour, Jean-Baptiste Alayrac, João Carreira, Jesse Engel
- **Comment**: ICML 2022
- **Journal**: None
- **Summary**: Real-world data is high-dimensional: a book, image, or musical performance can easily contain hundreds of thousands of elements even after compression. However, the most commonly used autoregressive models, Transformers, are prohibitively expensive to scale to the number of inputs and layers needed to capture this long-range structure. We develop Perceiver AR, an autoregressive, modality-agnostic architecture which uses cross-attention to map long-range inputs to a small number of latents while also maintaining end-to-end causal masking. Perceiver AR can directly attend to over a hundred thousand tokens, enabling practical long-context density estimation without the need for hand-crafted sparsity patterns or memory mechanisms. When trained on images or music, Perceiver AR generates outputs with clear long-term coherence and structure. Our architecture also obtains state-of-the-art likelihood on long-sequence benchmarks, including 64 x 64 ImageNet images and PG-19 books.



### Beyond Deterministic Translation for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2202.07778v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.07778v3)
- **Published**: 2022-02-15 23:03:33+00:00
- **Updated**: 2022-11-20 23:34:24+00:00
- **Authors**: Eleni Chiou, Eleftheria Panagiotaki, Iasonas Kokkinos
- **Comment**: Accepted at BMVC 2022. Code is available at
  https://github.com/elchiou/Beyond-deterministic-translation-for-UDA
- **Journal**: None
- **Summary**: In this work we challenge the common approach of using a one-to-one mapping ('translation') between the source and target domains in unsupervised domain adaptation (UDA). Instead, we rely on stochastic translation to capture inherent translation ambiguities. This allows us to (i) train more accurate target networks by generating multiple outputs conditioned on the same source image, leveraging both accurate translation and data augmentation for appearance variability, (ii) impute robust pseudo-labels for the target data by averaging the predictions of a source network on multiple translated versions of a single target image and (iii) train and ensemble diverse networks in the target domain by modulating the degree of stochasticity in the translations. We report improvements over strong recent baselines, leading to state-of-the-art UDA results on two challenging semantic segmentation benchmarks. Our code is available at https://github.com/elchiou/Beyond-deterministic-translation-for-UDA.



