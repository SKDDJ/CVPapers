# Arxiv Papers in cs.CV on 2022-02-12
### Domain-Invariant Proposals based on a Balanced Domain Classifier for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.05941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.05941v1)
- **Published**: 2022-02-12 00:21:27+00:00
- **Updated**: 2022-02-12 00:21:27+00:00
- **Authors**: Zhize Wu, Xiaofeng Wang, Tong Xu, Xuebin Yang, Le Zou, Lixiang Xu, Thomas Weise
- **Comment**: None
- **Journal**: None
- **Summary**: Object recognition from images means to automatically find object(s) of interest and to return their category and location information. Benefiting from research on deep learning, like convolutional neural networks~(CNNs) and generative adversarial networks, the performance in this field has been improved significantly, especially when training and test data are drawn from similar distributions. However, mismatching distributions, i.e., domain shifts, lead to a significant performance drop. In this paper, we build domain-invariant detectors by learning domain classifiers via adversarial training. Based on the previous works that align image and instance level features, we mitigate the domain shift further by introducing a domain adaptation component at the region level within Faster \mbox{R-CNN}. We embed a domain classification network in the region proposal network~(RPN) using adversarial learning. The RPN can now generate accurate region proposals in different domains by effectively aligning the features between them. To mitigate the unstable convergence during the adversarial learning, we introduce a balanced domain classifier as well as a network learning rate adjustment strategy. We conduct comprehensive experiments using four standard datasets. The results demonstrate the effectiveness and robustness of our object detection approach in domain shift scenarios.



### Open-set Adversarial Defense with Clean-Adversarial Mutual Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.05953v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.05953v1)
- **Published**: 2022-02-12 02:13:55+00:00
- **Updated**: 2022-02-12 02:13:55+00:00
- **Authors**: Rui Shao, Pramuditha Perera, Pong C. Yuen, Vishal M. Patel
- **Comment**: Accepted by International Journal of Computer Vision (IJCV) 2022.
  Code will be available at https://github.com/rshaojimmy/ECCV2020-OSAD. arXiv
  admin note: text overlap with arXiv:2009.00814
- **Journal**: None
- **Summary**: Open-set recognition and adversarial defense study two key aspects of deep learning that are vital for real-world deployment. The objective of open-set recognition is to identify samples from open-set classes during testing, while adversarial defense aims to robustify the network against images perturbed by imperceptible adversarial noise. This paper demonstrates that open-set recognition systems are vulnerable to adversarial samples. Furthermore, this paper shows that adversarial defense mechanisms trained on known classes are unable to generalize well to open-set samples. Motivated by these observations, we emphasize the necessity of an Open-Set Adversarial Defense (OSAD) mechanism. This paper proposes an Open-Set Defense Network with Clean-Adversarial Mutual Learning (OSDN-CAML) as a solution to the OSAD problem. The proposed network designs an encoder with dual-attentive feature-denoising layers coupled with a classifier to learn a noise-free latent feature representation, which adaptively removes adversarial noise guided by channel and spatial-wise attentive filters. Several techniques are exploited to learn a noise-free and informative latent feature space with the aim of improving the performance of adversarial defense and open-set recognition. First, we incorporate a decoder to ensure that clean images can be well reconstructed from the obtained latent features. Then, self-supervision is used to ensure that the latent features are informative enough to carry out an auxiliary task. Finally, to exploit more complementary knowledge from clean image classification to facilitate feature denoising and search for a more generalized local minimum for open-set recognition, we further propose clean-adversarial mutual learning, where a peer network (classifying clean images) is further introduced to mutually learn with the classifier (classifying adversarial images).



### Audio-Visual Fusion Layers for Event Type Aware Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.05961v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.05961v1)
- **Published**: 2022-02-12 02:56:22+00:00
- **Updated**: 2022-02-12 02:56:22+00:00
- **Authors**: Arda Senocak, Junsik Kim, Tae-Hyun Oh, Hyeonggon Ryu, Dingzeyu Li, In So Kweon
- **Comment**: None
- **Journal**: None
- **Summary**: Human brain is continuously inundated with the multisensory information and their complex interactions coming from the outside world at any given moment. Such information is automatically analyzed by binding or segregating in our brain. While this task might seem effortless for human brains, it is extremely challenging to build a machine that can perform similar tasks since complex interactions cannot be dealt with single type of integration but requires more sophisticated approaches. In this paper, we propose a new model to address the multisensory integration problem with individual event-specific layers in a multi-task learning scheme. Unlike previous works where single type of fusion is used, we design event-specific layers to deal with different audio-visual relationship tasks, enabling different ways of audio-visual formation. Experimental results show that our event-specific layers can discover unique properties of the audio-visual relationships in the videos. Moreover, although our network is formulated with single labels, it can output additional true multi-labels to represent the given videos. We demonstrate that our proposed framework also exposes the modality bias of the video data category-wise and dataset-wise manner in popular benchmark datasets.



### Low-light Image Enhancement by Retinex Based Algorithm Unrolling and Adjustment
- **Arxiv ID**: http://arxiv.org/abs/2202.05972v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.05972v2)
- **Published**: 2022-02-12 03:59:38+00:00
- **Updated**: 2022-02-15 08:25:36+00:00
- **Authors**: Xinyi Liu, Qi Xie, Qian Zhao, Hong Wang, Deyu Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Motivated by their recent advances, deep learning techniques have been widely applied to low-light image enhancement (LIE) problem. Among which, Retinex theory based ones, mostly following a decomposition-adjustment pipeline, have taken an important place due to its physical interpretation and promising performance. However, current investigations on Retinex based deep learning are still not sufficient, ignoring many useful experiences from traditional methods. Besides, the adjustment step is either performed with simple image processing techniques, or by complicated networks, both of which are unsatisfactory in practice. To address these issues, we propose a new deep learning framework for the LIE problem. The proposed framework contains a decomposition network inspired by algorithm unrolling, and adjustment networks considering both global brightness and local brightness sensitivity. By virtue of algorithm unrolling, both implicit priors learned from data and explicit priors borrowed from traditional methods can be embedded in the network, facilitate to better decomposition. Meanwhile, the consideration of global and local brightness can guide designing simple yet effective network modules for adjustment. Besides, to avoid manually parameter tuning, we also propose a self-supervised fine-tuning strategy, which can always guarantee a promising performance. Experiments on a series of typical LIE datasets demonstrated the effectiveness of the proposed method, both quantitatively and visually, as compared with existing methods.



### Uncalibrated Models Can Improve Human-AI Collaboration
- **Arxiv ID**: http://arxiv.org/abs/2202.05983v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.05983v3)
- **Published**: 2022-02-12 04:51:00+00:00
- **Updated**: 2022-10-28 01:43:51+00:00
- **Authors**: Kailas Vodrahalli, Tobias Gerstenberg, James Zou
- **Comment**: 21 pages, 12 figures, NeurIPS 2022
- **Journal**: None
- **Summary**: In many practical applications of AI, an AI model is used as a decision aid for human users. The AI provides advice that a human (sometimes) incorporates into their decision-making process. The AI advice is often presented with some measure of "confidence" that the human can use to calibrate how much they depend on or trust the advice. In this paper, we present an initial exploration that suggests showing AI models as more confident than they actually are, even when the original AI is well-calibrated, can improve human-AI performance (measured as the accuracy and confidence of the human's final prediction after seeing the AI advice). We first train a model to predict human incorporation of AI advice using data from thousands of human-AI interactions. This enables us to explicitly estimate how to transform the AI's prediction confidence, making the AI uncalibrated, in order to improve the final human prediction. We empirically validate our results across four different tasks--dealing with images, text and tabular data--involving hundreds of human participants. We further support our findings with simulation analysis. Our findings suggest the importance of jointly optimizing the human-AI system as opposed to the standard paradigm of optimizing the AI model alone.



### RSINet: Inpainting Remotely Sensed Images Using Triple GAN Framework
- **Arxiv ID**: http://arxiv.org/abs/2202.05988v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.05988v1)
- **Published**: 2022-02-12 05:19:37+00:00
- **Updated**: 2022-02-12 05:19:37+00:00
- **Authors**: Advait Kumar, Dipesh Tamboli, Shivam Pande, Biplab Banerjee
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of image inpainting in the remote sensing domain. Remote sensing images possess high resolution and geographical variations, that render the conventional inpainting methods less effective. This further entails the requirement of models with high complexity to sufficiently capture the spectral, spatial and textural nuances within an image, emerging from its high spatial variability. To this end, we propose a novel inpainting method that individually focuses on each aspect of an image such as edges, colour and texture using a task specific GAN. Moreover, each individual GAN also incorporates the attention mechanism that explicitly extracts the spectral and spatial features. To ensure consistent gradient flow, the model uses residual learning paradigm, thus simultaneously working with high and low level features. We evaluate our model, alongwith previous state of the art models, on the two well known remote sensing datasets, Open Cities AI and Earth on Canvas, and achieve competitive performance.



### Multi-direction and Multi-scale Pyramid in Transformer for Video-based Pedestrian Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2202.06014v2
- **DOI**: 10.1109/TII.2022.3151766
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06014v2)
- **Published**: 2022-02-12 08:22:47+00:00
- **Updated**: 2022-04-06 03:00:43+00:00
- **Authors**: Xianghao Zang, Ge Li, Wei Gao
- **Comment**: 10 pages, 6 figures, Accepted for publication in IEEE Transactions on
  Industrial Informatics
- **Journal**: IEEE Transactions on Industrial Informatics 2022
- **Summary**: In video surveillance, pedestrian retrieval (also called person re-identification) is a critical task. This task aims to retrieve the pedestrian of interest from non-overlapping cameras. Recently, transformer-based models have achieved significant progress for this task. However, these models still suffer from ignoring fine-grained, part-informed information. This paper proposes a multi-direction and multi-scale Pyramid in Transformer (PiT) to solve this problem. In transformer-based architecture, each pedestrian image is split into many patches. Then, these patches are fed to transformer layers to obtain the feature representation of this image. To explore the fine-grained information, this paper proposes to apply vertical division and horizontal division on these patches to generate different-direction human parts. These parts provide more fine-grained information. To fuse multi-scale feature representation, this paper presents a pyramid structure containing global-level information and many pieces of local-level information from different scales. The feature pyramids of all the pedestrian images from the same video are fused to form the final multi-direction and multi-scale feature representation. Experimental results on two challenging video-based benchmarks, MARS and iLIDS-VID, show the proposed PiT achieves state-of-the-art performance. Extensive ablation studies demonstrate the superiority of the proposed pyramid structure. The code is available at https://git.openi.org.cn/zangxh/PiT.git.



### Fun Selfie Filters in Face Recognition: Impact Assessment and Removal
- **Arxiv ID**: http://arxiv.org/abs/2202.06022v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06022v1)
- **Published**: 2022-02-12 09:12:31+00:00
- **Updated**: 2022-02-12 09:12:31+00:00
- **Authors**: Cristian Botezatu, Mathias Ibsen, Christian Rathgeb, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: This work investigates the impact of fun selfie filters, which are frequently used to modify selfies, on face recognition systems. Based on a qualitative assessment and classification of freely available mobile applications, ten relevant fun selfie filters are selected to create a database. To this end, the selected filters are automatically applied to face images of public face image databases. Different state-of-the-art methods are used to evaluate the influence of fun selfie filters on the performance of face detection using dlib, RetinaFace, and a COTS method, sample quality estimated by FaceQNet and MagFace, and recognition accuracy employing ArcFace and a COTS algorithm. The obtained results indicate that selfie filters negatively affect face recognition modules, especially if fun selfie filters cover a large region of the face, where the mouth, nose, and eyes are covered. To mitigate such unwanted effects, a GAN-based selfie filter removal algorithm is proposed which consists of a segmentation module, a perceptual network, and a generation module. In a cross-database experiment the application of the presented selfie filter removal technique has shown to significantly improve the biometric performance of the underlying face recognition systems.



### Reversible data hiding with dual pixel-value-ordering and1minimum prediction error expansion
- **Arxiv ID**: http://arxiv.org/abs/2202.08100v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2202.08100v1)
- **Published**: 2022-02-12 09:35:56+00:00
- **Updated**: 2022-02-12 09:35:56+00:00
- **Authors**: Md. Abdul Wahed, Hussain Nyeem
- **Comment**: Submitted to Plos One [PONE-D-21-21353R1]
- **Journal**: None
- **Summary**: Pixel Value Ordering (PVO) holds an impressive property for high fidelity Reversible Data Hiding (RDH). In this paper, we introduce a dual-PVO (dPVO) for Prediction Error Expansion(PEE), and thereby develop a new RDH scheme to offer a better rate-distortion performance. Particularly, we propose to embed in two phases: forward and backward. In the forward phase, PVO with classic PEE is applied to every non-overlapping image block of size 1x3. In the backward phase,minimum-set and maximum-set of pixels are determined from the pixels predicted in the forward phase. The minimum set only contains the lowest predicted pixels and the maximum set contains the largest predicted pixels of each image block. Proposed dPVO withPEE is then applied to both sets, so that the pixel values of the minimum set are increased and that of the maximum set are decreased by a unit value. Thereby, the pixels predicted in the forward embedding can partially be restored to their original values resulting in both better-embedded image quality and a higher embedding rate. Experimental results have recorded a promising rate-distortion performance of our scheme with a significant improvement of embedded image quality at higher embedding rates compared to the popular and state-of-the-art PVO-based RDHschemes.



### End-to-end Reinforcement Learning of Robotic Manipulation with Robust Keypoints Representation
- **Arxiv ID**: http://arxiv.org/abs/2202.06027v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.06027v1)
- **Published**: 2022-02-12 09:58:09+00:00
- **Updated**: 2022-02-12 09:58:09+00:00
- **Authors**: Tianying Wang, En Yen Puang, Marcus Lee, Yan Wu, Wei Jing
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: We present an end-to-end Reinforcement Learning(RL) framework for robotic manipulation tasks, using a robust and efficient keypoints representation. The proposed method learns keypoints from camera images as the state representation, through a self-supervised autoencoder architecture. The keypoints encode the geometric information, as well as the relationship of the tool and target in a compact representation to ensure efficient and robust learning. After keypoints learning, the RL step then learns the robot motion from the extracted keypoints state representation. The keypoints and RL learning processes are entirely done in the simulated environment. We demonstrate the effectiveness of the proposed method on robotic manipulation tasks including grasping and pushing, in different scenarios. We also investigate the generalization capability of the trained model. In addition to the robust keypoints representation, we further apply domain randomization and adversarial training examples to achieve zero-shot sim-to-real transfer in real-world robotic manipulation tasks.



### OctAttention: Octree-Based Large-Scale Contexts Model for Point Cloud Compression
- **Arxiv ID**: http://arxiv.org/abs/2202.06028v2
- **DOI**: 10.1609/aaai.v36i1.19942
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06028v2)
- **Published**: 2022-02-12 10:06:12+00:00
- **Updated**: 2022-05-08 02:40:28+00:00
- **Authors**: Chunyang Fu, Ge Li, Rui Song, Wei Gao, Shan Liu
- **Comment**: Accepted by AAAI 2022
- **Journal**: Proceedings of the AAAI Conference on Artificial Intelligence
  36(1):625-633, June 2022
- **Summary**: In point cloud compression, sufficient contexts are significant for modeling the point cloud distribution. However, the contexts gathered by the previous voxel-based methods decrease when handling sparse point clouds. To address this problem, we propose a multiple-contexts deep learning framework called OctAttention employing the octree structure, a memory-efficient representation for point clouds. Our approach encodes octree symbol sequences in a lossless way by gathering the information of sibling and ancestor nodes. Expressly, we first represent point clouds with octree to reduce spatial redundancy, which is robust for point clouds with different resolutions. We then design a conditional entropy model with a large receptive field that models the sibling and ancestor contexts to exploit the strong dependency among the neighboring nodes and employ an attention mechanism to emphasize the correlated nodes in the context. Furthermore, we introduce a mask operation during training and testing to make a trade-off between encoding time and performance. Compared to the previous state-of-the-art works, our approach obtains a 10%-35% BD-Rate gain on the LiDAR benchmark (e.g. SemanticKITTI) and object point cloud dataset (e.g. MPEG 8i, MVUB), and saves 95% coding time compared to the voxel-based baseline. The code is available at https://github.com/zb12138/OctAttention.



### An Automated Analysis Framework for Trajectory Datasets
- **Arxiv ID**: http://arxiv.org/abs/2202.07438v1
- **DOI**: 10.48550/arXiv.2202.07438
- **Categories**: **cs.CV**, 68T45 (Primary) 68T10 (Secondary), I.5.3; J.4
- **Links**: [PDF](http://arxiv.org/pdf/2202.07438v1)
- **Published**: 2022-02-12 10:55:53+00:00
- **Updated**: 2022-02-12 10:55:53+00:00
- **Authors**: Christoph Glasmacher, Robert Krajewski, Lutz Eckstein
- **Comment**: 13 pages, 13 figures
- **Journal**: None
- **Summary**: Trajectory datasets of road users have become more important in the last years for safety validation of highly automated vehicles. Several naturalistic trajectory datasets with each more than 10.000 tracks were released and others will follow. Considering this amount of data, it is necessary to be able to compare these datasets in-depth with ease to get an overview. By now, the datasets' own provided information is mainly limited to meta-data and qualitative descriptions which are mostly not consistent with other datasets. This is insufficient for users to differentiate the emerging datasets for application-specific selection. Therefore, an automated analysis framework is proposed in this work. Starting with analyzing individual tracks, fourteen elementary characteristics, so-called detection types, are derived and used as the base of this framework. To describe each traffic scenario precisely, the detections are subdivided into common metrics, clustering methods and anomaly detection. Those are combined using a modular approach. The detections are composed into new scores to describe three defined attributes of each track data quantitatively: interaction, anomaly and relevance. These three scores are calculated hierarchically for different abstract layers to provide an overview not just between datasets but also for tracks, spatial regions and individual situations. So, an objective comparison between datasets can be realized. Furthermore, it can help to get a deeper understanding of the recorded infrastructure and its effect on road user behavior. To test the validity of the framework, a study is conducted to compare the scores with human perception. Additionally, several datasets are compared.



### Fuzzy Pooling
- **Arxiv ID**: http://arxiv.org/abs/2202.08372v1
- **DOI**: 10.1109/TFUZZ.2020.3024023
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.08372v1)
- **Published**: 2022-02-12 11:18:32+00:00
- **Updated**: 2022-02-12 11:18:32+00:00
- **Authors**: Dimitrios E. Diamantis, Dimitris K. Iakovidis
- **Comment**: The final version of the paper has been published in
  https://ieeexplore.ieee.org/document/9197689
- **Journal**: IEEE Transactions on Fuzzy Systems, vol. 29, no. 11, pp. 3481 -
  3488, Nov. 2020
- **Summary**: Convolutional Neural Networks (CNNs) are artificial learning systems typically based on two operations: convolution, which implements feature extraction through filtering, and pooling, which implements dimensionality reduction. The impact of pooling in the classification performance of the CNNs has been highlighted in several previous works, and a variety of alternative pooling operators have been proposed. However, only a few of them tackle with the uncertainty that is naturally propagated from the input layer to the feature maps of the hidden layers through convolutions. In this paper we present a novel pooling operation based on (type-1) fuzzy sets to cope with the local imprecision of the feature maps, and we investigate its performance in the context of image classification. Fuzzy pooling is performed by fuzzification, aggregation and defuzzification of feature map neighborhoods. It is used for the construction of a fuzzy pooling layer that can be applied as a drop-in replacement of the current, crisp, pooling layers of CNN architectures. Several experiments using publicly available datasets show that the proposed approach can enhance the classification performance of a CNN. A comparative evaluation shows that it outperforms state-of-the-art pooling approaches.



### Depth-Cooperated Trimodal Network for Video Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.06060v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06060v2)
- **Published**: 2022-02-12 13:04:16+00:00
- **Updated**: 2022-07-11 11:52:49+00:00
- **Authors**: Yukang Lu, Dingyao Min, Keren Fu, Qijun Zhao
- **Comment**: 5 pages, 3 figures, Accepted at ICIP-2022
- **Journal**: None
- **Summary**: Depth can provide useful geographical cues for salient object detection (SOD), and has been proven helpful in recent RGB-D SOD methods. However, existing video salient object detection (VSOD) methods only utilize spatiotemporal information and seldom exploit depth information for detection. In this paper, we propose a depth-cooperated trimodal network, called DCTNet for VSOD, which is a pioneering work to incorporate depth information to assist VSOD. To this end, we first generate depth from RGB frames, and then propose an approach to treat the three modalities unequally. Specifically, a multi-modal attention module (MAM) is designed to model multi-modal long-range dependencies between the main modality (RGB) and the two auxiliary modalities (depth, optical flow). We also introduce a refinement fusion module (RFM) to suppress noises in each modality and select useful information dynamically for further feature refinement. Lastly, a progressive fusion strategy is adopted after the refined features to achieve final cross-modal fusion. Experiments on five benchmark datasets demonstrate the superiority of our depth-cooperated model against 12 state-of-the-art methods, and the necessity of depth is also validated.



### Classification of Microscopy Images of Breast Tissue: Region Duplication based Self-Supervision vs. Off-the Shelf Deep Representations
- **Arxiv ID**: http://arxiv.org/abs/2202.06073v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.06073v1)
- **Published**: 2022-02-12 14:12:13+00:00
- **Updated**: 2022-02-12 14:12:13+00:00
- **Authors**: Aravind Ravi
- **Comment**: None
- **Journal**: None
- **Summary**: Breast cancer is one of the leading causes of female mortality in the world. This can be reduced when diagnoses are performed at the early stages of progression. Further, the efficiency of the process can be significantly improved with computer aided diagnosis. Deep learning based approaches have been successfully applied to achieve this. One of the limiting factors for training deep networks in a supervised manner is the dependency on large amounts of expert annotated data. In reality, large amounts of unlabelled data and only small amounts of expert annotated data are available. In such scenarios, transfer learning approaches and self-supervised learning (SSL) based approaches can be leveraged. In this study, we propose a novel self-supervision pretext task to train a convolutional neural network (CNN) and extract domain specific features. This method was compared with deep features extracted using pre-trained CNNs such as DenseNet-121 and ResNet-50 trained on ImageNet. Additionally, two types of patch-combination methods were introduced and compared with majority voting. The methods were validated on the BACH microscopy images dataset. Results indicated that the best performance of 99% sensitivity was achieved for the deep features extracted using ResNet50 with concatenation of patch-level embedding. Preliminary results of SSL to extract domain specific features indicated that with just 15% of unlabelled data a high sensitivity of 94% can be achieved for a four class classification of microscopy images.



### Indication as Prior Knowledge for Multimodal Disease Classification in Chest Radiographs with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2202.06076v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.06076v1)
- **Published**: 2022-02-12 14:23:30+00:00
- **Updated**: 2022-02-12 14:23:30+00:00
- **Authors**: Grzegorz Jacenków, Alison Q. O'Neil, Sotirios A. Tsaftaris
- **Comment**: Accepted at the IEEE International Symposium on Biomedical Imaging
  (ISBI) 2022 as an oral presentation
- **Journal**: None
- **Summary**: When a clinician refers a patient for an imaging exam, they include the reason (e.g. relevant patient history, suspected disease) in the scan request; this appears as the indication field in the radiology report. The interpretation and reporting of the image are substantially influenced by this request text, steering the radiologist to focus on particular aspects of the image. We use the indication field to drive better image classification, by taking a transformer network which is unimodally pre-trained on text (BERT) and fine-tuning it for multimodal classification of a dual image-text input. We evaluate the method on the MIMIC-CXR dataset, and present ablation studies to investigate the effect of the indication field on the classification performance. The experimental results show our approach achieves 87.8 average micro AUROC, outperforming the state-of-the-art methods for unimodal (84.4) and multimodal (86.0) classification. Our code is available at https://github.com/jacenkow/mmbt.



### Text and Image Guided 3D Avatar Generation and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2202.06079v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.06079v1)
- **Published**: 2022-02-12 14:37:29+00:00
- **Updated**: 2022-02-12 14:37:29+00:00
- **Authors**: Zehranaz Canfes, M. Furkan Atasoy, Alara Dirik, Pinar Yanardag
- **Comment**: None
- **Journal**: None
- **Summary**: The manipulation of latent space has recently become an interesting topic in the field of generative models. Recent research shows that latent directions can be used to manipulate images towards certain attributes. However, controlling the generation process of 3D generative models remains a challenge. In this work, we propose a novel 3D manipulation method that can manipulate both the shape and texture of the model using text or image-based prompts such as 'a young face' or 'a surprised face'. We leverage the power of Contrastive Language-Image Pre-training (CLIP) model and a pre-trained 3D GAN model designed to generate face avatars, and create a fully differentiable rendering pipeline to manipulate meshes. More specifically, our method takes an input latent code and modifies it such that the target attribute specified by a text or image prompt is present or enhanced, while leaving other attributes largely unaffected. Our method requires only 5 minutes per manipulation, and we demonstrate the effectiveness of our approach with extensive results and comparisons.



### Recognition-free Question Answering on Handwritten Document Collections
- **Arxiv ID**: http://arxiv.org/abs/2202.06080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06080v1)
- **Published**: 2022-02-12 14:47:44+00:00
- **Updated**: 2022-02-12 14:47:44+00:00
- **Authors**: Oliver Tüselmann, Friedrich Müller, Fabian Wolf, Gernot A. Fink
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, considerable progress has been made in the research area of Question Answering (QA) on document images. Current QA approaches from the Document Image Analysis community are mainly focusing on machine-printed documents and perform rather limited on handwriting. This is mainly due to the reduced recognition performance on handwritten documents. To tackle this problem, we propose a recognition-free QA approach, especially designed for handwritten document image collections. We present a robust document retrieval method, as well as two QA models. Our approaches outperform the state-of-the-art recognition-free models on the challenging BenthamQA and HW-SQuAD datasets.



### NeuVV: Neural Volumetric Videos with Immersive Rendering and Editing
- **Arxiv ID**: http://arxiv.org/abs/2202.06088v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2202.06088v1)
- **Published**: 2022-02-12 15:23:16+00:00
- **Updated**: 2022-02-12 15:23:16+00:00
- **Authors**: Jiakai Zhang, Liao Wang, Xinhang Liu, Fuqiang Zhao, Minzhang Li, Haizhao Dai, Boyuan Zhang, Wei Yang, Lan Xu, Jingyi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Some of the most exciting experiences that Metaverse promises to offer, for instance, live interactions with virtual characters in virtual environments, require real-time photo-realistic rendering. 3D reconstruction approaches to rendering, active or passive, still require extensive cleanup work to fix the meshes or point clouds. In this paper, we present a neural volumography technique called neural volumetric video or NeuVV to support immersive, interactive, and spatial-temporal rendering of volumetric video contents with photo-realism and in real-time. The core of NeuVV is to efficiently encode a dynamic neural radiance field (NeRF) into renderable and editable primitives. We introduce two types of factorization schemes: a hyper-spherical harmonics (HH) decomposition for modeling smooth color variations over space and time and a learnable basis representation for modeling abrupt density and color changes caused by motion. NeuVV factorization can be integrated into a Video Octree (VOctree) analogous to PlenOctree to significantly accelerate training while reducing memory overhead. Real-time NeuVV rendering further enables a class of immersive content editing tools. Specifically, NeuVV treats each VOctree as a primitive and implements volume-based depth ordering and alpha blending to realize spatial-temporal compositions for content re-purposing. For example, we demonstrate positioning varied manifestations of the same performance at different 3D locations with different timing, adjusting color/texture of the performer's clothing, casting spotlight shadows and synthesizing distance falloff lighting, etc, all at an interactive speed. We further develop a hybrid neural-rasterization rendering framework to support consumer-level VR headsets so that the aforementioned volumetric video viewing and editing, for the first time, can be conducted immersively in virtual 3D space.



### Memory-augmented Deep Unfolding Network for Guided Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/2203.04960v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04960v1)
- **Published**: 2022-02-12 15:37:13+00:00
- **Updated**: 2022-02-12 15:37:13+00:00
- **Authors**: Man Zhou, Keyu Yan, Jinshan Pan, Wenqi Ren, Qi Xie, Xiangyong Cao
- **Comment**: 24 pages, 16 figures
- **Journal**: None
- **Summary**: Guided image super-resolution (GISR) aims to obtain a high-resolution (HR) target image by enhancing the spatial resolution of a low-resolution (LR) target image under the guidance of a HR image. However, previous model-based methods mainly takes the entire image as a whole, and assume the prior distribution between the HR target image and the HR guidance image, simply ignoring many non-local common characteristics between them. To alleviate this issue, we firstly propose a maximal a posterior (MAP) estimation model for GISR with two types of prior on the HR target image, i.e., local implicit prior and global implicit prior. The local implicit prior aims to model the complex relationship between the HR target image and the HR guidance image from a local perspective, and the global implicit prior considers the non-local auto-regression property between the two images from a global perspective. Secondly, we design a novel alternating optimization algorithm to solve this model for GISR. The algorithm is in a concise framework that facilitates to be replicated into commonly used deep network structures. Thirdly, to reduce the information loss across iterative stages, the persistent memory mechanism is introduced to augment the information representation by exploiting the Long short-term memory unit (LSTM) in the image and feature spaces. In this way, a deep network with certain interpretation and high representation ability is built. Extensive experimental results validate the superiority of our method on a variety of GISR tasks, including Pan-sharpening, depth image super-resolution, and MR image super-resolution.



### Proximal PanNet: A Model-Based Deep Network for Pansharpening
- **Arxiv ID**: http://arxiv.org/abs/2203.04286v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.04286v1)
- **Published**: 2022-02-12 15:49:13+00:00
- **Updated**: 2022-02-12 15:49:13+00:00
- **Authors**: Xiangyong Cao, Yang Chen, Wenfei Cao
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Recently, deep learning techniques have been extensively studied for pansharpening, which aims to generate a high resolution multispectral (HRMS) image by fusing a low resolution multispectral (LRMS) image with a high resolution panchromatic (PAN) image. However, existing deep learning-based pansharpening methods directly learn the mapping from LRMS and PAN to HRMS. These network architectures always lack sufficient interpretability, which limits further performance improvements. To alleviate this issue, we propose a novel deep network for pansharpening by combining the model-based methodology with the deep learning method. Firstly, we build an observation model for pansharpening using the convolutional sparse coding (CSC) technique and design a proximal gradient algorithm to solve this model. Secondly, we unfold the iterative algorithm into a deep network, dubbed as Proximal PanNet, by learning the proximal operators using convolutional neural networks. Finally, all the learnable modules can be automatically learned in an end-to-end manner. Experimental results on some benchmark datasets show that our network performs better than other advanced methods both quantitatively and qualitatively.



### A Review of Deep Learning-based Approaches for Deepfake Content Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.06095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.06095v1)
- **Published**: 2022-02-12 16:22:46+00:00
- **Updated**: 2022-02-12 16:22:46+00:00
- **Authors**: Leandro A. Passos, Danilo Jodas, Kelton A. P. da Costa, Luis A. Souza Júnior, Danilo Colombo, João Paulo Papa
- **Comment**: None
- **Journal**: None
- **Summary**: The fast-spreading information over the internet is essential to support the rapid supply of numerous public utility services and entertainment to users. Social networks and online media paved the way for modern, timely-communication-fashion and convenient access to all types of information. However, it also provides new chances for ill use of the massive amount of available data, such as spreading fake content to manipulate public opinion. Detection of counterfeit content has raised attention in the last few years for the advances in deepfake generation. The rapid growth of machine learning techniques, particularly deep learning, can predict fake content in several application domains, including fake image and video manipulation. This paper presents a comprehensive review of recent studies for deepfake content detection using deep learning-based approaches. We aim to broaden the state-of-the-art research by systematically reviewing the different categories of fake content detection. Furthermore, we report the advantages and drawbacks of the examined works and future directions towards the issues and shortcomings still unsolved on deepfake detection.



### Excitement Surfeited Turns to Errors: Deep Learning Testing Framework Based on Excitable Neurons
- **Arxiv ID**: http://arxiv.org/abs/2202.07464v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07464v2)
- **Published**: 2022-02-12 16:44:15+00:00
- **Updated**: 2022-11-20 14:50:45+00:00
- **Authors**: Haibo Jin, Ruoxi Chen, Haibin Zheng, Jinyin Chen, Yao Cheng, Yue Yu, Xianglong Liu
- **Comment**: 32 pages
- **Journal**: None
- **Summary**: Despite impressive capabilities and outstanding performance, deep neural networks (DNNs) have captured increasing public concern about their security problems, due to their frequently occurred erroneous behaviors. Therefore, it is necessary to conduct a systematical testing for DNNs before they are deployed to real-world applications. Existing testing methods have provided fine-grained metrics based on neuron coverage and proposed various approaches to improve such metrics. However, it has been gradually realized that a higher neuron coverage does \textit{not} necessarily represent better capabilities in identifying defects that lead to errors. Besides, coverage-guided methods cannot hunt errors due to faulty training procedure. So the robustness improvement of DNNs via retraining by these testing examples are unsatisfactory. To address this challenge, we introduce the concept of excitable neurons based on Shapley value and design a novel white-box testing framework for DNNs, namely DeepSensor. It is motivated by our observation that neurons with larger responsibility towards model loss changes due to small perturbations are more likely related to incorrect corner cases due to potential defects. By maximizing the number of excitable neurons concerning various wrong behaviors of models, DeepSensor can generate testing examples that effectively trigger more errors due to adversarial inputs, polluted data and incomplete training. Extensive experiments implemented on both image classification models and speaker recognition models have demonstrated the superiority of DeepSensor.



### Semi-supervised Medical Image Segmentation via Geometry-aware Consistency Training
- **Arxiv ID**: http://arxiv.org/abs/2202.06104v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.06104v1)
- **Published**: 2022-02-12 17:07:17+00:00
- **Updated**: 2022-02-12 17:07:17+00:00
- **Authors**: Zihang Liu, Chunhui Zhao
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: The performance of supervised deep learning methods for medical image segmentation is often limited by the scarcity of labeled data. As a promising research direction, semi-supervised learning addresses this dilemma by leveraging unlabeled data information to assist the learning process. In this paper, a novel geometry-aware semi-supervised learning framework is proposed for medical image segmentation, which is a consistency-based method. Considering that the hard-to-segment regions are mainly located around the object boundary, we introduce an auxiliary prediction task to learn the global geometric information. Based on the geometric constraint, the ambiguous boundary regions are emphasized through an exponentially weighted strategy for the model training to better exploit both labeled and unlabeled data. In addition, a dual-view network is designed to perform segmentation from different perspectives and reduce the prediction uncertainty. The proposed method is evaluated on the public left atrium benchmark dataset and improves fully supervised method by 8.7% in Dice with 10% labeled images, while 4.3% with 20% labeled images. Meanwhile, our framework outperforms six state-of-the-art semi-supervised segmentation methods.



### Breast Cancer Detection using Histopathological Images
- **Arxiv ID**: http://arxiv.org/abs/2202.06109v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.06109v1)
- **Published**: 2022-02-12 17:45:43+00:00
- **Updated**: 2022-02-12 17:45:43+00:00
- **Authors**: Jitendra Maan, Harsh Maan
- **Comment**: 6 pages, 10 figures, Published with International Journal of Computer
  Science Trends and Technology (IJCST)
- **Journal**: International Journal of Computer Science Trends and Technology
  (IJCST) V10(1):Page(53-58) Jan-Feb 2022. ISSN: 2347-8578.www.ijcstjournal.org
- **Summary**: Cancer is one of the most common and fatal diseases in the world. Breast cancer affects one in every eight women and one in every eight hundred men. Hence, our prime target should be early detection of cancer because the early detection of cancer can be helpful to cure cancer effectively. Therefore, we propose a saliency detection system with the help of advanced deep learning techniques, such that the machine will be taught to emulate actions of pathologists for localization of diagnostically pertinent regions. We study identification of five diagnostic categories of breast cancer by training a CNN (VGG16, ResNet architecture). We have used BreakHis dataset to train our model. We focus on both detection and classification of cancerous regions in histopathology images. The diagnostically relevant regions are salient. The detection system will be available as an open source web application which can be used by pathologists and medical institutions.



### Typography-MNIST (TMNIST): an MNIST-Style Image Dataset to Categorize Glyphs and Font-Styles
- **Arxiv ID**: http://arxiv.org/abs/2202.08112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.08112v1)
- **Published**: 2022-02-12 21:01:39+00:00
- **Updated**: 2022-02-12 21:01:39+00:00
- **Authors**: Nimish Magre, Nicholas Brown
- **Comment**: None
- **Journal**: None
- **Summary**: We present Typography-MNIST (TMNIST), a dataset comprising of 565,292 MNIST-style grayscale images representing 1,812 unique glyphs in varied styles of 1,355 Google-fonts. The glyph-list contains common characters from over 150 of the modern and historical language scripts with symbol sets, and each font-style represents varying subsets of the total unique glyphs. The dataset has been developed as part of the CognitiveType project which aims to develop eye-tracking tools for real-time mapping of type to cognition and to create computational tools that allow for the easy design of typefaces with cognitive properties such as readability. The dataset and scripts to generate MNIST-style images for glyphs in different font styles are freely available at https://github.com/aiskunks/CognitiveType.



### Multi-task Deep Learning for Cerebrovascular Disease Classification and MRI-to-PET Translation
- **Arxiv ID**: http://arxiv.org/abs/2202.06142v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.06142v1)
- **Published**: 2022-02-12 21:02:45+00:00
- **Updated**: 2022-02-12 21:02:45+00:00
- **Authors**: Ramy Hussein, Moss Zhao, David Shin, Jia Guo, Kevin T. Chen, Rui D. Armindo, Guido Davidzon, Michael Moseley, Greg Zaharchuk
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: Accurate quantification of cerebral blood flow (CBF) is essential for the diagnosis and assessment of cerebrovascular diseases such as Moyamoya, carotid stenosis, aneurysms, and stroke. Positron emission tomography (PET) is currently regarded as the gold standard for the measurement of CBF in the human brain. PET imaging, however, is not widely available because of its prohibitive costs, use of ionizing radiation, and logistical challenges, which require a co-localized cyclotron to deliver the 2 min half-life Oxygen-15 radioisotope. Magnetic resonance imaging (MRI), in contrast, is more readily available and does not involve ionizing radiation. In this study, we propose a multi-task learning framework for brain MRI-to-PET translation and disease diagnosis. The proposed framework comprises two prime networks: (1) an attention-based 3D encoder-decoder convolutional neural network (CNN) that synthesizes high-quality PET CBF maps from multi-contrast MRI images, and (2) a multi-scale 3D CNN that identifies the brain disease corresponding to the input MRI images. Our multi-task framework yields promising results on the task of MRI-to-PET translation, achieving an average structural similarity index (SSIM) of 0.94 and peak signal-to-noise ratio (PSNR) of 38dB on a cohort of 120 subjects. In addition, we show that integrating multiple MRI modalities can improve the clinical diagnosis of brain diseases.



### InfraredTags: Embedding Invisible AR Markers and Barcodes Using Low-Cost, Infrared-Based 3D Printing and Imaging Tools
- **Arxiv ID**: http://arxiv.org/abs/2202.06165v1
- **DOI**: 10.1145/3491102.3501951
- **Categories**: **cs.HC**, cs.CV, H.5.0; H.5.2
- **Links**: [PDF](http://arxiv.org/pdf/2202.06165v1)
- **Published**: 2022-02-12 23:45:18+00:00
- **Updated**: 2022-02-12 23:45:18+00:00
- **Authors**: Mustafa Doga Dogan, Ahmad Taka, Michael Lu, Yunyi Zhu, Akshat Kumar, Aakar Gupta, Stefanie Mueller
- **Comment**: 12 pages, 10 figures. To appear in the Proceedings of the 2022 ACM
  Conference on Human Factors in Computing Systems
- **Journal**: None
- **Summary**: Existing approaches for embedding unobtrusive tags inside 3D objects require either complex fabrication or high-cost imaging equipment. We present InfraredTags, which are 2D markers and barcodes imperceptible to the naked eye that can be 3D printed as part of objects, and detected rapidly by low-cost near-infrared cameras. We achieve this by printing objects from an infrared-transmitting filament, which infrared cameras can see through, and by having air gaps inside for the tag's bits, which appear at a different intensity in the infrared image.   We built a user interface that facilitates the integration of common tags (QR codes, ArUco markers) with the object geometry to make them 3D printable as InfraredTags. We also developed a low-cost infrared imaging module that augments existing mobile devices and decodes tags using our image processing pipeline. Our evaluation shows that the tags can be detected with little near-infrared illumination (0.2lux) and from distances as far as 250cm. We demonstrate how our method enables various applications, such as object tracking and embedding metadata for augmented reality and tangible interactions.



