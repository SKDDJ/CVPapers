# Arxiv Papers in cs.CV on 2022-02-08
### Phase-Stretch Adaptive Gradient-Field Extractor (PAGE)
- **Arxiv ID**: http://arxiv.org/abs/2202.03570v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03570v2)
- **Published**: 2022-02-08 00:14:50+00:00
- **Updated**: 2022-02-12 00:09:54+00:00
- **Authors**: Callen MacPhee, Madhuri Suthar, Bahram Jalali
- **Comment**: None
- **Journal**: None
- **Summary**: Phase-Stretch Adaptive Gradient-Field Extractor (PAGE) is an edge detection algorithm that is inspired by physics of electromagnetic diffraction and dispersion. A computational imaging algorithm, it identifies edges, their orientations and sharpness in a digital image where the image brightness changes abruptly. Edge detection is a basic operation performed by the eye and is crucial to visual perception. PAGE embeds an original image into a set of feature maps that can be used for object representation and classification. The algorithm performs exceptionally well as an edge and texture extractor in low light level and low contrast images. This manuscript is prepared to support the open-source code which is being simultaneously made available within the GitHub repository https://github.com/JalaliLabUCLA/Phase-Stretch-Adaptive-Gradient-field-Extractor/.



### Metal Artifact Reduction with Intra-Oral Scan Data for 3D Low Dose Maxillofacial CBCT Modeling
- **Arxiv ID**: http://arxiv.org/abs/2202.03571v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03571v1)
- **Published**: 2022-02-08 00:24:41+00:00
- **Updated**: 2022-02-08 00:24:41+00:00
- **Authors**: Chang Min Hyun, Taigyntuya Bayaraa, Hye Sun Yun, Tae Jun Jang, Hyoung Suk Park, Jin Keun Seo
- **Comment**: None
- **Journal**: None
- **Summary**: Low-dose dental cone beam computed tomography (CBCT) has been increasingly used for maxillofacial modeling. However, the presence of metallic inserts, such as implants, crowns, and dental filling, causes severe streaking and shading artifacts in a CBCT image and loss of the morphological structures of the teeth, which consequently prevents accurate segmentation of bones. A two-stage metal artifact reduction method is proposed for accurate 3D low-dose maxillofacial CBCT modeling, where a key idea is to utilize explicit tooth shape prior information from intra-oral scan data whose acquisition does not require any extra radiation exposure. In the first stage, an image-to-image deep learning network is employed to mitigate metal-related artifacts. To improve the learning ability, the proposed network is designed to take advantage of the intra-oral scan data as side-inputs and perform multi-task learning of auxiliary tooth segmentation. In the second stage, a 3D maxillofacial model is constructed by segmenting the bones from the dental CBCT image corrected in the first stage. For accurate bone segmentation, weighted thresholding is applied, wherein the weighting region is determined depending on the geometry of the intra-oral scan data. Because acquiring a paired training dataset of metal-artifact-free and metal artifact-affected dental CBCT images is challenging in clinical practice, an automatic method of generating a realistic dataset according to the CBCT physics model is introduced. Numerical simulations and clinical experiments show the feasibility of the proposed method, which takes advantage of tooth surface information from intra-oral scan data in 3D low dose maxillofacial CBCT modeling.



### Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2202.03583v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03583v2)
- **Published**: 2022-02-08 00:43:57+00:00
- **Updated**: 2023-05-07 14:23:27+00:00
- **Authors**: Dipkamal Bhusal, Sanjeeb Prasad Panday
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Traditional methods of identifying pathologies in X-ray images rely heavily on skilled human interpretation and are often time-consuming. The advent of deep learning techniques has enabled the development of automated disease diagnosis systems, but the performance of such systems is dependent on the quality of the model and the level of interpretability it provides. In this paper, we propose a multi-label disease diagnosis model for chest X-rays using a dense convolutional neural network (DenseNet) and model interpretability using GRADCAM. We trained our model using frontal X-rays and evaluated its performance using various quantitative metrics, including the area under the receiver operating characteristic curve (AUC). Our proposed model achieved the highest AUC score of 0.896 for the condition Cardiomegaly with an accuracy of 0.826, while the lowest AUC score was obtained for Nodule, at 0.655 with an accuracy of 0.66. To promote model interpretability and build trust in decision making, we generated heatmaps on X-rays to visualize the regions where the model paid attention to make certain predictions. Additionally, we estimated the uncertainty in model predictions by presenting the confidence interval of our measurements. Our proposed automated disease diagnosis model obtained high performance metrics in multi-label disease diagnosis tasks and provided visualization of model predictions for model interpretability.



### Detecting and Localizing Copy-Move and Image-Splicing Forgery
- **Arxiv ID**: http://arxiv.org/abs/2202.04069v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.04069v1)
- **Published**: 2022-02-08 01:14:30+00:00
- **Updated**: 2022-02-08 01:14:30+00:00
- **Authors**: Aditya Pandey, Anshuman Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: In the world of fake news and deepfakes, there have been an alarmingly large number of cases of images being tampered with and published in newspapers, used in court, and posted on social media for defamation purposes. Detecting these tampered images is an important task and one we try to tackle. In this paper, we focus on the methods to detect if an image has been tampered with using both Deep Learning and Image transformation methods and comparing the performances and robustness of each method. We then attempt to identify the tampered area of the image and predict the corresponding mask. Based on the results, suggestions and approaches are provided to achieve a more robust framework to detect and identify the forgeries.



### Fair SA: Sensitivity Analysis for Fairness in Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.03586v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03586v2)
- **Published**: 2022-02-08 01:16:09+00:00
- **Updated**: 2022-02-09 18:31:40+00:00
- **Authors**: Aparna R. Joshi, Xavier Suau, Nivedha Sivakumar, Luca Zappella, Nicholas Apostoloff
- **Comment**: 8 pages, 5 figures, to be published in NeurIPS 2021 Workshop,
  Algorithmic Fairness through the Lens of Causality and Robustness
- **Journal**: None
- **Summary**: As the use of deep learning in high impact domains becomes ubiquitous, it is increasingly important to assess the resilience of models. One such high impact domain is that of face recognition, with real world applications involving images affected by various degradations, such as motion blur or high exposure. Moreover, images captured across different attributes, such as gender and race, can also challenge the robustness of a face recognition algorithm. While traditional summary statistics suggest that the aggregate performance of face recognition models has continued to improve, these metrics do not directly measure the robustness or fairness of the models. Visual Psychophysics Sensitivity Analysis (VPSA) [1] provides a way to pinpoint the individual causes of failure by way of introducing incremental perturbations in the data. However, perturbations may affect subgroups differently. In this paper, we propose a new fairness evaluation based on robustness in the form of a generic framework that extends VPSA. With this framework, we can analyze the ability of a model to perform fairly for different subgroups of a population affected by perturbations, and pinpoint the exact failure modes for a subgroup by measuring targeted robustness. With the increasing focus on the fairness of models, we use face recognition as an example application of our framework and propose to compactly visualize the fairness analysis of a model via AUC matrices. We analyze the performance of common face recognition models and empirically show that certain subgroups are at a disadvantage when images are perturbed, thereby uncovering trends that were not visible using the model's performance on subgroups without perturbations.



### Model and predict age and sex in healthy subjects using brain white matter features: A deep learning approach
- **Arxiv ID**: http://arxiv.org/abs/2202.03595v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03595v1)
- **Published**: 2022-02-08 01:50:12+00:00
- **Updated**: 2022-02-08 01:50:12+00:00
- **Authors**: Hao He, Fan Zhang, Steve Pieper, Nikos Makris, Yogesh Rathi, William Wells III, Lauren J. O'Donnell
- **Comment**: accepted by ISBI 2022
- **Journal**: None
- **Summary**: The human brain's white matter (WM) structure is of immense interest to the scientific community. Diffusion MRI gives a powerful tool to describe the brain WM structure noninvasively. To potentially enable monitoring of age-related changes and investigation of sex-related brain structure differences on the mapping between the brain connectome and healthy subjects' age and sex, we extract fiber-cluster-based diffusion features and predict sex and age with a novel ensembled neural network classifier. We conduct experiments on the Human Connectome Project (HCP) young adult dataset and show that our model achieves 94.82% accuracy in sex prediction and 2.51 years MAE in age prediction. We also show that the fractional anisotropy (FA) is the most predictive of sex, while the number of fibers is the most predictive of age and the combination of different features can improve the model performance.



### MOST-Net: A Memory Oriented Style Transfer Network for Face Sketch Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2202.03596v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.03596v2)
- **Published**: 2022-02-08 01:51:24+00:00
- **Updated**: 2022-06-08 01:49:18+00:00
- **Authors**: Fan Ji, Muyi Sun, Xingqun Qi, Qi Li, Zhenan Sun
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: Face sketch synthesis has been widely used in multi-media entertainment and law enforcement. Despite the recent developments in deep neural networks, accurate and realistic face sketch synthesis is still a challenging task due to the diversity and complexity of human faces. Current image-to-image translation-based face sketch synthesis frequently encounters over-fitting problems when it comes to small-scale datasets. To tackle this problem, we present an end-to-end Memory Oriented Style Transfer Network (MOST-Net) for face sketch synthesis which can produce high-fidelity sketches with limited data. Specifically, an external self-supervised dynamic memory module is introduced to capture the domain alignment knowledge in the long term. In this way, our proposed model could obtain the domain-transfer ability by establishing the durable relationship between faces and corresponding sketches on the feature level. Furthermore, we design a novel Memory Refinement Loss (MR Loss) for feature alignment in the memory module, which enhances the accuracy of memory slots in an unsupervised manner. Extensive experiments on the CUFS and the CUFSF datasets show that our MOST-Net achieves state-of-the-art performance, especially in terms of the Structural Similarity Index(SSIM).



### Graph-Relational Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2202.03628v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.SI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2202.03628v2)
- **Published**: 2022-02-08 03:53:32+00:00
- **Updated**: 2023-04-21 02:40:15+00:00
- **Authors**: Zihao Xu, Hao He, Guang-He Lee, Yuyang Wang, Hao Wang
- **Comment**: Accepted by ICLR 2022
- **Journal**: None
- **Summary**: Existing domain adaptation methods tend to treat every domain equally and align them all perfectly. Such uniform alignment ignores topological structures among different domains; therefore it may be beneficial for nearby domains, but not necessarily for distant domains. In this work, we relax such uniform alignment by using a domain graph to encode domain adjacency, e.g., a graph of states in the US with each state as a domain and each edge indicating adjacency, thereby allowing domains to align flexibly based on the graph structure. We generalize the existing adversarial learning framework with a novel graph discriminator using encoding-conditioned graph embeddings. Theoretical analysis shows that at equilibrium, our method recovers classic domain adaptation when the graph is a clique, and achieves non-trivial alignment for other types of graphs. Empirical results show that our approach successfully generalizes uniform alignment, naturally incorporates domain information represented by graphs, and improves upon existing domain adaptation methods on both synthetic and real-world datasets. Code will soon be available at https://github.com/Wang-ML-Lab/GRDA.



### A Novel Encoder-Decoder Network with Guided Transmission Map for Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2202.04757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04757v1)
- **Published**: 2022-02-08 03:57:36+00:00
- **Updated**: 2022-02-08 03:57:36+00:00
- **Authors**: Le-Anh Tran, Seokyong Moon, Dong-Chul Park
- **Comment**: 8 pages, 5 figures, iSCSi'22
- **Journal**: None
- **Summary**: A novel Encoder-Decoder Network with Guided Transmission Map (EDN-GTM) for single image dehazing scheme is proposed in this paper. The proposed EDN-GTM takes conventional RGB hazy image in conjunction with its transmission map estimated by adopting dark channel prior as the inputs of the network. The proposed EDN-GTM utilizes U-Net for image segmentation as the core network and utilizes various modifications including spatial pyramid pooling module and Swish activation to achieve state-of-the-art dehazing performance. Experiments on benchmark datasets show that the proposed EDN-GTM outperforms most of traditional and deep learning-based image dehazing schemes in terms of PSNR and SSIM metrics. The proposed EDN-GTM furthermore proves its applicability to object detection problems. Specifically, when applied to an image preprocessing tool for driving object detection, the proposed EDN-GTM can efficiently remove haze and significantly improve detection accuracy by 4.73% in terms of mAP measure. The code is available at: https://github.com/tranleanh/edn-gtm.



### Causal Scene BERT: Improving object detection by searching for challenging groups of data
- **Arxiv ID**: http://arxiv.org/abs/2202.03651v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03651v2)
- **Published**: 2022-02-08 05:14:16+00:00
- **Updated**: 2022-04-21 14:24:12+00:00
- **Authors**: Cinjon Resnick, Or Litany, Amlan Kar, Karsten Kreis, James Lucas, Kyunghyun Cho, Sanja Fidler
- **Comment**: In submission at JMLR; 0xe5110eA3B5014cd9a585Dc76c74Ee509F504Be14
- **Journal**: None
- **Summary**: Modern computer vision applications rely on learning-based perception modules parameterized with neural networks for tasks like object detection. These modules frequently have low expected error overall but high error on atypical groups of data due to biases inherent in the training process. In building autonomous vehicles (AV), this problem is an especially important challenge because their perception modules are crucial to the overall system performance. After identifying failures in AV, a human team will comb through the associated data to group perception failures that share common causes. More data from these groups is then collected and annotated before retraining the model to fix the issue. In other words, error groups are found and addressed in hindsight. Our main contribution is a pseudo-automatic method to discover such groups in foresight by performing causal interventions on simulated scenes. To keep our interventions on the data manifold, we utilize masked language models. We verify that the prioritized groups found via intervention are challenging for the object detector and show that retraining with data collected from these groups helps inordinately compared to adding more IID data. We also plan to release software to run interventions in simulated scenes, which we hope will benefit the causality community.



### How to Understand Masked Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2202.03670v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03670v2)
- **Published**: 2022-02-08 06:15:07+00:00
- **Updated**: 2022-02-09 18:33:57+00:00
- **Authors**: Shuhao Cao, Peng Xu, David A. Clifton
- **Comment**: None
- **Journal**: None
- **Summary**: "Masked Autoencoders (MAE) Are Scalable Vision Learners" revolutionizes the self-supervised learning method in that it not only achieves the state-of-the-art for image pre-training, but is also a milestone that bridges the gap between visual and linguistic masked autoencoding (BERT-style) pre-trainings. However, to our knowledge, to date there are no theoretical perspectives to explain the powerful expressivity of MAE. In this paper, we, for the first time, propose a unified theoretical framework that provides a mathematical understanding for MAE. Specifically, we explain the patch-based attention approaches of MAE using an integral kernel under a non-overlapping domain decomposition setting. To help the research community to further comprehend the main reasons of the great success of MAE, based on our framework, we pose five questions and answer them with mathematical rigor using insights from operator theory.



### CAD-RADS Scoring using Deep Learning and Task-Specific Centerline Labeling
- **Arxiv ID**: http://arxiv.org/abs/2202.03671v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03671v1)
- **Published**: 2022-02-08 06:22:03+00:00
- **Updated**: 2022-02-08 06:22:03+00:00
- **Authors**: Felix Denzinger, Michael Wels, Oliver Taubmann, Mehmet A. Gülsün, Max Schöbinger, Florian André, Sebastian J. Buss, Johannes Görich, Michael Sühling, Andreas Maier, Katharina Breininger
- **Comment**: Under review MIDL 2020
- **Journal**: None
- **Summary**: With coronary artery disease (CAD) persisting to be one of the leading causes of death worldwide, interest in supporting physicians with algorithms to speed up and improve diagnosis is high. In clinical practice, the severeness of CAD is often assessed with a coronary CT angiography (CCTA) scan and manually graded with the CAD-Reporting and Data System (CAD-RADS) score. The clinical questions this score assesses are whether patients have CAD or not (rule-out) and whether they have severe CAD or not (hold-out). In this work, we reach new state-of-the-art performance for automatic CAD-RADS scoring. We propose using severity-based label encoding, test time augmentation (TTA) and model ensembling for a task-specific deep learning architecture. Furthermore, we introduce a novel task- and model-specific, heuristic coronary segment labeling, which subdivides coronary trees into consistent parts across patients. It is fast, robust, and easy to implement. We were able to raise the previously reported area under the receiver operating characteristic curve (AUC) from 0.914 to 0.942 in the rule-out and from 0.921 to 0.950 in the hold-out task respectively.



### Trained Model in Supervised Deep Learning is a Conditional Risk Minimizer
- **Arxiv ID**: http://arxiv.org/abs/2202.03674v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03674v1)
- **Published**: 2022-02-08 06:39:09+00:00
- **Updated**: 2022-02-08 06:39:09+00:00
- **Authors**: Yutong Xie, Dufan Wu, Bin Dong, Quanzheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: We proved that a trained model in supervised deep learning minimizes the conditional risk for each input (Theorem 2.1). This property provided insights into the behavior of trained models and established a connection between supervised and unsupervised learning in some cases. In addition, when the labels are intractable but can be written as a conditional risk minimizer, we proved an equivalent form of the original supervised learning problem with accessible labels (Theorem 2.2). We demonstrated that many existing works, such as Noise2Score, Noise2Noise and score function estimation can be explained by our theorem. Moreover, we derived a property of classification problem with noisy labels using Theorem 2.1 and validated it using MNIST dataset. Furthermore, We proposed a method to estimate uncertainty in image super-resolution based on Theorem 2.2 and validated it using ImageNet dataset. Our code is available on github.



### A Novel Image Descriptor with Aggregated Semantic Skeleton Representation for Long-term Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.03677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03677v1)
- **Published**: 2022-02-08 06:49:38+00:00
- **Updated**: 2022-02-08 06:49:38+00:00
- **Authors**: Nie Jiwei, Feng Joe-Mei, Xue Dingyu, Pan Feng, Liu Wei, Hu Jun, Cheng Shuai
- **Comment**: None
- **Journal**: None
- **Summary**: In a Simultaneous Localization and Mapping (SLAM) system, a loop-closure can eliminate accumulated errors, which is accomplished by Visual Place Recognition (VPR), a task that retrieves the current scene from a set of pre-stored sequential images through matching specific scene-descriptors. In urban scenes, the appearance variation caused by seasons and illumination has brought great challenges to the robustness of scene descriptors. Semantic segmentation images can not only deliver the shape information of objects but also their categories and spatial relations that will not be affected by the appearance variation of the scene. Innovated by the Vector of Locally Aggregated Descriptor (VLAD), in this paper, we propose a novel image descriptor with aggregated semantic skeleton representation (SSR), dubbed SSR-VLAD, for the VPR under drastic appearance-variation of environments. The SSR-VLAD of one image aggregates the semantic skeleton features of each category and encodes the spatial-temporal distribution information of the image semantic information. We conduct a series of experiments on three public datasets of challenging urban scenes. Compared with four state-of-the-art VPR methods- CoHOG, NetVLAD, LOST-X, and Region-VLAD, VPR by matching SSR-VLAD outperforms those methods and maintains competitive real-time performance at the same time.



### Quality Metric Guided Portrait Line Drawing Generation from Unpaired Training Data
- **Arxiv ID**: http://arxiv.org/abs/2202.03678v1
- **DOI**: 10.1109/TPAMI.2022.3147570
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2202.03678v1)
- **Published**: 2022-02-08 06:49:57+00:00
- **Updated**: 2022-02-08 06:49:57+00:00
- **Authors**: Ran Yi, Yong-Jin Liu, Yu-Kun Lai, Paul L. Rosin
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence, https://doi.org/10.1109/TPAMI.2022.3147570, code:
  https://github.com/yiranran/QMUPD
- **Journal**: None
- **Summary**: Face portrait line drawing is a unique style of art which is highly abstract and expressive. However, due to its high semantic constraints, many existing methods learn to generate portrait drawings using paired training data, which is costly and time-consuming to obtain. In this paper, we propose a novel method to automatically transform face photos to portrait drawings using unpaired training data with two new features; i.e., our method can (1) learn to generate high quality portrait drawings in multiple styles using a single network and (2) generate portrait drawings in a "new style" unseen in the training data. To achieve these benefits, we (1) propose a novel quality metric for portrait drawings which is learned from human perception, and (2) introduce a quality loss to guide the network toward generating better looking portrait drawings. We observe that existing unpaired translation methods such as CycleGAN tend to embed invisible reconstruction information indiscriminately in the whole drawings due to significant information imbalance between the photo and portrait drawing domains, which leads to important facial features missing. To address this problem, we propose a novel asymmetric cycle mapping that enforces the reconstruction information to be visible and only embedded in the selected facial regions. Along with localized discriminators for important facial regions, our method well preserves all important facial features in the generated drawings. Generator dissection further explains that our model learns to incorporate face semantic information during drawing generation. Extensive experiments including a user study show that our model outperforms state-of-the-art methods.



### Exploring Inter-Channel Correlation for Diversity-preserved KnowledgeDistillation
- **Arxiv ID**: http://arxiv.org/abs/2202.03680v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.03680v1)
- **Published**: 2022-02-08 07:01:56+00:00
- **Updated**: 2022-02-08 07:01:56+00:00
- **Authors**: Li Liu, Qingle Huang, Sihao Lin, Hongwei Xie, Bing Wang, Xiaojun Chang, Xiaodan Liang
- **Comment**: Accepted by ICCV 2021
- **Journal**: None
- **Summary**: Knowledge Distillation has shown very promising abil-ity in transferring learned representation from the largermodel (teacher) to the smaller one (student).Despitemany efforts, prior methods ignore the important role ofretaining inter-channel correlation of features, leading tothe lack of capturing intrinsic distribution of the featurespace and sufficient diversity properties of features in theteacher network.To solve the issue, we propose thenovel Inter-Channel Correlation for Knowledge Distillation(ICKD), with which the diversity and homology of the fea-ture space of the student network can align with that ofthe teacher network. The correlation between these twochannels is interpreted as diversity if they are irrelevantto each other, otherwise homology. Then the student isrequired to mimic the correlation within its own embed-ding space. In addition, we introduce the grid-level inter-channel correlation, making it capable of dense predictiontasks. Extensive experiments on two vision tasks, includ-ing ImageNet classification and Pascal VOC segmentation,demonstrate the superiority of our ICKD, which consis-tently outperforms many existing methods, advancing thestate-of-the-art in the fields of Knowledge Distillation. Toour knowledge, we are the first method based on knowl-edge distillation boosts ResNet18 beyond 72% Top-1 ac-curacy on ImageNet classification. Code is available at:https://github.com/ADLab-AutoDrive/ICKD.



### Network Comparison Study of Deep Activation Feature Discriminability with Novel Objects
- **Arxiv ID**: http://arxiv.org/abs/2202.03695v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03695v1)
- **Published**: 2022-02-08 07:40:53+00:00
- **Updated**: 2022-02-08 07:40:53+00:00
- **Authors**: Michael Karnes, Alper Yilmaz
- **Comment**: None
- **Journal**: None
- **Summary**: Feature extraction has always been a critical component of the computer vision field. More recently, state-of-the-art computer visions algorithms have incorporated Deep Neural Networks (DNN) in feature extracting roles, creating Deep Convolutional Activation Features (DeCAF). The transferability of DNN knowledge domains has enabled the wide use of pretrained DNN feature extraction for applications with novel object classes, especially those with limited training data. This study analyzes the general discriminability of novel object visual appearances encoded into the DeCAF space of six of the leading visual recognition DNN architectures. The results of this study characterize the Mahalanobis distances and cosine similarities between DeCAF object manifolds across two visual object tracking benchmark data sets. The backgrounds surrounding each object are also included as an object classes in the manifold analysis, providing a wider range of novel classes. This study found that different network architectures led to different network feature focuses that must to be considered in the network selection process. These results are generated from the VOT2015 and UAV123 benchmark data sets; however, the proposed methods can be applied to efficiently compare estimated network performance characteristics for any labeled visual data set.



### Self-Paced Imbalance Rectification for Class Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.03703v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03703v2)
- **Published**: 2022-02-08 07:58:13+00:00
- **Updated**: 2022-02-19 02:52:42+00:00
- **Authors**: Zhiheng Liu, Kai Zhu, Yang Cao
- **Comment**: There are some parts to be improved in the paper
- **Journal**: None
- **Summary**: Exemplar-based class-incremental learning is to recognize new classes while not forgetting old ones, whose samples can only be saved in limited memory. The ratio fluctuation of new samples to old exemplars, which is caused by the variation of memory capacity at different environments, will bring challenges to stabilize the incremental optimization process. To address this problem, we propose a novel self-paced imbalance rectification scheme, which dynamically maintains the incremental balance during the representation learning phase. Specifically, our proposed scheme consists of a frequency compensation strategy that adjusts the logits margin between old and new classes with the corresponding number ratio to strengthen the expression ability of the old classes, and an inheritance transfer strategy to reduce the representation confusion by estimating the similarity of different classes in the old embedding space. Furthermore, a chronological attenuation mechanism is proposed to mitigate the repetitive optimization of the older classes at multiple step-wise increments. Extensive experiments on three benchmarks demonstrate stable incremental performance, significantly outperforming the state-of-the-art methods.



### What's Cracking? A Review and Analysis of Deep Learning Methods for Structural Crack Segmentation, Detection and Quantification
- **Arxiv ID**: http://arxiv.org/abs/2202.03714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03714v1)
- **Published**: 2022-02-08 08:22:26+00:00
- **Updated**: 2022-02-08 08:22:26+00:00
- **Authors**: Jacob König, Mark Jenkins, Mike Mannion, Peter Barrie, Gordon Morison
- **Comment**: None
- **Journal**: None
- **Summary**: Surface cracks are a very common indicator of potential structural faults. Their early detection and monitoring is an important factor in structural health monitoring. Left untreated, they can grow in size over time and require expensive repairs or maintenance. With recent advances in computer vision and deep learning algorithms, the automatic detection and segmentation of cracks for this monitoring process have become a major topic of interest. This review aims to give researchers an overview of the published work within the field of crack analysis algorithms that make use of deep learning. It outlines the various tasks that are solved through applying computer vision algorithms to surface cracks in a structural health monitoring setting and also provides in-depth reviews of recent fully, semi and unsupervised approaches that perform crack classification, detection, segmentation and quantification. Additionally, this review also highlights popular datasets used for cracks and the metrics that are used to evaluate the performance of those algorithms. Finally, potential research gaps are outlined and further research directions are provided.



### Binary Neural Networks as a general-propose compute paradigm for on-device computer vision
- **Arxiv ID**: http://arxiv.org/abs/2202.03716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03716v1)
- **Published**: 2022-02-08 08:38:22+00:00
- **Updated**: 2022-02-08 08:38:22+00:00
- **Authors**: Guhong Nie, Lirui Xiao, Menglong Zhu, Dongliang Chu, Yue Shen, Peng Li, Kang Yang, Li Du, Bo Chen
- **Comment**: 13 pages, 3 figures
- **Journal**: None
- **Summary**: For binary neural networks (BNNs) to become the mainstream on-device computer vision algorithm, they must achieve a superior speed-vs-accuracy tradeoff than 8-bit quantization and establish a similar degree of general applicability in vision tasks. To this end, we propose a BNN framework comprising 1) a minimalistic inference scheme for hardware-friendliness, 2) an over-parameterized training scheme for high accuracy, and 3) a simple procedure to adapt to different vision tasks. The resultant framework overtakes 8-bit quantization in the speed-vs-accuracy tradeoff for classification, detection, segmentation, super-resolution and matching: our BNNs not only retain the accuracy levels of their 8-bit baselines but also showcase 1.3-2.4$\times$ faster FPS on mobile CPUs. Similar conclusions can be drawn for prototypical systolic-array-based AI accelerators, where our BNNs promise 2.8-7$\times$ fewer execution cycles than 8-bit and 2.1-2.7$\times$ fewer cycles than alternative BNN designs. These results suggest that the time for large-scale BNN adoption could be upon us.



### Hair Color Digitization through Imaging and Deep Inverse Graphics
- **Arxiv ID**: http://arxiv.org/abs/2202.03723v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2202.03723v1)
- **Published**: 2022-02-08 08:57:04+00:00
- **Updated**: 2022-02-08 08:57:04+00:00
- **Authors**: Robin Kips, Panagiotis-Alexandros Bokaris, Matthieu Perrot, Pietro Gori, Isabelle Bloch
- **Comment**: Electronic Imaging (EI) 2022
- **Journal**: None
- **Summary**: Hair appearance is a complex phenomenon due to hair geometry and how the light bounces on different hair fibers. For this reason, reproducing a specific hair color in a rendering environment is a challenging task that requires manual work and expert knowledge in computer graphics to tune the result visually. While current hair capture methods focus on hair shape estimation many applications could benefit from an automated method for capturing the appearance of a physical hair sample, from augmented/virtual reality to hair dying development. Building on recent advances in inverse graphics and material capture using deep neural networks, we introduce a novel method for hair color digitization. Our proposed pipeline allows capturing the color appearance of a physical hair sample and renders synthetic images of hair with a similar appearance, simulating different hair styles and/or lighting environments. Since rendering realistic hair images requires path-tracing rendering, the conventional inverse graphics approach based on differentiable rendering is untractable. Our method is based on the combination of a controlled imaging device, a path-tracing renderer, and an inverse graphics model based on self-supervised machine learning, which does not require to use differentiable rendering to be trained. We illustrate the performance of our hair digitization method on both real and synthetic images and show that our approach can accurately capture and render hair color.



### Navigating to Objects in Unseen Environments by Distance Prediction
- **Arxiv ID**: http://arxiv.org/abs/2202.03735v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03735v2)
- **Published**: 2022-02-08 09:22:50+00:00
- **Updated**: 2022-07-13 12:12:07+00:00
- **Authors**: Minzhao Zhu, Binglei Zhao, Tao Kong
- **Comment**: IROS 2022, Video at https://www.youtube.com/watch?v=R79pWVGFKS4
- **Journal**: None
- **Summary**: Object Goal Navigation (ObjectNav) task is to navigate an agent to an object category in unseen environments without a pre-built map. In this paper, we solve this task by predicting the distance to the target using semantically-related objects as cues. Based on the estimated distance to the target object, our method directly choose optimal mid-term goals that are more likely to have a shorter path to the target. Specifically, based on the learned knowledge, our model takes a bird's-eye view semantic map as input, and estimates the path length from the frontier map cells to the target object. With the estimated distance map, the agent could simultaneously explore the environment and navigate to the target objects based on a simple human-designed strategy. Empirical results in visually realistic simulation environments show that the proposed method outperforms a wide range of baselines on success rate and efficiency. Real-robot experiment also demonstrates that our method generalizes well to the real world. Video at https://www.youtube.com/watch?v=R79pWVGFKS4



### A Survey of Breast Cancer Screening Techniques: Thermography and Electrical Impedance Tomography
- **Arxiv ID**: http://arxiv.org/abs/2202.03737v1
- **DOI**: 10.1080/03091902.2019.1664672
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03737v1)
- **Published**: 2022-02-08 09:25:30+00:00
- **Updated**: 2022-02-08 09:25:30+00:00
- **Authors**: Juan Zuluaga-Gomez, N. Zerhouni, Z. Al Masry, C. Devalland, C. Varnier
- **Comment**: Article published at: Journal of Medical Engineering & Technology
  (Volume 43, 2019 - Issue 5)
- **Journal**: None
- **Summary**: Breast cancer is a disease that threatens many women's life, thus, early and accurate detection plays a key role in reducing the mortality rate. Mammography stands as the reference technique for breast cancer screening; nevertheless, many countries still lack access to mammograms due to economic, social, and cultural issues. Last advances in computational tools, infrared cameras, and devices for bio-impedance quantification allowed the development of parallel techniques like thermography, infrared imaging, and electrical impedance tomography, these being faster, reliable and cheaper. In the last decades, these have been considered as complement procedures for breast cancer diagnosis, where many studies concluded that false positive and false negative rates are greatly reduced. This work aims to review the last breakthroughs about the three above-mentioned techniques describing the benefits of mixing several computational skills to obtain a better global performance. In addition, we provide a comparison between several machine learning techniques applied to breast cancer diagnosis going from logistic regression, decision trees, and random forest to artificial, deep, and convolutional neural networks. Finally, it is mentioned several recommendations for 3D breast simulations, pre-processing techniques, biomedical devices in the research field, prediction of tumor location and size.



### Consistency-Regularized Region-Growing Network for Semantic Segmentation of Urban Scenes with Point-Level Annotations
- **Arxiv ID**: http://arxiv.org/abs/2202.03740v2
- **DOI**: 10.1109/TIP.2022.3189825
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03740v2)
- **Published**: 2022-02-08 09:27:01+00:00
- **Updated**: 2022-06-18 13:33:00+00:00
- **Authors**: Yonghao Xu, Pedram Ghamisi
- **Comment**: None
- **Journal**: IEEE Trans. Image Process., vol. 31, pp. 5038-5051, 2022
- **Summary**: Deep learning algorithms have obtained great success in semantic segmentation of very high-resolution (VHR) images. Nevertheless, training these models generally requires a large amount of accurate pixel-wise annotations, which is very laborious and time-consuming to collect. To reduce the annotation burden, this paper proposes a consistency-regularized region-growing network (CRGNet) to achieve semantic segmentation of VHR images with point-level annotations. The key idea of CRGNet is to iteratively select unlabeled pixels with high confidence to expand the annotated area from the original sparse points. However, since there may exist some errors and noises in the expanded annotations, directly learning from them may mislead the training of the network. To this end, we further propose the consistency regularization strategy, where a base classifier and an expanded classifier are employed. Specifically, the base classifier is supervised by the original sparse annotations, while the expanded classifier aims to learn from the expanded annotations generated by the base classifier with the region-growing mechanism. The consistency regularization is thereby achieved by minimizing the discrepancy between the predictions from both the base and the expanded classifiers. We find such a simple regularization strategy is yet very useful to control the quality of the region-growing mechanism. Extensive experiments on two benchmark datasets demonstrate that the proposed CRGNet significantly outperforms the existing state-of-the-art methods. Codes and pre-trained models are available online (https://github.com/YonghaoXu/CRGNet).



### STC: Spatio-Temporal Contrastive Learning for Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.03747v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03747v2)
- **Published**: 2022-02-08 09:34:26+00:00
- **Updated**: 2022-08-20 08:14:18+00:00
- **Authors**: Zhengkai Jiang, Zhangxuan Gu, Jinlong Peng, Hang Zhou, Liang Liu, Yabiao Wang, Ying Tai, Chengjie Wang, Liqing Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Video Instance Segmentation (VIS) is a task that simultaneously requires classification, segmentation, and instance association in a video. Recent VIS approaches rely on sophisticated pipelines to achieve this goal, including RoI-related operations or 3D convolutions. In contrast, we present a simple and efficient single-stage VIS framework based on the instance segmentation method CondInst by adding an extra tracking head. To improve instance association accuracy, a novel bi-directional spatio-temporal contrastive learning strategy for tracking embedding across frames is proposed. Moreover, an instance-wise temporal consistency scheme is utilized to produce temporally coherent results. Experiments conducted on the YouTube-VIS-2019, YouTube-VIS-2021, and OVIS-2021 datasets validate the effectiveness and efficiency of the proposed method. We hope the proposed framework can serve as a simple and strong alternative for many other instance-level video association tasks.



### Addressing Data Scarcity in Multimodal User State Recognition by Combining Semi-Supervised and Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.03775v1
- **DOI**: 10.1145/3461615.3486575
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03775v1)
- **Published**: 2022-02-08 10:41:41+00:00
- **Updated**: 2022-02-08 10:41:41+00:00
- **Authors**: Hendric Voß, Heiko Wersing, Stefan Kopp
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting mental states of human users is crucial for the development of cooperative and intelligent robots, as it enables the robot to understand the user's intentions and desires. Despite their importance, it is difficult to obtain a large amount of high quality data for training automatic recognition algorithms as the time and effort required to collect and label such data is prohibitively high. In this paper we present a multimodal machine learning approach for detecting dis-/agreement and confusion states in a human-robot interaction environment, using just a small amount of manually annotated data. We collect a data set by conducting a human-robot interaction study and develop a novel preprocessing pipeline for our machine learning approach. By combining semi-supervised and supervised architectures, we are able to achieve an average F1-score of 81.1\% for dis-/agreement detection with a small amount of labeled data and a large unlabeled data set, while simultaneously increasing the robustness of the model compared to the supervised approach.



### SCR: Smooth Contour Regression with Geometric Priors
- **Arxiv ID**: http://arxiv.org/abs/2202.03784v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03784v1)
- **Published**: 2022-02-08 11:07:51+00:00
- **Updated**: 2022-02-08 11:07:51+00:00
- **Authors**: Gaetan Bahl, Lionel Daniel, Florent Lafarge
- **Comment**: None
- **Journal**: None
- **Summary**: While object detection methods traditionally make use of pixel-level masks or bounding boxes, alternative representations such as polygons or active contours have recently emerged. Among them, methods based on the regression of Fourier or Chebyshev coefficients have shown high potential on freeform objects. By defining object shapes as polar functions, they are however limited to star-shaped domains. We address this issue with SCR: a method that captures resolution-free object contours as complex periodic functions. The method offers a good compromise between accuracy and compactness thanks to the design of efficient geometric shape priors. We benchmark SCR on the popular COCO 2017 instance segmentation dataset, and show its competitiveness against existing algorithms in the field. In addition, we design a compact version of our network, which we benchmark on embedded hardware with a wide range of power targets, achieving up to real-time performance.



### Latent gaze information in highly dynamic decision-tasks
- **Arxiv ID**: http://arxiv.org/abs/2202.04072v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.04072v1)
- **Published**: 2022-02-08 11:43:10+00:00
- **Updated**: 2022-02-08 11:43:10+00:00
- **Authors**: Benedikt Hosp
- **Comment**: Doctoral dissertation
- **Journal**: None
- **Summary**: Digitization is penetrating more and more areas of life. Tasks are increasingly being completed digitally, and are therefore not only fulfilled faster, more efficiently but also more purposefully and successfully. The rapid developments in the field of artificial intelligence in recent years have played a major role in this, as they brought up many helpful approaches to build on. At the same time, the eyes, their movements, and the meaning of these movements are being progressively researched. The combination of these developments has led to exciting approaches. In this dissertation, I present some of these approaches which I worked on during my Ph.D.   First, I provide insight into the development of models that use artificial intelligence to connect eye movements with visual expertise. This is demonstrated for two domains or rather groups of people: athletes in decision-making actions and surgeons in arthroscopic procedures. The resulting models can be considered as digital diagnostic models for automatic expertise recognition. Furthermore, I show approaches that investigate the transferability of eye movement patterns to different expertise domains and subsequently, important aspects of techniques for generalization. Finally, I address the temporal detection of confusion based on eye movement data. The results suggest the use of the resulting model as a clock signal for possible digital assistance options in the training of young professionals. An interesting aspect of my research is that I was able to draw on very valuable data from DFB youth elite athletes as well as on long-standing experts in arthroscopy. In particular, the work with the DFB data attracted the interest of radio and print media, namely DeutschlandFunk Nova and SWR DasDing. All resulting articles presented here have been published in internationally renowned journals or at conferences.



### Ada-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space
- **Arxiv ID**: http://arxiv.org/abs/2202.03800v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03800v3)
- **Published**: 2022-02-08 11:44:45+00:00
- **Updated**: 2022-10-08 08:17:05+00:00
- **Authors**: Yaohua Wang, Yaobin Zhang, Fangyi Zhang, Ming Lin, YuQi Zhang, Senzhang Wang, Xiuyu Sun
- **Comment**: Accepted by the ICLR 2022. Homepage: https://thomas-wyh.github.io/
- **Journal**: None
- **Summary**: Face clustering has attracted rising research interest recently to take advantage of massive amounts of face images on the web. State-of-the-art performance has been achieved by Graph Convolutional Networks (GCN) due to their powerful representation capacity. However, existing GCN-based methods build face graphs mainly according to kNN relations in the feature space, which may lead to a lot of noise edges connecting two faces of different classes. The face features will be polluted when messages pass along these noise edges, thus degrading the performance of GCNs. In this paper, a novel algorithm named Ada-NETS is proposed to cluster faces by constructing clean graphs for GCNs. In Ada-NETS, each face is transformed to a new structure space, obtaining robust features by considering face features of the neighbour images. Then, an adaptive neighbour discovery strategy is proposed to determine a proper number of edges connecting to each face image. It significantly reduces the noise edges while maintaining the good ones to build a graph with clean yet rich edges for GCNs to cluster faces. Experiments on multiple public clustering datasets show that Ada-NETS significantly outperforms current state-of-the-art methods, proving its superiority and generalization. Code is available at https://github.com/damo-cv/Ada-NETS.



### A Novel Plug-in Module for Fine-Grained Visual Classification
- **Arxiv ID**: http://arxiv.org/abs/2202.03822v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.03822v1)
- **Published**: 2022-02-08 12:35:58+00:00
- **Updated**: 2022-02-08 12:35:58+00:00
- **Authors**: Po-Yung Chou, Cheng-Hung Lin, Wen-Chung Kao
- **Comment**: None
- **Journal**: None
- **Summary**: Visual classification can be divided into coarse-grained and fine-grained classification. Coarse-grained classification represents categories with a large degree of dissimilarity, such as the classification of cats and dogs, while fine-grained classification represents classifications with a large degree of similarity, such as cat species, bird species, and the makes or models of vehicles. Unlike coarse-grained visual classification, fine-grained visual classification often requires professional experts to label data, which makes data more expensive. To meet this challenge, many approaches propose to automatically find the most discriminative regions and use local features to provide more precise features. These approaches only require image-level annotations, thereby reducing the cost of annotation. However, most of these methods require two- or multi-stage architectures and cannot be trained end-to-end. Therefore, we propose a novel plug-in module that can be integrated to many common backbones, including CNN-based or Transformer-based networks to provide strongly discriminative regions. The plugin module can output pixel-level feature maps and fuse filtered features to enhance fine-grained visual classification. Experimental results show that the proposed plugin module outperforms state-of-the-art approaches and significantly improves the accuracy to 92.77\% and 92.83\% on CUB200-2011 and NABirds, respectively. We have released our source code in Github https://github.com/chou141253/FGVC-PIM.git.



### On the Pitfalls of Using the Residual Error as Anomaly Score
- **Arxiv ID**: http://arxiv.org/abs/2202.03826v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03826v1)
- **Published**: 2022-02-08 12:45:10+00:00
- **Updated**: 2022-02-08 12:45:10+00:00
- **Authors**: Felix Meissen, Benedikt Wiestler, Georgios Kaissis, Daniel Rueckert
- **Comment**: 8 pages, 4 figures, under Review for MIDL 2022
- **Journal**: None
- **Summary**: Many current state-of-the-art methods for anomaly localization in medical images rely on calculating a residual image between a potentially anomalous input image and its "healthy" reconstruction. As the reconstruction of the unseen anomalous region should be erroneous, this yields large residuals as a score to detect anomalies in medical images. However, this assumption does not take into account residuals resulting from imperfect reconstructions of the machine learning models used. Such errors can easily overshadow residuals of interest and therefore strongly question the use of residual images as scoring function. Our work explores this fundamental problem of residual images in detail. We theoretically define the problem and thoroughly evaluate the influence of intensity and texture of anomalies against the effect of imperfect reconstructions in a series of experiments. Code and experiments are available under https://github.com/FeliMe/residual-score-pitfalls



### A Unified Multi-Task Learning Framework of Real-Time Drone Supervision for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2202.03843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03843v1)
- **Published**: 2022-02-08 13:07:38+00:00
- **Updated**: 2022-02-08 13:07:38+00:00
- **Authors**: Siqi Gu, Zhichao Lian
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a novel Unified Multi-Task Learning Framework of Real-Time Drone Supervision for Crowd Counting (MFCC) is proposed, which utilizes an image fusion network architecture to fuse images from the visible and thermal infrared image, and a crowd counting network architecture to estimate the density map. The purpose of our framework is to fuse two modalities, including visible and thermal infrared images captured by drones in real-time, that exploit the complementary information to accurately count the dense population and then automatically guide the flight of the drone to supervise the dense crowd. To this end, we propose the unified multi-task learning framework for crowd counting for the first time and re-design the unified training loss functions to align the image fusion network and crowd counting network. We also design the Assisted Learning Module (ALM) to fuse the density map feature to the image fusion encoder process for learning the counting features. To improve the accuracy, we propose the Extensive Context Extraction Module (ECEM) that is based on a dense connection architecture to encode multi-receptive-fields contextual information and apply the Multi-domain Attention Block (MAB) for concerning the head region in the drone view. Finally, we apply the prediction map to automatically guide the drones to supervise the dense crowd. The experimental results on the DroneRGBT dataset show that, compared with the existing methods, ours has comparable results on objective evaluations and an easier training process.



### Class Density and Dataset Quality in High-Dimensional, Unstructured Data
- **Arxiv ID**: http://arxiv.org/abs/2202.03856v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03856v1)
- **Published**: 2022-02-08 13:41:14+00:00
- **Updated**: 2022-02-08 13:41:14+00:00
- **Authors**: Adam Byerly, Tatiana Kalganova
- **Comment**: 13 pages, 27 tables
- **Journal**: None
- **Summary**: We provide a definition for class density that can be used to measure the aggregate similarity of the samples within each of the classes in a high-dimensional, unstructured dataset. We then put forth several candidate methods for calculating class density and analyze the correlation between the values each method produces with the corresponding individual class test accuracies achieved on a trained model. Additionally, we propose a definition for dataset quality for high-dimensional, unstructured data and show that those datasets that met a certain quality threshold (experimentally demonstrated to be > 10 for the datasets studied) were candidates for eliding redundant data based on the individual class densities.



### Learning Optical Flow with Adaptive Graph Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2202.03857v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03857v1)
- **Published**: 2022-02-08 13:41:20+00:00
- **Updated**: 2022-02-08 13:41:20+00:00
- **Authors**: Ao Luo, Fan Yang, Kunming Luo, Xin Li, Haoqiang Fan, Shuaicheng Liu
- **Comment**: To appear in AAAI-22
- **Journal**: None
- **Summary**: Estimating per-pixel motion between video frames, known as optical flow, is a long-standing problem in video understanding and analysis. Most contemporary optical flow techniques largely focus on addressing the cross-image matching with feature similarity, with few methods considering how to explicitly reason over the given scene for achieving a holistic motion understanding. In this work, taking a fresh perspective, we introduce a novel graph-based approach, called adaptive graph reasoning for optical flow (AGFlow), to emphasize the value of scene/context information in optical flow. Our key idea is to decouple the context reasoning from the matching procedure, and exploit scene information to effectively assist motion estimation by learning to reason over the adaptive graph. The proposed AGFlow can effectively exploit the context information and incorporate it within the matching procedure, producing more robust and accurate results. On both Sintel clean and final passes, our AGFlow achieves the best accuracy with EPE of 1.43 and 2.47 pixels, outperforming state-of-the-art approaches by 11.2% and 13.6%, respectively.



### BIQ2021: A Large-Scale Blind Image Quality Assessment Database
- **Arxiv ID**: http://arxiv.org/abs/2202.03879v2
- **DOI**: 10.1117/1.JEI.31.5.053010
- **Categories**: **cs.CV**, eess.IV, 68U10, 68T10, 65D19, I.4.4; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2202.03879v2)
- **Published**: 2022-02-08 14:07:38+00:00
- **Updated**: 2023-01-10 21:31:46+00:00
- **Authors**: Nisar Ahmed, Shahzad Asif
- **Comment**: Journal of Electronic Imaging, Vol. 31, Issue 5: 16 pages
- **Journal**: Journal of Electronic Imaging 31(5), 053010 (13 September 2022)
- **Summary**: The assessment of the perceptual quality of digital images is becoming increasingly important as a result of the widespread use of digital multimedia devices. Smartphones and high-speed internet are just two examples of technologies that have multiplied the amount of multimedia content available. Thus, obtaining a representative dataset, which is required for objective quality assessment training, is a significant challenge. The Blind Image Quality Assessment Database, BIQ2021, is presented in this article. By selecting images with naturally occurring distortions and reliable labeling, the dataset addresses the challenge of obtaining representative images for no-reference image quality assessment. The dataset consists of three sets of images: those taken without the intention of using them for image quality assessment, those taken with intentionally introduced natural distortions, and those taken from an open-source image-sharing platform. It is attempted to maintain a diverse collection of images from various devices, containing a variety of different types of objects and varying degrees of foreground and background information. To obtain reliable scores, these images are subjectively scored in a laboratory environment using a single stimulus method. The database contains information about subjective scoring, human subject statistics, and the standard deviation of each image. The dataset's Mean Opinion Scores (MOS) make it useful for assessing visual quality. Additionally, the proposed database is used to evaluate existing blind image quality assessment approaches, and the scores are analyzed using Pearson and Spearman's correlation coefficients. The image database and MOS are freely available for use and benchmarking.



### The EMory BrEast imaging Dataset (EMBED): A Racially Diverse, Granular Dataset of 3.5M Screening and Diagnostic Mammograms
- **Arxiv ID**: http://arxiv.org/abs/2202.04073v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.04073v1)
- **Published**: 2022-02-08 14:40:59+00:00
- **Updated**: 2022-02-08 14:40:59+00:00
- **Authors**: Jiwoong J. Jeong, Brianna L. Vey, Ananth Reddy, Thomas Kim, Thiago Santos, Ramon Correa, Raman Dutt, Marina Mosunjac, Gabriela Oprea-Ilies, Geoffrey Smith, Minjae Woo, Christopher R. McAdams, Mary S. Newell, Imon Banerjee, Judy Gichoya, Hari Trivedi
- **Comment**: None
- **Journal**: None
- **Summary**: Developing and validating artificial intelligence models in medical imaging requires datasets that are large, granular, and diverse. To date, the majority of publicly available breast imaging datasets lack in one or more of these areas. Models trained on these data may therefore underperform on patient populations or pathologies that have not previously been encountered. The EMory BrEast imaging Dataset (EMBED) addresses these gaps by providing 3650,000 2D and DBT screening and diagnostic mammograms for 116,000 women divided equally between White and African American patients. The dataset also contains 40,000 annotated lesions linked to structured imaging descriptors and 61 ground truth pathologic outcomes grouped into six severity classes. Our goal is to share this dataset with research partners to aid in development and validation of breast AI models that will serve all patients fairly and help decrease bias in medical AI.



### HALS: A Height-Aware Lidar Super-Resolution Framework for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2202.03901v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03901v2)
- **Published**: 2022-02-08 14:43:47+00:00
- **Updated**: 2022-12-07 17:07:54+00:00
- **Authors**: George Eskandar, Sanjeev Sudarsan, Karim Guirguis, Janaranjani Palaniswamy, Bharath Somashekar, Bin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Lidar sensors are costly yet critical for understanding the 3D environment in autonomous driving. High-resolution sensors provide more details about the surroundings because they contain more vertical beams, but they come at a much higher cost, limiting their inclusion in autonomous vehicles. Upsampling lidar pointclouds is a promising approach to gain the benefits of high resolution while maintaining an affordable cost. Although there exist many pointcloud upsampling frameworks, a consistent comparison of these works against each other on the same dataset using unified metrics is still missing. In the first part of this paper, we propose to benchmark existing methods on the Kitti dataset. In the second part, we introduce a novel lidar upsampling model, HALS: Height-Aware Lidar Super-resolution. HALS exploits the observation that lidar scans exhibit a height-aware range distribution and adopts a generator architecture with multiple upsampling branches of different receptive fields. HALS regresses polar coordinates instead of spherical coordinates and uses a surface-normal loss. Extensive experiments show that HALS achieves state-of-the-art performance on 3 real-world lidar datasets.



### Edge-based fever screening system over private 5G
- **Arxiv ID**: http://arxiv.org/abs/2202.03917v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03917v1)
- **Published**: 2022-02-08 15:09:16+00:00
- **Updated**: 2022-02-08 15:09:16+00:00
- **Authors**: Murugan Sankaradas, Kunal Rao, Ravi Rajendran, Amit Redkar, Srimat Chakradhar
- **Comment**: None
- **Journal**: None
- **Summary**: Edge computing and 5G have made it possible to perform analytics closer to the source of data and achieve super-low latency response times, which is not possible with centralized cloud deployment. In this paper, we present a novel fever-screening system, which uses edge machine learning techniques and leverages private 5G to accurately identify and screen individuals with fever in real-time. Particularly, we present deep-learning based novel techniques for fusion and alignment of cross-spectral visual and thermal data streams at the edge. Our novel Cross-Spectral Generative Adversarial Network (CS-GAN) synthesizes visual images that have the key, representative object level features required to uniquely associate objects across visual and thermal spectrum. Two key features of CS-GAN are a novel, feature-preserving loss function that results in high-quality pairing of corresponding cross-spectral objects, and dual bottleneck residual layers with skip connections (a new, network enhancement) to not only accelerate real-time inference, but to also speed up convergence during model training at the edge. To the best of our knowledge, this is the first technique that leverages 5G networks and limited edge resources to enable real-time feature-level association of objects in visual and thermal streams (30 ms per full HD frame on an Intel Core i7-8650 4-core, 1.9GHz mobile processor). To the best of our knowledge, this is also the first system to achieve real-time operation, which has enabled fever screening of employees and guests in arenas, theme parks, airports and other critical facilities. By leveraging edge computing and 5G, our fever screening system is able to achieve 98.5% accuracy and is able to process about 5X more people when compared to a centralized cloud deployment.



### Cross-level Contrastive Learning and Consistency Constraint for Semi-supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.04074v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.04074v2)
- **Published**: 2022-02-08 15:12:11+00:00
- **Updated**: 2022-02-13 21:31:13+00:00
- **Authors**: Xinkai Zhao, Chaowei Fang, De-Jun Fan, Xutao Lin, Feng Gao, Guanbin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised learning (SSL), which aims at leveraging a few labeled images and a large number of unlabeled images for network training, is beneficial for relieving the burden of data annotation in medical image segmentation. According to the experience of medical imaging experts, local attributes such as texture, luster and smoothness are very important factors for identifying target objects like lesions and polyps in medical images. Motivated by this, we propose a cross-level contrastive learning scheme to enhance representation capacity for local features in semi-supervised medical image segmentation. Compared to existing image-wise, patch-wise and point-wise contrastive learning algorithms, our devised method is capable of exploring more complex similarity cues, namely the relational characteristics between global and local patch-wise representations. Additionally, for fully making use of cross-level semantic relations, we devise a novel consistency constraint that compares the predictions of patches against those of the full image. With the help of the cross-level contrastive learning and consistency constraint, the unlabelled data can be effectively explored to improve segmentation performance on two medical image datasets for polyp and skin lesion segmentation respectively. Code of our approach is available.



### If a Human Can See It, So Should Your System: Reliability Requirements for Machine Vision Components
- **Arxiv ID**: http://arxiv.org/abs/2202.03930v1
- **DOI**: 10.1145/3510003.3510109
- **Categories**: **cs.SE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03930v1)
- **Published**: 2022-02-08 15:26:18+00:00
- **Updated**: 2022-02-08 15:26:18+00:00
- **Authors**: Boyue Caroline Hu, Lina Marsso, Krzysztof Czarnecki, Rick Salay, Huakun Shen, Marsha Chechik
- **Comment**: None
- **Journal**: None
- **Summary**: Machine Vision Components (MVC) are becoming safety-critical. Assuring their quality, including safety, is essential for their successful deployment. Assurance relies on the availability of precisely specified and, ideally, machine-verifiable requirements. MVCs with state-of-the-art performance rely on machine learning (ML) and training data but largely lack such requirements.   In this paper, we address the need for defining machine-verifiable reliability requirements for MVCs against transformations that simulate the full range of realistic and safety-critical changes in the environment. Using human performance as a baseline, we define reliability requirements as: 'if the changes in an image do not affect a human's decision, neither should they affect the MVC's.' To this end, we provide: (1) a class of safety-related image transformations; (2) reliability requirement classes to specify correctness-preservation and prediction-preservation for MVCs; (3) a method to instantiate machine-verifiable requirements from these requirements classes using human performance experiment data; (4) human performance experiment data for image recognition involving eight commonly used transformations, from about 2000 human participants; and (5) a method for automatically checking whether an MVC satisfies our requirements. Further, we show that our reliability requirements are feasible and reusable by evaluating our methods on 13 state-of-the-art pre-trained image classification models. Finally, we demonstrate that our approach detects reliability gaps in MVCs that other existing methods are unable to detect.



### Joint-bone Fusion Graph Convolutional Network for Semi-supervised Skeleton Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.04075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04075v1)
- **Published**: 2022-02-08 16:03:15+00:00
- **Updated**: 2022-02-08 16:03:15+00:00
- **Authors**: Zhigang Tu, Jiaxu Zhang, Hongyan Li, Yujin Chen, Junsong Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, graph convolutional networks (GCNs) play an increasingly critical role in skeleton-based human action recognition. However, most GCN-based methods still have two main limitations: 1) They only consider the motion information of the joints or process the joints and bones separately, which are unable to fully explore the latent functional correlation between joints and bones for action recognition. 2) Most of these works are performed in the supervised learning way, which heavily relies on massive labeled training data. To address these issues, we propose a semi-supervised skeleton-based action recognition method which has been rarely exploited before. We design a novel correlation-driven joint-bone fusion graph convolutional network (CD-JBF-GCN) as an encoder and use a pose prediction head as a decoder to achieve semi-supervised learning. Specifically, the CD-JBF-GC can explore the motion transmission between the joint stream and the bone stream, so that promoting both streams to learn more discriminative feature representations. The pose prediction based auto-encoder in the self-supervised training stage allows the network to learn motion representation from unlabeled data, which is essential for action recognition. Extensive experiments on two popular datasets, i.e. NTU-RGB+D and Kinetics-Skeleton, demonstrate that our model achieves the state-of-the-art performance for semi-supervised skeleton-based action recognition and is also useful for fully-supervised methods.



### Social-DualCVAE: Multimodal Trajectory Forecasting Based on Social Interactions Pattern Aware and Dual Conditional Variational Auto-Encoder
- **Arxiv ID**: http://arxiv.org/abs/2202.03954v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.03954v1)
- **Published**: 2022-02-08 16:04:47+00:00
- **Updated**: 2022-02-08 16:04:47+00:00
- **Authors**: Jiashi Gao, Xinming Shi, James J. Q. Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian trajectory forecasting is a fundamental task in multiple utility areas, such as self-driving, autonomous robots, and surveillance systems. The future trajectory forecasting is multi-modal, influenced by physical interaction with scene contexts and intricate social interactions among pedestrians. The mainly existing literature learns representations of social interactions by deep learning networks, while the explicit interaction patterns are not utilized. Different interaction patterns, such as following or collision avoiding, will generate different trends of next movement, thus, the awareness of social interaction patterns is important for trajectory forecasting. Moreover, the social interaction patterns are privacy concerned or lack of labels. To jointly address the above issues, we present a social-dual conditional variational auto-encoder (Social-DualCVAE) for multi-modal trajectory forecasting, which is based on a generative model conditioned not only on the past trajectories but also the unsupervised classification of interaction patterns. After generating the category distribution of the unlabeled social interaction patterns, DualCVAE, conditioned on the past trajectories and social interaction pattern, is proposed for multi-modal trajectory prediction by latent variables estimating. A variational bound is derived as the minimization objective during training. The proposed model is evaluated on widely used trajectory benchmarks and outperforms the prior state-of-the-art methods.



### Bingham Policy Parameterization for 3D Rotations in Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.03957v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03957v1)
- **Published**: 2022-02-08 16:09:02+00:00
- **Updated**: 2022-02-08 16:09:02+00:00
- **Authors**: Stephen James, Pieter Abbeel
- **Comment**: Project page and code: https://sites.google.com/view/rl-bpp
- **Journal**: None
- **Summary**: We propose a new policy parameterization for representing 3D rotations during reinforcement learning. Today in the continuous control reinforcement learning literature, many stochastic policy parameterizations are Gaussian. We argue that universally applying a Gaussian policy parameterization is not always desirable for all environments. One such case in particular where this is true are tasks that involve predicting a 3D rotation output, either in isolation, or coupled with translation as part of a full 6D pose output. Our proposed Bingham Policy Parameterization (BPP) models the Bingham distribution and allows for better rotation (quaternion) prediction over a Gaussian policy parameterization in a range of reinforcement learning tasks. We evaluate BPP on the rotation Wahba problem task, as well as a set of vision-based next-best pose robot manipulation tasks from RLBench. We hope that this paper encourages more research into developing other policy parameterization that are more suited for particular environments, rather than always assuming Gaussian.



### Uncertainty Modeling for Out-of-Distribution Generalization
- **Arxiv ID**: http://arxiv.org/abs/2202.03958v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03958v2)
- **Published**: 2022-02-08 16:09:12+00:00
- **Updated**: 2022-04-22 03:10:41+00:00
- **Authors**: Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, Ling-Yu Duan
- **Comment**: Accepted by ICLR 2022
- **Journal**: None
- **Summary**: Though remarkable progress has been achieved in various vision tasks, deep neural networks still suffer obvious performance degradation when tested in out-of-distribution scenarios. We argue that the feature statistics (mean and standard deviation), which carry the domain characteristics of the training data, can be properly manipulated to improve the generalization ability of deep learning models. Common methods often consider the feature statistics as deterministic values measured from the learned features and do not explicitly consider the uncertain statistics discrepancy caused by potential domain shifts during testing. In this paper, we improve the network generalization ability by modeling the uncertainty of domain shifts with synthesized feature statistics during training. Specifically, we hypothesize that the feature statistic, after considering the potential uncertainties, follows a multivariate Gaussian distribution. Hence, each feature statistic is no longer a deterministic value, but a probabilistic point with diverse distribution possibilities. With the uncertain feature statistics, the models can be trained to alleviate the domain perturbations and achieve better robustness against potential domain shifts. Our method can be readily integrated into networks without additional parameters. Extensive experiments demonstrate that our proposed method consistently improves the network generalization ability on multiple vision tasks, including image classification, semantic segmentation, and instance retrieval. The code can be available at https://github.com/lixiaotong97/DSU.



### Self-supervised Contrastive Learning for Cross-domain Hyperspectral Image Representation
- **Arxiv ID**: http://arxiv.org/abs/2202.03968v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.03968v1)
- **Published**: 2022-02-08 16:16:45+00:00
- **Updated**: 2022-02-08 16:16:45+00:00
- **Authors**: Hyungtae Lee, Heesung Kwon
- **Comment**: Accepted to ICASSP 2022
- **Journal**: None
- **Summary**: Recently, self-supervised learning has attracted attention due to its remarkable ability to acquire meaningful representations for classification tasks without using semantic labels. This paper introduces a self-supervised learning framework suitable for hyperspectral images that are inherently challenging to annotate. The proposed framework architecture leverages cross-domain CNN, allowing for learning representations from different hyperspectral images with varying spectral characteristics and no pixel-level annotation. In the framework, cross-domain representations are learned via contrastive learning where neighboring spectral vectors in the same image are clustered together in a common representation space encompassing multiple hyperspectral images. In contrast, spectral vectors in different hyperspectral images are separated into distinct clusters in the space. To verify that the learned representation through contrastive learning is effectively transferred into a downstream task, we perform a classification task on hyperspectral images. The experimental results demonstrate the advantage of the proposed self-supervised representation over models trained from scratch or other transfer learning methods.



### Wireless Transmission of Images With The Assistance of Multi-level Semantic Information
- **Arxiv ID**: http://arxiv.org/abs/2202.04754v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.04754v1)
- **Published**: 2022-02-08 16:25:26+00:00
- **Updated**: 2022-02-08 16:25:26+00:00
- **Authors**: Zhenguo Zhang, Qianqian Yang, Shibo He, Mingyang Sun, Jiming Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic-oriented communication has been considered as a promising to boost the bandwidth efficiency by only transmitting the semantics of the data. In this paper, we propose a multi-level semantic aware communication system for wireless image transmission, named MLSC-image, which is based on the deep learning techniques and trained in an end to end manner. In particular, the proposed model includes a multilevel semantic feature extractor, that extracts both the highlevel semantic information, such as the text semantics and the segmentation semantics, and the low-level semantic information, such as local spatial details of the images. We employ a pretrained image caption to capture the text semantics and a pretrained image segmentation model to obtain the segmentation semantics. These high-level and low-level semantic features are then combined and encoded by a joint semantic and channel encoder into symbols to transmit over the physical channel. The numerical results validate the effectiveness and efficiency of the proposed semantic communication system, especially under the limited bandwidth condition, which indicates the advantages of the high-level semantics in the compression of images.



### Segmentation by Test-Time Optimization (TTO) for CBCT-based Adaptive Radiation Therapy
- **Arxiv ID**: http://arxiv.org/abs/2202.03978v1
- **DOI**: 10.1002/mp.15960
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2202.03978v1)
- **Published**: 2022-02-08 16:34:22+00:00
- **Updated**: 2022-02-08 16:34:22+00:00
- **Authors**: Xiao Liang, Jaehee Chun, Howard Morgan, Ti Bai, Dan Nguyen, Justin C. Park, Steve Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Online adaptive radiotherapy (ART) requires accurate and efficient auto-segmentation of target volumes and organs-at-risk (OARs) in mostly cone-beam computed tomography (CBCT) images. Propagating expert-drawn contours from the pre-treatment planning CT (pCT) through traditional or deep learning (DL) based deformable image registration (DIR) can achieve improved results in many situations. Typical DL-based DIR models are population based, that is, trained with a dataset for a population of patients, so they may be affected by the generalizability problem. In this paper, we propose a method called test-time optimization (TTO) to refine a pre-trained DL-based DIR population model, first for each individual test patient, and then progressively for each fraction of online ART treatment. Our proposed method is less susceptible to the generalizability problem, and thus can improve overall performance of different DL-based DIR models by improving model accuracy, especially for outliers. Our experiments used data from 239 patients with head and neck squamous cell carcinoma to test the proposed method. Firstly, we trained a population model with 200 patients, and then applied TTO to the remaining 39 test patients by refining the trained population model to obtain 39 individualized models. We compared each of the individualized models with the population model in terms of segmentation accuracy. The number of patients with at least 0.05 DSC improvement or 2 mm HD95 improvement by TTO averaged over the 17 selected structures for the state-of-the-art architecture Voxelmorph is 10 out of 39 test patients. The average time for deriving the individualized model using TTO from the pre-trained population model is approximately four minutes. When adapting the individualized model to a later fraction of the same patient, the average time is reduced to about one minute and the accuracy is slightly improved.



### Equivariance versus Augmentation for Spherical Images
- **Arxiv ID**: http://arxiv.org/abs/2202.03990v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03990v2)
- **Published**: 2022-02-08 16:49:30+00:00
- **Updated**: 2022-07-12 13:02:52+00:00
- **Authors**: Jan E. Gerken, Oscar Carlsson, Hampus Linander, Fredrik Ohlsson, Christoffer Petersson, Daniel Persson
- **Comment**: Accepted to ICML2022, updated according to ICML-reviewer comments, 18
  pages of which 9 in main body, 16 figures,
- **Journal**: None
- **Summary**: We analyze the role of rotational equivariance in convolutional neural networks (CNNs) applied to spherical images. We compare the performance of the group equivariant networks known as S2CNNs and standard non-equivariant CNNs trained with an increasing amount of data augmentation. The chosen architectures can be considered baseline references for the respective design paradigms. Our models are trained and evaluated on single or multiple items from the MNIST or FashionMNIST dataset projected onto the sphere. For the task of image classification, which is inherently rotationally invariant, we find that by considerably increasing the amount of data augmentation and the size of the networks, it is possible for the standard CNNs to reach at least the same performance as the equivariant network. In contrast, for the inherently equivariant task of semantic segmentation, the non-equivariant networks are consistently outperformed by the equivariant networks with significantly fewer parameters. We also analyze and compare the inference latency and training times of the different networks, enabling detailed tradeoff considerations between equivariant architectures and data augmentation for practical problems. The equivariant spherical networks used in the experiments are available at https://github.com/JanEGerken/sem_seg_s2cnn .



### Results and findings of the 2021 Image Similarity Challenge
- **Arxiv ID**: http://arxiv.org/abs/2202.04007v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.04007v1)
- **Published**: 2022-02-08 17:23:32+00:00
- **Updated**: 2022-02-08 17:23:32+00:00
- **Authors**: Zoë Papakipos, Giorgos Tolias, Tomas Jenicek, Ed Pizzi, Shuhei Yokoo, Wenhao Wang, Yifan Sun, Weipu Zhang, Yi Yang, Sanjay Addicam, Sergio Manuel Papadakis, Cristian Canton Ferrer, Ondrej Chum, Matthijs Douze
- **Comment**: None
- **Journal**: None
- **Summary**: The 2021 Image Similarity Challenge introduced a dataset to serve as a new benchmark to evaluate recent image copy detection methods. There were 200 participants to the competition. This paper presents a quantitative and qualitative analysis of the top submissions. It appears that the most difficult image transformations involve either severe image crops or hiding into unrelated images, combined with local pixel perturbations. The key algorithmic elements in the winning submissions are: training on strong augmentations, self-supervised learning, score normalization, explicit overlay detection, and global descriptor matching followed by pairwise image comparison.



### NEWSKVQA: Knowledge-Aware News Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2202.04015v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2202.04015v1)
- **Published**: 2022-02-08 17:31:31+00:00
- **Updated**: 2022-02-08 17:31:31+00:00
- **Authors**: Pranay Gupta, Manish Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Answering questions in the context of videos can be helpful in video indexing, video retrieval systems, video summarization, learning management systems and surveillance video analysis. Although there exists a large body of work on visual question answering, work on video question answering (1) is limited to domains like movies, TV shows, gameplay, or human activity, and (2) is mostly based on common sense reasoning. In this paper, we explore a new frontier in video question answering: answering knowledge-based questions in the context of news videos. To this end, we curate a new dataset of 12K news videos spanning across 156 hours with 1M multiple-choice question-answer pairs covering 8263 unique entities. We make the dataset publicly available. Using this dataset, we propose a novel approach, NEWSKVQA (Knowledge-Aware News Video Question Answering) which performs multi-modal inferencing over textual multiple-choice questions, videos, their transcripts and knowledge base, and presents a strong baseline.



### Self-supervised Contrastive Learning for Volcanic Unrest Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.04030v1
- **DOI**: 10.1109/LGRS.2021.3104506
- **Categories**: **cs.CV**, eess.IV, I.2.10; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2202.04030v1)
- **Published**: 2022-02-08 17:54:51+00:00
- **Updated**: 2022-02-08 17:54:51+00:00
- **Authors**: Nikolaos Ioannis Bountos, Ioannis Papoutsis, Dimitrios Michail, Nantheera Anantrasirichai
- **Comment**: 5 pages, 3 figures
- **Journal**: IEEE Geoscience and Remote Sensing Letters, vol. 19, pp. 1-5, 2022
- **Summary**: Ground deformation measured from Interferometric Synthetic Aperture Radar (InSAR) data is considered a sign of volcanic unrest, statistically linked to a volcanic eruption. Recent studies have shown the potential of using Sentinel-1 InSAR data and supervised deep learning (DL) methods for the detection of volcanic deformation signals, towards global volcanic hazard mitigation. However, detection accuracy is compromised from the lack of labelled data and class imbalance. To overcome this, synthetic data are typically used for finetuning DL models pre-trained on the ImageNet dataset. This approach suffers from poor generalisation on real InSAR data. This letter proposes the use of self-supervised contrastive learning to learn quality visual representations hidden in unlabeled InSAR data. Our approach, based on the SimCLR framework, provides a solution that does not require a specialized architecture nor a large labelled or synthetic dataset. We show that our self-supervised pipeline achieves higher accuracy with respect to the state-of-the-art methods, and shows excellent generalisation even for out-of-distribution test data. Finally, we showcase the effectiveness of our approach for detecting the unrest episodes preceding the recent Icelandic Fagradalsfjall volcanic eruption.



### Residual Aligned: Gradient Optimization for Non-Negative Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2202.04036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04036v1)
- **Published**: 2022-02-08 18:04:32+00:00
- **Updated**: 2022-02-08 18:04:32+00:00
- **Authors**: Flora Yu Shen, Katie Luo, Guandao Yang, Harald Haraldsson, Serge Belongie
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we address an important problem of optical see through (OST) augmented reality: non-negative image synthesis. Most of the image generation methods fail under this condition, since they assume full control over each pixel and cannot create darker pixels by adding light. In order to solve the non-negative image generation problem in AR image synthesis, prior works have attempted to utilize optical illusion to simulate human vision but fail to preserve lightness constancy well under situations such as high dynamic range. In our paper, we instead propose a method that is able to preserve lightness constancy at a local level, thus capturing high frequency details. Compared with existing work, our method shows strong performance in image-to-image translation tasks, particularly in scenarios such as large scale images, high resolution images, and high dynamic range image transfer.



### Self-Conditioned Generative Adversarial Networks for Image Editing
- **Arxiv ID**: http://arxiv.org/abs/2202.04040v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.04040v1)
- **Published**: 2022-02-08 18:08:24+00:00
- **Updated**: 2022-02-08 18:08:24+00:00
- **Authors**: Yunzhe Liu, Rinon Gal, Amit H. Bermano, Baoquan Chen, Daniel Cohen-Or
- **Comment**: Project page: https://github.com/yzliu567/sc-gan
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are susceptible to bias, learned from either the unbalanced data, or through mode collapse. The networks focus on the core of the data distribution, leaving the tails - or the edges of the distribution - behind. We argue that this bias is responsible not only for fairness concerns, but that it plays a key role in the collapse of latent-traversal editing methods when deviating away from the distribution's core. Building on this observation, we outline a method for mitigating generative bias through a self-conditioning process, where distances in the latent-space of a pre-trained generator are used to provide initial labels for the data. By fine-tuning the generator on a re-sampled distribution drawn from these self-labeled data, we force the generator to better contend with rare semantic attributes and enable more realistic generation of these properties. We compare our models to a wide range of latent editing methods, and show that by alleviating the bias they achieve finer semantic control and better identity preservation through a wider range of transformations. Our code and models will be available at https://github.com/yzliu567/sc-gan



### DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models
- **Arxiv ID**: http://arxiv.org/abs/2202.04053v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2202.04053v3)
- **Published**: 2022-02-08 18:36:52+00:00
- **Updated**: 2023-08-30 18:41:01+00:00
- **Authors**: Jaemin Cho, Abhay Zala, Mohit Bansal
- **Comment**: ICCV 2023 (34 pages; see appendix for version changelog)
- **Journal**: None
- **Summary**: Recently, DALL-E, a multimodal transformer language model, and its variants, including diffusion models, have shown high-quality text-to-image generation capabilities. However, despite the realistic image generation results, there has not been a detailed analysis of how to evaluate such models. In this work, we investigate the visual reasoning capabilities and social biases of different text-to-image models, covering both multimodal transformer language models and diffusion models. First, we measure three visual reasoning skills: object recognition, object counting, and spatial relation understanding. For this, we propose PaintSkills, a compositional diagnostic evaluation dataset that measures these skills. Despite the high-fidelity image generation capability, a large gap exists between the performance of recent models and the upper bound accuracy in object counting and spatial relation understanding skills. Second, we assess the gender and skin tone biases by measuring the gender/skin tone distribution of generated images across various professions and attributes. We demonstrate that recent text-to-image generation models learn specific biases about gender and skin tone from web image-text pairs. We hope our work will help guide future progress in improving text-to-image generation models on visual reasoning skills and learning socially unbiased representations. Code and data: https://github.com/j-min/DallEval



### Face2PPG: An unsupervised pipeline for blood volume pulse extraction from faces
- **Arxiv ID**: http://arxiv.org/abs/2202.04101v3
- **DOI**: None
- **Categories**: **cs.CV**, I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2202.04101v3)
- **Published**: 2022-02-08 19:06:20+00:00
- **Updated**: 2023-05-04 18:25:35+00:00
- **Authors**: Constantino Álvarez Casado, Miguel Bordallo López
- **Comment**: 23 pages, 13 figures, 4 tables, 3 equations
- **Journal**: None
- **Summary**: Photoplethysmography (PPG) signals have become a key technology in many fields, such as medicine, well-being, or sports. Our work proposes a set of pipelines to extract remote PPG signals (rPPG) from the face robustly, reliably, and configurable. We identify and evaluate the possible choices in the critical steps of unsupervised rPPG methodologies. We assess a state-of-the-art processing pipeline in six different datasets, incorporating important corrections in the methodology that ensure reproducible and fair comparisons. In addition, we extend the pipeline by proposing three novel ideas; 1) a new method to stabilize the detected face based on a rigid mesh normalization; 2) a new method to dynamically select the different regions in the face that provide the best raw signals, and 3) a new RGB to rPPG transformation method, called Orthogonal Matrix Image Transformation (OMIT) based on QR decomposition, that increases robustness against compression artifacts. We show that all three changes introduce noticeable improvements in retrieving rPPG signals from faces, obtaining state-of-the-art results compared with unsupervised, non-learning-based methodologies and, in some databases, very close to supervised, learning-based methods. We perform a comparative study to quantify the contribution of each proposed idea. In addition, we depict a series of observations that could help in future implementations.



### Disentangle Saliency Detection into Cascaded Detail Modeling and Body Filling
- **Arxiv ID**: http://arxiv.org/abs/2202.04112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04112v1)
- **Published**: 2022-02-08 19:33:02+00:00
- **Updated**: 2022-02-08 19:33:02+00:00
- **Authors**: Yue Song, Hao Tang, Nicu Sebe, Wei Wang
- **Comment**: Accepted by TOMM; the first two authors contribute equally to this
  work
- **Journal**: None
- **Summary**: Salient object detection has been long studied to identify the most visually attractive objects in images/videos. Recently, a growing amount of approaches have been proposed all of which rely on the contour/edge information to improve detection performance. The edge labels are either put into the loss directly or used as extra supervision. The edge and body can also be learned separately and then fused afterward. Both methods either lead to high prediction errors near the edge or cannot be trained in an end-to-end manner. Another problem is that existing methods may fail to detect objects of various sizes due to the lack of efficient and effective feature fusion mechanisms. In this work, we propose to decompose the saliency detection task into two cascaded sub-tasks, \emph{i.e.}, detail modeling and body filling. Specifically, the detail modeling focuses on capturing the object edges by supervision of explicitly decomposed detail label that consists of the pixels that are nested on the edge and near the edge. Then the body filling learns the body part which will be filled into the detail map to generate more accurate saliency map. To effectively fuse the features and handle objects at different scales, we have also proposed two novel multi-scale detail attention and body attention blocks for precise detail and body modeling. Experimental results show that our method achieves state-of-the-art performances on six public datasets.



### Untrimmed Action Anticipation
- **Arxiv ID**: http://arxiv.org/abs/2202.04132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04132v1)
- **Published**: 2022-02-08 20:20:08+00:00
- **Updated**: 2022-02-08 20:20:08+00:00
- **Authors**: Ivan Rodin, Antonino Furnari, Dimitrios Mavroeidis, Giovanni Maria Farinella
- **Comment**: None
- **Journal**: None
- **Summary**: Egocentric action anticipation consists in predicting a future action the camera wearer will perform from egocentric video. While the task has recently attracted the attention of the research community, current approaches assume that the input videos are "trimmed", meaning that a short video sequence is sampled a fixed time before the beginning of the action. We argue that, despite the recent advances in the field, trimmed action anticipation has a limited applicability in real-world scenarios where it is important to deal with "untrimmed" video inputs and it cannot be assumed that the exact moment in which the action will begin is known at test time. To overcome such limitations, we propose an untrimmed action anticipation task, which, similarly to temporal action detection, assumes that the input video is untrimmed at test time, while still requiring predictions to be made before the actions actually take place. We design an evaluation procedure for methods designed to address this novel task, and compare several baselines on the EPIC-KITCHENS-100 dataset. Experiments show that the performance of current models designed for trimmed action anticipation is very limited and more research on this task is required.



### Federated Learning of Generative Image Priors for MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2202.04175v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.04175v2)
- **Published**: 2022-02-08 22:17:57+00:00
- **Updated**: 2022-04-06 18:21:59+00:00
- **Authors**: Gokberk Elmas, Salman UH Dar, Yilmaz Korkmaz, Emir Ceyani, Burak Susam, Muzaffer Özbey, Salman Avestimehr, Tolga Çukur
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-institutional efforts can facilitate training of deep MRI reconstruction models, albeit privacy risks arise during cross-site sharing of imaging data. Federated learning (FL) has recently been introduced to address privacy concerns by enabling distributed training without transfer of imaging data. Existing FL methods for MRI reconstruction employ conditional models to map from undersampled to fully-sampled acquisitions via explicit knowledge of the imaging operator. Since conditional models generalize poorly across different acceleration rates or sampling densities, imaging operators must be fixed between training and testing, and they are typically matched across sites. To improve generalization and flexibility in multi-institutional collaborations, here we introduce a novel method for MRI reconstruction based on Federated learning of Generative IMage Priors (FedGIMP). FedGIMP leverages a two-stage approach: cross-site learning of a generative MRI prior, and subject-specific injection of the imaging operator. The global MRI prior is learned via an unconditional adversarial model that synthesizes high-quality MR images based on latent variables. Specificity in the prior is preserved via a mapper subnetwork that produces site-specific latents. During inference, the prior is combined with subject-specific imaging operators to enable reconstruction, and further adapted to individual test samples by minimizing data-consistency loss. Comprehensive experiments on multi-institutional datasets clearly demonstrate enhanced generalization performance of FedGIMP against site-specific and federated methods based on conditional models, as well as traditional reconstruction methods.



### TransformNet: Self-supervised representation learning through predicting geometric transformations
- **Arxiv ID**: http://arxiv.org/abs/2202.04181v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.04181v1)
- **Published**: 2022-02-08 22:41:01+00:00
- **Updated**: 2022-02-08 22:41:01+00:00
- **Authors**: Sayed Hashim, Muhammad Ali
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks need a big amount of training data, while in the real world there is a scarcity of data available for training purposes. To resolve this issue unsupervised methods are used for training with limited data. In this report, we describe the unsupervised semantic feature learning approach for recognition of the geometric transformation applied to the input data. The basic concept of our approach is that if someone is unaware of the objects in the images, he/she would not be able to quantitatively predict the geometric transformation that was applied to them. This self supervised scheme is based on pretext task and the downstream task. The pretext classification task to quantify the geometric transformations should force the CNN to learn high-level salient features of objects useful for image classification. In our baseline model, we define image rotations by multiples of 90 degrees. The CNN trained on this pretext task will be used for the classification of images in the CIFAR-10 dataset as a downstream task. we run the baseline method using various models, including ResNet, DenseNet, VGG-16, and NIN with a varied number of rotations in feature extracting and fine-tuning settings. In extension of this baseline model we experiment with transformations other than rotation in pretext task. We compare performance of selected models in various settings with different transformations applied to images,various data augmentation techniques as well as using different optimizers. This series of different type of experiments will help us demonstrate the recognition accuracy of our self-supervised model when applied to a downstream task of classification.



### MaskGIT: Masked Generative Image Transformer
- **Arxiv ID**: http://arxiv.org/abs/2202.04200v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04200v1)
- **Published**: 2022-02-08 23:54:06+00:00
- **Updated**: 2022-02-08 23:54:06+00:00
- **Authors**: Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T. Freeman
- **Comment**: None
- **Journal**: None
- **Summary**: Generative transformers have experienced rapid popularity growth in the computer vision community in synthesizing high-fidelity and high-resolution images. The best generative transformer models so far, however, still treat an image naively as a sequence of tokens, and decode an image sequentially following the raster scan ordering (i.e. line-by-line). We find this strategy neither optimal nor efficient. This paper proposes a novel image synthesis paradigm using a bidirectional transformer decoder, which we term MaskGIT. During training, MaskGIT learns to predict randomly masked tokens by attending to tokens in all directions. At inference time, the model begins with generating all tokens of an image simultaneously, and then refines the image iteratively conditioned on the previous generation. Our experiments demonstrate that MaskGIT significantly outperforms the state-of-the-art transformer model on the ImageNet dataset, and accelerates autoregressive decoding by up to 64x. Besides, we illustrate that MaskGIT can be easily extended to various image editing tasks, such as inpainting, extrapolation, and image manipulation.



