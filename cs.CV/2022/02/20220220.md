# Arxiv Papers in cs.CV on 2022-02-20
### MANet: Improving Video Denoising with a Multi-Alignment Network
- **Arxiv ID**: http://arxiv.org/abs/2202.09704v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09704v2)
- **Published**: 2022-02-20 00:52:07+00:00
- **Updated**: 2022-07-11 13:21:50+00:00
- **Authors**: Yaping Zhao, Haitian Zheng, Zhongrui Wang, Jiebo Luo, Edmund Y. Lam
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: In video denoising, the adjacent frames often provide very useful information, but accurate alignment is needed before such information can be harnassed. In this work, we present a multi-alignment network, which generates multiple flow proposals followed by attention-based averaging. It serves to mimic the non-local mechanism, suppressing noise by averaging multiple observations. Our approach can be applied to various state-of-the-art models that are based on flow estimation. Experiments on a large-scale video dataset demonstrate that our method improves the denoising baseline model by 0.2dB, and further reduces the parameters by 47% with model distillation. Code is available at https://github.com/IndigoPurple/MANet.



### Statistical and Topological Summaries Aid Disease Detection for Segmented Retinal Vascular Images
- **Arxiv ID**: http://arxiv.org/abs/2202.09708v3
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09708v3)
- **Published**: 2022-02-20 01:29:36+00:00
- **Updated**: 2022-09-09 21:28:10+00:00
- **Authors**: John T. Nardini, Charles W. J. Pugh, Helen M. Byrne
- **Comment**: None
- **Journal**: None
- **Summary**: Disease complications can alter vascular network morphology and disrupt tissue functioning. Diabetic retinopathy, for example, is a complication of types 1 and 2 diabetes mellitus that can cause blindness. Microvascular diseases are assessed by visual inspection of retinal images, but this can be challenging when diseases exhibit silent symptoms or patients cannot attend in-person meetings. We examine the performance of machine learning algorithms in detecting microvascular disease when trained on statistical and topological summaries of segmented retinal vascular images. We apply our methods to three publicly-available datasets and find that, among the 13 total descriptor vectors we consider, either a statistical Box-counting descriptor vector or a topological Flooding descriptor vector achieves the highest accuracy levels on these datasets. We then created a fourth dataset by merging several datasets: the Box-counting vector outperforms all descriptors on this dataset, including the topological Flooding vector which is sensitive to differences in the annotation styles within the combined dataset. Our work represents a first step to establishing which computational methods are most suitable for identifying microvascular disease as well as some of their current limitations. In the longer term, these methods could be incorporated into automated disease assessment tools.



### ARM3D: Attention-based relation module for indoor 3D object detection
- **Arxiv ID**: http://arxiv.org/abs/2202.09715v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09715v1)
- **Published**: 2022-02-20 02:43:42+00:00
- **Updated**: 2022-02-20 02:43:42+00:00
- **Authors**: Yuqing Lan, Yao Duan, Chenyi Liu, Chenyang Zhu, Yueshan Xiong, Hui Huang, Kai Xu
- **Comment**: 16 pages, 9 figures
- **Journal**: None
- **Summary**: Relation context has been proved to be useful for many challenging vision tasks. In the field of 3D object detection, previous methods have been taking the advantage of context encoding, graph embedding, or explicit relation reasoning to extract relation context. However, there exists inevitably redundant relation context due to noisy or low-quality proposals. In fact, invalid relation context usually indicates underlying scene misunderstanding and ambiguity, which may, on the contrary, reduce the performance in complex scenes. Inspired by recent attention mechanism like Transformer, we propose a novel 3D attention-based relation module (ARM3D). It encompasses object-aware relation reasoning to extract pair-wise relation contexts among qualified proposals and an attention module to distribute attention weights towards different relation contexts. In this way, ARM3D can take full advantage of the useful relation context and filter those less relevant or even confusing contexts, which mitigates the ambiguity in detection. We have evaluated the effectiveness of ARM3D by plugging it into several state-of-the-art 3D object detectors and showing more accurate and robust detection results. Extensive experiments show the capability and generalization of ARM3D on 3D object detection. Our source code is available at https://github.com/lanlan96/ARM3D.



### Towards a Unified Approach to Homography Estimation Using Image Features and Pixel Intensities
- **Arxiv ID**: http://arxiv.org/abs/2202.09716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09716v1)
- **Published**: 2022-02-20 02:47:05+00:00
- **Updated**: 2022-02-20 02:47:05+00:00
- **Authors**: Lucas Nogueira, Ely C. de Paiva, Geraldo Silvera
- **Comment**: None
- **Journal**: International Conference on Autonomic and Autonomous Systems,
  (2020), 110-115
- **Summary**: The homography matrix is a key component in various vision-based robotic tasks. Traditionally, homography estimation algorithms are classified into feature- or intensity-based. The main advantages of the latter are their versatility, accuracy, and robustness to arbitrary illumination changes. On the other hand, they have a smaller domain of convergence than the feature-based solutions. Their combination is hence promising, but existing techniques only apply them sequentially. This paper proposes a new hybrid method that unifies both classes into a single nonlinear optimization procedure, applies the same minimization method, and uses the same homography parametrization and warping function. Experimental validation using a classical testing framework shows that the proposed unified approach has improved convergence properties compared to each individual class. These are also demonstrated in a visual tracking application. As a final contribution, our ready-to-use implementation of the algorithm is made publicly available to the research community.



### 3DRM:Pair-wise relation module for 3D object detection
- **Arxiv ID**: http://arxiv.org/abs/2202.09721v1
- **DOI**: 10.1016/j.cag.2021.04.033
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09721v1)
- **Published**: 2022-02-20 03:06:35+00:00
- **Updated**: 2022-02-20 03:06:35+00:00
- **Authors**: Yuqing Lan, Yao Duan, Yifei Shi, Hui Huang, Kai Xu
- **Comment**: 13 pages, 8 figures
- **Journal**: Computers & Graphics, 2021, Vol. 98, Pages: 58 - 70
- **Summary**: Context has proven to be one of the most important factors in object layout reasoning for 3D scene understanding. Existing deep contextual models either learn holistic features for context encoding or rely on pre-defined scene templates for context modeling. We argue that scene understanding benefits from object relation reasoning, which is capable of mitigating the ambiguity of 3D object detections and thus helps locate and classify the 3D objects more accurately and robustly. To achieve this, we propose a novel 3D relation module (3DRM) which reasons about object relations at pair-wise levels. The 3DRM predicts the semantic and spatial relationships between objects and extracts the object-wise relation features. We demonstrate the effects of 3DRM by plugging it into proposal-based and voting-based 3D object detection pipelines, respectively. Extensive evaluations show the effectiveness and generalization of 3DRM on 3D object detection. Our source code is available at https://github.com/lanlan96/3DRM.



### Overparametrization improves robustness against adversarial attacks: A replication study
- **Arxiv ID**: http://arxiv.org/abs/2202.09735v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09735v1)
- **Published**: 2022-02-20 05:50:54+00:00
- **Updated**: 2022-02-20 05:50:54+00:00
- **Authors**: Ali Borji
- **Comment**: None
- **Journal**: None
- **Summary**: Overparametrization has become a de facto standard in machine learning. Despite numerous efforts, our understanding of how and where overparametrization helps model accuracy and robustness is still limited. To this end, here we conduct an empirical investigation to systemically study and replicate previous findings in this area, in particular the study by Madry et al. Together with this study, our findings support the "universal law of robustness" recently proposed by Bubeck et al. We argue that while critical for robust perception, overparametrization may not be enough to achieve full robustness and smarter architectures e.g. the ones implemented by the human visual cortex) seem inevitable.



### The Loop Game: Quality Assessment and Optimization for Low-Light Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2202.09738v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09738v1)
- **Published**: 2022-02-20 06:20:06+00:00
- **Updated**: 2022-02-20 06:20:06+00:00
- **Authors**: Baoliang Chen, Lingyu Zhu, Hanwei Zhu, Wenhan Yang, Fangbo Lu, Shiqi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: There is an increasing consensus that the design and optimization of low light image enhancement methods need to be fully driven by perceptual quality. With numerous approaches proposed to enhance low-light images, much less work has been dedicated to quality assessment and quality optimization of low-light enhancement. In this paper, to close the gap between enhancement and assessment, we propose a loop enhancement framework that produces a clear picture of how the enhancement of low-light images could be optimized towards better visual quality. In particular, we create a large-scale database for QUality assessment Of The Enhanced LOw-Light Image (QUOTE-LOL), which serves as the foundation in studying and developing objective quality assessment measures. The objective quality assessment measure plays a critical bridging role between visual quality and enhancement and is further incorporated in the optimization in learning the enhancement model towards perceptual optimally. Finally, we iteratively perform the enhancement and optimization tasks, enhancing the low-light images continuously. The superiority of the proposed scheme is validated based on various low-light scenes. The database as well as the code will be available.



### Visual Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2202.09741v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09741v5)
- **Published**: 2022-02-20 06:35:18+00:00
- **Updated**: 2022-07-11 04:21:37+00:00
- **Authors**: Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu
- **Comment**: Code is available at https://github.com/Visual-Attention-Network
- **Journal**: None
- **Summary**: While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel linear attention named large kernel attention (LKA) to enable self-adaptive and long-range correlations in self-attention while avoiding its shortcomings. Furthermore, we present a neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN surpasses similar size vision transformers(ViTs) and convolutional neural networks(CNNs) in various tasks, including image classification, object detection, semantic segmentation, panoptic segmentation, pose estimation, etc. For example, VAN-B6 achieves 87.8% accuracy on ImageNet benchmark and set new state-of-the-art performance (58.2 PQ) for panoptic segmentation. Besides, VAN-B2 surpasses Swin-T 4% mIoU (50.1 vs. 46.1) for semantic segmentation on ADE20K benchmark, 2.6% AP (48.8 vs. 46.2) for object detection on COCO dataset. It provides a novel method and a simple yet strong baseline for the community. Code is available at https://github.com/Visual-Attention-Network.



### RDP-Net: Region Detail Preserving Network for Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.09745v4
- **DOI**: 10.1109/TGRS.2022.3227098
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09745v4)
- **Published**: 2022-02-20 07:10:07+00:00
- **Updated**: 2022-12-11 07:33:32+00:00
- **Authors**: Hongjia Chen, Fangling Pu, Rui Yang, Rui Tang, Xin Xu
- **Comment**: 10 pages, 10 figures, 55 references
- **Journal**: None
- **Summary**: Change detection (CD) is an essential earth observation technique. It captures the dynamic information of land objects. With the rise of deep learning, convolutional neural networks (CNN) have shown great potential in CD. However, current CNN models introduce backbone architectures that lose detailed information during learning. Moreover, current CNN models are heavy in parameters, which prevents their deployment on edge devices such as UAVs. In this work, we tackle this issue by proposing RDP-Net: a region detail preserving network for CD. We propose an efficient training strategy that constructs the training tasks during the warmup period of CNN training and lets the CNN learn from easy to hard. The training strategy enables CNN to learn more powerful features with fewer FLOPs and achieve better performance. Next, we propose an effective edge loss that increases the penalty for errors on details and improves the network's attention to details such as boundary regions and small areas. Furthermore, we provide a CNN model with a brand new backbone that achieves the state-of-the-art empirical performance in CD with only 1.70M parameters. We hope our RDP-Net would benefit the practical CD applications on compact devices and could inspire more people to bring change detection to a new level with the efficient training strategy. The code and models are publicly available at https://github.com/Chnja/RDPNet.



### Dynamic Spatial Propagation Network for Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/2202.09769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09769v1)
- **Published**: 2022-02-20 09:43:17+00:00
- **Updated**: 2022-02-20 09:43:17+00:00
- **Authors**: Yuankai Lin, Tao Cheng, Qi Zhong, Wending Zhou, Hua Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Image-guided depth completion aims to generate dense depth maps with sparse depth measurements and corresponding RGB images. Currently, spatial propagation networks (SPNs) are the most popular affinity-based methods in depth completion, but they still suffer from the representation limitation of the fixed affinity and the over smoothing during iterations. Our solution is to estimate independent affinity matrices in each SPN iteration, but it is over-parameterized and heavy calculation. This paper introduces an efficient model that learns the affinity among neighboring pixels with an attention-based, dynamic approach. Specifically, the Dynamic Spatial Propagation Network (DySPN) we proposed makes use of a non-linear propagation model (NLPM). It decouples the neighborhood into parts regarding to different distances and recursively generates independent attention maps to refine these parts into adaptive affinity matrices. Furthermore, we adopt a diffusion suppression (DS) operation so that the model converges at an early stage to prevent over-smoothing of dense depth. Finally, in order to decrease the computational cost required, we also introduce three variations that reduce the amount of neighbors and attentions needed while still retaining similar accuracy. In practice, our method requires less iteration to match the performance of other SPNs and yields better results overall. DySPN outperforms other state-of-the-art (SoTA) methods on KITTI Depth Completion (DC) evaluation by the time of submission and is able to yield SoTA performance in NYU Depth v2 dataset as well.



### Pseudo Numerical Methods for Diffusion Models on Manifolds
- **Arxiv ID**: http://arxiv.org/abs/2202.09778v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NA, math.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2202.09778v2)
- **Published**: 2022-02-20 10:37:52+00:00
- **Updated**: 2022-10-31 09:05:25+00:00
- **Authors**: Luping Liu, Yi Ren, Zhijie Lin, Zhou Zhao
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality samples such as image and audio samples. However, DDPMs require hundreds to thousands of iterations to produce final samples. Several prior works have successfully accelerated DDPMs through adjusting the variance schedule (e.g., Improved Denoising Diffusion Probabilistic Models) or the denoising equation (e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these acceleration methods cannot maintain the quality of samples and even introduce new noise at a high speedup rate, which limit their practicability. To accelerate the inference process while keeping the sample quality, we provide a fresh perspective that DDPMs should be treated as solving differential equations on manifolds. Under such a perspective, we propose pseudo numerical methods for diffusion models (PNDMs). Specifically, we figure out how to solve differential equations on manifolds and show that DDIMs are simple cases of pseudo numerical methods. We change several classical numerical methods to corresponding pseudo numerical methods and find that the pseudo linear multi-step method is the best in most situations. According to our experiments, by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can generate higher quality synthetic images with only 50 steps compared with 1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps (by around 0.4 in FID) and have good generalization on different variance schedules. Our implementation is available at https://github.com/luping-liu/PNDM.



### Clustering by the Probability Distributions from Extreme Value Theory
- **Arxiv ID**: http://arxiv.org/abs/2202.09784v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2202.09784v1)
- **Published**: 2022-02-20 10:52:43+00:00
- **Updated**: 2022-02-20 10:52:43+00:00
- **Authors**: Sixiao Zheng, Ke Fan, Yanxi Hou, Jianfeng Feng, Yanwei Fu
- **Comment**: IEEE Transactions on Artificial Intelligence
- **Journal**: None
- **Summary**: Clustering is an essential task to unsupervised learning. It tries to automatically separate instances into coherent subsets. As one of the most well-known clustering algorithms, k-means assigns sample points at the boundary to a unique cluster, while it does not utilize the information of sample distribution or density. Comparably, it would potentially be more beneficial to consider the probability of each sample in a possible cluster. To this end, this paper generalizes k-means to model the distribution of clusters. Our novel clustering algorithm thus models the distributions of distances to centroids over a threshold by Generalized Pareto Distribution (GPD) in Extreme Value Theory (EVT). Notably, we propose the concept of centroid margin distance, use GPD to establish a probability model for each cluster, and perform a clustering algorithm based on the covering probability function derived from GPD. Such a GPD k-means thus enables the clustering algorithm from the probabilistic perspective. Correspondingly, we also introduce a naive baseline, dubbed as Generalized Extreme Value (GEV) k-means. GEV fits the distribution of the block maxima. In contrast, the GPD fits the distribution of distance to the centroid exceeding a sufficiently large threshold, leading to a more stable performance of GPD k-means. Notably, GEV k-means can also estimate cluster structure and thus perform reasonably well over classical k-means. Thus, extensive experiments on synthetic datasets and real datasets demonstrate that GPD k-means outperforms competitors. The github codes are released in https://github.com/sixiaozheng/EVT-K-means.



### Image quality assessment by overlapping task-specific and task-agnostic measures: application to prostate multiparametric MR images for cancer segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.09798v1
- **DOI**: 10.59275/j.melba.2022-a1cc
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09798v1)
- **Published**: 2022-02-20 11:45:58+00:00
- **Updated**: 2022-02-20 11:45:58+00:00
- **Authors**: Shaheer U. Saeed, Wen Yan, Yunguan Fu, Francesco Giganti, Qianye Yang, Zachary M. C. Baum, Mirabela Rusu, Richard E. Fan, Geoffrey A. Sonn, Mark Emberton, Dean C. Barratt, Yipeng Hu
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://www.melba-journal.org
- **Journal**: None
- **Summary**: Image quality assessment (IQA) in medical imaging can be used to ensure that downstream clinical tasks can be reliably performed. Quantifying the impact of an image on the specific target tasks, also named as task amenability, is needed. A task-specific IQA has recently been proposed to learn an image-amenability-predicting controller simultaneously with a target task predictor. This allows for the trained IQA controller to measure the impact an image has on the target task performance, when this task is performed using the predictor, e.g. segmentation and classification neural networks in modern clinical applications. In this work, we propose an extension to this task-specific IQA approach, by adding a task-agnostic IQA based on auto-encoding as the target task. Analysing the intersection between low-quality images, deemed by both the task-specific and task-agnostic IQA, may help to differentiate the underpinning factors that caused the poor target task performance. For example, common imaging artefacts may not adversely affect the target task, which would lead to a low task-agnostic quality and a high task-specific quality, whilst individual cases considered clinically challenging, which can not be improved by better imaging equipment or protocols, is likely to result in a high task-agnostic quality but a low task-specific quality. We first describe a flexible reward shaping strategy which allows for the adjustment of weighting between task-agnostic and task-specific quality scoring. Furthermore, we evaluate the proposed algorithm using a clinically challenging target task of prostate tumour segmentation on multiparametric magnetic resonance (mpMR) images, from 850 patients. The proposed reward shaping strategy, with appropriately weighted task-specific and task-agnostic qualities, successfully identified samples that need re-acquisition due to defected imaging process.



### Distortion-Aware Loop Filtering of Intra 360^o Video Coding with Equirectangular Projection
- **Arxiv ID**: http://arxiv.org/abs/2202.09802v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09802v1)
- **Published**: 2022-02-20 12:00:18+00:00
- **Updated**: 2022-02-20 12:00:18+00:00
- **Authors**: Pingping Zhang, Xu Wang, Linwei Zhu, Yun Zhang, Shiqi Wang, Sam Kwong
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a distortion-aware loop filtering model to improve the performance of intra coding for 360$^o$ videos projected via equirectangular projection (ERP) format. To enable the awareness of distortion, our proposed module analyzes content characteristics based on a coding unit (CU) partition mask and processes them through partial convolution to activate the specified area. The feature recalibration module, which leverages cascaded residual channel-wise attention blocks (RCABs) to adjust the inter-channel and intra-channel features automatically, is capable of adapting with different quality levels. The perceptual geometry optimization combining with weighted mean squared error (WMSE) and the perceptual loss guarantees both the local field of view (FoV) and global image reconstruction with high quality. Extensive experimental results show that our proposed scheme achieves significant bitrate savings compared with the anchor (HM + 360Lib), leading to 8.9%, 9.0%, 7.1% and 7.4% on average bit rate reductions in terms of PSNR, WPSNR, and PSNR of two viewports for luminance component of 360^o videos, respectively.



### Alternative design of DeepPDNet in the context of image restoration
- **Arxiv ID**: http://arxiv.org/abs/2202.09810v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.09810v1)
- **Published**: 2022-02-20 13:15:39+00:00
- **Updated**: 2022-02-20 13:15:39+00:00
- **Authors**: Mingyuan Jiu, Nelly Pustelnik
- **Comment**: None
- **Journal**: IEEE Signal Processing Letters 2022
- **Summary**: This work designs an image restoration deep network relying on unfolded Chambolle-Pock primal-dual iterations. Each layer of our network is built from Chambolle-Pock iterations when specified for minimizing a sum of a $\ell_2$-norm data-term and an analysis sparse prior. The parameters of our network are the step-sizes of the Chambolle-Pock scheme and the linear operator involved in sparsity-based penalization, including implicitly the regularization parameter. A backpropagation procedure is fully described. Preliminary experiments illustrate the good behavior of such a deep primal-dual network in the context of image restoration on BSD68 database.



### Sparsity Winning Twice: Better Robust Generalization from More Efficient Training
- **Arxiv ID**: http://arxiv.org/abs/2202.09844v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.09844v3)
- **Published**: 2022-02-20 15:52:08+00:00
- **Updated**: 2022-02-27 07:42:30+00:00
- **Authors**: Tianlong Chen, Zhenyu Zhang, Pengjun Wang, Santosh Balachandra, Haoyu Ma, Zehao Wang, Zhangyang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies demonstrate that deep networks, even robustified by the state-of-the-art adversarial training (AT), still suffer from large robust generalization gaps, in addition to the much more expensive training costs than standard training. In this paper, we investigate this intriguing problem from a new perspective, i.e., injecting appropriate forms of sparsity during adversarial training. We introduce two alternatives for sparse adversarial training: (i) static sparsity, by leveraging recent results from the lottery ticket hypothesis to identify critical sparse subnetworks arising from the early training; (ii) dynamic sparsity, by allowing the sparse subnetwork to adaptively adjust its connectivity pattern (while sticking to the same sparsity ratio) throughout training. We find both static and dynamic sparse methods to yield win-win: substantially shrinking the robust generalization gap and alleviating the robust overfitting, meanwhile significantly saving training and inference FLOPs. Extensive experiments validate our proposals with multiple network architectures on diverse datasets, including CIFAR-10/100 and Tiny-ImageNet. For example, our methods reduce robust generalization gap and overfitting by 34.44% and 4.02%, with comparable robust/standard accuracy boosts and 87.83%/87.82% training/inference FLOPs savings on CIFAR-100 with ResNet-18. Besides, our approaches can be organically combined with existing regularizers, establishing new state-of-the-art results in AT. Codes are available in https://github.com/VITA-Group/Sparsity-Win-Robust-Generalization.



### A Novel Framework for Brain Tumor Detection Based on Convolutional Variational Generative Models
- **Arxiv ID**: http://arxiv.org/abs/2202.09850v1
- **DOI**: 10.1007/s11042-022-12362-9
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.09850v1)
- **Published**: 2022-02-20 16:14:01+00:00
- **Updated**: 2022-02-20 16:14:01+00:00
- **Authors**: Wessam M. Salama, Ahmed Shokry
- **Comment**: None
- **Journal**: Multimedia Tools and Applications 2022
- **Summary**: Brain tumor detection can make the difference between life and death. Recently, deep learning-based brain tumor detection techniques have gained attention due to their higher performance. However, obtaining the expected performance of such deep learning-based systems requires large amounts of classified images to train the deep models. Obtaining such data is usually boring, time-consuming, and can easily be exposed to human mistakes which hinder the utilization of such deep learning approaches. This paper introduces a novel framework for brain tumor detection and classification. The basic idea is to generate a large synthetic MRI images dataset that reflects the typical pattern of the brain MRI images from a small class-unbalanced collected dataset. The resulted dataset is then used for training a deep model for detection and classification. Specifically, we employ two types of deep models. The first model is a generative model to capture the distribution of the important features in a set of small class-unbalanced brain MRI images. Then by using this distribution, the generative model can synthesize any number of brain MRI images for each class. Hence, the system can automatically convert a small unbalanced dataset to a larger balanced one. The second model is the classifier that is trained using the large balanced dataset to detect brain tumors in MRI images. The proposed framework acquires an overall detection accuracy of 96.88% which highlights the promise of the proposed framework as an accurate low-overhead brain tumor detection system.



### Non-Deterministic Face Mask Removal Based On 3D Priors
- **Arxiv ID**: http://arxiv.org/abs/2202.09856v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09856v2)
- **Published**: 2022-02-20 16:27:44+00:00
- **Updated**: 2022-03-03 09:55:02+00:00
- **Authors**: Xiangnan Yin, Liming Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel image inpainting framework for face mask removal. Although current methods have demonstrated their impressive ability in recovering damaged face images, they suffer from two main problems: the dependence on manually labeled missing regions and the deterministic result corresponding to each input. The proposed approach tackles these problems by integrating a multi-task 3D face reconstruction module with a face inpainting module. Given a masked face image, the former predicts a 3DMM-based reconstructed face together with a binary occlusion map, providing dense geometrical and textural priors that greatly facilitate the inpainting task of the latter. By gradually controlling the 3D shape parameters, our method generates high-quality dynamic inpainting results with different expressions and mouth movements. Qualitative and quantitative experiments verify the effectiveness of the proposed method.



### SRL-SOA: Self-Representation Learning with Sparse 1D-Operational Autoencoder for Hyperspectral Image Band Selection
- **Arxiv ID**: http://arxiv.org/abs/2202.09918v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.09918v1)
- **Published**: 2022-02-20 22:17:01+00:00
- **Updated**: 2022-02-20 22:17:01+00:00
- **Authors**: Mete Ahishali, Serkan Kiranyaz, Iftikhar Ahmad, Moncef Gabbouj
- **Comment**: None
- **Journal**: None
- **Summary**: The band selection in the hyperspectral image (HSI) data processing is an important task considering its effect on the computational complexity and accuracy. In this work, we propose a novel framework for the band selection problem: Self-Representation Learning (SRL) with Sparse 1D-Operational Autoencoder (SOA). The proposed SLR-SOA approach introduces a novel autoencoder model, SOA, that is designed to learn a representation domain where the data are sparsely represented. Moreover, the network composes of 1D-operational layers with the non-linear neuron model. Hence, the learning capability of neurons (filters) is greatly improved with shallow architectures. Using compact architectures is especially crucial in autoencoders as they tend to overfit easily because of their identity mapping objective. Overall, we show that the proposed SRL-SOA band selection approach outperforms the competing methods over two HSI data including Indian Pines and Salinas-A considering the achieved land cover classification accuracies. The software implementation of the SRL-SOA approach is shared publicly at https://github.com/meteahishali/SRL-SOA.



### Deconstructing Distributions: A Pointwise Framework of Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.09931v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2202.09931v2)
- **Published**: 2022-02-20 23:25:28+00:00
- **Updated**: 2022-06-07 06:32:29+00:00
- **Authors**: Gal Kaplun, Nikhil Ghosh, Saurabh Garg, Boaz Barak, Preetum Nakkiran
- **Comment**: GK and NG contributed equally. v2: Added Figures 4, 5
- **Journal**: None
- **Summary**: In machine learning, we traditionally evaluate the performance of a single model, averaged over a collection of test inputs. In this work, we propose a new approach: we measure the performance of a collection of models when evaluated on a $\textit{single input point}$. Specifically, we study a point's $\textit{profile}$: the relationship between models' average performance on the test distribution and their pointwise performance on this individual point. We find that profiles can yield new insights into the structure of both models and data -- in and out-of-distribution. For example, we empirically show that real data distributions consist of points with qualitatively different profiles. On one hand, there are "compatible" points with strong correlation between the pointwise and average performance. On the other hand, there are points with weak and even $\textit{negative}$ correlation: cases where improving overall model accuracy actually $\textit{hurts}$ performance on these inputs. We prove that these experimental observations are inconsistent with the predictions of several simplified models of learning proposed in prior work. As an application, we use profiles to construct a dataset we call CIFAR-10-NEG: a subset of CINIC-10 such that for standard models, accuracy on CIFAR-10-NEG is $\textit{negatively correlated}$ with accuracy on CIFAR-10 test. This illustrates, for the first time, an OOD dataset that completely inverts "accuracy-on-the-line" (Miller, Taori, Raghunathan, Sagawa, Koh, Shankar, Liang, Carmon, and Schmidt 2021)



