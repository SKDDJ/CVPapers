# Arxiv Papers in cs.CV on 2022-02-01
### Learning-Based Framework for Camera Calibration with Distortion Correction and High Precision Feature Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.00158v3
- **DOI**: 10.1109/LRA.2022.3192610
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00158v3)
- **Published**: 2022-02-01 00:19:18+00:00
- **Updated**: 2023-04-29 08:21:27+00:00
- **Authors**: Yesheng Zhang, Xu Zhao, Dahong Qian
- **Comment**: None
- **Journal**: in IEEE Robotics and Automation Letters, vol. 7, no. 4, pp.
  10470-10477, Oct. 2022
- **Summary**: Camera calibration is a crucial technique which significantly influences the performance of many robotic systems. Robustness and high precision have always been the pursuit of diverse calibration methods. State-of-the-art calibration techniques based on classical Zhang's method, however, still suffer from environmental noise, radial lens distortion and sub-optimal parameter estimation. Therefore, in this paper, we propose a hybrid camera calibration framework which combines learning-based approaches with traditional methods to handle these bottlenecks. In particular, this framework leverages learning-based approaches to perform efficient distortion correction and robust chessboard corner coordinate encoding. For sub-pixel accuracy of corner detection, a specially-designed coordinate decoding algorithm with embed outlier rejection mechanism is proposed. To avoid sub-optimal estimation results, we improve the traditional parameter estimation by RANSAC algorithm and achieve stable results. Compared with two widely-used camera calibration toolboxes, experiment results on both real and synthetic datasets manifest the better robustness and higher precision of the proposed framework. The massive synthetic dataset is the basis of our framework's decent performance and will be publicly available along with the code at https://github.com/Easonyesheng/CCS.



### Dilated Continuous Random Field for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.00162v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.00162v1)
- **Published**: 2022-02-01 00:38:55+00:00
- **Updated**: 2022-02-01 00:38:55+00:00
- **Authors**: Xi Mo, Xiangyu Chen, Cuncong Zhong, Rui Li, Kaidong Li, Usman Sajid
- **Comment**: Manuscript accepted by IEEE International Conference on Robotics and
  Automation (ICRA 2022)
- **Journal**: None
- **Summary**: Mean field approximation methodology has laid the foundation of modern Continuous Random Field (CRF) based solutions for the refinement of semantic segmentation. In this paper, we propose to relax the hard constraint of mean field approximation - minimizing the energy term of each node from probabilistic graphical model, by a global optimization with the proposed dilated sparse convolution module (DSConv). In addition, adaptive global average-pooling and adaptive global max-pooling are implemented as replacements of fully connected layers. In order to integrate DSConv, we design an end-to-end, time-efficient DilatedCRF pipeline. The unary energy term is derived either from pre-softmax and post-softmax features, or the predicted affordance map using a conventional classifier, making it easier to implement DilatedCRF for varieties of classifiers. We also present superior experimental results of proposed approach on the suction dataset comparing to other CRF-based approaches.



### DexVIP: Learning Dexterous Grasping with Human Hand Pose Priors from Video
- **Arxiv ID**: http://arxiv.org/abs/2202.00164v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00164v1)
- **Published**: 2022-02-01 00:45:57+00:00
- **Updated**: 2022-02-01 00:45:57+00:00
- **Authors**: Priyanka Mandikal, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: Dexterous multi-fingered robotic hands have a formidable action space, yet their morphological similarity to the human hand holds immense potential to accelerate robot learning. We propose DexVIP, an approach to learn dexterous robotic grasping from human-object interactions present in in-the-wild YouTube videos. We do this by curating grasp images from human-object interaction videos and imposing a prior over the agent's hand pose when learning to grasp with deep reinforcement learning. A key advantage of our method is that the learned policy is able to leverage free-form in-the-wild visual data. As a result, it can easily scale to new objects, and it sidesteps the standard practice of collecting human demonstrations in a lab -- a much more expensive and indirect way to capture human expertise. Through experiments on 27 objects with a 30-DoF simulated robot hand, we demonstrate that DexVIP compares favorably to existing approaches that lack a hand pose prior or rely on specialized tele-operation equipment to obtain human demonstrations, while also being faster to train. Project page: https://vision.cs.utexas.edu/projects/dexvip-dexterous-grasp-pose-prior



### Fractional Motion Estimation for Point Cloud Compression
- **Arxiv ID**: http://arxiv.org/abs/2202.00172v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00172v1)
- **Published**: 2022-02-01 01:00:28+00:00
- **Updated**: 2022-02-01 01:00:28+00:00
- **Authors**: Haoran Hong, Eduardo Pavez, Antonio Ortega, Ryosuke Watanabe, Keisuke Nonaka
- **Comment**: ACCPTED by DCC2022
- **Journal**: None
- **Summary**: Motivated by the success of fractional pixel motion in video coding, we explore the design of motion estimation with fractional-voxel resolution for compression of color attributes of dynamic 3D point clouds. Our proposed block-based fractional-voxel motion estimation scheme takes into account the fundamental differences between point clouds and videos, i.e., the irregularity of the distribution of voxels within a frame and across frames. We show that motion compensation can benefit from the higher resolution reference and more accurate displacements provided by fractional precision. Our proposed scheme significantly outperforms comparable methods that only use integer motion. The proposed scheme can be combined with and add sizeable gains to state-of-the-art systems that use transforms such as Region Adaptive Graph Fourier Transform and Region Adaptive Haar Transform.



### Blind Image Deconvolution Using Variational Deep Image Prior
- **Arxiv ID**: http://arxiv.org/abs/2202.00179v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00179v3)
- **Published**: 2022-02-01 01:33:58+00:00
- **Updated**: 2023-06-06 02:16:47+00:00
- **Authors**: Dong Huo, Abbas Masoumzadeh, Rafsanjany Kushol, Yee-Hong Yang
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI), 2023
- **Journal**: None
- **Summary**: Conventional deconvolution methods utilize hand-crafted image priors to constrain the optimization. While deep-learning-based methods have simplified the optimization by end-to-end training, they fail to generalize well to blurs unseen in the training dataset. Thus, training image-specific models is important for higher generalization. Deep image prior (DIP) provides an approach to optimize the weights of a randomly initialized network with a single degraded image by maximum a posteriori (MAP), which shows that the architecture of a network can serve as the hand-crafted image prior. Different from the conventional hand-crafted image priors that are statistically obtained, it is hard to find a proper network architecture because the relationship between images and their corresponding network architectures is unclear. As a result, the network architecture cannot provide enough constraint for the latent sharp image. This paper proposes a new variational deep image prior (VDIP) for blind image deconvolution, which exploits additive hand-crafted image priors on latent sharp images and approximates a distribution for each pixel to avoid suboptimal solutions. Our mathematical analysis shows that the proposed method can better constrain the optimization. The experimental results further demonstrate that the generated images have better quality than that of the original DIP on benchmark datasets. The source code of our VDIP is available at https://github.com/Dong-Huo/VDIP-Deconvolution.



### CLA-NeRF: Category-Level Articulated Neural Radiance Field
- **Arxiv ID**: http://arxiv.org/abs/2202.00181v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.00181v3)
- **Published**: 2022-02-01 02:04:24+00:00
- **Updated**: 2022-03-04 00:34:34+00:00
- **Authors**: Wei-Cheng Tseng, Hung-Ju Liao, Lin Yen-Chen, Min Sun
- **Comment**: accepted by ICRA 2022
- **Journal**: None
- **Summary**: We propose CLA-NeRF -- a Category-Level Articulated Neural Radiance Field that can perform view synthesis, part segmentation, and articulated pose estimation. CLA-NeRF is trained at the object category level using no CAD models and no depth, but a set of RGB images with ground truth camera poses and part segments. During inference, it only takes a few RGB views (i.e., few-shot) of an unseen 3D object instance within the known category to infer the object part segmentation and the neural radiance field. Given an articulated pose as input, CLA-NeRF can perform articulation-aware volume rendering to generate the corresponding RGB image at any camera pose. Moreover, the articulated pose of an object can be estimated via inverse rendering. In our experiments, we evaluate the framework across five categories on both synthetic and real-world data. In all cases, our method shows realistic deformation results and accurate articulated pose estimation. We believe that both few-shot articulated object rendering and articulated pose estimation open doors for robots to perceive and interact with unseen articulated objects.



### Semi-supervised 3D Object Detection via Temporal Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.00182v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.00182v3)
- **Published**: 2022-02-01 02:06:54+00:00
- **Updated**: 2023-03-06 23:30:28+00:00
- **Authors**: Jianren Wang, Haiming Gang, Siddharth Ancha, Yi-Ting Chen, David Held
- **Comment**: 3DV 2021
- **Journal**: None
- **Summary**: 3D object detection plays an important role in autonomous driving and other robotics applications. However, these detectors usually require training on large amounts of annotated data that is expensive and time-consuming to collect. Instead, we propose leveraging large amounts of unlabeled point cloud videos by semi-supervised learning of 3D object detectors via temporal graph neural networks. Our insight is that temporal smoothing can create more accurate detection results on unlabeled data, and these smoothed detections can then be used to retrain the detector. We learn to perform this temporal reasoning with a graph neural network, where edges represent the relationship between candidate detections in different time frames. After semi-supervised learning, our method achieves state-of-the-art detection performance on the challenging nuScenes and H3D benchmarks, compared to baselines trained on the same amount of labeled data. Project and code are released at https://www.jianrenw.com/SOD-TGNN/.



### LayoutEnhancer: Generating Good Indoor Layouts from Imperfect Data
- **Arxiv ID**: http://arxiv.org/abs/2202.00185v2
- **DOI**: 10.1145/3550469.3555425
- **Categories**: **cs.GR**, cs.AI, cs.CV, cs.LG, I.3.6; I.2.1; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2202.00185v2)
- **Published**: 2022-02-01 02:25:04+00:00
- **Updated**: 2022-10-06 01:04:32+00:00
- **Authors**: Kurt Leimer, Paul Guerrero, Tomer Weiss, Przemyslaw Musialski
- **Comment**: preprint of ACM SIGGRAPH Asia 2022 Conference Paper, 14 pages
  including appendix and supplementary figures, 16 figures
- **Journal**: None
- **Summary**: We address the problem of indoor layout synthesis, which is a topic of continuing research interest in computer graphics. The newest works made significant progress using data-driven generative methods; however, these approaches rely on suitable datasets. In practice, desirable layout properties may not exist in a dataset, for instance, specific expert knowledge can be missing in the data. We propose a method that combines expert knowledge, for example, knowledge about ergonomics, with a data-driven generator based on the popular Transformer architecture. The knowledge is given as differentiable scalar functions, which can be used both as weights or as additional terms in the loss function. Using this knowledge, the synthesized layouts can be biased to exhibit desirable properties, even if these properties are not present in the dataset. Our approach can also alleviate problems of lack of data and imperfections in the data. Our work aims to improve generative machine learning for modeling and provide novel tools for designers and amateurs for the problem of interior layout creation.



### Recognition-Aware Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2202.00198v1
- **DOI**: 10.2352/EI.2022.34.14.COIMG-220
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00198v1)
- **Published**: 2022-02-01 03:33:51+00:00
- **Updated**: 2022-02-01 03:33:51+00:00
- **Authors**: Maxime Kawawa-Beaudan, Ryan Roggenkemper, Avideh Zakhor
- **Comment**: Electronic Imaging Symposium, Computational Imaging XX Conference,
  January 2022
- **Journal**: None
- **Summary**: Learned image compression methods generally optimize a rate-distortion loss, trading off improvements in visual distortion for added bitrate. Increasingly, however, compressed imagery is used as an input to deep learning networks for various tasks such as classification, object detection, and superresolution. We propose a recognition-aware learned compression method, which optimizes a rate-distortion loss alongside a task-specific loss, jointly learning compression and recognition networks. We augment a hierarchical autoencoder-based compression network with an EfficientNet recognition model and use two hyperparameters to trade off between distortion, bitrate, and recognition performance. We characterize the classification accuracy of our proposed method as a function of bitrate and find that for low bitrates our method achieves as much as 26% higher recognition accuracy at equivalent bitrates compared to traditional methods such as Better Portable Graphics (BPG).



### Disentangling multiple scattering with deep learning: application to strain mapping from electron diffraction patterns
- **Arxiv ID**: http://arxiv.org/abs/2202.00204v1
- **DOI**: None
- **Categories**: **cond-mat.mtrl-sci**, cs.CV, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2202.00204v1)
- **Published**: 2022-02-01 03:53:39+00:00
- **Updated**: 2022-02-01 03:53:39+00:00
- **Authors**: Joydeep Munshi, Alexander Rakowski, Benjamin H Savitzky, Steven E Zeltmann, Jim Ciston, Matthew Henderson, Shreyas Cholia, Andrew M Minor, Maria KY Chan, Colin Ophus
- **Comment**: 17 pages, 7 figures
- **Journal**: None
- **Summary**: Implementation of a fast, robust, and fully-automated pipeline for crystal structure determination and underlying strain mapping for crystalline materials is important for many technological applications. Scanning electron nanodiffraction offers a procedure for identifying and collecting strain maps with good accuracy and high spatial resolutions. However, the application of this technique is limited, particularly in thick samples where the electron beam can undergo multiple scattering, which introduces signal nonlinearities. Deep learning methods have the potential to invert these complex signals, but previous implementations are often trained only on specific crystal systems or a small subset of the crystal structure and microscope parameter phase space. In this study, we implement a Fourier space, complex-valued deep neural network called FCU-Net, to invert highly nonlinear electron diffraction patterns into the corresponding quantitative structure factor images. We trained the FCU-Net using over 200,000 unique simulated dynamical diffraction patterns which include many different combinations of crystal structures, orientations, thicknesses, microscope parameters, and common experimental artifacts. We evaluated the trained FCU-Net model against simulated and experimental 4D-STEM diffraction datasets, where it substantially out-performs conventional analysis methods. Our simulated diffraction pattern library, implementation of FCU-Net, and trained model weights are freely available in open source repositories, and can be adapted to many different diffraction measurement problems.



### Generalizability of Machine Learning Models: Quantitative Evaluation of Three Methodological Pitfalls
- **Arxiv ID**: http://arxiv.org/abs/2202.01337v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01337v2)
- **Published**: 2022-02-01 05:07:27+00:00
- **Updated**: 2022-09-07 21:39:24+00:00
- **Authors**: Farhad Maleki, Katie Ovens, Rajiv Gupta, Caroline Reinhold, Alan Spatz, Reza Forghani
- **Comment**: 18 pages, 7 Figures
- **Journal**: None
- **Summary**: Purpose: Despite the potential of machine learning models, the lack of generalizability has hindered their widespread adoption in clinical practice. We investigate three methodological pitfalls: (1) violation of independence assumption, (2) model evaluation with an inappropriate performance indicator or baseline for comparison, and (3) batch effect. Materials and Methods: Using several retrospective datasets, we implement machine learning models with and without the pitfalls to quantitatively illustrate these pitfalls' effect on model generalizability. Results: Violation of independence assumption, more specifically, applying oversampling, feature selection, and data augmentation before splitting data into train, validation, and test sets, respectively, led to misleading and superficial gains in F1 scores of 71.2% in predicting local recurrence and 5.0% in predicting 3-year overall survival in head and neck cancer as well as 46.0% in distinguishing histopathological patterns in lung cancer. Further, randomly distributing data points for a subject across training, validation, and test sets led to a 21.8% superficial increase in F1 score. Also, we showed the importance of the choice of performance measures and baseline for comparison. In the presence of batch effect, a model built for pneumonia detection led to F1 score of 98.7%. However, when the same model was applied to a new dataset of normal patients, it only correctly classified 3.86% of the samples. Conclusions: These methodological pitfalls cannot be captured using internal model evaluation, and the inaccurate predictions made by such models may lead to wrong conclusions and interpretations. Therefore, understanding and avoiding these pitfalls is necessary for developing generalizable models.



### Towards Ignoring Backgrounds and Improving Generalization: a Costless DNN Visual Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2202.00232v6
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.00232v6)
- **Published**: 2022-02-01 05:58:01+00:00
- **Updated**: 2023-06-23 22:41:32+00:00
- **Authors**: Pedro R. A. S. Bassi, Sergio S. J. Dertkigil, Andrea Cavalli
- **Comment**: Inclusion of new benchmark DNNs: Right for the Right Reasons and
  Vision Transformer. Improved theoretical analysis of the ISNet (Section 4.9
  and Appendix C). New experiments with synthetic background bias (Section 2.1)
- **Journal**: None
- **Summary**: This work introduces an attention mechanism for image classifiers and the corresponding deep neural network (DNN) architecture, dubbed ISNet. During training, the ISNet uses segmentation targets to learn how to find the image's region of interest and concentrate its attention on it. The proposal is based on a novel concept, background relevance minimization in LRP explanation heatmaps. It can be applied to virtually any classification neural network architecture, without any extra computational cost at run-time. Capable of ignoring the background, the resulting single DNN can substitute the common pipeline of a segmenter followed by a classifier, being faster and lighter. After injecting synthetic bias in images' backgrounds (in diverse applications), we compare the ISNet to multiple state-of-the-art neural networks, and quantitatively demonstrate its superior capacity of minimizing the bias influence over the classifier decisions. The tasks of COVID-19 and tuberculosis detection in chest X-rays commonly employ mixed training databases, which naturally foster background bias and shortcut learning. By focusing on lungs, the ISNet reduced shortcut learning, leading to significantly superior generalization to external (out-of-distribution) test datasets. ISNet presents an accurate, fast, and light methodology to ignore backgrounds and improve generalization.



### Adversarial Imitation Learning from Video using a State Observer
- **Arxiv ID**: http://arxiv.org/abs/2202.00243v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2202.00243v2)
- **Published**: 2022-02-01 06:46:48+00:00
- **Updated**: 2022-07-27 00:35:11+00:00
- **Authors**: Haresh Karnan, Garrett Warnell, Faraz Torabi, Peter Stone
- **Comment**: None
- **Journal**: International Conference on Robotics and Automation (ICRA) 2022
- **Summary**: The imitation learning research community has recently made significant progress towards the goal of enabling artificial agents to imitate behaviors from video demonstrations alone. However, current state-of-the-art approaches developed for this problem exhibit high sample complexity due, in part, to the high-dimensional nature of video observations. Towards addressing this issue, we introduce here a new algorithm called Visual Generative Adversarial Imitation from Observation using a State Observer VGAIfO-SO. At its core, VGAIfO-SO seeks to address sample inefficiency using a novel, self-supervised state observer, which provides estimates of lower-dimensional proprioceptive state representations from high-dimensional images. We show experimentally in several continuous control environments that VGAIfO-SO is more sample efficient than other IfO algorithms at learning from video-only demonstrations and can sometimes even achieve performance close to the Generative Adversarial Imitation from Observation (GAIfO) algorithm that has privileged access to the demonstrator's proprioceptive state information.



### Detecting Human-Object Interactions with Object-Guided Cross-Modal Calibrated Semantics
- **Arxiv ID**: http://arxiv.org/abs/2202.00259v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00259v1)
- **Published**: 2022-02-01 07:39:04+00:00
- **Updated**: 2022-02-01 07:39:04+00:00
- **Authors**: Hangjie Yuan, Mang Wang, Dong Ni, Liangpeng Xu
- **Comment**: Accepted to AAAI2022
- **Journal**: None
- **Summary**: Human-Object Interaction (HOI) detection is an essential task to understand human-centric images from a fine-grained perspective. Although end-to-end HOI detection models thrive, their paradigm of parallel human/object detection and verb class prediction loses two-stage methods' merit: object-guided hierarchy. The object in one HOI triplet gives direct clues to the verb to be predicted. In this paper, we aim to boost end-to-end models with object-guided statistical priors. Specifically, We propose to utilize a Verb Semantic Model (VSM) and use semantic aggregation to profit from this object-guided hierarchy. Similarity KL (SKL) loss is proposed to optimize VSM to align with the HOI dataset's priors. To overcome the static semantic embedding problem, we propose to generate cross-modality-aware visual and semantic features by Cross-Modal Calibration (CMC). The above modules combined composes Object-guided Cross-modal Calibration Network (OCN). Experiments conducted on two popular HOI detection benchmarks demonstrate the significance of incorporating the statistical prior knowledge and produce state-of-the-art performances. More detailed analysis indicates proposed modules serve as a stronger verb predictor and a more superior method of utilizing prior knowledge. The codes are available at \url{https://github.com/JacobYuan7/OCN-HOI-Benchmark}.



### Advances in MetaDL: AAAI 2021 challenge and workshop
- **Arxiv ID**: http://arxiv.org/abs/2202.01890v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.01890v1)
- **Published**: 2022-02-01 07:46:36+00:00
- **Updated**: 2022-02-01 07:46:36+00:00
- **Authors**: Adrian El Baz, Isabelle Guyon, Zhengying Liu, Jan van Rijn, Sebastien Treguer, Joaquin Vanschoren
- **Comment**: Proceedings of Machine Learning Research, PMLR, 2021
- **Journal**: None
- **Summary**: To stimulate advances in metalearning using deep learning techniques (MetaDL), we organized in 2021 a challenge and an associated workshop. This paper presents the design of the challenge and its results, and summarizes presentations made at the workshop. The challenge focused on few-shot learning classification tasks of small images. Participants' code submissions were run in a uniform manner, under tight computational constraints. This put pressure on solution designs to use existing architecture backbones and/or pre-trained networks. Winning methods featured various classifiers trained on top of the second last layer of popular CNN backbones, fined-tuned on the meta-training data (not necessarily in an episodic manner), then trained on the labeled support and tested on the unlabeled query sets of the meta-test data.



### Fully Online Meta-Learning Without Task Boundaries
- **Arxiv ID**: http://arxiv.org/abs/2202.00263v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00263v3)
- **Published**: 2022-02-01 07:51:24+00:00
- **Updated**: 2022-02-18 02:46:53+00:00
- **Authors**: Jathushan Rajasegaran, Chelsea Finn, Sergey Levine
- **Comment**: None
- **Journal**: None
- **Summary**: While deep networks can learn complex functions such as classifiers, detectors, and trackers, many applications require models that continually adapt to changing input distributions, changing tasks, and changing environmental conditions. Indeed, this ability to continuously accrue knowledge and use past experience to learn new tasks quickly in continual settings is one of the key properties of an intelligent system. For complex and high-dimensional problems, simply updating the model continually with standard learning algorithms such as gradient descent may result in slow adaptation. Meta-learning can provide a powerful tool to accelerate adaptation yet is conventionally studied in batch settings. In this paper, we study how meta-learning can be applied to tackle online problems of this nature, simultaneously adapting to changing tasks and input distributions and meta-training the model in order to adapt more quickly in the future. Extending meta-learning into the online setting presents its own challenges, and although several prior methods have studied related problems, they generally require a discrete notion of tasks, with known ground-truth task boundaries. Such methods typically adapt to each task in sequence, resetting the model between tasks, rather than adapting continuously across tasks. In many real-world settings, such discrete boundaries are unavailable, and may not even exist. To address these settings, we propose a Fully Online Meta-Learning (FOML) algorithm, which does not require any ground truth knowledge about the task boundaries and stays fully online without resetting back to pre-trained weights. Our experiments show that FOML was able to learn new tasks faster than the state-of-the-art online learning methods on Rainbow-MNIST, CIFAR100 and CELEBA datasets.



### Access Control of Object Detection Models Using Encrypted Feature Maps
- **Arxiv ID**: http://arxiv.org/abs/2202.00265v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.00265v2)
- **Published**: 2022-02-01 07:52:38+00:00
- **Updated**: 2022-03-10 06:15:16+00:00
- **Authors**: Teru Nagamori, Hiroki Ito, April Pyone Maung Maung, Hitoshi Kiya
- **Comment**: To appear in 2022 IEEE 4th Global Conference on Life Sciences and
  Technologies (LifeTech 2022)
- **Journal**: None
- **Summary**: In this paper, we propose an access control method for object detection models. The use of encrypted images or encrypted feature maps has been demonstrated to be effective in access control of models from unauthorized access. However, the effectiveness of the approach has been confirmed in only image classification models and semantic segmentation models, but not in object detection models. In this paper, the use of encrypted feature maps is shown to be effective in access control of object detection models for the first time.



### StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets
- **Arxiv ID**: http://arxiv.org/abs/2202.00273v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00273v2)
- **Published**: 2022-02-01 08:22:34+00:00
- **Updated**: 2022-05-05 09:18:29+00:00
- **Authors**: Axel Sauer, Katja Schwarz, Andreas Geiger
- **Comment**: To appear in SIGGRAPH 2022. Project Page:
  https://sites.google.com/view/stylegan-xl/
- **Journal**: None
- **Summary**: Computer graphics has experienced a recent surge of data-centric approaches for photorealistic and controllable content creation. StyleGAN in particular sets new standards for generative modeling regarding image quality and controllability. However, StyleGAN's performance severely degrades on large unstructured datasets such as ImageNet. StyleGAN was designed for controllability; hence, prior works suspect its restrictive design to be unsuitable for diverse datasets. In contrast, we find the main limiting factor to be the current training strategy. Following the recently introduced Projected GAN paradigm, we leverage powerful neural network priors and a progressive growing strategy to successfully train the latest StyleGAN3 generator on ImageNet. Our final model, StyleGAN-XL, sets a new state-of-the-art on large-scale image synthesis and is the first to generate images at a resolution of $1024^2$ at such a dataset scale. We demonstrate that this model can invert and edit images beyond the narrow domain of portraits or specific object classes.



### Laplacian2Mesh: Laplacian-Based Mesh Understanding
- **Arxiv ID**: http://arxiv.org/abs/2202.00307v2
- **DOI**: 10.1109/TVCG.2023.3259044
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.00307v2)
- **Published**: 2022-02-01 10:10:13+00:00
- **Updated**: 2023-03-16 10:57:44+00:00
- **Authors**: Qiujie Dong, Zixiong Wang, Manyi Li, Junjie Gao, Shuangmin Chen, Zhenyu Shu, Shiqing Xin, Changhe Tu, Wenping Wang
- **Comment**: Accepted by IEEE Transactions on Visualization and Computer Graphics
  (TVCG)
- **Journal**: None
- **Summary**: Geometric deep learning has sparked a rising interest in computer graphics to perform shape understanding tasks, such as shape classification and semantic segmentation. When the input is a polygonal surface, one has to suffer from the irregular mesh structure. Motivated by the geometric spectral theory, we introduce Laplacian2Mesh, a novel and flexible convolutional neural network (CNN) framework for coping with irregular triangle meshes (vertices may have any valence). By mapping the input mesh surface to the multi-dimensional Laplacian-Beltrami space, Laplacian2Mesh enables one to perform shape analysis tasks directly using the mature CNNs, without the need to deal with the irregular connectivity of the mesh structure. We further define a mesh pooling operation such that the receptive field of the network can be expanded while retaining the original vertex set as well as the connections between them. Besides, we introduce a channel-wise self-attention block to learn the individual importance of feature ingredients. Laplacian2Mesh not only decouples the geometry from the irregular connectivity of the mesh structure but also better captures the global features that are central to shape classification and segmentation. Extensive tests on various datasets demonstrate the effectiveness and efficiency of Laplacian2Mesh, particularly in terms of the capability of being vulnerable to noise to fulfill various learning tasks.



### From Explanations to Segmentation: Using Explainable AI for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.00315v1
- **DOI**: 10.5220/0010893600003124
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00315v1)
- **Published**: 2022-02-01 10:26:10+00:00
- **Updated**: 2022-02-01 10:26:10+00:00
- **Authors**: Clemens Seibold, Johannes Künzel, Anna Hilsmann, Peter Eisert
- **Comment**: to be published in: 17th International Conference on Computer Vision
  Theory and Applications (VISAPP), February 2022
- **Journal**: None
- **Summary**: The new era of image segmentation leveraging the power of Deep Neural Nets (DNNs) comes with a price tag: to train a neural network for pixel-wise segmentation, a large amount of training samples has to be manually labeled on pixel-precision. In this work, we address this by following an indirect solution. We build upon the advances of the Explainable AI (XAI) community and extract a pixel-wise binary segmentation from the output of the Layer-wise Relevance Propagation (LRP) explaining the decision of a classification network. We show that we achieve similar results compared to an established U-Net segmentation architecture, while the generation of the training data is significantly simplified. The proposed method can be trained in a weakly supervised fashion, as the training samples must be only labeled on image-level, at the same time enabling the output of a segmentation mask. This makes it especially applicable to a wider range of real applications where tedious pixel-level labelling is often not possible.



### Deep Learning for Ultrasound Speed-of-Sound Reconstruction: Impacts of Training Data Diversity on Stability and Robustness
- **Arxiv ID**: http://arxiv.org/abs/2202.01208v2
- **DOI**: 10.59275/j.melba.2023-4g6a
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01208v2)
- **Published**: 2022-02-01 11:09:35+00:00
- **Updated**: 2023-05-08 22:53:48+00:00
- **Authors**: Farnaz Khun Jush, Markus Biele, Peter M. Dueppenbecker, Andreas Maier
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2023:007
- **Journal**: Machine.Learning.for.Biomedical.Imaging. 2 (2023)
- **Summary**: Ultrasound b-mode imaging is a qualitative approach and diagnostic quality strongly depends on operators' training and experience. Quantitative approaches can provide information about tissue properties; therefore, can be used for identifying various tissue types, e.g., speed-of-sound in the tissue can be used as a biomarker for tissue malignancy, especially in breast imaging. Recent studies showed the possibility of speed-of-sound reconstruction using deep neural networks that are fully trained on simulated data. However, because of the ever-present domain shift between simulated and measured data, the stability and performance of these models in real setups are still under debate. In prior works, for training data generation, tissue structures were modeled as simplified geometrical structures which does not reflect the complexity of the real tissues. In this study, we proposed a new simulation setup for training data generation based on Tomosynthesis images. We combined our approach with the simplified geometrical model and investigated the impacts of training data diversity on the stability and robustness of an existing network architecture. We studied the sensitivity of the trained network to different simulation parameters, e.g., echogenicity, number of scatterers, noise, and geometry. We showed that the network trained with the joint set of data is more stable on out-of-domain simulated data as well as measured phantom data.



### Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space
- **Arxiv ID**: http://arxiv.org/abs/2202.00368v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.00368v1)
- **Published**: 2022-02-01 12:18:30+00:00
- **Updated**: 2022-02-01 12:18:30+00:00
- **Authors**: Steeven Janny, Fabien Baradel, Natalia Neverova, Madiha Nadri, Greg Mori, Christian Wolf
- **Comment**: None
- **Journal**: International Conference on Learning Representation (2022)
- **Summary**: Learning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often defined on low dimensional manifolds and must be extracted from complex signals dominated by appearance, lighting, textures and also spurious correlations in the data. We present a method for learning counterfactual reasoning of physical processes in pixel space, which requires the prediction of the impact of interventions on initial conditions. Going beyond the identification of structural relationships, we deal with the challenging problem of forecasting raw video over long horizons. Our method does not require the knowledge or supervision of any ground truth positions or other object or scene properties. Our model learns and acts on a suitable hybrid latent representation based on a combination of dense features, sets of 2D keypoints and an additional latent vector per keypoint. We show that this better captures the dynamics of physical processes than purely dense or sparse representations. We introduce a new challenging and carefully designed counterfactual benchmark for predictions in pixel space and outperform strong baselines in physics-inspired ML and video prediction.



### A Comparative Study of Calibration Methods for Imbalanced Class Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.00386v1
- **DOI**: 10.1007/s11042-020-10485-5
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00386v1)
- **Published**: 2022-02-01 12:56:17+00:00
- **Updated**: 2022-02-01 12:56:17+00:00
- **Authors**: Umang Aggarwal, Adrian Popescu, Eden Belouadah, Céline Hudelot
- **Comment**: None
- **Journal**: Multimedia Tools and Applications (2021)
- **Summary**: Deep learning approaches are successful in a wide range of AI problems and in particular for visual recognition tasks. However, there are still open problems among which is the capacity to handle streams of visual information and the management of class imbalance in datasets. Existing research approaches these two problems separately while they co-occur in real world applications. Here, we study the problem of learning incrementally from imbalanced datasets. We focus on algorithms which have a constant deep model complexity and use a bounded memory to store exemplars of old classes across incremental states. Since memory is bounded, old classes are learned with fewer images than new classes and an imbalance due to incremental learning is added to the initial dataset imbalance. A score prediction bias in favor of new classes appears and we evaluate a comprehensive set of score calibration methods to reduce it. Evaluation is carried with three datasets, using two dataset imbalance configurations and three bounded memory sizes. Results show that most calibration methods have beneficial effect and that they are most useful for lower bounded memory sizes, which are most interesting in practice. As a secondary contribution, we remove the usual distillation component from the loss function of incremental learning algorithms. We show that simpler vanilla fine tuning is a stronger backbone for imbalanced incremental learning algorithms.



### Minority Class Oriented Active Learning for Imbalanced Datasets
- **Arxiv ID**: http://arxiv.org/abs/2202.00390v1
- **DOI**: 10.1109/ICPR48806.2021.9412182
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00390v1)
- **Published**: 2022-02-01 13:13:41+00:00
- **Updated**: 2022-02-01 13:13:41+00:00
- **Authors**: Umang Aggarwal, Adrian Popescu, Céline Hudelot
- **Comment**: None
- **Journal**: 2020 25th International Conference on Pattern Recognition (ICPR)
- **Summary**: Active learning aims to optimize the dataset annotation process when resources are constrained. Most existing methods are designed for balanced datasets. Their practical applicability is limited by the fact that a majority of real-life datasets are actually imbalanced. Here, we introduce a new active learning method which is designed for imbalanced datasets. It favors samples likely to be in minority classes so as to reduce the imbalance of the labeled subset and create a better representation for these classes. We also compare two training schemes for active learning: (1) the one commonly deployed in deep active learning using model fine tuning for each iteration and (2) a scheme which is inspired by transfer learning and exploits generic pre-trained models and train shallow classifiers for each iteration. Evaluation is run with three imbalanced datasets. Results show that the proposed active learning method outperforms competitive baselines. Equally interesting, they also indicate that the transfer learning training scheme outperforms model fine tuning if features are transferable from the generic dataset to the unlabeled one. This last result is surprising and should encourage the community to explore the design of deep active learning methods.



### CAESR: Conditional Autoencoder and Super-Resolution for Learned Spatial Scalability
- **Arxiv ID**: http://arxiv.org/abs/2202.00416v1
- **DOI**: 10.1109/VCIP53242.2021.9675351
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2202.00416v1)
- **Published**: 2022-02-01 13:59:43+00:00
- **Updated**: 2022-02-01 13:59:43+00:00
- **Authors**: Charles Bonnineau, Wassim Hamidouche, Jean-François Travers, Naty Sidaty, Jean-Yves Aubié, Olivier Deforges
- **Comment**: None
- **Journal**: 2021 International Conference on Visual Communications and Image
  Processing (VCIP)
- **Summary**: In this paper, we present CAESR, an hybrid learning-based coding approach for spatial scalability based on the versatile video coding (VVC) standard. Our framework considers a low-resolution signal encoded with VVC intra-mode as a base-layer (BL), and a deep conditional autoencoder with hyperprior (AE-HP) as an enhancement-layer (EL) model. The EL encoder takes as inputs both the upscaled BL reconstruction and the original image. Our approach relies on conditional coding that learns the optimal mixture of the source and the upscaled BL image, enabling better performance than residual coding. On the decoder side, a super-resolution (SR) module is used to recover high-resolution details and invert the conditional coding process. Experimental results have shown that our solution is competitive with the VVC full-resolution intra coding while being scalable.



### Review of Serial and Parallel Min-Cut/Max-Flow Algorithms for Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2202.00418v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00418v2)
- **Published**: 2022-02-01 14:06:27+00:00
- **Updated**: 2022-04-20 11:25:46+00:00
- **Authors**: Patrick M. Jensen, Niels Jeppesen, Anders B. Dahl, Vedrana A. Dahl
- **Comment**: 20 pages, 13 figures, accepted for publication at T-PAMI
- **Journal**: None
- **Summary**: Minimum cut/maximum flow (min-cut/max-flow) algorithms solve a variety of problems in computer vision and thus significant effort has been put into developing fast min-cut/max-flow algorithms. As a result, it is difficult to choose an ideal algorithm for a given problem. Furthermore, parallel algorithms have not been thoroughly compared. In this paper, we evaluate the state-of-the-art serial and parallel min-cut/max-flow algorithms on the largest set of computer vision problems yet. We focus on generic algorithms, i.e., for unstructured graphs, but also compare with the specialized GridCut implementation. When applicable, GridCut performs best. Otherwise, the two pseudoflow algorithms, Hochbaum pseudoflow and excesses incremental breadth first search, achieves the overall best performance. The most memory efficient implementation tested is the Boykov-Kolmogorov algorithm. Amongst generic parallel algorithms, we find the bottom-up merging approach by Liu and Sun to be best, but no method is dominant. Of the generic parallel methods, only the parallel preflow push-relabel algorithm is able to efficiently scale with many processors across problem sizes, and no generic parallel method consistently outperforms serial algorithms. Finally, we provide and evaluate strategies for algorithm selection to obtain good expected performance. We make our dataset and implementations publicly available for further research.



### Sinogram Enhancement with Generative Adversarial Networks using Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/2202.00419v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.00419v1)
- **Published**: 2022-02-01 14:08:32+00:00
- **Updated**: 2022-02-01 14:08:32+00:00
- **Authors**: Emilien Valat, Katayoun Farrahi, Thomas Blumensath
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Compensating scarce measurements by inferring them from computational models is a way to address ill-posed inverse problems. We tackle Limited Angle Tomography by completing the set of acquisitions using a generative model and prior-knowledge about the scanned object. Using a Generative Adversarial Network as model and Computer-Assisted Design data as shape prior, we demonstrate a quantitative and qualitative advantage of our technique over other state-of-the-art methods. Inferring a substantial number of consecutive missing measurements, we offer an alternative to other image inpainting techniques that fall short of providing a satisfying answer to our research question: can X-Ray exposition be reduced by using generative models to infer lacking measurements?



### Continual Attentive Fusion for Incremental Learning in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.00432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00432v1)
- **Published**: 2022-02-01 14:38:53+00:00
- **Updated**: 2022-02-01 14:38:53+00:00
- **Authors**: Guanglei Yang, Enrico Fini, Dan Xu, Paolo Rota, Mingli Ding, Hao Tang, Xavier Alameda-Pineda, Elisa Ricci
- **Comment**: None
- **Journal**: None
- **Summary**: Over the past years, semantic segmentation, as many other tasks in computer vision, benefited from the progress in deep neural networks, resulting in significantly improved performance. However, deep architectures trained with gradient-based techniques suffer from catastrophic forgetting, which is the tendency to forget previously learned knowledge while learning new tasks. Aiming at devising strategies to counteract this effect, incremental learning approaches have gained popularity over the past years. However, the first incremental learning methods for semantic segmentation appeared only recently. While effective, these approaches do not account for a crucial aspect in pixel-level dense prediction problems, i.e. the role of attention mechanisms. To fill this gap, in this paper we introduce a novel attentive feature distillation approach to mitigate catastrophic forgetting while accounting for semantic spatial- and channel-level dependencies. Furthermore, we propose a {continual attentive fusion} structure, which takes advantage of the attention learned from the new and the old tasks while learning features for the new task. Finally, we also introduce a novel strategy to account for the background class in the distillation loss, thus preventing biased predictions. We demonstrate the effectiveness of our approach with an extensive evaluation on Pascal-VOC 2012 and ADE20K, setting a new state of the art.



### Multi-Order Networks for Action Unit Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.00446v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00446v2)
- **Published**: 2022-02-01 14:58:21+00:00
- **Updated**: 2023-03-06 09:06:49+00:00
- **Authors**: Gauthier Tallec, Arnaud Dapogny, Kevin Bailly
- **Comment**: None
- **Journal**: None
- **Summary**: Action Units (AU) are muscular activations used to describe facial expressions. Therefore accurate AU recognition unlocks unbiaised face representation which can improve face-based affective computing applications. From a learning standpoint AU detection is a multi-task problem with strong inter-task dependencies. To solve such problem, most approaches either rely on weight sharing, or add explicit dependency modelling by decomposing the joint task distribution using Bayes chain rule. If the latter strategy yields comprehensive inter-task relationships modelling, it requires imposing an arbitrary order into an unordered task set. Crucially, this ordering choice has been identified as a source of performance variations. In this paper, we present Multi-Order Network (MONET), a multi-task method with joint task order optimization. MONET uses a differentiable order selection to jointly learn task-wise modules with their optimal chaining order. Furthermore, we introduce warmup and order dropout to enhance order selection by encouraging order exploration. Experimentally, we first demonstrate MONET capacity to retrieve the optimal order in a toy environment. Second, we validate MONET architecture by showing that MONET outperforms existing multi-task baselines on multiple attribute detection problems chosen for their wide range of dependency settings. More importantly, we demonstrate that MONET significantly extends state-of-the-art performance in AU detection.



### Sim2Real Object-Centric Keypoint Detection and Description
- **Arxiv ID**: http://arxiv.org/abs/2202.00448v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.00448v2)
- **Published**: 2022-02-01 15:00:20+00:00
- **Updated**: 2022-02-03 10:37:09+00:00
- **Authors**: Chengliang Zhong, Chao Yang, Jinshan Qi, Fuchun Sun, Huaping Liu, Xiaodong Mu, Wenbing Huang
- **Comment**: accepted to AAAI2022
- **Journal**: None
- **Summary**: Keypoint detection and description play a central role in computer vision. Most existing methods are in the form of scene-level prediction, without returning the object classes of different keypoints. In this paper, we propose the object-centric formulation, which, beyond the conventional setting, requires further identifying which object each interest point belongs to. With such fine-grained information, our framework enables more downstream potentials, such as object-level matching and pose estimation in a clustered environment. To get around the difficulty of label collection in the real world, we develop a sim2real contrastive learning mechanism that can generalize the model trained in simulation to real-world applications. The novelties of our training method are three-fold: (i) we integrate the uncertainty into the learning framework to improve feature description of hard cases, e.g., less-textured or symmetric patches; (ii) we decouple the object descriptor into two output branches -- intra-object salience and inter-object distinctness, resulting in a better pixel-wise description; (iii) we enforce cross-view semantic consistency for enhanced robustness in representation learning. Comprehensive experiments on image matching and 6D pose estimation verify the encouraging generalization ability of our method from simulation to reality. Particularly for 6D pose estimation, our method significantly outperforms typical unsupervised/sim2real methods, achieving a closer gap with the fully supervised counterpart. Additional results and videos can be found at https://zhongcl-thu.github.io/rock/



### A Consistent and Efficient Evaluation Strategy for Attribution Methods
- **Arxiv ID**: http://arxiv.org/abs/2202.00449v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.00449v2)
- **Published**: 2022-02-01 15:00:26+00:00
- **Updated**: 2022-06-14 11:36:51+00:00
- **Authors**: Yao Rong, Tobias Leemann, Vadim Borisov, Gjergji Kasneci, Enkelejda Kasneci
- **Comment**: 26 pages. Accepted at ICML 2022
- **Journal**: None
- **Summary**: With a variety of local feature attribution methods being proposed in recent years, follow-up work suggested several evaluation strategies. To assess the attribution quality across different attribution techniques, the most popular among these evaluation strategies in the image domain use pixel perturbations. However, recent advances discovered that different evaluation strategies produce conflicting rankings of attribution methods and can be prohibitively expensive to compute. In this work, we present an information-theoretic analysis of evaluation strategies based on pixel perturbations. Our findings reveal that the results are strongly affected by information leakage through the shape of the removed pixels as opposed to their actual values. Using our theoretical insights, we propose a novel evaluation framework termed Remove and Debias (ROAD) which offers two contributions: First, it mitigates the impact of the confounders, which entails higher consistency among evaluation strategies. Second, ROAD does not require the computationally expensive retraining step and saves up to 99% in computational costs compared to the state-of-the-art. We release our source code at https://github.com/tleemann/road_evaluation.



### HCSC: Hierarchical Contrastive Selective Coding
- **Arxiv ID**: http://arxiv.org/abs/2202.00455v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00455v4)
- **Published**: 2022-02-01 15:04:40+00:00
- **Updated**: 2022-05-23 12:28:29+00:00
- **Authors**: Yuanfan Guo, Minghao Xu, Jiawen Li, Bingbing Ni, Xuanyu Zhu, Zhenbang Sun, Yi Xu
- **Comment**: Accepted by CVPR 2022. arXiv v3: 800 epoch multi-crop model released;
  arXiv v2: more model weights released; arXiv v1: code & model weights
  released
- **Journal**: None
- **Summary**: Hierarchical semantic structures naturally exist in an image dataset, in which several semantically relevant image clusters can be further integrated into a larger cluster with coarser-grained semantics. Capturing such structures with image representations can greatly benefit the semantic understanding on various downstream tasks. Existing contrastive representation learning methods lack such an important model capability. In addition, the negative pairs used in these methods are not guaranteed to be semantically distinct, which could further hamper the structural correctness of learned image representations. To tackle these limitations, we propose a novel contrastive learning framework called Hierarchical Contrastive Selective Coding (HCSC). In this framework, a set of hierarchical prototypes are constructed and also dynamically updated to represent the hierarchical semantic structures underlying the data in the latent space. To make image representations better fit such semantic structures, we employ and further improve conventional instance-wise and prototypical contrastive learning via an elaborate pair selection scheme. This scheme seeks to select more diverse positive pairs with similar semantics and more precise negative pairs with truly distinct semantics. On extensive downstream tasks, we verify the superior performance of HCSC over state-of-the-art contrastive methods, and the effectiveness of major model components is proved by plentiful analytical studies. We build a comprehensive model zoo in Sec. D. Our source code and model weights are available at https://github.com/gyfastas/HCSC



### A training-free recursive multiresolution framework for diffeomorphic deformable image registration
- **Arxiv ID**: http://arxiv.org/abs/2202.00675v1
- **DOI**: 10.1007/s10489-021-03062-2
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68U10, 68T07, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2202.00675v1)
- **Published**: 2022-02-01 15:17:17+00:00
- **Updated**: 2022-02-01 15:17:17+00:00
- **Authors**: Ameneh Sheikhjafari, Michelle Noga, Kumaradevan Punithakumar, Nilanjan Ray
- **Comment**: 15 pages, 5 figures, 3 tables, 1 algorithm, The International Journal
  of Research on Intelligent Systems for Real Life Complex Problems
- **Journal**: None
- **Summary**: Diffeomorphic deformable image registration is one of the crucial tasks in medical image analysis, which aims to find a unique transformation while preserving the topology and invertibility of the transformation. Deep convolutional neural networks (CNNs) have yielded well-suited approaches for image registration by learning the transformation priors from a large dataset. The improvement in the performance of these methods is related to their ability to learn information from several sample medical images that are difficult to obtain and bias the framework to the specific domain of data. In this paper, we propose a novel diffeomorphic training-free approach; this is built upon the principle of an ordinary differential equation.   Our formulation yields an Euler integration type recursive scheme to estimate the changes of spatial transformations between the fixed and the moving image pyramids at different resolutions. The proposed architecture is simple in design. The moving image is warped successively at each resolution and finally aligned to the fixed image; this procedure is recursive in a way that at each resolution, a fully convolutional network (FCN) models a progressive change of deformation for the current warped image. The entire system is end-to-end and optimized for each pair of images from scratch. In comparison to learning-based methods, the proposed method neither requires a dedicated training set nor suffers from any training bias. We evaluate our method on three cardiac image datasets. The evaluation results demonstrate that the proposed method achieves state-of-the-art registration accuracy while maintaining desirable diffeomorphic properties.



### A generalizable approach based on U-Net model for automatic Intra retinal cyst segmentation in SD-OCT images
- **Arxiv ID**: http://arxiv.org/abs/2202.00465v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00465v1)
- **Published**: 2022-02-01 15:23:00+00:00
- **Updated**: 2022-02-01 15:23:00+00:00
- **Authors**: Razieh Ganjee, Mohsen Ebrahimi Moghaddam, Ramin Nourinia
- **Comment**: 22pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: Intra retinal fluids or Cysts are one of the important symptoms of macular pathologies that are efficiently visualized in OCT images. Automatic segmentation of these abnormalities has been widely investigated in medical image processing studies. In this paper, we propose a new U-Net-based approach for Intra retinal cyst segmentation across different vendors that improves some of the challenges faced by previous deep-based techniques. The proposed method has two main steps: 1- prior information embedding and input data adjustment, and 2- IRC segmentation model. In the first step, we inject the information into the network in a way that overcomes some of the network limitations in receiving data and learning important contextual knowledge. And in the next step, we introduced a connection module between encoder and decoder parts of the standard U-Net architecture that transfers information more effectively from the encoder to the decoder part. Two public datasets namely OPTIMA and KERMANY were employed to evaluate the proposed method. Results showed that the proposed method is an efficient vendor-independent approach for IRC segmentation with mean Dice values of 0.78 and 0.81 on the OPTIMA and KERMANY datasets, respectively.



### A deep residual learning implementation of Metamorphosis
- **Arxiv ID**: http://arxiv.org/abs/2202.00676v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.00676v1)
- **Published**: 2022-02-01 15:39:34+00:00
- **Updated**: 2022-02-01 15:39:34+00:00
- **Authors**: Matthis Maillard, Anton François, Joan Glaunès, Isabelle Bloch, Pietro Gori
- **Comment**: ISBI 2022
- **Journal**: None
- **Summary**: In medical imaging, most of the image registration methods implicitly assume a one-to-one correspondence between the source and target images (i.e., diffeomorphism). However, this is not necessarily the case when dealing with pathological medical images (e.g., presence of a tumor, lesion, etc.). To cope with this issue, the Metamorphosis model has been proposed. It modifies both the shape and the appearance of an image to deal with the geometrical and topological differences. However, the high computational time and load have hampered its applications so far. Here, we propose a deep residual learning implementation of Metamorphosis that drastically reduces the computational time at inference. Furthermore, we also show that the proposed framework can easily integrate prior knowledge of the localization of topological changes (e.g., segmentation masks) that can act as spatial regularization to correctly disentangle appearance and shape changes. We test our method on the BraTS 2021 dataset, showing that it outperforms current state-of-the-art methods in the alignment of images with brain tumors.



### An Embarrassingly Simple Consistency Regularization Method for Semi-Supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.00677v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00677v2)
- **Published**: 2022-02-01 16:21:14+00:00
- **Updated**: 2022-02-03 08:27:08+00:00
- **Authors**: Hritam Basak, Rajarshi Bhattacharya, Rukhshanda Hussain, Agniv Chatterjee
- **Comment**: Accepted at ISBI 2022
- **Journal**: None
- **Summary**: The scarcity of pixel-level annotation is a prevalent problem in medical image segmentation tasks. In this paper, we introduce a novel regularization strategy involving interpolation-based mixing for semi-supervised medical image segmentation. The proposed method is a new consistency regularization strategy that encourages segmentation of interpolation of two unlabelled data to be consistent with the interpolation of segmentation maps of those data. This method represents a specific type of data-adaptive regularization paradigm which aids to minimize the overfitting of labelled data under high confidence values. The proposed method is advantageous over adversarial and generative models as it requires no additional computation. Upon evaluation on two publicly available MRI datasets: ACDC and MMWHS, experimental results demonstrate the superiority of the proposed method in comparison to existing semi-supervised models. Code is available at: https://github.com/hritam-98/ICT-MedSeg



### The impact of removing head movements on audio-visual speech enhancement
- **Arxiv ID**: http://arxiv.org/abs/2202.00538v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2202.00538v2)
- **Published**: 2022-02-01 16:27:44+00:00
- **Updated**: 2022-02-02 11:05:39+00:00
- **Authors**: Zhiqi Kang, Mostafa Sadeghi, Radu Horaud, Xavier Alameda-Pineda, Jacob Donley, Anurag Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: This paper investigates the impact of head movements on audio-visual speech enhancement (AVSE). Although being a common conversational feature, head movements have been ignored by past and recent studies: they challenge today's learning-based methods as they often degrade the performance of models that are trained on clean, frontal, and steady face images. To alleviate this problem, we propose to use robust face frontalization (RFF) in combination with an AVSE method based on a variational auto-encoder (VAE) model. We briefly describe the basic ingredients of the proposed pipeline and we perform experiments with a recently released audio-visual dataset. In the light of these experiments, and based on three standard metrics, namely STOI, PESQ and SI-SDR, we conclude that RFF improves the performance of AVSE by a considerable margin.



### Classification of Skin Cancer Images using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.00678v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2202.00678v1)
- **Published**: 2022-02-01 17:11:41+00:00
- **Updated**: 2022-02-01 17:11:41+00:00
- **Authors**: Kartikeya Agarwal, Tismeet Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Skin cancer is the most common human malignancy(American Cancer Society) which is primarily diagnosed visually, starting with an initial clinical screening and followed potentially by dermoscopic(related to skin) analysis, a biopsy and histopathological examination. Skin cancer occurs when errors (mutations) occur in the DNA of skin cells. The mutations cause the cells to grow out of control and form a mass of cancer cells. The aim of this study was to try to classify images of skin lesions with the help of convolutional neural networks. The deep neural networks show humongous potential for image classification while taking into account the large variability exhibited by the environment. Here we trained images based on the pixel values and classified them on the basis of disease labels. The dataset was acquired from an Open Source Kaggle Repository(Kaggle Dataset)which itself was acquired from ISIC(International Skin Imaging Collaboration) Archive. The training was performed on multiple models accompanied with Transfer Learning. The highest model accuracy achieved was over 86.65%. The dataset used is publicly available to ensure credibility and reproducibility of the aforementioned result.



### Fishing for User Data in Large-Batch Federated Learning via Gradient Magnification
- **Arxiv ID**: http://arxiv.org/abs/2202.00580v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00580v2)
- **Published**: 2022-02-01 17:26:11+00:00
- **Updated**: 2022-06-19 23:21:42+00:00
- **Authors**: Yuxin Wen, Jonas Geiping, Liam Fowl, Micah Goldblum, Tom Goldstein
- **Comment**: First three authors contributed equally, order chosen randomly. 21
  pages, 9 figures. Published at ICML 2022
- **Journal**: None
- **Summary**: Federated learning (FL) has rapidly risen in popularity due to its promise of privacy and efficiency. Previous works have exposed privacy vulnerabilities in the FL pipeline by recovering user data from gradient updates. However, existing attacks fail to address realistic settings because they either 1) require toy settings with very small batch sizes, or 2) require unrealistic and conspicuous architecture modifications. We introduce a new strategy that dramatically elevates existing attacks to operate on batches of arbitrarily large size, and without architectural modifications. Our model-agnostic strategy only requires modifications to the model parameters sent to the user, which is a realistic threat model in many scenarios. We demonstrate the strategy in challenging large-scale settings, obtaining high-fidelity data extraction in both cross-device and cross-silo federated learning.



### Datamodels: Predicting Predictions from Training Data
- **Arxiv ID**: http://arxiv.org/abs/2202.00622v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.00622v1)
- **Published**: 2022-02-01 18:15:24+00:00
- **Updated**: 2022-02-01 18:15:24+00:00
- **Authors**: Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, Aleksander Madry
- **Comment**: None
- **Journal**: None
- **Summary**: We present a conceptual framework, datamodeling, for analyzing the behavior of a model class in terms of the training data. For any fixed "target" example $x$, training set $S$, and learning algorithm, a datamodel is a parameterized function $2^S \to \mathbb{R}$ that for any subset of $S' \subset S$ -- using only information about which examples of $S$ are contained in $S'$ -- predicts the outcome of training a model on $S'$ and evaluating on $x$. Despite the potential complexity of the underlying process being approximated (e.g., end-to-end training and evaluation of deep neural networks), we show that even simple linear datamodels can successfully predict model outputs. We then demonstrate that datamodels give rise to a variety of applications, such as: accurately predicting the effect of dataset counterfactuals; identifying brittle predictions; finding semantically similar examples; quantifying train-test leakage; and embedding data into a well-behaved and feature-rich representation space. Data for this paper (including pre-computed datamodels as well as raw predictions from four million trained deep neural networks) is available at https://github.com/MadryLab/datamodels-data .



### Stay Positive: Non-Negative Image Synthesis for Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/2202.00659v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2202.00659v1)
- **Published**: 2022-02-01 18:55:11+00:00
- **Updated**: 2022-02-01 18:55:11+00:00
- **Authors**: Katie Luo, Guandao Yang, Wenqi Xian, Harald Haraldsson, Bharath Hariharan, Serge Belongie
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), 2021, pp. 10050-10060
- **Summary**: In applications such as optical see-through and projector augmented reality, producing images amounts to solving non-negative image generation, where one can only add light to an existing image. Most image generation methods, however, are ill-suited to this problem setting, as they make the assumption that one can assign arbitrary color to each pixel. In fact, naive application of existing methods fails even in simple domains such as MNIST digits, since one cannot create darker pixels by adding light. We know, however, that the human visual system can be fooled by optical illusions involving certain spatial configurations of brightness and contrast. Our key insight is that one can leverage this behavior to produce high quality images with negligible artifacts. For example, we can create the illusion of darker patches by brightening surrounding pixels. We propose a novel optimization procedure to produce images that satisfy both semantic and non-negativity constraints. Our approach can incorporate existing state-of-the-art methods, and exhibits strong performance in a variety of tasks including image-to-image translation and style transfer.



### Interactron: Embodied Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.00660v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.00660v3)
- **Published**: 2022-02-01 18:56:14+00:00
- **Updated**: 2022-07-22 19:37:10+00:00
- **Authors**: Klemen Kotar, Roozbeh Mottaghi
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: Over the years various methods have been proposed for the problem of object detection. Recently, we have witnessed great strides in this domain owing to the emergence of powerful deep neural networks. However, there are typically two main assumptions common among these approaches. First, the model is trained on a fixed training set and is evaluated on a pre-recorded test set. Second, the model is kept frozen after the training phase, so no further updates are performed after the training is finished. These two assumptions limit the applicability of these methods to real-world settings. In this paper, we propose Interactron, a method for adaptive object detection in an interactive setting, where the goal is to perform object detection in images observed by an embodied agent navigating in different environments. Our idea is to continue training during inference and adapt the model at test time without any explicit supervision via interacting with the environment. Our adaptive object detection model provides a 7.2 point improvement in AP (and 12.7 points in AP50) over DETR, a recent, high-performance object detector. Moreover, we show that our object detection model adapts to environments with completely different appearance characteristics, and performs well in them. The code is available at: https://github.com/allenai/interactron .



### DKM: Dense Kernelized Feature Matching for Geometry Estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.00667v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.00667v3)
- **Published**: 2022-02-01 18:58:46+00:00
- **Updated**: 2022-11-25 18:59:00+00:00
- **Authors**: Johan Edstedt, Ioannis Athanasiadis, Mårten Wadenbäck, Michael Felsberg
- **Comment**: None
- **Journal**: None
- **Summary**: Feature matching is a challenging computer vision task that involves finding correspondences between two images of a 3D scene. In this paper we consider the dense approach instead of the more common sparse paradigm, thus striving to find all correspondences. Perhaps counter-intuitively, dense methods have previously shown inferior performance to their sparse and semi-sparse counterparts for estimation of two-view geometry. This changes with our novel dense method, which outperforms both dense and sparse methods on geometry estimation. The novelty is threefold: First, we propose a kernel regression global matcher. Secondly, we propose warp refinement through stacked feature maps and depthwise convolution kernels. Thirdly, we propose learning dense confidence through consistent depth and a balanced sampling approach for dense confidence maps. Through extensive experiments we confirm that our proposed dense method, \textbf{D}ense \textbf{K}ernelized Feature \textbf{M}atching, sets a new state-of-the-art on multiple geometry estimation benchmarks. In particular, we achieve an improvement on MegaDepth-1500 of +4.9 and +8.9 AUC$@5^{\circ}$ compared to the best previous sparse method and dense method respectively. Our code is provided at https://github.com/Parskatt/dkm



### Should I take a walk? Estimating Energy Expenditure from Video Data
- **Arxiv ID**: http://arxiv.org/abs/2202.00712v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00712v2)
- **Published**: 2022-02-01 19:04:42+00:00
- **Updated**: 2022-04-08 11:04:44+00:00
- **Authors**: Kunyu Peng, Alina Roitberg, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen
- **Comment**: Accepted to CVPR 2022 CVPM Workshop. Dataset and code are available
  at https://github.com/KPeng9510/Vid2Burn
- **Journal**: None
- **Summary**: We explore the problem of automatically inferring the amount of kilocalories used by human during physical activity from his/her video observation. To study this underresearched task, we introduce Vid2Burn -- an omni-source benchmark for estimating caloric expenditure from video data featuring both, high- and low-intensity activities for which we derive energy expenditure annotations based on models established in medical literature. In practice, a training set would only cover a certain amount of activity types, and it is important to validate, if the model indeed captures the essence of energy expenditure, (e.g., how many and which muscles are involved and how intense they work) instead of memorizing fixed values of specific activity categories seen during training. Ideally, the models should look beyond such category-specific biases and regress the caloric cost in videos depicting activity categories not explicitly present during training. With this property in mind, Vid2Burn is accompanied with a cross-category benchmark, where the task is to regress caloric expenditure for types of physical activities not present during training. An extensive evaluation of state-of-the-art approaches for video recognition modified for the energy expenditure estimation task demonstrates the difficulty of this problem, especially for new activity types at test-time, marking a new research direction. Dataset and code are available at https://github.com/KPeng9510/Vid2Burn.



### IFOR: Iterative Flow Minimization for Robotic Object Rearrangement
- **Arxiv ID**: http://arxiv.org/abs/2202.00732v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00732v1)
- **Published**: 2022-02-01 20:03:56+00:00
- **Updated**: 2022-02-01 20:03:56+00:00
- **Authors**: Ankit Goyal, Arsalan Mousavian, Chris Paxton, Yu-Wei Chao, Brian Okorn, Jia Deng, Dieter Fox
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate object rearrangement from vision is a crucial problem for a wide variety of real-world robotics applications in unstructured environments. We propose IFOR, Iterative Flow Minimization for Robotic Object Rearrangement, an end-to-end method for the challenging problem of object rearrangement for unknown objects given an RGBD image of the original and final scenes. First, we learn an optical flow model based on RAFT to estimate the relative transformation of the objects purely from synthetic data. This flow is then used in an iterative minimization algorithm to achieve accurate positioning of previously unseen objects. Crucially, we show that our method applies to cluttered scenes, and in the real world, while training only on synthetic data. Videos are available at https://imankgoyal.github.io/ifor.html.



### Towards Positive Jacobian: Learn to Postprocess Diffeomorphic Image Registration with Matrix Exponential
- **Arxiv ID**: http://arxiv.org/abs/2202.00749v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00749v1)
- **Published**: 2022-02-01 20:47:28+00:00
- **Updated**: 2022-02-01 20:47:28+00:00
- **Authors**: Soumyadeep Pal, Matthew Tennant, Nilanjan Ray
- **Comment**: None
- **Journal**: None
- **Summary**: We present a postprocessing layer for deformable image registration to make a registration field more diffeomorphic by encouraging Jacobians of the transformation to be positive. Diffeomorphic image registration is important for medical imaging studies because of the properties like invertibility, smoothness of the transformation, and topology preservation/non-folding of the grid. Violation of these properties can lead to destruction of the neighbourhood and the connectivity of anatomical structures during image registration. Most of the recent deep learning methods do not explicitly address this folding problem and try to solve it with a smoothness regularization on the registration field. In this paper, we propose a differentiable layer, which takes any registration field as its input, computes exponential of the Jacobian matrices of the input and reconstructs a new registration field from the exponentiated Jacobian matrices using Poisson reconstruction. Our proposed Poisson reconstruction loss enforces positive Jacobians for the final registration field. Thus, our method acts as a post-processing layer without any learnable parameters of its own and can be placed at the end of any deep learning pipeline to form an end-to-end learnable framework. We show the effectiveness of our proposed method for a popular deep learning registration method Voxelmorph and evaluate it with a dataset containing 3D brain MRI scans. Our results show that our post-processing can effectively decrease the number of non-positive Jacobians by a significant amount without any noticeable deterioration of the registration accuracy, thus making the registration field more diffeomorphic. Our code is available online at https://github.com/Soumyadeep-Pal/Diffeomorphic-Image-Registration-Postprocess.



### ADG-Pose: Automated Dataset Generation for Real-World Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.00753v2
- **DOI**: 10.1007/978-3-031-09282-4_22
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00753v2)
- **Published**: 2022-02-01 20:51:58+00:00
- **Updated**: 2022-06-08 20:17:26+00:00
- **Authors**: Ghazal Alinezhad Noghre, Armin Danesh Pazho, Justin Sanchez, Nathan Hewitt, Christopher Neff, Hamed Tabkhi
- **Comment**: The first two authors (G. Alinezhad Noghre and A. Danesh Pazho) have
  equal contribution. Conference: International Conference on Pattern
  Recognition and Artificial Intelligence
- **Journal**: In International Conference on Pattern Recognition and Artificial
  Intelligence (pp. 258-270). Springer, Cham (2022)
- **Summary**: Recent advancements in computer vision have seen a rise in the prominence of applications using neural networks to understand human poses. However, while accuracy has been steadily increasing on State-of-the-Art datasets, these datasets often do not address the challenges seen in real-world applications. These challenges are dealing with people distant from the camera, people in crowds, and heavily occluded people. As a result, many real-world applications have trained on data that does not reflect the data present in deployment, leading to significant underperformance. This article presents ADG-Pose, a method for automatically generating datasets for real-world human pose estimation. These datasets can be customized to determine person distances, crowdedness, and occlusion distributions. Models trained with our method are able to perform in the presence of these challenges where those trained on other datasets fail. Using ADG-Pose, end-to-end accuracy for real-world skeleton-based action recognition sees a 20% increase on scenes with moderate distance and occlusion levels, and a 4X increase on distant scenes where other models failed to perform better than random.



### A Model for Multi-View Residual Covariances based on Perspective Deformation
- **Arxiv ID**: http://arxiv.org/abs/2202.00765v1
- **DOI**: 10.1109/LRA.2022.3142905
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00765v1)
- **Published**: 2022-02-01 21:21:56+00:00
- **Updated**: 2022-02-01 21:21:56+00:00
- **Authors**: Alejandro Fontan, Laura Oliva, Javier Civera, Rudolph Triebel
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we derive a model for the covariance of the visual residuals in multi-view SfM, odometry and SLAM setups. The core of our approach is the formulation of the residual covariances as a combination of geometric and photometric noise sources. And our key novel contribution is the derivation of a term modelling how local 2D patches suffer from perspective deformation when imaging 3D surfaces around a point. Together, these add up to an efficient and general formulation which not only improves the accuracy of both feature-based and direct methods, but can also be used to estimate more accurate measures of the state entropy and hence better founded point visibility thresholds. We validate our model with synthetic and real data and integrate it into photometric and feature-based Bundle Adjustment, improving their accuracy with a negligible overhead.



### Local Feature Matching with Transformers for low-end devices
- **Arxiv ID**: http://arxiv.org/abs/2202.00770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00770v1)
- **Published**: 2022-02-01 21:30:43+00:00
- **Updated**: 2022-02-01 21:30:43+00:00
- **Authors**: Kyrylo Kolodiazhnyi
- **Comment**: Project GitHub page https://github.com/Kolkir/Coarse_LoFTR_TRT
- **Journal**: None
- **Summary**: LoFTR arXiv:2104.00680 is an efficient deep learning method for finding appropriate local feature matches on image pairs. This paper reports on the optimization of this method to work on devices with low computational performance and limited memory. The original LoFTR approach is based on a ResNet arXiv:1512.03385 head and two modules based on Linear Transformer arXiv:2006.04768 architecture. In the presented work, only the coarse-matching block was left, the number of parameters was significantly reduced, and the network was trained using a knowledge distillation technique. The comparison showed that this approach allows to obtain an appropriate feature detection accuracy for the student model compared to the teacher model in the coarse matching block, despite the significant reduction of model size. Also, the paper shows additional steps required to make model compatible with NVIDIA TensorRT runtime, and shows an approach to optimize training method for low-end GPUs.



### Accelerating DNN Training with Structured Data Gradient Pruning
- **Arxiv ID**: http://arxiv.org/abs/2202.00774v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00774v1)
- **Published**: 2022-02-01 21:41:51+00:00
- **Updated**: 2022-02-01 21:41:51+00:00
- **Authors**: Bradley McDanel, Helia Dinh, John Magallanes
- **Comment**: None
- **Journal**: None
- **Summary**: Weight pruning is a technique to make Deep Neural Network (DNN) inference more computationally efficient by reducing the number of model parameters over the course of training. However, most weight pruning techniques generally does not speed up DNN training and can even require more iterations to reach model convergence. In this work, we propose a novel Structured Data Gradient Pruning (SDGP) method that can speed up training without impacting model convergence. This approach enforces a specific sparsity structure, where only N out of every M elements in a matrix can be nonzero, making it amenable to hardware acceleration. Modern accelerators such as the Nvidia A100 GPU support this type of structured sparsity for 2 nonzeros per 4 elements in a reduction. Assuming hardware support for 2:4 sparsity, our approach can achieve a 15-25\% reduction in total training time without significant impact to performance. Source code and pre-trained models are available at \url{https://github.com/BradMcDanel/sdgp}.



### On Regularizing Coordinate-MLPs
- **Arxiv ID**: http://arxiv.org/abs/2202.00790v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00790v1)
- **Published**: 2022-02-01 22:21:13+00:00
- **Updated**: 2022-02-01 22:21:13+00:00
- **Authors**: Sameera Ramasinghe, Lachlan MacDonald, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: We show that typical implicit regularization assumptions for deep neural networks (for regression) do not hold for coordinate-MLPs, a family of MLPs that are now ubiquitous in computer vision for representing high-frequency signals. Lack of such implicit bias disrupts smooth interpolations between training samples, and hampers generalizing across signal regions with different spectra. We investigate this behavior through a Fourier lens and uncover that as the bandwidth of a coordinate-MLP is enhanced, lower frequencies tend to get suppressed unless a suitable prior is provided explicitly. Based on these insights, we propose a simple regularization technique that can mitigate the above problem, which can be incorporated into existing networks without any architectural modifications.



### Mars Terrain Segmentation with Less Labels
- **Arxiv ID**: http://arxiv.org/abs/2202.00791v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.00791v1)
- **Published**: 2022-02-01 22:25:15+00:00
- **Updated**: 2022-02-01 22:25:15+00:00
- **Authors**: Edwin Goh, Jingdao Chen, Brian Wilson
- **Comment**: IEEE Aerospace Conference 2022
- **Journal**: None
- **Summary**: Planetary rover systems need to perform terrain segmentation to identify drivable areas as well as identify specific types of soil for sample collection. The latest Martian terrain segmentation methods rely on supervised learning which is very data hungry and difficult to train where only a small number of labeled samples are available. Moreover, the semantic classes are defined differently for different applications (e.g., rover traversal vs. geological) and as a result the network has to be trained from scratch each time, which is an inefficient use of resources. This research proposes a semi-supervised learning framework for Mars terrain segmentation where a deep segmentation network trained in an unsupervised manner on unlabeled images is transferred to the task of terrain segmentation trained on few labeled images. The network incorporates a backbone module which is trained using a contrastive loss function and an output atrous convolution module which is trained using a pixel-wise cross-entropy loss function. Evaluation results using the metric of segmentation accuracy show that the proposed method with contrastive pretraining outperforms plain supervised learning by 2%-10%. Moreover, the proposed model is able to achieve a segmentation accuracy of 91.1% using only 161 training images (1% of the original dataset) compared to 81.9% with plain supervised learning.



### A Graph Based Neural Network Approach to Immune Profiling of Multiplexed Tissue Samples
- **Arxiv ID**: http://arxiv.org/abs/2202.00813v1
- **DOI**: 10.1109/EMBC48229.2022.9871251
- **Categories**: **cs.LG**, cs.CV, eess.IV, q-bio.CB, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2202.00813v1)
- **Published**: 2022-02-01 23:48:40+00:00
- **Updated**: 2022-02-01 23:48:40+00:00
- **Authors**: Natalia Garcia Martin, Stefano Malacrino, Marta Wojciechowska, Leticia Campo, Helen Jones, David C. Wedge, Chris Holmes, Korsuk Sirinukunwattana, Heba Sailem, Clare Verrill, Jens Rittscher
- **Comment**: None
- **Journal**: 2022 44th Annual International Conference of the IEEE Engineering
  in Medicine & Biology Society (EMBC), 2022, pp. 3063-3067
- **Summary**: Multiplexed immunofluorescence provides an unprecedented opportunity for studying specific cell-to-cell and cell microenvironment interactions. We employ graph neural networks to combine features obtained from tissue morphology with measurements of protein expression to profile the tumour microenvironment associated with different tumour stages. Our framework presents a new approach to analysing and processing these complex multi-dimensional datasets that overcomes some of the key challenges in analysing these data and opens up the opportunity to abstract biologically meaningful interactions.



