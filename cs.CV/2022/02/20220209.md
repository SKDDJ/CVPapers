# Arxiv Papers in cs.CV on 2022-02-09
### Mathematical Cookbook for Snapshot Compressive Imaging
- **Arxiv ID**: http://arxiv.org/abs/2202.07437v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07437v3)
- **Published**: 2022-02-09 01:24:36+00:00
- **Updated**: 2023-03-19 13:11:59+00:00
- **Authors**: Yaping Zhao
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: The author intends to provide you with a beautiful, elegant, user-friendly cookbook for mathematics in Snapshot Compressive Imaging (SCI). Currently, the cookbook is composed of introduction, conventional optimization, and deep equilibrium models. The latest releases are strongly recommended! For any other questions, suggestions, or comments, feel free to email the author.



### Real-Time Event-Based Tracking and Detection for Maritime Environments
- **Arxiv ID**: http://arxiv.org/abs/2202.04231v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.04231v1)
- **Published**: 2022-02-09 02:30:27+00:00
- **Updated**: 2022-02-09 02:30:27+00:00
- **Authors**: Stephanie Aelmore, Richard C. Ordonez, Shibin Parameswaran, Justin Mauger
- **Comment**: 6 pages, 7 figures. Accepted by IEEE AIPR 2021 (Oral)
- **Journal**: None
- **Summary**: Event cameras are ideal for object tracking applications due to their ability to capture fast-moving objects while mitigating latency and data redundancy. Existing event-based clustering and feature tracking approaches for surveillance and object detection work well in the majority of cases, but fall short in a maritime environment. Our application of maritime vessel detection and tracking requires a process that can identify features and output a confidence score representing the likelihood that the feature was produced by a vessel, which may trigger a subsequent alert or activate a classification system. However, the maritime environment presents unique challenges such as the tendency of waves to produce the majority of events, demanding the majority of computational processing and producing false positive detections. By filtering redundant events and analyzing the movement of each event cluster, we can identify and track vessels while ignoring shorter lived and erratic features such as those produced by waves.



### Towards Compositional Adversarial Robustness: Generalizing Adversarial Training to Composite Semantic Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2202.04235v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04235v3)
- **Published**: 2022-02-09 02:41:56+00:00
- **Updated**: 2023-03-21 19:38:39+00:00
- **Authors**: Lei Hsiung, Yun-Yun Tsai, Pin-Yu Chen, Tsung-Yi Ho
- **Comment**: CVPR 2023. The research demo is at https://hsiung.cc/CARBEN/
- **Journal**: None
- **Summary**: Model robustness against adversarial examples of single perturbation type such as the $\ell_{p}$-norm has been widely studied, yet its generalization to more realistic scenarios involving multiple semantic perturbations and their composition remains largely unexplored. In this paper, we first propose a novel method for generating composite adversarial examples. Our method can find the optimal attack composition by utilizing component-wise projected gradient descent and automatic attack-order scheduling. We then propose generalized adversarial training (GAT) to extend model robustness from $\ell_{p}$-ball to composite semantic perturbations, such as the combination of Hue, Saturation, Brightness, Contrast, and Rotation. Results obtained using ImageNet and CIFAR-10 datasets indicate that GAT can be robust not only to all the tested types of a single attack, but also to any combination of such attacks. GAT also outperforms baseline $\ell_{\infty}$-norm bounded adversarial training approaches by a significant margin.



### Learning Robust Convolutional Neural Networks with Relevant Feature Focusing via Explanations
- **Arxiv ID**: http://arxiv.org/abs/2202.04237v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.04237v2)
- **Published**: 2022-02-09 02:46:26+00:00
- **Updated**: 2022-03-23 07:24:13+00:00
- **Authors**: Kazuki Adachi, Shin'ya Yamaguchi
- **Comment**: Accepted by ICME 2022
- **Journal**: None
- **Summary**: Existing image recognition techniques based on convolutional neural networks (CNNs) basically assume that the training and test datasets are sampled from i.i.d distributions. However, this assumption is easily broken in the real world because of the distribution shift that occurs when the co-occurrence relations between objects and backgrounds in input images change. Under this type of distribution shift, CNNs learn to focus on features that are not task-relevant, such as backgrounds from the training data, and degrade their accuracy on the test data. To tackle this problem, we propose relevant feature focusing (ReFF). ReFF detects task-relevant features and regularizes CNNs via explanation outputs (e.g., Grad-CAM). Since ReFF is composed of post-hoc explanation modules, it can be easily applied to off-the-shelf CNNs. Furthermore, ReFF requires no additional inference cost at test time because it is only used for regularization while training. We demonstrate that CNNs trained with ReFF focus on features relevant to the target task and that ReFF improves the test-time accuracy.



### A multiscale spatiotemporal approach for smallholder irrigation detection
- **Arxiv ID**: http://arxiv.org/abs/2202.04239v2
- **DOI**: 10.3389/frsen.2022.871942
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.04239v2)
- **Published**: 2022-02-09 02:50:42+00:00
- **Updated**: 2022-03-18 17:08:20+00:00
- **Authors**: Terence Conlon, Christopher Small, Vijay Modi
- **Comment**: None
- **Journal**: None
- **Summary**: In presenting an irrigation detection methodology that leverages multiscale satellite imagery of vegetation abundance, this paper introduces a process to supplement limited ground-collected labels and ensure classifier applicability in an area of interest. Spatiotemporal analysis of MODIS 250m Enhanced Vegetation Index (EVI) timeseries characterizes native vegetation phenologies at regional scale to provide the basis for a continuous phenology map that guides supplementary label collection over irrigated and non-irrigated agriculture. Subsequently, validated dry season greening and senescence cycles observed in 10m Sentinel-2 imagery are used to train a suite of classifiers for automated detection of potential smallholder irrigation. Strategies to improve model robustness are demonstrated, including a method of data augmentation that randomly shifts training samples; and an assessment of classifier types that produce the best performance in withheld target regions. The methodology is applied to detect smallholder irrigation in two states in the Ethiopian highlands, Tigray and Amhara. Results show that a transformer-based neural network architecture allows for the most robust prediction performance in withheld regions, followed closely by a CatBoost random forest model. Over withheld ground-collection survey labels, the transformer-based model achieves 96.7% accuracy over non-irrigated samples and 95.9% accuracy over irrigated samples. Over a larger set of samples independently collected via the introduced method of label supplementation, non-irrigated and irrigated labels are predicted with 98.3% and 95.5% accuracy, respectively. The detection model is then deployed over Tigray and Amhara, revealing crop rotation patterns and year-over-year irrigated area change. Predictions suggest that irrigated area in these two states has decreased by approximately 40% from 2020 to 2021.



### Distillation with Contrast is All You Need for Self-Supervised Point Cloud Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.04241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04241v1)
- **Published**: 2022-02-09 02:51:59+00:00
- **Updated**: 2022-02-09 02:51:59+00:00
- **Authors**: Kexue Fu, Peng Gao, Renrui Zhang, Hongsheng Li, Yu Qiao, Manning Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a simple and general framework for self-supervised point cloud representation learning. Human beings understand the 3D world by extracting two levels of information and establishing the relationship between them. One is the global shape of an object, and the other is the local structures of it. However, few existing studies in point cloud representation learning explored how to learn both global shapes and local-to-global relationships without a specified network architecture. Inspired by how human beings understand the world, we utilize knowledge distillation to learn both global shape information and the relationship between global shape and local structures. At the same time, we combine contrastive learning with knowledge distillation to make the teacher network be better updated. Our method achieves the state-of-the-art performance on linear classification and multiple other downstream tasks. Especially, we develop a variant of ViT for 3D point cloud feature extraction, which also achieves comparable results with existing backbones when combined with our framework, and visualization of the attention maps show that our model does understand the point cloud by combining the global shape information and multiple local structural information, which is consistent with the inspiration of our representation learning method. Our code will be released soon.



### Motion-Aware Transformer For Occluded Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2202.04243v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04243v2)
- **Published**: 2022-02-09 02:53:10+00:00
- **Updated**: 2022-02-10 11:18:49+00:00
- **Authors**: Mi Zhou, Hongye Liu, Zhekun Lv, Wei Hong, Xiai Chen
- **Comment**: 10 pages, 3 figures
- **Journal**: None
- **Summary**: Recently, occluded person re-identification(Re-ID) remains a challenging task that people are frequently obscured by other people or obstacles, especially in a crowd massing situation. In this paper, we propose a self-supervised deep learning method to improve the location performance for human parts through occluded person Re-ID. Unlike previous works, we find that motion information derived from the photos of various human postures can help identify major human body components. Firstly, a motion-aware transformer encoder-decoder architecture is designed to obtain keypoints heatmaps and part-segmentation maps. Secondly, an affine transformation module is utilized to acquire motion information from the keypoint detection branch. Then the motion information will support the segmentation branch to achieve refined human part segmentation maps, and effectively divide the human body into reasonable groups. Finally, several cases demonstrate the efficiency of the proposed model in distinguishing different representative parts of the human body, which can avoid the background and occlusion disturbs. Our method consistently achieves state-of-the-art results on several popular datasets, including occluded, partial, and holistic.



### GiraffeDet: A Heavy-Neck Paradigm for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.04256v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04256v2)
- **Published**: 2022-02-09 03:23:49+00:00
- **Updated**: 2022-06-22 09:28:14+00:00
- **Authors**: Yiqi Jiang, Zhiyu Tan, Junyan Wang, Xiuyu Sun, Ming Lin, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: In conventional object detection frameworks, a backbone body inherited from image recognition models extracts deep latent features and then a neck module fuses these latent features to capture information at different scales. As the resolution in object detection is much larger than in image recognition, the computational cost of the backbone often dominates the total inference cost. This heavy-backbone design paradigm is mostly due to the historical legacy when transferring image recognition models to object detection rather than an end-to-end optimized design for object detection. In this work, we show that such paradigm indeed leads to sub-optimal object detection models. To this end, we propose a novel heavy-neck paradigm, GiraffeDet, a giraffe-like network for efficient object detection. The GiraffeDet uses an extremely lightweight backbone and a very deep and large neck module which encourages dense information exchange among different spatial scales as well as different levels of latent semantics simultaneously. This design paradigm allows detectors to process the high-level semantic information and low-level spatial information at the same priority even in the early stage of the network, making it more effective in detection tasks. Numerical evaluations on multiple popular object detection benchmarks show that GiraffeDet consistently outperforms previous SOTA models across a wide spectrum of resource constraints. The source code is available at https://github.com/jyqi/GiraffeDet.



### Adversarial Detection without Model Information
- **Arxiv ID**: http://arxiv.org/abs/2202.04271v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04271v2)
- **Published**: 2022-02-09 04:38:16+00:00
- **Updated**: 2022-04-05 05:21:58+00:00
- **Authors**: Abhishek Moitra, Youngeun Kim, Priyadarshini Panda
- **Comment**: This paper has 14 pages of content and 2 pages of references
- **Journal**: None
- **Summary**: Prior state-of-the-art adversarial detection works are classifier model dependent, i.e., they require classifier model outputs and parameters for training the detector or during adversarial detection. This makes their detection approach classifier model specific. Furthermore, classifier model outputs and parameters might not always be accessible. To this end, we propose a classifier model independent adversarial detection method using a simple energy function to distinguish between adversarial and natural inputs. We train a standalone detector independent of the classifier model, with a layer-wise energy separation (LES) training to increase the separation between natural and adversarial energies. With this, we perform energy distribution-based adversarial detection. Our method achieves comparable performance with state-of-the-art detection works (ROC-AUC > 0.9) across a wide range of gradient, score and gaussian noise attacks on CIFAR10, CIFAR100 and TinyImagenet datasets. Furthermore, compared to prior works, our detection approach is light-weight, requires less amount of training data (40% of the actual dataset) and is transferable across different datasets. For reproducibility, we provide layer-wise energy separation training code at https://github.com/Intelligent-Computing-Lab-Yale/Energy-Separation-Training



### Amplitude Spectrum Transformation for Open Compound Domain Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.04287v1
- **DOI**: 10.1609/aaai.v36i2.20008
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.04287v1)
- **Published**: 2022-02-09 05:40:34+00:00
- **Updated**: 2022-02-09 05:40:34+00:00
- **Authors**: Jogendra Nath Kundu, Akshay Kulkarni, Suvaansh Bhambri, Varun Jampani, R. Venkatesh Babu
- **Comment**: AAAI 2022. Project page: http://sites.google.com/view/ast-ocdaseg
- **Journal**: None
- **Summary**: Open compound domain adaptation (OCDA) has emerged as a practical adaptation setting which considers a single labeled source domain against a compound of multi-modal unlabeled target data in order to generalize better on novel unseen domains. We hypothesize that an improved disentanglement of domain-related and task-related factors of dense intermediate layer features can greatly aid OCDA. Prior-arts attempt this indirectly by employing adversarial domain discriminators on the spatial CNN output. However, we find that latent features derived from the Fourier-based amplitude spectrum of deep CNN features hold a more tractable mapping with domain discrimination. Motivated by this, we propose a novel feature space Amplitude Spectrum Transformation (AST). During adaptation, we employ the AST auto-encoder for two purposes. First, carefully mined source-target instance pairs undergo a simulation of cross-domain feature stylization (AST-Sim) at a particular layer by altering the AST-latent. Second, AST operating at a later layer is tasked to normalize (AST-Norm) the domain content by fixing its latent to a mean prototype. Our simplified adaptation technique is not only clustering-free but also free from complex adversarial alignment. We achieve leading performance against the prior arts on the OCDA scene segmentation benchmarks.



### Learning to Bootstrap for Combating Label Noise
- **Arxiv ID**: http://arxiv.org/abs/2202.04291v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04291v1)
- **Published**: 2022-02-09 05:57:08+00:00
- **Updated**: 2022-02-09 05:57:08+00:00
- **Authors**: Yuyin Zhou, Xianhang Li, Fengze Liu, Xuxi Chen, Lequan Yu, Cihang Xie, Matthew P. Lungren, Lei Xing
- **Comment**: tech report; code is available at https://github.com/yuyinzhou/L2B
- **Journal**: None
- **Summary**: Deep neural networks are powerful tools for representation learning, but can easily overfit to noisy labels which are prevalent in many real-world scenarios. Generally, noisy supervision could stem from variation among labelers, label corruption by adversaries, etc. To combat such label noises, one popular line of approach is to apply customized weights to the training instances, so that the corrupted examples contribute less to the model learning. However, such learning mechanisms potentially erase important information about the data distribution and therefore yield suboptimal results. To leverage useful information from the corrupted instances, an alternative is the bootstrapping loss, which reconstructs new training targets on-the-fly by incorporating the network's own predictions (i.e., pseudo-labels).   In this paper, we propose a more generic learnable loss objective which enables a joint reweighting of instances and labels at once. Specifically, our method dynamically adjusts the per-sample importance weight between the real observed labels and pseudo-labels, where the weights are efficiently determined in a meta process. Compared to the previous instance reweighting methods, our approach concurrently conducts implicit relabeling, and thereby yield substantial improvements with almost no extra cost. Extensive experimental results demonstrated the strengths of our approach over existing methods on multiple natural and medical image benchmark datasets, including CIFAR-10, CIFAR-100, ISIC2019 and Clothing 1M. The code is publicly available at https://github.com/yuyinzhou/L2B.



### Image Difference Captioning with Pre-training and Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.04298v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.04298v1)
- **Published**: 2022-02-09 06:14:22+00:00
- **Updated**: 2022-02-09 06:14:22+00:00
- **Authors**: Linli Yao, Weiying Wang, Qin Jin
- **Comment**: Accepted to AAAI2022
- **Journal**: None
- **Summary**: The Image Difference Captioning (IDC) task aims to describe the visual differences between two similar images with natural language. The major challenges of this task lie in two aspects: 1) fine-grained visual differences that require learning stronger vision and language association and 2) high-cost of manual annotations that leads to limited supervised data. To address these challenges, we propose a new modeling framework following the pre-training-finetuning paradigm. Specifically, we design three self-supervised tasks and contrastive learning strategies to align visual differences and text descriptions at a fine-grained level. Moreover, we propose a data expansion strategy to utilize extra cross-task supervision information, such as data for fine-grained image classification, to alleviate the limitation of available supervised IDC data. Extensive experiments on two IDC benchmark datasets, CLEVR-Change and Birds-to-Words, demonstrate the effectiveness of the proposed modeling framework. The codes and models will be released at https://github.com/yaolinli/IDC.



### FCM-DNN: diagnosing coronary artery disease by deep accuracy Fuzzy C-Means clustering model
- **Arxiv ID**: http://arxiv.org/abs/2202.04645v2
- **DOI**: 10.3934/mbe.2022167
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.04645v2)
- **Published**: 2022-02-09 06:41:44+00:00
- **Updated**: 2022-02-28 06:00:18+00:00
- **Authors**: Javad Hassannataj Joloudari, Hamid Saadatfar, Mohammad GhasemiGol, Roohallah Alizadehsani, Zahra Alizadeh Sani, Fereshteh Hasanzadeh, Edris Hassannataj, Danial Sharifrazi, Zulkefli Mansor
- **Comment**: 27 pages, 13 figures
- **Journal**: None
- **Summary**: Cardiovascular disease is one of the most challenging diseases in middle-aged and older people, which causes high mortality. Coronary artery disease (CAD) is known as a common cardiovascular disease. A standard clinical tool for diagnosing CAD is angiography. The main challenges are dangerous side effects and high angiography costs. Today, the development of artificial intelligence-based methods is a valuable achievement for diagnosing disease. Hence, in this paper, artificial intelligence methods such as neural network (NN), deep neural network (DNN), and Fuzzy C-Means clustering combined with deep neural network (FCM-DNN) are developed for diagnosing CAD on a cardiac magnetic resonance imaging (CMRI) dataset. The original dataset is used in two different approaches. First, the labeled dataset is applied to the NN and DNN to create the NN and DNN models. Second, the labels are removed, and the unlabeled dataset is clustered via the FCM method, and then, the clustered dataset is fed to the DNN to create the FCM-DNN model. By utilizing the second clustering and modeling, the training process is improved, and consequently, the accuracy is increased. As a result, the proposed FCM-DNN model achieves the best performance with a 99.91% accuracy specifying 10 clusters, i.e., 5 clusters for healthy subjects and 5 clusters for sick subjects, through the 10-fold cross-validation technique compared to the NN and DNN models reaching the accuracies of 92.18% and 99.63%, respectively. To the best of our knowledge, no study has been conducted for CAD diagnosis on the CMRI dataset using artificial intelligence methods. The results confirm that the proposed FCM-DNN model can be helpful for scientific and research centers.



### Can Open Domain Question Answering Systems Answer Visual Knowledge Questions?
- **Arxiv ID**: http://arxiv.org/abs/2202.04306v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.04306v1)
- **Published**: 2022-02-09 06:47:40+00:00
- **Updated**: 2022-02-09 06:47:40+00:00
- **Authors**: Jiawen Zhang, Abhijit Mishra, Avinesh P. V. S, Siddharth Patwardhan, Sachin Agarwal
- **Comment**: 9 pages (including references), 5 figures
- **Journal**: None
- **Summary**: The task of Outside Knowledge Visual Question Answering (OKVQA) requires an automatic system to answer natural language questions about pictures and images using external knowledge. We observe that many visual questions, which contain deictic referential phrases referring to entities in the image, can be rewritten as "non-grounded" questions and can be answered by existing text-based question answering systems. This allows for the reuse of existing text-based Open Domain Question Answering (QA) Systems for visual question answering. In this work, we propose a potentially data-efficient approach that reuses existing systems for (a) image analysis, (b) question rewriting, and (c) text-based question answering to answer such visual questions. Given an image and a question pertaining to that image (a visual question), we first extract the entities present in the image using pre-trained object and scene classifiers. Using these detected entities, the visual questions can be rewritten so as to be answerable by open domain QA systems. We explore two rewriting strategies: (1) an unsupervised method using BERT for masking and rewriting, and (2) a weakly supervised approach that combines adaptive rewriting and reinforcement learning techniques to use the implicit feedback from the QA system. We test our strategies on the publicly available OKVQA dataset and obtain a competitive performance with state-of-the-art models while using only 10% of the training data.



### Conditional Motion In-betweening
- **Arxiv ID**: http://arxiv.org/abs/2202.04307v2
- **DOI**: 10.1016/j.patcog.2022.108894
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2202.04307v2)
- **Published**: 2022-02-09 06:47:56+00:00
- **Updated**: 2022-10-06 05:14:05+00:00
- **Authors**: Jihoon Kim, Taehyun Byun, Seungyoun Shin, Jungdam Won, Sungjoon Choi
- **Comment**: None
- **Journal**: Pattern Recognition, Volume 132, December 2022, 108894
- **Summary**: Motion in-betweening (MIB) is a process of generating intermediate skeletal movement between the given start and target poses while preserving the naturalness of the motion, such as periodic footstep motion while walking. Although state-of-the-art MIB methods are capable of producing plausible motions given sparse key-poses, they often lack the controllability to generate motions satisfying the semantic contexts required in practical applications. We focus on the method that can handle pose or semantic conditioned MIB tasks using a unified model. We also present a motion augmentation method to improve the quality of pose-conditioned motion generation via defining a distribution over smooth trajectories. Our proposed method outperforms the existing state-of-the-art MIB method in pose prediction errors while providing additional controllability.



### Anchor Graph Structure Fusion Hashing for Cross-Modal Similarity Search
- **Arxiv ID**: http://arxiv.org/abs/2202.04327v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2202.04327v1)
- **Published**: 2022-02-09 08:27:32+00:00
- **Updated**: 2022-02-09 08:27:32+00:00
- **Authors**: Lu Wang, Jie Yang, Masoumeh Zareapoor, Zhonglong Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-modal hashing still has some challenges needed to address: (1) most existing CMH methods take graphs as input to model data distribution. These methods omit to consider the correlation of graph structure among multiple modalities; (2) most existing CMH methods ignores considering the fusion affinity among multi-modalities data; (3) most existing CMH methods relax the discrete constraints to solve the optimization objective, significantly degrading the retrieval performance. To solve the above limitations, we propose a novel Anchor Graph Structure Fusion Hashing (AGSFH). AGSFH constructs the anchor graph structure fusion matrix from different anchor graphs of multiple modalities with the Hadamard product, which can fully exploit the geometric property of underlying data structure. Based on the anchor graph structure fusion matrix, AGSFH attempts to directly learn an intrinsic anchor graph, where the structure of the intrinsic anchor graph is adaptively tuned so that the number of components of the intrinsic graph is exactly equal to the number of clusters. Besides, AGSFH preserves the anchor fusion affinity into the common binary Hamming space. Furthermore, a discrete optimization framework is designed to learn the unified binary codes. Extensive experimental results on three public social datasets demonstrate the superiority of AGSFH.



### Deep Feature Rotation for Multimodal Image Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2202.04426v1
- **DOI**: 10.1109/NICS54270.2021.9701465
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04426v1)
- **Published**: 2022-02-09 12:36:24+00:00
- **Updated**: 2022-02-09 12:36:24+00:00
- **Authors**: Son Truong Nguyen, Nguyen Quang Tuyen, Nguyen Hong Phuc
- **Comment**: Accepted to NICS'21
- **Journal**: None
- **Summary**: Recently, style transfer is a research area that attracts a lot of attention, which transfers the style of an image onto a content target. Extensive research on style transfer has aimed at speeding up processing or generating high-quality stylized images. Most approaches only produce an output from a content and style image pair, while a few others use complex architectures and can only produce a certain number of outputs. In this paper, we propose a simple method for representing style features in many ways called Deep Feature Rotation (DFR), while not only producing diverse outputs but also still achieving effective stylization compared to more complex methods. Our approach is representative of the many ways of augmentation for intermediate feature embedding without consuming too much computational expense. We also analyze our method by visualizing output in different rotation weights. Our code is available at https://github.com/sonnguyen129/deep-feature-rotation.



### Object-Guided Day-Night Visual Localization in Urban Scenes
- **Arxiv ID**: http://arxiv.org/abs/2202.04445v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04445v1)
- **Published**: 2022-02-09 13:21:30+00:00
- **Updated**: 2022-02-09 13:21:30+00:00
- **Authors**: Assia Benbihi, Cédric Pradalier, Ondřej Chum
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Object-Guided Localization (OGuL) based on a novel method of local-feature matching. Direct matching of local features is sensitive to significant changes in illumination. In contrast, object detection often survives severe changes in lighting conditions. The proposed method first detects semantic objects and establishes correspondences of those objects between images. Object correspondences provide local coarse alignment of the images in the form of a planar homography. These homographies are consequently used to guide the matching of local features. Experiments on standard urban localization datasets (Aachen, Extended-CMU-Season, RobotCar-Season) show that OGuL significantly improves localization results with as simple local features as SIFT, and its performance competes with the state-of-the-art CNN-based methods trained for day-to-night localization.



### Predicting the intended action using internal simulation of perception
- **Arxiv ID**: http://arxiv.org/abs/2202.04466v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.04466v1)
- **Published**: 2022-02-09 13:52:42+00:00
- **Updated**: 2022-02-09 13:52:42+00:00
- **Authors**: Zahra Gharaee
- **Comment**: None
- **Journal**: None
- **Summary**: This article proposes an architecture, which allows the prediction of intention by internally simulating perceptual states represented by action pattern vectors. To this end, associative self-organising neural networks (A-SOM) is utilised to build a hierarchical cognitive architecture for recognition and simulation of the skeleton based human actions. The abilities of the proposed architecture in recognising and predicting actions is evaluated in experiments using three different datasets of 3D actions. Based on the experiments of this article, applying internally simulated perceptual states represented by action pattern vectors improves the performance of the recognition task in all experiments. Furthermore, internal simulation of perception addresses the problem of having limited access to the sensory input, and also the future prediction of the consecutive perceptual sequences. The performance of the system is compared and discussed with similar architecture using self-organizing neural networks (SOM).



### CRAT-Pred: Vehicle Trajectory Prediction with Crystal Graph Convolutional Neural Networks and Multi-Head Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2202.04488v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.04488v2)
- **Published**: 2022-02-09 14:36:36+00:00
- **Updated**: 2022-02-10 06:57:54+00:00
- **Authors**: Julian Schmidt, Julian Jordan, Franz Gritschneder, Klaus Dietmayer
- **Comment**: To appear in the proceedings of 2022 IEEE International Conference on
  Robotics and Automation (ICRA)
- **Journal**: None
- **Summary**: Predicting the motion of surrounding vehicles is essential for autonomous vehicles, as it governs their own motion plan. Current state-of-the-art vehicle prediction models heavily rely on map information. In reality, however, this information is not always available. We therefore propose CRAT-Pred, a multi-modal and non-rasterization-based trajectory prediction model, specifically designed to effectively model social interactions between vehicles, without relying on map information. CRAT-Pred applies a graph convolution method originating from the field of material science to vehicle prediction, allowing to efficiently leverage edge features, and combines it with multi-head self-attention. Compared to other map-free approaches, the model achieves state-of-the-art performance with a significantly lower number of model parameters. In addition to that, we quantitatively show that the self-attention mechanism is able to learn social interactions between vehicles, with the weights representing a measurable interaction score. The source code is publicly available.



### A Neural Network based Framework for Effective Laparoscopic Video Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2202.04517v2
- **DOI**: 10.1016/j.compmedimag.2022.102121
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.04517v2)
- **Published**: 2022-02-09 15:29:02+00:00
- **Updated**: 2022-04-14 16:25:21+00:00
- **Authors**: Zohaib Amjad Khan, Azeddine Beghdadi, Mounir Kaaniche, Faouzi Alaya Cheikh, Osama Gharbi
- **Comment**: None
- **Journal**: None
- **Summary**: Video quality assessment is a challenging problem having a critical significance in the context of medical imaging. For instance, in laparoscopic surgery, the acquired video data suffers from different kinds of distortion that not only hinder surgery performance but also affect the execution of subsequent tasks in surgical navigation and robotic surgeries. For this reason, we propose in this paper neural network-based approaches for distortion classification as well as quality prediction. More precisely, a Residual Network (ResNet) based approach is firstly developed for simultaneous ranking and classification task. Then, this architecture is extended to make it appropriate for the quality prediction task by using an additional Fully Connected Neural Network (FCNN). To train the overall architecture (ResNet and FCNN models), transfer learning and end-to-end learning approaches are investigated. Experimental results, carried out on a new laparoscopic video quality database, have shown the efficiency of the proposed methods compared to recent conventional and deep learning based approaches.



### Multi-modal unsupervised brain image registration using edge maps
- **Arxiv ID**: http://arxiv.org/abs/2202.04647v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.04647v3)
- **Published**: 2022-02-09 15:50:14+00:00
- **Updated**: 2022-03-15 10:23:15+00:00
- **Authors**: Vasiliki Sideri-Lampretsa, Georgios Kaissis, Daniel Rueckert
- **Comment**: Accepted to IEEE International Symposium on Biomedical Imaging (ISBI)
  2022
- **Journal**: None
- **Summary**: Diffeomorphic deformable multi-modal image registration is a challenging task which aims to bring images acquired by different modalities to the same coordinate space and at the same time to preserve the topology and the invertibility of the transformation. Recent research has focused on leveraging deep learning approaches for this task as these have been shown to achieve competitive registration accuracy while being computationally more efficient than traditional iterative registration methods. In this work, we propose a simple yet effective unsupervised deep learning-based {\em multi-modal} image registration approach that benefits from auxiliary information coming from the gradient magnitude of the image, i.e. the image edges, during the training. The intuition behind this is that image locations with a strong gradient are assumed to denote a transition of tissues, which are locations of high information value able to act as a geometry constraint. The task is similar to using segmentation maps to drive the training, but the edge maps are easier and faster to acquire and do not require annotations. We evaluate our approach in the context of registering multi-modal (T1w to T2w) magnetic resonance (MR) brain images of different subjects using three different loss functions that are said to assist multi-modal registration, showing that in all cases the auxiliary information leads to better results without compromising the runtime.



### NIMBLE: A Non-rigid Hand Model with Bones and Muscles
- **Arxiv ID**: http://arxiv.org/abs/2202.04533v5
- **DOI**: 10.1145/3503161.3548148
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2202.04533v5)
- **Published**: 2022-02-09 15:57:21+00:00
- **Updated**: 2022-07-18 06:41:58+00:00
- **Authors**: Yuwei Li, Longwen Zhang, Zesong Qiu, Yingwenqi Jiang, Nianyi Li, Yuexin Ma, Yuyao Zhang, Lan Xu, Jingyi Yu
- **Comment**: 17 pages, 18 figures
- **Journal**: None
- **Summary**: Emerging Metaverse applications demand reliable, accurate, and photorealistic reproductions of human hands to perform sophisticated operations as if in the physical world. While real human hand represents one of the most intricate coordination between bones, muscle, tendon, and skin, state-of-the-art techniques unanimously focus on modeling only the skeleton of the hand. In this paper, we present NIMBLE, a novel parametric hand model that includes the missing key components, bringing 3D hand model to a new level of realism. We first annotate muscles, bones and skins on the recent Magnetic Resonance Imaging hand (MRI-Hand) dataset and then register a volumetric template hand onto individual poses and subjects within the dataset. NIMBLE consists of 20 bones as triangular meshes, 7 muscle groups as tetrahedral meshes, and a skin mesh. Via iterative shape registration and parameter learning, it further produces shape blend shapes, pose blend shapes, and a joint regressor. We demonstrate applying NIMBLE to modeling, rendering, and visual inference tasks. By enforcing the inner bones and muscles to match anatomic and kinematic rules, NIMBLE can animate 3D hands to new poses at unprecedented realism. To model the appearance of skin, we further construct a photometric HandStage to acquire high-quality textures and normal maps to model wrinkles and palm print. Finally, NIMBLE also benefits learning-based hand pose and shape estimation by either synthesizing rich data or acting directly as a differentiable layer in the inference network.



### Semantic Segmentation of Anaemic RBCs Using Multilevel Deep Convolutional Encoder-Decoder Network
- **Arxiv ID**: http://arxiv.org/abs/2202.04650v1
- **DOI**: 10.1109/ACCESS.2021.3131768
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.04650v1)
- **Published**: 2022-02-09 17:31:50+00:00
- **Updated**: 2022-02-09 17:31:50+00:00
- **Authors**: Muhammad Shahzad, Arif Iqbal Umar, Syed Hamad Shirazi, Israr Ahmed Shaikh
- **Comment**: None
- **Journal**: None
- **Summary**: Pixel-level analysis of blood images plays a pivotal role in diagnosing blood-related diseases, especially Anaemia. These analyses mainly rely on an accurate diagnosis of morphological deformities like shape, size, and precise pixel counting. In traditional segmentation approaches, instance or object-based approaches have been adopted that are not feasible for pixel-level analysis. The convolutional neural network (CNN) model required a large dataset with detailed pixel-level information for the semantic segmentation of red blood cells in the deep learning domain. In current research work, we address these problems by proposing a multi-level deep convolutional encoder-decoder network along with two state-of-the-art healthy and Anaemic-RBC datasets. The proposed multi-level CNN model preserved pixel-level semantic information extracted in one layer and then passed to the next layer to choose relevant features. This phenomenon helps to precise pixel-level counting of healthy and anaemic-RBC elements along with morphological analysis. For experimental purposes, we proposed two state-of-the-art RBC datasets, i.e., Healthy-RBCs and Anaemic-RBCs dataset. Each dataset contains 1000 images, ground truth masks, relevant, complete blood count (CBC), and morphology reports for performance evaluation. The proposed model results were evaluated using crossmatch analysis with ground truth mask by finding IoU, individual training, validation, testing accuracies, and global accuracies using a 05-fold training procedure. This model got training, validation, and testing accuracies as 0.9856, 0.9760, and 0.9720 on the Healthy-RBC dataset and 0.9736, 0.9696, and 0.9591 on an Anaemic-RBC dataset. The IoU and BFScore of the proposed model were 0.9311, 0.9138, and 0.9032, 0.8978 on healthy and anaemic datasets, respectively.



### Exploring Structural Sparsity in Neural Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2202.04595v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68U10(primary), 94A08 68T07(secondary), I.2.6; I.4.2
- **Links**: [PDF](http://arxiv.org/pdf/2202.04595v4)
- **Published**: 2022-02-09 17:46:49+00:00
- **Updated**: 2022-03-11 13:47:55+00:00
- **Authors**: Shanzhi Yin, Chao Li, Wen Tan, Youneng Bao, Yongsheng Liang, Wei Liu
- **Comment**: 6 pages, 5 figures, submitted to ICIP 2022
- **Journal**: None
- **Summary**: Neural image compression have reached or out-performed traditional methods (such as JPEG, BPG, WebP). However,their sophisticated network structures with cascaded convolution layers bring heavy computational burden for practical deployment. In this paper, we explore the structural sparsity in neural image compression network to obtain real-time acceleration without any specialized hardware design or algorithm. We propose a simple plug-in adaptive binary channel masking(ABCM) to judge the importance of each convolution channel and introduce sparsity during training. During inference, the unimportant channels are pruned to obtain slimmer network and less computation. We implement our method into three neural image compression networks with different entropy models to verify its effectiveness and generalization, the experiment results show that up to 7x computation reduction and 3x acceleration can be achieved with negligible performance drop.



### Automated Distance Estimation for Wildlife Camera Trapping
- **Arxiv ID**: http://arxiv.org/abs/2202.04613v2
- **DOI**: 10.1016/j.ecoinf.2022.101734
- **Categories**: **cs.CV**, cs.AI, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2202.04613v2)
- **Published**: 2022-02-09 18:12:18+00:00
- **Updated**: 2022-05-11 16:04:11+00:00
- **Authors**: Peter Johanns, Timm Haucke, Volker Steinhage
- **Comment**: None
- **Journal**: Ecological Informatics, Volume 70, September 2022, 101734
- **Summary**: The ongoing biodiversity crisis calls for accurate estimation of animal density and abundance to identify sources of biodiversity decline and effectiveness of conservation interventions. Camera traps together with abundance estimation methods are often employed for this purpose. The necessary distances between camera and observed animals are traditionally derived in a laborious, fully manual or semi-automatic process. Both approaches require reference image material, which is both difficult to acquire and not available for existing datasets. We propose a fully automatic approach we call AUtomated DIstance esTimation (AUDIT) to estimate camera-to-animal distances. We leverage existing state-of-the-art relative monocular depth estimation and combine it with a novel alignment procedure to estimate metric distances. AUDIT is fully automated and requires neither the comparison of observations in camera trap imagery with reference images nor capturing of reference image material at all. AUDIT therefore relieves biologists and ecologists from a significant workload. We evaluate AUDIT on a zoo scenario dataset unseen during training where we achieve a mean absolute distance estimation error over all animal instances of only 0.9864 meters and mean relative error (REL) of 0.113. The code and usage instructions are available at https://github.com/PJ-cs/DistanceEstimationTracking



### Class Distance Weighted Cross-Entropy Loss for Ulcerative Colitis Severity Estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.05167v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.05167v2)
- **Published**: 2022-02-09 18:47:50+00:00
- **Updated**: 2022-06-12 09:51:33+00:00
- **Authors**: Gorkem Polat, Ilkay Ergenc, Haluk Tarik Kani, Yesim Ozen Alahdab, Ozlen Atug, Alptekin Temizel
- **Comment**: 26th UK Conference on Medical Image Understanding and Analysis. 15
  pages, 5 figures
- **Journal**: None
- **Summary**: In scoring systems used to measure the endoscopic activity of ulcerative colitis, such as Mayo endoscopic score or Ulcerative Colitis Endoscopic Index Severity, levels increase with severity of the disease activity. Such relative ranking among the scores makes it an ordinal regression problem. On the other hand, most studies use categorical cross-entropy loss function to train deep learning models, which is not optimal for the ordinal regression problem. In this study, we propose a novel loss function, class distance weighted cross-entropy (CDW-CE), that respects the order of the classes and takes the distance of the classes into account in calculation of the cost. Experimental evaluations show that models trained with CDW-CE outperform the models trained with conventional categorical cross-entropy and other commonly used loss functions which are designed for the ordinal regression problems. In addition, the class activation maps of models trained with CDW-CE loss are more class-discriminative and they are found to be more reasonable by the domain experts.



### Reducing Redundancy in the Bottleneck Representation of the Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2202.04629v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.04629v2)
- **Published**: 2022-02-09 18:48:02+00:00
- **Updated**: 2022-11-23 13:56:27+00:00
- **Authors**: Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, Moncef Gabbouj
- **Comment**: 6 pages,4 figures. The paper is under consideration at Pattern
  Recognition Letters
- **Journal**: None
- **Summary**: Autoencoders are a type of unsupervised neural networks, which can be used to solve various tasks, e.g., dimensionality reduction, image compression, and image denoising. An AE has two goals: (i) compress the original input to a low-dimensional space at the bottleneck of the network topology using an encoder, (ii) reconstruct the input from the representation at the bottleneck using a decoder. Both encoder and decoder are optimized jointly by minimizing a distortion-based loss which implicitly forces the model to keep only those variations of input data that are required to reconstruct the and to reduce redundancies. In this paper, we propose a scheme to explicitly penalize feature redundancies in the bottleneck representation. To this end, we propose an additional loss term, based on the pair-wise correlation of the neurons, which complements the standard reconstruction loss forcing the encoder to learn a more diverse and richer representation of the input. We tested our approach across different tasks: dimensionality reduction using three different dataset, image compression using the MNIST dataset, and image denoising using fashion MNIST. The experimental results show that the proposed loss leads consistently to superior performance compared to the standard AE loss.



### Point-Level Region Contrast for Object Detection Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2202.04639v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04639v2)
- **Published**: 2022-02-09 18:56:41+00:00
- **Updated**: 2022-04-19 03:46:07+00:00
- **Authors**: Yutong Bai, Xinlei Chen, Alexander Kirillov, Alan Yuille, Alexander C. Berg
- **Comment**: CVPR 2022 (Oral)
- **Journal**: None
- **Summary**: In this work we present point-level region contrast, a self-supervised pre-training approach for the task of object detection. This approach is motivated by the two key factors in detection: localization and recognition. While accurate localization favors models that operate at the pixel- or point-level, correct recognition typically relies on a more holistic, region-level view of objects. Incorporating this perspective in pre-training, our approach performs contrastive learning by directly sampling individual point pairs from different regions. Compared to an aggregated representation per region, our approach is more robust to the change in input region quality, and further enables us to implicitly improve initial region assignments via online knowledge distillation during training. Both advantages are important when dealing with imperfect regions encountered in the unsupervised setting. Experiments show point-level region contrast improves on state-of-the-art pre-training methods for object detection and segmentation across multiple tasks and datasets, and we provide extensive ablation studies and visualizations to aid understanding. Code will be made available.



### Can Humans Do Less-Than-One-Shot Learning?
- **Arxiv ID**: http://arxiv.org/abs/2202.04670v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.04670v1)
- **Published**: 2022-02-09 19:00:07+00:00
- **Updated**: 2022-02-09 19:00:07+00:00
- **Authors**: Maya Malaviya, Ilia Sucholutsky, Kerem Oktar, Thomas L. Griffiths
- **Comment**: 7 pages, 7 figures
- **Journal**: None
- **Summary**: Being able to learn from small amounts of data is a key characteristic of human intelligence, but exactly {\em how} small? In this paper, we introduce a novel experimental paradigm that allows us to examine classification in an extremely data-scarce setting, asking whether humans can learn more categories than they have exemplars (i.e., can humans do "less-than-one shot" learning?). An experiment conducted using this paradigm reveals that people are capable of learning in such settings, and provides several insights into underlying mechanisms. First, people can accurately infer and represent high-dimensional feature spaces from very little data. Second, having inferred the relevant spaces, people use a form of prototype-based categorization (as opposed to exemplar-based) to make categorical inferences. Finally, systematic, machine-learnable patterns in responses indicate that people may have efficient inductive biases for dealing with this class of data-scarce problems.



### Lifting-based variational multiclass segmentation: design, analysis and implementation
- **Arxiv ID**: http://arxiv.org/abs/2202.04680v2
- **DOI**: None
- **Categories**: **cs.CV**, math.FA
- **Links**: [PDF](http://arxiv.org/pdf/2202.04680v2)
- **Published**: 2022-02-09 19:03:05+00:00
- **Updated**: 2023-08-08 07:36:57+00:00
- **Authors**: Nadja Gruber, Johannes Schwab, Sebastien Court, Elke Gizewski, Markus Haltmeier
- **Comment**: None
- **Journal**: None
- **Summary**: We propose, analyze and realize a variational multiclass segmentation scheme that partitions a given image into multiple regions exhibiting specific properties. Our method determines multiple functions that encode the segmentation regions by minimizing an energy functional combining information from different channels. Multichannel image data can be obtained by lifting the image into a higher dimensional feature space using specific multichannel filtering or may already be provided by the imaging modality under consideration, such as an RGB image or multimodal medical data. Experimental results show that the proposed method performs well in various scenarios. In particular, promising results are presented for two medical applications involving classification of brain abscess and tumor growth, respectively. As main theoretical contributions, we prove the existence of global minimizers of the proposed energy functional and show its stability and convergence with respect to noisy inputs. In particular, these results also apply to the special case of binary segmentation, and these results are also novel in this particular situation.



### PINs: Progressive Implicit Networks for Multi-Scale Neural Representations
- **Arxiv ID**: http://arxiv.org/abs/2202.04713v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04713v2)
- **Published**: 2022-02-09 20:33:37+00:00
- **Updated**: 2022-06-16 22:12:01+00:00
- **Authors**: Zoe Landgraf, Alexander Sorkine Hornung, Ricardo Silveira Cabral
- **Comment**: ICML 2022 (spotlight)
- **Journal**: None
- **Summary**: Multi-layer perceptrons (MLP) have proven to be effective scene encoders when combined with higher-dimensional projections of the input, commonly referred to as \textit{positional encoding}. However, scenes with a wide frequency spectrum remain a challenge: choosing high frequencies for positional encoding introduces noise in low structure areas, while low frequencies result in poor fitting of detailed regions. To address this, we propose a progressive positional encoding, exposing a hierarchical MLP structure to incremental sets of frequency encodings. Our model accurately reconstructs scenes with wide frequency bands and learns a scene representation at progressive level of detail \textit{without explicit per-level supervision}. The architecture is modular: each level encodes a continuous implicit representation that can be leveraged separately for its respective resolution, meaning a smaller network for coarser reconstructions. Experiments on several 2D and 3D datasets show improvements in reconstruction accuracy, representational capacity and training speed compared to baselines.



### Graph Neural Network for Cell Tracking in Microscopy Videos
- **Arxiv ID**: http://arxiv.org/abs/2202.04731v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04731v2)
- **Published**: 2022-02-09 21:21:48+00:00
- **Updated**: 2022-07-17 12:30:54+00:00
- **Authors**: Tal Ben-Haim, Tammy Riklin Raviv
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: We present a novel graph neural network (GNN) approach for cell tracking in high-throughput microscopy videos. By modeling the entire time-lapse sequence as a direct graph where cell instances are represented by its nodes and their associations by its edges, we extract the entire set of cell trajectories by looking for the maximal paths in the graph. This is accomplished by several key contributions incorporated into an end-to-end deep learning framework. We exploit a deep metric learning algorithm to extract cell feature vectors that distinguish between instances of different biological cells and assemble same cell instances. We introduce a new GNN block type which enables a mutual update of node and edge feature vectors, thus facilitating the underlying message passing process. The message passing concept, whose extent is determined by the number of GNN blocks, is of fundamental importance as it enables the `flow' of information between nodes and edges much behind their neighbors in consecutive frames. Finally, we solve an edge classification problem and use the identified active edges to construct the cells' tracks and lineage trees. We demonstrate the strengths of the proposed cell tracking approach by applying it to 2D and 3D datasets of different cell types, imaging setups, and experimental conditions. We show that our framework outperforms current state-of-the-art methods on most of the evaluated datasets. The code is available at our repository: https://github.com/talbenha/cell-tracker-gnn.



### Estimation of Clinical Workload and Patient Activity using Deep Learning and Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2202.04748v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.04748v1)
- **Published**: 2022-02-09 22:19:01+00:00
- **Updated**: 2022-02-09 22:19:01+00:00
- **Authors**: Thanh Nguyen-Duc, Peter Y Chan, Andrew Tay, David Chen, John Tan Nguyen, Jessica Lyall, Maria De Freitas
- **Comment**: None
- **Journal**: None
- **Summary**: Contactless monitoring using thermal imaging has become increasingly proposed to monitor patient deterioration in hospital, most recently to detect fevers and infections during the COVID-19 pandemic. In this letter, we propose a novel method to estimate patient motion and observe clinical workload using a similar technical setup but combined with open source object detection algorithms (YOLOv4) and optical flow. Patient motion estimation was used to approximate patient agitation and sedation, while worker motion was used as a surrogate for caregiver workload. Performance was illustrated by comparing over 32000 frames from videos of patients recorded in an Intensive Care Unit, to clinical agitation scores recorded by clinical workers.



### Discovering Concepts in Learned Representations using Statistical Inference and Interactive Visualization
- **Arxiv ID**: http://arxiv.org/abs/2202.04753v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.04753v1)
- **Published**: 2022-02-09 22:29:48+00:00
- **Updated**: 2022-02-09 22:29:48+00:00
- **Authors**: Adrianna Janik, Kris Sankaran
- **Comment**: KDD'19, Workshop Explainable AI/ML (XAI) for Accountability,
  Fairness, and Transparency, August 04-08, 2019, Anchorage, AK, USA
- **Journal**: None
- **Summary**: Concept discovery is one of the open problems in the interpretability literature that is important for bridging the gap between non-deep learning experts and model end-users. Among current formulations, concepts defines them by as a direction in a learned representation space. This definition makes it possible to evaluate whether a particular concept significantly influences classification decisions for classes of interest. However, finding relevant concepts is tedious, as representation spaces are high-dimensional and hard to navigate. Current approaches include hand-crafting concept datasets and then converting them to latent space directions; alternatively, the process can be automated by clustering the latent space. In this study, we offer another two approaches to guide user discovery of meaningful concepts, one based on multiple hypothesis testing, and another on interactive visualization. We explore the potential value and limitations of these approaches through simulation experiments and an demo visual interface to real data. Overall, we find that these techniques offer a promising strategy for discovering relevant concepts in settings where users do not have predefined descriptions of them, but without completely automating the process.



### Sampling Strategy for Fine-Tuning Segmentation Models to Crisis Area under Scarcity of Data
- **Arxiv ID**: http://arxiv.org/abs/2202.04766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04766v1)
- **Published**: 2022-02-09 23:16:58+00:00
- **Updated**: 2022-02-09 23:16:58+00:00
- **Authors**: Adrianna Janik, Kris Sankaran
- **Comment**: None
- **Journal**: None
- **Summary**: The use of remote sensing in humanitarian crisis response missions is well-established and has proven relevant repeatedly. One of the problems is obtaining gold annotations as it is costly and time consuming which makes it almost impossible to fine-tune models to new regions affected by the crisis. Where time is critical, resources are limited and environment is constantly changing, models has to evolve and provide flexible ways to adapt to a new situation. The question that we want to answer is if prioritization of samples provide better results in fine-tuning vs other classical sampling methods under annotated data scarcity? We propose a method to guide data collection during fine-tuning, based on estimated model and sample properties, like predicted IOU score. We propose two formulas for calculating sample priority. Our approach blends techniques from interpretability, representation learning and active learning. We have applied our method to a deep learning model for semantic segmentation, U-Net, in a remote sensing application of building detection - one of the core use cases of remote sensing in humanitarian applications. Preliminary results shows utility in prioritization of samples for tuning semantic segmentation models under scarcity of data condition.



