# Arxiv Papers in cs.CV on 2022-02-22
### Disentangling Light Fields for Super-Resolution and Disparity Estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.10603v5
- **DOI**: 10.1109/TPAMI.2022.3152488
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10603v5)
- **Published**: 2022-02-22 01:04:41+00:00
- **Updated**: 2023-07-22 09:46:15+00:00
- **Authors**: Yingqian Wang, Longguang Wang, Gaochang Wu, Jungang Yang, Wei An, Jingyi Yu, Yulan Guo
- **Comment**: We have corrected a mistake in Table 1 and updated Fig. 6 by using HR
  GT depth maps for evaluation
- **Journal**: None
- **Summary**: Light field (LF) cameras record both intensity and directions of light rays, and encode 3D scenes into 4D LF images. Recently, many convolutional neural networks (CNNs) have been proposed for various LF image processing tasks. However, it is challenging for CNNs to effectively process LF images since the spatial and angular information are highly inter-twined with varying disparities. In this paper, we propose a generic mechanism to disentangle these coupled information for LF image processing. Specifically, we first design a class of domain-specific convolutions to disentangle LFs from different dimensions, and then leverage these disentangled features by designing task-specific modules. Our disentangling mechanism can well incorporate the LF structure prior and effectively handle 4D LF data. Based on the proposed mechanism, we develop three networks (i.e., DistgSSR, DistgASR and DistgDisp) for spatial super-resolution, angular super-resolution and disparity estimation. Experimental results show that our networks achieve state-of-the-art performance on all these three tasks, which demonstrates the effectiveness, efficiency, and generality of our disentangling mechanism. Project page: https://yingqianwang.github.io/DistgLF/.



### Local Sliced-Wasserstein Feature Sets for Illumination-invariant Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.10642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10642v1)
- **Published**: 2022-02-22 03:01:21+00:00
- **Updated**: 2022-02-22 03:01:21+00:00
- **Authors**: Yan Zhuang, Shiying Li, Mohammad Shifat-E-Rabbi, Xuwang Yin, Abu Hasnat Mohammad Rubaiyat, Gustavo K. Rohde
- **Comment**: 14 pages, 9 figures
- **Journal**: None
- **Summary**: We present a new method for face recognition from digital images acquired under varying illumination conditions. The method is based on mathematical modeling of local gradient distributions using the Radon Cumulative Distribution Transform (R-CDT). We demonstrate that lighting variations cause certain types of deformations of local image gradient distributions which, when expressed in R-CDT domain, can be modeled as a subspace. Face recognition is then performed using a nearest subspace in R-CDT domain of local gradient distributions. Experiment results demonstrate the proposed method outperforms other alternatives in several face recognition tasks with challenging illumination conditions. Python code implementing the proposed method is available, which is integrated as a part of the software package PyTransKit.



### Combining the Silhouette and Skeleton Data for Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.10645v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10645v3)
- **Published**: 2022-02-22 03:21:51+00:00
- **Updated**: 2023-03-24 07:14:27+00:00
- **Authors**: Likai Wang, Ruize Han, Wei Feng
- **Comment**: Accepted by IEEE ICASSP 2023
- **Journal**: None
- **Summary**: Gait recognition, a long-distance biometric technology, has aroused intense interest recently. Currently, the two dominant gait recognition works are appearance-based and model-based, which extract features from silhouettes and skeletons, respectively. However, appearance-based methods are greatly affected by clothes-changing and carrying conditions, while model-based methods are limited by the accuracy of pose estimation. To tackle this challenge, a simple yet effective two-branch network is proposed in this paper, which contains a CNN-based branch taking silhouettes as input and a GCN-based branch taking skeletons as input. In addition, for better gait representation in the GCN-based branch, we present a fully connected graph convolution operator to integrate multi-scale graph convolutions and alleviate the dependence on natural joint connections. Also, we deploy a multi-dimension attention module named STC-Att to learn spatial, temporal and channel-wise attention simultaneously. The experimental results on CASIA-B and OUMVLP show that our method achieves state-of-the-art performance in various conditions.



### Movies2Scenes: Using Movie Metadata to Learn Scene Representation
- **Arxiv ID**: http://arxiv.org/abs/2202.10650v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10650v3)
- **Published**: 2022-02-22 03:31:33+00:00
- **Updated**: 2023-03-30 00:51:47+00:00
- **Authors**: Shixing Chen, Chun-Hao Liu, Xiang Hao, Xiaohan Nie, Maxim Arap, Raffay Hamid
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: Understanding scenes in movies is crucial for a variety of applications such as video moderation, search, and recommendation. However, labeling individual scenes is a time-consuming process. In contrast, movie level metadata (e.g., genre, synopsis, etc.) regularly gets produced as part of the film production process, and is therefore significantly more commonly available. In this work, we propose a novel contrastive learning approach that uses movie metadata to learn a general-purpose scene representation. Specifically, we use movie metadata to define a measure of movie similarity, and use it during contrastive learning to limit our search for positive scene-pairs to only the movies that are considered similar to each other. Our learned scene representation consistently outperforms existing state-of-the-art methods on a diverse set of tasks evaluated using multiple benchmark datasets. Notably, our learned representation offers an average improvement of 7.9% on the seven classification tasks and 9.7% improvement on the two regression tasks in LVU dataset. Furthermore, using a newly collected movie dataset, we present comparative results of our scene representation on a set of video moderation tasks to demonstrate its generalizability on previously less explored tasks.



### ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Multi-Task Learning Challenges
- **Arxiv ID**: http://arxiv.org/abs/2202.10659v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10659v2)
- **Published**: 2022-02-22 04:02:17+00:00
- **Updated**: 2022-02-28 12:39:49+00:00
- **Authors**: Dimitrios Kollias
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes the third Affective Behavior Analysis in-the-wild (ABAW) Competition, held in conjunction with IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2022. The 3rd ABAW Competition is a continuation of the Competitions held at ICCV 2021, IEEE FG 2020 and IEEE CVPR 2017 Conferences, and aims at automatically analyzing affect. This year the Competition encompasses four Challenges: i) uni-task Valence-Arousal Estimation, ii) uni-task Expression Classification, iii) uni-task Action Unit Detection, and iv) Multi-Task-Learning. All the Challenges are based on a common benchmark database, Aff-Wild2, which is a large scale in-the-wild database and the first one to be annotated in terms of valence-arousal, expressions and action units. In this paper, we present the four Challenges, with the utilized Competition corpora, we outline the evaluation metrics and present the baseline systems along with their obtained results.



### Fast Eye Detector Using Siamese Network for NIR Partial Face Images
- **Arxiv ID**: http://arxiv.org/abs/2202.10671v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10671v2)
- **Published**: 2022-02-22 05:02:21+00:00
- **Updated**: 2023-01-04 06:32:47+00:00
- **Authors**: Yuka Ogino, Yuho Shoji, Takahiro Toizumi, Ryoma Oami, Masato Tsukada
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a fast eye detection method that is based on a Siamese network for near infrared (NIR) partial face images. NIR partial face images do not include the whole face of a subject since they are captured using iris recognition systems with the constraint of frame rate and resolution. The iris recognition systems such as the iris on the move (IOTM) system require fast and accurate eye detection as a pre-process. Our goal is to design eye detection with high speed, high discrimination performance between left and right eyes, and high positional accuracy of eye center. Our method adopts a Siamese network and coarse to fine position estimation with a fast lightweight CNN backbone. The network outputs features of images and the similarity map indicating coarse position of an eye. A regression on a portion of a feature with high similarity refines the coarse position of the eye to obtain the fine position with high accuracy. We demonstrate the effectiveness of the proposed method by comparing it with conventional methods, including SOTA, in terms of the positional accuracy, the discrimination performance, and the processing speed. Our method achieves superior performance in speed.



### Seeing is Living? Rethinking the Security of Facial Liveness Verification in the Deepfake Era
- **Arxiv ID**: http://arxiv.org/abs/2202.10673v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10673v1)
- **Published**: 2022-02-22 05:19:30+00:00
- **Updated**: 2022-02-22 05:19:30+00:00
- **Authors**: Changjiang Li, Li Wang, Shouling Ji, Xuhong Zhang, Zhaohan Xi, Shanqing Guo, Ting Wang
- **Comment**: Accepted as a full paper at USENIX Security '22
- **Journal**: None
- **Summary**: Facial Liveness Verification (FLV) is widely used for identity authentication in many security-sensitive domains and offered as Platform-as-a-Service (PaaS) by leading cloud vendors. Yet, with the rapid advances in synthetic media techniques (e.g., deepfake), the security of FLV is facing unprecedented challenges, about which little is known thus far.   To bridge this gap, in this paper, we conduct the first systematic study on the security of FLV in real-world settings. Specifically, we present LiveBugger, a new deepfake-powered attack framework that enables customizable, automated security evaluation of FLV. Leveraging LiveBugger, we perform a comprehensive empirical assessment of representative FLV platforms, leading to a set of interesting findings. For instance, most FLV APIs do not use anti-deepfake detection; even for those with such defenses, their effectiveness is concerning (e.g., it may detect high-quality synthesized videos but fail to detect low-quality ones). We then conduct an in-depth analysis of the factors impacting the attack performance of LiveBugger: a) the bias (e.g., gender or race) in FLV can be exploited to select victims; b) adversarial training makes deepfake more effective to bypass FLV; c) the input quality has a varying influence on different deepfake techniques to bypass FLV. Based on these findings, we propose a customized, two-stage approach that can boost the attack success rate by up to 70%. Further, we run proof-of-concept attacks on several representative applications of FLV (i.e., the clients of FLV APIs) to illustrate the practical implications: due to the vulnerability of the APIs, many downstream applications are vulnerable to deepfake. Finally, we discuss potential countermeasures to improve the security of FLV. Our findings have been confirmed by the corresponding vendors.



### Reinforcing Local Feature Representation for Weakly-Supervised Dense Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2202.10681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10681v1)
- **Published**: 2022-02-22 05:53:51+00:00
- **Updated**: 2022-02-22 05:53:51+00:00
- **Authors**: Xiaoshuang Chen, Hongtao Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Fully-supervised crowd counting is a laborious task due to the large amounts of annotations. Few works focus on weekly-supervised crowd counting, where only the global crowd numbers are available for training. The main challenge of weekly-supervised crowd counting is the lack of local supervision information. To address this problem, we propose a self-adaptive feature similarity learning (SFSL) network and a global-local consistency (GLC) loss to reinforce local feature representation. We introduce a feature vector which represents the unbiased feature estimation of persons. The network updates the feature vector self-adaptively and utilizes the feature similarity for the regression of crowd numbers. Besides, the proposed GLC loss leverages the consistency between the network estimations from global and local areas. The experimental results demonstrate that our proposed method based on different backbones narrows the gap between weakly-supervised and fully-supervised dense crowd counting.



### Cut and Continuous Paste towards Real-time Deep Fall Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.10687v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10687v1)
- **Published**: 2022-02-22 06:07:16+00:00
- **Updated**: 2022-02-22 06:07:16+00:00
- **Authors**: Sunhee Hwang, Minsong Ki, Seung-Hyun Lee, Sanghoon Park, Byoung-Ki Jeon
- **Comment**: Accepted to ICASSP 2022
- **Journal**: None
- **Summary**: Deep learning based fall detection is one of the crucial tasks for intelligent video surveillance systems, which aims to detect unintentional falls of humans and alarm dangerous situations. In this work, we propose a simple and efficient framework to detect falls through a single and small-sized convolutional neural network. To this end, we first introduce a new image synthesis method that represents human motion in a single frame. This simplifies the fall detection task as an image classification task. Besides, the proposed synthetic data generation method enables to generate a sufficient amount of training dataset, resulting in satisfactory performance even with the small model. At the inference step, we also represent real human motion in a single image by estimating mean of input frames. In the experiment, we conduct both qualitative and quantitative evaluations on URFD and AIHub airport datasets to show the effectiveness of our method.



### An Object Aware Hybrid U-Net for Breast Tumour Annotation
- **Arxiv ID**: http://arxiv.org/abs/2202.10691v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10691v1)
- **Published**: 2022-02-22 06:30:31+00:00
- **Updated**: 2022-02-22 06:30:31+00:00
- **Authors**: Suvidha Tripathi, Satish Kumar Singh
- **Comment**: None
- **Journal**: None
- **Summary**: In the clinical settings, during digital examination of histopathological slides, the pathologist annotate the slides by marking the rough boundary around the suspected tumour region. The marking or annotation is generally represented as a polygonal boundary that covers the extent of the tumour in the slide. These polygonal markings are difficult to imitate through CAD techniques since the tumour regions are heterogeneous and hence segmenting them would require exhaustive pixel wise ground truth annotation. Therefore, for CAD analysis, the ground truths are generally annotated by pathologist explicitly for research purposes. However, this kind of annotation which is generally required for semantic or instance segmentation is time consuming and tedious. In this proposed work, therefore, we have tried to imitate pathologist like annotation by segmenting tumour extents by polygonal boundaries. For polygon like annotation or segmentation, we have used Active Contours whose vertices or snake points move towards the boundary of the object of interest to find the region of minimum energy. To penalize the Active Contour we used modified U-Net architecture for learning penalization values. The proposed hybrid deep learning model fuses the modern deep learning segmentation algorithm with traditional Active Contours segmentation technique. The model is tested against both state-of-the-art semantic segmentation and hybrid models for performance evaluation against contemporary work. The results obtained show that the pathologist like annotation could be achieved by developing such hybrid models that integrate the domain knowledge through classical segmentation methods like Active Contours and global knowledge through semantic segmentation deep learning models.



### Universal adversarial perturbation for remote sensing images
- **Arxiv ID**: http://arxiv.org/abs/2202.10693v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10693v2)
- **Published**: 2022-02-22 06:43:28+00:00
- **Updated**: 2023-01-03 10:59:22+00:00
- **Authors**: Qingyu Wang, Guorui Feng, Zhaoxia Yin, Bin Luo
- **Comment**: Published in the Twenty-Fourth International Workshop on Multimedia
  Signal Processing, MMSP 2022
- **Journal**: None
- **Summary**: Recently, with the application of deep learning in the remote sensing image (RSI) field, the classification accuracy of the RSI has been dramatically improved compared with traditional technology. However, even the state-of-the-art object recognition convolutional neural networks are fooled by the universal adversarial perturbation (UAP). The research on UAP is mostly limited to ordinary images, and RSIs have not been studied. To explore the basic characteristics of UAPs of RSIs, this paper proposes a novel method combining an encoder-decoder network with an attention mechanism to generate the UAP of RSIs. Firstly, the former is used to generate the UAP, which can learn the distribution of perturbations better, and then the latter is used to find the sensitive regions concerned by the RSI classification model. Finally, the generated regions are used to fine-tune the perturbation making the model misclassified with fewer perturbations. The experimental results show that the UAP can make the classification model misclassify, and the attack success rate of our proposed method on the RSI data set is as high as 97.09%.



### Ensembling Handcrafted Features with Deep Features: An Analytical Study for Classification of Routine Colon Cancer Histopathological Nuclei Images
- **Arxiv ID**: http://arxiv.org/abs/2202.10694v1
- **DOI**: 10.1007/s11042-020-08891-w
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10694v1)
- **Published**: 2022-02-22 06:48:50+00:00
- **Updated**: 2022-02-22 06:48:50+00:00
- **Authors**: Suvidha Tripathi, Satish Kumar Singh
- **Comment**: None
- **Journal**: Multimedia Tools Application 79 34931-34954 2020
- **Summary**: The use of Deep Learning (DL) based methods in medical histopathology images have been one of the most sought after solutions to classify, segment, and detect diseased biopsy samples. However, given the complex nature of medical datasets due to the presence of intra-class variability and heterogeneity, the use of complex DL models might not give the optimal performance up to the level which is suitable for assisting pathologists. Therefore, ensemble DL methods with the scope of including domain agnostic handcrafted Features (HC-F) inspired this work. We have, through experiments, tried to highlight that a single DL network (domain-specific or state of the art pre-trained models) cannot be directly used as the base model without proper analysis with the relevant dataset. We have used F1-measure, Precision, Recall, AUC, and Cross-Entropy Loss to analyse the performance of our approaches. We observed from the results that the DL features ensemble bring a marked improvement in the overall performance of the model, whereas, domain agnostic HC-F remains dormant on the performance of the DL models.



### Rotationally Equivariant Super-Resolution of Velocity Fields in Two-Dimensional Fluids Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.11099v4
- **DOI**: None
- **Categories**: **physics.flu-dyn**, cs.CV, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2202.11099v4)
- **Published**: 2022-02-22 07:07:07+00:00
- **Updated**: 2022-10-25 10:09:38+00:00
- **Authors**: Yuki Yasuda, Ryo Onishi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper investigates the super-resolution (SR) of velocity fields in two-dimensional fluids from the viewpoint of rotational equivariance. SR refers to techniques that estimate high-resolution images from those in low resolution and has lately been applied in fluid mechanics. The rotational equivariance of SR models is defined as the property in which the super-resolved velocity field is rotated according to a rotation of the input, which leads to the inference covariant to the orientation of fluid systems. Generally, the covariance in physics is related to symmetries. To clarify a relationship to symmetries, the rotational consistency of datasets for SR is newly introduced as the invariance of pairs of low- and high-resolution velocity fields with respect to rotation. This consistency is sufficient and necessary for SR models to acquire rotational equivariance from large datasets with supervised learning. Such a large dataset is not required when rotational equivariance is imposed on SR models through weight sharing of convolution kernels as prior knowledge. Even if a fluid system has rotational symmetry, this symmetry may not carry over to a velocity dataset, which is not rotationally consistent. This inconsistency can occur when the rotation does not commute with the generation of low-resolution velocity fields. These theoretical suggestions are supported by the results from numerical experiments, where two existing convolutional neural networks (CNNs) are converted into rotationally equivariant CNNs and the inferences of the four CNNs are compared after the supervised training.



### Bag of Visual Words (BoVW) with Deep Features -- Patch Classification Model for Limited Dataset of Breast Tumours
- **Arxiv ID**: http://arxiv.org/abs/2202.10701v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10701v1)
- **Published**: 2022-02-22 07:19:18+00:00
- **Updated**: 2022-02-22 07:19:18+00:00
- **Authors**: Suvidha Tripathi, Satish Kumar Singh, Lee Hwee Kuan
- **Comment**: None
- **Journal**: None
- **Summary**: Currently, the computational complexity limits the training of high resolution gigapixel images using Convolutional Neural Networks. Therefore, such images are divided into patches or tiles. Since, these high resolution patches are encoded with discriminative information therefore; CNNs are trained on these patches to perform patch-level predictions. However, the problem with patch-level prediction is that pathologist generally annotates at image-level and not at patch level. Due to this limitation most of the patches may not contain enough class-relevant features. Through this work, we tried to incorporate patch descriptive capability within the deep framework by using Bag of Visual Words (BoVW) as a kind of regularisation to improve generalizability. Using this hypothesis, we aim to build a patch based classifier to discriminate between four classes of breast biopsy image patches (normal, benign, \textit{In situ} carcinoma, invasive carcinoma). The task is to incorporate quality deep features using CNN to describe relevant information in the images while simultaneously discarding irrelevant information using Bag of Visual Words (BoVW). The proposed method passes patches obtained from WSI and microscopy images through pre-trained CNN to extract features. BoVW is used as a feature selector to select most discriminative features among the CNN features. Finally, the selected feature sets are classified as one of the four classes. The hybrid model provides flexibility in terms of choice of pre-trained models for feature extraction. The pipeline is end-to-end since it does not require post processing of patch predictions to select discriminative patches. We compared our observations with state-of-the-art methods like ResNet50, DenseNet169, and InceptionV3 on the BACH-2018 challenge dataset. Our proposed method shows better performance than all the three methods.



### Privacy-Preserving In-Bed Pose Monitoring: A Fusion and Reconstruction Study
- **Arxiv ID**: http://arxiv.org/abs/2202.10704v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10704v1)
- **Published**: 2022-02-22 07:24:21+00:00
- **Updated**: 2022-02-22 07:24:21+00:00
- **Authors**: Thisun Dayarathna, Thamidu Muthukumarana, Yasiru Rathnayaka, Simon Denman, Chathura de Silva, Akila Pemasiri, David Ahmedt-Aristizabal
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, in-bed human pose estimation has attracted the interest of researchers due to its relevance to a wide range of healthcare applications. Compared to the general problem of human pose estimation, in-bed pose estimation has several inherent challenges, the most prominent being frequent and severe occlusions caused by bedding. In this paper we explore the effective use of images from multiple non-visual and privacy-preserving modalities such as depth, long-wave infrared (LWIR) and pressure maps for the task of in-bed pose estimation in two settings. First, we explore the effective fusion of information from different imaging modalities for better pose estimation. Secondly, we propose a framework that can estimate in-bed pose estimation when visible images are unavailable, and demonstrate the applicability of fusion methods to scenarios where only LWIR images are available. We analyze and demonstrate the effect of fusing features from multiple modalities. For this purpose, we consider four different techniques: 1) Addition, 2) Concatenation, 3) Fusion via learned modal weights, and 4) End-to-end fully trainable approach; with a state-of-the-art pose estimation model. We also evaluate the effect of reconstructing a data-rich modality (i.e., visible modality) from a privacy-preserving modality with data scarcity (i.e., long-wavelength infrared) for in-bed human pose estimation. For reconstruction, we use a conditional generative adversarial network. We conduct ablative studies across different design decisions of our framework. This includes selecting features with different levels of granularity, using different fusion techniques, and varying model parameters. Through extensive evaluations, we demonstrate that our method produces on par or better results compared to the state-of-the-art.



### PointMatch: A Consistency Training Framework for Weakly Supervised Semantic Segmentation of 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2202.10705v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10705v2)
- **Published**: 2022-02-22 07:26:31+00:00
- **Updated**: 2022-06-21 04:08:09+00:00
- **Authors**: Yushuang Wu, Shengcai Cai, Zizheng Yan, Guanbin Li, Yizhou Yu, Xiaoguang Han, Shuguang Cui
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Semantic segmentation of point cloud usually relies on dense annotation that is exhausting and costly, so it attracts wide attention to investigate solutions for the weakly supervised scheme with only sparse points annotated. Existing works start from the given labels and propagate them to highly-related but unlabeled points, with the guidance of data, e.g. intra-point relation. However, it suffers from (i) the inefficient exploitation of data information, and (ii) the strong reliance on labels thus is easily suppressed when given much fewer annotations. Therefore, we propose a novel framework, PointMatch, that stands on both data and label, by applying consistency regularization to sufficiently probe information from data itself and leveraging weak labels as assistance at the same time. By doing so, meaningful information can be learned from both data and label for better representation learning, which also enables the model more robust to the extent of label sparsity. Simple yet effective, the proposed PointMatch achieves the state-of-the-art performance under various weakly-supervised schemes on both ScanNet-v2 and S3DIS datasets, especially on the settings with extremely sparse labels, e.g. surpassing SQN by 21.2% and 17.2% on the 0.01% and 0.1% setting of ScanNet-v2, respectively.



### HRel: Filter Pruning based on High Relevance between Activation Maps and Class Labels
- **Arxiv ID**: http://arxiv.org/abs/2202.10716v1
- **DOI**: 10.1016/j.neunet.2021.12.017
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10716v1)
- **Published**: 2022-02-22 08:12:22+00:00
- **Updated**: 2022-02-22 08:12:22+00:00
- **Authors**: CH Sarvani, Mrinmoy Ghorai, Shiv Ram Dubey, SH Shabbeer Basha
- **Comment**: None
- **Journal**: "Neural Networks Volume 147, March 2022, Pages 186-197 "
  https://www.sciencedirect.com/science/article/abs/pii/S0893608021004962
- **Summary**: This paper proposes an Information Bottleneck theory based filter pruning method that uses a statistical measure called Mutual Information (MI). The MI between filters and class labels, also called \textit{Relevance}, is computed using the filter's activation maps and the annotations. The filters having High Relevance (HRel) are considered to be more important. Consequently, the least important filters, which have lower Mutual Information with the class labels, are pruned. Unlike the existing MI based pruning methods, the proposed method determines the significance of the filters purely based on their corresponding activation map's relationship with the class labels. Architectures such as LeNet-5, VGG-16, ResNet-56\textcolor{myblue}{, ResNet-110 and ResNet-50 are utilized to demonstrate the efficacy of the proposed pruning method over MNIST, CIFAR-10 and ImageNet datasets. The proposed method shows the state-of-the-art pruning results for LeNet-5, VGG-16, ResNet-56, ResNet-110 and ResNet-50 architectures. In the experiments, we prune 97.98 \%, 84.85 \%, 76.89\%, 76.95\%, and 63.99\% of Floating Point Operation (FLOP)s from LeNet-5, VGG-16, ResNet-56, ResNet-110, and ResNet-50 respectively.} The proposed HRel pruning method outperforms recent state-of-the-art filter pruning methods. Even after pruning the filters from convolutional layers of LeNet-5 drastically (i.e. from 20, 50 to 2, 3, respectively), only a small accuracy drop of 0.52\% is observed. Notably, for VGG-16, 94.98\% parameters are reduced, only with a drop of 0.36\% in top-1 accuracy. \textcolor{myblue}{ResNet-50 has shown a 1.17\% drop in the top-5 accuracy after pruning 66.42\% of the FLOPs.} In addition to pruning, the Information Plane dynamics of Information Bottleneck theory is analyzed for various Convolutional Neural Network architectures with the effect of pruning.



### Feature reconstruction from incomplete tomographic data without detour
- **Arxiv ID**: http://arxiv.org/abs/2202.10724v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2202.10724v1)
- **Published**: 2022-02-22 08:37:14+00:00
- **Updated**: 2022-02-22 08:37:14+00:00
- **Authors**: Simon Göppel, Jürgen Frikel, Markus Haltmeier
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider the problem of feature reconstruction from incomplete x-ray CT data. Such problems occurs, e.g., as a result of dose reduction in the context medical imaging. Since image reconstruction from incomplete data is a severely ill-posed problem, the reconstructed images may suffer from characteristic artefacts or missing features, and significantly complicate subsequent image processing tasks (e.g., edge detection or segmentation). In this paper, we introduce a novel framework for the robust reconstruction of convolutional image features directly from CT data, without the need of computing a reconstruction firs. Within our framework we use non-linear (variational) regularization methods that can be adapted to a variety of feature reconstruction tasks and to several limited data situations . In our numerical experiments, we consider several instances of edge reconstructions from angularly undersampled data and show that our approach is able to reliably reconstruct feature maps in this case.



### Convolutional Neural Network Modelling for MODIS Land Surface Temperature Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2202.10753v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/2202.10753v2)
- **Published**: 2022-02-22 09:12:40+00:00
- **Updated**: 2022-04-01 07:50:44+00:00
- **Authors**: Binh Minh Nguyen, Ganglin Tian, Minh-Triet Vo, Aurélie Michel, Thomas Corpetti, Carlos Granero-Belinchon
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, thermal infrared satellite remote sensors enable to extract very interesting information at large scale, in particular Land Surface Temperature (LST). However such data are limited in spatial and/or temporal resolutions which prevents from an analysis at fine scales. For example, MODIS satellite provides daily acquisitions with 1Km spatial resolutions which is not sufficient to deal with highly heterogeneous environments as agricultural parcels. Therefore, image super-resolution is a crucial task to better exploit MODIS LSTs. This issue is tackled in this paper. We introduce a deep learning-based algorithm, named Multi-residual U-Net, for super-resolution of MODIS LST single-images. Our proposed network is a modified version of U-Net architecture, which aims at super-resolving the input LST image from 1Km to 250m per pixel. The results show that our Multi-residual U-Net outperforms other state-of-the-art methods.



### Thinking the Fusion Strategy of Multi-reference Face Reenactment
- **Arxiv ID**: http://arxiv.org/abs/2202.10758v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10758v1)
- **Published**: 2022-02-22 09:17:26+00:00
- **Updated**: 2022-02-22 09:17:26+00:00
- **Authors**: Takuya Yashima, Takuya Narihira, Tamaki Kojima
- **Comment**: Submitted to ICIP2022, 5 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: In recent advances of deep generative models, face reenactment -manipulating and controlling human face, including their head movement-has drawn much attention for its wide range of applicability. Despite its strong expressiveness, it is inevitable that the models fail to reconstruct or accurately generate unseen side of the face of a given single reference image. Most of existing methods alleviate this problem by learning appearances of human faces from large amount of data and generate realistic texture at inference time. Rather than completely relying on what generative models learn, we show that simple extension by using multiple reference images significantly improves generation quality. We show this by 1) conducting the reconstruction task on publicly available dataset, 2) conducting facial motion transfer on our original dataset which consists of multi-person's head movement video sequences, and 3) using a newly proposed evaluation metric to validate that our method achieves better quantitative results.



### Deep learning based domain adaptation for mitochondria segmentation on EM volumes
- **Arxiv ID**: http://arxiv.org/abs/2202.10773v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10773v2)
- **Published**: 2022-02-22 09:49:25+00:00
- **Updated**: 2022-07-05 14:43:44+00:00
- **Authors**: Daniel Franco-Barranco, Julio Pastor-Tronch, Aitor Gonzalez-Marfil, Arrate Muñoz-Barrutia, Ignacio Arganda-Carreras
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of electron microscopy (EM) volumes of the brain is essential to characterize neuronal structures at a cell or organelle level. While supervised deep learning methods have led to major breakthroughs in that direction during the past years, they usually require large amounts of annotated data to be trained, and perform poorly on other data acquired under similar experimental and imaging conditions. This is a problem known as domain adaptation, since models that learned from a sample distribution (or source domain) struggle to maintain their performance on samples extracted from a different distribution or target domain. In this work, we address the complex case of deep learning based domain adaptation for mitochondria segmentation across EM datasets from different tissues and species. We present three unsupervised domain adaptation strategies to improve mitochondria segmentation in the target domain based on (1) state-of-the-art style transfer between images of both domains; (2) self-supervised learning to pre-train a model using unlabeled source and target images, and then fine-tune it only with the source labels; and (3) multi-task neural network architectures trained end-to-end with both labeled and unlabeled images. Additionally, we propose a new training stopping criterion based on morphological priors obtained exclusively in the source domain. We carried out all possible cross-dataset experiments using three publicly available EM datasets. We evaluated our proposed strategies on the mitochondria semantic labels predicted on the target datasets. The methods introduced here outperform the baseline methods and compare favorably to the state of the art. In the absence of validation labels, monitoring our proposed morphology-based metric is an intuitive and effective way to stop the training process and select in average optimal models.



### RuCLIP -- new models and experiments: a technical report
- **Arxiv ID**: http://arxiv.org/abs/2202.10784v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.10784v1)
- **Published**: 2022-02-22 10:15:13+00:00
- **Updated**: 2022-02-22 10:15:13+00:00
- **Authors**: Alex Shonenkov, Andrey Kuznetsov, Denis Dimitrov, Tatyana Shavrina, Daniil Chesakov, Anastasia Maltseva, Alena Fenogenova, Igor Pavlov, Anton Emelyanov, Sergey Markov, Daria Bakshandaeva, Vera Shybaeva, Andrey Chertok
- **Comment**: None
- **Journal**: None
- **Summary**: In the report we propose six new implementations of ruCLIP model trained on our 240M pairs. The accuracy results are compared with original CLIP model with Ru-En translation (OPUS-MT) on 16 datasets from different domains. Our best implementations outperform CLIP + OPUS-MT solution on most of the datasets in few-show and zero-shot tasks. In the report we briefly describe the implementations and concentrate on the conducted experiments. Inference execution time comparison is also presented in the report.



### VU-BERT: A Unified framework for Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/2202.10787v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10787v1)
- **Published**: 2022-02-22 10:20:14+00:00
- **Updated**: 2022-02-22 10:20:14+00:00
- **Authors**: Tong Ye, Shijing Si, Jianzong Wang, Rui Wang, Ning Cheng, Jing Xiao
- **Comment**: 5 pages, 2 figures, accepted by 2022 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP 2022)
- **Journal**: None
- **Summary**: The visual dialog task attempts to train an agent to answer multi-turn questions given an image, which requires the deep understanding of interactions between the image and dialog history. Existing researches tend to employ the modality-specific modules to model the interactions, which might be troublesome to use. To fill in this gap, we propose a unified framework for image-text joint embedding, named VU-BERT, and apply patch projection to obtain vision embedding firstly in visual dialog tasks to simplify the model. The model is trained over two tasks: masked language modeling and next utterance retrieval. These tasks help in learning visual concepts, utterances dependence, and the relationships between these two modalities. Finally, our VU-BERT achieves competitive performance (0.7287 NDCG scores) on VisDial v1.0 Datasets.



### A-Eye: Driving with the Eyes of AI for Corner Case Generation
- **Arxiv ID**: http://arxiv.org/abs/2202.10803v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10803v2)
- **Published**: 2022-02-22 10:42:23+00:00
- **Updated**: 2022-10-21 13:57:40+00:00
- **Authors**: Kamil Kowol, Stefan Bracke, Hanno Gottschalk
- **Comment**: None
- **Journal**: None
- **Summary**: The overall goal of this work is to enrich training data for automated driving with so called corner cases. In road traffic, corner cases are critical, rare and unusual situations that challenge the perception by AI algorithms. For this purpose, we present the design of a test rig to generate synthetic corner cases using a human-in-the-loop approach. For the test rig, a real-time semantic segmentation network is trained and integrated into the driving simulation software CARLA in such a way that a human can drive on the network's prediction. In addition, a second person gets to see the same scene from the original CARLA output and is supposed to intervene with the help of a second control unit as soon as the semantic driver shows dangerous driving behavior. Interventions potentially indicate poor recognition of a critical scene by the segmentation network and then represents a corner case. In our experiments, we show that targeted enrichment of training data with corner cases leads to improvements in pedestrian detection in safety relevant episodes in road traffic.



### One-shot Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2202.10824v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10824v2)
- **Published**: 2022-02-22 11:32:59+00:00
- **Updated**: 2022-02-26 03:01:37+00:00
- **Authors**: Yuyu Guo, Jingkuan Song, Lianli Gao, Heng Tao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: As a structured representation of the image content, the visual scene graph (visual relationship) acts as a bridge between computer vision and natural language processing. Existing models on the scene graph generation task notoriously require tens or hundreds of labeled samples. By contrast, human beings can learn visual relationships from a few or even one example. Inspired by this, we design a task named One-Shot Scene Graph Generation, where each relationship triplet (e.g., "dog-has-head") comes from only one labeled example. The key insight is that rather than learning from scratch, one can utilize rich prior knowledge. In this paper, we propose Multiple Structured Knowledge (Relational Knowledge and Commonsense Knowledge) for the one-shot scene graph generation task. Specifically, the Relational Knowledge represents the prior knowledge of relationships between entities extracted from the visual content, e.g., the visual relationships "standing in", "sitting in", and "lying in" may exist between "dog" and "yard", while the Commonsense Knowledge encodes "sense-making" knowledge like "dog can guard yard". By organizing these two kinds of knowledge in a graph structure, Graph Convolution Networks (GCNs) are used to extract knowledge-embedded semantic features of the entities. Besides, instead of extracting isolated visual features from each entity generated by Faster R-CNN, we utilize an Instance Relation Transformer encoder to fully explore their context information. Based on a constructed one-shot dataset, the experimental results show that our method significantly outperforms existing state-of-the-art methods by a large margin. Ablation studies also verify the effectiveness of the Instance Relation Transformer encoder and the Multiple Structured Knowledge.



### Relation Regularized Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2202.10826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10826v1)
- **Published**: 2022-02-22 11:36:49+00:00
- **Updated**: 2022-02-22 11:36:49+00:00
- **Authors**: Yuyu Guo, Lianli Gao, Jingkuan Song, Peng Wang, Nicu Sebe, Heng Tao Shen, Xuelong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Scene graph generation (SGG) is built on top of detected objects to predict object pairwise visual relations for describing the image content abstraction. Existing works have revealed that if the links between objects are given as prior knowledge, the performance of SGG is significantly improved. Inspired by this observation, in this article, we propose a relation regularized network (R2-Net), which can predict whether there is a relationship between two objects and encode this relation into object feature refinement and better SGG. Specifically, we first construct an affinity matrix among detected objects to represent the probability of a relationship between two objects. Graph convolution networks (GCNs) over this relation affinity matrix are then used as object encoders, producing relation-regularized representations of objects. With these relation-regularized features, our R2-Net can effectively refine object labels and generate scene graphs. Extensive experiments are conducted on the visual genome dataset for three SGG tasks (i.e., predicate classification, scene graph classification, and scene graph detection), demonstrating the effectiveness of our proposed method. Ablation studies also verify the key roles of our proposed components in performance improvement.



### Exploiting long-term temporal dynamics for video captioning
- **Arxiv ID**: http://arxiv.org/abs/2202.10828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10828v1)
- **Published**: 2022-02-22 11:40:09+00:00
- **Updated**: 2022-02-22 11:40:09+00:00
- **Authors**: Yuyu Guo, Jingqiu Zhang, Lianli Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically describing videos with natural language is a fundamental challenge for computer vision and natural language processing. Recently, progress in this problem has been achieved through two steps: 1) employing 2-D and/or 3-D Convolutional Neural Networks (CNNs) (e.g. VGG, ResNet or C3D) to extract spatial and/or temporal features to encode video contents; and 2) applying Recurrent Neural Networks (RNNs) to generate sentences to describe events in videos. Temporal attention-based model has gained much progress by considering the importance of each video frame. However, for a long video, especially for a video which consists of a set of sub-events, we should discover and leverage the importance of each sub-shot instead of each frame. In this paper, we propose a novel approach, namely temporal and spatial LSTM (TS-LSTM), which systematically exploits spatial and temporal dynamics within video sequences. In TS-LSTM, a temporal pooling LSTM (TP-LSTM) is designed to incorporate both spatial and temporal information to extract long-term temporal dynamics within video sub-shots; and a stacked LSTM is introduced to generate a list of words to describe the video. Experimental results obtained in two public video captioning benchmarks indicate that our TS-LSTM outperforms the state-of-the-art methods.



### SADN: Learned Light Field Image Compression with Spatial-Angular Decorrelation
- **Arxiv ID**: http://arxiv.org/abs/2202.10837v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10837v1)
- **Published**: 2022-02-22 11:53:52+00:00
- **Updated**: 2022-02-22 11:53:52+00:00
- **Authors**: Kedeng Tong, Xin Jin, Chen Wang, Fan Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Light field image becomes one of the most promising media types for immersive video applications. In this paper, we propose a novel end-to-end spatial-angular-decorrelated network (SADN) for high-efficiency light field image compression. Different from the existing methods that exploit either spatial or angular consistency in the light field image, SADN decouples the angular and spatial information by dilation convolution and stride convolution in spatial-angular interaction, and performs feature fusion to compress spatial and angular information jointly. To train a stable and robust algorithm, a large-scale dataset consisting of 7549 light field images is proposed and built. The proposed method provides 2.137 times and 2.849 times higher compression efficiency relative to H.266/VVC and H.265/HEVC inter coding, respectively. It also outperforms the end-to-end image compression networks by an average of 79.6% bitrate saving with much higher subjective quality and light field consistency.



### UncertaINR: Uncertainty Quantification of End-to-End Implicit Neural Representations for Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2202.10847v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10847v3)
- **Published**: 2022-02-22 12:19:03+00:00
- **Updated**: 2023-05-02 20:59:18+00:00
- **Authors**: Francisca Vasconcelos, Bobby He, Nalini Singh, Yee Whye Teh
- **Comment**: Published in the Transactions on Machine Learning Research (TMLR)
  April 2023 [https://openreview.net/forum?id=jdGMBgYvfX]
- **Journal**: None
- **Summary**: Implicit neural representations (INRs) have achieved impressive results for scene reconstruction and computer graphics, where their performance has primarily been assessed on reconstruction accuracy. As INRs make their way into other domains, where model predictions inform high-stakes decision-making, uncertainty quantification of INR inference is becoming critical. To that end, we study a Bayesian reformulation of INRs, UncertaINR, in the context of computed tomography, and evaluate several Bayesian deep learning implementations in terms of accuracy and calibration. We find that they achieve well-calibrated uncertainty, while retaining accuracy competitive with other classical, INR-based, and CNN-based reconstruction techniques. Contrary to common intuition in the Bayesian deep learning literature, we find that INRs obtain the best calibration with computationally efficient Monte Carlo dropout, outperforming Hamiltonian Monte Carlo and deep ensembles. Moreover, in contrast to the best-performing prior approaches, UncertaINR does not require a large training dataset, but only a handful of validation images.



### Deep learning classification of large-scale point clouds: A case study on cuneiform tablets
- **Arxiv ID**: http://arxiv.org/abs/2202.10851v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10851v2)
- **Published**: 2022-02-22 12:24:47+00:00
- **Updated**: 2022-07-11 07:57:41+00:00
- **Authors**: Frederik Hagelskjaer
- **Comment**: 5 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: This paper introduces a novel network architecture for the classification of large-scale point clouds. The network is used to classify metadata from cuneiform tablets. As more than half a million tablets remain unprocessed, this can help create an overview of the tablets. The network is tested on a comparison dataset and obtains state-of-the-art performance. We also introduce new metadata classification tasks on which the network shows promising results. Finally, we introduce the novel Maximum Attention visualization, demonstrating that the trained network focuses on the intended features. Code available at https://github.com/fhagelskjaer/dlc-cuneiform



### Data-Consistent Local Superresolution for Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2202.10875v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2202.10875v1)
- **Published**: 2022-02-22 13:18:38+00:00
- **Updated**: 2022-02-22 13:18:38+00:00
- **Authors**: Junqi Tang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we propose a new paradigm of iterative model-based reconstruction algorithms for providing real-time solution for zooming-in and refining a region of interest in medical and clinical tomographic (such as CT/MRI/PET, etc) images. This algorithmic framework is tailor for a clinical need in medical imaging practice, that after a reconstruction of the full tomographic image, the clinician may believe that some critical parts of the image are not clear enough, and may wish to see clearer these regions-of-interest. A naive approach (which is highly not recommended) would be performing the global reconstruction of a higher resolution image, which has two major limitations: firstly, it is computationally inefficient, and secondly, the image regularization is still applied globally which may over-smooth some local regions. Furthermore if one wish to fine-tune the regularization parameter for local parts, it would be computationally infeasible in practice for the case of using global reconstruction. Our new iterative approaches for such tasks are based on jointly utilizing the measurement information, efficient upsampling/downsampling across image spaces, and locally adjusted image prior for efficient and high-quality post-processing. The numerical results in low-dose X-ray CT image local zoom-in demonstrate the effectiveness of our approach.



### Coordinate-Aligned Multi-Camera Collaboration for Active Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2202.10881v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2202.10881v1)
- **Published**: 2022-02-22 13:28:40+00:00
- **Updated**: 2022-02-22 13:28:40+00:00
- **Authors**: Zeyu Fang, Jian Zhao, Mingyu Yang, Wengang Zhou, Zhenbo Lu, Houqiang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Active Multi-Object Tracking (AMOT) is a task where cameras are controlled by a centralized system to adjust their poses automatically and collaboratively so as to maximize the coverage of targets in their shared visual field. In AMOT, each camera only receives partial information from its observation, which may mislead cameras to take locally optimal action. Besides, the global goal, i.e., maximum coverage of objects, is hard to be directly optimized. To address the above issues, we propose a coordinate-aligned multi-camera collaboration system for AMOT. In our approach, we regard each camera as an agent and address AMOT with a multi-agent reinforcement learning solution. To represent the observation of each agent, we first identify the targets in the camera view with an image detector, and then align the coordinates of the targets in 3D environment. We define the reward of each agent based on both global coverage as well as four individual reward terms. The action policy of the agents is derived with a value-based Q-network. To the best of our knowledge, we are the first to study the AMOT task. To train and evaluate the efficacy of our system, we build a virtual yet credible 3D environment, named "Soccer Court", to mimic the real-world AMOT scenario. The experimental results show that our system achieves a coverage of 71.88%, outperforming the baseline method by 8.9%.



### HiP: Hierarchical Perceiver
- **Arxiv ID**: http://arxiv.org/abs/2202.10890v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10890v2)
- **Published**: 2022-02-22 13:39:14+00:00
- **Updated**: 2022-11-03 18:34:02+00:00
- **Authors**: Joao Carreira, Skanda Koppula, Daniel Zoran, Adria Recasens, Catalin Ionescu, Olivier Henaff, Evan Shelhamer, Relja Arandjelovic, Matt Botvinick, Oriol Vinyals, Karen Simonyan, Andrew Zisserman, Andrew Jaegle
- **Comment**: None
- **Journal**: None
- **Summary**: General perception systems such as Perceivers can process arbitrary modalities in any combination and are able to handle up to a few hundred thousand inputs. They achieve this generality by using exclusively global attention operations. This however hinders them from scaling up to the inputs sizes required to process raw high-resolution images or video. In this paper, we show that some degree of locality can be introduced back into these models, greatly improving their efficiency while preserving their generality. To scale them further, we introduce a self-supervised approach that enables learning dense low-dimensional positional embeddings for very large signals. We call the resulting model a Hierarchical Perceiver (HiP). In sum our contributions are: 1) scaling Perceiver-type models to raw high-resolution images and audio+video, 2) showing the feasibility of learning 1M+ positional embeddings from scratch using masked auto-encoding, 3) demonstrating competitive performance on raw data from ImageNet, AudioSet, PASCAL VOC, ModelNet40 and Kinetics datasets with the same exact, unchanged model and without specialized preprocessing or any tokenization.



### Sound Adversarial Audio-Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/2202.10910v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.RO, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2202.10910v1)
- **Published**: 2022-02-22 14:19:42+00:00
- **Updated**: 2022-02-22 14:19:42+00:00
- **Authors**: Yinfeng Yu, Wenbing Huang, Fuchun Sun, Changan Chen, Yikai Wang, Xiaohong Liu
- **Comment**: This work aims to do an adversarial sound intervention for robust
  audio-visual navigation
- **Journal**: None
- **Summary**: Audio-visual navigation task requires an agent to find a sound source in a realistic, unmapped 3D environment by utilizing egocentric audio-visual observations. Existing audio-visual navigation works assume a clean environment that solely contains the target sound, which, however, would not be suitable in most real-world applications due to the unexpected sound noise or intentional interference. In this work, we design an acoustically complex environment in which, besides the target sound, there exists a sound attacker playing a zero-sum game with the agent. More specifically, the attacker can move and change the volume and category of the sound to make the agent suffer from finding the sounding object while the agent tries to dodge the attack and navigate to the goal under the intervention. Under certain constraints to the attacker, we can improve the robustness of the agent towards unexpected sound attacks in audio-visual navigation. For better convergence, we develop a joint training mechanism by employing the property of a centralized critic with decentralized actors. Experiments on two real-world 3D scan datasets, Replica, and Matterport3D, verify the effectiveness and the robustness of the agent trained under our designed environment when transferred to the clean environment or the one containing sound attackers with random policy. Project: \url{https://yyf17.github.io/SAAVN}.



### Comparing Controller With the Hand Gestures Pinch and Grab for Picking Up and Placing Virtual Objects
- **Arxiv ID**: http://arxiv.org/abs/2202.10964v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10964v1)
- **Published**: 2022-02-22 15:12:06+00:00
- **Updated**: 2022-02-22 15:12:06+00:00
- **Authors**: Alexander Schäfer, Gerd Reis, Didier Stricker
- **Comment**: None
- **Journal**: None
- **Summary**: Grabbing virtual objects is one of the essential tasks for Augmented, Virtual, and Mixed Reality applications. Modern applications usually use a simple pinch gesture for grabbing and moving objects. However, picking up objects by pinching has disadvantages. It can be an unnatural gesture to pick up objects and prevents the implementation of other gestures which would be performed with thumb and index. Therefore it is not the optimal choice for many applications. In this work, different implementations for grabbing and placing virtual objects are proposed and compared. Performance and accuracy of the proposed techniques are measured and compared.



### Evaluating Feature Attribution Methods in the Image Domain
- **Arxiv ID**: http://arxiv.org/abs/2202.12270v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12270v1)
- **Published**: 2022-02-22 15:14:33+00:00
- **Updated**: 2022-02-22 15:14:33+00:00
- **Authors**: Arne Gevaert, Axel-Jan Rousseau, Thijs Becker, Dirk Valkenborg, Tijl De Bie, Yvan Saeys
- **Comment**: None
- **Journal**: None
- **Summary**: Feature attribution maps are a popular approach to highlight the most important pixels in an image for a given prediction of a model. Despite a recent growth in popularity and available methods, little attention is given to the objective evaluation of such attribution maps. Building on previous work in this domain, we investigate existing metrics and propose new variants of metrics for the evaluation of attribution maps. We confirm a recent finding that different attribution metrics seem to measure different underlying concepts of attribution maps, and extend this finding to a larger selection of attribution metrics. We also find that metric results on one dataset do not necessarily generalize to other datasets, and methods with desirable theoretical properties such as DeepSHAP do not necessarily outperform computationally cheaper alternatives. Based on these findings, we propose a general benchmarking approach to identify the ideal feature attribution method for a given use case. Implementations of attribution metrics and our experiments are available online.



### Improving Classification Model Performance on Chest X-Rays through Lung Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.10971v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10971v1)
- **Published**: 2022-02-22 15:24:06+00:00
- **Updated**: 2022-02-22 15:24:06+00:00
- **Authors**: Hilda Azimi, Jianxing Zhang, Pengcheng Xi, Hala Asad, Ashkan Ebadi, Stephane Tremblay, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Chest radiography is an effective screening tool for diagnosing pulmonary diseases. In computer-aided diagnosis, extracting the relevant region of interest, i.e., isolating the lung region of each radiography image, can be an essential step towards improved performance in diagnosing pulmonary disorders. Methods: In this work, we propose a deep learning approach to enhance abnormal chest x-ray (CXR) identification performance through segmentations. Our approach is designed in a cascaded manner and incorporates two modules: a deep neural network with criss-cross attention modules (XLSor) for localizing lung region in CXR images and a CXR classification model with a backbone of a self-supervised momentum contrast (MoCo) model pre-trained on large-scale CXR data sets. The proposed pipeline is evaluated on Shenzhen Hospital (SH) data set for the segmentation module, and COVIDx data set for both segmentation and classification modules. Novel statistical analysis is conducted in addition to regular evaluation metrics for the segmentation module. Furthermore, the results of the optimized approach are analyzed with gradient-weighted class activation mapping (Grad-CAM) to investigate the rationale behind the classification decisions and to interpret its choices. Results and Conclusion: Different data sets, methods, and scenarios for each module of the proposed pipeline are examined for designing an optimized approach, which has achieved an accuracy of 0.946 in distinguishing abnormal CXR images (i.e., Pneumonia and COVID-19) from normal ones. Numerical and visual validations suggest that applying automated segmentation as a pre-processing step for classification improves the generalization capability and the performance of the classification models.



### Estimation of Looming from LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2202.10972v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.10972v2)
- **Published**: 2022-02-22 15:26:20+00:00
- **Updated**: 2022-03-15 15:49:56+00:00
- **Authors**: Juan D. Yepes, Daniel Raviv
- **Comment**: None
- **Journal**: None
- **Summary**: Looming, traditionally defined as the relative expansion of objects in the observer's retina, is a fundamental visual cue for perception of threat and can be used to accomplish collision free navigation. The measurement of the looming cue is not only limited to vision, and can also be obtained from range sensors like LiDAR (Light Detection and Ranging). In this article we present two methods that process raw LiDAR data to estimate the looming cue. Using looming values we show how to obtain threat zones for collision avoidance tasks. The methods are general enough to be suitable for any six-degree-of-freedom motion and can be implemented in real-time without the need for fine matching, point-cloud registration, object classification or object segmentation. Quantitative results using the KITTI dataset shows advantages and limitations of the methods.



### The Winning Solution to the iFLYTEK Challenge 2021 Cultivated Land Extraction from High-Resolution Remote Sensing Image
- **Arxiv ID**: http://arxiv.org/abs/2202.10974v3
- **DOI**: 10.1109/ICACI55529.2022.9837765
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10974v3)
- **Published**: 2022-02-22 15:28:59+00:00
- **Updated**: 2022-02-25 05:36:33+00:00
- **Authors**: Zhen Zhao, Yuqiu Liu, Gang Zhang, Liang Tang, Xiaolin Hu
- **Comment**: None
- **Journal**: 2022 14th International Conference on Advanced Computational
  Intelligence (ICACI)
- **Summary**: Extracting cultivated land accurately from high-resolution remote images is a basic task for precision agriculture. This report introduces our solution to the iFLYTEK challenge 2021 cultivated land extraction from high-resolution remote sensing image. The challenge requires segmenting cultivated land objects in very high-resolution multispectral remote sensing images. We established a highly effective and efficient pipeline to solve this problem. We first divided the original images into small tiles and separately performed instance segmentation on each tile. We explored several instance segmentation algorithms that work well on natural images and developed a set of effective methods that are applicable to remote sensing images. Then we merged the prediction results of all small tiles into seamless, continuous segmentation results through our proposed overlap-tile fusion strategy. We achieved the first place among 486 teams in the challenge.



### Tracking perovskite crystallization via deep learning-based feature detection on 2D X-ray scattering data
- **Arxiv ID**: http://arxiv.org/abs/2202.10983v1
- **DOI**: 10.1038/s41524-022-00778-8
- **Categories**: **cs.CV**, cond-mat.soft
- **Links**: [PDF](http://arxiv.org/pdf/2202.10983v1)
- **Published**: 2022-02-22 15:39:00+00:00
- **Updated**: 2022-02-22 15:39:00+00:00
- **Authors**: Vladimir Starostin, Valentin Munteanu, Alessandro Greco, Ekaterina Kneschaurek, Alina Pleli, Florian Bertram, Alexander Gerlach, Alexander Hinderhofer, Frank Schreiber
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding the processes of perovskite crystallization is essential for improving the properties of organic solar cells. In situ real-time grazing-incidence X-ray diffraction (GIXD) is a key technique for this task, but it produces large amounts of data, frequently exceeding the capabilities of traditional data processing methods. We propose an automated pipeline for the analysis of GIXD images, based on the Faster R-CNN deep learning architecture for object detection, modified to conform to the specifics of the scattering data. The model exhibits high accuracy in detecting diffraction features on noisy patterns with various experimental artifacts. We demonstrate our method on real-time tracking of organic-inorganic perovskite structure crystallization and test it on two applications: 1. the automated phase identification and unit-cell determination of two coexisting phases of Ruddlesden-Popper 2D perovskites, and 2. the fast tracking of MAPbI$_3$ perovskite formation. By design, our approach is equally suitable for other crystalline thin-film materials.



### Does prior knowledge in the form of multiple low-dose PET images (at different dose levels) improve standard-dose PET prediction?
- **Arxiv ID**: http://arxiv.org/abs/2202.10998v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10998v1)
- **Published**: 2022-02-22 15:58:32+00:00
- **Updated**: 2022-02-22 15:58:32+00:00
- **Authors**: Behnoush Sanaei, Reza Faghihi, Hossein Arabi
- **Comment**: None
- **Journal**: None
- **Summary**: Reducing the injected dose would result in quality degradation and loss of information in PET imaging. To address this issue, deep learning methods have been introduced to predict standard PET images (S-PET) from the corresponding low-dose versions (L-PET). The existing deep learning-based denoising methods solely rely on a single dose level of PET images to predict the S-PET images. In this work, we proposed to exploit the prior knowledge in the form of multiple low-dose levels of PET images (in addition to the target low-dose level) to estimate the S-PET images.



### Multi-Objective Dual Simplex-Mesh Based Deformable Image Registration for 3D Medical Images -- Proof of Concept
- **Arxiv ID**: http://arxiv.org/abs/2202.11001v1
- **DOI**: 10.1117/12.2605498
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2202.11001v1)
- **Published**: 2022-02-22 16:07:29+00:00
- **Updated**: 2022-02-22 16:07:29+00:00
- **Authors**: Georgios Andreadis, Peter A. N. Bosman, Tanja Alderliesten
- **Comment**: To be published in the Proceedings of SPIE Medical Imaging 2022:
  Image Processing
- **Journal**: Proceedings Volume 12032, Medical Imaging 2022: Image Processing;
  120322T (2022)
- **Summary**: Reliably and physically accurately transferring information between images through deformable image registration with large anatomical differences is an open challenge in medical image analysis. Most existing methods have two key shortcomings: first, they require extensive up-front parameter tuning to each specific registration problem, and second, they have difficulty capturing large deformations and content mismatches between images. There have however been developments that have laid the foundation for potential solutions to both shortcomings. Towards the first shortcoming, a multi-objective optimization approach using the Real-Valued Gene-pool Optimal Mixing Evolutionary Algorithm (RV-GOMEA) has been shown to be capable of producing a diverse set of registrations for 2D images in one run of the algorithm, representing different trade-offs between conflicting objectives in the registration problem. This allows the user to select a registration afterwards and removes the need for up-front tuning. Towards the second shortcoming, a dual-dynamic grid transformation model has proven effective at capturing large differences in 2D images. These two developments have recently been accelerated through GPU parallelization, delivering large speed-ups. Based on this accelerated version, it is now possible to extend the approach to 3D images. Concordantly, this work introduces the first method for multi-objective 3D deformable image registration, using a 3D dual-dynamic grid transformation model based on simplex meshes while still supporting the incorporation of annotated guidance information and multi-resolution schemes. Our proof-of-concept prototype shows promising results on synthetic and clinical 3D registration problems, forming the foundation for a new, insightful method that can include bio-mechanical properties in the registration.



### Statistical and Spatio-temporal Hand Gesture Features for Sign Language Recognition using the Leap Motion Sensor
- **Arxiv ID**: http://arxiv.org/abs/2202.11005v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.11005v1)
- **Published**: 2022-02-22 16:16:47+00:00
- **Updated**: 2022-02-22 16:16:47+00:00
- **Authors**: Jordan J. Bird
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: In modern society, people should not be identified based on their disability, rather, it is environments that can disable people with impairments. Improvements to automatic Sign Language Recognition (SLR) will lead to more enabling environments via digital technology. Many state-of-the-art approaches to SLR focus on the classification of static hand gestures, but communication is a temporal activity, which is reflected by many of the dynamic gestures present. Given this, temporal information during the delivery of a gesture is not often considered within SLR. The experiments in this work consider the problem of SL gesture recognition regarding how dynamic gestures change during their delivery, and this study aims to explore how single types of features as well as mixed features affect the classification ability of a machine learning model. 18 common gestures recorded via a Leap Motion Controller sensor provide a complex classification problem. Two sets of features are extracted from a 0.6 second time window, statistical descriptors and spatio-temporal attributes. Features from each set are compared by their ANOVA F-Scores and p-values, arranged into bins grown by 10 features per step to a limit of the 250 highest-ranked features. Results show that the best statistical model selected 240 features and scored 85.96% accuracy, the best spatio-temporal model selected 230 features and scored 80.98%, and the best mixed-feature model selected 240 features from each set leading to a classification accuracy of 86.75%. When all three sets of results are compared (146 individual machine learning models), the overall distribution shows that the minimum results are increased when inputs are any number of mixed features compared to any number of either of the two single sets of features.



### Computing Multiple Image Reconstructions with a Single Hypernetwork
- **Arxiv ID**: http://arxiv.org/abs/2202.11009v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.11009v5)
- **Published**: 2022-02-22 16:27:23+00:00
- **Updated**: 2022-06-09 01:57:13+00:00
- **Authors**: Alan Q. Wang, Adrian V. Dalca, Mert R. Sabuncu
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://www.melba-journal.org/papers/2022:017.html
- **Journal**: None
- **Summary**: Deep learning based techniques achieve state-of-the-art results in a wide range of image reconstruction tasks like compressed sensing. These methods almost always have hyperparameters, such as the weight coefficients that balance the different terms in the optimized loss function. The typical approach is to train the model for a hyperparameter setting determined with some empirical or theoretical justification. Thus, at inference time, the model can only compute reconstructions corresponding to the pre-determined hyperparameter values. In this work, we present a hypernetwork-based approach, called HyperRecon, to train reconstruction models that are agnostic to hyperparameter settings. At inference time, HyperRecon can efficiently produce diverse reconstructions, which would each correspond to different hyperparameter values. In this framework, the user is empowered to select the most useful output(s) based on their own judgement. We demonstrate our method in compressed sensing, super-resolution and denoising tasks, using two large-scale and publicly-available MRI datasets. Our code is available at https://github.com/alanqrwang/hyperrecon.



### Constrained Visual-Inertial Localization With Application And Benchmark in Laparoscopic Surgery
- **Arxiv ID**: http://arxiv.org/abs/2202.11075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11075v1)
- **Published**: 2022-02-22 18:01:55+00:00
- **Updated**: 2022-02-22 18:01:55+00:00
- **Authors**: Regine Hartwig, Daniel Ostler, Jean-Claude Rosenthal, Hubertus Feußner, Dirk Wilhelm, Dirk Wollherr
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel method to tackle the visual-inertial localization problem for constrained camera movements. We use residuals from the different modalities to jointly optimize a global cost function. The residuals emerge from IMU measurements, stereoscopic feature points, and constraints on possible solutions in SE(3). In settings where dynamic disturbances are frequent, the residuals reduce the complexity of the problem and make localization feasible. We verify the advantages of our method in a suitable medical use case and produce a dataset capturing a minimally invasive surgery in the abdomen. Our novel clinical dataset MITI is comparable to state-of-the-art evaluation datasets, contains calibration and synchronization and is available at https://mediatum.ub.tum.de/1621941.



### Learning from the Pros: Extracting Professional Goalkeeper Technique from Broadcast Footage
- **Arxiv ID**: http://arxiv.org/abs/2202.12259v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.12259v1)
- **Published**: 2022-02-22 18:17:30+00:00
- **Updated**: 2022-02-22 18:17:30+00:00
- **Authors**: Matthew Wear, Ryan Beal, Tim Matthews, Tim Norman, Sarvapali Ramchurn
- **Comment**: 17 pages, 15 figures, MIT Sloan Sports Analytics Conference, March
  4-5 2022, Boston, USA
- **Journal**: None
- **Summary**: As an amateur goalkeeper playing grassroots soccer, who better to learn from than top professional goalkeepers? In this paper, we harness computer vision and machine learning models to appraise the save technique of professionals in a way those at lower levels can learn from. We train an unsupervised machine learning model using 3D body pose data extracted from broadcast footage to learn professional goalkeeper technique. Then, an "expected saves" model is developed, from which we can identify the optimal goalkeeper technique in different match contexts.



### ReorientBot: Learning Object Reorientation for Specific-Posed Placement
- **Arxiv ID**: http://arxiv.org/abs/2202.11092v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.11092v1)
- **Published**: 2022-02-22 18:46:52+00:00
- **Updated**: 2022-02-22 18:46:52+00:00
- **Authors**: Kentaro Wada, Stephen James, Andrew J. Davison
- **Comment**: 7 pages, 6 figures, IEEE International Conference on Robotics and
  Automation (ICRA) 2022
- **Journal**: None
- **Summary**: Robots need the capability of placing objects in arbitrary, specific poses to rearrange the world and achieve various valuable tasks. Object reorientation plays a crucial role in this as objects may not initially be oriented such that the robot can grasp and then immediately place them in a specific goal pose. In this work, we present a vision-based manipulation system, ReorientBot, which consists of 1) visual scene understanding with pose estimation and volumetric reconstruction using an onboard RGB-D camera; 2) learned waypoint selection for successful and efficient motion generation for reorientation; 3) traditional motion planning to generate a collision-free trajectory from the selected waypoints. We evaluate our method using the YCB objects in both simulation and the real world, achieving 93% overall success, 81% improvement in success rate, and 22% improvement in execution time compared to a heuristic approach. We demonstrate extended multi-object rearrangement showing the general capability of the system.



### GroupViT: Semantic Segmentation Emerges from Text Supervision
- **Arxiv ID**: http://arxiv.org/abs/2202.11094v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11094v5)
- **Published**: 2022-02-22 18:56:04+00:00
- **Updated**: 2022-07-18 05:04:01+00:00
- **Authors**: Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang
- **Comment**: CVPR 2022. Project page and code: https://jerryxu.net/GroupViT
- **Journal**: None
- **Summary**: Grouping and recognition are important components of visual scene understanding, e.g., for object detection and semantic segmentation. With end-to-end deep learning systems, grouping of image regions usually happens implicitly via top-down supervision from pixel-level recognition labels. Instead, in this paper, we propose to bring back the grouping mechanism into deep networks, which allows semantic segments to emerge automatically with only text supervision. We propose a hierarchical Grouping Vision Transformer (GroupViT), which goes beyond the regular grid structure representation and learns to group image regions into progressively larger arbitrary-shaped segments. We train GroupViT jointly with a text encoder on a large-scale image-text dataset via contrastive losses. With only text supervision and without any pixel-level annotations, GroupViT learns to group together semantic regions and successfully transfers to the task of semantic segmentation in a zero-shot manner, i.e., without any further fine-tuning. It achieves a zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on PASCAL Context datasets, and performs competitively to state-of-the-art transfer-learning methods requiring greater levels of supervision. We open-source our code at https://github.com/NVlabs/GroupViT .



### Learning with Free Object Segments for Long-Tailed Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.11124v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11124v3)
- **Published**: 2022-02-22 19:06:16+00:00
- **Updated**: 2022-10-05 00:19:04+00:00
- **Authors**: Cheng Zhang, Tai-Yu Pan, Tianle Chen, Jike Zhong, Wenjin Fu, Wei-Lun Chao
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: One fundamental challenge in building an instance segmentation model for a large number of classes in complex scenes is the lack of training examples, especially for rare objects. In this paper, we explore the possibility to increase the training examples without laborious data collection and annotation. We find that an abundance of instance segments can potentially be obtained freely from object-centric images, according to two insights: (i) an object-centric image usually contains one salient object in a simple background; (ii) objects from the same class often share similar appearances or similar contrasts to the background. Motivated by these insights, we propose a simple and scalable framework FreeSeg for extracting and leveraging these "free" object foreground segments to facilitate model training in long-tailed instance segmentation. Concretely, we investigate the similarity among object-centric images of the same class to propose candidate segments of foreground instances, followed by a novel ranking of segment quality. The resulting high-quality object segments can then be used to augment the existing long-tailed datasets, e.g., by copying and pasting the segments onto the original training images. Extensive experiments show that FreeSeg yields substantial improvements on top of strong baselines and achieves state-of-the-art accuracy for segmenting rare object categories.



### Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.11202v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.11202v3)
- **Published**: 2022-02-22 22:31:57+00:00
- **Updated**: 2023-03-09 05:16:49+00:00
- **Authors**: Hao He, Kaiwen Zha, Dina Katabi
- **Comment**: ICLR 2023 Spotlight (notable top 25%). The first two authors
  contributed equally to this paper
- **Journal**: None
- **Summary**: Indiscriminate data poisoning attacks are quite effective against supervised learning. However, not much is known about their impact on unsupervised contrastive learning (CL). This paper is the first to consider indiscriminate poisoning attacks of contrastive learning. We propose Contrastive Poisoning (CP), the first effective such attack on CL. We empirically show that Contrastive Poisoning, not only drastically reduces the performance of CL algorithms, but also attacks supervised learning models, making it the most generalizable indiscriminate poisoning attack. We also show that CL algorithms with a momentum encoder are more robust to indiscriminate poisoning, and propose a new countermeasure based on matrix completion. Code is available at: https://github.com/kaiwenzha/contrastive-poisoning.



### Arbitrary Shape Text Detection using Transformers
- **Arxiv ID**: http://arxiv.org/abs/2202.11221v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.11221v1)
- **Published**: 2022-02-22 22:36:29+00:00
- **Updated**: 2022-02-22 22:36:29+00:00
- **Authors**: Zobeir Raisi, Georges Younes, John Zelek
- **Comment**: None
- **Journal**: None
- **Summary**: Recent text detection frameworks require several handcrafted components such as anchor generation, non-maximum suppression (NMS), or multiple processing stages (e.g. label generation) to detect arbitrarily shaped text images. In contrast, we propose an end-to-end trainable architecture based on Detection using Transformers (DETR), that outperforms previous state-of-the-art methods in arbitrary-shaped text detection. At its core, our proposed method leverages a bounding box loss function that accurately measures the arbitrary detected text regions' changes in scale and aspect ratio. This is possible due to a hybrid shape representation made from Bezier curves, that are further split into piece-wise polygons. The proposed loss function is then a combination of a generalized-split-intersection-over-union loss defined over the piece-wise polygons and regularized by a Smooth-$\ln$ regression over the Bezier curve's control points. We evaluate our proposed model using Total-Text and CTW-1500 datasets for curved text, and MSRA-TD500 and ICDAR15 datasets for multi-oriented text, and show that the proposed method outperforms the previous state-of-the-art methods in arbitrary-shape text detection tasks.



### Enabling Efficient Deep Convolutional Neural Network-based Sensor Fusion for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2202.11231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11231v1)
- **Published**: 2022-02-22 23:35:30+00:00
- **Updated**: 2022-02-22 23:35:30+00:00
- **Authors**: Xiaoming Zeng, Zhendong Wang, Yang Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous driving demands accurate perception and safe decision-making. To achieve this, automated vehicles are now equipped with multiple sensors (e.g., camera, Lidar, etc.), enabling them to exploit complementary environmental context by fusing data from different sensing modalities. With the success of Deep Convolutional Neural Network(DCNN), the fusion between DCNNs has been proved as a promising strategy to achieve satisfactory perception accuracy. However, mainstream existing DCNN fusion schemes conduct fusion by directly element-wisely adding feature maps extracted from different modalities together at various stages, failing to consider whether the features being fused are matched or not. Therefore, we first propose a feature disparity metric to quantitatively measure the degree of feature disparity between the feature maps being fused. We then propose Fusion-filter as a feature-matching techniques to tackle the feature-mismatching issue. We also propose a Layer-sharing technique in the deep layer that can achieve better accuracy with less computational overhead. Together with the help of the feature disparity to be an additional loss, our proposed technologies enable DCNN to learn corresponding feature maps with similar characteristics and complementary visual context from different modalities to achieve better accuracy. Experimental results demonstrate that our proposed fusion technique can achieve better accuracy on KITTI dataset with less computational resources demand.



### Retrieval Augmented Classification for Long-Tail Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.11233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11233v1)
- **Published**: 2022-02-22 23:40:51+00:00
- **Updated**: 2022-02-22 23:40:51+00:00
- **Authors**: Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg, Alan Blair, Chunhua Shen, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Retrieval Augmented Classification (RAC), a generic approach to augmenting standard image classification pipelines with an explicit retrieval module. RAC consists of a standard base image encoder fused with a parallel retrieval branch that queries a non-parametric external memory of pre-encoded images and associated text snippets. We apply RAC to the problem of long-tail classification and demonstrate a significant improvement over previous state-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7% respectively), despite using only the training datasets themselves as the external information source. We demonstrate that RAC's retrieval module, without prompting, learns a high level of accuracy on tail classes. This, in turn, frees the base encoder to focus on common classes, and improve its performance thereon. RAC represents an alternative approach to utilizing large, pretrained models without requiring fine-tuning, as well as a first step towards more effectively making use of external memory within common computer vision architectures.



