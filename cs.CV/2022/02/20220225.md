# Arxiv Papers in cs.CV on 2022-02-25
### Understanding Adversarial Robustness from Feature Maps of Convolutional Layers
- **Arxiv ID**: http://arxiv.org/abs/2202.12435v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12435v1)
- **Published**: 2022-02-25 00:14:59+00:00
- **Updated**: 2022-02-25 00:14:59+00:00
- **Authors**: Cong Xu, Min Yang
- **Comment**: 10pages
- **Journal**: None
- **Summary**: The adversarial robustness of a neural network mainly relies on two factors, one is the feature representation capacity of the network, and the other is its resistance ability to perturbations. In this paper, we study the anti-perturbation ability of the network from the feature maps of convolutional layers. Our theoretical analysis discovers that larger convolutional features before average pooling can contribute to better resistance to perturbations, but the conclusion is not true for max pooling. Based on the theoretical findings, we present two feasible ways to improve the robustness of existing neural networks. The proposed approaches are very simple and only require upsampling the inputs or modifying the stride configuration of convolution operators. We test our approaches on several benchmark neural network architectures, including AlexNet, VGG16, RestNet18 and PreActResNet18, and achieve non-trivial improvements on both natural accuracy and robustness under various attacks. Our study brings new insights into the design of robust neural networks. The code is available at \url{https://github.com/MTandHJ/rcm}.



### Structure-aware Unsupervised Tagged-to-Cine MRI Synthesis with Self Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2202.12474v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12474v1)
- **Published**: 2022-02-25 03:09:46+00:00
- **Updated**: 2022-02-25 03:09:46+00:00
- **Authors**: Xiaofeng Liu, Fangxu Xing, Jerry L. Prince, Maureen Stone, Georges El Fakhri, Jonghye Woo
- **Comment**: SPIE Medical Imaging: Image Processing (Oral presentation)
- **Journal**: None
- **Summary**: Cycle reconstruction regularized adversarial training -- e.g., CycleGAN, DiscoGAN, and DualGAN -- has been widely used for image style transfer with unpaired training data. Several recent works, however, have shown that local distortions are frequent, and structural consistency cannot be guaranteed. Targeting this issue, prior works usually relied on additional segmentation or consistent feature extraction steps that are task-specific. To counter this, this work aims to learn a general add-on structural feature extractor, by explicitly enforcing the structural alignment between an input and its synthesized image. Specifically, we propose a novel input-output image patches self-training scheme to achieve a disentanglement of underlying anatomical structures and imaging modalities. The translator and structure encoder are updated, following an alternating training protocol. In addition, the information w.r.t. imaging modality can be eliminated with an asymmetric adversarial game. We train, validate, and test our network on 1,768, 416, and 1,560 unpaired subject-independent slices of tagged and cine magnetic resonance imaging from a total of twenty healthy subjects, respectively, demonstrating superior performance over competing methods.



### Learn From the Past: Experience Ensemble Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2202.12488v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12488v1)
- **Published**: 2022-02-25 04:05:09+00:00
- **Updated**: 2022-02-25 04:05:09+00:00
- **Authors**: Chaofei Wang, Shaowei Zhang, Shiji Song, Gao Huang
- **Comment**: Under review
- **Journal**: None
- **Summary**: Traditional knowledge distillation transfers "dark knowledge" of a pre-trained teacher network to a student network, and ignores the knowledge in the training process of the teacher, which we call teacher's experience. However, in realistic educational scenarios, learning experience is often more important than learning results. In this work, we propose a novel knowledge distillation method by integrating the teacher's experience for knowledge transfer, named experience ensemble knowledge distillation (EEKD). We save a moderate number of intermediate models from the training process of the teacher model uniformly, and then integrate the knowledge of these intermediate models by ensemble technique. A self-attention module is used to adaptively assign weights to different intermediate models in the process of knowledge transfer. Three principles of constructing EEKD on the quality, weights and number of intermediate models are explored. A surprising conclusion is found that strong ensemble teachers do not necessarily produce strong students. The experimental results on CIFAR-100 and ImageNet show that EEKD outperforms the mainstream knowledge distillation methods and achieves the state-of-the-art. In particular, EEKD even surpasses the standard ensemble distillation on the premise of saving training cost.



### Monogenic Wavelet Scattering Network for Texture Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2202.12491v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, math.CV, 94A08, 30A05, 68T10, 42C40, I.4.7; I.5.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2202.12491v1)
- **Published**: 2022-02-25 04:20:16+00:00
- **Updated**: 2022-02-25 04:20:16+00:00
- **Authors**: Wai Ho Chak, Naoki Saito
- **Comment**: None
- **Journal**: None
- **Summary**: The scattering transform network (STN), which has a similar structure as that of a popular convolutional neural network except its use of predefined convolution filters and a small number of layers, can generates a robust representation of an input signal relative to small deformations. We propose a novel Monogenic Wavelet Scattering Network (MWSN) for 2D texture image classification through a cascade of monogenic wavelet filtering with nonlinear modulus and averaging operators by replacing the 2D Morlet wavelet filtering in the standard STN. Our MWSN can extract useful hierarchical and directional features with interpretable coefficients, which can be further compressed by PCA and fed into a classifier. Using the CUReT texture image database, we demonstrate the superior performance of our MWSN over the standard STN. This performance improvement can be explained by the natural extension of 1D analyticity to 2D monogenicity.



### Diffeomorphic Image Registration with Neural Velocity Field
- **Arxiv ID**: http://arxiv.org/abs/2202.12498v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.12498v5)
- **Published**: 2022-02-25 05:04:29+00:00
- **Updated**: 2022-11-02 03:06:37+00:00
- **Authors**: Kun Han, Shanlin sun, Xiangyi Yan, Chenyu You, Hao Tang, Junayed Naushad, Haoyu Ma, Deying Kong, Xiaohui Xie
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: Diffeomorphic image registration, offering smooth transformation and topology preservation, is required in many medical image analysis tasks.Traditional methods impose certain modeling constraints on the space of admissible transformations and use optimization to find the optimal transformation between two images. Specifying the right space of admissible transformations is challenging: the registration quality can be poor if the space is too restrictive, while the optimization can be hard to solve if the space is too general. Recent learning-based methods, utilizing deep neural networks to learn the transformation directly, achieve fast inference, but face challenges in accuracy due to the difficulties in capturing the small local deformations and generalization ability. Here we propose a new optimization-based method named DNVF (Diffeomorphic Image Registration with Neural Velocity Field) which utilizes deep neural network to model the space of admissible transformations. A multilayer perceptron (MLP) with sinusoidal activation function is used to represent the continuous velocity field and assigns a velocity vector to every point in space, providing the flexibility of modeling complex deformations as well as the convenience of optimization. Moreover, we propose a cascaded image registration framework (Cas-DNVF) by combining the benefits of both optimization and learning based methods, where a fully convolutional neural network (FCN) is trained to predict the initial deformation, followed by DNVF for further refinement. Experiments on two large-scale 3D MR brain scan datasets demonstrate that our proposed methods significantly outperform the state-of-the-art registration methods.



### RRL:Regional Rotation Layer in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.12509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.12509v1)
- **Published**: 2022-02-25 06:07:53+00:00
- **Updated**: 2022-02-25 06:07:53+00:00
- **Authors**: Zongbo Hao, Tao Zhang, Mingwang Chen, Kaixu Zhou
- **Comment**: Accepted by AAAI22
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) perform very well in image classification and object detection in recent years, but even the most advanced models have limited rotation invariance. Known solutions include the enhancement of training data and the increase of rotation invariance by globally merging the rotation equivariant features. These methods either increase the workload of training or increase the number of model parameters. To address this problem, this paper proposes a module that can be inserted into the existing networks, and directly incorporates the rotation invariance into the feature extraction layers of the CNNs. This module does not have learnable parameters and will not increase the complexity of the model. At the same time, only by training the upright data, it can perform well on the rotated testing set. These advantages will be suitable for fields such as biomedicine and astronomy where it is difficult to obtain upright samples or the target has no directionality. Evaluate our module with LeNet-5, ResNet-18 and tiny-yolov3, we get impressive results.



### TeachAugment: Data Augmentation Optimization Using Teacher Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2202.12513v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.12513v3)
- **Published**: 2022-02-25 06:22:51+00:00
- **Updated**: 2022-03-28 23:19:38+00:00
- **Authors**: Teppei Suzuki
- **Comment**: To appear in CVPR2022 (Oral presentation) Code:
  https://github.com/DensoITLab/TeachAugment
- **Journal**: None
- **Summary**: Optimization of image transformation functions for the purpose of data augmentation has been intensively studied. In particular, adversarial data augmentation strategies, which search augmentation maximizing task loss, show significant improvement in the model generalization for many tasks. However, the existing methods require careful parameter tuning to avoid excessively strong deformations that take away image features critical for acquiring generalization. In this paper, we propose a data augmentation optimization method based on the adversarial strategy called TeachAugment, which can produce informative transformed images to the model without requiring careful tuning by leveraging a teacher model. Specifically, the augmentation is searched so that augmented images are adversarial for the target model and recognizable for the teacher model. We also propose data augmentation using neural networks, which simplifies the search space design and allows for updating of the data augmentation using the gradient method. We show that TeachAugment outperforms existing methods in experiments of image classification, semantic segmentation, and unsupervised representation learning tasks.



### Faithful learning with sure data for lung nodule diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2202.12515v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.12515v1)
- **Published**: 2022-02-25 06:33:11+00:00
- **Updated**: 2022-02-25 06:33:11+00:00
- **Authors**: Hanxiao Zhang, Liang Chen, Xiao Gu, Minghui Zhang, Yulei Qin, Feng Yao, Zhexin Wang, Yun Gu, Guang-Zhong Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent evolution in deep learning has proven its value for CT-based lung nodule classification. Most current techniques are intrinsically black-box systems, suffering from two generalizability issues in clinical practice. First, benign-malignant discrimination is often assessed by human observers without pathologic diagnoses at the nodule level. We termed these data as "unsure data". Second, a classifier does not necessarily acquire reliable nodule features for stable learning and robust prediction with patch-level labels during learning. In this study, we construct a sure dataset with pathologically-confirmed labels and propose a collaborative learning framework to facilitate sure nodule classification by integrating unsure data knowledge through nodule segmentation and malignancy score regression. A loss function is designed to learn reliable features by introducing interpretability constraints regulated with nodule segmentation maps. Furthermore, based on model inference results that reflect the understanding from both machine and experts, we explore a new nodule analysis method for similar historical nodule retrieval and interpretable diagnosis. Detailed experimental results demonstrate that our approach is beneficial for achieving improved performance coupled with faithful model reasoning for lung cancer prediction. Extensive cross-evaluation results further illustrate the effect of unsure data for deep-learning-based methods in lung nodule classification.



### A Novel Hand Gesture Detection and Recognition system based on ensemble-based Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2202.12519v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.12519v1)
- **Published**: 2022-02-25 06:46:58+00:00
- **Updated**: 2022-02-25 06:46:58+00:00
- **Authors**: Abir Sen, Tapas Kumar Mishra, Ratnakar Dash
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, hand gesture recognition has become an alternative for human-machine interaction. It has covered a large area of applications like 3D game technology, sign language interpreting, VR (virtual reality) environment, and robotics. But detection of the hand portion has become a challenging task in computer vision and pattern recognition communities. Deep learning algorithm like convolutional neural network (CNN) architecture has become a very popular choice for classification tasks, but CNN architectures suffer from some problems like high variance during prediction, overfitting problem and also prediction errors. To overcome these problems, an ensemble of CNN-based approaches is presented in this paper. Firstly, the gesture portion is detected by using the background separation method based on binary thresholding. After that, the contour portion is extracted, and the hand region is segmented. Then, the images have been resized and fed into three individual CNN models to train them in parallel. In the last part, the output scores of CNN models are averaged to construct an optimal ensemble model for the final prediction. Two publicly available datasets (labeled as Dataset-1 and Dataset-2) containing infrared images and one self-constructed dataset have been used to validate the proposed system. Experimental results are compared with the existing state-of-the-art approaches, and it is observed that our proposed ensemble model outperforms other existing proposed methods.



### Improved Dual Correlation Reduction Network
- **Arxiv ID**: http://arxiv.org/abs/2202.12533v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12533v1)
- **Published**: 2022-02-25 07:48:32+00:00
- **Updated**: 2022-02-25 07:48:32+00:00
- **Authors**: Yue Liu, Sihang Zhou, Xinwang Liu, Wenxuan Tu, Xihong Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep graph clustering, which aims to reveal the underlying graph structure and divide the nodes into different clusters without human annotations, is a fundamental yet challenging task. However, we observed that the existing methods suffer from the representation collapse problem and easily tend to encode samples with different classes into the same latent embedding. Consequently, the discriminative capability of nodes is limited, resulting in sub-optimal clustering performance. To address this problem, we propose a novel deep graph clustering algorithm termed Improved Dual Correlation Reduction Network (IDCRN) through improving the discriminative capability of samples. Specifically, by approximating the cross-view feature correlation matrix to an identity matrix, we reduce the redundancy between different dimensions of features, thus improving the discriminative capability of the latent space explicitly. Meanwhile, the cross-view sample correlation matrix is forced to approximate the designed clustering-refined adjacency matrix to guide the learned latent representation to recover the affinity matrix even across views, thus enhancing the discriminative capability of features implicitly. Moreover, we avoid the collapsed representation caused by the over-smoothing issue in Graph Convolutional Networks (GCNs) through an introduced propagation regularization term, enabling IDCRN to capture the long-range information with the shallow network structure. Extensive experimental results on six benchmarks have demonstrated the effectiveness and the efficiency of IDCRN compared to the existing state-of-the-art deep graph clustering algorithms.



### An Ensemble Approach for Patient Prognosis of Head and Neck Tumor Using Multimodal Data
- **Arxiv ID**: http://arxiv.org/abs/2202.12537v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12537v1)
- **Published**: 2022-02-25 07:50:59+00:00
- **Updated**: 2022-02-25 07:50:59+00:00
- **Authors**: Numan Saeed, Roba Al Majzoub, Ikboljon Sobirov, Mohammad Yaqub
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Accurate prognosis of a tumor can help doctors provide a proper course of treatment and, therefore, save the lives of many. Traditional machine learning algorithms have been eminently useful in crafting prognostic models in the last few decades. Recently, deep learning algorithms have shown significant improvement when developing diagnosis and prognosis solutions to different healthcare problems. However, most of these solutions rely solely on either imaging or clinical data. Utilizing patient tabular data such as demographics and patient medical history alongside imaging data in a multimodal approach to solve a prognosis task has started to gain more interest recently and has the potential to create more accurate solutions. The main issue when using clinical and imaging data to train a deep learning model is to decide on how to combine the information from these sources. We propose a multimodal network that ensembles deep multi-task logistic regression (MTLR), Cox proportional hazard (CoxPH) and CNN models to predict prognostic outcomes for patients with head and neck tumors using patients' clinical and imaging (CT and PET) data. Features from CT and PET scans are fused and then combined with patients' electronic health records for the prediction. The proposed model is trained and tested on 224 and 101 patient records respectively. Experimental results show that our proposed ensemble solution achieves a C-index of 0.72 on The HECKTOR test set that saved us the first place in prognosis task of the HECKTOR challenge. The full implementation based on PyTorch is available on \url{https://github.com/numanai/BioMedIA-Hecktor2021}.



### ANTLER: Bayesian Nonlinear Tensor Learning and Modeler for Unstructured, Varying-Size Point Cloud Data
- **Arxiv ID**: http://arxiv.org/abs/2202.13788v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.13788v1)
- **Published**: 2022-02-25 08:37:21+00:00
- **Updated**: 2022-02-25 08:37:21+00:00
- **Authors**: Michael Biehler, Hao Yan, Jianjun Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Unstructured point clouds with varying sizes are increasingly acquired in a variety of environments through laser triangulation or Light Detection and Ranging (LiDAR). Predicting a scalar response based on unstructured point clouds is a common problem that arises in a wide variety of applications. The current literature relies on several pre-processing steps such as structured subsampling and feature extraction to analyze the point cloud data. Those techniques lead to quantization artifacts and do not consider the relationship between the regression response and the point cloud during pre-processing. Therefore, we propose a general and holistic "Bayesian Nonlinear Tensor Learning and Modeler" (ANTLER) to model the relationship of unstructured, varying-size point cloud data with a scalar or multivariate response. The proposed ANTLER simultaneously optimizes a nonlinear tensor dimensionality reduction and a nonlinear regression model with a 3D point cloud input and a scalar or multivariate response. ANTLER has the ability to consider the complex data representation, high-dimensionality,and inconsistent size of the 3D point cloud data.



### 6D Rotation Representation For Unconstrained Head Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.12555v2
- **DOI**: 10.1109/ICIP46576.2022.9897219
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.12555v2)
- **Published**: 2022-02-25 08:41:13+00:00
- **Updated**: 2022-11-07 08:30:48+00:00
- **Authors**: Thorsten Hempel, Ahmed A. Abdelrahman, Ayoub Al-Hamadi
- **Comment**: ICIP 2022
- **Journal**: 2022 IEEE International Conference on Image Processing (ICIP),
  2022, pp. 2496-2500
- **Summary**: In this paper, we present a method for unconstrained end-to-end head pose estimation. We address the problem of ambiguous rotation labels by introducing the rotation matrix formalism for our ground truth data and propose a continuous 6D rotation matrix representation for efficient and robust direct regression. This way, our method can learn the full rotation appearance which is contrary to previous approaches that restrict the pose prediction to a narrow-angle for satisfactory results. In addition, we propose a geodesic distance-based loss to penalize our network with respect to the SO(3) manifold geometry. Experiments on the public AFLW2000 and BIWI datasets demonstrate that our proposed method significantly outperforms other state-of-the-art methods by up to 20\%. We open-source our training and testing code along with our pre-trained models: https://github.com/thohemp/6DRepNet.



### An exploration of the performances achievable by combining unsupervised background subtraction algorithms
- **Arxiv ID**: http://arxiv.org/abs/2202.12563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.12563v1)
- **Published**: 2022-02-25 08:51:23+00:00
- **Updated**: 2022-02-25 08:51:23+00:00
- **Authors**: Sébastien Piérard, Marc Braham, Marc Van Droogenbroeck
- **Comment**: Submitted to IEEE International Conference in Image Processing (ICIP)
  - this version contains more references than the submitted article
- **Journal**: None
- **Summary**: Background subtraction (BGS) is a common choice for performing motion detection in video. Hundreds of BGS algorithms are released every year, but combining them to detect motion remains largely unexplored. We found that combination strategies allow to capitalize on this massive amount of available BGS algorithms, and offer significant space for performance improvement. In this paper, we explore sets of performances achievable by 6 strategies combining, pixelwise, the outputs of 26 unsupervised BGS algorithms, on the CDnet 2014 dataset, both in the ROC space and in terms of the F1 score. The chosen strategies are representative for a large panel of strategies, including both deterministic and non-deterministic ones, voting and learning. In our experiments, we compare our results with the state-of-the-art combinations IUTIS-5 and CNN-SFC, and report six conclusions, among which the existence of an important gap between the performances of the individual algorithms and the best performances achievable by combining them.



### Local Intensity Order Transformation for Robust Curvilinear Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.12587v1
- **DOI**: 10.1109/TIP.2022.3155954
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.12587v1)
- **Published**: 2022-02-25 10:04:00+00:00
- **Updated**: 2022-02-25 10:04:00+00:00
- **Authors**: Tianyi Shi, Nicolas Boutry, Yongchao Xu, Thierry Géraud
- **Comment**: Accepted by IEEE TIP
- **Journal**: None
- **Summary**: Segmentation of curvilinear structures is important in many applications, such as retinal blood vessel segmentation for early detection of vessel diseases and pavement crack segmentation for road condition evaluation and maintenance. Currently, deep learning-based methods have achieved impressive performance on these tasks. Yet, most of them mainly focus on finding powerful deep architectures but ignore capturing the inherent curvilinear structure feature (e.g., the curvilinear structure is darker than the context) for a more robust representation. In consequence, the performance usually drops a lot on cross-datasets, which poses great challenges in practice. In this paper, we aim to improve the generalizability by introducing a novel local intensity order transformation (LIOT). Specifically, we transfer a gray-scale image into a contrast-invariant four-channel image based on the intensity order between each pixel and its nearby pixels along with the four (horizontal and vertical) directions. This results in a representation that preserves the inherent characteristic of the curvilinear structure while being robust to contrast changes. Cross-dataset evaluation on three retinal blood vessel segmentation datasets demonstrates that LIOT improves the generalizability of some state-of-the-art methods. Additionally, the cross-dataset evaluation between retinal blood vessel segmentation and pavement crack segmentation shows that LIOT is able to preserve the inherent characteristic of curvilinear structure with large appearance gaps. An implementation of the proposed method is available at https://github.com/TY-Shi/LIOT.



### Active Learning for Point Cloud Semantic Segmentation via Spatial-Structural Diversity Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2202.12588v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.12588v2)
- **Published**: 2022-02-25 10:06:47+00:00
- **Updated**: 2022-04-18 15:46:22+00:00
- **Authors**: Feifei Shao, Yawei Luo, Ping Liu, Jie Chen, Yi Yang, Yulei Lu, Jun Xiao
- **Comment**: 9 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: The expensive annotation cost is notoriously known as the main constraint for the development of the point cloud semantic segmentation technique. Active learning methods endeavor to reduce such cost by selecting and labeling only a subset of the point clouds, yet previous attempts ignore the spatial-structural diversity of the selected samples, inducing the model to select clustered candidates with similar shapes in a local area while missing other representative ones in the global environment. In this paper, we propose a new 3D region-based active learning method to tackle this problem. Dubbed SSDR-AL, our method groups the original point clouds into superpoints and incrementally selects the most informative and representative ones for label acquisition. We achieve the selection mechanism via a graph reasoning network that considers both the spatial and structural diversities of superpoints. To deploy SSDR-AL in a more practical scenario, we design a noise-aware iterative labeling strategy to confront the "noisy annotation" problem introduced by the previous "dominant labeling" strategy in superpoints. Extensive experiments on two point cloud benchmarks demonstrate the effectiveness of SSDR-AL in the semantic segmentation task. Particularly, SSDR-AL significantly outperforms the baseline method and reduces the annotation cost by up to 63.0% and 24.0% when achieving 90% performance of fully supervised learning, respectively.



### LF-VIO: A Visual-Inertial-Odometry Framework for Large Field-of-View Cameras with Negative Plane
- **Arxiv ID**: http://arxiv.org/abs/2202.12613v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.12613v3)
- **Published**: 2022-02-25 11:03:31+00:00
- **Updated**: 2022-07-18 12:27:59+00:00
- **Authors**: Ze Wang, Kailun Yang, Hao Shi, Peng Li, Fei Gao, Kaiwei Wang
- **Comment**: Accepted to IROS 2022. Dataset and code are publicly available at
  https://github.com/flysoaryun/LF-VIO
- **Journal**: None
- **Summary**: Visual-inertial-odometry has attracted extensive attention in the field of autonomous driving and robotics. The size of Field of View (FoV) plays an important role in Visual-Odometry (VO) and Visual-Inertial-Odometry (VIO), as a large FoV enables to perceive a wide range of surrounding scene elements and features. However, when the field of the camera reaches the negative half plane, one cannot simply use [u,v,1]^T to represent the image feature points anymore. To tackle this issue, we propose LF-VIO, a real-time VIO framework for cameras with extremely large FoV. We leverage a three-dimensional vector with unit length to represent feature points, and design a series of algorithms to overcome this challenge. To address the scarcity of panoramic visual odometry datasets with ground-truth location and pose, we present the PALVIO dataset, collected with a Panoramic Annular Lens (PAL) system with an entire FoV of 360{\deg}x(40{\deg}-120{\deg}) and an IMU sensor. With a comprehensive variety of experiments, the proposed LF-VIO is verified on both the established PALVIO benchmark and a public fisheye camera dataset with a FoV of 360{\deg}x(0{\deg}-93.5{\deg}). LF-VIO outperforms state-of-the-art visual-inertial-odometry methods. Our dataset and code are made publicly available at https://github.com/flysoaryun/LF-VIO



### Joint Answering and Explanation for Visual Commonsense Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2202.12626v2
- **DOI**: 10.1109/TIP.2023.3286259
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.12626v2)
- **Published**: 2022-02-25 11:26:52+00:00
- **Updated**: 2023-01-12 13:47:43+00:00
- **Authors**: Zhenyang Li, Yangyang Guo, Kejie Wang, Yinwei Wei, Liqiang Nie, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Commonsense Reasoning (VCR), deemed as one challenging extension of the Visual Question Answering (VQA), endeavors to pursue a more high-level visual comprehension. It is composed of two indispensable processes: question answering over a given image and rationale inference for answer explanation. Over the years, a variety of methods tackling VCR have advanced the performance on the benchmark dataset. Despite significant as these methods are, they often treat the two processes in a separate manner and hence decompose the VCR into two irrelevant VQA instances. As a result, the pivotal connection between question answering and rationale inference is interrupted, rendering existing efforts less faithful on visual reasoning. To empirically study this issue, we perform some in-depth explorations in terms of both language shortcuts and generalization capability to verify the pitfalls of this treatment. Based on our findings, in this paper, we present a plug-and-play knowledge distillation enhanced framework to couple the question answering and rationale inference processes. The key contribution is the introduction of a novel branch, which serves as the bridge to conduct processes connecting. Given that our framework is model-agnostic, we apply it to the existing popular baselines and validate its effectiveness on the benchmark dataset. As detailed in the experimental results, when equipped with our framework, these baselines achieve consistent and significant performance improvements, demonstrating the viability of processes coupling, as well as the superiority of the proposed framework.



### Predicting 4D Liver MRI for MR-guided Interventions
- **Arxiv ID**: http://arxiv.org/abs/2202.12628v1
- **DOI**: 10.1016/j.compmedimag.2022.102122
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.12628v1)
- **Published**: 2022-02-25 11:34:25+00:00
- **Updated**: 2022-02-25 11:34:25+00:00
- **Authors**: Gino Gulamhussene, Anneke Meyer, Marko Rak, Oleksii Bashkanov, Jazan Omari, Maciej Pech, Christian Hansen
- **Comment**: 17 pages, 8 figures, submitted to Computerized Medical Imaging and
  Graphics
- **Journal**: None
- **Summary**: Organ motion poses an unresolved challenge in image-guided interventions. In the pursuit of solving this problem, the research field of time-resolved volumetric magnetic resonance imaging (4D MRI) has evolved. However, current techniques are unsuitable for most interventional settings because they lack sufficient temporal and/or spatial resolution or have long acquisition times. In this work, we propose a novel approach for real-time, high-resolution 4D MRI with large fields of view for MR-guided interventions. To this end, we trained a convolutional neural network (CNN) end-to-end to predict a 3D liver MRI that correctly predicts the liver's respiratory state from a live 2D navigator MRI of a subject. Our method can be used in two ways: First, it can reconstruct near real-time 4D MRI with high quality and high resolution (209x128x128 matrix size with isotropic 1.8mm voxel size and 0.6s/volume) given a dynamic interventional 2D navigator slice for guidance during an intervention. Second, it can be used for retrospective 4D reconstruction with a temporal resolution of below 0.2s/volume for motion analysis and use in radiation therapy. We report a mean target registration error (TRE) of 1.19 $\pm$0.74mm, which is below voxel size. We compare our results with a state-of-the-art retrospective 4D MRI reconstruction. Visual evaluation shows comparable quality. We show that small training sizes with short acquisition times down to 2min can already achieve promising results and 24min are sufficient for high quality results. Because our method can be readily combined with earlier methods, acquisition time can be further decreased while also limiting quality loss. We show that an end-to-end, deep learning formulation is highly promising for 4D MRI reconstruction.



### Deep Dirichlet uncertainty for unsupervised out-of-distribution detection of eye fundus photographs in glaucoma screening
- **Arxiv ID**: http://arxiv.org/abs/2202.12634v2
- **DOI**: 10.1109/ISBIC56247.2022.9854763
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12634v2)
- **Published**: 2022-02-25 11:51:45+00:00
- **Updated**: 2022-03-18 12:14:34+00:00
- **Authors**: Teresa Araújo, Guilherme Aresta, Hrvoje Bogunovic
- **Comment**: Submitted to ISBI 2022
- **Journal**: 2022 IEEE International Symposium on Biomedical Imaging Challenges
  (ISBIC), 2022, pp. 1-5
- **Summary**: The development of automatic tools for early glaucoma diagnosis with color fundus photographs can significantly reduce the impact of this disease. However, current state-of-the-art solutions are not robust to real-world scenarios, providing over-confident predictions for out-of-distribution cases. With this in mind, we propose a model based on the Dirichlet distribution that allows to obtain class-wise probabilities together with an uncertainty estimation without exposure to out-of-distribution cases. We demonstrate our approach on the AIROGS challenge. At the start of the final test phase (8 Feb. 2022), our method had the highest average score among all submissions.



### Improving Amharic Handwritten Word Recognition Using Auxiliary Task
- **Arxiv ID**: http://arxiv.org/abs/2202.12687v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.12687v1)
- **Published**: 2022-02-25 13:45:39+00:00
- **Updated**: 2022-02-25 13:45:39+00:00
- **Authors**: Mesay Samuel Gondere, Lars Schmidt-Thieme, Durga Prasad Sharma, Abiot Sinamo Boltena
- **Comment**: None
- **Journal**: None
- **Summary**: Amharic is one of the official languages of the Federal Democratic Republic of Ethiopia. It is one of the languages that use an Ethiopic script which is derived from Gee'z, ancient and currently a liturgical language. Amharic is also one of the most widely used literature-rich languages of Ethiopia. There are very limited innovative and customized research works in Amharic optical character recognition (OCR) in general and Amharic handwritten text recognition in particular. In this study, Amharic handwritten word recognition will be investigated. State-of-the-art deep learning techniques including convolutional neural networks together with recurrent neural networks and connectionist temporal classification (CTC) loss were used to make the recognition in an end-to-end fashion. More importantly, an innovative way of complementing the loss function using the auxiliary task from the row-wise similarities of the Amharic alphabet was tested to show a significant recognition improvement over a baseline method. Such findings will promote innovative problem-specific solutions as well as will open insight to a generalized solution that emerges from problem-specific domains.



### On Modality Bias Recognition and Reduction
- **Arxiv ID**: http://arxiv.org/abs/2202.12690v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.12690v2)
- **Published**: 2022-02-25 13:47:09+00:00
- **Updated**: 2022-09-27 01:34:26+00:00
- **Authors**: Yangyang Guo, Liqiang Nie, Harry Cheng, Zhiyong Cheng, Mohan Kankanhalli, Alberto Del Bimbo
- **Comment**: Accepted by ToMM
- **Journal**: None
- **Summary**: Making each modality in multi-modal data contribute is of vital importance to learning a versatile multi-modal model. Existing methods, however, are often dominated by one or few of modalities during model training, resulting in sub-optimal performance. In this paper, we refer to this problem as modality bias and attempt to study it in the context of multi-modal classification systematically and comprehensively. After stepping into several empirical analysis, we recognize that one modality affects the model prediction more just because this modality has a spurious correlation with instance labels. In order to primarily facilitate the evaluation on the modality bias problem, we construct two datasets respectively for the colored digit recognition and video action recognition tasks in line with the Out-of-Distribution (OoD) protocol. Collaborating with the benchmarks in the visual question answering task, we empirically justify the performance degradation of the existing methods on these OoD datasets, which serves as evidence to justify the modality bias learning. In addition, to overcome this problem, we propose a plug-and-play loss function method, whereby the feature space for each label is adaptively learned according to the training set statistics. Thereafter, we apply this method on eight baselines in total to test its effectiveness. From the results on four datasets regarding the above three tasks, our method yields remarkable performance improvements compared with the baselines, demonstrating its superiority on reducing the modality bias problem.



### ciscNet -- A Single-Branch Cell Instance Segmentation and Classification Network
- **Arxiv ID**: http://arxiv.org/abs/2202.13960v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13960v1)
- **Published**: 2022-02-25 13:47:25+00:00
- **Updated**: 2022-02-25 13:47:25+00:00
- **Authors**: Moritz Böhland, Oliver Neumann, Marcel P. Schilling, Markus Reischl, Ralf Mikut, Katharina Löffler, Tim Scherr
- **Comment**: CoNIC Challenge 2022 submission
- **Journal**: None
- **Summary**: Automated cell nucleus segmentation and classification are required to assist pathologists in their decision making. The Colon Nuclei Identification and Counting Challenge 2022 (CoNIC Challenge 2022) supports the development and comparability of segmentation and classification methods for histopathological images. In this contribution, we describe our CoNIC Challenge 2022 method ciscNet to segment, classify and count cell nuclei, and report preliminary evaluation results. Our code is available at https://git.scc.kit.edu/ciscnet/ciscnet-conic-2022.



### Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs
- **Arxiv ID**: http://arxiv.org/abs/2202.12692v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2202.12692v1)
- **Published**: 2022-02-25 13:51:00+00:00
- **Updated**: 2022-02-25 13:51:00+00:00
- **Authors**: Furkan Ozcelik, Bhavin Choksi, Milad Mozafari, Leila Reddy, Rufin VanRullen
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstructing perceived natural images from fMRI signals is one of the most engaging topics of neural decoding research. Prior studies had success in reconstructing either the low-level image features or the semantic/high-level aspects, but rarely both. In this study, we utilized an Instance-Conditioned GAN (IC-GAN) model to reconstruct images from fMRI patterns with both accurate semantic attributes and preserved low-level details. The IC-GAN model takes as input a 119-dim noise vector and a 2048-dim instance feature vector extracted from a target image via a self-supervised learning model (SwAV ResNet-50); these instance features act as a conditioning for IC-GAN image generation, while the noise vector introduces variability between samples. We trained ridge regression models to predict instance features, noise vectors, and dense vectors (the output of the first dense layer of the IC-GAN generator) of stimuli from corresponding fMRI patterns. Then, we used the IC-GAN generator to reconstruct novel test images based on these fMRI-predicted variables. The generated images presented state-of-the-art results in terms of capturing the semantic attributes of the original test images while remaining relatively faithful to low-level image details. Finally, we use the learned regression model and the IC-GAN generator to systematically explore and visualize the semantic features that maximally drive each of several regions-of-interest in the human brain.



### Data refinement for fully unsupervised visual inspection using pre-trained networks
- **Arxiv ID**: http://arxiv.org/abs/2202.12759v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12759v1)
- **Published**: 2022-02-25 15:25:05+00:00
- **Updated**: 2022-02-25 15:25:05+00:00
- **Authors**: Antoine Cordier, Benjamin Missaoui, Pierre Gutierrez
- **Comment**: 18 pages, 13 figures
- **Journal**: None
- **Summary**: Anomaly detection has recently seen great progress in the field of visual inspection. More specifically, the use of classical outlier detection techniques on features extracted by deep pre-trained neural networks have been shown to deliver remarkable performances on the MVTec Anomaly Detection (MVTec AD) dataset. However, like most other anomaly detection strategies, these pre-trained methods assume all training data to be normal. As a consequence, they cannot be considered as fully unsupervised. There exists to our knowledge no work studying these pre-trained methods under fully unsupervised setting. In this work, we first assess the robustness of these pre-trained methods to fully unsupervised context, using polluted training sets (i.e. containing defective samples), and show that these methods are more robust to pollution compared to methods such as CutPaste. We then propose SROC, a Simple Refinement strategy for One Class classification. SROC enables to remove most of the polluted images from the training set, and to recover some of the lost AUC. We further show that our simple heuristic competes with, and even outperforms much more complex strategies from the existing literature.



### Towards Safe, Real-Time Systems: Stereo vs Images and LiDAR for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.12773v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.12773v1)
- **Published**: 2022-02-25 15:41:58+00:00
- **Updated**: 2022-02-25 15:41:58+00:00
- **Authors**: Matthew Levine
- **Comment**: Submitted to IROS RA-L
- **Journal**: None
- **Summary**: As object detectors rapidly improve, attention has expanded past image-only networks to include a range of 3D and multimodal frameworks, especially ones that incorporate LiDAR. However, due to cost, logistics, and even some safety considerations, stereo can be an appealing alternative. Towards understanding the efficacy of stereo as a replacement for monocular input or LiDAR in object detectors, we show that multimodal learning with traditional disparity algorithms can improve image-based results without increasing the number of parameters, and that learning over stereo error can impart similar 3D localization power to LiDAR in certain contexts. Furthermore, doing so also has calibration benefits with respect to image-only methods. We benchmark on the public dataset KITTI, and in doing so, reveal a few small but common algorithmic mistakes currently used in computing metrics on that set, and offer efficient, provably correct alternatives.



### Confidence Calibration for Object Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.12785v4
- **DOI**: 10.1007/978-3-031-01233-4_8
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2202.12785v4)
- **Published**: 2022-02-25 15:59:51+00:00
- **Updated**: 2022-06-20 05:56:58+00:00
- **Authors**: Fabian Küppers, Anselm Haselhoff, Jan Kronenberger, Jonas Schneider
- **Comment**: Book chapter in: Tim Fingerscheidt, Hanno Gottschalk, Sebastian
  Houben (eds.): "Deep Neural Networks and Data for Automated Driving", pp.
  225--250, Springer Nature, Switzerland, 2022
- **Journal**: In: Tim Fingerscheidt, Hanno Gottschalk, Sebastian Houben (eds.):
  "Deep Neural Networks and Data for Automated Driving", pp. 225--250, Springer
  Nature, Switzerland, 2022
- **Summary**: Calibrated confidence estimates obtained from neural networks are crucial, particularly for safety-critical applications such as autonomous driving or medical image diagnosis. However, although the task of confidence calibration has been investigated on classification problems, thorough investigations on object detection and segmentation problems are still missing. Therefore, we focus on the investigation of confidence calibration for object detection and segmentation models in this chapter. We introduce the concept of multivariate confidence calibration that is an extension of well-known calibration methods to the task of object detection and segmentation. This allows for an extended confidence calibration that is also aware of additional features such as bounding box/pixel position, shape information, etc. Furthermore, we extend the expected calibration error (ECE) to measure miscalibration of object detection and segmentation models. We examine several network architectures on MS COCO as well as on Cityscapes and show that especially object detection as well as instance segmentation models are intrinsically miscalibrated given the introduced definition of calibration. Using our proposed calibration methods, we have been able to improve calibration so that it also has a positive impact on the quality of segmentation masks as well.



### Sensing accident-prone features in urban scenes for proactive driving and accident prevention
- **Arxiv ID**: http://arxiv.org/abs/2202.12788v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.12788v2)
- **Published**: 2022-02-25 16:05:53+00:00
- **Updated**: 2022-11-18 13:54:28+00:00
- **Authors**: Sumit Mishra, Praveen Kumar Rajendran, Luiz Felipe Vecchietti, Dongsoo Har
- **Comment**: (13 pages, 9 figures, 6 tables, under review in IEEE Transactions on
  Intelligent Transportation Systems)
- **Journal**: None
- **Summary**: In urban cities, visual information on and along roadways is likely to distract drivers and lead to missing traffic signs and other accident-prone (AP) features. To avoid accidents due to missing these visual cues, this paper proposes a visual notification of AP-features to drivers based on real-time images obtained via dashcam. For this purpose, Google Street View images around accident hotspots (areas of dense accident occurrence) identified by a real-accident dataset are used to train a novel attention module to classify a given urban scene into an accident hotspot or a non-hotspot (area of sparse accident occurrence). The proposed module leverages channel, point, and spatial-wise attention learning on top of different CNN backbones. This leads to better classification results and more certain AP-features with better contextual knowledge when compared with CNN backbones alone. Our proposed module achieves up to 92% classification accuracy. The capability of detecting AP-features by the proposed model were analyzed by a comparative study of three different class activation map (CAM) methods, which were used to inspect specific AP-features causing the classification decision. Outputs of CAM methods were processed by an image processing pipeline to extract only the AP-features that are explainable to drivers and notified using a visual notification system. Range of experiments was performed to prove the efficacy and AP-features of the system. Ablation of the AP-features taking 9.61%, on average, of the total area in each image increased the chance of a given area to be classified as a non-hotspot by up to 21.8%.



### Improving generalization with synthetic training data for deep learning based quality inspection
- **Arxiv ID**: http://arxiv.org/abs/2202.12818v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12818v1)
- **Published**: 2022-02-25 16:51:01+00:00
- **Updated**: 2022-02-25 16:51:01+00:00
- **Authors**: Antoine Cordier, Pierre Gutierrez, Victoire Plessis
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: Automating quality inspection with computer vision techniques is often a very data-demanding task. Specifically, supervised deep learning requires a large amount of annotated images for training. In practice, collecting and annotating such data is not only costly and laborious, but also inefficient, given the fact that only a few instances may be available for certain defect classes. If working with video frames can increase the number of these instances, it has a major disadvantage: the resulting images will be highly correlated with one another. As a consequence, models trained under such constraints are expected to be very sensitive to input distribution changes, which may be caused in practice by changes in the acquisition system (cameras, lights), in the parts or in the defects aspect. In this work, we demonstrate the use of randomly generated synthetic training images can help tackle domain instability issues, making the trained models more robust to contextual changes. We detail both our synthetic data generation pipeline and our deep learning methodology for answering these questions.



### NeuralHOFusion: Neural Volumetric Rendering under Human-object Interactions
- **Arxiv ID**: http://arxiv.org/abs/2202.12825v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.12825v3)
- **Published**: 2022-02-25 17:10:07+00:00
- **Updated**: 2022-03-28 07:58:03+00:00
- **Authors**: Yuheng Jiang, Suyi Jiang, Guoxing Sun, Zhuo Su, Kaiwen Guo, Minye Wu, Jingyi Yu, Lan Xu
- **Comment**: None
- **Journal**: None
- **Summary**: 4D modeling of human-object interactions is critical for numerous applications. However, efficient volumetric capture and rendering of complex interaction scenarios, especially from sparse inputs, remain challenging. In this paper, we propose NeuralHOFusion, a neural approach for volumetric human-object capture and rendering using sparse consumer RGBD sensors. It marries traditional non-rigid fusion with recent neural implicit modeling and blending advances, where the captured humans and objects are layerwise disentangled. For geometry modeling, we propose a neural implicit inference scheme with non-rigid key-volume fusion, as well as a template-aid robust object tracking pipeline. Our scheme enables detailed and complete geometry generation under complex interactions and occlusions. Moreover, we introduce a layer-wise human-object texture rendering scheme, which combines volumetric and image-based rendering in both spatial and temporal domains to obtain photo-realistic results. Extensive experiments demonstrate the effectiveness and efficiency of our approach in synthesizing photo-realistic free-view results under complex human-object interactions.



### RelMobNet: End-to-end relative camera pose estimation using a robust two-stage training
- **Arxiv ID**: http://arxiv.org/abs/2202.12838v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.12838v2)
- **Published**: 2022-02-25 17:27:26+00:00
- **Updated**: 2022-07-10 15:31:47+00:00
- **Authors**: Praveen Kumar Rajendran, Sumit Mishra, Luiz Felipe Vecchietti, Dongsoo Har
- **Comment**: 15 pages, 7 figures, 2 tables - RelMobNet revised draft
- **Journal**: None
- **Summary**: Relative camera pose estimation, i.e. estimating the translation and rotation vectors using a pair of images taken in different locations, is an important part of systems in augmented reality and robotics. In this paper, we present an end-to-end relative camera pose estimation network using a siamese architecture that is independent of camera parameters. The network is trained using the Cambridge Landmarks data with four individual scene datasets and a dataset combining the four scenes. To improve generalization, we propose a novel two-stage training that alleviates the need of a hyperparameter to balance the translation and rotation loss scale. The proposed method is compared with one-stage training CNN-based methods such as RPNet and RCPNet and demonstrate that the proposed model improves translation vector estimation by 16.11%, 28.88%, and 52.27% on the Kings College, Old Hospital, and St Marys Church scenes, respectively. For proving texture invariance, we investigate the generalization of the proposed method augmenting the datasets to different scene styles, as ablation studies, using generative adversarial networks. Also, we present a qualitative assessment of epipolar lines of our network predictions and ground truth poses.



### ARIA: Adversarially Robust Image Attribution for Content Provenance
- **Arxiv ID**: http://arxiv.org/abs/2202.12860v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12860v1)
- **Published**: 2022-02-25 18:11:45+00:00
- **Updated**: 2022-02-25 18:11:45+00:00
- **Authors**: Maksym Andriushchenko, Xiaoyang Rebecca Li, Geoffrey Oxholm, Thomas Gittings, Tu Bui, Nicolas Flammarion, John Collomosse
- **Comment**: None
- **Journal**: None
- **Summary**: Image attribution -- matching an image back to a trusted source -- is an emerging tool in the fight against online misinformation. Deep visual fingerprinting models have recently been explored for this purpose. However, they are not robust to tiny input perturbations known as adversarial examples. First we illustrate how to generate valid adversarial images that can easily cause incorrect image attribution. Then we describe an approach to prevent imperceptible adversarial attacks on deep visual fingerprinting models, via robust contrastive learning. The proposed training procedure leverages training on $\ell_\infty$-bounded adversarial examples, it is conceptually simple and incurs only a small computational overhead. The resulting models are substantially more robust, are accurate even on unperturbed images, and perform well even over a database with millions of images. In particular, we achieve 91.6% standard and 85.1% adversarial recall under $\ell_\infty$-bounded perturbations on manipulated images compared to 80.1% and 0.0% from prior work. We also show that robustness generalizes to other types of imperceptible perturbations unseen during training. Finally, we show how to train an adversarially robust image comparator model for detecting editorial changes in matched images.



### Learning to Identify Perceptual Bugs in 3D Video Games
- **Arxiv ID**: http://arxiv.org/abs/2202.12884v1
- **DOI**: None
- **Categories**: **cs.SE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12884v1)
- **Published**: 2022-02-25 18:50:11+00:00
- **Updated**: 2022-02-25 18:50:11+00:00
- **Authors**: Benedict Wilkins, Kostas Stathis
- **Comment**: None
- **Journal**: None
- **Summary**: Automated Bug Detection (ABD) in video games is composed of two distinct but complementary problems: automated game exploration and bug identification. Automated game exploration has received much recent attention, spurred on by developments in fields such as reinforcement learning. The complementary problem of identifying the bugs present in a player's experience has for the most part relied on the manual specification of rules. Although it is widely recognised that many bugs of interest cannot be identified with such methods, little progress has been made in this direction. In this work we show that it is possible to identify a range of perceptual bugs using learning-based methods by making use of only the rendered game screen as seen by the player. To support our work, we have developed World of Bugs (WOB) an open platform for testing ABD methods in 3D game environments.



### Refining Self-Supervised Learning in Imaging: Beyond Linear Metric
- **Arxiv ID**: http://arxiv.org/abs/2202.12921v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.12921v2)
- **Published**: 2022-02-25 19:25:05+00:00
- **Updated**: 2022-10-13 17:05:42+00:00
- **Authors**: Bo Jiang, Hamid Krim, Tianfu Wu, Derya Cansever
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce in this paper a new statistical perspective, exploiting the Jaccard similarity metric, as a measure-based metric to effectively invoke non-linear features in the loss of self-supervised contrastive learning. Specifically, our proposed metric may be interpreted as a dependence measure between two adapted projections learned from the so-called latent representations. This is in contrast to the cosine similarity measure in the conventional contrastive learning model, which accounts for correlation information. To the best of our knowledge, this effectively non-linearly fused information embedded in the Jaccard similarity, is novel to self-supervision learning with promising results. The proposed approach is compared to two state-of-the-art self-supervised contrastive learning methods on three image datasets. We not only demonstrate its amenable applicability in current ML problems, but also its improved performance and training efficiency.



### OptGAN: Optimizing and Interpreting the Latent Space of the Conditional Text-to-Image GANs
- **Arxiv ID**: http://arxiv.org/abs/2202.12929v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12929v1)
- **Published**: 2022-02-25 20:00:33+00:00
- **Updated**: 2022-02-25 20:00:33+00:00
- **Authors**: Zhenxing Zhang, Lambert Schomaker
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: Text-to-image generation intends to automatically produce a photo-realistic image, conditioned on a textual description. It can be potentially employed in the field of art creation, data augmentation, photo-editing, etc. Although many efforts have been dedicated to this task, it remains particularly challenging to generate believable, natural scenes. To facilitate the real-world applications of text-to-image synthesis, we focus on studying the following three issues: 1) How to ensure that generated samples are believable, realistic or natural? 2) How to exploit the latent space of the generator to edit a synthesized image? 3) How to improve the explainability of a text-to-image generation framework? In this work, we constructed two novel data sets (i.e., the Good & Bad bird and face data sets) consisting of successful as well as unsuccessful generated samples, according to strict criteria. To effectively and efficiently acquire high-quality images by increasing the probability of generating Good latent codes, we use a dedicated Good/Bad classifier for generated images. It is based on a pre-trained front end and fine-tuned on the basis of the proposed Good & Bad data set. After that, we present a novel algorithm which identifies semantically-understandable directions in the latent space of a conditional text-to-image GAN architecture by performing independent component analysis on the pre-trained weight values of the generator. Furthermore, we develop a background-flattening loss (BFL), to improve the background appearance in the edited image. Subsequently, we introduce linear interpolation analysis between pairs of keywords. This is extended into a similar triangular `linguistic' interpolation in order to take a deep look into what a text-to-image synthesis model has learned within the linguistic embeddings. Our data set is available at https://zenodo.org/record/6283798#.YhkN_ujMI2w.



### Model Attribution of Face-swap Deepfake Videos
- **Arxiv ID**: http://arxiv.org/abs/2202.12951v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2202.12951v1)
- **Published**: 2022-02-25 20:05:18+00:00
- **Updated**: 2022-02-25 20:05:18+00:00
- **Authors**: Shan Jia, Xin Li, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: AI-created face-swap videos, commonly known as Deepfakes, have attracted wide attention as powerful impersonation attacks. Existing research on Deepfakes mostly focuses on binary detection to distinguish between real and fake videos. However, it is also important to determine the specific generation model for a fake video, which can help attribute it to the source for forensic investigation. In this paper, we fill this gap by studying the model attribution problem of Deepfake videos. We first introduce a new dataset with DeepFakes from Different Models (DFDM) based on several Autoencoder models. Specifically, five generation models with variations in encoder, decoder, intermediate layer, input resolution, and compression ratio have been used to generate a total of 6,450 Deepfake videos based on the same input. Then we take Deepfakes model attribution as a multiclass classification task and propose a spatial and temporal attention based method to explore the differences among Deepfakes in the new dataset. Experimental evaluation shows that most existing Deepfakes detection methods failed in Deepfakes model attribution, while the proposed method achieved over 70% accuracy on the high-quality DFDM dataset.



### Image reconstruction algorithms in radio interferometry: from handcrafted to learned regularization denoisers
- **Arxiv ID**: http://arxiv.org/abs/2202.12959v2
- **DOI**: 10.1093/mnras/stac2672
- **Categories**: **eess.IV**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.12959v2)
- **Published**: 2022-02-25 20:26:33+00:00
- **Updated**: 2022-07-26 15:17:34+00:00
- **Authors**: Matthieu Terris, Arwa Dabbech, Chao Tang, Yves Wiaux
- **Comment**: To match revision for MNRAS publication. The new version includes the
  development of, and benchmarking with, a pure end-to-end DNN approach
- **Journal**: None
- **Summary**: We introduce a new class of iterative image reconstruction algorithms for radio interferometry, at the interface of convex optimization and deep learning, inspired by plug-and-play methods. The approach consists in learning a prior image model by training a deep neural network (DNN) as a denoiser, and substituting it for the handcrafted proximal regularization operator of an optimization algorithm. The proposed AIRI (``AI for Regularization in radio-interferometric Imaging'') framework, for imaging complex intensity structure with diffuse and faint emission from visibility data, inherits the robustness and interpretability of optimization, and the learning power and speed of networks. Our approach relies on three steps. Firstly, we design a low dynamic range training database from optical intensity images. Secondly, we train a DNN denoiser at a noise level inferred from the signal-to-noise ratio of the data. We use training losses enhanced with a nonexpansiveness term ensuring algorithm convergence, and including on-the-fly database dynamic range enhancement via exponentiation. Thirdly, we plug the learned denoiser into the forward-backward optimization algorithm, resulting in a simple iterative structure alternating a denoising step with a gradient-descent data-fidelity step. We have validated AIRI against CLEAN, optimization algorithms of the SARA family, and a DNN trained to reconstruct the image directly from visibility data. Simulation results show that AIRI is competitive in imaging quality with SARA and its unconstrained forward-backward-based version uSARA, while providing significant acceleration. CLEAN remains faster but offers lower quality. The end-to-end DNN offers further acceleration, but with far lower quality than AIRI.



### FSGANv2: Improved Subject Agnostic Face Swapping and Reenactment
- **Arxiv ID**: http://arxiv.org/abs/2202.12972v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12972v1)
- **Published**: 2022-02-25 21:04:39+00:00
- **Updated**: 2022-02-25 21:04:39+00:00
- **Authors**: Yuval Nirkin, Yosi Keller, Tal Hassner
- **Comment**: arXiv admin note: text overlap with arXiv:1908.05932
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (PAMI), 2022
- **Summary**: We present Face Swapping GAN (FSGAN) for face swapping and reenactment. Unlike previous work, we offer a subject agnostic swapping scheme that can be applied to pairs of faces without requiring training on those faces. We derive a novel iterative deep learning--based approach for face reenactment which adjusts significant pose and expression variations that can be applied to a single image or a video sequence. For video sequences, we introduce a continuous interpolation of the face views based on reenactment, Delaunay Triangulation, and barycentric coordinates. Occluded face regions are handled by a face completion network. Finally, we use a face blending network for seamless blending of the two faces while preserving the target skin color and lighting conditions. This network uses a novel Poisson blending loss combining Poisson optimization with a perceptual loss. We compare our approach to existing state-of-the-art systems and show our results to be both qualitatively and quantitatively superior. This work describes extensions of the FSGAN method, proposed in an earlier conference version of our work, as well as additional experiments and results.



### OCR-IDL: OCR Annotations for Industry Document Library Dataset
- **Arxiv ID**: http://arxiv.org/abs/2202.12985v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.12985v1)
- **Published**: 2022-02-25 21:30:48+00:00
- **Updated**: 2022-02-25 21:30:48+00:00
- **Authors**: Ali Furkan Biten, Rubèn Tito, Lluis Gomez, Ernest Valveny, Dimosthenis Karatzas
- **Comment**: None
- **Journal**: None
- **Summary**: Pretraining has proven successful in Document Intelligence tasks where deluge of documents are used to pretrain the models only later to be finetuned on downstream tasks. One of the problems of the pretraining approaches is the inconsistent usage of pretraining data with different OCR engines leading to incomparable results between models. In other words, it is not obvious whether the performance gain is coming from diverse usage of amount of data and distinct OCR engines or from the proposed models. To remedy the problem, we make public the OCR annotations for IDL documents using commercial OCR engine given their superior performance over open source OCR models. The contributed dataset (OCR-IDL) has an estimated monetary value over 20K US$. It is our hope that OCR-IDL can be a starting point for future works on Document Intelligence. All of our data and its collection process with the annotations can be found in https://github.com/furkanbiten/idl_data.



### Extracting Effective Subnetworks with Gumbel-Softmax
- **Arxiv ID**: http://arxiv.org/abs/2202.12986v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.12986v5)
- **Published**: 2022-02-25 21:31:30+00:00
- **Updated**: 2023-02-14 16:39:48+00:00
- **Authors**: Robin Dupont, Mohammed Amine Alaoui, Hichem Sahbi, Alice Lebois
- **Comment**: 5 pages, 1 table. Published at ICIP 2022, Oral Session
- **Journal**: None
- **Summary**: Large and performant neural networks are often overparameterized and can be drastically reduced in size and complexity thanks to pruning. Pruning is a group of methods, which seeks to remove redundant or unnecessary weights or groups of weights in a network. These techniques allow the creation of lightweight networks, which are particularly critical in embedded or mobile applications. In this paper, we devise an alternative pruning method that allows extracting effective subnetworks from larger untrained ones. Our method is stochastic and extracts subnetworks by exploring different topologies which are sampled using Gumbel Softmax. The latter is also used to train probability distributions which measure the relevance of weights in the sampled topologies. The resulting subnetworks are further enhanced using a highly efficient rescaling mechanism that reduces training time and improves performance. Extensive experiments conducted on CIFAR show the outperformance of our subnetwork extraction method against the related work.



### A Brief Survey on Adaptive Video Streaming Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2202.12987v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.12987v1)
- **Published**: 2022-02-25 21:38:14+00:00
- **Updated**: 2022-02-25 21:38:14+00:00
- **Authors**: Wei Zhou, Xiongkuo Min, Hong Li, Qiuping Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Quality of experience (QoE) assessment for adaptive video streaming plays a significant role in advanced network management systems. It is especially challenging in case of dynamic adaptive streaming schemes over HTTP (DASH) which has increasingly complex characteristics including additional playback issues. In this paper, we provide a brief overview of adaptive video streaming quality assessment. Upon our review of related works, we analyze and compare different variations of objective QoE assessment models with or without using machine learning techniques for adaptive video streaming. Through the performance analysis, we observe that hybrid models perform better than both quality-of-service (QoS) driven QoE approaches and signal fidelity measurement. Moreover, the machine learning-based model slightly outperforms the model without using machine learning for the same setting. In addition, we find that existing video streaming QoE assessment models still have limited performance, which makes it difficult to be applied in practical communication systems. Therefore, based on the success of deep learned feature representations for traditional video quality prediction, we also apply the off-the-shelf deep convolutional neural network (DCNN) to evaluate the perceptual quality of streaming videos, where the spatio-temporal properties of streaming videos are taken into consideration. Experiments demonstrate its superiority, which sheds light on the future development of specifically designed deep learning frameworks for adaptive video streaming quality assessment. We believe this survey can serve as a guideline for QoE assessment of adaptive video streaming.



### se-Shweshwe Inspired Fashion Generation
- **Arxiv ID**: http://arxiv.org/abs/2203.00435v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.00435v1)
- **Published**: 2022-02-25 22:10:23+00:00
- **Updated**: 2022-02-25 22:10:23+00:00
- **Authors**: Lindiwe Brigitte Malobola, Negar Rostamzadeh, Shakir Mohamed
- **Comment**: CVPR 2021 Beyond Fairness workshop
- **Journal**: None
- **Summary**: Fashion is one of the ways in which we show ourselves to the world. It is a reflection of our personal decisions and one of the ways in which people distinguish and represent themselves. In this paper, we focus on the fashion design process and expand computer vision for fashion beyond its current focus on western fashion. We discuss the history of Southern African se-Shweshwe fabric fashion, the collection of a se-Shweshwe dataset, and the application of sketch-to-design image generation for affordable fashion-design. The application to fashion raises both technical questions of training with small amounts of data, and also important questions for computer vision beyond fairness, in particular ethical considerations on creating and employing fashion datasets, and how computer vision supports cultural representation and might avoid algorithmic cultural appropriation.



### Weakly Supervised Instance Segmentation using Motion Information via Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2202.13006v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13006v1)
- **Published**: 2022-02-25 22:41:54+00:00
- **Updated**: 2022-02-25 22:41:54+00:00
- **Authors**: Jun Ikeda, Junichiro Mori
- **Comment**: 5 pages, 3 figures, submitted to the 29th IEEE International
  Conference on Image Processing (ICIP)
- **Journal**: None
- **Summary**: Weakly supervised instance segmentation has gained popularity because it reduces high annotation cost of pixel-level masks required for model training. Recent approaches for weakly supervised instance segmentation detect and segment objects using appearance information obtained from a static image. However, it poses the challenge of identifying objects with a non-discriminatory appearance. In this study, we address this problem by using motion information from image sequences. We propose a two-stream encoder that leverages appearance and motion features extracted from images and optical flows. Additionally, we propose a novel pairwise loss that considers both appearance and motion information to supervise segmentation. We conducted extensive evaluations on the YouTube-VIS 2019 benchmark dataset. Our results demonstrate that the proposed method improves the Average Precision of the state-of-the-art method by 3.1.



### Multi-view Gradient Consistency for SVBRDF Estimation of Complex Scenes under Natural Illumination
- **Arxiv ID**: http://arxiv.org/abs/2202.13017v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2202.13017v1)
- **Published**: 2022-02-25 23:49:39+00:00
- **Updated**: 2022-02-25 23:49:39+00:00
- **Authors**: Alen Joy, Charalambos Poullis
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a process for estimating the spatially varying surface reflectance of complex scenes observed under natural illumination. In contrast to previous methods, our process is not limited to scenes viewed under controlled lighting conditions but can handle complex indoor and outdoor scenes viewed under arbitrary illumination conditions. An end-to-end process uses a model of the scene's geometry and several images capturing the scene's surfaces from arbitrary viewpoints and under various natural illumination conditions. We develop a differentiable path tracer that leverages least-square conformal mapping for handling multiple disjoint objects appearing in the scene. We follow a two-step optimization process and introduce a multi-view gradient consistency loss which results in up to 30-50% improvement in the image reconstruction loss and can further achieve better disentanglement of the diffuse and specular BRDFs compared to other state-of-the-art. We demonstrate the process in real-world indoor and outdoor scenes from images in the wild and show that we can produce realistic renders consistent with actual images using the estimated reflectance properties. Experiments show that our technique produces realistic results for arbitrary outdoor scenes with complex geometry. The source code is publicly available at: https://gitlab.com/alen.joy/multi-view-gradient-consistency-for-svbrdf-estimation-of-complex-scenes-under-natural-illumination



### HCIL: Hierarchical Class Incremental Learning for Longline Fishing Visual Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2202.13018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13018v1)
- **Published**: 2022-02-25 23:53:11+00:00
- **Updated**: 2022-02-25 23:53:11+00:00
- **Authors**: Jie Mei, Suzanne Romain, Craig Rose, Kelsey Magrane, Jenq-Neng Hwang
- **Comment**: Preprint for ICIP 2022
- **Journal**: None
- **Summary**: The goal of electronic monitoring of longline fishing is to visually monitor the fish catching activities on fishing vessels based on cameras, either for regulatory compliance or catch counting. The previous hierarchical classification method demonstrates efficient fish species identification of catches from longline fishing, where fishes are under severe deformation and self-occlusion during the catching process. Although the hierarchical classification mitigates the laborious efforts of human reviews by providing confidence scores in different hierarchical levels, its performance drops dramatically under the class incremental learning (CIL) scenario. A CIL system should be able to learn about more and more classes over time from a stream of data, i.e., only the training data for a small number of classes have to be present at the beginning and new classes can be added progressively. In this work, we introduce a Hierarchical Class Incremental Learning (HCIL) model, which significantly improves the state-of-the-art hierarchical classification methods under the CIL scenario.



