# Arxiv Papers in cs.CV on 2022-02-26
### Building a visual semantics aware object hierarchy
- **Arxiv ID**: http://arxiv.org/abs/2202.13021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13021v1)
- **Published**: 2022-02-26 00:10:21+00:00
- **Updated**: 2022-02-26 00:10:21+00:00
- **Authors**: Xiaolei Diao
- **Comment**: None
- **Journal**: None
- **Summary**: The semantic gap is defined as the difference between the linguistic representations of the same concept, which usually leads to misunderstanding between individuals with different knowledge backgrounds. Since linguistically annotated images are extensively used for training machine learning models, semantic gap problem (SGP) also results in inevitable bias on image annotations and further leads to poor performance on current computer vision tasks. To address this problem, we propose a novel unsupervised method to build visual semantics aware object hierarchy, aiming to get a classification model by learning from pure-visual information and to dissipate the bias of linguistic representations caused by SGP. Our intuition in this paper comes from real-world knowledge representation where concepts are hierarchically organized, and each concept can be described by a set of features rather than a linguistic annotation, namely visual semantic. The evaluation consists of two parts, firstly we apply the constructed hierarchy on the object recognition task and then we compare our visual hierarchy and existing lexical hierarchies to show the validity of our method. The preliminary results reveal the efficiency and potential of our proposed method.



### Generalized Label Shift Correction via Minimum Uncertainty Principle: Theory and Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2202.13043v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13043v1)
- **Published**: 2022-02-26 02:39:47+00:00
- **Updated**: 2022-02-26 02:39:47+00:00
- **Authors**: You-Wei Luo, Chuan-Xian Ren
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: As a fundamental problem in machine learning, dataset shift induces a paradigm to learn and transfer knowledge under changing environment. Previous methods assume the changes are induced by covariate, which is less practical for complex real-world data. We consider the Generalized Label Shift (GLS), which provides an interpretable insight into the learning and transfer of desirable knowledge. Current GLS methods: 1) are not well-connected with the statistical learning theory; 2) usually assume the shifting conditional distributions will be matched with an implicit transformation, but its explicit modeling is unexplored. In this paper, we propose a conditional adaptation framework to deal with these challenges. From the perspective of learning theory, we prove that the generalization error of conditional adaptation is lower than previous covariate adaptation. Following the theoretical results, we propose the minimum uncertainty principle to learn conditional invariant transformation via discrepancy optimization. Specifically, we propose the \textit{conditional metric operator} on Hilbert space to characterize the distinctness of conditional distributions. For finite observations, we prove that the empirical estimation is always well-defined and will converge to underlying truth as sample size increases. The results of extensive experiments demonstrate that the proposed model achieves competitive performance under different GLS scenarios.



### Optical flow-based branch segmentation for complex orchard environments
- **Arxiv ID**: http://arxiv.org/abs/2202.13050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13050v1)
- **Published**: 2022-02-26 03:38:20+00:00
- **Updated**: 2022-02-26 03:38:20+00:00
- **Authors**: Alexander You, Cindy Grimm, Joseph R. Davidson
- **Comment**: None
- **Journal**: None
- **Summary**: Machine vision is a critical subsystem for enabling robots to be able to perform a variety of tasks in orchard environments. However, orchards are highly visually complex environments, and computer vision algorithms operating in them must be able to contend with variable lighting conditions and background noise. Past work on enabling deep learning algorithms to operate in these environments has typically required large amounts of hand-labeled data to train a deep neural network or physically controlling the conditions under which the environment is perceived. In this paper, we train a neural network system in simulation only using simulated RGB data and optical flow. This resulting neural network is able to perform foreground segmentation of branches in a busy orchard environment without additional real-world training or using any special setup or equipment beyond a standard camera. Our results show that our system is highly accurate and, when compared to a network using manually labeled RGBD data, achieves significantly more consistent and robust performance across environments that differ from the training set.



### Deep Depth from Focal Stack with Defocus Model for Camera-Setting Invariance
- **Arxiv ID**: http://arxiv.org/abs/2202.13055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13055v1)
- **Published**: 2022-02-26 04:21:08+00:00
- **Updated**: 2022-02-26 04:21:08+00:00
- **Authors**: Yuki Fujimura, Masaaki Iiyama, Takuya Funatomi, Yasuhiro Mukaigawa
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: We propose a learning-based depth from focus/defocus (DFF), which takes a focal stack as input for estimating scene depth. Defocus blur is a useful cue for depth estimation. However, the size of the blur depends on not only scene depth but also camera settings such as focus distance, focal length, and f-number. Current learning-based methods without any defocus models cannot estimate a correct depth map if camera settings are different at training and test times. Our method takes a plane sweep volume as input for the constraint between scene depth, defocus images, and camera settings, and this intermediate representation enables depth estimation with different camera settings at training and test times. This camera-setting invariance can enhance the applicability of learning-based DFF methods. The experimental results also indicate that our method is robust against a synthetic-to-real domain gap, and exhibits state-of-the-art performance.



### An End-to-End Transformer Model for Crowd Localization
- **Arxiv ID**: http://arxiv.org/abs/2202.13065v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13065v2)
- **Published**: 2022-02-26 05:21:30+00:00
- **Updated**: 2022-08-08 10:56:39+00:00
- **Authors**: Dingkang Liang, Wei Xu, Xiang Bai
- **Comment**: Accepted by ECCV 2022. The project page is at
  https://dk-liang.github.io/CLTR/
- **Journal**: None
- **Summary**: Crowd localization, predicting head positions, is a more practical and high-level task than simply counting. Existing methods employ pseudo-bounding boxes or pre-designed localization maps, relying on complex post-processing to obtain the head positions. In this paper, we propose an elegant, end-to-end Crowd Localization Transformer named CLTR that solves the task in the regression-based paradigm. The proposed method views the crowd localization as a direct set prediction problem, taking extracted features and trainable embeddings as input of the transformer-decoder. To reduce the ambiguous points and generate more reasonable matching results, we introduce a KMO-based Hungarian matcher, which adopts the nearby context as the auxiliary matching cost. Extensive experiments conducted on five datasets in various data settings show the effectiveness of our method. In particular, the proposed method achieves the best localization performance on the NWPU-Crowd, UCF-QNRF, and ShanghaiTech Part A datasets.



### A Robust Document Image Watermarking Scheme using Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2202.13067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13067v1)
- **Published**: 2022-02-26 05:28:52+00:00
- **Updated**: 2022-02-26 05:28:52+00:00
- **Authors**: Sulong Ge, Zhihua Xia, Jianwei Fei, Xingming Sun, Jian Weng
- **Comment**: None
- **Journal**: None
- **Summary**: Watermarking is an important copyright protection technology which generally embeds the identity information into the carrier imperceptibly. Then the identity can be extracted to prove the copyright from the watermarked carrier even after suffering various attacks. Most of the existing watermarking technologies take the nature images as carriers. Different from the natural images, document images are not so rich in color and texture, and thus have less redundant information to carry watermarks. This paper proposes an end-to-end document image watermarking scheme using the deep neural network. Specifically, an encoder and a decoder are designed to embed and extract the watermark. A noise layer is added to simulate the various attacks that could be encountered in reality, such as the Cropout, Dropout, Gaussian blur, Gaussian noise, Resize, and JPEG Compression. A text-sensitive loss function is designed to limit the embedding modification on characters. An embedding strength adjustment strategy is proposed to improve the quality of watermarked image with little loss of extraction accuracy. Experimental results show that the proposed document image watermarking technology outperforms three state-of-the-arts in terms of the robustness and image quality.



### Uncertainty-Aware Deep Multi-View Photometric Stereo
- **Arxiv ID**: http://arxiv.org/abs/2202.13071v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13071v2)
- **Published**: 2022-02-26 05:45:52+00:00
- **Updated**: 2022-03-28 10:26:54+00:00
- **Authors**: Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari, Luc Van Gool
- **Comment**: Accepted for publication in IEEE/CVF CVPR 2022. (11 Pages, 6 Figures,
  3 Tables)
- **Journal**: None
- **Summary**: This paper presents a simple and effective solution to the longstanding classical multi-view photometric stereo (MVPS) problem. It is well-known that photometric stereo (PS) is excellent at recovering high-frequency surface details, whereas multi-view stereo (MVS) can help remove the low-frequency distortion due to PS and retain the global geometry of the shape. This paper proposes an approach that can effectively utilize such complementary strengths of PS and MVS. Our key idea is to combine them suitably while considering the per-pixel uncertainty of their estimates. To this end, we estimate per-pixel surface normals and depth using an uncertainty-aware deep-PS network and deep-MVS network, respectively. Uncertainty modeling helps select reliable surface normal and depth estimates at each pixel which then act as a true representative of the dense surface geometry. At each pixel, our approach either selects or discards deep-PS and deep-MVS network prediction depending on the prediction uncertainty measure. For dense, detailed, and precise inference of the object's surface profile, we propose to learn the implicit neural shape representation via a multilayer perceptron (MLP). Our approach encourages the MLP to converge to a natural zero-level set surface using the confident prediction from deep-PS and deep-MVS networks, providing superior dense surface reconstruction. Extensive experiments on the DiLiGenT-MV benchmark dataset show that our method provides high-quality shape recovery with a much lower memory footprint while outperforming almost all of the existing approaches.



### Adversarial Contrastive Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.13072v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2202.13072v1)
- **Published**: 2022-02-26 05:57:45+00:00
- **Updated**: 2022-02-26 05:57:45+00:00
- **Authors**: Wentao Zhu, Hang Shang, Tingxun Lv, Chao Liao, Sen Yang, Ji Liu
- **Comment**: 8 pages, 2 figures
- **Journal**: None
- **Summary**: Recently, learning from vast unlabeled data, especially self-supervised learning, has been emerging and attracted widespread attention. Self-supervised learning followed by the supervised fine-tuning on a few labeled examples can significantly improve label efficiency and outperform standard supervised training using fully annotated data. In this work, we present a novel self-supervised deep learning paradigm based on online hard negative pair mining. Specifically, we design a student-teacher network to generate multi-view of the data for self-supervised learning and integrate hard negative pair mining into the training. Then we derive a new triplet-like loss considering both positive sample pairs and mined hard negative sample pairs. Extensive experiments demonstrate the effectiveness of the proposed method and its components on ILSVRC-2012.



### Global Instance Tracking: Locating Target More Like Humans
- **Arxiv ID**: http://arxiv.org/abs/2202.13073v1
- **DOI**: 10.1109/TPAMI.2022.3153312
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13073v1)
- **Published**: 2022-02-26 06:16:34+00:00
- **Updated**: 2022-02-26 06:16:34+00:00
- **Authors**: Shiyu Hu, Xin Zhao, Lianghua Huang, Kaiqi Huang
- **Comment**: This paper is published in IEEE TPAMI (refer to DOI). Please cite the
  published IEEE TPAMI
- **Journal**: None
- **Summary**: Target tracking, the essential ability of the human visual system, has been simulated by computer vision tasks. However, existing trackers perform well in austere experimental environments but fail in challenges like occlusion and fast motion. The massive gap indicates that researches only measure tracking performance rather than intelligence. How to scientifically judge the intelligence level of trackers? Distinct from decision-making problems, lacking three requirements (a challenging task, a fair environment, and a scientific evaluation procedure) makes it strenuous to answer the question. In this article, we first propose the global instance tracking (GIT) task, which is supposed to search an arbitrary user-specified instance in a video without any assumptions about camera or motion consistency, to model the human visual tracking ability. Whereafter, we construct a high-quality and large-scale benchmark VideoCube to create a challenging environment. Finally, we design a scientific evaluation procedure using human capabilities as the baseline to judge tracking intelligence. Additionally, we provide an online platform with toolkit and an updated leaderboard. Although the experimental results indicate a definite gap between trackers and humans, we expect to take a step forward to generate authentic human-like trackers. The database, toolkit, evaluation server, and baseline results are available at http://videocube.aitestunion.com.



### Utility and Feasibility of a Center Surround Event Camera
- **Arxiv ID**: http://arxiv.org/abs/2202.13076v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, B.7.1; I.4.1
- **Links**: [PDF](http://arxiv.org/pdf/2202.13076v1)
- **Published**: 2022-02-26 06:32:54+00:00
- **Updated**: 2022-02-26 06:32:54+00:00
- **Authors**: Tobi Delbruck, Chenghan Li, Rui Graca, Brian Mcreynolds
- **Comment**: 5 pages, submitted to 29th IEEE International Conference on Image
  Processing (IEEE ICIP 2022)
- **Journal**: None
- **Summary**: Standard dynamic vision sensor (DVS) event cameras output a stream of spatially-independent log-intensity brightness change events so they cannot suppress spatial redundancy. Nearly all biological retinas use an antagonistic center-surround organization. This paper proposes a practical method of implementing a compact, energy-efficient Center Surround DVS (CSDVS) with a surround smoothing network that uses compact polysilicon resistors for lateral resistance. The paper includes behavioral simulation results for the CSDVS (see sites.google.com/view/csdvs/home). The CSDVS would significantly reduce events caused by low spatial frequencies, but amplify the informative high frequency spatiotemporal events.



### SWIS: Self-Supervised Representation Learning For Writer Independent Offline Signature Verification
- **Arxiv ID**: http://arxiv.org/abs/2202.13078v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13078v2)
- **Published**: 2022-02-26 06:33:25+00:00
- **Updated**: 2022-07-12 07:05:09+00:00
- **Authors**: Siladittya Manna, Soumitri Chattopadhyay, Saumik Bhattacharya, Umapada Pal
- **Comment**: Accepted at IEEE ICIP 2022
- **Journal**: None
- **Summary**: Writer independent offline signature verification is one of the most challenging tasks in pattern recognition as there is often a scarcity of training data. To handle such data scarcity problem, in this paper, we propose a novel self-supervised learning (SSL) framework for writer independent offline signature verification. To our knowledge, this is the first attempt to utilize self-supervised setting for the signature verification task. The objective of self-supervised representation learning from the signature images is achieved by minimizing the cross-covariance between two random variables belonging to different feature directions and ensuring a positive cross-covariance between the random variables denoting the same feature direction. This ensures that the features are decorrelated linearly and the redundant information is discarded. Through experimental results on different data sets, we obtained encouraging results.



### Improved Hard Example Mining Approach for Single Shot Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2202.13080v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13080v2)
- **Published**: 2022-02-26 06:44:53+00:00
- **Updated**: 2022-07-12 15:35:27+00:00
- **Authors**: Aybora Koksal, Onder Tuzcuoglu, Kutalmis Gokalp Ince, Yoldas Ataseven, A. Aydin Alatan
- **Comment**: ICIP 2022. 5 pages, 2 figures, 7 tables. The codes are available at
  https://github.com/aybora/yolov5Loss
- **Journal**: None
- **Summary**: Hard example mining methods generally improve the performance of the object detectors, which suffer from imbalanced training sets. In this work, two existing hard example mining approaches (LRM and focal loss, FL) are adapted and combined in a state-of-the-art real-time object detector, YOLOv5. The effectiveness of the proposed approach for improving the performance on hard examples is extensively evaluated. The proposed method increases mAP by 3% compared to using the original loss function and around 1-2% compared to using the hard-mining methods (LRM or FL) individually on 2021 Anti-UAV Challenge Dataset.



### An Improved Deep Learning Approach For Product Recognition on Racks in Retail Stores
- **Arxiv ID**: http://arxiv.org/abs/2202.13081v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13081v1)
- **Published**: 2022-02-26 06:51:36+00:00
- **Updated**: 2022-02-26 06:51:36+00:00
- **Authors**: Ankit Sinha, Soham Banerjee, Pratik Chattopadhyay
- **Comment**: submitted to Machine Vision and Applications, Springer
- **Journal**: None
- **Summary**: Automated product recognition in retail stores is an important real-world application in the domain of Computer Vision and Pattern Recognition. In this paper, we consider the problem of automatically identifying the classes of the products placed on racks in retail stores from an image of the rack and information about the query/product images. We improve upon the existing approaches in terms of effectiveness and memory requirement by developing a two-stage object detection and recognition pipeline comprising of a Faster-RCNN-based object localizer that detects the object regions in the rack image and a ResNet-18-based image encoder that classifies the detected regions into the appropriate classes. Each of the models is fine-tuned using appropriate data sets for better prediction and data augmentation is performed on each query image to prepare an extensive gallery set for fine-tuning the ResNet-18-based product recognition model. This encoder is trained using a triplet loss function following the strategy of online-hard-negative-mining for improved prediction. The proposed models are lightweight and can be connected in an end-to-end manner during deployment for automatically identifying each product object placed in a rack image. Extensive experiments using Grozi-32k and GP-180 data sets verify the effectiveness of the proposed model.



### Visual Speech Recognition for Multiple Languages in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2202.13084v2
- **DOI**: 10.1038/s42256-022-00550-z
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2202.13084v2)
- **Published**: 2022-02-26 07:21:00+00:00
- **Updated**: 2022-10-30 17:29:57+00:00
- **Authors**: Pingchuan Ma, Stavros Petridis, Maja Pantic
- **Comment**: Published in Nature Machine Intelligence
- **Journal**: None
- **Summary**: Visual speech recognition (VSR) aims to recognize the content of speech based on lip movements, without relying on the audio stream. Advances in deep learning and the availability of large audio-visual datasets have led to the development of much more accurate and robust VSR models than ever before. However, these advances are usually due to the larger training sets rather than the model design. Here we demonstrate that designing better models is equally as important as using larger training sets. We propose the addition of prediction-based auxiliary tasks to a VSR model, and highlight the importance of hyperparameter optimization and appropriate data augmentations. We show that such a model works for different languages and outperforms all previous methods trained on publicly available datasets by a large margin. It even outperforms models that were trained on non-publicly available datasets containing up to to 21 times more data. We show, furthermore, that using additional training data, even in other languages or with automatically generated transcriptions, results in further improvement.



### RIConv++: Effective Rotation Invariant Convolutions for 3D Point Clouds Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.13094v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.13094v2)
- **Published**: 2022-02-26 08:32:44+00:00
- **Updated**: 2022-03-20 07:58:36+00:00
- **Authors**: Zhiyuan Zhang, Binh-Son Hua, Sai-Kit Yeung
- **Comment**: Authors' version. Accepted to International Journal of Computer
  Vision (IJCV) 2022
- **Journal**: None
- **Summary**: 3D point clouds deep learning is a promising field of research that allows a neural network to learn features of point clouds directly, making it a robust tool for solving 3D scene understanding tasks. While recent works show that point cloud convolutions can be invariant to translation and point permutation, investigations of the rotation invariance property for point cloud convolution has been so far scarce. Some existing methods perform point cloud convolutions with rotation-invariant features, existing methods generally do not perform as well as translation-invariant only counterpart. In this work, we argue that a key reason is that compared to point coordinates, rotation-invariant features consumed by point cloud convolution are not as distinctive. To address this problem, we propose a simple yet effective convolution operator that enhances feature distinction by designing powerful rotation invariant features from the local regions. We consider the relationship between the point of interest and its neighbors as well as the internal relationship of the neighbors to largely improve the feature descriptiveness. Our network architecture can capture both local and global context by simply tuning the neighborhood size in each convolution layer. We conduct several experiments on synthetic and real-world point cloud classifications, part segmentation, and shape retrieval to evaluate our method, which achieves the state-of-the-art accuracy under challenging rotations.



### Continuous Human Action Recognition for Human-Machine Interaction: A Review
- **Arxiv ID**: http://arxiv.org/abs/2202.13096v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.13096v1)
- **Published**: 2022-02-26 09:25:44+00:00
- **Updated**: 2022-02-26 09:25:44+00:00
- **Authors**: Harshala Gammulle, David Ahmedt-Aristizabal, Simon Denman, Lachlan Tychsen-Smith, Lars Petersson, Clinton Fookes
- **Comment**: Preprint submitted to ACM Computing Surveys
- **Journal**: None
- **Summary**: With advances in data-driven machine learning research, a wide variety of prediction models have been proposed to capture spatio-temporal features for the analysis of video streams. Recognising actions and detecting action transitions within an input video are challenging but necessary tasks for applications that require real-time human-machine interaction. By reviewing a large body of recent related work in the literature, we thoroughly analyse, explain and compare action segmentation methods and provide details on the feature extraction and learning strategies that are used on most state-of-the-art methods. We cover the impact of the performance of object detection and tracking techniques on human action segmentation methodologies. We investigate the application of such models to real-world scenarios and discuss several limitations and key research directions towards improving interpretability, generalisation, optimisation and deployment.



### Symmetric Convolutional Filters: A Novel Way to Constrain Parameters in CNN
- **Arxiv ID**: http://arxiv.org/abs/2202.13099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13099v1)
- **Published**: 2022-02-26 09:45:30+00:00
- **Updated**: 2022-02-26 09:45:30+00:00
- **Authors**: Harish Agrawal, Sumana T., S. K. Nandy
- **Comment**: Copyright 2022 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: We propose a novel technique to constrain parameters in CNN based on symmetric filters. We investigate the impact on SOTA networks when varying the combinations of symmetricity. We demonstrate that our models offer effective generalisation and a structured elimination of redundancy in parameters. We conclude by comparing our method with other pruning techniques.



### SemSup: Semantic Supervision for Simple and Scalable Zero-shot Generalization
- **Arxiv ID**: http://arxiv.org/abs/2202.13100v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13100v4)
- **Published**: 2022-02-26 09:55:54+00:00
- **Updated**: 2023-01-31 04:55:22+00:00
- **Authors**: Austin W. Hanjie, Ameet Deshpande, Karthik Narasimhan
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot learning is the problem of predicting instances over classes not seen during training. One approach to zero-shot learning is providing auxiliary class information to the model. Prior work along this vein have largely used expensive per-instance annotation or singular class-level descriptions, but per-instance descriptions are hard to scale and single class descriptions may not be rich enough. Furthermore, these works have used natural-language descriptions exclusively, simple bi-encoders models, and modality or task-specific methods. These approaches have several limitations: text supervision may not always be available or optimal and bi-encoders may only learn coarse relations between inputs and class descriptions. In this work, we present SemSup, a novel approach that uses (1) a scalable multiple description sampling method which improves performance over single descriptions, (2) alternative description formats such as JSON that are easy to generate and outperform text on certain settings, and (3) hybrid lexical-semantic similarity to leverage fine-grained information in class descriptions. We demonstrate the effectiveness of SemSup across four datasets, two modalities, and three generalization settings. For example, across text and image datasets, SemSup increases unseen class generalization accuracy by 15 points on average compared to the closest baseline.



### Analysis of Visual Reasoning on One-Stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.13115v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.13115v1)
- **Published**: 2022-02-26 11:11:59+00:00
- **Updated**: 2022-02-26 11:11:59+00:00
- **Authors**: Tolga Aksoy, Ugur Halici
- **Comment**: Submitted to IEEE International Conference on Image Processing (ICIP)
  2022
- **Journal**: None
- **Summary**: Current state-of-the-art one-stage object detectors are limited by treating each image region separately without considering possible relations of the objects. This causes dependency solely on high-quality convolutional feature representations for detecting objects successfully. However, this may not be possible sometimes due to some challenging conditions. In this paper, the usage of reasoning features on one-stage object detection is analyzed. We attempted different architectures that reason the relations of the image regions by using self-attention. YOLOv3-Reasoner2 model spatially and semantically enhances features in the reasoning layer and fuses them with the original convolutional features to improve performance. The YOLOv3-Reasoner2 model achieves around 2.5% absolute improvement with respect to baseline YOLOv3 on COCO in terms of mAP while still running in real-time.



### An Unsupervised Cross-Modal Hashing Method Robust to Noisy Training Image-Text Correspondences in Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/2202.13117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13117v1)
- **Published**: 2022-02-26 11:22:24+00:00
- **Updated**: 2022-02-26 11:22:24+00:00
- **Authors**: Georgii Mikriukov, Mahdyar Ravanbakhsh, Begüm Demir
- **Comment**: https://git.tu-berlin.de/rsim/chnr
- **Journal**: None
- **Summary**: The development of accurate and scalable cross-modal image-text retrieval methods, where queries from one modality (e.g., text) can be matched to archive entries from another (e.g., remote sensing image) has attracted great attention in remote sensing (RS). Most of the existing methods assume that a reliable multi-modal training set with accurately matched text-image pairs is existing. However, this assumption may not always hold since the multi-modal training sets may include noisy pairs (i.e., textual descriptions/captions associated to training images can be noisy), distorting the learning process of the retrieval methods. To address this problem, we propose a novel unsupervised cross-modal hashing method robust to the noisy image-text correspondences (CHNR). CHNR consists of three modules: 1) feature extraction module, which extracts feature representations of image-text pairs; 2) noise detection module, which detects potential noisy correspondences; and 3) hashing module that generates cross-modal binary hash codes. The proposed CHNR includes two training phases: i) meta-learning phase that uses a small portion of clean (i.e., reliable) data to train the noise detection module in an adversarial fashion; and ii) the main training phase for which the trained noise detection module is used to identify noisy correspondences while the hashing module is trained on the noisy multi-modal training set. Experimental results show that the proposed CHNR outperforms state-of-the-art methods. Our code is publicly available at https://git.tu-berlin.de/rsim/chnr



### Accurate Human Body Reconstruction for Volumetric Video
- **Arxiv ID**: http://arxiv.org/abs/2202.13118v1
- **DOI**: 10.1109/IC3D53758.2021.9687256
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13118v1)
- **Published**: 2022-02-26 11:37:08+00:00
- **Updated**: 2022-02-26 11:37:08+00:00
- **Authors**: Decai Chen, Markus Worchel, Ingo Feldmann, Oliver Schreer, Peter Eisert
- **Comment**: 2021 International Conference on 3D Immersion (IC3D)
- **Journal**: None
- **Summary**: In this work, we enhance a professional end-to-end volumetric video production pipeline to achieve high-fidelity human body reconstruction using only passive cameras. While current volumetric video approaches estimate depth maps using traditional stereo matching techniques, we introduce and optimize deep learning-based multi-view stereo networks for depth map estimation in the context of professional volumetric video reconstruction. Furthermore, we propose a novel depth map post-processing approach including filtering and fusion, by taking into account photometric confidence, cross-view geometric consistency, foreground masks as well as camera viewing frustums. We show that our method can generate high levels of geometric detail for reconstructed human bodies.



### Person Re-identification: A Retrospective on Domain Specific Open Challenges and Future Trends
- **Arxiv ID**: http://arxiv.org/abs/2202.13121v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.13121v1)
- **Published**: 2022-02-26 11:55:57+00:00
- **Updated**: 2022-02-26 11:55:57+00:00
- **Authors**: Asmat Zahra, Nazia Perwaiz, Muhammad Shahzad, Muhammad Moazam Fraz
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) is one of the primary components of an automated visual surveillance system. It aims to automatically identify/search persons in a multi-camera network having non-overlapping field-of-views. Owing to its potential in various applications and research significance, a plethora of deep learning based re-Id approaches have been proposed in the recent years. However, there exist several vision related challenges, e.g., occlusion, pose scale \& viewpoint variance, background clutter, person misalignment and cross-domain generalization across camera modalities, which makes the problem of re-Id still far from being solved. Majority of the proposed approaches directly or indirectly aim to solve one or multiple of these existing challenges. In this context, a comprehensive review of current re-ID approaches in solving theses challenges is needed to analyze and focus on particular aspects for further advancements. At present, such a focused review does not exist and henceforth in this paper, we have presented a systematic challenge-specific literature survey of 230+ papers between the years of 2015-21. For the first time a survey of this type have been presented where the person re-Id approaches are reviewed in such solution-oriented perspective. Moreover, we have presented several diversified prominent developing trends in the respective research domain which will provide a visionary perspective regarding ongoing person re-Id research and eventually help to develop practical real world solutions.



### Content-Variant Reference Image Quality Assessment via Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2202.13123v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13123v1)
- **Published**: 2022-02-26 12:04:56+00:00
- **Updated**: 2022-02-26 12:04:56+00:00
- **Authors**: Guanghao Yin, Wei Wang, Zehuan Yuan, Chuchu Han, Wei Ji, Shouqian Sun, Changhu Wang
- **Comment**: AAAI2022 oral accepted
- **Journal**: None
- **Summary**: Generally, humans are more skilled at perceiving differences between high-quality (HQ) and low-quality (LQ) images than directly judging the quality of a single LQ image. This situation also applies to image quality assessment (IQA). Although recent no-reference (NR-IQA) methods have made great progress to predict image quality free from the reference image, they still have the potential to achieve better performance since HQ image information is not fully exploited. In contrast, full-reference (FR-IQA) methods tend to provide more reliable quality evaluation, but its practicability is affected by the requirement for pixel-level aligned reference images. To address this, we firstly propose the content-variant reference method via knowledge distillation (CVRKD-IQA). Specifically, we use non-aligned reference (NAR) images to introduce various prior distributions of high-quality images. The comparisons of distribution differences between HQ and LQ images can help our model better assess the image quality. Further, the knowledge distillation transfers more HQ-LQ distribution difference information from the FR-teacher to the NAR-student and stabilizing CVRKD-IQA performance. Moreover, to fully mine the local-global combined information, while achieving faster inference speed, our model directly processes multiple image patches from the input with the MLP-mixer. Cross-dataset experiments verify that our model can outperform all NAR/NR-IQA SOTAs, even reach comparable performance with FR-IQA methods on some occasions. Since the content-variant and non-aligned reference HQ images are easy to obtain, our model can support more IQA applications with its relative robustness to content variations. Our code and more detailed elaborations of supplements are available: https://github.com/guanghaoyin/CVRKD-IQA.



### Multi-image Super-resolution via Quality Map Associated Attention Network
- **Arxiv ID**: http://arxiv.org/abs/2202.13124v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13124v3)
- **Published**: 2022-02-26 12:10:08+00:00
- **Updated**: 2022-08-16 01:39:54+00:00
- **Authors**: Minji Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-image super-resolution, which aims to fuse and restore a high-resolution image from multiple images at the same location, is crucial for utilizing satellite images. The satellite images are often occluded by atmospheric disturbances such as clouds, and the position of the disturbances varies by the images. Many radiometric and geometric approaches are proposed to detect atmospheric disturbances. Still, the utilization of detection results, i.e., quality maps in deep learning was limited to pre-processing or computation of loss. In this paper, we present a quality map-associated attention network (QA-Net), an architecture that fully incorporates QMs into a deep learning scheme for the first time. Our proposed attention modules process QMs alongside the low-resolution images and utilize the QM features to distinguish the disturbances and attend to image features. As a result, QA-Net has achieved state-of-the-art results in the PROBA-V dataset.



### Automation of reversible steganographic coding with nonlinear discrete optimisation
- **Arxiv ID**: http://arxiv.org/abs/2202.13133v2
- **DOI**: 10.1080/09540091.2022.2078792
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2202.13133v2)
- **Published**: 2022-02-26 13:02:32+00:00
- **Updated**: 2023-03-05 12:16:33+00:00
- **Authors**: Ching-Chun Chang
- **Comment**: None
- **Journal**: Connection Science (2022), vol. 34, no. 1, 1719-1735
- **Summary**: Authentication mechanisms are at the forefront of defending the world from various types of cybercrime. Steganography can serve as an authentication solution through the use of a digital signature embedded in a carrier object to ensure the integrity of the object and simultaneously lighten the burden of metadata management. Nevertheless, despite being generally imperceptible to human sensory systems, any degree of steganographic distortion might be inadmissible in fidelity-sensitive situations such as forensic science, legal proceedings, medical diagnosis and military reconnaissance. This has led to the development of reversible steganography. A fundamental element of reversible steganography is predictive analytics, for which powerful neural network models have been effectively deployed. Another core element is reversible steganographic coding. Contemporary coding is based primarily on heuristics, which offers a shortcut towards sufficient, but not necessarily optimal, capacity--distortion performance. While attempts have been made to realise automatic coding with neural networks, perfect reversibility is unattainable via such learning machinery. Instead of relying on heuristics and machine learning, we aim to derive optimal coding by means of mathematical optimisation. In this study, we formulate reversible steganographic coding as a nonlinear discrete optimisation problem with a logarithmic capacity constraint and a quadratic distortion objective. Linearisation techniques are developed to enable iterative mixed-integer linear programming. Experimental results validate the near-optimality of the proposed optimisation algorithm when benchmarked against a brute-force method.



### RONELDv2: A faster, improved lane tracking method
- **Arxiv ID**: http://arxiv.org/abs/2202.13137v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.13137v1)
- **Published**: 2022-02-26 13:12:09+00:00
- **Updated**: 2022-02-26 13:12:09+00:00
- **Authors**: Zhe Ming Chng, Joseph Mun Hung Lew, Jimmy Addison Lee
- **Comment**: 9 pages, 8 figures, 6 tables
- **Journal**: None
- **Summary**: Lane detection is an integral part of control systems in autonomous vehicles and lane departure warning systems as lanes are a key component of the operating environment for road vehicles. In a previous paper, a robust neural network output enhancement for active lane detection (RONELD) method augmenting deep learning lane detection models to improve active, or ego, lane accuracy performance was presented. This paper extends the work by further investigating the lane tracking methods used to increase robustness of the method to lane changes and different lane dimensions (e.g. lane marking thickness) and proposes an improved, lighter weight lane detection method, RONELDv2. It improves on the previous RONELD method by detecting the lane point variance, merging lanes to find a more accurate set of lane parameters, and using an exponential moving average method to calculate more robust lane weights. Experiments using the proposed improvements show a consistent increase in lane detection accuracy results across different datasets and deep learning models, as well as a decrease in computational complexity observed via an up to two-fold decrease in runtime, which enhances its suitability for real-time use on autonomous vehicles and lane departure warning systems.



### Real-World Blind Super-Resolution via Feature Matching with Implicit High-Resolution Priors
- **Arxiv ID**: http://arxiv.org/abs/2202.13142v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13142v2)
- **Published**: 2022-02-26 13:38:11+00:00
- **Updated**: 2022-07-03 12:57:22+00:00
- **Authors**: Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, Shihui Guo
- **Comment**: Accepted to ACM MM2022
- **Journal**: None
- **Summary**: A key challenge of real-world image super-resolution (SR) is to recover the missing details in low-resolution (LR) images with complex unknown degradations (e.g., downsampling, noise and compression). Most previous works restore such missing details in the image space. To cope with the high diversity of natural images, they either rely on the unstable GANs that are difficult to train and prone to artifacts, or resort to explicit references from high-resolution (HR) images that are usually unavailable. In this work, we propose Feature Matching SR (FeMaSR), which restores realistic HR images in a much more compact feature space. Unlike image-space methods, our FeMaSR restores HR images by matching distorted LR image {\it features} to their distortion-free HR counterparts in our pretrained HR priors, and decoding the matched features to obtain realistic HR images. Specifically, our HR priors contain a discrete feature codebook and its associated decoder, which are pretrained on HR images with a Vector Quantized Generative Adversarial Network (VQGAN). Notably, we incorporate a novel semantic regularization in VQGAN to improve the quality of reconstructed images. For the feature matching, we first extract LR features with an LR encoder consisting of several Swin Transformer blocks and then follow a simple nearest neighbour strategy to match them with the pretrained codebook. In particular, we equip the LR encoder with residual shortcut connections to the decoder, which is critical to the optimization of feature matching loss and also helps to complement the possible feature matching errors. Experimental results show that our approach produces more realistic HR images than previous methods. Codes are released at \url{https://github.com/chaofengc/FeMaSR}.



### DGSS : Domain Generalized Semantic Segmentation using Iterative Style Mining and Latent Representation Alignment
- **Arxiv ID**: http://arxiv.org/abs/2202.13144v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.13144v1)
- **Published**: 2022-02-26 13:54:57+00:00
- **Updated**: 2022-02-26 13:54:57+00:00
- **Authors**: Pranjay Shyam, Antyanta Bangunharcana, Kuk-Jin Yoon, Kyung-Soo Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation algorithms require access to well-annotated datasets captured under diverse illumination conditions to ensure consistent performance. However, poor visibility conditions at varying illumination conditions result in laborious and error-prone labeling. Alternatively, using synthetic samples to train segmentation algorithms has gained interest with the drawback of domain gap that results in sub-optimal performance. While current state-of-the-art (SoTA) have proposed different mechanisms to bridge the domain gap, they still perform poorly in low illumination conditions with an average performance drop of - 10.7 mIOU. In this paper, we focus upon single source domain generalization to overcome the domain gap and propose a two-step framework wherein we first identify an adversarial style that maximizes the domain gap between stylized and source images. Subsequently, these stylized images are used to categorically align features such that features belonging to the same class are clustered together in latent space, irrespective of domain gap. Furthermore, to increase intra-class variance while training, we propose a style mixing mechanism wherein the same objects from different styles are mixed to construct a new training image. This framework allows us to achieve a domain generalized semantic segmentation algorithm with consistent performance without prior information of the target domain while relying on a single source. Based on extensive experiments, we match SoTA performance on SYNTHIA $\to$ Cityscapes, GTAV $\to$ Cityscapes while setting new SoTA on GTAV $\to$ Dark Zurich and GTAV $\to$ Night Driving benchmarks without retraining.



### Pix2NeRF: Unsupervised Conditional $π$-GAN for Single Image to Neural Radiance Fields Translation
- **Arxiv ID**: http://arxiv.org/abs/2202.13162v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.13162v1)
- **Published**: 2022-02-26 15:28:05+00:00
- **Updated**: 2022-02-26 15:28:05+00:00
- **Authors**: Shengqu Cai, Anton Obukhov, Dengxin Dai, Luc Van Gool
- **Comment**: 16 pages, 10 figures
- **Journal**: None
- **Summary**: We propose a pipeline to generate Neural Radiance Fields~(NeRF) of an object or a scene of a specific class, conditioned on a single input image. This is a challenging task, as training NeRF requires multiple views of the same scene, coupled with corresponding poses, which are hard to obtain. Our method is based on $\pi$-GAN, a generative model for unconditional 3D-aware image synthesis, which maps random latent codes to radiance fields of a class of objects. We jointly optimize (1) the $\pi$-GAN objective to utilize its high-fidelity 3D-aware generation and (2) a carefully designed reconstruction objective. The latter includes an encoder coupled with $\pi$-GAN generator to form an auto-encoder. Unlike previous few-shot NeRF approaches, our pipeline is unsupervised, capable of being trained with independent images without 3D, multi-view, or pose supervision. Applications of our pipeline include 3d avatar generation, object-centric novel view synthesis with a single input image, and 3d-aware super-resolution, to name a few.



### Edge Augmentation for Large-Scale Sketch Recognition without Sketches
- **Arxiv ID**: http://arxiv.org/abs/2202.13164v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13164v2)
- **Published**: 2022-02-26 15:37:38+00:00
- **Updated**: 2022-06-02 18:46:37+00:00
- **Authors**: Nikos Efthymiadis, Giorgos Tolias, Ondrej Chum
- **Comment**: None
- **Journal**: None
- **Summary**: This work addresses scaling up the sketch classification task into a large number of categories. Collecting sketches for training is a slow and tedious process that has so far precluded any attempts to large-scale sketch recognition. We overcome the lack of training sketch data by exploiting labeled collections of natural images that are easier to obtain. To bridge the domain gap we present a novel augmentation technique that is tailored to the task of learning sketch recognition from a training set of natural images. Randomization is introduced in the parameters of edge detection and edge selection. Natural images are translated to a pseudo-novel domain called "randomized Binary Thin Edges" (rBTE), which is used as a training domain instead of natural images. The ability to scale up is demonstrated by training CNN-based sketch recognition of more than 2.5 times larger number of categories than used previously. For this purpose, a dataset of natural images from 874 categories is constructed by combining a number of popular computer vision datasets. The categories are selected to be suitable for sketch recognition. To estimate the performance, a subset of 393 categories with sketches is also collected.



### Unsupervised Domain Adaptive Salient Object Detection Through Uncertainty-Aware Pseudo-Label Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.13170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13170v1)
- **Published**: 2022-02-26 16:03:55+00:00
- **Updated**: 2022-02-26 16:03:55+00:00
- **Authors**: Pengxiang Yan, Ziyi Wu, Mengmeng Liu, Kun Zeng, Liang Lin, Guanbin Li
- **Comment**: Accepted by AAAI2022, code is available at
  https://github.com/Kinpzz/UDASOD-UPL
- **Journal**: None
- **Summary**: Recent advances in deep learning significantly boost the performance of salient object detection (SOD) at the expense of labeling larger-scale per-pixel annotations. To relieve the burden of labor-intensive labeling, deep unsupervised SOD methods have been proposed to exploit noisy labels generated by handcrafted saliency methods. However, it is still difficult to learn accurate saliency details from rough noisy labels. In this paper, we propose to learn saliency from synthetic but clean labels, which naturally has higher pixel-labeling quality without the effort of manual annotations. Specifically, we first construct a novel synthetic SOD dataset by a simple copy-paste strategy. Considering the large appearance differences between the synthetic and real-world scenarios, directly training with synthetic data will lead to performance degradation on real-world scenarios. To mitigate this problem, we propose a novel unsupervised domain adaptive SOD method to adapt between these two domains by uncertainty-aware self-training. Experimental results show that our proposed method outperforms the existing state-of-the-art deep unsupervised SOD methods on several benchmark datasets, and is even comparable to fully-supervised ones.



### Relational Surrogate Loss Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.13197v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13197v1)
- **Published**: 2022-02-26 17:32:57+00:00
- **Updated**: 2022-02-26 17:32:57+00:00
- **Authors**: Tao Huang, Zekang Li, Hua Lu, Yong Shan, Shusheng Yang, Yang Feng, Fei Wang, Shan You, Chang Xu
- **Comment**: Accepted to ICLR 2022
- **Journal**: None
- **Summary**: Evaluation metrics in machine learning are often hardly taken as loss functions, as they could be non-differentiable and non-decomposable, e.g., average precision and F1 score. This paper aims to address this problem by revisiting the surrogate loss learning, where a deep neural network is employed to approximate the evaluation metrics. Instead of pursuing an exact recovery of the evaluation metric through a deep neural network, we are reminded of the purpose of the existence of these evaluation metrics, which is to distinguish whether one model is better or worse than another. In this paper, we show that directly maintaining the relation of models between surrogate losses and metrics suffices, and propose a rank correlation-based optimization method to maximize this relation and learn surrogate losses. Compared to previous works, our method is much easier to optimize and enjoys significant efficiency and performance gains. Extensive experiments show that our method achieves improvements on various tasks including image classification and neural machine translation, and even outperforms state-of-the-art methods on human pose estimation and machine reading comprehension tasks. Code is available at: https://github.com/hunto/ReLoss.



### Dropout can Simulate Exponential Number of Models for Sample Selection Techniques
- **Arxiv ID**: http://arxiv.org/abs/2202.13203v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13203v1)
- **Published**: 2022-02-26 17:53:26+00:00
- **Updated**: 2022-02-26 17:53:26+00:00
- **Authors**: Lakshya
- **Comment**: None
- **Journal**: None
- **Summary**: Following Coteaching, generally in the literature, two models are used in sample selection based approaches for training with noisy labels. Meanwhile, it is also well known that Dropout when present in a network trains an ensemble of sub-networks. We show how to leverage this property of Dropout to train an exponential number of shared models, by training a single model with Dropout. We show how we can modify existing two model-based sample selection methodologies to use an exponential number of shared models. Not only is it more convenient to use a single model with Dropout, but this approach also combines the natural benefits of Dropout with that of training an exponential number of models, leading to improved results.



### How Much Depth Information can Radar Contribute to a Depth Estimation Model?
- **Arxiv ID**: http://arxiv.org/abs/2202.13220v2
- **DOI**: 10.2352/EI.2023.35.16.AVM-122
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13220v2)
- **Published**: 2022-02-26 20:02:47+00:00
- **Updated**: 2023-03-15 10:26:32+00:00
- **Authors**: Chen-Chou Lo, Patrick Vandewalle
- **Comment**: published on EI2023, 7 pages, 4 figures, 2 tables
- **Journal**: None
- **Summary**: Recently, several works have proposed fusing radar data as an additional perceptual signal into monocular depth estimation models because radar data is robust against varying light and weather conditions. Although improved performances were reported in prior works, it is still hard to tell how much depth information radar can contribute to a depth estimation model. In this paper, we propose radar inference and supervision experiments to investigate the intrinsic depth potential of radar data using state-of-the-art depth estimation models on the nuScenes dataset. In the inference experiment, the model predicts depth by taking only radar as input to demonstrate the inference capability using radar data. In the supervision experiment, a monocular depth estimation model is trained under radar supervision to show the intrinsic depth information that radar can contribute. Our experiments demonstrate that the model using only sparse radar as input can detect the shape of surroundings to a certain extent in the predicted depth. Furthermore, the monocular depth estimation model supervised by preprocessed radar achieves a good performance compared to the baseline model trained with sparse lidar supervision.



### Orientation-Discriminative Feature Representation for Decentralized Pedestrian Tracking
- **Arxiv ID**: http://arxiv.org/abs/2202.13237v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.13237v1)
- **Published**: 2022-02-26 22:03:58+00:00
- **Updated**: 2022-02-26 22:03:58+00:00
- **Authors**: Vikram Shree, Carlos Diaz-Ruiz, Chang Liu, Bharath Hariharan, Mark Campbell
- **Comment**: 8 pages, 4 figures, submitted to IEEE/RSJ International Conference on
  Intelligent Robots and Systems
- **Journal**: None
- **Summary**: This paper focuses on the problem of decentralized pedestrian tracking using a sensor network. Traditional works on pedestrian tracking usually use a centralized framework, which becomes less practical for robotic applications due to limited communication bandwidth. Our paper proposes a communication-efficient, orientation-discriminative feature representation to characterize pedestrian appearance information, that can be shared among sensors. Building upon that representation, our work develops a cross-sensor track association approach to achieve decentralized tracking. Extensive evaluations are conducted on publicly available datasets and results show that our proposed approach leads to improved performance in multi-sensor tracking.



### QOC: Quantum On-Chip Training with Parameter Shift and Gradient Pruning
- **Arxiv ID**: http://arxiv.org/abs/2202.13239v2
- **DOI**: None
- **Categories**: **quant-ph**, cs.AR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.13239v2)
- **Published**: 2022-02-26 22:27:36+00:00
- **Updated**: 2022-04-22 20:07:36+00:00
- **Authors**: Hanrui Wang, Zirui Li, Jiaqi Gu, Yongshan Ding, David Z. Pan, Song Han
- **Comment**: Published as a conference paper in DAC 2022; 7 pages, 8 figures;
  open-source at https://github.com/mit-han-lab/torchquantum
- **Journal**: None
- **Summary**: Parameterized Quantum Circuits (PQC) are drawing increasing research interest thanks to its potential to achieve quantum advantages on near-term Noisy Intermediate Scale Quantum (NISQ) hardware. In order to achieve scalable PQC learning, the training process needs to be offloaded to real quantum machines instead of using exponential-cost classical simulators. One common approach to obtain PQC gradients is parameter shift whose cost scales linearly with the number of qubits. We present QOC, the first experimental demonstration of practical on-chip PQC training with parameter shift. Nevertheless, we find that due to the significant quantum errors (noises) on real machines, gradients obtained from naive parameter shift have low fidelity and thus degrading the training accuracy. To this end, we further propose probabilistic gradient pruning to firstly identify gradients with potentially large errors and then remove them. Specifically, small gradients have larger relative errors than large ones, thus having a higher probability to be pruned. We perform extensive experiments with the Quantum Neural Network (QNN) benchmarks on 5 classification tasks using 5 real quantum machines. The results demonstrate that our on-chip training achieves over 90% and 60% accuracy for 2-class and 4-class image classification tasks. The probabilistic gradient pruning brings up to 7% PQC accuracy improvements over no pruning. Overall, we successfully obtain similar on-chip training accuracy compared with noise-free simulation but have much better training scalability. The QOC code is available in the TorchQuantum library.



### Supervising Remote Sensing Change Detection Models with 3D Surface Semantics
- **Arxiv ID**: http://arxiv.org/abs/2202.13251v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.13251v1)
- **Published**: 2022-02-26 23:35:43+00:00
- **Updated**: 2022-02-26 23:35:43+00:00
- **Authors**: Isaac Corley, Peyman Najafirad
- **Comment**: None
- **Journal**: None
- **Summary**: Remote sensing change detection, identifying changes between scenes of the same location, is an active area of research with a broad range of applications. Recent advances in multimodal self-supervised pretraining have resulted in state-of-the-art methods which surpass vision models trained solely on optical imagery. In the remote sensing field, there is a wealth of overlapping 2D and 3D modalities which can be exploited to supervise representation learning in vision models. In this paper we propose Contrastive Surface-Image Pretraining (CSIP) for joint learning using optical RGB and above ground level (AGL) map pairs. We then evaluate these pretrained models on several building segmentation and change detection datasets to show that our method does, in fact, extract features relevant to downstream applications where natural and artificial surface information is relevant.



