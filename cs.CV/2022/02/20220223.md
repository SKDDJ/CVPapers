# Arxiv Papers in cs.CV on 2022-02-23
### FUNQUE: Fusion of Unified Quality Evaluators
- **Arxiv ID**: http://arxiv.org/abs/2202.11241v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.11241v2)
- **Published**: 2022-02-23 00:21:43+00:00
- **Updated**: 2022-07-06 17:18:13+00:00
- **Authors**: Abhinau K. Venkataramanan, Cosmin Stejerean, Alan C. Bovik
- **Comment**: Accepted at ICIP 2022
- **Journal**: None
- **Summary**: Fusion-based quality assessment has emerged as a powerful method for developing high-performance quality models from quality models that individually achieve lower performances. A prominent example of such an algorithm is VMAF, which has been widely adopted as an industry standard for video quality prediction along with SSIM. In addition to advancing the state-of-the-art, it is imperative to alleviate the computational burden presented by the use of a heterogeneous set of quality models. In this paper, we unify "atom" quality models by computing them on a common transform domain that accounts for the Human Visual System, and we propose FUNQUE, a quality model that fuses unified quality evaluators. We demonstrate that in comparison to the state-of-the-art, FUNQUE offers significant improvements in both correlation against subjective scores and efficiency, due to computation sharing.



### An End-to-End Cascaded Image Deraining and Object Detection Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2202.11279v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.11279v1)
- **Published**: 2022-02-23 02:48:34+00:00
- **Updated**: 2022-02-23 02:48:34+00:00
- **Authors**: Kaige Wang, Tianming Wang, Jianchuang Qu, Huatao Jiang, Qing Li, Lin Chang
- **Comment**: None
- **Journal**: None
- **Summary**: While the deep learning-based image deraining methods have made great progress in recent years, there are two major shortcomings in their application in real-world situations. Firstly, the gap between the low-level vision task represented by rain removal and the high-level vision task represented by object detection is significant, and the low-level vision task can hardly contribute to the high-level vision task. Secondly, the quality of the deraining dataset needs to be improved. In fact, the rain lines in many baselines have a large gap with the real rain lines, and the resolution of the deraining dataset images is generally not ideally. Meanwhile, there are few common datasets for both the low-level vision task and the high-level vision task. In this paper, we explore the combination of the low-level vision task with the high-level vision task. Specifically, we propose an end-to-end object detection network for reducing the impact of rainfall, which consists of two cascaded networks, an improved image deraining network and an object detection network, respectively. We also design the components of the loss function to accommodate the characteristics of the different sub-networks. We then propose a dataset based on the KITTI dataset for rainfall removal and object detection, on which our network surpasses the state-of-the-art with a significant improvement in metrics. Besides, our proposed network is measured on driving videos collected by self-driving vehicles and shows positive results for rain removal and object detection.



### LPF-Defense: 3D Adversarial Defense based on Frequency Analysis
- **Arxiv ID**: http://arxiv.org/abs/2202.11287v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.11287v2)
- **Published**: 2022-02-23 03:31:25+00:00
- **Updated**: 2022-08-24 12:50:31+00:00
- **Authors**: Hanieh Naderi, Kimia Noorbakhsh, Arian Etemadi, Shohreh Kasaei
- **Comment**: 15 pages, 7 figures
- **Journal**: None
- **Summary**: Although 3D point cloud classification has recently been widely deployed in different application scenarios, it is still very vulnerable to adversarial attacks. This increases the importance of robust training of 3D models in the face of adversarial attacks. Based on our analysis on the performance of existing adversarial attacks, more adversarial perturbations are found in the mid and high-frequency components of input data. Therefore, by suppressing the high-frequency content in the training phase, the models robustness against adversarial examples is improved. Experiments showed that the proposed defense method decreases the success rate of six attacks on PointNet, PointNet++ ,, and DGCNN models. In particular, improvements are achieved with an average increase of classification accuracy by 3.8 % on drop100 attack and 4.26 % on drop200 attack compared to the state-of-the-art methods. The method also improves models accuracy on the original dataset compared to other available methods.



### Reliable Inlier Evaluation for Unsupervised Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2202.11292v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11292v1)
- **Published**: 2022-02-23 03:46:42+00:00
- **Updated**: 2022-02-23 03:46:42+00:00
- **Authors**: Yaqi Shen, Le Hui, Haobo Jiang, Jin Xie, Jian Yang
- **Comment**: Accepted by AAAI 2022
- **Journal**: None
- **Summary**: Unsupervised point cloud registration algorithm usually suffers from the unsatisfied registration precision in the partially overlapping problem due to the lack of effective inlier evaluation. In this paper, we propose a neighborhood consensus based reliable inlier evaluation method for robust unsupervised point cloud registration. It is expected to capture the discriminative geometric difference between the source neighborhood and the corresponding pseudo target neighborhood for effective inlier distinction. Specifically, our model consists of a matching map refinement module and an inlier evaluation module. In our matching map refinement module, we improve the point-wise matching map estimation by integrating the matching scores of neighbors into it. The aggregated neighborhood information potentially facilitates the discriminative map construction so that high-quality correspondences can be provided for generating the pseudo target point cloud. Based on the observation that the outlier has the significant structure-wise difference between its source neighborhood and corresponding pseudo target neighborhood while this difference for inlier is small, the inlier evaluation module exploits this difference to score the inlier confidence for each estimated correspondence. In particular, we construct an effective graph representation for capturing this geometric difference between the neighborhoods. Finally, with the learned correspondences and the corresponding inlier confidence, we use the weighted SVD algorithm for transformation estimation. Under the unsupervised setting, we exploit the Huber function based global alignment loss, the local neighborhood consensus loss, and spatial consistency loss for model optimization. The experimental results on extensive datasets demonstrate that our unsupervised point cloud registration method can yield comparable performance.



### Are We Ready for Robust and Resilient SLAM? A Framework For Quantitative Characterization of SLAM Datasets
- **Arxiv ID**: http://arxiv.org/abs/2202.11312v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.11312v2)
- **Published**: 2022-02-23 05:12:45+00:00
- **Updated**: 2022-07-15 07:33:48+00:00
- **Authors**: Islam Ali, Hong Zhang
- **Comment**: 7 Pages, Accepted to IROS 2022, updating to the latest submitted
  version
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems, IROS 2022, Kyoto, Japan
- **Summary**: Reliability of SLAM systems is considered one of the critical requirements in modern autonomous systems. This directed the efforts to developing many state-of-the-art systems, creating challenging datasets, and introducing rigorous metrics to measure SLAM performance. However, the link between datasets and performance in the robustness/resilience context has rarely been explored. In order to fill this void, characterization of the operating conditions of SLAM systems is essential in order to provide an environment for quantitative measurement of robustness and resilience. In this paper, we argue that for proper evaluation of SLAM performance, the characterization of SLAM datasets serves as a critical first step. The study starts by reviewing previous efforts for quantitative characterization of SLAM datasets. Then, the problem of perturbation characterization is discussed and the linkage to SLAM robustness/resilience is established. After that, we propose a novel, generic and extendable framework for quantitative analysis and comparison of SLAM datasets. Additionally, a description of different characterization parameters is provided. Finally, we demonstrate the application of our framework by presenting the characterization results of three SLAM datasets: KITTI, EuroC-MAV, and TUM-VI highlighting the level of insights achieved by the proposed framework.



### Absolute Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.11319v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.11319v1)
- **Published**: 2022-02-23 05:33:21+00:00
- **Updated**: 2022-02-23 05:33:21+00:00
- **Authors**: Rui Gao, Fan Wan, Daniel Organisciak, Jiyao Pu, Junyan Wang, Haoran Duan, Peng Zhang, Xingsong Hou, Yang Long
- **Comment**: None
- **Journal**: None
- **Summary**: Considering the increasing concerns about data copyright and privacy issues, we present a novel Absolute Zero-Shot Learning (AZSL) paradigm, i.e., training a classifier with zero real data. The key innovation is to involve a teacher model as the data safeguard to guide the AZSL model training without data leaking. The AZSL model consists of a generator and student network, which can achieve date-free knowledge transfer while maintaining the performance of the teacher network. We investigate `black-box' and `white-box' scenarios in AZSL task as different levels of model security. Besides, we also provide discussion of teacher model in both inductive and transductive settings. Despite embarrassingly simple implementations and data-missing disadvantages, our AZSL framework can retain state-of-the-art ZSL and GZSL performance under the `white-box' scenario. Extensive qualitative and quantitative analysis also demonstrates promising results when deploying the model under `black-box' scenario.



### EcoFusion: Energy-Aware Adaptive Sensor Fusion for Efficient Autonomous Vehicle Perception
- **Arxiv ID**: http://arxiv.org/abs/2202.11330v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2202.11330v1)
- **Published**: 2022-02-23 07:21:21+00:00
- **Updated**: 2022-02-23 07:21:21+00:00
- **Authors**: Arnav Vaibhav Malawade, Trier Mortlock, Mohammad Abdullah Al Faruque
- **Comment**: Accepted to be published in the 59th ACM/IEEE Design Automation
  Conference (DAC 2022)
- **Journal**: None
- **Summary**: Autonomous vehicles use multiple sensors, large deep-learning models, and powerful hardware platforms to perceive the environment and navigate safely. In many contexts, some sensing modalities negatively impact perception while increasing energy consumption. We propose EcoFusion: an energy-aware sensor fusion approach that uses context to adapt the fusion method and reduce energy consumption without affecting perception performance. EcoFusion performs up to 9.5% better at object detection than existing fusion methods with approximately 60% less energy and 58% lower latency on the industry-standard Nvidia Drive PX2 hardware platform. We also propose several context-identification strategies, implement a joint optimization between energy and performance, and present scenario-specific results.



### Commonsense Reasoning for Identifying and Understanding the Implicit Need of Help and Synthesizing Assistive Actions
- **Arxiv ID**: http://arxiv.org/abs/2202.11337v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.HC, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2202.11337v1)
- **Published**: 2022-02-23 07:50:25+00:00
- **Updated**: 2022-02-23 07:50:25+00:00
- **Authors**: Maëlic Neau, Paulo Santos, Anne-Gwenn Bosser, Nathan Beu, Cédric Buche
- **Comment**: None
- **Journal**: None
- **Summary**: Human-Robot Interaction (HRI) is an emerging subfield of service robotics. While most existing approaches rely on explicit signals (i.e. voice, gesture) to engage, current literature is lacking solutions to address implicit user needs. In this paper, we present an architecture to (a) detect user implicit need of help and (b) generate a set of assistive actions without prior learning. Task (a) will be performed using state-of-the-art solutions for Scene Graph Generation coupled to the use of commonsense knowledge; whereas, task (b) will be performed using additional commonsense knowledge as well as a sentiment analysis on graph structure. Finally, we propose an evaluation of our solution using established benchmarks (e.g. ActionGenome dataset) along with human experiments. The main motivation of our approach is the embedding of the perception-decision-action loop in a single architecture.



### Deepfake Detection for Facial Images with Facemasks
- **Arxiv ID**: http://arxiv.org/abs/2202.11359v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.11359v1)
- **Published**: 2022-02-23 09:01:27+00:00
- **Updated**: 2022-02-23 09:01:27+00:00
- **Authors**: Donggeun Ko, Sangjun Lee, Jinyong Park, Saebyeol Shin, Donghee Hong, Simon S. Woo
- **Comment**: This submission has been removed by arXiv administrators because the
  submitter did not have the authority to grant the license at the time of
  submission
- **Journal**: None
- **Summary**: Hyper-realistic face image generation and manipulation have givenrise to numerous unethical social issues, e.g., invasion of privacy,threat of security, and malicious political maneuvering, which re-sulted in the development of recent deepfake detection methods with the rising demands of deepfake forensics. Proposed deepfake detection methods to date have shown remarkable detection performance and robustness. However, none of the suggested deepfake detection methods assessed the performance of deepfakes with the facemask during the pandemic crisis after the outbreak of theCovid-19. In this paper, we thoroughly evaluate the performance of state-of-the-art deepfake detection models on the deepfakes with the facemask. Also, we propose two approaches to enhance the masked deepfakes detection: face-patch and face-crop. The experimental evaluations on both methods are assessed through the base-line deepfake detection models on the various deepfake datasets. Our extensive experiments show that, among the two methods, face-crop performs better than the face-patch, and could be a train method for deepfake detection models to detect fake faces with facemask in real world.



### Localizing Small Apples in Complex Apple Orchard Environments
- **Arxiv ID**: http://arxiv.org/abs/2202.11372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11372v1)
- **Published**: 2022-02-23 09:25:37+00:00
- **Updated**: 2022-02-23 09:25:37+00:00
- **Authors**: Christian Wilms, Robert Johanson, Simone Frintrop
- **Comment**: None
- **Journal**: None
- **Summary**: The localization of fruits is an essential first step in automated agricultural pipelines for yield estimation or fruit picking. One example of this is the localization of apples in images of entire apple trees. Since the apples are very small objects in such scenarios, we tackle this problem by adapting the object proposal generation system AttentionMask that focuses on small objects. We adapt AttentionMask by either adding a new module for very small apples or integrating it into a tiling framework. Both approaches clearly outperform standard object proposal generation systems on the MinneApple dataset covering complex apple orchard environments. Our evaluation further analyses the improvement w.r.t. the apple sizes and shows the different characteristics of our two approaches.



### Skeleton Sequence and RGB Frame Based Multi-Modality Feature Fusion Network for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.11374v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11374v1)
- **Published**: 2022-02-23 09:29:53+00:00
- **Updated**: 2022-02-23 09:29:53+00:00
- **Authors**: Xiaoguang Zhu, Ye Zhu, Haoyu Wang, Honglin Wen, Yan Yan, Peilin Liu
- **Comment**: Accepted by ACM Transactions on Multimedia Computing, Communications,
  and Applications (TOMM)
- **Journal**: None
- **Summary**: Action recognition has been a heated topic in computer vision for its wide application in vision systems. Previous approaches achieve improvement by fusing the modalities of the skeleton sequence and RGB video. However, such methods have a dilemma between the accuracy and efficiency for the high complexity of the RGB video network. To solve the problem, we propose a multi-modality feature fusion network to combine the modalities of the skeleton sequence and RGB frame instead of the RGB video, as the key information contained by the combination of skeleton sequence and RGB frame is close to that of the skeleton sequence and RGB video. In this way, the complementary information is retained while the complexity is reduced by a large margin. To better explore the correspondence of the two modalities, a two-stage fusion framework is introduced in the network. In the early fusion stage, we introduce a skeleton attention module that projects the skeleton sequence on the single RGB frame to help the RGB frame focus on the limb movement regions. In the late fusion stage, we propose a cross-attention module to fuse the skeleton feature and the RGB feature by exploiting the correlation. Experiments on two benchmarks NTU RGB+D and SYSU show that the proposed model achieves competitive performance compared with the state-of-the-art methods while reduces the complexity of the network.



### Multi-scale Sparse Representation-Based Shadow Inpainting for Retinal OCT Images
- **Arxiv ID**: http://arxiv.org/abs/2202.11377v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.11377v1)
- **Published**: 2022-02-23 09:37:14+00:00
- **Updated**: 2022-02-23 09:37:14+00:00
- **Authors**: Yaoqi Tang, Yufan Li, Hongshan Liu, Jiaxuan Li, Peiyao Jin, Yu Gan, Yuye Ling, Yikai Su
- **Comment**: None
- **Journal**: None
- **Summary**: Inpainting shadowed regions cast by superficial blood vessels in retinal optical coherence tomography (OCT) images is critical for accurate and robust machine analysis and clinical diagnosis. Traditional sequence-based approaches such as propagating neighboring information to gradually fill in the missing regions are cost-effective. But they generate less satisfactory outcomes when dealing with larger missing regions and texture-rich structures. Emerging deep learning-based methods such as encoder-decoder networks have shown promising results in natural image inpainting tasks. However, they typically need a long computational time for network training in addition to the high demand on the size of datasets, which makes it difficult to be applied on often small medical datasets. To address these challenges, we propose a novel multi-scale shadow inpainting framework for OCT images by synergically applying sparse representation and deep learning: sparse representation is used to extract features from a small amount of training images for further inpainting and to regularize the image after the multi-scale image fusion, while convolutional neural network (CNN) is employed to enhance the image quality. During the image inpainting, we divide preprocessed input images into different branches based on the shadow width to harvest complementary information from different scales. Finally, a sparse representation-based regularizing module is designed to refine the generated contents after multi-scale feature aggregation. Experiments are conducted to compare our proposal versus both traditional and deep learning-based techniques on synthetic and real-world shadows. Results demonstrate that our proposed method achieves favorable image inpainting in terms of visual quality and quantitative metrics, especially when wide shadows are presented.



### Multi-Teacher Knowledge Distillation for Incremental Implicitly-Refined Classification
- **Arxiv ID**: http://arxiv.org/abs/2202.11384v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11384v2)
- **Published**: 2022-02-23 09:51:40+00:00
- **Updated**: 2022-02-24 07:10:56+00:00
- **Authors**: Longhui Yu, Zhenyu Weng, Yuqing Wang, Yuesheng Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Incremental learning methods can learn new classes continually by distilling knowledge from the last model (as a teacher model) to the current model (as a student model) in the sequentially learning process. However, these methods cannot work for Incremental Implicitly-Refined Classification (IIRC), an incremental learning extension where the incoming classes could have two granularity levels, a superclass label and a subclass label. This is because the previously learned superclass knowledge may be occupied by the subclass knowledge learned sequentially. To solve this problem, we propose a novel Multi-Teacher Knowledge Distillation (MTKD) strategy. To preserve the subclass knowledge, we use the last model as a general teacher to distill the previous knowledge for the student model. To preserve the superclass knowledge, we use the initial model as a superclass teacher to distill the superclass knowledge as the initial model contains abundant superclass knowledge. However, distilling knowledge from two teacher models could result in the student model making some redundant predictions. We further propose a post-processing mechanism, called as Top-k prediction restriction to reduce the redundant predictions. Our experimental results on IIRC-ImageNet120 and IIRC-CIFAR100 show that the proposed method can achieve better classification accuracy compared with existing state-of-the-art methods.



### Deep Metric Learning-Based Semi-Supervised Regression With Alternate Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.11388v2
- **DOI**: 10.1109/ICIP46576.2022.9897939
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.11388v2)
- **Published**: 2022-02-23 10:04:15+00:00
- **Updated**: 2022-07-12 09:10:42+00:00
- **Authors**: Adina Zell, Gencer Sumbul, Begüm Demir
- **Comment**: Accepted at IEEE International Conference on Image Processing (ICIP)
  2022. Our code is available at https://git.tu-berlin.de/rsim/DML-S2R
- **Journal**: None
- **Summary**: This paper introduces a novel deep metric learning-based semi-supervised regression (DML-S2R) method for parameter estimation problems. The proposed DML-S2R method aims to mitigate the problems of insufficient amount of labeled samples without collecting any additional sample with a target value. To this end, it is made up of two main steps: i) pairwise similarity modeling with scarce labeled data; and ii) triplet-based metric learning with abundant unlabeled data. The first step aims to model pairwise sample similarities by using a small number of labeled samples. This is achieved by estimating the target value differences of labeled samples with a Siamese neural network (SNN). The second step aims to learn a triplet-based metric space (in which similar samples are close to each other and dissimilar samples are far apart from each other) when the number of labeled samples is insufficient. This is achieved by employing the SNN of the first step for triplet-based deep metric learning that exploits not only labeled samples but also unlabeled samples. For the end-to-end training of DML-S2R, we investigate an alternate learning strategy for the two steps. Due to this strategy, the encoded information in each step becomes a guidance for learning phase of the other step. The experimental results confirm the success of DML-S2R compared to the state-of-the-art semi-supervised regression methods. The code of the proposed method is publicly available at https://git.tu-berlin.de/rsim/DML-S2R.



### Mixed-Block Neural Architecture Search for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.11401v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2202.11401v1)
- **Published**: 2022-02-23 10:32:35+00:00
- **Updated**: 2022-02-23 10:32:35+00:00
- **Authors**: Martijn M. A. Bosma, Arkadiy Dushatskiy, Monika Grewal, Tanja Alderliesten, Peter A. N. Bosman
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have the potential for making various clinical procedures more time-efficient by automating medical image segmentation. Due to their strong, in some cases human-level, performance, they have become the standard approach in this field. The design of the best possible medical image segmentation DNNs, however, is task-specific. Neural Architecture Search (NAS), i.e., the automation of neural network design, has been shown to have the capability to outperform manually designed networks for various tasks. However, the existing NAS methods for medical image segmentation have explored a quite limited range of types of DNN architectures that can be discovered. In this work, we propose a novel NAS search space for medical image segmentation networks. This search space combines the strength of a generalised encoder-decoder structure, well known from U-Net, with network blocks that have proven to have a strong performance in image classification tasks. The search is performed by looking for the best topology of multiple cells simultaneously with the configuration of each cell within, allowing for interactions between topology and cell-level attributes. From experiments on two publicly available datasets, we find that the networks discovered by our proposed NAS method have better performance than well-known handcrafted segmentation networks, and outperform networks found with other NAS approaches that perform only topology search, and topology-level search followed by cell-level search.



### Delving Deep into One-Shot Skeleton-based Action Recognition with Diverse Occlusions
- **Arxiv ID**: http://arxiv.org/abs/2202.11423v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.11423v3)
- **Published**: 2022-02-23 11:11:54+00:00
- **Updated**: 2023-01-09 20:55:30+00:00
- **Authors**: Kunyu Peng, Alina Roitberg, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen
- **Comment**: Accepted to IEEE Transactions on Multimedia (TMM). Code is publicly
  available at https://github.com/KPeng9510/Trans4SOAR
- **Journal**: None
- **Summary**: Occlusions are universal disruptions constantly present in the real world. Especially for sparse representations, such as human skeletons, a few occluded points might destroy the geometrical and temporal continuity critically affecting the results. Yet, the research of data-scarce recognition from skeleton sequences, such as one-shot action recognition, does not explicitly consider occlusions despite their everyday pervasiveness. In this work, we explicitly tackle body occlusions for Skeleton-based One-shot Action Recognition (SOAR). We mainly consider two occlusion variants: 1) random occlusions and 2) more realistic occlusions caused by diverse everyday objects, which we generate by projecting the existing IKEA 3D furniture models into the camera coordinate system of the 3D skeletons with different geometric parameters. We leverage the proposed pipeline to blend out portions of skeleton sequences of the three popular action recognition datasets and formalize the first benchmark for SOAR from partially occluded body poses. Another key property of our benchmark are the more realistic occlusions generated by everyday objects, as even in standard recognition from 3D skeletons, only randomly missing joints were considered. We re-evaluate existing state-of-the-art frameworks for SOAR in the light of this new task and further introduce Trans4SOAR - a new transformer-based model which leverages three data streams and mixed attention fusion mechanism to alleviate the adverse effects caused by occlusions. While our experiments demonstrate a clear decline in accuracy with missing skeleton portions, this effect is smaller with Trans4SOAR, which outperforms other architectures on all datasets. Although we specifically focus on occlusions, Trans4SOAR additionally yields state-of-the-art in the standard SOAR without occlusion, surpassing the best published approach by 2.85% on NTU-120.



### Technological evaluation of two AFIS systems
- **Arxiv ID**: http://arxiv.org/abs/2203.00447v1
- **DOI**: 10.1109/MAES.2005.7035262
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.00447v1)
- **Published**: 2022-02-23 11:17:48+00:00
- **Updated**: 2022-02-23 11:17:48+00:00
- **Authors**: Marcos Faundez-Zanuy
- **Comment**: 4 pages
- **Journal**: IEEE Aerospace and Electronic Systems Magazine, vol. 20, no. 4,
  pp. 13-17, April 2005
- **Summary**: This paper provides a technological evaluation of two Automatic Fingerprint Identification Systems (AFIS) used in forensic applications. Both of them are installed and working in Spanish police premises. The first one is a Printrak AFIS 2000 system with a database of more than 450,000 fingerprints, while the second one is a NEC AFIS 21 SAID NT-LEXS Release 2.4.4 with a database of more than 15 million fingerprints. Our experiments reveal that although both systems can manage inkless fingerprints, the latest one offers better experimental results



### A Novel Self-Supervised Cross-Modal Image Retrieval Method In Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/2202.11429v2
- **DOI**: 10.1109/ICIP46576.2022.9897475
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11429v2)
- **Published**: 2022-02-23 11:20:24+00:00
- **Updated**: 2022-07-12 09:15:47+00:00
- **Authors**: Gencer Sumbul, Markus Müller, Begüm Demir
- **Comment**: Accepted at IEEE International Conference on Image Processing (ICIP)
  2022. Our code is available at https://git.tu-berlin.de/rsim/SS-CM-RSIR
- **Journal**: None
- **Summary**: Due to the availability of multi-modal remote sensing (RS) image archives, one of the most important research topics is the development of cross-modal RS image retrieval (CM-RSIR) methods that search semantically similar images across different modalities. Existing CM-RSIR methods require the availability of a high quality and quantity of annotated training images. The collection of a sufficient number of reliable labeled images is time consuming, complex and costly in operational scenarios, and can significantly affect the final accuracy of CM-RSIR. In this paper, we introduce a novel self-supervised CM-RSIR method that aims to: i) model mutual-information between different modalities in a self-supervised manner; ii) retain the distributions of modal-specific feature spaces similar to each other; and iii) define the most similar images within each modality without requiring any annotated training image. To this end, we propose a novel objective including three loss functions that simultaneously: i) maximize mutual information of different modalities for inter-modal similarity preservation; ii) minimize the angular distance of multi-modal image tuples for the elimination of inter-modal discrepancies; and iii) increase cosine similarity of the most similar images within each modality for the characterization of intra-modal similarities. Experimental results show the effectiveness of the proposed method compared to state-of-the-art methods. The code of the proposed method is publicly available at https://git.tu-berlin.de/rsim/SS-CM-RSIR.



### Synthesizing Photorealistic Images with Deep Generative Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.12752v1
- **DOI**: 10.32657/10356/153008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.12752v1)
- **Published**: 2022-02-23 11:35:39+00:00
- **Updated**: 2022-02-23 11:35:39+00:00
- **Authors**: Chuanxia Zheng
- **Comment**: PhD thesis
- **Journal**: None
- **Summary**: The goal of this thesis is to present my research contributions towards solving various visual synthesis and generation tasks, comprising image translation, image completion, and completed scene decomposition. This thesis consists of five pieces of work, each of which presents a new learning-based approach for synthesizing images with plausible content as well as visually realistic appearance. Each work demonstrates the superiority of the proposed approach on image synthesis, with some further contributing to other tasks, such as depth estimation.



### Biometric recognition: why not massively adopted yet?
- **Arxiv ID**: http://arxiv.org/abs/2203.03719v2
- **DOI**: 10.1109/MAES.2005.1499300
- **Categories**: **cs.CY**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.03719v2)
- **Published**: 2022-02-23 11:45:54+00:00
- **Updated**: 2022-03-11 12:45:41+00:00
- **Authors**: Marcos Faundez-Zanuy
- **Comment**: 5 pages
- **Journal**: IEEE Aerospace and Electronic Systems Magazine, vol. 20, no. 8,
  pp. 25-28, Aug. 2005
- **Summary**: Although there has been a dramatically reduction on the prices of capturing devices and an increase on computing power in the last decade, it seems that biometric systems are still far from massive adoption for civilian applications. This paper deals with the causes of this phenomenon, as well as some misconceptions regarding biometric identification.



### On PAC-Bayesian reconstruction guarantees for VAEs
- **Arxiv ID**: http://arxiv.org/abs/2202.11455v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.ST, stat.ML, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/2202.11455v1)
- **Published**: 2022-02-23 12:11:05+00:00
- **Updated**: 2022-02-23 12:11:05+00:00
- **Authors**: Badr-Eddine Chérief-Abdellatif, Yuyang Shi, Arnaud Doucet, Benjamin Guedj
- **Comment**: 14 pages
- **Journal**: Proceedings of the 25th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2022, Valencia, Spain. PMLR: Volume 151
- **Summary**: Despite its wide use and empirical successes, the theoretical understanding and study of the behaviour and performance of the variational autoencoder (VAE) have only emerged in the past few years. We contribute to this recent line of work by analysing the VAE's reconstruction ability for unseen test data, leveraging arguments from the PAC-Bayes theory. We provide generalisation bounds on the theoretical reconstruction error, and provide insights on the regularisation effect of VAE objectives. We illustrate our theoretical results with supporting experiments on classical benchmark datasets.



### SLOGAN: Handwriting Style Synthesis for Arbitrary-Length and Out-of-Vocabulary Text
- **Arxiv ID**: http://arxiv.org/abs/2202.11456v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11456v1)
- **Published**: 2022-02-23 12:13:27+00:00
- **Updated**: 2022-02-23 12:13:27+00:00
- **Authors**: Canjie Luo, Yuanzhi Zhu, Lianwen Jin, Zhe Li, Dezhi Peng
- **Comment**: Accepted to appear in IEEE Transactions on Neural Networks and
  Learning Systems (TNNLS)
- **Journal**: None
- **Summary**: Large amounts of labeled data are urgently required for the training of robust text recognizers. However, collecting handwriting data of diverse styles, along with an immense lexicon, is considerably expensive. Although data synthesis is a promising way to relieve data hunger, two key issues of handwriting synthesis, namely, style representation and content embedding, remain unsolved. To this end, we propose a novel method that can synthesize parameterized and controllable handwriting Styles for arbitrary-Length and Out-of-vocabulary text based on a Generative Adversarial Network (GAN), termed SLOGAN. Specifically, we propose a style bank to parameterize the specific handwriting styles as latent vectors, which are input to a generator as style priors to achieve the corresponding handwritten styles. The training of the style bank requires only the writer identification of the source images, rather than attribute annotations. Moreover, we embed the text content by providing an easily obtainable printed style image, so that the diversity of the content can be flexibly achieved by changing the input printed image. Finally, the generator is guided by dual discriminators to handle both the handwriting characteristics that appear as separated characters and in a series of cursive joins. Our method can synthesize words that are not included in the training vocabulary and with various new styles. Extensive experiments have shown that high-quality text images with great style diversity and rich vocabulary can be synthesized using our method, thereby enhancing the robustness of the recognizer.



### ISDA: Position-Aware Instance Segmentation with Deformable Attention
- **Arxiv ID**: http://arxiv.org/abs/2202.12251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.12251v1)
- **Published**: 2022-02-23 12:30:18+00:00
- **Updated**: 2022-02-23 12:30:18+00:00
- **Authors**: Kaining Ying, Zhenhua Wang, Cong Bai, Pengfei Zhou
- **Comment**: Accepted to ICASSP 2022
- **Journal**: None
- **Summary**: Most instance segmentation models are not end-to-end trainable due to either the incorporation of proposal estimation (RPN) as a pre-processing or non-maximum suppression (NMS) as a post-processing. Here we propose a novel end-to-end instance segmentation method termed ISDA. It reshapes the task into predicting a set of object masks, which are generated via traditional convolution operation with learned position-aware kernels and features of objects. Such kernels and features are learned by leveraging a deformable attention network with multi-scale representation. Thanks to the introduced set-prediction mechanism, the proposed method is NMS-free. Empirically, ISDA outperforms Mask R-CNN (the strong baseline) by 2.6 points on MS-COCO, and achieves leading performance compared with recent models. Code will be available soon.



### Thermal hand image segmentation for biometric recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.11462v1
- **DOI**: 10.1109/MAES.2013.6533739
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.11462v1)
- **Published**: 2022-02-23 12:30:50+00:00
- **Updated**: 2022-02-23 12:30:50+00:00
- **Authors**: Xavier Font-Aragones, Marcos Faundez-Zanuy, Jiri Mekyska
- **Comment**: 12 pages
- **Journal**: IEEE Aerospace and Electronic Systems Magazine, vol. 28, no. 6,
  pp. 4-14, June 2013
- **Summary**: In this paper we present a method to identify people by means of thermal (TH) and visible (VIS) hand images acquired simultaneously with a TESTO 882-3 camera. In addition, we also present a new database specially acquired for this work. The real challenge when dealing with TH images is the cold finger areas, which can be confused with the acquisition surface. This problem is solved by taking advantage of the VIS information. We have performed different tests to show how TH and VIS images work in identification problems. Experimental results reveal that TH hand image is as suitable for biometric recognition systems as VIS hand images, and better results are obtained when combining this information. A Biometric Dispersion Matcher has been used as a feature vector dimensionality reduction technique as well as a classification task. Its selection criteria helps to reduce the length of the vectors used to perform identification up to a hundred measurements. Identification rates reach a maximum value of 98.3% under these conditions, when using a database of 104 people.



### Reconstruction Task Finds Universal Winning Tickets
- **Arxiv ID**: http://arxiv.org/abs/2202.11484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11484v1)
- **Published**: 2022-02-23 13:04:32+00:00
- **Updated**: 2022-02-23 13:04:32+00:00
- **Authors**: Ruichen Li, Binghui Li, Qi Qian, Liwei Wang
- **Comment**: Under review
- **Journal**: None
- **Summary**: Pruning well-trained neural networks is effective to achieve a promising accuracy-efficiency trade-off in computer vision regimes. However, most of existing pruning algorithms only focus on the classification task defined on the source domain. Different from the strong transferability of the original model, a pruned network is hard to transfer to complicated downstream tasks such as object detection arXiv:arch-ive/2012.04643. In this paper, we show that the image-level pretrain task is not capable of pruning models for diverse downstream tasks. To mitigate this problem, we introduce image reconstruction, a pixel-level task, into the traditional pruning framework. Concretely, an autoencoder is trained based on the original model, and then the pruning process is optimized with both autoencoder and classification losses. The empirical study on benchmark downstream tasks shows that the proposed method can outperform state-of-the-art results explicitly.



### Augmentation based unsupervised domain adaptation
- **Arxiv ID**: http://arxiv.org/abs/2202.11486v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.11486v1)
- **Published**: 2022-02-23 13:06:07+00:00
- **Updated**: 2022-02-23 13:06:07+00:00
- **Authors**: Mauricio Orbes-Arteaga, Thomas Varsavsky, Lauge Sorensen, Mads Nielsen, Akshay Pai, Sebastien Ourselin, Marc Modat, M Jorge Cardoso
- **Comment**: None
- **Journal**: None
- **Summary**: The insertion of deep learning in medical image analysis had lead to the development of state-of-the art strategies in several applications such a disease classification, as well as abnormality detection and segmentation. However, even the most advanced methods require a huge and diverse amount of data to generalize. Because in realistic clinical scenarios, data acquisition and annotation is expensive, deep learning models trained on small and unrepresentative data tend to outperform when deployed in data that differs from the one used for training (e.g data from different scanners). In this work, we proposed a domain adaptation methodology to alleviate this problem in segmentation models. Our approach takes advantage of the properties of adversarial domain adaptation and consistency training to achieve more robust adaptation. Using two datasets with white matter hyperintensities (WMH) annotations, we demonstrated that the proposed method improves model generalization even in corner cases where individual strategies tend to fail.



### MITI: SLAM Benchmark for Laparoscopic Surgery
- **Arxiv ID**: http://arxiv.org/abs/2202.11496v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.11496v1)
- **Published**: 2022-02-23 13:23:52+00:00
- **Updated**: 2022-02-23 13:23:52+00:00
- **Authors**: Regine Hartwig, Daniel Ostler, Jean-Claude Rosenthal, Hubertus Feußner, Dirk Wilhelm, Dirk Wollherr
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2202.11075
- **Journal**: None
- **Summary**: We propose a new benchmark for evaluating stereoscopic visual-inertial computer vision algorithms (SLAM/ SfM/ 3D Reconstruction/ Visual-Inertial Odometry) for minimally invasive surgical (MIS) interventions in the abdomen. Our MITI Dataset available at [https://mediatum.ub.tum.de/1621941] provides all the necessary data by a complete recording of a handheld surgical intervention at Research Hospital Rechts der Isar of TUM. It contains multimodal sensor information from IMU, stereoscopic video, and infrared (IR) tracking as ground truth for evaluation. Furthermore, calibration for the stereoscope, accelerometer, magnetometer, the rigid transformations in the sensor setup, and time-offsets are available. We wisely chose a suitable intervention that contains very few cutting and tissue deformation and shows a full scan of the abdomen with a handheld camera such that it is ideal for testing SLAM algorithms. Intending to promote the progress of visual-inertial algorithms designed for MIS application, we hope that our clinical training dataset helps and enables researchers to enhance algorithms.



### Visual-Tactile Sensing for Real-time Liquid Volume Estimation in Grasping
- **Arxiv ID**: http://arxiv.org/abs/2202.11503v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.11503v2)
- **Published**: 2022-02-23 13:38:31+00:00
- **Updated**: 2022-08-16 02:32:39+00:00
- **Authors**: Fan Zhu, Ruixing Jia, Lei Yang, Youcan Yan, Zheng Wang, Jia Pan, Wenping Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deep visuo-tactile model for realtime estimation of the liquid inside a deformable container in a proprioceptive way.We fuse two sensory modalities, i.e., the raw visual inputs from the RGB camera and the tactile cues from our specific tactile sensor without any extra sensor calibrations.The robotic system is well controlled and adjusted based on the estimation model in real time. The main contributions and novelties of our work are listed as follows: 1) Explore a proprioceptive way for liquid volume estimation by developing an end-to-end predictive model with multi-modal convolutional networks, which achieve a high precision with an error of around 2 ml in the experimental validation. 2) Propose a multi-task learning architecture which comprehensively considers the losses from both classification and regression tasks, and comparatively evaluate the performance of each variant on the collected data and actual robotic platform. 3) Utilize the proprioceptive robotic system to accurately serve and control the requested volume of liquid, which is continuously flowing into a deformable container in real time. 4) Adaptively adjust the grasping plan to achieve more stable grasping and manipulation according to the real-time liquid volume prediction.



### Weakly-supervised learning for image-based classification of primary melanomas into genomic immune subgroups
- **Arxiv ID**: http://arxiv.org/abs/2202.11524v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2202.11524v1)
- **Published**: 2022-02-23 13:57:35+00:00
- **Updated**: 2022-02-23 13:57:35+00:00
- **Authors**: Lucy Godson, Navid Alemi, Jeremie Nsengimana, Graham P. Cook, Emily L. Clarke, Darren Treanor, D. Timothy Bishop, Julia Newton-Bishop, Ali Gooya
- **Comment**: 8 pages (without appendices), 2 figures, submitted to MIDL 2022
  conference proceedings
- **Journal**: None
- **Summary**: Determining early-stage prognostic markers and stratifying patients for effective treatment are two key challenges for improving outcomes for melanoma patients. Previous studies have used tumour transcriptome data to stratify patients into immune subgroups, which were associated with differential melanoma specific survival and potential treatment strategies. However, acquiring transcriptome data is a time-consuming and costly process. Moreover, it is not routinely used in the current clinical workflow. Here we attempt to overcome this by developing deep learning models to classify gigapixel H&E stained pathology slides, which are well established in clinical workflows, into these immune subgroups. Previous subtyping approaches have employed supervised learning which requires fully annotated data, or have only examined single genetic mutations in melanoma patients. We leverage a multiple-instance learning approach, which only requires slide-level labels and uses an attention mechanism to highlight regions of high importance to the classification. Moreover, we show that pathology-specific self-supervised models generate better representations compared to pathology-agnostic models for improving our model performance, achieving a mean AUC of 0.76 for classifying histopathology images as high or low immune subgroups. We anticipate that this method may allow us to find new biomarkers of high importance and could act as a tool for clinicians to infer the immune landscape of tumours and stratify patients, without needing to carry out additional expensive genetic tests.



### Diffractive optical system design by cascaded propagation
- **Arxiv ID**: http://arxiv.org/abs/2202.11535v1
- **DOI**: 10.1364/OE.465230
- **Categories**: **physics.optics**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.11535v1)
- **Published**: 2022-02-23 14:21:09+00:00
- **Updated**: 2022-02-23 14:21:09+00:00
- **Authors**: Boris Ferdman, Alon Saguy, Onit Alalouf, Yoav Shechtman
- **Comment**: Manuscript and supplementary information combined
- **Journal**: None
- **Summary**: Modern design of complex optical systems relies heavily on computational tools. These typically utilize geometrical optics as well as Fourier optics, which enables the use of diffractive elements to manipulate light with features on the scale of a wavelength. Fourier optics is typically used for designing thin elements, placed in the system's aperture, generating a shift-invariant Point Spread Function (PSF). A major bottleneck in applying Fourier Optics in many cases of interest, e.g. when dealing with multiple, or out-of-aperture elements, comes from numerical complexity. In this work, we propose and implement an efficient and differentiable propagation model based on the Collins integral, which enables the optimization of diffraction optical systems with unprecedented design freedom using backpropagation. We demonstrate the applicability of our method, numerically and experimentally, by engineering shift-variant PSFs via thin plate elements placed in arbitrary planes inside complex imaging systems, performing cascaded optimization of multiple planes, and designing optimal machine-vision systems by deep learning.



### Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut
- **Arxiv ID**: http://arxiv.org/abs/2202.11539v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2202.11539v2)
- **Published**: 2022-02-23 14:27:36+00:00
- **Updated**: 2022-03-24 09:38:24+00:00
- **Authors**: Yangtao Wang, Xi Shen, Shell Hu, Yuan Yuan, James Crowley, Dominique Vaufreydaz
- **Comment**: None
- **Journal**: CVPR 2022 - Conference on Computer Vision and Pattern Recognition,
  Jun 2022, New Orleans, United States
- **Summary**: Transformers trained with self-supervised learning using self-distillation loss (DINO) have been shown to produce attention maps that highlight salient foreground objects. In this paper, we demonstrate a graph-based approach that uses the self-supervised transformer features to discover an object from an image. Visual tokens are viewed as nodes in a weighted graph with edges representing a connectivity score based on the similarity of tokens. Foreground objects can then be segmented using a normalized graph-cut to group self-similar regions. We solve the graph-cut problem using spectral clustering with generalized eigen-decomposition and show that the second smallest eigenvector provides a cutting solution since its absolute value indicates the likelihood that a token belongs to a foreground object. Despite its simplicity, this approach significantly boosts the performance of unsupervised object discovery: we improve over the recent state of the art LOST by a margin of 6.9%, 8.1%, and 8.1% respectively on the VOC07, VOC12, and COCO20K. The performance can be further improved by adding a second stage class-agnostic detector (CAD). Our proposed method can be easily extended to unsupervised saliency detection and weakly supervised object detection. For unsupervised saliency detection, we improve IoU for 4.9%, 5.2%, 12.9% on ECSSD, DUTS, DUT-OMRON respectively compared to previous state of the art. For weakly supervised object detection, we achieve competitive performance on CUB and ImageNet.



### A Method for Waste Segregation using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.12258v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.12258v1)
- **Published**: 2022-02-23 14:32:10+00:00
- **Updated**: 2022-02-23 14:32:10+00:00
- **Authors**: Jash Shah, Sagar Kamat
- **Comment**: None
- **Journal**: None
- **Summary**: Segregation of garbage is a primary concern in many nations across the world. Even though we are in the modern era, many people still do not know how to distinguish between organic and recyclable waste. It is because of this that the world is facing a major crisis of waste disposal. In this paper, we try to use deep learning algorithms to help solve this problem of waste classification. The waste is classified into two categories like organic and recyclable. Our proposed model achieves an accuracy of 94.9%. Although the other two models also show promising results, the Proposed Model stands out with the greatest accuracy. With the help of deep learning, one of the greatest obstacles to efficient waste management can finally be removed.



### Amodal Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.11542v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.11542v1)
- **Published**: 2022-02-23 14:41:59+00:00
- **Updated**: 2022-02-23 14:41:59+00:00
- **Authors**: Rohit Mohan, Abhinav Valada
- **Comment**: None
- **Journal**: None
- **Summary**: Humans have the remarkable ability to perceive objects as a whole, even when parts of them are occluded. This ability of amodal perception forms the basis of our perceptual and cognitive understanding of our world. To enable robots to reason with this capability, we formulate and propose a novel task that we name amodal panoptic segmentation. The goal of this task is to simultaneously predict the pixel-wise semantic segmentation labels of the visible regions of stuff classes and the instance segmentation labels of both the visible and occluded regions of thing classes. To facilitate research on this new task, we extend two established benchmark datasets with pixel-level amodal panoptic segmentation labels that we make publicly available as KITTI-360-APS and BDD100K-APS. We present several strong baselines, along with the amodal panoptic quality (APQ) and amodal parsing coverage (APC) metrics to quantify the performance in an interpretable manner. Furthermore, we propose the novel amodal panoptic segmentation network (APSNet), as a first step towards addressing this task by explicitly modeling the complex relationships between the occluders and occludes. Extensive experimental evaluations demonstrate that APSNet achieves state-of-the-art performance on both benchmarks and more importantly exemplifies the utility of amodal recognition. The benchmarks are available at http://amodal-panoptic.cs.uni-freiburg.de.



### EMOTHAW: A novel database for emotional state recognition from handwriting
- **Arxiv ID**: http://arxiv.org/abs/2202.12245v1
- **DOI**: 10.1109/THMS.2016.2635441
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12245v1)
- **Published**: 2022-02-23 15:15:44+00:00
- **Updated**: 2022-02-23 15:15:44+00:00
- **Authors**: Laurence Likforman-Sulem, Anna Esposito, Marcos Faundez-Zanuy, Stephan Clemençon, Gennaro Cordasco
- **Comment**: 31 pages
- **Journal**: IEEE Transactions on Human-Machine Systems, vol. 47, no. 2, pp.
  273-284, April 2017
- **Summary**: The detection of negative emotions through daily activities such as handwriting is useful for promoting well-being. The spread of human-machine interfaces such as tablets makes the collection of handwriting samples easier. In this context, we present a first publicly available handwriting database which relates emotional states to handwriting, that we call EMOTHAW. This database includes samples of 129 participants whose emotional states, namely anxiety, depression and stress, are assessed by the Depression Anxiety Stress Scales (DASS) questionnaire. Seven tasks are recorded through a digitizing tablet: pentagons and house drawing, words copied in handprint, circles and clock drawing, and one sentence copied in cursive writing. Records consist in pen positions, on-paper and in-air, time stamp, pressure, pen azimuth and altitude. We report our analysis on this database. From collected data, we first compute measurements related to timing and ductus. We compute separate measurements according to the position of the writing device: on paper or in-air. We analyse and classify this set of measurements (referred to as features) using a random forest approach. This latter is a machine learning method [2], based on an ensemble of decision trees, which includes a feature ranking process. We use this ranking process to identify the features which best reveal a targeted emotional state.   We then build random forest classifiers associated to each emotional state. Our results, obtained from cross-validation experiments, show that the targeted emotional states can be identified with accuracies ranging from 60% to 71%.



### On-line signature verification system with failure to enroll managing
- **Arxiv ID**: http://arxiv.org/abs/2202.12242v1
- **DOI**: 10.1016/j.patcog.2009.01.019
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12242v1)
- **Published**: 2022-02-23 15:22:03+00:00
- **Updated**: 2022-02-23 15:22:03+00:00
- **Authors**: Joan Fabregas, Marcos Faundez-Zanuy
- **Comment**: 35 pages
- **Journal**: Pattern Recognition Volume 42, Issue 9, September 2009, Pages
  2117-2126
- **Summary**: In this paper we simulate a real biometric verification system based on on-line signatures. For this purpose we have split the MCYT signature database in three subsets: one for classifier training, another for system adjustment and a third one for system testing simulating enrollment and verification. This context corresponds to a real operation, where a new user tries to enroll an existing system and must be automatically guided by the system in order to detect the failure to enroll situations. The main contribution of this work is the management of failure to enroll situations by means of a new proposal, called intelligent enrollment, which consists of consistency checking in order to automatically reject low quality samples. This strategy lets to enhance the verification errors up to 22% when leaving out 8% of the users. In this situation 8% of the people cannot be enrolled in the system and must be verified by other biometrics or by human abilities. These people are identified with intelligent enrollment and the situation can be thus managed. In addition we also propose a DCT-based feature extractor with threshold coding and discriminability criteria.



### A comparative study of in-air trajectories at short and long distances in online handwriting
- **Arxiv ID**: http://arxiv.org/abs/2202.12237v1
- **DOI**: 10.1007/s12559-017-9501-5
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.12237v1)
- **Published**: 2022-02-23 15:32:04+00:00
- **Updated**: 2022-02-23 15:32:04+00:00
- **Authors**: Carlos Alonso-Martinez, Marcos Faundez-Zanuy, Jiri Mekyska
- **Comment**: 11 pages
- **Journal**: Cognitive Computation, vol 9, 2017
- **Summary**: Introduction Existing literature about online handwriting analysis to support pathology diagnosis has taken advantage of in-air trajectories. A similar situation occurred in biometric security applications where the goal is to identify or verify an individual using his signature or handwriting. These studies do not consider the distance of the pen tip to the writing surface. This is due to the fact that current acquisition devices do not provide height formation. However, it is quite straightforward to differentiate movements at two different heights: a) short distance: height lower or equal to 1 cm above a surface of digitizer, the digitizer provides x and y coordinates. b) long distance: height exceeding 1 cm, the only information available is a time stamp that indicates the time that a specific stroke has spent at long distance. Although short distance has been used in several papers, long distances have been ignored and will be investigated in this paper. Methods In this paper, we will analyze a large set of databases (BIOSECURID, EMOTHAW, PaHaW, Oxygen-Therapy and SALT), which contain a total amount of 663 users and 17951 files. We have specifically studied: a) the percentage of time spent on-surface, in-air at short distance, and in-air at long distance for different user profiles (pathological and healthy users) and different tasks; b) The potential use of these signals to improve classification rates. Results and conclusions Our experimental results reveal that long-distance movements represent a very small portion of the total execution time (0.5 % in the case of signatures and 10.4% for uppercase words of BIOSECUR-ID, which is the largest database). In addition, significant differences have been found in the comparison of pathological versus control group for letter l in PaHaW database (p=0.0157) and crossed pentagons in SALT database (p=0.0122)



### Deep Bayesian ICP Covariance Estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.11607v1
- **DOI**: 10.1109/ICRA46639.2022.9811899
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.11607v1)
- **Published**: 2022-02-23 16:42:04+00:00
- **Updated**: 2022-02-23 16:42:04+00:00
- **Authors**: Andrea De Maio, Simon Lacroix
- **Comment**: None
- **Journal**: None
- **Summary**: Covariance estimation for the Iterative Closest Point (ICP) point cloud registration algorithm is essential for state estimation and sensor fusion purposes. We argue that a major source of error for ICP is in the input data itself, from the sensor noise to the scene geometry. Benefiting from recent developments in deep learning for point clouds, we propose a data-driven approach to learn an error model for ICP. We estimate covariances modeling data-dependent heteroscedastic aleatoric uncertainty, and epistemic uncertainty using a variational Bayesian approach. The system evaluation is performed on LiDAR odometry on different datasets, highlighting good results in comparison to the state of the art.



### ChimeraMix: Image Classification on Small Datasets via Masked Feature Mixing
- **Arxiv ID**: http://arxiv.org/abs/2202.11616v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11616v2)
- **Published**: 2022-02-23 16:51:22+00:00
- **Updated**: 2022-07-29 15:40:07+00:00
- **Authors**: Christoph Reinders, Frederik Schubert, Bodo Rosenhahn
- **Comment**: Published at IJCAI-22
- **Journal**: None
- **Summary**: Deep convolutional neural networks require large amounts of labeled data samples. For many real-world applications, this is a major limitation which is commonly treated by augmentation methods. In this work, we address the problem of learning deep neural networks on small datasets. Our proposed architecture called ChimeraMix learns a data augmentation by generating compositions of instances. The generative model encodes images in pairs, combines the features guided by a mask, and creates new samples. For evaluation, all methods are trained from scratch without any additional data. Several experiments on benchmark datasets, e.g. ciFAIR-10, STL-10, and ciFAIR-100, demonstrate the superior performance of ChimeraMix compared to current state-of-the-art methods for classification on small datasets.



### Anomaly Detection in 3D Point Clouds using Deep Geometric Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2202.11660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11660v1)
- **Published**: 2022-02-23 18:07:51+00:00
- **Updated**: 2022-02-23 18:07:51+00:00
- **Authors**: Paul Bergmann, David Sattlegger
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new method for the unsupervised detection of geometric anomalies in high-resolution 3D point clouds. In particular, we propose an adaptation of the established student-teacher anomaly detection framework to three dimensions. A student network is trained to match the output of a pretrained teacher network on anomaly-free point clouds. When applied to test data, regression errors between the teacher and the student allow reliable localization of anomalous structures. To construct an expressive teacher network that extracts dense local geometric descriptors, we introduce a novel self-supervised pretraining strategy. The teacher is trained by reconstructing local receptive fields and does not require annotations. Extensive experiments on the comprehensive MVTec 3D Anomaly Detection dataset highlight the effectiveness of our approach, which outperforms the next-best method by a large margin. Ablation studies show that our approach meets the requirements of practical applications regarding performance, runtime, and memory consumption.



### Human Motion Detection Using Sharpened Dimensionality Reduction and Clustering
- **Arxiv ID**: http://arxiv.org/abs/2202.11667v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11667v1)
- **Published**: 2022-02-23 18:18:25+00:00
- **Updated**: 2022-02-23 18:18:25+00:00
- **Authors**: Jeewon Heo, Youngjoo Kim, Jos B. T. M. Roerdink
- **Comment**: This paper was accepted and presented at International Conference on
  Biomedical and Health Informatics (ICBHI2021)
  (https://conference21.kosombe.or.kr/register/2021_fall/main.html)
- **Journal**: None
- **Summary**: Sharpened dimensionality reduction (SDR), which belongs to the class of multidimensional projection techniques, has recently been introduced to tackle the challenges in the exploratory and visual analysis of high-dimensional data. SDR has been applied to various real-world datasets, such as human activity sensory data and astronomical datasets. However, manually labeling the samples from the generated projection are expensive. To address this problem, we propose here to use clustering methods such as k-means, Hierarchical Clustering, Density-Based Spatial Clustering of Applications with Noise (DBSCAN), and Spectral Clustering to easily label the 2D projections of high-dimensional data. We test our pipeline of SDR and the clustering methods on a range of synthetic and real-world datasets, including two different public human activity datasets extracted from smartphone accelerometer or gyroscope recordings of various movements. We apply clustering to assess the visual cluster separation of SDR, both qualitatively and quantitatively. We conclude that clustering SDR results yields better labeling results than clustering plain DR, and that k-means is the recommended clustering method for SDR in terms of clustering accuracy, ease-of-use, and computational scalability.



### U-Attention to Textures: Hierarchical Hourglass Vision Transformer for Universal Texture Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2202.11703v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.11703v2)
- **Published**: 2022-02-23 18:58:56+00:00
- **Updated**: 2022-06-30 07:16:09+00:00
- **Authors**: Shouchang Guo, Valentin Deschaintre, Douglas Noll, Arthur Roullier
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel U-Attention vision Transformer for universal texture synthesis. We exploit the natural long-range dependencies enabled by the attention mechanism to allow our approach to synthesize diverse textures while preserving their structures in a single inference. We propose a hierarchical hourglass backbone that attends to the global structure and performs patch mapping at varying scales in a coarse-to-fine-to-coarse stream. Completed by skip connection and convolution designs that propagate and fuse information at different scales, our hierarchical U-Attention architecture unifies attention to features from macro structures to micro details, and progressively refines synthesis results at successive stages. Our method achieves stronger 2$\times$ synthesis than previous work on both stochastic and structured textures while generalizing to unseen textures without fine-tuning. Ablation studies demonstrate the effectiveness of each component of our architecture.



### Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2202.11742v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11742v1)
- **Published**: 2022-02-23 19:06:53+00:00
- **Updated**: 2022-02-23 19:06:53+00:00
- **Authors**: Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, Ivan Laptev
- **Comment**: None
- **Journal**: None
- **Summary**: Following language instructions to navigate in unseen environments is a challenging problem for autonomous embodied agents. The agent not only needs to ground languages in visual scenes, but also should explore the environment to reach its target. In this work, we propose a dual-scale graph transformer (DUET) for joint long-term action planning and fine-grained cross-modal understanding. We build a topological map on-the-fly to enable efficient exploration in global action space. To balance the complexity of large action space reasoning and fine-grained language grounding, we dynamically combine a fine-scale encoding over local observations and a coarse-scale encoding on a global map via graph transformers. The proposed approach, DUET, significantly outperforms state-of-the-art methods on goal-oriented vision-and-language navigation (VLN) benchmarks REVERIE and SOON. It also improves the success rate on the fine-grained VLN benchmark R2R.



### When do GANs replicate? On the choice of dataset size
- **Arxiv ID**: http://arxiv.org/abs/2202.11765v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.11765v1)
- **Published**: 2022-02-23 20:15:19+00:00
- **Updated**: 2022-02-23 20:15:19+00:00
- **Authors**: Qianli Feng, Chenqi Guo, Fabian Benitez-Quiroz, Aleix Martinez
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision 2021 (pp. 6701-6710)
- **Summary**: Do GANs replicate training images? Previous studies have shown that GANs do not seem to replicate training data without significant change in the training procedure. This leads to a series of research on the exact condition needed for GANs to overfit to the training data. Although a number of factors has been theoretically or empirically identified, the effect of dataset size and complexity on GANs replication is still unknown. With empirical evidence from BigGAN and StyleGAN2, on datasets CelebA, Flower and LSUN-bedroom, we show that dataset size and its complexity play an important role in GANs replication and perceptual quality of the generated images. We further quantify this relationship, discovering that replication percentage decays exponentially with respect to dataset size and complexity, with a shared decaying factor across GAN-dataset combinations. Meanwhile, the perceptual image quality follows a U-shape trend w.r.t dataset size. This finding leads to a practical tool for one-shot estimation on minimal dataset size to prevent GAN replication which can be used to guide datasets construction and selection.



### Discovering Multiple and Diverse Directions for Cognitive Image Properties
- **Arxiv ID**: http://arxiv.org/abs/2202.11772v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.11772v1)
- **Published**: 2022-02-23 20:39:14+00:00
- **Updated**: 2022-02-23 20:39:14+00:00
- **Authors**: Umut Kocasari, Alperen Bag, Oguz Kaan Yuksel, Pinar Yanardag
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research has shown that it is possible to find interpretable directions in the latent spaces of pre-trained GANs. These directions enable controllable generation and support a variety of semantic editing operations. While previous work has focused on discovering a single direction that performs a desired editing operation such as zoom-in, limited work has been done on the discovery of multiple and diverse directions that can achieve the desired edit. In this work, we propose a novel framework that discovers multiple and diverse directions for a given property of interest. In particular, we focus on the manipulation of cognitive properties such as Memorability, Emotional Valence and Aesthetics. We show with extensive experiments that our method successfully manipulates these properties while producing diverse outputs. Our project page and source code can be found at http://catlab-team.github.io/latentcognitive.



### Art Creation with Multi-Conditional StyleGANs
- **Arxiv ID**: http://arxiv.org/abs/2202.11777v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.11777v1)
- **Published**: 2022-02-23 20:45:41+00:00
- **Updated**: 2022-02-23 20:45:41+00:00
- **Authors**: Konstantin Dobler, Florian Hübscher, Jan Westphal, Alejandro Sierra-Múnera, Gerard de Melo, Ralf Krestel
- **Comment**: None
- **Journal**: None
- **Summary**: Creating meaningful art is often viewed as a uniquely human endeavor. A human artist needs a combination of unique skills, understanding, and genuine intention to create artworks that evoke deep feelings and emotions. In this paper, we introduce a multi-conditional Generative Adversarial Network (GAN) approach trained on large amounts of human paintings to synthesize realistic-looking paintings that emulate human art. Our approach is based on the StyleGAN neural network architecture, but incorporates a custom multi-conditional control mechanism that provides fine-granular control over characteristics of the generated paintings, e.g., with regard to the perceived emotion evoked in a spectator. For better control, we introduce the conditional truncation trick, which adapts the standard truncation trick for the conditional setting and diverse datasets. Finally, we develop a diverse set of evaluation techniques tailored to multi-conditional generation.



### RadioTransformer: A Cascaded Global-Focal Transformer for Visual Attention-guided Disease Classification
- **Arxiv ID**: http://arxiv.org/abs/2202.11781v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.11781v2)
- **Published**: 2022-02-23 20:52:30+00:00
- **Updated**: 2022-07-21 20:36:16+00:00
- **Authors**: Moinak Bhattacharya, Shubham Jain, Prateek Prasanna
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present RadioTransformer, a novel visual attention-driven transformer framework, that leverages radiologists' gaze patterns and models their visuo-cognitive behavior for disease diagnosis on chest radiographs. Domain experts, such as radiologists, rely on visual information for medical image interpretation. On the other hand, deep neural networks have demonstrated significant promise in similar tasks even where visual interpretation is challenging. Eye-gaze tracking has been used to capture the viewing behavior of domain experts, lending insights into the complexity of visual search. However, deep learning frameworks, even those that rely on attention mechanisms, do not leverage this rich domain information. RadioTransformer fills this critical gap by learning from radiologists' visual search patterns, encoded as 'human visual attention regions' in a cascaded global-focal transformer framework. The overall 'global' image characteristics and the more detailed 'local' features are captured by the proposed global and focal modules, respectively. We experimentally validate the efficacy of our student-teacher approach for 8 datasets involving different disease classification tasks where eye-gaze data is not available during the inference phase. Code: https://github.com/bmi-imaginelab/radiotransformer.



### Nuclei panoptic segmentation and composition regression with multi-task deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/2202.11804v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.11804v1)
- **Published**: 2022-02-23 22:09:37+00:00
- **Updated**: 2022-02-23 22:09:37+00:00
- **Authors**: Satoshi Kondo, Satoshi Kasai
- **Comment**: None
- **Journal**: None
- **Summary**: Nuclear segmentation, classification and quantification within Haematoxylin & Eosin stained histology images enables the extraction of interpretable cell-based features that can be used in downstream explainable models in computational pathology. The Colon Nuclei Identification and Counting (CoNIC) Challenge is held to help drive forward research and innovation for automatic nuclei recognition in computational pathology. This report describes our proposed method submitted to the CoNIC challenge. Our method employs a multi-task learning framework, which performs a panoptic segmentation task and a regression task. For the panoptic segmentation task, we use encoder-decoder type deep neural networks predicting a direction map in addition to a segmentation map in order to separate neighboring nuclei into different instances



### A modification of the conjugate direction method for motion estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.11831v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.11831v1)
- **Published**: 2022-02-23 23:54:57+00:00
- **Updated**: 2022-02-23 23:54:57+00:00
- **Authors**: Marcos Faundez-Zanuy, Francesc Tarres-Ruiz
- **Comment**: 4 pages
- **Journal**: EUSIPCO 1992, Brussels
- **Summary**: A comparative study of different block matching alternatives for motion estimation is presented. The study is focused on computational burden and objective measures on the accuracy of prediction. Together with existing algorithms several new variations have been tested. An interesting modification of the conjugate direction method previously related in literature is reported. This new algorithm shows a good trade-off between computational complexity and accuracy of motion vector estimation. Computational complexity is evaluated using a sequence of artificial images designed to incorporate a great variety of motion vectors. The performance of block matching methods has been measured in terms of the entropy in the error signal between the motion compensated and the original frames.



### Near Perfect GAN Inversion
- **Arxiv ID**: http://arxiv.org/abs/2202.11833v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.11833v1)
- **Published**: 2022-02-23 23:58:13+00:00
- **Updated**: 2022-02-23 23:58:13+00:00
- **Authors**: Qianli Feng, Viraj Shah, Raghudeep Gadde, Pietro Perona, Aleix Martinez
- **Comment**: None
- **Journal**: None
- **Summary**: To edit a real photo using Generative Adversarial Networks (GANs), we need a GAN inversion algorithm to identify the latent vector that perfectly reproduces it. Unfortunately, whereas existing inversion algorithms can synthesize images similar to real photos, they cannot generate the identical clones needed in most applications. Here, we derive an algorithm that achieves near perfect reconstructions of photos. Rather than relying on encoder- or optimization-based methods to find an inverse mapping on a fixed generator $G(\cdot)$, we derive an approach to locally adjust $G(\cdot)$ to more optimally represent the photos we wish to synthesize. This is done by locally tweaking the learned mapping $G(\cdot)$ s.t. $\| {\bf x} - G({\bf z}) \|<\epsilon$, with ${\bf x}$ the photo we wish to reproduce, ${\bf z}$ the latent vector, $\|\cdot\|$ an appropriate metric, and $\epsilon > 0$ a small scalar. We show that this approach can not only produce synthetic images that are indistinguishable from the real photos we wish to replicate, but that these images are readily editable. We demonstrate the effectiveness of the derived algorithm on a variety of datasets including human faces, animals, and cars, and discuss its importance for diversity and inclusion.



