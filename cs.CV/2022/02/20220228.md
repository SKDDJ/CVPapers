# Arxiv Papers in cs.CV on 2022-02-28
### Long-Tailed Classification with Gradual Balanced Loss and Adaptive Feature Generation
- **Arxiv ID**: http://arxiv.org/abs/2203.00452v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.00452v1)
- **Published**: 2022-02-28 01:20:35+00:00
- **Updated**: 2022-02-28 01:20:35+00:00
- **Authors**: Zihan Zhang, Xiang Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: The real-world data distribution is essentially long-tailed, which poses great challenge to the deep model. In this work, we propose a new method, Gradual Balanced Loss and Adaptive Feature Generator (GLAG) to alleviate imbalance. GLAG first learns a balanced and robust feature model with Gradual Balanced Loss, then fixes the feature model and augments the under-represented tail classes on the feature level with the knowledge from well-represented head classes. And the generated samples are mixed up with real training samples during training epochs. Gradual Balanced Loss is a general loss and it can combine with different decoupled training methods to improve the original performance. State-of-the-art results have been achieved on long-tail datasets such as CIFAR100-LT, ImageNetLT, and iNaturalist, which demonstrates the effectiveness of GLAG for long-tailed visual recognition.



### Globally Optimal Boresight Alignment of UAV-LiDAR Systems
- **Arxiv ID**: http://arxiv.org/abs/2202.13501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13501v1)
- **Published**: 2022-02-28 01:48:10+00:00
- **Updated**: 2022-02-28 01:48:10+00:00
- **Authors**: Smitha Gopinath, Hassan L. Hijazi, Adam Collins, Julian Dann Nathan Lemons, Emily Schultz-Fellenz, Russell Bent, Amira Hijazi, Gert Riemersma
- **Comment**: None
- **Journal**: None
- **Summary**: In airborne light detection and ranging (LiDAR) systems, misalignments between the LiDAR-scanner and the inertial navigation system (INS) mounted on an unmanned aerial vehicle (UAV)'s frame can lead to inaccurate 3D point clouds. Determining the orientation offset, or boresight error is key to many LiDAR-based applications. In this work, we introduce a mixed-integer quadratically constrained quadratic program (MIQCQP) that can globally solve this misalignment problem. We also propose a nested spatial branch and bound (nsBB) algorithm that improves computational performance. The nsBB relies on novel preprocessing steps that progressively reduce the problem size. In addition, an adaptive grid search (aGS) allowing us to obtain quick heuristic solutions is presented. Our algorithms are open-source, multi-threaded and multi-machine compatible.



### ESW Edge-Weights : Ensemble Stochastic Watershed Edge-Weights for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2202.13502v1
- **DOI**: 10.1109/LGRS.2022.3173793
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13502v1)
- **Published**: 2022-02-28 01:53:22+00:00
- **Updated**: 2022-02-28 01:53:22+00:00
- **Authors**: Rohan Agarwal, Aman Aziz, Aditya Suraj Krishnan, Aditya Challa, Sravan Danda
- **Comment**: This article is under review at Geoscience and Remote Sensing
  Letters. Copyright could be transferred at any time
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) classification is a topic of active research. One of the main challenges of HSI classification is the lack of reliable labelled samples. Various semi-supervised and unsupervised classification methods are proposed to handle the low number of labelled samples. Chief among them are graph convolution networks (GCN) and their variants. These approaches exploit the graph structure for semi-supervised and unsupervised classification. While several of these methods implicitly construct edge-weights, to our knowledge, not much work has been done to estimate the edge-weights explicitly. In this article, we estimate the edge-weights explicitly and use them for the downstream classification tasks - both semi-supervised and unsupervised. The proposed edge-weights are based on two key insights - (a) Ensembles reduce the variance and (b) Classes in HSI datasets and feature similarity have only one-sided implications. That is, while same classes would have similar features, similar features do not necessarily imply the same classes. Exploiting these, we estimate the edge-weights using an aggregate of ensembles of watersheds over subsamples of features. These edge weights are evaluated for both semi-supervised and unsupervised classification tasks. The evaluation for semi-supervised tasks uses Random-Walk based approach. For the unsupervised case, we use a simple filter using a graph convolution network (GCN). In both these cases, the proposed edge weights outperform the traditional approaches to compute edge-weights - Euclidean distances and cosine similarities. Fascinatingly, with the proposed edge-weights, the simplest GCN obtained results comparable to the recent state-of-the-art.



### Cyber Mobility Mirror: A Deep Learning-based Real-World Object Perception Platform Using Roadside LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2202.13505v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13505v2)
- **Published**: 2022-02-28 01:58:24+00:00
- **Updated**: 2022-04-07 23:31:07+00:00
- **Authors**: Zhengwei Bai, Saswat Priyadarshi Nayak, Xuanpeng Zhao, Guoyuan Wu, Matthew J. Barth, Xuewei Qi, Yongkang Liu, Emrah Akin Sisbot, Kentaro Oguchi
- **Comment**: None
- **Journal**: None
- **Summary**: Object perception plays a fundamental role in Cooperative Driving Automation (CDA) which is regarded as a revolutionary promoter for the next-generation transportation systems. However, the vehicle-based perception may suffer from the limited sensing range and occlusion as well as low penetration rates in connectivity. In this paper, we propose Cyber Mobility Mirror (CMM), a next-generation real-time traffic surveillance system for 3D object perception and reconstruction, to explore the potential of roadside sensors for enabling CDA in the real world. The CMM system consists of six main components: 1) the data pre-processor to retrieve and preprocess the raw data; 2) the roadside 3D object detector to generate 3D detection results; 3) the multi-object tracker to identify detected objects; 4) the global locator to map positioning information from the LiDAR coordinate to geographic coordinate using coordinate transformation; 5) the cloud-based communicator to transmit perception information from roadside sensors to equipped vehicles, and 6) the onboard advisor to reconstruct and display the real-time traffic conditions via Graphical User Interface (GUI). In this study, a field-operational system is deployed at a real-world intersection, University Avenue and Iowa Avenue in Riverside, California to assess the feasibility and performance of our CMM system. Results from field tests demonstrate that our CMM prototype system can provide satisfactory perception performance with 96.99% precision and 83.62% recall. High-fidelity real-time traffic conditions (at the object level) can be geo-localized with an average error of 0.14m and displayed on the GUI of the equipped vehicle with a frequency of 3-4 Hz.



### StrongSORT: Make DeepSORT Great Again
- **Arxiv ID**: http://arxiv.org/abs/2202.13514v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13514v2)
- **Published**: 2022-02-28 02:37:19+00:00
- **Updated**: 2023-02-22 02:22:40+00:00
- **Authors**: Yunhao Du, Zhicheng Zhao, Yang Song, Yanyun Zhao, Fei Su, Tao Gong, Hongying Meng
- **Comment**: Accepted by IEEE Transactions on Multimedia 2023
- **Journal**: None
- **Summary**: Recently, Multi-Object Tracking (MOT) has attracted rising attention, and accordingly, remarkable progresses have been achieved. However, the existing methods tend to use various basic models (e.g, detector and embedding model), and different training or inference tricks, etc. As a result, the construction of a good baseline for a fair comparison is essential. In this paper, a classic tracker, i.e., DeepSORT, is first revisited, and then is significantly improved from multiple perspectives such as object detection, feature embedding, and trajectory association. The proposed tracker, named StrongSORT, contributes a strong and fair baseline for the MOT community. Moreover, two lightweight and plug-and-play algorithms are proposed to address two inherent "missing" problems of MOT: missing association and missing detection. Specifically, unlike most methods, which associate short tracklets into complete trajectories at high computation complexity, we propose an appearance-free link model (AFLink) to perform global association without appearance information, and achieve a good balance between speed and accuracy. Furthermore, we propose a Gaussian-smoothed interpolation (GSI) based on Gaussian process regression to relieve the missing detection. AFLink and GSI can be easily plugged into various trackers with a negligible extra computational cost (1.7 ms and 7.1 ms per image, respectively, on MOT17). Finally, by fusing StrongSORT with AFLink and GSI, the final tracker (StrongSORT++) achieves state-of-the-art results on multiple public benchmarks, i.e., MOT17, MOT20, DanceTrack and KITTI. Codes are available at https://github.com/dyhBUPT/StrongSORT and https://github.com/open-mmlab/mmtracking.



### CTformer: Convolution-free Token2Token Dilated Vision Transformer for Low-dose CT Denoising
- **Arxiv ID**: http://arxiv.org/abs/2202.13517v1
- **DOI**: 10.1088/1361-6560/acc000
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13517v1)
- **Published**: 2022-02-28 02:58:16+00:00
- **Updated**: 2022-02-28 02:58:16+00:00
- **Authors**: Dayang Wang, Fenglei Fan, Zhan Wu, Rui Liu, Fei Wang, Hengyong Yu
- **Comment**: 11 pages, 14 figures
- **Journal**: None
- **Summary**: Low-dose computed tomography (LDCT) denoising is an important problem in CT research. Compared to the normal dose CT (NDCT), LDCT images are subjected to severe noise and artifacts. Recently in many studies, vision transformers have shown superior feature representation ability over convolutional neural networks (CNNs). However, unlike CNNs, the potential of vision transformers in LDCT denoising was little explored so far. To fill this gap, we propose a Convolution-free Token2Token Dilated Vision Transformer for low-dose CT denoising. The CTformer uses a more powerful token rearrangement to encompass local contextual information and thus avoids convolution. It also dilates and shifts feature maps to capture longer-range interaction. We interpret the CTformer by statically inspecting patterns of its internal attention maps and dynamically tracing the hierarchical attention flow with an explanatory graph. Furthermore, an overlapped inference mechanism is introduced to effectively eliminate the boundary artifacts that are common for encoder-decoder-based denoising models. Experimental results on Mayo LDCT dataset suggest that the CTformer outperforms the state-of-the-art denoising methods with a low computation overhead.



### PartAfford: Part-level Affordance Discovery from 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/2202.13519v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13519v1)
- **Published**: 2022-02-28 02:58:36+00:00
- **Updated**: 2022-02-28 02:58:36+00:00
- **Authors**: Chao Xu, Yixin Chen, He Wang, Song-Chun Zhu, Yixin Zhu, Siyuan Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding what objects could furnish for humans-namely, learning object affordance-is the crux to bridge perception and action. In the vision community, prior work primarily focuses on learning object affordance with dense (e.g., at a per-pixel level) supervision. In stark contrast, we humans learn the object affordance without dense labels. As such, the fundamental question to devise a computational model is: What is the natural way to learn the object affordance from visual appearance and geometry with humanlike sparse supervision? In this work, we present a new task of part-level affordance discovery (PartAfford): Given only the affordance labels per object, the machine is tasked to (i) decompose 3D shapes into parts and (ii) discover how each part of the object corresponds to a certain affordance category. We propose a novel learning framework for PartAfford, which discovers part-level representations by leveraging only the affordance set supervision and geometric primitive regularization, without dense supervision. The proposed approach consists of two main components: (i) an abstraction encoder with slot attention for unsupervised clustering and abstraction, and (ii) an affordance decoder with branches for part reconstruction, affordance prediction, and cuboidal primitive regularization. To learn and evaluate PartAfford, we construct a part-level, cross-category 3D object affordance dataset, annotated with 24 affordance categories shared among >25, 000 objects. We demonstrate that our method enables both the abstraction of 3D objects and part-level affordance discovery, with generalizability to difficult and cross-category examples. Further ablations reveal the contribution of each component.



### Towards Class-agnostic Tracking Using Feature Decorrelation in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2202.13524v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13524v2)
- **Published**: 2022-02-28 03:33:03+00:00
- **Updated**: 2022-03-19 01:44:30+00:00
- **Authors**: Shengjing Tian, Jun Liu, Xiuping Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Single object tracking in point clouds has been attracting more and more attention owing to the presence of LiDAR sensors in 3D vision. However, the existing methods based on deep neural networks focus mainly on training different models for different categories, which makes them unable to perform well in real-world applications when encountering classes unseen during the training phase. In this work, we investigate a more challenging task in the LiDAR point clouds, class-agnostic tracking, where a general model is supposed to be learned for any specified targets of both observed and unseen categories. In particular, we first investigate the class-agnostic performances of the state-of-the-art trackers via exposing the unseen categories to them during testing, finding that a key factor for class-agnostic tracking is how to constrain fused features between the template and search region to maintain generalization when the distribution is shifted from observed to unseen classes. Therefore, we propose a feature decorrelation method to address this problem, which eliminates the spurious correlations of the fused features through a set of learned weights and further makes the search region consistent among foreground points and distinctive between foreground and background points. Experiments on the KITTI and NuScenes demonstrate that the proposed method can achieve considerable improvements by benchmarking against the advanced trackers P2B and BAT, especially when tracking unseen objects.



### Pattern Based Multivariable Regression using Deep Learning (PBMR-DP)
- **Arxiv ID**: http://arxiv.org/abs/2202.13541v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13541v3)
- **Published**: 2022-02-28 04:37:45+00:00
- **Updated**: 2022-03-09 23:08:40+00:00
- **Authors**: Jiztom Kavalakkatt Francis, Chandan Kumar, Jansel Herrera-Gerena, Kundan Kumar, Matthew J Darr
- **Comment**: 7 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: We propose a deep learning methodology for multivariate regression that is based on pattern recognition that triggers fast learning over sensor data. We used a conversion of sensors-to-image which enables us to take advantage of Computer Vision architectures and training processes. In addition to this data preparation methodology, we explore the use of state-of-the-art architectures to generate regression outputs to predict agricultural crop continuous yield information. Finally, we compare with some of the top models reported in MLCAS2021. We found that using a straightforward training process, we were able to accomplish an MAE of 4.394, RMSE of 5.945, and R^2 of 0.861.



### Towards A Device-Independent Deep Learning Approach for the Automated Segmentation of Sonographic Fetal Brain Structures: A Multi-Center and Multi-Device Validation
- **Arxiv ID**: http://arxiv.org/abs/2202.13553v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.13553v1)
- **Published**: 2022-02-28 05:42:03+00:00
- **Updated**: 2022-02-28 05:42:03+00:00
- **Authors**: Abhi Lad, Adithya Narayan, Hari Shankar, Shefali Jain, Pooja Punjani Vyas, Divya Singh, Nivedita Hegde, Jagruthi Atada, Jens Thang, Saw Shier Nee, Arunkumar Govindarajan, Roopa PS, Muralidhar V Pai, Akhila Vasudeva, Prathima Radhakrishnan, Sripad Krishna Devalla
- **Comment**: SPIE Medical Imaging 2022: Computer Aided Diagnosis (12033-75), 11
  pages, 7 figures
- **Journal**: None
- **Summary**: Quality assessment of prenatal ultrasonography is essential for the screening of fetal central nervous system (CNS) anomalies. The interpretation of fetal brain structures is highly subjective, expertise-driven, and requires years of training experience, limiting quality prenatal care for all pregnant mothers. With recent advancement in Artificial Intelligence (AI), specifically deep learning (DL), assistance in precise anatomy identification through semantic segmentation essential for the reliable assessment of growth and neurodevelopment, and detection of structural abnormalities have been proposed. However, existing works only identify certain structures (e.g., cavum septum pellucidum, lateral ventricles, cerebellum) from either of the axial views (transventricular, transcerebellar), limiting the scope for a thorough anatomical assessment as per practice guidelines necessary for the screening of CNS anomalies. Further, existing works do not analyze the generalizability of these DL algorithms across images from multiple ultrasound devices and centers, thus, limiting their real-world clinical impact. In this study, we propose a DL based segmentation framework for the automated segmentation of 10 key fetal brain structures from 2 axial planes from fetal brain USG images (2D). We developed a custom U-Net variant that uses inceptionv4 block as a feature extractor and leverages custom domain-specific data augmentation. Quantitatively, the mean (10 structures; test sets 1/2/3/4) Dice-coefficients were: 0.827, 0.802, 0.731, 0.783. Irrespective of the USG device/center, the DL segmentations were qualitatively comparable to their manual segmentations. The proposed DL system offered a promising and generalizable performance (multi-centers, multi-device) and also presents evidence in support of device-induced variation in image quality (a challenge to generalizibility) by using UMAP analysis.



### ConvNeXt-backbone HoVerNet for nuclei segmentation and classification
- **Arxiv ID**: http://arxiv.org/abs/2202.13560v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13560v2)
- **Published**: 2022-02-28 06:06:41+00:00
- **Updated**: 2022-03-29 01:49:49+00:00
- **Authors**: Jiachen Li, Chixin Wang, Banban Huang, Zekun Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: This manuscript gives a brief description of the algorithm used to participate in CoNIC Challenge 2022. After the baseline was made available, we follow the method in it and replace the ResNet baseline with ConvNeXt one. Moreover, we propose to first convert RGB space to Haematoxylin-Eosin-DAB(HED) space, then use Haematoxylin composition of origin image to smooth semantic one hot label. Afterwards, nuclei distribution of train and valid set are explored to select the best fold split for training model for final test phase submission. Results on validation set shows that even with channel of each stage smaller in number, HoVerNet with ConvNeXt-tiny backbone still improves the mPQ+ by 0.04 and multi r2 by 0.0144



### Name Your Style: An Arbitrary Artist-aware Image Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2202.13562v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13562v2)
- **Published**: 2022-02-28 06:21:38+00:00
- **Updated**: 2022-03-05 00:47:30+00:00
- **Authors**: Zhi-Song Liu, Li-Wen Wang, Wan-Chi Siu, Vicky Kalogeiton
- **Comment**: 15 pages, 15 figures
- **Journal**: None
- **Summary**: Image style transfer has attracted widespread attention in the past few years. Despite its remarkable results, it requires additional style images available as references, making it less flexible and inconvenient. Using text is the most natural way to describe the style. More importantly, text can describe implicit abstract styles, like styles of specific artists or art movements. In this paper, we propose a text-driven image style transfer (TxST) that leverages advanced image-text encoders to control arbitrary style transfer. We introduce a contrastive training strategy to effectively extract style descriptions from the image-text model (i.e., CLIP), which aligns stylization with the text description. To this end, we also propose a novel and efficient attention module that explores cross-attentions to fuse style and content features. Finally, we achieve an arbitrary artist-aware image style transfer to learn and transfer specific artistic characters such as Picasso, oil painting, or a rough sketch. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods on both image and textual styles. Moreover, it can mimic the styles of one or many artists to achieve attractive results, thus highlighting a promising direction in image style transfer.



### Point Set Self-Embedding
- **Arxiv ID**: http://arxiv.org/abs/2202.13577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13577v1)
- **Published**: 2022-02-28 07:03:33+00:00
- **Updated**: 2022-02-28 07:03:33+00:00
- **Authors**: Ruihui Li, Xianzhi Li, Tien-Tsin Wong, Chi-Wing Fu
- **Comment**: Accepted by IEEE Transactions on Visualization and Computer Graphics
  (IEEE TVCG), 2022. All resources can be found at https://liruihui.github.io/
- **Journal**: None
- **Summary**: This work presents an innovative method for point set self-embedding, that encodes the structural information of a dense point set into its sparser version in a visual but imperceptible form. The self-embedded point set can function as the ordinary downsampled one and be visualized efficiently on mobile devices. Particularly, we can leverage the self-embedded information to fully restore the original point set for detailed analysis on remote servers. This task is challenging since both the self-embedded point set and the restored point set should resemble the original one. To achieve a learnable self-embedding scheme, we design a novel framework with two jointly-trained networks: one to encode the input point set into its self-embedded sparse point set and the other to leverage the embedded information for inverting the original point set back. Further, we develop a pair of up-shuffle and down-shuffle units in the two networks, and formulate loss terms to encourage the shape similarity and point distribution in the results. Extensive qualitative and quantitative results demonstrate the effectiveness of our method on both synthetic and real-scanned datasets.



### Using Multi-scale SwinTransformer-HTC with Data augmentation in CoNIC Challenge
- **Arxiv ID**: http://arxiv.org/abs/2202.13588v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13588v2)
- **Published**: 2022-02-28 07:44:59+00:00
- **Updated**: 2022-03-02 14:56:10+00:00
- **Authors**: Chia-Yen Lee, Hsiang-Chin Chien, Ching-Ping Wang, Hong Yen, Kai-Wen Zhen, Hong-Kun Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Colorectal cancer is one of the most common cancers worldwide, so early pathological examination is very important. However, it is time-consuming and labor-intensive to identify the number and type of cells on H&E images in clinical. Therefore, automatic segmentation and classification task and counting the cellular composition of H&E images from pathological sections is proposed by CoNIC Challenge 2022. We proposed a multi-scale Swin transformer with HTC for this challenge, and also applied the known normalization methods to generate more augmentation data. Finally, our strategy showed that the multi-scale played a crucial role to identify different scale features and the augmentation arose the recognition of model.



### Unsupervised Point Cloud Representation Learning with Deep Neural Networks: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2202.13589v3
- **DOI**: 10.1109/TPAMI.2023.3262786
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.13589v3)
- **Published**: 2022-02-28 07:46:05+00:00
- **Updated**: 2023-03-27 02:07:59+00:00
- **Authors**: Aoran Xiao, Jiaxing Huang, Dayan Guan, Xiaoqin Zhang, Shijian Lu, Ling Shao
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: Point cloud data have been widely explored due to its superior accuracy and robustness under various adverse situations. Meanwhile, deep neural networks (DNNs) have achieved very impressive success in various applications such as surveillance and autonomous driving. The convergence of point cloud and DNNs has led to many deep point cloud models, largely trained under the supervision of large-scale and densely-labelled point cloud data. Unsupervised point cloud representation learning, which aims to learn general and useful point cloud representations from unlabelled point cloud data, has recently attracted increasing attention due to the constraint in large-scale point cloud labelling. This paper provides a comprehensive review of unsupervised point cloud representation learning using DNNs. It first describes the motivation, general pipelines as well as terminologies of the recent studies. Relevant background including widely adopted point cloud datasets and DNN architectures is then briefly presented. This is followed by an extensive discussion of existing unsupervised point cloud representation learning methods according to their technical approaches. We also quantitatively benchmark and discuss the reviewed methods over multiple widely adopted point cloud datasets. Finally, we share our humble opinion about several challenges and problems that could be pursued in future research in unsupervised point cloud representation learning. A project associated with this survey has been built at https://github.com/xiaoaoran/3d_url_survey.



### Interactive Machine Learning for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2202.13623v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2202.13623v1)
- **Published**: 2022-02-28 09:02:32+00:00
- **Updated**: 2022-02-28 09:02:32+00:00
- **Authors**: Mareike Hartmann, Aliki Anagnostopoulou, Daniel Sonntag
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an approach for interactive learning for an image captioning model. As human feedback is expensive and modern neural network based approaches often require large amounts of supervised data to be trained, we envision a system that exploits human feedback as good as possible by multiplying the feedback using data augmentation methods, and integrating the resulting training examples into the model in a smart way. This approach has three key components, for which we need to find suitable practical implementations: feedback collection, data augmentation, and model update. We outline our idea and review different possibilities to address these tasks.



### Weakly Supervised Disentangled Representation for Goal-conditioned Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.13624v1
- **DOI**: 10.1109/LRA.2022.3141148
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13624v1)
- **Published**: 2022-02-28 09:05:14+00:00
- **Updated**: 2022-02-28 09:05:14+00:00
- **Authors**: Zhifeng Qian, Mingyu You, Hongjun Zhou, Bin He
- **Comment**: 8 pages; Accepted by RAL with ICRA 2022
- **Journal**: IEEE RAL 2022
- **Summary**: Goal-conditioned reinforcement learning is a crucial yet challenging algorithm which enables agents to achieve multiple user-specified goals when learning a set of skills in a dynamic environment. However, it typically requires millions of the environmental interactions explored by agents, which is sample-inefficient. In the paper, we propose a skill learning framework DR-GRL that aims to improve the sample efficiency and policy generalization by combining the Disentangled Representation learning and Goal-conditioned visual Reinforcement Learning. In a weakly supervised manner, we propose a Spatial Transform AutoEncoder (STAE) to learn an interpretable and controllable representation in which different parts correspond to different object attributes (shape, color, position). Due to the high controllability of the representations, STAE can simply recombine and recode the representations to generate unseen goals for agents to practice themselves. The manifold structure of the learned representation maintains consistency with the physical position, which is beneficial for reward calculation. We empirically demonstrate that DR-GRL significantly outperforms the previous methods in sample efficiency and policy generalization. In addition, DR-GRL is also easy to expand to the real robot.



### Enhance transferability of adversarial examples with model architecture
- **Arxiv ID**: http://arxiv.org/abs/2202.13625v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13625v1)
- **Published**: 2022-02-28 09:05:58+00:00
- **Updated**: 2022-02-28 09:05:58+00:00
- **Authors**: Mingyuan Fan, Wenzhong Guo, Shengxing Yu, Zuobin Ying, Ximeng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Transferability of adversarial examples is of critical importance to launch black-box adversarial attacks, where attackers are only allowed to access the output of the target model. However, under such a challenging but practical setting, the crafted adversarial examples are always prone to overfitting to the proxy model employed, presenting poor transferability. In this paper, we suggest alleviating the overfitting issue from a novel perspective, i.e., designing a fitted model architecture. Specifically, delving the bottom of the cause of poor transferability, we arguably decompose and reconstruct the existing model architecture into an effective model architecture, namely multi-track model architecture (MMA). The adversarial examples crafted on the MMA can maximumly relieve the effect of model-specified features to it and toward the vulnerable directions adopted by diverse architectures. Extensive experimental evaluation demonstrates that the transferability of adversarial examples based on the MMA significantly surpass other state-of-the-art model architectures by up to 40% with comparable overhead.



### Avalanche RL: a Continual Reinforcement Learning Library
- **Arxiv ID**: http://arxiv.org/abs/2202.13657v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13657v2)
- **Published**: 2022-02-28 10:01:22+00:00
- **Updated**: 2022-03-24 14:32:41+00:00
- **Authors**: Nicolò Lucchesi, Antonio Carta, Vincenzo Lomonaco, Davide Bacciu
- **Comment**: Presented at the 21st International Conference on Image Analysis and
  Processing (ICIAP 2021)
- **Journal**: None
- **Summary**: Continual Reinforcement Learning (CRL) is a challenging setting where an agent learns to interact with an environment that is constantly changing over time (the stream of experiences). In this paper, we describe Avalanche RL, a library for Continual Reinforcement Learning which allows to easily train agents on a continuous stream of tasks. Avalanche RL is based on PyTorch and supports any OpenAI Gym environment. Its design is based on Avalanche, one of the more popular continual learning libraries, which allow us to reuse a large number of continual learning strategies and improve the interaction between reinforcement learning and continual learning researchers. Additionally, we propose Continual Habitat-Lab, a novel benchmark and a high-level library which enables the usage of the photorealistic simulator Habitat-Sim for CRL research. Overall, Avalanche RL attempts to unify under a common framework continual reinforcement learning applications, which we hope will foster the growth of the field.



### FusionCount: Efficient Crowd Counting via Multiscale Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/2202.13660v1
- **DOI**: 10.1109/ICIP46576.2022.9897322
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13660v1)
- **Published**: 2022-02-28 10:04:07+00:00
- **Updated**: 2022-02-28 10:04:07+00:00
- **Authors**: Yiming Ma, Victor Sanchez, Tanaya Guha
- **Comment**: 5 pages, 11 figures, submit to ICIP
- **Journal**: None
- **Summary**: State-of-the-art crowd counting models follow an encoder-decoder approach. Images are first processed by the encoder to extract features. Then, to account for perspective distortion, the highest-level feature map is fed to extra components to extract multiscale features, which are the input to the decoder to generate crowd densities. However, in these methods, features extracted at earlier stages during encoding are underutilised, and the multiscale modules can only capture a limited range of receptive fields, albeit with considerable computational cost. This paper proposes a novel crowd counting architecture (FusionCount), which exploits the adaptive fusion of a large majority of encoded features instead of relying on additional extraction components to obtain multiscale features. Thus, it can cover a more extensive scope of receptive field sizes and lower the computational cost. We also introduce a new channel reduction block, which can extract saliency information during decoding and further enhance the model's performance. Experiments on two benchmark databases demonstrate that our model achieves state-of-the-art results with reduced computational complexity.



### Bina-Rep Event Frames: a Simple and Effective Representation for Event-based cameras
- **Arxiv ID**: http://arxiv.org/abs/2202.13662v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13662v1)
- **Published**: 2022-02-28 10:23:09+00:00
- **Updated**: 2022-02-28 10:23:09+00:00
- **Authors**: Sami Barchid, José Mennesson, Chaabane Djéraba
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents "Bina-Rep", a simple representation method that converts asynchronous streams of events from event cameras to a sequence of sparse and expressive event frames. By representing multiple binary event images as a single frame of $N$-bit numbers, our method is able to obtain sparser and more expressive event frames thanks to the retained information about event orders in the original stream. Coupled with our proposed model based on a convolutional neural network, the reported results achieve state-of-the-art performance and repeatedly outperforms other common event representation methods. Our approach also shows competitive robustness against common image corruptions, compared to other representation techniques.



### Neural Adaptive SCEne Tracing
- **Arxiv ID**: http://arxiv.org/abs/2202.13664v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13664v2)
- **Published**: 2022-02-28 10:27:23+00:00
- **Updated**: 2022-03-16 18:45:40+00:00
- **Authors**: Rui Li, Darius Rückert, Yuanhao Wang, Ramzi Idoughi, Wolfgang Heidrich
- **Comment**: 27 pages
- **Journal**: None
- **Summary**: Neural rendering with implicit neural networks has recently emerged as an attractive proposition for scene reconstruction, achieving excellent quality albeit at high computational cost. While the most recent generation of such methods has made progress on the rendering (inference) times, very little progress has been made on improving the reconstruction (training) times. In this work, we present Neural Adaptive Scene Tracing (NAScenT), the first neural rendering method based on directly training a hybrid explicit-implicit neural representation. NAScenT uses a hierarchical octree representation with one neural network per leaf node and combines this representation with a two-stage sampling process that concentrates ray samples where they matter most near object surfaces. As a result, NAScenT is capable of reconstructing challenging scenes including both large, sparsely populated volumes like UAV captured outdoor environments, as well as small scenes with high geometric complexity. NAScenT outperforms existing neural rendering approaches in terms of both quality and training time.



### FedDrive: Generalizing Federated Learning to Semantic Segmentation in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2202.13670v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13670v2)
- **Published**: 2022-02-28 10:34:31+00:00
- **Updated**: 2022-09-23 11:28:31+00:00
- **Authors**: Lidia Fantauzzo, Eros Fani', Debora Caldarola, Antonio Tavera, Fabio Cermelli, Marco Ciccone, Barbara Caputo
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic Segmentation is essential to make self-driving vehicles autonomous, enabling them to understand their surroundings by assigning individual pixels to known categories. However, it operates on sensible data collected from the users' cars; thus, protecting the clients' privacy becomes a primary concern. For similar reasons, Federated Learning has been recently introduced as a new machine learning paradigm aiming to learn a global model while preserving privacy and leveraging data on millions of remote devices. Despite several efforts on this topic, no work has explicitly addressed the challenges of federated learning in semantic segmentation for driving so far. To fill this gap, we propose FedDrive, a new benchmark consisting of three settings and two datasets, incorporating the real-world challenges of statistical heterogeneity and domain generalization. We benchmark state-of-the-art algorithms from the federated learning literature through an in-depth analysis, combining them with style transfer methods to improve their generalization ability. We demonstrate that correctly handling normalization statistics is crucial to deal with the aforementioned challenges. Furthermore, style transfer improves performance when dealing with significant appearance shifts. Official website: https://feddrive.github.io.



### Recent Advances and Challenges in Deep Audio-Visual Correlation Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.13673v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.IR, cs.LG, eess.AS, 68T99
- **Links**: [PDF](http://arxiv.org/pdf/2202.13673v1)
- **Published**: 2022-02-28 10:43:01+00:00
- **Updated**: 2022-02-28 10:43:01+00:00
- **Authors**: Luís Vilaça, Yi Yu, Paula Viana
- **Comment**: 8 pages, 1 figure
- **Journal**: None
- **Summary**: Audio-visual correlation learning aims to capture essential correspondences and understand natural phenomena between audio and video. With the rapid growth of deep learning, an increasing amount of attention has been paid to this emerging research issue. Through the past few years, various methods and datasets have been proposed for audio-visual correlation learning, which motivate us to conclude a comprehensive survey. This survey paper focuses on state-of-the-art (SOTA) models used to learn correlations between audio and video, but also discusses some tasks of definition and paradigm applied in AI multimedia. In addition, we investigate some objective functions frequently used for optimizing audio-visual correlation learning models and discuss how audio-visual data is exploited in the optimization process. Most importantly, we provide an extensive comparison and summarization of the recent progress of SOTA audio-visual correlation learning and discuss future research directions.



### AGMR-Net: Attention Guided Multiscale Recovery framework for stroke segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.13687v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13687v2)
- **Published**: 2022-02-28 11:12:16+00:00
- **Updated**: 2022-04-16 15:19:23+00:00
- **Authors**: Xiuquan Du, Kunpeng Ma, Yuhui Song
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic and accurate lesion segmentation is critical for clinically estimating the lesion statuses of stroke diseases and developing appropriate diagnostic systems. Although existing methods have achieved remarkable results, further adoption of the models is hindered by: (1) inter-class indistinction, the normal brain tissue resembles the lesion in appearance. (2) intra-class inconsistency, large variability exists between different areas of the lesion. To solve these challenges in stroke segmentation, we propose a novel method, namely Attention Guided Multiscale Recovery framework (AGMR-Net) in this paper. Firstly, a coarse-grained patch attention module in the encoding is adopted to get a patch-based coarse-grained attention map in a multi-stage explicitly supervised way, enabling target spatial context saliency representation with a patch-based weighting technique that eliminates the effect of intra-class inconsistency. Secondly, to obtain a more detailed boundary partitioning to solve the challenge of the inter-class indistinction, a newly designed cross-dimensional feature fusion module is used to capture global contextual information to further guide the selective aggregation of 2D and 3D features, which can compensate for the lack of boundary learning capability of 2D convolution. Lastly, in the decoding stage, an innovative designed multi-scale deconvolution upsampling instead of linear interpolation enhances the recovery of target space and boundary information. The AGMR-Net is evaluated on the open dataset Anatomical Tracings of Lesions-After-Stroke (ATLAS), achieving the highest dice similarity coefficient (DSC) score of 0.594, Hausdorff distance of 27.005 mm, and average symmetry surface distance of 7.137 mm, which demonstrate that our proposed method outperforms other state-of-the-art methods and has great potential in the diagnosis of stroke.



### Evaluating the Adversarial Robustness of Adaptive Test-time Defenses
- **Arxiv ID**: http://arxiv.org/abs/2202.13711v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13711v2)
- **Published**: 2022-02-28 12:11:40+00:00
- **Updated**: 2022-07-13 16:01:17+00:00
- **Authors**: Francesco Croce, Sven Gowal, Thomas Brunner, Evan Shelhamer, Matthias Hein, Taylan Cemgil
- **Comment**: ICML'22
- **Journal**: None
- **Summary**: Adaptive defenses, which optimize at test time, promise to improve adversarial robustness. We categorize such adaptive test-time defenses, explain their potential benefits and drawbacks, and evaluate a representative variety of the latest adaptive defenses for image classification. Unfortunately, none significantly improve upon static defenses when subjected to our careful case study evaluation. Some even weaken the underlying static model while simultaneously increasing inference computation. While these results are disappointing, we still believe that adaptive test-time defenses are a promising avenue of research and, as such, we provide recommendations for their thorough evaluation. We extend the checklist of Carlini et al. (2019) by providing concrete steps specific to adaptive defenses.



### Towards Robust Stacked Capsule Autoencoder with Hybrid Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2202.13755v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.13755v2)
- **Published**: 2022-02-28 13:17:21+00:00
- **Updated**: 2022-03-01 07:08:23+00:00
- **Authors**: Jiazhu Dai, Siwei Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule networks (CapsNets) are new neural networks that classify images based on the spatial relationships of features. By analyzing the pose of features and their relative positions, it is more capable to recognize images after affine transformation. The stacked capsule autoencoder (SCAE) is a state-of-the-art CapsNet, and achieved unsupervised classification of CapsNets for the first time. However, the security vulnerabilities and the robustness of the SCAE has rarely been explored. In this paper, we propose an evasion attack against SCAE, where the attacker can generate adversarial perturbations based on reducing the contribution of the object capsules in SCAE related to the original category of the image. The adversarial perturbations are then applied to the original images, and the perturbed images will be misclassified. Furthermore, we propose a defense method called Hybrid Adversarial Training (HAT) against such evasion attacks. HAT makes use of adversarial training and adversarial distillation to achieve better robustness and stability. We evaluate the defense method and the experimental results show that the refined SCAE model can achieve 82.14% classification accuracy under evasion attack. The source code is available at https://github.com/FrostbiteXSW/SCAE_Defense.



### One-shot Ultra-high-Resolution Generative Adversarial Network That Synthesizes 16K Images On A Single GPU
- **Arxiv ID**: http://arxiv.org/abs/2202.13799v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13799v3)
- **Published**: 2022-02-28 13:48:41+00:00
- **Updated**: 2023-08-28 04:52:53+00:00
- **Authors**: Junseok Oh, Donghwee Yoon, Injung Kim
- **Comment**: 36 pages, 26 figures
- **Journal**: None
- **Summary**: We propose a one-shot ultra-high-resolution generative adversarial network (OUR-GAN) framework that generates non-repetitive 16K (16, 384 x 8, 640) images from a single training image and is trainable on a single consumer GPU. OUR-GAN generates an initial image that is visually plausible and varied in shape at low resolution, and then gradually increases the resolution by adding detail through super-resolution. Since OUR-GAN learns from a real ultra-high-resolution (UHR) image, it can synthesize large shapes with fine details and long-range coherence, which is difficult to achieve with conventional generative models that rely on the patch distribution learned from relatively small images. OUR-GAN can synthesize high-quality 16K images with 12.5 GB of GPU memory and 4K images with only 4.29 GB as it synthesizes a UHR image part by part through seamless subregion-wise super-resolution. Additionally, OUR-GAN improves visual coherence while maintaining diversity by applying vertical positional convolution. In experiments on the ST4K and RAISE datasets, OUR-GAN exhibited improved fidelity, visual coherency, and diversity compared with the baseline one-shot synthesis models. To the best of our knowledge, OUR-GAN is the first one-shot image synthesizer that generates non-repetitive UHR images on a single consumer GPU. The synthesized image samples are presented at https://our-gan.github.io.



### RestainNet: a self-supervised digital re-stainer for stain normalization
- **Arxiv ID**: http://arxiv.org/abs/2202.13804v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13804v1)
- **Published**: 2022-02-28 14:05:42+00:00
- **Updated**: 2022-02-28 14:05:42+00:00
- **Authors**: Bingchao Zhao, Jiatai Lin, Changhong Liang, Zongjian Yi, Xin Chen, Bingbing Li, Weihao Qiu, Danyi Li, Li Liang, Chu Han, Zaiyi Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Color inconsistency is an inevitable challenge in computational pathology, which generally happens because of stain intensity variations or sections scanned by different scanners. It harms the pathological image analysis methods, especially the learning-based models. A series of approaches have been proposed for stain normalization. However, most of them are lack flexibility in practice. In this paper, we formulated stain normalization as a digital re-staining process and proposed a self-supervised learning model, which is called RestainNet. Our network is regarded as a digital restainer which learns how to re-stain an unstained (grayscale) image. Two digital stains, Hematoxylin (H) and Eosin (E) were extracted from the original image by Beer-Lambert's Law. We proposed a staining loss to maintain the correctness of stain intensity during the restaining process. Thanks to the self-supervised nature, paired training samples are no longer necessary, which demonstrates great flexibility in practical usage. Our RestainNet outperforms existing approaches and achieves state-of-the-art performance with regard to color correctness and structure preservation. We further conducted experiments on the segmentation and classification tasks and the proposed RestainNet achieved outstanding performance compared with SOTA methods. The self-supervised design allows the network to learn any staining style with no extra effort.



### Active learning with binary models for real time data labelling
- **Arxiv ID**: http://arxiv.org/abs/2203.00439v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00439v2)
- **Published**: 2022-02-28 14:09:19+00:00
- **Updated**: 2022-05-03 06:52:38+00:00
- **Authors**: Ankush Deshmukh, Bhargava B C, A V Narasimhadhan
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning (ML) and Deep Learning (DL) tasks primarily depend on data. Most of the ML and DL applications involve supervised learning which requires labelled data. In the initial phases of ML realm lack of data used to be a problem, now we are in a new era of big data. The supervised ML algorithms require data to be labelled and of good quality. Labelling task requires a large amount of money and time investment. Data labelling require a skilled person who will charge high for this task, consider the case of the medical field or the data is in bulk that requires a lot of people assigned to label it. The amount of data that is well enough for training needs to be known, money and time can not be wasted to label the whole data. This paper mainly aims to propose a strategy that helps in labelling the data along with oracle in real-time. With balancing on model contribution for labelling is 89 and 81.1 for furniture type and intel scene image data sets respectively. Further with balancing being kept off model contribution is found to be 83.47 and 78.71 for furniture type and flower data sets respectively.



### DropIT: Dropping Intermediate Tensors for Memory-Efficient DNN Training
- **Arxiv ID**: http://arxiv.org/abs/2202.13808v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13808v3)
- **Published**: 2022-02-28 14:12:00+00:00
- **Updated**: 2023-03-02 13:44:43+00:00
- **Authors**: Joya Chen, Kai Xu, Yuhui Wang, Yifei Cheng, Angela Yao
- **Comment**: ICLR 2023
- **Journal**: None
- **Summary**: A standard hardware bottleneck when training deep neural networks is GPU memory. The bulk of memory is occupied by caching intermediate tensors for gradient computation in the backward pass. We propose a novel method to reduce this footprint - Dropping Intermediate Tensors (DropIT). DropIT drops min-k elements of the intermediate tensors and approximates gradients from the sparsified tensors in the backward pass. Theoretically, DropIT reduces noise on estimated gradients and therefore has a higher rate of convergence than vanilla-SGD. Experiments show that we can drop up to 90\% of the intermediate tensor elements in fully-connected and convolutional layers while achieving higher testing accuracy for Visual Transformers and Convolutional Neural Networks on various tasks (e.g., classification, object detection, instance segmentation). Our code and models are available at https://github.com/chenjoya/dropit.



### Fuse Local and Global Semantics in Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.13837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13837v1)
- **Published**: 2022-02-28 14:48:51+00:00
- **Updated**: 2022-02-28 14:48:51+00:00
- **Authors**: Yuchi Zhao, Yuhao Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Fuse Local and Global Semantics in Representation Learning (FLAGS) to generate richer representations. FLAGS aims at extract both global and local semantics from images to benefit various downstream tasks. It shows promising results under common linear evaluation protocol. We also conduct detection and segmentation on PASCAL VOC and COCO to show the representations extracted by FLAGS are transferable.



### Deepfake Network Architecture Attribution
- **Arxiv ID**: http://arxiv.org/abs/2202.13843v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13843v2)
- **Published**: 2022-02-28 14:54:30+00:00
- **Updated**: 2022-03-14 11:24:41+00:00
- **Authors**: Tianyun Yang, Ziyao Huang, Juan Cao, Lei Li, Xirong Li
- **Comment**: Accepted to AAAI'22
- **Journal**: None
- **Summary**: With the rapid progress of generation technology, it has become necessary to attribute the origin of fake images. Existing works on fake image attribution perform multi-class classification on several Generative Adversarial Network (GAN) models and obtain high accuracies. While encouraging, these works are restricted to model-level attribution, only capable of handling images generated by seen models with a specific seed, loss and dataset, which is limited in real-world scenarios when fake images may be generated by privately trained models. This motivates us to ask whether it is possible to attribute fake images to the source models' architectures even if they are finetuned or retrained under different configurations. In this work, we present the first study on Deepfake Network Architecture Attribution to attribute fake images on architecture-level. Based on an observation that GAN architecture is likely to leave globally consistent fingerprints while traces left by model weights vary in different regions, we provide a simple yet effective solution named DNA-Det for this problem. Extensive experiments on multiple cross-test setups and a large-scale dataset demonstrate the effectiveness of DNA-Det.



### TEScalib: Targetless Extrinsic Self-Calibration of LiDAR and Stereo Camera for Automated Driving Vehicles with Uncertainty Analysis
- **Arxiv ID**: http://arxiv.org/abs/2202.13847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13847v1)
- **Published**: 2022-02-28 15:04:00+00:00
- **Updated**: 2022-02-28 15:04:00+00:00
- **Authors**: Haohao Hu, Fengze Han, Frank Bieder, Jan-Hendrik Pauls, Christoph Stiller
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: In this paper, we present TEScalib, a novel extrinsic self-calibration approach of LiDAR and stereo camera using the geometric and photometric information of surrounding environments without any calibration targets for automated driving vehicles. Since LiDAR and stereo camera are widely used for sensor data fusion on automated driving vehicles, their extrinsic calibration is highly important. However, most of the LiDAR and stereo camera calibration approaches are mainly target-based and therefore time consuming. Even the newly developed targetless approaches in last years are either inaccurate or unsuitable for driving platforms.   To address those problems, we introduce TEScalib. By applying a 3D mesh reconstruction-based point cloud registration, the geometric information is used to estimate the LiDAR to stereo camera extrinsic parameters accurately and robustly. To calibrate the stereo camera, a photometric error function is builded and the LiDAR depth is involved to transform key points from one camera to another. During driving, these two parts are processed iteratively. Besides that, we also propose an uncertainty analysis for reflecting the reliability of the estimated extrinsic parameters. Our TEScalib approach evaluated on the KITTI dataset achieves very promising results.



### Severity classification in cases of Collagen VI-related myopathy with Convolutional Neural Networks and handcrafted texture features
- **Arxiv ID**: http://arxiv.org/abs/2202.13853v2
- **DOI**: 10.1109/ICIP46576.2022.9897961
- **Categories**: **eess.IV**, cs.CV, I.4.7; I.4.9; I.5.1; I.5.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2202.13853v2)
- **Published**: 2022-02-28 15:09:42+00:00
- **Updated**: 2022-07-04 20:15:01+00:00
- **Authors**: Rafael Rodrigues, Susana Quijano-Roy, Robert-Yves Carlier, Antonio M. G. Pinheiro
- **Comment**: (C) 2022 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: 2022 IEEE International Conference on Image Processing (ICIP),
  Bordeaux, France
- **Summary**: Magnetic Resonance Imaging (MRI) is a non-invasive tool for the clinical assessment of low-prevalence neuromuscular disorders. Automated diagnosis methods might reduce the need for biopsies and provide valuable information on disease follow-up. In this paper, three methods are proposed to classify target muscles in Collagen VI-related myopathy cases, based on their degree of involvement, notably a Convolutional Neural Network, a Fully Connected Network to classify texture features, and a hybrid method combining the two feature sets. The proposed methods were evaluated on axial T1-weighted Turbo Spin-Echo MRI from 26 subjects, including Ullrich Congenital Muscular Dystrophy and Bethlem Myopathy patients at different evolution stages. The hybrid model achieved the best cross-validation results, with a global accuracy of 93.8%, and F-scores of 0.99, 0.82, and 0.95, for healthy, mild and moderate/severe cases, respectively.



### Large-Scale 3D Semantic Reconstruction for Automated Driving Vehicles with Adaptive Truncated Signed Distance Function
- **Arxiv ID**: http://arxiv.org/abs/2202.13855v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.13855v1)
- **Published**: 2022-02-28 15:11:25+00:00
- **Updated**: 2022-02-28 15:11:25+00:00
- **Authors**: Haohao Hu, Hexing Yang, Jian Wu, Xiao Lei, Frank Bieder, Jan-Hendrik Pauls, Christoph Stiller
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: The Large-scale 3D reconstruction, texturing and semantic mapping are nowadays widely used for automated driving vehicles, virtual reality and automatic data generation. However, most approaches are developed for RGB-D cameras with colored dense point clouds and not suitable for large-scale outdoor environments using sparse LiDAR point clouds. Since a 3D surface can be usually observed from multiple camera images with different view poses, an optimal image patch selection for the texturing and an optimal semantic class estimation for the semantic mapping are still challenging.   To address these problems, we propose a novel 3D reconstruction, texturing and semantic mapping system using LiDAR and camera sensors. An Adaptive Truncated Signed Distance Function is introduced to describe surfaces implicitly, which can deal with different LiDAR point sparsities and improve model quality. The from this implicit function extracted triangle mesh map is then textured from a series of registered camera images by applying an optimal image patch selection strategy. Besides that, a Markov Random Field-based data fusion approach is proposed to estimate the optimal semantic class for each triangle mesh. Our approach is evaluated on a synthetic dataset, the KITTI dataset and a dataset recorded with our experimental vehicle. The results show that the 3D models generated using our approach are more accurate in comparison to using other state-of-the-art approaches. The texturing and semantic mapping achieve also very promising results.



### Variable Rate Compression for Raw 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2202.13862v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13862v2)
- **Published**: 2022-02-28 15:15:39+00:00
- **Updated**: 2022-05-16 15:18:21+00:00
- **Authors**: Md Ahmed Al Muzaddid, William J. Beksi
- **Comment**: To be published in the 2022 IEEE International Conference on Robotics
  and Automation (ICRA)
- **Journal**: None
- **Summary**: In this paper, we propose a novel variable rate deep compression architecture that operates on raw 3D point cloud data. The majority of learning-based point cloud compression methods work on a downsampled representation of the data. Moreover, many existing techniques require training multiple networks for different compression rates to generate consolidated point clouds of varying quality. In contrast, our network is capable of explicitly processing point clouds and generating a compressed description at a comprehensive range of bitrates. Furthermore, our approach ensures that there is no loss of information as a result of the voxelization process and the density of the point cloud does not affect the encoder/decoder performance. An extensive experimental evaluation shows that our model obtains state-of-the-art results, it is computationally efficient, and it can work directly with point cloud data thus avoiding an expensive voxelized representation.



### EdgeMixup: Improving Fairness for Skin Disease Classification and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.13883v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.13883v1)
- **Published**: 2022-02-28 15:33:31+00:00
- **Updated**: 2022-02-28 15:33:31+00:00
- **Authors**: Haolin Yuan, Armin Hadzic, William Paul, Daniella Villegas de Flores, Philip Mathew, John Aucott, Yinzhi Cao, Philippe Burlina
- **Comment**: None
- **Journal**: None
- **Summary**: Skin lesions can be an early indicator of a wide range of infectious and other diseases. The use of deep learning (DL) models to diagnose skin lesions has great potential in assisting clinicians with prescreening patients. However, these models often learn biases inherent in training data, which can lead to a performance gap in the diagnosis of people with light and/or dark skin tones. To the best of our knowledge, limited work has been done on identifying, let alone reducing, model bias in skin disease classification and segmentation. In this paper, we examine DL fairness and demonstrate the existence of bias in classification and segmentation models for subpopulations with darker skin tones compared to individuals with lighter skin tones, for specific diseases including Lyme, Tinea Corporis and Herpes Zoster. Then, we propose a novel preprocessing, data alteration method, called EdgeMixup, to improve model fairness with a linear combination of an input skin lesion image and a corresponding a predicted edge detection mask combined with color saturation alteration. For the task of skin disease classification, EdgeMixup outperforms much more complex competing methods such as adversarial approaches, achieving a 10.99% reduction in accuracy gap between light and dark skin tone samples, and resulting in 8.4% improved performance for an underrepresented subpopulation.



### A Novel Viewport-Adaptive Motion Compensation Technique for Fisheye Video
- **Arxiv ID**: http://arxiv.org/abs/2202.13892v1
- **DOI**: 10.1109/ICASSP39728.2021.9413576
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.13892v1)
- **Published**: 2022-02-28 15:41:08+00:00
- **Updated**: 2022-02-28 15:41:08+00:00
- **Authors**: Andy Regensky, Christian Herglotz, André Kaup
- **Comment**: 5 pages, 5 figures, 2 tables
- **Journal**: ICASSP 2021
- **Summary**: Although fisheye cameras are in high demand in many application areas due to their large field of view, many image and video signal processing tasks such as motion compensation suffer from the introduced strong radial distortions. A recently proposed projection-based approach takes the fisheye projection into account to improve fisheye motion compensation. However, the approach does not consider the large field of view of fisheye lenses that requires the consideration of different motion planes in 3D space. We propose a novel viewport-adaptive motion compensation technique that applies the motion vectors in different perspective viewports in order to realize these motion planes. Thereby, some pixels are mapped to so-called virtual image planes and require special treatment to obtain reliable mappings between the perspective viewports and the original fisheye image. While the state-of-the-art ultra wide-angle compensation is sufficiently accurate, we propose a virtual image plane compensation that leads to perfect mappings. All in all, we achieve average gains of +2.40 dB in terms of PSNR compared to the state of the art in fisheye motion compensation.



### ReCasNet: Improving consistency within the two-stage mitosis detection framework
- **Arxiv ID**: http://arxiv.org/abs/2202.13912v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/2202.13912v1)
- **Published**: 2022-02-28 16:03:14+00:00
- **Updated**: 2022-02-28 16:03:14+00:00
- **Authors**: Chawan Piansaddhayanon, Sakun Santisukwongchote, Shanop Shuangshoti, Qingyi Tao, Sira Sriswasdi, Ekapol Chuangsuwanich
- **Comment**: None
- **Journal**: None
- **Summary**: Mitotic count (MC) is an important histological parameter for cancer diagnosis and grading, but the manual process for obtaining MC from whole-slide histopathological images is very time-consuming and prone to error. Therefore, deep learning models have been proposed to facilitate this process. Existing approaches utilize a two-stage pipeline: the detection stage for identifying the locations of potential mitotic cells and the classification stage for refining prediction confidences. However, this pipeline formulation can lead to inconsistencies in the classification stage due to the poor prediction quality of the detection stage and the mismatches in training data distributions between the two stages. In this study, we propose a Refine Cascade Network (ReCasNet), an enhanced deep learning pipeline that mitigates the aforementioned problems with three improvements. First, window relocation was used to reduce the number of poor quality false positives generated during the detection stage. Second, object re-cropping was performed with another deep learning model to adjust poorly centered objects. Third, improved data selection strategies were introduced during the classification stage to reduce the mismatches in training data distributions. ReCasNet was evaluated on two large-scale mitotic figure recognition datasets, canine cutaneous mast cell tumor (CCMCT) and canine mammary carcinoma (CMC), which resulted in up to 4.8% percentage point improvements in the F1 scores for mitotic cell detection and 44.1% reductions in mean absolute percentage error (MAPE) for MC prediction. Techniques that underlie ReCasNet can be generalized to other two-stage object detection networks and should contribute to improving the performances of deep learning models in broad digital pathology applications.



### Background Mixup Data Augmentation for Hand and Object-in-Contact Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.13941v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.13941v2)
- **Published**: 2022-02-28 16:47:01+00:00
- **Updated**: 2022-03-01 02:33:16+00:00
- **Authors**: Koya Tango, Takehiko Ohkawa, Ryosuke Furuta, Yoichi Sato
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Detecting the positions of human hands and objects-in-contact (hand-object detection) in each video frame is vital for understanding human activities from videos. For training an object detector, a method called Mixup, which overlays two training images to mitigate data bias, has been empirically shown to be effective for data augmentation. However, in hand-object detection, mixing two hand-manipulation images produces unintended biases, e.g., the concentration of hands and objects in a specific region degrades the ability of the hand-object detector to identify object boundaries. We propose a data-augmentation method called Background Mixup that leverages data-mixing regularization while reducing the unintended effects in hand-object detection. Instead of mixing two images where a hand and an object in contact appear, we mix a target training image with background images without hands and objects-in-contact extracted from external image sources, and use the mixed images for training the detector. Our experiments demonstrated that the proposed method can effectively reduce false positives and improve the performance of hand-object detection in both supervised and semi-supervised learning settings.



### Defect detection and segmentation in X-Ray images of magnesium alloy castings using the Detectron2 framework
- **Arxiv ID**: http://arxiv.org/abs/2202.13945v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.13945v1)
- **Published**: 2022-02-28 16:53:09+00:00
- **Updated**: 2022-02-28 16:53:09+00:00
- **Authors**: Francisco Javier Yagüe, Jose Francisco Diez-Pastor, Pedro Latorre-Carmona, Cesar Ignacio Garcia Osorio
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: New production techniques have emerged that have made it possible to produce metal parts with more complex shapes, making the quality control process more difficult. This implies that the visual and superficial analysis has become even more inefficient. On top of that, it is also not possible to detect internal defects that these parts could have. The use of X-Ray images has made this process much easier, allowing not only to detect superficial defects in a much simpler way, but also to detect welding or casting defects that could represent a serious hazard for the physical integrity of the metal parts. On the other hand, the use of an automatic segmentation approach for detecting defects would help diminish the dependence of defect detection on the subjectivity of the factory operators and their time dependence variability. The aim of this paper is to apply a deep learning system based on Detectron2, a state-of-the-art library applied to object detection and segmentation in images, for the identification and segmentation of these defects on X-Ray images obtained mainly from automotive parts



### Precision-medicine-toolbox: An open-source python package for facilitation of quantitative medical imaging and radiomics analysis
- **Arxiv ID**: http://arxiv.org/abs/2202.13965v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2202.13965v1)
- **Published**: 2022-02-28 17:16:07+00:00
- **Updated**: 2022-02-28 17:16:07+00:00
- **Authors**: Sergey Primakov, Elizaveta Lavrova, Zohaib Salahuddin, Henry C Woodruff, Philippe Lambin
- **Comment**: 15 pages, 6 figures,
  https://github.com/primakov/precision-medicine-toolbox
- **Journal**: None
- **Summary**: Medical image analysis plays a key role in precision medicine as it allows the clinicians to identify anatomical abnormalities and it is routinely used in clinical assessment. Data curation and pre-processing of medical images are critical steps in the quantitative medical image analysis that can have a significant impact on the resulting model performance. In this paper, we introduce a precision-medicine-toolbox that allows researchers to perform data curation, image pre-processing and handcrafted radiomics extraction (via Pyradiomics) and feature exploration tasks with Python. With this open-source solution, we aim to address the data preparation and exploration problem, bridge the gap between the currently existing packages, and improve the reproducibility of quantitative medical imaging research.



### "If you could see me through my eyes": Predicting Pedestrian Perception
- **Arxiv ID**: http://arxiv.org/abs/2202.13981v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2202.13981v2)
- **Published**: 2022-02-28 17:36:12+00:00
- **Updated**: 2022-03-22 10:43:32+00:00
- **Authors**: Julian Petzold, Mostafa Wahby, Franek Stark, Ulrich Behrje, Heiko Hamann
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrians are particularly vulnerable road users in urban traffic. With the arrival of autonomous driving, novel technologies can be developed specifically to protect pedestrians. We propose a machine learning toolchain to train artificial neural networks as models of pedestrian behavior. In a preliminary study, we use synthetic data from simulations of a specific pedestrian crossing scenario to train a variational autoencoder and a long short-term memory network to predict a pedestrian's future visual perception. We can accurately predict a pedestrian's future perceptions within relevant time horizons. By iteratively feeding these predicted frames into these networks, they can be used as simulations of pedestrians as indicated by our results. Such trained networks can later be used to predict pedestrian behaviors even from the perspective of the autonomous car. Another future extension will be to re-train these networks with real-world video data.



### Deep, Deep Learning with BART
- **Arxiv ID**: http://arxiv.org/abs/2202.14005v2
- **DOI**: 10.1002/mrm.29485
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2202.14005v2)
- **Published**: 2022-02-28 18:23:41+00:00
- **Updated**: 2022-09-23 12:05:08+00:00
- **Authors**: Moritz Blumenthal, Guanxiong Luo, Martin Schilling, H. Christian M. Holme, Martin Uecker
- **Comment**: Submitted to Magnetic Resonance in Medicine
- **Journal**: Magnetic Resonance in Medicine 2022;89:678-693
- **Summary**: Purpose: To develop a deep-learning-based image reconstruction framework for reproducible research in MRI.   Methods: The BART toolbox offers a rich set of implementations of calibration and reconstruction algorithms for parallel imaging and compressed sensing. In this work, BART was extended by a non-linear operator framework that provides automatic differentiation to allow computation of gradients. Existing MRI-specific operators of BART, such as the non-uniform fast Fourier transform, are directly integrated into this framework and are complemented by common building blocks used in neural networks. To evaluate the use of the framework for advanced deep-learning-based reconstruction, two state-of-the-art unrolled reconstruction networks, namely the Variational Network [1] and MoDL [2], were implemented.   Results: State-of-the-art deep image-reconstruction networks can be constructed and trained using BART's gradient based optimization algorithms. The BART implementation achieves a similar performance in terms of training time and reconstruction quality compared to the original implementations based on TensorFlow.   Conclusion: By integrating non-linear operators and neural networks into BART, we provide a general framework for deep-learning-based reconstruction in MRI.



### SUNet: Swin Transformer UNet for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2202.14009v1
- **DOI**: 10.1109/ISCAS48785.2022.9937486
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.14009v1)
- **Published**: 2022-02-28 18:26:57+00:00
- **Updated**: 2022-02-28 18:26:57+00:00
- **Authors**: Chi-Mao Fan, Tsung-Jung Liu, Kuan-Hsien Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Image restoration is a challenging ill-posed problem which also has been a long-standing issue. In the past few years, the convolution neural networks (CNNs) almost dominated the computer vision and had achieved considerable success in different levels of vision tasks including image restoration. However, recently the Swin Transformer-based model also shows impressive performance, even surpasses the CNN-based methods to become the state-of-the-art on high-level vision tasks. In this paper, we proposed a restoration model called SUNet which uses the Swin Transformer layer as our basic block and then is applied to UNet architecture for image denoising. The source code and pre-trained models are available at https://github.com/FanChiMao/SUNet.



### Domain Knowledge-Informed Self-Supervised Representations for Workout Form Assessment
- **Arxiv ID**: http://arxiv.org/abs/2202.14019v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.14019v2)
- **Published**: 2022-02-28 18:40:02+00:00
- **Updated**: 2022-10-21 17:10:15+00:00
- **Authors**: Paritosh Parmar, Amol Gharat, Helge Rhodin
- **Comment**: None
- **Journal**: None
- **Summary**: Maintaining proper form while exercising is important for preventing injuries and maximizing muscle mass gains. Detecting errors in workout form naturally requires estimating human's body pose. However, off-the-shelf pose estimators struggle to perform well on the videos recorded in gym scenarios due to factors such as camera angles, occlusion from gym equipment, illumination, and clothing. To aggravate the problem, the errors to be detected in the workouts are very subtle. To that end, we propose to learn exercise-oriented image and video representations from unlabeled samples such that a small dataset annotated by experts suffices for supervised error detection. In particular, our domain knowledge-informed self-supervised approaches (pose contrastive learning and motion disentangling) exploit the harmonic motion of the exercise actions, and capitalize on the large variances in camera angles, clothes, and illumination to learn powerful representations. To facilitate our self-supervised pretraining, and supervised finetuning, we curated a new exercise dataset, \emph{Fitness-AQA} (\url{https://github.com/ParitoshParmar/Fitness-AQA}), comprising of three exercises: BackSquat, BarbellRow, and OverheadPress. It has been annotated by expert trainers for multiple crucial and typically occurring exercise errors. Experimental results show that our self-supervised representations outperform off-the-shelf 2D- and 3D-pose estimators and several other baselines. We also show that our approaches can be applied to other domains/tasks such as pose estimation and dive quality assessment.



### State-of-the-Art in the Architecture, Methods and Applications of StyleGAN
- **Arxiv ID**: http://arxiv.org/abs/2202.14020v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.14020v1)
- **Published**: 2022-02-28 18:42:04+00:00
- **Updated**: 2022-02-28 18:42:04+00:00
- **Authors**: Amit H. Bermano, Rinon Gal, Yuval Alaluf, Ron Mokady, Yotam Nitzan, Omer Tov, Or Patashnik, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have established themselves as a prevalent approach to image synthesis. Of these, StyleGAN offers a fascinating case study, owing to its remarkable visual quality and an ability to support a large array of downstream tasks. This state-of-the-art report covers the StyleGAN architecture, and the ways it has been employed since its conception, while also analyzing its severe limitations. It aims to be of use for both newcomers, who wish to get a grasp of the field, and for more experienced readers that might benefit from seeing current research trends and existing tools laid out. Among StyleGAN's most interesting aspects is its learned latent space. Despite being learned with no supervision, it is surprisingly well-behaved and remarkably disentangled. Combined with StyleGAN's visual quality, these properties gave rise to unparalleled editing capabilities. However, the control offered by StyleGAN is inherently limited to the generator's learned distribution, and can only be applied to images generated by StyleGAN itself. Seeking to bring StyleGAN's latent control to real-world scenarios, the study of GAN inversion and latent space embedding has quickly gained in popularity. Meanwhile, this same study has helped shed light on the inner workings and limitations of StyleGAN. We map out StyleGAN's impressive story through these investigations, and discuss the details that have made StyleGAN the go-to generator. We further elaborate on the visual priors StyleGAN constructs, and discuss their use in downstream discriminative tasks. Looking forward, we point out StyleGAN's limitations and speculate on current trends and promising directions for future research, such as task and target specific fine-tuning.



### Robust Training under Label Noise by Over-parameterization
- **Arxiv ID**: http://arxiv.org/abs/2202.14026v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2202.14026v2)
- **Published**: 2022-02-28 18:50:10+00:00
- **Updated**: 2022-08-02 21:37:23+00:00
- **Authors**: Sheng Liu, Zhihui Zhu, Qing Qu, Chong You
- **Comment**: 25 pages, 4 figures and 6 tables. Code is available at
  https://github.com/shengliu66/SOP
- **Journal**: None
- **Summary**: Recently, over-parameterized deep networks, with increasingly more network parameters than training samples, have dominated the performances of modern machine learning. However, when the training data is corrupted, it has been well-known that over-parameterized networks tend to overfit and do not generalize. In this work, we propose a principled approach for robust training of over-parameterized deep networks in classification tasks where a proportion of training labels are corrupted. The main idea is yet very simple: label noise is sparse and incoherent with the network learned from clean data, so we model the noise and learn to separate it from the data. Specifically, we model the label noise via another sparse over-parameterization term, and exploit implicit algorithmic regularizations to recover and separate the underlying corruptions. Remarkably, when trained using such a simple method in practice, we demonstrate state-of-the-art test accuracy against label noise on a variety of real datasets. Furthermore, our experimental results are corroborated by theory on simplified linear models, showing that exact separation between sparse noise and low-rank data can be achieved under incoherent conditions. The work opens many interesting directions for improving over-parameterized models by using sparse over-parameterization and implicit regularization.



### Learning Semantic Segmentation from Multiple Datasets with Label Shifts
- **Arxiv ID**: http://arxiv.org/abs/2202.14030v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.14030v1)
- **Published**: 2022-02-28 18:55:19+00:00
- **Updated**: 2022-02-28 18:55:19+00:00
- **Authors**: Dongwan Kim, Yi-Hsuan Tsai, Yumin Suh, Masoud Faraki, Sparsh Garg, Manmohan Chandraker, Bohyung Han
- **Comment**: None
- **Journal**: None
- **Summary**: With increasing applications of semantic segmentation, numerous datasets have been proposed in the past few years. Yet labeling remains expensive, thus, it is desirable to jointly train models across aggregations of datasets to enhance data volume and diversity. However, label spaces differ across datasets and may even be in conflict with one another. This paper proposes UniSeg, an effective approach to automatically train models across multiple datasets with differing label spaces, without any manual relabeling efforts. Specifically, we propose two losses that account for conflicting and co-occurring labels to achieve better generalization performance in unseen domains. First, a gradient conflict in training due to mismatched label spaces is identified and a class-independent binary cross-entropy loss is proposed to alleviate such label conflicts. Second, a loss function that considers class-relationships across datasets is proposed for a better multi-dataset training scheme. Extensive quantitative and qualitative analyses on road-scene datasets show that UniSeg improves over multi-dataset baselines, especially on unseen datasets, e.g., achieving more than 8% gain in IoU on KITTI averaged over all the settings.



### Attribute Descent: Simulating Object-Centric Datasets on the Content Level and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2202.14034v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.14034v1)
- **Published**: 2022-02-28 18:58:05+00:00
- **Updated**: 2022-02-28 18:58:05+00:00
- **Authors**: Yue Yao, Liang Zheng, Xiaodong Yang, Milind Napthade, Tom Gedeon
- **Comment**: None
- **Journal**: None
- **Summary**: This article aims to use graphic engines to simulate a large number of training data that have free annotations and possibly strongly resemble to real-world data. Between synthetic and real, a two-level domain gap exists, involving content level and appearance level. While the latter is concerned with appearance style, the former problem arises from a different mechanism, i.e., content mismatch in attributes such as camera viewpoint, object placement and lighting conditions. In contrast to the widely-studied appearance-level gap, the content-level discrepancy has not been broadly studied. To address the content-level misalignment, we propose an attribute descent approach that automatically optimizes engine attributes to enable synthetic data to approximate real-world data. We verify our method on object-centric tasks, wherein an object takes up a major portion of an image. In these tasks, the search space is relatively small, and the optimization of each attribute yields sufficiently obvious supervision signals. We collect a new synthetic asset VehicleX, and reformat and reuse existing the synthetic assets ObjectX and PersonX. Extensive experiments on image classification and object re-identification confirm that adapted synthetic data can be effectively used in three scenarios: training with synthetic data only, training data augmentation and numerically understanding dataset content.



### Spatio-temporal Vision Transformer for Super-resolution Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2203.00030v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2203.00030v1)
- **Published**: 2022-02-28 19:01:10+00:00
- **Updated**: 2022-02-28 19:01:10+00:00
- **Authors**: Charles N. Christensen, Meng Lu, Edward N. Ward, Pietro Lio, Clemens F. Kaminski
- **Comment**: 8 pages, 9 figures. Source code:
  https://github.com/charlesnchr/vsr-sim
- **Journal**: None
- **Summary**: Structured illumination microscopy (SIM) is an optical super-resolution technique that enables live-cell imaging beyond the diffraction limit. Reconstruction of SIM data is prone to artefacts, which becomes problematic when imaging highly dynamic samples because previous methods rely on the assumption that samples are static. We propose a new transformer-based reconstruction method, VSR-SIM, that uses shifted 3-dimensional window multi-head attention in addition to channel attention mechanism to tackle the problem of video super-resolution (VSR) in SIM. The attention mechanisms are found to capture motion in sequences without the need for common motion estimation techniques such as optical flow. We take an approach to training the network that relies solely on simulated data using videos of natural scenery with a model for SIM image formation. We demonstrate a use case enabled by VSR-SIM referred to as rolling SIM imaging, which increases temporal resolution in SIM by a factor of 9. Our method can be applied to any SIM setup enabling precise recordings of dynamic processes in biomedical research with high temporal resolution.



### Voxelmorph++ Going beyond the cranial vault with keypoint supervision and multi-channel instance optimisation
- **Arxiv ID**: http://arxiv.org/abs/2203.00046v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00046v1)
- **Published**: 2022-02-28 19:23:29+00:00
- **Updated**: 2022-02-28 19:23:29+00:00
- **Authors**: Mattias P. Heinrich, Lasse Hansen
- **Comment**: 10 pages, accepted at WBIR 2022
- **Journal**: None
- **Summary**: The majority of current research in deep learning based image registration addresses inter-patient brain registration with moderate deformation magnitudes. The recent Learn2Reg medical registration benchmark has demonstrated that single-scale U-Net architectures, such as VoxelMorph that directly employ a spatial transformer loss, often do not generalise well beyond the cranial vault and fall short of state-of-the-art performance for abdominal or intra-patient lung registration. Here, we propose two straightforward steps that greatly reduce this gap in accuracy. First, we employ keypoint self-supervision with a novel network head that predicts a discretised heatmap and robustly reduces large deformations for better robustness. Second, we replace multiple learned fine-tuning steps by a single instance optimisation with hand-crafted features and the Adam optimiser. Different to other related work, including FlowNet or PDD-Net, our approach does not require a fully discretised architecture with correlation layer. Our ablation study demonstrates the importance of keypoints in both self-supervised and unsupervised (using only a MIND metric) settings. On a multi-centric inspiration-exhale lung CT dataset, including very challenging COPD scans, our method outperforms VoxelMorph by improving nonlinear alignment by 77% compared to 19% - reaching target registration errors of 2 mm that outperform all but one learning methods published to date. Extending the method to semantic features sets new stat-of-the-art performance on inter-subject abdominal CT registration.



### Local and Global GANs with Semantic-Aware Upsampling for Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2203.00047v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.00047v1)
- **Published**: 2022-02-28 19:24:25+00:00
- **Updated**: 2022-02-28 19:24:25+00:00
- **Authors**: Hao Tang, Ling Shao, Philip H. S. Torr, Nicu Sebe
- **Comment**: Accepted to TPAMI, an extended version of a paper published in CVPR
  2020. arXiv admin note: substantial text overlap with arXiv:1912.12215
- **Journal**: None
- **Summary**: In this paper, we address the task of semantic-guided image generation. One challenge common to most existing image-level generation methods is the difficulty in generating small objects and detailed local textures. To address this, in this work we consider generating images using local context. As such, we design a local class-specific generative network using semantic maps as guidance, which separately constructs and learns subgenerators for different classes, enabling it to capture finer details. To learn more discriminative class-specific feature representations for the local generation, we also propose a novel classification module. To combine the advantages of both global image-level and local class-specific generation, a joint generation network is designed with an attention fusion module and a dual-discriminator structure embedded. Lastly, we propose a novel semantic-aware upsampling method, which has a larger receptive field and can take far-away pixels that are semantically related for feature upsampling, enabling it to better preserve semantic consistency for instances with the same semantic labels. Extensive experiments on two image generation tasks show the superior performance of the proposed method. State-of-the-art results are established by large margins on both tasks and on nine challenging public benchmarks. The source code and trained models are available at https://github.com/Ha0Tang/LGGAN.



### Multi-modal Alignment using Representation Codebook
- **Arxiv ID**: http://arxiv.org/abs/2203.00048v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2203.00048v3)
- **Published**: 2022-02-28 19:26:37+00:00
- **Updated**: 2022-03-28 00:11:06+00:00
- **Authors**: Jiali Duan, Liqun Chen, Son Tran, Jinyu Yang, Yi Xu, Belinda Zeng, Trishul Chilimbi
- **Comment**: Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Aligning signals from different modalities is an important step in vision-language representation learning as it affects the performance of later stages such as cross-modality fusion. Since image and text typically reside in different regions of the feature space, directly aligning them at instance level is challenging especially when features are still evolving during training. In this paper, we propose to align at a higher and more stable level using cluster representation. Specifically, we treat image and text as two "views" of the same entity, and encode them into a joint vision-language coding space spanned by a dictionary of cluster centers (codebook). We contrast positive and negative samples via their cluster assignments while simultaneously optimizing the cluster centers. To further smooth out the learning process, we adopt a teacher-student distillation paradigm, where the momentum teacher of one view guides the student learning of the other. We evaluated our approach on common vision language benchmarks and obtain new SoTA on zero-shot cross modality retrieval while being competitive on various other transfer tasks.



### Towards Targeted Change Detection with Heterogeneous Remote Sensing Images for Forest Mortality Mapping
- **Arxiv ID**: http://arxiv.org/abs/2203.00049v2
- **DOI**: 10.1080/07038992.2022.2135497
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2203.00049v2)
- **Published**: 2022-02-28 19:32:52+00:00
- **Updated**: 2023-03-12 15:48:39+00:00
- **Authors**: Jørgen A. Agersborg, Luigi T. Luppino, Stian Normann Anfinsen, Jane Uhd Jepsen
- **Comment**: 46 pages, 11 figures
- **Journal**: Canadian Journal of Remote Sensing, 48(6), 826-848 (2022)
- **Summary**: Several generic methods have recently been developed for change detection in heterogeneous remote sensing data, such as images from synthetic aperture radar (SAR) and multispectral radiometers. However, these are not well suited to detect weak signatures of certain disturbances of ecological systems. To resolve this problem we propose a new approach based on image-to-image translation and one-class classification (OCC). We aim to map forest mortality caused by an outbreak of geometrid moths in a sparsely forested forest-tundra ecotone using multisource satellite images. The images preceding and following the event are collected by Landsat-5 and RADARSAT-2, respectively. Using a recent deep learning method for change-aware image translation, we compute difference images in both satellites' respective domains. These differences are stacked with the original pre- and post-event images and passed to an OCC trained on a small sample from the targeted change class. The classifier produces a credible map of the complex pattern of forest mortality.



### ERF: Explicit Radiance Field Reconstruction From Scratch
- **Arxiv ID**: http://arxiv.org/abs/2203.00051v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, I.3.3; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2203.00051v1)
- **Published**: 2022-02-28 19:37:12+00:00
- **Updated**: 2022-02-28 19:37:12+00:00
- **Authors**: Samir Aroudj, Steven Lovegrove, Eddy Ilg, Tanner Schmidt, Michael Goesele, Richard Newcombe
- **Comment**: 23 pages, 18 figures
- **Journal**: None
- **Summary**: We propose a novel explicit dense 3D reconstruction approach that processes a set of images of a scene with sensor poses and calibrations and estimates a photo-real digital model. One of the key innovations is that the underlying volumetric representation is completely explicit in contrast to neural network-based (implicit) alternatives. We encode scenes explicitly using clear and understandable mappings of optimization variables to scene geometry and their outgoing surface radiance. We represent them using hierarchical volumetric fields stored in a sparse voxel octree. Robustly reconstructing such a volumetric scene model with millions of unknown variables from registered scene images only is a highly non-convex and complex optimization problem. To this end, we employ stochastic gradient descent (Adam) which is steered by an inverse differentiable renderer.   We demonstrate that our method can reconstruct models of high quality that are comparable to state-of-the-art implicit methods. Importantly, we do not use a sequential reconstruction pipeline where individual steps suffer from incomplete or unreliable information from previous stages, but start our optimizations from uniformed initial solutions with scene geometry and radiance that is far off from the ground truth. We show that our method is general and practical. It does not require a highly controlled lab setup for capturing, but allows for reconstructing scenes with a vast variety of objects, including challenging ones, such as outdoor plants or furry toys. Finally, our reconstructed scene models are versatile thanks to their explicit design. They can be edited interactively which is computationally too costly for implicit alternatives.



### Optimal Transport-based Graph Matching for 3D retinal OCT image registration
- **Arxiv ID**: http://arxiv.org/abs/2203.00069v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00069v1)
- **Published**: 2022-02-28 20:15:12+00:00
- **Updated**: 2022-02-28 20:15:12+00:00
- **Authors**: Xin Tian, Nantheera Anantrasirichai, Lindsay Nicholson, Alin Achim
- **Comment**: None
- **Journal**: None
- **Summary**: Registration of longitudinal optical coherence tomography (OCT) images assists disease monitoring and is essential in image fusion applications. Mouse retinal OCT images are often collected for longitudinal study of eye disease models such as uveitis, but their quality is often poor compared with human imaging. This paper presents a novel but efficient framework involving an optimal transport based graph matching (OT-GM) method for 3D mouse OCT image registration. We first perform registration of fundus-like images obtained by projecting all b-scans of a volume on a plane orthogonal to them, hereafter referred to as the x-y plane. We introduce Adaptive Weighted Vessel Graph Descriptors (AWVGD) and 3D Cube Descriptors (CD) to identify the correspondence between nodes of graphs extracted from segmented vessels within the OCT projection images. The AWVGD comprises scaling, translation and rotation, which are computationally efficient, whereas CD exploits 3D spatial and frequency domain information. The OT-GM method subsequently performs the correct alignment in the x-y plane. Finally, registration along the direction orthogonal to the x-y plane (the z-direction) is guided by the segmentation of two important anatomical features peculiar to mouse b-scans, the Internal Limiting Membrane (ILM) and the hyaloid remnant (HR). Both subjective and objective evaluation results demonstrate that our framework outperforms other well-established methods on mouse OCT images within a reasonable execution time.



### One Model is All You Need: Multi-Task Learning Enables Simultaneous Histology Image Segmentation and Classification
- **Arxiv ID**: http://arxiv.org/abs/2203.00077v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00077v2)
- **Published**: 2022-02-28 20:22:39+00:00
- **Updated**: 2022-11-14 16:29:34+00:00
- **Authors**: Simon Graham, Quoc Dang Vu, Mostafa Jahanifar, Shan E Ahmed Raza, Fayyaz Minhas, David Snead, Nasir Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: The recent surge in performance for image analysis of digitised pathology slides can largely be attributed to the advances in deep learning. Deep models can be used to initially localise various structures in the tissue and hence facilitate the extraction of interpretable features for biomarker discovery. However, these models are typically trained for a single task and therefore scale poorly as we wish to adapt the model for an increasing number of different tasks. Also, supervised deep learning models are very data hungry and therefore rely on large amounts of training data to perform well. In this paper, we present a multi-task learning approach for segmentation and classification of nuclei, glands, lumina and different tissue regions that leverages data from multiple independent data sources. While ensuring that our tasks are aligned by the same tissue type and resolution, we enable meaningful simultaneous prediction with a single network. As a result of feature sharing, we also show that the learned representation can be used to improve the performance of additional tasks via transfer learning, including nuclear classification and signet ring cell detection. As part of this work, we train our developed Cerberus model on a huge amount of data, consisting of over 600K objects for segmentation and 440K patches for classification. We use our approach to process 599 colorectal whole-slide images from TCGA, where we localise 377 million, 900K and 2.1 million nuclei, glands and lumina, respectively and make the results available to the community for downstream analysis.



### Deep Camera Pose Regression Using Pseudo-LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2203.00080v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2203.00080v1)
- **Published**: 2022-02-28 20:30:37+00:00
- **Updated**: 2022-02-28 20:30:37+00:00
- **Authors**: Ali Raza, Lazar Lolic, Shahmir Akhter, Alfonso Dela Cruz, Michael Liut
- **Comment**: 7 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: An accurate and robust large-scale localization system is an integral component for active areas of research such as autonomous vehicles and augmented reality. To this end, many learning algorithms have been proposed that predict 6DOF camera pose from RGB or RGB-D images. However, previous methods that incorporate depth typically treat the data the same way as RGB images, often adding depth maps as additional channels to RGB images and passing them through convolutional neural networks (CNNs). In this paper, we show that converting depth maps into pseudo-LiDAR signals, previously shown to be useful for 3D object detection, is a better representation for camera localization tasks by projecting point clouds that can accurately determine 6DOF camera pose. This is demonstrated by first comparing localization accuracies of a network operating exclusively on pseudo-LiDAR representations, with networks operating exclusively on depth maps. We then propose FusionLoc, a novel architecture that uses pseudo-LiDAR to regress a 6DOF camera pose. FusionLoc is a dual stream neural network, which aims to remedy common issues with typical 2D CNNs operating on RGB-D images. The results from this architecture are compared against various other state-of-the-art deep pose regression implementations using the 7 Scenes dataset. The findings are that FusionLoc performs better than a number of other camera localization methods, with a notable improvement being, on average, 0.33m and 4.35{\deg} more accurate than RGB-D PoseNet. By proving the validity of using pseudo-LiDAR signals over depth maps for localization, there are new considerations when implementing large-scale localization systems.



### MRI-GAN: A Generalized Approach to Detect DeepFakes using Perceptual Image Assessment
- **Arxiv ID**: http://arxiv.org/abs/2203.00108v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00108v1)
- **Published**: 2022-02-28 21:53:07+00:00
- **Updated**: 2022-02-28 21:53:07+00:00
- **Authors**: Pratikkumar Prajapati, Chris Pollett
- **Comment**: 9 pages, 11 figures, 2 tables
- **Journal**: None
- **Summary**: DeepFakes are synthetic videos generated by swapping a face of an original image with the face of somebody else. In this paper, we describe our work to develop general, deep learning-based models to classify DeepFake content. We propose a novel framework for using Generative Adversarial Network (GAN)-based models, we call MRI-GAN, that utilizes perceptual differences in images to detect synthesized videos. We test our MRI-GAN approach and a plain-frames-based model using the DeepFake Detection Challenge Dataset. Our plain frames-based-model achieves 91% test accuracy and a model which uses our MRI-GAN framework with Structural Similarity Index Measurement (SSIM) for the perceptual differences achieves 74% test accuracy. The results of MRI-GAN are preliminary and may be improved further by modifying the choice of loss function, tuning hyper-parameters, or by using a more advanced perceptual similarity metric.



### The Right Spin: Learning Object Motion from Rotation-Compensated Flow Fields
- **Arxiv ID**: http://arxiv.org/abs/2203.00115v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00115v1)
- **Published**: 2022-02-28 22:05:09+00:00
- **Updated**: 2022-02-28 22:05:09+00:00
- **Authors**: Pia Bideau, Erik Learned-Miller, Cordelia Schmid, Karteek Alahari
- **Comment**: None
- **Journal**: None
- **Summary**: Both a good understanding of geometrical concepts and a broad familiarity with objects lead to our excellent perception of moving objects. The human ability to detect and segment moving objects works in the presence of multiple objects, complex background geometry, motion of the observer and even camouflage. How humans perceive moving objects so reliably is a longstanding research question in computer vision and borrows findings from related areas such as psychology, cognitive science and physics. One approach to the problem is to teach a deep network to model all of these effects. This contrasts with the strategy used by human vision, where cognitive processes and body design are tightly coupled and each is responsible for certain aspects of correctly identifying moving objects. Similarly from the computer vision perspective, there is evidence that classical, geometry-based techniques are better suited to the "motion-based" parts of the problem, while deep networks are more suitable for modeling appearance. In this work, we argue that the coupling of camera rotation and camera translation can create complex motion fields that are difficult for a deep network to untangle directly. We present a novel probabilistic model to estimate the camera's rotation given the motion field. We then rectify the flow field to obtain a rotation-compensated motion field for subsequent segmentation. This strategy of first estimating camera motion, and then allowing a network to learn the remaining parts of the problem, yields improved results on the widely used DAVIS benchmark as well as the recently published motion segmentation data set MoCA (Moving Camouflaged Animals).



### Enhancing Satellite Imagery using Deep Learning for the Sensor To Shooter Timeline
- **Arxiv ID**: http://arxiv.org/abs/2203.00116v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00116v3)
- **Published**: 2022-02-28 22:10:02+00:00
- **Updated**: 2022-03-30 20:08:29+00:00
- **Authors**: Matthew Ciolino, Dominick Hambrick, David Noever
- **Comment**: 5 Pages, 3 Figures, 1 Table, 39 References
- **Journal**: None
- **Summary**: The sensor to shooter timeline is affected by two main variables: satellite positioning and asset positioning. Speeding up satellite positioning by adding more sensors or by decreasing processing time is important only if there is a prepared shooter, otherwise the main source of time is getting the shooter into position. However, the intelligence community should work towards the exploitation of sensors to the highest speed and effectiveness possible. Achieving a high effectiveness while keeping speed high is a tradeoff that must be considered in the sensor to shooter timeline. In this paper we investigate two main ideas, increasing the effectiveness of satellite imagery through image manipulation and how on-board image manipulation would affect the sensor to shooter timeline. We cover these ideas in four scenarios: Discrete Event Simulation of onboard processing versus ground station processing, quality of information with cloud cover removal, information improvement with super resolution, and data reduction with image to caption. This paper will show how image manipulation techniques such as Super Resolution, Cloud Removal, and Image to Caption will improve the quality of delivered information in addition to showing how those processes effect the sensor to shooter timeline.



### Rectifying homographies for stereo vision: analytical solution for minimal distortion
- **Arxiv ID**: http://arxiv.org/abs/2203.00123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00123v1)
- **Published**: 2022-02-28 22:35:47+00:00
- **Updated**: 2022-02-28 22:35:47+00:00
- **Authors**: Pasquale Lafiosca, Marta Ceccaroni
- **Comment**: None
- **Journal**: Lecture Notes in Networks and Systems, 2022
- **Summary**: Stereo rectification is the determination of two image transformations (or homographies) that map corresponding points on the two images, projections of the same point in the 3D space, onto the same horizontal line in the transformed images. Rectification is used to simplify the subsequent stereo correspondence problem and speeding up the matching process. Rectifying transformations, in general, introduce perspective distortion on the obtained images, which shall be minimised to improve the accuracy of the following algorithm dealing with the stereo correspondence problem. The search for the optimal transformations is usually carried out relying on numerical optimisation. This work proposes a closed-form solution for the rectifying homographies that minimise perspective distortion. The experimental comparison confirms its capability to solve the convergence issues of the previous formulation. Its Python implementation is provided.



### BlazeNeo: Blazing fast polyp segmentation and neoplasm detection
- **Arxiv ID**: http://arxiv.org/abs/2203.00129v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00129v1)
- **Published**: 2022-02-28 22:58:47+00:00
- **Updated**: 2022-02-28 22:58:47+00:00
- **Authors**: Nguyen Sy An, Phan Ngoc Lan, Dao Viet Hang, Dao Van Long, Tran Quang Trung, Nguyen Thi Thuy, Dinh Viet Sang
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, computer-aided automatic polyp segmentation and neoplasm detection have been an emerging topic in medical image analysis, providing valuable support to colonoscopy procedures. Attentions have been paid to improving the accuracy of polyp detection and segmentation. However, not much focus has been given to latency and throughput for performing these tasks on dedicated devices, which can be crucial for practical applications. This paper introduces a novel deep neural network architecture called BlazeNeo, for the task of polyp segmentation and neoplasm detection with an emphasis on compactness and speed while maintaining high accuracy. The model leverages the highly efficient HarDNet backbone alongside lightweight Receptive Field Blocks for computational efficiency, and an auxiliary training mechanism to take full advantage of the training data for the segmentation quality. Our experiments on a challenging dataset show that BlazeNeo achieves improvements in latency and model size while maintaining comparable accuracy against state-of-the-art methods. When deploying on the Jetson AGX Xavier edge device in INT8 precision, our BlazeNeo achieves over 155 fps while yielding the best accuracy among all compared methods.



### A Data-scalable Transformer for Medical Image Segmentation: Architecture, Model Efficiency, and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2203.00131v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00131v5)
- **Published**: 2022-02-28 22:59:42+00:00
- **Updated**: 2023-04-05 01:56:21+00:00
- **Authors**: Yunhe Gao, Mu Zhou, Di Liu, Zhennan Yan, Shaoting Zhang, Dimitris N. Metaxas
- **Comment**: None
- **Journal**: None
- **Summary**: Transformers have demonstrated remarkable performance in natural language processing and computer vision. However, existing vision Transformers struggle to learn from limited medical data and are unable to generalize on diverse medical image tasks. To tackle these challenges, we present MedFormer, a data-scalable Transformer designed for generalizable 3D medical image segmentation. Our approach incorporates three key elements: a desirable inductive bias, hierarchical modeling with linear-complexity attention, and multi-scale feature fusion that integrates spatial and semantic information globally. MedFormer can learn across tiny- to large-scale data without pre-training. Comprehensive experiments demonstrate MedFormer's potential as a versatile segmentation backbone, outperforming CNNs and vision Transformers on seven public datasets covering multiple modalities (e.g., CT and MRI) and various medical targets (e.g., healthy organs, diseased tissues, and tumors). We provide public access to our models and evaluation pipeline, offering solid baselines and unbiased comparisons to advance a wide range of downstream clinical applications.



### Learning Cross-Video Neural Representations for High-Quality Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2203.00137v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2203.00137v1)
- **Published**: 2022-02-28 23:16:02+00:00
- **Updated**: 2022-02-28 23:16:02+00:00
- **Authors**: Wentao Shangguan, Yu Sun, Weijie Gan, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: This paper considers the problem of temporal video interpolation, where the goal is to synthesize a new video frame given its two neighbors. We propose Cross-Video Neural Representation (CURE) as the first video interpolation method based on neural fields (NF). NF refers to the recent class of methods for the neural representation of complex 3D scenes that has seen widespread success and application across computer vision. CURE represents the video as a continuous function parameterized by a coordinate-based neural network, whose inputs are the spatiotemporal coordinates and outputs are the corresponding RGB values. CURE introduces a new architecture that conditions the neural network on the input frames for imposing space-time consistency in the synthesized video. This not only improves the final interpolation quality, but also enables CURE to learn a prior across multiple videos. Experimental evaluations show that CURE achieves the state-of-the-art performance on video interpolation on several benchmark datasets.



### Spatiotemporal Transformer Attention Network for 3D Voxel Level Joint Segmentation and Motion Prediction in Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2203.00138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2203.00138v1)
- **Published**: 2022-02-28 23:18:27+00:00
- **Updated**: 2022-02-28 23:18:27+00:00
- **Authors**: Zhensong Wei, Xuewei Qi, Zhengwei Bai, Guoyuan Wu, Saswat Nayak, Peng Hao, Matthew Barth, Yongkang Liu, Kentaro Oguchi
- **Comment**: Submitted to IV 2022
- **Journal**: None
- **Summary**: Environment perception including detection, classification, tracking, and motion prediction are key enablers for automated driving systems and intelligent transportation applications. Fueled by the advances in sensing technologies and machine learning techniques, LiDAR-based sensing systems have become a promising solution. The current challenges of this solution are how to effectively combine different perception tasks into a single backbone and how to efficiently learn the spatiotemporal features directly from point cloud sequences. In this research, we propose a novel spatiotemporal attention network based on a transformer self-attention mechanism for joint semantic segmentation and motion prediction within a point cloud at the voxel level. The network is trained to simultaneously outputs the voxel level class and predicted motion by learning directly from a sequence of point cloud datasets. The proposed backbone includes both a temporal attention module (TAM) and a spatial attention module (SAM) to learn and extract the complex spatiotemporal features. This approach has been evaluated with the nuScenes dataset, and promising performance has been achieved.



