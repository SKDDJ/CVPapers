# Arxiv Papers in cs.CV on 2022-02-03
### Exploring Sub-skeleton Trajectories for Interpretable Recognition of Sign Language
- **Arxiv ID**: http://arxiv.org/abs/2202.01390v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, I.5.0; H.3.3; I.2.6; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/2202.01390v1)
- **Published**: 2022-02-03 03:32:28+00:00
- **Updated**: 2022-02-03 03:32:28+00:00
- **Authors**: Joachim Gudmundsson, Martin P. Seybold, John Pfeifer
- **Comment**: To appear in Proc. of the 27th International Conference on Database
  Systems for Advanced Applications (DASFAA-2022)
- **Journal**: None
- **Summary**: Recent advances in tracking sensors and pose estimation software enable smart systems to use trajectories of skeleton joint locations for supervised learning. We study the problem of accurately recognizing sign language words, which is key to narrowing the communication gap between hard and non-hard of hearing people.   Our method explores a geometric feature space that we call `sub-skeleton' aspects of movement. We assess similarity of feature space trajectories using natural, speed invariant distance measures, which enables clear and insightful nearest neighbor classification. The simplicity and scalability of our basic method allows for immediate application in different data domains with little to no parameter tuning.   We demonstrate the effectiveness of our basic method, and a boosted variation, with experiments on data from different application domains and tracking technologies. Surprisingly, our simple methods improve sign recognition over recent, state-of-the-art approaches.



### DocBed: A Multi-Stage OCR Solution for Documents with Complex Layouts
- **Arxiv ID**: http://arxiv.org/abs/2202.01414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.01414v1)
- **Published**: 2022-02-03 05:21:31+00:00
- **Updated**: 2022-02-03 05:21:31+00:00
- **Authors**: Wenzhen Zhu, Negin Sokhandan, Guang Yang, Sujitha Martin, Suchitra Sathyanarayana
- **Comment**: 7 pages, 6 figures, The Thirty-Fourth Annual Conference on Innovative
  Applications of Artificial Intelligence (IAAI-22), Collocated with AAAI-22
- **Journal**: None
- **Summary**: Digitization of newspapers is of interest for many reasons including preservation of history, accessibility and search ability, etc. While digitization of documents such as scientific articles and magazines is prevalent in literature, one of the main challenges for digitization of newspaper lies in its complex layout (e.g. articles spanning multiple columns, text interrupted by images) analysis, which is necessary to preserve human read-order. This work provides a major breakthrough in the digitization of newspapers on three fronts: first, releasing a dataset of 3000 fully-annotated, real-world newspaper images from 21 different U.S. states representing an extensive variety of complex layouts for document layout analysis; second, proposing layout segmentation as a precursor to existing optical character recognition (OCR) engines, where multiple state-of-the-art image segmentation models and several post-processing methods are explored for document layout segmentation; third, providing a thorough and structured evaluation protocol for isolated layout segmentation and end-to-end OCR.



### Characterization of Semantic Segmentation Models on Mobile Platforms for Self-Navigation in Disaster-Struck Zones
- **Arxiv ID**: http://arxiv.org/abs/2202.01421v1
- **DOI**: 10.1109/ACCESS.2022.3190014
- **Categories**: **cs.CV**, cs.AR
- **Links**: [PDF](http://arxiv.org/pdf/2202.01421v1)
- **Published**: 2022-02-03 05:40:49+00:00
- **Updated**: 2022-02-03 05:40:49+00:00
- **Authors**: Ryan Zelek, Hyeran Jeon
- **Comment**: 12 pages, 18 figures
- **Journal**: None
- **Summary**: The role of unmanned vehicles for searching and localizing the victims in disaster impacted areas such as earthquake-struck zones is getting more important. Self-navigation on an earthquake zone has a unique challenge of detecting irregularly shaped obstacles such as road cracks, debris on the streets, and water puddles. In this paper, we characterize a number of state-of-the-art FCN models on mobile embedded platforms for self-navigation at these sites containing extremely irregular obstacles. We evaluate the models in terms of accuracy, performance, and energy efficiency. We present a few optimizations for our designed vision system. Lastly, we discuss the trade-offs of these models for a couple of mobile platforms that can each perform self-navigation. To enable vehicles to safely navigate earthquake-struck zones, we compiled a new annotated image database of various earthquake impacted regions that is different than traditional road damage databases. We train our database with a number of state-of-the-art semantic segmentation models in order to identify obstacles unique to earthquake-struck zones. Based on the statistics and tradeoffs, an optimal CNN model is selected for the mobile vehicular platforms, which we apply to both low-power and extremely low-power configurations of our design. To our best knowledge, this is the first study that identifies unique challenges and discusses the accuracy, performance, and energy impact of edge-based self-navigation mobile vehicles for earthquake-struck zones. Our proposed database and trained models are publicly available.



### Optimized Potential Initialization for Low-latency Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.01440v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.01440v1)
- **Published**: 2022-02-03 07:15:43+00:00
- **Updated**: 2022-02-03 07:15:43+00:00
- **Authors**: Tong Bu, Jianhao Ding, Zhaofei Yu, Tiejun Huang
- **Comment**: Accepted by AAAI 2022
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) have been attached great importance due to the distinctive properties of low power consumption, biological plausibility, and adversarial robustness. The most effective way to train deep SNNs is through ANN-to-SNN conversion, which have yielded the best performance in deep network structure and large-scale datasets. However, there is a trade-off between accuracy and latency. In order to achieve high precision as original ANNs, a long simulation time is needed to match the firing rate of a spiking neuron with the activation value of an analog neuron, which impedes the practical application of SNN. In this paper, we aim to achieve high-performance converted SNNs with extremely low latency (fewer than 32 time-steps). We start by theoretically analyzing ANN-to-SNN conversion and show that scaling the thresholds does play a similar role as weight normalization. Instead of introducing constraints that facilitate ANN-to-SNN conversion at the cost of model capacity, we applied a more direct way by optimizing the initial membrane potential to reduce the conversion loss in each layer. Besides, we demonstrate that optimal initialization of membrane potentials can implement expected error-free ANN-to-SNN conversion. We evaluate our algorithm on the CIFAR-10, CIFAR-100 and ImageNet datasets and achieve state-of-the-art accuracy, using fewer time-steps. For example, we reach top-1 accuracy of 93.38\% on CIFAR-10 with 16 time-steps. Moreover, our method can be applied to other ANN-SNN conversion methodologies and remarkably promote performance when the time-steps is small.



### Concept Bottleneck Model with Additional Unsupervised Concepts
- **Arxiv ID**: http://arxiv.org/abs/2202.01459v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.01459v1)
- **Published**: 2022-02-03 08:30:51+00:00
- **Updated**: 2022-02-03 08:30:51+00:00
- **Authors**: Yoshihide Sawada, Keigo Nakamura
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: With the increasing demands for accountability, interpretability is becoming an essential capability for real-world AI applications. However, most methods utilize post-hoc approaches rather than training the interpretable model. In this article, we propose a novel interpretable model based on the concept bottleneck model (CBM). CBM uses concept labels to train an intermediate layer as the additional visible layer. However, because the number of concept labels restricts the dimension of this layer, it is difficult to obtain high accuracy with a small number of labels. To address this issue, we integrate supervised concepts with unsupervised ones trained with self-explaining neural networks (SENNs). By seamlessly training these two types of concepts while reducing the amount of computation, we can obtain both supervised and unsupervised concepts simultaneously, even for large-sized images. We refer to the proposed model as the concept bottleneck model with additional unsupervised concepts (CBM-AUC). We experimentally confirmed that the proposed model outperformed CBM and SENN. We also visualized the saliency map of each concept and confirmed that it was consistent with the semantic meanings.



### Towards 3D Scene Reconstruction from Locally Scale-Aligned Monocular Video Depth
- **Arxiv ID**: http://arxiv.org/abs/2202.01470v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.01470v4)
- **Published**: 2022-02-03 08:52:54+00:00
- **Updated**: 2023-04-06 03:08:03+00:00
- **Authors**: Guangkai Xu, Wei Yin, Hao Chen, Chunhua Shen, Kai Cheng, Feng Wu, Feng Zhao
- **Comment**: Technical error
- **Journal**: None
- **Summary**: Existing monocular depth estimation methods have achieved excellent robustness in diverse scenes, but they can only retrieve affine-invariant depth, up to an unknown scale and shift. However, in some video-based scenarios such as video depth estimation and 3D scene reconstruction from a video, the unknown scale and shift residing in per-frame prediction may cause the depth inconsistency. To solve this problem, we propose a locally weighted linear regression method to recover the scale and shift with very sparse anchor points, which ensures the scale consistency along consecutive frames. Extensive experiments show that our method can boost the performance of existing state-of-the-art approaches by 50% at most over several zero-shot benchmarks. Besides, we merge over 6.3 million RGBD images to train strong and robust depth models. Our produced ResNet50-backbone model even outperforms the state-of-the-art DPT ViT-Large model. Combining with geometry-based reconstruction methods, we formulate a new dense 3D scene reconstruction pipeline, which benefits from both the scale consistency of sparse points and the robustness of monocular methods. By performing the simple per-frame prediction over a video, the accurate 3D scene shape can be recovered.



### Trajectory Forecasting from Detection with Uncertainty-Aware Motion Encoding
- **Arxiv ID**: http://arxiv.org/abs/2202.01478v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.01478v2)
- **Published**: 2022-02-03 09:09:56+00:00
- **Updated**: 2022-02-10 11:13:04+00:00
- **Authors**: Pu Zhang, Lei Bai, Jianru Xue, Jianwu Fang, Nanning Zheng, Wanli Ouyang
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Trajectory forecasting is critical for autonomous platforms to make safe planning and actions. Currently, most trajectory forecasting methods assume that object trajectories have been extracted and directly develop trajectory predictors based on the ground truth trajectories. However, this assumption does not hold in practical situations. Trajectories obtained from object detection and tracking are inevitably noisy, which could cause serious forecasting errors to predictors built on ground truth trajectories. In this paper, we propose a trajectory predictor directly based on detection results without relying on explicitly formed trajectories. Different from the traditional methods which encode the motion cue of an agent based on its clearly defined trajectory, we extract the motion information only based on the affinity cues among detection results, in which an affinity-aware state update mechanism is designed to take the uncertainty of association into account. In addition, considering that there could be multiple plausible matching candidates, we aggregate the states of them. This design relaxes the undesirable effect of noisy trajectory obtained from data association. Extensive ablation experiments validate the effectiveness of our method and its generalization ability on different detectors. Cross-comparison to other forecasting schemes further proves the superiority of our method. Code will be released upon acceptance.



### Spatial Computing and Intuitive Interaction: Bringing Mixed Reality and Robotics Together
- **Arxiv ID**: http://arxiv.org/abs/2202.01493v1
- **DOI**: 10.1109/MRA.2021.3138384
- **Categories**: **cs.RO**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2202.01493v1)
- **Published**: 2022-02-03 10:04:26+00:00
- **Updated**: 2022-02-03 10:04:26+00:00
- **Authors**: Jeffrey Delmerico, Roi Poranne, Federica Bogo, Helen Oleynikova, Eric Vollenweider, Stelian Coros, Juan Nieto, Marc Pollefeys
- **Comment**: None
- **Journal**: None
- **Summary**: Spatial computing -- the ability of devices to be aware of their surroundings and to represent this digitally -- offers novel capabilities in human-robot interaction. In particular, the combination of spatial computing and egocentric sensing on mixed reality devices enables them to capture and understand human actions and translate these to actions with spatial meaning, which offers exciting new possibilities for collaboration between humans and robots. This paper presents several human-robot systems that utilize these capabilities to enable novel robot use cases: mission planning for inspection, gesture-based control, and immersive teleoperation. These works demonstrate the power of mixed reality as a tool for human-robot interaction, and the potential of spatial computing and mixed reality to drive the future of human-robot interaction.



### PARCEL: Physics-based Unsupervised Contrastive Representation Learning for Multi-coil MR Imaging
- **Arxiv ID**: http://arxiv.org/abs/2202.01494v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01494v3)
- **Published**: 2022-02-03 10:09:19+00:00
- **Updated**: 2022-11-14 05:11:27+00:00
- **Authors**: Shanshan Wang, Ruoyou Wu, Cheng Li, Juan Zou, Ziyao Zhang, Qiegen Liu, Yan Xi, Hairong Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: With the successful application of deep learning to magnetic resonance (MR) imaging, parallel imaging techniques based on neural networks have attracted wide attention. However, in the absence of high-quality, fully sampled datasets for training, the performance of these methods is limited. And the interpretability of models is not strong enough. To tackle this issue, this paper proposes a Physics-bAsed unsupeRvised Contrastive rEpresentation Learning (PARCEL) method to speed up parallel MR imaging. Specifically, PARCEL has a parallel framework to contrastively learn two branches of model-based unrolling networks from augmented undersampled multi-coil k-space data. A sophisticated co-training loss with three essential components has been designed to guide the two networks in capturing the inherent features and representations for MR images. And the final MR image is reconstructed with the trained contrastive networks. PARCEL was evaluated on two vivo datasets and compared to five state-of-the-art methods. The results show that PARCEL is able to learn essential representations for accurate MR reconstruction without relying on fully sampled datasets.



### Predicting the impact of urban change in pedestrian and road safety
- **Arxiv ID**: http://arxiv.org/abs/2202.01781v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.01781v1)
- **Published**: 2022-02-03 10:37:51+00:00
- **Updated**: 2022-02-03 10:37:51+00:00
- **Authors**: Cristina Bustos, Daniel Rhoads, Agata Lapedriza, Javier Borge-Holthoefer, Albert Solé-Ribalta
- **Comment**: None
- **Journal**: None
- **Summary**: Increased interaction between and among pedestrians and vehicles in the crowded urban environments of today gives rise to a negative side-effect: a growth in traffic accidents, with pedestrians being the most vulnerable elements. Recent work has shown that Convolutional Neural Networks are able to accurately predict accident rates exploiting Street View imagery along urban roads. The promising results point to the plausibility of aided design of safe urban landscapes, for both pedestrians and vehicles. In this paper, by considering historical accident data and Street View images, we detail how to automatically predict the impact (increase or decrease) of urban interventions on accident incidence. The results are positive, rendering an accuracies ranging from 60 to 80%. We additionally provide an interpretability analysis to unveil which specific categories of urban features impact accident rates positively or negatively. Considering the transportation network substrates (sidewalk and road networks) and their demand, we integrate these results to a complex network framework, to estimate the effective impact of urban change on the safety of pedestrians and vehicles. Results show that public authorities may leverage on machine learning tools to prioritize targeted interventions, since our analysis show that limited improvement is obtained with current tools. Further, our findings have a wider application range such as the design of safe urban routes for pedestrians or to the field of driver-assistance technologies.



### Bending Graphs: Hierarchical Shape Matching using Gated Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/2202.01537v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2202.01537v1)
- **Published**: 2022-02-03 11:41:46+00:00
- **Updated**: 2022-02-03 11:41:46+00:00
- **Authors**: Mahdi Saleh, Shun-Cheng Wu, Luca Cosmo, Nassir Navab, Benjamin Busam, Federico Tombari
- **Comment**: None
- **Journal**: None
- **Summary**: Shape matching has been a long-studied problem for the computer graphics and vision community. The objective is to predict a dense correspondence between meshes that have a certain degree of deformation. Existing methods either consider the local description of sampled points or discover correspondences based on global shape information. In this work, we investigate a hierarchical learning design, to which we incorporate local patch-level information and global shape-level structures. This flexible representation enables correspondence prediction and provides rich features for the matching stage. Finally, we propose a novel optimal transport solver by recurrently updating features on non-confident nodes to learn globally consistent correspondences between the shapes. Our results on publicly available datasets suggest robust performance in presence of severe deformations without the need for extensive training or refinement.



### Weakly Supervised Nuclei Segmentation via Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.01564v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2202.01564v2)
- **Published**: 2022-02-03 12:51:30+00:00
- **Updated**: 2022-02-11 13:06:29+00:00
- **Authors**: Weizhen Liu, Qian He, Xuming He
- **Comment**: Accepted by ISBI 2022 as Oral Presentation
- **Journal**: None
- **Summary**: Weakly supervised nuclei segmentation is a critical problem for pathological image analysis and greatly benefits the community due to the significant reduction of labeling cost. Adopting point annotations, previous methods mostly rely on less expressive representations for nuclei instances and thus have difficulty in handling crowded nuclei. In this paper, we propose to decouple weakly supervised semantic and instance segmentation in order to enable more effective subtask learning and to promote instance-aware representation learning. To achieve this, we design a modular deep network with two branches: a semantic proposal network and an instance encoding network, which are trained in a two-stage manner with an instance-sensitive loss. Empirical results show that our approach achieves the state-of-the-art performance on two public benchmarks of pathological images from different types of organs.



### Retinal Vessel Segmentation with Pixel-wise Adaptive Filters
- **Arxiv ID**: http://arxiv.org/abs/2202.01782v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01782v1)
- **Published**: 2022-02-03 14:40:36+00:00
- **Updated**: 2022-02-03 14:40:36+00:00
- **Authors**: Mingxing Li, Shenglong Zhou, Chang Chen, Yueyi Zhang, Dong Liu, Zhiwei Xiong
- **Comment**: Accepted by ISBI 2022
- **Journal**: None
- **Summary**: Accurate retinal vessel segmentation is challenging because of the complex texture of retinal vessels and low imaging contrast. Previous methods generally refine segmentation results by cascading multiple deep networks, which are time-consuming and inefficient. In this paper, we propose two novel methods to address these challenges. First, we devise a light-weight module, named multi-scale residual similarity gathering (MRSG), to generate pixel-wise adaptive filters (PA-Filters). Different from cascading multiple deep networks, only one PA-Filter layer can improve the segmentation results. Second, we introduce a response cue erasing (RCE) strategy to enhance the segmentation accuracy. Experimental results on the DRIVE, CHASE_DB1, and STARE datasets demonstrate that our proposed method outperforms state-of-the-art methods while maintaining a compact structure. Code is available at https://github.com/Limingxing00/Retinal-Vessel-Segmentation-ISBI20222.



### Optical skin: Sensor-integration-free multimodal flexible sensing
- **Arxiv ID**: http://arxiv.org/abs/2202.03189v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, physics.app-ph, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2202.03189v1)
- **Published**: 2022-02-03 14:58:27+00:00
- **Updated**: 2022-02-03 14:58:27+00:00
- **Authors**: Sho Shimadera, Kei Kitagawa, Koyo Sagehashi, Tomoaki Niiyama, Satoshi Sunada
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: The biological skin enables animals to sense various stimuli. Extensive efforts have been made recently to develop smart skin-like sensors to extend the capabilities of biological skins; however, simultaneous sensing of several types of stimuli in a large area remains challenging because this requires large-scale sensor integration with numerous wire connections. We propose a simple, highly sensitive, and multimodal sensing approach, which does not require integrating multiple sensors. The proposed approach is based on an optical interference technique, which can encode the information of various stimuli as a spatial pattern. In contrast to the existing approach, the proposed approach, combined with a deep neural network, enables us to freely select the sensing mode according to our purpose. As a key example, we demonstrate simultaneous sensing mode of three different physical quantities, contact force, contact location, and temperature, using a single soft material without requiring complex integration. Another unique property of the proposed approach is spatially continuous sensing with ultrahigh resolution of few tens of micrometers, which enables identifying the shape of the object in contact. Furthermore, we present a haptic soft device for a human-machine interface. The proposed approach encourages the development of high-performance optical skins.



### Oral cancer detection and interpretation: Deep multiple instance learning versus conventional deep single instance learning
- **Arxiv ID**: http://arxiv.org/abs/2202.01783v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.01783v1)
- **Published**: 2022-02-03 15:04:26+00:00
- **Updated**: 2022-02-03 15:04:26+00:00
- **Authors**: Nadezhda Koriakina, Nataša Sladoje, Vladimir Bašić, Joakim Lindblad
- **Comment**: None
- **Journal**: None
- **Summary**: The current medical standard for setting an oral cancer (OC) diagnosis is histological examination of a tissue sample from the oral cavity. This process is time consuming and more invasive than an alternative approach of acquiring a brush sample followed by cytological analysis. Skilled cytotechnologists are able to detect changes due to malignancy, however, to introduce this approach into clinical routine is associated with challenges such as a lack of experts and labour-intensive work. To design a trustworthy OC detection system that would assist cytotechnologists, we are interested in AI-based methods that reliably can detect cancer given only per-patient labels (minimizing annotation bias), and also provide information on which cells are most relevant for the diagnosis (enabling supervision and understanding). We, therefore, perform a comparison of a conventional single instance learning (SIL) approach and a modern multiple instance learning (MIL) method suitable for OC detection and interpretation, utilizing three different neural network architectures. To facilitate systematic evaluation of the considered approaches, we introduce a synthetic PAP-QMNIST dataset, that serves as a model of OC data, while offering access to per-instance ground truth. Our study indicates that on PAP-QMNIST, the SIL performs better, on average, than the MIL approach. Performance at the bag level on real-world cytological data is similar for both methods, yet the single instance approach performs better on average. Visual examination by cytotechnologist indicates that the methods manage to identify cells which deviate from normality, including malignant cells as well as those suspicious for dysplasia. We share the code as open source at https://github.com/MIDA-group/OralCancerMILvsSIL



### FORML: Learning to Reweight Data for Fairness
- **Arxiv ID**: http://arxiv.org/abs/2202.01719v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01719v2)
- **Published**: 2022-02-03 17:36:07+00:00
- **Updated**: 2022-07-20 03:27:47+00:00
- **Authors**: Bobby Yan, Skyler Seto, Nicholas Apostoloff
- **Comment**: 9 pages, 2 figures, Presented at ICML 2022 DataPerf Workshop
- **Journal**: None
- **Summary**: Machine learning models are trained to minimize the mean loss for a single metric, and thus typically do not consider fairness and robustness. Neglecting such metrics in training can make these models prone to fairness violations when training data are imbalanced or test distributions differ. This work introduces Fairness Optimized Reweighting via Meta-Learning (FORML), a training algorithm that balances fairness and robustness with accuracy by jointly learning training sample weights and neural network parameters. The approach increases model fairness by learning to balance the contributions from both over- and under-represented sub-groups through dynamic reweighting of the data learned from a user-specified held-out set representative of the distribution under which fairness is desired. FORML improves equality of opportunity fairness criteria on image classification tasks, reduces bias of corrupted labels, and facilitates building more fair datasets via data condensation. These improvements are achieved without pre-processing data or post-processing model outputs, without learning an additional weighting function, without changing model architecture, and while maintaining accuracy on the original predictive metric.



### Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations
- **Arxiv ID**: http://arxiv.org/abs/2202.03576v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03576v1)
- **Published**: 2022-02-03 17:38:11+00:00
- **Updated**: 2022-02-03 17:38:11+00:00
- **Authors**: Weiqi Peng, Jinghui Chen
- **Comment**: Accepted at ICLR 2022
- **Journal**: None
- **Summary**: Owing much to the revolution of information technology, the recent progress of deep learning benefits incredibly from the vastly enhanced access to data available in various digital formats. However, in certain scenarios, people may not want their data being used for training commercial models and thus studied how to attack the learnability of deep learning models. Previous works on learnability attack only consider the goal of preventing unauthorized exploitation on the specific dataset but not the process of restoring the learnability for authorized cases. To tackle this issue, this paper introduces and investigates a new concept called "learnability lock" for controlling the model's learnability on a specific dataset with a special key. In particular, we propose adversarial invertible transformation, that can be viewed as a mapping from image to image, to slightly modify data samples so that they become "unlearnable" by machine learning models with negligible loss of visual features. Meanwhile, one can unlock the learnability of the dataset and train models normally using the corresponding key. The proposed learnability lock leverages class-wise perturbation that applies a universal transformation function on data samples of the same label. This ensures that the learnability can be easily restored with a simple inverse transformation while remaining difficult to be detected or reverse-engineered. We empirically demonstrate the success and practicability of our method on visual classification tasks.



### Skeleton-Based Action Segmentation with Multi-Stage Spatial-Temporal Graph Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.01727v2
- **DOI**: 10.1109/TETC.2022.3230912
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.01727v2)
- **Published**: 2022-02-03 17:42:04+00:00
- **Updated**: 2022-10-09 19:00:59+00:00
- **Authors**: Benjamin Filtjens, Bart Vanrumste, Peter Slaets
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to identify and temporally segment fine-grained actions in motion capture sequences is crucial for applications in human movement analysis. Motion capture is typically performed with optical or inertial measurement systems, which encode human movement as a time series of human joint locations and orientations or their higher-order representations. State-of-the-art action segmentation approaches use multiple stages of temporal convolutions. The main idea is to generate an initial prediction with several layers of temporal convolutions and refine these predictions over multiple stages, also with temporal convolutions. Although these approaches capture long-term temporal patterns, the initial predictions do not adequately consider the spatial hierarchy among the human joints. To address this limitation, we recently introduced multi-stage spatial-temporal graph convolutional neural networks (MS-GCN). Our framework replaces the initial stage of temporal convolutions with spatial graph convolutions and dilated temporal convolutions, which better exploit the spatial configuration of the joints and their long-term temporal dynamics. Our framework was compared to four strong baselines on five tasks. Experimental results demonstrate that our framework is a strong baseline for skeleton-based action segmentation.



### Fast Online Video Super-Resolution with Deformable Attention Pyramid
- **Arxiv ID**: http://arxiv.org/abs/2202.01731v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01731v2)
- **Published**: 2022-02-03 17:49:04+00:00
- **Updated**: 2022-04-06 13:20:51+00:00
- **Authors**: Dario Fuoli, Martin Danelljan, Radu Timofte, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Video super-resolution (VSR) has many applications that pose strict causal, real-time, and latency constraints, including video streaming and TV. We address the VSR problem under these settings, which poses additional important challenges since information from future frames is unavailable. Importantly, designing efficient, yet effective frame alignment and fusion modules remain central problems. In this work, we propose a recurrent VSR architecture based on a deformable attention pyramid (DAP). Our DAP aligns and integrates information from the recurrent state into the current frame prediction. To circumvent the computational cost of traditional attention-based methods, we only attend to a limited number of spatial locations, which are dynamically predicted by the DAP. Comprehensive experiments and analysis of the proposed key innovations show the effectiveness of our approach. We significantly reduce processing time and computational complexity in comparison to state-of-the-art methods, while maintaining a high performance. We surpass state-of-the-art method EDVR-M on two standard benchmarks with a speed-up of over $3\times$.



### The Met Dataset: Instance-level Recognition for Artworks
- **Arxiv ID**: http://arxiv.org/abs/2202.01747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.01747v1)
- **Published**: 2022-02-03 18:13:30+00:00
- **Updated**: 2022-02-03 18:13:30+00:00
- **Authors**: Nikolaos-Antonios Ypsilantis, Noa Garcia, Guangxing Han, Sarah Ibrahimi, Nanne Van Noord, Giorgos Tolias
- **Comment**: None
- **Journal**: None
- **Summary**: This work introduces a dataset for large-scale instance-level recognition in the domain of artworks. The proposed benchmark exhibits a number of different challenges such as large inter-class similarity, long tail distribution, and many classes. We rely on the open access collection of The Met museum to form a large training set of about 224k classes, where each class corresponds to a museum exhibit with photos taken under studio conditions. Testing is primarily performed on photos taken by museum guests depicting exhibits, which introduces a distribution shift between training and testing. Testing is additionally performed on a set of images not related to Met exhibits making the task resemble an out-of-distribution detection problem. The proposed benchmark follows the paradigm of other recent datasets for instance-level recognition on different domains to encourage research on domain independent approaches. A number of suitable approaches are evaluated to offer a testbed for future comparisons. Self-supervised and supervised contrastive learning are effectively combined to train the backbone which is used for non-parametric classification that is shown as a promising direction. Dataset webpage: http://cmp.felk.cvut.cz/met/



### Deep Surface Reconstruction from Point Clouds with Visibility Information
- **Arxiv ID**: http://arxiv.org/abs/2202.01810v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.01810v1)
- **Published**: 2022-02-03 19:33:47+00:00
- **Updated**: 2022-02-03 19:33:47+00:00
- **Authors**: Raphael Sulzer, Loic Landrieu, Alexandre Boulch, Renaud Marlet, Bruno Vallet
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Most current neural networks for reconstructing surfaces from point clouds ignore sensor poses and only operate on raw point locations. Sensor visibility, however, holds meaningful information regarding space occupancy and surface orientation. In this paper, we present two simple ways to augment raw point clouds with visibility information, so it can directly be leveraged by surface reconstruction networks with minimal adaptation. Our proposed modifications consistently improve the accuracy of generated surfaces as well as the generalization ability of the networks to unseen shape domains. Our code and data is available at https://github.com/raphaelsulzer/dsrv-data.



### ObjectSeeker: Certifiably Robust Object Detection against Patch Hiding Attacks via Patch-agnostic Masking
- **Arxiv ID**: http://arxiv.org/abs/2202.01811v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.01811v2)
- **Published**: 2022-02-03 19:34:25+00:00
- **Updated**: 2022-12-28 19:03:52+00:00
- **Authors**: Chong Xiang, Alexander Valtchanov, Saeed Mahloujifar, Prateek Mittal
- **Comment**: IEEE Symposium on Security and Privacy 2023; extended version
- **Journal**: None
- **Summary**: Object detectors, which are widely deployed in security-critical systems such as autonomous vehicles, have been found vulnerable to patch hiding attacks. An attacker can use a single physically-realizable adversarial patch to make the object detector miss the detection of victim objects and undermine the functionality of object detection applications. In this paper, we propose ObjectSeeker for certifiably robust object detection against patch hiding attacks. The key insight in ObjectSeeker is patch-agnostic masking: we aim to mask out the entire adversarial patch without knowing the shape, size, and location of the patch. This masking operation neutralizes the adversarial effect and allows any vanilla object detector to safely detect objects on the masked images. Remarkably, we can evaluate ObjectSeeker's robustness in a certifiable manner: we develop a certification procedure to formally determine if ObjectSeeker can detect certain objects against any white-box adaptive attack within the threat model, achieving certifiable robustness. Our experiments demonstrate a significant (~10%-40% absolute and ~2-6x relative) improvement in certifiable robustness over the prior work, as well as high clean performance (~1% drop compared with undefended models).



### SAFE-OCC: A Novelty Detection Framework for Convolutional Neural Network Sensors and its Application in Process Control
- **Arxiv ID**: http://arxiv.org/abs/2202.01816v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01816v1)
- **Published**: 2022-02-03 19:47:55+00:00
- **Updated**: 2022-02-03 19:47:55+00:00
- **Authors**: Joshua L. Pulsipher, Luke D. J. Coutinho, Tyler A. Soderstrom, Victor M. Zavala
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novelty detection framework for Convolutional Neural Network (CNN) sensors that we call Sensor-Activated Feature Extraction One-Class Classification (SAFE-OCC). We show that this framework enables the safe use of computer vision sensors in process control architectures. Emergent control applications use CNN models to map visual data to a state signal that can be interpreted by the controller. Incorporating such sensors introduces a significant system operation vulnerability because CNN sensors can exhibit high prediction errors when exposed to novel (abnormal) visual data. Unfortunately, identifying such novelties in real-time is nontrivial. To address this issue, the SAFE-OCC framework leverages the convolutional blocks of the CNN to create an effective feature space to conduct novelty detection using a desired one-class classification technique. This approach engenders a feature space that directly corresponds to that used by the CNN sensor and avoids the need to derive an independent latent space. We demonstrate the effectiveness of SAFE-OCC via simulated control environments.



### Danish Airs and Grounds: A Dataset for Aerial-to-Street-Level Place Recognition and Localization
- **Arxiv ID**: http://arxiv.org/abs/2202.01821v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.01821v1)
- **Published**: 2022-02-03 19:58:09+00:00
- **Updated**: 2022-02-03 19:58:09+00:00
- **Authors**: Andrea Vallone, Frederik Warburg, Hans Hansen, Søren Hauberg, Javier Civera
- **Comment**: Submitted to RA-L (IROS)
- **Journal**: None
- **Summary**: Place recognition and visual localization are particularly challenging in wide baseline configurations. In this paper, we contribute with the \emph{Danish Airs and Grounds} (DAG) dataset, a large collection of street-level and aerial images targeting such cases. Its main challenge lies in the extreme viewing-angle difference between query and reference images with consequent changes in illumination and perspective. The dataset is larger and more diverse than current publicly available data, including more than 50 km of road in urban, suburban and rural areas. All images are associated with accurate 6-DoF metadata that allows the benchmarking of visual localization methods.   We also propose a map-to-image re-localization pipeline, that first estimates a dense 3D reconstruction from the aerial images and then matches query street-level images to street-level renderings of the 3D model. The dataset can be downloaded at: https://frederikwarburg.github.io/DAG



### HRBF-Fusion: Accurate 3D reconstruction from RGB-D data using on-the-fly implicits
- **Arxiv ID**: http://arxiv.org/abs/2202.01829v2
- **DOI**: 10.1145/3516521
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.01829v2)
- **Published**: 2022-02-03 20:20:32+00:00
- **Updated**: 2022-02-13 18:37:26+00:00
- **Authors**: Yabin Xu, Liangliang Nan, Laishui Zhou, Jun Wang, Charlie C. L. Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstruction of high-fidelity 3D objects or scenes is a fundamental research problem. Recent advances in RGB-D fusion have demonstrated the potential of producing 3D models from consumer-level RGB-D cameras. However, due to the discrete nature and limited resolution of their surface representations (e.g., point- or voxel-based), existing approaches suffer from the accumulation of errors in camera tracking and distortion in the reconstruction, which leads to an unsatisfactory 3D reconstruction. In this paper, we present a method using on-the-fly implicits of Hermite Radial Basis Functions (HRBFs) as a continuous surface representation for camera tracking in an existing RGB-D fusion framework. Furthermore, curvature estimation and confidence evaluation are coherently derived from the inherent surface properties of the on-the-fly HRBF implicits, which devote to a data fusion with better quality. We argue that our continuous but on-the-fly surface representation can effectively mitigate the impact of noise with its robustness and constrain the reconstruction with inherent surface smoothness when being compared with discrete representations. Experimental results on various real-world and synthetic datasets demonstrate that our HRBF-fusion outperforms the state-of-the-art approaches in terms of tracking robustness and reconstruction accuracy.



### Mapping DNN Embedding Manifolds for Network Generalization Prediction
- **Arxiv ID**: http://arxiv.org/abs/2202.03868v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03868v1)
- **Published**: 2022-02-03 21:03:32+00:00
- **Updated**: 2022-02-03 21:03:32+00:00
- **Authors**: Molly O'Brien, Julia Bukowski, Mathias Unberath, Aria Pezeshk, Greg Hager
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Understanding Deep Neural Network (DNN) performance in changing conditions is essential for deploying DNNs in safety critical applications with unconstrained environments, e.g., perception for self-driving vehicles or medical image analysis. Recently, the task of Network Generalization Prediction (NGP) has been proposed to predict how a DNN will generalize in a new operating domain. Previous NGP approaches have relied on labeled metadata and known distributions for the new operating domains. In this study, we propose the first NGP approach that predicts DNN performance based solely on how unlabeled images from an external operating domain map in the DNN embedding space. We demonstrate this technique for pedestrian, melanoma, and animal classification tasks and show state of the art NGP in 13 of 15 NGP tasks without requiring domain knowledge. Additionally, we show that our NGP embedding maps can be used to identify misclassified images when the DNN performance is poor.



### Brain Cancer Survival Prediction on Treatment-na ive MRI using Deep Anchor Attention Learning with Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2202.01857v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01857v1)
- **Published**: 2022-02-03 21:33:08+00:00
- **Updated**: 2022-02-03 21:33:08+00:00
- **Authors**: Xuan Xu, Prateek Prasanna
- **Comment**: None
- **Journal**: None
- **Summary**: Image-based brain cancer prediction models, based on radiomics, quantify the radiologic phenotype from magnetic resonance imaging (MRI). However, these features are difficult to reproduce because of variability in acquisition and preprocessing pipelines. Despite evidence of intra-tumor phenotypic heterogeneity, the spatial diversity between different slices within an MRI scan has been relatively unexplored using such methods. In this work, we propose a deep anchor attention aggregation strategy with a Vision Transformer to predict survival risk for brain cancer patients. A Deep Anchor Attention Learning (DAAL) algorithm is proposed to assign different weights to slice-level representations with trainable distance measurements. We evaluated our method on N = 326 MRIs. Our results outperformed attention multiple instance learning-based techniques. DAAL highlights the importance of critical slices and corroborates the clinical intuition that inter-slice spatial diversity can reflect disease severity and is implicated in outcome.



### Best Practices and Scoring System on Reviewing A.I. based Medical Imaging Papers: Part 1 Classification
- **Arxiv ID**: http://arxiv.org/abs/2202.01863v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.01863v1)
- **Published**: 2022-02-03 21:46:59+00:00
- **Updated**: 2022-02-03 21:46:59+00:00
- **Authors**: Timothy L. Kline, Felipe Kitamura, Ian Pan, Amine M. Korchi, Neil Tenenholtz, Linda Moy, Judy Wawira Gichoya, Igor Santos, Steven Blumer, Misha Ysabel Hwang, Kim-Ann Git, Abishek Shroff, Elad Walach, George Shih, Steve Langer
- **Comment**: None
- **Journal**: None
- **Summary**: With the recent advances in A.I. methodologies and their application to medical imaging, there has been an explosion of related research programs utilizing these techniques to produce state-of-the-art classification performance. Ultimately, these research programs culminate in submission of their work for consideration in peer reviewed journals. To date, the criteria for acceptance vs. rejection is often subjective; however, reproducible science requires reproducible review. The Machine Learning Education Sub-Committee of SIIM has identified a knowledge gap and a serious need to establish guidelines for reviewing these studies. Although there have been several recent papers with this goal, this present work is written from the machine learning practitioners standpoint. In this series, the committee will address the best practices to be followed in an A.I.-based study and present the required sections in terms of examples and discussion of what should be included to make the studies cohesive, reproducible, accurate, and self-contained. This first entry in the series focuses on the task of image classification. Elements such as dataset curation, data pre-processing steps, defining an appropriate reference standard, data partitioning, model architecture and training are discussed. The sections are presented as they would be detailed in a typical manuscript, with content describing the necessary information that should be included to make sure the study is of sufficient quality to be considered for publication. The goal of this series is to provide resources to not only help improve the review process for A.I.-based medical imaging papers, but to facilitate a standard for the information that is presented within all components of the research study. We hope to provide quantitative metrics in what otherwise may be a qualitative review process.



### Enhancing Organ at Risk Segmentation with Improved Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.01866v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01866v1)
- **Published**: 2022-02-03 21:55:16+00:00
- **Updated**: 2022-02-03 21:55:16+00:00
- **Authors**: Ilkin Isler, Curtis Lisle, Justin Rineer, Patrick Kelly, Damla Turgut, Jacob Ricci, Ulas Bagci
- **Comment**: 7 pages, 3 figures, 6 tables, The paper is published in SPIE Medical
  Imaging 2022
- **Journal**: None
- **Summary**: Organ at risk (OAR) segmentation is a crucial step for treatment planning and outcome determination in radiotherapy treatments of cancer patients. Several deep learning based segmentation algorithms have been developed in recent years, however, U-Net remains the de facto algorithm designed specifically for biomedical image segmentation and has spawned many variants with known weaknesses. In this study, our goal is to present simple architectural changes in U-Net to improve its accuracy and generalization properties. Unlike many other available studies evaluating their algorithms on single center data, we thoroughly evaluate several variations of U-Net as well as our proposed enhanced architecture on multiple data sets for an extensive and reliable study of the OAR segmentation problem. Our enhanced segmentation model includes (a)architectural changes in the loss function, (b)optimization framework, and (c)convolution type. Testing on three publicly available multi-object segmentation data sets, we achieved an average of 80% dice score compared to the baseline U-Net performance of 63%.



### Modified ResNet Model for MSI and MSS Classification of Gastrointestinal Cancer
- **Arxiv ID**: http://arxiv.org/abs/2202.01905v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01905v1)
- **Published**: 2022-02-03 23:16:46+00:00
- **Updated**: 2022-02-03 23:16:46+00:00
- **Authors**: CH Sai Venkatesh, Caleb Meriga, M. G. V. L Geethika, T Lakshmi Gayatri, V. B. K. L Aruna
- **Comment**: 10 pages, 7 figures, 2 tables, Springer Nature
- **Journal**: None
- **Summary**: In this work, a modified ResNet model is proposed for the classification of Microsatellite instability(MSI) and Microsatellite stability(MSS) of gastrointestinal cancer. The performance of this model is analyzed and compared with existing models. The proposed model surpassed the existing models with an accuracy of 0.8981 and F1 score of 0.9178.



