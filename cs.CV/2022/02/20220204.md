# Arxiv Papers in cs.CV on 2022-02-04
### Projection-based Point Convolution for Efficient Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.01991v1
- **DOI**: 10.1109/ACCESS.2022.3144449
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.01991v1)
- **Published**: 2022-02-04 06:22:33+00:00
- **Updated**: 2022-02-04 06:22:33+00:00
- **Authors**: Pyunghwan Ahn, Juyoung Yang, Eojindl Yi, Chanho Lee, Junmo Kim
- **Comment**: Published in IEEE Access (Early Access)
- **Journal**: None
- **Summary**: Understanding point cloud has recently gained huge interests following the development of 3D scanning devices and the accumulation of large-scale 3D data. Most point cloud processing algorithms can be classified as either point-based or voxel-based methods, both of which have severe limitations in processing time or memory, or both. To overcome these limitations, we propose Projection-based Point Convolution (PPConv), a point convolutional module that uses 2D convolutions and multi-layer perceptrons (MLPs) as its components. In PPConv, point features are processed through two branches: point branch and projection branch. Point branch consists of MLPs, while projection branch transforms point features into a 2D feature map and then apply 2D convolutions. As PPConv does not use point-based or voxel-based convolutions, it has advantages in fast point cloud processing. When combined with a learnable projection and effective feature fusion strategy, PPConv achieves superior efficiency compared to state-of-the-art methods, even with a simple architecture based on PointNet++. We demonstrate the efficiency of PPConv in terms of the trade-off between inference time and segmentation performance. The experimental results on S3DIS and ShapeNetPart show that PPConv is the most efficient method among the compared ones. The code is available at github.com/pahn04/PPConv.



### Grounding Answers for Visual Questions Asked by Visually Impaired People
- **Arxiv ID**: http://arxiv.org/abs/2202.01993v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2202.01993v3)
- **Published**: 2022-02-04 06:47:16+00:00
- **Updated**: 2022-04-08 21:57:30+00:00
- **Authors**: Chongyan Chen, Samreen Anjum, Danna Gurari
- **Comment**: Computer Vision and Pattern Recognition
- **Journal**: None
- **Summary**: Visual question answering is the task of answering questions about images. We introduce the VizWiz-VQA-Grounding dataset, the first dataset that visually grounds answers to visual questions asked by people with visual impairments. We analyze our dataset and compare it with five VQA-Grounding datasets to demonstrate what makes it similar and different. We then evaluate the SOTA VQA and VQA-Grounding models and demonstrate that current SOTA algorithms often fail to identify the correct visual evidence where the answer is located. These models regularly struggle when the visual evidence occupies a small fraction of the image, for images that are higher quality, as well as for visual questions that require skills in text recognition. The dataset, evaluation server, and leaderboard all can be found at the following link: https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.



### Neural Dual Contouring
- **Arxiv ID**: http://arxiv.org/abs/2202.01999v3
- **DOI**: 10.1145/3528223.3530108
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.01999v3)
- **Published**: 2022-02-04 07:03:39+00:00
- **Updated**: 2022-05-31 23:24:39+00:00
- **Authors**: Zhiqin Chen, Andrea Tagliasacchi, Thomas Funkhouser, Hao Zhang
- **Comment**: Accepted to SIGGRAPH (journal) 2022. Code:
  https://github.com/czq142857/NDC
- **Journal**: Neural Dual Contouring. ACM Trans. Graph. 41, 4, Article 104 (July
  2022), 13 pages
- **Summary**: We introduce neural dual contouring (NDC), a new data-driven approach to mesh reconstruction based on dual contouring (DC). Like traditional DC, it produces exactly one vertex per grid cell and one quad for each grid edge intersection, a natural and efficient structure for reproducing sharp features. However, rather than computing vertex locations and edge crossings with hand-crafted functions that depend directly on difficult-to-obtain surface gradients, NDC uses a neural network to predict them. As a result, NDC can be trained to produce meshes from signed or unsigned distance fields, binary voxel grids, or point clouds (with or without normals); and it can produce open surfaces in cases where the input represents a sheet or partial surface. During experiments with five prominent datasets, we find that NDC, when trained on one of the datasets, generalizes well to the others. Furthermore, NDC provides better surface reconstruction accuracy, feature preservation, output complexity, triangle quality, and inference time in comparison to previous learned (e.g., neural marching cubes, convolutional occupancy networks) and traditional (e.g., Poisson) methods. Code and data are available at https://github.com/czq142857/NDC.



### Cross-Modality Multi-Atlas Segmentation via Deep Registration and Label Fusion
- **Arxiv ID**: http://arxiv.org/abs/2202.02000v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02000v3)
- **Published**: 2022-02-04 07:10:00+00:00
- **Updated**: 2022-03-28 11:18:05+00:00
- **Authors**: Wangbin Ding, Lei Li, Xiahai Zhuang, Liqin Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-atlas segmentation (MAS) is a promising framework for medical image segmentation. Generally, MAS methods register multiple atlases, i.e., medical images with corresponding labels, to a target image; and the transformed atlas labels can be combined to generate target segmentation via label fusion schemes. Many conventional MAS methods employed the atlases from the same modality as the target image. However, the number of atlases with the same modality may be limited or even missing in many clinical applications. Besides, conventional MAS methods suffer from the computational burden of registration or label fusion procedures. In this work, we design a novel cross-modality MAS framework, which uses available atlases from a certain modality to segment a target image from another modality. To boost the computational efficiency of the framework, both the image registration and label fusion are achieved by well-designed deep neural networks. For the atlas-to-target image registration, we propose a bi-directional registration network (BiRegNet), which can efficiently align images from different modalities. For the label fusion, we design a similarity estimation network (SimNet), which estimates the fusion weight of each atlas by measuring its similarity to the target image. SimNet can learn multi-scale information for similarity estimation to improve the performance of label fusion. The proposed framework was evaluated by the left ventricle and liver segmentation tasks on the MM-WHS and CHAOS datasets, respectively. Results have shown that the framework is effective for cross-modality MAS in both registration and label fusion.



### The devil is in the labels: Semantic segmentation from sentences
- **Arxiv ID**: http://arxiv.org/abs/2202.02002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02002v1)
- **Published**: 2022-02-04 07:19:09+00:00
- **Updated**: 2022-02-04 07:19:09+00:00
- **Authors**: Wei Yin, Yifan Liu, Chunhua Shen, Anton van den Hengel, Baichuan Sun
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: We propose an approach to semantic segmentation that achieves state-of-the-art supervised performance when applied in a zero-shot setting. It thus achieves results equivalent to those of the supervised methods, on each of the major semantic segmentation datasets, without training on those datasets. This is achieved by replacing each class label with a vector-valued embedding of a short paragraph that describes the class. The generality and simplicity of this approach enables merging multiple datasets from different domains, each with varying class labels and semantics. The resulting merged semantic segmentation dataset of over 2 Million images enables training a model that achieves performance equal to that of state-of-the-art supervised methods on 7 benchmark datasets, despite not using any images therefrom. By fine-tuning the model on standard semantic segmentation datasets, we also achieve a significant improvement over the state-of-the-art supervised segmentation on NYUD-V2 and PASCAL-context at 60% and 65% mIoU, respectively. Based on the closeness of language embeddings, our method can even segment unseen labels. Extensive experiments demonstrate strong generalization to unseen image domains and unseen labels, and that the method enables impressive performance improvements in downstream applications, including depth estimation and instance segmentation.



### TransFollower: Long-Sequence Car-Following Trajectory Prediction through Transformer
- **Arxiv ID**: http://arxiv.org/abs/2202.03183v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03183v1)
- **Published**: 2022-02-04 07:59:22+00:00
- **Updated**: 2022-02-04 07:59:22+00:00
- **Authors**: Meixin Zhu, Simon S. Du, Xuesong Wang, Hao, Yang, Ziyuan Pu, Yinhai Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Car-following refers to a control process in which the following vehicle (FV) tries to keep a safe distance between itself and the lead vehicle (LV) by adjusting its acceleration in response to the actions of the vehicle ahead. The corresponding car-following models, which describe how one vehicle follows another vehicle in the traffic flow, form the cornerstone for microscopic traffic simulation and intelligent vehicle development. One major motivation of car-following models is to replicate human drivers' longitudinal driving trajectories. To model the long-term dependency of future actions on historical driving situations, we developed a long-sequence car-following trajectory prediction model based on the attention-based Transformer model. The model follows a general format of encoder-decoder architecture. The encoder takes historical speed and spacing data as inputs and forms a mixed representation of historical driving context using multi-head self-attention. The decoder takes the future LV speed profile as input and outputs the predicted future FV speed profile in a generative way (instead of an auto-regressive way, avoiding compounding errors). Through cross-attention between encoder and decoder, the decoder learns to build a connection between historical driving and future LV speed, based on which a prediction of future FV speed can be obtained. We train and test our model with 112,597 real-world car-following events extracted from the Shanghai Naturalistic Driving Study (SH-NDS). Results show that the model outperforms the traditional intelligent driver model (IDM), a fully connected neural network model, and a long short-term memory (LSTM) based model in terms of long-sequence trajectory prediction accuracy. We also visualized the self-attention and cross-attention heatmaps to explain how the model derives its predictions.



### Image-to-Image MLP-mixer for Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2202.02018v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02018v1)
- **Published**: 2022-02-04 08:36:34+00:00
- **Updated**: 2022-02-04 08:36:34+00:00
- **Authors**: Youssef Mansour, Kang Lin, Reinhard Heckel
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks are highly effective tools for image reconstruction problems such as denoising and compressive sensing. To date, neural networks for image reconstruction are almost exclusively convolutional. The most popular architecture is the U-Net, a convolutional network with a multi-resolution architecture. In this work, we show that a simple network based on the multi-layer perceptron (MLP)-mixer enables state-of-the art image reconstruction performance without convolutions and without a multi-resolution architecture, provided that the training set and the size of the network are moderately large. Similar to the original MLP-mixer, the image-to-image MLP-mixer is based exclusively on MLPs operating on linearly-transformed image patches. Contrary to the original MLP-mixer, we incorporate structure by retaining the relative positions of the image patches. This imposes an inductive bias towards natural images which enables the image-to-image MLP-mixer to learn to denoise images based on fewer examples than the original MLP-mixer. Moreover, the image-to-image MLP-mixer requires fewer parameters to achieve the same denoising performance than the U-Net and its parameters scale linearly in the image resolution instead of quadratically as for the original MLP-mixer. If trained on a moderate amount of examples for denoising, the image-to-image MLP-mixer outperforms the U-Net by a slight margin. It also outperforms the vision transformer tailored for image reconstruction and classical un-trained methods such as BM3D, making it a very effective tool for image reconstruction problems.



### Color Image Inpainting via Robust Pure Quaternion Matrix Completion: Error Bound and Weighted Loss
- **Arxiv ID**: http://arxiv.org/abs/2202.02063v3
- **DOI**: 10.1137/22M1476897
- **Categories**: **cs.CV**, math.ST, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/2202.02063v3)
- **Published**: 2022-02-04 10:31:43+00:00
- **Updated**: 2022-10-26 17:19:57+00:00
- **Authors**: Junren Chen, Michael K. Ng
- **Comment**: None
- **Journal**: SIAM Journal on Imaging Sciences, 15(2022), pp. 1469-1498
- **Summary**: In this paper, we study color image inpainting as a pure quaternion matrix completion problem. In the literature, the theoretical guarantee for quaternion matrix completion is not well-established. Our main aim is to propose a new minimization problem with an objective combining nuclear norm and a quadratic loss weighted among three channels. To fill the theoretical vacancy, we obtain the error bound in both clean and corrupted regimes, which relies on some new results of quaternion matrices. A general Gaussian noise is considered in robust completion where all observations are corrupted. Motivated by the error bound, we propose to handle unbalanced or correlated noise via a cross-channel weight in the quadratic loss, with the main purpose of rebalancing noise level, or removing noise correlation. Extensive experimental results on synthetic and color image data are presented to confirm and demonstrate our theoretical findings.



### CGiS-Net: Aggregating Colour, Geometry and Implicit Semantic Features for Indoor Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.02070v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02070v2)
- **Published**: 2022-02-04 10:51:25+00:00
- **Updated**: 2022-07-11 22:06:40+00:00
- **Authors**: Yuhang Ming, Xingrui Yang, Guofeng Zhang, Andrew Calway
- **Comment**: Accepted by 2022 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2022)
- **Journal**: None
- **Summary**: We describe a novel approach to indoor place recognition from RGB point clouds based on aggregating low-level colour and geometry features with high-level implicit semantic features. It uses a 2-stage deep learning framework, in which the first stage is trained for the auxiliary task of semantic segmentation and the second stage uses features from layers in the first stage to generate discriminate descriptors for place recognition. The auxiliary task encourages the features to be semantically meaningful, hence aggregating the geometry and colour in the RGB point cloud data with implicit semantic information. We use an indoor place recognition dataset derived from the ScanNet dataset for training and evaluation, with a test set comprising 3,608 point clouds generated from 100 different rooms. Comparison with a traditional feature-based method and four state-of-the-art deep learning methods demonstrate that our approach significantly outperforms all five methods, achieving, for example, a top-3 average recall rate of 75% compared with 41% for the closest rival method. Our code is available at: https://github.com/YuhangMing/Semantic-Indoor-Place-Recognition



### Heed the Noise in Performance Evaluations in Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2202.02078v2
- **DOI**: 10.1145/3520304.3533995
- **Categories**: **cs.NE**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02078v2)
- **Published**: 2022-02-04 11:20:46+00:00
- **Updated**: 2022-05-02 12:49:19+00:00
- **Authors**: Arkadiy Dushatskiy, Tanja Alderliesten, Peter A. N. Bosman
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has recently become a topic of great interest. However, there is a potentially impactful issue within NAS that remains largely unrecognized: noise. Due to stochastic factors in neural network initialization, training, and the chosen train/validation dataset split, the performance evaluation of a neural network architecture, which is often based on a single learning run, is also stochastic. This may have a particularly large impact if a dataset is small. We therefore propose to reduce this noise by evaluating architectures based on average performance over multiple network training runs using different random seeds and cross-validation. We perform experiments for a combinatorial optimization formulation of NAS in which we vary noise reduction levels. We use the same computational budget for each noise level in terms of network training runs, i.e., we allow less architecture evaluations when averaging over more training runs. Multiple search algorithms are considered, including evolutionary algorithms which generally perform well for NAS. We use two publicly available datasets from the medical image segmentation domain where datasets are often limited and variability among samples is often high. Our results show that reducing noise in architecture evaluations enables finding better architectures by all considered search algorithms.



### Structured Prediction Problem Archive
- **Arxiv ID**: http://arxiv.org/abs/2202.03574v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.03574v3)
- **Published**: 2022-02-04 12:30:49+00:00
- **Updated**: 2023-02-10 10:12:51+00:00
- **Authors**: Paul Swoboda, Andrea Hornakova, Paul Roetzer, Bogdan Savchynskyy, Ahmed Abbas
- **Comment**: Added complete factorized multicat section
- **Journal**: None
- **Summary**: Structured prediction problems are one of the fundamental tools in machine learning. In order to facilitate algorithm development for their numerical solution, we collect in one place a large number of datasets in easy to read formats for a diverse set of problem classes. We provide archival links to datasets, description of the considered problems and problem formats, and a short summary of problem characteristics including size, number of instances etc. For reference we also give a non-exhaustive selection of algorithms proposed in the literature for their solution. We hope that this central repository will make benchmarking and comparison to established works easier. We welcome submission of interesting new datasets and algorithms for inclusion in our archive.



### Edge-Selective Feature Weaving for Point Cloud Matching
- **Arxiv ID**: http://arxiv.org/abs/2202.02149v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02149v3)
- **Published**: 2022-02-04 14:27:09+00:00
- **Updated**: 2022-03-08 05:09:55+00:00
- **Authors**: Rintaro Yanagi, Atsushi Hashimoto, Shusaku Sone, Naoya Chiba, Jiaxin Ma, Yoshitaka Ushiku
- **Comment**: None
- **Journal**: None
- **Summary**: This paper tackles the problem of accurately matching the points of two 3D point clouds. Most conventional methods improve their performance by extracting representative features from each point via deep-learning-based algorithms. On the other hand, the correspondence calculation between the extracted features has not been examined in depth, and non-trainable algorithms (e.g. the Sinkhorn algorithm) are frequently applied. As a result, the extracted features may be forcibly fitted to a non-trainable algorithm. Furthermore, the extracted features frequently contain stochastically unavoidable errors, which degrades the matching accuracy. In this paper, instead of using a non-trainable algorithm, we propose a differentiable matching network that can be jointly optimized with the feature extraction procedure. Our network first constructs graphs with edges connecting the points of each point cloud and then extracts discriminative edge features by using two main components: a shared set-encoder and an edge-selective cross-concatenation. These components enable us to symmetrically consider two point clouds and to extract discriminative edge features, respectively. By using the extracted discriminative edge features, our network can accurately calculate the correspondence between points. Our experimental results show that the proposed network can significantly improve the performance of point cloud matching. Our code is available at https://github.com/yanarin/ESFW



### NeAT: Neural Adaptive Tomography
- **Arxiv ID**: http://arxiv.org/abs/2202.02171v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02171v1)
- **Published**: 2022-02-04 14:59:48+00:00
- **Updated**: 2022-02-04 14:59:48+00:00
- **Authors**: Darius Rückert, Yuanhao Wang, Rui Li, Ramzi Idoughi, Wolfgang Heidrich
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present Neural Adaptive Tomography (NeAT), the first adaptive, hierarchical neural rendering pipeline for multi-view inverse rendering. Through a combination of neural features with an adaptive explicit representation, we achieve reconstruction times far superior to existing neural inverse rendering methods. The adaptive explicit representation improves efficiency by facilitating empty space culling and concentrating samples in complex regions, while the neural features act as a neural regularizer for the 3D reconstruction. The NeAT framework is designed specifically for the tomographic setting, which consists only of semi-transparent volumetric scenes instead of opaque objects. In this setting, NeAT outperforms the quality of existing optimization-based tomography solvers while being substantially faster.



### Feature-Style Encoder for Style-Based GAN Inversion
- **Arxiv ID**: http://arxiv.org/abs/2202.02183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02183v1)
- **Published**: 2022-02-04 15:19:34+00:00
- **Updated**: 2022-02-04 15:19:34+00:00
- **Authors**: Xu Yao, Alasdair Newson, Yann Gousseau, Pierre Hellier
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel architecture for GAN inversion, which we call Feature-Style encoder. The style encoder is key for the manipulation of the obtained latent codes, while the feature encoder is crucial for optimal image reconstruction. Our model achieves accurate inversion of real images from the latent space of a pre-trained style-based GAN model, obtaining better perceptual quality and lower reconstruction error than existing methods. Thanks to its encoder structure, the model allows fast and accurate image editing. Additionally, we demonstrate that the proposed encoder is especially well-suited for inversion and editing on videos. We conduct extensive experiments for several style-based generators pre-trained on different data domains. Our proposed method yields state-of-the-art results for style-based GAN inversion, significantly outperforming competing approaches. Source codes are available at https://github.com/InterDigitalInc/FeatureStyleEncoder .



### Learning with Neighbor Consistency for Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2202.02200v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02200v2)
- **Published**: 2022-02-04 15:46:27+00:00
- **Updated**: 2022-07-06 11:14:27+00:00
- **Authors**: Ahmet Iscen, Jack Valmadre, Anurag Arnab, Cordelia Schmid
- **Comment**: The code is available at
  https://github.com/google-research/scenic/tree/main/scenic/projects/ncr
- **Journal**: None
- **Summary**: Recent advances in deep learning have relied on large, labelled datasets to train high-capacity models. However, collecting large datasets in a time- and cost-efficient manner often results in label noise. We present a method for learning from noisy labels that leverages similarities between training examples in feature space, encouraging the prediction of each example to be similar to its nearest neighbours. Compared to training algorithms that use multiple models or distinct stages, our approach takes the form of a simple, additional regularization term. It can be interpreted as an inductive version of the classical, transductive label propagation algorithm. We thoroughly evaluate our method on datasets evaluating both synthetic (CIFAR-10, CIFAR-100) and realistic (mini-WebVision, WebVision, Clothing1M, mini-ImageNet-Red) noise, and achieve competitive or state-of-the-art accuracies across all of them.



### Video Violence Recognition and Localization Using a Semi-Supervised Hard Attention Model
- **Arxiv ID**: http://arxiv.org/abs/2202.02212v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02212v4)
- **Published**: 2022-02-04 16:15:26+00:00
- **Updated**: 2022-09-05 20:09:05+00:00
- **Authors**: Hamid Mohammadi, Ehsan Nazerfard
- **Comment**: None
- **Journal**: None
- **Summary**: The significant growth of surveillance camera networks necessitates scalable AI solutions to efficiently analyze the large amount of video data produced by these networks. As a typical analysis performed on surveillance footage, video violence detection has recently received considerable attention. The majority of research has focused on improving existing methods using supervised methods, with little, if any, attention to the semi-supervised learning approaches. In this study, a reinforcement learning model is introduced that can outperform existing models through a semi-supervised approach. The main novelty of the proposed method lies in the introduction of a semi-supervised hard attention mechanism. Using hard attention, the essential regions of videos are identified and separated from the non-informative parts of the data. A model's accuracy is improved by removing redundant data and focusing on useful visual information in a higher resolution. Implementing hard attention mechanisms using semi-supervised reinforcement learning algorithms eliminates the need for attention annotations in video violence datasets, thus making them readily applicable. The proposed model utilizes a pre-trained I3D backbone to accelerate and stabilize the training process. The proposed model achieved state-of-the-art accuracy of 90.4% and 98.7% on RWF and Hockey datasets, respectively.



### Bootstrapped Representation Learning for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.02232v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02232v2)
- **Published**: 2022-02-04 16:58:06+00:00
- **Updated**: 2022-04-19 09:20:14+00:00
- **Authors**: Olivier Moliner, Sangxia Huang, Kalle Åström
- **Comment**: Accepted: 2022 IEEE CVPR Workshop on Learning with Limited Labelled
  Data for Image and Video Understanding (L3D-IVU)
- **Journal**: None
- **Summary**: In this work, we study self-supervised representation learning for 3D skeleton-based action recognition. We extend Bootstrap Your Own Latent (BYOL) for representation learning on skeleton sequence data and propose a new data augmentation strategy including two asymmetric transformation pipelines. We also introduce a multi-viewpoint sampling method that leverages multiple viewing angles of the same action captured by different cameras. In the semi-supervised setting, we show that the performance can be further improved by knowledge distillation from wider networks, leveraging once more the unlabeled samples. We conduct extensive experiments on the NTU-60 and NTU-120 datasets to demonstrate the performance of our proposed method. Our method consistently outperforms the current state of the art on both linear evaluation and semi-supervised benchmarks.



### Personalized visual encoding model construction with small data
- **Arxiv ID**: http://arxiv.org/abs/2202.02245v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02245v2)
- **Published**: 2022-02-04 17:24:50+00:00
- **Updated**: 2022-05-15 02:52:47+00:00
- **Authors**: Zijin Gu, Keith Jamison, Mert Sabuncu, Amy Kuceyeski
- **Comment**: None
- **Journal**: None
- **Summary**: Encoding models that predict brain response patterns to stimuli are one way to capture this relationship between variability in bottom-up neural systems and individual's behavior or pathological state. However, they generally need a large amount of training data to achieve optimal accuracy. Here, we propose and test an alternative personalized ensemble encoding model approach to utilize existing encoding models, to create encoding models for novel individuals with relatively little stimuli-response data. We show that these personalized ensemble encoding models trained with small amounts of data for a specific individual, i.e. ~300 image-response pairs, achieve accuracy not different from models trained on ~20,000 image-response pairs for the same individual. Importantly, the personalized ensemble encoding models preserve patterns of inter-individual variability in the image-response relationship. Additionally, we show the proposed approach is robust against domain shift by validating on a prospectively collected set of image-response data in novel individuals with a different scanner and experimental setup. Finally, we use our personalized ensemble encoding model within the recently developed NeuroGen framework to generate optimal stimuli designed to maximize specific regions' activations for a specific individual. We show that the inter-individual differences in face areas responses to images of animal vs human faces observed previously is replicated using NeuroGen with the ensemble encoding model. Our approach shows the potential to use previously collected, deeply sampled data to efficiently create accurate, personalized encoding models and, subsequently, personalized optimal synthetic images for new individuals scanned under different experimental conditions.



### Iterative Self Knowledge Distillation -- From Pothole Classification to Fine-Grained and COVID Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.02265v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02265v1)
- **Published**: 2022-02-04 17:47:43+00:00
- **Updated**: 2022-02-04 17:47:43+00:00
- **Authors**: Kuan-Chuan Peng
- **Comment**: ICASSP 2022
- **Journal**: None
- **Summary**: Pothole classification has become an important task for road inspection vehicles to save drivers from potential car accidents and repair bills. Given the limited computational power and fixed number of training epochs, we propose iterative self knowledge distillation (ISKD) to train lightweight pothole classifiers. Designed to improve both the teacher and student models over time in knowledge distillation, ISKD outperforms the state-of-the-art self knowledge distillation method on three pothole classification datasets across four lightweight network architectures, which supports that self knowledge distillation should be done iteratively instead of just once. The accuracy relation between the teacher and student models shows that the student model can still benefit from a moderately trained teacher model. Implying that better teacher models generally produce better student models, our results justify the design of ISKD. In addition to pothole classification, we also demonstrate the efficacy of ISKD on six additional datasets associated with generic classification, fine-grained classification, and medical imaging application, which supports that ISKD can serve as a general-purpose performance booster without the need of a given teacher model and extra trainable parameters.



### Quality Assessment of Low Light Restored Images: A Subjective Study and an Unsupervised Model
- **Arxiv ID**: http://arxiv.org/abs/2202.02277v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2202.02277v1)
- **Published**: 2022-02-04 18:06:07+00:00
- **Updated**: 2022-02-04 18:06:07+00:00
- **Authors**: Vignesh Kannan, Sameer Malik, Rajiv Soundararajan
- **Comment**: None
- **Journal**: None
- **Summary**: The quality assessment (QA) of restored low light images is an important tool for benchmarking and improving low light restoration (LLR) algorithms. While several LLR algorithms exist, the subjective perception of the restored images has been much less studied. Challenges in capturing aligned low light and well-lit image pairs and collecting a large number of human opinion scores of quality for training, warrant the design of unsupervised (or opinion unaware) no-reference (NR) QA methods. This work studies the subjective perception of low light restored images and their unsupervised NR QA. Our contributions are two-fold. We first create a dataset of restored low light images using various LLR methods, conduct a subjective QA study and benchmark the performance of existing QA methods. We then present a self-supervised contrastive learning technique to extract distortion aware features from the restored low light images. We show that these features can be effectively used to build an opinion unaware image quality analyzer. Detailed experiments reveal that our unsupervised NR QA model achieves state-of-the-art performance among all such quality measures for low light restored images.



### Multi-task head pose estimation in-the-wild
- **Arxiv ID**: http://arxiv.org/abs/2202.02299v1
- **DOI**: 10.1109/TPAMI.2020.3046323
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02299v1)
- **Published**: 2022-02-04 18:35:52+00:00
- **Updated**: 2022-02-04 18:35:52+00:00
- **Authors**: Roberto Valle, José Miguel Buenaposada, Luis Baumela
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  2021
- **Summary**: We present a deep learning-based multi-task approach for head pose estimation in images. We contribute with a network architecture and training strategy that harness the strong dependencies among face pose, alignment and visibility, to produce a top performing model for all three tasks. Our architecture is an encoder-decoder CNN with residual blocks and lateral skip connections. We show that the combination of head pose estimation and landmark-based face alignment significantly improve the performance of the former task. Further, the location of the pose task at the bottleneck layer, at the end of the encoder, and that of tasks depending on spatial information, such as visibility and alignment, in the final decoder layer, also contribute to increase the final performance. In the experiments conducted the proposed model outperforms the state-of-the-art in the face pose and visibility tasks. By including a final landmark regression step it also produces face alignment results on par with the state-of-the-art.



### A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility
- **Arxiv ID**: http://arxiv.org/abs/2202.02312v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2202.02312v3)
- **Published**: 2022-02-04 18:51:50+00:00
- **Updated**: 2022-08-15 00:24:24+00:00
- **Authors**: Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, Bryan A. Plummer
- **Comment**: Accepted at the European Conference on Computer Vision (ECCV) 2022.
  This is a new version of the paper with additional experimental results and a
  few prior implementation bugs fixed
- **Journal**: None
- **Summary**: Vision-language navigation (VLN), in which an agent follows language instruction in a visual environment, has been studied under the premise that the input command is fully feasible in the environment. Yet in practice, a request may not be possible due to language ambiguity or environment changes. To study VLN with unknown command feasibility, we introduce a new dataset Mobile app Tasks with Iterative Feedback (MoTIF), where the goal is to complete a natural language command in a mobile app. Mobile apps provide a scalable domain to study real downstream uses of VLN methods. Moreover, mobile app commands provide instruction for interactive navigation, as they result in action sequences with state changes via clicking, typing, or swiping. MoTIF is the first to include feasibility annotations, containing both binary feasibility labels and fine-grained labels for why tasks are unsatisfiable. We further collect follow-up questions for ambiguous queries to enable research on task uncertainty resolution. Equipped with our dataset, we propose the new problem of feasibility prediction, in which a natural language instruction and multimodal app environment are used to predict command feasibility. MoTIF provides a more realistic app dataset as it contains many diverse environments, high-level goals, and longer action sequences than prior work. We evaluate interactive VLN methods using MoTIF, quantify the generalization ability of current approaches to new app environments, and measure the effect of task feasibility on navigation performance.



### Towards To-a-T Spatio-Temporal Focus for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.02314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02314v1)
- **Published**: 2022-02-04 18:52:29+00:00
- **Updated**: 2022-02-04 18:52:29+00:00
- **Authors**: Lipeng Ke, Kuan-Chuan Peng, Siwei Lyu
- **Comment**: AAAI 2022
- **Journal**: None
- **Summary**: Graph Convolutional Networks (GCNs) have been widely used to model the high-order dynamic dependencies for skeleton-based action recognition. Most existing approaches do not explicitly embed the high-order spatio-temporal importance to joints' spatial connection topology and intensity, and they do not have direct objectives on their attention module to jointly learn when and where to focus on in the action sequence. To address these problems, we propose the To-a-T Spatio-Temporal Focus (STF), a skeleton-based action recognition framework that utilizes the spatio-temporal gradient to focus on relevant spatio-temporal features. We first propose the STF modules with learnable gradient-enforced and instance-dependent adjacency matrices to model the high-order spatio-temporal dynamics. Second, we propose three loss terms defined on the gradient-based spatio-temporal focus to explicitly guide the classifier when and where to look at, distinguish confusing classes, and optimize the stacked STF modules. STF outperforms the state-of-the-art methods on the NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets in all 15 settings over different views, subjects, setups, and input modalities, and STF also shows better accuracy on scarce data and dataset shifting settings.



### Webly Supervised Concept Expansion for General Purpose Vision Models
- **Arxiv ID**: http://arxiv.org/abs/2202.02317v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2202.02317v2)
- **Published**: 2022-02-04 18:58:36+00:00
- **Updated**: 2022-07-21 00:45:49+00:00
- **Authors**: Amita Kamath, Christopher Clark, Tanmay Gupta, Eric Kolve, Derek Hoiem, Aniruddha Kembhavi
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: General Purpose Vision (GPV) systems are models that are designed to solve a wide array of visual tasks without requiring architectural changes. Today, GPVs primarily learn both skills and concepts from large fully supervised datasets. Scaling GPVs to tens of thousands of concepts by acquiring data to learn each concept for every skill quickly becomes prohibitive. This work presents an effective and inexpensive alternative: learn skills from supervised datasets, learn concepts from web image search, and leverage a key characteristic of GPVs: the ability to transfer visual knowledge across skills. We use a dataset of 1M+ images spanning 10k+ visual concepts to demonstrate webly-supervised concept expansion for two existing GPVs (GPV-1 and VL-T5) on 3 benchmarks: 5 COCO-based datasets (80 primary concepts), a newly curated series of 5 datasets based on the OpenImages and VisualGenome repositories (~500 concepts), and the Web-derived dataset (10k+ concepts). We also propose a new architecture, GPV-2 that supports a variety of tasks -- from vision tasks like classification and localization to vision+language tasks like QA and captioning, to more niche ones like human-object interaction detection. GPV-2 benefits hugely from web data and outperforms GPV-1 and VL-T5 across these benchmarks. Our data, code, and web demo are available at https://prior.allenai.org/projects/gpv2.



### Boundary-aware Information Maximization for Self-supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.02371v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02371v2)
- **Published**: 2022-02-04 20:18:00+00:00
- **Updated**: 2022-02-16 17:02:24+00:00
- **Authors**: Jizong Peng, Ping Wang, Marco Pedersoli, Christian Desrosiers
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised pre-training has been proven as an effective approach to boost various downstream tasks given limited labeled data. Among various methods, contrastive learning learns a discriminative representation by constructing positive and negative pairs. However, it is not trivial to build reasonable pairs for a segmentation task in an unsupervised way. In this work, we propose a novel unsupervised pre-training framework that avoids the drawback of contrastive learning. Our framework consists of two principles: unsupervised over-segmentation as a pre-train task using mutual information maximization and boundary-aware preserving learning. Experimental results on two benchmark medical segmentation datasets reveal our method's effectiveness in improving segmentation performance when few annotated images are available.



### Fully Automated Tree Topology Estimation and Artery-Vein Classification
- **Arxiv ID**: http://arxiv.org/abs/2202.02382v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02382v2)
- **Published**: 2022-02-04 20:40:01+00:00
- **Updated**: 2022-08-08 17:11:37+00:00
- **Authors**: Aashis Khanal, Saeid Motevali, Rolando Estrada
- **Comment**: None
- **Journal**: None
- **Summary**: We present a fully automatic, graph-based technique for extracting the retinal vascular topology -- that is, how different vessels are connected to each other -- given a single color fundus image. Determining this connectivity is very challenging because vessels cross each other in a 2D image, obscuring their true paths. We quantitatively validated the usefulness of our extraction method by using it to achieve comparable state-of-the-art results in retinal artery-vein classification. Our proposed approach works as follows: We first segment the retinal vessels using our previously developed state-of-the-art segmentation method. Then, we estimate an initial graph from the extracted vessels and assign the most likely blood flow to each edge. We then use a handful of high-level operations (HLOs) to fix errors in the graph. These HLOs include detaching neighboring nodes, shifting the endpoints of an edge, and reversing the estimated blood flow direction for a branch. We use a novel cost function to find the optimal set of HLO operations for a given graph. Finally, we show that our extracted vascular structure is correct by propagating artery/vein labels along the branches. As our experiments show, our topology-based artery-vein labeling achieved state-of-the-art results on three datasets: DRIVE, AV-WIDE, and INSPIRE. We also performed several ablation studies to separately verify the importance of the segmentation and AV labeling steps of our proposed method. These ablation studies further confirmed that our graph extraction pipeline correctly models the underlying vascular anatomy.



### StandardSim: A Synthetic Dataset For Retail Environments
- **Arxiv ID**: http://arxiv.org/abs/2202.02418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02418v1)
- **Published**: 2022-02-04 22:28:35+00:00
- **Updated**: 2022-02-04 22:28:35+00:00
- **Authors**: Cristina Mata, Nick Locascio, Mohammed Azeem Sheikh, Kenny Kihara, Dan Fischetti
- **Comment**: ICIAP 2022
- **Journal**: None
- **Summary**: Autonomous checkout systems rely on visual and sensory inputs to carry out fine-grained scene understanding in retail environments. Retail environments present unique challenges compared to typical indoor scenes owing to the vast number of densely packed, unique yet similar objects. The problem becomes even more difficult when only RGB input is available, especially for data-hungry tasks such as instance segmentation. To address the lack of datasets for retail, we present StandardSim, a large-scale photorealistic synthetic dataset featuring annotations for semantic segmentation, instance segmentation, depth estimation, and object detection. Our dataset provides multiple views per scene, enabling multi-view representation learning. Further, we introduce a novel task central to autonomous checkout called change detection, requiring pixel-level classification of takes, puts and shifts in objects over time. We benchmark widely-used models for segmentation and depth estimation on our dataset, show that our test set constitutes a difficult benchmark compared to current smaller-scale datasets and that our training set provides models with crucial information for autonomous checkout tasks.



### The influence of labeling techniques in classifying human manipulation movement of different speed
- **Arxiv ID**: http://arxiv.org/abs/2202.02426v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02426v1)
- **Published**: 2022-02-04 23:04:22+00:00
- **Updated**: 2022-02-04 23:04:22+00:00
- **Authors**: Sadique Adnan Siddiqui, Lisa Gutzeit, Frank Kirchner
- **Comment**: None
- **Journal**: In Proceedings of the 11th International Conference on Pattern
  Recognition Applications and Methods, 2022
- **Summary**: In this work, we investigate the influence of labeling methods on the classification of human movements on data recorded using a marker-based motion capture system. The dataset is labeled using two different approaches, one based on video data of the movements, the other based on the movement trajectories recorded using the motion capture system. The dataset is labeled using two different approaches, one based on video data of the movements, the other based on the movement trajectories recorded using the motion capture system. The data was recorded from one participant performing a stacking scenario comprising simple arm movements at three different speeds (slow, normal, fast). Machine learning algorithms that include k-Nearest Neighbor, Random Forest, Extreme Gradient Boosting classifier, Convolutional Neural networks (CNN), Long Short-Term Memory networks (LSTM), and a combination of CNN-LSTM networks are compared on their performance in recognition of these arm movements. The models were trained on actions performed on slow and normal speed movements segments and generalized on actions consisting of fast-paced human movement. It was observed that all the models trained on normal-paced data labeled using trajectories have almost 20% improvement in accuracy on test data in comparison to the models trained on data labeled using videos of the performed experiments.



### Stratification of carotid atheromatous plaque using interpretable deep learning methods on B-mode ultrasound images
- **Arxiv ID**: http://arxiv.org/abs/2202.02428v1
- **DOI**: 10.1109/EMBC46164.2021.9630402
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02428v1)
- **Published**: 2022-02-04 23:10:24+00:00
- **Updated**: 2022-02-04 23:10:24+00:00
- **Authors**: Theofanis Ganitidis, Maria Athanasiou, Kalliopi Dalakleidi, Nikos Melanitis, Spyretta Golemati, Konstantina S Nikita
- **Comment**: Accepted at 2021 43rd Annual International Conference of the IEEE
  Engineering in Medicine & Biology Society (EMBC)
- **Journal**: None
- **Summary**: Carotid atherosclerosis is the major cause of ischemic stroke resulting in significant rates of mortality and disability annually. Early diagnosis of such cases is of great importance, since it enables clinicians to apply a more effective treatment strategy. This paper introduces an interpretable classification approach of carotid ultrasound images for the risk assessment and stratification of patients with carotid atheromatous plaque. To address the highly imbalanced distribution of patients between the symptomatic and asymptomatic classes (16 vs 58, respectively), an ensemble learning scheme based on a sub-sampling approach was applied along with a two-phase, cost-sensitive strategy of learning, that uses the original and a resampled data set. Convolutional Neural Networks (CNNs) were utilized for building the primary models of the ensemble. A six-layer deep CNN was used to automatically extract features from the images, followed by a classification stage of two fully connected layers. The obtained results (Area Under the ROC Curve (AUC): 73%, sensitivity: 75%, specificity: 70%) indicate that the proposed approach achieved acceptable discrimination performance. Finally, interpretability methods were applied on the model's predictions in order to reveal insights on the model's decision process as well as to enable the identification of novel image biomarkers for the stratification of patients with carotid atheromatous plaque.Clinical Relevance-The integration of interpretability methods with deep learning strategies can facilitate the identification of novel ultrasound image biomarkers for the stratification of patients with carotid atheromatous plaque.



