# Arxiv Papers in cs.CV on 2022-02-02
### On-Sensor Binarized Fully Convolutional Neural Network with A Pixel Processor Array
- **Arxiv ID**: http://arxiv.org/abs/2202.00836v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.ET, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2202.00836v2)
- **Published**: 2022-02-02 01:18:40+00:00
- **Updated**: 2022-06-11 06:41:34+00:00
- **Authors**: Yanan Liu, Laurie Bose, Yao Lu, Piotr Dudek, Walterio Mayol-Cuevas
- **Comment**: lack of experiments for method validation
- **Journal**: None
- **Summary**: This work presents a method to implement fully convolutional neural networks (FCNs) on Pixel Processor Array (PPA) sensors, and demonstrates coarse segmentation and object localisation tasks. We design and train binarized FCN for both binary weights and activations using batchnorm, group convolution, and learnable threshold for binarization, producing networks small enough to be embedded on the focal plane of the PPA, with limited local memory resources, and using parallel elementary add/subtract, shifting, and bit operations only. We demonstrate the first implementation of an FCN on a PPA device, performing three convolution layers entirely in the pixel-level processors. We use this architecture to demonstrate inference generating heat maps for object segmentation and localisation at over 280 FPS using the SCAMP-5 PPA vision chip.



### Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks
- **Arxiv ID**: http://arxiv.org/abs/2202.00838v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2202.00838v2)
- **Published**: 2022-02-02 01:19:40+00:00
- **Updated**: 2022-02-04 00:24:45+00:00
- **Authors**: Anne Harrington, Arturo Deza
- **Comment**: Accepted to ICLR 2022 as a Spotlight
- **Journal**: None
- **Summary**: Recent work suggests that representations learned by adversarially robust networks are more human perceptually-aligned than non-robust networks via image manipulations. Despite appearing closer to human visual perception, it is unclear if the constraints in robust DNN representations match biological constraints found in human vision. Human vision seems to rely on texture-based/summary statistic representations in the periphery, which have been shown to explain phenomena such as crowding and performance on visual search tasks. To understand how adversarially robust optimizations/representations compare to human vision, we performed a psychophysics experiment using a set of metameric discrimination tasks where we evaluated how well human observers could distinguish between images synthesized to match adversarially robust representations compared to non-robust representations and a texture synthesis model of peripheral vision (Texforms). We found that the discriminability of robust representation and texture model images decreased to near chance performance as stimuli were presented farther in the periphery. Moreover, performance on robust and texture-model images showed similar trends within participants, while performance on non-robust representations changed minimally across the visual field. These results together suggest that (1) adversarially robust representations capture peripheral computation better than non-robust representations and (2) robust representations capture peripheral computation similar to current state-of-the-art texture peripheral vision models. More broadly, our findings support the idea that localized texture summary statistic representations may drive human invariance to adversarial perturbations and that the incorporation of such representations in DNNs could give rise to useful properties like adversarial robustness.



### Pose Guided Image Generation from Misaligned Sources via Residual Flow Based Correction
- **Arxiv ID**: http://arxiv.org/abs/2202.00843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00843v1)
- **Published**: 2022-02-02 01:30:15+00:00
- **Updated**: 2022-02-02 01:30:15+00:00
- **Authors**: Jiawei Lu, He Wang, Tianjia Shao, Yin Yang, Kun Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Generating new images with desired properties (e.g. new view/poses) from source images has been enthusiastically pursued recently, due to its wide range of potential applications. One way to ensure high-quality generation is to use multiple sources with complementary information such as different views of the same object. However, as source images are often misaligned due to the large disparities among the camera settings, strong assumptions have been made in the past with respect to the camera(s) or/and the object in interest, limiting the application of such techniques. Therefore, we propose a new general approach which models multiple types of variations among sources, such as view angles, poses, facial expressions, in a unified framework, so that it can be employed on datasets of vastly different nature. We verify our approach on a variety of data including humans bodies, faces, city scenes and 3D objects. Both the qualitative and quantitative results demonstrate the better performance of our method than the state of the art.



### Active Audio-Visual Separation of Dynamic Sound Sources
- **Arxiv ID**: http://arxiv.org/abs/2202.00850v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00850v2)
- **Published**: 2022-02-02 02:03:28+00:00
- **Updated**: 2022-07-25 06:49:20+00:00
- **Authors**: Sagnik Majumder, Kristen Grauman
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: We explore active audio-visual separation for dynamic sound sources, where an embodied agent moves intelligently in a 3D environment to continuously isolate the time-varying audio stream being emitted by an object of interest. The agent hears a mixed stream of multiple audio sources (e.g., multiple people conversing and a band playing music at a noisy party). Given a limited time budget, it needs to extract the target sound accurately at every step using egocentric audio-visual observations. We propose a reinforcement learning agent equipped with a novel transformer memory that learns motion policies to control its camera and microphone to recover the dynamic target audio, using self-attention to make high-quality estimates for current timesteps and also simultaneously improve its past estimates. Using highly realistic acoustic SoundSpaces simulations in real-world scanned Matterport3D environments, we show that our model is able to learn efficient behavior to carry out continuous separation of a dynamic audio target. Project: https://vision.cs.utexas.edu/projects/active-av-dynamic-separation/.



### Extension: Adaptive Sampling with Implicit Radiance Field
- **Arxiv ID**: http://arxiv.org/abs/2202.00855v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00855v3)
- **Published**: 2022-02-02 02:29:12+00:00
- **Updated**: 2022-02-08 12:47:32+00:00
- **Authors**: Yuchi Huo
- **Comment**: None
- **Journal**: None
- **Summary**: This manuscript discusses the extension of adaptive light field sampling with implicit radiance fields.



### Decoupled IoU Regression for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.00866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00866v1)
- **Published**: 2022-02-02 04:01:11+00:00
- **Updated**: 2022-02-02 04:01:11+00:00
- **Authors**: Yan Gao, Qimeng Wang, Xu Tang, Haochen Wang, Fei Ding, Jing Li, Yao Hu
- **Comment**: ACMMM 2021 Poster
- **Journal**: None
- **Summary**: Non-maximum suppression (NMS) is widely used in object detection pipelines for removing duplicated bounding boxes. The inconsistency between the confidence for NMS and the real localization confidence seriously affects detection performance. Prior works propose to predict Intersection-over-Union (IoU) between bounding boxes and corresponding ground-truths to improve NMS, while accurately predicting IoU is still a challenging problem. We argue that the complex definition of IoU and feature misalignment make it difficult to predict IoU accurately. In this paper, we propose a novel Decoupled IoU Regression (DIR) model to handle these problems. The proposed DIR decouples the traditional localization confidence metric IoU into two new metrics, Purity and Integrity. Purity reflects the proportion of the object area in the detected bounding box, and Integrity refers to the completeness of the detected object area. Separately predicting Purity and Integrity can divide the complex mapping between the bounding box and its IoU into two clearer mappings and model them independently. In addition, a simple but effective feature realignment approach is also introduced to make the IoU regressor work in a hindsight manner, which can make the target mapping more stable. The proposed DIR can be conveniently integrated with existing two-stage detectors and significantly improve their performance. Through a simple implementation of DIR with HTC, we obtain 51.3% AP on MS COCO benchmark, which outperforms previous methods and achieves state-of-the-art.



### Automotive Parts Assessment: Applying Real-time Instance-Segmentation Models to Identify Vehicle Parts
- **Arxiv ID**: http://arxiv.org/abs/2202.00884v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2202.00884v1)
- **Published**: 2022-02-02 05:38:13+00:00
- **Updated**: 2022-02-02 05:38:13+00:00
- **Authors**: Syed Adnan Yusuf, Abdulmalik Ali Aldawsari, Riad Souissi
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of automated car damage assessment presents a major challenge in the auto repair and damage assessment industry. The domain has several application areas ranging from car assessment companies such as car rentals and body shops to accidental damage assessment for car insurance companies. In vehicle assessment, the damage can take any form including scratches, minor and major dents to missing parts. More often, the assessment area has a significant level of noise such as dirt, grease, oil or rush that makes an accurate identification challenging. Moreover, the identification of a particular part is the first step in the repair industry to have an accurate labour and part assessment where the presence of different car models, shapes and sizes makes the task even more challenging for a machine-learning model to perform well. To address these challenges, this research explores and applies various instance segmentation methodologies to evaluate the best performing models.   The scope of this work focusses on two genres of real-time instance segmentation models due to their industrial significance, namely SipMask and Yolact. These methodologies are evaluated against a previously reported car parts dataset (DSMLR) and an internally curated dataset extracted from local car repair workshops. The Yolact-based part localization and segmentation method performed well when compared to other real-time instance mechanisms with a mAP of 66.5. For the workshop repair dataset, SipMask++ reported better accuracies for object detection with a mAP of 57.0 with outcomes for AP_IoU=.50and AP_IoU=.75 reporting 72.0 and 67.0 respectively while Yolact was found to be a better performer for AP_s with 44.0 and 2.6 for object detection and segmentation categories respectively.



### Accurate calibration of multi-perspective cameras from a generalization of the hand-eye constraint
- **Arxiv ID**: http://arxiv.org/abs/2202.00886v5
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00886v5)
- **Published**: 2022-02-02 05:44:08+00:00
- **Updated**: 2022-04-13 13:27:24+00:00
- **Authors**: Yifu Wang, Wenqing Jiang, Kun Huang, Sören Schwertfeger, Laurent Kneip
- **Comment**: accepted in the 2022 IEEE International Conference on Robotics and
  Automation (ICRA), Philadelphia (PA), USA
- **Journal**: None
- **Summary**: Multi-perspective cameras are quickly gaining importance in many applications such as smart vehicles and virtual or augmented reality. However, a large system size or absence of overlap in neighbouring fields-of-view often complicate their calibration. We present a novel solution which relies on the availability of an external motion capture system. Our core contribution consists of an extension to the hand-eye calibration problem which jointly solves multi-eye-to-base problems in closed form. We furthermore demonstrate its equivalence to the multi-eye-in-hand problem. The practical validity of our approach is supported by our experiments, indicating that the method is highly efficient and accurate, and outperforms existing closed-form alternatives.



### Does Video Compression Impact Tracking Accuracy?
- **Arxiv ID**: http://arxiv.org/abs/2202.00892v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00892v1)
- **Published**: 2022-02-02 06:43:29+00:00
- **Updated**: 2022-02-02 06:43:29+00:00
- **Authors**: Takehiro Tanaka, Alon Harell, Ivan V. Bajić
- **Comment**: 5 pages, 6 figures, 3 tables, IEEE International Symposium on
  Circuits and Systems (ISCAS) 2022
- **Journal**: None
- **Summary**: Everyone "knows" that compressing a video will degrade the accuracy of object tracking. Yet, a literature search on this topic reveals that there is very little documented evidence for this presumed fact. Part of the reason is that, until recently, there were no object tracking datasets for uncompressed video, which made studying the effects of compression on tracking accuracy difficult. In this paper, using a recently published dataset that contains tracking annotations for uncompressed videos, we examined the degradation of tracking accuracy due to video compression using rigorous statistical methods. Specifically, we examined the impact of quantization parameter (QP) and motion search range (MSR) on Multiple Object Tracking Accuracy (MOTA). The results show that QP impacts MOTA at the 95% confidence level, while there is insufficient evidence to claim that MSR impacts MOTA. Moreover, regression analysis allows us to derive a quantitative relationship between MOTA and QP for the specific tracker used in the experiments.



### Image Forgery Detection with Interpretability
- **Arxiv ID**: http://arxiv.org/abs/2202.00908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.00908v1)
- **Published**: 2022-02-02 08:16:50+00:00
- **Updated**: 2022-02-02 08:16:50+00:00
- **Authors**: Ankit Katiyar, Arnav Bhavsar
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a learning based method focusing on the convolutional neural network (CNN) architecture to detect these forgeries. We consider the detection of both copy-move forgeries and inpainting based forgeries. For these, we synthesize our own large dataset. In addition to classification, the focus is also on interpretability of the forgery detection. As the CNN classification yields the image-level label, it is important to understand if forged region has indeed contributed to the classification. For this purpose, we demonstrate using the Grad-CAM heatmap, that in various correctly classified examples, that the forged region is indeed the region contributing to the classification. Interestingly, this is also applicable for small forged regions, as is depicted in our results. Such an analysis can also help in establishing the reliability of the classification.



### CSFlow: Learning Optical Flow via Cross Strip Correlation for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2202.00909v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00909v1)
- **Published**: 2022-02-02 08:17:45+00:00
- **Updated**: 2022-02-02 08:17:45+00:00
- **Authors**: Hao Shi, Yifan Zhou, Kailun Yang, Xiaoting Yin, Kaiwei Wang
- **Comment**: Code is publicly available at https://github.com/MasterHow/CSFlow
- **Journal**: None
- **Summary**: Optical flow estimation is an essential task in self-driving systems, which helps autonomous vehicles perceive temporal continuity information of surrounding scenes. The calculation of all-pair correlation plays an important role in many existing state-of-the-art optical flow estimation methods. However, the reliance on local knowledge often limits the model's accuracy under complex street scenes. In this paper, we propose a new deep network architecture for optical flow estimation in autonomous driving--CSFlow, which consists of two novel modules: Cross Strip Correlation module (CSC) and Correlation Regression Initialization module (CRI). CSC utilizes a striping operation across the target image and the attended image to encode global context into correlation volumes, while maintaining high efficiency. CRI is used to maximally exploit the global context for optical flow initialization. Our method has achieved state-of-the-art accuracy on the public autonomous driving dataset KITTI-2015. Code is publicly available at https://github.com/MasterHow/CSFlow.



### Eikonal Fields for Refractive Novel-View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2202.00948v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00948v3)
- **Published**: 2022-02-02 10:49:08+00:00
- **Updated**: 2022-06-15 13:25:40+00:00
- **Authors**: Mojtaba Bemana, Karol Myszkowski, Jeppe Revall Frisvad, Hans-Peter Seidel, Tobias Ritschel
- **Comment**: 8 pages, 6 figures, project webpage:
  https://eikonalfield.mpi-inf.mpg.de
- **Journal**: None
- **Summary**: We tackle the problem of generating novel-view images from collections of 2D images showing refractive and reflective objects. Current solutions assume opaque or transparent light transport along straight paths following the emission-absorption model. Instead, we optimize for a field of 3D-varying Index of Refraction (IoR) and trace light through it that bends toward the spatial gradients of said IoR according to the laws of eikonal light transport.



### GANSlider: How Users Control Generative Models for Images using Multiple Sliders with and without Feedforward Information
- **Arxiv ID**: http://arxiv.org/abs/2202.00965v1
- **DOI**: 10.1145/3491102.3502141
- **Categories**: **cs.HC**, cs.CV, cs.GR, H.5.2
- **Links**: [PDF](http://arxiv.org/pdf/2202.00965v1)
- **Published**: 2022-02-02 11:25:07+00:00
- **Updated**: 2022-02-02 11:25:07+00:00
- **Authors**: Hai Dang, Lukas Mecke, Daniel Buschek
- **Comment**: 15 pages, 10 figures, ACM CHI 2022
- **Journal**: None
- **Summary**: We investigate how multiple sliders with and without feedforward visualizations influence users' control of generative models. In an online study (N=138), we collected a dataset of people interacting with a generative adversarial network (StyleGAN2) in an image reconstruction task. We found that more control dimensions (sliders) significantly increase task difficulty and user actions. Visual feedforward partly mitigates this by enabling more goal-directed interaction. However, we found no evidence of faster or more accurate task performance. This indicates a tradeoff between feedforward detail and implied cognitive costs, such as attention. Moreover, we found that visualizations alone are not always sufficient for users to understand individual control dimensions. Our study quantifies fundamental UI design factors and resulting interaction behavior in this context, revealing opportunities for improvement in the UI design for interactive applications of generative models. We close by discussing design directions and further aspects.



### DCSAU-Net: A Deeper and More Compact Split-Attention U-Net for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.00972v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.00972v2)
- **Published**: 2022-02-02 11:36:15+00:00
- **Updated**: 2022-09-24 12:00:04+00:00
- **Authors**: Qing Xu, Zhicheng Ma, Na HE, Wenting Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning architecture with convolutional neural network (CNN) achieves outstanding success in the field of computer vision. Where U-Net, an encoder-decoder architecture structured by CNN, makes a great breakthrough in biomedical image segmentation and has been applied in a wide range of practical scenarios. However, the equal design of every downsampling layer in the encoder part and simply stacked convolutions do not allow U-Net to extract sufficient information of features from different depths. The increasing complexity of medical images brings new challenges to the existing methods. In this paper, we propose a deeper and more compact split-attention u-shape network (DCSAU-Net), which efficiently utilises low-level and high-level semantic information based on two novel frameworks: primary feature conservation and compact split-attention block. We evaluate the proposed model on CVC-ClinicDB, 2018 Data Science Bowl, ISIC-2018 and SegPC-2021 datasets. As a result, DCSAU-Net displays better performance than other state-of-the-art (SOTA) methods in terms of the mean Intersection over Union (mIoU) and F1-socre. More significantly, the proposed model demonstrates excellent segmentation performance on challenging images. The code for our work and more technical details can be found at https://github.com/xq141839/DCSAU-Net.



### Training Semantic Descriptors for Image-Based Localization
- **Arxiv ID**: http://arxiv.org/abs/2202.01212v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, 68T45, I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2202.01212v1)
- **Published**: 2022-02-02 12:17:59+00:00
- **Updated**: 2022-02-02 12:17:59+00:00
- **Authors**: Ibrahim Cinaroglu, Yalin Bastanlar
- **Comment**: 4 pages, 4 figures, Accepted and Presented at Workshop on Perception
  for Autonomous Driving (PAD) / ECCV 2020
- **Journal**: None
- **Summary**: Vision based solutions for the localization of vehicles have become popular recently. We employ an image retrieval based visual localization approach. The database images are kept with GPS coordinates and the location of the retrieved database image serves as an approximate position of the query image. We show that localization can be performed via descriptors solely extracted from semantically segmented images. It is reliable especially when the environment is subjected to severe illumination and seasonal changes. Our experiments reveal that the localization performance of a semantic descriptor can increase up to the level of state-of-the-art RGB image based methods.



### Dictionary learning for clustering on hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/2202.00990v1
- **DOI**: 10.1007/s11760-020-01750-z
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.2.10; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2202.00990v1)
- **Published**: 2022-02-02 12:22:33+00:00
- **Updated**: 2022-02-02 12:22:33+00:00
- **Authors**: Joshua Bruton, Hairong Wang
- **Comment**: Springer Machine Learning Journal, 8 pages, 3 figures
- **Journal**: Signal, Image and Video Processing 15.2 (2021): 255-261
- **Summary**: Dictionary learning and sparse coding have been widely studied as mechanisms for unsupervised feature learning. Unsupervised learning could bring enormous benefit to the processing of hyperspectral images and to other remote sensing data analysis because labelled data are often scarce in this field. We propose a method for clustering the pixels of hyperspectral images using sparse coefficients computed from a representative dictionary as features. We show empirically that the proposed method works more effectively than clustering on the original pixels. We also demonstrate that our approach, in certain circumstances, outperforms the clustering results of features extracted using principal component analysis and non-negative matrix factorisation. Furthermore, our method is suitable for applications in repetitively clustering an ever-growing amount of high-dimensional data, which is the case when working with hyperspectral satellite imagery.



### Gradient Variance Loss for Structure-Enhanced Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2202.00997v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.00997v1)
- **Published**: 2022-02-02 12:31:05+00:00
- **Updated**: 2022-02-02 12:31:05+00:00
- **Authors**: Lusine Abrahamyan, Anh Minh Truong, Wilfried Philips, Nikos Deligiannis
- **Comment**: ICASSP 2022
- **Journal**: None
- **Summary**: Recent success in the field of single image super-resolution (SISR) is achieved by optimizing deep convolutional neural networks (CNNs) in the image space with the L1 or L2 loss. However, when trained with these loss functions, models usually fail to recover sharp edges present in the high-resolution (HR) images for the reason that the model tends to give a statistical average of potential HR solutions. During our research, we observe that gradient maps of images generated by the models trained with the L1 or L2 loss have significantly lower variance than the gradient maps of the original high-resolution images. In this work, we propose to alleviate the above issue by introducing a structure-enhancing loss function, coined Gradient Variance (GV) loss, and generate textures with perceptual-pleasant details. Specifically, during the training of the model, we extract patches from the gradient maps of the target and generated output, calculate the variance of each patch and form variance maps for these two images. Further, we minimize the distance between the computed variance maps to enforce the model to produce high variance gradient maps that will lead to the generation of high-resolution images with sharper edges. Experimental results show that the GV loss can significantly improve both Structure Similarity (SSIM) and peak signal-to-noise ratio (PSNR) performance of existing image super-resolution (SR) deep learning models.



### Auto-Transfer: Learning to Route Transferrable Representations
- **Arxiv ID**: http://arxiv.org/abs/2202.01011v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01011v4)
- **Published**: 2022-02-02 13:09:27+00:00
- **Updated**: 2022-03-16 15:17:30+00:00
- **Authors**: Keerthiram Murugesan, Vijay Sadashivaiah, Ronny Luss, Karthikeyan Shanmugam, Pin-Yu Chen, Amit Dhurandhar
- **Comment**: Camera ready ICLR 2022
- **Journal**: None
- **Summary**: Knowledge transfer between heterogeneous source and target networks and tasks has received a lot of attention in recent times as large amounts of quality labeled data can be difficult to obtain in many applications. Existing approaches typically constrain the target deep neural network (DNN) feature representations to be close to the source DNNs feature representations, which can be limiting. We, in this paper, propose a novel adversarial multi-armed bandit approach that automatically learns to route source representations to appropriate target representations following which they are combined in meaningful ways to produce accurate target models. We see upwards of 5\% accuracy improvements compared with the state-of-the-art knowledge transfer methods on four benchmark (target) image datasets CUB200, Stanford Dogs, MIT67, and Stanford40 where the source dataset is ImageNet. We qualitatively analyze the goodness of our transfer scheme by showing individual examples of the important features focused on by our target network at different layers compared with the (closest) competitors. We also observe that our improvement over other methods is higher for smaller target datasets making it an effective tool for small data applications that may benefit from transfer learning.



### MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray
- **Arxiv ID**: http://arxiv.org/abs/2202.01020v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4; J.7
- **Links**: [PDF](http://arxiv.org/pdf/2202.01020v3)
- **Published**: 2022-02-02 13:25:23+00:00
- **Updated**: 2022-04-08 12:52:22+00:00
- **Authors**: Abril Corona-Figueroa, Jonathan Frawley, Sam Bond-Taylor, Sarath Bethapudi, Hubert P. H. Shum, Chris G. Willcocks
- **Comment**: 6 pages, 4 figures, accepted at IEEE EMBC 2022
- **Journal**: None
- **Summary**: Computed tomography (CT) is an effective medical imaging modality, widely used in the field of clinical medicine for the diagnosis of various pathologies. Advances in Multidetector CT imaging technology have enabled additional functionalities, including generation of thin slice multiplanar cross-sectional body imaging and 3D reconstructions. However, this involves patients being exposed to a considerable dose of ionising radiation. Excessive ionising radiation can lead to deterministic and harmful effects on the body. This paper proposes a Deep Learning model that learns to reconstruct CT projections from a few or even a single-view X-ray. This is based on a novel architecture that builds from neural radiance fields, which learns a continuous representation of CT scans by disentangling the shape and volumetric depth of surface and internal anatomical structures from 2D images. Our model is trained on chest and knee datasets, and we demonstrate qualitative and quantitative high-fidelity renderings and compare our approach to other recent radiance field-based methods. Our code and link to our datasets are available at https://github.com/abrilcf/mednerf



### MMSys'22 Grand Challenge on AI-based Video Production for Soccer
- **Arxiv ID**: http://arxiv.org/abs/2202.01031v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2202.01031v1)
- **Published**: 2022-02-02 13:53:42+00:00
- **Updated**: 2022-02-02 13:53:42+00:00
- **Authors**: Cise Midoglu, Steven A. Hicks, Vajira Thambawita, Tomas Kupka, Pål Halvorsen
- **Comment**: None
- **Journal**: None
- **Summary**: Soccer has a considerable market share of the global sports industry, and the interest in viewing videos from soccer games continues to grow. In this respect, it is important to provide game summaries and highlights of the main game events. However, annotating and producing events and summaries often require expensive equipment and a lot of tedious, cumbersome, manual labor. Therefore, automating the video production pipeline providing fast game highlights at a much lower cost is seen as the "holy grail". In this context, recent developments in Artificial Intelligence (AI) technology have shown great potential. Still, state-of-the-art approaches are far from being adequate for practical scenarios that have demanding real-time requirements, as well as strict performance criteria (where at least the detection of official events such as goals and cards must be 100% accurate). In addition, event detection should be thoroughly enhanced by annotation and classification, proper clipping, generating short descriptions, selecting appropriate thumbnails for highlight clips, and finally, combining the event highlights into an overall game summary, similar to what is commonly aired during sports news. Even though the event tagging operation has by far received the most attention, an end-to-end video production pipeline also includes various other operations which serve the overall purpose of automated soccer analysis. This challenge aims to assist the automation of such a production pipeline using AI. In particular, we focus on the enhancement operations that take place after an event has been detected, namely event clipping (Task 1), thumbnail selection (Task 2), and game summarization (Task 3). Challenge website: https://mmsys2022.ie/authors/grand-challenge.



### Image-based Navigation in Real-World Environments via Multiple Mid-level Representations: Fusion Models, Benchmark and Efficient Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2202.01069v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01069v1)
- **Published**: 2022-02-02 15:00:44+00:00
- **Updated**: 2022-02-02 15:00:44+00:00
- **Authors**: Marco Rosano, Antonino Furnari, Luigi Gulino, Corrado Santoro, Giovanni Maria Farinella
- **Comment**: None
- **Journal**: None
- **Summary**: Navigating complex indoor environments requires a deep understanding of the space the robotic agent is acting into to correctly inform the navigation process of the agent towards the goal location. In recent learning-based navigation approaches, the scene understanding and navigation abilities of the agent are achieved simultaneously by collecting the required experience in simulation. Unfortunately, even if simulators represent an efficient tool to train navigation policies, the resulting models often fail when transferred into the real world. One possible solution is to provide the navigation model with mid-level visual representations containing important domain-invariant properties of the scene. But, what are the best representations that facilitate the transfer of a model to the real-world? How can they be combined? In this work we address these issues by proposing a benchmark of Deep Learning architectures to combine a range of mid-level visual representations, to perform a PointGoal navigation task following a Reinforcement Learning setup. All the proposed navigation models have been trained with the Habitat simulator on a synthetic office environment and have been tested on the same real-world environment using a real robotic platform. To efficiently assess their performance in a real context, a validation tool has been proposed to generate realistic navigation episodes inside the simulator. Our experiments showed that navigation models can benefit from the multi-modal input and that our validation tool can provide good estimation of the expected navigation performance in the real world, while saving time and resources. The acquired synthetic and real 3D models of the environment, together with the code of our validation tool built on top of Habitat, are publicly available at the following link: https://iplab.dmi.unict.it/EmbodiedVN/



### An Optimal Transport Perspective on Unpaired Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2202.01116v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.01116v2)
- **Published**: 2022-02-02 16:21:20+00:00
- **Updated**: 2022-10-11 16:15:58+00:00
- **Authors**: Milena Gazdieva, Litu Rout, Alexander Korotin, Andrey Kravchenko, Alexander Filippov, Evgeny Burnaev
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world image super-resolution (SR) tasks often do not have paired datasets, which limits the application of supervised techniques. As a result, the tasks are usually approached by unpaired techniques based on Generative Adversarial Networks (GANs), which yield complex training losses with several regularization terms, e.g., content or identity losses. We theoretically investigate optimization problems which arise in such models and find two surprizing observations. First, the learned SR map is always an optimal transport (OT) map. Second, we theoretically prove and empirically show that the learned map is biased, i.e., it does not actually transform the distribution of low-resolution images to high-resolution ones. Inspired by these findings, we propose an algorithm for unpaired SR which learns an unbiased OT map for the perceptual transport cost. Unlike the existing GAN-based alternatives, our algorithm has a simple optimization objective reducing the need for complex hyperparameter selection and an application of additional regularizations. At the same time, it provides a nearly state-of-the-art performance on the large-scale unpaired AIM19 dataset.



### An Eye for an Eye: Defending against Gradient-based Attacks with Gradients
- **Arxiv ID**: http://arxiv.org/abs/2202.01117v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.01117v1)
- **Published**: 2022-02-02 16:22:28+00:00
- **Updated**: 2022-02-02 16:22:28+00:00
- **Authors**: Hanbin Hong, Yuan Hong, Yu Kong
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have been shown to be vulnerable to adversarial attacks. In particular, gradient-based attacks have demonstrated high success rates recently. The gradient measures how each image pixel affects the model output, which contains critical information for generating malicious perturbations. In this paper, we show that the gradients can also be exploited as a powerful weapon to defend against adversarial attacks. By using both gradient maps and adversarial images as inputs, we propose a Two-stream Restoration Network (TRN) to restore the adversarial images. To optimally restore the perturbed images with two streams of inputs, a Gradient Map Estimation Mechanism is proposed to estimate the gradients of adversarial images, and a Fusion Block is designed in TRN to explore and fuse the information in two streams. Once trained, our TRN can defend against a wide range of attack methods without significantly degrading the performance of benign inputs. Also, our method is generalizable, scalable, and hard to bypass. Experimental results on CIFAR10, SVHN, and Fashion MNIST demonstrate that our method outperforms state-of-the-art defense methods.



### Probabilistically Robust Learning: Balancing Average- and Worst-case Performance
- **Arxiv ID**: http://arxiv.org/abs/2202.01136v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2202.01136v3)
- **Published**: 2022-02-02 17:01:38+00:00
- **Updated**: 2022-06-07 18:08:41+00:00
- **Authors**: Alexander Robey, Luiz F. O. Chamon, George J. Pappas, Hamed Hassani
- **Comment**: None
- **Journal**: None
- **Summary**: Many of the successes of machine learning are based on minimizing an averaged loss function. However, it is well-known that this paradigm suffers from robustness issues that hinder its applicability in safety-critical domains. These issues are often addressed by training against worst-case perturbations of data, a technique known as adversarial training. Although empirically effective, adversarial training can be overly conservative, leading to unfavorable trade-offs between nominal performance and robustness. To this end, in this paper we propose a framework called probabilistic robustness that bridges the gap between the accurate, yet brittle average case and the robust, yet conservative worst case by enforcing robustness to most rather than to all perturbations. From a theoretical point of view, this framework overcomes the trade-offs between the performance and the sample-complexity of worst-case and average-case learning. From a practical point of view, we propose a novel algorithm based on risk-aware optimization that effectively balances average- and worst-case performance at a considerably lower computational cost relative to adversarial training. Our results on MNIST, CIFAR-10, and SVHN illustrate the advantages of this framework on the spectrum from average- to worst-case robustness.



### Make Some Noise: Reliable and Efficient Single-Step Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2202.01181v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01181v3)
- **Published**: 2022-02-02 18:10:01+00:00
- **Updated**: 2022-10-17 22:10:03+00:00
- **Authors**: Pau de Jorge, Adel Bibi, Riccardo Volpi, Amartya Sanyal, Philip H. S. Torr, Grégory Rogez, Puneet K. Dokania
- **Comment**: Published in NeurIPS 2022
- **Journal**: None
- **Summary**: Recently, Wong et al. showed that adversarial training with single-step FGSM leads to a characteristic failure mode named Catastrophic Overfitting (CO), in which a model becomes suddenly vulnerable to multi-step attacks. Experimentally they showed that simply adding a random perturbation prior to FGSM (RS-FGSM) could prevent CO. However, Andriushchenko and Flammarion observed that RS-FGSM still leads to CO for larger perturbations, and proposed a computationally expensive regularizer (GradAlign) to avoid it. In this work, we methodically revisit the role of noise and clipping in single-step adversarial training. Contrary to previous intuitions, we find that using a stronger noise around the clean sample combined with \textit{not clipping} is highly effective in avoiding CO for large perturbation radii. We then propose Noise-FGSM (N-FGSM) that, while providing the benefits of single-step adversarial training, does not suffer from CO. Empirical analyses on a large suite of experiments show that N-FGSM is able to match or surpass the performance of previous state-of-the-art GradAlign, while achieving 3x speed-up. Code can be found in https://github.com/pdejorge/N-FGSM



### VOS: Learning What You Don't Know by Virtual Outlier Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2202.01197v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01197v4)
- **Published**: 2022-02-02 18:43:01+00:00
- **Updated**: 2022-05-09 20:12:32+00:00
- **Authors**: Xuefeng Du, Zhaoning Wang, Mu Cai, Yixuan Li
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: Out-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves competitive performance on both object detection and image classification models, reducing the FPR95 by up to 9.36% compared to the previous best method on object detectors. Code is available at https://github.com/deeplearning-wisc/vos.



### Human Activity Recognition Using Tools of Convolutional Neural Networks: A State of the Art Review, Data Sets, Challenges and Future Prospects
- **Arxiv ID**: http://arxiv.org/abs/2202.03274v1
- **DOI**: 10.1016/j.compbiomed.2022.106060
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03274v1)
- **Published**: 2022-02-02 18:52:13+00:00
- **Updated**: 2022-02-02 18:52:13+00:00
- **Authors**: Md. Milon Islam, Sheikh Nooruddin, Fakhri Karray, Ghulam Muhammad
- **Comment**: 32 pages, 4 figures, 4 Tables
- **Journal**: Comput. Biol. Med.C149:106060,2022
- **Summary**: Human Activity Recognition (HAR) plays a significant role in the everyday life of people because of its ability to learn extensive high-level information about human activity from wearable or stationary devices. A substantial amount of research has been conducted on HAR and numerous approaches based on deep learning and machine learning have been exploited by the research community to classify human activities. The main goal of this review is to summarize recent works based on a wide range of deep neural networks architecture, namely convolutional neural networks (CNNs) for human activity recognition. The reviewed systems are clustered into four categories depending on the use of input devices like multimodal sensing devices, smartphones, radar, and vision devices. This review describes the performances, strengths, weaknesses, and the used hyperparameters of CNN architectures for each reviewed system with an overview of available public data sources. In addition, a discussion with the current challenges to CNN-based HAR systems is presented. Finally, this review is concluded with some potential future directions that would be of great assistance for the researchers who would like to contribute to this field.



### Automated processing of X-ray computed tomography images via panoptic segmentation for modeling woven composite textiles
- **Arxiv ID**: http://arxiv.org/abs/2202.01265v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2202.01265v1)
- **Published**: 2022-02-02 19:59:53+00:00
- **Updated**: 2022-02-02 19:59:53+00:00
- **Authors**: Aaron Allred, Lauren J. Abbott, Alireza Doostan, Kurt Maute
- **Comment**: None
- **Journal**: None
- **Summary**: A new, machine learning-based approach for automatically generating 3D digital geometries of woven composite textiles is proposed to overcome the limitations of existing analytical descriptions and segmentation methods. In this approach, panoptic segmentation is leveraged to produce instance segmented semantic masks from X-ray computed tomography (CT) images. This effort represents the first deep learning based automated process for segmenting unique yarn instances in a woven composite textile. Furthermore, it improves on existing methods by providing instance-level segmentation on low contrast CT datasets. Frame-to-frame instance tracking is accomplished via an intersection-over-union (IoU) approach adopted from video panoptic segmentation for assembling a 3D geometric model. A corrective recognition algorithm is developed to improve the recognition quality (RQ). The panoptic quality (PQ) metric is adopted to provide a new universal evaluation metric for reconstructed woven composite textiles. It is found that the panoptic segmentation network generalizes well to new CT images that are similar to the training set but does not extrapolate well to CT images of differing geometry, texture, and contrast. The utility of this approach is demonstrated by capturing yarn flow directions, contact regions between individual yarns, and the spatially varying cross-sectional areas of the yarns.



### Beyond Images: Label Noise Transition Matrix Estimation for Tasks with Lower-Quality Features
- **Arxiv ID**: http://arxiv.org/abs/2202.01273v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01273v2)
- **Published**: 2022-02-02 20:36:09+00:00
- **Updated**: 2022-06-17 19:19:49+00:00
- **Authors**: Zhaowei Zhu, Jialu Wang, Yang Liu
- **Comment**: ICML 2022
- **Journal**: None
- **Summary**: The label noise transition matrix, denoting the transition probabilities from clean labels to noisy labels, is crucial for designing statistically robust solutions. Existing estimators for noise transition matrices, e.g., using either anchor points or clusterability, focus on computer vision tasks that are relatively easier to obtain high-quality representations. We observe that tasks with lower-quality features fail to meet the anchor-point or clusterability condition, due to the coexistence of both uninformative and informative representations. To handle this issue, we propose a generic and practical information-theoretic approach to down-weight the less informative parts of the lower-quality features. This improvement is crucial to identifying and estimating the label noise transition matrix. The salient technical challenge is to compute the relevant information-theoretical metrics using only noisy labels instead of clean ones. We prove that the celebrated $f$-mutual information measure can often preserve the order when calculated using noisy labels. We then build our transition matrix estimator using this distilled version of features. The necessity and effectiveness of the proposed method are also demonstrated by evaluating the estimation error on a varied set of tabular data and text classification tasks with lower-quality features. Code is available at github.com/UCSC-REAL/BeyondImages.



### Cyclical Pruning for Sparse Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.01290v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.01290v1)
- **Published**: 2022-02-02 21:29:07+00:00
- **Updated**: 2022-02-02 21:29:07+00:00
- **Authors**: Suraj Srinivas, Andrey Kuzmin, Markus Nagel, Mart van Baalen, Andrii Skliar, Tijmen Blankevoort
- **Comment**: None
- **Journal**: None
- **Summary**: Current methods for pruning neural network weights iteratively apply magnitude-based pruning on the model weights and re-train the resulting model to recover lost accuracy. In this work, we show that such strategies do not allow for the recovery of erroneously pruned weights. To enable weight recovery, we propose a simple strategy called \textit{cyclical pruning} which requires the pruning schedule to be periodic and allows for weights pruned erroneously in one cycle to recover in subsequent ones. Experimental results on both linear models and large-scale deep neural networks show that cyclical pruning outperforms existing pruning algorithms, especially at high sparsity ratios. Our approach is easy to tune and can be readily incorporated into existing pruning pipelines to boost performance.



### Multi-Resolution Factor Graph Based Stereo Correspondence Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2202.01309v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.01309v1)
- **Published**: 2022-02-02 22:27:10+00:00
- **Updated**: 2022-02-02 22:27:10+00:00
- **Authors**: Hanieh Shabanian, Madhusudhanan Balasubramanian
- **Comment**: 18 pages, 4 figures and 2 tables. arXiv admin note: text overlap with
  arXiv:2109.11077
- **Journal**: None
- **Summary**: A dense depth-map of a scene at an arbitrary view orientation can be estimated from dense view correspondences among multiple lower-dimensional views of the scene. These low-dimensional view correspondences are dependent on the geometrical relationship among the views and the scene. Determining dense view correspondences is difficult in part due to presence of homogeneous regions in the scene and due to presence of occluded regions and illumination differences among the views. We present a new multi-resolution factor graph-based stereo matching algorithm (MR-FGS) that utilizes both intra- and inter-resolution dependencies among the views as well as among the disparity estimates. The proposed framework allows exchange of information among multiple resolutions of the correspondence problem and is useful for handling larger homogeneous regions in a scene. The MR-FGS algorithm was evaluated qualitatively and quantitatively using stereo pairs in the Middlebury stereo benchmark dataset based on commonly used performance measures. When compared to a recently developed factor graph model (FGS), the MR-FGS algorithm provided more accurate disparity estimates without requiring the commonly used post-processing procedure known as the left-right consistency check. The multi-resolution dependency constraint within the factor-graph model significantly improved contrast along depth boundaries in the MR-FGS generated disparity maps.



### PanoDepth: A Two-Stage Approach for Monocular Omnidirectional Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.01323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.01323v1)
- **Published**: 2022-02-02 23:08:06+00:00
- **Updated**: 2022-02-02 23:08:06+00:00
- **Authors**: Yuyan Li, Zhixin Yan, Ye Duan, Liu Ren
- **Comment**: Accepted by International Conference on 3D Vision (3DV). IEEE, 2021
- **Journal**: None
- **Summary**: Omnidirectional 3D information is essential for a wide range of applications such as Virtual Reality, Autonomous Driving, Robotics, etc. In this paper, we propose a novel, model-agnostic, two-stage pipeline for omnidirectional monocular depth estimation. Our proposed framework PanoDepth takes one 360 image as input, produces one or more synthesized views in the first stage, and feeds the original image and the synthesized images into the subsequent stereo matching stage. In the second stage, we propose a differentiable Spherical Warping Layer to handle omnidirectional stereo geometry efficiently and effectively. By utilizing the explicit stereo-based geometric constraints in the stereo matching stage, PanoDepth can generate dense high-quality depth. We conducted extensive experiments and ablation studies to evaluate PanoDepth with both the full pipeline as well as the individual modules in each stage. Our results show that PanoDepth outperforms the state-of-the-art approaches by a large margin for 360 monocular depth estimation.



