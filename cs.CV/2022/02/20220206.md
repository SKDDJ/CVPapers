# Arxiv Papers in cs.CV on 2022-02-06
### LiDAR dataset distillation within bayesian active learning framework: Understanding the effect of data augmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.02661v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02661v1)
- **Published**: 2022-02-06 00:04:21+00:00
- **Updated**: 2022-02-06 00:04:21+00:00
- **Authors**: Ngoc Phuong Anh Duong, Alexandre Almin, Léo Lemarié, B Ravi Kiran
- **Comment**: Accepted at VISAPP 2022
- **Journal**: None
- **Summary**: Autonomous driving (AD) datasets have progressively grown in size in the past few years to enable better deep representation learning. Active learning (AL) has re-gained attention recently to address reduction of annotation costs and dataset size. AL has remained relatively unexplored for AD datasets, especially on point cloud data from LiDARs. This paper performs a principled evaluation of AL based dataset distillation on (1/4th) of the large Semantic-KITTI dataset. Further on, the gains in model performance due to data augmentation (DA) are demonstrated across different subsets of the AL loop. We also demonstrate how DA improves the selection of informative samples to annotate. We observe that data augmentation achieves full dataset accuracy using only 60\% of samples from the selected dataset configuration. This provides faster training time and subsequent gains in annotation costs.



### Simulation-to-Reality domain adaptation for offline 3D object annotation on pointclouds with correlation alignment
- **Arxiv ID**: http://arxiv.org/abs/2202.02666v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02666v2)
- **Published**: 2022-02-06 00:40:18+00:00
- **Updated**: 2022-02-26 13:32:07+00:00
- **Authors**: Weishuang Zhang, B Ravi Kiran, Thomas Gauthier, Yanis Mazouz, Theo Steger
- **Comment**: Accepted at IMPROVE 2022
- **Journal**: None
- **Summary**: Annotating objects with 3D bounding boxes in LiDAR pointclouds is a costly human driven process in an autonomous driving perception system. In this paper, we present a method to semi-automatically annotate real-world pointclouds collected by deployment vehicles using simulated data. We train a 3D object detector model on labeled simulated data from CARLA jointly with real world pointclouds from our target vehicle. The supervised object detection loss is augmented with a CORAL loss term to reduce the distance between labeled simulated and unlabeled real pointcloud feature representations. The goal here is to learn representations that are invariant to simulated (labeled) and real-world (unlabeled) target domains. We also provide an updated survey on domain adaptation methods for pointclouds.



### SRPCN: Structure Retrieval based Point Completion Network
- **Arxiv ID**: http://arxiv.org/abs/2202.02669v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02669v3)
- **Published**: 2022-02-06 01:20:50+00:00
- **Updated**: 2023-03-20 10:19:37+00:00
- **Authors**: Kaiyi Zhang, Ximing Yang, Yuan Wu, Cheng Jin
- **Comment**: I think the proposed method has some defects
- **Journal**: None
- **Summary**: Given partial objects and some complete ones as references, point cloud completion aims to recover authentic shapes. However, existing methods pay little attention to general shapes, which leads to the poor authenticity of completion results. Besides, the missing patterns are diverse in reality, but existing methods can only handle fixed ones, which means a poor generalization ability. Considering that a partial point cloud is a subset of the corresponding complete one, we regard them as different samples of the same distribution and propose Structure Retrieval based Point Completion Network (SRPCN). It first uses k-means clustering to extract structure points and disperses them into distributions, and then KL Divergence is used as a metric to find the complete structure point cloud that best matches the input in a database. Finally, a PCN-like decoder network is adopted to generate the final results based on the retrieved structure point clouds. As structure plays an important role in describing the general shape of an object and the proposed structure retrieval method is robust to missing patterns, experiments show that our method can generate more authentic results and has a stronger generalization ability.



### Hyper-Convolutions via Implicit Kernels for Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/2202.02701v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02701v1)
- **Published**: 2022-02-06 03:56:19+00:00
- **Updated**: 2022-02-06 03:56:19+00:00
- **Authors**: Tianyu Ma, Alan Q. Wang, Adrian V. Dalca, Mert R. Sabuncu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2105.10559
- **Journal**: None
- **Summary**: The convolutional neural network (CNN) is one of the most commonly used architectures for computer vision tasks. The key building block of a CNN is the convolutional kernel that aggregates information from the pixel neighborhood and shares weights across all pixels. A standard CNN's capacity, and thus its performance, is directly related to the number of learnable kernel weights, which is determined by the number of channels and the kernel size (support). In this paper, we present the \textit{hyper-convolution}, a novel building block that implicitly encodes the convolutional kernel using spatial coordinates. Hyper-convolutions decouple kernel size from the total number of learnable parameters, enabling a more flexible architecture design. We demonstrate in our experiments that replacing regular convolutions with hyper-convolutions can improve performance with less parameters, and increase robustness against noise. We provide our code here: \emph{https://github.com/tym002/Hyper-Convolution}



### Multi-modal Sensor Fusion for Auto Driving Perception: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2202.02703v2
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2202.02703v2)
- **Published**: 2022-02-06 04:18:45+00:00
- **Updated**: 2022-02-27 05:48:08+00:00
- **Authors**: Keli Huang, Botian Shi, Xiang Li, Xin Li, Siyuan Huang, Yikang Li
- **Comment**: 14 pages, 8 figures
- **Journal**: None
- **Summary**: Multi-modal fusion is a fundamental task for the perception of an autonomous driving system, which has recently intrigued many researchers. However, achieving a rather good performance is not an easy task due to the noisy raw data, underutilized information, and the misalignment of multi-modal sensors. In this paper, we provide a literature review of the existing multi-modal-based methods for perception tasks in autonomous driving. Generally, we make a detailed analysis including over 50 papers leveraging perception sensors including LiDAR and camera trying to solve object detection and semantic segmentation tasks. Different from traditional fusion methodology for categorizing fusion models, we propose an innovative way that divides them into two major classes, four minor classes by a more reasonable taxonomy in the view of the fusion stage. Moreover, we dive deep into the current fusion methods, focusing on the remaining problems and open-up discussions on the potential research opportunities. In conclusion, what we expect to do in this paper is to present a new taxonomy of multi-modal fusion methods for the autonomous driving perception tasks and provoke thoughts of the fusion-based techniques in the future.



### Portrait Segmentation Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.02705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02705v1)
- **Published**: 2022-02-06 04:28:15+00:00
- **Updated**: 2022-02-06 04:28:15+00:00
- **Authors**: Sumedh Vilas Datar and, Jesus Gonzales Bernal
- **Comment**: None
- **Journal**: None
- **Summary**: A portrait is a painting, drawing, photograph, or engraving of a person, especially one depicting only the face or head and shoulders. In the digital world the portrait of a person is captured by having the person as a subject in the image and capturing the image of the person such that the background is blurred. DSLRs generally do it by reducing the aperture to focus on very close regions of interest and automatically blur the background. In this paper I have come up with a novel approach to replicate the portrait mode from DSLR using any smartphone to generate high quality portrait images.



### FEAT: Face Editing with Attention
- **Arxiv ID**: http://arxiv.org/abs/2202.02713v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02713v1)
- **Published**: 2022-02-06 06:07:34+00:00
- **Updated**: 2022-02-06 06:07:34+00:00
- **Authors**: Xianxu Hou, Linlin Shen, Or Patashnik, Daniel Cohen-Or, Hui Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Employing the latent space of pretrained generators has recently been shown to be an effective means for GAN-based face manipulation. The success of this approach heavily relies on the innate disentanglement of the latent space axes of the generator. However, face manipulation often intends to affect local regions only, while common generators do not tend to have the necessary spatial disentanglement. In this paper, we build on the StyleGAN generator, and present a method that explicitly encourages face manipulation to focus on the intended regions by incorporating learned attention maps. During the generation of the edited image, the attention map serves as a mask that guides a blending between the original features and the modified ones. The guidance for the latent space edits is achieved by employing CLIP, which has recently been shown to be effective for text-driven edits. We perform extensive experiments and show that our method can perform disentangled and controllable face manipulations based on text descriptions by attending to the relevant regions only. Both qualitative and quantitative experimental results demonstrate the superiority of our method for facial region editing over alternative methods.



### Enhancing variational generation through self-decomposition
- **Arxiv ID**: http://arxiv.org/abs/2202.02738v2
- **DOI**: 10.1109/ACCESS.2022.3185654
- **Categories**: **cs.CV**, cs.NE, 68T07 Artificial neural networks and deep learning, I.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2202.02738v2)
- **Published**: 2022-02-06 08:49:21+00:00
- **Updated**: 2022-07-14 10:57:30+00:00
- **Authors**: Andrea Asperti, Laura Bugo, Daniele Filippini
- **Comment**: None
- **Journal**: IEEE Access, vol. 10, pp. 67510-67520, 2022
- **Summary**: In this article we introduce the notion of Split Variational Autoencoder (SVAE), whose output $\hat{x}$ is obtained as a weighted sum $\sigma \odot \hat{x_1} + (1-\sigma) \odot \hat{x_2}$ of two generated images $\hat{x_1},\hat{x_2}$, and $\sigma$ is a {\em learned} compositional map. The composing images $\hat{x_1},\hat{x_2}$, as well as the $\sigma$-map are automatically synthesized by the model. The network is trained as a usual Variational Autoencoder with a negative loglikelihood loss between training and reconstructed images. No additional loss is required for $\hat{x_1},\hat{x_2}$ or $\sigma$, neither any form of human tuning. The decomposition is nondeterministic, but follows two main schemes, that we may roughly categorize as either \say{syntactic} or \say{semantic}. In the first case, the map tends to exploit the strong correlation between adjacent pixels, splitting the image in two complementary high frequency sub-images. In the second case, the map typically focuses on the contours of objects, splitting the image in interesting variations of its content, with more marked and distinctive features. In this case, according to empirical observations, the Fr\'echet Inception Distance (FID) of $\hat{x_1}$ and $\hat{x_2}$ is usually lower (hence better) than that of $\hat{x}$, that clearly suffers from being the average of the former. In a sense, a SVAE forces the Variational Autoencoder to make choices, in contrast with its intrinsic tendency to {\em average} between alternatives with the aim to minimize the reconstruction loss towards a specific sample. According to the FID metric, our technique, tested on typical datasets such as Mnist, Cifar10 and CelebA, allows us to outperform all previous purely variational architectures (not relying on normalization flows).



### On Smart Gaze based Annotation of Histopathology Images for Training of Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.02764v1
- **DOI**: 10.1109/JBHI.2022.3148944
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02764v1)
- **Published**: 2022-02-06 12:07:12+00:00
- **Updated**: 2022-02-06 12:07:12+00:00
- **Authors**: Komal Mariam, Osama Mohammed Afzal, Wajahat Hussain, Muhammad Umar Javed, Amber Kiyani, Nasir Rajpoot, Syed Ali Khurram, Hassan Aqeel Khan
- **Comment**: 12 pages, 10 figures, 2 tables, journal
- **Journal**: None
- **Summary**: Unavailability of large training datasets is a bottleneck that needs to be overcome to realize the true potential of deep learning in histopathology applications. Although slide digitization via whole slide imaging scanners has increased the speed of data acquisition, labeling of virtual slides requires a substantial time investment from pathologists. Eye gaze annotations have the potential to speed up the slide labeling process. This work explores the viability and timing comparisons of eye gaze labeling compared to conventional manual labeling for training object detectors. Challenges associated with gaze based labeling and methods to refine the coarse data annotations for subsequent object detection are also discussed. Results demonstrate that gaze tracking based labeling can save valuable pathologist time and delivers good performance when employed for training a deep object detector. Using the task of localization of Keratin Pearls in cases of oral squamous cell carcinoma as a test case, we compare the performance gap between deep object detectors trained using hand-labelled and gaze-labelled data. On average, compared to `Bounding-box' based hand-labeling, gaze-labeling required $57.6\%$ less time per label and compared to `Freehand' labeling, gaze-labeling required on average $85\%$ less time per label.



### Learning Features with Parameter-Free Layers
- **Arxiv ID**: http://arxiv.org/abs/2202.02777v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02777v2)
- **Published**: 2022-02-06 14:03:36+00:00
- **Updated**: 2022-03-20 14:53:13+00:00
- **Authors**: Dongyoon Han, YoungJoon Yoo, Beomyoung Kim, Byeongho Heo
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: Trainable layers such as convolutional building blocks are the standard network design choices by learning parameters to capture the global context through successive spatial operations. When designing an efficient network, trainable layers such as the depthwise convolution is the source of efficiency in the number of parameters and FLOPs, but there was little improvement to the model speed in practice. This paper argues that simple built-in parameter-free operations can be a favorable alternative to the efficient trainable layers replacing spatial operations in a network architecture. We aim to break the stereotype of organizing the spatial operations of building blocks into trainable layers. Extensive experimental analyses based on layer-level studies with fully-trained models and neural architecture searches are provided to investigate whether parameter-free operations such as the max-pool are functional. The studies eventually give us a simple yet effective idea for redesigning network architectures, where the parameter-free operations are heavily used as the main building block without sacrificing the model accuracy as much. Experimental results on the ImageNet dataset demonstrate that the network architectures with parameter-free operations could enjoy the advantages of further efficiency in terms of model speed, the number of the parameters, and FLOPs. Code and ImageNet pretrained models are available at https://github.com/naver-ai/PfLayer.



### Multi-domain Unsupervised Image-to-Image Translation with Appearance Adaptive Convolution
- **Arxiv ID**: http://arxiv.org/abs/2202.02779v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02779v1)
- **Published**: 2022-02-06 14:12:34+00:00
- **Updated**: 2022-02-06 14:12:34+00:00
- **Authors**: Somi Jeong, Jiyoung Lee, Kwanghoon Sohn
- **Comment**: ICASSP 2022
- **Journal**: None
- **Summary**: Over the past few years, image-to-image (I2I) translation methods have been proposed to translate a given image into diverse outputs. Despite the impressive results, they mainly focus on the I2I translation between two domains, so the multi-domain I2I translation still remains a challenge. To address this problem, we propose a novel multi-domain unsupervised image-to-image translation (MDUIT) framework that leverages the decomposed content feature and appearance adaptive convolution to translate an image into a target appearance while preserving the given geometric content. We also exploit a contrast learning objective, which improves the disentanglement ability and effectively utilizes multi-domain image data in the training process by pairing the semantically similar images. This allows our method to learn the diverse mappings between multiple visual domains with only a single framework. We show that the proposed method produces visually diverse and plausible results in multiple domains compared to the state-of-the-art methods.



### Energy awareness in low precision neural networks
- **Arxiv ID**: http://arxiv.org/abs/2202.02783v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02783v1)
- **Published**: 2022-02-06 14:44:55+00:00
- **Updated**: 2022-02-06 14:44:55+00:00
- **Authors**: Nurit Spingarn Eliezer, Ron Banner, Elad Hoffer, Hilla Ben-Yaakov, Tomer Michaeli
- **Comment**: None
- **Journal**: None
- **Summary**: Power consumption is a major obstacle in the deployment of deep neural networks (DNNs) on end devices. Existing approaches for reducing power consumption rely on quite general principles, including avoidance of multiplication operations and aggressive quantization of weights and activations. However, these methods do not take into account the precise power consumed by each module in the network, and are therefore not optimal. In this paper we develop accurate power consumption models for all arithmetic operations in the DNN, under various working conditions. We reveal several important factors that have been overlooked to date. Based on our analysis, we present PANN (power-aware neural network), a simple approach for approximating any full-precision network by a low-power fixed-precision variant. Our method can be applied to a pre-trained network, and can also be used during training to achieve improved performance. In contrast to previous methods, PANN incurs only a minor degradation in accuracy w.r.t. the full-precision version of the network, even when working at the power-budget of a 2-bit quantized variant. In addition, our scheme enables to seamlessly traverse the power-accuracy trade-off at deployment time, which is a major advantage over existing quantization methods that are constrained to specific bit widths.



### GLPanoDepth: Global-to-Local Panoramic Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.02796v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02796v2)
- **Published**: 2022-02-06 15:11:58+00:00
- **Updated**: 2022-02-08 11:51:33+00:00
- **Authors**: Jiayang Bai, Shuichang Lai, Haoyu Qin, Jie Guo, Yanwen Guo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a learning-based method for predicting dense depth values of a scene from a monocular omnidirectional image. An omnidirectional image has a full field-of-view, providing much more complete descriptions of the scene than perspective images. However, fully-convolutional networks that most current solutions rely on fail to capture rich global contexts from the panorama. To address this issue and also the distortion of equirectangular projection in the panorama, we propose Cubemap Vision Transformers (CViT), a new transformer-based architecture that can model long-range dependencies and extract distortion-free global features from the panorama. We show that cubemap vision transformers have a global receptive field at every stage and can provide globally coherent predictions for spherical signals. To preserve important local features, we further design a convolution-based branch in our pipeline (dubbed GLPanoDepth) and fuse global features from cubemap vision transformers at multiple scales. This global-to-local strategy allows us to fully exploit useful global and local features in the panorama, achieving state-of-the-art performance in panoramic depth estimation.



### Low-confidence Samples Matter for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2202.02802v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02802v2)
- **Published**: 2022-02-06 15:45:45+00:00
- **Updated**: 2022-03-05 02:45:45+00:00
- **Authors**: Yixin Zhang, Junjie Li, Zilei Wang
- **Comment**: 11 pages, 5 figures; an outdated version that will be updated
- **Journal**: None
- **Summary**: Domain adaptation (DA) aims to transfer knowledge from a label-rich source domain to a related but label-scarce target domain. The conventional DA strategy is to align the feature distributions of the two domains. Recently, increasing researches have focused on self-training or other semi-supervised algorithms to explore the data structure of the target domain. However, the bulk of them depend largely on confident samples in order to build reliable pseudo labels, prototypes or cluster centers. Representing the target data structure in such a way would overlook the huge low-confidence samples, resulting in sub-optimal transferability that is biased towards the samples similar to the source domain. To overcome this issue, we propose a novel contrastive learning method by processing low-confidence samples, which encourages the model to make use of the target data structure through the instance discrimination process. To be specific, we create positive and negative pairs only using low-confidence samples, and then re-represent the original features with the classifier weights rather than directly utilizing them, which can better encode the task-specific semantic information. Furthermore, we combine cross-domain mixup to augment the proposed contrastive loss. Consequently, the domain gap can be well bridged through contrastive learning of intermediate representations across domains. We evaluate the proposed method in both unsupervised and semi-supervised DA settings, and extensive experimental results on benchmarks reveal that our method is effective and achieves state-of-the-art performance. The code can be found in https://github.com/zhyx12/MixLRCo.



### A Coding Framework and Benchmark towards Compressed Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2202.02813v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02813v2)
- **Published**: 2022-02-06 16:29:15+00:00
- **Updated**: 2022-02-19 12:11:48+00:00
- **Authors**: Yuan Tian, Guo Lu, Yichao Yan, Guangtao Zhai, Li Chen, Zhiyong Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Most video understanding methods are learned on high-quality videos. However, in real-world scenarios, the videos are first compressed before the transportation and then decompressed for understanding. The decompressed videos may have lost the critical information to the downstream tasks. To address this issue, we propose the first coding framework for compressed video understanding, where another learnable analytic bitstream is simultaneously transported with the original video bitstream. With the dedicatedly designed self-supervised optimization target and dynamic network architectures, this new stream largely boosts the downstream tasks yet with a small bit cost. By only one-time training, our framework can be deployed for multiple downstream tasks. Our framework also enjoys the best of both two worlds, (1) high efficiency of industrial video codec and (2) flexible coding capability of neural networks (NNs). Finally, we build a rigorous benchmark for compressed video understanding on three popular tasks over seven large-scale datasets and four different compression levels. The proposed Understanding oriented Video Coding framework UVC consistently demonstrates significantly stronger performances than the baseline industrial codec.



### Block shuffling learning for Deepfake Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.02819v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.02819v2)
- **Published**: 2022-02-06 17:16:46+00:00
- **Updated**: 2023-07-13 09:13:40+00:00
- **Authors**: Sitong Liu, Zhichao Lian, Siqi Gu, Liang Xiao
- **Comment**: None
- **Journal**: None
- **Summary**: Deepfake detection methods based on convolutional neural networks (CNN) have demonstrated high accuracy. \textcolor{black}{However, these methods often suffer from decreased performance when faced with unknown forgery methods and common transformations such as resizing and blurring, resulting in deviations between training and testing domains.} This phenomenon, known as overfitting, poses a significant challenge. To address this issue, we propose a novel block shuffling regularization method. Firstly, our approach involves dividing the images into blocks and applying both intra-block and inter-block shuffling techniques. This process indirectly achieves weight-sharing across different dimensions. Secondly, we introduce an adversarial loss algorithm to mitigate the overfitting problem induced by the shuffling noise. Finally, we restore the spatial layout of the blocks to capture the semantic associations among them. Extensive experiments validate the effectiveness of our proposed method, which surpasses existing approaches in forgery face detection. Notably, our method exhibits excellent generalization capabilities, demonstrating robustness against cross-dataset evaluations and common image transformations. Especially our method can be easily integrated with various CNN models. Source code is available at \href{https://github.com/NoWindButRain/BlockShuffleLearning}{Github}.



### Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/2202.02832v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02832v4)
- **Published**: 2022-02-06 18:53:06+00:00
- **Updated**: 2022-07-29 08:20:11+00:00
- **Authors**: Peter J. Bevan, Amir Atapour-Abarghouei
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Convolutional Neural Networks have demonstrated human-level performance in the classification of melanoma and other skin lesions, but evident performance disparities between differing skin tones should be addressed before widespread deployment. In this work, we propose an efficient yet effective algorithm for automatically labelling the skin tone of lesion images, and use this to annotate the benchmark ISIC dataset. We subsequently use these automated labels as the target for two leading bias unlearning techniques towards mitigating skin tone bias. Our experimental results provide evidence that our skin tone detection algorithm outperforms existing solutions and that unlearning skin tone may improve generalisation and can reduce the performance disparity between melanoma detection in lighter and darker skin tones.



### CheXstray: Real-time Multi-Modal Data Concordance for Drift Detection in Medical Imaging AI
- **Arxiv ID**: http://arxiv.org/abs/2202.02833v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02833v2)
- **Published**: 2022-02-06 18:58:35+00:00
- **Updated**: 2022-03-17 21:24:19+00:00
- **Authors**: Arjun Soin, Jameson Merkow, Jin Long, Joseph Paul Cohen, Smitha Saligrama, Stephen Kaiser, Steven Borg, Ivan Tarapov, Matthew P Lungren
- **Comment**: Added code url
- **Journal**: None
- **Summary**: Clinical Artificial lntelligence (AI) applications are rapidly expanding worldwide, and have the potential to impact to all areas of medical practice. Medical imaging applications constitute a vast majority of approved clinical AI applications. Though healthcare systems are eager to adopt AI solutions a fundamental question remains: \textit{what happens after the AI model goes into production?} We use the CheXpert and PadChest public datasets to build and test a medical imaging AI drift monitoring workflow to track data and model drift without contemporaneous ground truth. We simulate drift in multiple experiments to compare model performance with our novel multi-modal drift metric, which uses DICOM metadata, image appearance representation from a variational autoencoder (VAE), and model output probabilities as input. Through experimentation, we demonstrate a strong proxy for ground truth performance using unsupervised distributional shifts in relevant metadata, predicted probabilities, and VAE latent representation. Our key contributions include (1) proof-of-concept for medical imaging drift detection that includes the use of VAE and domain specific statistical methods, (2) a multi-modal methodology to measure and unify drift metrics, (3) new insights into the challenges and solutions to observe deployed medical imaging AI, and (4) creation of open-source tools that enable others to easily run their own workflows and scenarios. This work has important implications. It addresses the concerning translation gap found in continuous medical imaging AI model monitoring common in dynamic healthcare environments.



