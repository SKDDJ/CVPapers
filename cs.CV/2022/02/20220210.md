# Arxiv Papers in cs.CV on 2022-02-10
### Adversarial Attack and Defense of YOLO Detectors in Autonomous Driving Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2202.04781v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04781v2)
- **Published**: 2022-02-10 00:47:36+00:00
- **Updated**: 2022-07-03 13:29:54+00:00
- **Authors**: Jung Im Choi, Qing Tian
- **Comment**: Accepted by 2022 IEEE Intelligent Vehicles Symposium (IV 2022)
- **Journal**: None
- **Summary**: Visual detection is a key task in autonomous driving, and it serves as a crucial foundation for self-driving planning and control. Deep neural networks have achieved promising results in various visual tasks, but they are known to be vulnerable to adversarial attacks. A comprehensive understanding of deep visual detectors' vulnerability is required before people can improve their robustness. However, only a few adversarial attack/defense works have focused on object detection, and most of them employed only classification and/or localization losses, ignoring the objectness aspect. In this paper, we identify a serious objectness-related adversarial vulnerability in YOLO detectors and present an effective attack strategy targeting the objectness aspect of visual detection in autonomous vehicles. Furthermore, to address such vulnerability, we propose a new objectness-aware adversarial training approach for visual detection. Experiments show that the proposed attack targeting the objectness aspect is 45.17% and 43.50% more effective than those generated from classification and/or localization losses on the KITTI and COCO traffic datasets, respectively. Also, the proposed adversarial defense approach can improve the detectors' robustness against objectness-oriented attacks by up to 21% and 12% mAP on KITTI and COCO traffic, respectively.



### Multiclass histogram-based thresholding using kernel density estimation and scale-space representations
- **Arxiv ID**: http://arxiv.org/abs/2202.04785v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.04785v1)
- **Published**: 2022-02-10 01:03:43+00:00
- **Updated**: 2022-02-10 01:03:43+00:00
- **Authors**: S. Korneev, J. Gilles, I. Battiato
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new method for multiclass thresholding of a histogram which is based on the nonparametric Kernel Density (KD) estimation, where the unknown parameters of the KD estimate are defined using the Expectation-Maximization (EM) iterations. The method compares the number of extracted minima of the KD estimate with the number of the requested clusters minus one. If these numbers match, the algorithm returns positions of the minima as the threshold values, otherwise, the method gradually decreases/increases the kernel bandwidth until the numbers match. We verify the method using synthetic histograms with known threshold values and using the histogram of real X-ray computed tomography images. After thresholding of the real histogram, we estimated the porosity of the sample and compare it with the direct experimental measurements. The comparison shows the meaningfulness of the thresholding.



### The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2202.04800v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2202.04800v2)
- **Published**: 2022-02-10 02:26:45+00:00
- **Updated**: 2022-07-25 17:26:06+00:00
- **Authors**: Jack Hessel, Jena D. Hwang, Jae Sung Park, Rowan Zellers, Chandra Bhagavatula, Anna Rohrbach, Kate Saenko, Yejin Choi
- **Comment**: code, data, models at http://visualabduction.com/
- **Journal**: ECCV 2022
- **Summary**: Humans have remarkable capacity to reason abductively and hypothesize about what lies beyond the literal content of an image. By identifying concrete visual clues scattered throughout a scene, we almost can't help but draw probable inferences beyond the literal scene based on our everyday experience and knowledge about the world. For example, if we see a "20 mph" sign alongside a road, we might assume the street sits in a residential area (rather than on a highway), even if no houses are pictured. Can machines perform similar visual reasoning?   We present Sherlock, an annotated corpus of 103K images for testing machine capacity for abductive reasoning beyond literal image contents. We adopt a free-viewing paradigm: participants first observe and identify salient clues within images (e.g., objects, actions) and then provide a plausible inference about the scene, given the clue. In total, we collect 363K (clue, inference) pairs, which form a first-of-its-kind abductive visual reasoning dataset. Using our corpus, we test three complementary axes of abductive reasoning. We evaluate the capacity of models to: i) retrieve relevant inferences from a large candidate corpus; ii) localize evidence for inferences via bounding boxes, and iii) compare plausible inferences to match human judgments on a newly-collected diagnostic corpus of 19K Likert-scale judgments. While we find that fine-tuning CLIP-RN50x64 with a multitask objective outperforms strong baselines, significant headroom exists between model performance and human agreement. Data, models, and leaderboard available at http://visualabduction.com/



### On Real-time Image Reconstruction with Neural Networks for MRI-guided Radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/2202.05267v2
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.05267v2)
- **Published**: 2022-02-10 02:43:39+00:00
- **Updated**: 2022-05-19 02:39:58+00:00
- **Authors**: David E. J. Waddington, Nicholas Hindley, Neha Koonjoo, Christopher Chiu, Tess Reynolds, Paul Z. Y. Liu, Bo Zhu, Danyal Bhutto, Chiara Paganelli, Paul J. Keall, Matthew S. Rosen
- **Comment**: 12 pages, 6 figures, 1 table. v2 has a typo in eqn 1 corrected and
  references added to the discussion
- **Journal**: None
- **Summary**: MRI-guidance techniques that dynamically adapt radiation beams to follow tumor motion in real-time will lead to more accurate cancer treatments and reduced collateral healthy tissue damage. The gold-standard for reconstruction of undersampled MR data is compressed sensing (CS) which is computationally slow and limits the rate that images can be available for real-time adaptation. Here, we demonstrate the use of automated transform by manifold approximation (AUTOMAP), a generalized framework that maps raw MR signal to the target image domain, to rapidly reconstruct images from undersampled radial k-space data. The AUTOMAP neural network was trained to reconstruct images from a golden-angle radial acquisition, a benchmark for motion-sensitive imaging, on lung cancer patient data and generic images from ImageNet. Model training was subsequently augmented with motion-encoded k-space data derived from videos in the YouTube-8M dataset to encourage motion robust reconstruction. We find that AUTOMAP-reconstructed radial k-space has equivalent accuracy to CS but with much shorter processing times after initial fine-tuning on retrospectively acquired lung cancer patient data. Validation of motion-trained models with a virtual dynamic lung tumor phantom showed that the generalized motion properties learned from YouTube lead to improved target tracking accuracy. Our work shows that AUTOMAP can achieve real-time, accurate reconstruction of radial data. These findings imply that neural-network-based reconstruction is potentially superior to existing approaches for real-time image guidance applications.



### Weakly-Supervised Semantic Segmentation with Visual Words Learning and Hybrid Pooling
- **Arxiv ID**: http://arxiv.org/abs/2202.04812v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04812v1)
- **Published**: 2022-02-10 03:19:08+00:00
- **Updated**: 2022-02-10 03:19:08+00:00
- **Authors**: Lixiang Ru, Bo Du, Yibing Zhan, Chen Wu
- **Comment**: Accepted to IJCV
- **Journal**: None
- **Summary**: Weakly-Supervised Semantic Segmentation (WSSS) methods with image-level labels generally train a classification network to generate the Class Activation Maps (CAMs) as the initial coarse segmentation labels. However, current WSSS methods still perform far from satisfactorily because their adopted CAMs 1) typically focus on partial discriminative object regions and 2) usually contain useless background regions. These two problems are attributed to the sole image-level supervision and aggregation of global information when training the classification networks. In this work, we propose the visual words learning module and hybrid pooling approach, and incorporate them in the classification network to mitigate the above problems. In the visual words learning module, we counter the first problem by enforcing the classification network to learn fine-grained visual word labels so that more object extents could be discovered. Specifically, the visual words are learned with a codebook, which could be updated via two proposed strategies, i.e. learning-based strategy and memory-bank strategy. The second drawback of CAMs is alleviated with the proposed hybrid pooling, which incorporates the global average and local discriminative information to simultaneously ensure object completeness and reduce background regions. We evaluated our methods on PASCAL VOC 2012 and MS COCO 2014 datasets. Without any extra saliency prior, our method achieved 70.6% and 70.7% mIoU on the $val$ and $test$ set of PASCAL VOC dataset, respectively, and 36.2% mIoU on the $val$ set of MS COCO dataset, which significantly surpassed the performance of state-of-the-art WSSS methods.



### Decreasing Annotation Burden of Pairwise Comparisons with Human-in-the-Loop Sorting: Application in Medical Image Artifact Rating
- **Arxiv ID**: http://arxiv.org/abs/2202.04823v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV, I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2202.04823v1)
- **Published**: 2022-02-10 04:02:45+00:00
- **Updated**: 2022-02-10 04:02:45+00:00
- **Authors**: Ikbeom Jang, Garrison Danley, Ken Chang, Jayashree Kalpathy-Cramer
- **Comment**: 5 pages, 2 figures, NeurIPS Data-Centric AI Workshop 2021
- **Journal**: None
- **Summary**: Ranking by pairwise comparisons has shown improved reliability over ordinal classification. However, as the annotations of pairwise comparisons scale quadratically, this becomes less practical when the dataset is large. We propose a method for reducing the number of pairwise comparisons required to rank by a quantitative metric, demonstrating the effectiveness of the approach in ranking medical images by image quality in this proof of concept study. Using the medical image annotation software that we developed, we actively subsample pairwise comparisons using a sorting algorithm with a human rater in the loop. We find that this method substantially reduces the number of comparisons required for a full ordinal ranking without compromising inter-rater reliability when compared to pairwise comparisons without sorting.



### Bias-Eliminated Semantic Refinement for Any-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.04827v1
- **DOI**: 10.1109/TIP.2022.3152631
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.04827v1)
- **Published**: 2022-02-10 04:15:50+00:00
- **Updated**: 2022-02-10 04:15:50+00:00
- **Authors**: Liangjun Feng, Chunhui Zhao, Xi Li
- **Comment**: None
- **Journal**: None
- **Summary**: When training samples are scarce, the semantic embedding technique, ie, describing class labels with attributes, provides a condition to generate visual features for unseen objects by transferring the knowledge from seen objects. However, semantic descriptions are usually obtained in an external paradigm, such as manual annotation, resulting in weak consistency between descriptions and visual features. In this paper, we refine the coarse-grained semantic description for any-shot learning tasks, ie, zero-shot learning (ZSL), generalized zero-shot learning (GZSL), and few-shot learning (FSL). A new model, namely, the semantic refinement Wasserstein generative adversarial network (SRWGAN) model, is designed with the proposed multihead representation and hierarchical alignment techniques. Unlike conventional methods, semantic refinement is performed with the aim of identifying a bias-eliminated condition for disjoint-class feature generation and is applicable in both inductive and transductive settings. We extensively evaluate model performance on six benchmark datasets and observe state-of-the-art results for any-shot learning; eg, we obtain 70.2% harmonic accuracy for the Caltech UCSD Birds (CUB) dataset and 82.2% harmonic accuracy for the Oxford Flowers (FLO) dataset in the standard GZSL setting. Various visualizations are also provided to show the bias-eliminated generation of SRWGAN. Our code is available.



### Geometric Digital Twinning of Industrial Facilities: Retrieval of Industrial Shapes
- **Arxiv ID**: http://arxiv.org/abs/2202.04834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04834v1)
- **Published**: 2022-02-10 04:47:47+00:00
- **Updated**: 2022-02-10 04:47:47+00:00
- **Authors**: Eva Agapaki, Ioannis Brilakis
- **Comment**: None
- **Journal**: None
- **Summary**: This paper devises, implements and benchmarks a novel shape retrieval method that can accurately match individual labelled point clusters (instances) of existing industrial facilities with their respective CAD models. It employs a combination of image and point cloud deep learning networks to classify and match instances to their geometrically similar CAD model. It extends our previous research on geometric digital twin generation from point cloud data, which currently is a tedious, manual process. Experiments with our joint network reveal that it can reliably retrieve CAD models at 85.2\% accuracy. The proposed research is a fundamental framework to enable the geometric Digital Twin (gDT) pipeline and incorporate the real geometric configuration into the Digital Twin.



### Consistency and Diversity induced Human Motion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.04861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04861v1)
- **Published**: 2022-02-10 06:23:56+00:00
- **Updated**: 2022-02-10 06:23:56+00:00
- **Authors**: Tao Zhou, Huazhu Fu, Chen Gong, Ling Shao, Fatih Porikli, Haibin Ling, Jianbing Shen
- **Comment**: This paper has been accepted by IEEE TPAMI
- **Journal**: None
- **Summary**: Subspace clustering is a classical technique that has been widely used for human motion segmentation and other related tasks. However, existing segmentation methods often cluster data without guidance from prior knowledge, resulting in unsatisfactory segmentation results. To this end, we propose a novel Consistency and Diversity induced human Motion Segmentation (CDMS) algorithm. Specifically, our model factorizes the source and target data into distinct multi-layer feature spaces, in which transfer subspace learning is conducted on different layers to capture multi-level information. A multi-mutual consistency learning strategy is carried out to reduce the domain gap between the source and target data. In this way, the domain-specific knowledge and domain-invariant properties can be explored simultaneously. Besides, a novel constraint based on the Hilbert Schmidt Independence Criterion (HSIC) is introduced to ensure the diversity of multi-level subspace representations, which enables the complementarity of multi-level representations to be explored to boost the transfer learning performance. Moreover, to preserve the temporal correlations, an enhanced graph regularizer is imposed on the learned representation coefficients and the multi-level representations of the source data. The proposed model can be efficiently solved using the Alternating Direction Method of Multipliers (ADMM) algorithm. Extensive experimental results on public human motion datasets demonstrate the effectiveness of our method against several state-of-the-art approaches.



### HNF-Netv2 for Brain Tumor Segmentation using multi-modal MR Imaging
- **Arxiv ID**: http://arxiv.org/abs/2202.05268v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.05268v1)
- **Published**: 2022-02-10 06:34:32+00:00
- **Updated**: 2022-02-10 06:34:32+00:00
- **Authors**: Haozhe Jia, Chao Bai, Weidong Cai, Heng Huang, Yong Xia
- **Comment**: RSNA 2021 Brain Tumor AI Challenge Top Solution. arXiv admin note:
  substantial text overlap with arXiv:2012.15318
- **Journal**: None
- **Summary**: In our previous work, $i.e.$, HNF-Net, high-resolution feature representation and light-weight non-local self-attention mechanism are exploited for brain tumor segmentation using multi-modal MR imaging. In this paper, we extend our HNF-Net to HNF-Netv2 by adding inter-scale and intra-scale semantic discrimination enhancing blocks to further exploit global semantic discrimination for the obtained high-resolution features. We trained and evaluated our HNF-Netv2 on the multi-modal Brain Tumor Segmentation Challenge (BraTS) 2021 dataset. The result on the test set shows that our HNF-Netv2 achieved the average Dice scores of 0.878514, 0.872985, and 0.924919, as well as the Hausdorff distances ($95\%$) of 8.9184, 16.2530, and 4.4895 for the enhancing tumor, tumor core, and whole tumor, respectively. Our method won the RSNA 2021 Brain Tumor AI Challenge Prize (Segmentation Task), which ranks 8th out of all 1250 submitted results.



### Memory-based gaze prediction in deep imitation learning for robot manipulation
- **Arxiv ID**: http://arxiv.org/abs/2202.04877v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.04877v1)
- **Published**: 2022-02-10 07:30:08+00:00
- **Updated**: 2022-02-10 07:30:08+00:00
- **Authors**: Heecheol Kim, Yoshiyuki Ohmura, Yasuo Kuniyoshi
- **Comment**: 7 pages. Accepted in 2022 IEEE/RSJ International Conference on
  Robotics and Automation (ICRA)
- **Journal**: None
- **Summary**: Deep imitation learning is a promising approach that does not require hard-coded control rules in autonomous robot manipulation. The current applications of deep imitation learning to robot manipulation have been limited to reactive control based on the states at the current time step. However, future robots will also be required to solve tasks utilizing their memory obtained by experience in complicated environments (e.g., when the robot is asked to find a previously used object on a shelf). In such a situation, simple deep imitation learning may fail because of distractions caused by complicated environments. We propose that gaze prediction from sequential visual input enables the robot to perform a manipulation task that requires memory. The proposed algorithm uses a Transformer-based self-attention architecture for the gaze estimation based on sequential data to implement memory. The proposed method was evaluated with a real robot multi-object manipulation task that requires memory of the previous states.



### PVSeRF: Joint Pixel-, Voxel- and Surface-Aligned Radiance Field for Single-Image Novel View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2202.04879v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04879v1)
- **Published**: 2022-02-10 07:39:47+00:00
- **Updated**: 2022-02-10 07:39:47+00:00
- **Authors**: Xianggang Yu, Jiapeng Tang, Yipeng Qin, Chenghong Li, Linchao Bao, Xiaoguang Han, Shuguang Cui
- **Comment**: None
- **Journal**: None
- **Summary**: We present PVSeRF, a learning framework that reconstructs neural radiance fields from single-view RGB images, for novel view synthesis. Previous solutions, such as pixelNeRF, rely only on pixel-aligned features and suffer from feature ambiguity issues. As a result, they struggle with the disentanglement of geometry and appearance, leading to implausible geometries and blurry results. To address this challenge, we propose to incorporate explicit geometry reasoning and combine it with pixel-aligned features for radiance field prediction. Specifically, in addition to pixel-aligned features, we further constrain the radiance field learning to be conditioned on i) voxel-aligned features learned from a coarse volumetric grid and ii) fine surface-aligned features extracted from a regressed point cloud. We show that the introduction of such geometry-aware features helps to achieve a better disentanglement between appearance and geometry, i.e. recovering more accurate geometries and synthesizing higher quality images of novel views. Extensive experiments against state-of-the-art methods on ShapeNet benchmarks demonstrate the superiority of our approach for single-image novel view synthesis.



### Towards the automated large-scale reconstruction of past road networks from historical maps
- **Arxiv ID**: http://arxiv.org/abs/2202.04883v2
- **DOI**: 10.1016/j.compenvurbsys.2022.101794
- **Categories**: **cs.CV**, physics.soc-ph
- **Links**: [PDF](http://arxiv.org/pdf/2202.04883v2)
- **Published**: 2022-02-10 07:51:10+00:00
- **Updated**: 2022-02-11 09:44:02+00:00
- **Authors**: Johannes H. Uhl, Stefan Leyk, Yao-Yi Chiang, Craig A. Knoblock
- **Comment**: 36 pages, 22 figures
- **Journal**: None
- **Summary**: Transportation infrastructure, such as road or railroad networks, represent a fundamental component of our civilization. For sustainable planning and informed decision making, a thorough understanding of the long-term evolution of transportation infrastructure such as road networks is crucial. However, spatially explicit, multi-temporal road network data covering large spatial extents are scarce and rarely available prior to the 2000s. Herein, we propose a framework that employs increasingly available scanned and georeferenced historical map series to reconstruct past road networks, by integrating abundant, contemporary road network data and color information extracted from historical maps. Specifically, our method uses contemporary road segments as analytical units and extracts historical roads by inferring their existence in historical map series based on image processing and clustering techniques. We tested our method on over 300,000 road segments representing more than 50,000 km of the road network in the United States, extending across three study areas that cover 53 historical topographic map sheets dated between 1890 and 1950. We evaluated our approach by comparison to other historical datasets and against manually created reference data, achieving F-1 scores of up to 0.95, and showed that the extracted road network statistics are highly plausible over time, i.e., following general growth patterns. We demonstrated that contemporary geospatial data integrated with information extracted from historical map series open up new avenues for the quantitative analysis of long-term urbanization processes and landscape changes far beyond the era of operational remote sensing and digital cartography.



### Improving performance of aircraft detection in satellite imagery while limiting the labelling effort: Hybrid active learning
- **Arxiv ID**: http://arxiv.org/abs/2202.04890v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2202.04890v1)
- **Published**: 2022-02-10 08:24:07+00:00
- **Updated**: 2022-02-10 08:24:07+00:00
- **Authors**: Julie Imbert, Gohar Dashyan, Alex Goupilleau, Tugdual Ceillier, Marie-Caroline Corbineau
- **Comment**: None
- **Journal**: International Symposium on Geoscience and Remote Sensing (IGARSS),
  Jul 2021, Brussels, Belgium
- **Summary**: The earth observation industry provides satellite imagery with high spatial resolution and short revisit time. To allow efficient operational employment of these images, automating certain tasks has become necessary. In the defense domain, aircraft detection on satellite imagery is a valuable tool for analysts. Obtaining high performance detectors on such a task can only be achieved by leveraging deep learning and thus us-ing a large amount of labeled data. To obtain labels of a high enough quality, the knowledge of military experts is needed.We propose a hybrid clustering active learning method to select the most relevant data to label, thus limiting the amount of data required and further improving the performances. It combines diversity- and uncertainty-based active learning selection methods. For aircraft detection by segmentation, we show that this method can provide better or competitive results compared to other active learning methods.



### FILM: Frame Interpolation for Large Motion
- **Arxiv ID**: http://arxiv.org/abs/2202.04901v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04901v4)
- **Published**: 2022-02-10 08:48:18+00:00
- **Updated**: 2022-07-16 04:41:01+00:00
- **Authors**: Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru, Brian Curless
- **Comment**: Accepted to ECCV 2022. Project website: https://film-net.github.io.
  Code: https://github.com/google-research/frame-interpolation. YouTube:
  https://www.youtube.com/watch?v=OAD-BieIjH4
- **Journal**: None
- **Summary**: We present a frame interpolation algorithm that synthesizes multiple intermediate frames from two input images with large in-between motion. Recent methods use multiple networks to estimate optical flow or depth and a separate network dedicated to frame synthesis. This is often complex and requires scarce optical flow or depth ground-truth. In this work, we present a single unified network, distinguished by a multi-scale feature extractor that shares weights at all scales, and is trainable from frames alone. To synthesize crisp and pleasing frames, we propose to optimize our network with the Gram matrix loss that measures the correlation difference between feature maps. Our approach outperforms state-of-the-art methods on the Xiph large motion benchmark. We also achieve higher scores on Vimeo-90K, Middlebury and UCF101, when comparing to methods that use perceptual losses. We study the effect of weight sharing and of training with datasets of increasing motion range. Finally, we demonstrate our model's effectiveness in synthesizing high quality and temporally coherent videos on a challenging near-duplicate photos dataset. Codes and pre-trained models are available at https://film-net.github.io.



### A Plug-and-Play Approach to Multiparametric Quantitative MRI: Image Reconstruction using Pre-Trained Deep Denoisers
- **Arxiv ID**: http://arxiv.org/abs/2202.05269v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.05269v1)
- **Published**: 2022-02-10 09:35:25+00:00
- **Updated**: 2022-02-10 09:35:25+00:00
- **Authors**: Ketan Fatania, Carolin M. Pirkl, Marion I. Menzel, Peter Hall, Mohammad Golbabaee
- **Comment**: None
- **Journal**: None
- **Summary**: Current spatiotemporal deep learning approaches to Magnetic Resonance Fingerprinting (MRF) build artefact-removal models customised to a particular k-space subsampling pattern which is used for fast (compressed) acquisition. This may not be useful when the acquisition process is unknown during training of the deep learning model and/or changes during testing time. This paper proposes an iterative deep learning plug-and-play reconstruction approach to MRF which is adaptive to the forward acquisition process. Spatiotemporal image priors are learned by an image denoiser i.e. a Convolutional Neural Network (CNN), trained to remove generic white gaussian noise (not a particular subsampling artefact) from data. This CNN denoiser is then used as a data-driven shrinkage operator within the iterative reconstruction algorithm. This algorithm with the same denoiser model is then tested on two simulated acquisition processes with distinct subsampling patterns. The results show consistent de-aliasing performance against both acquisition schemes and accurate mapping of tissues' quantitative bio-properties. Software available: https://github.com/ketanfatania/QMRI-PnP-Recon-POC



### Spherical Transformer
- **Arxiv ID**: http://arxiv.org/abs/2202.04942v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4; I.2
- **Links**: [PDF](http://arxiv.org/pdf/2202.04942v2)
- **Published**: 2022-02-10 10:24:24+00:00
- **Updated**: 2022-02-11 07:29:03+00:00
- **Authors**: Sungmin Cho, Raehyuk Jung, Junseok Kwon
- **Comment**: 9 pages with 10 figures
- **Journal**: None
- **Summary**: Using convolutional neural networks for 360images can induce sub-optimal performance due to distortions entailed by a planar projection. The distortion gets deteriorated when a rotation is applied to the 360image. Thus, many researches based on convolutions attempt to reduce the distortions to learn accurate representation. In contrast, we leverage the transformer architecture to solve image classification problems for 360images. Using the proposed transformer for 360images has two advantages. First, our method does not require the erroneous planar projection process by sampling pixels from the sphere surface. Second, our sampling method based on regular polyhedrons makes low rotation equivariance errors, because specific rotations can be reduced to permutations of faces. In experiments, we validate our network on two aspects, as follows. First, we show that using a transformer with highly uniform sampling methods can help reduce the distortion. Second, we demonstrate that the transformer architecture can achieve rotation equivariance on specific rotations. We compare our method to other state-of-the-art algorithms using the SPH-MNIST, SPH-CIFAR, and SUN360 datasets and show that our method is competitive with other methods.



### OWL (Observe, Watch, Listen): Audiovisual Temporal Context for Localizing Actions in Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/2202.04947v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2202.04947v3)
- **Published**: 2022-02-10 10:50:52+00:00
- **Updated**: 2022-10-26 13:24:39+00:00
- **Authors**: Merey Ramazanova, Victor Escorcia, Fabian Caba Heilbron, Chen Zhao, Bernard Ghanem
- **Comment**: None
- **Journal**: None
- **Summary**: Egocentric videos capture sequences of human activities from a first-person perspective and can provide rich multimodal signals. However, most current localization methods use third-person videos and only incorporate visual information. In this work, we take a deep look into the effectiveness of audiovisual context in detecting actions in egocentric videos and introduce a simple-yet-effective approach via Observing, Watching, and Listening (OWL). OWL leverages audiovisual information and context for egocentric temporal action localization (TAL). We validate our approach in two large-scale datasets, EPIC-Kitchens, and HOMAGE. Extensive experiments demonstrate the relevance of the audiovisual temporal context. Namely, we boost the localization performance (mAP) over visual-only models by +2.23% and +3.35% in the above datasets.



### A Deep Learning Approach for Digital Color Reconstruction of Lenticular Films
- **Arxiv ID**: http://arxiv.org/abs/2202.05270v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.05270v2)
- **Published**: 2022-02-10 11:08:50+00:00
- **Updated**: 2022-04-04 13:28:46+00:00
- **Authors**: Stefano D'Aronco, Giorgio Trumpy, David Pfluger, Jan Dirk Wegner
- **Comment**: None
- **Journal**: None
- **Summary**: We propose the first accurate digitization and color reconstruction process for historical lenticular film that is robust to artifacts. Lenticular films emerged in the 1920s and were one of the first technologies that permitted to capture full color information in motion. The technology leverages an RGB filter and cylindrical lenticules embossed on the film surface to encode the color in the horizontal spatial dimension of the image. To project the pictures the encoding process was reversed using an appropriate analog device. In this work, we introduce an automated, fully digital pipeline to process the scan of lenticular films and colorize the image. Our method merges deep learning with a model-based approach in order to maximize the performance while making sure that the reconstructed colored images truthfully match the encoded color information. Our model employs different strategies to achieve an effective color reconstruction, in particular (i) we use data augmentation to create a robust lenticule segmentation network, (ii) we fit the lenticules raster prediction to obtain a precise vectorial lenticule localization, and (iii) we train a colorization network that predicts interpolation coefficients in order to obtain a truthful colorization. We validate the proposed method on a lenticular film dataset and compare it to other approaches. Since no colored groundtruth is available as reference, we conduct a user study to validate our method in a subjective manner. The results of the study show that the proposed method is largely preferred with respect to other existing and baseline methods.



### Monotonically Convergent Regularization by Denoising
- **Arxiv ID**: http://arxiv.org/abs/2202.04961v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.04961v1)
- **Published**: 2022-02-10 11:32:41+00:00
- **Updated**: 2022-02-10 11:32:41+00:00
- **Authors**: Yuyang Hu, Jiaming Liu, Xiaojian Xu, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: Regularization by denoising (RED) is a widely-used framework for solving inverse problems by leveraging image denoisers as image priors. Recent work has reported the state-of-the-art performance of RED in a number of imaging applications using pre-trained deep neural nets as denoisers. Despite the recent progress, the stable convergence of RED algorithms remains an open problem. The existing RED theory only guarantees stability for convex data-fidelity terms and nonexpansive denoisers. This work addresses this issue by developing a new monotone RED (MRED) algorithm, whose convergence does not require nonexpansiveness of the deep denoising prior. Simulations on image deblurring and compressive sensing recovery from random matrices show the stability of MRED even when the traditional RED algorithm diverges.



### Real-Time Siamese Multiple Object Tracker with Enhanced Proposals
- **Arxiv ID**: http://arxiv.org/abs/2202.04966v2
- **DOI**: 10.1016/j.patcog.2022.109141
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04966v2)
- **Published**: 2022-02-10 11:41:27+00:00
- **Updated**: 2022-11-08 10:33:32+00:00
- **Authors**: Lorenzo Vaquero, Víctor M. Brea, Manuel Mucientes
- **Comment**: Accepted at Pattern Recognition. Code available at
  https://github.com/lorenzovaquero/SiamMOTION
- **Journal**: None
- **Summary**: Maintaining the identity of multiple objects in real-time video is a challenging task, as it is not always feasible to run a detector on every frame. Thus, motion estimation systems are often employed, which either do not scale well with the number of targets or produce features with limited semantic information. To solve the aforementioned problems and allow the tracking of dozens of arbitrary objects in real-time, we propose SiamMOTION. SiamMOTION includes a novel proposal engine that produces quality features through an attention mechanism and a region-of-interest extractor fed by an inertia module and powered by a feature pyramid network. Finally, the extracted tensors enter a comparison head that efficiently matches pairs of exemplars and search areas, generating quality predictions via a pairwise depthwise region proposal network and a multi-object penalization module. SiamMOTION has been validated on five public benchmarks, achieving leading performance against current state-of-the-art trackers. Code available at: https://github.com/lorenzovaquero/SiamMOTION



### A Field of Experts Prior for Adapting Neural Networks at Test Time
- **Arxiv ID**: http://arxiv.org/abs/2202.05271v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2202.05271v1)
- **Published**: 2022-02-10 11:44:45+00:00
- **Updated**: 2022-02-10 11:44:45+00:00
- **Authors**: Neerav Karani, Georg Brunner, Ertunc Erdil, Simin Fei, Kerem Tezcan, Krishna Chaitanya, Ender Konukoglu
- **Comment**: Manuscript under review
- **Journal**: None
- **Summary**: Performance of convolutional neural networks (CNNs) in image analysis tasks is often marred in the presence of acquisition-related distribution shifts between training and test images. Recently, it has been proposed to tackle this problem by fine-tuning trained CNNs for each test image. Such test-time-adaptation (TTA) is a promising and practical strategy for improving robustness to distribution shifts as it requires neither data sharing between institutions nor annotating additional data. Previous TTA methods use a helper model to increase similarity between outputs and/or features extracted from a test image with those of the training images. Such helpers, which are typically modeled using CNNs, can be task-specific and themselves vulnerable to distribution shifts in their inputs. To overcome these problems, we propose to carry out TTA by matching the feature distributions of test and training images, as modelled by a field-of-experts (FoE) prior. FoEs model complicated probability distributions as products of many simpler expert distributions. We use 1D marginal distributions of a trained task CNN's features as experts in the FoE model. Further, we compute principal components of patches of the task CNN's features, and consider the distributions of PCA loadings as additional experts. We validate the method on 5 MRI segmentation tasks (healthy tissues in 4 anatomical regions and lesions in 1 one anatomy), using data from 17 clinics, and on a MRI registration task, using data from 3 clinics. We find that the proposed FoE-based TTA is generically applicable in multiple tasks, and outperforms all previous TTA methods for lesion segmentation. For healthy tissue segmentation, the proposed method outperforms other task-agnostic methods, but a previous TTA method which is specifically designed for segmentation performs the best for most of the tested datasets. Our code is publicly available.



### Towards Assessing and Characterizing the Semantic Robustness of Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.04978v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.04978v1)
- **Published**: 2022-02-10 12:22:09+00:00
- **Updated**: 2022-02-10 12:22:09+00:00
- **Authors**: Juan C. Pérez, Motasem Alfarra, Ali Thabet, Pablo Arbeláez, Bernard Ghanem
- **Comment**: 26 pages, 18 figures
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) lack robustness against imperceptible perturbations to their input. Face Recognition Models (FRMs) based on DNNs inherit this vulnerability. We propose a methodology for assessing and characterizing the robustness of FRMs against semantic perturbations to their input. Our methodology causes FRMs to malfunction by designing adversarial attacks that search for identity-preserving modifications to faces. In particular, given a face, our attacks find identity-preserving variants of the face such that an FRM fails to recognize the images belonging to the same identity. We model these identity-preserving semantic modifications via direction- and magnitude-constrained perturbations in the latent space of StyleGAN. We further propose to characterize the semantic robustness of an FRM by statistically describing the perturbations that induce the FRM to malfunction. Finally, we combine our methodology with a certification technique, thus providing (i) theoretical guarantees on the performance of an FRM, and (ii) a formal description of how an FRM may model the notion of face identity.



### NÜWA-LIP: Language Guided Image Inpainting with Defect-free VQGAN
- **Arxiv ID**: http://arxiv.org/abs/2202.05009v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2202.05009v1)
- **Published**: 2022-02-10 13:10:23+00:00
- **Updated**: 2022-02-10 13:10:23+00:00
- **Authors**: Minheng Ni, Chenfei Wu, Haoyang Huang, Daxin Jiang, Wangmeng Zuo, Nan Duan
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Language guided image inpainting aims to fill in the defective regions of an image under the guidance of text while keeping non-defective regions unchanged. However, the encoding process of existing models suffers from either receptive spreading of defective regions or information loss of non-defective regions, giving rise to visually unappealing inpainting results. To address the above issues, this paper proposes N\"UWA-LIP by incorporating defect-free VQGAN (DF-VQGAN) with multi-perspective sequence to sequence (MP-S2S). In particular, DF-VQGAN introduces relative estimation to control receptive spreading and adopts symmetrical connections to protect information. MP-S2S further enhances visual information from complementary perspectives, including both low-level pixels and high-level tokens. Experiments show that DF-VQGAN performs more robustness than VQGAN. To evaluate the inpainting performance of our model, we built up 3 open-domain benchmarks, where N\"UWA-LIP is also superior to recent strong baselines.



### Towards a Guideline for Evaluation Metrics in Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.05273v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.05273v1)
- **Published**: 2022-02-10 13:38:05+00:00
- **Updated**: 2022-02-10 13:38:05+00:00
- **Authors**: Dominik Müller, Iñaki Soto-Rey, Frank Kramer
- **Comment**: Source Code: https://github.com/frankkramer-lab/miseval.analysis
- **Journal**: None
- **Summary**: In the last decade, research on artificial intelligence has seen rapid growth with deep learning models, especially in the field of medical image segmentation. Various studies demonstrated that these models have powerful prediction capabilities and achieved similar results as clinicians. However, recent studies revealed that the evaluation in image segmentation studies lacks reliable model performance assessment and showed statistical bias by incorrect metric implementation or usage. Thus, this work provides an overview and interpretation guide on the following metrics for medical image segmentation evaluation in binary as well as multi-class problems: Dice similarity coefficient, Jaccard, Sensitivity, Specificity, Rand index, ROC curves, Cohen's Kappa, and Hausdorff distance. As a summary, we propose a guideline for standardized medical image segmentation evaluation to improve evaluation quality, reproducibility, and comparability in the research field.



### Exploiting Spatial Sparsity for Event Cameras with Visual Transformers
- **Arxiv ID**: http://arxiv.org/abs/2202.05054v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.05054v1)
- **Published**: 2022-02-10 14:26:37+00:00
- **Updated**: 2022-02-10 14:26:37+00:00
- **Authors**: Zuowen Wang, Yuhuang Hu, Shih-Chii Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Event cameras report local changes of brightness through an asynchronous stream of output events. Events are spatially sparse at pixel locations with little brightness variation. We propose using a visual transformer (ViT) architecture to leverage its ability to process a variable-length input. The input to the ViT consists of events that are accumulated into time bins and spatially separated into non-overlapping sub-regions called patches. Patches are selected when the number of nonzero pixel locations within a sub-region is above a threshold. We show that by fine-tuning a ViT model on the selected active patches, we can reduce the average number of patches fed into the backbone during the inference by at least 50% with only a minor drop (0.34%) of the classification accuracy on the N-Caltech101 dataset. This reduction translates into a decrease of 51% in Multiply-Accumulate (MAC) operations and an increase of 46% in the inference speed using a server CPU.



### Equivariance Regularization for Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2202.05062v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.05062v2)
- **Published**: 2022-02-10 14:38:08+00:00
- **Updated**: 2022-02-12 15:30:04+00:00
- **Authors**: Junqi Tang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose Regularization-by-Equivariance (REV), a novel structure-adaptive regularization scheme for solving imaging inverse problems under incomplete measurements. This regularization scheme utilizes the equivariant structure in the physics of the measurements -- which is prevalent in many inverse problems such as tomographic image reconstruction -- to mitigate the ill-poseness of the inverse problem. Our proposed scheme can be applied in a plug-and-play manner alongside with any classic first-order optimization algorithm such as the accelerated gradient descent/FISTA for simplicity and fast convergence. The numerical experiments in sparse-view X-ray CT image reconstruction tasks demonstrate the effectiveness of our approach.



### Deep Learning for Computational Cytology: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2202.05126v2
- **DOI**: 10.1016/j.media.2022.102691
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.05126v2)
- **Published**: 2022-02-10 16:22:10+00:00
- **Updated**: 2022-02-16 05:19:50+00:00
- **Authors**: Hao Jiang, Yanning Zhou, Yi Lin, Ronald CK Chan, Jiang Liu, Hao Chen
- **Comment**: None
- **Journal**: Medical Image Analysis, Nov 2022
- **Summary**: Computational cytology is a critical, rapid-developing, yet challenging topic in the field of medical image computing which analyzes the digitized cytology image by computer-aided technologies for cancer screening. Recently, an increasing number of deep learning (DL) algorithms have made significant progress in medical image analysis, leading to the boosting publications of cytological studies. To investigate the advanced methods and comprehensive applications, we survey more than 120 publications of DL-based cytology image analysis in this article. We first introduce various deep learning methods, including fully supervised, weakly supervised, unsupervised, and transfer learning. Then, we systematically summarize the public datasets, evaluation metrics, versatile cytology image analysis applications including classification, detection, segmentation, and other related tasks. Finally, we discuss current challenges and potential research directions of computational cytology.



### Feature-level augmentation to improve robustness of deep neural networks to affine transformations
- **Arxiv ID**: http://arxiv.org/abs/2202.05152v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.05152v4)
- **Published**: 2022-02-10 17:14:58+00:00
- **Updated**: 2022-08-20 07:12:09+00:00
- **Authors**: Adrian Sandru, Mariana-Iuliana Georgescu, Radu Tudor Ionescu
- **Comment**: Accepted at ECCV Workshop on Adversarial Robustness in the Real World
  (AROW 2022)
- **Journal**: None
- **Summary**: Recent studies revealed that convolutional neural networks do not generalize well to small image transformations, e.g. rotations by a few degrees or translations of a few pixels. To improve the robustness to such transformations, we propose to introduce data augmentation at intermediate layers of the neural architecture, in addition to the common data augmentation applied on the input images. By introducing small perturbations to activation maps (features) at various levels, we develop the capacity of the neural network to cope with such transformations. We conduct experiments on three image classification benchmarks (Tiny ImageNet, Caltech-256 and Food-101), considering two different convolutional architectures (ResNet-18 and DenseNet-121). When compared with two state-of-the-art stabilization methods, the empirical results show that our approach consistently attains the best trade-off between accuracy and mean flip rate.



### Adults as Augmentations for Children in Facial Emotion Recognition with Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.05187v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2202.05187v1)
- **Published**: 2022-02-10 17:43:11+00:00
- **Updated**: 2022-02-10 17:43:11+00:00
- **Authors**: Marco Virgolin, Andrea De Lorenzo, Tanja Alderliesten, Peter A. N. Bosman
- **Comment**: None
- **Journal**: None
- **Summary**: Emotion recognition in children can help the early identification of, and intervention on, psychological complications that arise in stressful situations such as cancer treatment. Though deep learning models are increasingly being adopted, data scarcity is often an issue in pediatric medicine, including for facial emotion recognition in children. In this paper, we study the application of data augmentation-based contrastive learning to overcome data scarcity in facial emotion recognition for children. We explore the idea of ignoring generational gaps, by adding abundantly available adult data to pediatric data, to learn better representations. We investigate different ways by which adult facial expression images can be used alongside those of children. In particular, we propose to explicitly incorporate within each mini-batch adult images as augmentations for children's. Out of $84$ combinations of learning approaches and training set sizes, we find that supervised contrastive learning with the proposed training scheme performs best, reaching a test accuracy that typically surpasses the one of the second-best approach by 2% to 3%. Our results indicate that adult data can be considered to be a meaningful augmentation of pediatric data for the recognition of emotional facial expression in children, and open up the possibility for other applications of contrastive learning to improve pediatric care by complementing data of children with that of adults.



### A Human-Centered Machine-Learning Approach for Muscle-Tendon Junction Tracking in Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2202.05199v1
- **DOI**: 10.1109/TBME.2021.3130548
- **Categories**: **cs.CV**, q-bio.QM, I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2202.05199v1)
- **Published**: 2022-02-10 18:02:46+00:00
- **Updated**: 2022-02-10 18:02:46+00:00
- **Authors**: Christoph Leitner, Robert Jarolim, Bernhard Englmair, Annika Kruse, Karen Andrea Lara Hernandez, Andreas Konrad, Eric Su, Jörg Schröttner, Luke A. Kelly, Glen A. Lichtwark, Markus Tilp, Christian Baumgartner
- **Comment**: in IEEE Transactions on Biomedical Engineering
- **Journal**: None
- **Summary**: Biomechanical and clinical gait research observes muscles and tendons in limbs to study their functions and behaviour. Therefore, movements of distinct anatomical landmarks, such as muscle-tendon junctions, are frequently measured. We propose a reliable and time efficient machine-learning approach to track these junctions in ultrasound videos and support clinical biomechanists in gait analysis. In order to facilitate this process, a method based on deep-learning was introduced. We gathered an extensive dataset, covering 3 functional movements, 2 muscles, collected on 123 healthy and 38 impaired subjects with 3 different ultrasound systems, and providing a total of 66864 annotated ultrasound images in our network training. Furthermore, we used data collected across independent laboratories and curated by researchers with varying levels of experience. For the evaluation of our method a diverse test-set was selected that is independently verified by four specialists. We show that our model achieves similar performance scores to the four human specialists in identifying the muscle-tendon junction position. Our method provides time-efficient tracking of muscle-tendon junctions, with prediction times of up to 0.078 seconds per frame (approx. 100 times faster than manual labeling). All our codes, trained models and test-set were made publicly available and our model is provided as a free-to-use online service on https://deepmtj.org/.



### Visual Servoing for Pose Control of Soft Continuum Arm in a Structured Environment
- **Arxiv ID**: http://arxiv.org/abs/2202.05200v3
- **DOI**: 10.1109/LRA.2022.3155821
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.05200v3)
- **Published**: 2022-02-10 18:03:28+00:00
- **Updated**: 2022-03-12 17:05:46+00:00
- **Authors**: Shivani Kamtikar, Samhita Marri, Benjamin Walt, Naveen Kumar Uppalapati, Girish Krishnan, Girish Chowdhary
- **Comment**: Published in RA-L + RoboSoft 2022
- **Journal**: IEEE RA-L 2022
- **Summary**: For soft continuum arms, visual servoing is a popular control strategy that relies on visual feedback to close the control loop. However, robust visual servoing is challenging as it requires reliable feature extraction from the image, accurate control models and sensors to perceive the shape of the arm, both of which can be hard to implement in a soft robot. This letter circumvents these challenges by presenting a deep neural network-based method to perform smooth and robust 3D positioning tasks on a soft arm by visual servoing using a camera mounted at the distal end of the arm. A convolutional neural network is trained to predict the actuations required to achieve the desired pose in a structured environment. Integrated and modular approaches for estimating the actuations from the image are proposed and are experimentally compared. A proportional control law is implemented to reduce the error between the desired and current image as seen by the camera. The model together with the proportional feedback control makes the described approach robust to several variations such as new targets, lighting, loads, and diminution of the soft arm. Furthermore, the model lends itself to be transferred to a new environment with minimal effort.



### Towards Predicting Fine Finger Motions from Ultrasound Images via Kinematic Representation
- **Arxiv ID**: http://arxiv.org/abs/2202.05204v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.05204v2)
- **Published**: 2022-02-10 18:05:09+00:00
- **Updated**: 2022-09-28 10:54:56+00:00
- **Authors**: Dean Zadok, Oren Salzman, Alon Wolf, Alex M. Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: A central challenge in building robotic prostheses is the creation of a sensor-based system able to read physiological signals from the lower limb and instruct a robotic hand to perform various tasks. Existing systems typically perform discrete gestures such as pointing or grasping, by employing electromyography (EMG) or ultrasound (US) technologies to analyze muscle states. While estimating finger gestures has been done in the past by detecting prominent gestures, we are interested in detection, or inference, done in the context of fine motions that evolve over time. Examples include motions occurring when performing fine and dexterous tasks such as keyboard typing or piano playing. We consider this task as an important step towards higher adoption rates of robotic prostheses among arm amputees, as it has the potential to dramatically increase functionality in performing daily tasks. To this end, we present an end-to-end robotic system, which can successfully infer fine finger motions. This is achieved by modeling the hand as a robotic manipulator and using it as an intermediate representation to encode muscles' dynamics from a sequence of US images. We evaluated our method by collecting data from a group of subjects and demonstrating how it can be used to replay music played or text typed. To the best of our knowledge, this is the first study demonstrating these downstream tasks within an end-to-end system.



### F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization
- **Arxiv ID**: http://arxiv.org/abs/2202.05239v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.AR, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2202.05239v1)
- **Published**: 2022-02-10 18:48:56+00:00
- **Updated**: 2022-02-10 18:48:56+00:00
- **Authors**: Qing Jin, Jian Ren, Richard Zhuang, Sumant Hanumante, Zhengang Li, Zhiyu Chen, Yanzhi Wang, Kaiyuan Yang, Sergey Tulyakov
- **Comment**: ICLR 2022 (Oral)
- **Journal**: None
- **Summary**: Neural network quantization is a promising compression technique to reduce memory footprint and save energy consumption, potentially leading to real-time inference. However, there is a performance gap between quantized and full-precision models. To reduce it, existing quantization approaches require high-precision INT32 or full-precision multiplication during inference for scaling or dequantization. This introduces a noticeable cost in terms of memory, speed, and required energy. To tackle these issues, we present F8Net, a novel quantization framework consisting of only fixed-point 8-bit multiplication. To derive our method, we first discuss the advantages of fixed-point multiplication with different formats of fixed-point numbers and study the statistical behavior of the associated fixed-point numbers. Second, based on the statistical and algorithmic analysis, we apply different fixed-point formats for weights and activations of different layers. We introduce a novel algorithm to automatically determine the right format for each layer during training. Third, we analyze a previous quantization algorithm -- parameterized clipping activation (PACT) -- and reformulate it using fixed-point arithmetic. Finally, we unify the recently proposed method for quantization fine-tuning and our fixed-point approach to show the potential of our method. We verify F8Net on ImageNet for MobileNet V1/V2 and ResNet18/50. Our approach achieves comparable and better performance, when compared not only to existing quantization techniques with INT32 multiplication or floating-point arithmetic, but also to the full-precision counterparts, achieving state-of-the-art performance.



### Block-NeRF: Scalable Large Scene Neural View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2202.05263v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2202.05263v1)
- **Published**: 2022-02-10 18:59:56+00:00
- **Updated**: 2022-02-10 18:59:56+00:00
- **Authors**: Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P. Srinivasan, Jonathan T. Barron, Henrik Kretzschmar
- **Comment**: Project page: https://waymo.com/research/block-nerf/
- **Journal**: None
- **Summary**: We present Block-NeRF, a variant of Neural Radiance Fields that can represent large-scale environments. Specifically, we demonstrate that when scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to decompose the scene into individually trained NeRFs. This decomposition decouples rendering time from scene size, enables rendering to scale to arbitrarily large environments, and allows per-block updates of the environment. We adopt several architectural changes to make NeRF robust to data captured over months under different environmental conditions. We add appearance embeddings, learned pose refinement, and controllable exposure to each individual NeRF, and introduce a procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to create the largest neural scene representation to date, capable of rendering an entire neighborhood of San Francisco.



### Image-to-Image Regression with Distribution-Free Uncertainty Quantification and Applications in Imaging
- **Arxiv ID**: http://arxiv.org/abs/2202.05265v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2202.05265v1)
- **Published**: 2022-02-10 18:59:56+00:00
- **Updated**: 2022-02-10 18:59:56+00:00
- **Authors**: Anastasios N Angelopoulos, Amit P Kohli, Stephen Bates, Michael I Jordan, Jitendra Malik, Thayer Alshaabi, Srigokul Upadhyayula, Yaniv Romano
- **Comment**: Code available at https://github.com/aangelopoulos/im2im-uq
- **Journal**: None
- **Summary**: Image-to-image regression is an important learning task, used frequently in biological imaging. Current algorithms, however, do not generally offer statistical guarantees that protect against a model's mistakes and hallucinations. To address this, we develop uncertainty quantification techniques with rigorous statistical guarantees for image-to-image regression problems. In particular, we show how to derive uncertainty intervals around each pixel that are guaranteed to contain the true value with a user-specified confidence probability. Our methods work in conjunction with any base machine learning model, such as a neural network, and endow it with formal mathematical guarantees -- regardless of the true unknown data distribution or choice of model. Furthermore, they are simple to implement and computationally inexpensive. We evaluate our procedure on three image-to-image regression tasks: quantitative phase microscopy, accelerated magnetic resonance imaging, and super-resolution transmission electron microscopy of a Drosophila melanogaster brain.



### Face Beneath the Ink: Synthetic Data and Tattoo Removal with Application to Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.05297v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.05297v2)
- **Published**: 2022-02-10 19:35:28+00:00
- **Updated**: 2022-05-06 11:45:04+00:00
- **Authors**: Mathias Ibsen, Christian Rathgeb, Pawel Drozdowski, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: Systems that analyse faces have seen significant improvements in recent years and are today used in numerous application scenarios. However, these systems have been found to be negatively affected by facial alterations such as tattoos. To better understand and mitigate the effect of facial tattoos in facial analysis systems, large datasets of images of individuals with and without tattoos are needed. To this end, we propose a generator for automatically adding realistic tattoos to facial images. Moreover, we demonstrate the feasibility of the generation by using a deep learning-based model for removing tattoos from face images. The experimental results show that it is possible to remove facial tattoos from real images without degrading the quality of the image. Additionally, we show that it is possible to improve face recognition accuracy by using the proposed deep learning-based tattoo removal before extracting and comparing facial features.



### Motion Puzzle: Arbitrary Motion Style Transfer by Body Part
- **Arxiv ID**: http://arxiv.org/abs/2202.05274v2
- **DOI**: 10.1145/3516429
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.05274v2)
- **Published**: 2022-02-10 19:56:46+00:00
- **Updated**: 2022-07-11 00:02:08+00:00
- **Authors**: Deok-Kyeong Jang, Soomin Park, Sung-Hee Lee
- **Comment**: 16 pages
- **Journal**: ACM Transactions on Graphics 2022, presented at ACM SIGGRAPH 2022
- **Summary**: This paper presents Motion Puzzle, a novel motion style transfer network that advances the state-of-the-art in several important respects. The Motion Puzzle is the first that can control the motion style of individual body parts, allowing for local style editing and significantly increasing the range of stylized motions. Designed to keep the human's kinematic structure, our framework extracts style features from multiple style motions for different body parts and transfers them locally to the target body parts. Another major advantage is that it can transfer both global and local traits of motion style by integrating the adaptive instance normalization and attention modules while keeping the skeleton topology. Thus, it can capture styles exhibited by dynamic movements, such as flapping and staggering, significantly better than previous work. In addition, our framework allows for arbitrary motion style transfer without datasets with style labeling or motion pairing, making many publicly available motion datasets available for training. Our framework can be easily integrated with motion generation frameworks to create many applications, such as real-time motion transfer. We demonstrate the advantages of our framework with a number of examples and comparisons with previous work.



### Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/2202.05306v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.05306v3)
- **Published**: 2022-02-10 20:11:21+00:00
- **Updated**: 2022-09-16 19:56:14+00:00
- **Authors**: Nan Wu, Stanisław Jastrzębski, Kyunghyun Cho, Krzysztof J. Geras
- **Comment**: None
- **Journal**: None
- **Summary**: We hypothesize that due to the greedy nature of learning in multi-modal deep neural networks, these models tend to rely on just one modality while under-fitting the other modalities. Such behavior is counter-intuitive and hurts the models' generalization, as we observe empirically. To estimate the model's dependence on each modality, we compute the gain on the accuracy when the model has access to it in addition to another modality. We refer to this gain as the conditional utilization rate. In the experiments, we consistently observe an imbalance in conditional utilization rates between modalities, across multiple tasks and architectures. Since conditional utilization rate cannot be computed efficiently during training, we introduce a proxy for it based on the pace at which the model learns from each modality, which we refer to as the conditional learning speed. We propose an algorithm to balance the conditional learning speeds between modalities during training and demonstrate that it indeed addresses the issue of greedy learning. The proposed algorithm improves the model's generalization on three datasets: Colored MNIST, ModelNet40, and NVIDIA Dynamic Hand Gesture.



### Using Navigational Information to Learn Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2202.08114v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.08114v1)
- **Published**: 2022-02-10 20:17:55+00:00
- **Updated**: 2022-02-10 20:17:55+00:00
- **Authors**: Lizhen Zhu, Brad Wyble, James Z. Wang
- **Comment**: Abstract submission to Computational and Systems Neuroscience
  (Cosyne) 2022, accepted
- **Journal**: None
- **Summary**: Children learn to build a visual representation of the world from unsupervised exploration and we hypothesize that a key part of this learning ability is the use of self-generated navigational information as a similarity label to drive a learning objective for self-supervised learning. The goal of this work is to exploit navigational information in a visual environment to provide performance in training that exceeds the state-of-the-art self-supervised training. Here, we show that using spatial and temporal information in the pretraining stage of contrastive learning can improve the performance of downstream classification relative to conventional contrastive learning approaches that use instance discrimination to discriminate between two alterations of the same image or two different images. We designed a pipeline to generate egocentric-vision images from a photorealistic ray-tracing environment (ThreeDWorld) and record relevant navigational information for each image. Modifying the Momentum Contrast (MoCo) model, we introduced spatial and temporal information to evaluate the similarity of two views in the pretraining stage instead of instance discrimination. This work reveals the effectiveness and efficiency of contextual information for improving representation learning. The work informs our understanding of the means by which children might learn to see the world without external supervision.



### Mining the manifolds of deep generative models for multiple data-consistent solutions of ill-posed tomographic imaging problems
- **Arxiv ID**: http://arxiv.org/abs/2202.05311v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.05311v2)
- **Published**: 2022-02-10 20:27:31+00:00
- **Updated**: 2022-07-26 18:55:32+00:00
- **Authors**: Sayantan Bhadra, Umberto Villa, Mark A. Anastasio
- **Comment**: Submitted to IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Tomographic imaging is in general an ill-posed inverse problem. Typically, a single regularized image estimate of the sought-after object is obtained from tomographic measurements. However, there may be multiple objects that are all consistent with the same measurement data. The ability to generate such alternate solutions is important because it may enable new assessments of imaging systems. In principle, this can be achieved by means of posterior sampling methods. In recent years, deep neural networks have been employed for posterior sampling with promising results. However, such methods are not yet for use with large-scale tomographic imaging applications. On the other hand, empirical sampling methods may be computationally feasible for large-scale imaging systems and enable uncertainty quantification for practical applications. Empirical sampling involves solving a regularized inverse problem within a stochastic optimization framework to obtain alternate data-consistent solutions. In this work, a new empirical sampling method is proposed that computes multiple solutions of a tomographic inverse problem that are consistent with the same acquired measurement data. The method operates by repeatedly solving an optimization problem in the latent space of a style-based generative adversarial network (StyleGAN), and was inspired by the Photo Upsampling via Latent Space Exploration (PULSE) method that was developed for super-resolution tasks. The proposed method is demonstrated and analyzed via numerical studies that involve two stylized tomographic imaging modalities. These studies establish the ability of the method to perform efficient empirical sampling and uncertainty quantification.



### Describing image focused in cognitive and visual details for visually impaired people: An approach to generating inclusive paragraphs
- **Arxiv ID**: http://arxiv.org/abs/2202.05331v2
- **DOI**: 10.5220/0010845700003124
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.05331v2)
- **Published**: 2022-02-10 21:20:53+00:00
- **Updated**: 2022-02-15 20:38:22+00:00
- **Authors**: Daniel Louzada Fernandes, Marcos Henrique Fonseca Ribeiro, Fabio Ribeiro Cerqueira, Michel Melo Silva
- **Comment**: Accepted in the 17th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications (VISAPP) 2022
- **Journal**: None
- **Summary**: Several services for people with visual disabilities have emerged recently due to achievements in Assistive Technologies and Artificial Intelligence areas. Despite the growth in assistive systems availability, there is a lack of services that support specific tasks, such as understanding the image context presented in online content, e.g., webinars. Image captioning techniques and their variants are limited as Assistive Technologies as they do not match the needs of visually impaired people when generating specific descriptions. We propose an approach for generating context of webinar images combining a dense captioning technique with a set of filters, to fit the captions in our domain, and a language model for the abstractive summary task. The results demonstrated that we can produce descriptions with higher interpretability and focused on the relevant information for that group of people by combining image analysis methods and neural language models.



### Learning the Pedestrian-Vehicle Interaction for Pedestrian Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2202.05334v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO, I.2, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2202.05334v2)
- **Published**: 2022-02-10 21:26:54+00:00
- **Updated**: 2022-04-25 13:25:02+00:00
- **Authors**: Chi Zhang, Christian Berger
- **Comment**: 7 pages, 3 figures. Accepted in 2022 the 8th International Conference
  on Control Automation and Robotics (ICCAR), IEEE, 2022
- **Journal**: None
- **Summary**: In this paper, we study the interaction between pedestrians and vehicles and propose a novel neural network structure called the Pedestrian-Vehicle Interaction (PVI) extractor for learning the pedestrian-vehicle interaction. We implement the proposed PVI extractor on both sequential approaches (long short-term memory (LSTM) models) and non-sequential approaches (convolutional models). We use the Waymo Open Dataset that contains real-world urban traffic scenes with both pedestrian and vehicle annotations. For the LSTM-based models, our proposed model is compared with Social-LSTM and Social-GAN, and using our proposed PVI extractor reduces the average displacement error (ADE) and the final displacement error (FDE) by 7.46% and 5.24%, respectively. For the convolutional-based models, our proposed model is compared with Social-STGCNN and Social-IWSTCNN, and using our proposed PVI extractor reduces the ADE and FDE by 2.10% and 1.27%, respectively. The results show that the pedestrian-vehicle interaction influences pedestrian behavior, and the models using the proposed PVI extractor can capture the interaction between pedestrians and vehicles, and thereby outperform the compared methods.



### Dynamic Background Subtraction by Generative Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.05336v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.05336v1)
- **Published**: 2022-02-10 21:29:10+00:00
- **Updated**: 2022-02-10 21:29:10+00:00
- **Authors**: Fateme Bahri, Nilanjan Ray
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Background subtraction is a significant task in computer vision and an essential step for many real world applications. One of the challenges for background subtraction methods is dynamic background, which constitute stochastic movements in some parts of the background. In this paper, we have proposed a new background subtraction method, called DBSGen, which uses two generative neural networks, one for dynamic motion removal and another for background generation. At the end, the foreground moving objects are obtained by a pixel-wise distance threshold based on a dynamic entropy map. The proposed method has a unified framework that can be optimized in an end-to-end and unsupervised fashion. The performance of the method is evaluated over dynamic background sequences and it outperforms most of state-of-the-art methods. Our code is publicly available at https://github.com/FatemeBahri/DBSGen.



### Coded ResNeXt: a network for designing disentangled information paths
- **Arxiv ID**: http://arxiv.org/abs/2202.05343v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2202.05343v1)
- **Published**: 2022-02-10 21:45:49+00:00
- **Updated**: 2022-02-10 21:45:49+00:00
- **Authors**: Apostolos Avranas, Marios Kountouris
- **Comment**: None
- **Journal**: None
- **Summary**: To avoid treating neural networks as highly complex black boxes, the deep learning research community has tried to build interpretable models allowing humans to understand the decisions taken by the model. Unfortunately, the focus is mostly on manipulating only the very high-level features associated with the last layers. In this work, we look at neural network architectures for classification in a more general way and introduce an algorithm which defines before the training the paths of the network through which the per-class information flows. We show that using our algorithm we can extract a lighter single-purpose binary classifier for a particular class by removing the parameters that do not participate in the predefined information path of that class, which is approximately 60% of the total parameters. Notably, leveraging coding theory to design the information paths enables us to use intermediate network layers for making early predictions without having to evaluate the full network. We demonstrate that a slightly modified ResNeXt model, trained with our algorithm, can achieve higher classification accuracy on CIFAR-10/100 and ImageNet than the original ResNeXt, while having all the aforementioned properties.



### Domain Adversarial Training: A Game Perspective
- **Arxiv ID**: http://arxiv.org/abs/2202.05352v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GT
- **Links**: [PDF](http://arxiv.org/pdf/2202.05352v1)
- **Published**: 2022-02-10 22:17:30+00:00
- **Updated**: 2022-02-10 22:17:30+00:00
- **Authors**: David Acuna, Marc T Law, Guojun Zhang, Sanja Fidler
- **Comment**: ICLR 2022
- **Journal**: None
- **Summary**: The dominant line of work in domain adaptation has focused on learning invariant representations using domain-adversarial training. In this paper, we interpret this approach from a game theoretical perspective. Defining optimal solutions in domain-adversarial training as a local Nash equilibrium, we show that gradient descent in domain-adversarial training can violate the asymptotic convergence guarantees of the optimizer, oftentimes hindering the transfer performance. Our analysis leads us to replace gradient descent with high-order ODE solvers (i.e., Runge-Kutta), for which we derive asymptotic convergence guarantees. This family of optimizers is significantly more stable and allows more aggressive learning rates, leading to high performance gains when used as a drop-in replacement over standard optimizers. Our experiments show that in conjunction with state-of-the-art domain-adversarial methods, we achieve up to 3.5% improvement with less than of half training iterations. Our optimizers are easy to implement, free of additional parameters, and can be plugged into any domain-adversarial framework.



### Optimal Transport for Super Resolution Applied to Astronomy Imaging
- **Arxiv ID**: http://arxiv.org/abs/2202.05354v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP, 68T10, 85-08,, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2202.05354v2)
- **Published**: 2022-02-10 22:20:38+00:00
- **Updated**: 2022-02-14 19:54:26+00:00
- **Authors**: Michael Rawson, Jakob Hultgren
- **Comment**: 5 pages, 2 figures
- **Journal**: Proceedings of the 30th European Signal Processing Conference,
  EUSIPCO, 2022. ISBN: 978-1-6654-6798-8
- **Summary**: Super resolution is an essential tool in optics, especially on interstellar scales, due to physical laws restricting possible imaging resolution. We propose using optimal transport and entropy for super resolution applications. We prove that the reconstruction is accurate when sparsity is known and noise or distortion is small enough. We prove that the optimizer is stable and robust to noise and perturbations. We compare this method to a state of the art convolutional neural network and get similar results for much less computational cost and greater methodological flexibility.



### The MeLa BitChute Dataset
- **Arxiv ID**: http://arxiv.org/abs/2202.05364v1
- **DOI**: None
- **Categories**: **cs.SI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.05364v1)
- **Published**: 2022-02-10 23:12:28+00:00
- **Updated**: 2022-02-10 23:12:28+00:00
- **Authors**: Milo Trujillo, Maurício Gruppi, Cody Buntain, Benjamin D. Horne
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a near-complete dataset of over 3M videos from 61K channels over 2.5 years (June 2019 to December 2021) from the social video hosting platform BitChute, a commonly used alternative to YouTube. Additionally, we include a variety of video-level metadata, including comments, channel descriptions, and views for each video. The MeLa-BitChute dataset can be found at: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KRD1VS.



