# Arxiv Papers in cs.CV on 2022-02-18
### Functional Connectivity Based Classification of ADHD Using Different Atlases
- **Arxiv ID**: http://arxiv.org/abs/2202.08953v2
- **DOI**: 10.1109/PIC50277.2020.9350749
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.08953v2)
- **Published**: 2022-02-18 01:32:55+00:00
- **Updated**: 2022-03-01 04:33:12+00:00
- **Authors**: Sartaj Ahmed Salman, Zhichao Lian, Marva Saleem, Yuduo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: These days, computational diagnosis strategies of neuropsychiatric disorders are gaining attention day by day. It's critical to determine the brain's functional connectivity based on Functional-Magnetic-Resonance-Imaging(fMRI) to diagnose the disorder. It's known as a chronic disease, and millions of children amass the symptoms of this disease, so there is much vacuum for the researcher to formulate a model to improve the accuracy to diagnose ADHD accurately. In this paper, we consider the functional connectivity of a brain extracted using various time templates/Atlases. Local-Binary Encoding-Method (LBEM) algorithm is utilized for feature extraction, while Hierarchical- Extreme-Learning-Machine (HELM) is used to classify the extracted features. To validate our approach, fMRI data of 143 normal and 100 ADHD affected children is used for experimental purpose. Our experimental results are based on comparing various Atlases given as CC400, CC200, and AAL. Our model achieves high performance with CC400 as compared to other Atlases



### R2-D2: Repetitive Reprediction Deep Decipher for Semi-Supervised Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.08955v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.08955v1)
- **Published**: 2022-02-18 01:46:23+00:00
- **Updated**: 2022-02-18 01:46:23+00:00
- **Authors**: Guo-Hua Wang, Jianxin Wu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1908.04345
- **Journal**: None
- **Summary**: Most recent semi-supervised deep learning (deep SSL) methods used a similar paradigm: use network predictions to update pseudo-labels and use pseudo-labels to update network parameters iteratively. However, they lack theoretical support and cannot explain why predictions are good candidates for pseudo-labels in the deep learning paradigm. In this paper, we propose a principled end-to-end framework named deep decipher (D2) for SSL. Within the D2 framework, we prove that pseudo-labels are related to network predictions by an exponential link function, which gives a theoretical support for using predictions as pseudo-labels. Furthermore, we demonstrate that updating pseudo-labels by network predictions will make them uncertain. To mitigate this problem, we propose a training strategy called repetitive reprediction (R2). Finally, the proposed R2-D2 method is tested on the large-scale ImageNet dataset and outperforms state-of-the-art methods by 5 percentage points.



### Joint Learning of Frequency and Spatial Domains for Dense Predictions
- **Arxiv ID**: http://arxiv.org/abs/2202.08991v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.08991v1)
- **Published**: 2022-02-18 02:53:29+00:00
- **Updated**: 2022-02-18 02:53:29+00:00
- **Authors**: Shaocheng Jia, Wei Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Current artificial neural networks mainly conduct the learning process in the spatial domain but neglect the frequency domain learning. However, the learning course performed in the frequency domain can be more efficient than that in the spatial domain. In this paper, we fully explore frequency domain learning and propose a joint learning paradigm of frequency and spatial domains. This paradigm can take full advantage of the preponderances of frequency learning and spatial learning; specifically, frequency and spatial domain learning can effectively capture global and local information, respectively. Exhaustive experiments on two dense prediction tasks, i.e., self-supervised depth estimation and semantic segmentation, demonstrate that the proposed joint learning paradigm can 1) achieve performance competitive to those of state-of-the-art methods in both depth estimation and semantic segmentation tasks, even without pretraining; and 2) significantly reduce the number of parameters compared to other state-of-the-art methods, which provides more chance to develop real-world applications. We hope that the proposed method can encourage more research in cross-domain learning.



### REFUGE2 Challenge: A Treasure Trove for Multi-Dimension Analysis and Evaluation in Glaucoma Screening
- **Arxiv ID**: http://arxiv.org/abs/2202.08994v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.08994v3)
- **Published**: 2022-02-18 02:56:21+00:00
- **Updated**: 2022-12-29 07:48:59+00:00
- **Authors**: Huihui Fang, Fei Li, Junde Wu, Huazhu Fu, Xu Sun, Jaemin Son, Shuang Yu, Menglu Zhang, Chenglang Yuan, Cheng Bian, Baiying Lei, Benjian Zhao, Xinxing Xu, Shaohua Li, Francisco Fumero, José Sigut, Haidar Almubarak, Yakoub Bazi, Yuanhao Guo, Yating Zhou, Ujjwal Baid, Shubham Innani, Tianjiao Guo, Jie Yang, José Ignacio Orlando, Hrvoje Bogunović, Xiulan Zhang, Yanwu Xu
- **Comment**: 29 pages, 21 figures
- **Journal**: None
- **Summary**: With the rapid development of artificial intelligence (AI) in medical image processing, deep learning in color fundus photography (CFP) analysis is also evolving. Although there are some open-source, labeled datasets of CFPs in the ophthalmology community, large-scale datasets for screening only have labels of disease categories, and datasets with annotations of fundus structures are usually small in size. In addition, labeling standards are not uniform across datasets, and there is no clear information on the acquisition device. Here we release a multi-annotation, multi-quality, and multi-device color fundus image dataset for glaucoma analysis on an original challenge -- Retinal Fundus Glaucoma Challenge 2nd Edition (REFUGE2). The REFUGE2 dataset contains 2000 color fundus images with annotations of glaucoma classification, optic disc/cup segmentation, as well as fovea localization. Meanwhile, the REFUGE2 challenge sets three sub-tasks of automatic glaucoma diagnosis and fundus structure analysis and provides an online evaluation framework. Based on the characteristics of multi-device and multi-quality data, some methods with strong generalizations are provided in the challenge to make the predictions more robust. This shows that REFUGE2 brings attention to the characteristics of real-world multi-domain data, bridging the gap between scientific research and clinical application.



### An Active and Contrastive Learning Framework for Fine-Grained Off-Road Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.09002v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.09002v1)
- **Published**: 2022-02-18 03:16:31+00:00
- **Updated**: 2022-02-18 03:16:31+00:00
- **Authors**: Biao Gao, Xijun Zhao, Huijing Zhao
- **Comment**: 13 pages, 13 figures
- **Journal**: None
- **Summary**: Off-road semantic segmentation with fine-grained labels is necessary for autonomous vehicles to understand driving scenes, as the coarse-grained road detection can not satisfy off-road vehicles with various mechanical properties. Fine-grained semantic segmentation in off-road scenes usually has no unified category definition due to ambiguous nature environments, and the cost of pixel-wise labeling is extremely high. Furthermore, semantic properties of off-road scenes can be very changeable due to various precipitations, temperature, defoliation, etc. To address these challenges, this research proposes an active and contrastive learning-based method that does not rely on pixel-wise labels, but only on patch-based weak annotations for model learning. There is no need for predefined semantic categories, the contrastive learning-based feature representation and adaptive clustering will discover the category model from scene data. In order to actively adapt to new scenes, a risk evaluation method is proposed to discover and select hard frames with high-risk predictions for supplemental labeling, so as to update the model efficiently. Experiments conducted on our self-developed off-road dataset and DeepScene dataset demonstrate that fine-grained semantic segmentation can be learned with only dozens of weakly labeled frames, and the model can efficiently adapt across scenes by weak supervision, while achieving almost the same level of performance as typical fully supervised baselines.



### KINet: Unsupervised Forward Models for Robotic Pushing Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2202.09006v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2202.09006v3)
- **Published**: 2022-02-18 03:32:08+00:00
- **Updated**: 2023-08-05 21:39:42+00:00
- **Authors**: Alireza Rezazadeh, Changhyun Choi
- **Comment**: None
- **Journal**: None
- **Summary**: Object-centric representation is an essential abstraction for forward prediction. Most existing forward models learn this representation through extensive supervision (e.g., object class and bounding box) although such ground-truth information is not readily accessible in reality. To address this, we introduce KINet (Keypoint Interaction Network) -- an end-to-end unsupervised framework to reason about object interactions based on a keypoint representation. Using visual observations, our model learns to associate objects with keypoint coordinates and discovers a graph representation of the system as a set of keypoint embeddings and their relations. It then learns an action-conditioned forward model using contrastive estimation to predict future keypoint states. By learning to perform physical reasoning in the keypoint space, our model automatically generalizes to scenarios with a different number of objects, novel backgrounds, and unseen object geometries. Experiments demonstrate the effectiveness of our model in accurately performing forward prediction and learning plannable object-centric representations for downstream robotic pushing manipulation tasks.



### LG-LSQ: Learned Gradient Linear Symmetric Quantization
- **Arxiv ID**: http://arxiv.org/abs/2202.09009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09009v1)
- **Published**: 2022-02-18 03:38:12+00:00
- **Updated**: 2022-02-18 03:38:12+00:00
- **Authors**: Shih-Ting Lin, Zhaofang Li, Yu-Hsiang Cheng, Hao-Wen Kuo, Chih-Cheng Lu, Kea-Tiong Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks with lower precision weights and operations at inference time have advantages in terms of the cost of memory space and accelerator power. The main challenge associated with the quantization algorithm is maintaining accuracy at low bit-widths. We propose learned gradient linear symmetric quantization (LG-LSQ) as a method for quantizing weights and activation functions to low bit-widths with high accuracy in integer neural network processors. First, we introduce the scaling simulated gradient (SSG) method for determining the appropriate gradient for the scaling factor of the linear quantizer during the training process. Second, we introduce the arctangent soft round (ASR) method, which differs from the straight-through estimator (STE) method in its ability to prevent the gradient from becoming zero, thereby solving the discrete problem caused by the rounding process. Finally, to bridge the gap between full-precision and low-bit quantization networks, we propose the minimize discretization error (MDE) method to determine an accurate gradient in backpropagation. The ASR+MDE method is a simple alternative to the STE method and is practical for use in different uniform quantization methods. In our evaluation, the proposed quantizer achieved full-precision baseline accuracy in various 3-bit networks, including ResNet18, ResNet34, and ResNet50, and an accuracy drop of less than 1% in the quantization of 4-bit weights and 4-bit activations in lightweight models such as MobileNetV2 and ShuffleNetV2.



### How Well Do Self-Supervised Methods Perform in Cross-Domain Few-Shot Learning?
- **Arxiv ID**: http://arxiv.org/abs/2202.09014v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09014v1)
- **Published**: 2022-02-18 04:03:53+00:00
- **Updated**: 2022-02-18 04:03:53+00:00
- **Authors**: Yiyi Zhang, Ying Zheng, Xiaogang Xu, Jun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-domain few-shot learning (CDFSL) remains a largely unsolved problem in the area of computer vision, while self-supervised learning presents a promising solution. Both learning methods attempt to alleviate the dependency of deep networks on the requirement of large-scale labeled data. Although self-supervised methods have recently advanced dramatically, their utility on CDFSL is relatively unexplored. In this paper, we investigate the role of self-supervised representation learning in the context of CDFSL via a thorough evaluation of existing methods. It comes as a surprise that even with shallow architectures or small training datasets, self-supervised methods can perform favorably compared to the existing SOTA methods. Nevertheless, no single self-supervised approach dominates all datasets indicating that existing self-supervised methods are not universally applicable. In addition, we find that representations extracted from self-supervised methods exhibit stronger robustness than the supervised method. Intriguingly, whether self-supervised representations perform well on the source domain has little correlation with their applicability on the target domain. As part of our study, we conduct an objective measurement of the performance for six kinds of representative classifiers. The results suggest Prototypical Classifier as the standard evaluation recipe for CDFSL.



### A Comprehensive Survey with Quantitative Comparison of Image Analysis Methods for Microorganism Biovolume Measurements
- **Arxiv ID**: http://arxiv.org/abs/2202.09020v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2202.09020v2)
- **Published**: 2022-02-18 04:58:04+00:00
- **Updated**: 2022-05-02 17:27:59+00:00
- **Authors**: Jiawei Zhang, Chen Li, Md Mamunur Rahaman, Yudong Yao, Pingli Ma, Jinghua Zhang, Xin Zhao, Tao Jiang, Marcin Grzegorzek
- **Comment**: None
- **Journal**: None
- **Summary**: With the acceleration of urbanization and living standards, microorganisms play increasingly important roles in industrial production, bio-technique, and food safety testing. Microorganism biovolume measurements are one of the essential parts of microbial analysis. However, traditional manual measurement methods are time-consuming and challenging to measure the characteristics precisely. With the development of digital image processing techniques, the characteristics of the microbial population can be detected and quantified. The changing trend can be adjusted in time and provided a basis for the improvement. The applications of the microorganism biovolume measurement method have developed since the 1980s. More than 62 articles are reviewed in this study, and the articles are grouped by digital image segmentation methods with periods. This study has high research significance and application value, which can be referred to microbial researchers to have a comprehensive understanding of microorganism biovolume measurements using digital image analysis methods and potential applications.



### Critical Checkpoints for Evaluating Defence Models Against Adversarial Attack and Robustness
- **Arxiv ID**: http://arxiv.org/abs/2202.09039v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.09039v1)
- **Published**: 2022-02-18 06:15:49+00:00
- **Updated**: 2022-02-18 06:15:49+00:00
- **Authors**: Kanak Tekwani, Manojkumar Parmar
- **Comment**: 16 pages, 8 figures
- **Journal**: None
- **Summary**: From past couple of years there is a cycle of researchers proposing a defence model for adversaries in machine learning which is arguably defensible to most of the existing attacks in restricted condition (they evaluate on some bounded inputs or datasets). And then shortly another set of researcher finding the vulnerabilities in that defence model and breaking it by proposing a stronger attack model. Some common flaws are been noticed in the past defence models that were broken in very short time. Defence models being broken so easily is a point of concern as decision of many crucial activities are taken with the help of machine learning models. So there is an utter need of some defence checkpoints that any researcher should keep in mind while evaluating the soundness of technique and declaring it to be decent defence technique. In this paper, we have suggested few checkpoints that should be taken into consideration while building and evaluating the soundness of defence models. All these points are recommended after observing why some past defence models failed and how some model remained adamant and proved their soundness against some of the very strong attacks.



### Task Specific Attention is one more thing you need for object detection
- **Arxiv ID**: http://arxiv.org/abs/2202.09048v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09048v4)
- **Published**: 2022-02-18 07:09:33+00:00
- **Updated**: 2022-06-15 04:02:27+00:00
- **Authors**: Sang Yon Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Various models have been proposed to perform object detection. However, most require many handdesigned components such as anchors and non-maximum-suppression(NMS) to demonstrate good performance. To mitigate these issues, Transformer-based DETR and its variant, Deformable DETR, were suggested. These have solved much of the complex issue in designing a head for object detection models; however, doubts about performance still exist when considering Transformer-based models as state-of-the-art methods in object detection for other models depending on anchors and NMS revealed better results. Furthermore, it has been unclear whether it would be possible to build an end-to-end pipeline in combination only with attention modules, because the DETR-adapted Transformer method used a convolutional neural network (CNN) for the backbone body. In this study, we propose that combining several attention modules with our new Task Specific Split Transformer (TSST) is a powerful method to produce the state-of-the art performance on COCO results without traditionally hand-designed components. By splitting the general-purpose attention module into two separated goal-specific attention modules, the proposed method allows for the design of simpler object detection models. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is available at https://github.com/navervision/tsst



### Guide Local Feature Matching by Overlap Estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.09050v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09050v2)
- **Published**: 2022-02-18 07:11:36+00:00
- **Updated**: 2022-02-23 02:51:36+00:00
- **Authors**: Ying Chen, Dihe Huang, Shang Xu, Jianlin Liu, Yong Liu
- **Comment**: Accepted by AAAI2022
- **Journal**: None
- **Summary**: Local image feature matching under large appearance, viewpoint, and distance changes is challenging yet important. Conventional methods detect and match tentative local features across the whole images, with heuristic consistency checks to guarantee reliable matches. In this paper, we introduce a novel Overlap Estimation method conditioned on image pairs with TRansformer, named OETR, to constrain local feature matching in the commonly visible region. OETR performs overlap estimation in a two-step process of feature correlation and then overlap regression. As a preprocessing module, OETR can be plugged into any existing local feature detection and matching pipeline, to mitigate potential view angle or scale variance. Intensive experiments show that OETR can boost state-of-the-art local feature matching performance substantially, especially for image pairs with small shared regions. The code will be publicly available at https://github.com/AbyssGaze/OETR.



### Towards better understanding and better generalization of few-shot classification in histology images with contrastive learning
- **Arxiv ID**: http://arxiv.org/abs/2202.09059v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09059v1)
- **Published**: 2022-02-18 07:48:34+00:00
- **Updated**: 2022-02-18 07:48:34+00:00
- **Authors**: Jiawei Yang, Hanbo Chen, Jiangpeng Yan, Xiaoyu Chen, Jianhua Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning is an established topic in natural images for years, but few work is attended to histology images, which is of high clinical value since well-labeled datasets and rare abnormal samples are expensive to collect. Here, we facilitate the study of few-shot learning in histology images by setting up three cross-domain tasks that simulate real clinics problems. To enable label-efficient learning and better generalizability, we propose to incorporate contrastive learning (CL) with latent augmentation (LA) to build a few-shot system. CL learns useful representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. These two components fully exploit unlabeled training data and can scale gracefully to other label-hungry problems. In experiments, we find i) models learned by CL generalize better than supervised learning for histology images in unseen classes, and ii) LA brings consistent gains over baselines. Prior studies of self-supervised learning mainly focus on ImageNet-like images, which only present a dominant object in their centers. Recent attention has been paid to images with multi-objects and multi-textures. Histology images are a natural choice for such a study. We show the superiority of CL over supervised learning in terms of generalization for such data and provide our empirical understanding for this observation. The findings in this work could contribute to understanding how the model generalizes in the context of both representation learning and histological image analysis. Code is available.



### VLP: A Survey on Vision-Language Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2202.09061v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2202.09061v4)
- **Published**: 2022-02-18 07:54:02+00:00
- **Updated**: 2022-07-30 14:38:11+00:00
- **Authors**: Feilong Chen, Duzhen Zhang, Minglun Han, Xiuyi Chen, Jing Shi, Shuang Xu, Bo Xu
- **Comment**: A Survey on Vision-Language Pre-training
- **Journal**: None
- **Summary**: In the past few years, the emergence of pre-training models has brought uni-modal fields such as computer vision (CV) and natural language processing (NLP) to a new era. Substantial works have shown they are beneficial for downstream uni-modal tasks and avoid training a new model from scratch. So can such pre-trained models be applied to multi-modal tasks? Researchers have explored this problem and made significant progress. This paper surveys recent advances and new frontiers in vision-language pre-training (VLP), including image-text and video-text pre-training. To give readers a better overall grasp of VLP, we first review its recent advances from five aspects: feature extraction, model architecture, pre-training objectives, pre-training datasets, and downstream tasks. Then, we summarize the specific VLP models in detail. Finally, we discuss the new frontiers in VLP. To the best of our knowledge, this is the first survey focused on VLP. We hope that this survey can shed light on future research in the VLP field.



### VCVTS: Multi-speaker Video-to-Speech synthesis via cross-modal knowledge transfer from voice conversion
- **Arxiv ID**: http://arxiv.org/abs/2202.09081v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.AI, cs.CV, cs.MM, cs.SD, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09081v1)
- **Published**: 2022-02-18 08:58:45+00:00
- **Updated**: 2022-02-18 08:58:45+00:00
- **Authors**: Disong Wang, Shan Yang, Dan Su, Xunying Liu, Dong Yu, Helen Meng
- **Comment**: Accepted to ICASSP 2022. Demo page is available at
  https://wendison.github.io/VCVTS-demo/
- **Journal**: None
- **Summary**: Though significant progress has been made for speaker-dependent Video-to-Speech (VTS) synthesis, little attention is devoted to multi-speaker VTS that can map silent video to speech, while allowing flexible control of speaker identity, all in a single system. This paper proposes a novel multi-speaker VTS system based on cross-modal knowledge transfer from voice conversion (VC), where vector quantization with contrastive predictive coding (VQCPC) is used for the content encoder of VC to derive discrete phoneme-like acoustic units, which are transferred to a Lip-to-Index (Lip2Ind) network to infer the index sequence of acoustic units. The Lip2Ind network can then substitute the content encoder of VC to form a multi-speaker VTS system to convert silent video to acoustic units for reconstructing accurate spoken content. The VTS system also inherits the advantages of VC by using a speaker encoder to produce speaker representations to effectively control the speaker identity of generated speech. Extensive evaluations verify the effectiveness of proposed approach, which can be applied in both constrained vocabulary and open vocabulary conditions, achieving state-of-the-art performance in generating high-quality speech with high naturalness, intelligibility and speaker similarity. Our demo page is released here: https://wendison.github.io/VCVTS-demo/



### Lightweight Multi-Drone Detection and 3D-Localization via YOLO
- **Arxiv ID**: http://arxiv.org/abs/2202.09097v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09097v1)
- **Published**: 2022-02-18 09:41:23+00:00
- **Updated**: 2022-02-18 09:41:23+00:00
- **Authors**: Aryan Sharma, Nitik Jain, Mangal Kothari
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present and evaluate a method to perform real-time multiple drone detection and three-dimensional localization using state-of-the-art tiny-YOLOv4 object detection algorithm and stereo triangulation. Our computer vision approach eliminates the need for computationally expensive stereo matching algorithms, thereby significantly reducing the memory footprint and making it deployable on embedded systems. Our drone detection system is highly modular (with support for various detection algorithms) and capable of identifying multiple drones in a system, with real-time detection accuracy of up to 77\% with an average FPS of 332 (on Nvidia Titan Xp). We also test the complete pipeline in AirSim environment, detecting drones at a maximum distance of 8 meters, with a mean error of $23\%$ of the distance. We also release the source code for the project, with pre-trained models and the curated synthetic stereo dataset.



### AF$_2$: Adaptive Focus Framework for Aerial Imagery Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.10322v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.10322v1)
- **Published**: 2022-02-18 10:14:45+00:00
- **Updated**: 2022-02-18 10:14:45+00:00
- **Authors**: Lin Huang, Qiyuan Dong, Lijun Wu, Jia Zhang, Jiang Bian, Tie-Yan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: As a specific semantic segmentation task, aerial imagery segmentation has been widely employed in high spatial resolution (HSR) remote sensing images understanding. Besides common issues (e.g. large scale variation) faced by general semantic segmentation tasks, aerial imagery segmentation has some unique challenges, the most critical one among which lies in foreground-background imbalance. There have been some recent efforts that attempt to address this issue by proposing sophisticated neural network architectures, since they can be used to extract informative multi-scale feature representations and increase the discrimination of object boundaries. Nevertheless, many of them merely utilize those multi-scale representations in ad-hoc measures but disregard the fact that the semantic meaning of objects with various sizes could be better identified via receptive fields of diverse ranges. In this paper, we propose Adaptive Focus Framework (AF$_2$), which adopts a hierarchical segmentation procedure and focuses on adaptively utilizing multi-scale representations generated by widely adopted neural network architectures. Particularly, a learnable module, called Adaptive Confidence Mechanism (ACM), is proposed to determine which scale of representation should be used for the segmentation of different objects. Comprehensive experiments show that AF$_2$ has significantly improved the accuracy on three widely used aerial benchmarks, as fast as the mainstream method.



### Iterative Learning for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.09110v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09110v1)
- **Published**: 2022-02-18 10:25:02+00:00
- **Updated**: 2022-02-18 10:25:02+00:00
- **Authors**: Tuomas Sormunen, Arttu Lämsä, Miguel Bordallo Lopez
- **Comment**: 5 pages, 6 figures
- **Journal**: None
- **Summary**: Instance segmentation is a computer vision task where separate objects in an image are detected and segmented. State-of-the-art deep neural network models require large amounts of labeled data in order to perform well in this task. Making these annotations is time-consuming. We propose for the first time, an iterative learning and annotation method that is able to detect, segment and annotate instances in datasets composed of multiple similar objects. The approach requires minimal human intervention and needs only a bootstrapping set containing very few annotations. Experiments on two different datasets show the validity of the approach in different applications related to visual inspection.



### Towards Simple and Accurate Human Pose Estimation with Stair Network
- **Arxiv ID**: http://arxiv.org/abs/2202.09115v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09115v3)
- **Published**: 2022-02-18 10:37:13+00:00
- **Updated**: 2022-11-22 02:43:47+00:00
- **Authors**: Chenru Jiang, Kaizhu Huang, Shufei Zhang, Shufei Zhang, Jimin Xiao, Zhenxing Niu, Amir Hussain
- **Comment**: The paper has been accepted by IEEE Transactions on Emerging Topics
  in Computational Intelligence
- **Journal**: None
- **Summary**: In this paper, we focus on tackling the precise keypoint coordinates regression task. Most existing approaches adopt complicated networks with a large number of parameters, leading to a heavy model with poor cost-effectiveness in practice. To overcome this limitation, we develop a small yet discrimicative model called STair Network, which can be simply stacked towards an accurate multi-stage pose estimation system. Specifically, to reduce computational cost, STair Network is composed of novel basic feature extraction blocks which focus on promoting feature diversity and obtaining rich local representations with fewer parameters, enabling a satisfactory balance on efficiency and performance. To further improve the performance, we introduce two mechanisms with negligible computational cost, focusing on feature fusion and replenish. We demonstrate the effectiveness of the STair Network on two standard datasets, e.g., 1-stage STair Network achieves a higher accuracy than HRNet by 5.5% on COCO test dataset with 80\% fewer parameters and 68% fewer GFLOPs.



### Generalizing Aggregation Functions in GNNs:High-Capacity GNNs via Nonlinear Neighborhood Aggregators
- **Arxiv ID**: http://arxiv.org/abs/2202.09145v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09145v1)
- **Published**: 2022-02-18 11:49:59+00:00
- **Updated**: 2022-02-18 11:49:59+00:00
- **Authors**: Beibei Wang, Bo Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Graph neural networks (GNNs) have achieved great success in many graph learning tasks. The main aspect powering existing GNNs is the multi-layer network architecture to learn the nonlinear graph representations for the specific learning tasks. The core operation in GNNs is message propagation in which each node updates its representation by aggregating its neighbors' representations. Existing GNNs mainly adopt either linear neighborhood aggregation (mean,sum) or max aggregator in their message propagation. (1) For linear aggregators, the whole nonlinearity and network's capacity of GNNs are generally limited due to deeper GNNs usually suffer from over-smoothing issue. (2) For max aggregator, it usually fails to be aware of the detailed information of node representations within neighborhood. To overcome these issues, we re-think the message propagation mechanism in GNNs and aim to develop the general nonlinear aggregators for neighborhood information aggregation in GNNs. One main aspect of our proposed nonlinear aggregators is that they provide the optimally balanced aggregators between max and mean/sum aggregations. Thus, our aggregators can inherit both (i) high nonlinearity that increases network's capacity and (ii) detail-sensitivity that preserves the detailed information of representations together in GNNs' message propagation. Promising experiments on several datasets show the effectiveness of the proposed nonlinear aggregators.



### MultiRes-NetVLAD: Augmenting Place Recognition Training with Low-Resolution Imagery
- **Arxiv ID**: http://arxiv.org/abs/2202.09146v1
- **DOI**: 10.1109/LRA.2022.3147257
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.09146v1)
- **Published**: 2022-02-18 11:53:01+00:00
- **Updated**: 2022-02-18 11:53:01+00:00
- **Authors**: Ahmad Khaliq, Michael Milford, Sourav Garg
- **Comment**: 12 pages, 6 Figures, Accepted for publication in IEEE RA-L 2022 and
  ICRA 2022, includes supplementary material
- **Journal**: IEEE Robotics and Automation Letters vol. 7 no. 2 (April 2022) pp.
  3882-3889
- **Summary**: Visual Place Recognition (VPR) is a crucial component of 6-DoF localization, visual SLAM and structure-from-motion pipelines, tasked to generate an initial list of place match hypotheses by matching global place descriptors. However, commonly-used CNN-based methods either process multiple image resolutions after training or use a single resolution and limit multi-scale feature extraction to the last convolutional layer during training. In this paper, we augment NetVLAD representation learning with low-resolution image pyramid encoding which leads to richer place representations. The resultant multi-resolution feature pyramid can be conveniently aggregated through VLAD into a single compact representation, avoiding the need for concatenation or summation of multiple patches in recent multi-scale approaches. Furthermore, we show that the underlying learnt feature tensor can be combined with existing multi-scale approaches to improve their baseline performance. Evaluation on 15 viewpoint-varying and viewpoint-consistent benchmarking datasets confirm that the proposed MultiRes-NetVLAD leads to state-of-the-art Recall@N performance for global descriptor based retrieval, compared against 11 existing techniques. Source code is publicly available at https://github.com/Ahmedest61/MultiRes-NetVLAD.



### Incorporating Texture Information into Dimensionality Reduction for High-Dimensional Images
- **Arxiv ID**: http://arxiv.org/abs/2202.09179v2
- **DOI**: 10.1109/PacificVis53943.2022.00010
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.09179v2)
- **Published**: 2022-02-18 13:17:43+00:00
- **Updated**: 2022-03-02 17:30:03+00:00
- **Authors**: Alexander Vieth, Anna Vilanova, Boudewijn Lelieveldt, Elmar Eisemann, Thomas Höllt
- **Comment**: 10 pages main paper, 8 pages supplemental material. To appear at IEEE
  15th Pacific Visualization Symposium 2022
- **Journal**: None
- **Summary**: High-dimensional imaging is becoming increasingly relevant in many fields from astronomy and cultural heritage to systems biology. Visual exploration of such high-dimensional data is commonly facilitated by dimensionality reduction. However, common dimensionality reduction methods do not include spatial information present in images, such as local texture features, into the construction of low-dimensional embeddings. Consequently, exploration of such data is typically split into a step focusing on the attribute space followed by a step focusing on spatial information, or vice versa. In this paper, we present a method for incorporating spatial neighborhood information into distance-based dimensionality reduction methods, such as t-Distributed Stochastic Neighbor Embedding (t-SNE). We achieve this by modifying the distance measure between high-dimensional attribute vectors associated with each pixel such that it takes the pixel's spatial neighborhood into account. Based on a classification of different methods for comparing image patches, we explore a number of different approaches. We compare these approaches from a theoretical and experimental point of view. Finally, we illustrate the value of the proposed methods by qualitative and quantitative evaluation on synthetic data and two real-world use cases.



### Resurrecting Trust in Facial Recognition: Mitigating Backdoor Attacks in Face Recognition to Prevent Potential Privacy Breaches
- **Arxiv ID**: http://arxiv.org/abs/2202.10320v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10320v1)
- **Published**: 2022-02-18 13:53:55+00:00
- **Updated**: 2022-02-18 13:53:55+00:00
- **Authors**: Reena Zelenkova, Jack Swallow, M. A. P. Chamikara, Dongxi Liu, Mohan Baruwal Chhetri, Seyit Camtepe, Marthie Grobler, Mahathir Almashor
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Biometric data, such as face images, are often associated with sensitive information (e.g medical, financial, personal government records). Hence, a data breach in a system storing such information can have devastating consequences. Deep learning is widely utilized for face recognition (FR); however, such models are vulnerable to backdoor attacks executed by malicious parties. Backdoor attacks cause a model to misclassify a particular class as a target class during recognition. This vulnerability can allow adversaries to gain access to highly sensitive data protected by biometric authentication measures or allow the malicious party to masquerade as an individual with higher system permissions. Such breaches pose a serious privacy threat. Previous methods integrate noise addition mechanisms into face recognition models to mitigate this issue and improve the robustness of classification against backdoor attacks. However, this can drastically affect model accuracy. We propose a novel and generalizable approach (named BA-BAM: Biometric Authentication - Backdoor Attack Mitigation), that aims to prevent backdoor attacks on face authentication deep learning models through transfer learning and selective image perturbation. The empirical evidence shows that BA-BAM is highly robust and incurs a maximal accuracy drop of 2.4%, while reducing the attack success rate to a maximum of 20%. Comparisons with existing approaches show that BA-BAM provides a more practical backdoor mitigation approach for face recognition.



### Spatio-Temporal Outdoor Lighting Aggregation on Image Sequences using Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.09206v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09206v1)
- **Published**: 2022-02-18 14:11:16+00:00
- **Updated**: 2022-02-18 14:11:16+00:00
- **Authors**: Haebom Lee, Christian Homeyer, Robert Herzog, Jan Rexilius, Carsten Rother
- **Comment**: 11 pages, 7 figures, 1 table, currently under a review process
- **Journal**: None
- **Summary**: In this work, we focus on outdoor lighting estimation by aggregating individual noisy estimates from images, exploiting the rich image information from wide-angle cameras and/or temporal image sequences. Photographs inherently encode information about the scene's lighting in the form of shading and shadows. Recovering the lighting is an inverse rendering problem and as that ill-posed. Recent work based on deep neural networks has shown promising results for single image lighting estimation, but suffers from robustness. We tackle this problem by combining lighting estimates from several image views sampled in the angular and temporal domain of an image sequence. For this task, we introduce a transformer architecture that is trained in an end-2-end fashion without any statistical post-processing as required by previous work. Thereby, we propose a positional encoding that takes into account the camera calibration and ego-motion estimation to globally register the individual estimates when computing attention between visual words. We show that our method leads to improved lighting estimation while requiring less hyper-parameters compared to the state-of-the-art.



### A Survey of Vision-Language Pre-Trained Models
- **Arxiv ID**: http://arxiv.org/abs/2202.10936v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10936v2)
- **Published**: 2022-02-18 15:15:46+00:00
- **Updated**: 2022-07-16 01:27:59+00:00
- **Authors**: Yifan Du, Zikang Liu, Junyi Li, Wayne Xin Zhao
- **Comment**: Accepted by IJCAI-2022 survey track
- **Journal**: None
- **Summary**: As transformer evolves, pre-trained models have advanced at a breakneck pace in recent years. They have dominated the mainstream techniques in natural language processing (NLP) and computer vision (CV). How to adapt pre-training to the field of Vision-and-Language (V-L) learning and improve downstream task performance becomes a focus of multimodal learning. In this paper, we review the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the core content, we first briefly introduce several ways to encode raw images and texts to single-modal embeddings before pre-training. Then, we dive into the mainstream architectures of VL-PTMs in modeling the interaction between text and image representations. We further present widely-used pre-training tasks, and then we introduce some common downstream tasks. We finally conclude this paper and present some promising research directions. Our survey aims to provide researchers with synthesis and pointer to related research.



### Autoencoding Low-Resolution MRI for Semantically Smooth Interpolation of Anisotropic MRI
- **Arxiv ID**: http://arxiv.org/abs/2202.09258v1
- **DOI**: 10.1016/j.media.2022.102393
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2202.09258v1)
- **Published**: 2022-02-18 15:40:00+00:00
- **Updated**: 2022-02-18 15:40:00+00:00
- **Authors**: Jörg Sander, Bob D. de Vos, Ivana Išgum
- **Comment**: Medical Image Analysis, 2022
- **Journal**: None
- **Summary**: High-resolution medical images are beneficial for analysis but their acquisition may not always be feasible. Alternatively, high-resolution images can be created from low-resolution acquisitions using conventional upsampling methods, but such methods cannot exploit high-level contextual information contained in the images. Recently, better performing deep-learning based super-resolution methods have been introduced. However, these methods are limited by their supervised character, i.e. they require high-resolution examples for training. Instead, we propose an unsupervised deep learning semantic interpolation approach that synthesizes new intermediate slices from encoded low-resolution examples. To achieve semantically smooth interpolation in through-plane direction, the method exploits the latent space generated by autoencoders. To generate new intermediate slices, latent space encodings of two spatially adjacent slices are combined using their convex combination. Subsequently, the combined encoding is decoded to an intermediate slice. To constrain the model, a notion of semantic similarity is defined for a given dataset. For this, a new loss is introduced that exploits the spatial relationship between slices of the same volume. During training, an existing in-between slice is generated using a convex combination of its neighboring slice encodings. The method was trained and evaluated using publicly available cardiac cine, neonatal brain and adult brain MRI scans. In all evaluations, the new method produces significantly better results in terms of Structural Similarity Index Measure and Peak Signal-to-Noise Ratio (p< 0.001 using one-sided Wilcoxon signed-rank test) than a cubic B-spline interpolation approach. Given the unsupervised nature of the method, high-resolution training data is not required and hence, the method can be readily applied in clinical settings.



### (2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2202.09277v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.09277v2)
- **Published**: 2022-02-18 15:58:54+00:00
- **Updated**: 2022-03-26 16:53:26+00:00
- **Authors**: Anoop Cherian, Chiori Hori, Tim K. Marks, Jonathan Le Roux
- **Comment**: Accepted at AAAI 2022 (Oral)
- **Journal**: None
- **Summary**: Spatio-temporal scene-graph approaches to video-based reasoning tasks, such as video question-answering (QA), typically construct such graphs for every video frame. These approaches often ignore the fact that videos are essentially sequences of 2D "views" of events happening in a 3D space, and that the semantics of the 3D scene can thus be carried over from frame to frame. Leveraging this insight, we propose a (2.5+1)D scene graph representation to better capture the spatio-temporal information flows inside the videos. Specifically, we first create a 2.5D (pseudo-3D) scene graph by transforming every 2D frame to have an inferred 3D structure using an off-the-shelf 2D-to-3D transformation module, following which we register the video frames into a shared (2.5+1)D spatio-temporal space and ground each 2D scene graph within it. Such a (2.5+1)D graph is then segregated into a static sub-graph and a dynamic sub-graph, corresponding to whether the objects within them usually move in the world. The nodes in the dynamic graph are enriched with motion features capturing their interactions with other graph nodes. Next, for the video QA task, we present a novel transformer-based reasoning pipeline that embeds the (2.5+1)D graph into a spatio-temporal hierarchical latent space, where the sub-graphs and their interactions are captured at varied granularity. To demonstrate the effectiveness of our approach, we present experiments on the NExT-QA and AVSD-QA datasets. Our results show that our proposed (2.5+1)D representation leads to faster training and inference, while our hierarchical model showcases superior performance on the video QA task versus the state of the art.



### Exploring Adversarially Robust Training for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2202.09300v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.09300v2)
- **Published**: 2022-02-18 17:05:19+00:00
- **Updated**: 2022-10-04 04:40:09+00:00
- **Authors**: Shao-Yuan Lo, Vishal M. Patel
- **Comment**: Accepted at Asian Conference on Computer Vision (ACCV) 2022
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) methods aim to transfer knowledge from a labeled source domain to an unlabeled target domain. UDA has been extensively studied in the computer vision literature. Deep networks have been shown to be vulnerable to adversarial attacks. However, very little focus is devoted to improving the adversarial robustness of deep UDA models, causing serious concerns about model reliability. Adversarial Training (AT) has been considered to be the most successful adversarial defense approach. Nevertheless, conventional AT requires ground-truth labels to generate adversarial examples and train models, which limits its effectiveness in the unlabeled target domain. In this paper, we aim to explore AT to robustify UDA models: How to enhance the unlabeled data robustness via AT while learning domain-invariant features for UDA? To answer this question, we provide a systematic study into multiple AT variants that can potentially be applied to UDA. Moreover, we propose a novel Adversarially Robust Training method for UDA accordingly, referred to as ARTUDA. Extensive experiments on multiple adversarial attacks and UDA benchmarks show that ARTUDA consistently improves the adversarial robustness of UDA models. Code is available at https://github.com/shaoyuanlo/ARTUDA



### Snowflake Point Deconvolution for Point Cloud Completion and Generation with Skip-Transformer
- **Arxiv ID**: http://arxiv.org/abs/2202.09367v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09367v3)
- **Published**: 2022-02-18 17:09:49+00:00
- **Updated**: 2022-10-28 06:36:31+00:00
- **Authors**: Peng Xiang, Xin Wen, Yu-Shen Liu, Yan-Pei Cao, Pengfei Wan, Wen Zheng, Zhizhong Han
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI), 2022. This work is a journal extension of our ICCV 2021 paper
  arXiv:2108.04444 . The first two authors contributed equally
- **Journal**: None
- **Summary**: Most existing point cloud completion methods suffer from the discrete nature of point clouds and the unstructured prediction of points in local regions, which makes it difficult to reveal fine local geometric details. To resolve this issue, we propose SnowflakeNet with snowflake point deconvolution (SPD) to generate complete point clouds. SPD models the generation of point clouds as the snowflake-like growth of points, where child points are generated progressively by splitting their parent points after each SPD. Our insight into the detailed geometry is to introduce a skip-transformer in the SPD to learn the point splitting patterns that can best fit the local regions. The skip-transformer leverages attention mechanism to summarize the splitting patterns used in the previous SPD layer to produce the splitting in the current layer. The locally compact and structured point clouds generated by SPD precisely reveal the structural characteristics of the 3D shape in local patches, which enables us to predict highly detailed geometries. Moreover, since SPD is a general operation that is not limited to completion, we explore its applications in other generative tasks, including point cloud auto-encoding, generation, single image reconstruction, and upsampling. Our experimental results outperform state-of-the-art methods under widely used benchmarks.



### Unsupervised Multiple-Object Tracking with a Dynamical Variational Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2202.09315v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09315v2)
- **Published**: 2022-02-18 17:27:27+00:00
- **Updated**: 2022-02-21 13:55:45+00:00
- **Authors**: Xiaoyu Lin, Laurent Girin, Xavier Alameda-Pineda
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an unsupervised probabilistic model and associated estimation algorithm for multi-object tracking (MOT) based on a dynamical variational autoencoder (DVAE), called DVAE-UMOT. The DVAE is a latent-variable deep generative model that can be seen as an extension of the variational autoencoder for the modeling of temporal sequences. It is included in DVAE-UMOT to model the objects' dynamics, after being pre-trained on an unlabeled synthetic dataset of single-object trajectories. Then the distributions and parameters of DVAE-UMOT are estimated on each multi-object sequence to track using the principles of variational inference: Definition of an approximate posterior distribution of the latent variables and maximization of the corresponding evidence lower bound of the data likehood function. DVAE-UMOT is shown experimentally to compete well with and even surpass the performance of two state-of-the-art probabilistic MOT models. Code and data are publicly available.



### A Machine Learning Paradigm for Studying Pictorial Realism: Are Constable's Clouds More Real than His Contemporaries?
- **Arxiv ID**: http://arxiv.org/abs/2202.09348v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.09348v1)
- **Published**: 2022-02-18 18:36:01+00:00
- **Updated**: 2022-02-18 18:36:01+00:00
- **Authors**: Zhuomin Zhang, Elizabeth C. Mansfield, Jia Li, John Russell, George S. Young, Catherine Adams, James Z. Wang
- **Comment**: None
- **Journal**: None
- **Summary**: European artists have sought to create life-like images since the Renaissance. The techniques used by artists to impart realism to their paintings often rely on approaches based in mathematics, like linear perspective; yet the means used to assess the verisimilitude of realist paintings have remained subjective, even intuitive. An exploration of alternative and relatively objective methods for evaluating pictorial realism could enhance existing art historical research. We propose a machine-learning-based paradigm for studying pictorial realism in an explainable way. Unlike subjective evaluations made by art historians or computer-based painting analysis exploiting inexplicable learned features, our framework assesses realism by measuring the similarity between clouds painted by exceptionally skillful 19th-century landscape painters like John Constable and photographs of clouds. The experimental results of cloud classification show that Constable approximates more consistently than his contemporaries the formal features of actual clouds in his paintings. Our analyses suggest that artists working in the decades leading up to the invention of photography worked in a mode that anticipated some of the stylistic features of photography. The study is a springboard for deeper analyses of pictorial realism using computer vision and machine learning.



### A Molecular Prior Distribution for Bayesian Inference Based on Wilson Statistics
- **Arxiv ID**: http://arxiv.org/abs/2202.09388v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09388v2)
- **Published**: 2022-02-18 19:14:03+00:00
- **Updated**: 2022-05-02 14:48:23+00:00
- **Authors**: Marc Aurèle Gilles, Amit Singer
- **Comment**: None
- **Journal**: None
- **Summary**: Background and Objective: Wilson statistics describe well the power spectrum of proteins at high frequencies. Therefore, it has found several applications in structural biology, e.g., it is the basis for sharpening steps used in cryogenic electron microscopy (cryo-EM). A recent paper gave the first rigorous proof of Wilson statistics based on a formalism of Wilson's original argument. This new analysis also leads to statistical estimates of the scattering potential of proteins that reveal a correlation between neighboring Fourier coefficients. Here we exploit these estimates to craft a novel prior that can be used for Bayesian inference of molecular structures. Methods: We describe the properties of the prior and the computation of its hyperparameters. We then evaluate the prior on two synthetic linear inverse problems, and compare against a popular prior in cryo-EM reconstruction at a range of SNRs. Results: We show that the new prior effectively suppresses noise and fills-in low SNR regions in the spectral domain. Furthermore, it improves the resolution of estimates on the problems considered for a wide range of SNR and produces Fourier Shell Correlation curves that are insensitive to masking effects. Conclusions: We analyze the assumptions in the model, discuss relations to other regularization strategies, and postulate on potential implications for structure determination in cryo-EM.



### Equivariant Transporter Network
- **Arxiv ID**: http://arxiv.org/abs/2202.09400v5
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.09400v5)
- **Published**: 2022-02-18 19:41:53+00:00
- **Updated**: 2022-09-20 20:22:38+00:00
- **Authors**: Haojie Huang, Dian Wang, Robin Walters, Robert Platt
- **Comment**: Project Website: https://haojhuang.github.io/etp_page/
- **Journal**: RSS 2022
- **Summary**: Transporter Net is a recently proposed framework for pick and place that is able to learn good manipulation policies from a very few expert demonstrations. A key reason why Transporter Net is so sample efficient is that the model incorporates rotational equivariance into the pick module, i.e. the model immediately generalizes learned pick knowledge to objects presented in different orientations. This paper proposes a novel version of Transporter Net that is equivariant to both pick and place orientation. As a result, our model immediately generalizes place knowledge to different place orientations in addition to generalizing pick knowledge as before. Ultimately, our new model is more sample efficient and achieves better pick and place success rates than the baseline Transporter Net model.



### Learning Representations Robust to Group Shifts and Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/2202.09446v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.09446v1)
- **Published**: 2022-02-18 22:06:25+00:00
- **Updated**: 2022-02-18 22:06:25+00:00
- **Authors**: Ming-Chang Chiu, Xuezhe Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the high performance achieved by deep neural networks on various tasks, extensive studies have demonstrated that small tweaks in the input could fail the model predictions. This issue of deep neural networks has led to a number of methods to improve model robustness, including adversarial training and distributionally robust optimization. Though both of these two methods are geared towards learning robust models, they have essentially different motivations: adversarial training attempts to train deep neural networks against perturbations, while distributional robust optimization aims at improving model performance on the most difficult "uncertain distributions". In this work, we propose an algorithm that combines adversarial training and group distribution robust optimization to improve robust representation learning. Experiments on three image benchmark datasets illustrate that the proposed method achieves superior results on robust metrics without sacrificing much of the standard measures.



### Modern Augmented Reality: Applications, Trends, and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/2202.09450v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.09450v2)
- **Published**: 2022-02-18 22:12:37+00:00
- **Updated**: 2022-02-24 23:59:00+00:00
- **Authors**: Shervin Minaee, Xiaodan Liang, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Augmented reality (AR) is one of the relatively old, yet trending areas in the intersection of computer vision and computer graphics with numerous applications in several areas, from gaming and entertainment, to education and healthcare. Although it has been around for nearly fifty years, it has seen a lot of interest by the research community in the recent years, mainly because of the huge success of deep learning models for various computer vision and AR applications, which made creating new generations of AR technologies possible. This work tries to provide an overview of modern augmented reality, from both application-level and technical perspective. We first give an overview of main AR applications, grouped into more than ten categories. We then give an overview of around 100 recent promising machine learning based works developed for AR systems, such as deep learning works for AR shopping (clothing, makeup), AR based image filters (such as Snapchat's lenses), AR animations, and more. In the end we discuss about some of the current challenges in AR domain, and the future directions in this area.



### BLPnet: A new DNN model and Bengali OCR engine for Automatic License Plate Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.12250v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2202.12250v1)
- **Published**: 2022-02-18 22:58:53+00:00
- **Updated**: 2022-02-18 22:58:53+00:00
- **Authors**: Md. Saif Hassan Onim, Hussain Nyeem, Koushik Roy, Mahmudul Hasan, Abtahi Ishmam, Md. Akiful Hoque Akif, Tareque Bashar Ovi
- **Comment**: Submitted to Neurocomputing
  (https://www.sciencedirect.com/journal/neurocomputing/about/aims-and-scope)
- **Journal**: None
- **Summary**: The development of the Automatic License Plate Recognition (ALPR) system has received much attention for the English license plate. However, despite being the sixth largest population around the world, no significant progress can be tracked in the Bengali language countries or states for the ALPR system addressing their more alarming traffic management with inadequate road-safety measures. This paper reports a computationally efficient and reasonably accurate Automatic License Plate Recognition (ALPR) system for Bengali characters with a new end-to-end DNN model that we call Bengali License Plate Network(BLPnet). The cascaded architecture for detecting vehicle regions prior to vehicle license plate (VLP) in the model is proposed to eliminate false positives resulting in higher detection accuracy of VLP. Besides, a lower set of trainable parameters is considered for reducing the computational cost making the system faster and more compatible for a real-time application. With a Computational Neural Network (CNN)based new Bengali OCR engine and word-mapping process, the model is characters rotation invariant, and can readily extract, detect and output the complete license plate number of a vehicle. The model feeding with17 frames per second (fps) on real-time video footage can detect a vehicle with the Mean Squared Error (MSE) of 0.0152, and the mean license plate character recognition accuracy of 95%. While compared to the other models, an improvement of 5% and 20% were recorded for the BLPnetover the prominent YOLO-based ALPR model and the Tesseract model for the number-plate detection accuracy and time requirement, respectively.



