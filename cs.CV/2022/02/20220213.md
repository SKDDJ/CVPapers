# Arxiv Papers in cs.CV on 2022-02-13
### Source-Free Progressive Graph Learning for Open-Set Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2202.06174v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06174v2)
- **Published**: 2022-02-13 01:19:41+00:00
- **Updated**: 2023-01-23 01:58:50+00:00
- **Authors**: Yadan Luo, Zijian Wang, Zhuoxiao Chen, Zi Huang, Mahsa Baktashmotlagh
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2006.12087
- **Journal**: None
- **Summary**: Open-set domain adaptation (OSDA) has gained considerable attention in many visual recognition tasks. However, most existing OSDA approaches are limited due to three main reasons, including: (1) the lack of essential theoretical analysis of generalization bound, (2) the reliance on the coexistence of source and target data during adaptation, and (3) failing to accurately estimate the uncertainty of model predictions. We propose a Progressive Graph Learning (PGL) framework that decomposes the target hypothesis space into the shared and unknown subspaces, and then progressively pseudo-labels the most confident known samples from the target domain for hypothesis adaptation. Moreover, we tackle a more realistic source-free open-set domain adaptation (SF-OSDA) setting that makes no assumption about the coexistence of source and target domains, and introduce a balanced pseudo-labeling (BP-L) strategy in a two-stage framework, namely SF-PGL. Different from PGL that applies a class-agnostic constant threshold for all target samples for pseudo-labeling, the SF-PGL model uniformly selects the most confident target instances from each category at a fixed ratio. The confidence thresholds in each class are regarded as the 'uncertainty' of learning the semantic information, which are then used to weigh the classification loss in the adaptation step. We conducted unsupervised and semi-supervised OSDA and SF-OSDA experiments on the benchmark image classification and action recognition datasets. Additionally, we find that balanced pseudo-labeling plays a significant role in improving calibration, which makes the trained model less prone to over-confident or under-confident predictions on the target data. Source code is available at https://github.com/Luoyadan/SF-PGL.



### Data standardization for robust lip sync
- **Arxiv ID**: http://arxiv.org/abs/2202.06198v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06198v2)
- **Published**: 2022-02-13 04:09:21+00:00
- **Updated**: 2023-01-23 14:50:26+00:00
- **Authors**: Chun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Lip sync is a fundamental audio-visual task. However, existing lip sync methods fall short of being robust to the incredible diversity of videos taken in the wild, and the majority of the diversity is caused by compound distracting factors that could degrade existing lip sync methods. To address these issues, this paper proposes a data standardization pipeline that can produce standardized expressive images while preserving lip motion information from the input and reducing the effects of compound distracting factors. Based on recent advances in 3D face reconstruction, we first create a model that can consistently disentangle expressions, with lip motion information embedded. Then, to reduce the effects of compound distracting factors on synthesized images, we synthesize images with only expressions from the input, intentionally setting all other attributes at predefined values independent of the input. Using synthesized images, existing lip sync methods improve their data efficiency and robustness, and they achieve competitive performance for the active speaker detection task.



### Unsupervised Disentanglement with Tensor Product Representations on the Torus
- **Arxiv ID**: http://arxiv.org/abs/2202.06201v1
- **DOI**: None
- **Categories**: **cs.LG**, cond-mat.dis-nn, cs.CV, quant-ph
- **Links**: [PDF](http://arxiv.org/pdf/2202.06201v1)
- **Published**: 2022-02-13 04:23:12+00:00
- **Updated**: 2022-02-13 04:23:12+00:00
- **Authors**: Michael Rotman, Amit Dekel, Shir Gur, Yaron Oz, Lior Wolf
- **Comment**: Accepted to ICLR 2022
- **Journal**: None
- **Summary**: The current methods for learning representations with auto-encoders almost exclusively employ vectors as the latent representations. In this work, we propose to employ a tensor product structure for this purpose. This way, the obtained representations are naturally disentangled. In contrast to the conventional variations methods, which are targeted toward normally distributed features, the latent space in our representation is distributed uniformly over a set of unit circles. We argue that the torus structure of the latent space captures the generative factors effectively. We employ recent tools for measuring unsupervised disentanglement, and in an extensive set of experiments demonstrate the advantage of our method in terms of disentanglement, completeness, and informativeness. The code for our proposed method is available at https://github.com/rotmanmi/Unsupervised-Disentanglement-Torus.



### Robust Deepfake On Unrestricted Media: Generation And Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.06228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06228v1)
- **Published**: 2022-02-13 06:53:39+00:00
- **Updated**: 2022-02-13 06:53:39+00:00
- **Authors**: Trung-Nghia Le, Huy H Nguyen, Junichi Yamagishi, Isao Echizen
- **Comment**: This article will appear as one chapter for a new book called
  Frontiers in Fake Media Generation and Detection, edited by Mahdi Khosravy,
  Isao Echizen, and Noboru Babaguchi
- **Journal**: None
- **Summary**: Recent advances in deep learning have led to substantial improvements in deepfake generation, resulting in fake media with a more realistic appearance. Although deepfake media have potential application in a wide range of areas and are drawing much attention from both the academic and industrial communities, it also leads to serious social and criminal concerns. This chapter explores the evolution of and challenges in deepfake generation and detection. It also discusses possible ways to improve the robustness of deepfake detection for a wide variety of media (e.g., in-the-wild images and videos). Finally, it suggests a focus for future fake media research.



### FairStyle: Debiasing StyleGAN2 with Style Channel Manipulations
- **Arxiv ID**: http://arxiv.org/abs/2202.06240v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.06240v1)
- **Published**: 2022-02-13 07:39:48+00:00
- **Updated**: 2022-02-13 07:39:48+00:00
- **Authors**: Cemre Karakas, Alara Dirik, Eylul Yalcinkaya, Pinar Yanardag
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in generative adversarial networks have shown that it is possible to generate high-resolution and hyperrealistic images. However, the images produced by GANs are only as fair and representative as the datasets on which they are trained. In this paper, we propose a method for directly modifying a pre-trained StyleGAN2 model that can be used to generate a balanced set of images with respect to one (e.g., eyeglasses) or more attributes (e.g., gender and eyeglasses). Our method takes advantage of the style space of the StyleGAN2 model to perform disentangled control of the target attributes to be debiased. Our method does not require training additional models and directly debiases the GAN model, paving the way for its use in various downstream applications. Our experiments show that our method successfully debiases the GAN model within a few minutes without compromising the quality of the generated images. To promote fair generative models, we share the code and debiased models at http://catlab-team.github.io/fairstyle.



### Privacy protection based on mask template
- **Arxiv ID**: http://arxiv.org/abs/2202.06250v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.06250v1)
- **Published**: 2022-02-13 08:11:04+00:00
- **Updated**: 2022-02-13 08:11:04+00:00
- **Authors**: Hao Wang, Yu Bai, Guangmin Sun, Jie Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Powerful recognition algorithms are widely used in the Internet or important medical systems, which poses a serious threat to personal privacy. Although the law provides for diversity protection, e.g. The General Data Protection Regulation (GDPR) in Europe and Articles 1032 to 1039 of the civil code in China. However, as an important privacy disclosure event, biometric data is often hidden, which is difficult for the owner to detect and trace to the source. Human biometrics generally exist in images. In order to avoid the disclosure of personal privacy, we should prevent unauthorized recognition algorithms from acquiring the real features of the original image.



### Autonomous Drone Swarm Navigation and Multi-target Tracking in 3D Environments with Dynamic Obstacles
- **Arxiv ID**: http://arxiv.org/abs/2202.06253v1
- **DOI**: 10.1109/ACCESS.2022.3202208
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.06253v1)
- **Published**: 2022-02-13 08:26:28+00:00
- **Updated**: 2022-02-13 08:26:28+00:00
- **Authors**: Suleman Qamar, Saddam Hussain Khan, Muhammad Arif Arshad, Maryam Qamar, Asifullah Khan
- **Comment**: Pages: 19, Figures: 17, Tables: 8
- **Journal**: None
- **Summary**: Autonomous modeling of artificial swarms is necessary because manual creation is a time intensive and complicated procedure which makes it impractical. An autonomous approach employing deep reinforcement learning is presented in this study for swarm navigation. In this approach, complex 3D environments with static and dynamic obstacles and resistive forces (like linear drag, angular drag, and gravity) are modeled to track multiple dynamic targets. Moreover, reward functions for robust swarm formation and target tracking are devised for learning complex swarm behaviors. Since the number of agents is not fixed and has only the partial observance of the environment, swarm formation and navigation become challenging. In this regard, the proposed strategy consists of three main phases to tackle the aforementioned challenges: 1) A methodology for dynamic swarm management, 2) Avoiding obstacles, Finding the shortest path towards the targets, 3) Tracking the targets and Island modeling. The dynamic swarm management phase translates basic sensory input to high level commands to enhance swarm navigation and decentralized setup while maintaining the swarms size fluctuations. While, in the island modeling, the swarm can split into individual subswarms according to the number of targets, conversely, these subswarms may join to form a single huge swarm, giving the swarm ability to track multiple targets. Customized state of the art policy based deep reinforcement learning algorithms are employed to achieve significant results. The promising results show that our proposed strategy enhances swarm navigation and can track multiple static and dynamic targets in complex dynamic environments.



### RandomSEMO: Normality Learning Of Moving Objects For Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.06256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06256v1)
- **Published**: 2022-02-13 08:39:49+00:00
- **Updated**: 2022-02-13 08:39:49+00:00
- **Authors**: Chaewon Park, Minhyeok Lee, MyeongAh Cho, Sangyoun Lee
- **Comment**: 4 pages, ICIP 2022 under review
- **Journal**: None
- **Summary**: Recent anomaly detection algorithms have shown powerful performance by adopting frame predicting autoencoders. However, these methods face two challenging circumstances. First, they are likely to be trained to be excessively powerful, generating even abnormal frames well, which leads to failure in detecting anomalies. Second, they are distracted by the large number of objects captured in both foreground and background. To solve these problems, we propose a novel superpixel-based video data transformation technique named Random Superpixel Erasing on Moving Objects (RandomSEMO) and Moving Object Loss (MOLoss), built on top of a simple lightweight autoencoder. RandomSEMO is applied to the moving object regions by randomly erasing their superpixels. It enforces the network to pay attention to the foreground objects and learn the normal features more effectively, rather than simply predicting the future frame. Moreover, MOLoss urges the model to focus on learning normal objects captured within RandomSEMO by amplifying the loss on the pixels near the moving objects. The experimental results show that our model outperforms state-of-the-arts on three benchmarks.



### LTSP: Long-Term Slice Propagation for Accurate Airway Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.06260v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.06260v1)
- **Published**: 2022-02-13 08:47:01+00:00
- **Updated**: 2022-02-13 08:47:01+00:00
- **Authors**: Yangqian Wu, Minghui Zhang, Weihao Yu, Hao Zheng, Jiasheng Xu, Yun Gu
- **Comment**: Accepted by IPCAI 2022
- **Journal**: None
- **Summary**: Purpose: Bronchoscopic intervention is a widely-used clinical technique for pulmonary diseases, which requires an accurate and topological complete airway map for its localization and guidance. The airway map could be extracted from chest computed tomography (CT) scans automatically by airway segmentation methods. Due to the complex tree-like structure of the airway, preserving its topology completeness while maintaining the segmentation accuracy is a challenging task.   Methods: In this paper, a long-term slice propagation (LTSP) method is proposed for accurate airway segmentation from pathological CT scans. We also design a two-stage end-to-end segmentation framework utilizing the LTSP method in the decoding process. Stage 1 is used to generate a coarse feature map by an encoder-decoder architecture. Stage 2 is to adopt the proposed LTSP method for exploiting the continuity information and enhancing the weak airway features in the coarse feature map. The final segmentation result is predicted from the refined feature map.   Results: Extensive experiments were conducted to evaluate the performance of the proposed method on 70 clinical CT scans. The results demonstrate the considerable improvements of the proposed method compared to some state-of-the-art methods as most breakages are eliminated and more tiny bronchi are detected. The ablation studies further confirm the effectiveness of the constituents of the proposed method.   Conclusion: Slice continuity information is beneficial to accurate airway segmentation. Furthermore, by propagating the long-term slice feature, the airway topology connectivity is preserved with overall segmentation accuracy maintained.



### LighTN: Light-weight Transformer Network for Performance-overhead Tradeoff in Point Cloud Downsampling
- **Arxiv ID**: http://arxiv.org/abs/2202.06263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06263v1)
- **Published**: 2022-02-13 08:55:53+00:00
- **Updated**: 2022-02-13 08:55:53+00:00
- **Authors**: Xu Wang, Yi Jin, Yigang Cen, Tao Wang, Bowen Tang, Yidong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Compared with traditional task-irrelevant downsampling methods, task-oriented neural networks have shown improved performance in point cloud downsampling range. Recently, Transformer family of networks has shown a more powerful learning capacity in visual tasks. However, Transformer-based architectures potentially consume too many resources which are usually worthless for low overhead task networks in downsampling range. This paper proposes a novel light-weight Transformer network (LighTN) for task-oriented point cloud downsampling, as an end-to-end and plug-and-play solution. In LighTN, a single-head self-correlation module is presented to extract refined global contextual features, where three projection matrices are simultaneously eliminated to save resource overhead, and the output of symmetric matrix satisfies the permutation invariant. Then, we design a novel downsampling loss function to guide LighTN focuses on critical point cloud regions with more uniform distribution and prominent points coverage. Furthermore, We introduce a feed-forward network scaling mechanism to enhance the learnable capacity of LighTN according to the expand-reduce strategy. The result of extensive experiments on classification and registration tasks demonstrates LighTN can achieve state-of-the-art performance with limited resource overhead.



### Improve Deep Image Inpainting by Emphasizing the Complexity of Missing Regions
- **Arxiv ID**: http://arxiv.org/abs/2202.06266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06266v1)
- **Published**: 2022-02-13 09:14:52+00:00
- **Updated**: 2022-02-13 09:14:52+00:00
- **Authors**: Yufeng Wang, Dan Li, Cong Xu, Min Yang
- **Comment**: 8pages
- **Journal**: None
- **Summary**: Deep image inpainting research mainly focuses on constructing various neural network architectures or imposing novel optimization objectives. However, on the one hand, building a state-of-the-art deep inpainting model is an extremely complex task, and on the other hand, the resulting performance gains are sometimes very limited. We believe that besides the frameworks of inpainting models, lightweight traditional image processing techniques, which are often overlooked, can actually be helpful to these deep models. In this paper, we enhance the deep image inpainting models with the help of classical image complexity metrics. A knowledge-assisted index composed of missingness complexity and forward loss is presented to guide the batch selection in the training procedure. This index helps find samples that are more conducive to optimization in each iteration and ultimately boost the overall inpainting performance. The proposed approach is simple and can be plugged into many deep inpainting models by changing only a few lines of code. We experimentally demonstrate the improvements for several recently developed image inpainting models on various datasets.



### BViT: Broad Attention based Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2202.06268v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06268v2)
- **Published**: 2022-02-13 09:23:29+00:00
- **Updated**: 2023-06-09 06:08:37+00:00
- **Authors**: Nannan Li, Yaran Chen, Weifan Li, Zixiang Ding, Dongbin Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works have demonstrated that transformer can achieve promising performance in computer vision, by exploiting the relationship among image patches with self-attention. While they only consider the attention in a single feature layer, but ignore the complementarity of attention in different levels. In this paper, we propose the broad attention to improve the performance by incorporating the attention relationship of different layers for vision transformer, which is called BViT. The broad attention is implemented by broad connection and parameter-free attention. Broad connection of each transformer layer promotes the transmission and integration of information for BViT. Without introducing additional trainable parameters, parameter-free attention jointly focuses on the already available attention information in different layers for extracting useful information and building their relationship. Experiments on image classification tasks demonstrate that BViT delivers state-of-the-art accuracy of 74.8\%/81.6\% top-1 accuracy on ImageNet with 5M/22M parameters. Moreover, we transfer BViT to downstream object recognition benchmarks to achieve 98.9\% and 89.9\% on CIFAR10 and CIFAR100 respectively that exceed ViT with fewer parameters. For the generalization test, the broad attention in Swin Transformer and T2T-ViT also bring an improvement of more than 1\%. To sum up, broad attention is promising to promote the performance of attention based models. Code and pre-trained models are available at https://github.com/DRL-CASIA/Broad_ViT.



### Natural Image Stitching Using Depth Maps
- **Arxiv ID**: http://arxiv.org/abs/2202.06276v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06276v2)
- **Published**: 2022-02-13 10:05:53+00:00
- **Updated**: 2023-02-22 10:21:46+00:00
- **Authors**: Tianli Liao, Nan Li
- **Comment**: 10 pages, 8 figures, under review
- **Journal**: None
- **Summary**: Natural image stitching (NIS) aims to create one natural-looking mosaic from two overlapping images that capture the same 3D scene from different viewing positions. Challenges inevitably arise when the scene is non-planar and the camera baseline is wide, since parallax becomes not negligible in such cases. In this paper, we propose a novel NIS method using depth maps, which generates natural-looking mosaics against parallax in both overlapping and non-overlapping regions. Firstly, we construct a robust fitting method to filter out the outliers in feature matches and estimate the epipolar geometry between input images. Then, we draw a triangulation of the target image and estimate multiple local homographies, one per triangle, based on the locations of their vertices, the rectified depth values and the epipolar geometry. Finally, the warping image is rendered by the backward mapping of piece-wise homographies. Panorama is then produced via average blending and image inpainting. Experimental results demonstrate that the proposed method not only provides accurate alignment in the overlapping regions but also virtual naturalness in the non-overlapping region.



### Zero-Reference Image Restoration for Under-Display Camera of UAV
- **Arxiv ID**: http://arxiv.org/abs/2202.06283v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06283v1)
- **Published**: 2022-02-13 11:12:00+00:00
- **Updated**: 2022-02-13 11:12:00+00:00
- **Authors**: Zhuoran Zheng, Xiuyi Jia, Yunliang Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: The exposed cameras of UAV can shake, shift, or even malfunction under the influence of harsh weather, while the add-on devices (Dupont lines) are very vulnerable to damage.   We can place a low-cost T-OLED overlay around the camera to protect it, but this would also introduce image degradation issues.   In particular, the temperature variations in the atmosphere can create mist that adsorbs to the T-OLED, which can cause secondary disasters (i.e., more severe image degradation) during the UAV's filming process.   To solve the image degradation problem caused by overlaying T-OLEDs, in this paper we propose a new method to enhance the visual experience by enhancing the texture and color of images.   Specifically, our method trains a lightweight network to estimate a low-rank affine grid on the input image, and then utilizes the grid to enhance the input image at block granularity.   The advantages of our method are that no reference image is required and the loss function is developed from visual experience.   In addition, our model can perform high-quality recovery of images of arbitrary resolution in real time.   In the end, the limitations of our model and the collected datasets (including the daytime and nighttime scenes) are discussed.



### Motion Sickness Modeling with Visual Vertical Estimation and Its Application to Autonomous Personal Mobility Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2202.06299v4
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.06299v4)
- **Published**: 2022-02-13 12:46:19+00:00
- **Updated**: 2022-05-12 11:58:41+00:00
- **Authors**: Hailong Liu, Shota Inoue, Takahiro Wada
- **Comment**: https://www.researchgate.net/publication/358703507 Accepted as a
  Contributed Paper by IEEE IV2022
- **Journal**: None
- **Summary**: Passengers (drivers) of level 3-5 autonomous personal mobility vehicles (APMV) and cars can perform non-driving tasks, such as reading books and smartphones, while driving. It has been pointed out that such activities may increase motion sickness. Many studies have been conducted to build countermeasures, of which various computational motion sickness models have been developed. Many of these are based on subjective vertical conflict (SVC) theory, which describes vertical changes in direction sensed by human sensory organs vs. those expected by the central nervous system. Such models are expected to be applied to autonomous driving scenarios. However, no current computational model can integrate visual vertical information with vestibular sensations.   We proposed a 6 DoF SVC-VV model which add a visually perceived vertical block into a conventional six-degrees-of-freedom SVC model to predict VV directions from image data simulating the visual input of a human. Hence, a simple image-based VV estimation method is proposed.   As the validation of the proposed model, this paper focuses on describing the fact that the motion sickness increases as a passenger reads a book while using an AMPV, assuming that visual vertical (VV) plays an important role. In the static experiment, it is demonstrated that the estimated VV by the proposed method accurately described the gravitational acceleration direction with a low mean absolute deviation. In addition, the results of the driving experiment using an APMV demonstrated that the proposed 6 DoF SVC-VV model could describe that the increased motion sickness experienced when the VV and gravitational acceleration directions were different.



### Deep Graph Learning for Spatially-Varying Indoor Lighting Prediction
- **Arxiv ID**: http://arxiv.org/abs/2202.06300v1
- **DOI**: 10.1007/s11432-022-3576-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06300v1)
- **Published**: 2022-02-13 12:49:37+00:00
- **Updated**: 2022-02-13 12:49:37+00:00
- **Authors**: Jiayang Bai, Jie Guo, Chenchen Wan, Zhenyu Chen, Zhen He, Shan Yang, Piaopiao Yu, Yan Zhang, Yanwen Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Lighting prediction from a single image is becoming increasingly important in many vision and augmented reality (AR) applications in which shading and shadow consistency between virtual and real objects should be guaranteed. However, this is a notoriously ill-posed problem, especially for indoor scenarios, because of the complexity of indoor luminaires and the limited information involved in 2D images. In this paper, we propose a graph learning-based framework for indoor lighting estimation. At its core is a new lighting model (dubbed DSGLight) based on depth-augmented Spherical Gaussians (SG) and a Graph Convolutional Network (GCN) that infers the new lighting representation from a single LDR image of limited field-of-view. Our lighting model builds 128 evenly distributed SGs over the indoor panorama, where each SG encoding the lighting and the depth around that node. The proposed GCN then learns the mapping from the input image to DSGLight. Compared with existing lighting models, our DSGLight encodes both direct lighting and indirect environmental lighting more faithfully and compactly. It also makes network training and inference more stable. The estimated depth distribution enables temporally stable shading and shadows under spatially-varying lighting. Through thorough experiments, we show that our method obviously outperforms existing methods both qualitatively and quantitatively.



### Progressive Backdoor Erasing via connecting Backdoor and Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2202.06312v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.06312v4)
- **Published**: 2022-02-13 13:41:15+00:00
- **Updated**: 2022-12-27 01:16:05+00:00
- **Authors**: Bingxu Mu, Zhenxing Niu, Le Wang, Xue Wang, Rong Jin, Gang Hua
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are known to be vulnerable to both backdoor attacks as well as adversarial attacks. In the literature, these two types of attacks are commonly treated as distinct problems and solved separately, since they belong to training-time and inference-time attacks respectively. However, in this paper we find an intriguing connection between them: for a model planted with backdoors, we observe that its adversarial examples have similar behaviors as its triggered images, i.e., both activate the same subset of DNN neurons. It indicates that planting a backdoor into a model will significantly affect the model's adversarial examples. Based on these observations, a novel Progressive Backdoor Erasing (PBE) algorithm is proposed to progressively purify the infected model by leveraging untargeted adversarial attacks. Different from previous backdoor defense methods, one significant advantage of our approach is that it can erase backdoor even when the clean extra dataset is unavailable. We empirically show that, against 5 state-of-the-art backdoor attacks, our PBE can effectively erase the backdoor without obvious performance degradation on clean samples and significantly outperforms existing defense methods.



### A Data Augmentation Method for Fully Automatic Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.06344v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.06344v2)
- **Published**: 2022-02-13 15:28:49+00:00
- **Updated**: 2022-02-18 02:55:59+00:00
- **Authors**: Yu Wang, Yarong Ji, Hongbing Xiao
- **Comment**: 15 pages, 7 figures, 4tables
- **Journal**: None
- **Summary**: Automatic segmentation of glioma and its subregions is of great significance for diagnosis, treatment and monitoring of disease. In this paper, an augmentation method, called TensorMixup, was proposed and applied to the three dimensional U-Net architecture for brain tumor segmentation. The main ideas included that first, two image patches with size of 128 in three dimensions were selected according to glioma information of ground truth labels from the magnetic resonance imaging data of any two patients with the same modality. Next, a tensor in which all elements were independently sampled from Beta distribution was used to mix the image patches. Then the tensor was mapped to a matrix which was used to mix the one-hot encoded labels of the above image patches. Therefore, a new image and its one-hot encoded label were synthesized. Finally, the new data was used to train the model which could be used to segment glioma. The experimental results show that the mean accuracy of Dice scores are 91.32%, 85.67%, and 82.20% respectively on the whole tumor, tumor core, and enhancing tumor segmentation, which proves that the proposed TensorMixup is feasible and effective for brain tumor segmentation.



### Do Inpainting Yourself: Generative Facial Inpainting Guided by Exemplars
- **Arxiv ID**: http://arxiv.org/abs/2202.06358v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.MM, I.4.9; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2202.06358v3)
- **Published**: 2022-02-13 16:29:45+00:00
- **Updated**: 2022-08-14 14:56:57+00:00
- **Authors**: Wanglong Lu, Hanli Zhao, Xianta Jiang, Xiaogang Jin, Yongliang Yang, Min Wang, Jiankai Lyu, Kaijie Shi
- **Comment**: There are 16 pages, 18 figures in this paper. Homepage:
  https://longlongaaago.github.io/EXE-GAN/
- **Journal**: None
- **Summary**: We present EXE-GAN, a novel exemplar-guided facial inpainting framework using generative adversarial networks. Our approach can not only preserve the quality of the input facial image but also complete the image with exemplar-like facial attributes. We achieve this by simultaneously leveraging the global style of the input image, the stochastic style generated from the random latent code, and the exemplar style of exemplar image. We introduce a novel attribute similarity metric to encourage networks to learn the style of facial attributes from the exemplar in a self-supervised way. To guarantee the natural transition across the boundaries of inpainted regions, we introduce a novel spatial variant gradient backpropagation technique to adjust the loss gradients based on the spatial location. Extensive evaluations and practical applications on public CelebA-HQ and FFHQ datasets validate the superiority of EXE-GAN in terms of the visual quality in facial inpainting.



### Learning Perspective Deformation in X-Ray Transmission Imaging
- **Arxiv ID**: http://arxiv.org/abs/2202.06366v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.06366v3)
- **Published**: 2022-02-13 17:25:01+00:00
- **Updated**: 2023-01-04 14:29:34+00:00
- **Authors**: Yixing Huang, Andreas Maier, Fuxin Fan, Björn Kreher, Xiaolin Huang, Rainer Fietkau, Christoph Bert, Florian Putz
- **Comment**: 19 pages, 26 figures
- **Journal**: None
- **Summary**: In cone-beam X-ray transmission imaging, perspective deformation causes difficulty in direct, accurate geometric assessments of anatomical structures. In this work, the perspective deformation correction problem is formulated and addressed in a framework using two complementary (180{\deg}) views. The complementary view setting provides a practical way to identify perspectively deformed structures by assessing the deviation between the two views. It also provides bounding information and reduces uncertainty for learning perspective deformation. Two representative networks Pix2pixGAN and TransU-Net for correcting perspective deformation are investigated. Experiments on numerical bead phantom data demonstrate the advantage of complementary views over orthogonal views or a single view. They show that Pix2pixGAN as a fully convolutional network achieves better performance in polar space than Cartesian space, while TransU-Net as a transformer-based hybrid network achieves comparable performance in Cartesian space to polar space. Further study demonstrates that the trained model has certain tolerance to geometric inaccuracy within calibration accuracy. The efficacy of the proposed framework on synthetic projection images from patients' chest and head data as well as real cadaver CBCT projection data and its robustness in the presence of bulky metal implants and surgical screws indicate the promising aspects of future real applications.



### Omnifont Persian OCR System Using Primitives
- **Arxiv ID**: http://arxiv.org/abs/2202.06371v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.06371v1)
- **Published**: 2022-02-13 17:43:40+00:00
- **Updated**: 2022-02-13 17:43:40+00:00
- **Authors**: Azarakhsh Keipour, Mohammad Eshghi, Sina Mohammadzadeh Ghadikolaei, Negin Mohammadi, Shahab Ensafi
- **Comment**: Accepted in IEEE International Conference on Industrial Technology
  (ICIT 2013); Cape Town, South Africa, 25-27th February 2013 (Not Presented)
- **Journal**: None
- **Summary**: In this paper, we introduce a model-based omnifont Persian OCR system. The system uses a set of 8 primitive elements as structural features for recognition. First, the scanned document is preprocessed. After normalizing the preprocessed image, text rows and sub-words are separated and then thinned. After recognition of dots in sub-words, strokes are extracted and primitive elements of each sub-word are recognized using the strokes. Finally, the primitives are compared with a predefined set of character identification vectors in order to identify sub-word characters. The separation and recognition steps of the system are concurrent, eliminating unavoidable errors of independent separation of letters. The system has been tested on documents with 14 standard Persian fonts in 6 sizes. The achieved precision is 97.06%.



### A Survey of Deep Learning Techniques for the Analysis of COVID-19 and their usability for Detecting Omicron
- **Arxiv ID**: http://arxiv.org/abs/2202.06372v2
- **DOI**: 10.1080/0952813X.2023.2165724
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.06372v2)
- **Published**: 2022-02-13 17:44:33+00:00
- **Updated**: 2022-04-04 17:53:02+00:00
- **Authors**: Asifullah Khan, Saddam Hussain Khan, Mahrukh Saif, Asiya Batool, Anabia Sohail, Muhammad Waleed Khan
- **Comment**: Pages: 44, Figures: 7, Tables: 14
- **Journal**: None
- **Summary**: The Coronavirus (COVID-19) outbreak in December 2019 has become an ongoing threat to humans worldwide, creating a health crisis that infected millions of lives, as well as devastating the global economy. Deep learning (DL) techniques have proved helpful in analysis and delineation of infectious regions in radiological images in a timely manner. This paper makes an in-depth survey of DL techniques and draws a taxonomy based on diagnostic strategies and learning approaches. DL techniques are systematically categorized into classification, segmentation, and multi-stage approaches for COVID-19 diagnosis at image and region level analysis. Each category includes pre-trained and custom-made Convolutional Neural Network architectures for detecting COVID-19 infection in radiographic imaging modalities; X-Ray, and Computer Tomography (CT). Furthermore, a discussion is made on challenges in developing diagnostic techniques such as cross-platform interoperability and examining imaging modality. Similarly, a review of the various methodologies and performance measures used in these techniques is also presented. This survey provides an insight into the promising areas of research in DL for analyzing radiographic images, and further accelerates the research in designing customized DL based diagnostic tools for effectively dealing with new variants of COVID-19 and emerging challenges.



### Scheduling Techniques for Liver Segmentation: ReduceLRonPlateau Vs OneCycleLR
- **Arxiv ID**: http://arxiv.org/abs/2202.06373v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.06373v1)
- **Published**: 2022-02-13 17:52:49+00:00
- **Updated**: 2022-02-13 17:52:49+00:00
- **Authors**: Ayman Al-Kababji, Faycal Bensaali, Sarada Prasad Dakua
- **Comment**: 8 pages, 4 figures, 1 table, currently submitted The 2nd
  International Conference on Intelligent Systems and Patterns Recognition
  (ISPR'2022)
- **Journal**: None
- **Summary**: Machine learning and computer vision techniques have influenced many fields including the biomedical one. The aim of this paper is to investigate the important concept of schedulers in manipulating the learning rate (LR), for the liver segmentation task, throughout the training process, focusing on the newly devised OneCycleLR against the ReduceLRonPlateau. A dataset, published in 2018 and produced by the Medical Segmentation Decathlon Challenge organizers, called Task 8 Hepatic Vessel (MSDC-T8) has been used for testing and validation. The reported results that have the same number of maximum epochs (75), and are the average of 5-fold cross-validation, indicate that ReduceLRonPlateau converges faster while maintaining a similar or even better loss score on the validation set when compared to OneCycleLR. The epoch at which the peak LR occurs perhaps should be made early for the OneCycleLR such that the super-convergence feature can be observed. Moreover, the overall results outperform the state-of-the-art results from the researchers who published the liver masks for this dataset. To conclude, both schedulers are suitable for medical segmentation challenges, especially the MSDC-T8 dataset, and can be used confidently in rapidly converging the validation loss with a minimal number of epochs.



### Visual Sound Localization in the Wild by Cross-Modal Interference Erasing
- **Arxiv ID**: http://arxiv.org/abs/2202.06406v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2202.06406v1)
- **Published**: 2022-02-13 21:06:19+00:00
- **Updated**: 2022-02-13 21:06:19+00:00
- **Authors**: Xian Liu, Rui Qian, Hang Zhou, Di Hu, Weiyao Lin, Ziwei Liu, Bolei Zhou, Xiaowei Zhou
- **Comment**: Accepted by AAAI Conference on Artificial Intelligence (AAAI) 2022.
  16 pages
- **Journal**: None
- **Summary**: The task of audio-visual sound source localization has been well studied under constrained scenes, where the audio recordings are clean. However, in real-world scenarios, audios are usually contaminated by off-screen sound and background noise. They will interfere with the procedure of identifying desired sources and building visual-sound connections, making previous studies non-applicable. In this work, we propose the Interference Eraser (IEr) framework, which tackles the problem of audio-visual sound source localization in the wild. The key idea is to eliminate the interference by redefining and carving discriminative audio representations. Specifically, we observe that the previous practice of learning only a single audio representation is insufficient due to the additive nature of audio signals. We thus extend the audio representation with our Audio-Instance-Identifier module, which clearly distinguishes sounding instances when audio signals of different volumes are unevenly mixed. Then we erase the influence of the audible but off-screen sounds and the silent but visible objects by a Cross-modal Referrer module with cross-modality distillation. Quantitative and qualitative evaluations demonstrate that our proposed framework achieves superior results on sound localization tasks, especially under real-world scenarios. Code is available at https://github.com/alvinliu0/Visual-Sound-Localization-in-the-Wild.



### Hierarchical Point Cloud Encoding and Decoding with Lightweight Self-Attention based Model
- **Arxiv ID**: http://arxiv.org/abs/2202.06407v1
- **DOI**: 10.1109/LRA.2022.3149569
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2202.06407v1)
- **Published**: 2022-02-13 21:10:06+00:00
- **Updated**: 2022-02-13 21:10:06+00:00
- **Authors**: En Yen Puang, Hao Zhang, Hongyuan Zhu, Wei Jing
- **Comment**: Accepted by RA-Letters and ICRA 2022
- **Journal**: None
- **Summary**: In this paper we present SA-CNN, a hierarchical and lightweight self-attention based encoding and decoding architecture for representation learning of point cloud data. The proposed SA-CNN introduces convolution and transposed convolution stacks to capture and generate contextual information among unordered 3D points. Following conventional hierarchical pipeline, the encoding process extracts feature in local-to-global manner, while the decoding process generates feature and point cloud in coarse-to-fine, multi-resolution stages. We demonstrate that SA-CNN is capable of a wide range of applications, namely classification, part segmentation, reconstruction, shape retrieval, and unsupervised classification. While achieving the state-of-the-art or comparable performance in the benchmarks, SA-CNN maintains its model complexity several order of magnitude lower than the others. In term of qualitative results, we visualize the multi-stage point cloud reconstructions and latent walks on rigid objects as well as deformable non-rigid human and robot models.



### AI can evolve without labels: self-evolving vision transformer for chest X-ray diagnosis through knowledge distillation
- **Arxiv ID**: http://arxiv.org/abs/2202.06431v1
- **DOI**: 10.1038/s41467-022-31514-x
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.06431v1)
- **Published**: 2022-02-13 22:40:46+00:00
- **Updated**: 2022-02-13 22:40:46+00:00
- **Authors**: Sangjoon Park, Gwanghyun Kim, Yujin Oh, Joon Beom Seo, Sang Min Lee, Jin Hwan Kim, Sungjun Moon, Jae-Kwang Lim, Chang Min Park, Jong Chul Ye
- **Comment**: 24 pages
- **Journal**: None
- **Summary**: Although deep learning-based computer-aided diagnosis systems have recently achieved expert-level performance, developing a robust deep learning model requires large, high-quality data with manual annotation, which is expensive to obtain. This situation poses the problem that the chest x-rays collected annually in hospitals cannot be used due to the lack of manual labeling by experts, especially in deprived areas. To address this, here we present a novel deep learning framework that uses knowledge distillation through self-supervised learning and self-training, which shows that the performance of the original model trained with a small number of labels can be gradually improved with more unlabeled data. Experimental results show that the proposed framework maintains impressive robustness against a real-world environment and has general applicability to several diagnostic tasks such as tuberculosis, pneumothorax, and COVID-19. Notably, we demonstrated that our model performs even better than those trained with the same amount of labeled data. The proposed framework has a great potential for medical imaging, where plenty of data is accumulated every year, but ground truth annotations are expensive to obtain.



### Perception-Aware Perching on Powerlines with Multirotors
- **Arxiv ID**: http://arxiv.org/abs/2202.06434v1
- **DOI**: 10.1109/LRA.2022.3145514
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.06434v1)
- **Published**: 2022-02-13 23:15:53+00:00
- **Updated**: 2022-02-13 23:15:53+00:00
- **Authors**: Julio L. Paneque, Jose Ramiro Martínez de Dios, Aníbal Ollero. Drew Hanover, Sihao Sun, Ángel Romero, Davide Scaramuzza
- **Comment**: IEEE Robotics and Automation Letters (2022)
- **Journal**: None
- **Summary**: Multirotor aerial robots are becoming widely used for the inspection of powerlines. To enable continuous, robust inspection without human intervention, the robots must be able to perch on the powerlines to recharge their batteries. Highly versatile perching capabilities are necessary to adapt to the variety of configurations and constraints that are present in real powerline systems. This paper presents a novel perching trajectory generation framework that computes perception-aware, collision-free, and dynamically-feasible maneuvers to guide the robot to the desired final state. Trajectory generation is achieved via solving a Nonlinear Programming problem using the Primal-Dual Interior Point method. The problem considers the full dynamic model of the robot down to its single rotor thrusts and minimizes the final pose and velocity errors while avoiding collisions and maximizing the visibility of the powerline during the maneuver. The generated maneuvers consider both the perching and the posterior recovery trajectories. The framework adopts costs and constraints defined by efficient mathematical representations of powerlines, enabling online onboard execution in resource-constrained hardware. The method is validated on-board an agile quadrotor conducting powerline inspection and various perching maneuvers with final pitch values of up to 180 degrees. The developed code is available online at: https://github.com/grvcPerception/pa_powerline_perching



