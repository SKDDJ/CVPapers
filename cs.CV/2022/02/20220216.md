# Arxiv Papers in cs.CV on 2022-02-16
### Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations
- **Arxiv ID**: http://arxiv.org/abs/2202.07800v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.07800v2)
- **Published**: 2022-02-16 00:19:42+00:00
- **Updated**: 2022-04-13 23:46:08+00:00
- **Authors**: Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, Pengtao Xie
- **Comment**: ICLR 2022 Spotlight
- **Journal**: None
- **Summary**: Vision Transformers (ViTs) take all the image patches as tokens and construct multi-head self-attention (MHSA) among them. Complete leverage of these image tokens brings redundant computations since not all the tokens are attentive in MHSA. Examples include that tokens containing semantically meaningless or distractive image backgrounds do not positively contribute to the ViT predictions. In this work, we propose to reorganize image tokens during the feed-forward process of ViT models, which is integrated into ViT during training. For each forward inference, we identify the attentive image tokens between MHSA and FFN (i.e., feed-forward network) modules, which is guided by the corresponding class token attention. Then, we reorganize image tokens by preserving attentive image tokens and fusing inattentive ones to expedite subsequent MHSA and FFN computations. To this end, our method EViT improves ViTs from two perspectives. First, under the same amount of input image tokens, our method reduces MHSA and FFN computation for efficient inference. For instance, the inference speed of DeiT-S is increased by 50% while its recognition accuracy is decreased by only 0.3% for ImageNet classification. Second, by maintaining the same computational cost, our method empowers ViTs to take more image tokens as input for recognition accuracy improvement, where the image tokens are from higher resolution images. An example is that we improve the recognition accuracy of DeiT-S by 1% for ImageNet classification at the same computational cost of a vanilla DeiT-S. Meanwhile, our method does not introduce more parameters to ViTs. Experiments on the standard benchmarks show the effectiveness of our method. The code is available at https://github.com/youweiliang/evit



### Applying adversarial networks to increase the data efficiency and reliability of Self-Driving Cars
- **Arxiv ID**: http://arxiv.org/abs/2202.07815v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV, I.2.1; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2202.07815v1)
- **Published**: 2022-02-16 01:34:55+00:00
- **Updated**: 2022-02-16 01:34:55+00:00
- **Authors**: Aakash Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are vulnerable to misclassifying images when small perturbations are present. With the increasing prevalence of CNNs in self-driving cars, it is vital to ensure these algorithms are robust to prevent collisions from occurring due to failure in recognizing a situation. In the Adversarial Self-Driving framework, a Generative Adversarial Network (GAN) is implemented to generate realistic perturbations in an image that cause a classifier CNN to misclassify data. This perturbed data is then used to train the classifier CNN further. The Adversarial Self-driving framework is applied to an image classification algorithm to improve the classification accuracy on perturbed images and is later applied to train a self-driving car to drive in a simulation. A small-scale self-driving car is also built to drive around a track and classify signs. The Adversarial Self-driving framework produces perturbed images through learning a dataset, as a result removing the need to train on significant amounts of data. Experiments demonstrate that the Adversarial Self-driving framework identifies situations where CNNs are vulnerable to perturbations and generates new examples of these situations for the CNN to train on. The additional data generated by the Adversarial Self-driving framework provides sufficient data for the CNN to generalize to the environment. Therefore, it is a viable tool to increase the resilience of CNNs to perturbations. Particularly, in the real-world self-driving car, the application of the Adversarial Self-Driving framework resulted in an 18 % increase in accuracy, and the simulated self-driving model had no collisions in 30 minutes of driving.



### Cross-view and Cross-domain Underwater Localization based on Optical Aerial and Acoustic Underwater Images
- **Arxiv ID**: http://arxiv.org/abs/2202.07817v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.07817v1)
- **Published**: 2022-02-16 01:43:39+00:00
- **Updated**: 2022-02-16 01:43:39+00:00
- **Authors**: Matheus M. Dos Santos, Giovanni G. De Giacomo, Paulo L. J. Drews-Jr, Silvia S. C. Botelho
- **Comment**: This work has been submitted to the IEEE Robotics and Automation
  Letters (RA-L) for possible publication. Copyright may be transferred without
  notice, after which this version may no longer be accessible
- **Journal**: None
- **Summary**: Cross-view image matches have been widely explored on terrestrial image localization using aerial images from drones or satellites. This study expands the cross-view image match idea and proposes a cross-domain and cross-view localization framework. The method identifies the correlation between color aerial images and underwater acoustic images to improve the localization of underwater vehicles that travel in partially structured environments such as harbors and marinas. The approach is validated on a real dataset acquired by an underwater vehicle in a marina. The results show an improvement in the localization when compared to the dead reckoning of the vehicle.



### A Survey of Semen Quality Evaluation in Microscopic Videos Using Computer Assisted Sperm Analysis
- **Arxiv ID**: http://arxiv.org/abs/2202.07820v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07820v2)
- **Published**: 2022-02-16 01:50:58+00:00
- **Updated**: 2022-02-17 10:13:03+00:00
- **Authors**: Wenwei Zhao, Pingli Ma, Chen Li, Xiaoning Bu, Shuojia Zou, Tao Jiang, Marcin Grzegorzek
- **Comment**: None
- **Journal**: None
- **Summary**: The Computer Assisted Sperm Analysis (CASA) plays a crucial role in male reproductive health diagnosis and Infertility treatment. With the development of the computer industry in recent years, a great of accurate algorithms are proposed. With the assistance of those novel algorithms, it is possible for CASA to achieve a faster and higher quality result. Since image processing is the technical basis of CASA, including pre-processing,feature extraction, target detection and tracking, these methods are important technical steps in dealing with CASA. The various works related to Computer Assisted Sperm Analysis methods in the last 30 years (since 1988) are comprehensively introduced and analysed in this survey. To facilitate understanding, the methods involved are analysed in the sequence of general steps in sperm analysis. In other words, the methods related to sperm detection (localization) are first analysed, and then the methods of sperm tracking are analysed. Beside this, we analyse and prospect the present situation and future of CASA. According to our work, the feasible for applying in sperm microscopic video of methods mentioned in this review is explained. Moreover, existing challenges of object detection and tracking in microscope video are potential to be solved inspired by this survey.



### Segmentation and Risk Score Prediction of Head and Neck Cancers in PET/CT Volumes with 3D U-Net and Cox Proportional Hazard Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2202.07823v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07823v1)
- **Published**: 2022-02-16 01:59:33+00:00
- **Updated**: 2022-02-16 01:59:33+00:00
- **Authors**: Fereshteh Yousefirizi, Ian Janzen, Natalia Dubljevic, Yueh-En Liu, Chloe Hill, Calum MacAulay, Arman Rahmim
- **Comment**: None
- **Journal**: None
- **Summary**: We utilized a 3D nnU-Net model with residual layers supplemented by squeeze and excitation (SE) normalization for tumor segmentation from PET/CT images provided by the Head and Neck Tumor segmentation chal-lenge (HECKTOR). Our proposed loss function incorporates the Unified Fo-cal and Mumford-Shah losses to take the advantage of distribution, region, and boundary-based loss functions. The results of leave-one-out-center-cross-validation performed on different centers showed a segmentation performance of 0.82 average Dice score (DSC) and 3.16 median Hausdorff Distance (HD), and our results on the test set achieved 0.77 DSC and 3.01 HD. Following lesion segmentation, we proposed training a case-control proportional hazard Cox model with an MLP neural net backbone to predict the hazard risk score for each discrete lesion. This hazard risk prediction model (CoxCC) was to be trained on a number of PET/CT radiomic features extracted from the segmented lesions, patient and lesion demographics, and encoder features provided from the penultimate layer of a multi-input 2D PET/CT convolutional neural network tasked with predicting time-to-event for each lesion. A 10-fold cross-validated CoxCC model resulted in a c-index validation score of 0.89, and a c-index score of 0.61 on the HECKTOR challenge test dataset.



### RNGDet: Road Network Graph Detection by Transformer in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2202.07824v2
- **DOI**: 10.1109/TGRS.2022.3186993
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07824v2)
- **Published**: 2022-02-16 01:59:41+00:00
- **Updated**: 2022-06-26 11:08:18+00:00
- **Authors**: Zhenhua Xu, Yuxuan Liu, Lu Gan, Yuxiang Sun, Xinyu Wu, Ming Liu, Lujia Wang
- **Comment**: Accepted by IEEE Transactions on Geoscience and Remote Sensing
- **Journal**: None
- **Summary**: Road network graphs provide critical information for autonomous-vehicle applications, such as drivable areas that can be used for motion planning algorithms. To find road network graphs, manually annotation is usually inefficient and labor-intensive. Automatically detecting road network graphs could alleviate this issue, but existing works still have some limitations. For example, segmentation-based approaches could not ensure satisfactory topology correctness, and graph-based approaches could not present precise enough detection results. To provide a solution to these problems, we propose a novel approach based on transformer and imitation learning in this paper. In view of that high-resolution aerial images could be easily accessed all over the world nowadays, we make use of aerial images in our approach. Taken as input an aerial image, our approach iteratively generates road network graphs vertex-by-vertex. Our approach can handle complicated intersection points with various numbers of incident road segments. We evaluate our approach on a publicly available dataset. The superiority of our approach is demonstrated through the comparative experiments. Our work is accompanied with a demonstration video which is available at \url{https://tonyxuqaq.github.io/projects/RNGDet/}.



### Reducing Overconfidence Predictions for Autonomous Driving Perception
- **Arxiv ID**: http://arxiv.org/abs/2202.07825v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, 68T10, I.2.6; I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2202.07825v2)
- **Published**: 2022-02-16 01:59:55+00:00
- **Updated**: 2022-05-12 03:32:33+00:00
- **Authors**: Gledson Melotti, Cristiano Premebida, Jordan J. Bird, Diego R. Faria, Nuno Gonçalves
- **Comment**: This work has been accepted for publication in IEEE Access
- **Journal**: None
- **Summary**: In state-of-the-art deep learning for object recognition, SoftMax and Sigmoid functions are most commonly employed as the predictor outputs. Such layers often produce overconfident predictions rather than proper probabilistic scores, which can thus harm the decision-making of `critical' perception systems applied in autonomous driving and robotics. Given this, the experiments in this work propose a probabilistic approach based on distributions calculated out of the Logit layer scores of pre-trained networks. We demonstrate that Maximum Likelihood (ML) and Maximum a-Posteriori (MAP) functions are more suitable for probabilistic interpretations than SoftMax and Sigmoid-based predictions for object recognition. We explore distinct sensor modalities via RGB images and LiDARs (RV: range-view) data from the KITTI and Lyft Level-5 datasets, where our approach shows promising performance compared to the usual SoftMax and Sigmoid layers, with the benefit of enabling interpretable probabilistic predictions. Another advantage of the approach introduced in this paper is that the ML and MAP functions can be implemented in existing trained networks, that is, the approach benefits from the output of the Logit layer of pre-trained networks. Thus, there is no need to carry out a new training phase since the ML and MAP functions are used in the test/prediction phase.



### Spatial Transformer K-Means
- **Arxiv ID**: http://arxiv.org/abs/2202.07829v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07829v1)
- **Published**: 2022-02-16 02:25:46+00:00
- **Updated**: 2022-02-16 02:25:46+00:00
- **Authors**: Romain Cosentino, Randall Balestriero, Yanis Bahroun, Anirvan Sengupta, Richard Baraniuk, Behnaam Aazhang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2012.09743
- **Journal**: None
- **Summary**: K-means defines one of the most employed centroid-based clustering algorithms with performances tied to the data's embedding. Intricate data embeddings have been designed to push $K$-means performances at the cost of reduced theoretical guarantees and interpretability of the results. Instead, we propose preserving the intrinsic data space and augment K-means with a similarity measure invariant to non-rigid transformations. This enables (i) the reduction of intrinsic nuisances associated with the data, reducing the complexity of the clustering task and increasing performances and producing state-of-the-art results, (ii) clustering in the input space of the data, leading to a fully interpretable clustering algorithm, and (iii) the benefit of convergence guarantees.



### PCRP: Unsupervised Point Cloud Object Retrieval and Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.07843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07843v1)
- **Published**: 2022-02-16 03:37:43+00:00
- **Updated**: 2022-02-16 03:37:43+00:00
- **Authors**: Pranav Kadam, Qingyang Zhou, Shan Liu, C. -C. Jay Kuo
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: An unsupervised point cloud object retrieval and pose estimation method, called PCRP, is proposed in this work. It is assumed that there exists a gallery point cloud set that contains point cloud objects with given pose orientation information. PCRP attempts to register the unknown point cloud object with those in the gallery set so as to achieve content-based object retrieval and pose estimation jointly, where the point cloud registration task is built upon an enhanced version of the unsupervised R-PointHop method. Experiments on the ModelNet40 dataset demonstrate the superior performance of PCRP in comparison with traditional and learning based methods.



### Knowledge Distillation with Deep Supervision
- **Arxiv ID**: http://arxiv.org/abs/2202.07846v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07846v2)
- **Published**: 2022-02-16 03:58:21+00:00
- **Updated**: 2023-05-25 14:07:50+00:00
- **Authors**: Shiya Luo, Defang Chen, Can Wang
- **Comment**: IJCNN-2023
- **Journal**: None
- **Summary**: Knowledge distillation aims to enhance the performance of a lightweight student model by exploiting the knowledge from a pre-trained cumbersome teacher model. However, in the traditional knowledge distillation, teacher predictions are only used to provide the supervisory signal for the last layer of the student model, which may result in those shallow student layers lacking accurate training guidance in the layer-by-layer back propagation and thus hinders effective knowledge transfer. To address this issue, we propose Deeply-Supervised Knowledge Distillation (DSKD), which fully utilizes class predictions and feature maps of the teacher model to supervise the training of shallow student layers. A loss-based weight allocation strategy is developed in DSKD to adaptively balance the learning process of each shallow layer, so as to further improve the student performance. Extensive experiments on CIFAR-100 and TinyImageNet with various teacher-student models show significantly performance, confirming the effectiveness of our proposed method. Code is available at: $\href{https://github.com/luoshiya/DSKD}{https://github.com/luoshiya/DSKD}$



### Practical Network Acceleration with Tiny Sets
- **Arxiv ID**: http://arxiv.org/abs/2202.07861v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.07861v2)
- **Published**: 2022-02-16 05:04:38+00:00
- **Updated**: 2023-03-12 12:16:34+00:00
- **Authors**: Guo-Hua Wang, Jianxin Wu
- **Comment**: CVPR 2023
- **Journal**: None
- **Summary**: Due to data privacy issues, accelerating networks with tiny training sets has become a critical need in practice. Previous methods mainly adopt filter-level pruning to accelerate networks with scarce training samples. In this paper, we reveal that dropping blocks is a fundamentally superior approach in this scenario. It enjoys a higher acceleration ratio and results in a better latency-accuracy performance under the few-shot setting. To choose which blocks to drop, we propose a new concept namely recoverability to measure the difficulty of recovering the compressed network. Our recoverability is efficient and effective for choosing which blocks to drop. Finally, we propose an algorithm named PRACTISE to accelerate networks using only tiny sets of training images. PRACTISE outperforms previous methods by a significant margin. For 22% latency reduction, PRACTISE surpasses previous methods by on average 7% on ImageNet-1k. It also enjoys high generalization ability, working well under data-free or out-of-domain data settings, too. Our code is at https://github.com/DoctorKey/Practise.



### IPD:An Incremental Prototype based DBSCAN for large-scale data with cluster representatives
- **Arxiv ID**: http://arxiv.org/abs/2202.07870v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2202.07870v1)
- **Published**: 2022-02-16 05:47:31+00:00
- **Updated**: 2022-02-16 05:47:31+00:00
- **Authors**: Jayasree Saha, Jayanta Mukherjee
- **Comment**: None
- **Journal**: None
- **Summary**: DBSCAN is a fundamental density-based clustering technique that identifies any arbitrary shape of the clusters. However, it becomes infeasible while handling big data. On the other hand, centroid-based clustering is important for detecting patterns in a dataset since unprocessed data points can be labeled to their nearest centroid. However, it can not detect non-spherical clusters. For a large data, it is not feasible to store and compute labels of every samples. These can be done as and when the information is required. The purpose can be accomplished when clustering act as a tool to identify cluster representatives and query is served by assigning cluster labels of nearest representative. In this paper, we propose an Incremental Prototype-based DBSCAN (IPD) algorithm which is designed to identify arbitrary-shaped clusters for large-scale data. Additionally, it chooses a set of representatives for each cluster.



### Auxiliary Cross-Modal Representation Learning with Triplet Loss Functions for Online Handwriting Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.07901v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, 68T30, 68T35, I.2.4
- **Links**: [PDF](http://arxiv.org/pdf/2202.07901v3)
- **Published**: 2022-02-16 07:09:04+00:00
- **Updated**: 2023-08-03 11:36:06+00:00
- **Authors**: Felix Ott, David Rügamer, Lucas Heublein, Bernd Bischl, Christopher Mutschler
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-modal representation learning learns a shared embedding between two or more modalities to improve performance in a given task compared to using only one of the modalities. Cross-modal representation learning from different data types -- such as images and time-series data (e.g., audio or text data) -- requires a deep metric learning loss that minimizes the distance between the modality embeddings. In this paper, we propose to use the contrastive or triplet loss, which uses positive and negative identities to create sample pairs with different labels, for cross-modal representation learning between image and time-series modalities (CMR-IS). By adapting the triplet loss for cross-modal representation learning, higher accuracy in the main (time-series classification) task can be achieved by exploiting additional information of the auxiliary (image classification) task. We present a triplet loss with a dynamic margin for single label and sequence-to-sequence classification tasks. We perform extensive evaluations on synthetic image and time-series data, and on data for offline handwriting recognition (HWR) and on online HWR from sensor-enhanced pens for classifying written words. Our experiments show an improved classification accuracy, faster convergence, and better generalizability due to an improved cross-modal representation. Furthermore, the more suitable generalizability leads to a better adaptability between writers for online HWR.



### Multi-View Fusion Transformer for Sensor-Based Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2202.12949v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.AI, cs.CV, cs.LG, I.5.1; I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/2202.12949v1)
- **Published**: 2022-02-16 07:15:22+00:00
- **Updated**: 2022-02-16 07:15:22+00:00
- **Authors**: Yimu Wang, Kun Yu, Yan Wang, Hui Xue
- **Comment**: 5 pages, 1 figure
- **Journal**: None
- **Summary**: As a fundamental problem in ubiquitous computing and machine learning, sensor-based human activity recognition (HAR) has drawn extensive attention and made great progress in recent years. HAR aims to recognize human activities based on the availability of rich time-series data collected from multi-modal sensors such as accelerometers and gyroscopes. However, recent deep learning methods are focusing on one view of the data, i.e., the temporal view, while shallow methods tend to utilize the hand-craft features for recognition, e.g., the statistics view. In this paper, to extract a better feature for advancing the performance, we propose a novel method, namely multi-view fusion transformer (MVFT) along with a novel attention mechanism. First, MVFT encodes three views of information, i.e., the temporal, frequent, and statistical views to generate multi-view features. Second, the novel attention mechanism uncovers inner- and cross-view clues to catalyze mutual interactions between three views for detailed relation modeling. Moreover, extensive experiments on two datasets illustrate the superiority of our methods over several state-of-the-art methods.



### Can Deep Learning be Applied to Model-Based Multi-Object Tracking?
- **Arxiv ID**: http://arxiv.org/abs/2202.07909v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2202.07909v1)
- **Published**: 2022-02-16 07:43:08+00:00
- **Updated**: 2022-02-16 07:43:08+00:00
- **Authors**: Juliano Pinto, Georg Hess, William Ljungbergh, Yuxuan Xia, Henk Wymeersch, Lennart Svensson
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-object tracking (MOT) is the problem of tracking the state of an unknown and time-varying number of objects using noisy measurements, with important applications such as autonomous driving, tracking animal behavior, defense systems, and others. In recent years, deep learning (DL) has been increasingly used in MOT for improving tracking performance, but mostly in settings where the measurements are high-dimensional and there are no available models of the measurement likelihood and the object dynamics. The model-based setting instead has not attracted as much attention, and it is still unclear if DL methods can outperform traditional model-based Bayesian methods, which are the state of the art (SOTA) in this context. In this paper, we propose a Transformer-based DL tracker and evaluate its performance in the model-based setting, comparing it to SOTA model-based Bayesian methods in a variety of different tasks. Our results show that the proposed DL method can match the performance of the model-based methods in simple tasks, while outperforming them when the task gets more complicated, either due to an increase in the data association complexity, or to stronger nonlinearities of the models of the environment.



### Evaluation and Analysis of Different Aggregation and Hyperparameter Selection Methods for Federated Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.08261v2
- **DOI**: 10.1007/978-3-031-09002-8_36
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.DC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.08261v2)
- **Published**: 2022-02-16 07:49:04+00:00
- **Updated**: 2022-04-12 10:14:29+00:00
- **Authors**: Ece Isik-Polat, Gorkem Polat, Altan Kocyigit, Alptekin Temizel
- **Comment**: MICCAI 2021, Brain Lesion Workshop
- **Journal**: In: Crimi, A., Bakas, S. (eds) Brainlesion: Glioma, Multiple
  Sclerosis, Stroke and Traumatic Brain Injuries. BrainLes 2021. Lecture Notes
  in Computer Science, vol 12963. Springer, Cham
- **Summary**: Availability of large, diverse, and multi-national datasets is crucial for the development of effective and clinically applicable AI systems in the medical imaging domain. However, forming a global model by bringing these datasets together at a central location, comes along with various data privacy and ownership problems. To alleviate these problems, several recent studies focus on the federated learning paradigm, a distributed learning approach for decentralized data. Federated learning leverages all the available data without any need for sharing collaborators' data with each other or collecting them on a central server. Studies show that federated learning can provide competitive performance with conventional central training, while having a good generalization capability. In this work, we have investigated several federated learning approaches on the brain tumor segmentation problem. We explore different strategies for faster convergence and better performance which can also work on strong Non-IID cases.



### Edge Data Based Trailer Inception Probabilistic Matrix Factorization for Context-Aware Movie Recommendation
- **Arxiv ID**: http://arxiv.org/abs/2202.10236v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10236v1)
- **Published**: 2022-02-16 08:12:48+00:00
- **Updated**: 2022-02-16 08:12:48+00:00
- **Authors**: Honglong Chen, Zhe Li, Zhu Wang, Zhichen Ni, Junjian Li, Ge Xu, Abdul Aziz, Feng Xia
- **Comment**: None
- **Journal**: None
- **Summary**: The rapid growth of edge data generated by mobile devices and applications deployed at the edge of the network has exacerbated the problem of information overload. As an effective way to alleviate information overload, recommender system can improve the quality of various services by adding application data generated by users on edge devices, such as visual and textual information, on the basis of sparse rating data. The visual information in the movie trailer is a significant part of the movie recommender system. However, due to the complexity of visual information extraction, data sparsity cannot be remarkably alleviated by merely using the rough visual features to improve the rating prediction accuracy. Fortunately, the convolutional neural network can be used to extract the visual features precisely. Therefore, the end-to-end neural image caption (NIC) model can be utilized to obtain the textual information describing the visual features of movie trailers. This paper proposes a trailer inception probabilistic matrix factorization model called Ti-PMF, which combines NIC, recurrent convolutional neural network, and probabilistic matrix factorization models as the rating prediction model. We implement the proposed Ti-PMF model with extensive experiments on three real-world datasets to validate its effectiveness. The experimental results illustrate that the proposed Ti-PMF outperforms the existing ones.



### VRConvMF: Visual Recurrent Convolutional Matrix Factorization for Movie Recommendation
- **Arxiv ID**: http://arxiv.org/abs/2202.10241v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.10241v1)
- **Published**: 2022-02-16 08:21:03+00:00
- **Updated**: 2022-02-16 08:21:03+00:00
- **Authors**: Zhu Wang, Honglong Chen, Zhe Li, Kai Lin, Nan Jiang, Feng Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Sparsity of user-to-item rating data becomes one of challenging issues in the recommender systems, which severely deteriorates the recommendation performance. Fortunately, context-aware recommender systems can alleviate the sparsity problem by making use of some auxiliary information, such as the information of both the users and items. In particular, the visual information of items, such as the movie poster, can be considered as the supplement for item description documents, which helps to obtain more item features. In this paper, we focus on movie recommender system and propose a probabilistic matrix factorization based recommendation scheme called visual recurrent convolutional matrix factorization (VRConvMF), which utilizes the textual and multi-level visual features extracted from the descriptive texts and posters respectively. We implement the proposed VRConvMF and conduct extensive experiments on three commonly used real world datasets to validate its effectiveness. The experimental results illustrate that the proposed VRConvMF outperforms the existing schemes.



### ActionFormer: Localizing Moments of Actions with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2202.07925v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07925v2)
- **Published**: 2022-02-16 08:34:11+00:00
- **Updated**: 2022-08-28 19:46:29+00:00
- **Authors**: Chenlin Zhang, Jianxin Wu, Yin Li
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Self-attention based Transformer models have demonstrated impressive results for image classification and object detection, and more recently for video understanding. Inspired by this success, we investigate the application of Transformer networks for temporal action localization in videos. To this end, we present ActionFormer -- a simple yet powerful model to identify actions in time and recognize their categories in a single shot, without using action proposals or relying on pre-defined anchor windows. ActionFormer combines a multiscale feature representation with local self-attention, and uses a light-weighted decoder to classify every moment in time and estimate the corresponding action boundaries. We show that this orchestrated design results in major improvements upon prior works. Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points. Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.6% average mAP) and EPIC-Kitchens 100 (+13.5% average mAP over prior works). Our code is available at http://github.com/happyharrycn/actionformer_release.



### Meta Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2202.07940v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07940v1)
- **Published**: 2022-02-16 09:09:51+00:00
- **Updated**: 2022-02-16 09:09:51+00:00
- **Authors**: Jihao Liu, Boxiao Liu, Hongsheng Li, Yu Liu
- **Comment**: preprint
- **Journal**: None
- **Summary**: Recent studies pointed out that knowledge distillation (KD) suffers from two degradation problems, the teacher-student gap and the incompatibility with strong data augmentations, making it not applicable to training state-of-the-art models, which are trained with advanced augmentations. However, we observe that a key factor, i.e., the temperatures in the softmax functions for generating probabilities of both the teacher and student models, was mostly overlooked in previous methods. With properly tuned temperatures, such degradation problems of KD can be much mitigated. However, instead of relying on a naive grid search, which shows poor transferability, we propose Meta Knowledge Distillation (MKD) to meta-learn the distillation with learnable meta temperature parameters. The meta parameters are adaptively adjusted during training according to the gradients of the learning objective. We validate that MKD is robust to different dataset scales, different teacher/student architectures, and different types of data augmentation. With MKD, we achieve the best performance with popular ViT architectures among compared methods that use only ImageNet-1K as training data, ranging from tiny to large models. With ViT-L, we achieve 86.5% with 600 epochs of training, 0.6% better than MAE that trains for 1,650 epochs.



### Unified smoke and fire detection in an evolutionary framework with self-supervised progressive data augment
- **Arxiv ID**: http://arxiv.org/abs/2202.07954v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.07954v1)
- **Published**: 2022-02-16 09:48:03+00:00
- **Updated**: 2022-02-16 09:48:03+00:00
- **Authors**: Hang Zhang, Su Yang, Hongyong Wang, zhongyan lu, helin sun
- **Comment**: None
- **Journal**: None
- **Summary**: Few researches have studied simultaneous detection of smoke and flame accompanying fires due to their different physical natures that lead to uncertain fluid patterns. In this study, we collect a large image data set to re-label them as a multi-label image classification problem so as to identify smoke and flame simultaneously. In order to solve the generalization ability of the detection model on account of the movable fluid objects with uncertain shapes like fire and smoke, and their not compactible natures as well as the complex backgrounds with high variations, we propose a data augment method by random image stitch to deploy resizing, deforming, position variation, and background altering so as to enlarge the view of the learner. Moreover, we propose a self-learning data augment method by using the class activation map to extract the highly trustable region as new data source of positive examples to further enhance the data augment. By the mutual reinforcement between the data augment and the detection model that are performed iteratively, both modules make progress in an evolutionary manner. Experiments show that the proposed method can effectively improve the generalization performance of the model for concurrent smoke and fire detection.



### Ensemble Learning techniques for object detection in high-resolution satellite images
- **Arxiv ID**: http://arxiv.org/abs/2202.10554v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.10554v1)
- **Published**: 2022-02-16 10:19:21+00:00
- **Updated**: 2022-02-16 10:19:21+00:00
- **Authors**: Arthur Vilhelm, Matthieu Limbert, Clément Audebert, Tugdual Ceillier
- **Comment**: Conference on Artificial Intelligence for Defense, Nov 2019, Rennes,
  France
- **Journal**: None
- **Summary**: Ensembling is a method that aims to maximize the detection performance by fusing individual detectors. While rarely mentioned in deep-learning articles applied to remote sensing, ensembling methods have been widely used to achieve high scores in recent data science com-petitions, such as Kaggle. The few remote sensing articles mentioning ensembling mainly focus on mid resolution images and earth observation applications such as land use classification, but never on Very High Resolution (VHR) images for defense-related applications or object detection.This study aims at reviewing the most relevant ensembling techniques to be used for object detection on very high resolution imagery and shows an example of the value of such techniques on a relevant operational use-case (vehicle detection in desert areas).



### ADAM Challenge: Detecting Age-related Macular Degeneration from Fundus Images
- **Arxiv ID**: http://arxiv.org/abs/2202.07983v3
- **DOI**: 10.1109/TMI.2022.3172773
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.07983v3)
- **Published**: 2022-02-16 10:49:49+00:00
- **Updated**: 2022-05-06 06:04:25+00:00
- **Authors**: Huihui Fang, Fei Li, Huazhu Fu, Xu Sun, Xingxing Cao, Fengbin Lin, Jaemin Son, Sunho Kim, Gwenole Quellec, Sarah Matta, Sharath M Shankaranarayana, Yi-Ting Chen, Chuen-heng Wang, Nisarg A. Shah, Chia-Yen Lee, Chih-Chung Hsu, Hai Xie, Baiying Lei, Ujjwal Baid, Shubham Innani, Kang Dang, Wenxiu Shi, Ravi Kamble, Nitin Singhal, Ching-Wei Wang, Shih-Chang Lo, José Ignacio Orlando, Hrvoje Bogunović, Xiulan Zhang, Yanwu Xu, iChallenge-AMD study group
- **Comment**: 31 pages, 17 figures
- **Journal**: None
- **Summary**: Age-related macular degeneration (AMD) is the leading cause of visual impairment among elderly in the world. Early detection of AMD is of great importance, as the vision loss caused by this disease is irreversible and permanent. Color fundus photography is the most cost-effective imaging modality to screen for retinal disorders. Cutting edge deep learning based algorithms have been recently developed for automatically detecting AMD from fundus images. However, there are still lack of a comprehensive annotated dataset and standard evaluation benchmarks. To deal with this issue, we set up the Automatic Detection challenge on Age-related Macular degeneration (ADAM), which was held as a satellite event of the ISBI 2020 conference. The ADAM challenge consisted of four tasks which cover the main aspects of detecting and characterizing AMD from fundus images, including detection of AMD, detection and segmentation of optic disc, localization of fovea, and detection and segmentation of lesions. As part of the challenge, we have released a comprehensive dataset of 1200 fundus images with AMD diagnostic labels, pixel-wise segmentation masks for both optic disc and AMD-related lesions (drusen, exudates, hemorrhages and scars, among others), as well as the coordinates corresponding to the location of the macular fovea. A uniform evaluation framework has been built to make a fair comparison of different models using this dataset. During the challenge, 610 results were submitted for online evaluation, with 11 teams finally participating in the onsite challenge. This paper introduces the challenge, the dataset and the evaluation methods, as well as summarizes the participating methods and analyzes their results for each task. In particular, we observed that the ensembling strategy and the incorporation of clinical domain knowledge were the key to improve the performance of the deep learning models.



### Planckian Jitter: countering the color-crippling effects of color jitter on self-supervised training
- **Arxiv ID**: http://arxiv.org/abs/2202.07993v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.07993v3)
- **Published**: 2022-02-16 11:13:37+00:00
- **Updated**: 2023-02-02 15:14:47+00:00
- **Authors**: Simone Zini, Alex Gomez-Villa, Marco Buzzelli, Bartłomiej Twardowski, Andrew D. Bagdanov, Joost van de Weijer
- **Comment**: Accepted at Eleventh International Conference on Learning
  Representations (ICLR 2023)
- **Journal**: None
- **Summary**: Several recent works on self-supervised learning are trained by mapping different augmentations of the same image to the same feature representation. The data augmentations used are of crucial importance to the quality of learned feature representations. In this paper, we analyze how the color jitter traditionally used in data augmentation negatively impacts the quality of the color features in learned feature representations. To address this problem, we propose a more realistic, physics-based color data augmentation - which we call Planckian Jitter - that creates realistic variations in chromaticity and produces a model robust to illumination changes that can be commonly observed in real life, while maintaining the ability to discriminate image content based on color information. Experiments confirm that such a representation is complementary to the representations learned with the currently-used color jitter augmentation and that a simple concatenation leads to significant performance gains on a wide range of downstream datasets. In addition, we present a color sensitivity analysis that documents the impact of different training methods on model neurons and shows that the performance of the learned features is robust with respect to illuminant variations.



### 360 Depth Estimation in the Wild -- The Depth360 Dataset and the SegFuse Network
- **Arxiv ID**: http://arxiv.org/abs/2202.08010v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2202.08010v1)
- **Published**: 2022-02-16 11:56:31+00:00
- **Updated**: 2022-02-16 11:56:31+00:00
- **Authors**: Qi Feng, Hubert P. H. Shum, Shigeo Morishima
- **Comment**: 10 pages, 10 figures, 5 tables, submitted to IEEE VR 2022
- **Journal**: None
- **Summary**: Single-view depth estimation from omnidirectional images has gained popularity with its wide range of applications such as autonomous driving and scene reconstruction. Although data-driven learning-based methods demonstrate significant potential in this field, scarce training data and ineffective 360 estimation algorithms are still two key limitations hindering accurate estimation across diverse domains. In this work, we first establish a large-scale dataset with varied settings called Depth360 to tackle the training data problem. This is achieved by exploring the use of a plenteous source of data, 360 videos from the internet, using a test-time training method that leverages unique information in each omnidirectional sequence. With novel geometric and temporal constraints, our method generates consistent and convincing depth samples to facilitate single-view estimation. We then propose an end-to-end two-branch multi-task learning network, SegFuse, that mimics the human eye to effectively learn from the dataset and estimate high-quality depth maps from diverse monocular RGB images. With a peripheral branch that uses equirectangular projection for depth estimation and a foveal branch that uses cubemap projection for semantic segmentation, our method predicts consistent global depth while maintaining sharp details at local regions. Experimental results show favorable performance against the state-of-the-art methods.



### Phase Aberration Robust Beamformer for Planewave US Using Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.08262v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.08262v1)
- **Published**: 2022-02-16 12:17:01+00:00
- **Updated**: 2022-02-16 12:17:01+00:00
- **Authors**: Shujaat Khan, Jaeyoung Huh, Jong Chul Ye
- **Comment**: 10 pages, 12 figures, submitted to IEEE-TMI
- **Journal**: None
- **Summary**: Ultrasound (US) is widely used for clinical imaging applications thanks to its real-time and non-invasive nature. However, its lesion detectability is often limited in many applications due to the phase aberration artefact caused by variations in the speed of sound (SoS) within body parts. To address this, here we propose a novel self-supervised 3D CNN that enables phase aberration robust plane-wave imaging. Instead of aiming at estimating the SoS distribution as in conventional methods, our approach is unique in that the network is trained in a self-supervised manner to robustly generate a high-quality image from various phase aberrated images by modeling the variation in the speed of sound as stochastic. Experimental results using real measurements from tissue-mimicking phantom and \textit{in vivo} scans confirmed that the proposed method can significantly reduce the phase aberration artifacts and improve the visual quality of deep scans.



### Continuously Learning to Detect People on the Fly: A Bio-inspired Visual System for Drones
- **Arxiv ID**: http://arxiv.org/abs/2202.08023v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.08023v2)
- **Published**: 2022-02-16 12:36:31+00:00
- **Updated**: 2022-02-20 10:03:56+00:00
- **Authors**: Ali Safa, Ilja Ocket, André Bourdoux, Hichem Sahli, Francky Catthoor, Georges Gielen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper demonstrates for the first time that a biologically-plausible spiking neural network (SNN) equipped with Spike-Timing-Dependent Plasticity (STDP) can continuously learn to detect walking people on the fly using retina-inspired, event-based cameras. Our pipeline works as follows. First, a short sequence of event data ($<2$ minutes), capturing a walking human by a flying drone, is forwarded to a convolutional SNNSTDP system which also receives teacher spiking signals from a readout (forming a semi-supervised system). Then, STDP adaptation is stopped and the learned system is assessed on testing sequences. We conduct several experiments to study the effect of key parameters in our system and to compare it against conventionally-trained CNNs. We show that our system reaches a higher peak $F_1$ score (+19%) compared to CNNs with event-based camera frames, while enabling on-line adaptation.



### Diagnosing Batch Normalization in Class Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.08025v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.08025v1)
- **Published**: 2022-02-16 12:38:43+00:00
- **Updated**: 2022-02-16 12:38:43+00:00
- **Authors**: Minghao Zhou, Quanziang Wang, Jun Shu, Qian Zhao, Deyu Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Extensive researches have applied deep neural networks (DNNs) in class incremental learning (Class-IL). As building blocks of DNNs, batch normalization (BN) standardizes intermediate feature maps and has been widely validated to improve training stability and convergence. However, we claim that the direct use of standard BN in Class-IL models is harmful to both the representation learning and the classifier training, thus exacerbating catastrophic forgetting. In this paper we investigate the influence of BN on Class-IL models by illustrating such BN dilemma. We further propose BN Tricks to address the issue by training a better feature extractor while eliminating classification bias. Without inviting extra hyperparameters, we apply BN Tricks to three baseline rehearsal-based methods, ER, DER++ and iCaRL. Through comprehensive experiments conducted on benchmark datasets of Seq-CIFAR-10, Seq-CIFAR-100 and Seq-Tiny-ImageNet, we show that BN Tricks can bring significant performance gains to all adopted baselines, revealing its potential generality along this line of research.



### Learning to Generalize across Domains on Single Test Samples
- **Arxiv ID**: http://arxiv.org/abs/2202.08045v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.08045v1)
- **Published**: 2022-02-16 13:21:04+00:00
- **Updated**: 2022-02-16 13:21:04+00:00
- **Authors**: Zehao Xiao, Xiantong Zhen, Ling Shao, Cees G. M. Snoek
- **Comment**: None
- **Journal**: None
- **Summary**: We strive to learn a model from a set of source domains that generalizes well to unseen target domains. The main challenge in such a domain generalization scenario is the unavailability of any target domain data during training, resulting in the learned model not being explicitly adapted to the unseen target domains. We propose learning to generalize across domains on single test samples. We leverage a meta-learning paradigm to learn our model to acquire the ability of adaptation with single samples at training time so as to further adapt itself to each single test sample at test time. We formulate the adaptation to the single test sample as a variational Bayesian inference problem, which incorporates the test sample as a conditional into the generation of model parameters. The adaptation to each test sample requires only one feed-forward computation at test time without any fine-tuning or self-supervised training on additional data from the unseen domains. Extensive ablation studies demonstrate that our model learns the ability to adapt models to each single sample by mimicking domain shifts during training. Further, our model achieves at least comparable -- and often better -- performance than state-of-the-art methods on multiple benchmarks for domain generalization.



### Image translation of Ultrasound to Pseudo Anatomical Display by CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/2202.08053v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.08053v2)
- **Published**: 2022-02-16 13:31:49+00:00
- **Updated**: 2022-06-10 10:46:04+00:00
- **Authors**: Lilach Barkat, Moti Freiman, Haim Azhari
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Ultrasound is the second most used modality in medical imaging. It is cost effective, hazardless, portable and implemented routinely in numerous clinical procedures. Nonetheless, image quality is characterized by granulated appearance, poor SNR and speckle noise. Specific for malignant tumors, the margins are blurred and indistinct. Thus, there is a great need for improving ultrasound image quality. We hypothesize that this can be achieved, using neural networks, by translation into a more realistic display which mimics an anatomical cut through the tissue. In order to achieve this goal, the preferable approach would be to use a set of paired images. However, this is practically impossible in our case. Therefore, Cycle Generative Adversarial Network (CycleGAN) was used, in order to learn each domain properties separately and enforce cross domain cycle consistency. The two datasets which were used for training the model were "Breast Ultrasound Images" (BUSI) and a set of optic images of poultry breast tissue samples acquired at our lab. The generated pseudo anatomical images provide improved visual discrimination of the lesions with clearer border definition and pronounced contrast. In order to evaluate the preservation of the anatomical features, the lesions in the ultrasonic images and the generated pseudo anatomical images were both automatically segmented and compared. This comparison yielded median dice score of 0.91 for the benign tumors and 0.70 for the malignant ones. The median lesion center error was 0.58% and 3.27% for the benign and malignancies respectively and the median area error index was 0.40% and 4.34% for the benign and malignancies respectively. In conclusion, these generated pseudo anatomical images, which are presented in a more intuitive way, enhance tissue anatomy and can potentially simplify the diagnosis and improve the clinical outcome.



### Learning to Adapt to Light
- **Arxiv ID**: http://arxiv.org/abs/2202.08098v1
- **DOI**: 10.1007/s11263-022-01745-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.08098v1)
- **Published**: 2022-02-16 14:36:25+00:00
- **Updated**: 2022-02-16 14:36:25+00:00
- **Authors**: Kai-Fu Yang, Cheng Cheng, Shi-Xuan Zhao, Xian-Shi Zhang, Yong-Jie Li
- **Comment**: 10 pages, 9 figures
- **Journal**: International Journal of Computer Vision, 2023
- **Summary**: Light adaptation or brightness correction is a key step in improving the contrast and visual appeal of an image. There are multiple light-related tasks (for example, low-light enhancement and exposure correction) and previous studies have mainly investigated these tasks individually. However, it is interesting to consider whether these light-related tasks can be executed by a unified model, especially considering that our visual system adapts to external light in such way. In this study, we propose a biologically inspired method to handle light-related image-enhancement tasks with a unified network (called LA-Net). First, a frequency-based decomposition module is designed to decouple the common and characteristic sub-problems of light-related tasks into two pathways. Then, a new module is built inspired by biological visual adaptation to achieve unified light adaptation in the low-frequency pathway. In addition, noise suppression or detail enhancement is achieved effectively in the high-frequency pathway regardless of the light levels. Extensive experiments on three tasks -- low-light enhancement, exposure correction, and tone mapping -- demonstrate that the proposed method almost obtains state-of-the-art performance compared with recent methods designed for these individual tasks.



### When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs
- **Arxiv ID**: http://arxiv.org/abs/2202.08138v2
- **DOI**: 10.1145/3495211
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2202.08138v2)
- **Published**: 2022-02-16 15:26:12+00:00
- **Updated**: 2022-02-21 16:48:52+00:00
- **Authors**: Oana Ignat, Santiago Castro, Yuhang Zhou, Jiajun Bao, Dandan Shan, Rada Mihalcea
- **Comment**: arXiv admin note: text overlap with arXiv:1906.04236
- **Journal**: None
- **Summary**: We consider the task of temporal human action localization in lifestyle vlogs. We introduce a novel dataset consisting of manual annotations of temporal localization for 13,000 narrated actions in 1,200 video clips. We present an extensive analysis of this data, which allows us to better understand how the language and visual modalities interact throughout the videos. We propose a simple yet effective method to localize the narrated actions based on their expected duration. Through several experiments and analyses, we show that our method brings complementary information with respect to previous methods, and leads to improvements over previous work for the task of temporal action localization.



### FUN-SIS: a Fully UNsupervised approach for Surgical Instrument Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2202.08141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.08141v1)
- **Published**: 2022-02-16 15:32:02+00:00
- **Updated**: 2022-02-16 15:32:02+00:00
- **Authors**: Luca Sestini, Benoit Rosa, Elena De Momi, Giancarlo Ferrigno, Nicolas Padoy
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic surgical instrument segmentation of endoscopic images is a crucial building block of many computer-assistance applications for minimally invasive surgery. So far, state-of-the-art approaches completely rely on the availability of a ground-truth supervision signal, obtained via manual annotation, thus expensive to collect at large scale. In this paper, we present FUN-SIS, a Fully-UNsupervised approach for binary Surgical Instrument Segmentation. FUN-SIS trains a per-frame segmentation model on completely unlabelled endoscopic videos, by solely relying on implicit motion information and instrument shape-priors. We define shape-priors as realistic segmentation masks of the instruments, not necessarily coming from the same dataset/domain as the videos. The shape-priors can be collected in various and convenient ways, such as recycling existing annotations from other datasets. We leverage them as part of a novel generative-adversarial approach, allowing to perform unsupervised instrument segmentation of optical-flow images during training. We then use the obtained instrument masks as pseudo-labels in order to train a per-frame segmentation model; to this aim, we develop a learning-from-noisy-labels architecture, designed to extract a clean supervision signal from these pseudo-labels, leveraging their peculiar noise properties. We validate the proposed contributions on three surgical datasets, including the MICCAI 2017 EndoVis Robotic Instrument Segmentation Challenge dataset. The obtained fully-unsupervised results for surgical instrument segmentation are almost on par with the ones of fully-supervised state-of-the-art approaches. This suggests the tremendous potential of the proposed method to leverage the great amount of unlabelled data produced in the context of minimally invasive surgery.



### Bias in Automated Image Colorization: Metrics and Error Types
- **Arxiv ID**: http://arxiv.org/abs/2202.08143v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/2202.08143v1)
- **Published**: 2022-02-16 15:34:09+00:00
- **Updated**: 2022-02-16 15:34:09+00:00
- **Authors**: Frank Stapel, Floris Weers, Doina Bucur
- **Comment**: 5 pages, 8 figures
- **Journal**: None
- **Summary**: We measure the color shifts present in colorized images from the ADE20K dataset, when colorized by the automatic GAN-based DeOldify model. We introduce fine-grained local and regional bias measurements between the original and the colorized images, and observe many colorization effects. We confirm a general desaturation effect, and also provide novel observations: a shift towards the training average, a pervasive blue shift, different color shifts among image categories, and a manual categorization of colorization errors in three classes.



### Generative modeling with projected entangled-pair states
- **Arxiv ID**: http://arxiv.org/abs/2202.08177v1
- **DOI**: None
- **Categories**: **quant-ph**, cond-mat.stat-mech, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.08177v1)
- **Published**: 2022-02-16 16:29:25+00:00
- **Updated**: 2022-02-16 16:29:25+00:00
- **Authors**: Tom Vieijra, Laurens Vanderstraeten, Frank Verstraete
- **Comment**: None
- **Journal**: None
- **Summary**: We argue and demonstrate that projected entangled-pair states (PEPS) outperform matrix product states significantly for the task of generative modeling of datasets with an intrinsic two-dimensional structure such as images. Our approach builds on a recently introduced algorithm for sampling PEPS, which allows for the efficient optimization and sampling of the distributions.



### Flexible-Modal Face Anti-Spoofing: A Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2202.08192v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.08192v3)
- **Published**: 2022-02-16 16:55:39+00:00
- **Updated**: 2023-03-16 00:52:59+00:00
- **Authors**: Zitong Yu, Ajian Liu, Chenxu Zhao, Kevin H. M. Cheng, Xu Cheng, Guoying Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Face anti-spoofing (FAS) plays a vital role in securing face recognition systems from presentation attacks. Benefitted from the maturing camera sensors, single-modal (RGB) and multi-modal (e.g., RGB+Depth) FAS has been applied in various scenarios with different configurations of sensors/modalities. Existing single- and multi-modal FAS methods usually separately train and deploy models for each possible modality scenario, which might be redundant and inefficient. Can we train a unified model, and flexibly deploy it under various modality scenarios? In this paper, we establish the first flexible-modal FAS benchmark with the principle `train one for all'. To be specific, with trained multi-modal (RGB+Depth+IR) FAS models, both intra- and cross-dataset testings are conducted on four flexible-modal sub-protocols (RGB, RGB+Depth, RGB+IR, and RGB+Depth+IR). We also investigate prevalent deep models and feature fusion strategies for flexible-modal FAS. We hope this new benchmark will facilitate the future research of the multi-modal FAS. The protocols and codes are available at https://github.com/ZitongYu/Flex-Modal-FAS.



### Nuclei Segmentation with Point Annotations from Pathology Images via Self-Supervised Learning and Co-Training
- **Arxiv ID**: http://arxiv.org/abs/2202.08195v2
- **DOI**: 10.1016/j.media.2023.102933
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2202.08195v2)
- **Published**: 2022-02-16 17:08:44+00:00
- **Updated**: 2023-08-17 09:56:32+00:00
- **Authors**: Yi Lin, Zhiyong Qu, Hao Chen, Zhongke Gao, Yuexiang Li, Lili Xia, Kai Ma, Yefeng Zheng, Kwang-Ting Cheng
- **Comment**: Accepted by MedIA
- **Journal**: None
- **Summary**: Nuclei segmentation is a crucial task for whole slide image analysis in digital pathology. Generally, the segmentation performance of fully-supervised learning heavily depends on the amount and quality of the annotated data. However, it is time-consuming and expensive for professional pathologists to provide accurate pixel-level ground truth, while it is much easier to get coarse labels such as point annotations. In this paper, we propose a weakly-supervised learning method for nuclei segmentation that only requires point annotations for training. First, coarse pixel-level labels are derived from the point annotations based on the Voronoi diagram and the k-means clustering method to avoid overfitting. Second, a co-training strategy with an exponential moving average method is designed to refine the incomplete supervision of the coarse labels. Third, a self-supervised visual representation learning method is tailored for nuclei segmentation of pathology images that transforms the hematoxylin component images into the H&E stained images to gain better understanding of the relationship between the nuclei and cytoplasm. We comprehensively evaluate the proposed method using two public datasets. Both visual and quantitative results demonstrate the superiority of our method to the state-of-the-art methods, and its competitive performance compared to the fully-supervised methods. Code: https://github.com/hust-linyi/SC-Net



### Less is More: Surgical Phase Recognition from Timestamp Supervision
- **Arxiv ID**: http://arxiv.org/abs/2202.08199v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.08199v2)
- **Published**: 2022-02-16 17:18:38+00:00
- **Updated**: 2022-12-01 03:09:22+00:00
- **Authors**: Xinpeng Ding, Xinjian Yan, Zixun Wang, Wei Zhao, Jian Zhuang, Xiaowei Xu, Xiaomeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Surgical phase recognition is a fundamental task in computer-assisted surgery systems. Most existing works are under the supervision of expensive and time-consuming full annotations, which require the surgeons to repeat watching videos to find the precise start and end time for a surgical phase. In this paper, we introduce timestamp supervision for surgical phase recognition to train the models with timestamp annotations, where the surgeons are asked to identify only a single timestamp within the temporal boundary of a phase. This annotation can significantly reduce the manual annotation cost compared to the full annotations. To make full use of such timestamp supervisions, we propose a novel method called uncertainty-aware temporal diffusion (UATD) to generate trustworthy pseudo labels for training. Our proposed UATD is motivated by the property of surgical videos, i.e., the phases are long events consisting of consecutive frames. To be specific, UATD diffuses the single labelled timestamp to its corresponding high confident ( i.e., low uncertainty) neighbour frames in an iterative way. Our study uncovers unique insights of surgical phase recognition with timestamp supervisions: 1) timestamp annotation can reduce 74% annotation time compared with the full annotation, and surgeons tend to annotate those timestamps near the middle of phases; 2) extensive experiments demonstrate that our method can achieve competitive results compared with full supervision methods, while reducing manual annotation cost; 3) less is more in surgical phase recognition, i.e., less but discriminative pseudo labels outperform full but containing ambiguous frames; 4) the proposed UATD can be used as a plug and play method to clean ambiguous labels near boundaries between phases, and improve the performance of the current surgical phase recognition methods.



### Ditto: Building Digital Twins of Articulated Objects from Interaction
- **Arxiv ID**: http://arxiv.org/abs/2202.08227v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.08227v3)
- **Published**: 2022-02-16 18:12:14+00:00
- **Updated**: 2022-04-29 22:46:18+00:00
- **Authors**: Zhenyu Jiang, Cheng-Chun Hsu, Yuke Zhu
- **Comment**: CVPR 2022 Oral. Code and additional results are available at
  https://ut-austin-rpl.github.io/Ditto
- **Journal**: None
- **Summary**: Digitizing physical objects into the virtual world has the potential to unlock new research and applications in embodied AI and mixed reality. This work focuses on recreating interactive digital twins of real-world articulated objects, which can be directly imported into virtual environments. We introduce Ditto to learn articulation model estimation and 3D geometry reconstruction of an articulated object through interactive perception. Given a pair of visual observations of an articulated object before and after interaction, Ditto reconstructs part-level geometry and estimates the articulation model of the object. We employ implicit neural representations for joint geometry and articulation modeling. Our experiments show that Ditto effectively builds digital twins of articulated objects in a category-agnostic way. We also apply Ditto to real-world objects and deploy the recreated digital twins in physical simulation. Code and additional results are available at https://ut-austin-rpl.github.io/Ditto



### A multi-reconstruction study of breast density estimation using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2202.08238v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, I.2.1; J.3; I.4
- **Links**: [PDF](http://arxiv.org/pdf/2202.08238v3)
- **Published**: 2022-02-16 18:34:08+00:00
- **Updated**: 2022-10-10 16:43:53+00:00
- **Authors**: Vikash Gupta, Mutlu Demirer, Robert W. Maxwell, Richard D. White, Barbaros Selnur Erdal
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Breast density estimation is one of the key tasks in recognizing individuals predisposed to breast cancer. It is often challenging because of low contrast and fluctuations in mammograms' fatty tissue background. Most of the time, the breast density is estimated manually where a radiologist assigns one of the four density categories decided by the Breast Imaging and Reporting Data Systems (BI-RADS). There have been efforts in the direction of automating a breast density classification pipeline.   Breast density estimation is one of the key tasks performed during a screening exam. Dense breasts are more susceptible to breast cancer. The density estimation is challenging because of low contrast and fluctuations in mammograms' fatty tissue background. Traditional mammograms are being replaced by tomosynthesis and its other low radiation dose variants (for example Hologic' Intelligent 2D and C-View). Because of the low-dose requirement, increasingly more screening centers are favoring the Intelligent 2D view and C-View. Deep-learning studies for breast density estimation use only a single modality for training a neural network. However, doing so restricts the number of images in the dataset. In this paper, we show that a neural network trained on all the modalities at once performs better than a neural network trained on any single modality. We discuss these results using the area under the receiver operator characteristics curves.



### Cyclical Focal Loss
- **Arxiv ID**: http://arxiv.org/abs/2202.08978v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.08978v2)
- **Published**: 2022-02-16 18:56:15+00:00
- **Updated**: 2022-06-16 17:41:17+00:00
- **Authors**: Leslie N. Smith
- **Comment**: None
- **Journal**: None
- **Summary**: The cross-entropy softmax loss is the primary loss function used to train deep neural networks. On the other hand, the focal loss function has been demonstrated to provide improved performance when there is an imbalance in the number of training samples in each class, such as in long-tailed datasets. In this paper, we introduce a novel cyclical focal loss and demonstrate that it is a more universal loss function than cross-entropy softmax loss or focal loss. We describe the intuition behind the cyclical focal loss and our experiments provide evidence that cyclical focal loss provides superior performance for balanced, imbalanced, or long-tailed datasets. We provide numerous experimental results for CIFAR-10/CIFAR-100, ImageNet, balanced and imbalanced 4,000 training sample versions of CIFAR-10/CIFAR-100, and ImageNet-LT and Places-LT from the Open Long-Tailed Recognition (OLTR) challenge. Implementing the cyclical focal loss function requires only a few lines of code and does not increase training time. In the spirit of reproducibility, our code is available at \url{https://github.com/lnsmith54/CFL}.



### Guidelines and Evaluation of Clinical Explainable AI in Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2202.10553v3
- **DOI**: 10.1016/j.media.2022.102684
- **Categories**: **cs.LG**, cs.AI, cs.CV, eess.IV, 92C55, 92C50, 68T45, 68T01
- **Links**: [PDF](http://arxiv.org/pdf/2202.10553v3)
- **Published**: 2022-02-16 19:09:42+00:00
- **Updated**: 2022-12-08 08:50:52+00:00
- **Authors**: Weina Jin, Xiaoxiao Li, Mostafa Fatehi, Ghassan Hamarneh
- **Comment**: Code: http://github.com/weinajin/multimodal_explanation,
  Supplementary Material S1 and S2:
  https://github.com/weinajin/multimodal_explanation/tree/main/paper
- **Journal**: Medical Image Analysis, 2022
- **Summary**: Explainable artificial intelligence (XAI) is essential for enabling clinical users to get informed decision support from AI and comply with evidence-based medical practice. Applying XAI in clinical settings requires proper evaluation criteria to ensure the explanation technique is both technically sound and clinically useful, but specific support is lacking to achieve this goal. To bridge the research gap, we propose the Clinical XAI Guidelines that consist of five criteria a clinical XAI needs to be optimized for. The guidelines recommend choosing an explanation form based on Guideline 1 (G1) Understandability and G2 Clinical relevance. For the chosen explanation form, its specific XAI technique should be optimized for G3 Truthfulness, G4 Informative plausibility, and G5 Computational efficiency. Following the guidelines, we conducted a systematic evaluation on a novel problem of multi-modal medical image explanation with two clinical tasks, and proposed new evaluation metrics accordingly. Sixteen commonly-used heatmap XAI techniques were evaluated and found to be insufficient for clinical use due to their failure in G3 and G4. Our evaluation demonstrated the use of Clinical XAI Guidelines to support the design and evaluation of clinically viable XAI.



### Subtyping brain diseases from imaging data
- **Arxiv ID**: http://arxiv.org/abs/2202.10945v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.10945v1)
- **Published**: 2022-02-16 19:13:53+00:00
- **Updated**: 2022-02-16 19:13:53+00:00
- **Authors**: Junhao Wen, Erdem Varol, Zhijian Yang, Gyujoon Hwang, Dominique Dwyer, Anahita Fathi Kazerooni, Paris Alexandros Lalousis, Christos Davatzikos
- **Comment**: None
- **Journal**: None
- **Summary**: The imaging community has increasingly adopted machine learning (ML) methods to provide individualized imaging signatures related to disease diagnosis, prognosis, and response to treatment. Clinical neuroscience and cancer imaging have been two areas in which ML has offered particular promise. However, many neurologic and neuropsychiatric diseases, as well as cancer, are often heterogeneous in terms of their clinical manifestations, neuroanatomical patterns or genetic underpinnings. Therefore, in such cases, seeking a single disease signature might be ineffectual in delivering individualized precision diagnostics. The current chapter focuses on ML methods, especially semi-supervised clustering, that seek disease subtypes using imaging data. Work from Alzheimer Disease and its prodromal stages, psychosis, depression, autism, and brain cancer are discussed. Our goal is to provide the readers with a broad overview in terms of methodology and clinical applications.



### OpenKBP-Opt: An international and reproducible evaluation of 76 knowledge-based planning pipelines
- **Arxiv ID**: http://arxiv.org/abs/2202.08303v1
- **DOI**: 10.1088/1361-6560/ac8044
- **Categories**: **physics.med-ph**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.08303v1)
- **Published**: 2022-02-16 19:18:42+00:00
- **Updated**: 2022-02-16 19:18:42+00:00
- **Authors**: Aaron Babier, Rafid Mahmood, Binghao Zhang, Victor G. L. Alves, Ana Maria Barragán-Montero, Joel Beaudry, Carlos E. Cardenas, Yankui Chang, Zijie Chen, Jaehee Chun, Kelly Diaz, Harold David Eraso, Erik Faustmann, Sibaji Gaj, Skylar Gay, Mary Gronberg, Bingqi Guo, Junjun He, Gerd Heilemann, Sanchit Hira, Yuliang Huang, Fuxin Ji, Dashan Jiang, Jean Carlo Jimenez Giraldo, Hoyeon Lee, Jun Lian, Shuolin Liu, Keng-Chi Liu, José Marrugo, Kentaro Miki, Kunio Nakamura, Tucker Netherton, Dan Nguyen, Hamidreza Nourzadeh, Alexander F. I. Osman, Zhao Peng, José Darío Quinto Muñoz, Christian Ramsl, Dong Joo Rhee, Juan David Rodriguez, Hongming Shan, Jeffrey V. Siebers, Mumtaz H. Soomro, Kay Sun, Andrés Usuga Hoyos, Carlos Valderrama, Rob Verbeek, Enpei Wang, Siri Willems, Qi Wu, Xuanang Xu, Sen Yang, Lulin Yuan, Simeng Zhu, Lukas Zimmermann, Kevin L. Moore, Thomas G. Purdie, Andrea L. McNiven, Timothy C. Y. Chan
- **Comment**: 19 pages, 7 tables, 6 figures
- **Journal**: None
- **Summary**: We establish an open framework for developing plan optimization models for knowledge-based planning (KBP) in radiotherapy. Our framework includes reference plans for 100 patients with head-and-neck cancer and high-quality dose predictions from 19 KBP models that were developed by different research groups during the OpenKBP Grand Challenge. The dose predictions were input to four optimization models to form 76 unique KBP pipelines that generated 7600 plans. The predictions and plans were compared to the reference plans via: dose score, which is the average mean absolute voxel-by-voxel difference in dose a model achieved; the deviation in dose-volume histogram (DVH) criterion; and the frequency of clinical planning criteria satisfaction. We also performed a theoretical investigation to justify our dose mimicking models. The range in rank order correlation of the dose score between predictions and their KBP pipelines was 0.50 to 0.62, which indicates that the quality of the predictions is generally positively correlated with the quality of the plans. Additionally, compared to the input predictions, the KBP-generated plans performed significantly better (P<0.05; one-sided Wilcoxon test) on 18 of 23 DVH criteria. Similarly, each optimization model generated plans that satisfied a higher percentage of criteria than the reference plans. Lastly, our theoretical investigation demonstrated that the dose mimicking models generated plans that are also optimal for a conventional planning model. This was the largest international effort to date for evaluating the combination of KBP prediction and optimization models. In the interest of reproducibility, our data and code is freely available at https://github.com/ababier/open-kbp-opt.



### Contextualize differential privacy in image database: a lightweight image differential privacy approach based on principle component analysis inverse
- **Arxiv ID**: http://arxiv.org/abs/2202.08309v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.08309v2)
- **Published**: 2022-02-16 19:36:49+00:00
- **Updated**: 2022-02-19 12:31:43+00:00
- **Authors**: Shiliang Zhang, Xuehui Ma, Hui Cao, Tengyuan Zhao, Yajie Yu, Zhuzhu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Differential privacy (DP) has been the de-facto standard to preserve privacy-sensitive information in database. Nevertheless, there lacks a clear and convincing contextualization of DP in image database, where individual images' indistinguishable contribution to a certain analysis can be achieved and observed when DP is exerted. As a result, the privacy-accuracy trade-off due to integrating DP is insufficiently demonstrated in the context of differentially-private image database. This work aims at contextualizing DP in image database by an explicit and intuitive demonstration of integrating conceptional differential privacy with images. To this end, we design a lightweight approach dedicating to privatizing image database as a whole and preserving the statistical semantics of the image database to an adjustable level, while making individual images' contribution to such statistics indistinguishable. The designed approach leverages principle component analysis (PCA) to reduce the raw image with large amount of attributes to a lower dimensional space whereby DP is performed, so as to decrease the DP load of calculating sensitivity attribute-by-attribute. The DP-exerted image data, which is not visible in its privatized format, is visualized through PCA inverse such that both a human and machine inspector can evaluate the privatization and quantify the privacy-accuracy trade-off in an analysis on the privatized image database. Using the devised approach, we demonstrate the contextualization of DP in images by two use cases based on deep learning models, where we show the indistinguishability of individual images induced by DP and the privatized images' retention of statistical semantics in deep learning tasks, which is elaborated by quantitative analyses on the privacy-accuracy trade-off under different privatization settings.



### A Data-Augmentation Is Worth A Thousand Samples: Exact Quantification From Analytical Augmented Sample Moments
- **Arxiv ID**: http://arxiv.org/abs/2202.08325v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.08325v1)
- **Published**: 2022-02-16 20:41:57+00:00
- **Updated**: 2022-02-16 20:41:57+00:00
- **Authors**: Randall Balestriero, Ishan Misra, Yann LeCun
- **Comment**: None
- **Journal**: None
- **Summary**: Data-Augmentation (DA) is known to improve performance across tasks and datasets. We propose a method to theoretically analyze the effect of DA and study questions such as: how many augmented samples are needed to correctly estimate the information encoded by that DA? How does the augmentation policy impact the final parameters of a model? We derive several quantities in close-form, such as the expectation and variance of an image, loss, and model's output under a given DA distribution. Those derivations open new avenues to quantify the benefits and limitations of DA. For example, we show that common DAs require tens of thousands of samples for the loss at hand to be correctly estimated and for the model training to converge. We show that for a training loss to be stable under DA sampling, the model's saliency map (gradient of the loss with respect to the model's input) must align with the smallest eigenvector of the sample variance under the considered DA augmentation, hinting at a possible explanation on why models tend to shift their focus from edges to textures.



### CortexODE: Learning Cortical Surface Reconstruction by Neural ODEs
- **Arxiv ID**: http://arxiv.org/abs/2202.08329v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.08329v2)
- **Published**: 2022-02-16 20:57:59+00:00
- **Updated**: 2022-09-10 12:07:24+00:00
- **Authors**: Qiang Ma, Liu Li, Emma C. Robinson, Bernhard Kainz, Daniel Rueckert, Amir Alansary
- **Comment**: Accepted by IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: We present CortexODE, a deep learning framework for cortical surface reconstruction. CortexODE leverages neural ordinary differential equations (ODEs) to deform an input surface into a target shape by learning a diffeomorphic flow. The trajectories of the points on the surface are modeled as ODEs, where the derivatives of their coordinates are parameterized via a learnable Lipschitz-continuous deformation network. This provides theoretical guarantees for the prevention of self-intersections. CortexODE can be integrated to an automatic learning-based pipeline, which reconstructs cortical surfaces efficiently in less than 5 seconds. The pipeline utilizes a 3D U-Net to predict a white matter segmentation from brain Magnetic Resonance Imaging (MRI) scans, and further generates a signed distance function that represents an initial surface. Fast topology correction is introduced to guarantee homeomorphism to a sphere. Following the isosurface extraction step, two CortexODE models are trained to deform the initial surface to white matter and pial surfaces respectively. The proposed pipeline is evaluated on large-scale neuroimage datasets in various age groups including neonates (25-45 weeks), young adults (22-36 years) and elderly subjects (55-90 years). Our experiments demonstrate that the CortexODE-based pipeline can achieve less than 0.2mm average geometric error while being orders of magnitude faster compared to conventional processing pipelines.



### A Developmentally-Inspired Examination of Shape versus Texture Bias in Machines
- **Arxiv ID**: http://arxiv.org/abs/2202.08340v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.08340v2)
- **Published**: 2022-02-16 21:15:40+00:00
- **Updated**: 2022-05-17 17:56:00+00:00
- **Authors**: Alexa R. Tartaglini, Wai Keen Vong, Brenden M. Lake
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: Early in development, children learn to extend novel category labels to objects with the same shape, a phenomenon known as the shape bias. Inspired by these findings, Geirhos et al. (2019) examined whether deep neural networks show a shape or texture bias by constructing images with conflicting shape and texture cues. They found that convolutional neural networks strongly preferred to classify familiar objects based on texture as opposed to shape, suggesting a texture bias. However, there are a number of differences between how the networks were tested in this study versus how children are typically tested. In this work, we re-examine the inductive biases of neural networks by adapting the stimuli and procedure from Geirhos et al. (2019) to more closely follow the developmental paradigm and test on a wide range of pre-trained neural networks. Across three experiments, we find that deep neural networks exhibit a preference for shape rather than texture when tested under conditions that more closely replicate the developmental procedure.



### Anomalib: A Deep Learning Library for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2202.08341v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.08341v1)
- **Published**: 2022-02-16 21:15:59+00:00
- **Updated**: 2022-02-16 21:15:59+00:00
- **Authors**: Samet Akcay, Dick Ameln, Ashwin Vaidya, Barath Lakshmanan, Nilesh Ahuja, Utku Genc
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces anomalib, a novel library for unsupervised anomaly detection and localization. With reproducibility and modularity in mind, this open-source library provides algorithms from the literature and a set of tools to design custom anomaly detection algorithms via a plug-and-play approach. Anomalib comprises state-of-the-art anomaly detection algorithms that achieve top performance on the benchmarks and that can be used off-the-shelf. In addition, the library provides components to design custom algorithms that could be tailored towards specific needs. Additional tools, including experiment trackers, visualizers, and hyper-parameter optimizers, make it simple to design and implement anomaly detection models. The library also supports OpenVINO model optimization and quantization for real-time deployment. Overall, anomalib is an extensive library for the design, implementation, and deployment of unsupervised anomaly detection models from data to the edge.



### Learning Smooth Neural Functions via Lipschitz Regularization
- **Arxiv ID**: http://arxiv.org/abs/2202.08345v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2202.08345v2)
- **Published**: 2022-02-16 21:24:54+00:00
- **Updated**: 2022-05-10 17:24:38+00:00
- **Authors**: Hsueh-Ti Derek Liu, Francis Williams, Alec Jacobson, Sanja Fidler, Or Litany
- **Comment**: None
- **Journal**: None
- **Summary**: Neural implicit fields have recently emerged as a useful representation for 3D shapes. These fields are commonly represented as neural networks which map latent descriptors and 3D coordinates to implicit function values. The latent descriptor of a neural field acts as a deformation handle for the 3D shape it represents. Thus, smoothness with respect to this descriptor is paramount for performing shape-editing operations. In this work, we introduce a novel regularization designed to encourage smooth latent spaces in neural fields by penalizing the upper bound on the field's Lipschitz constant. Compared with prior Lipschitz regularized networks, ours is computationally fast, can be implemented in four lines of code, and requires minimal hyperparameter tuning for geometric applications. We demonstrate the effectiveness of our approach on shape interpolation and extrapolation as well as partial shape reconstruction from 3D point clouds, showing both qualitative and quantitative improvements over existing state-of-the-art and non-regularized baselines.



### Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision
- **Arxiv ID**: http://arxiv.org/abs/2202.08360v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2202.08360v2)
- **Published**: 2022-02-16 22:26:47+00:00
- **Updated**: 2022-02-22 18:15:21+00:00
- **Authors**: Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Ishan Misra, Levent Sagun, Armand Joulin, Piotr Bojanowski
- **Comment**: None
- **Journal**: None
- **Summary**: Discriminative self-supervised learning allows training models on any random group of internet images, and possibly recover salient information that helps differentiate between the images. Applied to ImageNet, this leads to object centric features that perform on par with supervised features on most object-centric downstream tasks. In this work, we question if using this ability, we can learn any salient and more representative information present in diverse unbounded set of images from across the globe. To do so, we train models on billions of random images without any data pre-processing or prior assumptions about what we want the model to learn. We scale our model size to dense 10 billion parameters to avoid underfitting on a large data size. We extensively study and validate our model performance on over 50 benchmarks including fairness, robustness to distribution shift, geographical diversity, fine grained recognition, image copy detection and many image classification datasets. The resulting model, not only captures well semantic information, it also captures information about artistic style and learns salient information such as geolocations and multilingual word embeddings based on visual content only. More importantly, we discover that such model is more robust, more fair, less harmful and less biased than supervised models or models trained on object centric datasets such as ImageNet.



### How to Fill the Optimum Set? Population Gradient Descent with Harmless Diversity
- **Arxiv ID**: http://arxiv.org/abs/2202.08376v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.08376v1)
- **Published**: 2022-02-16 23:40:18+00:00
- **Updated**: 2022-02-16 23:40:18+00:00
- **Authors**: Chengyue Gong, Lemeng Wu, Qiang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Although traditional optimization methods focus on finding a single optimal solution, most objective functions in modern machine learning problems, especially those in deep learning, often have multiple or infinite numbers of optima. Therefore, it is useful to consider the problem of finding a set of diverse points in the optimum set of an objective function. In this work, we frame this problem as a bi-level optimization problem of maximizing a diversity score inside the optimum set of the main loss function, and solve it with a simple population gradient descent framework that iteratively updates the points to maximize the diversity score in a fashion that does not hurt the optimization of the main loss. We demonstrate that our method can efficiently generate diverse solutions on a variety of applications, including text-to-image generation, text-to-mesh generation, molecular conformation generation and ensemble neural network training.



