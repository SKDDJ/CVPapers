# Arxiv Papers in cs.CV on 2022-02-05
### Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/2202.02440v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02440v2)
- **Published**: 2022-02-05 00:07:21+00:00
- **Updated**: 2022-04-29 00:21:36+00:00
- **Authors**: Ziad Al-Halah, Santhosh K. Ramakrishnan, Kristen Grauman
- **Comment**: CVPR 2022. Project page: https://vision.cs.utexas.edu/projects/zsel/
- **Journal**: None
- **Summary**: In reinforcement learning for visual navigation, it is common to develop a model for each new task, and train that model from scratch with task-specific interactions in 3D environments. However, this process is expensive; massive amounts of interactions are needed for the model to generalize well. Moreover, this process is repeated whenever there is a change in the task type or the goal modality. We present a unified approach to visual navigation using a novel modular transfer learning model. Our model can effectively leverage its experience from one source task and apply it to multiple target tasks (e.g., ObjectNav, RoomNav, ViewNav) with various goal modalities (e.g., image, sketch, audio, label). Furthermore, our model enables zero-shot experience learning, whereby it can solve the target tasks without receiving any task-specific interactive training. Our experiments on multiple photorealistic datasets and challenging tasks show that our approach learns faster, generalizes better, and outperforms SoTA models by a significant margin.



### Machine Learning Method for Functional Assessment of Retinal Models
- **Arxiv ID**: http://arxiv.org/abs/2202.02443v1
- **DOI**: 10.1109/EMBC46164.2021.9629599
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02443v1)
- **Published**: 2022-02-05 00:35:38+00:00
- **Updated**: 2022-02-05 00:35:38+00:00
- **Authors**: Nikolas Papadopoulos, Nikos Melanitis, Antonio Lozano, Cristina Soto-Sanchez, Eduardo Fernandez, Konstantina S Nikita
- **Comment**: Accepted at 2021 43rd Annual International Conference of the IEEE
  Engineering in Medicine & Biology Society (EMBC)
- **Journal**: None
- **Summary**: Challenges in the field of retinal prostheses motivate the development of retinal models to accurately simulate Retinal Ganglion Cells (RGCs) responses. The goal of retinal prostheses is to enable blind individuals to solve complex, reallife visual tasks. In this paper, we introduce the functional assessment (FA) of retinal models, which describes the concept of evaluating the performance of retinal models on visual understanding tasks. We present a machine learning method for FA: we feed traditional machine learning classifiers with RGC responses generated by retinal models, to solve object and digit recognition tasks (CIFAR-10, MNIST, Fashion MNIST, Imagenette). We examined critical FA aspects, including how the performance of FA depends on the task, how to optimally feed RGC responses to the classifiers and how the number of output neurons correlates with the model's accuracy. To increase the number of output neurons, we manipulated input images - by splitting and then feeding them to the retinal model and we found that image splitting does not significantly improve the model's accuracy. We also show that differences in the structure of datasets result in largely divergent performance of the retinal model (MNIST and Fashion MNIST exceeded 80% accuracy, while CIFAR-10 and Imagenette achieved ~40%). Furthermore, retinal models which perform better in standard evaluation, i.e. more accurately predict RGC response, perform better in FA as well. However, unlike standard evaluation, FA results can be straightforwardly interpreted in the context of comparing the quality of visual perception.



### Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces via Range Analysis
- **Arxiv ID**: http://arxiv.org/abs/2202.02444v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02444v3)
- **Published**: 2022-02-05 00:37:08+00:00
- **Updated**: 2022-06-24 16:45:26+00:00
- **Authors**: Nicholas Sharp, Alec Jacobson
- **Comment**: appearing in ACM Transactions on Graphics / SIGGRAPH 2022 Journal
  Papers
- **Journal**: None
- **Summary**: Neural implicit representations, which encode a surface as the level set of a neural network applied to spatial coordinates, have proven to be remarkably effective for optimizing, compressing, and generating 3D geometry. Although these representations are easy to fit, it is not clear how to best evaluate geometric queries on the shape, such as intersecting against a ray or finding a closest point. The predominant approach is to encourage the network to have a signed distance property. However, this property typically holds only approximately, leading to robustness issues, and holds only at the conclusion of training, inhibiting the use of queries in loss functions. Instead, this work presents a new approach to perform queries directly on general neural implicit functions for a wide range of existing architectures. Our key tool is the application of range analysis to neural networks, using automatic arithmetic rules to bound the output of a network over a region; we conduct a study of range analysis on neural networks, and identify variants of affine arithmetic which are highly effective. We use the resulting bounds to develop geometric queries including ray casting, intersection testing, constructing spatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating bulk properties, and more. Our queries can be efficiently evaluated on GPUs, and offer concrete accuracy guarantees even on randomly-initialized networks, enabling their use in training objectives and beyond. We also show a preliminary application to inverse rendering.



### Few-shot Learning as Cluster-induced Voronoi Diagrams: A Geometric Approach
- **Arxiv ID**: http://arxiv.org/abs/2202.02471v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02471v1)
- **Published**: 2022-02-05 02:52:06+00:00
- **Updated**: 2022-02-05 02:52:06+00:00
- **Authors**: Chunwei Ma, Ziyun Huang, Mingchen Gao, Jinhui Xu
- **Comment**: Accepted for publication in ICLR 2022;
  https://openreview.net/forum?id=6kCiVaoQdx9
- **Journal**: None
- **Summary**: Few-shot learning (FSL) is the process of rapid generalization from abundant base samples to inadequate novel samples. Despite extensive research in recent years, FSL is still not yet able to generate satisfactory solutions for a wide range of real-world applications. To confront this challenge, we study the FSL problem from a geometric point of view in this paper. One observation is that the widely embraced ProtoNet model is essentially a Voronoi Diagram (VD) in the feature space. We retrofit it by making use of a recent advance in computational geometry called Cluster-induced Voronoi Diagram (CIVD). Starting from the simplest nearest neighbor model, CIVD gradually incorporates cluster-to-point and then cluster-to-cluster relationships for space subdivision, which is used to improve the accuracy and robustness at multiple stages of FSL. Specifically, we use CIVD (1) to integrate parametric and nonparametric few-shot classifiers; (2) to combine feature representation and surrogate representation; (3) and to leverage feature-level, transformation-level, and geometry-level heterogeneities for a better ensemble. Our CIVD-based workflow enables us to achieve new state-of-the-art results on mini-ImageNet, CUB, and tiered-ImagenNet datasets, with ${\sim}2\%{-}5\%$ improvements upon the next best. To summarize, CIVD provides a mathematically elegant and geometrically interpretable framework that compensates for extreme data insufficiency, prevents overfitting, and allows for fast geometric ensemble for thousands of individual VD. These together make FSL stronger.



### Tensor-CSPNet: A Novel Geometric Deep Learning Framework for Motor Imagery Classification
- **Arxiv ID**: http://arxiv.org/abs/2202.02472v3
- **DOI**: 10.1109/TNNLS.2022.3172108
- **Categories**: **eess.SP**, cs.CV, cs.LG, eess.IV, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/2202.02472v3)
- **Published**: 2022-02-05 02:52:23+00:00
- **Updated**: 2022-09-23 14:39:20+00:00
- **Authors**: Ce Ju, Cuntai Guan
- **Comment**: 15 pages, 10 figures, 12 tables; This work has been accepted by the
  IEEE Transactions on Neural Networks and Learning Systems. Copyright will be
  transferred without notice, after which this version may no longer be
  accessible
- **Journal**: None
- **Summary**: Deep learning (DL) has been widely investigated in a vast majority of applications in electroencephalography (EEG)-based brain-computer interfaces (BCIs), especially for motor imagery (MI) classification in the past five years. The mainstream DL methodology for the MI-EEG classification exploits the temporospatial patterns of EEG signals using convolutional neural networks (CNNs), which have remarkably succeeded in visual images. However, since the statistical characteristics of visual images depart radically from EEG signals, a natural question arises whether an alternative network architecture exists apart from CNNs. To address this question, we propose a novel geometric deep learning (GDL) framework called Tensor-CSPNet, which characterizes spatial covariance matrices derived from EEG signals on symmetric positive definite (SPD) manifolds and fully captures the temporospatiofrequency patterns using existing deep neural networks on SPD manifolds, integrating with experiences from many successful MI-EEG classifiers to optimize the framework. In the experiments, Tensor-CSPNet attains or slightly outperforms the current state-of-the-art performance on the cross-validation and holdout scenarios in two commonly-used MI-EEG datasets. Moreover, the visualization and interpretability analyses also exhibit the validity of Tensor-CSPNet for the MI-EEG classification. To conclude, in this study, we provide a feasible answer to the question by generalizing the DL methodologies on SPD manifolds, which indicates the start of a specific GDL methodology for the MI-EEG classification.



### Backdoor Defense via Decoupling the Training Process
- **Arxiv ID**: http://arxiv.org/abs/2202.03423v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.03423v1)
- **Published**: 2022-02-05 03:34:01+00:00
- **Updated**: 2022-02-05 03:34:01+00:00
- **Authors**: Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, Kui Ren
- **Comment**: This work is accepted by the ICLR 2022. The first two authors
  contributed equally to this work. 25 pages
- **Journal**: None
- **Summary**: Recent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by poisoning a few training samples. The attacked model behaves normally on benign samples, whereas its prediction will be maliciously changed when the backdoor is activated. We reveal that poisoned samples tend to cluster together in the feature space of the attacked DNN model, which is mostly due to the end-to-end supervised training paradigm. Inspired by this observation, we propose a novel backdoor defense via decoupling the original end-to-end training process into three stages. Specifically, we first learn the backbone of a DNN model via \emph{self-supervised learning} based on training samples without their labels. The learned backbone will map samples with the same ground-truth label to similar locations in the feature space. Then, we freeze the parameters of the learned backbone and train the remaining fully connected layers via standard training with all (labeled) training samples. Lastly, to further alleviate side-effects of poisoned samples in the second stage, we remove labels of some `low-credible' samples determined based on the learned model and conduct a \emph{semi-supervised fine-tuning} of the whole model. Extensive experiments on multiple benchmark datasets and DNN models verify that the proposed defense is effective in reducing backdoor threats while preserving high accuracy in predicting benign samples. Our code is available at \url{https://github.com/SCLBD/DBD}.



### Investigating the Challenges of Class Imbalance and Scale Variation in Object Detection in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2202.02489v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02489v1)
- **Published**: 2022-02-05 04:48:33+00:00
- **Updated**: 2022-02-05 04:48:33+00:00
- **Authors**: Ahmed Elhagry, Mohamed Saeed
- **Comment**: None
- **Journal**: None
- **Summary**: While object detection is a common problem in computer vision, it is even more challenging when dealing with aerial satellite images. The variety in object scales and orientations can make them difficult to identify. In addition, there can be large amounts of densely packed small objects such as cars. In this project, we propose a few changes to the Faster-RCNN architecture. First, we experiment with different backbones to extract better features. We also modify the data augmentations and generated anchor sizes for region proposals in order to better handle small objects. Finally, we investigate the effects of different loss functions. Our proposed design achieves an improvement of 4.7 mAP over the baseline which used a vanilla Faster R-CNN with a ResNet-101 FPN backbone.



### Adversarial Detector with Robust Classifier
- **Arxiv ID**: http://arxiv.org/abs/2202.02503v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02503v1)
- **Published**: 2022-02-05 07:21:05+00:00
- **Updated**: 2022-02-05 07:21:05+00:00
- **Authors**: Takayuki Osakabe, Maungmaung Aprilpyone, Sayaka Shiota, Hitoshi Kiya
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural network (DNN) models are wellknown to easily misclassify prediction results by using input images with small perturbations, called adversarial examples. In this paper, we propose a novel adversarial detector, which consists of a robust classifier and a plain one, to highly detect adversarial examples. The proposed adversarial detector is carried out in accordance with the logits of plain and robust classifiers. In an experiment, the proposed detector is demonstrated to outperform a state-of-the-art detector without any robust classifier.



### On the predictability in reversible steganography
- **Arxiv ID**: http://arxiv.org/abs/2202.02518v2
- **DOI**: 10.1007/s11235-022-00985-0
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02518v2)
- **Published**: 2022-02-05 09:04:50+00:00
- **Updated**: 2023-03-07 12:55:55+00:00
- **Authors**: Ching-Chun Chang, Xu Wang, Sisheng Chen, Hitoshi Kiya, Isao Echizen
- **Comment**: None
- **Journal**: Telecommunication Systems (2023), vol. 82, no. 2, pp. 301-313
- **Summary**: Artificial neural networks have advanced the frontiers of reversible steganography. The core strength of neural networks is the ability to render accurate predictions for a bewildering variety of data. Residual modulation is recognised as the most advanced reversible steganographic algorithm for digital images. The pivot of this algorithm is predictive analytics in which pixel intensities are predicted given some pixel-wise contextual information. This task can be perceived as a low-level vision problem and hence neural networks for addressing a similar class of problems can be deployed. On top of the prior art, this paper investigates predictability of pixel intensities based on supervised and unsupervised learning frameworks. Predictability analysis enables adaptive data embedding, which in turn leads to a better trade-off between capacity and imperceptibility. While conventional methods estimate predictability by the statistics of local image patterns, learning-based frameworks consider further the degree to which correct predictions can be made by a designated predictor. Not only should the image patterns be taken into account but also the predictor in use. Experimental results show that steganographic performance can be significantly improved by incorporating the learning-based predictability analysers into a reversible steganographic system.



### Comparative study of 3D object detection frameworks based on LiDAR data and sensor fusion techniques
- **Arxiv ID**: http://arxiv.org/abs/2202.02521v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2202.02521v3)
- **Published**: 2022-02-05 09:34:58+00:00
- **Updated**: 2022-03-15 12:41:51+00:00
- **Authors**: Sreenivasa Hikkal Venugopala
- **Comment**: 2021 International Conference on Industrial Automation, Robotics and
  Control Engineering (IARCE 2021)
- **Journal**: None
- **Summary**: Estimating and understanding the surroundings of the vehicle precisely forms the basic and crucial step for the autonomous vehicle. The perception system plays a significant role in providing an accurate interpretation of a vehicle's environment in real-time. Generally, the perception system involves various subsystems such as localization, obstacle (static and dynamic) detection, and avoidance, mapping systems, and others. For perceiving the environment, these vehicles will be equipped with various exteroceptive (both passive and active) sensors in particular cameras, Radars, LiDARs, and others. These systems are equipped with deep learning techniques that transform the huge amount of data from the sensors into semantic information on which the object detection and localization tasks are performed. For numerous driving tasks, to provide accurate results, the location and depth information of a particular object is necessary. 3D object detection methods, by utilizing the additional pose data from the sensors such as LiDARs, stereo cameras, provides information on the size and location of the object. Based on recent research, 3D object detection frameworks performing object detection and localization on LiDAR data and sensor fusion techniques show significant improvement in their performance. In this work, a comparative study of the effect of using LiDAR data for object detection frameworks and the performance improvement seen by using sensor fusion techniques are performed. Along with discussing various state-of-the-art methods in both the cases, performing experimental analysis, and providing future research directions.



### PrivPAS: A real time Privacy-Preserving AI System and applied ethics
- **Arxiv ID**: http://arxiv.org/abs/2202.02524v2
- **DOI**: 10.1109/ICSC52841.2022.00010
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2202.02524v2)
- **Published**: 2022-02-05 09:52:54+00:00
- **Updated**: 2022-02-08 14:23:15+00:00
- **Authors**: Harichandana B S S, Vibhav Agarwal, Sourav Ghosh, Gopi Ramena, Sumit Kumar, Barath Raj Kandur Raja
- **Comment**: Accepted at 16th IEEE International Conference on Semantic Computing
  (ICSC), January 26-28, 2022 [update: Best Paper candidate at ICSC 2022]
- **Journal**: 2022 IEEE 16th International Conference on Semantic Computing
  (ICSC), Laguna Hills, CA, USA, 2022, pp. 9-16
- **Summary**: With 3.78 billion social media users worldwide in 2021 (48% of the human population), almost 3 billion images are shared daily. At the same time, a consistent evolution of smartphone cameras has led to a photography explosion with 85% of all new pictures being captured using smartphones. However, lately, there has been an increased discussion of privacy concerns when a person being photographed is unaware of the picture being taken or has reservations about the same being shared. These privacy violations are amplified for people with disabilities, who may find it challenging to raise dissent even if they are aware. Such unauthorized image captures may also be misused to gain sympathy by third-party organizations, leading to a privacy breach. Privacy for people with disabilities has so far received comparatively less attention from the AI community. This motivates us to work towards a solution to generate privacy-conscious cues for raising awareness in smartphone users of any sensitivity in their viewfinder content. To this end, we introduce PrivPAS (A real time Privacy-Preserving AI System) a novel framework to identify sensitive content. Additionally, we curate and annotate a dataset to identify and localize accessibility markers and classify whether an image is sensitive to a featured subject with a disability. We demonstrate that the proposed lightweight architecture, with a memory footprint of a mere 8.49MB, achieves a high mAP of 89.52% on resource-constrained devices. Furthermore, our pipeline, trained on face anonymized data, achieves an F1-score of 73.1%.



### Unsupervised Learning on 3D Point Clouds by Clustering and Contrasting
- **Arxiv ID**: http://arxiv.org/abs/2202.02543v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2202.02543v2)
- **Published**: 2022-02-05 12:54:17+00:00
- **Updated**: 2022-02-14 21:11:52+00:00
- **Authors**: Guofeng Mei, Litao Yu, Qiang Wu, Jian Zhang, Mohammed Bennamoun
- **Comment**: None
- **Journal**: None
- **Summary**: Learning from unlabeled or partially labeled data to alleviate human labeling remains a challenging research topic in 3D modeling. Along this line, unsupervised representation learning is a promising direction to auto-extract features without human intervention. This paper proposes a general unsupervised approach, named \textbf{ConClu}, to perform the learning of point-wise and global features by jointly leveraging point-level clustering and instance-level contrasting. Specifically, for one thing, we design an Expectation-Maximization (EM) like soft clustering algorithm that provides local supervision to extract discriminating local features based on optimal transport. We show that this criterion extends standard cross-entropy minimization to an optimal transport problem, which we solve efficiently using a fast variant of the Sinkhorn-Knopp algorithm. For another, we provide an instance-level contrasting method to learn the global geometry, which is formulated by maximizing the similarity between two augmentations of one point cloud. Experimental evaluations on downstream applications such as 3D object classification and semantic segmentation demonstrate the effectiveness of our framework and show that it can outperform state-of-the-art techniques.



### DEVO: Depth-Event Camera Visual Odometry in Challenging Conditions
- **Arxiv ID**: http://arxiv.org/abs/2202.02556v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02556v1)
- **Published**: 2022-02-05 13:46:47+00:00
- **Updated**: 2022-02-05 13:46:47+00:00
- **Authors**: Yi-Fan Zuo, Jiaqi Yang, Jiaben Chen, Xia Wang, Yifu Wang, Laurent Kneip
- **Comment**: accepted in the 2022 IEEE International Conference on Robotics and
  Automation (ICRA), Philadelphia (PA), USA
- **Journal**: None
- **Summary**: We present a novel real-time visual odometry framework for a stereo setup of a depth and high-resolution event camera. Our framework balances accuracy and robustness against computational efficiency towards strong performance in challenging scenarios. We extend conventional edge-based semi-dense visual odometry towards time-surface maps obtained from event streams. Semi-dense depth maps are generated by warping the corresponding depth values of the extrinsically calibrated depth camera. The tracking module updates the camera pose through efficient, geometric semi-dense 3D-2D edge alignment. Our approach is validated on both public and self-collected datasets captured under various conditions. We show that the proposed method performs comparable to state-of-the-art RGB-D camera-based alternatives in regular conditions, and eventually outperforms in challenging conditions such as high dynamics or low illumination.



### Catch Me if You Can: A Novel Task for Detection of Covert Geo-Locations (CGL)
- **Arxiv ID**: http://arxiv.org/abs/2202.02567v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2202.02567v1)
- **Published**: 2022-02-05 14:40:14+00:00
- **Updated**: 2022-02-05 14:40:14+00:00
- **Authors**: Binoy Saha, Sukhendu Das
- **Comment**: This is an updated version of our accepted paper in: fourth workshop
  on Computer Vision Applications (WCVA), 12th Indian Conference on Computer
  Vision, Graphics and Image Processing (ICVGIP 20-21), IIT Jodhpur, India,
  December 2021. [work sponsored under IMPRINT grant]
- **Journal**: None
- **Summary**: Most visual scene understanding tasks in the field of computer vision involve identification of the objects present in the scene. Image regions like hideouts, turns, & other obscured regions of the scene also contain crucial information, for specific surveillance tasks. Task proposed in this paper involves the design of an intelligent visual aid for identification of such locations in an image, which has either the potential to create an imminent threat from an adversary or appear as the target zones needing further investigation. Covert places (CGL) for hiding behind an occluding object are concealed 3D locations, not detectable from the viewpoint (camera). Hence this involves delineating specific image regions around the projections of outer boundary of the occluding objects, as places to be accessed around the potential hideouts. CGL detection finds applications in military counter-insurgency operations, surveillance with path planning for an exploratory robot. Given an RGB image, the goal is to identify all CGLs in the 2D scene. Identification of such regions would require knowledge about the 3D boundaries of obscuring items (pillars, furniture), their spatial location with respect to the neighboring regions of the scene. We propose this as a novel task, termed Covert Geo-Location (CGL) Detection. Classification of any region of an image as a CGL (as boundary sub-segments of an occluding object that conceals the hideout) requires examining the 3D relation between boundaries of occluding objects and their neighborhoods & surroundings. Our method successfully extracts relevant depth features from a single RGB image and quantitatively yields significant improvement over existing object detection and segmentation models adapted and trained for CGL detection. We also introduce a novel hand-annotated CGL detection dataset containing 1.5K real-world images for experimentation.



### Decision boundaries and convex hulls in the feature space that deep learning functions learn from images
- **Arxiv ID**: http://arxiv.org/abs/2202.04052v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2202.04052v3)
- **Published**: 2022-02-05 15:09:51+00:00
- **Updated**: 2022-05-03 16:18:44+00:00
- **Authors**: Roozbeh Yousefzadeh
- **Comment**: None
- **Journal**: None
- **Summary**: The success of deep neural networks in image classification and learning can be partly attributed to the features they extract from images. It is often speculated about the properties of a low-dimensional manifold that models extract and learn from images. However, there is not sufficient understanding about this low-dimensional space based on theory or empirical evidence. For image classification models, their last hidden layer is the one where images of each class is separated from other classes and it also has the least number of features. Here, we develop methods and formulations to study that feature space for any model. We study the partitioning of the domain in feature space, identify regions guaranteed to have certain classifications, and investigate its implications for the pixel space. We observe that geometric arrangements of decision boundaries in feature space is significantly different compared to pixel space, providing insights about adversarial vulnerabilities, image morphing, extrapolation, ambiguity in classification, and the mathematical understanding of image classification models.



### VIS-iTrack: Visual Intention through Gaze Tracking using Low-Cost Webcam
- **Arxiv ID**: http://arxiv.org/abs/2202.02587v1
- **DOI**: 10.1109/ACCESS.2022.3187969
- **Categories**: **cs.HC**, cs.CV, cs.LG, I.4; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/2202.02587v1)
- **Published**: 2022-02-05 16:00:03+00:00
- **Updated**: 2022-02-05 16:00:03+00:00
- **Authors**: Shahed Anzarus Sabab, Mohammad Ridwan Kabir, Sayed Rizban Hussain, Hasan Mahmud, Md. Kamrul Hasan, Husne Ara Rubaiyeat
- **Comment**: 15 pages, 9 figures, 4 tables
- **Journal**: None
- **Summary**: Human intention is an internal, mental characterization for acquiring desired information. From interactive interfaces containing either textual or graphical information, intention to perceive desired information is subjective and strongly connected with eye gaze. In this work, we determine such intention by analyzing real-time eye gaze data with a low-cost regular webcam. We extracted unique features (e.g., Fixation Count, Eye Movement Ratio) from the eye gaze data of 31 participants to generate a dataset containing 124 samples of visual intention for perceiving textual or graphical information, labeled as either TEXT or IMAGE, having 48.39% and 51.61% distribution, respectively. Using this dataset, we analyzed 5 classifiers, including Support Vector Machine (SVM) (Accuracy: 92.19%). Using the trained SVM, we investigated the variation of visual intention among 30 participants, distributed in 3 age groups, and found out that young users were more leaned towards graphical contents whereas older adults felt more interested in textual ones. This finding suggests that real-time eye gaze data can be a potential source of identifying visual intention, analyzing which intention aware interactive interfaces can be designed and developed to facilitate human cognition.



### Memory Defense: More Robust Classification via a Memory-Masking Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2202.02595v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02595v1)
- **Published**: 2022-02-05 16:30:32+00:00
- **Updated**: 2022-02-05 16:30:32+00:00
- **Authors**: Eashan Adhikarla, Dan Luo, Brian D. Davison
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Many deep neural networks are susceptible to minute perturbations of images that have been carefully crafted to cause misclassification. Ideally, a robust classifier would be immune to small variations in input images, and a number of defensive approaches have been created as a result. One method would be to discern a latent representation which could ignore small changes to the input. However, typical autoencoders easily mingle inter-class latent representations when there are strong similarities between classes, making it harder for a decoder to accurately project the image back to the original high-dimensional space. We propose a novel framework, Memory Defense, an augmented classifier with a memory-masking autoencoder to counter this challenge. By masking other classes, the autoencoder learns class-specific independent latent representations. We test the model's robustness against four widely used attacks. Experiments on the Fashion-MNIST & CIFAR-10 datasets demonstrate the superiority of our model. We make available our source code at GitHub repository: https://github.com/eashanadhikarla/MemDefense



### ROMNet: Renovate the Old Memories
- **Arxiv ID**: http://arxiv.org/abs/2202.02606v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02606v2)
- **Published**: 2022-02-05 17:48:15+00:00
- **Updated**: 2022-04-27 05:03:54+00:00
- **Authors**: Runsheng Xu, Zhengzhong Tu, Yuanqi Du, Xiaoyu Dong, Jinlong Li, Zibo Meng, Jiaqi Ma, Hongkai Yu
- **Comment**: Paper major revision
- **Journal**: None
- **Summary**: Renovating the memories in old photos is an intriguing research topic in computer vision fields. These legacy images often suffer from severe and commingled degradations such as cracks, noise, and color-fading, while lack of large-scale paired old photo datasets makes this restoration task very challenging. In this work, we present a novel reference-based end-to-end learning framework that can jointly repair and colorize the degraded legacy pictures. Specifically, the proposed framework consists of three modules: a restoration sub-network for degradation restoration, a similarity sub-network for color histogram matching and transfer, and a colorization subnet that learns to predict the chroma elements of the images conditioned on chromatic reference signals. The whole system takes advantage of the color histogram priors in a given reference image, which vastly reduces the dependency on large-scale training data. Apart from the proposed method, we also create, to our knowledge, the first public and real-world old photo dataset with paired ground truth for evaluating old photo restoration models, wherein each old photo is paired with a manually restored pristine image by PhotoShop experts. Our extensive experiments conducted on both synthetic and real-world datasets demonstrate that our method significantly outperforms state-of-the-arts both quantitatively and qualitatively.



### DSSIM: a structural similarity index for floating-point data
- **Arxiv ID**: http://arxiv.org/abs/2202.02616v2
- **DOI**: None
- **Categories**: **stat.CO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02616v2)
- **Published**: 2022-02-05 19:18:33+00:00
- **Updated**: 2023-03-19 20:45:27+00:00
- **Authors**: Allison H. Baker, Alexander Pinard, Dorit M. Hammerling
- **Comment**: None
- **Journal**: None
- **Summary**: Data visualization is a critical component in terms of interacting with floating-point output data from large model simulation codes. Indeed, postprocessing analysis workflows on simulation data often generate a large number of images from the raw data, many of which are then compared to each other or to specified reference images. In this image-comparison scenario, image quality assessment (IQA) measures are quite useful, and the Structural Similarity Index (SSIM) continues to be a popular choice. However, generating large numbers of images can be costly, and plot-specific (but data independent) choices can affect the SSIM value. A natural question is whether we can apply the SSIM directly to the floating-point simulation data and obtain an indication of whether differences in the data are likely to impact a visual assessment, effectively bypassing the creation of a specific set of images from the data. To this end, we propose an alternative to the popular SSIM that can be applied directly to the floating point data, which we refer to as the Data SSIM (DSSIM). While we demonstrate the usefulness of the DSSIM in the context of evaluating differences due to lossy compression on large volumes of simulation data from a popular climate model, the DSSIM may prove useful for many other applications involving simulation or image data.



### Layer-wise Regularized Adversarial Training using Layers Sustainability Analysis (LSA) framework
- **Arxiv ID**: http://arxiv.org/abs/2202.02626v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2202.02626v3)
- **Published**: 2022-02-05 20:05:53+00:00
- **Updated**: 2022-02-15 08:44:54+00:00
- **Authors**: Mohammad Khalooei, Mohammad Mehdi Homayounpour, Maryam Amirmazlaghani
- **Comment**: Layers Sustainability Analysis (LSA) framework
- **Journal**: None
- **Summary**: Deep neural network models are used today in various applications of artificial intelligence, the strengthening of which, in the face of adversarial attacks is of particular importance. An appropriate solution to adversarial attacks is adversarial training, which reaches a trade-off between robustness and generalization. This paper introduces a novel framework (Layer Sustainability Analysis (LSA)) for the analysis of layer vulnerability in an arbitrary neural network in the scenario of adversarial attacks. LSA can be a helpful toolkit to assess deep neural networks and to extend the adversarial training approaches towards improving the sustainability of model layers via layer monitoring and analysis. The LSA framework identifies a list of Most Vulnerable Layers (MVL list) of the given network. The relative error, as a comparison measure, is used to evaluate representation sustainability of each layer against adversarial inputs. The proposed approach for obtaining robust neural networks to fend off adversarial attacks is based on a layer-wise regularization (LR) over LSA proposal(s) for adversarial training (AT); i.e. the AT-LR procedure. AT-LR could be used with any benchmark adversarial attack to reduce the vulnerability of network layers and to improve conventional adversarial training approaches. The proposed idea performs well theoretically and experimentally for state-of-the-art multilayer perceptron and convolutional neural network architectures. Compared with the AT-LR and its corresponding base adversarial training, the classification accuracy of more significant perturbations increased by 16.35%, 21.79%, and 10.730% on Moon, MNIST, and CIFAR-10 benchmark datasets, respectively. The LSA framework is available and published at https://github.com/khalooei/LSA.



### The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training
- **Arxiv ID**: http://arxiv.org/abs/2202.02643v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2202.02643v1)
- **Published**: 2022-02-05 21:19:41+00:00
- **Updated**: 2022-02-05 21:19:41+00:00
- **Authors**: Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, Zhangyang Wang, Mykola Pechenizkiy
- **Comment**: Published as a conference paper at ICLR 2022. Code is available at
  https://github.com/VITA-Group/Random_Pruning
- **Journal**: None
- **Summary**: Random pruning is arguably the most naive way to attain sparsity in neural networks, but has been deemed uncompetitive by either post-training pruning or sparse training. In this paper, we focus on sparse training and highlight a perhaps counter-intuitive finding, that random pruning at initialization can be quite powerful for the sparse training of modern neural networks. Without any delicate pruning criteria or carefully pursued sparsity structures, we empirically demonstrate that sparsely training a randomly pruned network from scratch can match the performance of its dense equivalent. There are two key factors that contribute to this revival: (i) the network sizes matter: as the original dense networks grow wider and deeper, the performance of training a randomly pruned sparse network will quickly grow to matching that of its dense equivalent, even at high sparsity ratios; (ii) appropriate layer-wise sparsity ratios can be pre-chosen for sparse training, which shows to be another important performance booster. Simple as it looks, a randomly pruned subnetwork of Wide ResNet-50 can be sparsely trained to outperforming a dense Wide ResNet-50, on ImageNet. We also observed such randomly pruned networks outperform dense counterparts in other favorable aspects, such as out-of-distribution detection, uncertainty estimation, and adversarial robustness. Overall, our results strongly suggest there is larger-than-expected room for sparse training at scale, and the benefits of sparsity might be more universal beyond carefully designed pruning. Our source code can be found at https://github.com/VITA-Group/Random_Pruning.



### A survey of top-down approaches for human pose estimation
- **Arxiv ID**: http://arxiv.org/abs/2202.02656v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2202.02656v1)
- **Published**: 2022-02-05 23:27:46+00:00
- **Updated**: 2022-02-05 23:27:46+00:00
- **Authors**: Thong Duy Nguyen, Milan Kresovic
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Human pose estimation in two-dimensional images videos has been a hot topic in the computer vision problem recently due to its vast benefits and potential applications for improving human life, such as behaviors recognition, motion capture and augmented reality, training robots, and movement tracking. Many state-of-the-art methods implemented with Deep Learning have addressed several challenges and brought tremendous remarkable results in the field of human pose estimation. Approaches are classified into two kinds: the two-step framework (top-down approach) and the part-based framework (bottom-up approach). While the two-step framework first incorporates a person detector and then estimates the pose within each box independently, detecting all body parts in the image and associating parts belonging to distinct persons is conducted in the part-based framework. This paper aims to provide newcomers with an extensive review of deep learning methods-based 2D images for recognizing the pose of people, which only focuses on top-down approaches since 2016. The discussion through this paper presents significant detectors and estimators depending on mathematical background, the challenges and limitations, benchmark datasets, evaluation metrics, and comparison between methods.



