# Arxiv Papers in cs.CV on 2022-07-11
### Brain-Aware Replacements for Supervised Contrastive Learning in Detection of Alzheimer's Disease
- **Arxiv ID**: http://arxiv.org/abs/2207.04574v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.04574v2)
- **Published**: 2022-07-11 01:17:35+00:00
- **Updated**: 2022-07-20 23:19:53+00:00
- **Authors**: Mehmet Saygın Seyfioğlu, Zixuan Liu, Pranav Kamath, Sadjyot Gangolli, Sheng Wang, Thomas Grabowski, Linda Shapiro
- **Comment**: None
- **Journal**: MICCAI 2022
- **Summary**: We propose a novel framework for Alzheimer's disease (AD) detection using brain MRIs. The framework starts with a data augmentation method called Brain-Aware Replacements (BAR), which leverages a standard brain parcellation to replace medically-relevant 3D brain regions in an anchor MRI from a randomly picked MRI to create synthetic samples. Ground truth "hard" labels are also linearly mixed depending on the replacement ratio in order to create "soft" labels. BAR produces a great variety of realistic-looking synthetic MRIs with higher local variability compared to other mix-based methods, such as CutMix. On top of BAR, we propose using a soft-label-capable supervised contrastive loss, aiming to learn the relative similarity of representations that reflect how mixed are the synthetic MRIs using our soft labels. This way, we do not fully exhaust the entropic capacity of our hard labels, since we only use them to create soft labels and synthetic MRIs through BAR. We show that a model pre-trained using our framework can be further fine-tuned with a cross-entropy loss using the hard labels that were used to create the synthetic samples. We validated the performance of our framework in a binary AD detection task against both from-scratch supervised training and state-of-the-art self-supervised training plus fine-tuning approaches. Then we evaluated BAR's individual performance compared to another mix-based method CutMix by integrating it within our framework. We show that our framework yields superior results in both precision and recall for the AD detection task.



### A Waste Copper Granules Rating System Based on Machine Vision
- **Arxiv ID**: http://arxiv.org/abs/2207.04575v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04575v2)
- **Published**: 2022-07-11 01:21:35+00:00
- **Updated**: 2022-07-14 01:39:43+00:00
- **Authors**: Kaikai Zhao, Yajie Cui, Zhaoxiang Liu, Shiguo Lian
- **Comment**: None
- **Journal**: None
- **Summary**: In the field of waste copper granules recycling, engineers should be able to identify all different sorts of impurities in waste copper granules and estimate their mass proportion relying on experience before rating. This manual rating method is costly, lacking in objectivity and comprehensiveness. To tackle this problem, we propose a waste copper granules rating system based on machine vision and deep learning. We firstly formulate the rating task into a 2D image recognition and purity regression task. Then we design a two-stage convolutional rating network to compute the mass purity and rating level of waste copper granules. Our rating network includes a segmentation network and a purity regression network, which respectively calculate the semantic segmentation heatmaps and purity results of the waste copper granules. After training the rating network on the augmented datasets, experiments on real waste copper granules demonstrate the effectiveness and superiority of the proposed network. Specifically, our system is superior to the manual method in terms of accuracy, effectiveness, robustness, and objectivity.



### Gradual Domain Adaptation without Indexed Intermediate Domains
- **Arxiv ID**: http://arxiv.org/abs/2207.04587v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.04587v1)
- **Published**: 2022-07-11 02:25:39+00:00
- **Updated**: 2022-07-11 02:25:39+00:00
- **Authors**: Hong-You Chen, Wei-Lun Chao
- **Comment**: Accepted to NeurIPS 2021
- **Journal**: None
- **Summary**: The effectiveness of unsupervised domain adaptation degrades when there is a large discrepancy between the source and target domains. Gradual domain adaptation (GDA) is one promising way to mitigate such an issue, by leveraging additional unlabeled data that gradually shift from the source to the target. Through sequentially adapting the model along the "indexed" intermediate domains, GDA substantially improves the overall adaptation performance. In practice, however, the extra unlabeled data may not be separated into intermediate domains and indexed properly, limiting the applicability of GDA. In this paper, we investigate how to discover the sequence of intermediate domains when it is not already available. Concretely, we propose a coarse-to-fine framework, which starts with a coarse domain discovery step via progressive domain discriminator training. This coarse domain sequence then undergoes a fine indexing step via a novel cycle-consistency loss, which encourages the next intermediate domain to preserve sufficient discriminative knowledge of the current intermediate domain. The resulting domain sequence can then be used by a GDA algorithm. On benchmark data sets of GDA, we show that our approach, which we name Intermediate DOmain Labeler (IDOL), can lead to comparable or even better adaptation performance compared to the pre-defined domain sequence, making GDA more applicable and robust to the quality of domain sequences. Codes are available at https://github.com/hongyouc/IDOL.



### Learned Video Compression via Heterogeneous Deformable Compensation Network
- **Arxiv ID**: http://arxiv.org/abs/2207.04589v3
- **DOI**: 10.1109/TMM.2023.3289763
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.04589v3)
- **Published**: 2022-07-11 02:31:31+00:00
- **Updated**: 2023-06-29 07:05:06+00:00
- **Authors**: Huairui Wang, Zhenzhong Chen, Chang Wen Chen
- **Comment**: None
- **Journal**: IEEE Transactions on Multimedia, 2023
- **Summary**: Learned video compression has recently emerged as an essential research topic in developing advanced video compression technologies, where motion compensation is considered one of the most challenging issues. In this paper, we propose a learned video compression framework via heterogeneous deformable compensation strategy (HDCVC) to tackle the problems of unstable compression performance caused by single-size deformable kernels in downsampled feature domain. More specifically, instead of utilizing optical flow warping or single-size-kernel deformable alignment, the proposed algorithm extracts features from the two adjacent frames to estimate content-adaptive heterogeneous deformable (HetDeform) kernel offsets. Then we transform the reference features with the HetDeform convolution to accomplish motion compensation. Moreover, we design a Spatial-Neighborhood-Conditioned Divisive Normalization (SNCDN) to achieve more effective data Gaussianization combined with the Generalized Divisive Normalization. Furthermore, we propose a multi-frame enhanced reconstruction module for exploiting context and temporal information for final quality enhancement. Experimental results indicate that HDCVC achieves superior performance than the recent state-of-the-art learned video compression approaches.



### Adaptive Fine-Grained Predicates Learning for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2207.04602v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.04602v1)
- **Published**: 2022-07-11 03:37:57+00:00
- **Updated**: 2022-07-11 03:37:57+00:00
- **Authors**: Xinyu Lyu, Lianli Gao, Pengpeng Zeng, Heng Tao Shen, Jingkuan Song
- **Comment**: arXiv admin note: text overlap with arXiv:2204.02597
- **Journal**: None
- **Summary**: The performance of current Scene Graph Generation (SGG) models is severely hampered by hard-to-distinguish predicates, e.g., woman-on/standing on/walking on-beach. As general SGG models tend to predict head predicates and re-balancing strategies prefer tail categories, none of them can appropriately handle hard-to-distinguish predicates. To tackle this issue, inspired by fine-grained image classification, which focuses on differentiating hard-to-distinguish objects, we propose an Adaptive Fine-Grained Predicates Learning (FGPL-A) which aims at differentiating hard-to-distinguish predicates for SGG. First, we introduce an Adaptive Predicate Lattice (PL-A) to figure out hard-to-distinguish predicates, which adaptively explores predicate correlations in keeping with model's dynamic learning pace. Practically, PL-A is initialized from SGG dataset, and gets refined by exploring model's predictions of current mini-batch. Utilizing PL-A, we propose an Adaptive Category Discriminating Loss (CDL-A) and an Adaptive Entity Discriminating Loss (EDL-A), which progressively regularize model's discriminating process with fine-grained supervision concerning model's dynamic learning status, ensuring balanced and efficient learning process. Extensive experimental results show that our proposed model-agnostic strategy significantly boosts performance of benchmark models on VG-SGG and GQA-SGG datasets by up to 175% and 76% on Mean Recall@100, achieving new state-of-the-art performance. Moreover, experiments on Sentence-to-Graph Retrieval and Image Captioning tasks further demonstrate practicability of our method.



### A Penalty Approach for Normalizing Feature Distributions to Build Confounder-Free Models
- **Arxiv ID**: http://arxiv.org/abs/2207.04607v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04607v1)
- **Published**: 2022-07-11 04:02:12+00:00
- **Updated**: 2022-07-11 04:02:12+00:00
- **Authors**: Anthony Vento, Qingyu Zhao, Robert Paul, Kilian M. Pohl, Ehsan Adeli
- **Comment**: None
- **Journal**: None
- **Summary**: Translating machine learning algorithms into clinical applications requires addressing challenges related to interpretability, such as accounting for the effect of confounding variables (or metadata). Confounding variables affect the relationship between input training data and target outputs. When we train a model on such data, confounding variables will bias the distribution of the learned features. A recent promising solution, MetaData Normalization (MDN), estimates the linear relationship between the metadata and each feature based on a non-trainable closed-form solution. However, this estimation is confined by the sample size of a mini-batch and thereby may cause the approach to be unstable during training. In this paper, we extend the MDN method by applying a Penalty approach (referred to as PDMN). We cast the problem into a bi-level nested optimization problem. We then approximate this optimization problem using a penalty method so that the linear parameters within the MDN layer are trainable and learned on all samples. This enables PMDN to be plugged into any architectures, even those unfit to run batch-level operations, such as transformers and recurrent models. We show improvement in model accuracy and greater independence from confounders using PMDN over MDN in a synthetic experiment and a multi-label, multi-site dataset of magnetic resonance images (MRIs).



### Instance Shadow Detection with A Single-Stage Detector
- **Arxiv ID**: http://arxiv.org/abs/2207.04614v1
- **DOI**: 10.1109/TPAMI.2022.3185628
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2207.04614v1)
- **Published**: 2022-07-11 04:15:42+00:00
- **Updated**: 2022-07-11 04:15:42+00:00
- **Authors**: Tianyu Wang, Xiaowei Hu, Pheng-Ann Heng, Chi-Wing Fu
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). This is the journal version of arXiv:1911.07034 and
  https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Single-Stage_Instance_Shadow_Detection_With_Bidirectional_Relation_Learning_CVPR_2021_paper.pdf
- **Journal**: None
- **Summary**: This paper formulates a new problem, instance shadow detection, which aims to detect shadow instance and the associated object instance that cast each shadow in the input image. To approach this task, we first compile a new dataset with the masks for shadow instances, object instances, and shadow-object associations. We then design an evaluation metric for quantitative evaluation of the performance of instance shadow detection. Further, we design a single-stage detector to perform instance shadow detection in an end-to-end manner, where the bidirectional relation learning module and the deformable maskIoU head are proposed in the detector to directly learn the relation between shadow instances and object instances and to improve the accuracy of the predicted masks. Finally, we quantitatively and qualitatively evaluate our method on the benchmark dataset of instance shadow detection and show the applicability of our method on light direction estimation and photo editing.



### Discovering Domain Disentanglement for Generalized Multi-source Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2207.05070v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.05070v1)
- **Published**: 2022-07-11 04:33:08+00:00
- **Updated**: 2022-07-11 04:33:08+00:00
- **Authors**: Zixin Wang, Yadan Luo, Peng-Fei Zhang, Sen Wang, Zi Huang
- **Comment**: None
- **Journal**: None
- **Summary**: A typical multi-source domain adaptation (MSDA) approach aims to transfer knowledge learned from a set of labeled source domains, to an unlabeled target domain. Nevertheless, prior works strictly assume that each source domain shares the identical group of classes with the target domain, which could hardly be guaranteed as the target label space is not observable. In this paper, we consider a more versatile setting of MSDA, namely Generalized Multi-source Domain Adaptation, wherein the source domains are partially overlapped, and the target domain is allowed to contain novel categories that are not presented in any source domains. This new setting is more elusive than any existing domain adaptation protocols due to the coexistence of the domain and category shifts across the source and target domains. To address this issue, we propose a variational domain disentanglement (VDD) framework, which decomposes the domain representations and semantic features for each instance by encouraging dimension-wise independence. To identify the target samples of unknown classes, we leverage online pseudo labeling, which assigns the pseudo-labels to unlabeled target data based on the confidence scores. Quantitative and qualitative experiments conducted on two benchmark datasets demonstrate the validity of the proposed framework.



### Edge-preserving Near-light Photometric Stereo with Neural Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2207.04622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04622v1)
- **Published**: 2022-07-11 04:51:43+00:00
- **Updated**: 2022-07-11 04:51:43+00:00
- **Authors**: Heng Guo, Hiroaki Santo, Boxin Shi, Yasuyuki Matsushita
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a near-light photometric stereo method that faithfully preserves sharp depth edges in the 3D reconstruction. Unlike previous methods that rely on finite differentiation for approximating depth partial derivatives and surface normals, we introduce an analytically differentiable neural surface in near-light photometric stereo for avoiding differentiation errors at sharp depth edges, where the depth is represented as a neural function of the image coordinates. By further formulating the Lambertian albedo as a dependent variable resulting from the surface normal and depth, our method is insusceptible to inaccurate depth initialization. Experiments on both synthetic and real-world scenes demonstrate the effectiveness of our method for detailed shape recovery with edge preservation.



### Hierarchical Latent Structure for Multi-Modal Vehicle Trajectory Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2207.04624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04624v1)
- **Published**: 2022-07-11 04:52:28+00:00
- **Updated**: 2022-07-11 04:52:28+00:00
- **Authors**: Dooseop Choi, KyoungWook Min
- **Comment**: ECCV2022
- **Journal**: None
- **Summary**: Variational autoencoder (VAE) has widely been utilized for modeling data distributions because it is theoretically elegant, easy to train, and has nice manifold representations. However, when applied to image reconstruction and synthesis tasks, VAE shows the limitation that the generated sample tends to be blurry. We observe that a similar problem, in which the generated trajectory is located between adjacent lanes, often arises in VAE-based trajectory forecasting models. To mitigate this problem, we introduce a hierarchical latent structure into the VAE-based forecasting model. Based on the assumption that the trajectory distribution can be approximated as a mixture of simple distributions (or modes), the low-level latent variable is employed to model each mode of the mixture and the high-level latent variable is employed to represent the weights for the modes. To model each mode accurately, we condition the low-level latent variable using two lane-level context vectors computed in novel ways, one corresponds to vehicle-lane interaction and the other to vehicle-vehicle interaction. The context vectors are also used to model the weights via the proposed mode selection network. To evaluate our forecasting model, we use two large-scale real-world datasets. Experimental results show that our model is not only capable of generating clear multi-modal trajectory distributions but also outperforms the state-of-the-art (SOTA) models in terms of prediction accuracy. Our code is available at https://github.com/d1024choi/HLSTrajForecast.



### On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence
- **Arxiv ID**: http://arxiv.org/abs/2207.04630v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.IT, cs.LG, math.IT, math.OC, I.2
- **Links**: [PDF](http://arxiv.org/pdf/2207.04630v3)
- **Published**: 2022-07-11 05:06:08+00:00
- **Updated**: 2022-07-28 03:20:04+00:00
- **Authors**: Yi Ma, Doris Tsao, Heung-Yeung Shum
- **Comment**: 24 pages, 11 figures. This updated version makes changes in languages
  and adds a few additional references. This is the final version to be
  published
- **Journal**: None
- **Summary**: Ten years into the revival of deep networks and artificial intelligence, we propose a theoretical framework that sheds light on understanding deep networks within a bigger picture of Intelligence in general. We introduce two fundamental principles, Parsimony and Self-consistency, that address two fundamental questions regarding Intelligence: what to learn and how to learn, respectively. We believe the two principles are the cornerstones for the emergence of Intelligence, artificial or natural. While these two principles have rich classical roots, we argue that they can be stated anew in entirely measurable and computable ways. More specifically, the two principles lead to an effective and efficient computational framework, compressive closed-loop transcription, that unifies and explains the evolution of modern deep networks and many artificial intelligence practices. While we mainly use modeling of visual data as an example, we believe the two principles will unify understanding of broad families of autonomous intelligent systems and provide a framework for understanding the brain.



### SkexGen: Autoregressive Generation of CAD Construction Sequences with Disentangled Codebooks
- **Arxiv ID**: http://arxiv.org/abs/2207.04632v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.04632v1)
- **Published**: 2022-07-11 05:10:51+00:00
- **Updated**: 2022-07-11 05:10:51+00:00
- **Authors**: Xiang Xu, Karl D. D. Willis, Joseph G. Lambourne, Chin-Yi Cheng, Pradeep Kumar Jayaraman, Yasutaka Furukawa
- **Comment**: Accepted to ICML 2022
- **Journal**: None
- **Summary**: We present SkexGen, a novel autoregressive generative model for computer-aided design (CAD) construction sequences containing sketch-and-extrude modeling operations. Our model utilizes distinct Transformer architectures to encode topological, geometric, and extrusion variations of construction sequences into disentangled codebooks. Autoregressive Transformer decoders generate CAD construction sequences sharing certain properties specified by the codebook vectors. Extensive experiments demonstrate that our disentangled codebook representation generates diverse and high-quality CAD models, enhances user control, and enables efficient exploration of the design space. The code is available at https://samxuxiang.github.io/skexgen.



### A Dual-Polarization Information Guided Network for SAR Ship Classification
- **Arxiv ID**: http://arxiv.org/abs/2207.04639v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04639v1)
- **Published**: 2022-07-11 05:47:27+00:00
- **Updated**: 2022-07-11 05:47:27+00:00
- **Authors**: Tianwen Zhang, Xiaoling Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: How to fully utilize polarization to enhance synthetic aperture radar (SAR) ship classification remains an unresolved issue. Thus, we propose a dual-polarization information guided network (DPIG-Net) to solve it.



### A Lexicon and Depth-wise Separable Convolution Based Handwritten Text Recognition System
- **Arxiv ID**: http://arxiv.org/abs/2207.04651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04651v1)
- **Published**: 2022-07-11 06:24:26+00:00
- **Updated**: 2022-07-11 06:24:26+00:00
- **Authors**: Lalita Kumari, Sukhdeep Singh, VVS Rathore, Anuj Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: Cursive handwritten text recognition is a challenging research problem in the domain of pattern recognition. The current state-of-the-art approaches include models based on convolutional recurrent neural networks and multi-dimensional long short-term memory recurrent neural networks techniques. These methods are highly computationally extensive as well model is complex at design level. In recent studies, combination of convolutional neural network and gated convolutional neural networks based models demonstrated less number of parameters in comparison to convolutional recurrent neural networks based models. In the direction to reduced the total number of parameters to be trained, in this work, we have used depthwise convolution in place of standard convolutions with a combination of gated-convolutional neural network and bidirectional gated recurrent unit to reduce the total number of parameters to be trained. Additionally, we have also included a lexicon based word beam search decoder at testing step. It also helps in improving the the overall accuracy of the model. We have obtained 3.84% character error rate and 9.40% word error rate on IAM dataset; 4.88% character error rate and 14.56% word error rate in George Washington dataset, respectively.



### Personalizing Federated Medical Image Segmentation via Local Calibration
- **Arxiv ID**: http://arxiv.org/abs/2207.04655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04655v1)
- **Published**: 2022-07-11 06:30:31+00:00
- **Updated**: 2022-07-11 06:30:31+00:00
- **Authors**: Jiacheng Wang, Yueming Jin, Liansheng Wang
- **Comment**: Accepted at ECCV 2022
- **Journal**: None
- **Summary**: Medical image segmentation under federated learning (FL) is a promising direction by allowing multiple clinical sites to collaboratively learn a global model without centralizing datasets. However, using a single model to adapt to various data distributions from different sites is extremely challenging. Personalized FL tackles this issue by only utilizing partial model parameters shared from global server, while keeping the rest to adapt to its own data distribution in the local training of each site. However, most existing methods concentrate on the partial parameter splitting, while do not consider the \textit{inter-site in-consistencies} during the local training, which in fact can facilitate the knowledge communication over sites to benefit the model learning for improving the local accuracy. In this paper, we propose a personalized federated framework with \textbf{L}ocal \textbf{C}alibration (LC-Fed), to leverage the inter-site in-consistencies in both \textit{feature- and prediction- levels} to boost the segmentation. Concretely, as each local site has its alternative attention on the various features, we first design the contrastive site embedding coupled with channel selection operation to calibrate the encoded features. Moreover, we propose to exploit the knowledge of prediction-level in-consistency to guide the personalized modeling on the ambiguous regions, e.g., anatomical boundaries. It is achieved by computing a disagreement-aware map to calibrate the prediction. Effectiveness of our method has been verified on three medical image segmentation tasks with different modalities, where our method consistently shows superior performance to the state-of-the-art personalized FL methods. Code is available at https://github.com/jcwang123/FedLC.



### Learning Spatial and Temporal Variations for 4D Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.04673v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04673v1)
- **Published**: 2022-07-11 07:36:26+00:00
- **Updated**: 2022-07-11 07:36:26+00:00
- **Authors**: Shi Hanyu, Wei Jiacheng, Wang Hao, Liu Fayao, Lin Guosheng
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: LiDAR-based 3D scene perception is a fundamental and important task for autonomous driving. Most state-of-the-art methods on LiDAR-based 3D recognition tasks focus on single frame 3D point cloud data, and the temporal information is ignored in those methods. We argue that the temporal information across the frames provides crucial knowledge for 3D scene perceptions, especially in the driving scenario. In this paper, we focus on spatial and temporal variations to better explore the temporal information across the 3D frames. We design a temporal variation-aware interpolation module and a temporal voxel-point refiner to capture the temporal variation in the 4D point cloud. The temporal variation-aware interpolation generates local features from the previous and current frames by capturing spatial coherence and temporal variation information. The temporal voxel-point refiner builds a temporal graph on the 3D point cloud sequences and captures the temporal variation with a graph convolution module. The temporal voxel-point refiner also transforms the coarse voxel-level predictions into fine point-level predictions. With our proposed modules, the new network TVSN achieves state-of-the-art performance on SemanticKITTI and SemantiPOSS. Specifically, our method achieves 52.5\% in mIoU (+5.5% against previous best approaches) on the multiple scan segmentation task on SemanticKITTI, and 63.0% on SemanticPOSS (+2.8% against previous best approaches).



### COO: Comic Onomatopoeia Dataset for Recognizing Arbitrary or Truncated Texts
- **Arxiv ID**: http://arxiv.org/abs/2207.04675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04675v1)
- **Published**: 2022-07-11 07:39:35+00:00
- **Updated**: 2022-07-11 07:39:35+00:00
- **Authors**: Jeonghun Baek, Yusuke Matsui, Kiyoharu Aizawa
- **Comment**: Accepted at ECCV 2022. 25 pages, 16 figures
- **Journal**: None
- **Summary**: Recognizing irregular texts has been a challenging topic in text recognition. To encourage research on this topic, we provide a novel comic onomatopoeia dataset (COO), which consists of onomatopoeia texts in Japanese comics. COO has many arbitrary texts, such as extremely curved, partially shrunk texts, or arbitrarily placed texts. Furthermore, some texts are separated into several parts. Each part is a truncated text and is not meaningful by itself. These parts should be linked to represent the intended meaning. Thus, we propose a novel task that predicts the link between truncated texts. We conduct three tasks to detect the onomatopoeia region and capture its intended meaning: text detection, text recognition, and link prediction. Through extensive experiments, we analyze the characteristics of the COO. Our data and code are available at \url{https://github.com/ku21fan/COO-Comic-Onomatopoeia}.



### Towards Scale-Aware, Robust, and Generalizable Unsupervised Monocular Depth Estimation by Integrating IMU Motion Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2207.04680v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04680v3)
- **Published**: 2022-07-11 07:50:22+00:00
- **Updated**: 2022-07-21 02:11:03+00:00
- **Authors**: Sen Zhang, Jing Zhang, Dacheng Tao
- **Comment**: Accepted to ECCV 2022. Code is released at
  https://github.com/SenZHANG-GitHub/ekf-imu-depth
- **Journal**: None
- **Summary**: Unsupervised monocular depth and ego-motion estimation has drawn extensive research attention in recent years. Although current methods have reached a high up-to-scale accuracy, they usually fail to learn the true scale metric due to the inherent scale ambiguity from training with monocular sequences. In this work, we tackle this problem and propose DynaDepth, a novel scale-aware framework that integrates information from vision and IMU motion dynamics. Specifically, we first propose an IMU photometric loss and a cross-sensor photometric consistency loss to provide dense supervision and absolute scales. To fully exploit the complementary information from both sensors, we further drive a differentiable camera-centric extended Kalman filter (EKF) to update the IMU preintegrated motions when observing visual measurements. In addition, the EKF formulation enables learning an ego-motion uncertainty measure, which is non-trivial for unsupervised methods. By leveraging IMU during training, DynaDepth not only learns an absolute scale, but also provides a better generalization ability and robustness against vision degradation such as illumination change and moving objects. We validate the effectiveness of DynaDepth by conducting extensive experiments and simulations on the KITTI and Make3D datasets.



### PUF-Phenotype: A Robust and Noise-Resilient Approach to Aid Intra-Group-based Authentication with DRAM-PUFs Using Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.04692v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04692v1)
- **Published**: 2022-07-11 08:13:08+00:00
- **Updated**: 2022-07-11 08:13:08+00:00
- **Authors**: Owen Millwood, Jack Miskelly, Bohao Yang, Prosanta Gope, Elif Kavun, Chenghua Lin
- **Comment**: 13 pages main text, 7 pages supplementary material (total 20 pages),
  8 figures, submitted to IEEE Transactions on Information Forensics and
  Security
- **Journal**: None
- **Summary**: As the demand for highly secure and dependable lightweight systems increases in the modern world, Physically Unclonable Functions (PUFs) continue to promise a lightweight alternative to high-cost encryption techniques and secure key storage. While the security features promised by PUFs are highly attractive for secure system designers, they have been shown to be vulnerable to various sophisticated attacks - most notably Machine Learning (ML) based modelling attacks (ML-MA) which attempt to digitally clone the PUF behaviour and thus undermine their security. More recent ML-MA have even exploited publicly known helper data required for PUF error correction in order to predict PUF responses without requiring knowledge of response data. In response to this, research is beginning to emerge regarding the authentication of PUF devices with the assistance of ML as opposed to traditional PUF techniques of storage and comparison of pre-known Challenge-Response pairs (CRPs). In this article, we propose a classification system using ML based on a novel `PUF-Phenotype' concept to accurately identify the origin and determine the validity of noisy memory derived (DRAM) PUF responses as an alternative to helper data-reliant denoising techniques. To our best knowledge, we are the first to perform classification over multiple devices per model to enable a group-based PUF authentication scheme. We achieve up to 98\% classification accuracy using a modified deep convolutional neural network (CNN) for feature extraction in conjunction with several well-established classifiers. We also experimentally verified the performance of our model on a Raspberry Pi device to determine the suitability of deploying our proposed model in a resource-constrained environment.



### Exploring Contextual Relationships for Cervical Abnormal Cell Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.04693v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04693v2)
- **Published**: 2022-07-11 08:15:29+00:00
- **Updated**: 2022-07-18 04:58:01+00:00
- **Authors**: Yixiong Liang, Shuo Feng, Qing Liu, Hulin Kuang, Jianfeng Liu, Liyan Liao, Yun Du, Jianxin Wang
- **Comment**: 10 pages, 14 tables, and 3 figures
- **Journal**: None
- **Summary**: Cervical abnormal cell detection is a challenging task as the morphological discrepancies between abnormal and normal cells are usually subtle. To determine whether a cervical cell is normal or abnormal, cytopathologists always take surrounding cells as references to identify its abnormality. To mimic these behaviors, we propose to explore contextual relationships to boost the performance of cervical abnormal cell detection. Specifically, both contextual relationships between cells and cell-to-global images are exploited to enhance features of each region of interest (RoI) proposals. Accordingly, two modules, dubbed as RoI-relationship attention module (RRAM) and global RoI attention module (GRAM), are developed and their combination strategies are also investigated. We establish a strong baseline by using Double-Head Faster R-CNN with feature pyramid network (FPN) and integrate our RRAM and GRAM into it to validate the effectiveness of the proposed modules. Experiments conducted on a large cervical cell detection dataset reveal that the introduction of RRAM and GRAM both achieves better average precision (AP) than the baseline methods. Moreover, when cascading RRAM and GRAM, our method outperforms the state-of-the-art (SOTA) methods. Furthermore, we also show the proposed feature enhancing scheme can facilitate both image-level and smear-level classification. The code and trained models are publicly available at https://github.com/CVIU-CSU/CR4CACD.



### Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches
- **Arxiv ID**: http://arxiv.org/abs/2207.04718v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04718v1)
- **Published**: 2022-07-11 08:59:09+00:00
- **Updated**: 2022-07-11 08:59:09+00:00
- **Authors**: Zhiyuan Cheng, James Liang, Hongjun Choi, Guanhong Tao, Zhiwen Cao, Dongfang Liu, Xiangyu Zhang
- **Comment**: ECCV2022
- **Journal**: None
- **Summary**: Deep learning has substantially boosted the performance of Monocular Depth Estimation (MDE), a critical component in fully vision-based autonomous driving (AD) systems (e.g., Tesla and Toyota). In this work, we develop an attack against learning-based MDE. In particular, we use an optimization-based method to systematically generate stealthy physical-object-oriented adversarial patches to attack depth estimation. We balance the stealth and effectiveness of our attack with object-oriented adversarial design, sensitive region localization, and natural style camouflage. Using real-world driving scenarios, we evaluate our attack on concurrent MDE models and a representative downstream task for AD (i.e., 3D object detection). Experimental results show that our method can generate stealthy, effective, and robust adversarial patches for different target objects and models and achieves more than 6 meters mean depth estimation error and 93% attack success rate (ASR) in object detection with a patch of 1/9 of the vehicle's rear area. Field tests on three different driving routes with a real vehicle indicate that we cause over 6 meters mean depth estimation error and reduce the object detection rate from 90.70% to 5.16% in continuous video frames.



### Hybrid Skip: A Biologically Inspired Skip Connection for the UNet Architecture
- **Arxiv ID**: http://arxiv.org/abs/2207.04721v1
- **DOI**: 10.1109/ACCESS.2022.3175864
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04721v1)
- **Published**: 2022-07-11 09:00:09+00:00
- **Updated**: 2022-07-11 09:00:09+00:00
- **Authors**: Nikolaos Zioulis, Georgios Albanis, Petros Drakoulis, Federico Alvarez, Dimitrios Zarpalas, Petros Daras
- **Comment**: Project page at https://vcl3d.github.io/HybridSkip/
- **Journal**: IEEE Access, Volume 10, 53928 - 53939, 17 May 2022
- **Summary**: In this work we introduce a biologically inspired long-range skip connection for the UNet architecture that relies on the perceptual illusion of hybrid images, being images that simultaneously encode two images. The fusion of early encoder features with deeper decoder ones allows UNet models to produce finer-grained dense predictions. While proven in segmentation tasks, the network's benefits are down-weighted for dense regression tasks as these long-range skip connections additionally result in texture transfer artifacts. Specifically for depth estimation, this hurts smoothness and introduces false positive edges which are detrimental to the task due to the depth maps' piece-wise smooth nature. The proposed HybridSkip connections show improved performance in balancing the trade-off between edge preservation, and the minimization of texture transfer artifacts that hurt smoothness. This is achieved by the proper and balanced exchange of information that Hybrid-Skip connections offer between the high and low frequency, encoder and decoder features, respectively.



### Interpretability by design using computer vision for behavioral sensing in child and adolescent psychiatry
- **Arxiv ID**: http://arxiv.org/abs/2207.04724v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.04724v1)
- **Published**: 2022-07-11 09:07:08+00:00
- **Updated**: 2022-07-11 09:07:08+00:00
- **Authors**: Flavia D. Frumosu, Nicole N. Lønfeldt, A. -R. Cecilie Mora-Jensen, Sneha Das, Nicklas Leander Lund, A. Katrine Pagsberg, Line K. H. Clemmensen
- **Comment**: Presented at 2nd Workshop on Interpretable Machine Learning in
  Healthcare (IMLH) - International Conference on Machine Learning (ICML) 2022
- **Journal**: None
- **Summary**: Observation is an essential tool for understanding and studying human behavior and mental states. However, coding human behavior is a time-consuming, expensive task, in which reliability can be difficult to achieve and bias is a risk. Machine learning (ML) methods offer ways to improve reliability, decrease cost, and scale up behavioral coding for application in clinical and research settings. Here, we use computer vision to derive behavioral codes or concepts of a gold standard behavioral rating system, offering familiar interpretation for mental health professionals. Features were extracted from videos of clinical diagnostic interviews of children and adolescents with and without obsessive-compulsive disorder. Our computationally-derived ratings were comparable to human expert ratings for negative emotions, activity-level/arousal and anxiety. For the attention and positive affect concepts, our ML ratings performed reasonably. However, results for gaze and vocalization indicate a need for improved data quality or additional data modalities.



### 2nd Place Solution to Google Landmark Retrieval 2020
- **Arxiv ID**: http://arxiv.org/abs/2210.01624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01624v1)
- **Published**: 2022-07-11 10:14:14+00:00
- **Updated**: 2022-07-11 10:14:14+00:00
- **Authors**: Min Yang, Cheng Cui, Xuetong Xue, Hui Ren, Kai Wei
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents the 2nd place solution to the Google Landmark Retrieval Competition 2020. We propose a training method of global feature model for landmark retrieval without post-processing, such as local feature and spatial verification. There are two parts in our retrieval method in this competition. This training scheme mainly includes training by increasing margin value of arcmargin loss and increasing image resolution step by step. Models are trained by PaddlePaddle framework and Pytorch framework, and then converted to tensorflow 2.2. Using this method, we got a public score of 0.40176 and a private score of 0.36278 and achieved 2nd place in the Google Landmark Retrieval Competition 2020.



### Geometry-aware Single-image Full-body Human Relighting
- **Arxiv ID**: http://arxiv.org/abs/2207.04750v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04750v2)
- **Published**: 2022-07-11 10:21:02+00:00
- **Updated**: 2022-07-12 15:08:16+00:00
- **Authors**: Chaonan Ji, Tao Yu, Kaiwen Guo, Jingxin Liu, Yebin Liu
- **Comment**: accepted by ECCV2022
- **Journal**: None
- **Summary**: Single-image human relighting aims to relight a target human under new lighting conditions by decomposing the input image into albedo, shape and lighting. Although plausible relighting results can be achieved, previous methods suffer from both the entanglement between albedo and lighting and the lack of hard shadows, which significantly decrease the realism. To tackle these two problems, we propose a geometry-aware single-image human relighting framework that leverages single-image geometry reconstruction for joint deployment of traditional graphics rendering and neural rendering techniques. For the de-lighting, we explore the shortcomings of UNet architecture and propose a modified HRNet, achieving better disentanglement between albedo and lighting. For the relighting, we introduce a ray tracing-based per-pixel lighting representation that explicitly models high-frequency shadows and propose a learning-based shading refinement module to restore realistic shadows (including hard cast shadows) from the ray-traced shading maps. Our framework is able to generate photo-realistic high-frequency shadows such as cast shadows under challenging lighting conditions. Extensive experiments demonstrate that our proposed method outperforms previous methods on both synthetic and real images.



### Snow Mask Guided Adaptive Residual Network for Image Snow Removal
- **Arxiv ID**: http://arxiv.org/abs/2207.04754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04754v1)
- **Published**: 2022-07-11 10:30:46+00:00
- **Updated**: 2022-07-11 10:30:46+00:00
- **Authors**: Bodong Cheng, Juncheng Li, Ying Chen, Shuyi Zhang, Tieyong Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Image restoration under severe weather is a challenging task. Most of the past works focused on removing rain and haze phenomena in images. However, snow is also an extremely common atmospheric phenomenon that will seriously affect the performance of high-level computer vision tasks, such as object detection and semantic segmentation. Recently, some methods have been proposed for snow removing, and most methods deal with snow images directly as the optimization object. However, the distribution of snow location and shape is complex. Therefore, failure to detect snowflakes / snow streak effectively will affect snow removing and limit the model performance. To solve these issues, we propose a Snow Mask Guided Adaptive Residual Network (SMGARN). Specifically, SMGARN consists of three parts, Mask-Net, Guidance-Fusion Network (GF-Net), and Reconstruct-Net. Firstly, we build a Mask-Net with Self-pixel Attention (SA) and Cross-pixel Attention (CA) to capture the features of snowflakes and accurately localized the location of the snow, thus predicting an accurate snow mask. Secondly, the predicted snow mask is sent into the specially designed GF-Net to adaptively guide the model to remove snow. Finally, an efficient Reconstruct-Net is used to remove the veiling effect and correct the image to reconstruct the final snow-free image. Extensive experiments show that our SMGARN numerically outperforms all existing snow removal methods, and the reconstructed images are clearer in visual contrast. All codes will be available.



### A Late Fusion Framework with Multiple Optimization Methods for Media Interestingness
- **Arxiv ID**: http://arxiv.org/abs/2207.04762v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04762v1)
- **Published**: 2022-07-11 10:48:34+00:00
- **Updated**: 2022-07-11 10:48:34+00:00
- **Authors**: Maria Shoukat, Khubaib Ahmad, Naina Said, Nasir Ahmad, Mohammed Hassanuzaman, Kashif Ahmad
- **Comment**: 10 pages, 1 figure
- **Journal**: None
- **Summary**: The recent advancement in Multimedia Analytical, Computer Vision (CV), and Artificial Intelligence (AI) algorithms resulted in several interesting tools allowing an automatic analysis and retrieval of multimedia content of users' interests. However, retrieving the content of interest generally involves analysis and extraction of semantic features, such as emotions and interestingness-level. The extraction of such meaningful information is a complex task and generally, the performance of individual algorithms is very low. One way to enhance the performance of the individual algorithms is to combine the predictive capabilities of multiple algorithms using fusion schemes. This allows the individual algorithms to complement each other, leading to improved performance. This paper proposes several fusion methods for the media interestingness score prediction task introduced in CLEF Fusion 2022. The proposed methods include both a naive fusion scheme, where all the inducers are treated equally and a merit-based fusion scheme where multiple weight optimization methods are employed to assign weights to the individual inducers. In total, we used six optimization methods including a Particle Swarm Optimization (PSO), a Genetic Algorithm (GA), Nelder Mead, Trust Region Constrained (TRC), and Limited-memory Broyden Fletcher Goldfarb Shanno Algorithm (LBFGSA), and Truncated Newton Algorithm (TNA). Overall better results are obtained with PSO and TNA achieving 0.109 mean average precision at 10. The task is complex and generally, scores are low. We believe the presented analysis will provide a baseline for future research in the domain.



### MT-Net Submission to the Waymo 3D Detection Leaderboard
- **Arxiv ID**: http://arxiv.org/abs/2207.04781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04781v1)
- **Published**: 2022-07-11 11:32:47+00:00
- **Updated**: 2022-07-11 11:32:47+00:00
- **Authors**: Shaoxiang Chen, Zequn Jie, Xiaolin Wei, Lin Ma
- **Comment**: None
- **Journal**: None
- **Summary**: In this technical report, we introduce our submission to the Waymo 3D Detection leaderboard. Our network is based on the Centerpoint architecture, but with significant improvements. We design a 2D backbone to utilize multi-scale features for better detecting objects with various sizes, together with an optimal transport-based target assignment strategy, which dynamically assigns richer supervision signals to the detection candidates. We also apply test-time augmentation and model-ensemble for further improvements. Our submission currently ranks 4th place with 78.45 mAPH on the Waymo 3D Detection leaderboard.



### DCCF: Deep Comprehensible Color Filter Learning Framework for High-Resolution Image Harmonization
- **Arxiv ID**: http://arxiv.org/abs/2207.04788v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.04788v3)
- **Published**: 2022-07-11 11:42:10+00:00
- **Updated**: 2022-07-20 03:50:20+00:00
- **Authors**: Ben Xue, Shenghui Ran, Quan Chen, Rongfei Jia, Binqiang Zhao, Xing Tang
- **Comment**: ECCV 2022 (Oral)
- **Journal**: None
- **Summary**: Image color harmonization algorithm aims to automatically match the color distribution of foreground and background images captured in different conditions. Previous deep learning based models neglect two issues that are critical for practical applications, namely high resolution (HR) image processing and model comprehensibility. In this paper, we propose a novel Deep Comprehensible Color Filter (DCCF) learning framework for high-resolution image harmonization. Specifically, DCCF first downsamples the original input image to its low-resolution (LR) counter-part, then learns four human comprehensible neural filters (i.e. hue, saturation, value and attentive rendering filters) in an end-to-end manner, finally applies these filters to the original input image to get the harmonized result. Benefiting from the comprehensible neural filters, we could provide a simple yet efficient handler for users to cooperate with deep model to get the desired results with very little effort when necessary. Extensive experiments demonstrate the effectiveness of DCCF learning framework and it outperforms state-of-the-art post-processing method on iHarmony4 dataset on images' full-resolutions by achieving 7.63% and 1.69% relative improvements on MSE and PSNR respectively.



### PCCT: Progressive Class-Center Triplet Loss for Imbalanced Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2207.04793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04793v1)
- **Published**: 2022-07-11 11:43:51+00:00
- **Updated**: 2022-07-11 11:43:51+00:00
- **Authors**: Kanghao Chen, Weixian Lei, Rong Zhang, Shen Zhao, Wei-shi Zheng, Ruixuan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Imbalanced training data is a significant challenge for medical image classification. In this study, we propose a novel Progressive Class-Center Triplet (PCCT) framework to alleviate the class imbalance issue particularly for diagnosis of rare diseases, mainly by carefully designing the triplet sampling strategy and the triplet loss formation. Specifically, the PCCT framework includes two successive stages. In the first stage, PCCT trains the diagnosis system via a class-balanced triplet loss to coarsely separate distributions of different classes. In the second stage, the PCCT framework further improves the diagnosis system via a class-center involved triplet loss to cause a more compact distribution for each class. For the class-balanced triplet loss, triplets are sampled equally for each class at each training iteration, thus alleviating the imbalanced data issue. For the class-center involved triplet loss, the positive and negative samples in each triplet are replaced by their corresponding class centers, which enforces data representations of the same class closer to the class center. Furthermore, the class-center involved triplet loss is extended to the pair-wise ranking loss and the quadruplet loss, which demonstrates the generalization of the proposed framework. Extensive experiments support that the PCCT framework works effectively for medical image classification with imbalanced training images. On two skin image datasets and one chest X-ray dataset, the proposed approach respectively obtains the mean F1 score 86.2, 65.2, and 90.66 over all classes and 81.4, 63.87, and 81.92 for rare classes, achieving state-of-the-art performance and outperforming the widely used methods for the class imbalance issue.



### CCPL: Contrastive Coherence Preserving Loss for Versatile Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/2207.04808v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04808v4)
- **Published**: 2022-07-11 12:09:41+00:00
- **Updated**: 2022-07-19 09:10:40+00:00
- **Authors**: Zijie Wu, Zhen Zhu, Junping Du, Xiang Bai
- **Comment**: Accepted by ECCV2022 as an oral paper; code url:
  https://github.com/JarrentWu1031/CCPL Video demo:
  https://youtu.be/scZuJCXhL14
- **Journal**: None
- **Summary**: In this paper, we aim to devise a universally versatile style transfer method capable of performing artistic, photo-realistic, and video style transfer jointly, without seeing videos during training. Previous single-frame methods assume a strong constraint on the whole image to maintain temporal consistency, which could be violated in many cases. Instead, we make a mild and reasonable assumption that global inconsistency is dominated by local inconsistencies and devise a generic Contrastive Coherence Preserving Loss (CCPL) applied to local patches. CCPL can preserve the coherence of the content source during style transfer without degrading stylization. Moreover, it owns a neighbor-regulating mechanism, resulting in a vast reduction of local distortions and considerable visual quality improvement. Aside from its superior performance on versatile style transfer, it can be easily extended to other tasks, such as image-to-image translation. Besides, to better fuse content and style features, we propose Simple Covariance Transformation (SCT) to effectively align second-order statistics of the content feature with the style feature. Experiments demonstrate the effectiveness of the resulting model for versatile style transfer, when armed with CCPL.



### Fingerprint Liveness Detection Based on Quality Measures
- **Arxiv ID**: http://arxiv.org/abs/2207.04809v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04809v1)
- **Published**: 2022-07-11 12:15:27+00:00
- **Updated**: 2022-07-11 12:15:27+00:00
- **Authors**: Javier Galbally, Fernando Alonso-Fernandez, Julian Fierrez, Javier Ortega-Garcia
- **Comment**: Published at IEEE International Conference on Biometrics, Identity
  and Security (BIdS). arXiv admin note: substantial text overlap with
  arXiv:2111.01898
- **Journal**: None
- **Summary**: A new fingerprint parameterization for liveness detection based on quality measures is presented. The novel feature set is used in a complete liveness detection system and tested on the development set of the LivDET competition, comprising over 4,500 real and fake images acquired with three different optical sensors. The proposed solution proves to be robust to the multi-sensor scenario, and presents an overall rate of 93% of correctly classified samples. Furthermore, the liveness detection method presented has the added advantage over previously studied techniques of needing just one image from a finger to decide whether it is real or fake.



### A clinically motivated self-supervised approach for content-based image retrieval of CT liver images
- **Arxiv ID**: http://arxiv.org/abs/2207.04812v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2207.04812v1)
- **Published**: 2022-07-11 12:16:29+00:00
- **Updated**: 2022-07-11 12:16:29+00:00
- **Authors**: Kristoffer Knutsen Wickstrøm, Eirik Agnalt Østmo, Keyur Radiya, Karl Øyvind Mikalsen, Michael Christian Kampffmeyer, Robert Jenssen
- **Comment**: Code:
  https://github.com/Wickstrom/clinical-self-supervised-CBIR-ct-liver
- **Journal**: None
- **Summary**: Deep learning-based approaches for content-based image retrieval (CBIR) of CT liver images is an active field of research, but suffers from some critical limitations. First, they are heavily reliant on labeled data, which can be challenging and costly to acquire. Second, they lack transparency and explainability, which limits the trustworthiness of deep CBIR systems. We address these limitations by (1) proposing a self-supervised learning framework that incorporates domain-knowledge into the training procedure and (2) providing the first representation learning explainability analysis in the context of CBIR of CT liver images. Results demonstrate improved performance compared to the standard self-supervised approach across several metrics, as well as improved generalisation across datasets. Further, we conduct the first representation learning explainability analysis in the context of CBIR, which reveals new insights into the feature extraction process. Lastly, we perform a case study with cross-examination CBIR that demonstrates the usability of our proposed framework. We believe that our proposed framework could play a vital role in creating trustworthy deep CBIR systems that can successfully take advantage of unlabeled data.



### On the vulnerability of fingerprint verification systems to fake fingerprint attacks
- **Arxiv ID**: http://arxiv.org/abs/2207.04813v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04813v1)
- **Published**: 2022-07-11 12:22:52+00:00
- **Updated**: 2022-07-11 12:22:52+00:00
- **Authors**: Javier Galbally, Julian Fierrez-Aguilar, Joaquin Rodriguez-Gonzalez, Fernando Alonso-Fernandez, Javier Ortega-Garcia, Marino Tapiador
- **Comment**: Published at IEEE International Carnahan Conference on Security
  Technology (ICCST)
- **Journal**: None
- **Summary**: A new method to generate gummy fingers is presented. A medium-size fake fingerprint database is described and two different fingerprint verification systems are evaluated on it. Three different scenarios are considered in the experiments, namely: enrollment and test with real fingerprints, enrollment and test with fake fingerprints, and enrollment with real fingerprints and test with fake fingerprints. Results for an optical and a thermal sweeping sensors are given. Both systems are shown to be vulnerable to direct attacks.



### Cross-modal Prototype Driven Network for Radiology Report Generation
- **Arxiv ID**: http://arxiv.org/abs/2207.04818v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2207.04818v1)
- **Published**: 2022-07-11 12:29:33+00:00
- **Updated**: 2022-07-11 12:29:33+00:00
- **Authors**: Jun Wang, Abhir Bhalerao, Yulan He
- **Comment**: Accepted to ECCV2022
- **Journal**: None
- **Summary**: Radiology report generation (RRG) aims to describe automatically a radiology image with human-like language and could potentially support the work of radiologists, reducing the burden of manual reporting. Previous approaches often adopt an encoder-decoder architecture and focus on single-modal feature learning, while few studies explore cross-modal feature interaction. Here we propose a Cross-modal PROtotype driven NETwork (XPRONET) to promote cross-modal pattern learning and exploit it to improve the task of radiology report generation. This is achieved by three well-designed, fully differentiable and complementary modules: a shared cross-modal prototype matrix to record the cross-modal prototypes; a cross-modal prototype network to learn the cross-modal prototypes and embed the cross-modal information into the visual and textual features; and an improved multi-label contrastive loss to enable and enhance multi-label prototype learning. XPRONET obtains substantial improvements on the IU-Xray and MIMIC-CXR benchmarks, where its performance exceeds recent state-of-the-art approaches by a large margin on IU-Xray and comparable performance on MIMIC-CXR.



### Bayesian Experimental Design for Computed Tomography with the Linearised Deep Image Prior
- **Arxiv ID**: http://arxiv.org/abs/2207.05714v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.05714v1)
- **Published**: 2022-07-11 12:45:31+00:00
- **Updated**: 2022-07-11 12:45:31+00:00
- **Authors**: Riccardo Barbano, Johannes Leuschner, Javier Antorán, Bangti Jin, José Miguel Hernández-Lobato
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate adaptive design based on a single sparse pilot scan for generating effective scanning strategies for computed tomography reconstruction. We propose a novel approach using the linearised deep image prior. It allows incorporating information from the pilot measurements into the angle selection criteria, while maintaining the tractability of a conjugate Gaussian-linear model. On a synthetically generated dataset with preferential directions, linearised DIP design allows reducing the number of scans by up to 30% relative to an equidistant angle baseline.



### Forward Error Correction applied to JPEG-XS codestreams
- **Arxiv ID**: http://arxiv.org/abs/2207.04825v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/2207.04825v2)
- **Published**: 2022-07-11 12:46:33+00:00
- **Updated**: 2022-10-07 11:11:04+00:00
- **Authors**: Antoine Legrand, Benoît Macq, Christophe De Vleeschouwer
- **Comment**: None
- **Journal**: None
- **Summary**: JPEG-XS offers low complexity image compression for applications with constrained but reasonable bit-rate, and low latency. Our paper explores the deployment of JPEG-XS on lossy packet networks. To preserve low latency, Forward Error Correction (FEC) is envisioned as the protection mechanism of interest. Despite the JPEG-XS codestream is not scalable in essence, we observe that the loss of a codestream fraction impacts the decoded image quality differently, depending on whether this codestream fraction corresponds to codestream headers, to coefficients significance information, or to low/high frequency data, respectively. Hence, we propose a rate-distortion optimal unequal error protection scheme that adapts the redundancy level of Reed-Solomon codes according to the rate of channel losses and the type of information protected by the code. Our experiments demonstrate that, at 5% loss rates, it reduces the Mean Squared Error by up to 92% and 65%, compared to a transmission without and with optimal but equal protection, respectively.



### LaT: Latent Translation with Cycle-Consistency for Video-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2207.04858v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.04858v2)
- **Published**: 2022-07-11 13:37:32+00:00
- **Updated**: 2023-02-13 18:00:34+00:00
- **Authors**: Jinbin Bai, Chunhui Liu, Feiyue Ni, Haofan Wang, Mengying Hu, Xiaofeng Guo, Lele Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Video-text retrieval is a class of cross-modal representation learning problems, where the goal is to select the video which corresponds to the text query between a given text query and a pool of candidate videos. The contrastive paradigm of vision-language pretraining has shown promising success with large-scale datasets and unified transformer architecture, and demonstrated the power of a joint latent space. Despite this, the intrinsic divergence between the visual domain and textual domain is still far from being eliminated, and projecting different modalities into a joint latent space might result in the distorting of the information inside the single modality. To overcome the above issue, we present a novel mechanism for learning the translation relationship from a source modality space $\mathcal{S}$ to a target modality space $\mathcal{T}$ without the need for a joint latent space, which bridges the gap between visual and textual domains. Furthermore, to keep cycle consistency between translations, we adopt a cycle loss involving both forward translations from $\mathcal{S}$ to the predicted target space $\mathcal{T'}$, and backward translations from $\mathcal{T'}$ back to $\mathcal{S}$. Extensive experiments conducted on MSR-VTT, MSVD, and DiDeMo datasets demonstrate the superiority and effectiveness of our LaT approach compared with vanilla state-of-the-art methods.



### SDFEst: Categorical Pose and Shape Estimation of Objects from RGB-D using Signed Distance Fields
- **Arxiv ID**: http://arxiv.org/abs/2207.04880v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.04880v1)
- **Published**: 2022-07-11 13:53:50+00:00
- **Updated**: 2022-07-11 13:53:50+00:00
- **Authors**: Leonard Bruns, Patric Jensfelt
- **Comment**: Accepted to IEEE Robotics and Automation Letters (and IROS 2022).
  Project page: https://github.com/roym899/sdfest
- **Journal**: None
- **Summary**: Rich geometric understanding of the world is an important component of many robotic applications such as planning and manipulation. In this paper, we present a modular pipeline for pose and shape estimation of objects from RGB-D images given their category. The core of our method is a generative shape model, which we integrate with a novel initialization network and a differentiable renderer to enable 6D pose and shape estimation from a single or multiple views. We investigate the use of discretized signed distance fields as an efficient shape representation for fast analysis-by-synthesis optimization. Our modular framework enables multi-view optimization and extensibility. We demonstrate the benefits of our approach over state-of-the-art methods in several experiments on both synthetic and real data. We open-source our approach at https://github.com/roym899/sdfest.



### Adversarial Style Augmentation for Domain Generalized Urban-Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.04892v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04892v2)
- **Published**: 2022-07-11 14:01:25+00:00
- **Updated**: 2022-10-13 03:31:36+00:00
- **Authors**: Zhun Zhong, Yuyang Zhao, Gim Hee Lee, Nicu Sebe
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: In this paper, we consider the problem of domain generalization in semantic segmentation, which aims to learn a robust model using only labeled synthetic (source) data. The model is expected to perform well on unseen real (target) domains. Our study finds that the image style variation can largely influence the model's performance and the style features can be well represented by the channel-wise mean and standard deviation of images. Inspired by this, we propose a novel adversarial style augmentation (AdvStyle) approach, which can dynamically generate hard stylized images during training and thus can effectively prevent the model from overfitting on the source domain. Specifically, AdvStyle regards the style feature as a learnable parameter and updates it by adversarial training. The learned adversarial style feature is used to construct an adversarial image for robust model training. AdvStyle is easy to implement and can be readily applied to different models. Experiments on two synthetic-to-real semantic segmentation benchmarks demonstrate that AdvStyle can significantly improve the model performance on unseen real domains and show that we can achieve the state of the art. Moreover, AdvStyle can be employed to domain generalized image classification and produces a clear improvement on the considered datasets.



### Going the Extra Mile in Face Image Quality Assessment: A Novel Database and Model
- **Arxiv ID**: http://arxiv.org/abs/2207.04904v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04904v2)
- **Published**: 2022-07-11 14:28:18+00:00
- **Updated**: 2023-07-30 14:12:09+00:00
- **Authors**: Shaolin Su, Hanhe Lin, Vlad Hosu, Oliver Wiedemann, Jinqiu Sun, Yu Zhu, Hantao Liu, Yanning Zhang, Dietmar Saupe
- **Comment**: Appearing in IEEE TMM
- **Journal**: None
- **Summary**: An accurate computational model for image quality assessment (IQA) benefits many vision applications, such as image filtering, image processing, and image generation. Although the study of face images is an important subfield in computer vision research, the lack of face IQA data and models limits the precision of current IQA metrics on face image processing tasks such as face superresolution, face enhancement, and face editing. To narrow this gap, in this paper, we first introduce the largest annotated IQA database developed to date, which contains 20,000 human faces -- an order of magnitude larger than all existing rated datasets of faces -- of diverse individuals in highly varied circumstances. Based on the database, we further propose a novel deep learning model to accurately predict face image quality, which, for the first time, explores the use of generative priors for IQA. By taking advantage of rich statistics encoded in well pretrained off-the-shelf generative models, we obtain generative prior information and use it as latent references to facilitate blind IQA. The experimental results demonstrate both the value of the proposed dataset for face IQA and the superior performance of the proposed model.



### Detection of Condensed Vehicle Gas Exhaust in LiDAR Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2207.04908v1
- **DOI**: 10.1109/ITSC55140.2022.9922475
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04908v1)
- **Published**: 2022-07-11 14:36:27+00:00
- **Updated**: 2022-07-11 14:36:27+00:00
- **Authors**: Aldi Piroli, Vinzenz Dallabetta, Marc Walessa, Daniel Meissner, Johannes Kopp, Klaus Dietmayer
- **Comment**: Accepted for ITSC2022
- **Journal**: 2022 IEEE 25th International Conference on Intelligent
  Transportation Systems (ITSC)
- **Summary**: LiDAR sensors used in autonomous driving applications are negatively affected by adverse weather conditions. One common, but understudied effect, is the condensation of vehicle gas exhaust in cold weather. This everyday phenomenon can severely impact the quality of LiDAR measurements, resulting in a less accurate environment perception by creating artifacts like ghost object detections. In the literature, the semantic segmentation of adverse weather effects like rain and fog is achieved using learning-based approaches. However, such methods require large sets of labeled data, which can be extremely expensive and laborious to get. We address this problem by presenting a two-step approach for the detection of condensed vehicle gas exhaust. First, we identify for each vehicle in a scene its emission area and detect gas exhaust if present. Then, isolated clouds are detected by modeling through time the regions of space where gas exhaust is likely to be present. We test our method on real urban data, showing that our approach can reliably detect gas exhaust in different scenarios, making it appealing for offline pre-labeling and online applications such as ghost object detection.



### Physical Passive Patch Adversarial Attacks on Visual Odometry Systems
- **Arxiv ID**: http://arxiv.org/abs/2207.05729v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.05729v2)
- **Published**: 2022-07-11 14:41:06+00:00
- **Updated**: 2022-10-04 06:22:32+00:00
- **Authors**: Yaniv Nemcovsky, Matan Jacoby, Alex M. Bronstein, Chaim Baskin
- **Comment**: Accepted to ACCV 2022
- **Journal**: None
- **Summary**: Deep neural networks are known to be susceptible to adversarial perturbations -- small perturbations that alter the output of the network and exist under strict norm limitations. While such perturbations are usually discussed as tailored to a specific input, a universal perturbation can be constructed to alter the model's output on a set of inputs. Universal perturbations present a more realistic case of adversarial attacks, as awareness of the model's exact input is not required. In addition, the universal attack setting raises the subject of generalization to unseen data, where given a set of inputs, the universal perturbations aim to alter the model's output on out-of-sample data. In this work, we study physical passive patch adversarial attacks on visual odometry-based autonomous navigation systems. A visual odometry system aims to infer the relative camera motion between two corresponding viewpoints, and is frequently used by vision-based autonomous navigation systems to estimate their state. For such navigation systems, a patch adversarial perturbation poses a severe security issue, as it can be used to mislead a system onto some collision course. To the best of our knowledge, we show for the first time that the error margin of a visual odometry model can be significantly increased by deploying patch adversarial attacks in the scene. We provide evaluation on synthetic closed-loop drone navigation data and demonstrate that a comparable vulnerability exists in real data. A reference implementation of the proposed method and the reported experiments is provided at https://github.com/patchadversarialattacks/patchadversarialattacks.



### Generalizing to Unseen Domains with Wasserstein Distributional Robustness under Limited Source Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2207.04913v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04913v1)
- **Published**: 2022-07-11 14:46:50+00:00
- **Updated**: 2022-07-11 14:46:50+00:00
- **Authors**: Jingge Wang, Liyan Xie, Yao Xie, Shao-Lun Huang, Yang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Domain generalization aims at learning a universal model that performs well on unseen target domains, incorporating knowledge from multiple source domains. In this research, we consider the scenario where different domain shifts occur among conditional distributions of different classes across domains. When labeled samples in the source domains are limited, existing approaches are not sufficiently robust. To address this problem, we propose a novel domain generalization framework called Wasserstein Distributionally Robust Domain Generalization (WDRDG), inspired by the concept of distributionally robust optimization. We encourage robustness over conditional distributions within class-specific Wasserstein uncertainty sets and optimize the worst-case performance of a classifier over these uncertainty sets. We further develop a test-time adaptation module leveraging optimal transport to quantify the relationship between the unseen target domain and source domains to make adaptive inference for target data. Experiments on the Rotated MNIST, PACS and the VLCS datasets demonstrate that our method could effectively balance the robustness and discriminability in challenging generalization scenarios.



### Multi-level Geometric Optimization for Regularised Constrained Linear Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/2207.04934v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/2207.04934v1)
- **Published**: 2022-07-11 15:15:33+00:00
- **Updated**: 2022-07-11 15:15:33+00:00
- **Authors**: Sebastian Müller, Stefania Petra, Matthias Zisler
- **Comment**: 26 pages, 6 figures
- **Journal**: None
- **Summary**: We present a geometric multi-level optimization approach that smoothly incorporates box constraints. Given a box constrained optimization problem, we consider a hierarchy of models with varying discretization levels. Finer models are accurate but expensive to compute, while coarser models are less accurate but cheaper to compute. When working at the fine level, multi-level optimisation computes the search direction based on a coarser model which speeds up updates at the fine level. Moreover, exploiting geometry induced by the hierarchy the feasibility of the updates is preserved. In particular, our approach extends classical components of multigrid methods like restriction and prolongation to the Riemannian structure of our constraints.



### A Skeleton-aware Graph Convolutional Network for Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.05733v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.05733v1)
- **Published**: 2022-07-11 15:20:18+00:00
- **Updated**: 2022-07-11 15:20:18+00:00
- **Authors**: Manli Zhu, Edmond S. L. Ho, Hubert P. H. Shum
- **Comment**: Accepted by IEEE SMC 2022
- **Journal**: None
- **Summary**: Detecting human-object interactions is essential for comprehensive understanding of visual scenes. In particular, spatial connections between humans and objects are important cues for reasoning interactions. To this end, we propose a skeleton-aware graph convolutional network for human-object interaction detection, named SGCN4HOI. Our network exploits the spatial connections between human keypoints and object keypoints to capture their fine-grained structural interactions via graph convolutions. It fuses such geometric features with visual features and spatial configuration features obtained from human-object pairs. Furthermore, to better preserve the object structural information and facilitate human-object interaction detection, we propose a novel skeleton-based object keypoints representation. The performance of SGCN4HOI is evaluated in the public benchmark V-COCO dataset. Experimental results show that the proposed approach outperforms the state-of-the-art pose-based models and achieves competitive performance against other models.



### SHREC'22 Track: Sketch-Based 3D Shape Retrieval in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2207.04945v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.04945v1)
- **Published**: 2022-07-11 15:26:52+00:00
- **Updated**: 2022-07-11 15:26:52+00:00
- **Authors**: Jie Qin, Shuaihang Yuan, Jiaxin Chen, Boulbaba Ben Amor, Yi Fang, Nhat Hoang-Xuan, Chi-Bien Chu, Khoi-Nguyen Nguyen-Ngoc, Thien-Tri Cao, Nhat-Khang Ngo, Tuan-Luc Huynh, Hai-Dang Nguyen, Minh-Triet Tran, Haoyang Luo, Jianning Wang, Zheng Zhang, Zihao Xin, Yang Wang, Feng Wang, Ying Tang, Haiqin Chen, Yan Wang, Qunying Zhou, Ji Zhang, Hongyuan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Sketch-based 3D shape retrieval (SBSR) is an important yet challenging task, which has drawn more and more attention in recent years. Existing approaches address the problem in a restricted setting, without appropriately simulating real application scenarios. To mimic the realistic setting, in this track, we adopt large-scale sketches drawn by amateurs of different levels of drawing skills, as well as a variety of 3D shapes including not only CAD models but also models scanned from real objects. We define two SBSR tasks and construct two benchmarks consisting of more than 46,000 CAD models, 1,700 realistic models, and 145,000 sketches in total. Four teams participated in this track and submitted 15 runs for the two tasks, evaluated by 7 commonly-adopted metrics. We hope that, the benchmarks, the comparative results, and the open-sourced evaluation code will foster future research in this direction among the 3D object retrieval community.



### From Correlation to Causation: Formalizing Interpretable Machine Learning as a Statistical Process
- **Arxiv ID**: http://arxiv.org/abs/2207.04969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.04969v1)
- **Published**: 2022-07-11 15:49:14+00:00
- **Updated**: 2022-07-11 15:49:14+00:00
- **Authors**: Lukas Klein, Mennatallah El-Assady, Paul F. Jäger
- **Comment**: Accepted at IJCAI 2022 Workshop on Explainable Artificial
  Intelligence (XAI)
- **Journal**: None
- **Summary**: Explainable AI (XAI) is a necessity in safety-critical systems such as in clinical diagnostics due to a high risk for fatal decisions. Currently, however, XAI resembles a loose collection of methods rather than a well-defined process. In this work, we elaborate on conceptual similarities between the largest subgroup of XAI, interpretable machine learning (IML), and classical statistics. Based on these similarities, we present a formalization of IML along the lines of a statistical process. Adopting this statistical view allows us to interpret machine learning models and IML methods as sophisticated statistical tools. Based on this interpretation, we infer three key questions, which we identify as crucial for the success and adoption of IML in safety-critical settings. By formulating these questions, we further aim to spark a discussion about what distinguishes IML from classical statistics and what our perspective implies for the future of the field.



### Dual Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2207.04976v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.04976v2)
- **Published**: 2022-07-11 16:03:44+00:00
- **Updated**: 2022-07-12 08:26:22+00:00
- **Authors**: Ting Yao, Yehao Li, Yingwei Pan, Yu Wang, Xiao-Ping Zhang, Tao Mei
- **Comment**: Source code is available at
  \url{https://github.com/YehLi/ImageNetModel}
- **Journal**: None
- **Summary**: Prior works have proposed several strategies to reduce the computational cost of self-attention mechanism. Many of these works consider decomposing the self-attention procedure into regional and local feature extraction procedures that each incurs a much smaller computational complexity. However, regional information is typically only achieved at the expense of undesirable information lost owing to down-sampling. In this paper, we propose a novel Transformer architecture that aims to mitigate the cost issue, named Dual Vision Transformer (Dual-ViT). The new architecture incorporates a critical semantic pathway that can more efficiently compress token vectors into global semantics with reduced order of complexity. Such compressed global semantics then serve as useful prior information in learning finer pixel level details, through another constructed pixel pathway. The semantic pathway and pixel pathway are then integrated together and are jointly trained, spreading the enhanced self-attention information in parallel through both of the pathways. Dual-ViT is henceforth able to reduce the computational complexity without compromising much accuracy. We empirically demonstrate that Dual-ViT provides superior accuracy than SOTA Transformer architectures with reduced training complexity. Source code is available at \url{https://github.com/YehLi/ImageNetModel}.



### Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.04978v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.04978v1)
- **Published**: 2022-07-11 16:03:51+00:00
- **Updated**: 2022-07-11 16:03:51+00:00
- **Authors**: Ting Yao, Yingwei Pan, Yehao Li, Chong-Wah Ngo, Tao Mei
- **Comment**: ECCV 2022. Source code is available at
  \url{https://github.com/YehLi/ImageNetModel}
- **Journal**: None
- **Summary**: Multi-scale Vision Transformer (ViT) has emerged as a powerful backbone for computer vision tasks, while the self-attention computation in Transformer scales quadratically w.r.t. the input patch number. Thus, existing solutions commonly employ down-sampling operations (e.g., average pooling) over keys/values to dramatically reduce the computational cost. In this work, we argue that such over-aggressive down-sampling design is not invertible and inevitably causes information dropping especially for high-frequency components in objects (e.g., texture details). Motivated by the wavelet theory, we construct a new Wavelet Vision Transformer (\textbf{Wave-ViT}) that formulates the invertible down-sampling with wavelet transforms and self-attention learning in a unified way. This proposal enables self-attention learning with lossless down-sampling over keys/values, facilitating the pursuing of a better efficiency-vs-accuracy trade-off. Furthermore, inverse wavelet transforms are leveraged to strengthen self-attention outputs by aggregating local contexts with enlarged receptive field. We validate the superiority of Wave-ViT through extensive experiments over multiple vision tasks (e.g., image recognition, object detection and instance segmentation). Its performances surpass state-of-the-art ViT backbones with comparable FLOPs. Source code is available at \url{https://github.com/YehLi/ImageNetModel}.



### A Closer Look at Invariances in Self-supervised Pre-training for 3D Vision
- **Arxiv ID**: http://arxiv.org/abs/2207.04997v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04997v2)
- **Published**: 2022-07-11 16:44:15+00:00
- **Updated**: 2022-07-13 16:17:57+00:00
- **Authors**: Lanxiao Li, Michael Heizmann
- **Comment**: Accepted on ECCV 2022
- **Journal**: None
- **Summary**: Self-supervised pre-training for 3D vision has drawn increasing research interest in recent years. In order to learn informative representations, a lot of previous works exploit invariances of 3D features, e.g., perspective-invariance between views of the same scene, modality-invariance between depth and RGB images, format-invariance between point clouds and voxels. Although they have achieved promising results, previous researches lack a systematic and fair comparison of these invariances. To address this issue, our work, for the first time, introduces a unified framework, under which various pre-training methods can be investigated. We conduct extensive experiments and provide a closer look at the contributions of different invariances in 3D pre-training. Also, we propose a simple but effective method that jointly pre-trains a 3D encoder and a depth map encoder using contrastive learning. Models pre-trained with our method gain significant performance boost in downstream tasks. For instance, a pre-trained VoteNet outperforms previous methods on SUN RGB-D and ScanNet object detection benchmarks with a clear margin.



### Consistency is the key to further mitigating catastrophic forgetting in continual learning
- **Arxiv ID**: http://arxiv.org/abs/2207.04998v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04998v1)
- **Published**: 2022-07-11 16:44:49+00:00
- **Updated**: 2022-07-11 16:44:49+00:00
- **Authors**: Prashant Bhat, Bahram Zonooz, Elahe Arani
- **Comment**: Accepted at Conference on Lifelong Learning Agents (CoLLAs 2022)
- **Journal**: None
- **Summary**: Deep neural networks struggle to continually learn multiple sequential tasks due to catastrophic forgetting of previously learned tasks. Rehearsal-based methods which explicitly store previous task samples in the buffer and interleave them with the current task samples have proven to be the most effective in mitigating forgetting. However, Experience Replay (ER) does not perform well under low-buffer regimes and longer task sequences as its performance is commensurate with the buffer size. Consistency in predictions of soft-targets can assist ER in preserving information pertaining to previous tasks better as soft-targets capture the rich similarity structure of the data. Therefore, we examine the role of consistency regularization in ER framework under various continual learning scenarios. We also propose to cast consistency regularization as a self-supervised pretext task thereby enabling the use of a wide variety of self-supervised learning methods as regularizers. While simultaneously enhancing model calibration and robustness to natural corruptions, regularizing consistency in predictions results in lesser forgetting across all continual learning scenarios. Among the different families of regularizers, we find that stricter consistency constraints preserve previous task information in ER better.



### Intra-Modal Constraint Loss For Image-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2207.05024v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.05024v2)
- **Published**: 2022-07-11 17:21:25+00:00
- **Updated**: 2022-07-13 16:09:24+00:00
- **Authors**: Jianan Chen, Lu Zhang, Qiong Wang, Cong Bai, Kidiyo Kpalma
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-modal retrieval has drawn much attention in both computer vision and natural language processing domains. With the development of convolutional and recurrent neural networks, the bottleneck of retrieval across image-text modalities is no longer the extraction of image and text features but an efficient loss function learning in embedding space. Many loss functions try to closer pairwise features from heterogeneous modalities. This paper proposes a method for learning joint embedding of images and texts using an intra-modal constraint loss function to reduce the violation of negative pairs from the same homogeneous modality. Experimental results show that our approach outperforms state-of-the-art bi-directional image-text retrieval methods on Flickr30K and Microsoft COCO datasets. Our code is publicly available: https://github.com/CanonChen/IMC.



### PSP-HDRI$+$: A Synthetic Dataset Generator for Pre-Training of Human-Centric Computer Vision Models
- **Arxiv ID**: http://arxiv.org/abs/2207.05025v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.DB, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.05025v1)
- **Published**: 2022-07-11 17:22:58+00:00
- **Updated**: 2022-07-11 17:22:58+00:00
- **Authors**: Salehe Erfanian Ebadi, Saurav Dhakad, Sanjay Vishwakarma, Chunpu Wang, You-Cyuan Jhang, Maciek Chociej, Adam Crespi, Alex Thaman, Sujoy Ganguly
- **Comment**: PSP-HDRI$+$ template Unity environment, benchmark binaries, and
  source code will be made available at:
  https://github.com/Unity-Technologies/PeopleSansPeople
- **Journal**: None
- **Summary**: We introduce a new synthetic data generator PSP-HDRI$+$ that proves to be a superior pre-training alternative to ImageNet and other large-scale synthetic data counterparts. We demonstrate that pre-training with our synthetic data will yield a more general model that performs better than alternatives even when tested on out-of-distribution (OOD) sets. Furthermore, using ablation studies guided by person keypoint estimation metrics with an off-the-shelf model architecture, we show how to manipulate our synthetic data generator to further improve model performance.



### Unsupervised Semantic Segmentation with Self-supervised Object-centric Representations
- **Arxiv ID**: http://arxiv.org/abs/2207.05027v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.05027v2)
- **Published**: 2022-07-11 17:28:24+00:00
- **Updated**: 2023-04-30 17:39:42+00:00
- **Authors**: Andrii Zadaianchuk, Matthaeus Kleindessner, Yi Zhu, Francesco Locatello, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we show that recent advances in self-supervised feature learning enable unsupervised object discovery and semantic segmentation with a performance that matches the state of the field on supervised semantic segmentation 10 years ago. We propose a methodology based on unsupervised saliency masks and self-supervised feature clustering to kickstart object discovery followed by training a semantic segmentation network on pseudo-labels to bootstrap the system on images with multiple objects. We present results on PASCAL VOC that go far beyond the current state of the art (50.0 mIoU), and we report for the first time results on MS COCO for the whole set of 81 classes: our method discovers 34 categories with more than $20\%$ IoU, while obtaining an average IoU of 19.6 for all 81 categories.



### Audio-Visual Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.05042v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.05042v3)
- **Published**: 2022-07-11 17:50:36+00:00
- **Updated**: 2023-02-17 14:15:01+00:00
- **Authors**: Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, Yiran Zhong
- **Comment**: ECCV 2022; Code is available at
  https://github.com/OpenNLPLab/AVSBench
- **Journal**: None
- **Summary**: We propose to explore a new problem called audio-visual segmentation (AVS), in which the goal is to output a pixel-level map of the object(s) that produce sound at the time of the image frame. To facilitate this research, we construct the first audio-visual segmentation benchmark (AVSBench), providing pixel-wise annotations for the sounding objects in audible videos. Two settings are studied with this benchmark: 1) semi-supervised audio-visual segmentation with a single sound source and 2) fully-supervised audio-visual segmentation with multiple sound sources. To deal with the AVS problem, we propose a novel method that uses a temporal pixel-wise audio-visual interaction module to inject audio semantics as guidance for the visual segmentation process. We also design a regularization loss to encourage the audio-visual mapping during training. Quantitative and qualitative experiments on the AVSBench compare our approach to several existing methods from related tasks, demonstrating that the proposed method is promising for building a bridge between the audio and pixel-wise visual semantics. Code is available at https://github.com/OpenNLPLab/AVSBench.



### Fast-Vid2Vid: Spatial-Temporal Compression for Video-to-Video Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2207.05049v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.05049v1)
- **Published**: 2022-07-11 17:57:57+00:00
- **Updated**: 2022-07-11 17:57:57+00:00
- **Authors**: Long Zhuo, Guangcong Wang, Shikai Li, Wayne Wu, Ziwei Liu
- **Comment**: ECCV 2022, Project Page: https://fast-vid2vid.github.io/ , Code:
  https://github.com/fast-vid2vid/fast-vid2vid
- **Journal**: None
- **Summary**: Video-to-Video synthesis (Vid2Vid) has achieved remarkable results in generating a photo-realistic video from a sequence of semantic maps. However, this pipeline suffers from high computational cost and long inference latency, which largely depends on two essential factors: 1) network architecture parameters, 2) sequential data stream. Recently, the parameters of image-based generative models have been significantly compressed via more efficient network architectures. Nevertheless, existing methods mainly focus on slimming network architectures and ignore the size of the sequential data stream. Moreover, due to the lack of temporal coherence, image-based compression is not sufficient for the compression of the video task. In this paper, we present a spatial-temporal compression framework, \textbf{Fast-Vid2Vid}, which focuses on data aspects of generative models. It makes the first attempt at time dimension to reduce computational resources and accelerate inference. Specifically, we compress the input data stream spatially and reduce the temporal redundancy. After the proposed spatial-temporal knowledge distillation, our model can synthesize key-frames using the low-resolution data stream. Finally, Fast-Vid2Vid interpolates intermediate frames by motion compensation with slight latency. On standard benchmarks, Fast-Vid2Vid achieves around real-time performance as 20 FPS and saves around 8x computational cost on a single V100 GPU.



### Learning Continuous Grasping Function with a Dexterous Hand from Human Demonstrations
- **Arxiv ID**: http://arxiv.org/abs/2207.05053v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.05053v3)
- **Published**: 2022-07-11 17:59:50+00:00
- **Updated**: 2023-03-19 05:12:15+00:00
- **Authors**: Jianglong Ye, Jiashun Wang, Binghao Huang, Yuzhe Qin, Xiaolong Wang
- **Comment**: Accepted to RA-L 2023 & IROS 2023. Project page:
  https://jianglongye.com/cgf
- **Journal**: None
- **Summary**: We propose to learn to generate grasping motion for manipulation with a dexterous hand using implicit functions. With continuous time inputs, the model can generate a continuous and smooth grasping plan. We name the proposed model Continuous Grasping Function (CGF). CGF is learned via generative modeling with a Conditional Variational Autoencoder using 3D human demonstrations. We will first convert the large-scale human-object interaction trajectories to robot demonstrations via motion retargeting, and then use these demonstrations to train CGF. During inference, we perform sampling with CGF to generate different grasping plans in the simulator and select the successful ones to transfer to the real robot. By training on diverse human data, our CGF allows generalization to manipulate multiple objects. Compared to previous planning algorithms, CGF is more efficient and achieves significant improvement on success rate when transferred to grasping with the real Allegro Hand. Our project page is available at https://jianglongye.com/cgf .



### Demystifying Unsupervised Semantic Correspondence Estimation
- **Arxiv ID**: http://arxiv.org/abs/2207.05054v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.05054v1)
- **Published**: 2022-07-11 17:59:51+00:00
- **Updated**: 2022-07-11 17:59:51+00:00
- **Authors**: Mehmet Aygün, Oisin Mac Aodha
- **Comment**: ECCV22, project page https://mehmetaygun.github.io/demistfy.html
- **Journal**: None
- **Summary**: We explore semantic correspondence estimation through the lens of unsupervised learning. We thoroughly evaluate several recently proposed unsupervised methods across multiple challenging datasets using a standardized evaluation protocol where we vary factors such as the backbone architecture, the pre-training strategy, and the pre-training and finetuning datasets. To better understand the failure modes of these methods, and in order to provide a clearer path for improvement, we provide a new diagnostic framework along with a new performance metric that is better suited to the semantic matching task. Finally, we introduce a new unsupervised correspondence approach which utilizes the strength of pre-trained features while encouraging better matches during training. This results in significantly better matching performance compared to current state-of-the-art methods.



### RUSH: Robust Contrastive Learning via Randomized Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2207.05127v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.05127v2)
- **Published**: 2022-07-11 18:45:14+00:00
- **Updated**: 2022-10-30 01:14:32+00:00
- **Authors**: Yijiang Pang, Boyang Liu, Jiayu Zhou
- **Comment**: incomplete validation, the defense strategy will fail when
  considering Expectation Over Test (EOT)
- **Journal**: None
- **Summary**: Recently, adversarial training has been incorporated in self-supervised contrastive pre-training to augment label efficiency with exciting adversarial robustness. However, the robustness came at a cost of expensive adversarial training. In this paper, we show a surprising fact that contrastive pre-training has an interesting yet implicit connection with robustness, and such natural robustness in the pre trained representation enables us to design a powerful robust algorithm against adversarial attacks, RUSH, that combines the standard contrastive pre-training and randomized smoothing. It boosts both standard accuracy and robust accuracy, and significantly reduces training costs as compared with adversarial training. We use extensive empirical studies to show that the proposed RUSH outperforms robust classifiers from adversarial training, by a significant margin on common benchmarks (CIFAR-10, CIFAR-100, and STL-10) under first-order attacks. In particular, under $\ell_{\infty}$-norm perturbations of size 8/255 PGD attack on CIFAR-10, our model using ResNet-18 as backbone reached 77.8% robust accuracy and 87.9% standard accuracy. Our work has an improvement of over 15% in robust accuracy and a slight improvement in standard accuracy, compared to the state-of-the-arts.



### Towards Effective Multi-Label Recognition Attacks via Knowledge Graph Consistency
- **Arxiv ID**: http://arxiv.org/abs/2207.05137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.05137v1)
- **Published**: 2022-07-11 19:08:32+00:00
- **Updated**: 2022-07-11 19:08:32+00:00
- **Authors**: Hassan Mahmood, Ehsan Elhamifar
- **Comment**: None
- **Journal**: None
- **Summary**: Many real-world applications of image recognition require multi-label learning, whose goal is to find all labels in an image. Thus, robustness of such systems to adversarial image perturbations is extremely important. However, despite a large body of recent research on adversarial attacks, the scope of the existing works is mainly limited to the multi-class setting, where each image contains a single label. We show that the naive extensions of multi-class attacks to the multi-label setting lead to violating label relationships, modeled by a knowledge graph, and can be detected using a consistency verification scheme. Therefore, we propose a graph-consistent multi-label attack framework, which searches for small image perturbations that lead to misclassifying a desired target set while respecting label hierarchies. By extensive experiments on two datasets and using several multi-label recognition models, we show that our method generates extremely successful attacks that, unlike naive multi-label perturbations, can produce model predictions consistent with the knowledge graph.



### Accelerated Deep Lossless Image Coding with Unified Paralleleized GPU Coding Architecture
- **Arxiv ID**: http://arxiv.org/abs/2207.05152v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.05152v1)
- **Published**: 2022-07-11 19:34:56+00:00
- **Updated**: 2022-07-11 19:34:56+00:00
- **Authors**: Benjamin Lukas Cajus Barzen, Fedor Glazov, Jonas Geistert, Thomas Sikora
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Deep Lossless Image Coding (DLIC), a full resolution learned lossless image compression algorithm. Our algorithm is based on a neural network combined with an entropy encoder. The neural network performs a density estimation on each pixel of the source image. The density estimation is then used to code the target pixel, beating FLIF in terms of compression rate. Similar approaches have been attempted. However, long run times make them unfeasible for real world applications. We introduce a parallelized GPU based implementation, allowing for encoding and decoding of grayscale, 8-bit images in less than one second. Because DLIC uses a neural network to estimate the probabilities used for the entropy coder, DLIC can be trained on domain specific image data. We demonstrate this capability by adapting and training DLIC with Magnet Resonance Imaging (MRI) images.



### Denoising single images by feature ensemble revisited
- **Arxiv ID**: http://arxiv.org/abs/2207.05176v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.05176v1)
- **Published**: 2022-07-11 20:23:55+00:00
- **Updated**: 2022-07-11 20:23:55+00:00
- **Authors**: Masud An Nur Islam Fahim, Nazmus Saqib, Shafkat Khan Siam, Ho Yub Jung
- **Comment**: None
- **Journal**: None
- **Summary**: Image denoising is still a challenging issue in many computer vision sub-domains. Recent studies show that significant improvements are made possible in a supervised setting. However, few challenges, such as spatial fidelity and cartoon-like smoothing remain unresolved or decisively overlooked. Our study proposes a simple yet efficient architecture for the denoising problem that addresses the aforementioned issues. The proposed architecture revisits the concept of modular concatenation instead of long and deeper cascaded connections, to recover a cleaner approximation of the given image. We find that different modules can capture versatile representations, and concatenated representation creates a richer subspace for low-level image restoration. The proposed architecture's number of parameters remains smaller than the number for most of the previous networks and still achieves significant improvements over the current state-of-the-art networks.



### Fine-grained Activities of People Worldwide
- **Arxiv ID**: http://arxiv.org/abs/2207.05182v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.05182v2)
- **Published**: 2022-07-11 20:38:10+00:00
- **Updated**: 2022-10-20 14:37:31+00:00
- **Authors**: Jeffrey Byrne, Greg Castanon, Zhongheng Li, Gil Ettinger
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: Every day, humans perform many closely related activities that involve subtle discriminative motions, such as putting on a shirt vs. putting on a jacket, or shaking hands vs. giving a high five. Activity recognition by ethical visual AI could provide insights into our patterns of daily life, however existing activity recognition datasets do not capture the massive diversity of these human activities around the world. To address this limitation, we introduce Collector, a free mobile app to record video while simultaneously annotating objects and activities of consented subjects. This new data collection platform was used to curate the Consented Activities of People (CAP) dataset, the first large-scale, fine-grained activity dataset of people worldwide. The CAP dataset contains 1.45M video clips of 512 fine grained activity labels of daily life, collected by 780 subjects in 33 countries. We provide activity classification and activity detection benchmarks for this dataset, and analyze baseline results to gain insight into how people around with world perform common activities. The dataset, benchmarks, evaluation tools, public leaderboards and mobile apps are available for use at visym.github.io/cap.



### Patch-level instance-group discrimination with pretext-invariant learning for colitis scoring
- **Arxiv ID**: http://arxiv.org/abs/2207.05192v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.05192v1)
- **Published**: 2022-07-11 21:06:29+00:00
- **Updated**: 2022-07-11 21:06:29+00:00
- **Authors**: Ziang Xu, Sharib Ali, Soumya Gupta, Simon Leedham, James E East, Jens Rittscher
- **Comment**: 11
- **Journal**: None
- **Summary**: Inflammatory bowel disease (IBD), in particular ulcerative colitis (UC), is graded by endoscopists and this assessment is the basis for risk stratification and therapy monitoring. Presently, endoscopic characterisation is largely operator dependant leading to sometimes undesirable clinical outcomes for patients with IBD. We focus on the Mayo Endoscopic Scoring (MES) system which is widely used but requires the reliable identification of subtle changes in mucosal inflammation. Most existing deep learning classification methods cannot detect these fine-grained changes which make UC grading such a challenging task. In this work, we introduce a novel patch-level instance-group discrimination with pretext-invariant representation learning (PLD-PIRL) for self-supervised learning (SSL). Our experiments demonstrate both improved accuracy and robustness compared to the baseline supervised network and several state-of-the-art SSL methods. Compared to the baseline (ResNet50) supervised classification our proposed PLD-PIRL obtained an improvement of 4.75% on hold-out test data and 6.64% on unseen center test data for top-1 accuracy.



### Collaborative Uncertainty Benefits Multi-Agent Multi-Modal Trajectory Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2207.05195v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2207.05195v1)
- **Published**: 2022-07-11 21:17:41+00:00
- **Updated**: 2022-07-11 21:17:41+00:00
- **Authors**: Bohan Tang, Yiqi Zhong, Chenxin Xu, Wei-Tao Wu, Ulrich Neumann, Yanfeng Wang, Ya Zhang, Siheng Chen
- **Comment**: arXiv admin note: text overlap with arXiv:2110.13947
- **Journal**: None
- **Summary**: In multi-modal multi-agent trajectory forecasting, two major challenges have not been fully tackled: 1) how to measure the uncertainty brought by the interaction module that causes correlations among the predicted trajectories of multiple agents; 2) how to rank the multiple predictions and select the optimal predicted trajectory. In order to handle these challenges, this work first proposes a novel concept, collaborative uncertainty (CU), which models the uncertainty resulting from interaction modules. Then we build a general CU-aware regression framework with an original permutation-equivariant uncertainty estimator to do both tasks of regression and uncertainty estimation. Further, we apply the proposed framework to current SOTA multi-agent multi-modal forecasting systems as a plugin module, which enables the SOTA systems to 1) estimate the uncertainty in the multi-agent multi-modal trajectory forecasting task; 2) rank the multiple predictions and select the optimal one based on the estimated uncertainty. We conduct extensive experiments on a synthetic dataset and two public large-scale multi-agent trajectory forecasting benchmarks. Experiments show that: 1) on the synthetic dataset, the CU-aware regression framework allows the model to appropriately approximate the ground-truth Laplace distribution; 2) on the multi-agent trajectory forecasting benchmarks, the CU-aware regression framework steadily helps SOTA systems improve their performances. Specially, the proposed framework helps VectorNet improve by 262 cm regarding the Final Displacement Error of the chosen optimal prediction on the nuScenes dataset; 3) for multi-agent multi-modal trajectory forecasting systems, prediction uncertainty is positively correlated with future stochasticity; and 4) the estimated CU values are highly related to the interactive information among agents.



### Real-Time And Robust 3D Object Detection with Roadside LiDARs
- **Arxiv ID**: http://arxiv.org/abs/2207.05200v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.05200v1)
- **Published**: 2022-07-11 21:33:42+00:00
- **Updated**: 2022-07-11 21:33:42+00:00
- **Authors**: Walter Zimmer, Jialong Wu, Xingcheng Zhou, Alois C. Knoll
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2204.00132
- **Journal**: None
- **Summary**: This work aims to address the challenges in autonomous driving by focusing on the 3D perception of the environment using roadside LiDARs. We design a 3D object detection model that can detect traffic participants in roadside LiDARs in real-time. Our model uses an existing 3D detector as a baseline and improves its accuracy. To prove the effectiveness of our proposed modules, we train and evaluate the model on three different vehicle and infrastructure datasets. To show the domain adaptation ability of our detector, we train it on an infrastructure dataset from China and perform transfer learning on a different dataset recorded in Germany. We do several sets of experiments and ablation studies for each module in the detector that show that our model outperforms the baseline by a significant margin, while the inference speed is at 45 Hz (22 ms). We make a significant contribution with our LiDAR-based 3D detector that can be used for smart city applications to provide connected and automated vehicles with a far-reaching view. Vehicles that are connected to the roadside sensors can get information about other vehicles around the corner to improve their path and maneuver planning and to increase road traffic safety.



### Scaling Novel Object Detection with Weakly Supervised Detection Transformers
- **Arxiv ID**: http://arxiv.org/abs/2207.05205v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.05205v3)
- **Published**: 2022-07-11 21:45:54+00:00
- **Updated**: 2023-05-25 19:11:28+00:00
- **Authors**: Tyler LaBonte, Yale Song, Xin Wang, Vibhav Vineet, Neel Joshi
- **Comment**: WACV 2023. Preliminary version appeared in CVPR 2022 Workshop on
  Transformers for Vision
- **Journal**: None
- **Summary**: A critical object detection task is finetuning an existing model to detect novel objects, but the standard workflow requires bounding box annotations which are time-consuming and expensive to collect. Weakly supervised object detection (WSOD) offers an appealing alternative, where object detectors can be trained using image-level labels. However, the practical application of current WSOD models is limited, as they only operate at small data scales and require multiple rounds of training and refinement. To address this, we propose the Weakly Supervised Detection Transformer, which enables efficient knowledge transfer from a large-scale pretraining dataset to WSOD finetuning on hundreds of novel objects. Additionally, we leverage pretrained knowledge to improve the multiple instance learning (MIL) framework often used in WSOD methods. Our experiments show that our approach outperforms previous state-of-the-art models on large-scale novel object detection datasets, and our scaling study reveals that class quantity is more important than image quantity for WSOD pretraining. The code is available at https://github.com/tmlabonte/weakly-supervised-DETR.



### Susceptibility of Continual Learning Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2207.05225v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.05225v3)
- **Published**: 2022-07-11 23:45:12+00:00
- **Updated**: 2022-07-14 15:28:31+00:00
- **Authors**: Hikmat Khan, Pir Masoom Shah, Syed Farhan Alam Zaidi, Saif ul Islam
- **Comment**: 18 pages, 13 figures
- **Journal**: None
- **Summary**: The recent advances in continual (incremental or lifelong) learning have concentrated on the prevention of forgetting that can lead to catastrophic consequences, but there are two outstanding challenges that must be addressed. The first is the evaluation of the robustness of the proposed methods. The second is ensuring the security of learned tasks remains largely unexplored. This paper presents a comprehensive study of the susceptibility of the continually learned tasks (including both current and previously learned tasks) that are vulnerable to forgetting. Such vulnerability of tasks against adversarial attacks raises profound issues in data integrity and privacy. We consider all three scenarios (i.e, task-incremental leaning, domain-incremental learning and class-incremental learning) of continual learning and explore three regularization-based experiments, three replay-based experiments, and one hybrid technique based on the reply and exemplar approach. We examine the robustness of these methods. In particular, we consider cases where we demonstrate that any class belonging to the current or previously learned tasks is prone to misclassification. Our observations, we identify potential limitations in continual learning approaches against adversarial attacks. Our empirical study recommends that the research community consider the robustness of the proposed continual learning approaches and invest extensive efforts in mitigating catastrophic forgetting.



