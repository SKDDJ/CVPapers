# Arxiv Papers in cs.CV on 2022-07-23
### Two-Aspect Information Fusion Model For ABAW4 Multi-task Challenge
- **Arxiv ID**: http://arxiv.org/abs/2207.11389v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.11389v1)
- **Published**: 2022-07-23 01:48:51+00:00
- **Updated**: 2022-07-23 01:48:51+00:00
- **Authors**: Haiyang Sun, Zheng Lian, Bin Liu, Jianhua Tao, Licai Sun, Cong Cai
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose the solution to the Multi-Task Learning (MTL) Challenge of the 4th Affective Behavior Analysis in-the-wild (ABAW) competition. The task of ABAW is to predict frame-level emotion descriptors from videos: discrete emotional state; valence and arousal; and action units. Although researchers have proposed several approaches and achieved promising results in ABAW, current works in this task rarely consider interactions between different emotion descriptors. To this end, we propose a novel end to end architecture to achieve full integration of different types of information. Experimental results demonstrate the effectiveness of our proposed solution.



### Deep Pneumonia: Attention-Based Contrastive Learning for Class-Imbalanced Pneumonia Lesion Recognition in Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/2207.11393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11393v1)
- **Published**: 2022-07-23 02:28:37+00:00
- **Updated**: 2022-07-23 02:28:37+00:00
- **Authors**: Xinxu Wei, Haohan Bai, Xianshi Zhang, Yongjie Li
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: Computer-aided X-ray pneumonia lesion recognition is important for accurate diagnosis of pneumonia. With the emergence of deep learning, the identification accuracy of pneumonia has been greatly improved, but there are still some challenges due to the fuzzy appearance of chest X-rays. In this paper, we propose a deep learning framework named Attention-Based Contrastive Learning for Class-Imbalanced X-Ray Pneumonia Lesion Recognition (denoted as Deep Pneumonia). We adopt self-supervised contrastive learning strategy to pre-train the model without using extra pneumonia data for fully mining the limited available dataset. In order to leverage the location information of the lesion area that the doctor has painstakingly marked, we propose mask-guided hard attention strategy and feature learning with contrastive regulation strategy which are applied on the attention map and the extracted features respectively to guide the model to focus more attention on the lesion area where contains more discriminative features for improving the recognition performance. In addition, we adopt Class-Balanced Loss instead of traditional Cross-Entropy as the loss function of classification to tackle the problem of serious class imbalance between different classes of pneumonia in the dataset. The experimental results show that our proposed framework can be used as a reliable computer-aided pneumonia diagnosis system to assist doctors to better diagnose pneumonia cases accurately.



### Orientation and Context Entangled Network for Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.11396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11396v1)
- **Published**: 2022-07-23 02:37:37+00:00
- **Updated**: 2022-07-23 02:37:37+00:00
- **Authors**: Xinxu Wei, Kaifu Yang, Danilo Bzdok, Yongjie Li
- **Comment**: 62 pages, 14 figures
- **Journal**: None
- **Summary**: Most of the existing deep learning based methods for vessel segmentation neglect two important aspects of retinal vessels, one is the orientation information of vessels, and the other is the contextual information of the whole fundus region. In this paper, we propose a robust Orientation and Context Entangled Network (denoted as OCE-Net), which has the capability of extracting complex orientation and context information of the blood vessels. To achieve complex orientation aware, a Dynamic Complex Orientation Aware Convolution (DCOA Conv) is proposed to extract complex vessels with multiple orientations for improving the vessel continuity. To simultaneously capture the global context information and emphasize the important local information, a Global and Local Fusion Module (GLFM) is developed to simultaneously model the long-range dependency of vessels and focus sufficient attention on local thin vessels. A novel Orientation and Context Entangled Non-local (OCE-NL) module is proposed to entangle the orientation and context information together. In addition, an Unbalanced Attention Refining Module (UARM) is proposed to deal with the unbalanced pixel numbers of background, thick and thin vessels. Extensive experiments were performed on several commonly used datasets (DRIVE, STARE and CHASEDB1) and some more challenging datasets (AV-WIDE, UoA-DR, RFMiD and UK Biobank). The ablation study shows that the proposed method achieves promising performance on maintaining the continuity of thin vessels and the comparative experiments demonstrate that our OCE-Net can achieve state-of-the-art performance on retinal vessel segmentation.



### Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations
- **Arxiv ID**: http://arxiv.org/abs/2207.11401v2
- **DOI**: 10.1145/3503161.3548284
- **Categories**: **cs.CL**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.11401v2)
- **Published**: 2022-07-23 03:19:50+00:00
- **Updated**: 2022-12-02 09:15:39+00:00
- **Authors**: Qian Yang, Yunxin Li, Baotian Hu, Lin Ma, Yuxing Ding, Min Zhang
- **Comment**: 11 pages (including Supplementary Materials); Accepted to ACM MM 2022
- **Journal**: ACM International Conference on Multimedia. 2022. 3587-3597
- **Summary**: Visual Entailment with natural language explanations aims to infer the relationship between a text-image pair and generate a sentence to explain the decision-making process. Previous methods rely mainly on a pre-trained vision-language model to perform the relation inference and a language model to generate the corresponding explanation. However, the pre-trained vision-language models mainly build token-level alignment between text and image yet ignore the high-level semantic alignment between the phrases (chunks) and visual contents, which is critical for vision-language reasoning. Moreover, the explanation generator based only on the encoded joint representation does not explicitly consider the critical decision-making points of relation inference. Thus the generated explanations are less faithful to visual-language reasoning. To mitigate these problems, we propose a unified Chunk-aware Alignment and Lexical Constraint based method, dubbed as CALeC. It contains a Chunk-aware Semantic Interactor (arr. CSI), a relation inferrer, and a Lexical Constraint-aware Generator (arr. LeCG). Specifically, CSI exploits the sentence structure inherent in language and various image regions to build chunk-aware semantic alignment. Relation inferrer uses an attention-based reasoning network to incorporate the token-level and chunk-level vision-language representations. LeCG utilizes lexical constraints to expressly incorporate the words or chunks focused by the relation inferrer into explanation generation, improving the faithfulness and informativeness of the explanations. We conduct extensive experiments on three datasets, and experimental results indicate that CALeC significantly outperforms other competitor models on inference accuracy and quality of generated explanations.



### PS-NeRF: Neural Inverse Rendering for Multi-view Photometric Stereo
- **Arxiv ID**: http://arxiv.org/abs/2207.11406v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.11406v2)
- **Published**: 2022-07-23 03:55:18+00:00
- **Updated**: 2022-12-22 06:54:21+00:00
- **Authors**: Wenqi Yang, Guanying Chen, Chaofeng Chen, Zhenfang Chen, Kwan-Yee K. Wong
- **Comment**: ECCV 2022, Project page: https://ywq.github.io/psnerf
- **Journal**: None
- **Summary**: Traditional multi-view photometric stereo (MVPS) methods are often composed of multiple disjoint stages, resulting in noticeable accumulated errors. In this paper, we present a neural inverse rendering method for MVPS based on implicit representation. Given multi-view images of a non-Lambertian object illuminated by multiple unknown directional lights, our method jointly estimates the geometry, materials, and lights. Our method first employs multi-light images to estimate per-view surface normal maps, which are used to regularize the normals derived from the neural radiance field. It then jointly optimizes the surface normals, spatially-varying BRDFs, and lights based on a shadow-aware differentiable rendering layer. After optimization, the reconstructed object can be used for novel-view rendering, relighting, and material editing. Experiments on both synthetic and real datasets demonstrate that our method achieves far more accurate shape reconstruction than existing MVPS and neural rendering methods. Our code and model can be found at https://ywq.github.io/psnerf.



### Halftoning with Multi-Agent Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.11408v1
- **DOI**: 10.1109/ICIP46576.2022.9897198
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2207.11408v1)
- **Published**: 2022-07-23 04:16:03+00:00
- **Updated**: 2022-07-23 04:16:03+00:00
- **Authors**: Haitian Jiang, Dongliang Xiong, Xiaowen Jiang, Aiguo Yin, Li Ding, Kai Huang
- **Comment**: ICIP 2022
- **Journal**: None
- **Summary**: Deep neural networks have recently succeeded in digital halftoning using vanilla convolutional layers with high parallelism. However, existing deep methods fail to generate halftones with a satisfying blue-noise property and require complex training schemes. In this paper, we propose a halftoning method based on multi-agent deep reinforcement learning, called HALFTONERS, which learns a shared policy to generate high-quality halftone images. Specifically, we view the decision of each binary pixel value as an action of a virtual agent, whose policy is trained by a low-variance policy gradient. Moreover, the blue-noise property is achieved by a novel anisotropy suppressing loss function. Experiments show that our halftoning method produces high-quality halftones while staying relatively fast.



### Satellite Detection in Unresolved Space Imagery for Space Domain Awareness Using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2207.11412v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.11412v1)
- **Published**: 2022-07-23 04:28:45+00:00
- **Updated**: 2022-07-23 04:28:45+00:00
- **Authors**: Jarred Jordan, Daniel Posada, David Zuehlke, Angelica Radulovic, Aryslan Malik, Troy Henderson
- **Comment**: AAS 22-775 2022 AAS/AIAA Astrodynamics Specialist Conference,
  Charlotte, North Carolina, August 7-11 2022
- **Journal**: None
- **Summary**: This work utilizes a MobileNetV2 Convolutional Neural Network (CNN) for fast, mobile detection of satellites, and rejection of stars, in cluttered unresolved space imagery. First, a custom database is created using imagery from a synthetic satellite image program and labeled with bounding boxes over satellites for "satellite-positive" images. The CNN is then trained on this database and the inference is validated by checking the accuracy of the model on an external dataset constructed of real telescope imagery. In doing so, the trained CNN provides a method of rapid satellite identification for subsequent utilization in ground-based orbit estimation.



### Detection and Initial Assessment of Lunar Landing Sites Using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2207.11413v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.11413v1)
- **Published**: 2022-07-23 04:29:18+00:00
- **Updated**: 2022-07-23 04:29:18+00:00
- **Authors**: Daniel Posada, Jarred Jordan, Angelica Radulovic, Lillian Hong, Aryslan Malik, Troy Henderson
- **Comment**: AAS 22-811 2022 AAS/AIAA Astrodynamics Specialist Conference,
  Charlotte, North Carolina, August 7-11 2022
- **Journal**: None
- **Summary**: Robotic and human lunar landings are a focus of future NASA missions. Precision landing capabilities are vital to guarantee the success of the mission, and the safety of the lander and crew. During the approach to the surface there are multiple challenges associated with Hazard Relative Navigation to ensure safe landings. This paper will focus on a passive autonomous hazard detection and avoidance sub-system to generate an initial assessment of possible landing regions for the guidance system. The system uses a single camera and the MobileNetV2 neural network architecture to detect and discern between safe landing sites and hazards such as rocks, shadows, and craters. Then a monocular structure from motion will recreate the surface to provide slope and roughness analysis.



### Arbitrary Style Transfer with Structure Enhancement by Combining the Global and Local Loss
- **Arxiv ID**: http://arxiv.org/abs/2207.11438v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.11438v1)
- **Published**: 2022-07-23 07:02:57+00:00
- **Updated**: 2022-07-23 07:02:57+00:00
- **Authors**: Lizhen Long, Chi-Man Pun
- **Comment**: None
- **Journal**: None
- **Summary**: Arbitrary style transfer generates an artistic image which combines the structure of a content image and the artistic style of the artwork by using only one trained network. The image representation used in this method contains content structure representation and the style patterns representation, which is usually the features representation of high-level in the pre-trained classification networks. However, the traditional classification networks were designed for classification which usually focus on high-level features and ignore other features. As the result, the stylized images distribute style elements evenly throughout the image and make the overall image structure unrecognizable. To solve this problem, we introduce a novel arbitrary style transfer method with structure enhancement by combining the global and local loss. The local structure details are represented by Lapstyle and the global structure is controlled by the image depth. Experimental results demonstrate that our method can generate higher-quality images with impressive visual effects on several common datasets, comparing with other state-of-the-art methods.



### Meta Spatio-Temporal Debiasing for Video Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/2207.11441v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11441v2)
- **Published**: 2022-07-23 07:06:06+00:00
- **Updated**: 2022-07-30 12:15:28+00:00
- **Authors**: Li Xu, Haoxuan Qu, Jason Kuen, Jiuxiang Gu, Jun Liu
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Video scene graph generation (VidSGG) aims to parse the video content into scene graphs, which involves modeling the spatio-temporal contextual information in the video. However, due to the long-tailed training data in datasets, the generalization performance of existing VidSGG models can be affected by the spatio-temporal conditional bias problem. In this work, from the perspective of meta-learning, we propose a novel Meta Video Scene Graph Generation (MVSGG) framework to address such a bias problem. Specifically, to handle various types of spatio-temporal conditional biases, our framework first constructs a support set and a group of query sets from the training data, where the data distribution of each query set is different from that of the support set w.r.t. a type of conditional bias. Then, by performing a novel meta training and testing process to optimize the model to obtain good testing performance on these query sets after training on the support set, our framework can effectively guide the model to learn to well generalize against biases. Extensive experiments demonstrate the efficacy of our proposed framework.



### BuyTheDips: PathLoss for improved topology-preserving deep learning-based image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.11446v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11446v2)
- **Published**: 2022-07-23 07:19:30+00:00
- **Updated**: 2022-08-18 06:57:54+00:00
- **Authors**: Minh On Vu Ngoc, Yizi Chen, Nicolas Boutry, Jonathan Fabrizio, Clement Mallet
- **Comment**: There are some details in the paper that we need to revise again.
  This preprint can be confused for some readers. We would like to withdraw
  this preprint and we will update it as soon as possible
- **Journal**: None
- **Summary**: Capturing the global topology of an image is essential for proposing an accurate segmentation of its domain. However, most of existing segmentation methods do not preserve the initial topology of the given input, which is detrimental for numerous downstream object-based tasks. This is all the more true for deep learning models which most work at local scales. In this paper, we propose a new topology-preserving deep image segmentation method which relies on a new leakage loss: the Pathloss. Our method is an extension of the BALoss [1], in which we want to improve the leakage detection for better recovering the closeness property of the image segmentation. This loss allows us to correctly localize and fix the critical points (a leakage in the boundaries) that could occur in the predictions, and is based on a shortest-path search algorithm. This way, loss minimization enforces connectivity only where it is necessary and finally provides a good localization of the boundaries of the objects in the image. Moreover, according to our research, our Pathloss learns to preserve stronger elongated structure compared to methods without using topology-preserving loss. Training with our topological loss function, our method outperforms state-of-the-art topology-aware methods on two representative datasets of different natures: Electron Microscopy and Historical Map.



### UC-OWOD: Unknown-Classified Open World Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.11455v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11455v1)
- **Published**: 2022-07-23 08:15:30+00:00
- **Updated**: 2022-07-23 08:15:30+00:00
- **Authors**: Zhiheng Wu, Yue Lu, Xingyu Chen, Zhengxing Wu, Liwen Kang, Junzhi Yu
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Open World Object Detection (OWOD) is a challenging computer vision problem that requires detecting unknown objects and gradually learning the identified unknown classes. However, it cannot distinguish unknown instances as multiple unknown classes. In this work, we propose a novel OWOD problem called Unknown-Classified Open World Object Detection (UC-OWOD). UC-OWOD aims to detect unknown instances and classify them into different unknown classes. Besides, we formulate the problem and devise a two-stage object detector to solve UC-OWOD. First, unknown label-aware proposal and unknown-discriminative classification head are used to detect known and unknown objects. Then, similarity-based unknown classification and unknown clustering refinement modules are constructed to distinguish multiple unknown classes. Moreover, two novel evaluation protocols are designed to evaluate unknown-class detection. Abundant experiments and visualizations prove the effectiveness of the proposed method. Code is available at https://github.com/JohnWuzh/UC-OWOD.



### When Counting Meets HMER: Counting-Aware Network for Handwritten Mathematical Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.11463v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.11463v1)
- **Published**: 2022-07-23 08:39:32+00:00
- **Updated**: 2022-07-23 08:39:32+00:00
- **Authors**: Bohan Li, Ye Yuan, Dingkang Liang, Xiao Liu, Zhilong Ji, Jinfeng Bai, Wenyu Liu, Xiang Bai
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Recently, most handwritten mathematical expression recognition (HMER) methods adopt the encoder-decoder networks, which directly predict the markup sequences from formula images with the attention mechanism. However, such methods may fail to accurately read formulas with complicated structure or generate long markup sequences, as the attention results are often inaccurate due to the large variance of writing styles or spatial layouts. To alleviate this problem, we propose an unconventional network for HMER named Counting-Aware Network (CAN), which jointly optimizes two tasks: HMER and symbol counting. Specifically, we design a weakly-supervised counting module that can predict the number of each symbol class without the symbol-level position annotations, and then plug it into a typical attention-based encoder-decoder model for HMER. Experiments on the benchmark datasets for HMER validate that both joint optimization and counting results are beneficial for correcting the prediction errors of encoder-decoder models, and CAN consistently outperforms the state-of-the-art methods. In particular, compared with an encoder-decoder model for HMER, the extra time cost caused by the proposed counting module is marginal. The source code is available at https://github.com/LBH1024/CAN.



### Learning Object Placement via Dual-path Graph Completion
- **Arxiv ID**: http://arxiv.org/abs/2207.11464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11464v1)
- **Published**: 2022-07-23 08:39:39+00:00
- **Updated**: 2022-07-23 08:39:39+00:00
- **Authors**: Siyuan Zhou, Liu Liu, Li Niu, Liqing Zhang
- **Comment**: 25 pages, 9 figures
- **Journal**: None
- **Summary**: Object placement aims to place a foreground object over a background image with a suitable location and size. In this work, we treat object placement as a graph completion problem and propose a novel graph completion module (GCM). The background scene is represented by a graph with multiple nodes at different spatial locations with various receptive fields. The foreground object is encoded as a special node that should be inserted at a reasonable place in this graph. We also design a dual-path framework upon the structure of GCM to fully exploit annotated composite images. With extensive experiments on OPA dataset, our method proves to significantly outperform existing methods in generating plausible object placement without loss of diversity.



### CompNVS: Novel View Synthesis with Scene Completion
- **Arxiv ID**: http://arxiv.org/abs/2207.11467v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.11467v1)
- **Published**: 2022-07-23 09:03:13+00:00
- **Updated**: 2022-07-23 09:03:13+00:00
- **Authors**: Zuoyue Li, Tianxing Fan, Zhenqiang Li, Zhaopeng Cui, Yoichi Sato, Marc Pollefeys, Martin R. Oswald
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: We introduce a scalable framework for novel view synthesis from RGB-D images with largely incomplete scene coverage. While generative neural approaches have demonstrated spectacular results on 2D images, they have not yet achieved similar photorealistic results in combination with scene completion where a spatial 3D scene understanding is essential. To this end, we propose a generative pipeline performing on a sparse grid-based neural scene representation to complete unobserved scene parts via a learned distribution of scenes in a 2.5D-3D-2.5D manner. We process encoded image features in 3D space with a geometry completion network and a subsequent texture inpainting network to extrapolate the missing area. Photorealistic image sequences can be finally obtained via consistency-relevant differentiable rendering. Comprehensive experiments show that the graphical outputs of our method outperform the state of the art, especially within unobserved scene parts.



### Progressive Scene Text Erasing with Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/2207.11469v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11469v2)
- **Published**: 2022-07-23 09:05:13+00:00
- **Updated**: 2023-04-28 09:36:53+00:00
- **Authors**: Xiangcheng Du, Zhao Zhou, Yingbin Zheng, Xingjiao Wu, Tianlong Ma, Cheng Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Scene text erasing seeks to erase text contents from scene images and current state-of-the-art text erasing models are trained on large-scale synthetic data. Although data synthetic engines can provide vast amounts of annotated training samples, there are differences between synthetic and real-world data. In this paper, we employ self-supervision for feature representation on unlabeled real-world scene text images. A novel pretext task is designed to keep consistent among text stroke masks of image variants. We design the Progressive Erasing Network in order to remove residual texts. The scene text is erased progressively by leveraging the intermediate generated results which provide the foundation for subsequent higher quality results. Experiments show that our method significantly improves the generalization of the text erasing task and achieves state-of-the-art performance on public benchmarks.



### 3D Labeling Tool
- **Arxiv ID**: http://arxiv.org/abs/2207.11479v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2207.11479v1)
- **Published**: 2022-07-23 09:53:41+00:00
- **Updated**: 2022-07-23 09:53:41+00:00
- **Authors**: John Rachwan, Charbel Zalaket
- **Comment**: None
- **Journal**: None
- **Summary**: Training and testing supervised object detection models require a large collection of images with ground truth labels. Labels define object classes in the image, as well as their locations, shape, and possibly other information such as pose. The labeling process has proven extremely time consuming, even with the presence of manpower. We introduce a novel labeling tool for 2D images as well as 3D triangular meshes: 3D Labeling Tool (3DLT). This is a standalone, feature-heavy and cross-platform software that does not require installation and can run on Windows, macOS and Linux-based distributions. Instead of labeling the same object on every image separately like current tools, we use depth information to reconstruct a triangular mesh from said images and label the object only once on the aforementioned mesh. We use registration to simplify 3D labeling, outlier detection to improve 2D bounding box calculation and surface reconstruction to expand labeling possibility to large point clouds. Our tool is tested against state of the art methods and it greatly surpasses them in terms of speed while preserving accuracy and ease of use.



### Multimodal Emotion Recognition with Modality-Pairwise Unsupervised Contrastive Loss
- **Arxiv ID**: http://arxiv.org/abs/2207.11482v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.11482v1)
- **Published**: 2022-07-23 10:11:24+00:00
- **Updated**: 2022-07-23 10:11:24+00:00
- **Authors**: Riccardo Franceschini, Enrico Fini, Cigdem Beyan, Alessandro Conti, Federica Arrigoni, Elisa Ricci
- **Comment**: Accepted to 26th International Conference on Pattern Recognition
  (ICPR) 2022
- **Journal**: None
- **Summary**: Emotion recognition is involved in several real-world applications. With an increase in available modalities, automatic understanding of emotions is being performed more accurately. The success in Multimodal Emotion Recognition (MER), primarily relies on the supervised learning paradigm. However, data annotation is expensive, time-consuming, and as emotion expression and perception depends on several factors (e.g., age, gender, culture) obtaining labels with a high reliability is hard. Motivated by these, we focus on unsupervised feature learning for MER. We consider discrete emotions, and as modalities text, audio and vision are used. Our method, as being based on contrastive loss between pairwise modalities, is the first attempt in MER literature. Our end-to-end feature learning approach has several differences (and advantages) compared to existing MER methods: i) it is unsupervised, so the learning is lack of data labelling cost; ii) it does not require data spatial augmentation, modality alignment, large number of batch size or epochs; iii) it applies data fusion only at inference; and iv) it does not require backbones pre-trained on emotion recognition task. The experiments on benchmark datasets show that our method outperforms several baseline approaches and unsupervised learning methods applied in MER. Particularly, it even surpasses a few supervised MER state-of-the-art.



### GraphFit: Learning Multi-scale Graph-Convolutional Representation for Point Cloud Normal Estimation
- **Arxiv ID**: http://arxiv.org/abs/2207.11484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11484v1)
- **Published**: 2022-07-23 10:29:26+00:00
- **Updated**: 2022-07-23 10:29:26+00:00
- **Authors**: Keqiang Li, Mingyang Zhao, Huaiyu Wu, Dong-Ming Yan, Zhen Shen, Fei-Yue Wang, Gang Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a precise and efficient normal estimation method that can deal with noise and nonuniform density for unstructured 3D point clouds. Unlike existing approaches that directly take patches and ignore the local neighborhood relationships, which make them susceptible to challenging regions such as sharp edges, we propose to learn graph convolutional feature representation for normal estimation, which emphasizes more local neighborhood geometry and effectively encodes intrinsic relationships. Additionally, we design a novel adaptive module based on the attention mechanism to integrate point features with their neighboring features, hence further enhancing the robustness of the proposed normal estimator against point density variations. To make it more distinguishable, we introduce a multi-scale architecture in the graph block to learn richer geometric features. Our method outperforms competitors with the state-of-the-art accuracy on various benchmark datasets, and is quite robust against noise, outliers, as well as the density variations.



### Implementation Of Tiny Machine Learning Models On Arduino 33 BLE For Gesture And Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.12866v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/2207.12866v1)
- **Published**: 2022-07-23 10:53:26+00:00
- **Updated**: 2022-07-23 10:53:26+00:00
- **Authors**: Viswanatha V, Ramachandra A. C, Raghavendra Prasanna, Prem Chowdary Kakarla, Viveka Simha PJ, Nishant Mohan
- **Comment**: None
- **Journal**: None
- **Summary**: In this article gesture recognition and speech recognition applications are implemented on embedded systems with Tiny Machine Learning (TinyML). It features 3-axis accelerometer, 3-axis gyroscope and 3-axis magnetometer. The gesture recognition,provides an innovative approach nonverbal communication. It has wide applications in human-computer interaction and sign language. Here in the implementation of hand gesture recognition, TinyML model is trained and deployed from EdgeImpulse framework for hand gesture recognition and based on the hand movements, Arduino Nano 33 BLE device having 6-axis IMU can find out the direction of movement of hand. The Speech is a mode of communication. Speech recognition is a way by which the statements or commands of human speech is understood by the computer which reacts accordingly. The main aim of speech recognition is to achieve communication between man and machine. Here in the implementation of speech recognition, TinyML model is trained and deployed from EdgeImpulse framework for speech recognition and based on the keywords pronounced by human, Arduino Nano 33 BLE device having built-in microphone can make an RGB LED glow like red, green or blue based on keyword pronounced. The results of each application are obtained and listed in the results section and given the analysis upon the results.



### Real Time Object Detection System with YOLO and CNN Models: A Review
- **Arxiv ID**: http://arxiv.org/abs/2208.00773v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.00773v1)
- **Published**: 2022-07-23 11:00:11+00:00
- **Updated**: 2022-07-23 11:00:11+00:00
- **Authors**: Viswanatha V, Chandana R K, Ramachandra A. C.
- **Comment**: None
- **Journal**: None
- **Summary**: The field of artificial intelligence is built on object detection techniques. YOU ONLY LOOK ONCE (YOLO) algorithm and it's more evolved versions are briefly described in this research survey. This survey is all about YOLO and convolution neural networks (CNN)in the direction of real time object detection.YOLO does generalized object representation more effectively without precision losses than other object detection models.CNN architecture models have the ability to eliminate highlights and identify objects in any given image. When implemented appropriately, CNN models can address issues like deformity diagnosis, creating educational or instructive application, etc. This article reached atnumber of observations and perspective findings through the analysis.Also it provides support for the focused visual information and feature extraction in the financial and other industries, highlights the method of target detection and feature selection, and briefly describe the development process of YOLO algorithm.



### Active Pointly-Supervised Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.11493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11493v1)
- **Published**: 2022-07-23 11:25:24+00:00
- **Updated**: 2022-07-23 11:25:24+00:00
- **Authors**: Chufeng Tang, Lingxi Xie, Gang Zhang, Xiaopeng Zhang, Qi Tian, Xiaolin Hu
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: The requirement of expensive annotations is a major burden for training a well-performed instance segmentation model. In this paper, we present an economic active learning setting, named active pointly-supervised instance segmentation (APIS), which starts with box-level annotations and iteratively samples a point within the box and asks if it falls on the object. The key of APIS is to find the most desirable points to maximize the segmentation accuracy with limited annotation budgets. We formulate this setting and propose several uncertainty-based sampling strategies. The model developed with these strategies yields consistent performance gain on the challenging MS-COCO dataset, compared against other learning strategies. The results suggest that APIS, integrating the advantages of active learning and point-based supervision, is an effective learning paradigm for label-efficient instance segmentation.



### Intelligent 3D Network Protocol for Multimedia Data Classification using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.11504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.11; H.4; C.2.2
- **Links**: [PDF](http://arxiv.org/pdf/2207.11504v1)
- **Published**: 2022-07-23 12:24:52+00:00
- **Updated**: 2022-07-23 12:24:52+00:00
- **Authors**: Arslan Syed, Eman A. Aldhahri, Muhammad Munawar Iqbal, Abid Ali, Ammar Muthanna, Harun Jamil, Faisal Jamil
- **Comment**: 21 pages, 10 figures
- **Journal**: None
- **Summary**: In videos, the human's actions are of three-dimensional (3D) signals. These videos investigate the spatiotemporal knowledge of human behavior. The promising ability is investigated using 3D convolution neural networks (CNNs). The 3D CNNs have not yet achieved high output for their well-established two-dimensional (2D) equivalents in still photographs. Board 3D Convolutional Memory and Spatiotemporal fusion face training difficulty preventing 3D CNN from accomplishing remarkable evaluation. In this paper, we implement Hybrid Deep Learning Architecture that combines STIP and 3D CNN features to enhance the performance of 3D videos effectively. After implementation, the more detailed and deeper charting for training in each circle of space-time fusion. The training model further enhances the results after handling complicated evaluations of models. The video classification model is used in this implemented model. Intelligent 3D Network Protocol for Multimedia Data Classification using Deep Learning is introduced to further understand spacetime association in human endeavors. In the implementation of the result, the well-known dataset, i.e., UCF101 to, evaluates the performance of the proposed hybrid technique. The results beat the proposed hybrid technique that substantially beats the initial 3D CNNs. The results are compared with state-of-the-art frameworks from literature for action recognition on UCF101 with an accuracy of 95%.



### SSBNet: Improving Visual Recognition Efficiency by Adaptive Sampling
- **Arxiv ID**: http://arxiv.org/abs/2207.11511v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.11511v1)
- **Published**: 2022-07-23 13:01:55+00:00
- **Updated**: 2022-07-23 13:01:55+00:00
- **Authors**: Ho Man Kwan, Shenghui Song
- **Comment**: None
- **Journal**: None
- **Summary**: Downsampling is widely adopted to achieve a good trade-off between accuracy and latency for visual recognition. Unfortunately, the commonly used pooling layers are not learned, and thus cannot preserve important information. As another dimension reduction method, adaptive sampling weights and processes regions that are relevant to the task, and is thus able to better preserve useful information. However, the use of adaptive sampling has been limited to certain layers. In this paper, we show that using adaptive sampling in the building blocks of a deep neural network can improve its efficiency. In particular, we propose SSBNet which is built by inserting sampling layers repeatedly into existing networks like ResNet. Experiment results show that the proposed SSBNet can achieve competitive image classification and object detection performance on ImageNet and COCO datasets. For example, the SSB-ResNet-RS-200 achieved 82.6% accuracy on ImageNet dataset, which is 0.6% higher than the baseline ResNet-RS-152 with a similar complexity. Visualization shows the advantage of SSBNet in allowing different layers to focus on different positions, and ablation studies further validate the advantage of adaptive sampling over uniform methods.



### Combining Self-Training and Hybrid Architecture for Semi-supervised Abdominal Organ Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.11512v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11512v4)
- **Published**: 2022-07-23 13:02:43+00:00
- **Updated**: 2022-10-18 08:33:22+00:00
- **Authors**: Wentao Liu, Weijin Xu, Songlin Yan, Lemeng Wang, Haoyuan Li, Huihua Yang
- **Comment**: arXiv admin note: text overlap with arXiv:2203.04568
- **Journal**: None
- **Summary**: Abdominal organ segmentation has many important clinical applications, such as organ quantification, surgical planning, and disease diagnosis. However, manually annotating organs from CT scans is time-consuming and labor-intensive. Semi-supervised learning has shown the potential to alleviate this challenge by learning from a large set of unlabeled images and limited labeled samples. In this work, we follow the self-training strategy and employ a high-performance hybrid architecture (PHTrans) consisting of CNN and Swin Transformer for the teacher model to generate precise pseudo labels for unlabeled data. Afterward, we introduce them with labeled data together into a two-stage segmentation framework with lightweight PHTrans for training to improve the performance and generalization ability of the model while remaining efficient. Experiments on the validation set of FLARE2022 demonstrate that our method achieves excellent segmentation performance as well as fast and low-resource model inference. The average DSC and NSD are 0.8956 and 0.9316, respectively. Under our development environments, the average inference time is 18.62 s, the average maximum GPU memory is 1995.04 MB, and the area under the GPU memory-time curve and the average area under the CPU utilization-time curve are 23196.84 and 319.67. The code is available at https://github.com/lseventeen/FLARE22-TwoStagePHTrans.



### Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2207.11514v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2207.11514v2)
- **Published**: 2022-07-23 13:10:25+00:00
- **Updated**: 2022-12-06 11:09:39+00:00
- **Authors**: Huy Ha, Shuran Song
- **Comment**: 16 pages, 9 figures, project website at
  https://semantic-abstraction.cs.columbia.edu/
- **Journal**: None
- **Summary**: We study open-world 3D scene understanding, a family of tasks that require agents to reason about their 3D environment with an open-set vocabulary and out-of-domain visual inputs - a critical skill for robots to operate in the unstructured 3D world. Towards this end, we propose Semantic Abstraction (SemAbs), a framework that equips 2D Vision-Language Models (VLMs) with new 3D spatial capabilities, while maintaining their zero-shot robustness. We achieve this abstraction using relevancy maps extracted from CLIP, and learn 3D spatial and geometric reasoning skills on top of those abstractions in a semantic-agnostic manner. We demonstrate the usefulness of SemAbs on two open-world 3D scene understanding tasks: 1) completing partially observed objects and 2) localizing hidden objects from language descriptions. Experiments show that SemAbs can generalize to novel vocabulary, materials/lighting, classes, and domains (i.e., real-world scans) from training on limited 3D synthetic data. Code and data is available at https://semantic-abstraction.cs.columbia.edu/



### Marior: Margin Removal and Iterative Content Rectification for Document Dewarping in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2207.11515v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11515v1)
- **Published**: 2022-07-23 13:14:27+00:00
- **Updated**: 2022-07-23 13:14:27+00:00
- **Authors**: Jiaxin Zhang, Canjie Luo, Lianwen Jin, Fengjun Guo, Kai Ding
- **Comment**: This paper has been accepted by ACM Multimedia 2022
- **Journal**: None
- **Summary**: Camera-captured document images usually suffer from perspective and geometric deformations. It is of great value to rectify them when considering poor visual aesthetics and the deteriorated performance of OCR systems. Recent learning-based methods intensively focus on the accurately cropped document image. However, this might not be sufficient for overcoming practical challenges, including document images either with large marginal regions or without margins. Due to this impracticality, users struggle to crop documents precisely when they encounter large marginal regions. Simultaneously, dewarping images without margins is still an insurmountable problem. To the best of our knowledge, there is still no complete and effective pipeline for rectifying document images in the wild. To address this issue, we propose a novel approach called Marior (Margin Removal and \Iterative Content Rectification). Marior follows a progressive strategy to iteratively improve the dewarping quality and readability in a coarse-to-fine manner. Specifically, we divide the pipeline into two modules: margin removal module (MRM) and iterative content rectification module (ICRM). First, we predict the segmentation mask of the input image to remove the margin, thereby obtaining a preliminary result. Then we refine the image further by producing dense displacement flows to achieve content-aware rectification. We determine the number of refinement iterations adaptively. Experiments demonstrate the state-of-the-art performance of our method on public benchmarks. The resources are available at https://github.com/ZZZHANG-jx/Marior for further comparison.



### Contrastive Monotonic Pixel-Level Modulation
- **Arxiv ID**: http://arxiv.org/abs/2207.11517v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11517v1)
- **Published**: 2022-07-23 13:21:24+00:00
- **Updated**: 2022-07-23 13:21:24+00:00
- **Authors**: Kun Lu, Rongpeng Li, Honggang Zhang
- **Comment**: ECCV'2022 Oral presentation, including both main paper and supp
- **Journal**: None
- **Summary**: Continuous one-to-many mapping is a less investigated yet important task in both low-level visions and neural image translation. In this paper, we present a new formulation called MonoPix, an unsupervised and contrastive continuous modulation model, and take a step further to enable a pixel-level spatial control which is critical but can not be properly handled previously. The key feature of this work is to model the monotonicity between controlling signals and the domain discriminator with a novel contrastive modulation framework and corresponding monotonicity constraints. We have also introduced a selective inference strategy with logarithmic approximation complexity and support fast domain adaptations. The state-of-the-art performance is validated on a variety of continuous mapping tasks, including AFHQ cat-dog and Yosemite summer-winter translation. The introduced approach also helps to provide a new solution for many low-level tasks like low-light enhancement and natural noise generation, which is beyond the long-established practice of one-to-one training and inference. Code is available at https://github.com/lukun199/MonoPix.



### Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.11518v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.11518v2)
- **Published**: 2022-07-23 13:39:01+00:00
- **Updated**: 2023-03-27 14:12:20+00:00
- **Authors**: Chuanguang Yang, Zhulin An, Helong Zhou, Fuzhen Zhuang, Yongjun Xu, Qian Zhan
- **Comment**: 18 pages, accepted by IEEE Transactions on Pattern Analysis and
  Machine Intelligence (TPAMI-2023)
- **Journal**: None
- **Summary**: The teacher-free online Knowledge Distillation (KD) aims to train an ensemble of multiple student models collaboratively and distill knowledge from each other. Although existing online KD methods achieve desirable performance, they often focus on class probabilities as the core knowledge type, ignoring the valuable feature representational information. We present a Mutual Contrastive Learning (MCL) framework for online KD. The core idea of MCL is to perform mutual interaction and transfer of contrastive distributions among a cohort of networks in an online manner. Our MCL can aggregate cross-network embedding information and maximize the lower bound to the mutual information between two networks. This enables each network to learn extra contrastive knowledge from others, leading to better feature representations, thus improving the performance of visual recognition tasks. Beyond the final layer, we extend MCL to intermediate layers and perform an adaptive layer-matching mechanism trained by meta-optimization. Experiments on image classification and transfer learning to visual recognition tasks show that layer-wise MCL can lead to consistent performance gains against state-of-the-art online KD approaches. The superiority demonstrates that layer-wise MCL can guide the network to generate better feature representations. Our code is publicly avaliable at https://github.com/winycg/L-MCL.



### Unstructured Road Segmentation using Hypercolumn based Random Forests of Local experts
- **Arxiv ID**: http://arxiv.org/abs/2207.11523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11523v1)
- **Published**: 2022-07-23 13:56:37+00:00
- **Updated**: 2022-07-23 13:56:37+00:00
- **Authors**: Prassanna Ganesh Ravishankar, Antonio M. Lopez, Gemma M. Sanchez
- **Comment**: for associated dataset, see
  https://prassanna-ravishankar.github.io/LandscapeDataset/
- **Journal**: None
- **Summary**: Monocular vision based road detection methods are mostly based on machine learning methods, relying on classification and feature extraction accuracy, and suffer from appearance, illumination and weather changes. Traditional methods introduce the predictions into conditional random fields or markov random fields models to improve the intermediate predictions based on structure. These methods are optimization based and therefore resource heavy and slow, making it unsuitable for real time applications. We propose a method to detect and segment roads with a random forest classifier of local experts with superpixel based machine-learned features. The random forest takes in machine learnt descriptors from a pre-trained convolutional neural network - VGG-16. The features are also pooled into their respective superpixels, allowing for local structure to be continuous. We compare our algorithm against Nueral Network based methods and Traditional approaches (based on Hand-crafted features), on both Structured Road (CamVid and Kitti) and Unstructured Road Datasets. Finally, we introduce a Road Scene Dataset with 1000 annotated images, and verify that our algorithm works well in non-urban and rural road scenarios.



### Audio-driven Neural Gesture Reenactment with Video Motion Graphs
- **Arxiv ID**: http://arxiv.org/abs/2207.11524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11524v1)
- **Published**: 2022-07-23 14:02:57+00:00
- **Updated**: 2022-07-23 14:02:57+00:00
- **Authors**: Yang Zhou, Jimei Yang, Dingzeyu Li, Jun Saito, Deepali Aneja, Evangelos Kalogerakis
- **Comment**: 15 pages, 10 figures. Accepted by CVPR 2022
- **Journal**: None
- **Summary**: Human speech is often accompanied by body gestures including arm and hand gestures. We present a method that reenacts a high-quality video with gestures matching a target speech audio. The key idea of our method is to split and re-assemble clips from a reference video through a novel video motion graph encoding valid transitions between clips. To seamlessly connect different clips in the reenactment, we propose a pose-aware video blending network which synthesizes video frames around the stitched frames between two clips. Moreover, we developed an audio-based gesture searching algorithm to find the optimal order of the reenacted frames. Our system generates reenactments that are consistent with both the audio rhythms and the speech content. We evaluate our synthesized video quality quantitatively, qualitatively, and with user studies, demonstrating that our method produces videos of much higher quality and consistency with the target audio compared to previous work and baselines.



### Comparative Validation of AI and non-AI Methods in MRI Volumetry to Diagnose Parkinsonian Syndromes
- **Arxiv ID**: http://arxiv.org/abs/2207.11534v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.11534v1)
- **Published**: 2022-07-23 14:55:38+00:00
- **Updated**: 2022-07-23 14:55:38+00:00
- **Authors**: Joomee Song, Juyoung Hahm, Jisoo Lee, Chae Yeon Lim, Myung Jin Chung, Jinyoung Youn, Jin Whan Cho, Jong Hyeon Ahn, Kyung-Su Kim
- **Comment**: Joomee Song and Juyoung Hahm contributed equally to this work as the
  co-first author. Jong Hyeon Ahn and Kyung-Su Kim (kskim.doc@gmail.com)
  contributed equally to this work as the co-corresponding author
- **Journal**: None
- **Summary**: Automated segmentation and volumetry of brain magnetic resonance imaging (MRI) scans are essential for the diagnosis of Parkinson's disease (PD) and Parkinson's plus syndromes (P-plus). To enhance the diagnostic performance, we adopt deep learning (DL) models in brain segmentation and compared their performance with the gold-standard non-DL method. We collected brain MRI scans of healthy controls (n=105) and patients with PD (n=105), multiple systemic atrophy (n=132), and progressive supranuclear palsy (n=69) at Samsung Medical Center from January 2017 to December 2020. Using the gold-standard non-DL model, FreeSurfer (FS), we segmented six brain structures: midbrain, pons, caudate, putamen, pallidum, and third ventricle, and considered them as annotating data for DL models, the representative V-Net and UNETR. The Dice scores and area under the curve (AUC) for differentiating normal, PD, and P-plus cases were calculated. The segmentation times of V-Net and UNETR for the six brain structures per patient were 3.48 +- 0.17 and 48.14 +- 0.97 s, respectively, being at least 300 times faster than FS (15,735 +- 1.07 s). Dice scores of both DL models were sufficiently high (>0.85), and their AUCs for disease classification were superior to that of FS. For classification of normal vs. P-plus and PD vs. multiple systemic atrophy (cerebellar type), the DL models and FS showed AUCs above 0.8. DL significantly reduces the analysis time without compromising the performance of brain segmentation and differential diagnosis. Our findings may contribute to the adoption of DL brain MRI segmentation in clinical settings and advance brain research.



### HPS-Det: Dynamic Sample Assignment with Hyper-Parameter Search for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.11539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11539v1)
- **Published**: 2022-07-23 15:13:57+00:00
- **Updated**: 2022-07-23 15:13:57+00:00
- **Authors**: Ji Liu, Dong Li, Zekun Li, Han Liu, Wenjing Ke, Lu Tian, Yi Shan
- **Comment**: None
- **Journal**: None
- **Summary**: Sample assignment plays a prominent part in modern object detection approaches. However, most existing methods rely on manual design to assign positive / negative samples, which do not explicitly establish the relationships between sample assignment and object detection performance. In this work, we propose a novel dynamic sample assignment scheme based on hyper-parameter search. We first define the number of positive samples assigned to each ground truth as the hyper-parameters and employ a surrogate optimization algorithm to derive the optimal choices. Then, we design a dynamic sample assignment procedure to dynamically select the optimal number of positives at each training iteration. Experiments demonstrate that the resulting HPS-Det brings improved performance over different object detection baselines. Moreover, We analyze the hyper-parameter reusability when transferring between different datasets and between different backbones for object detection, which exhibits the superiority and versatility of our method.



### Interaction Mix and Match: Synthesizing Close Interaction using Conditional Hierarchical GAN with Multi-Hot Class Embedding
- **Arxiv ID**: http://arxiv.org/abs/2208.00774v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.00774v2)
- **Published**: 2022-07-23 16:13:10+00:00
- **Updated**: 2022-08-04 12:54:29+00:00
- **Authors**: Aman Goel, Qianhui Men, Edmond S. L. Ho
- **Comment**: Accepted to SCA 2022 (will be published in CGF)
- **Journal**: None
- **Summary**: Synthesizing multi-character interactions is a challenging task due to the complex and varied interactions between the characters. In particular, precise spatiotemporal alignment between characters is required in generating close interactions such as dancing and fighting. Existing work in generating multi-character interactions focuses on generating a single type of reactive motion for a given sequence which results in a lack of variety of the resultant motions. In this paper, we propose a novel way to create realistic human reactive motions which are not presented in the given dataset by mixing and matching different types of close interactions. We propose a Conditional Hierarchical Generative Adversarial Network with Multi-Hot Class Embedding to generate the Mix and Match reactive motions of the follower from a given motion sequence of the leader. Experiments are conducted on both noisy (depth-based) and high-quality (MoCap-based) interaction datasets. The quantitative and qualitative results show that our approach outperforms the state-of-the-art methods on the given datasets. We also provide an augmented dataset with realistic reactive motions to stimulate future research in this area. The code is available at https://github.com/Aman-Goel1/IMM



### Self-Support Few-Shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.11549v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11549v1)
- **Published**: 2022-07-23 16:28:07+00:00
- **Updated**: 2022-07-23 16:28:07+00:00
- **Authors**: Qi Fan, Wenjie Pei, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Existing few-shot segmentation methods have achieved great progress based on the support-query matching framework. But they still heavily suffer from the limited coverage of intra-class variations from the few-shot supports provided. Motivated by the simple Gestalt principle that pixels belonging to the same object are more similar than those to different objects of same class, we propose a novel self-support matching strategy to alleviate this problem, which uses query prototypes to match query features, where the query prototypes are collected from high-confidence query predictions. This strategy can effectively capture the consistent underlying characteristics of the query objects, and thus fittingly match query features. We also propose an adaptive self-support background prototype generation module and self-support loss to further facilitate the self-support matching procedure. Our self-support network substantially improves the prototype quality, benefits more improvement from stronger backbones and more supports, and achieves SOTA on multiple datasets. Codes are at \url{https://github.com/fanq15/SSP}.



### High-Resolution Swin Transformer for Automatic Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.11553v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.11553v1)
- **Published**: 2022-07-23 16:55:37+00:00
- **Updated**: 2022-07-23 16:55:37+00:00
- **Authors**: Chen Wei, Shenghan Ren, Kaitai Guo, Haihong Hu, Jimin Liang
- **Comment**: 14 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: The Resolution of feature maps is critical for medical image segmentation. Most of the existing Transformer-based networks for medical image segmentation are U-Net-like architecture that contains an encoder that utilizes a sequence of Transformer blocks to convert the input medical image from high-resolution representation into low-resolution feature maps and a decoder that gradually recovers the high-resolution representation from low-resolution feature maps. Unlike previous studies, in this paper, we utilize the network design style from the High-Resolution Network (HRNet), replace the convolutional layers with Transformer blocks, and continuously exchange information from the different resolution feature maps that are generated by Transformer blocks. The newly Transformer-based network presented in this paper is denoted as High-Resolution Swin Transformer Network (HRSTNet). Extensive experiments illustrate that HRSTNet can achieve comparable performance with the state-of-the-art Transformer-based U-Net-like architecture on Brain Tumor Segmentation(BraTS) 2021 and the liver dataset from Medical Segmentation Decathlon. The code of HRSTNet will be publicly available at https://github.com/auroua/HRSTNet.



### 3DOS: Towards 3D Open Set Learning -- Benchmarking and Understanding Semantic Novelty Detection on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2207.11554v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11554v3)
- **Published**: 2022-07-23 17:00:45+00:00
- **Updated**: 2023-01-17 13:33:13+00:00
- **Authors**: Antonio Alliegro, Francesco Cappio Borlino, Tatiana Tommasi
- **Comment**: Accepted by NeurIPS 2022 Datasets and Benchmarks Track. Code:
  https://github.com/antoalli/3D_OS
- **Journal**: None
- **Summary**: In recent years there has been significant progress in the field of 3D learning on classification, detection and segmentation problems. The vast majority of the existing studies focus on canonical closed-set conditions, neglecting the intrinsic open nature of the real-world. This limits the abilities of robots and autonomous systems involved in safety-critical applications that require managing novel and unknown signals. In this context exploiting 3D data can be a valuable asset since it provides rich information about the geometry of perceived objects and scenes. With this paper we provide the first broad study on 3D Open Set learning. We introduce 3DOS: a novel testbed for semantic novelty detection that considers several settings with increasing difficulties in terms of semantic (category) shift, and covers both in-domain (synthetic-to-synthetic, real-to-real) and cross-domain (synthetic-to-real) scenarios. Moreover, we investigate the related 2D Open Set literature to understand if and how its recent improvements are effective on 3D data. Our extensive benchmark positions several algorithms in the same coherent picture, revealing their strengths and limitations. The results of our analysis may serve as a reliable foothold for future tailored 3D Open Set methods.



### Robots Enact Malignant Stereotypes
- **Arxiv ID**: http://arxiv.org/abs/2207.11569v1
- **DOI**: 10.1145/3531146.3533138
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.11569v1)
- **Published**: 2022-07-23 18:08:12+00:00
- **Updated**: 2022-07-23 18:08:12+00:00
- **Authors**: Andrew Hundt, William Agnew, Vicky Zeng, Severin Kacianka, Matthew Gombolay
- **Comment**: 30 pages, 10 figures, 5 tables. Website:
  https://sites.google.com/view/robots-enact-stereotypes . Published in the
  2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT 22),
  June 21-24, 2022, Seoul, Republic of Korea. ACM, DOI:
  https://doi.org/10.1145/3531146.3533138 . FAccT22 Submission dates: Abstract
  Dec 13, 2021; Submitted Jan 22, 2022; Accepted Apr 7, 2022
- **Journal**: In 2022 ACM Conference on Fairness, Accountability, and
  Transparency (FAccT 22). ACM, New York, NY, USA, 743-756
- **Summary**: Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural Language Processing (NLP) [6], or both, in the case of large image and caption models such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called "foundation models", e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.



### Self-Supervised Learning of Echocardiogram Videos Enables Data-Efficient Clinical Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2207.11581v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11581v1)
- **Published**: 2022-07-23 19:17:26+00:00
- **Updated**: 2022-07-23 19:17:26+00:00
- **Authors**: Gregory Holste, Evangelos K. Oikonomou, Bobak Mortazavi, Zhangyang Wang, Rohan Khera
- **Comment**: Accepted to IMLH 2022 (https://sites.google.com/view/imlh2022)
- **Journal**: None
- **Summary**: Given the difficulty of obtaining high-quality labels for medical image recognition tasks, there is a need for deep learning techniques that can be adequately fine-tuned on small labeled data sets. Recent advances in self-supervised learning techniques have shown that such an in-domain representation learning approach can provide a strong initialization for supervised fine-tuning, proving much more data-efficient than standard transfer learning from a supervised pretraining task. However, these applications are not adapted to applications to medical diagnostics captured in a video format. With this progress in mind, we developed a self-supervised learning approach catered to echocardiogram videos with the goal of learning strong representations for downstream fine-tuning on the task of diagnosing aortic stenosis (AS), a common and dangerous disease of the aortic valve. When fine-tuned on 1% of the training data, our best self-supervised learning model achieves 0.818 AUC (95% CI: 0.794, 0.840), while the standard transfer learning approach reaches 0.644 AUC (95% CI: 0.610, 0.677). We also find that our self-supervised model attends more closely to the aortic valve when predicting severe AS as demonstrated by saliency map visualizations.



### Defining an action of SO(d)-rotations on images generated by projections of d-dimensional objects: Applications to pose inference with Geometric VAEs
- **Arxiv ID**: http://arxiv.org/abs/2207.11582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.11582v1)
- **Published**: 2022-07-23 19:22:28+00:00
- **Updated**: 2022-07-23 19:22:28+00:00
- **Authors**: Nicolas Legendre, Khanh Dao Duc, Nina Miolane
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in variational autoencoders (VAEs) have enabled learning latent manifolds as compact Lie groups, such as $SO(d)$. Since this approach assumes that data lies on a subspace that is homeomorphic to the Lie group itself, we here investigate how this assumption holds in the context of images that are generated by projecting a $d$-dimensional volume with unknown pose in $SO(d)$. Upon examining different theoretical candidates for the group and image space, we show that the attempt to define a group action on the data space generally fails, as it requires more specific geometric constraints on the volume. Using geometric VAEs, our experiments confirm that this constraint is key to proper pose inference, and we discuss the potential of these results for applications and future work.



### Generative Artisan: A Semantic-Aware and Controllable CLIPstyler
- **Arxiv ID**: http://arxiv.org/abs/2207.11598v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.11598v1)
- **Published**: 2022-07-23 20:26:47+00:00
- **Updated**: 2022-07-23 20:26:47+00:00
- **Authors**: Zhenling Yang, Huacheng Song, Qiunan Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Recall that most of the current image style transfer methods require the user to give an image of a particular style and then extract that styling feature and texture to generate the style of an image, but there are still some problems: the user may not have a reference style image, or it may be difficult to summarise the desired style in mind with just one image. The recently proposed CLIPstyler has solved this problem, which is able to perform style transfer based only on the provided description of the style image. Although CLIPstyler can achieve good performance when landscapes or portraits appear alone, it can blur the people and lose the original semantics when people and landscapes coexist. Based on these issues, we demonstrate a novel framework that uses a pre-trained CLIP text-image embedding model and guides image style transfer through an FCN semantic segmentation network. Specifically, we solve the portrait over-styling problem for both selfies and real-world landscape with human subjects photos, enhance the contrast between the effect of style transfer in portrait and landscape, and make the degree of image style transfer in different semantic parts fully controllable. Our Generative Artisan resolve the failure case of CLIPstyler and yield both qualitative and quantitative methods to prove ours have much better results than CLIPstyler in both selfies and real-world landscape with human subjects photos. This improvement makes it possible to commercialize our framework for business scenarios such as retouching graphics software.



### Face Deblurring using Dual Camera Fusion on Mobile Phones
- **Arxiv ID**: http://arxiv.org/abs/2207.11617v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2207.11617v1)
- **Published**: 2022-07-23 22:50:46+00:00
- **Updated**: 2022-07-23 22:50:46+00:00
- **Authors**: Wei-Sheng Lai, YiChang Shih, Lun-Cheng Chu, Xiaotong Wu, Sung-Fang Tsai, Michael Krainin, Deqing Sun, Chia-Kai Liang
- **Comment**: Accepted to SIGGRAPH 2022 (ACM TOG). Project websit:
  https://www.wslai.net/publications/fusion_deblur/
- **Journal**: None
- **Summary**: Motion blur of fast-moving subjects is a longstanding problem in photography and very common on mobile phones due to limited light collection efficiency, particularly in low-light conditions. While we have witnessed great progress in image deblurring in recent years, most methods require significant computational power and have limitations in processing high-resolution photos with severe local motions. To this end, we develop a novel face deblurring system based on the dual camera fusion technique for mobile phones. The system detects subject motion to dynamically enable a reference camera, e.g., ultrawide angle camera commonly available on recent premium phones, and captures an auxiliary photo with faster shutter settings. While the main shot is low noise but blurry, the reference shot is sharp but noisy. We learn ML models to align and fuse these two shots and output a clear photo without motion blur. Our algorithm runs efficiently on Google Pixel 6, which takes 463 ms overhead per shot. Our experiments demonstrate the advantage and robustness of our system against alternative single-image, multi-frame, face-specific, and video deblurring algorithms as well as commercial products. To the best of our knowledge, our work is the first mobile solution for face motion deblurring that works reliably and robustly over thousands of images in diverse motion and lighting conditions.



