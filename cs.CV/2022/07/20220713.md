# Arxiv Papers in cs.CV on 2022-07-13
### Hybrid Spatial-Temporal Entropy Modelling for Neural Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2207.05894v1
- **DOI**: 10.1145/3503161.3547845
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.05894v1)
- **Published**: 2022-07-13 00:03:54+00:00
- **Updated**: 2022-07-13 00:03:54+00:00
- **Authors**: Jiahao Li, Bin Li, Yan Lu
- **Comment**: Accepted by ACM MM 2022. Codes are at
  https://github.com/microsoft/DCVC
- **Journal**: None
- **Summary**: For neural video codec, it is critical, yet challenging, to design an efficient entropy model which can accurately predict the probability distribution of the quantized latent representation. However, most existing video codecs directly use the ready-made entropy model from image codec to encode the residual or motion, and do not fully leverage the spatial-temporal characteristics in video. To this end, this paper proposes a powerful entropy model which efficiently captures both spatial and temporal dependencies. In particular, we introduce the latent prior which exploits the correlation among the latent representation to squeeze the temporal redundancy. Meanwhile, the dual spatial prior is proposed to reduce the spatial redundancy in a parallel-friendly manner. In addition, our entropy model is also versatile. Besides estimating the probability distribution, our entropy model also generates the quantization step at spatial-channel-wise. This content-adaptive quantization mechanism not only helps our codec achieve the smooth rate adjustment in single model but also improves the final rate-distortion performance by dynamic bit allocation. Experimental results show that, powered by the proposed entropy model, our neural codec can achieve 18.2% bitrate saving on UVG dataset when compared with H.266 (VTM) using the highest compression ratio configuration. It makes a new milestone in the development of neural video codec. The codes are at https://github.com/microsoft/DCVC.



### D-CBRS: Accounting For Intra-Class Diversity in Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.05897v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2207.05897v1)
- **Published**: 2022-07-13 00:08:27+00:00
- **Updated**: 2022-07-13 00:08:27+00:00
- **Authors**: Yasin Findik, Farhad Pourkamali-Anaraki
- **Comment**: To appear in IEEE ICIP 2022
- **Journal**: None
- **Summary**: Continual learning -- accumulating knowledge from a sequence of learning experiences -- is an important yet challenging problem. In this paradigm, the model's performance for previously encountered instances may substantially drop as additional data are seen. When dealing with class-imbalanced data, forgetting is further exacerbated. Prior work has proposed replay-based approaches which aim at reducing forgetting by intelligently storing instances for future replay. Although Class-Balancing Reservoir Sampling (CBRS) has been successful in dealing with imbalanced data, the intra-class diversity has not been accounted for, implicitly assuming that each instance of a class is equally informative. We present Diverse-CBRS (D-CBRS), an algorithm that allows us to consider within class diversity when storing instances in the memory. Our results show that D-CBRS outperforms state-of-the-art memory management continual learning algorithms on data sets with considerable intra-class diversity.



### Verifying Attention Robustness of Deep Neural Networks against Semantic Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2207.05902v1
- **DOI**: None
- **Categories**: **cs.CV**, D.2.4; I.1.4
- **Links**: [PDF](http://arxiv.org/pdf/2207.05902v1)
- **Published**: 2022-07-13 00:26:56+00:00
- **Updated**: 2022-07-13 00:26:56+00:00
- **Authors**: Satoshi Munakata, Caterina Urban, Haruki Yokoyama, Koji Yamamoto, Kazuki Munakata
- **Comment**: 25 pages, 12 figures
- **Journal**: None
- **Summary**: It is known that deep neural networks (DNNs) classify an input image by paying particular attention to certain specific pixels; a graphical representation of the magnitude of attention to each pixel is called a saliency-map. Saliency-maps are used to check the validity of the classification decision basis, e.g., it is not a valid basis for classification if a DNN pays more attention to the background rather than the subject of an image. Semantic perturbations can significantly change the saliency-map. In this work, we propose the first verification method for attention robustness, i.e., the local robustness of the changes in the saliency-map against combinations of semantic perturbations. Specifically, our method determines the range of the perturbation parameters (e.g., the brightness change) that maintains the difference between the actual saliency-map change and the expected saliency-map change below a given threshold value. Our method is based on activation region traversals, focusing on the outermost robust boundary for scalability on larger DNNs. Experimental results demonstrate that our method can show the extent to which DNNs can classify with the same basis regardless of semantic perturbations and report on performance and performance factors of activation region traversals.



### Diverse Dance Synthesis via Keyframes with Transformer Controllers
- **Arxiv ID**: http://arxiv.org/abs/2207.05906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.05906v1)
- **Published**: 2022-07-13 00:56:46+00:00
- **Updated**: 2022-07-13 00:56:46+00:00
- **Authors**: Junjun Pan, Siyuan Wang, Junxuan Bai, Ju Dai
- **Comment**: None
- **Journal**: None
- **Summary**: Existing keyframe-based motion synthesis mainly focuses on the generation of cyclic actions or short-term motion, such as walking, running, and transitions between close postures. However, these methods will significantly degrade the naturalness and diversity of the synthesized motion when dealing with complex and impromptu movements, e.g., dance performance and martial arts. In addition, current research lacks fine-grained control over the generated motion, which is essential for intelligent human-computer interaction and animation creation. In this paper, we propose a novel keyframe-based motion generation network based on multiple constraints, which can achieve diverse dance synthesis via learned knowledge. Specifically, the algorithm is mainly formulated based on the recurrent neural network (RNN) and the Transformer architecture. The backbone of our network is a hierarchical RNN module composed of two long short-term memory (LSTM) units, in which the first LSTM is utilized to embed the posture information of the historical frames into a latent space, and the second one is employed to predict the human posture for the next frame. Moreover, our framework contains two Transformer-based controllers, which are used to model the constraints of the root trajectory and the velocity factor respectively, so as to better utilize the temporal context of the frames and achieve fine-grained motion control. We verify the proposed approach on a dance dataset containing a wide range of contemporary dance. The results of three quantitative analyses validate the superiority of our algorithm. The video and qualitative experimental results demonstrate that the complex motion sequences generated by our algorithm can achieve diverse and smooth motion transitions between keyframes, even for long-term synthesis.



### Texture-guided Saliency Distilling for Unsupervised Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.05921v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.05921v3)
- **Published**: 2022-07-13 02:01:07+00:00
- **Updated**: 2023-05-09 04:21:48+00:00
- **Authors**: Huajun Zhou, Bo Qiao, Lingxiao Yang, Jianhuang Lai, Xiaohua Xie
- **Comment**: 8 pages, accepted to CVPR 2023
- **Journal**: None
- **Summary**: Deep Learning-based Unsupervised Salient Object Detection (USOD) mainly relies on the noisy saliency pseudo labels that have been generated from traditional handcraft methods or pre-trained networks. To cope with the noisy labels problem, a class of methods focus on only easy samples with reliable labels but ignore valuable knowledge in hard samples. In this paper, we propose a novel USOD method to mine rich and accurate saliency knowledge from both easy and hard samples. First, we propose a Confidence-aware Saliency Distilling (CSD) strategy that scores samples conditioned on samples' confidences, which guides the model to distill saliency knowledge from easy samples to hard samples progressively. Second, we propose a Boundary-aware Texture Matching (BTM) strategy to refine the boundaries of noisy labels by matching the textures around the predicted boundary. Extensive experiments on RGB, RGB-D, RGB-T, and video SOD benchmarks prove that our method achieves state-of-the-art USOD performance.



### Rapid Person Re-Identification via Sub-space Consistency Regularization
- **Arxiv ID**: http://arxiv.org/abs/2207.05933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.05933v1)
- **Published**: 2022-07-13 02:44:05+00:00
- **Updated**: 2022-07-13 02:44:05+00:00
- **Authors**: Qingze Yin, Guanan Wang, Guodong Ding, Qilei Li, Shaogang Gong, Zhenmin Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Person Re-Identification (ReID) matches pedestrians across disjoint cameras. Existing ReID methods adopting real-value feature descriptors have achieved high accuracy, but they are low in efficiency due to the slow Euclidean distance computation as well as complex quick-sort algorithms. Recently, some works propose to yield binary encoded person descriptors which instead only require fast Hamming distance computation and simple counting-sort algorithms. However, the performances of such binary encoded descriptors, especially with short code (e.g., 32 and 64 bits), are hardly satisfactory given the sparse binary space. To strike a balance between the model accuracy and efficiency, we propose a novel Sub-space Consistency Regularization (SCR) algorithm that can speed up the ReID procedure by $0.25$ times than real-value features under the same dimensions whilst maintaining a competitive accuracy, especially under short codes. SCR transforms real-value features vector (e.g., 2048 float32) with short binary codes (e.g., 64 bits) by first dividing real-value features vector into $M$ sub-spaces, each with $C$ clustered centroids. Thus the distance between two samples can be expressed as the summation of the respective distance to the centroids, which can be sped up by offline calculation and maintained via a look-up table. On the other side, these real-value centroids help to achieve significantly higher accuracy than using binary code. Lastly, we convert the distance look-up table to be integer and apply the counting-sort algorithm to speed up the ranking stage.   We also propose a novel consistency regularization with an iterative framework. Experimental results on Market-1501 and DukeMTMC-reID show promising and exciting results. Under short code, our proposed SCR enjoys Real-value-level accuracy and Hashing-level speed.



### Prediction of the motion of chest internal points using a recurrent neural network trained with real-time recurrent learning for latency compensation in lung cancer radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/2207.05951v1
- **DOI**: 10.1016/j.compmedimag.2021.101941
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.05951v1)
- **Published**: 2022-07-13 04:08:21+00:00
- **Updated**: 2022-07-13 04:08:21+00:00
- **Authors**: Michel Pohl, Mitsuru Uesaka, Kazuyuki Demachi, Ritu Bhusal Chhatkuli
- **Comment**: 21 pages, 24 figures; accepted manuscript version
- **Journal**: Computerized Medical Imaging and Graphics, Volume 91, 2021,
  p.101941
- **Summary**: During the radiotherapy treatment of patients with lung cancer, the radiation delivered to healthy tissue around the tumor needs to be minimized, which is difficult because of respiratory motion and the latency of linear accelerator systems. In the proposed study, we first use the Lucas-Kanade pyramidal optical flow algorithm to perform deformable image registration of chest computed tomography scan images of four patients with lung cancer. We then track three internal points close to the lung tumor based on the previously computed deformation field and predict their position with a recurrent neural network (RNN) trained using real-time recurrent learning (RTRL) and gradient clipping. The breathing data is quite regular, sampled at approximately 2.5Hz, and includes artificial drift in the spine direction. The amplitude of the motion of the tracked points ranged from 12.0mm to 22.7mm. Finally, we propose a simple method for recovering and predicting 3D tumor images from the tracked points and the initial tumor image based on a linear correspondence model and Nadaraya-Watson non-linear regression. The root-mean-square error, maximum error, and jitter corresponding to the RNN prediction on the test set were smaller than the same performance measures obtained with linear prediction and least mean squares (LMS). In particular, the maximum prediction error associated with the RNN, equal to 1.51mm, is respectively 16.1% and 5.0% lower than the maximum error associated with linear prediction and LMS. The average prediction time per time step with RTRL is equal to 119ms, which is less than the 400ms marker position sampling time. The tumor position in the predicted images appears visually correct, which is confirmed by the high mean cross-correlation between the original and predicted images, equal to 0.955.



### Orthogonal-Coding-Based Feature Generation for Transductive Open-Set Recognition via Dual-Space Consistent Sampling
- **Arxiv ID**: http://arxiv.org/abs/2207.05957v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.05957v1)
- **Published**: 2022-07-13 04:29:20+00:00
- **Updated**: 2022-07-13 04:29:20+00:00
- **Authors**: Jiayin Sun, Qiulei Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Open-set recognition (OSR) aims to simultaneously detect unknown-class samples and classify known-class samples. Most of the existing OSR methods are inductive methods, which generally suffer from the domain shift problem that the learned model from the known-class domain might be unsuitable for the unknown-class domain. Addressing this problem, inspired by the success of transductive learning for alleviating the domain shift problem in many other visual tasks, we propose an Iterative Transductive OSR framework, called IT-OSR, which implements three explored modules iteratively, including a reliability sampling module, a feature generation module, and a baseline update module. Specifically, at each iteration, a dual-space consistent sampling approach is presented in the explored reliability sampling module for selecting some relatively more reliable ones from the test samples according to their pseudo labels assigned by a baseline method, which could be an arbitrary inductive OSR method. Then, a conditional dual-adversarial generative network under an orthogonal coding condition is designed in the feature generation module to generate discriminative sample features of both known and unknown classes according to the selected test samples with their pseudo labels. Finally, the baseline method is updated for sample re-prediction in the baseline update module by jointly utilizing the generated features, the selected test samples with pseudo labels, and the training samples. Extensive experimental results on both the standard-dataset and the cross-dataset settings demonstrate that the derived transductive methods, by introducing two typical inductive OSR methods into the proposed IT-OSR framework, achieve better performances than 15 state-of-the-art methods in most cases.



### RESECT-SEG: Open access annotations of intra-operative brain tumor ultrasound images
- **Arxiv ID**: http://arxiv.org/abs/2207.07494v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.07494v1)
- **Published**: 2022-07-13 05:53:30+00:00
- **Updated**: 2022-07-13 05:53:30+00:00
- **Authors**: Bahareh Behboodi, Francois-Xavier Carton, Matthieu Chabanas, Sandrine De Ribaupierre, Ole Solheim, Bodil K. R. Munkvold, Hassan Rivaz, Yiming Xiao, Ingerid Reinertsen
- **Comment**: Bahareh Behboodi and Francois-Xavier Carton share the first
  authorship
- **Journal**: None
- **Summary**: Purpose: Registration and segmentation of magnetic resonance (MR) and ultrasound (US) images play an essential role in surgical planning and resection of brain tumors. However, validating these techniques is challenging due to the scarcity of publicly accessible sources with high-quality ground truth information. To this end, we propose a unique annotation dataset of tumor tissues and resection cavities from the previously published RESECT dataset (Xiao et al. 2017) to encourage a more rigorous assessments of image processing techniques. Acquisition and validation methods: The RESECT database consists of MR and intraoperative US (iUS) images of 23 patients who underwent resection surgeries. The proposed dataset contains tumor tissues and resection cavity annotations of the iUS images. The quality of annotations were validated by two highly experienced neurosurgeons through several assessment criteria. Data format and availability: Annotations of tumor tissues and resection cavities are provided in 3D NIFTI formats. Both sets of annotations are accessible online in the \url{https://osf.io/6y4db}. Discussion and potential applications: The proposed database includes tumor tissue and resection cavity annotations from real-world clinical ultrasound brain images to evaluate segmentation and registration methods. These labels could also be used to train deep learning approaches. Eventually, this dataset should further improve the quality of image guidance in neurosurgery.



### A new database of Houma Alliance Book ancient handwritten characters and classifier fusion approach
- **Arxiv ID**: http://arxiv.org/abs/2207.05993v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.05993v3)
- **Published**: 2022-07-13 06:56:21+00:00
- **Updated**: 2022-08-02 13:06:08+00:00
- **Authors**: Xiaoyu Yuan, Zhibo Zhang, Yabo Sun, Zekai Xue, Xiuyan Shao, Xiaohua Huang
- **Comment**: 11 pages, 7 figures, accepted by 14th International Conference on
  Graphics and Image Processing (ICGIP 2022)
- **Journal**: None
- **Summary**: The Houma Alliance Book is one of the national treasures of the Museum in Shanxi Museum Town in China. It has great historical significance in researching ancient history. To date, the research on the Houma Alliance Book has been staying in the identification of paper documents, which is inefficient to identify and difficult to display, study and publicize. Therefore, the digitization of the recognized ancient characters of Houma League can effectively improve the efficiency of recognizing ancient characters and provide more reliable technical support and text data. This paper proposes a new database of Houma Alliance Book ancient handwritten characters and a multi-modal fusion method to recognize ancient handwritten characters. In the database, 297 classes and 3,547 samples of Houma Alliance ancient handwritten characters are collected from the original book collection and by human imitative writing. Furthermore, the decision-level classifier fusion strategy is applied to fuse three well-known deep neural network architectures for ancient handwritten character recognition. Experiments are performed on our new database. The experimental results first provide the baseline result of the new database to the research community and then demonstrate the efficiency of our proposed method.



### Visual Context-driven Audio Feature Enhancement for Robust End-to-End Audio-Visual Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.06020v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.AI, cs.CV, cs.MM, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.06020v1)
- **Published**: 2022-07-13 08:07:19+00:00
- **Updated**: 2022-07-13 08:07:19+00:00
- **Authors**: Joanna Hong, Minsu Kim, Daehun Yoo, Yong Man Ro
- **Comment**: Accepted at Interspeech 2022
- **Journal**: None
- **Summary**: This paper focuses on designing a noise-robust end-to-end Audio-Visual Speech Recognition (AVSR) system. To this end, we propose Visual Context-driven Audio Feature Enhancement module (V-CAFE) to enhance the input noisy audio speech with a help of audio-visual correspondence. The proposed V-CAFE is designed to capture the transition of lip movements, namely visual context and to generate a noise reduction mask by considering the obtained visual context. Through context-dependent modeling, the ambiguity in viseme-to-phoneme mapping can be refined for mask generation. The noisy representations are masked out with the noise reduction mask resulting in enhanced audio features. The enhanced audio features are fused with the visual features and taken to an encoder-decoder model composed of Conformer and Transformer for speech recognition. We show the proposed end-to-end AVSR with the V-CAFE can further improve the noise-robustness of AVSR. The effectiveness of the proposed method is evaluated in noisy speech recognition and overlapped speech recognition experiments using the two largest audio-visual datasets, LRS2 and LRS3.



### Perturbation Inactivation Based Adversarial Defense for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.06035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06035v1)
- **Published**: 2022-07-13 08:33:15+00:00
- **Updated**: 2022-07-13 08:33:15+00:00
- **Authors**: Min Ren, Yuhao Zhu, Yunlong Wang, Zhenan Sun
- **Comment**: Accepted by IEEE Transactions on Information Forensics & Security
  (T-IFS)
- **Journal**: None
- **Summary**: Deep learning-based face recognition models are vulnerable to adversarial attacks. To curb these attacks, most defense methods aim to improve the robustness of recognition models against adversarial perturbations. However, the generalization capacities of these methods are quite limited. In practice, they are still vulnerable to unseen adversarial attacks. Deep learning models are fairly robust to general perturbations, such as Gaussian noises. A straightforward approach is to inactivate the adversarial perturbations so that they can be easily handled as general perturbations. In this paper, a plug-and-play adversarial defense method, named perturbation inactivation (PIN), is proposed to inactivate adversarial perturbations for adversarial defense. We discover that the perturbations in different subspaces have different influences on the recognition model. There should be a subspace, called the immune space, in which the perturbations have fewer adverse impacts on the recognition model than in other subspaces. Hence, our method estimates the immune space and inactivates the adversarial perturbations by restricting them to this subspace. The proposed method can be generalized to unseen adversarial perturbations since it does not rely on a specific kind of adversarial attack method. This approach not only outperforms several state-of-the-art adversarial defense methods but also demonstrates a superior generalization capacity through exhaustive experiments. Moreover, the proposed method can be successfully applied to four commercial APIs without additional training, indicating that it can be easily generalized to existing face recognition systems. The source code is available at https://github.com/RenMin1991/Perturbation-Inactivate



### Color Coding of Large Value Ranges Applied to Meteorological Data
- **Arxiv ID**: http://arxiv.org/abs/2207.12399v2
- **DOI**: 10.1109/VIS54862.2022.00034
- **Categories**: **eess.IV**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2207.12399v2)
- **Published**: 2022-07-13 08:48:43+00:00
- **Updated**: 2022-10-24 09:59:02+00:00
- **Authors**: Daniel Braun, Kerstin Ebell, Vera Schemann, Laura Pelchmann, Susanne Crewell, Rita Borgo, Tatiana von Landesberger
- **Comment**: Preprint and Author Version of a Short Paper, accepted to the 2022
  IEEE Visualization Conference (VIS)
- **Journal**: None
- **Summary**: This paper presents a novel color scheme designed to address the challenge of visualizing data series with large value ranges, where scale transformation provides limited support. We focus on meteorological data, where the presence of large value ranges is common. We apply our approach to meteorological scatterplots, as one of the most common plots used in this domain area. Our approach leverages the numerical representation of mantissa and exponent of the values to guide the design of novel "nested" color schemes, able to emphasize differences between magnitudes. Our user study evaluates the new designs, the state of the art color scales and representative color schemes used in the analysis of meteorological data: ColorCrafter, Viridis, and Rainbow. We assess accuracy, time and confidence in the context of discrimination (comparison) and interpretation (reading) tasks. Our proposed color scheme significantly outperforms the others in interpretation tasks, while showing comparable performances in discrimination tasks.



### Experiments on Anomaly Detection in Autonomous Driving by Forward-Backward Style Transfers
- **Arxiv ID**: http://arxiv.org/abs/2207.06055v2
- **DOI**: 10.1109/ICECCME55909.2022.9988287
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.06055v2)
- **Published**: 2022-07-13 08:58:55+00:00
- **Updated**: 2022-09-01 10:28:21+00:00
- **Authors**: Daniel Bogdoll, Meng Zhang, Maximilian Nitsche, J. Marius Zöllner
- **Comment**: Daniel Bogdoll and Meng Zhang contributed equally. Accepted for
  publication at ICECCME 2022
- **Journal**: None
- **Summary**: Great progress has been achieved in the community of autonomous driving in the past few years. As a safety-critical problem, however, anomaly detection is a huge hurdle towards a large-scale deployment of autonomous vehicles in the real world. While many approaches, such as uncertainty estimation or segmentation-based image resynthesis, are extremely promising, there is more to be explored. Especially inspired by works on anomaly detection based on image resynthesis, we propose a novel approach for anomaly detection through style transfer. We leverage generative models to map an image from its original style domain of road traffic to an arbitrary one and back to generate pixelwise anomaly scores. However, our experiments have proven our hypothesis wrong, and we were unable to produce significant results. Nevertheless, we want to share our findings, so that others can learn from our experiments.



### Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras
- **Arxiv ID**: http://arxiv.org/abs/2207.06058v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.06058v3)
- **Published**: 2022-07-13 09:05:35+00:00
- **Updated**: 2023-01-29 03:55:53+00:00
- **Authors**: Fangwen Shu, Jiaxuan Wang, Alain Pagani, Didier Stricker
- **Comment**: Accepted to ICRA 2023, supplementary materials attached, code
  open-source: https://github.com/PeterFWS/Structure-PLP-SLAM
- **Journal**: None
- **Summary**: This paper presents a visual SLAM system that uses both points and lines for robust camera localization, and simultaneously performs a piece-wise planar reconstruction (PPR) of the environment to provide a structural map in real-time. One of the biggest challenges in parallel tracking and mapping with a monocular camera is to keep the scale consistent when reconstructing the geometric primitives. This further introduces difficulties in graph optimization of the bundle adjustment (BA) step. We solve these problems by proposing several run-time optimizations on the reconstructed lines and planes. Our system is able to run with depth and stereo sensors in addition to the monocular setting. Our proposed SLAM tightly incorporates the semantic and geometric features to boost both frontend pose tracking and backend map optimization. We evaluate our system exhaustively on various datasets, and show that we outperform state-of-the-art methods in terms of trajectory precision. The code of PLP-SLAM has been made available in open-source for the research community (https://github.com/PeterFWS/Structure-PLP-SLAM).



### Pyramid Transformer for Traffic Sign Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.06067v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06067v2)
- **Published**: 2022-07-13 09:21:19+00:00
- **Updated**: 2022-07-22 07:17:55+00:00
- **Authors**: Omid Nejati Manzari, Amin Boudesh, Shahriar B. Shokouhi
- **Comment**: None
- **Journal**: None
- **Summary**: Traffic sign detection is a vital task in the visual system of self-driving cars and the automated driving system. Recently, novel Transformer-based models have achieved encouraging results for various computer vision tasks. We still observed that vanilla ViT could not yield satisfactory results in traffic sign detection because the overall size of the datasets is very small and the class distribution of traffic signs is extremely unbalanced. To overcome this problem, a novel Pyramid Transformer with locality mechanisms is proposed in this paper. Specifically, Pyramid Transformer has several spatial pyramid reduction layers to shrink and embed the input image into tokens with rich multi-scale context by using atrous convolutions. Moreover, it inherits an intrinsic scale invariance inductive bias and is able to learn local feature representation for objects at various scales, thereby enhancing the network robustness against the size discrepancy of traffic signs. The experiments are conducted on the German Traffic Sign Detection Benchmark (GTSDB). The results demonstrate the superiority of the proposed model in the traffic sign detection tasks. More specifically, Pyramid Transformer achieves 77.8% mAP on GTSDB when applied to the Cascade RCNN as the backbone, which surpasses most well-known and widely-used state-of-the-art models.



### DSPNet: Towards Slimmable Pretrained Networks based on Discriminative Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.06075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06075v1)
- **Published**: 2022-07-13 09:32:54+00:00
- **Updated**: 2022-07-13 09:32:54+00:00
- **Authors**: Shaoru Wang, Zeming Li, Jin Gao, Liang Li, Weiming Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning (SSL) has achieved promising downstream performance. However, when facing various resource budgets in real-world applications, it costs a huge computation burden to pretrain multiple networks of various sizes one by one. In this paper, we propose Discriminative-SSL-based Slimmable Pretrained Networks (DSPNet), which can be trained at once and then slimmed to multiple sub-networks of various sizes, each of which faithfully learns good representation and can serve as good initialization for downstream tasks with various resource budgets. Specifically, we extend the idea of slimmable networks to a discriminative SSL paradigm, by integrating SSL and knowledge distillation gracefully. We show comparable or improved performance of DSPNet on ImageNet to the networks individually pretrained one by one under the linear evaluation and semi-supervised evaluation protocols, while reducing large training cost. The pretrained models also generalize well on downstream detection and segmentation tasks. Code will be made public.



### MultiStream: A Simple and Fast Multiple Cameras Visual Monitor and Directly Streaming
- **Arxiv ID**: http://arxiv.org/abs/2207.06078v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.06078v1)
- **Published**: 2022-07-13 09:38:04+00:00
- **Updated**: 2022-07-13 09:38:04+00:00
- **Authors**: Jinwei Lin
- **Comment**: 6 pages, 9 figures
- **Journal**: None
- **Summary**: Monitoring and streaming is one of the most important applications for the real time cameras. The research of this has provided a novel design idea that uses the FFmpeg and Tkinter, combining with the libraries: OpenCV and PIL to develop a simple but fast streaming toolkit MultiSteam that can achieve the function of visible monitoring streaming for multiple simultaneously. MultiStream is able to automatically arrange the layout of the displays of multiple camera windows and intelligently analyze the input streaming URL to select the correct corresponding streaming communication protocol. Multiple cameras can be streamed with different communication protocols or the same protocol. Besides, the paper has tested the different streaming speeds for different protocols in camera streaming. MultiStream is able to gain the information of media equipment on the computer. The configuration information for media-id selection and multiple cameras streaming can be saved as json files.



### Teachers in concordance for pseudo-labeling of 3D sequential data
- **Arxiv ID**: http://arxiv.org/abs/2207.06079v2
- **DOI**: 10.1109/LRA.2022.3226029
- **Categories**: **cs.CV**, cs.RO, 68T07, I.4.6; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2207.06079v2)
- **Published**: 2022-07-13 09:40:22+00:00
- **Updated**: 2023-07-05 04:31:58+00:00
- **Authors**: Awet Haileslassie Gebrehiwot, Patrik Vacek, David Hurych, Karel Zimmermann, Patrick Perez, Tomáš Svoboda
- **Comment**: This work has been submitted to the IEEE for publication
- **Journal**: in IEEE Robotics and Automation Letters, vol. 8, no. 2, pp.
  536-543, Feb. 2023
- **Summary**: Automatic pseudo-labeling is a powerful tool to tap into large amounts of sequential unlabeled data. It is specially appealing in safety-critical applications of autonomous driving, where performance requirements are extreme, datasets are large, and manual labeling is very challenging. We propose to leverage sequences of point clouds to boost the pseudolabeling technique in a teacher-student setup via training multiple teachers, each with access to different temporal information. This set of teachers, dubbed Concordance, provides higher quality pseudo-labels for student training than standard methods. The output of multiple teachers is combined via a novel pseudo label confidence-guided criterion. Our experimental evaluation focuses on the 3D point cloud domain and urban driving scenarios. We show the performance of our method applied to 3D semantic segmentation and 3D object detection on three benchmark datasets. Our approach, which uses only 20% manual labels, outperforms some fully supervised methods. A notable performance boost is achieved for classes rarely appearing in training data.



### Semi-supervised Ranking for Object Image Blur Assessment
- **Arxiv ID**: http://arxiv.org/abs/2207.06085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06085v1)
- **Published**: 2022-07-13 09:49:22+00:00
- **Updated**: 2022-07-13 09:49:22+00:00
- **Authors**: Qiang Li, Zhaoliang Yao, Jingjing Wang, Ye Tian, Pengju Yang, Di Xie, Shiliang Pu
- **Comment**: The first two authors contributed equally to this work. Dataset is
  available at https://github.com/yzliangHIK2022/SSRanking-for-Object-BA.
  Accepted to ICIP 2022
- **Journal**: None
- **Summary**: Assessing the blurriness of an object image is fundamentally important to improve the performance for object recognition and retrieval. The main challenge lies in the lack of abundant images with reliable labels and effective learning strategies. Current datasets are labeled with limited and confused quality levels. To overcome this limitation, we propose to label the rank relationships between pairwise images rather their quality levels, since it is much easier for humans to label, and establish a large-scale realistic face image blur assessment dataset with reliable labels. Based on this dataset, we propose a method to obtain the blur scores only with the pairwise rank labels as supervision. Moreover, to further improve the performance, we propose a self-supervised method based on quadruplet ranking consistency to leverage the unlabeled data more effectively. The supervised and self-supervised methods constitute a final semi-supervised learning framework, which can be trained end-to-end. Experimental results demonstrate the effectiveness of our method.



### Eliminating Gradient Conflict in Reference-based Line-Art Colorization
- **Arxiv ID**: http://arxiv.org/abs/2207.06095v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06095v3)
- **Published**: 2022-07-13 10:08:37+00:00
- **Updated**: 2022-07-20 06:24:07+00:00
- **Authors**: Zekun Li, Zhengyang Geng, Zhao Kang, Wenyu Chen, Yibo Yang
- **Comment**: Accepted by ECCV2022
- **Journal**: None
- **Summary**: Reference-based line-art colorization is a challenging task in computer vision. The color, texture, and shading are rendered based on an abstract sketch, which heavily relies on the precise long-range dependency modeling between the sketch and reference. Popular techniques to bridge the cross-modal information and model the long-range dependency employ the attention mechanism. However, in the context of reference-based line-art colorization, several techniques would intensify the existing training difficulty of attention, for instance, self-supervised training protocol and GAN-based losses. To understand the instability in training, we detect the gradient flow of attention and observe gradient conflict among attention branches. This phenomenon motivates us to alleviate the gradient issue by preserving the dominant gradient branch while removing the conflict ones. We propose a novel attention mechanism using this training strategy, Stop-Gradient Attention (SGA), outperforming the attention baseline by a large margin with better training stability. Compared with state-of-the-art modules in line-art colorization, our approach demonstrates significant improvements in Fr\'echet Inception Distance (FID, up to 27.21%) and structural similarity index measure (SSIM, up to 25.67%) on several benchmarks. The code of SGA is available at https://github.com/kunkun0w0/SGA .



### Global-local Motion Transformer for Unsupervised Skeleton-based Action Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.06101v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.06101v1)
- **Published**: 2022-07-13 10:18:07+00:00
- **Updated**: 2022-07-13 10:18:07+00:00
- **Authors**: Boeun Kim, Hyung Jin Chang, Jungho Kim, Jin Young Choi
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: We propose a new transformer model for the task of unsupervised learning of skeleton motion sequences. The existing transformer model utilized for unsupervised skeleton-based action learning is learned the instantaneous velocity of each joint from adjacent frames without global motion information. Thus, the model has difficulties in learning the attention globally over whole-body motions and temporally distant joints. In addition, person-to-person interactions have not been considered in the model. To tackle the learning of whole-body motion, long-range temporal dynamics, and person-to-person interactions, we design a global and local attention mechanism, where, global body motions and local joint motions pay attention to each other. In addition, we propose a novel pretraining strategy, multi-interval pose displacement prediction, to learn both global and local attention in diverse time ranges. The proposed model successfully learns local dynamics of the joints and captures global context from the motion sequences. Our model outperforms state-of-the-art models by notable margins in the representative benchmarks. Codes are available at https://github.com/Boeun-Kim/GL-Transformer.



### Learnability Enhancement for Low-light Raw Denoising: Where Paired Real Data Meets Noise Modeling
- **Arxiv ID**: http://arxiv.org/abs/2207.06103v2
- **DOI**: 10.1145/3503161.3548186
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.06103v2)
- **Published**: 2022-07-13 10:23:28+00:00
- **Updated**: 2022-08-18 07:17:20+00:00
- **Authors**: Hansen Feng, Lizhi Wang, Yuzhi Wang, Hua Huang
- **Comment**: 9 pages, 8 figures, to be published in ACMMM 2022
- **Journal**: None
- **Summary**: Low-light raw denoising is an important and valuable task in computational photography where learning-based methods trained with paired real data are mainstream. However, the limited data volume and complicated noise distribution have constituted a learnability bottleneck for paired real data, which limits the denoising performance of learning-based methods. To address this issue, we present a learnability enhancement strategy to reform paired real data according to noise modeling. Our strategy consists of two efficient techniques: shot noise augmentation (SNA) and dark shading correction (DSC). Through noise model decoupling, SNA improves the precision of data mapping by increasing the data volume and DSC reduces the complexity of data mapping by reducing the noise complexity. Extensive results on the public datasets and real imaging scenarios collectively demonstrate the state-of-the-art performance of our method. Our code is available at: https://github.com/megvii-research/PMN.



### Automated Detection of Label Errors in Semantic Segmentation Datasets via Deep Learning and Uncertainty Quantification
- **Arxiv ID**: http://arxiv.org/abs/2207.06104v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T45, 62-07, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2207.06104v1)
- **Published**: 2022-07-13 10:25:23+00:00
- **Updated**: 2022-07-13 10:25:23+00:00
- **Authors**: Matthias Rottmann, Marco Reese
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we for the first time present a method for detecting label errors in image datasets with semantic segmentation, i.e., pixel-wise class labels. Annotation acquisition for semantic segmentation datasets is time-consuming and requires plenty of human labor. In particular, review processes are time consuming and label errors can easily be overlooked by humans. The consequences are biased benchmarks and in extreme cases also performance degradation of deep neural networks (DNNs) trained on such datasets. DNNs for semantic segmentation yield pixel-wise predictions, which makes detection of label errors via uncertainty quantification a complex task. Uncertainty is particularly pronounced at the transitions between connected components of the prediction. By lifting the consideration of uncertainty to the level of predicted components, we enable the usage of DNNs together with component-level uncertainty quantification for the detection of label errors. We present a principled approach to benchmarking the task of label error detection by dropping labels from the Cityscapes dataset as well from a dataset extracted from the CARLA driving simulator, where in the latter case we have the labels under control. Our experiments show that our approach is able to detect the vast majority of label errors while controlling the number of false label error detections. Furthermore, we apply our method to semantic segmentation datasets frequently used by the computer vision community and present a collection of label errors along with sample statistics.



### DynaST: Dynamic Sparse Transformer for Exemplar-Guided Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2207.06124v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06124v3)
- **Published**: 2022-07-13 11:12:03+00:00
- **Updated**: 2023-03-27 07:55:32+00:00
- **Authors**: Songhua Liu, Jingwen Ye, Sucheng Ren, Xinchao Wang
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: One key challenge of exemplar-guided image generation lies in establishing fine-grained correspondences between input and guided images. Prior approaches, despite the promising results, have relied on either estimating dense attention to compute per-point matching, which is limited to only coarse scales due to the quadratic memory cost, or fixing the number of correspondences to achieve linear complexity, which lacks flexibility. In this paper, we propose a dynamic sparse attention based Transformer model, termed Dynamic Sparse Transformer (DynaST), to achieve fine-level matching with favorable efficiency. The heart of our approach is a novel dynamic-attention unit, dedicated to covering the variation on the optimal number of tokens one position should focus on. Specifically, DynaST leverages the multi-layer nature of Transformer structure, and performs the dynamic attention scheme in a cascaded manner to refine matching results and synthesize visually-pleasing outputs. In addition, we introduce a unified training objective for DynaST, making it a versatile reference-based image translation framework for both supervised and unsupervised scenarios. Extensive experiments on three applications, pose-guided person image generation, edge-based face synthesis, and undistorted image style transfer, demonstrate that DynaST achieves superior performance in local details, outperforming the state of the art while reducing the computational cost significantly. Our code is available at https://github.com/Huage001/DynaST



### Robust and accurate depth estimation by fusing LiDAR and Stereo
- **Arxiv ID**: http://arxiv.org/abs/2207.06139v1
- **DOI**: 10.1088/1361-6501/acef47
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.06139v1)
- **Published**: 2022-07-13 11:55:15+00:00
- **Updated**: 2022-07-13 11:55:15+00:00
- **Authors**: Guangyao Xu, Junfeng Fan, En Li, Xiaoyu Long, Rui Guo
- **Comment**: None
- **Journal**: Meas. Sci. Technol. 34 125107 (2023)
- **Summary**: Depth estimation is one of the key technologies in some fields such as autonomous driving and robot navigation. However, the traditional method of using a single sensor is inevitably limited by the performance of the sensor. Therefore, a precision and robust method for fusing the LiDAR and stereo cameras is proposed. This method fully combines the advantages of the LiDAR and stereo camera, which can retain the advantages of the high precision of the LiDAR and the high resolution of images respectively. Compared with the traditional stereo matching method, the texture of the object and lighting conditions have less influence on the algorithm. Firstly, the depth of the LiDAR data is converted to the disparity of the stereo camera. Because the density of the LiDAR data is relatively sparse on the y-axis, the converted disparity map is up-sampled using the interpolation method. Secondly, in order to make full use of the precise disparity map, the disparity map and stereo matching are fused to propagate the accurate disparity. Finally, the disparity map is converted to the depth map. Moreover, the converted disparity map can also increase the speed of the algorithm. We evaluate the proposed pipeline on the KITTI benchmark. The experiment demonstrates that our algorithm has higher accuracy than several classic methods.



### Estimating the Power Consumption of Heterogeneous Devices when performing AI Inference
- **Arxiv ID**: http://arxiv.org/abs/2207.06150v1
- **DOI**: None
- **Categories**: **cs.AR**, cs.AI, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.06150v1)
- **Published**: 2022-07-13 12:19:01+00:00
- **Updated**: 2022-07-13 12:19:01+00:00
- **Authors**: Pedro Machado, Ivica Matic, Francisco de Lemos, Isibor Kennedy Ihianle, David Ada Adama
- **Comment**: None
- **Journal**: None
- **Summary**: Modern-day life is driven by electronic devices connected to the internet. The emerging research field of the Internet-of-Things (IoT) has become popular, just as there has been a steady increase in the number of connected devices - now over 50 billion. Since many of these devices are utilised to perform \gls*{cv} tasks, it is essential to understand their power consumption against performance. We report the power consumption profile and analysis of the NVIDIA Jetson Nano board while performing object classification. The authors present an extensive analysis regarding power consumption per frame and the output in frames per second (FPS) using YOLOv5 models. The results show that the YOLOv5n outperforms other YOLOV5 variants in terms of throughput (i.e. 12.34 fps) and low power consumption (i.e. 0.154 mWh/frame).



### A comparison between PMBM Bayesian track initiation and labelled RFS adaptive birth
- **Arxiv ID**: http://arxiv.org/abs/2207.06156v1
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2207.06156v1)
- **Published**: 2022-07-13 12:34:22+00:00
- **Updated**: 2022-07-13 12:34:22+00:00
- **Authors**: Ángel F. García-Fernández, Yuxuan Xia, Lennart Svensson
- **Comment**: Matlab implementations of PMBM filters can be found at
  https://github.com/Agarciafernandez/MTT and https://github.com/yuhsuansia
- **Journal**: Proceedings of the 25th International Conference on Information
  Fusion, 2022
- **Summary**: This paper provides a comparative analysis between the adaptive birth model used in the labelled random finite set literature and the track initiation in the Poisson multi-Bernoulli mixture (PMBM) filter, with point-target models. The PMBM track initiation is obtained via Bayes' rule applied on the predicted PMBM density, and creates one Bernoulli component for each received measurement, representing that this measurement may be clutter or a detection from a new target. Adaptive birth mimics this procedure by creating a Bernoulli component for each measurement using a different rule to determine the probability of existence and a user-defined single-target density. This paper first provides an analysis of the differences that arise in track initiation based on isolated measurements. Then, it shows that adaptive birth underestimates the number of objects present in the surveillance area under common modelling assumptions. Finally, we provide numerical simulations to further illustrate the differences.



### Unsupervised Visual Representation Learning by Synchronous Momentum Grouping
- **Arxiv ID**: http://arxiv.org/abs/2207.06167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06167v1)
- **Published**: 2022-07-13 13:04:15+00:00
- **Updated**: 2022-07-13 13:04:15+00:00
- **Authors**: Bo Pang, Yifan Zhang, Yaoyi Li, Jia Cai, Cewu Lu
- **Comment**: Conference Paper of ECCV 2022
- **Journal**: None
- **Summary**: In this paper, we propose a genuine group-level contrastive visual representation learning method whose linear evaluation performance on ImageNet surpasses the vanilla supervised learning. Two mainstream unsupervised learning schemes are the instance-level contrastive framework and clustering-based schemes. The former adopts the extremely fine-grained instance-level discrimination whose supervisory signal is not efficient due to the false negatives. Though the latter solves this, they commonly come with some restrictions affecting the performance. To integrate their advantages, we design the SMoG method. SMoG follows the framework of contrastive learning but replaces the contrastive unit from instance to group, mimicking clustering-based methods. To achieve this, we propose the momentum grouping scheme which synchronously conducts feature grouping with representation learning. In this way, SMoG solves the problem of supervisory signal hysteresis which the clustering-based method usually faces, and reduces the false negatives of instance contrastive methods. We conduct exhaustive experiments to show that SMoG works well on both CNN and Transformer backbones. Results prove that SMoG has surpassed the current SOTA unsupervised representation learning methods. Moreover, its linear evaluation results surpass the performances obtained by vanilla supervised learning and the representation can be well transferred to downstream tasks.



### MRF-UNets: Searching UNet with Markov Random Fields
- **Arxiv ID**: http://arxiv.org/abs/2207.06168v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.06168v1)
- **Published**: 2022-07-13 13:04:18+00:00
- **Updated**: 2022-07-13 13:04:18+00:00
- **Authors**: Zifu Wang, Matthew B. Blaschko
- **Comment**: ECML-PKDD 2022
- **Journal**: None
- **Summary**: UNet [27] is widely used in semantic segmentation due to its simplicity and effectiveness. However, its manually-designed architecture is applied to a large number of problem settings, either with no architecture optimizations, or with manual tuning, which is time consuming and can be sub-optimal. In this work, firstly, we propose Markov Random Field Neural Architecture Search (MRF-NAS) that extends and improves the recent Adaptive and Optimal Network Width Search (AOWS) method [4] with (i) a more general MRF framework (ii) diverse M-best loopy inference (iii) differentiable parameter learning. This provides the necessary NAS framework to efficiently explore network architectures that induce loopy inference graphs, including loops that arise from skip connections. With UNet as the backbone, we find an architecture, MRF-UNet, that shows several interesting characteristics. Secondly, through the lens of these characteristics, we identify the sub-optimality of the original UNet architecture and further improve our results with MRF-UNetV2. Experiments show that our MRF-UNets significantly outperform several benchmarks on three aerial image datasets and two medical image datasets while maintaining low computational costs. The code is available at: https://github.com/zifuwanggg/MRF-UNets.



### RTN: Reinforced Transformer Network for Coronary CT Angiography Vessel-level Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2207.06177v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.06177v1)
- **Published**: 2022-07-13 13:17:42+00:00
- **Updated**: 2022-07-13 13:17:42+00:00
- **Authors**: Yiting Lu, Jun Fu, Xin Li, Wei Zhou, Sen Liu, Xinxin Zhang, Congfu Jia, Ying Liu, Zhibo Chen
- **Comment**: To appear in MICCAI2022
- **Journal**: None
- **Summary**: Coronary CT Angiography (CCTA) is susceptible to various distortions (e.g., artifacts and noise), which severely compromise the exact diagnosis of cardiovascular diseases. The appropriate CCTA Vessel-level Image Quality Assessment (CCTA VIQA) algorithm can be used to reduce the risk of error diagnosis. The primary challenges of CCTA VIQA are that the local part of coronary that determines final quality is hard to locate. To tackle the challenge, we formulate CCTA VIQA as a multiple-instance learning (MIL) problem, and exploit Transformer-based MIL backbone (termed as T-MIL) to aggregate the multiple instances along the coronary centerline into the final quality. However, not all instances are informative for final quality. There are some quality-irrelevant/negative instances intervening the exact quality assessment(e.g., instances covering only background or the coronary in instances is not identifiable). Therefore, we propose a Progressive Reinforcement learning based Instance Discarding module (termed as PRID) to progressively remove quality-irrelevant/negative instances for CCTA VIQA. Based on the above two modules, we propose a Reinforced Transformer Network (RTN) for automatic CCTA VIQA based on end-to-end optimization. Extensive experimental results demonstrate that our proposed method achieves the state-of-the-art performance on the real-world CCTA dataset, exceeding previous MIL methods by a large margin.



### Multi-modal Depression Estimation based on Sub-attentional Fusion
- **Arxiv ID**: http://arxiv.org/abs/2207.06180v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.06180v2)
- **Published**: 2022-07-13 13:19:32+00:00
- **Updated**: 2022-08-18 11:05:35+00:00
- **Authors**: Ping-Cheng Wei, Kunyu Peng, Alina Roitberg, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen
- **Comment**: Accepted to ECCV 2022 ACVR Workshop. Code is publicly available at
  https://github.com/PingCheng-Wei/DepressionEstimation
- **Journal**: None
- **Summary**: Failure to timely diagnose and effectively treat depression leads to over 280 million people suffering from this psychological disorder worldwide. The information cues of depression can be harvested from diverse heterogeneous resources, e.g., audio, visual, and textual data, raising demand for new effective multi-modal fusion approaches for automatic estimation. In this work, we tackle the task of automatically identifying depression from multi-modal data and introduce a sub-attention mechanism for linking heterogeneous information while leveraging Convolutional Bidirectional LSTM as our backbone. To validate this idea, we conduct extensive experiments on the public DAIC-WOZ benchmark for depression assessment featuring different evaluation modes and taking gender-specific biases into account. The proposed model yields effective results with 0.89 precision and 0.70 F1-score in detecting major depression and 4.92 MAE in estimating the severity. Our attention-based fusion module consistently outperforms conventional late fusion approaches and achieves competitive performance compared to the previously published depression estimation frameworks, while learning to diagnose the disorder end-to-end and relying on far fewer preprocessing steps.



### Collaborative Quantization Embeddings for Intra-Subject Prostate MR Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2207.06189v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.06189v2)
- **Published**: 2022-07-13 13:32:18+00:00
- **Updated**: 2022-07-14 07:41:55+00:00
- **Authors**: Ziyi Shen, Qianye Yang, Yuming Shen, Francesco Giganti, Vasilis Stavrinides, Richard Fan, Caroline Moore, Mirabela Rusu, Geoffrey Sonn, Philip Torr, Dean Barratt, Yipeng Hu
- **Comment**: preprint version, accepted for MICCAI 2022 (25th International
  Conference on Medical Image Computing and Computer Assisted Intervention)
- **Journal**: None
- **Summary**: Image registration is useful for quantifying morphological changes in longitudinal MR images from prostate cancer patients. This paper describes a development in improving the learning-based registration algorithms, for this challenging clinical application often with highly variable yet limited training data. First, we report that the latent space can be clustered into a much lower dimensional space than that commonly found as bottleneck features at the deep layer of a trained registration network. Based on this observation, we propose a hierarchical quantization method, discretizing the learned feature vectors using a jointly-trained dictionary with a constrained size, in order to improve the generalisation of the registration networks. Furthermore, a novel collaborative dictionary is independently optimised to incorporate additional prior information, such as the segmentation of the gland or other regions of interest, in the latent quantized space. Based on 216 real clinical images from 86 prostate cancer patients, we show the efficacy of both the designed components. Improved registration accuracy was obtained with statistical significance, in terms of both Dice on gland and target registration error on corresponding landmarks, the latter of which achieved 5.46 mm, an improvement of 28.7\% from the baseline without quantization. Experimental results also show that the difference in performance was indeed minimised between training and testing data.



### Domain adaptation strategies for cancer-independent detection of lymph node metastases
- **Arxiv ID**: http://arxiv.org/abs/2207.06193v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.06193v1)
- **Published**: 2022-07-13 13:41:20+00:00
- **Updated**: 2022-07-13 13:41:20+00:00
- **Authors**: Péter Bándi, Maschenka Balkenhol, Marcory van Dijk, Bram van Ginneken, Jeroen van der Laak, Geert Litjens
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, large, high-quality public datasets have led to the development of convolutional neural networks that can detect lymph node metastases of breast cancer at the level of expert pathologists. Many cancers, regardless of the site of origin, can metastasize to lymph nodes. However, collecting and annotating high-volume, high-quality datasets for every cancer type is challenging. In this paper we investigate how to leverage existing high-quality datasets most efficiently in multi-task settings for closely related tasks. Specifically, we will explore different training and domain adaptation strategies, including prevention of catastrophic forgetting, for colon and head-and-neck cancer metastasis detection in lymph nodes.   Our results show state-of-the-art performance on both cancer metastasis detection tasks. Furthermore, we show the effectiveness of repeated adaptation of networks from one cancer type to another to obtain multi-task metastasis detection networks. Last, we show that leveraging existing high-quality datasets can significantly boost performance on new target tasks and that catastrophic forgetting can be effectively mitigated using regularization.



### Adversarially-Aware Robust Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2207.06202v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06202v3)
- **Published**: 2022-07-13 13:59:59+00:00
- **Updated**: 2022-07-22 03:16:36+00:00
- **Authors**: Ziyi Dong, Pengxu Wei, Liang Lin
- **Comment**: ECCV2022 oral paper
- **Journal**: European Conference on Computer Vision (ECCV), 2022
- **Summary**: Object detection, as a fundamental computer vision task, has achieved a remarkable progress with the emergence of deep neural networks. Nevertheless, few works explore the adversarial robustness of object detectors to resist adversarial attacks for practical applications in various real-world scenarios. Detectors have been greatly challenged by unnoticeable perturbation, with sharp performance drop on clean images and extremely poor performance on adversarial images. In this work, we empirically explore the model training for adversarial robustness in object detection, which greatly attributes to the conflict between learning clean images and adversarial images. To mitigate this issue, we propose a Robust Detector (RobustDet) based on adversarially-aware convolution to disentangle gradients for model learning on clean and adversarial images. RobustDet also employs the Adversarial Image Discriminator (AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable robustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that our model effectively disentangles gradients and significantly enhances the detection robustness with maintaining the detection ability on clean images.



### Trans4Map: Revisiting Holistic Bird's-Eye-View Mapping from Egocentric Images to Allocentric Semantics with Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2207.06205v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.06205v2)
- **Published**: 2022-07-13 14:01:00+00:00
- **Updated**: 2022-10-14 13:23:28+00:00
- **Authors**: Chang Chen, Jiaming Zhang, Kailun Yang, Kunyu Peng, Rainer Stiefelhagen
- **Comment**: Accepted to WACV 2023. Code is publicly available at
  https://github.com/jamycheung/Trans4Map
- **Journal**: None
- **Summary**: Humans have an innate ability to sense their surroundings, as they can extract the spatial representation from the egocentric perception and form an allocentric semantic map via spatial transformation and memory updating. However, endowing mobile agents with such a spatial sensing ability is still a challenge, due to two difficulties: (1) the previous convolutional models are limited by the local receptive field, thus, struggling to capture holistic long-range dependencies during observation; (2) the excessive computational budgets required for success, often lead to a separation of the mapping pipeline into stages, resulting the entire mapping process inefficient. To address these issues, we propose an end-to-end one-stage Transformer-based framework for Mapping, termed Trans4Map. Our egocentric-to-allocentric mapping process includes three steps: (1) the efficient transformer extracts the contextual features from a batch of egocentric images; (2) the proposed Bidirectional Allocentric Memory (BAM) module projects egocentric features into the allocentric memory; (3) the map decoder parses the accumulated memory and predicts the top-down semantic segmentation map. In contrast, Trans4Map achieves state-of-the-art results, reducing 67.2% parameters, yet gaining a +3.25% mIoU and a +4.09% mBF1 improvements on the Matterport3D dataset. Code at: https://github.com/jamycheung/Trans4Map.



### Sample-dependent Adaptive Temperature Scaling for Improved Calibration
- **Arxiv ID**: http://arxiv.org/abs/2207.06211v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06211v2)
- **Published**: 2022-07-13 14:13:49+00:00
- **Updated**: 2022-07-22 16:28:13+00:00
- **Authors**: Tom Joy, Francesco Pinto, Ser-Nam Lim, Philip H. S. Torr, Puneet K. Dokania
- **Comment**: None
- **Journal**: None
- **Summary**: It is now well known that neural networks can be wrong with high confidence in their predictions, leading to poor calibration. The most common post-hoc approach to compensate for this is to perform temperature scaling, which adjusts the confidences of the predictions on any input by scaling the logits by a fixed value. Whilst this approach typically improves the average calibration across the whole test dataset, this improvement typically reduces the individual confidences of the predictions irrespective of whether the classification of a given input is correct or incorrect. With this insight, we base our method on the observation that different samples contribute to the calibration error by varying amounts, with some needing to increase their confidence and others needing to decrease it. Therefore, for each input, we propose to predict a different temperature value, allowing us to adjust the mismatch between confidence and accuracy at a finer granularity. Furthermore, we observe improved results on OOD detection and can also extract a notion of hardness for the data-points. Our method is applied post-hoc, consequently using very little computation time and with a negligible memory footprint and is applied to off-the-shelf pre-trained classifiers. We test our method on the ResNet50 and WideResNet28-10 architectures using the CIFAR10/100 and Tiny-ImageNet datasets, showing that producing per-data-point temperatures is beneficial also for the expected calibration error across the whole test set. Code is available at: https://github.com/thwjoy/adats.



### Is one annotation enough? A data-centric image classification benchmark for noisy and ambiguous label estimation
- **Arxiv ID**: http://arxiv.org/abs/2207.06214v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06214v3)
- **Published**: 2022-07-13 14:17:21+00:00
- **Updated**: 2022-11-04 14:04:01+00:00
- **Authors**: Lars Schmarje, Vasco Grossmann, Claudius Zelenka, Sabine Dippel, Rainer Kiko, Mariusz Oszust, Matti Pastell, Jenny Stracke, Anna Valros, Nina Volkmann, Reinhard Koch
- **Comment**: Accepted at NeurIPS 2022, Benchmark and Dataset Track, Code and Link
  to data available at https://github.com/Emprime/dcic
- **Journal**: None
- **Summary**: High-quality data is necessary for modern machine learning. However, the acquisition of such data is difficult due to noisy and ambiguous annotations of humans. The aggregation of such annotations to determine the label of an image leads to a lower data quality. We propose a data-centric image classification benchmark with ten real-world datasets and multiple annotations per image to allow researchers to investigate and quantify the impact of such data quality issues. With the benchmark we can study the impact of annotation costs and (semi-)supervised methods on the data quality for image classification by applying a novel methodology to a range of different algorithms and diverse datasets. Our benchmark uses a two-phase approach via a data label improvement method in the first phase and a fixed evaluation model in the second phase. Thereby, we give a measure for the relation between the input labeling effort and the performance of (semi-)supervised algorithms to enable a deeper insight into how labels should be created for effective model training. Across thousands of experiments, we show that one annotation is not enough and that the inclusion of multiple annotations allows for a better approximation of the real underlying class distribution. We identify that hard labels can not capture the ambiguity of the data and this might lead to the common issue of overconfident models. Based on the presented datasets, benchmarked methods, and analysis, we create multiple research opportunities for the future directed at the improvement of label noise estimation approaches, data annotation schemes, realistic (semi-)supervised learning, or more reliable image collection.



### YOLO2U-Net: Detection-Guided 3D Instance Segmentation for Microscopy
- **Arxiv ID**: http://arxiv.org/abs/2207.06215v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.CB
- **Links**: [PDF](http://arxiv.org/pdf/2207.06215v1)
- **Published**: 2022-07-13 14:17:52+00:00
- **Updated**: 2022-07-13 14:17:52+00:00
- **Authors**: Amirkoushyar Ziabari, Derek C. Rose, Abbas Shirinifard, David Solecki
- **Comment**: None
- **Journal**: None
- **Summary**: Microscopy imaging techniques are instrumental for characterization and analysis of biological structures. As these techniques typically render 3D visualization of cells by stacking 2D projections, issues such as out-of-plane excitation and low resolution in the $z$-axis may pose challenges (even for human experts) to detect individual cells in 3D volumes as these non-overlapping cells may appear as overlapping. In this work, we introduce a comprehensive method for accurate 3D instance segmentation of cells in the brain tissue. The proposed method combines the 2D YOLO detection method with a multi-view fusion algorithm to construct a 3D localization of the cells. Next, the 3D bounding boxes along with the data volume are input to a 3D U-Net network that is designed to segment the primary cell in each 3D bounding box, and in turn, to carry out instance segmentation of cells in the entire volume. The promising performance of the proposed method is shown in comparison with some current deep learning-based 3D instance segmentation methods.



### Beyond Hard Labels: Investigating data label distributions
- **Arxiv ID**: http://arxiv.org/abs/2207.06224v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.06224v2)
- **Published**: 2022-07-13 14:25:30+00:00
- **Updated**: 2022-10-06 08:22:10+00:00
- **Authors**: Vasco Grossmann, Lars Schmarje, Reinhard Koch
- **Comment**: https://icml.cc/virtual/2022/workshop/13477
- **Journal**: ICML 2022 Workshop DataPerf: Benchmarking Data for Data-Centric AI
- **Summary**: High-quality data is a key aspect of modern machine learning. However, labels generated by humans suffer from issues like label noise and class ambiguities. We raise the question of whether hard labels are sufficient to represent the underlying ground truth distribution in the presence of these inherent imprecision. Therefore, we compare the disparity of learning with hard and soft labels quantitatively and qualitatively for a synthetic and a real-world dataset. We show that the application of soft labels leads to improved performance and yields a more regular structure of the internal feature space.



### Open High-Resolution Satellite Imagery: The WorldStrat Dataset -- With Application to Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2207.06418v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.AP, 68-04 (Primary), 68T45, 68U10(Secondary), I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2207.06418v1)
- **Published**: 2022-07-13 14:30:20+00:00
- **Updated**: 2022-07-13 14:30:20+00:00
- **Authors**: Julien Cornebise, Ivan Oršolić, Freddie Kalaitzis
- **Comment**: Submitted, under review
- **Journal**: None
- **Summary**: Analyzing the planet at scale with satellite imagery and machine learning is a dream that has been constantly hindered by the cost of difficult-to-access highly-representative high-resolution imagery. To remediate this, we introduce here the WorldStrat dataset. The largest and most varied such publicly available dataset, at Airbus SPOT 6/7 satellites' high resolution of up to 1.5 m/pixel, empowered by European Space Agency's Phi-Lab as part of the ESA-funded QueryPlanet project, we curate nearly 10,000 sqkm of unique locations to ensure stratified representation of all types of land-use across the world: from agriculture to ice caps, from forests to multiple urbanization densities. We also enrich those with locations typically under-represented in ML datasets: sites of humanitarian interest, illegal mining sites, and settlements of persons at risk. We temporally-match each high-resolution image with multiple low-resolution images from the freely accessible lower-resolution Sentinel-2 satellites at 10 m/pixel. We accompany this dataset with an open-source Python package to: rebuild or extend the WorldStrat dataset, train and infer baseline algorithms, and learn with abundant tutorials, all compatible with the popular EO-learn toolbox. We hereby hope to foster broad-spectrum applications of ML to satellite imagery, and possibly develop from free public low-resolution Sentinel2 imagery the same power of analysis allowed by costly private high-resolution imagery. We illustrate this specific point by training and releasing several highly compute-efficient baselines on the task of Multi-Frame Super-Resolution. High-resolution Airbus imagery is CC BY-NC, while the labels and Sentinel2 imagery are CC BY, and the source code and pre-trained models under BSD. The dataset is available at https://zenodo.org/record/6810792 and the software package at https://github.com/worldstrat/worldstrat .



### Entry-Flipped Transformer for Inference and Prediction of Participant Behavior
- **Arxiv ID**: http://arxiv.org/abs/2207.06235v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06235v2)
- **Published**: 2022-07-13 14:31:09+00:00
- **Updated**: 2022-07-14 12:35:04+00:00
- **Authors**: Bo Hu, Tat-Jen Cham
- **Comment**: Accepted in ECCV 2022
- **Journal**: None
- **Summary**: Some group activities, such as team sports and choreographed dances, involve closely coupled interaction between participants. Here we investigate the tasks of inferring and predicting participant behavior, in terms of motion paths and actions, under such conditions. We narrow the problem to that of estimating how a set target participants react to the behavior of other observed participants. Our key idea is to model the spatio-temporal relations among participants in a manner that is robust to error accumulation during frame-wise inference and prediction. We propose a novel Entry-Flipped Transformer (EF-Transformer), which models the relations of participants by attention mechanisms on both spatial and temporal domains. Unlike typical transformers, we tackle the problem of error accumulation by flipping the order of query, key, and value entries, to increase the importance and fidelity of observed features in the current frame. Comparative experiments show that our EF-Transformer achieves the best performance on a newly-collected tennis doubles dataset, a Ceilidh dance dataset, and two pedestrian datasets. Furthermore, it is also demonstrated that our EF-Transformer is better at limiting accumulated errors and recovering from wrong estimations.



### SlimSeg: Slimmable Semantic Segmentation with Boundary Supervision
- **Arxiv ID**: http://arxiv.org/abs/2207.06242v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06242v2)
- **Published**: 2022-07-13 14:41:05+00:00
- **Updated**: 2023-02-16 21:59:51+00:00
- **Authors**: Danna Xue, Fei Yang, Pei Wang, Luis Herranz, Jinqiu Sun, Yu Zhu, Yanning Zhang
- **Comment**: 10 pages, 4 figures, accepted by ACMMM 2022, corrected the legend in
  fig.3
- **Journal**: None
- **Summary**: Accurate semantic segmentation models typically require significant computational resources, inhibiting their use in practical applications. Recent works rely on well-crafted lightweight models to achieve fast inference. However, these models cannot flexibly adapt to varying accuracy and efficiency requirements. In this paper, we propose a simple but effective slimmable semantic segmentation (SlimSeg) method, which can be executed at different capacities during inference depending on the desired accuracy-efficiency tradeoff. More specifically, we employ parametrized channel slimming by stepwise downward knowledge distillation during training. Motivated by the observation that the differences between segmentation results of each submodel are mainly near the semantic borders, we introduce an additional boundary guided semantic segmentation loss to further improve the performance of each submodel. We show that our proposed SlimSeg with various mainstream networks can produce flexible models that provide dynamic adjustment of computational cost and better performance than independent models. Extensive experiments on semantic segmentation benchmarks, Cityscapes and CamVid, demonstrate the generalization ability of our framework.



### Context-Consistent Semantic Image Editing with Style-Preserved Modulation
- **Arxiv ID**: http://arxiv.org/abs/2207.06252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06252v1)
- **Published**: 2022-07-13 14:49:00+00:00
- **Updated**: 2022-07-13 14:49:00+00:00
- **Authors**: Wuyang Luo, Su Yang, Hong Wang, Bo Long, Weishan Zhang
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Semantic image editing utilizes local semantic label maps to generate the desired content in the edited region. A recent work borrows SPADE block to achieve semantic image editing. However, it cannot produce pleasing results due to style discrepancy between the edited region and surrounding pixels. We attribute this to the fact that SPADE only uses an image-independent local semantic layout but ignores the image-specific styles included in the known pixels. To address this issue, we propose a style-preserved modulation (SPM) comprising two modulations processes: The first modulation incorporates the contextual style and semantic layout, and then generates two fused modulation parameters. The second modulation employs the fused parameters to modulate feature maps. By using such two modulations, SPM can inject the given semantic layout while preserving the image-specific context style. Moreover, we design a progressive architecture for generating the edited content in a coarse-to-fine manner. The proposed method can obtain context-consistent results and significantly alleviate the unpleasant boundary between the generated regions and the known pixels.



### Image warp preserving content intensity
- **Arxiv ID**: http://arxiv.org/abs/2207.06256v1
- **DOI**: 10.1137/21M1452688
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2207.06256v1)
- **Published**: 2022-07-13 14:55:46+00:00
- **Updated**: 2022-07-13 14:55:46+00:00
- **Authors**: Enrico Segre
- **Comment**: in production on SIAM Journal on Imaging Sciences
- **Journal**: SIAM Journal on Imaging Sciences Vol. 15, No. 4, pp. 1623-1645,
  2022
- **Summary**: An accurate method for warping images is presented. Differently from most commonly used techniques, this method guarantees the conservation of the intensity of the transformed image, evaluated as the sum of its pixel values over the whole image or over corresponding transformed subregions of it. Such property is mandatory for quantitative analysis, as, for instance, when deformed images are used to assess radiances, to measure optical fluxes from light sources, or to characterize material optical densities. The proposed method enforces area resampling by decomposing each rectangular pixel in two triangles, and projecting the pixel intensity onto half pixels of the transformed image, with weights proportional to the area of overlap of the triangular half-pixels. The result is quantitatively exact, as long as the original pixel value is assumed to represent a constant image density within the pixel area, and as long as the coordinate transformation is diffeomorphic. Implementation details and possible variations of the method are discussed.



### Is Appearance Free Action Recognition Possible?
- **Arxiv ID**: http://arxiv.org/abs/2207.06261v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.06261v1)
- **Published**: 2022-07-13 15:04:53+00:00
- **Updated**: 2022-07-13 15:04:53+00:00
- **Authors**: Filip Ilic, Thomas Pock, Richard P. Wildes
- **Comment**: None
- **Journal**: None
- **Summary**: Intuition might suggest that motion and dynamic information are key to video-based action recognition. In contrast, there is evidence that state-of-the-art deep-learning video understanding architectures are biased toward static information available in single frames. Presently, a methodology and corresponding dataset to isolate the effects of dynamic information in video are missing. Their absence makes it difficult to understand how well contemporary architectures capitalize on dynamic vs. static information. We respond with a novel Appearance Free Dataset (AFD) for action recognition. AFD is devoid of static information relevant to action recognition in a single frame. Modeling of the dynamics is necessary for solving the task, as the action is only apparent through consideration of the temporal dimension. We evaluated 11 contemporary action recognition architectures on AFD as well as its related RGB video. Our results show a notable decrease in performance for all architectures on AFD compared to RGB. We also conducted a complimentary study with humans that shows their recognition accuracy on AFD and RGB is very similar and much better than the evaluated architectures on AFD. Our results motivate a novel architecture that revives explicit recovery of optical flow, within a contemporary design for best performance on AFD and RGB.



### Organic Priors in Non-Rigid Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/2207.06262v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2207.06262v3)
- **Published**: 2022-07-13 15:07:50+00:00
- **Updated**: 2022-07-16 09:47:34+00:00
- **Authors**: Suryansh Kumar, Luc Van Gool
- **Comment**: To appear in ECCV 2022 Conference (Oral Presentation). Draft info: 18
  Pages, 4 Figures, and 6 Tables. Project webpage:
  https://suryanshkumar.github.io/Organic_Prior_NRSfM/
- **Journal**: None
- **Summary**: This paper advocates the use of organic priors in classical non-rigid structure from motion (NRSfM). By organic priors, we mean invaluable intermediate prior information intrinsic to the NRSfM matrix factorization theory. It is shown that such priors reside in the factorized matrices, and quite surprisingly, existing methods generally disregard them. The paper's main contribution is to put forward a simple, methodical, and practical method that can effectively exploit such organic priors to solve NRSfM. The proposed method does not make assumptions other than the popular one on the low-rank shape and offers a reliable solution to NRSfM under orthographic projection. Our work reveals that the accessibility of organic priors is independent of the camera motion and shape deformation type. Besides that, the paper provides insights into the NRSfM factorization -- both in terms of shape and motion -- and is the first approach to show the benefit of single rotation averaging for NRSfM. Furthermore, we outline how to effectively recover motion and non-rigid 3D shape using the proposed organic prior based approach and demonstrate results that outperform prior-free NRSfM performance by a significant margin. Finally, we present the benefits of our method via extensive experiments and evaluations on several benchmark datasets.



### Task Agnostic Representation Consolidation: a Self-supervised based Continual Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2207.06267v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.06267v1)
- **Published**: 2022-07-13 15:16:51+00:00
- **Updated**: 2022-07-13 15:16:51+00:00
- **Authors**: Prashant Bhat, Bahram Zonooz, Elahe Arani
- **Comment**: Accepted at Conference on Lifelong Learning Agents (CoLLAs 2022)
- **Journal**: None
- **Summary**: Continual learning (CL) over non-stationary data streams remains one of the long-standing challenges in deep neural networks (DNNs) as they are prone to catastrophic forgetting. CL models can benefit from self-supervised pre-training as it enables learning more generalizable task-agnostic features. However, the effect of self-supervised pre-training diminishes as the length of task sequences increases. Furthermore, the domain shift between pre-training data distribution and the task distribution reduces the generalizability of the learned representations. To address these limitations, we propose Task Agnostic Representation Consolidation (TARC), a two-stage training paradigm for CL that intertwines task-agnostic and task-specific learning whereby self-supervised training is followed by supervised learning for each task. To further restrict the deviation from the learned representations in the self-supervised stage, we employ a task-agnostic auxiliary loss during the supervised stage. We show that our training paradigm can be easily added to memory- or regularization-based approaches and provides consistent performance gain across more challenging CL settings. We further show that it leads to more robust and well-calibrated models.



### ACLNet: An Attention and Clustering-based Cloud Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/2207.06277v1
- **DOI**: 10.1080/2150704X.2022.2097031
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.06277v1)
- **Published**: 2022-07-13 15:22:50+00:00
- **Updated**: 2022-07-13 15:22:50+00:00
- **Authors**: Dhruv Makwana, Subhrajit Nag, Onkar Susladkar, Gayatri Deshmukh, Sai Chandra Teja R, Sparsh Mittal, C Krishna Mohan
- **Comment**: 11 pages, 3 figures, 5 tables, Published in remote sensing letters
- **Journal**: volume 13, pages 865-875, year 2022
- **Summary**: We propose a novel deep learning model named ACLNet, for cloud segmentation from ground images. ACLNet uses both deep neural network and machine learning (ML) algorithm to extract complementary features. Specifically, it uses EfficientNet-B0 as the backbone, "`a trous spatial pyramid pooling" (ASPP) to learn at multiple receptive fields, and "global attention module" (GAM) to extract finegrained details from the image. ACLNet also uses k-means clustering to extract cloud boundaries more precisely. ACLNet is effective for both daytime and nighttime images. It provides lower error rate, higher recall and higher F1-score than state-of-art cloud segmentation models. The source-code of ACLNet is available here: https://github.com/ckmvigil/ACLNet.



### Implicit Neural Representations for Generative Modeling of Living Cell Shapes
- **Arxiv ID**: http://arxiv.org/abs/2207.06283v2
- **DOI**: 10.1007/978-3-031-16440-8_6
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.06283v2)
- **Published**: 2022-07-13 15:28:07+00:00
- **Updated**: 2022-10-06 12:15:36+00:00
- **Authors**: David Wiesner, Julian Suk, Sven Dummer, David Svoboda, Jelmer M. Wolterink
- **Comment**: MICCAI 2022
- **Journal**: Medical Image Computing and Computer Assisted Intervention -
  MICCAI 2022
- **Summary**: Methods allowing the synthesis of realistic cell shapes could help generate training data sets to improve cell tracking and segmentation in biomedical images. Deep generative models for cell shape synthesis require a light-weight and flexible representation of the cell shape. However, commonly used voxel-based representations are unsuitable for high-resolution shape synthesis, and polygon meshes have limitations when modeling topology changes such as cell growth or mitosis. In this work, we propose to use level sets of signed distance functions (SDFs) to represent cell shapes. We optimize a neural network as an implicit neural representation of the SDF value at any point in a 3D+time domain. The model is conditioned on a latent code, thus allowing the synthesis of new and unseen shape sequences. We validate our approach quantitatively and qualitatively on C. elegans cells that grow and divide, and lung cancer cells with growing complex filopodial protrusions. Our results show that shape descriptors of synthetic cells resemble those of real cells, and that our model is able to generate topologically plausible sequences of complex cell shapes in 3D+time.



### PointNorm: Dual Normalization is All You Need for Point Cloud Analysis
- **Arxiv ID**: http://arxiv.org/abs/2207.06324v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06324v4)
- **Published**: 2022-07-13 16:24:47+00:00
- **Updated**: 2023-04-17 18:03:00+00:00
- **Authors**: Shen Zheng, Jinqian Pan, Changjie Lu, Gaurav Gupta
- **Comment**: IJCNN 2023
- **Journal**: None
- **Summary**: Point cloud analysis is challenging due to the irregularity of the point cloud data structure. Existing works typically employ the ad-hoc sampling-grouping operation of PointNet++, followed by sophisticated local and/or global feature extractors for leveraging the 3D geometry of the point cloud. Unfortunately, the sampling-grouping operations do not address the point cloud's irregularity, whereas the intricate local and/or global feature extractors led to poor computational efficiency. In this paper, we introduce a novel DualNorm module after the sampling-grouping operation to effectively and efficiently address the irregularity issue. The DualNorm module consists of Point Normalization, which normalizes the grouped points to the sampled points, and Reverse Point Normalization, which normalizes the sampled points to the grouped points. The proposed framework, PointNorm, utilizes local mean and global standard deviation to benefit from both local and global features while maintaining a faithful inference speed. Experiments show that we achieved excellent accuracy and efficiency on ModelNet40 classification, ScanObjectNN classification, ShapeNetPart Part Segmentation, and S3DIS Semantic Segmentation. Code is available at https://github.com/ShenZheng2000/PointNorm-for-Point-Cloud-Analysis.



### Left Ventricle Contouring of Apical Three-Chamber Views on 2D Echocardiography
- **Arxiv ID**: http://arxiv.org/abs/2207.06330v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.06330v1)
- **Published**: 2022-07-13 16:35:33+00:00
- **Updated**: 2022-07-13 16:35:33+00:00
- **Authors**: Alberto Gomez, Mihaela Porumb, Angela Mumith, Thierry Judge, Shan Gao, Woo-Jin Cho Kim, Jorge Oliveira, Agis Chartsias
- **Comment**: Submitted to MICCAI-ASMUS 2022
- **Journal**: None
- **Summary**: We propose a new method to automatically contour the left ventricle on 2D echocardiographic images. Unlike most existing segmentation methods, which are based on predicting segmentation masks, we focus at predicting the endocardial contour and the key landmark points within this contour (basal points and apex). This provides a representation that is closer to how experts perform manual annotations and hence produce results that are physiologically more plausible.   Our proposed method uses a two-headed network based on the U-Net architecture. One head predicts the 7 contour points, and the other head predicts a distance map to the contour. This approach was compared to the U-Net and to a point based approach, achieving performance gains of up to 30\% in terms of landmark localisation (<4.5mm) and distance to the ground truth contour (<3.5mm).



### Improved $α$-GAN architecture for generating 3D connected volumes with an application to radiosurgery treatment planning
- **Arxiv ID**: http://arxiv.org/abs/2207.11223v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.11223v1)
- **Published**: 2022-07-13 16:39:47+00:00
- **Updated**: 2022-07-13 16:39:47+00:00
- **Authors**: Sanaz Mohammadjafari, Mucahit Cevik, Ayse Basar
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have gained significant attention in several computer vision tasks for generating high-quality synthetic data. Various medical applications including diagnostic imaging and radiation therapy can benefit greatly from synthetic data generation due to data scarcity in the domain. However, medical image data is typically kept in 3D space, and generative models suffer from the curse of dimensionality issues in generating such synthetic data. In this paper, we investigate the potential of GANs for generating connected 3D volumes. We propose an improved version of 3D $\alpha$-GAN by incorporating various architectural enhancements. On a synthetic dataset of connected 3D spheres and ellipsoids, our model can generate fully connected 3D shapes with similar geometrical characteristics to that of training data. We also show that our 3D GAN model can successfully generate high-quality 3D tumor volumes and associated treatment specifications (e.g., isocenter locations). Similar moment invariants to the training data as well as fully connected 3D shapes confirm that improved 3D $\alpha$-GAN implicitly learns the training data distribution, and generates realistic-looking samples. The capability of improved 3D $\alpha$-GAN makes it a valuable source for generating synthetic medical image data that can help future research in this domain.



### Symmetry-Aware Transformer-based Mirror Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.06332v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06332v3)
- **Published**: 2022-07-13 16:40:01+00:00
- **Updated**: 2022-09-04 04:58:07+00:00
- **Authors**: Tianyu Huang, Bowen Dong, Jiaying Lin, Xiaohui Liu, Rynson W. H. Lau, Wangmeng Zuo
- **Comment**: None
- **Journal**: None
- **Summary**: Mirror detection aims to identify the mirror regions in the given input image. Existing works mainly focus on integrating the semantic features and structural features to mine specific relations between mirror and non-mirror regions, or introducing mirror properties like depth or chirality to help analyze the existence of mirrors. In this work, we observe that a real object typically forms a loose symmetry relationship with its corresponding reflection in the mirror, which is beneficial in distinguishing mirrors from real objects. Based on this observation, we propose a dual-path Symmetry-Aware Transformer-based mirror detection Network (SATNet), which includes two novel modules: Symmetry-Aware Attention Module (SAAM) and Contrast and Fusion Decoder Module (CFDM). Specifically, we first adopt a transformer backbone to model global information aggregation in images, extracting multi-scale features in two paths. We then feed the high-level dual-path features to SAAMs to capture the symmetry relations. Finally, we fuse the dual-path features and refine our prediction maps progressively with CFDMs to obtain the final mirror mask. Experimental results show that SATNet outperforms both RGB and RGB-D mirror detection methods on all available mirror detection datasets. Codes and trained models are available at: https://github.com/tyhuang0428/SATNet.



### 6D Camera Relocalization in Visually Ambiguous Extreme Environments
- **Arxiv ID**: http://arxiv.org/abs/2207.06333v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06333v1)
- **Published**: 2022-07-13 16:40:02+00:00
- **Updated**: 2022-07-13 16:40:02+00:00
- **Authors**: Yang Zheng, Tolga Birdal, Fei Xia, Yanchao Yang, Yueqi Duan, Leonidas J. Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel method to reliably estimate the pose of a camera given a sequence of images acquired in extreme environments such as deep seas or extraterrestrial terrains. Data acquired under these challenging conditions are corrupted by textureless surfaces, image degradation, and presence of repetitive and highly ambiguous structures. When naively deployed, the state-of-the-art methods can fail in those scenarios as confirmed by our empirical analysis. In this paper, we attempt to make camera relocalization work in these extreme situations. To this end, we propose: (i) a hierarchical localization system, where we leverage temporal information and (ii) a novel environment-aware image enhancement method to boost the robustness and accuracy. Our extensive experimental results demonstrate superior performance in favor of our method under two extreme settings: localizing an autonomous underwater vehicle and localizing a planetary rover in a Mars-like desert. In addition, our method achieves comparable performance with state-of-the-art methods on the indoor benchmark (7-Scenes dataset) using only 20% training data.



### You Only Align Once: Bidirectional Interaction for Spatial-Temporal Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2207.06345v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06345v1)
- **Published**: 2022-07-13 17:01:16+00:00
- **Updated**: 2022-07-13 17:01:16+00:00
- **Authors**: Mengshun Hu, Kui Jiang, Zhixiang Nie, Zheng Wang
- **Comment**: ACMMM 2022
- **Journal**: None
- **Summary**: Spatial-Temporal Video Super-Resolution (ST-VSR) technology generates high-quality videos with higher resolution and higher frame rates. Existing advanced methods accomplish ST-VSR tasks through the association of Spatial and Temporal video super-resolution (S-VSR and T-VSR). These methods require two alignments and fusions in S-VSR and T-VSR, which is obviously redundant and fails to sufficiently explore the information flow of consecutive spatial LR frames. Although bidirectional learning (future-to-past and past-to-future) was introduced to cover all input frames, the direct fusion of final predictions fails to sufficiently exploit intrinsic correlations of bidirectional motion learning and spatial information from all frames. We propose an effective yet efficient recurrent network with bidirectional interaction for ST-VSR, where only one alignment and fusion is needed. Specifically, it first performs backward inference from future to past, and then follows forward inference to super-resolve intermediate frames. The backward and forward inferences are assigned to learn structures and details to simplify the learning task with joint optimizations. Furthermore, a Hybrid Fusion Module (HFM) is designed to aggregate and distill information to refine spatial information and reconstruct high-quality video frames. Extensive experiments on two public datasets demonstrate that our method outperforms state-of-the-art methods in efficiency, and reduces calculation cost by about 22%.



### Joint Prediction of Monocular Depth and Structure using Planar and Parallax Geometry
- **Arxiv ID**: http://arxiv.org/abs/2207.06351v1
- **DOI**: 10.1016/j.patcog.2022.108806
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06351v1)
- **Published**: 2022-07-13 17:04:05+00:00
- **Updated**: 2022-07-13 17:04:05+00:00
- **Authors**: Hao Xing, Yifan Cao, Maximilian Biber, Mingchuan Zhou, Darius Burschka
- **Comment**: Pattern Recognition, May 2022
- **Journal**: None
- **Summary**: Supervised learning depth estimation methods can achieve good performance when trained on high-quality ground-truth, like LiDAR data. However, LiDAR can only generate sparse 3D maps which causes losing information. Obtaining high-quality ground-truth depth data per pixel is difficult to acquire. In order to overcome this limitation, we propose a novel approach combining structure information from a promising Plane and Parallax geometry pipeline with depth information into a U-Net supervised learning network, which results in quantitative and qualitative improvement compared to existing popular learning-based methods. In particular, the model is evaluated on two large-scale and challenging datasets: KITTI Vision Benchmark and Cityscapes dataset and achieve the best performance in terms of relative error. Compared with pure depth supervision models, our model has impressive performance on depth prediction of thin objects and edges, and compared to structure prediction baseline, our model performs more robustly.



### A General Framework for Partial to Full Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2207.06387v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06387v1)
- **Published**: 2022-07-13 17:44:49+00:00
- **Updated**: 2022-07-13 17:44:49+00:00
- **Authors**: Carlos Francisco Moreno-Garcia, Francesc Serratosa
- **Comment**: None
- **Journal**: None
- **Summary**: Image registration is a research field in which images must be compared and aligned independently of the point of view or camera characteristics. In some applications (such as forensic biometrics, satellite photography or outdoor scene identification) classical image registration systems fail due to one of the images compared represents a tiny piece of the other image. For instance, in forensics palmprint recognition, it is usual to find only a small piece of the palmprint, but in the database, the whole palmprint has been enrolled. The main reason of the poor behaviour of classical image registration methods is the gap between the amounts of salient points of both images, which is related to the number of points to be considered as outliers. Usually, the difficulty of finding a good match increases when the image that represents the tiny part of the scene has been drastically rotated. Again, in the case of palmprint forensics, it is difficult to decide a priori the orientation of the found tiny palmprint image. We present a rotation invariant registration method that explicitly considers that the image to be matched is a small piece of a larger image. We have experimentally validated our method in two different scenarios; palmprint identification and outdoor image registration.



### River Surface Patch-wise Detector Using Mixture Augmentation for Scum-cover-index
- **Arxiv ID**: http://arxiv.org/abs/2207.06388v4
- **DOI**: None
- **Categories**: **cs.CV**, I.4.7; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2207.06388v4)
- **Published**: 2022-07-13 17:45:25+00:00
- **Updated**: 2022-09-22 14:47:03+00:00
- **Authors**: Takato Yasuno, Junichiro Fujii, Masazumi Amakata
- **Comment**: 15 figures, 3 table
- **Journal**: 16th International Conference on Computer Vision, ICCV2022
  Vancouver, ID:22CA09ICCV007
- **Summary**: Urban rivers provide a water environment that influences residential living. River surface monitoring has become crucial for making decisions about where to prioritize cleaning and when to automatically start the cleaning treatment. We focus on the organic mud, or "scum", that accumulates on the river's surface and contributes to the river's odor and has external economic effects on the landscape. Because of its feature of a sparsely distributed and unstable pattern of organic shape, automating the monitoring process has proved difficult. We propose a patch-wise classification pipeline to detect scum features on the river surface using mixture image augmentation to increase the diversity between the scum floating on the river and the entangled background on the river surface reflected by nearby structures like buildings, bridges, poles, and barriers. Furthermore, we propose a scum-index cover on rivers to help monitor worse grade online, collect floating scum, and decide on chemical treatment policies. Finally, we demonstrate the application of our method on a time series dataset with frames every ten minutes recording river scum events over several days. We discuss the significance of our pipeline and its experimental findings.



### PyMAF-X: Towards Well-aligned Full-body Model Regression from Monocular Images
- **Arxiv ID**: http://arxiv.org/abs/2207.06400v3
- **DOI**: 10.1109/TPAMI.2023.3271691
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06400v3)
- **Published**: 2022-07-13 17:58:33+00:00
- **Updated**: 2023-04-28 02:33:10+00:00
- **Authors**: Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun, Yebin Liu
- **Comment**: Accepted to IEEE TPAMI, Project page:
  https://www.liuyebin.com/pymaf-x, An eXpressive extension of PyMAF
  [arXiv:2103.16507] for monocular human/hand/face/full-body mesh recovery
- **Journal**: None
- **Summary**: We present PyMAF-X, a regression-based approach to recovering parametric full-body models from monocular images. This task is very challenging since minor parametric deviation may lead to noticeable misalignment between the estimated mesh and the input image. Moreover, when integrating part-specific estimations into the full-body model, existing solutions tend to either degrade the alignment or produce unnatural wrist poses. To address these issues, we propose a Pyramidal Mesh Alignment Feedback (PyMAF) loop in our regression network for well-aligned human mesh recovery and extend it as PyMAF-X for the recovery of expressive full-body models. The core idea of PyMAF is to leverage a feature pyramid and rectify the predicted parameters explicitly based on the mesh-image alignment status. Specifically, given the currently predicted parameters, mesh-aligned evidence will be extracted from finer-resolution features accordingly and fed back for parameter rectification. To enhance the alignment perception, an auxiliary dense supervision is employed to provide mesh-image correspondence guidance while spatial alignment attention is introduced to enable the awareness of the global contexts for our network. When extending PyMAF for full-body mesh recovery, an adaptive integration strategy is proposed in PyMAF-X to produce natural wrist poses while maintaining the well-aligned performance of the part-specific estimations. The efficacy of our approach is validated on several benchmark datasets for body, hand, face, and full-body mesh recovery, where PyMAF and PyMAF-X effectively improve the mesh-image alignment and achieve new state-of-the-art results. The project page with code and video results can be found at https://www.liuyebin.com/pymaf-x.



### 3D Concept Grounding on Neural Fields
- **Arxiv ID**: http://arxiv.org/abs/2207.06403v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.06403v1)
- **Published**: 2022-07-13 17:59:33+00:00
- **Updated**: 2022-07-13 17:59:33+00:00
- **Authors**: Yining Hong, Yilun Du, Chunru Lin, Joshua B. Tenenbaum, Chuang Gan
- **Comment**: Project page: http://3d-cg.csail.mit.edu
- **Journal**: None
- **Summary**: In this paper, we address the challenging problem of 3D concept grounding (i.e. segmenting and learning visual concepts) by looking at RGBD images and reasoning about paired questions and answers. Existing visual reasoning approaches typically utilize supervised methods to extract 2D segmentation masks on which concepts are grounded. In contrast, humans are capable of grounding concepts on the underlying 3D representation of images. However, traditionally inferred 3D representations (e.g., point clouds, voxelgrids, and meshes) cannot capture continuous 3D features flexibly, thus making it challenging to ground concepts to 3D regions based on the language description of the object being referred to. To address both issues, we propose to leverage the continuous, differentiable nature of neural fields to segment and learn concepts. Specifically, each 3D coordinate in a scene is represented as a high-dimensional descriptor. Concept grounding can then be performed by computing the similarity between the descriptor vector of a 3D coordinate and the vector embedding of a language concept, which enables segmentations and concept learning to be jointly learned on neural fields in a differentiable fashion. As a result, both 3D semantic and instance segmentations can emerge directly from question answering supervision using a set of defined neural operators on top of neural fields (e.g., filtering and counting). Experimental results show that our proposed framework outperforms unsupervised/language-mediated segmentation models on semantic and instance segmentation tasks, as well as outperforms existing models on the challenging 3D aware visual reasoning tasks. Furthermore, our framework can generalize well to unseen shape categories and real scans.



### Graph CNN for Moving Object Detection in Complex Environments from Unseen Videos
- **Arxiv ID**: http://arxiv.org/abs/2207.06440v1
- **DOI**: 10.1109/ICCVW54120.2021.00030
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06440v1)
- **Published**: 2022-07-13 18:00:12+00:00
- **Updated**: 2022-07-13 18:00:12+00:00
- **Authors**: Jhony H. Giraldo, Sajid Javed, Naoufel Werghi, Thierry Bouwmans
- **Comment**: None
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV) Workshops, 2021, pp. 225-233
- **Summary**: Moving Object Detection (MOD) is a fundamental step for many computer vision applications. MOD becomes very challenging when a video sequence captured from a static or moving camera suffers from the challenges: camouflage, shadow, dynamic backgrounds, and lighting variations, to name a few. Deep learning methods have been successfully applied to address MOD with competitive performance. However, in order to handle the overfitting problem, deep learning methods require a large amount of labeled data which is a laborious task as exhaustive annotations are always not available. Moreover, some MOD deep learning methods show performance degradation in the presence of unseen video sequences because the testing and training splits of the same sequences are involved during the network learning process. In this work, we pose the problem of MOD as a node classification problem using Graph Convolutional Neural Networks (GCNNs). Our algorithm, dubbed as GraphMOD-Net, encompasses instance segmentation, background initialization, feature extraction, and graph construction. GraphMOD-Net is tested on unseen videos and outperforms state-of-the-art methods in unsupervised, semi-supervised, and supervised learning in several challenges of the Change Detection 2014 (CDNet2014) and UCSD background subtraction datasets.



### Rich Feature Distillation with Feature Affinity Module for Efficient Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2207.11250v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.11250v1)
- **Published**: 2022-07-13 18:32:44+00:00
- **Updated**: 2022-07-13 18:32:44+00:00
- **Authors**: Sai Mitheran, Anushri Suresh, Nisha J. S., Varun P. Gopi
- **Comment**: Preprint version. Accepted at Optik
- **Journal**: None
- **Summary**: Single-image haze removal is a long-standing hurdle for computer vision applications. Several works have been focused on transferring advances from image classification, detection, and segmentation to the niche of image dehazing, primarily focusing on contrastive learning and knowledge distillation. However, these approaches prove computationally expensive, raising concern regarding their applicability to on-the-edge use-cases. This work introduces a simple, lightweight, and efficient framework for single-image haze removal, exploiting rich "dark-knowledge" information from a lightweight pre-trained super-resolution model via the notion of heterogeneous knowledge distillation. We designed a feature affinity module to maximize the flow of rich feature semantics from the super-resolution teacher to the student dehazing network. In order to evaluate the efficacy of our proposed framework, its performance as a plug-and-play setup to a baseline model is examined. Our experiments are carried out on the RESIDE-Standard dataset to demonstrate the robustness of our framework to the synthetic and real-world domains. The extensive qualitative and quantitative results provided establish the effectiveness of the framework, achieving gains of upto 15\% (PSNR) while reducing the model size by $\sim$20 times.



### Imaging through the Atmosphere using Turbulence Mitigation Transformer
- **Arxiv ID**: http://arxiv.org/abs/2207.06465v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.06465v1)
- **Published**: 2022-07-13 18:33:26+00:00
- **Updated**: 2022-07-13 18:33:26+00:00
- **Authors**: Xingguang Zhang, Zhiyuan Mao, Nicholas Chimitt, Stanley H. Chan
- **Comment**: 13 pages, 12 figures, project page: https://xg416.github.io/TMT/
- **Journal**: None
- **Summary**: Restoring images distorted by atmospheric turbulence is a long-standing problem due to the spatially varying nature of the distortion, nonlinearity of the image formation process, and scarcity of training and testing data. Existing methods often have strong statistical assumptions on the distortion model which in many cases will lead to a limited performance in real-world scenarios as they do not generalize. To overcome the challenge, this paper presents an end-to-end physics-driven approach that is efficient and can generalize to real-world turbulence. On the data synthesis front, we significantly increase the image resolution that can be handled by the SOTA turbulence simulator by approximating the random field via wide-sense stationarity. The new data synthesis process enables the generation of large-scale multi-level turbulence and ground truth pairs for training. On the network design front, we propose the turbulence mitigation transformer (TMT), a two stage U-Net shaped multi-frame restoration network which has a noval efficient self-attention mechanism named temporal channel joint attention (TCJA). We also introduce a new training scheme that is enabled by the new simulator, and we design new transformer units to reduce the memory consumption. Experimental results on both static and dynamic scenes are promising, including various real turbulence scenarios.



### A Data-Efficient Deep Learning Framework for Segmentation and Classification of Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/2207.06489v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.06489v5)
- **Published**: 2022-07-13 19:23:49+00:00
- **Updated**: 2022-10-22 08:51:17+00:00
- **Authors**: Pranav Singh, Jacopo Cirrone
- **Comment**: Originally published at the ECCV 2022 Medical Computer Vision
  Workshop (ECCV-MCV 2022)
- **Journal**: None
- **Summary**: The current study of cell architecture of inflammation in histopathology images commonly performed for diagnosis and research purposes excludes a lot of information available on the biopsy slide. In autoimmune diseases, major outstanding research questions remain regarding which cell types participate in inflammation at the tissue level, and how they interact with each other. While these questions can be partially answered using traditional methods, artificial intelligence approaches for segmentation and classification provide a much more efficient method to understand the architecture of inflammation in autoimmune disease, holding great promise for novel insights. In this paper, we empirically develop deep learning approaches that use dermatomyositis biopsies of human tissue to detect and identify inflammatory cells. Our approach improves classification performance by 26% and segmentation performance by 5%. We also propose a novel post-processing autoencoder architecture that improves segmentation performance by an additional 3%.



### One Model to Unite Them All: Personalized Federated Learning of Multi-Contrast MRI Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2207.06509v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.06509v2)
- **Published**: 2022-07-13 20:14:16+00:00
- **Updated**: 2022-08-23 07:43:42+00:00
- **Authors**: Onat Dalmaz, Usama Mirza, Gökberk Elmas, Muzaffer Özbey, Salman UH Dar, Emir Ceyani, Salman Avestimehr, Tolga Çukur
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-institutional collaborations are key for learning generalizable MRI synthesis models that translate source- onto target-contrast images. To facilitate collaboration, federated learning (FL) adopts decentralized training and mitigates privacy concerns by avoiding sharing of imaging data. However, FL-trained synthesis models can be impaired by the inherent heterogeneity in the data distribution, with domain shifts evident when common or variable translation tasks are prescribed across sites. Here we introduce the first personalized FL method for MRI Synthesis (pFLSynth) to improve reliability against domain shifts. pFLSynth is based on an adversarial model that produces latents specific to individual sites and source-target contrasts, and leverages novel personalization blocks to adaptively tune the statistics and weighting of feature maps across the generator stages given latents. To further promote site specificity, partial model aggregation is employed over downstream layers of the generator while upstream layers are retained locally. As such, pFLSynth enables training of a unified synthesis model that can reliably generalize across multiple sites and translation tasks. Comprehensive experiments on multi-site datasets clearly demonstrate the enhanced performance of pFLSynth against prior federated methods in multi-contrast MRI synthesis.



### Lipschitz Continuity Retained Binary Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2207.06540v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.06540v2)
- **Published**: 2022-07-13 22:55:04+00:00
- **Updated**: 2022-07-16 04:07:25+00:00
- **Authors**: Yuzhang Shang, Dan Xu, Bin Duan, Ziliang Zong, Liqiang Nie, Yan Yan
- **Comment**: Paper accepted to ECCV 2022
- **Journal**: None
- **Summary**: Relying on the premise that the performance of a binary neural network can be largely restored with eliminated quantization error between full-precision weight vectors and their corresponding binary vectors, existing works of network binarization frequently adopt the idea of model robustness to reach the aforementioned objective. However, robustness remains to be an ill-defined concept without solid theoretical support. In this work, we introduce the Lipschitz continuity, a well-defined functional property, as the rigorous criteria to define the model robustness for BNN. We then propose to retain the Lipschitz continuity as a regularization term to improve the model robustness. Particularly, while the popular Lipschitz-involved regularization methods often collapse in BNN due to its extreme sparsity, we design the Retention Matrices to approximate spectral norms of the targeted weight matrices, which can be deployed as the approximation for the Lipschitz constant of BNNs without the exact Lipschitz constant computation (NP-hard). Our experiments prove that our BNN-specific regularization method can effectively strengthen the robustness of BNN (testified on ImageNet-C), achieving state-of-the-art performance on CIFAR and ImageNet.



### Body Composition Assessment with Limited Field-of-view Computed Tomography: A Semantic Image Extension Perspective
- **Arxiv ID**: http://arxiv.org/abs/2207.06551v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.06551v2)
- **Published**: 2022-07-13 23:19:22+00:00
- **Updated**: 2023-04-16 00:40:40+00:00
- **Authors**: Kaiwen Xu, Thomas Li, Mirza S. Khan, Riqiang Gao, Sanja L. Antic, Yuankai Huo, Kim L. Sandler, Fabien Maldonado, Bennett A. Landman
- **Comment**: Updated with additional evaluation and clarification
- **Journal**: None
- **Summary**: Field-of-view (FOV) tissue truncation beyond the lungs is common in routine lung screening computed tomography (CT). This poses limitations for opportunistic CT- based body composition (BC) assessment as key anatomical structures are missing. Traditionally, extending the FOV of CT is considered as a CT reconstruction problem using limited data. However, this approach relies on the projection domain data which might not be available in application. In this work, we formulate the problem from the semantic image extension perspective which only requires image data as inputs. The proposed two-stage method identifies a new FOV border based on the estimated extent of the complete body and imputes missing tissues in the truncated region. The training samples are simulated using CT slices with complete body in FOV, making the model development self-supervised. We evaluate the validity of the proposed method in automatic BC assessment using lung screening CT with limited FOV. The proposed method effectively restores the missing tissues and reduces BC assessment error introduced by FOV tissue truncation. In the BC assessment for a large-scale lung screening CT dataset, this correction improves both the intra-subject consistency and the correlation with anthropometric approximations. The developed method is available at https://github.com/MASILab/S-EFOV.



### QML for Argoverse 2 Motion Forecasting Challenge
- **Arxiv ID**: http://arxiv.org/abs/2207.06553v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.06553v1)
- **Published**: 2022-07-13 23:25:30+00:00
- **Updated**: 2022-07-13 23:25:30+00:00
- **Authors**: Tong Su, Xishun Wang, Xiaodong Yang
- **Comment**: None
- **Journal**: None
- **Summary**: To safely navigate in various complex traffic scenarios, autonomous driving systems are generally equipped with a motion forecasting module to provide vital information for the downstream planning module. For the real-world onboard applications, both accuracy and latency of a motion forecasting model are essential. In this report, we present an effective and efficient solution, which ranks the 3rd place in the Argoverse 2 Motion Forecasting Challenge 2022.



### Supervised Attribute Information Removal and Reconstruction for Image Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2207.06555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.06555v1)
- **Published**: 2022-07-13 23:30:44+00:00
- **Updated**: 2022-07-13 23:30:44+00:00
- **Authors**: Nannan Li, Bryan A. Plummer
- **Comment**: Accepted at ECCV 2022
- **Journal**: None
- **Summary**: The goal of attribute manipulation is to control specified attribute(s) in given images. Prior work approaches this problem by learning disentangled representations for each attribute that enables it to manipulate the encoded source attributes to the target attributes. However, encoded attributes are often correlated with relevant image content. Thus, the source attribute information can often be hidden in the disentangled features, leading to unwanted image editing effects. In this paper, we propose an Attribute Information Removal and Reconstruction (AIRR) network that prevents such information hiding by learning how to remove the attribute information entirely, creating attribute excluded features, and then learns to directly inject the desired attributes in a reconstructed image. We evaluate our approach on four diverse datasets with a variety of attributes including DeepFashion Synthesis, DeepFashion Fine-grained Attribute, CelebA and CelebA-HQ, where our model improves attribute manipulation accuracy and top-k retrieval rate by 10% on average over prior work. A user study also reports that AIRR manipulated images are preferred over prior work in up to 76% of cases.



### Improving the diagnosis of breast cancer based on biophysical ultrasound features utilizing machine learning
- **Arxiv ID**: http://arxiv.org/abs/2207.06560v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2207.06560v1)
- **Published**: 2022-07-13 23:53:09+00:00
- **Updated**: 2022-07-13 23:53:09+00:00
- **Authors**: Jihye Baek, Avice M. O'Connell, Kevin J. Parker
- **Comment**: None
- **Journal**: None
- **Summary**: The improved diagnostic accuracy of ultrasound breast examinations remains an important goal. In this study, we propose a biophysical feature based machine learning method for breast cancer detection to improve the performance beyond a benchmark deep learning algorithm and to furthermore provide a color overlay visual map of the probability of malignancy within a lesion. This overall framework is termed disease specific imaging. Previously, 150 breast lesions were segmented and classified utilizing a modified fully convolutional network and a modified GoogLeNet, respectively. In this study multiparametric analysis was performed within the contoured lesions. Features were extracted from ultrasound radiofrequency, envelope, and log compressed data based on biophysical and morphological models. The support vector machine with a Gaussian kernel constructed a nonlinear hyperplane, and we calculated the distance between the hyperplane and data point of each feature in multiparametric space. The distance can quantitatively assess a lesion, and suggest the probability of malignancy that is color coded and overlaid onto B mode images. Training and evaluation were performed on in vivo patient data. The overall accuracy for the most common types and sizes of breast lesions in our study exceeded 98.0% for classification and 0.98 for an area under the receiver operating characteristic curve, which is more precise than the performance of radiologists and a deep learning system. Further, the correlation between the probability and BI RADS enables a quantitative guideline to predict breast cancer. Therefore, we anticipate that the proposed framework can help radiologists achieve more accurate and convenient breast cancer classification and detection.



