# Arxiv Papers in cs.CV on 2022-07-02
### Turning to a Teacher for Timestamp Supervised Temporal Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.00712v1
- **DOI**: 10.1109/ICME52920.2022.9859626
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00712v1)
- **Published**: 2022-07-02 02:00:55+00:00
- **Updated**: 2022-07-02 02:00:55+00:00
- **Authors**: Yang Zhao, Yan Song
- **Comment**: 6 pages, ICME 2022 oral
- **Journal**: None
- **Summary**: Temporal action segmentation in videos has drawn much attention recently. Timestamp supervision is a cost-effective way for this task. To obtain more information to optimize the model, the existing method generated pseudo frame-wise labels iteratively based on the output of a segmentation model and the timestamp annotations. However, this practice may introduce noise and oscillation during the training, and lead to performance degeneration. To address this problem, we propose a new framework for timestamp supervised temporal action segmentation by introducing a teacher model parallel to the segmentation model to help stabilize the process of model optimization. The teacher model can be seen as an ensemble of the segmentation model, which helps to suppress the noise and to improve the stability of pseudo labels. We further introduce a segmentally smoothing loss, which is more focused and cohesive, to enforce the smooth transition of the predicted probabilities within action instances. The experiments on three datasets show that our method outperforms the state-of-the-art method and performs comparably against the fully-supervised methods at a much lower annotation cost.



### Noise and Edge Based Dual Branch Image Manipulation Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.00724v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00724v1)
- **Published**: 2022-07-02 03:28:51+00:00
- **Updated**: 2022-07-02 03:28:51+00:00
- **Authors**: Zhongyuan Zhang, Yi Qian, Yanxiang Zhao, Lin Zhu, Jinjin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Unlike ordinary computer vision tasks that focus more on the semantic content of images, the image manipulation detection task pays more attention to the subtle information of image manipulation. In this paper, the noise image extracted by the improved constrained convolution is used as the input of the model instead of the original image to obtain more subtle traces of manipulation. Meanwhile, the dual-branch network, consisting of a high-resolution branch and a context branch, is used to capture the traces of artifacts as much as possible. In general, most manipulation leaves manipulation artifacts on the manipulation edge. A specially designed manipulation edge detection module is constructed based on the dual-branch network to identify these artifacts better. The correlation between pixels in an image is closely related to their distance. The farther the two pixels are, the weaker the correlation. We add a distance factor to the self-attention module to better describe the correlation between pixels. Experimental results on four publicly available image manipulation datasets demonstrate the effectiveness of our model.



### Multi-scale Attentive Image De-raining Networks via Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2207.00728v3
- **DOI**: 10.1109/TCSVT.2022.3207516
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.00728v3)
- **Published**: 2022-07-02 03:47:13+00:00
- **Updated**: 2023-04-04 12:41:44+00:00
- **Authors**: Lei Cai, Yuli Fu, Wanliang Huo, Youjun Xiang, Tao Zhu, Ying Zhang, Huanqiang Zeng, Delu Zeng
- **Comment**: None
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology,
  vol.33, no.2, pp.618-633 September 2022
- **Summary**: Multi-scale architectures and attention modules have shown effectiveness in many deep learning-based image de-raining methods. However, manually designing and integrating these two components into a neural network requires a bulk of labor and extensive expertise. In this article, a high-performance multi-scale attentive neural architecture search (MANAS) framework is technically developed for image deraining. The proposed method formulates a new multi-scale attention search space with multiple flexible modules that are favorite to the image de-raining task. Under the search space, multi-scale attentive cells are built, which are further used to construct a powerful image de-raining network. The internal multiscale attentive architecture of the de-raining network is searched automatically through a gradient-based search algorithm, which avoids the daunting procedure of the manual design to some extent. Moreover, in order to obtain a robust image de-raining model, a practical and effective multi-to-one training strategy is also presented to allow the de-raining network to get sufficient background information from multiple rainy images with the same background scene, and meanwhile, multiple loss functions including external loss, internal loss, architecture regularization loss, and model complexity loss are jointly optimized to achieve robust de-raining performance and controllable model complexity. Extensive experimental results on both synthetic and realistic rainy images, as well as the down-stream vision applications (i.e., objection detection and segmentation) consistently demonstrate the superiority of our proposed method. The code is publicly available at https://github.com/lcai-gz/MANAS.



### SketchCleanNet -- A deep learning approach to the enhancement and correction of query sketches for a 3D CAD model retrieval system
- **Arxiv ID**: http://arxiv.org/abs/2207.00732v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.00732v1)
- **Published**: 2022-07-02 04:08:11+00:00
- **Updated**: 2022-07-02 04:08:11+00:00
- **Authors**: Bharadwaj Manda, Prasad Kendre, Subhrajit Dey, Ramanathan Muthuganapathy
- **Comment**: None
- **Journal**: None
- **Summary**: Search and retrieval remains a major research topic in several domains, including computer graphics, computer vision, engineering design, etc. A search engine requires primarily an input search query and a database of items to search from. In engineering, which is the primary context of this paper, the database consists of 3D CAD models, such as washers, pistons, connecting rods, etc. A query from a user is typically in the form of a sketch, which attempts to capture the details of a 3D model. However, sketches have certain typical defects such as gaps, over-drawn portions (multi-strokes), etc. Since the retrieved results are only as good as the input query, sketches need cleaning-up and enhancement for better retrieval results.   In this paper, a deep learning approach is proposed to improve or clean the query sketches. Initially, sketches from various categories are analysed in order to understand the many possible defects that may occur. A dataset of cleaned-up or enhanced query sketches is then created based on an understanding of these defects. Consequently, an end-to-end training of a deep neural network is carried out in order to provide a mapping between the defective and the clean sketches. This network takes the defective query sketch as the input and generates a clean or an enhanced query sketch. Qualitative and quantitative comparisons of the proposed approach with other state-of-the-art techniques show that the proposed approach is effective. The results of the search engine are reported using both the defective and enhanced query sketches, and it is shown that using the enhanced query sketches from the developed approach yields improved search results.



### Contrastive Cross-Modal Knowledge Sharing Pre-training for Vision-Language Representation Learning and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2207.00733v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.00733v2)
- **Published**: 2022-07-02 04:08:44+00:00
- **Updated**: 2022-07-08 15:28:15+00:00
- **Authors**: Keyu Wen, Zhenshan Tan, Qingrong Cheng, Cheng Chen, Xiaodong Gu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the cross-modal pre-training task has been a hotspot because of its wide application in various down-streaming researches including retrieval, captioning, question answering and so on. However, exiting methods adopt a one-stream pre-training model to explore the united vision-language representation for conducting cross-modal retrieval, which easily suffer from the calculation explosion. Moreover, although the conventional double-stream structures are quite efficient, they still lack the vital cross-modal interactions, resulting in low performances. Motivated by these challenges, we put forward a Contrastive Cross-Modal Knowledge Sharing Pre-training (COOKIE) to grasp the joint text-image representations. Structurally, COOKIE adopts the traditional double-stream structure because of the acceptable time consumption. To overcome the inherent defects of double-stream structure as mentioned above, we elaborately design two effective modules. Concretely, the first module is a weight-sharing transformer that builds on the head of the visual and textual encoders, aiming to semantically align text and image. This design enables visual and textual paths focus on the same semantics. The other one is three specially designed contrastive learning, aiming to share knowledge between different models. The shared cross-modal knowledge develops the study of unimodal representation greatly, promoting the single-modal retrieval tasks. Extensive experimental results on multi-modal matching researches that includes cross-modal retrieval, text matching, and image retrieval reveal the superiors in calculation efficiency and statistical indicators of our pre-training model.



### Golfer: Trajectory Prediction with Masked Goal Conditioning MnM Network
- **Arxiv ID**: http://arxiv.org/abs/2207.00738v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.00738v1)
- **Published**: 2022-07-02 04:57:44+00:00
- **Updated**: 2022-07-02 04:57:44+00:00
- **Authors**: Xiaocheng Tang, Soheil Sadeghi Eshkevari, Haoyu Chen, Weidan Wu, Wei Qian, Xiaoming Wang
- **Comment**: A preliminary version is presented at CVPR 2022 Workshop on
  Autonomous Driving https://cvpr2022.wad.vision
- **Journal**: None
- **Summary**: Transformers have enabled breakthroughs in NLP and computer vision, and have recently began to show promising performance in trajectory prediction for Autonomous Vehicle (AV). How to efficiently model the interactive relationships between the ego agent and other road and dynamic objects remains challenging for the standard attention module. In this work we propose a general Transformer-like architectural module MnM network equipped with novel masked goal conditioning training procedures for AV trajectory prediction. The resulted model, named golfer, achieves state-of-the-art performance, winning the 2nd place in the 2022 Waymo Open Dataset Motion Prediction Challenge and ranked 1st place according to minADE.



### CoVA: Exploiting Compressed-Domain Analysis to Accelerate Video Analytics
- **Arxiv ID**: http://arxiv.org/abs/2207.00588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00588v1)
- **Published**: 2022-07-02 05:55:29+00:00
- **Updated**: 2022-07-02 05:55:29+00:00
- **Authors**: Jinwoo Hwang, Minsu Kim, Daeun Kim, Seungho Nam, Yoonsung Kim, Dohee Kim, Hardik Sharma, Jongse Park
- **Comment**: ATC 2022
- **Journal**: None
- **Summary**: Modern retrospective analytics systems leverage cascade architecture to mitigate bottleneck for computing deep neural networks (DNNs). However, the existing cascades suffer two limitations: (1) decoding bottleneck is either neglected or circumvented, paying significant compute and storage cost for pre-processing; and (2) the systems are specialized for temporal queries and lack spatial query support. This paper presents CoVA, a novel cascade architecture that splits the cascade computation between compressed domain and pixel domain to address the decoding bottleneck, supporting both temporal and spatial queries. CoVA cascades analysis into three major stages where the first two stages are performed in compressed domain while the last one in pixel domain. First, CoVA detects occurrences of moving objects (called blobs) over a set of compressed frames (called tracks). Then, using the track results, CoVA prudently selects a minimal set of frames to obtain the label information and only decode them to compute the full DNNs, alleviating the decoding bottleneck. Lastly, CoVA associates tracks with labels to produce the final analysis results on which users can process both temporal and spatial queries. Our experiments demonstrate that CoVA offers 4.8x throughput improvement over modern cascade systems, while imposing modest accuracy loss.



### Gaussian Kernel-based Cross Modal Network for Spatio-Temporal Video Grounding
- **Arxiv ID**: http://arxiv.org/abs/2207.00744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00744v1)
- **Published**: 2022-07-02 05:59:28+00:00
- **Updated**: 2022-07-02 05:59:28+00:00
- **Authors**: Zeyu Xiong, Daizong Liu, Pan Zhou
- **Comment**: Accepted by ICIP2022
- **Journal**: None
- **Summary**: Spatial-Temporal Video Grounding (STVG) is a challenging task which aims to localize the spatio-temporal tube of the interested object semantically according to a natural language query. Most previous works not only severely rely on the anchor boxes extracted by Faster R-CNN, but also simply regard the video as a series of individual frames, thus lacking their temporal modeling. Instead, in this paper, we are the first to propose an anchor-free framework for STVG, called Gaussian Kernel-based Cross Modal Network (GKCMN). Specifically, we utilize the learned Gaussian Kernel-based heatmaps of each video frame to locate the query-related object. A mixed serial and parallel connection network is further developed to leverage both spatial and temporal relations among frames for better grounding. Experimental results on VidSTG dataset demonstrate the effectiveness of our proposed GKCMN.



### PhotoScene: Photorealistic Material and Lighting Transfer for Indoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/2207.00757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00757v1)
- **Published**: 2022-07-02 06:52:44+00:00
- **Updated**: 2022-07-02 06:52:44+00:00
- **Authors**: Yu-Ying Yeh, Zhengqin Li, Yannick Hold-Geoffroy, Rui Zhu, Zexiang Xu, Miloš Hašan, Kalyan Sunkavalli, Manmohan Chandraker
- **Comment**: Accepted to CVPR 2022; Code is available at
  https://github.com/ViLab-UCSD/photoscene
- **Journal**: None
- **Summary**: Most indoor 3D scene reconstruction methods focus on recovering 3D geometry and scene layout. In this work, we go beyond this to propose PhotoScene, a framework that takes input image(s) of a scene along with approximately aligned CAD geometry (either reconstructed automatically or manually specified) and builds a photorealistic digital twin with high-quality materials and similar lighting. We model scene materials using procedural material graphs; such graphs represent photorealistic and resolution-independent materials. We optimize the parameters of these graphs and their texture scale and rotation, as well as the scene lighting to best match the input image via a differentiable rendering layer. We evaluate our technique on objects and layout reconstructions from ScanNet, SUN RGB-D and stock photographs, and demonstrate that our method reconstructs high-quality, fully relightable 3D scenes that can be re-rendered under arbitrary viewpoints, zooms and lighting.



### Backdoor Attack is a Devil in Federated GAN-based Medical Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2207.00762v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.00762v2)
- **Published**: 2022-07-02 07:20:35+00:00
- **Updated**: 2022-07-30 22:43:04+00:00
- **Authors**: Ruinan Jin, Xiaoxiao Li
- **Comment**: 13 pages, 4 figures, Accepted by MICCAI 2022 SASHIMI Workshop
- **Journal**: None
- **Summary**: Deep Learning-based image synthesis techniques have been applied in healthcare research for generating medical images to support open research. Training generative adversarial neural networks (GAN) usually requires large amounts of training data. Federated learning (FL) provides a way of training a central model using distributed data from different medical institutions while keeping raw data locally. However, FL is vulnerable to backdoor attack, an adversarial by poisoning training data, given the central server cannot access the original data directly. Most backdoor attack strategies focus on classification models and centralized domains. In this study, we propose a way of attacking federated GAN (FedGAN) by treating the discriminator with a commonly used data poisoning strategy in backdoor attack classification models. We demonstrate that adding a small trigger with size less than 0.5 percent of the original image size can corrupt the FL-GAN model. Based on the proposed attack, we provide two effective defense strategies: global malicious detection and local training regularization. We show that combining the two defense strategies yields a robust medical image generation.



### Test-time Adaptation with Calibration of Medical Image Classification Nets for Label Distribution Shift
- **Arxiv ID**: http://arxiv.org/abs/2207.00769v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.00769v2)
- **Published**: 2022-07-02 07:55:23+00:00
- **Updated**: 2022-07-09 09:37:38+00:00
- **Authors**: Wenao Ma, Cheng Chen, Shuang Zheng, Jing Qin, Huimao Zhang, Qi Dou
- **Comment**: This paper has been accepted by MICCAI 2022
- **Journal**: None
- **Summary**: Class distribution plays an important role in learning deep classifiers. When the proportion of each class in the test set differs from the training set, the performance of classification nets usually degrades. Such a label distribution shift problem is common in medical diagnosis since the prevalence of disease vary over location and time. In this paper, we propose the first method to tackle label shift for medical image classification, which effectively adapt the model learned from a single training label distribution to arbitrary unknown test label distribution. Our approach innovates distribution calibration to learn multiple representative classifiers, which are capable of handling different one-dominating-class distributions. When given a test image, the diverse classifiers are dynamically aggregated via the consistency-driven test-time adaptation, to deal with the unknown test label distribution. We validate our method on two important medical image classification tasks including liver fibrosis staging and COVID-19 severity prediction. Our experiments clearly show the decreased model performance under label shift. With our method, model performance significantly improves on all the test datasets with different label shifts for both medical image diagnosis tasks.



### Learning Cross-Image Object Semantic Relation in Transformer for Few-Shot Fine-Grained Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2207.00784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00784v1)
- **Published**: 2022-07-02 09:43:38+00:00
- **Updated**: 2022-07-02 09:43:38+00:00
- **Authors**: Bo Zhang, Jiakang Yuan, Baopu Li, Tao Chen, Jiayuan Fan, Botian Shi
- **Comment**: Accepted by ACM MM-2022
- **Journal**: None
- **Summary**: Few-shot fine-grained learning aims to classify a query image into one of a set of support categories with fine-grained differences. Although learning different objects' local differences via Deep Neural Networks has achieved success, how to exploit the query-support cross-image object semantic relations in Transformer-based architecture remains under-explored in the few-shot fine-grained scenario. In this work, we propose a Transformer-based double-helix model, namely HelixFormer, to achieve the cross-image object semantic relation mining in a bidirectional and symmetrical manner. The HelixFormer consists of two steps: 1) Relation Mining Process (RMP) across different branches, and 2) Representation Enhancement Process (REP) within each individual branch. By the designed RMP, each branch can extract fine-grained object-level Cross-image Semantic Relation Maps (CSRMs) using information from the other branch, ensuring better cross-image interaction in semantically related local object regions. Further, with the aid of CSRMs, the developed REP can strengthen the extracted features for those discovered semantically-related local regions in each branch, boosting the model's ability to distinguish subtle feature differences of fine-grained objects. Extensive experiments conducted on five public fine-grained benchmarks demonstrate that HelixFormer can effectively enhance the cross-image object semantic relation matching for recognizing fine-grained objects, achieving much better performance over most state-of-the-art methods under 1-shot and 5-shot scenarios. Our code is available at: https://github.com/JiakangYuan/HelixFormer



### Object Representations as Fixed Points: Training Iterative Refinement Algorithms with Implicit Differentiation
- **Arxiv ID**: http://arxiv.org/abs/2207.00787v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2207.00787v3)
- **Published**: 2022-07-02 10:00:35+00:00
- **Updated**: 2023-01-01 02:31:43+00:00
- **Authors**: Michael Chang, Thomas L. Griffiths, Sergey Levine
- **Comment**: 19 pages, 13 figures, Accepted to the 36th Conference on Neural
  Information Processing Systems (NeurIPS 2022)
- **Journal**: None
- **Summary**: Iterative refinement -- start with a random guess, then iteratively improve the guess -- is a useful paradigm for representation learning because it offers a way to break symmetries among equally plausible explanations for the data. This property enables the application of such methods to infer representations of sets of entities, such as objects in physical scenes, structurally resembling clustering algorithms in latent space. However, most prior works differentiate through the unrolled refinement process, which can make optimization challenging. We observe that such methods can be made differentiable by means of the implicit function theorem, and develop an implicit differentiation approach that improves the stability and tractability of training by decoupling the forward and backward passes. This connection enables us to apply advances in optimizing implicit layers to not only improve the optimization of the slot attention module in SLATE, a state-of-the-art method for learning entity representations, but do so with constant space and time complexity in backpropagation and only one additional line of code.



### Boundary-Guided Camouflaged Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.00794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00794v1)
- **Published**: 2022-07-02 10:48:35+00:00
- **Updated**: 2022-07-02 10:48:35+00:00
- **Authors**: Yujia Sun, Shuo Wang, Chenglizhao Chen, Tian-Zhu Xiang
- **Comment**: Accepted by IJCAI2022
- **Journal**: IJCAI2022
- **Summary**: Camouflaged object detection (COD), segmenting objects that are elegantly blended into their surroundings, is a valuable yet challenging task. Existing deep-learning methods often fall into the difficulty of accurately identifying the camouflaged object with complete and fine object structure. To this end, in this paper, we propose a novel boundary-guided network (BGNet) for camouflaged object detection. Our method explores valuable and extra object-related edge semantics to guide representation learning of COD, which forces the model to generate features that highlight object structure, thereby promoting camouflaged object detection of accurate boundary localization. Extensive experiments on three challenging benchmark datasets demonstrate that our BGNet significantly outperforms the existing 18 state-of-the-art methods under four widely-used evaluation metrics. Our code is publicly available at: https://github.com/thograce/BGNet.



### Benchmarks for Industrial Inspection Based on Structured Light
- **Arxiv ID**: http://arxiv.org/abs/2207.00796v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.00796v1)
- **Published**: 2022-07-02 11:09:05+00:00
- **Updated**: 2022-07-02 11:09:05+00:00
- **Authors**: Yuping Ye, Siyuan Chen, Zhan Song
- **Comment**: None
- **Journal**: None
- **Summary**: Robustness and accuracy are two critical metrics for industrial inspection. In this paper, we propose benchmarks that can evaluate the structured light method's performance. Our evaluation metric was learning from a lot of inspection tasks from the factories. The metric we proposed consists of four detailed criteria such as flatness, length, height and sphericity. Then we can judge whether the structured light method/device can be applied to a specified inspection task by our evaluation metric quickly. A structured light device built for TypeC pin needles inspection performance is evaluated via our metrics in the final experimental section.



### Pair-Relationship Modeling for Latent Fingerprint Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.00587v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.00587v1)
- **Published**: 2022-07-02 11:31:31+00:00
- **Updated**: 2022-07-02 11:31:31+00:00
- **Authors**: Yanming Zhu, Xuefei Yin, Xiuping Jia, Jiankun Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Latent fingerprints are important for identifying criminal suspects. However, recognizing a latent fingerprint in a collection of reference fingerprints remains a challenge. Most, if not all, of existing methods would extract representation features of each fingerprint independently and then compare the similarity of these representation features for recognition in a different process. Without the supervision of similarity for the feature extraction process, the extracted representation features are hard to optimally reflect the similarity of the two compared fingerprints which is the base for matching decision making. In this paper, we propose a new scheme that can model the pair-relationship of two fingerprints directly as the similarity feature for recognition. The pair-relationship is modeled by a hybrid deep network which can handle the difficulties of random sizes and corrupted areas of latent fingerprints. Experimental results on two databases show that the proposed method outperforms the state of the art.



### Less is More: Adaptive Curriculum Learning for Thyroid Nodule Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2207.00807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00807v1)
- **Published**: 2022-07-02 11:50:02+00:00
- **Updated**: 2022-07-02 11:50:02+00:00
- **Authors**: Haifan Gong, Hui Cheng, Yifan Xie, Shuangyi Tan, Guanqi Chen, Fei Chen, Guanbin Li
- **Comment**: Accepted to MICCAI 2022 with Student Travel Award
- **Journal**: None
- **Summary**: Thyroid nodule classification aims at determining whether the nodule is benign or malignant based on a given ultrasound image. However, the label obtained by the cytological biopsy which is the golden standard in clinical medicine is not always consistent with the ultrasound imaging TI-RADS criteria. The information difference between the two causes the existing deep learning-based classification methods to be indecisive. To solve the Inconsistent Label problem, we propose an Adaptive Curriculum Learning (ACL) framework, which adaptively discovers and discards the samples with inconsistent labels. Specifically, ACL takes both hard sample and model certainty into account, and could accurately determine the threshold to distinguish the samples with Inconsistent Label. Moreover, we contribute TNCD: a Thyroid Nodule Classification Dataset to facilitate future related research on the thyroid nodules. Extensive experimental results on TNCD based on three different backbone networks not only demonstrate the superiority of our method but also prove that the less-is-more principle which strategically discards the samples with Inconsistent Label could yield performance gains. Source code and data are available at https://github.com/chenghui-666/ACL/.



### ImLoveNet: Misaligned Image-supported Registration Network for Low-overlap Point Cloud Pairs
- **Arxiv ID**: http://arxiv.org/abs/2207.00826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00826v1)
- **Published**: 2022-07-02 13:17:34+00:00
- **Updated**: 2022-07-02 13:17:34+00:00
- **Authors**: Honghua Chen, Zeyong Wei, Yabin Xu, Mingqiang Wei, Jun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Low-overlap regions between paired point clouds make the captured features very low-confidence, leading cutting edge models to point cloud registration with poor quality. Beyond the traditional wisdom, we raise an intriguing question: Is it possible to exploit an intermediate yet misaligned image between two low-overlap point clouds to enhance the performance of cutting-edge registration models? To answer it, we propose a misaligned image supported registration network for low-overlap point cloud pairs, dubbed ImLoveNet. ImLoveNet first learns triple deep features across different modalities and then exports these features to a two-stage classifier, for progressively obtaining the high-confidence overlap region between the two point clouds. Therefore, soft correspondences are well established on the predicted overlap region, resulting in accurate rigid transformations for registration. ImLoveNet is simple to implement yet effective, since 1) the misaligned image provides clearer overlap information for the two low-overlap point clouds to better locate overlap parts; 2) it contains certain geometry knowledge to extract better deep features; and 3) it does not require the extrinsic parameters of the imaging device with respect to the reference frame of the 3D point cloud. Extensive qualitative and quantitative evaluations on different kinds of benchmarks demonstrate the effectiveness and superiority of our ImLoveNet over state-of-the-art approaches.



### UTD-Yolov5: A Real-time Underwater Targets Detection Method based on Attention Improved YOLOv5
- **Arxiv ID**: http://arxiv.org/abs/2207.00837v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.00837v1)
- **Published**: 2022-07-02 14:09:08+00:00
- **Updated**: 2022-07-02 14:09:08+00:00
- **Authors**: Jingyao Wang, Naigong Yu
- **Comment**: None
- **Journal**: None
- **Summary**: As the treasure house of nature, the ocean contains abundant resources. But the coral reefs, which are crucial to the sustainable development of marine life, are facing a huge crisis because of the existence of COTS and other organisms. The protection of society through manual labor is limited and inefficient. The unpredictable nature of the marine environment also makes manual operations risky. The use of robots for underwater operations has become a trend. However, the underwater image acquisition has defects such as weak light, low resolution, and many interferences, while the existing target detection algorithms are not effective. Based on this, we propose an underwater target detection algorithm based on Attention Improved YOLOv5, called UTD-Yolov5. It can quickly and efficiently detect COTS, which in turn provides a prerequisite for complex underwater operations. We adjusted the original network architecture of YOLOv5 in multiple stages, including: replacing the original Backbone with a two-stage cascaded CSP (CSP2); introducing the visual channel attention mechanism module SE; designing random anchor box similarity calculation method etc. These operations enable UTD-Yolov5 to detect more flexibly and capture features more accurately. In order to make the network more efficient, we also propose optimization methods such as WBF and iterative refinement mechanism. This paper conducts a lot of experiments based on the CSIRO dataset [1]. The results show that the average accuracy of our UTD-Yolov5 reaches 78.54%, which is a great improvement compared to the baseline.



### Domain-Adaptive 3D Medical Image Synthesis: An Efficient Unsupervised Approach
- **Arxiv ID**: http://arxiv.org/abs/2207.00844v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.00844v1)
- **Published**: 2022-07-02 14:24:19+00:00
- **Updated**: 2022-07-02 14:24:19+00:00
- **Authors**: Qingqiao Hu, Hongwei Li, Jianguo Zhang
- **Comment**: camera-ready version in MICCAI 2022
- **Journal**: None
- **Summary**: Medical image synthesis has attracted increasing attention because it could generate missing image data, improving diagnosis and benefits many downstream tasks. However, so far the developed synthesis model is not adaptive to unseen data distribution that presents domain shift, limiting its applicability in clinical routine. This work focuses on exploring domain adaptation (DA) of 3D image-to-image synthesis models. First, we highlight the technical difference in DA between classification, segmentation and synthesis models. Second, we present a novel efficient adaptation approach based on 2D variational autoencoder which approximates 3D distributions. Third, we present empirical studies on the effect of the amount of adaptation data and the key hyper-parameters. Our results show that the proposed approach can significantly improve the synthesis accuracy on unseen domains in a 3D setting. The code is publicly available at https://github.com/WinstonHuTiger/2D_VAE_UDA_for_3D_sythesis



### Less Is More: A Comparison of Active Learning Strategies for 3D Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.00845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.4.6; I.2.10; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2207.00845v1)
- **Published**: 2022-07-02 14:27:58+00:00
- **Updated**: 2022-07-02 14:27:58+00:00
- **Authors**: Josafat-Mattias Burmeister, Marcel Fernandez Rosas, Johannes Hagemann, Jonas Kordt, Jasper Blum, Simon Shabo, Benjamin Bergner, Christoph Lippert
- **Comment**: 22 pages, 5 figures, to be published at ReALML @ ICML2022
- **Journal**: None
- **Summary**: Since labeling medical image data is a costly and labor-intensive process, active learning has gained much popularity in the medical image segmentation domain in recent years. A variety of active learning strategies have been proposed in the literature, but their effectiveness is highly dependent on the dataset and training scenario. To facilitate the comparison of existing strategies and provide a baseline for evaluating novel strategies, we evaluate the performance of several well-known active learning strategies on three datasets from the Medical Segmentation Decathlon. Additionally, we consider a strided sampling strategy specifically tailored to 3D image data. We demonstrate that both random and strided sampling act as strong baselines and discuss the advantages and disadvantages of the studied methods. To allow other researchers to compare their work to our results, we provide an open-source framework for benchmarking active learning strategies on a variety of medical segmentation datasets.



### CCTV-Exposure: An open-source system for measuring user's privacy exposure to mapped CCTV cameras based on geo-location (Extended Version)
- **Arxiv ID**: http://arxiv.org/abs/2208.02159v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.02159v1)
- **Published**: 2022-07-02 14:43:44+00:00
- **Updated**: 2022-07-02 14:43:44+00:00
- **Authors**: Hannu Turtiainen, Andrei Costin, Timo Hamalainen
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present CCTV-Exposure -- the first CCTV-aware solution to evaluate potential privacy exposure to closed-circuit television (CCTV) cameras. The objective was to develop a toolset for quantifying human exposure to CCTV cameras from a privacy perspective. Our novel approach is trajectory analysis of the individuals, coupled with a database of geo-location mapped CCTV cameras annotated with minimal yet sufficient meta-information. For this purpose, CCTV-Exposure model based on a Global Positioning System (GPS) tracking was applied to estimate individual privacy exposure in different scenarios. The current investigation provides an application example and validation of the modeling approach. The methodology and toolset developed and implemented in this work provide time-sequence and location-sequence of the exposure events, thus making possible association of the exposure with the individual activities and cameras, and delivers main statistics on individual's exposure to CCTV cameras with high spatio-temporal resolution.



### Hardware architecture for high throughput event visual data filtering with matrix of IIR filters algorithm
- **Arxiv ID**: http://arxiv.org/abs/2207.00860v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2207.00860v1)
- **Published**: 2022-07-02 15:18:53+00:00
- **Updated**: 2022-07-02 15:18:53+00:00
- **Authors**: Marcin Kowalczyk, Tomasz Kryjak
- **Comment**: Accepted for the DSD 2022 conference
- **Journal**: None
- **Summary**: Neuromorphic vision is a rapidly growing field with numerous applications in the perception systems of autonomous vehicles. Unfortunately, due to the sensors working principle, there is a significant amount of noise in the event stream. In this paper we present a novel algorithm based on an IIR filter matrix for filtering this type of noise and a hardware architecture that allows its acceleration using an SoC FPGA. Our method has a very good filtering efficiency for uncorrelated noise - over 99% of noisy events are removed. It has been tested for several event data sets with added random noise. We designed the hardware architecture in such a way as to reduce the utilisation of the FPGA's internal BRAM resources. This enabled a very low latency and a throughput of up to 385.8 MEPS million events per second.The proposed hardware architecture was verified in simulation and in hardware on the Xilinx Zynq Ultrascale+ MPSoC chip on the Mercury+ XU9 module with the Mercury+ ST1 base board.



### ORA3D: Overlap Region Aware Multi-view 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.00865v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00865v4)
- **Published**: 2022-07-02 15:28:44+00:00
- **Updated**: 2023-06-29 09:20:24+00:00
- **Authors**: Wonseok Roh, Gyusam Chang, Seokha Moon, Giljoo Nam, Chanyoung Kim, Younghyun Kim, Jinkyu Kim, Sangpil Kim
- **Comment**: BMVC2022
- **Journal**: None
- **Summary**: Current multi-view 3D object detection methods often fail to detect objects in the overlap region properly, and the networks' understanding of the scene is often limited to that of a monocular detection network. Moreover, objects in the overlap region are often largely occluded or suffer from deformation due to camera distortion, causing a domain shift. To mitigate this issue, we propose using the following two main modules: (1) Stereo Disparity Estimation for Weak Depth Supervision and (2) Adversarial Overlap Region Discriminator. The former utilizes the traditional stereo disparity estimation method to obtain reliable disparity information from the overlap region. Given the disparity estimates as supervision, we propose regularizing the network to fully utilize the geometric potential of binocular images and improve the overall detection accuracy accordingly. Further, the latter module minimizes the representational gap between non-overlap and overlapping regions. We demonstrate the effectiveness of the proposed method with the nuScenes large-scale multi-view 3D object detection data. Our experiments show that our proposed method outperforms current state-of-the-art models, i.e., DETR3D and BEVDet.



### Towards Robust Video Object Segmentation with Adaptive Object Calibration
- **Arxiv ID**: http://arxiv.org/abs/2207.00887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00887v1)
- **Published**: 2022-07-02 17:51:29+00:00
- **Updated**: 2022-07-02 17:51:29+00:00
- **Authors**: Xiaohao Xu, Jinglu Wang, Xiang Ming, Yan Lu
- **Comment**: 19 pages, 17 figures, ACM Multimedia 2022 Accepted
- **Journal**: None
- **Summary**: In the booming video era, video segmentation attracts increasing research attention in the multimedia community. Semi-supervised video object segmentation (VOS) aims at segmenting objects in all target frames of a video, given annotated object masks of reference frames. Most existing methods build pixel-wise reference-target correlations and then perform pixel-wise tracking to obtain target masks. Due to neglecting object-level cues, pixel-level approaches make the tracking vulnerable to perturbations, and even indiscriminate among similar objects. Towards robust VOS, the key insight is to calibrate the representation and mask of each specific object to be expressive and discriminative. Accordingly, we propose a new deep network, which can adaptively construct object representations and calibrate object masks to achieve stronger robustness. First, we construct the object representations by applying an adaptive object proxy (AOP) aggregation method, where the proxies represent arbitrary-shaped segments at multi-levels for reference. Then, prototype masks are initially generated from the reference-target correlations based on AOP. Afterwards, such proto-masks are further calibrated through network modulation, conditioning on the object proxy representations. We consolidate this conditional mask calibration process in a progressive manner, where the object representations and proto-masks evolve to be discriminative iteratively. Extensive experiments are conducted on the standard VOS benchmarks, YouTube-VOS-18/19 and DAVIS-17. Our model achieves the state-of-the-art performance among existing published works, and also exhibits superior robustness against perturbations. Our project repo is at https://github.com/JerryX1110/Robust-Video-Object-Segmentation



### Face Morphing Attack Detection Using Privacy-Aware Training Data
- **Arxiv ID**: http://arxiv.org/abs/2207.00899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.00899v1)
- **Published**: 2022-07-02 19:00:48+00:00
- **Updated**: 2022-07-02 19:00:48+00:00
- **Authors**: Marija Ivanovska, Andrej Kronovšek, Peter Peer, Vitomir Štruc, Borut Batagelj
- **Comment**: None
- **Journal**: None
- **Summary**: Images of morphed faces pose a serious threat to face recognition--based security systems, as they can be used to illegally verify the identity of multiple people with a single morphed image. Modern detection algorithms learn to identify such morphing attacks using authentic images of real individuals. This approach raises various privacy concerns and limits the amount of publicly available training data. In this paper, we explore the efficacy of detection algorithms that are trained only on faces of non--existing people and their respective morphs. To this end, two dedicated algorithms are trained with synthetic data and then evaluated on three real-world datasets, i.e.: FRLL-Morphs, FERET-Morphs and FRGC-Morphs. Our results show that synthetic facial images can be successfully employed for the training process of the detection algorithms and generalize well to real-world scenarios.



### Drift Reduction for Monocular Visual Odometry of Intelligent Vehicles using Feedforward Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2207.00909v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.00909v1)
- **Published**: 2022-07-02 21:28:41+00:00
- **Updated**: 2022-07-02 21:28:41+00:00
- **Authors**: Hassan Wagih, Mostafa Osman, Mohamed I. Awad, Sherif Hammad
- **Comment**: This paper contains 6 pages and 10 figures. The paper was accepted
  for publication in the 25th IEEE International Conference on Intelligent
  Transportation Systems (IEEE ITSC 2022)
- **Journal**: None
- **Summary**: In this paper, an approach for reducing the drift in monocular visual odometry algorithms is proposed based on a feedforward neural network. A visual odometry algorithm computes the incremental motion of the vehicle between the successive camera frames, then integrates these increments to determine the pose of the vehicle. The proposed neural network reduces the errors in the pose estimation of the vehicle which results from the inaccuracies in features detection and matching, camera intrinsic parameters, and so on. These inaccuracies are propagated to the motion estimation of the vehicle causing larger amounts of estimation errors. The drift reducing neural network identifies such errors based on the motion of features in the successive camera frames leading to more accurate incremental motion estimates. The proposed drift reducing neural network is trained and validated using the KITTI dataset and the results show the efficacy of the proposed approach in reducing the errors in the incremental orientation estimation, thus reducing the overall error in the pose estimation.



### SKIPP'D: a SKy Images and Photovoltaic Power Generation Dataset for Short-term Solar Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2207.00913v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.00913v1)
- **Published**: 2022-07-02 21:52:50+00:00
- **Updated**: 2022-07-02 21:52:50+00:00
- **Authors**: Yuhao Nie, Xiatong Li, Andea Scott, Yuchi Sun, Vignesh Venugopal, Adam Brandt
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale integration of photovoltaics (PV) into electricity grids is challenged by the intermittent nature of solar power. Sky-image-based solar forecasting using deep learning has been recognized as a promising approach to predicting the short-term fluctuations. However, there are few publicly available standardized benchmark datasets for image-based solar forecasting, which limits the comparison of different forecasting models and the exploration of forecasting methods. To fill these gaps, we introduce SKIPP'D -- a SKy Images and Photovoltaic Power Generation Dataset. The dataset contains three years (2017-2019) of quality-controlled down-sampled sky images and PV power generation data that is ready-to-use for short-term solar forecasting using deep learning. In addition, to support the flexibility in research, we provide the high resolution, high frequency sky images and PV power generation data as well as the concurrent sky video footage. We also include a code base containing data processing scripts and baseline model implementations for researchers to reproduce our previous work and accelerate their research in solar forecasting.



