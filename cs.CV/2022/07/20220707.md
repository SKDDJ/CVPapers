# Arxiv Papers in cs.CV on 2022-07-07
### Partial Shape Similarity via Alignment of Multi-Metric Hamiltonian Spectra
- **Arxiv ID**: http://arxiv.org/abs/2207.03018v1
- **DOI**: None
- **Categories**: **cs.CV**, math.DG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03018v1)
- **Published**: 2022-07-07 00:03:50+00:00
- **Updated**: 2022-07-07 00:03:50+00:00
- **Authors**: David Bensa√Ød, Amit Bracha, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: Evaluating the similarity of non-rigid shapes with significant partiality is a fundamental task in numerous computer vision applications. Here, we propose a novel axiomatic method to match similar regions across shapes. Matching similar regions is formulated as the alignment of the spectra of operators closely related to the Laplace-Beltrami operator (LBO). The main novelty of the proposed approach is the consideration of differential operators defined on a manifold with multiple metrics. The choice of a metric relates to fundamental shape properties while considering the same manifold under different metrics can thus be viewed as analyzing the underlying manifold from different perspectives. Specifically, we examine the scale-invariant metric and the corresponding scale-invariant Laplace-Beltrami operator (SI-LBO) along with the regular metric and the regular LBO. We demonstrate that the scale-invariant metric emphasizes the locations of important semantic features in articulated shapes. A truncated spectrum of the SI-LBO consequently better captures locally curved regions and complements the global information encapsulated in the truncated spectrum of the regular LBO. We show that matching these dual spectra outperforms competing axiomatic frameworks when tested on standard benchmarks. We introduced a new dataset and compare the proposed method with the state-of-the-art learning based approach in a cross-database configuration. Specifically, we show that, when trained on one data set and tested on another, the proposed axiomatic approach which does not involve training, outperforms the deep learning alternative.



### Dual-Stream Transformer for Generic Event Boundary Captioning
- **Arxiv ID**: http://arxiv.org/abs/2207.03038v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2207.03038v3)
- **Published**: 2022-07-07 01:47:19+00:00
- **Updated**: 2023-03-24 09:55:12+00:00
- **Authors**: Xin Gu, Hanhua Ye, Guang Chen, Yufei Wang, Libo Zhang, Longyin Wen
- **Comment**: Accepted to CVPR 2022 Workshop
- **Journal**: None
- **Summary**: This paper describes our champion solution for the CVPR2022 Generic Event Boundary Captioning (GEBC) competition. GEBC requires the captioning model to have a comprehension of instantaneous status changes around the given video boundary, which makes it much more challenging than conventional video captioning task. In this paper, a Dual-Stream Transformer with improvements on both video content encoding and captions generation is proposed: (1) We utilize three pre-trained models to extract the video features from different granularities. Moreover, we exploit the types of boundary as hints to help the model generate captions. (2) We particularly design an model, termed as Dual-Stream Transformer, to learn discriminative representations for boundary captioning. (3) Towards generating content-relevant and human-like captions, we improve the description quality by designing a word-level ensemble strategy. The promising results on the GEBC test split demonstrate the efficacy of our proposed model.



### Vision Transformers: State of the Art and Research Challenges
- **Arxiv ID**: http://arxiv.org/abs/2207.03041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03041v1)
- **Published**: 2022-07-07 02:01:56+00:00
- **Updated**: 2022-07-07 02:01:56+00:00
- **Authors**: Bo-Kai Ruan, Hong-Han Shuai, Wen-Huang Cheng
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: Transformers have achieved great success in natural language processing. Due to the powerful capability of self-attention mechanism in transformers, researchers develop the vision transformers for a variety of computer vision tasks, such as image recognition, object detection, image segmentation, pose estimation, and 3D reconstruction. This paper presents a comprehensive overview of the literature on different architecture designs and training tricks (including self-supervised learning) for vision transformers. Our goal is to provide a systematic review with the open research opportunities.



### Self-Supervised RF Signal Representation Learning for NextG Signal Classification with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.03046v2
- **DOI**: 10.1109/LWC.2022.3217292
- **Categories**: **cs.NI**, cs.AI, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2207.03046v2)
- **Published**: 2022-07-07 02:07:03+00:00
- **Updated**: 2022-11-01 16:45:34+00:00
- **Authors**: Kemal Davaslioglu, Serdar Boztas, Mehmet Can Ertem, Yalin E. Sagduyu, Ender Ayanoglu
- **Comment**: 5 pages, 3 figures, 3 tables
- **Journal**: IEEE Wireless Communications Letters, 2022
- **Summary**: Deep learning (DL) finds rich applications in the wireless domain to improve spectrum awareness. Typically, DL models are either randomly initialized following a statistical distribution or pretrained on tasks from other domains in the form of transfer learning without accounting for the unique characteristics of wireless signals. Self-supervised learning (SSL) enables the learning of useful representations from Radio Frequency (RF) signals themselves even when only limited training data samples with labels are available. We present a self-supervised RF signal representation learning method and apply it to the automatic modulation recognition (AMR) task by specifically formulating a set of transformations to capture the wireless signal characteristics. We show that the sample efficiency (the number of labeled samples needed to achieve a certain performance) of AMR can be significantly increased (almost an order of magnitude) by learning signal representations with SSL. This translates to substantial time and cost savings. Furthermore, SSL increases the model accuracy compared to the state-of-the-art DL methods and maintains high accuracy when limited training data is available.



### Single-image Defocus Deblurring by Integration of Defocus Map Prediction Tracing the Inverse Problem Computation
- **Arxiv ID**: http://arxiv.org/abs/2207.03047v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03047v1)
- **Published**: 2022-07-07 02:15:33+00:00
- **Updated**: 2022-07-07 02:15:33+00:00
- **Authors**: Qian Ye, Masanori Suganuma, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider the problem in defocus image deblurring. Previous classical methods follow two-steps approaches, i.e., first defocus map estimation and then the non-blind deblurring. In the era of deep learning, some researchers have tried to address these two problems by CNN. However, the simple concatenation of defocus map, which represents the blur level, leads to suboptimal performance. Considering the spatial variant property of the defocus blur and the blur level indicated in the defocus map, we employ the defocus map as conditional guidance to adjust the features from the input blurring images instead of simple concatenation. Then we propose a simple but effective network with spatial modulation based on the defocus map. To achieve this, we design a network consisting of three sub-networks, including the defocus map estimation network, a condition network that encodes the defocus map into condition features, and the defocus deblurring network that performs spatially dynamic modulation based on the condition features. Moreover, the spatially dynamic modulation is based on an affine transform function to adjust the features from the input blurry images. Experimental results show that our method can achieve better quantitative and qualitative evaluation performance than the existing state-of-the-art methods on the commonly used public test datasets.



### AV-Gaze: A Study on the Effectiveness of Audio Guided Visual Attention Estimation for Non-Profilic Faces
- **Arxiv ID**: http://arxiv.org/abs/2207.03048v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03048v2)
- **Published**: 2022-07-07 02:23:02+00:00
- **Updated**: 2022-08-12 01:50:27+00:00
- **Authors**: Shreya Ghosh, Abhinav Dhall, Munawar Hayat, Jarrod Knibbe
- **Comment**: None
- **Journal**: None
- **Summary**: In challenging real-life conditions such as extreme head-pose, occlusions, and low-resolution images where the visual information fails to estimate visual attention/gaze direction, audio signals could provide important and complementary information. In this paper, we explore if audio-guided coarse head-pose can further enhance visual attention estimation performance for non-prolific faces. Since it is difficult to annotate audio signals for estimating the head-pose of the speaker, we use off-the-shelf state-of-the-art models to facilitate cross-modal weak-supervision. During the training phase, the framework learns complementary information from synchronized audio-visual modality. Our model can utilize any of the available modalities i.e. audio, visual or audio-visual for task-specific inference. It is interesting to note that, when AV-Gaze is tested on benchmark datasets with these specific modalities, it achieves competitive results on multiple datasets, while being highly adaptive toward challenging scenarios.



### Multi-Task Lung Nodule Detection in Chest Radiographs with a Dual Head Network
- **Arxiv ID**: http://arxiv.org/abs/2207.03050v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03050v1)
- **Published**: 2022-07-07 02:29:34+00:00
- **Updated**: 2022-07-07 02:29:34+00:00
- **Authors**: Chen-Han Tsai, Yu-Shao Peng
- **Comment**: 11 pages, 3 figures, Accepted to the MICCAI Conference 2022
- **Journal**: None
- **Summary**: Lung nodules can be an alarming precursor to potential lung cancer. Missed nodule detections during chest radiograph analysis remains a common challenge among thoracic radiologists. In this work, we present a multi-task lung nodule detection algorithm for chest radiograph analysis. Unlike past approaches, our algorithm predicts a global-level label indicating nodule presence along with local-level labels predicting nodule locations using a Dual Head Network (DHN). We demonstrate the favorable nodule detection performance that our multi-task formulation yields in comparison to conventional methods. In addition, we introduce a novel Dual Head Augmentation (DHA) strategy tailored for DHN, and we demonstrate its significance in further enhancing global and local nodule predictions.



### Deep Rotation Correction without Angle Prior
- **Arxiv ID**: http://arxiv.org/abs/2207.03054v2
- **DOI**: 10.1109/TIP.2023.3275869
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03054v2)
- **Published**: 2022-07-07 02:46:27+00:00
- **Updated**: 2023-05-11 08:04:28+00:00
- **Authors**: Lang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao
- **Comment**: Accepted by IEEE Transactions on Image Processing(TIP); code and
  dataset:https://github.com/nie-lang/RotationCorrection
- **Journal**: None
- **Summary**: Not everybody can be equipped with professional photography skills and sufficient shooting time, and there can be some tilts in the captured images occasionally. In this paper, we propose a new and practical task, named Rotation Correction, to automatically correct the tilt with high content fidelity in the condition that the rotated angle is unknown. This task can be easily integrated into image editing applications, allowing users to correct the rotated images without any manual operations. To this end, we leverage a neural network to predict the optical flows that can warp the tilted images to be perceptually horizontal. Nevertheless, the pixel-wise optical flow estimation from a single image is severely unstable, especially in large-angle tilted images. To enhance its robustness, we propose a simple but effective prediction strategy to form a robust elastic warp. Particularly, we first regress the mesh deformation that can be transformed into robust initial optical flows. Then we estimate residual optical flows to facilitate our network the flexibility of pixel-wise deformation, further correcting the details of the tilted images. To establish an evaluation benchmark and train the learning framework, a comprehensive rotation correction dataset is presented with a large diversity in scenes and rotated angles. Extensive experiments demonstrate that even in the absence of the angle prior, our algorithm can outperform other state-of-the-art solutions requiring this prior. The code and dataset are available at https://github.com/nie-lang/RotationCorrection.



### Back to the Basics: Revisiting Out-of-Distribution Detection Baselines
- **Arxiv ID**: http://arxiv.org/abs/2207.03061v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2207.03061v1)
- **Published**: 2022-07-07 03:04:09+00:00
- **Updated**: 2022-07-07 03:04:09+00:00
- **Authors**: Johnson Kuan, Jonas Mueller
- **Comment**: ICML Workshop on Principles of Distribution Shift 2022
- **Journal**: None
- **Summary**: We study simple methods for out-of-distribution (OOD) image detection that are compatible with any already trained classifier, relying on only its predictions or learned representations. Evaluating the OOD detection performance of various methods when utilized with ResNet-50 and Swin Transformer models, we find methods that solely consider the model's predictions can be easily outperformed by also considering the learned representations. Based on our analysis, we advocate for a dead-simple approach that has been neglected in other studies: simply flag as OOD images whose average distance to their K nearest neighbors is large (in the representation space of an image classifier trained on the in-distribution data).



### Shadow-Background-Noise 3D Spatial Decomposition Using Sparse Low-Rank Gaussian Properties for Video-SAR Moving Target Shadow Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2207.03064v2
- **DOI**: 10.1109/LGRS.2022.3223514
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03064v2)
- **Published**: 2022-07-07 03:13:12+00:00
- **Updated**: 2022-12-07 13:05:44+00:00
- **Authors**: Xiaowo Xu, Xiaoling Zhang, Tianwen Zhang, Zhenyu Yang, Jun Shi, Xu Zhan
- **Comment**: None
- **Journal**: None
- **Summary**: Moving target shadows among video synthetic aperture radar (Video-SAR) images are always interfered by low scattering backgrounds and cluttered noises, causing poor detec-tion-tracking accuracy. Thus, a shadow-background-noise 3D spatial decomposition (SBN-3D-SD) model is proposed to enhance shadows for higher detection-tracking accuracy. It leverages the sparse property of shadows, the low-rank property of back-grounds, and the Gaussian property of noises to perform 3D spatial three-decomposition. It separates shadows from back-grounds and noises by the alternating direction method of multi-pliers (ADMM). Results on the Sandia National Laboratories (SNL) data verify its effectiveness. It boosts the shadow saliency from the qualitative and quantitative evaluation. It boosts the shadow detection accuracy of Faster R-CNN, RetinaNet and YOLOv3. It also boosts the shadow tracking accuracy of TransTrack, FairMOT and ByteTrack.



### Contrastive Learning from Spatio-Temporal Mixed Skeleton Sequences for Self-Supervised Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.03065v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03065v1)
- **Published**: 2022-07-07 03:18:09+00:00
- **Updated**: 2022-07-07 03:18:09+00:00
- **Authors**: Zhan Chen, Hong Liu, Tianyu Guo, Zhengyan Chen, Pinhao Song, Hao Tang
- **Comment**: 12 pages, 5 figures, submitted to TMM
- **Journal**: None
- **Summary**: Self-supervised skeleton-based action recognition with contrastive learning has attracted much attention. Recent literature shows that data augmentation and large sets of contrastive pairs are crucial in learning such representations. In this paper, we found that directly extending contrastive pairs based on normal augmentations brings limited returns in terms of performance, because the contribution of contrastive pairs from the normal data augmentation to the loss get smaller as training progresses. Therefore, we delve into hard contrastive pairs for contrastive learning. Motivated by the success of mixing augmentation strategy which improves the performance of many tasks by synthesizing novel samples, we propose SkeleMixCLR: a contrastive learning framework with a spatio-temporal skeleton mixing augmentation (SkeleMix) to complement current contrastive learning approaches by providing hard contrastive samples. First, SkeleMix utilizes the topological information of skeleton data to mix two skeleton sequences by randomly combing the cropped skeleton fragments (the trimmed view) with the remaining skeleton sequences (the truncated view). Second, a spatio-temporal mask pooling is applied to separate these two views at the feature level. Third, we extend contrastive pairs with these two views. SkeleMixCLR leverages the trimmed and truncated views to provide abundant hard contrastive pairs since they involve some context information from each other due to the graph convolution operations, which allows the model to learn better motion representations for action recognition. Extensive experiments on NTU-RGB+D, NTU120-RGB+D, and PKU-MMD datasets show that SkeleMixCLR achieves state-of-the-art performance. Codes are available at https://github.com/czhaneva/SkeleMixCLR.



### What Makes for Automatic Reconstruction of Pulmonary Segments
- **Arxiv ID**: http://arxiv.org/abs/2207.03078v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03078v3)
- **Published**: 2022-07-07 04:24:17+00:00
- **Updated**: 2022-07-14 13:51:02+00:00
- **Authors**: Kaiming Kuang, Li Zhang, Jingyu Li, Hongwei Li, Jiajun Chen, Bo Du, Jiancheng Yang
- **Comment**: MICCAI 2022
- **Journal**: None
- **Summary**: 3D reconstruction of pulmonary segments plays an important role in surgical treatment planning of lung cancer, which facilitates preservation of pulmonary function and helps ensure low recurrence rates. However, automatic reconstruction of pulmonary segments remains unexplored in the era of deep learning. In this paper, we investigate what makes for automatic reconstruction of pulmonary segments. First and foremost, we formulate, clinically and geometrically, the anatomical definitions of pulmonary segments, and propose evaluation metrics adhering to these definitions. Second, we propose ImPulSe (Implicit Pulmonary Segment), a deep implicit surface model designed for pulmonary segment reconstruction. The automatic reconstruction of pulmonary segments by ImPulSe is accurate in metrics and visually appealing. Compared with canonical segmentation methods, ImPulSe outputs continuous predictions of arbitrary resolutions with higher training efficiency and fewer parameters. Lastly, we experiment with different network inputs to analyze what matters in the task of pulmonary segment reconstruction. Our code is available at https://github.com/M3DV/ImPulSe.



### DRL-ISP: Multi-Objective Camera ISP with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.03081v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03081v1)
- **Published**: 2022-07-07 04:34:05+00:00
- **Updated**: 2022-07-07 04:34:05+00:00
- **Authors**: Ukcheol Shin, Kyunghyun Lee, In So Kweon
- **Comment**: Accepted by IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS), 2022 (*First two authors are equal contributed)
- **Journal**: None
- **Summary**: In this paper, we propose a multi-objective camera ISP framework that utilizes Deep Reinforcement Learning (DRL) and camera ISP toolbox that consist of network-based and conventional ISP tools. The proposed DRL-based camera ISP framework iteratively selects a proper tool from the toolbox and applies it to the image to maximize a given vision task-specific reward function. For this purpose, we implement total 51 ISP tools that include exposure correction, color-and-tone correction, white balance, sharpening, denoising, and the others. We also propose an efficient DRL network architecture that can extract the various aspects of an image and make a rigid mapping relationship between images and a large number of actions. Our proposed DRL-based ISP framework effectively improves the image quality according to each vision task such as RAW-to-RGB image restoration, 2D object detection, and monocular depth estimation.



### Adaptation of Surgical Activity Recognition Models Across Operating Rooms
- **Arxiv ID**: http://arxiv.org/abs/2207.03083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03083v1)
- **Published**: 2022-07-07 04:41:34+00:00
- **Updated**: 2022-07-07 04:41:34+00:00
- **Authors**: Ali Mottaghi, Aidean Sharghi, Serena Yeung, Omid Mohareri
- **Comment**: MICCAI 2022
- **Journal**: None
- **Summary**: Automatic surgical activity recognition enables more intelligent surgical devices and a more efficient workflow. Integration of such technology in new operating rooms has the potential to improve care delivery to patients and decrease costs. Recent works have achieved a promising performance on surgical activity recognition; however, the lack of generalizability of these models is one of the critical barriers to the wide-scale adoption of this technology. In this work, we study the generalizability of surgical activity recognition models across operating rooms. We propose a new domain adaptation method to improve the performance of the surgical activity recognition model in a new operating room for which we only have unlabeled videos. Our approach generates pseudo labels for unlabeled video clips that it is confident about and trains the model on the augmented version of the clips. We extend our method to a semi-supervised domain adaptation setting where a small portion of the target domain is also labeled. In our experiments, our proposed method consistently outperforms the baselines on a dataset of more than 480 long surgical videos collected from two operating rooms.



### EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022: Team HNU-FPV Technical Report
- **Arxiv ID**: http://arxiv.org/abs/2207.03095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03095v1)
- **Published**: 2022-07-07 05:27:32+00:00
- **Updated**: 2022-07-07 05:27:32+00:00
- **Authors**: Nie Lin, Minjie Cai
- **Comment**: Technical report for CVPR 2022 EPIC-Ego4D Workshop (EPIC Challenge)
- **Journal**: None
- **Summary**: In this report, we present the technical details of our submission to the 2022 EPIC-Kitchens Unsupervised Domain Adaptation (UDA) Challenge. Existing UDA methods align the global features extracted from the whole video clips across the source and target domains but suffer from the spatial redundancy of feature matching in video recognition. Motivated by the observation that in most cases a small image region in each video frame can be informative enough for the action recognition task, we propose to exploit informative image regions to perform efficient domain alignment. Specifically, we first use lightweight CNNs to extract the global information of the input two-stream video frames and select the informative image patches by a differentiable interpolation-based selection strategy. Then the global information from videos frames and local information from image patches are processed by an existing video adaptation method, i.e., TA3N, in order to perform feature alignment for the source domain and the target domain. Our method (without model ensemble) ranks 4th among this year's teams on the test set of EPIC-KITCHENS-100.



### Uncertainty-Aware Self-supervised Neural Network for Liver $T_{1œÅ}$ Mapping with Relaxation Constraint
- **Arxiv ID**: http://arxiv.org/abs/2207.03105v2
- **DOI**: 10.1088/1361-6560/ac9e3e
- **Categories**: **q-bio.TO**, cs.CV, eess.IV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2207.03105v2)
- **Published**: 2022-07-07 06:10:34+00:00
- **Updated**: 2022-10-26 02:09:14+00:00
- **Authors**: Chaoxing Huang, Yurui Qian, Simon Chun Ho Yu, Jian Hou, Baiyan Jiang, Queenie Chan, Vincent Wai-Sun Wong, Winnie Chiu-Wing Chu, Weitian Chen
- **Comment**: Provisionally accepted by Physics in Medicine and Biology
- **Journal**: None
- **Summary**: $T_{1\rho}$ mapping is a promising quantitative MRI technique for the non-invasive assessment of tissue properties. Learning-based approaches can map $T_{1\rho}$ from a reduced number of $T_{1\rho}$ weighted images, but requires significant amounts of high quality training data. Moreover, existing methods do not provide the confidence level of the $T_{1\rho}$ estimation. To address these problems, we proposed a self-supervised learning neural network that learns a $T_{1\rho}$ mapping using the relaxation constraint in the learning process. Epistemic uncertainty and aleatoric uncertainty are modelled for the $T_{1\rho}$ quantification network to provide a Bayesian confidence estimation of the $T_{1\rho}$ mapping. The uncertainty estimation can also regularize the model to prevent it from learning imperfect data. We conducted experiments on $T_{1\rho}$ data collected from 52 patients with non-alcoholic fatty liver disease. The results showed that our method outperformed the existing methods for $T_{1\rho}$ quantification of the liver using as few as two $T_{1\rho}$-weighted images. Our uncertainty estimation provided a feasible way of modelling the confidence of the self-supervised learning based $T_{1\rho}$ estimation, which is consistent with the reality in liver $T_{1\rho}$ imaging.



### Masked Surfel Prediction for Self-Supervised Point Cloud Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.03111v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.03111v1)
- **Published**: 2022-07-07 06:47:26+00:00
- **Updated**: 2022-07-07 06:47:26+00:00
- **Authors**: Yabin Zhang, Jiehong Lin, Chenhang He, Yongwei Chen, Kui Jia, Lei Zhang
- **Comment**: Codes will be available at https://github.com/YBZh/MaskSurf
- **Journal**: None
- **Summary**: Masked auto-encoding is a popular and effective self-supervised learning approach to point cloud learning. However, most of the existing methods reconstruct only the masked points and overlook the local geometry information, which is also important to understand the point cloud data. In this work, we make the first attempt, to the best of our knowledge, to consider the local geometry information explicitly into the masked auto-encoding, and propose a novel Masked Surfel Prediction (MaskSurf) method. Specifically, given the input point cloud masked at a high ratio, we learn a transformer-based encoder-decoder network to estimate the underlying masked surfels by simultaneously predicting the surfel positions (i.e., points) and per-surfel orientations (i.e., normals). The predictions of points and normals are supervised by the Chamfer Distance and a newly introduced Position-Indexed Normal Distance in a set-to-set manner. Our MaskSurf is validated on six downstream tasks under three fine-tuning strategies. In particular, MaskSurf outperforms its closest competitor, Point-MAE, by 1.2\% on the real-world dataset of ScanObjectNN under the OBJ-BG setting, justifying the advantages of masked surfel prediction over masked point cloud reconstruction. Codes will be available at https://github.com/YBZh/MaskSurf.



### Deep learning based Hand gesture recognition system and design of a Human-Machine Interface
- **Arxiv ID**: http://arxiv.org/abs/2207.03112v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2207.03112v3)
- **Published**: 2022-07-07 06:50:08+00:00
- **Updated**: 2023-01-16 13:14:52+00:00
- **Authors**: Abir Sen, Tapas Kumar Mishra, Ratnakar Dash
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, a real-time hand gesture recognition system-based human-computer interface (HCI) is presented. The system consists of six stages: (1) hand detection, (2) gesture segmentation, (3) use of five pre-trained convolutional neural network models (CNN) and vision transformer (ViT), (4) building an interactive human-machine interface (HMI), (5) development of a gesture-controlled virtual mouse, (6) use of Kalman filter to estimate the hand position, based on that the smoothness of the motion of pointer is improved. In our work, five pre-trained CNN (VGG16, VGG19, ResNet50, ResNet101, and Inception-V1) models and ViT have been employed to classify hand gesture images. Two multi-class datasets (one public and one custom) have been used to validate the models. Considering the model's performances, it is observed that Inception-V1 has significantly shown a better classification performance compared to the other four CNN models and ViT in terms of accuracy, precision, recall, and F-score values. We have also expanded this system to control some desktop applications (such as VLC player, audio player, file management, playing 2D Super-Mario-Bros game, etc.) with different customized gesture commands in real-time scenarios. The average speed of this system has reached 25 fps (frames per second), which meets the requirements for the real-time scenario. Performance of the proposed gesture control system obtained the average response time in milisecond for each control which makes it suitable for real-time. This model (prototype) will benefit physically disabled people interacting with desktops.



### PointMCD: Boosting Deep Point Cloud Encoders via Multi-view Cross-modal Distillation for 3D Shape Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.03128v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03128v4)
- **Published**: 2022-07-07 07:23:20+00:00
- **Updated**: 2023-06-15 06:21:09+00:00
- **Authors**: Qijian Zhang, Junhui Hou, Yue Qian
- **Comment**: Accepted to TMM
- **Journal**: None
- **Summary**: As two fundamental representation modalities of 3D objects, 3D point clouds and multi-view 2D images record shape information from different domains of geometric structures and visual appearances. In the current deep learning era, remarkable progress in processing such two data modalities has been achieved through respectively customizing compatible 3D and 2D network architectures. However, unlike multi-view image-based 2D visual modeling paradigms, which have shown leading performance in several common 3D shape recognition benchmarks, point cloud-based 3D geometric modeling paradigms are still highly limited by insufficient learning capacity, due to the difficulty of extracting discriminative features from irregular geometric signals. In this paper, we explore the possibility of boosting deep 3D point cloud encoders by transferring visual knowledge extracted from deep 2D image encoders under a standard teacher-student distillation workflow. Generally, we propose PointMCD, a unified multi-view cross-modal distillation architecture, including a pretrained deep image encoder as the teacher and a deep point encoder as the student. To perform heterogeneous feature alignment between 2D visual and 3D geometric domains, we further investigate visibility-aware feature projection (VAFP), by which point-wise embeddings are reasonably aggregated into view-specific geometric descriptors. By pair-wisely aligning multi-view visual and geometric descriptors, we can obtain more powerful deep point encoders without exhausting and complicated network modification. Experiments on 3D shape classification, part segmentation, and unsupervised learning strongly validate the effectiveness of our method. The code and data will be publicly available at https://github.com/keeganhk/PointMCD.



### Style Interleaved Learning for Generalizable Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2207.03132v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03132v3)
- **Published**: 2022-07-07 07:41:32+00:00
- **Updated**: 2023-06-08 01:32:38+00:00
- **Authors**: Wentao Tan, Changxing Ding, Pengfei Wang, Mingming Gong, Kui Jia
- **Comment**: accepted version to IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Domain generalization (DG) for person re-identification (ReID) is a challenging problem, as access to target domain data is not permitted during the training process. Most existing DG ReID methods update the feature extractor and classifier parameters based on the same features. This common practice causes the model to overfit to existing feature styles in the source domain, resulting in sub-optimal generalization ability on target domains. To solve this problem, we propose a novel style interleaved learning (IL) framework. Unlike conventional learning strategies, IL incorporates two forward propagations and one backward propagation for each iteration. We employ the features of interleaved styles to update the feature extractor and classifiers using different forward propagations, which helps to prevent the model from overfitting to certain domain styles. To generate interleaved feature styles, we further propose a new feature stylization approach. It produces a wide range of meaningful styles that are both different and independent from the original styles in the source domain, which caters to the IL methodology. Extensive experimental results show that our model not only consistently outperforms state-of-the-art methods on large-scale benchmarks for DG ReID, but also has clear advantages in computational efficiency. The code is available at https://github.com/WentaoTan/Interleaved-Learning.



### Improving Few-Shot Image Classification Using Machine- and User-Generated Natural Language Descriptions
- **Arxiv ID**: http://arxiv.org/abs/2207.03133v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03133v1)
- **Published**: 2022-07-07 07:48:06+00:00
- **Updated**: 2022-07-07 07:48:06+00:00
- **Authors**: Kosuke Nishida, Kyosuke Nishida, Shuichi Nishioka
- **Comment**: Findings of NAACL2022
- **Journal**: None
- **Summary**: Humans can obtain the knowledge of novel visual concepts from language descriptions, and we thus use the few-shot image classification task to investigate whether a machine learning model can have this capability. Our proposed model, LIDE (Learning from Image and DEscription), has a text decoder to generate the descriptions and a text encoder to obtain the text representations of machine- or user-generated descriptions. We confirmed that LIDE with machine-generated descriptions outperformed baseline models. Moreover, the performance was improved further with high-quality user-generated descriptions. The generated descriptions can be viewed as the explanations of the model's predictions, and we observed that such explanations were consistent with prediction results. We also investigated why the language description improved the few-shot image classification performance by comparing the image representations and the text representations in the feature spaces.



### Self-Supervised Velocity Estimation for Automotive Radar Object Detection Networks
- **Arxiv ID**: http://arxiv.org/abs/2207.03146v1
- **DOI**: 10.1109/IV51971.2022.9827295
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.03146v1)
- **Published**: 2022-07-07 08:15:12+00:00
- **Updated**: 2022-07-07 08:15:12+00:00
- **Authors**: Daniel Niederl√∂hner, Michael Ulrich, Sascha Braun, Daniel K√∂hler, Florian Faion, Claudius Gl√§ser, Andr√© Treptow, Holger Blume
- **Comment**: Accepted for presentation at the 2022 33rd IEEE Intelligent Vehicles
  Symposium (IV) (IV 2022), June 5-9, 2022, in Aachen, Germany
- **Journal**: 2022 IEEE Intelligent Vehicles Symposium (IV), 04-09 June 2022,
  Aachen Germany, pp. 352-359
- **Summary**: This paper presents a method to learn the Cartesian velocity of objects using an object detection network on automotive radar data. The proposed method is self-supervised in terms of generating its own training signal for the velocities. Labels are only required for single-frame, oriented bounding boxes (OBBs). Labels for the Cartesian velocities or contiguous sequences, which are expensive to obtain, are not required. The general idea is to pre-train an object detection network without velocities using single-frame OBB labels, and then exploit the network's OBB predictions on unlabelled data for velocity training. In detail, the network's OBB predictions of the unlabelled frames are updated to the timestamp of a labelled frame using the predicted velocities and the distances between the updated OBBs of the unlabelled frame and the OBB predictions of the labelled frame are used to generate a self-supervised training signal for the velocities. The detection network architecture is extended by a module to account for the temporal relation of multiple scans and a module to represent the radars' radial velocity measurements explicitly. A two-step approach of first training only OBB detection, followed by training OBB detection and velocities is used. Further, a pre-training with pseudo-labels generated from radar radial velocity measurements bootstraps the self-supervised method of this paper. Experiments on the publicly available nuScenes dataset show that the proposed method almost reaches the velocity estimation performance of a fully supervised training, but does not require expensive velocity labels. Furthermore, we outperform a baseline method which uses only radial velocity measurements as labels.



### FastHebb: Scaling Hebbian Training of Deep Neural Networks to ImageNet Level
- **Arxiv ID**: http://arxiv.org/abs/2207.03172v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03172v1)
- **Published**: 2022-07-07 09:04:55+00:00
- **Updated**: 2022-07-07 09:04:55+00:00
- **Authors**: Gabriele Lagani, Claudio Gennaro, Hannes Fassold, Giuseppe Amato
- **Comment**: 14 pages, 3 figures, 2 tables. Submitted at SISAP 2022
- **Journal**: None
- **Summary**: Learning algorithms for Deep Neural Networks are typically based on supervised end-to-end Stochastic Gradient Descent (SGD) training with error backpropagation (backprop). Backprop algorithms require a large number of labelled training samples to achieve high performance. However, in many realistic applications, even if there is plenty of image samples, very few of them are labelled, and semi-supervised sample-efficient training strategies have to be used. Hebbian learning represents a possible approach towards sample efficient training; however, in current solutions, it does not scale well to large datasets. In this paper, we present FastHebb, an efficient and scalable solution for Hebbian learning which achieves higher efficiency by 1) merging together update computation and aggregation over a batch of inputs, and 2) leveraging efficient matrix multiplication algorithms on GPU. We validate our approach on different computer vision benchmarks, in a semi-supervised learning scenario. FastHebb outperforms previous solutions by up to 50 times in terms of training speed, and notably, for the first time, we are able to bring Hebbian algorithms to ImageNet scale.



### Deformer: Towards Displacement Field Learning for Unsupervised Medical Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2207.03180v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03180v1)
- **Published**: 2022-07-07 09:14:40+00:00
- **Updated**: 2022-07-07 09:14:40+00:00
- **Authors**: Jiashun Chen, Donghuan Lu, Yu Zhang, Dong Wei, Munan Ning, Xinyu Shi, Zhe Xu, Yefeng Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep-learning-based approaches have been widely studied for deformable image registration task. However, most efforts directly map the composite image representation to spatial transformation through the convolutional neural network, ignoring its limited ability to capture spatial correspondence. On the other hand, Transformer can better characterize the spatial relationship with attention mechanism, its long-range dependency may be harmful to the registration task, where voxels with too large distances are unlikely to be corresponding pairs. In this study, we propose a novel Deformer module along with a multi-scale framework for the deformable image registration task. The Deformer module is designed to facilitate the mapping from image representation to spatial transformation by formulating the displacement vector prediction as the weighted summation of several bases. With the multi-scale framework to predict the displacement fields in a coarse-to-fine manner, superior performance can be achieved compared with traditional and learning-based approaches. Comprehensive experiments on two public datasets are conducted to demonstrate the effectiveness of the proposed Deformer module as well as the multi-scale framework.



### Uncertainty of Atmospheric Motion Vectors by Sampling Tempered Posterior Distributions
- **Arxiv ID**: http://arxiv.org/abs/2207.03182v2
- **DOI**: None
- **Categories**: **stat.ME**, cs.CV, math.PR, math.ST, stat.AP, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/2207.03182v2)
- **Published**: 2022-07-07 09:23:03+00:00
- **Updated**: 2022-07-08 12:14:01+00:00
- **Authors**: Patrick H√©as, Fr√©d√©ric C√©rou, Mathias Rousset
- **Comment**: None
- **Journal**: None
- **Summary**: Atmospheric motion vectors (AMVs) extracted from satellite imagery are the only wind observations with good global coverage. They are important features for feeding numerical weather prediction (NWP) models. Several Bayesian models have been proposed to estimate AMVs. Although critical for correct assimilation into NWP models, very few methods provide a thorough characterization of the estimation errors. The difficulty of estimating errors stems from the specificity of the posterior distribution, which is both very high dimensional, and highly ill-conditioned due to a singular likelihood, which becomes critical in particular in the case of missing data (unobserved pixels). This work studies the evaluation of the expected error of AMVs using gradient-based Markov Chain Monte Carlo (MCMC) algorithms. Our main contribution is to propose a tempering strategy, which amounts to sampling a local approximation of the joint posterior distribution of AMVs and image variables in the neighborhood of a point estimate. In addition, we provide efficient preconditioning with the covariance related to the prior family itself (fractional Brownian motion), with possibly different hyper-parameters. From a theoretical point of view, we show that under regularity assumptions, the family of tempered posterior distributions converges in distribution as temperature decreases to an {optimal} Gaussian approximation at a point estimate given by the Maximum A Posteriori (MAP) log-density. From an empirical perspective, we evaluate the proposed approach based on some quantitative Bayesian evaluation criteria. Our numerical simulations performed on synthetic and real meteorological data reveal a significant gain in terms of accuracy of the AMV point estimates and of their associated expected error estimates, but also a substantial acceleration in the convergence speed of the MCMC algorithms.



### Learning Music-Dance Representations through Explicit-Implicit Rhythm Synchronization
- **Arxiv ID**: http://arxiv.org/abs/2207.03190v2
- **DOI**: 10.1109/TMM.2023.3303690
- **Categories**: **cs.SD**, cs.CV, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2207.03190v2)
- **Published**: 2022-07-07 09:44:44+00:00
- **Updated**: 2023-08-10 08:06:05+00:00
- **Authors**: Jiashuo Yu, Junfu Pu, Ying Cheng, Rui Feng, Ying Shan
- **Comment**: Accepted for publication in IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Although audio-visual representation has been proved to be applicable in many downstream tasks, the representation of dancing videos, which is more specific and always accompanied by music with complex auditory contents, remains challenging and uninvestigated. Considering the intrinsic alignment between the cadent movement of dancer and music rhythm, we introduce MuDaR, a novel Music-Dance Representation learning framework to perform the synchronization of music and dance rhythms both in explicit and implicit ways. Specifically, we derive the dance rhythms based on visual appearance and motion cues inspired by the music rhythm analysis. Then the visual rhythms are temporally aligned with the music counterparts, which are extracted by the amplitude of sound intensity. Meanwhile, we exploit the implicit coherence of rhythms implied in audio and visual streams by contrastive learning. The model learns the joint embedding by predicting the temporal consistency between audio-visual pairs. The music-dance representation, together with the capability of detecting audio and visual rhythms, can further be applied to three downstream tasks: (a) dance classification, (b) music-dance retrieval, and (c) music-dance retargeting. Extensive experiments demonstrate that our proposed framework outperforms other self-supervised methods by a large margin.



### MCTS with Refinement for Proposals Selection Games in Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/2207.03204v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GT
- **Links**: [PDF](http://arxiv.org/pdf/2207.03204v1)
- **Published**: 2022-07-07 10:15:54+00:00
- **Updated**: 2022-07-07 10:15:54+00:00
- **Authors**: Sinisa Stekovic, Mahdi Rad, Alireza Moradi, Friedrich Fraundorfer, Vincent Lepetit
- **Comment**: Submitted to: TPAMI Special Section on the Best Papers of ICCV2021
  GitHub Repository: https://github.com/vevenom/MonteScene. arXiv admin note:
  substantial text overlap with arXiv:2103.11161
- **Journal**: None
- **Summary**: We propose a novel method applicable in many scene understanding problems that adapts the Monte Carlo Tree Search (MCTS) algorithm, originally designed to learn to play games of high-state complexity. From a generated pool of proposals, our method jointly selects and optimizes proposals that minimize the objective term. In our first application for floor plan reconstruction from point clouds, our method selects and refines the room proposals, modelled as 2D polygons, by optimizing on an objective function combining the fitness as predicted by a deep network and regularizing terms on the room shapes. We also introduce a novel differentiable method for rendering the polygonal shapes of these proposals. Our evaluations on the recent and challenging Structured3D and Floor-SP datasets show significant improvements over the state-of-the-art, without imposing hard constraints nor assumptions on the floor plan configurations. In our second application, we extend our approach to reconstruct general 3D room layouts from a color image and obtain accurate room layouts. We also show that our differentiable renderer can easily be extended for rendering 3D planar polygons and polygon embeddings. Our method shows high performance on the Matterport3D-Layout dataset, without introducing hard constraints on room layout configurations.



### Dual Stream Computer-Generated Image Detection Network Based On Channel Joint And Softpool
- **Arxiv ID**: http://arxiv.org/abs/2207.03205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03205v1)
- **Published**: 2022-07-07 10:19:04+00:00
- **Updated**: 2022-07-07 10:19:04+00:00
- **Authors**: Ziyi Xi, Hao Lin, Weiqi Luo
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: With the development of computer graphics technology, the images synthesized by computer software become more and more closer to the photographs. While computer graphics technology brings us a grand visual feast in the field of games and movies, it may also be utilized by someone with bad intentions to guide public opinions and cause political crisis or social unrest. Therefore, how to distinguish the computer-generated graphics (CG) from the photographs (PG) has become an important topic in the field of digital image forensics. This paper proposes a dual stream convolutional neural network based on channel joint and softpool. The proposed network architecture includes a residual module for extracting image noise information and a joint channel information extraction module for capturing the shallow semantic information of image. In addition, we also design a residual structure to enhance feature extraction and reduce the loss of information in residual flow. The joint channel information extraction module can obtain the shallow semantic information of the input image which can be used as the information supplement block of the residual module. The whole network uses SoftPool to reduce the information loss of down-sampling for image. Finally, we fuse the two flows to get the classification results. Experiments on SPL2018 and DsTok show that the proposed method outperforms existing methods, especially on the DsTok dataset. For example, the performance of our model surpasses the state-of-the-art by a large margin of 3%.



### BMD-GAN: Bone mineral density estimation using x-ray image decomposition into projections of bone-segmented quantitative computed tomography using hierarchical learning
- **Arxiv ID**: http://arxiv.org/abs/2207.03210v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03210v1)
- **Published**: 2022-07-07 10:33:12+00:00
- **Updated**: 2022-07-07 10:33:12+00:00
- **Authors**: Yi Gu, Yoshito Otake, Keisuke Uemura, Mazen Soufi, Masaki Takao, Nobuhiko Sugano, Yoshinobu Sato
- **Comment**: MICCAI 2022 Provisional Acceptance
- **Journal**: None
- **Summary**: We propose a method for estimating the bone mineral density (BMD) from a plain x-ray image. Dual-energy X-ray absorptiometry (DXA) and quantitative computed tomography (QCT) provide high accuracy in diagnosing osteoporosis; however, these modalities require special equipment and scan protocols. Measuring BMD from an x-ray image provides an opportunistic screening, which is potentially useful for early diagnosis. The previous methods that directly learn the relationship between x-ray images and BMD require a large training dataset to achieve high accuracy because of large intensity variations in the x-ray images. Therefore, we propose an approach using the QCT for training a generative adversarial network (GAN) and decomposing an x-ray image into a projection of bone-segmented QCT. The proposed hierarchical learning improved the robustness and accuracy of quantitatively decomposing a small-area target. The evaluation of 200 patients with osteoarthritis using the proposed method, which we named BMD-GAN, demonstrated a Pearson correlation coefficient of 0.888 between the predicted and ground truth DXA-measured BMD. Besides not requiring a large-scale training database, another advantage of our method is its extensibility to other anatomical areas, such as the vertebrae and rib bones.



### Entropy-Based Feature Extraction For Real-Time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.03233v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.03233v1)
- **Published**: 2022-07-07 11:37:18+00:00
- **Updated**: 2022-07-07 11:37:18+00:00
- **Authors**: Lusine Abrahamyan, Nikos Deligiannis
- **Comment**: ICIP 2022
- **Journal**: None
- **Summary**: This paper introduces an efficient patch-based computational module, coined Entropy-based Patch Encoder (EPE) module, for resource-constrained semantic segmentation. The EPE module consists of three lightweight fully-convolutional encoders, each extracting features from image patches with a different amount of entropy. Patches with high entropy are being processed by the encoder with the largest number of parameters, patches with moderate entropy are processed by the encoder with a moderate number of parameters, and patches with low entropy are processed by the smallest encoder. The intuition behind the module is the following: as patches with high entropy contain more information, they need an encoder with more parameters, unlike low entropy patches, which can be processed using a small encoder. Consequently, processing part of the patches via the smaller encoder can significantly reduce the computational cost of the module. Experiments show that EPE can boost the performance of existing real-time semantic segmentation models with a slight increase in the computational cost. Specifically, EPE increases the mIOU performance of DFANet A by 0.9% with only 1.2% increase in the number of parameters and the mIOU performance of EDANet by 1% with 10% increase of the model parameters.



### D2HNet: Joint Denoising and Deblurring with Hierarchical Network for Robust Night Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2207.03294v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03294v2)
- **Published**: 2022-07-07 13:42:05+00:00
- **Updated**: 2022-07-14 04:13:12+00:00
- **Authors**: Yuzhi Zhao, Yongzhe Xu, Qiong Yan, Dingdong Yang, Xuehui Wang, Lai-Man Po
- **Comment**: Accepted by the ECCV 2022, including supplementary material
- **Journal**: None
- **Summary**: Night imaging with modern smartphone cameras is troublesome due to low photon count and unavoidable noise in the imaging system. Directly adjusting exposure time and ISO ratings cannot obtain sharp and noise-free images at the same time in low-light conditions. Though many methods have been proposed to enhance noisy or blurry night images, their performances on real-world night photos are still unsatisfactory due to two main reasons: 1) Limited information in a single image and 2) Domain gap between synthetic training images and real-world photos (e.g., differences in blur area and resolution). To exploit the information from successive long- and short-exposure images, we propose a learning-based pipeline to fuse them. A D2HNet framework is developed to recover a high-quality image by deblurring and enhancing a long-exposure image under the guidance of a short-exposure image. To shrink the domain gap, we leverage a two-phase DeblurNet-EnhanceNet architecture, which performs accurate blur removal on a fixed low resolution so that it is able to handle large ranges of blur in different resolution inputs. In addition, we synthesize a D2-Dataset from HD videos and experiment on it. The results on the validation set and real photos demonstrate our methods achieve better visual quality and state-of-the-art quantitative scores. The D2HNet codes and D2-Dataset can be found at https://github.com/zhaoyuzhi/D2HNet.



### Exploring the sequence length bottleneck in the Transformer for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2207.03327v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03327v5)
- **Published**: 2022-07-07 14:37:02+00:00
- **Updated**: 2022-12-24 10:25:48+00:00
- **Authors**: Jia Cheng Hu, Roberto Cavicchioli, Alessandro Capotondi
- **Comment**: None
- **Journal**: None
- **Summary**: Most recent state of the art architectures rely on combinations and variations of three approaches: convolutional, recurrent and self-attentive methods. Our work attempts in laying the basis for a new research direction for sequence modeling based upon the idea of modifying the sequence length. In order to do that, we propose a new method called "Expansion Mechanism" which transforms either dynamically or statically the input sequence into a new one featuring a different sequence length. Furthermore, we introduce a novel architecture that exploits such method and achieves competitive performances on the MS-COCO 2014 data set, yielding 134.6 and 131.4 CIDEr-D on the Karpathy test split in the ensemble and single model configuration respectively and 130 CIDEr-D in the official online evaluation server, despite being neither recurrent nor fully attentive. At the same time we address the efficiency aspect in our design and introduce a convenient training strategy suitable for most computational resources in contrast to the standard one. Source code is available at https://github.com/jchenghu/exploring



### Enhancing Fairness of Visual Attribute Predictors
- **Arxiv ID**: http://arxiv.org/abs/2207.05727v3
- **DOI**: None
- **Categories**: **cs.CV**, math.PR
- **Links**: [PDF](http://arxiv.org/pdf/2207.05727v3)
- **Published**: 2022-07-07 15:02:04+00:00
- **Updated**: 2022-10-01 08:27:04+00:00
- **Authors**: Tobias H√§nel, Nishant Kumar, Dmitrij Schlesinger, Mengze Li, Erdem √únal, Abouzar Eslami, Stefan Gumhold
- **Comment**: Camera Ready, ACCV 2022
- **Journal**: None
- **Summary**: The performance of deep neural networks for image recognition tasks such as predicting a smiling face is known to degrade with under-represented classes of sensitive attributes. We address this problem by introducing fairness-aware regularization losses based on batch estimates of Demographic Parity, Equalized Odds, and a novel Intersection-over-Union measure. The experiments performed on facial and medical images from CelebA, UTKFace, and the SIIM-ISIC melanoma classification challenge show the effectiveness of our proposed fairness losses for bias mitigation as they improve model fairness while maintaining high classification performance. To the best of our knowledge, our work is the first attempt to incorporate these types of losses in an end-to-end training scheme for mitigating biases of visual attribute predictors. Our code is available at https://github.com/nish03/FVAP.



### A simple normalization technique using window statistics to improve the out-of-distribution generalization on medical images
- **Arxiv ID**: http://arxiv.org/abs/2207.03366v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03366v2)
- **Published**: 2022-07-07 15:14:37+00:00
- **Updated**: 2022-07-14 03:36:21+00:00
- **Authors**: Chengfeng Zhou, Songchang Chen, Chenming Xu, Jun Wang, Feng Liu, Chun Zhang, Juan Ye, Hefeng Huang, Dahong Qian
- **Comment**: None
- **Journal**: None
- **Summary**: Since data scarcity and data heterogeneity are prevailing for medical images, well-trained Convolutional Neural Networks (CNNs) using previous normalization methods may perform poorly when deployed to a new site. However, a reliable model for real-world clinical applications should be able to generalize well both on in-distribution (IND) and out-of-distribution (OOD) data (e.g., the new site data). In this study, we present a novel normalization technique called window normalization (WIN) to improve the model generalization on heterogeneous medical images, which is a simple yet effective alternative to existing normalization methods. Specifically, WIN perturbs the normalizing statistics with the local statistics computed on the window of features. This feature-level augmentation technique regularizes the models well and improves their OOD generalization significantly. Taking its advantage, we propose a novel self-distillation method called WIN-WIN for classification tasks. WIN-WIN is easily implemented with twice forward passes and a consistency constraint, which can be a simple extension for existing methods. Extensive experimental results on various tasks (6 tasks) and datasets (24 datasets) demonstrate the generality and effectiveness of our methods.



### Joint Super-Resolution and Inverse Tone-Mapping: A Feature Decomposition Aggregation Network and A New Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2207.03367v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03367v3)
- **Published**: 2022-07-07 15:16:36+00:00
- **Updated**: 2023-05-30 04:14:37+00:00
- **Authors**: Gang Xu, Yu-chen Yang, Liang Wang, Xian-Tong Zhen, Jun Xu
- **Comment**: update authors info
- **Journal**: None
- **Summary**: Joint Super-Resolution and Inverse Tone-Mapping (joint SR-ITM) aims to increase the resolution and dynamic range of low-resolution and standard dynamic range images. Recent networks mainly resort to image decomposition techniques with complex multi-branch architectures. However, the fixed decomposition techniques would largely restricts their power on versatile images. To exploit the potential power of decomposition mechanism, in this paper, we generalize it from the image domain to the broader feature domain. To this end, we propose a lightweight Feature Decomposition Aggregation Network (FDAN). In particular, we design a Feature Decomposition Block (FDB) to achieve learnable separation of detail and base feature maps, and develop a Hierarchical Feature Decomposition Group by cascading FDBs for powerful multi-level feature decomposition. Moreover, to better evaluate the comparison methods, we collect a large-scale dataset for joint SR-ITM, i.e., SRITM-4K, which provides versatile scenarios for robust model training and evaluation. Experimental results on two benchmark datasets demonstrate that our FDAN is efficient and outperforms state-of-the-art methods on joint SR-ITM. The code of our FDAN and the SRITM-4K dataset are available at https://github.com/CS-GangXu/FDAN.



### Diagnosing and Remedying Shot Sensitivity with Cosine Few-Shot Learners
- **Arxiv ID**: http://arxiv.org/abs/2207.03398v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03398v1)
- **Published**: 2022-07-07 16:05:28+00:00
- **Updated**: 2022-07-07 16:05:28+00:00
- **Authors**: Davis Wertheimer, Luming Tang, Bharath Hariharan
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot recognition involves training an image classifier to distinguish novel concepts at test time using few examples (shot). Existing approaches generally assume that the shot number at test time is known in advance. This is not realistic, and the performance of a popular and foundational method has been shown to suffer when train and test shots do not match. We conduct a systematic empirical study of this phenomenon. In line with prior work, we find that shot sensitivity is broadly present across metric-based few-shot learners, but in contrast to prior work, larger neural architectures provide a degree of built-in robustness to varying test shot. More importantly, a simple, previously known but greatly overlooked class of approaches based on cosine distance consistently and greatly improves robustness to shot variation, by removing sensitivity to sample noise. We derive cosine alternatives to popular and recent few-shot classifiers, broadening their applicability to realistic settings. These cosine models consistently improve shot-robustness, outperform prior shot-robust state of the art, and provide competitive accuracy on a range of benchmarks and architectures, including notable gains in the very-low-shot regime.



### Robust Watermarking for Video Forgery Detection with Improved Imperceptibility and Robustness
- **Arxiv ID**: http://arxiv.org/abs/2207.03409v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03409v1)
- **Published**: 2022-07-07 16:27:10+00:00
- **Updated**: 2022-07-07 16:27:10+00:00
- **Authors**: Yangming Zhou, Qichao Ying, Xiangyu Zhang, Zhenxing Qian, Sheng Li, Xinpeng Zhang
- **Comment**: Submitted to MMSP 2022
- **Journal**: None
- **Summary**: Videos are prone to tampering attacks that alter the meaning and deceive the audience. Previous video forgery detection schemes find tiny clues to locate the tampered areas. However, attackers can successfully evade supervision by destroying such clues using video compression or blurring. This paper proposes a video watermarking network for tampering localization. We jointly train a 3D-UNet-based watermark embedding network and a decoder that predicts the tampering mask. The perturbation made by watermark embedding is close to imperceptible. Considering that there is no off-the-shelf differentiable video codec simulator, we propose to mimic video compression by ensembling simulation results of other typical attacks, e.g., JPEG compression and blurring, as an approximation. Experimental results demonstrate that our method generates watermarked videos with good imperceptibility and robustly and accurately locates tampered areas within the attacked version.



### VecGAN: Image-to-Image Translation with Interpretable Latent Directions
- **Arxiv ID**: http://arxiv.org/abs/2207.03411v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03411v1)
- **Published**: 2022-07-07 16:31:05+00:00
- **Updated**: 2022-07-07 16:31:05+00:00
- **Authors**: Yusuf Dalva, Said Fahri Altindis, Aysegul Dundar
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: We propose VecGAN, an image-to-image translation framework for facial attribute editing with interpretable latent directions. Facial attribute editing task faces the challenges of precise attribute editing with controllable strength and preservation of the other attributes of an image. For this goal, we design the attribute editing by latent space factorization and for each attribute, we learn a linear direction that is orthogonal to the others. The other component is the controllable strength of the change, a scalar value. In our framework, this scalar can be either sampled or encoded from a reference image by projection. Our work is inspired by the latent space factorization works of fixed pretrained GANs. However, while those models cannot be trained end-to-end and struggle to edit encoded images precisely, VecGAN is end-to-end trained for image translation task and successful at editing an attribute while preserving the others. Our extensive experiments show that VecGAN achieves significant improvements over state-of-the-arts for both local and global edits.



### Domain Knowledge Driven 3D Dose Prediction Using Moment-Based Loss Function
- **Arxiv ID**: http://arxiv.org/abs/2207.03414v2
- **DOI**: 10.1088/1361-6560/ac8d45
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03414v2)
- **Published**: 2022-07-07 16:35:06+00:00
- **Updated**: 2022-09-05 15:08:32+00:00
- **Authors**: Gourav Jhanwar, Navdeep Dahiya, Parmida Ghahremani, Masoud Zarepisheh, Saad Nadeem
- **Comment**: Physics in Medicine & Biology 2022. **First two authors contributed
  equally. Last two authors are co-senior authors. arXiv admin note:
  substantial text overlap with arXiv:2106.03705
- **Journal**: None
- **Summary**: Dose volume histogram (DVH) metrics are widely accepted evaluation criteria in the clinic. However, incorporating these metrics into deep learning dose prediction models is challenging due to their non-convexity and non-differentiability. We propose a novel moment-based loss function for predicting 3D dose distribution for the challenging conventional lung intensity modulated radiation therapy (IMRT) plans. The moment-based loss function is convex and differentiable and can easily incorporate DVH metrics in any deep learning framework without computational overhead. The moments can also be customized to reflect the clinical priorities in 3D dose prediction. For instance, using high-order moments allows better prediction in high-dose areas for serial structures. We used a large dataset of 360 conventional lung patients with 2Gy $\times$ 30 fractions to train the deep learning (DL) model using clinically treated plans. We trained a UNet-like CNN architecture using computed tomography (CT), planning target volume (PTV) and organ-at-risk contours (OAR) as input to infer corresponding voxel-wise 3D dose distribution. We evaluated three different loss functions: (1) Mean Absolute Error (MAE) Loss, (2) MAE + DVH Loss, and (3) the proposed MAE + Moments Loss. The quality of the predictions was compared using different DVH metrics as well as dose-score and DVH-score, recently introduced by the AAPM knowledge-based planning grand challenge. Model with (MAE + Moment) loss function outperformed the model with MAE loss by significantly improving the DVH-score (11%, p$<$0.01) while having similar computational cost. It also outperformed the model trained with (MAE+DVH) by significantly improving the computational cost (48%) and the DVH-score (8%, p$<$0.01). The code, models, docker container, and Google Colab project are available on our DoseRTX GitHub (https://github.com/nadeemlab/DoseRTX).



### A Novel Unified Conditional Score-based Generative Framework for Multi-modal Medical Image Completion
- **Arxiv ID**: http://arxiv.org/abs/2207.03430v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03430v1)
- **Published**: 2022-07-07 16:57:21+00:00
- **Updated**: 2022-07-07 16:57:21+00:00
- **Authors**: Xiangxi Meng, Yuning Gu, Yongsheng Pan, Nizhuan Wang, Peng Xue, Mengkang Lu, Xuming He, Yiqiang Zhan, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modal medical image completion has been extensively applied to alleviate the missing modality issue in a wealth of multi-modal diagnostic tasks. However, for most existing synthesis methods, their inferences of missing modalities can collapse into a deterministic mapping from the available ones, ignoring the uncertainties inherent in the cross-modal relationships. Here, we propose the Unified Multi-Modal Conditional Score-based Generative Model (UMM-CSGM) to take advantage of Score-based Generative Model (SGM) in modeling and stochastically sampling a target probability distribution, and further extend SGM to cross-modal conditional synthesis for various missing-modality configurations in a unified framework. Specifically, UMM-CSGM employs a novel multi-in multi-out Conditional Score Network (mm-CSN) to learn a comprehensive set of cross-modal conditional distributions via conditional diffusion and reverse generation in the complete modality space. In this way, the generation process can be accurately conditioned by all available information, and can fit all possible configurations of missing modalities in a single network. Experiments on BraTS19 dataset show that the UMM-CSGM can more reliably synthesize the heterogeneous enhancement and irregular area in tumor-induced lesions for any missing modalities.



### Semi-supervised Object Detection via Virtual Category Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.03433v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03433v2)
- **Published**: 2022-07-07 16:59:53+00:00
- **Updated**: 2022-10-17 11:03:12+00:00
- **Authors**: Changrui Chen, Kurt Debattista, Jungong Han
- **Comment**: ECCV2022 Oral
- **Journal**: None
- **Summary**: Due to the costliness of labelled data in real-world applications, semi-supervised object detectors, underpinned by pseudo labelling, are appealing. However, handling confusing samples is nontrivial: discarding valuable confusing samples would compromise the model generalisation while using them for training would exacerbate the confirmation bias issue caused by inevitable mislabelling. To solve this problem, this paper proposes to use confusing samples proactively without label correction. Specifically, a virtual category (VC) is assigned to each confusing sample such that they can safely contribute to the model optimisation even without a concrete label. It is attributed to specifying the embedding distance between the training sample and the virtual category as the lower bound of the inter-class distance. Moreover, we also modify the localisation loss to allow high-quality boundaries for location regression. Extensive experiments demonstrate that the proposed VC learning significantly surpasses the state-of-the-art, especially with small amounts of available labels.



### LASSIE: Learning Articulated Shapes from Sparse Image Ensemble via 3D Part Discovery
- **Arxiv ID**: http://arxiv.org/abs/2207.03434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03434v1)
- **Published**: 2022-07-07 17:00:07+00:00
- **Updated**: 2022-07-07 17:00:07+00:00
- **Authors**: Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Rubinstein, Ming-Hsuan Yang, Varun Jampani
- **Comment**: None
- **Journal**: None
- **Summary**: Creating high-quality articulated 3D models of animals is challenging either via manual creation or using 3D scanning tools. Therefore, techniques to reconstruct articulated 3D objects from 2D images are crucial and highly useful. In this work, we propose a practical problem setting to estimate 3D pose and shape of animals given only a few (10-30) in-the-wild images of a particular animal species (say, horse). Contrary to existing works that rely on pre-defined template shapes, we do not assume any form of 2D or 3D ground-truth annotations, nor do we leverage any multi-view or temporal information. Moreover, each input image ensemble can contain animal instances with varying poses, backgrounds, illuminations, and textures. Our key insight is that 3D parts have much simpler shape compared to the overall animal and that they are robust w.r.t. animal pose articulations. Following these insights, we propose LASSIE, a novel optimization framework which discovers 3D parts in a self-supervised manner with minimal user intervention. A key driving force behind LASSIE is the enforcing of 2D-3D part consistency using self-supervisory deep features. Experiments on Pascal-Part and self-collected in-the-wild animal datasets demonstrate considerably better 3D reconstructions as well as both 2D and 3D part discovery compared to prior arts. Project page: chhankyao.github.io/lassie/



### Back to the Source: Diffusion-Driven Test-Time Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2207.03442v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03442v2)
- **Published**: 2022-07-07 17:14:10+00:00
- **Updated**: 2023-06-21 16:57:19+00:00
- **Authors**: Jin Gao, Jialing Zhang, Xihui Liu, Trevor Darrell, Evan Shelhamer, Dequan Wang
- **Comment**: published at CVPR 2023
- **Journal**: None
- **Summary**: Test-time adaptation harnesses test inputs to improve the accuracy of a model trained on source data when tested on shifted target data. Existing methods update the source model by (re-)training on each target domain. While effective, re-training is sensitive to the amount and order of the data and the hyperparameters for optimization. We instead update the target data, by projecting all test inputs toward the source domain with a generative diffusion model. Our diffusion-driven adaptation method, DDA, shares its models for classification and generation across all domains. Both models are trained on the source domain, then fixed during testing. We augment diffusion with image guidance and self-ensembling to automatically decide how much to adapt. Input adaptation by DDA is more robust than prior model adaptation approaches across a variety of corruptions, architectures, and data regimes on the ImageNet-C benchmark. With its input-wise updates, DDA succeeds where model adaptation degrades on too little data in small batches, dependent data in non-uniform order, or mixed data with multiple corruptions.



### Fairness and Bias in Robot Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.03444v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03444v1)
- **Published**: 2022-07-07 17:20:15+00:00
- **Updated**: 2022-07-07 17:20:15+00:00
- **Authors**: Laura Londo√±o, Juana Valeria Hurtado, Nora Hertz, Philipp Kellmeyer, Silja Voeneky, Abhinav Valada
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning has significantly enhanced the abilities of robots, enabling them to perform a wide range of tasks in human environments and adapt to our uncertain real world. Recent works in various domains of machine learning have highlighted the importance of accounting for fairness to ensure that these algorithms do not reproduce human biases and consequently lead to discriminatory outcomes. With robot learning systems increasingly performing more and more tasks in our everyday lives, it is crucial to understand the influence of such biases to prevent unintended behavior toward certain groups of people. In this work, we present the first survey on fairness in robot learning from an interdisciplinary perspective spanning technical, ethical, and legal challenges. We propose a taxonomy for sources of bias and the resulting types of discrimination due to them. Using examples from different robot learning domains, we examine scenarios of unfair outcomes and strategies to mitigate them. We present early advances in the field by covering different fairness definitions, ethical and legal considerations, and methods for fair robot learning. With this work, we aim at paving the road for groundbreaking developments in fair robot learning.



### Learning to restore images degraded by atmospheric turbulence using uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2207.03447v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03447v1)
- **Published**: 2022-07-07 17:24:52+00:00
- **Updated**: 2022-07-07 17:24:52+00:00
- **Authors**: Rajeev Yasarla, Vishal M. Patel
- **Comment**: Recognized as Best Paper at IEEE International Conference on Image
  Processing, 2021. arXiv admin note: substantial text overlap with
  arXiv:2007.08404
- **Journal**: None
- **Summary**: Atmospheric turbulence can significantly degrade the quality of images acquired by long-range imaging systems by causing spatially and temporally random fluctuations in the index of refraction of the atmosphere. Variations in the refractive index causes the captured images to be geometrically distorted and blurry. Hence, it is important to compensate for the visual degradation in images caused by atmospheric turbulence. In this paper, we propose a deep learning-based approach for restring a single image degraded by atmospheric turbulence. We make use of the epistemic uncertainty based on Monte Carlo dropouts to capture regions in the image where the network is having hard time restoring. The estimated uncertainty maps are then used to guide the network to obtain the restored image. Extensive experiments are conducted on synthetic and real images to show the significance of the proposed work. Code is available at : https://github.com/rajeevyasarla/AT-Net



### TFCNs: A CNN-Transformer Hybrid Network for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.03450v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.03450v1)
- **Published**: 2022-07-07 17:28:18+00:00
- **Updated**: 2022-07-07 17:28:18+00:00
- **Authors**: Zihan Li, Dihan Li, Cangbai Xu, Weice Wang, Qingqi Hong, Qingde Li, Jie Tian
- **Comment**: Accepted by ICANN 2022
- **Journal**: None
- **Summary**: Medical image segmentation is one of the most fundamental tasks concerning medical information analysis. Various solutions have been proposed so far, including many deep learning-based techniques, such as U-Net, FC-DenseNet, etc. However, high-precision medical image segmentation remains a highly challenging task due to the existence of inherent magnification and distortion in medical images as well as the presence of lesions with similar density to normal tissues. In this paper, we propose TFCNs (Transformers for Fully Convolutional denseNets) to tackle the problem by introducing ResLinear-Transformer (RL-Transformer) and Convolutional Linear Attention Block (CLAB) to FC-DenseNet. TFCNs is not only able to utilize more latent information from the CT images for feature extraction, but also can capture and disseminate semantic features and filter non-semantic features more effectively through the CLAB module. Our experimental results show that TFCNs can achieve state-of-the-art performance with dice scores of 83.72\% on the Synapse dataset. In addition, we evaluate the robustness of TFCNs for lesion area effects on the COVID-19 public datasets. The Python code will be made publicly available on https://github.com/HUANGLIZI/TFCNs.



### Red PANDA: Disambiguating Anomaly Detection by Removing Nuisance Factors
- **Arxiv ID**: http://arxiv.org/abs/2207.03478v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03478v1)
- **Published**: 2022-07-07 17:58:03+00:00
- **Updated**: 2022-07-07 17:58:03+00:00
- **Authors**: Niv Cohen, Jonathan Kahana, Yedid Hoshen
- **Comment**: None
- **Journal**: None
- **Summary**: Anomaly detection methods strive to discover patterns that differ from the norm in a semantic way. This goal is ambiguous as a data point differing from the norm by an attribute e.g., age, race or gender, may be considered anomalous by some operators while others may consider this attribute irrelevant. Breaking from previous research, we present a new anomaly detection method that allows operators to exclude an attribute from being considered as relevant for anomaly detection. Our approach then learns representations which do not contain information over the nuisance attributes. Anomaly scoring is performed using a density-based approach. Importantly, our approach does not require specifying the attributes that are relevant for detecting anomalies, which is typically impossible in anomaly detection, but only attributes to ignore. An empirical investigation is presented verifying the effectiveness of our approach.



### Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.03482v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.03482v3)
- **Published**: 2022-07-07 17:59:56+00:00
- **Updated**: 2022-11-29 17:47:13+00:00
- **Authors**: Hanoona Rasheed, Muhammad Maaz, Muhammad Uzair Khattak, Salman Khan, Fahad Shahbaz Khan
- **Comment**: Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: Existing open-vocabulary object detectors typically enlarge their vocabulary sizes by leveraging different forms of weak supervision. This helps generalize to novel objects at inference. Two popular forms of weak-supervision used in open-vocabulary detection (OVD) include pretrained CLIP model and image-level supervision. We note that both these modes of supervision are not optimally aligned for the detection task: CLIP is trained with image-text pairs and lacks precise localization of objects while the image-level supervision has been used with heuristics that do not accurately specify local object regions. In this work, we propose to address this problem by performing object-centric alignment of the language embeddings from the CLIP model. Furthermore, we visually ground the objects with only image-level supervision using a pseudo-labeling process that provides high-quality object proposals and helps expand the vocabulary during training. We establish a bridge between the above two object-alignment strategies via a novel weight transfer function that aggregates their complimentary strengths. In essence, the proposed model seeks to minimize the gap between object and image-centric representations in the OVD setting. On the COCO benchmark, our proposed approach achieves 36.6 AP50 on novel classes, an absolute 8.2 gain over the previous best performance. For LVIS, we surpass the state-of-the-art ViLD model by 5.0 mask AP for rare categories and 3.4 overall. Code: https://github.com/hanoonaR/object-centric-ovd.



### Finding Fallen Objects Via Asynchronous Audio-Visual Integration
- **Arxiv ID**: http://arxiv.org/abs/2207.03483v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2207.03483v1)
- **Published**: 2022-07-07 17:59:59+00:00
- **Updated**: 2022-07-07 17:59:59+00:00
- **Authors**: Chuang Gan, Yi Gu, Siyuan Zhou, Jeremy Schwartz, Seth Alter, James Traer, Dan Gutfreund, Joshua B. Tenenbaum, Josh McDermott, Antonio Torralba
- **Comment**: CVPR 2022. Project page: http://fallen-object.csail.mit.edu
- **Journal**: None
- **Summary**: The way an object looks and sounds provide complementary reflections of its physical properties. In many settings cues from vision and audition arrive asynchronously but must be integrated, as when we hear an object dropped on the floor and then must find it. In this paper, we introduce a setting in which to study multi-modal object localization in 3D virtual environments. An object is dropped somewhere in a room. An embodied robot agent, equipped with a camera and microphone, must determine what object has been dropped -- and where -- by combining audio and visual signals with knowledge of the underlying physics. To study this problem, we have generated a large-scale dataset -- the Fallen Objects dataset -- that includes 8000 instances of 30 physical object categories in 64 rooms. The dataset uses the ThreeDWorld platform which can simulate physics-based impact sounds and complex physical interactions between objects in a photorealistic setting. As a first step toward addressing this challenge, we develop a set of embodied agent baselines, based on imitation learning, reinforcement learning, and modular planning, and perform an in-depth analysis of the challenge of this new task.



### False Negative Reduction in Semantic Segmentation under Domain Shift using Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2207.03513v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03513v2)
- **Published**: 2022-07-07 18:02:29+00:00
- **Updated**: 2022-12-07 14:16:37+00:00
- **Authors**: Kira Maag, Matthias Rottmann
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art deep neural networks demonstrate outstanding performance in semantic segmentation. However, their performance is tied to the domain represented by the training data. Open world scenarios cause inaccurate predictions which is hazardous in safety relevant applications like automated driving. In this work, we enhance semantic segmentation predictions using monocular depth estimation to improve segmentation by reducing the occurrence of non-detected objects in presence of domain shift. To this end, we infer a depth heatmap via a modified segmentation network which generates foreground-background masks, operating in parallel to a given semantic segmentation network. Both segmentation masks are aggregated with a focus on foreground classes (here road users) to reduce false negatives. To also reduce the occurrence of false positives, we apply a pruning based on uncertainty estimates. Our approach is modular in a sense that it post-processes the output of any semantic segmentation network. In our experiments, we observe less non-detected objects of most important classes and an enhanced generalization to other domains compared to the basic semantic segmentation prediction.



### Should All Proposals be Treated Equally in Object Detection?
- **Arxiv ID**: http://arxiv.org/abs/2207.03520v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03520v1)
- **Published**: 2022-07-07 18:26:32+00:00
- **Updated**: 2022-07-07 18:26:32+00:00
- **Authors**: Yunsheng Li, Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Pei Yu, Jing Yin, Lu Yuan, Zicheng Liu, Nuno Vasconcelos
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: The complexity-precision trade-off of an object detector is a critical problem for resource constrained vision tasks. Previous works have emphasized detectors implemented with efficient backbones. The impact on this trade-off of proposal processing by the detection head is investigated in this work. It is hypothesized that improved detection efficiency requires a paradigm shift, towards the unequal processing of proposals, assigning more computation to good proposals than poor ones. This results in better utilization of available computational budget, enabling higher accuracy for the same FLOPS. We formulate this as a learning problem where the goal is to assign operators to proposals, in the detection head, so that the total computational cost is constrained and the precision is maximized. The key finding is that such matching can be learned as a function that maps each proposal embedding into a one-hot code over operators. While this function induces a complex dynamic network routing mechanism, it can be implemented by a simple MLP and learned end-to-end with off-the-shelf object detectors. This 'dynamic proposal processing' (DPP) is shown to outperform state-of-the-art end-to-end object detectors (DETR, Sparse R-CNN) by a clear margin for a given computational complexity.



### RWT-SLAM: Robust Visual SLAM for Highly Weak-textured Environments
- **Arxiv ID**: http://arxiv.org/abs/2207.03539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03539v1)
- **Published**: 2022-07-07 19:24:03+00:00
- **Updated**: 2022-07-07 19:24:03+00:00
- **Authors**: Qihao Peng, Zhiyu Xiang, YuanGang Fan, Tengqi Zhao, Xijun Zhao
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: As a fundamental task for intelligent robots, visual SLAM has made great progress over the past decades. However, robust SLAM under highly weak-textured environments still remains very challenging. In this paper, we propose a novel visual SLAM system named RWT-SLAM to tackle this problem. We modify LoFTR network which is able to produce dense point matching under low-textured scenes to generate feature descriptors. To integrate the new features into the popular ORB-SLAM framework, we develop feature masks to filter out the unreliable features and employ KNN strategy to strengthen the matching robustness. We also retrained visual vocabulary upon new descriptors for efficient loop closing. The resulting RWT-SLAM is tested in various public datasets such as TUM and OpenLORIS, as well as our own data. The results shows very promising performance under highly weak-textured environments.



### Highlight Specular Reflection Separation based on Tensor Low-rank and Sparse Decomposition Using Polarimetric Cues
- **Arxiv ID**: http://arxiv.org/abs/2207.03543v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2207.03543v1)
- **Published**: 2022-07-07 19:28:46+00:00
- **Updated**: 2022-07-07 19:28:46+00:00
- **Authors**: Moein Shakeri, Hong Zhang
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: This paper is concerned with specular reflection removal based on tensor low-rank decomposition framework with the help of polarization information. Our method is motivated by the observation that the specular highlight of an image is sparsely distributed while the remaining diffuse reflection can be well approximated by a linear combination of several distinct colors using a low-rank and sparse decomposition framework. Unlike current solutions, our tensor low-rank decomposition keeps the spatial structure of specular and diffuse information which enables us to recover the diffuse image under strong specular reflection or in saturated regions. We further define and impose a new polarization regularization term as constraint on color channels. This regularization boosts the performance of the method to recover an accurate diffuse image by handling the color distortion, a common problem of chromaticity-based methods, especially in case of strong specular reflection. Through comprehensive experiments on both synthetic and real polarization images, we demonstrate that our method is able to significantly improve the accuracy of highlight specular removal, and outperform the competitive methods to recover the diffuse image, especially in regions of strong specular reflection or in saturated areas.



### An Embedding-Dynamic Approach to Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.03552v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03552v1)
- **Published**: 2022-07-07 19:56:20+00:00
- **Updated**: 2022-07-07 19:56:20+00:00
- **Authors**: Suhong Moon, Domas Buracas, Seunghyun Park, Jinkyu Kim, John Canny
- **Comment**: 24 pages, 3 figures, submitted to CVPR 2022
- **Journal**: None
- **Summary**: A number of recent self-supervised learning methods have shown impressive performance on image classification and other tasks. A somewhat bewildering variety of techniques have been used, not always with a clear understanding of the reasons for their benefits, especially when used in combination. Here we treat the embeddings of images as point particles and consider model optimization as a dynamic process on this system of particles. Our dynamic model combines an attractive force for similar images, a locally dispersive force to avoid local collapse, and a global dispersive force to achieve a globally-homogeneous distribution of particles. The dynamic perspective highlights the advantage of using a delayed-parameter image embedding (a la BYOL) together with multiple views of the same image. It also uses a purely-dynamic local dispersive force (Brownian motion) that shows improved performance over other methods and does not require knowledge of other particle coordinates. The method is called MSBReg which stands for (i) a Multiview centroid loss, which applies an attractive force to pull different image view embeddings toward their centroid, (ii) a Singular value loss, which pushes the particle system toward spatially homogeneous density, (iii) a Brownian diffusive loss. We evaluate downstream classification performance of MSBReg on ImageNet as well as transfer learning tasks including fine-grained classification, multi-class object classification, object detection, and instance segmentation. In addition, we also show that applying our regularization term to other methods further improves their performance and stabilize the training by preventing a mode collapse.



### Mirror Complementary Transformer Network for RGB-thermal Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.03558v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03558v1)
- **Published**: 2022-07-07 20:26:09+00:00
- **Updated**: 2022-07-07 20:26:09+00:00
- **Authors**: Xiurong Jiang, Lin Zhu, Yifan Hou, Hui Tian
- **Comment**: None
- **Journal**: None
- **Summary**: RGB-thermal salient object detection (RGB-T SOD) aims to locate the common prominent objects of an aligned visible and thermal infrared image pair and accurately segment all the pixels belonging to those objects. It is promising in challenging scenes such as nighttime and complex backgrounds due to the insensitivity to lighting conditions of thermal images. Thus, the key problem of RGB-T SOD is to make the features from the two modalities complement and adjust each other flexibly, since it is inevitable that any modalities of RGB-T image pairs failure due to challenging scenes such as extreme light conditions and thermal crossover. In this paper, we propose a novel mirror complementary Transformer network (MCNet) for RGB-T SOD. Specifically, we introduce a Transformer-based feature extraction module to effective extract hierarchical features of RGB and thermal images. Then, through the attention-based feature interaction and serial multiscale dilated convolution (SDC) based feature fusion modules, the proposed model achieves the complementary interaction of low-level features and the semantic fusion of deep features. Finally, based on the mirror complementary structure, the salient regions of the two modalities can be accurately extracted even one modality is invalid. To demonstrate the robustness of the proposed model under challenging scenes in real world, we build a novel RGB-T SOD dataset VT723 based on a large public semantic segmentation RGB-T dataset used in the autonomous driving domain. Expensive experiments on benchmark and VT723 datasets show that the proposed method outperforms state-of-the-art approaches, including CNN-based and Transformer-based methods. The code and dataset will be released later at https://github.com/jxr326/SwinMCNet.



### The use of deep learning enables high diagnostic accuracy in detecting syndesmotic instability on weight-bearing CT scanning
- **Arxiv ID**: http://arxiv.org/abs/2207.03568v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03568v1)
- **Published**: 2022-07-07 20:49:37+00:00
- **Updated**: 2022-07-07 20:49:37+00:00
- **Authors**: Alireza Borjali, Soheil Ashkani-Esfahani, Rohan Bhimani, Daniel Guss, Orhun K. Muratoglu, Christopher W. DiGiovanni, Kartik Mangudi Varadarajan, Bart Lubberts
- **Comment**: None
- **Journal**: None
- **Summary**: Delayed diagnosis of syndesmosis instability can lead to significant morbidity and accelerated arthritic change in the ankle joint. Weight-bearing computed tomography (WBCT) has shown promising potential for early and reliable detection of isolated syndesmotic instability using 3D volumetric measurements. While these measurements have been reported to be highly accurate, they are also experience-dependent, time-consuming, and need a particular 3D measurement software tool that leads the clinicians to still show more interest in the conventional diagnostic methods for syndesmotic instability. The purpose of this study was to increase accuracy, accelerate analysis time, and reduce inter-observer bias by automating 3D volume assessment of syndesmosis anatomy using WBCT scans. We conducted a retrospective study using previously collected WBCT scans of patients with unilateral syndesmotic instability. 144 bilateral ankle WBCT scans were evaluated (48 unstable, 96 control). We developed three deep learning (DL) models for analyzing WBCT scans to recognize syndesmosis instability. These three models included two state-of-the-art models (Model 1 - 3D convolutional neural network [CNN], and Model 2 - CNN with long short-term memory [LSTM]), and a new model (Model 3 - differential CNN LSTM) that we introduced in this study. Model 1 failed to analyze the WBCT scans (F1-score = 0). Model 2 only misclassified two cases (F1-score = 0.80). Model 3 outperformed Model 2 and achieved a nearly perfect performance, misclassifying only one case (F1-score = 0.91) in the control group as unstable while being faster than Model 2.



### GaitTAKE: Gait Recognition by Temporal Attention and Keypoint-guided Embedding
- **Arxiv ID**: http://arxiv.org/abs/2207.03608v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03608v2)
- **Published**: 2022-07-07 22:38:54+00:00
- **Updated**: 2022-07-12 18:05:58+00:00
- **Authors**: Hung-Min Hsu, Yizhou Wang, Cheng-Yen Yang, Jenq-Neng Hwang, Hoang Le Uyen Thuc, Kwang-Ju Kim
- **Comment**: IEEE International Conference on Image Processing 2022
- **Journal**: None
- **Summary**: Gait recognition, which refers to the recognition or identification of a person based on their body shape and walking styles, derived from video data captured from a distance, is widely used in crime prevention, forensic identification, and social security. However, to the best of our knowledge, most of the existing methods use appearance, posture and temporal feautures without considering a learned temporal attention mechanism for global and local information fusion. In this paper, we propose a novel gait recognition framework, called Temporal Attention and Keypoint-guided Embedding (GaitTAKE), which effectively fuses temporal-attention-based global and local appearance feature and temporal aggregated human pose feature. Experimental results show that our proposed method achieves a new SOTA in gait recognition with rank-1 accuracy of 98.0% (normal), 97.5% (bag) and 92.2% (coat) on the CASIA-B gait dataset; 90.4% accuracy on the OU-MVLP gait dataset.



### Correspondences between word learning in children and captioning models
- **Arxiv ID**: http://arxiv.org/abs/2207.09847v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.09847v2)
- **Published**: 2022-07-07 22:49:32+00:00
- **Updated**: 2022-10-10 13:40:11+00:00
- **Authors**: Sunayana Rane, Mira L. Nencheva, Zeyu Wang, Casey Lew-Williams, Olga Russakovsky, Thomas L. Griffiths
- **Comment**: None
- **Journal**: None
- **Summary**: For human children as well as machine learning systems, a key challenge in learning a word is linking the word to the visual phenomena it describes. By organizing model output into word categories used to analyze child language learning data, we show a correspondence between word learning in children and the performance of image captioning models. Although captioning models are trained only on standard machine learning data, we find that their performance in producing words from a variety of word categories correlates with the age at which children acquire words from each of those categories. To explain why this correspondence exists, we show that the performance of captioning models is correlated with human judgments of the concreteness of words, suggesting that these models are capturing the complex real-world association between words and visual phenomena.



### PoseGU: 3D Human Pose Estimation with Novel Human Pose Generator and Unbiased Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.03618v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.03618v1)
- **Published**: 2022-07-07 23:43:53+00:00
- **Updated**: 2022-07-07 23:43:53+00:00
- **Authors**: Shannan Guan, Haiyan Lu, Linchao Zhu, Gengfa Fang
- **Comment**: None
- **Journal**: None
- **Summary**: 3D pose estimation has recently gained substantial interests in computer vision domain. Existing 3D pose estimation methods have a strong reliance on large size well-annotated 3D pose datasets, and they suffer poor model generalization on unseen poses due to limited diversity of 3D poses in training sets. In this work, we propose PoseGU, a novel human pose generator that generates diverse poses with access only to a small size of seed samples, while equipping the Counterfactual Risk Minimization to pursue an unbiased evaluation objective. Extensive experiments demonstrate PoseGU outforms almost all the state-of-the-art 3D human pose methods under consideration over three popular benchmark datasets. Empirical analysis also proves PoseGU generates 3D poses with improved data diversity and better generalization ability.



### More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity
- **Arxiv ID**: http://arxiv.org/abs/2207.03620v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.03620v3)
- **Published**: 2022-07-07 23:55:52+00:00
- **Updated**: 2023-03-03 19:04:38+00:00
- **Authors**: Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi K√§rkk√§inen, Mykola Pechenizkiy, Decebal Mocanu, Zhangyang Wang
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Transformers have quickly shined in the computer vision world since the emergence of Vision Transformers (ViTs). The dominant role of convolutional neural networks (CNNs) seems to be challenged by increasingly effective transformer-based models. Very recently, a couple of advanced convolutional models strike back with large kernels motivated by the local-window attention mechanism, showing appealing performance and efficiency. While one of them, i.e. RepLKNet, impressively manages to scale the kernel size to 31x31 with improved performance, the performance starts to saturate as the kernel size continues growing, compared to the scaling trend of advanced ViTs such as Swin Transformer. In this paper, we explore the possibility of training extreme convolutions larger than 31x31 and test whether the performance gap can be eliminated by strategically enlarging convolutions. This study ends up with a recipe for applying extremely large kernels from the perspective of sparsity, which can smoothly scale up kernels to 61x61 with better performance. Built on this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN architecture equipped with sparse factorized 51x51 kernels that can perform on par with or better than state-of-the-art hierarchical Transformers and modern ConvNet architectures like ConvNeXt and RepLKNet, on ImageNet classification as well as a wide range of downstream tasks including semantic segmentation on ADE20K, object detection on PASCAL VOC 2007, and object detection/segmentation on MS COCO.



