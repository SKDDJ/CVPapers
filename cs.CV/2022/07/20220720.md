# Arxiv Papers in cs.CV on 2022-07-20
### AiATrack: Attention in Attention for Transformer Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2207.09603v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09603v2)
- **Published**: 2022-07-20 00:44:03+00:00
- **Updated**: 2022-07-22 13:26:14+00:00
- **Authors**: Shenyuan Gao, Chunluan Zhou, Chao Ma, Xinggang Wang, Junsong Yuan
- **Comment**: Accepted by ECCV 2022. Code and models are publicly available at
  https://github.com/Little-Podi/AiATrack
- **Journal**: None
- **Summary**: Transformer trackers have achieved impressive advancements recently, where the attention mechanism plays an important role. However, the independent correlation computation in the attention mechanism could result in noisy and ambiguous attention weights, which inhibits further performance improvement. To address this issue, we propose an attention in attention (AiA) module, which enhances appropriate correlations and suppresses erroneous ones by seeking consensus among all correlation vectors. Our AiA module can be readily applied to both self-attention blocks and cross-attention blocks to facilitate feature aggregation and information propagation for visual tracking. Moreover, we propose a streamlined Transformer tracking framework, dubbed AiATrack, by introducing efficient feature reuse and target-background embeddings to make full use of temporal references. Experiments show that our tracker achieves state-of-the-art performance on six tracking benchmarks while running at a real-time speed.



### Towards Accurate and Robust Classification in Continuously Transitioning Industrial Sprays with Mixup
- **Arxiv ID**: http://arxiv.org/abs/2207.09609v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.09609v1)
- **Published**: 2022-07-20 01:15:47+00:00
- **Updated**: 2022-07-20 01:15:47+00:00
- **Authors**: Hongjiang Li, Huanyi Shui, Alemayehu Admasu, Praveen Narayanan, Devesh Upadhyay
- **Comment**: 9 pages, 5 figures, 5 tables
- **Journal**: None
- **Summary**: Image classification with deep neural networks has seen a surge of technological breakthroughs with promising applications in areas such as face recognition, medical imaging, and autonomous driving. In engineering problems, however, such as high-speed imaging of engine fuel injector sprays or body paint sprays, deep neural networks face a fundamental challenge related to the availability of adequate and diverse data. Typically, only thousands or sometimes even hundreds of samples are available for training. In addition, the transition between different spray classes is a continuum and requires a high level of domain expertise to label the images accurately. In this work, we used Mixup as an approach to systematically deal with the data scarcity and ambiguous class boundaries found in industrial spray applications. We show that data augmentation can mitigate the over-fitting problem of large neural networks on small data sets, to a certain level, but cannot fundamentally resolve the issue. We discuss how a convex linear interpolation of different classes naturally aligns with the continuous transition between different classes in our application. Our experiments demonstrate Mixup as a simple yet effective method to train an accurate and robust deep neural network classifier with only a few hundred samples.



### Unsupervised Deep Multi-Shape Matching
- **Arxiv ID**: http://arxiv.org/abs/2207.09610v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2207.09610v1)
- **Published**: 2022-07-20 01:22:08+00:00
- **Updated**: 2022-07-20 01:22:08+00:00
- **Authors**: Dongliang Cao, Florian Bernard
- **Comment**: to be published in ECCV2022
- **Journal**: None
- **Summary**: 3D shape matching is a long-standing problem in computer vision and computer graphics. While deep neural networks were shown to lead to state-of-the-art results in shape matching, existing learning-based approaches are limited in the context of multi-shape matching: (i) either they focus on matching pairs of shapes only and thus suffer from cycle-inconsistent multi-matchings, or (ii) they require an explicit template shape to address the matching of a collection of shapes. In this paper, we present a novel approach for deep multi-shape matching that ensures cycle-consistent multi-matchings while not depending on an explicit template shape. To this end, we utilise a shape-to-universe multi-matching representation that we combine with powerful functional map regularisation, so that our multi-shape matching neural network can be trained in a fully unsupervised manner. While the functional map regularisation is only considered during training time, functional maps are not computed for predicting correspondences, thereby allowing for fast inference. We demonstrate that our method achieves state-of-the-art results on several challenging benchmark datasets, and, most remarkably, that our unsupervised method even outperforms recent supervised methods.



### Exploiting Domain Transferability for Collaborative Inter-level Domain Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.09613v1
- **DOI**: 10.1016/j.eswa.2022.117697
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09613v1)
- **Published**: 2022-07-20 01:50:26+00:00
- **Updated**: 2022-07-20 01:50:26+00:00
- **Authors**: Mirae Do, Seogkyu Jeon, Pilhyeon Lee, Kibeom Hong, Yu-seung Ma, Hyeran Byun
- **Comment**: Accepted to Expert Systems with Applications. The first three authors
  contributed equally
- **Journal**: Expert Systems with Applications 205 (2022): 117697
- **Summary**: Domain adaptation for object detection (DAOD) has recently drawn much attention owing to its capability of detecting target objects without any annotations. To tackle the problem, previous works focus on aligning features extracted from partial levels (e.g., image-level, instance-level, RPN-level) in a two-stage detector via adversarial training. However, individual levels in the object detection pipeline are closely related to each other and this inter-level relation is unconsidered yet. To this end, we introduce a novel framework for DAOD with three proposed components: Multi-scale-aware Uncertainty Attention (MUA), Transferable Region Proposal Network (TRPN), and Dynamic Instance Sampling (DIS). With these modules, we seek to reduce the negative transfer effect during training while maximizing transferability as well as discriminability in both domains. Finally, our framework implicitly learns domain invariant regions for object detection via exploiting the transferable information and enhances the complementarity between different detection levels by collaboratively utilizing their domain information. Through ablation studies and experiments, we show that the proposed modules contribute to the performance improvement in a synergic way, demonstrating the effectiveness of our method. Moreover, our model achieves a new state-of-the-art performance on various benchmarks.



### Overlooked factors in concept-based explanations: Dataset choice, concept learnability, and human capability
- **Arxiv ID**: http://arxiv.org/abs/2207.09615v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09615v2)
- **Published**: 2022-07-20 01:59:39+00:00
- **Updated**: 2023-05-12 15:48:51+00:00
- **Authors**: Vikram V. Ramaswamy, Sunnie S. Y. Kim, Ruth Fong, Olga Russakovsky
- **Comment**: Published at CVPR 2023
- **Journal**: None
- **Summary**: Concept-based interpretability methods aim to explain deep neural network model predictions using a predefined set of semantic concepts. These methods evaluate a trained model on a new, "probe" dataset and correlate model predictions with the visual concepts labeled in that dataset. Despite their popularity, they suffer from limitations that are not well-understood and articulated by the literature. In this work, we analyze three commonly overlooked factors in concept-based explanations. First, the choice of the probe dataset has a profound impact on the generated explanations. Our analysis reveals that different probe datasets may lead to very different explanations, and suggests that the explanations are not generalizable outside the probe dataset. Second, we find that concepts in the probe dataset are often less salient and harder to learn than the classes they claim to explain, calling into question the correctness of the explanations. We argue that only visually salient concepts should be used in concept-based explanations. Finally, while existing methods use hundreds or even thousands of concepts, our human studies reveal a much stricter upper bound of 32 concepts or less, beyond which the explanations are much less practically useful. We make suggestions for future development and analysis of concept-based interpretability methods. Code for our analysis and user interface can be found at \url{https://github.com/princetonvisualai/OverlookedFactors}



### Learning from few examples: Classifying sex from retinal images via deep learning
- **Arxiv ID**: http://arxiv.org/abs/2207.09624v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T07, 62P10, 92C50, 92C55, 94A08, 94A12, I.2.1; I.4.7; I.4.9; I.4.10; I.5.1; I.5.2; I.5.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2207.09624v1)
- **Published**: 2022-07-20 02:47:29+00:00
- **Updated**: 2022-07-20 02:47:29+00:00
- **Authors**: Aaron Berk, Gulcenur Ozturan, Parsa Delavari, David Maberley, Özgür Yılmaz, Ipek Oruc
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has seen tremendous interest in medical imaging, particularly in the use of convolutional neural networks (CNNs) for developing automated diagnostic tools. The facility of its non-invasive acquisition makes retinal fundus imaging amenable to such automated approaches. Recent work in analyzing fundus images using CNNs relies on access to massive data for training and validation - hundreds of thousands of images. However, data residency and data privacy restrictions stymie the applicability of this approach in medical settings where patient confidentiality is a mandate. Here, we showcase results for the performance of DL on small datasets to classify patient sex from fundus images - a trait thought not to be present or quantifiable in fundus images until recently. We fine-tune a Resnet-152 model whose last layer has been modified for binary classification. In several experiments, we assess performance in the small dataset context using one private (DOVS) and one public (ODIR) data source. Our models, developed using approximately 2500 fundus images, achieved test AUC scores of up to 0.72 (95% CI: [0.67, 0.77]). This corresponds to a mere 25% decrease in performance despite a nearly 1000-fold decrease in the dataset size compared to prior work in the literature. Even with a hard task like sex categorization from retinal images, we find that classification is possible with very small datasets. Additionally, we perform domain adaptation experiments between DOVS and ODIR; explore the effect of data curation on training and generalizability; and investigate model ensembling to maximize CNN classifier performance in the context of small development datasets.



### Explicit Image Caption Editing
- **Arxiv ID**: http://arxiv.org/abs/2207.09625v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09625v1)
- **Published**: 2022-07-20 02:54:43+00:00
- **Updated**: 2022-07-20 02:54:43+00:00
- **Authors**: Zhen Wang, Long Chen, Wenbo Ma, Guangxing Han, Yulei Niu, Jian Shao, Jun Xiao
- **Comment**: ECCV 2022, dataset and code are available at
  https://github.com/baaaad/ECE
- **Journal**: None
- **Summary**: Given an image and a reference caption, the image caption editing task aims to correct the misalignment errors and generate a refined caption. However, all existing caption editing works are implicit models, ie, they directly produce the refined captions without explicit connections to the reference captions. In this paper, we introduce a new task: Explicit Caption Editing (ECE). ECE models explicitly generate a sequence of edit operations, and this edit operation sequence can translate the reference caption into a refined one. Compared to the implicit editing, ECE has multiple advantages: 1) Explainable: it can trace the whole editing path. 2) Editing Efficient: it only needs to modify a few words. 3) Human-like: it resembles the way that humans perform caption editing, and tries to keep original sentence structures. To solve this new task, we propose the first ECE model: TIger. TIger is a non-autoregressive transformer-based model, consisting of three modules: Tagger_del, Tagger_add, and Inserter. Specifically, Tagger_del decides whether each word should be preserved or not, Tagger_add decides where to add new words, and Inserter predicts the specific word for adding. To further facilitate ECE research, we propose two new ECE benchmarks by re-organizing two existing datasets, dubbed COCO-EE and Flickr30K-EE, respectively. Extensive ablations on both two benchmarks have demonstrated the effectiveness of TIger.



### EVHA: Explainable Vision System for Hardware Testing and Assurance -- An Overview
- **Arxiv ID**: http://arxiv.org/abs/2207.09627v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2207.09627v1)
- **Published**: 2022-07-20 02:58:46+00:00
- **Updated**: 2022-07-20 02:58:46+00:00
- **Authors**: Md Mahfuz Al Hasan, Mohammad Tahsin Mostafiz, Thomas An Le, Jake Julia, Nidish Vashistha, Shayan Taheri, Navid Asadizanjani
- **Comment**: Please contact Dr. Shayan Taheri for any questions and/or comments
  regarding the paper arXiv submission at: "www.shayan-taheri.com". The Paper
  Initial Submission: The ACM Journal on Emerging Technologies in Computing
  Systems (JETC)
- **Journal**: None
- **Summary**: Due to the ever-growing demands for electronic chips in different sectors the semiconductor companies have been mandated to offshore their manufacturing processes. This unwanted matter has made security and trustworthiness of their fabricated chips concerning and caused creation of hardware attacks. In this condition, different entities in the semiconductor supply chain can act maliciously and execute an attack on the design computing layers, from devices to systems. Our attack is a hardware Trojan that is inserted during mask generation/fabrication in an untrusted foundry. The Trojan leaves a footprint in the fabricated through addition, deletion, or change of design cells. In order to tackle this problem, we propose Explainable Vision System for Hardware Testing and Assurance (EVHA) in this work that can detect the smallest possible change to a design in a low-cost, accurate, and fast manner. The inputs to this system are Scanning Electron Microscopy (SEM) images acquired from the Integrated Circuits (ICs) under examination. The system output is determination of IC status in terms of having any defect and/or hardware Trojan through addition, deletion, or change in the design cells at the cell-level. This article provides an overview on the design, development, implementation, and analysis of our defense system.



### Perspective Phase Angle Model for Polarimetric 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2207.09629v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09629v2)
- **Published**: 2022-07-20 03:11:30+00:00
- **Updated**: 2022-07-21 03:15:54+00:00
- **Authors**: Guangcheng Chen, Li He, Yisheng Guan, Hong Zhang
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Current polarimetric 3D reconstruction methods, including those in the well-established shape from polarization literature, are all developed under the orthographic projection assumption. In the case of a large field of view, however, this assumption does not hold and may result in significant reconstruction errors in methods that make this assumption. To address this problem, we present the perspective phase angle (PPA) model that is applicable to perspective cameras. Compared with the orthographic model, the proposed PPA model accurately describes the relationship between polarization phase angle and surface normal under perspective projection. In addition, the PPA model makes it possible to estimate surface normals from only one single-view phase angle map and does not suffer from the so-called $\pi$-ambiguity problem. Experiments on real data show that the PPA model is more accurate for surface normal estimation with a perspective camera than the orthographic model.



### HyperNet: Self-Supervised Hyperspectral Spatial-Spectral Feature Understanding Network for Hyperspectral Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.09634v1
- **DOI**: 10.1109/TGRS.2022.3218795
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09634v1)
- **Published**: 2022-07-20 03:26:03+00:00
- **Updated**: 2022-07-20 03:26:03+00:00
- **Authors**: Meiqi Hu, Chen Wu, Liangpei Zhang
- **Comment**: 14 pages, 17 figures
- **Journal**: None
- **Summary**: The fast development of self-supervised learning lowers the bar learning feature representation from massive unlabeled data and has triggered a series of research on change detection of remote sensing images. Challenges in adapting self-supervised learning from natural images classification to remote sensing images change detection arise from difference between the two tasks. The learned patch-level feature representations are not satisfying for the pixel-level precise change detection. In this paper, we proposed a novel pixel-level self-supervised hyperspectral spatial-spectral understanding network (HyperNet) to accomplish pixel-wise feature representation for effective hyperspectral change detection. Concretely, not patches but the whole images are fed into the network and the multi-temporal spatial-spectral features are compared pixel by pixel. Instead of processing the two-dimensional imaging space and spectral response dimension in hybrid style, a powerful spatial-spectral attention module is put forward to explore the spatial correlation and discriminative spectral features of multi-temporal hyperspectral images (HSIs), separately. Only the positive samples at the same location of bi-temporal HSIs are created and forced to be aligned, aiming at learning the spectral difference-invariant features. Moreover, a new similarity loss function named focal cosine is proposed to solve the problem of imbalanced easy and hard positive samples comparison, where the weights of those hard samples are enlarged and highlighted to promote the network training. Six hyperspectral datasets have been adopted to test the validity and generalization of proposed HyperNet. The extensive experiments demonstrate the superiority of HyperNet over the state-of-the-art algorithms on downstream hyperspectral change detection tasks.



### DC-BENCH: Dataset Condensation Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2207.09639v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.09639v2)
- **Published**: 2022-07-20 03:54:05+00:00
- **Updated**: 2022-10-17 07:47:01+00:00
- **Authors**: Justin Cui, Ruochen Wang, Si Si, Cho-Jui Hsieh
- **Comment**: None
- **Journal**: None
- **Summary**: Dataset Condensation is a newly emerging technique aiming at learning a tiny dataset that captures the rich information encoded in the original dataset. As the size of datasets contemporary machine learning models rely on becomes increasingly large, condensation methods become a prominent direction for accelerating network training and reducing data storage. Despite numerous methods have been proposed in this rapidly growing field, evaluating and comparing different condensation methods is non-trivial and still remains an open issue. The quality of condensed dataset are often shadowed by many critical contributing factors to the end performance, such as data augmentation and model architectures. The lack of a systematic way to evaluate and compare condensation methods not only hinders our understanding of existing techniques, but also discourages practical usage of the synthesized datasets. This work provides the first large-scale standardized benchmark on Dataset Condensation. It consists of a suite of evaluations to comprehensively reflect the generability and effectiveness of condensation methods through the lens of their generated dataset. Leveraging this benchmark, we conduct a large-scale study of current condensation methods, and report many insightful findings that open up new possibilities for future development. The benchmark library, including evaluators, baseline methods, and generated datasets, is open-sourced to facilitate future research and application.



### Hierarchically Self-Supervised Transformer for Human Skeleton Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.09644v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09644v3)
- **Published**: 2022-07-20 04:21:05+00:00
- **Updated**: 2023-03-27 10:35:11+00:00
- **Authors**: Yuxiao Chen, Long Zhao, Jianbo Yuan, Yu Tian, Zhaoyang Xia, Shijie Geng, Ligong Han, Dimitris N. Metaxas
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Despite the success of fully-supervised human skeleton sequence modeling, utilizing self-supervised pre-training for skeleton sequence representation learning has been an active field because acquiring task-specific skeleton annotations at large scales is difficult. Recent studies focus on learning video-level temporal and discriminative information using contrastive learning, but overlook the hierarchical spatial-temporal nature of human skeletons. Different from such superficial supervision at the video level, we propose a self-supervised hierarchical pre-training scheme incorporated into a hierarchical Transformer-based skeleton sequence encoder (Hi-TRS), to explicitly capture spatial, short-term, and long-term temporal dependencies at frame, clip, and video levels, respectively. To evaluate the proposed self-supervised pre-training scheme with Hi-TRS, we conduct extensive experiments covering three skeleton-based downstream tasks including action recognition, action detection, and motion prediction. Under both supervised and semi-supervised evaluation protocols, our method achieves the state-of-the-art performance. Additionally, we demonstrate that the prior knowledge learned by our model in the pre-training stage has strong transfer capability for different downstream tasks.



### Aware of the History: Trajectory Forecasting with the Local Behavior Data
- **Arxiv ID**: http://arxiv.org/abs/2207.09646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09646v1)
- **Published**: 2022-07-20 04:35:38+00:00
- **Updated**: 2022-07-20 04:35:38+00:00
- **Authors**: Yiqi Zhong, Zhenyang Ni, Siheng Chen, Ulrich Neumann
- **Comment**: This paper has been accepted by ECCV 2022
- **Journal**: None
- **Summary**: The historical trajectories previously passing through a location may help infer the future trajectory of an agent currently at this location. Despite great improvements in trajectory forecasting with the guidance of high-definition maps, only a few works have explored such local historical information. In this work, we re-introduce this information as a new type of input data for trajectory forecasting systems: the local behavior data, which we conceptualize as a collection of location-specific historical trajectories. Local behavior data helps the systems emphasize the prediction locality and better understand the impact of static map objects on moving agents. We propose a novel local-behavior-aware (LBA) prediction framework that improves forecasting accuracy by fusing information from observed trajectories, HD maps, and local behavior data. Also, where such historical data is insufficient or unavailable, we employ a local-behavior-free (LBF) prediction framework, which adopts a knowledge-distillation-based architecture to infer the impact of missing data. Extensive experiments demonstrate that upgrading existing methods with these two frameworks significantly improves their performances. Especially, the LBA framework boosts the SOTA methods' performance on the nuScenes dataset by at least 14% for the K=1 metrics.



### GenText: Unsupervised Artistic Text Generation via Decoupled Font and Texture Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2207.09649v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09649v1)
- **Published**: 2022-07-20 04:42:47+00:00
- **Updated**: 2022-07-20 04:42:47+00:00
- **Authors**: Qirui Huang, Bin Fu, Aozhong Zhang, Yu Qiao
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic artistic text generation is an emerging topic which receives increasing attention due to its wide applications. The artistic text can be divided into three components, content, font, and texture, respectively. Existing artistic text generation models usually focus on manipulating one aspect of the above components, which is a sub-optimal solution for controllable general artistic text generation. To remedy this issue, we propose a novel approach, namely GenText, to achieve general artistic text style transfer by separably migrating the font and texture styles from the different source images to the target images in an unsupervised manner. Specifically, our current work incorporates three different stages, stylization, destylization, and font transfer, respectively, into a unified platform with a single powerful encoder network and two separate style generator networks, one for font transfer, the other for stylization and destylization. The destylization stage first extracts the font style of the font reference image, then the font transfer stage generates the target content with the desired font style. Finally, the stylization stage renders the resulted font image with respect to the texture style in the reference image. Moreover, considering the difficult data acquisition of paired artistic text images, our model is designed under the unsupervised setting, where all stages can be effectively optimized from unpaired data. Qualitative and quantitative results are performed on artistic text benchmarks, which demonstrate the superior performance of our proposed model. The code with models will become publicly available in the future.



### Learning Topological Interactions for Multi-Class Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.09654v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09654v1)
- **Published**: 2022-07-20 05:09:43+00:00
- **Updated**: 2022-07-20 05:09:43+00:00
- **Authors**: Saumya Gupta, Xiaoling Hu, James Kaan, Michael Jin, Mutshipay Mpoy, Katherine Chung, Gagandeep Singh, Mary Saltz, Tahsin Kurc, Joel Saltz, Apostolos Tassiopoulos, Prateek Prasanna, Chao Chen
- **Comment**: Accepted to ECCV 2022 (Oral); 32 pages, 19 figures
- **Journal**: None
- **Summary**: Deep learning methods have achieved impressive performance for multi-class medical image segmentation. However, they are limited in their ability to encode topological interactions among different classes (e.g., containment and exclusion). These constraints naturally arise in biomedical images and can be crucial in improving segmentation quality. In this paper, we introduce a novel topological interaction module to encode the topological interactions into a deep neural network. The implementation is completely convolution-based and thus can be very efficient. This empowers us to incorporate the constraints into end-to-end training and enrich the feature representation of neural networks. The efficacy of the proposed method is validated on different types of interactions. We also demonstrate the generalizability of the method on both proprietary and public challenge datasets, in both 2D and 3D settings, as well as across different modalities such as CT and Ultrasound. Code is available at: https://github.com/TopoXLab/TopoInteraction



### Unsupervised Domain Adaptation for One-stage Object Detector using Offsets to Bounding Box
- **Arxiv ID**: http://arxiv.org/abs/2207.09656v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09656v1)
- **Published**: 2022-07-20 05:17:12+00:00
- **Updated**: 2022-07-20 05:17:12+00:00
- **Authors**: Jayeon Yoo, Inseop Chung, Nojun Kwak
- **Comment**: ECCV 2022, 24 pages
- **Journal**: None
- **Summary**: Most existing domain adaptive object detection methods exploit adversarial feature alignment to adapt the model to a new domain. Recent advances in adversarial feature alignment strives to reduce the negative effect of alignment, or negative transfer, that occurs because the distribution of features varies depending on the category of objects. However, by analyzing the features of the anchor-free one-stage detector, in this paper, we find that negative transfer may occur because the feature distribution varies depending on the regression value for the offset to the bounding box as well as the category. To obtain domain invariance by addressing this issue, we align the feature conditioned on the offset value, considering the modality of the feature distribution. With a very simple and effective conditioning method, we propose OADA (Offset-Aware Domain Adaptive object detector) that achieves state-of-the-art performances in various experimental settings. In addition, by analyzing through singular value decomposition, we find that our model enhances both discriminability and transferability.



### Learning Depth from Focus in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2207.09658v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09658v2)
- **Published**: 2022-07-20 05:23:29+00:00
- **Updated**: 2022-09-30 01:03:57+00:00
- **Authors**: Changyeon Won, Hae-Gon Jeon
- **Comment**: None
- **Journal**: None
- **Summary**: For better photography, most recent commercial cameras including smartphones have either adopted large-aperture lens to collect more light or used a burst mode to take multiple images within short times. These interesting features lead us to examine depth from focus/defocus.   In this work, we present a convolutional neural network-based depth estimation from single focal stacks. Our method differs from relevant state-of-the-art works with three unique features. First, our method allows depth maps to be inferred in an end-to-end manner even with image alignment. Second, we propose a sharp region detection module to reduce blur ambiguities in subtle focus changes and weakly texture-less regions. Third, we design an effective downsampling module to ease flows of focal information in feature extractions. In addition, for the generalization of the proposed network, we develop a simulator to realistically reproduce the features of commercial cameras, such as changes in field of view, focal length and principal points.   By effectively incorporating these three unique features, our network achieves the top rank in the DDFF 12-Scene benchmark on most metrics. We also demonstrate the effectiveness of the proposed method on various quantitative evaluations and real-world images taken from various off-the-shelf cameras compared with state-of-the-art methods. Our source code is publicly available at https://github.com/wcy199705/DfFintheWild.



### Hand-Assisted Expression Recognition Method from Synthetic Images at the Fourth ABAW Challenge
- **Arxiv ID**: http://arxiv.org/abs/2207.09661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09661v1)
- **Published**: 2022-07-20 05:38:02+00:00
- **Updated**: 2022-07-20 05:38:02+00:00
- **Authors**: Xiangyu Miao, Jiahe Wang, Yanan Chang, Yi Wu, Shangfei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Learning from synthetic images plays an important role in facial expression recognition task due to the difficulties of labeling the real images, and it is challenging because of the gap between the synthetic images and real images. The fourth Affective Behavior Analysis in-the-wild Competition raises the challenge and provides the synthetic images generated from Aff-Wild2 dataset. In this paper, we propose a hand-assisted expression recognition method to reduce the gap between the synthetic data and real data. Our method consists of two parts: expression recognition module and hand prediction module. Expression recognition module extracts expression information and hand prediction module predicts whether the image contains hands. Decision mode is used to combine the results of two modules, and post-pruning is used to improve the result. F1 score is used to verify the effectiveness of our method.



### HTNet: Anchor-free Temporal Action Localization with Hierarchical Transformers
- **Arxiv ID**: http://arxiv.org/abs/2207.09662v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2207.09662v2)
- **Published**: 2022-07-20 05:40:03+00:00
- **Updated**: 2022-07-21 01:42:00+00:00
- **Authors**: Tae-Kyung Kang, Gun-Hee Lee, Seong-Whan Lee
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Temporal action localization (TAL) is a task of identifying a set of actions in a video, which involves localizing the start and end frames and classifying each action instance. Existing methods have addressed this task by using predefined anchor windows or heuristic bottom-up boundary-matching strategies, which are major bottlenecks in inference time. Additionally, the main challenge is the inability to capture long-range actions due to a lack of global contextual information. In this paper, we present a novel anchor-free framework, referred to as HTNet, which predicts a set of <start time, end time, class> triplets from a video based on a Transformer architecture. After the prediction of coarse boundaries, we refine it through a background feature sampling (BFS) module and hierarchical Transformers, which enables our model to aggregate global contextual information and effectively exploit the inherent semantic relationships in a video. We demonstrate how our method localizes accurate action instances and achieves state-of-the-art performance on two TAL benchmark datasets: THUMOS14 and ActivityNet 1.3.



### Streamable Neural Fields
- **Arxiv ID**: http://arxiv.org/abs/2207.09663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09663v1)
- **Published**: 2022-07-20 05:42:02+00:00
- **Updated**: 2022-07-20 05:42:02+00:00
- **Authors**: Junwoo Cho, Seungtae Nam, Daniel Rho, Jong Hwan Ko, Eunbyung Park
- **Comment**: To appear in ECCV 2022
- **Journal**: None
- **Summary**: Neural fields have emerged as a new data representation paradigm and have shown remarkable success in various signal representations. Since they preserve signals in their network parameters, the data transfer by sending and receiving the entire model parameters prevents this emerging technology from being used in many practical scenarios. We propose streamable neural fields, a single model that consists of executable sub-networks of various widths. The proposed architectural and training techniques enable a single network to be streamable over time and reconstruct different qualities and parts of signals. For example, a smaller sub-network produces smooth and low-frequency signals, while a larger sub-network can represent fine details. Experimental results have shown the effectiveness of our method in various domains, such as 2D images, videos, and 3D signed distance functions. Finally, we demonstrate that our proposed method improves training stability, by exploiting parameter sharing.



### Pseudo-label Guided Cross-video Pixel Contrast for Robotic Surgical Scene Segmentation with Limited Annotations
- **Arxiv ID**: http://arxiv.org/abs/2207.09664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09664v1)
- **Published**: 2022-07-20 05:42:19+00:00
- **Updated**: 2022-07-20 05:42:19+00:00
- **Authors**: Yang Yu, Zixu Zhao, Yueming Jin, Guangyong Chen, Qi Dou, Pheng-Ann Heng
- **Comment**: Accepted by IROS 2022
- **Journal**: None
- **Summary**: Surgical scene segmentation is fundamentally crucial for prompting cognitive assistance in robotic surgery. However, pixel-wise annotating surgical video in a frame-by-frame manner is expensive and time consuming. To greatly reduce the labeling burden, in this work, we study semi-supervised scene segmentation from robotic surgical video, which is practically essential yet rarely explored before. We consider a clinically suitable annotation situation under the equidistant sampling. We then propose PGV-CL, a novel pseudo-label guided cross-video contrast learning method to boost scene segmentation. It effectively leverages unlabeled data for a trusty and global model regularization that produces more discriminative feature representation. Concretely, for trusty representation learning, we propose to incorporate pseudo labels to instruct the pair selection, obtaining more reliable representation pairs for pixel contrast. Moreover, we expand the representation learning space from previous image-level to cross-video, which can capture the global semantics to benefit the learning process. We extensively evaluate our method on a public robotic surgery dataset EndoVis18 and a public cataract dataset CaDIS. Experimental results demonstrate the effectiveness of our method, consistently outperforming the state-of-the-art semi-supervised methods under different labeling ratios, and even surpassing fully supervised training on EndoVis18 with 10.1% labeling.



### GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features
- **Arxiv ID**: http://arxiv.org/abs/2207.09666v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2207.09666v1)
- **Published**: 2022-07-20 05:49:01+00:00
- **Updated**: 2022-07-20 05:49:01+00:00
- **Authors**: Van-Quang Nguyen, Masanori Suganuma, Takayuki Okatani
- **Comment**: Accepted to ECCV 2022; 14 pages with appendix; Code:
  https://github.com/davidnvq/grit
- **Journal**: None
- **Summary**: Current state-of-the-art methods for image captioning employ region-based features, as they provide object-level information that is essential to describe the content of images; they are usually extracted by an object detector such as Faster R-CNN. However, they have several issues, such as lack of contextual information, the risk of inaccurate detection, and the high computational cost. The first two could be resolved by additionally using grid-based features. However, how to extract and fuse these two types of features is uncharted. This paper proposes a Transformer-only neural architecture, dubbed GRIT (Grid- and Region-based Image captioning Transformer), that effectively utilizes the two visual features to generate better captions. GRIT replaces the CNN-based detector employed in previous methods with a DETR-based one, making it computationally faster. Moreover, its monolithic design consisting only of Transformers enables end-to-end training of the model. This innovative design and the integration of the dual visual features bring about significant performance improvement. The experimental results on several image captioning benchmarks show that GRIT outperforms previous methods in inference accuracy and speed.



### ERA: Expert Retrieval and Assembly for Early Action Prediction
- **Arxiv ID**: http://arxiv.org/abs/2207.09675v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09675v3)
- **Published**: 2022-07-20 06:09:26+00:00
- **Updated**: 2022-07-22 06:20:42+00:00
- **Authors**: Lin Geng Foo, Tianjiao Li, Hossein Rahmani, Qiuhong Ke, Jun Liu
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Early action prediction aims to successfully predict the class label of an action before it is completely performed. This is a challenging task because the beginning stages of different actions can be very similar, with only minor subtle differences for discrimination. In this paper, we propose a novel Expert Retrieval and Assembly (ERA) module that retrieves and assembles a set of experts most specialized at using discriminative subtle differences, to distinguish an input sample from other highly similar samples. To encourage our model to effectively use subtle differences for early action prediction, we push experts to discriminate exclusively between samples that are highly similar, forcing these experts to learn to use subtle differences that exist between those samples. Additionally, we design an effective Expert Learning Rate Optimization method that balances the experts' optimization and leads to better performance. We evaluate our ERA module on four public action datasets and achieve state-of-the-art performance.



### Explaining Deepfake Detection by Analysing Image Matching
- **Arxiv ID**: http://arxiv.org/abs/2207.09679v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09679v1)
- **Published**: 2022-07-20 06:23:11+00:00
- **Updated**: 2022-07-20 06:23:11+00:00
- **Authors**: Shichao Dong, Jin Wang, Jiajun Liang, Haoqiang Fan, Renhe Ji
- **Comment**: Accepted at ECCV 2022, Code is available at:
  https://github.com/megvii-research/FST-Matching
- **Journal**: None
- **Summary**: This paper aims to interpret how deepfake detection models learn artifact features of images when just supervised by binary labels. To this end, three hypotheses from the perspective of image matching are proposed as follows. 1. Deepfake detection models indicate real/fake images based on visual concepts that are neither source-relevant nor target-relevant, that is, considering such visual concepts as artifact-relevant. 2. Besides the supervision of binary labels, deepfake detection models implicitly learn artifact-relevant visual concepts through the FST-Matching (i.e. the matching fake, source, target images) in the training set. 3. Implicitly learned artifact visual concepts through the FST-Matching in the raw training set are vulnerable to video compression. In experiments, the above hypotheses are verified among various DNNs. Furthermore, based on this understanding, we propose the FST-Matching Deepfake Detection Model to boost the performance of forgery detection on compressed videos. Experiment results show that our method achieves great performance, especially on highly-compressed (e.g. c40) videos.



### On the Versatile Uses of Partial Distance Correlation in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.09684v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09684v3)
- **Published**: 2022-07-20 06:36:11+00:00
- **Updated**: 2022-11-08 19:52:39+00:00
- **Authors**: Xingjian Zhen, Zihang Meng, Rudrasis Chakraborty, Vikas Singh
- **Comment**: This paper has been selected as best paper award for ECCV 2022!
- **Journal**: ECCV 2022
- **Summary**: Comparing the functional behavior of neural network models, whether it is a single network over time or two (or more networks) during or post-training, is an essential step in understanding what they are learning (and what they are not), and for identifying strategies for regularization or efficiency improvements. Despite recent progress, e.g., comparing vision transformers to CNNs, systematic comparison of function, especially across different networks, remains difficult and is often carried out layer by layer. Approaches such as canonical correlation analysis (CCA) are applicable in principle, but have been sparingly used so far. In this paper, we revisit a (less widely known) from statistics, called distance correlation (and its partial variant), designed to evaluate correlation between feature spaces of different dimensions. We describe the steps necessary to carry out its deployment for large scale models -- this opens the door to a surprising array of applications ranging from conditioning one deep model w.r.t. another, learning disentangled representations as well as optimizing diverse models that would directly be more robust to adversarial attacks. Our experiments suggest a versatile regularizer (or constraint) with many advantages, which avoids some of the common difficulties one faces in such analyses. Code is at https://github.com/zhenxingjian/Partial_Distance_Correlation.



### BigColor: Colorization using a Generative Color Prior for Natural Images
- **Arxiv ID**: http://arxiv.org/abs/2207.09685v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09685v1)
- **Published**: 2022-07-20 06:36:46+00:00
- **Updated**: 2022-07-20 06:36:46+00:00
- **Authors**: Geonung Kim, Kyoungkook Kang, Seongtae Kim, Hwayoon Lee, Sehoon Kim, Jonghyun Kim, Seung-Hwan Baek, Sunghyun Cho
- **Comment**: None
- **Journal**: None
- **Summary**: For realistic and vivid colorization, generative priors have recently been exploited. However, such generative priors often fail for in-the-wild complex images due to their limited representation space. In this paper, we propose BigColor, a novel colorization approach that provides vivid colorization for diverse in-the-wild images with complex structures. While previous generative priors are trained to synthesize both image structures and colors, we learn a generative color prior to focus on color synthesis given the spatial structure of an image. In this way, we reduce the burden of synthesizing image structures from the generative prior and expand its representation space to cover diverse images. To this end, we propose a BigGAN-inspired encoder-generator network that uses a spatial feature map instead of a spatially-flattened BigGAN latent code, resulting in an enlarged representation space. Our method enables robust colorization for diverse inputs in a single forward pass, supports arbitrary input resolutions, and provides multi-modal colorization results. We demonstrate that BigColor significantly outperforms existing methods especially on in-the-wild images with complex structures.



### Object-Compositional Neural Implicit Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2207.09686v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09686v2)
- **Published**: 2022-07-20 06:38:04+00:00
- **Updated**: 2022-08-01 09:53:52+00:00
- **Authors**: Qianyi Wu, Xian Liu, Yuedong Chen, Kejie Li, Chuanxia Zheng, Jianfei Cai, Jianmin Zheng
- **Comment**: ECCV2022, Project Page: https://qianyiwu.github.io/objectsdf/ Code:
  https://github.com/QianyiWu/objsdf
- **Journal**: None
- **Summary**: The neural implicit representation has shown its effectiveness in novel view synthesis and high-quality 3D reconstruction from multi-view images. However, most approaches focus on holistic scene representation yet ignore individual objects inside it, thus limiting potential downstream applications. In order to learn object-compositional representation, a few works incorporate the 2D semantic map as a cue in training to grasp the difference between objects. But they neglect the strong connections between object geometry and instance semantic information, which leads to inaccurate modeling of individual instance. This paper proposes a novel framework, ObjectSDF, to build an object-compositional neural implicit representation with high fidelity in 3D reconstruction and object representation. Observing the ambiguity of conventional volume rendering pipelines, we model the scene by combining the Signed Distance Functions (SDF) of individual object to exert explicit surface constraint. The key in distinguishing different instances is to revisit the strong association between an individual object's SDF and semantic label. Particularly, we convert the semantic information to a function of object SDF and develop a unified and compact representation for scene and objects. Experimental results show the superiority of ObjectSDF framework in representing both the holistic object-compositional scene and the individual instances. Code can be found at https://qianyiwu.github.io/objectsdf/



### Uncertainty Inspired Underwater Image Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2207.09689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09689v1)
- **Published**: 2022-07-20 06:42:28+00:00
- **Updated**: 2022-07-20 06:42:28+00:00
- **Authors**: Zhenqi Fu, Wu Wang, Yue Huang, Xinghao Ding, Kai-Kuang Ma
- **Comment**: None
- **Journal**: None
- **Summary**: A main challenge faced in the deep learning-based Underwater Image Enhancement (UIE) is that the ground truth high-quality image is unavailable. Most of the existing methods first generate approximate reference maps and then train an enhancement network with certainty. This kind of method fails to handle the ambiguity of the reference map. In this paper, we resolve UIE into distribution estimation and consensus process. We present a novel probabilistic network to learn the enhancement distribution of degraded underwater images. Specifically, we combine conditional variational autoencoder with adaptive instance normalization to construct the enhancement distribution. After that, we adopt a consensus process to predict a deterministic result based on a set of samples from the distribution. By learning the enhancement distribution, our method can cope with the bias introduced in the reference map labeling to some extent. Additionally, the consensus process is useful to capture a robust and stable result. We examined the proposed method on two widely used real-world underwater image enhancement datasets. Experimental results demonstrate that our approach enables sampling possible enhancement predictions. Meanwhile, the consensus estimate yields competitive performance compared with state-of-the-art UIE methods. Code available at https://github.com/zhenqifu/PUIE-Net.



### Efficient Meta-Tuning for Content-aware Neural Video Delivery
- **Arxiv ID**: http://arxiv.org/abs/2207.09691v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.09691v1)
- **Published**: 2022-07-20 06:47:10+00:00
- **Updated**: 2022-07-20 06:47:10+00:00
- **Authors**: Xiaoqi Li, Jiaming Liu, Shizun Wang, Cheng Lyu, Ming Lu, Yurong Chen, Anbang Yao, Yandong Guo, Shanghang Zhang
- **Comment**: Accepted at ECCV2022
- **Journal**: None
- **Summary**: Recently, Deep Neural Networks (DNNs) are utilized to reduce the bandwidth and improve the quality of Internet video delivery. Existing methods train corresponding content-aware super-resolution (SR) model for each video chunk on the server, and stream low-resolution (LR) video chunks along with SR models to the client. Although they achieve promising results, the huge computational cost of network training limits their practical applications. In this paper, we present a method named Efficient Meta-Tuning (EMT) to reduce the computational cost. Instead of training from scratch, EMT adapts a meta-learned model to the first chunk of the input video. As for the following chunks, it fine-tunes the partial parameters selected by gradient masking of previous adapted model. In order to achieve further speedup for EMT, we propose a novel sampling strategy to extract the most challenging patches from video frames. The proposed strategy is highly efficient and brings negligible additional cost. Our method significantly reduces the computational cost and achieves even better performance, paving the way for applying neural video delivery techniques to practical applications. We conduct extensive experiments based on various efficient SR architectures, including ESPCN, SRCNN, FSRCNN and EDSR-1, demonstrating the generalization ability of our work. The code is released at \url{https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022}.



### Robust Object Detection With Inaccurate Bounding Boxes
- **Arxiv ID**: http://arxiv.org/abs/2207.09697v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09697v1)
- **Published**: 2022-07-20 06:57:30+00:00
- **Updated**: 2022-07-20 06:57:30+00:00
- **Authors**: Chengxin Liu, Kewei Wang, Hao Lu, Zhiguo Cao, Ziming Zhang
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Learning accurate object detectors often requires large-scale training data with precise object bounding boxes. However, labeling such data is expensive and time-consuming. As the crowd-sourcing labeling process and the ambiguities of the objects may raise noisy bounding box annotations, the object detectors will suffer from the degenerated training data. In this work, we aim to address the challenge of learning robust object detectors with inaccurate bounding boxes. Inspired by the fact that localization precision suffers significantly from inaccurate bounding boxes while classification accuracy is less affected, we propose leveraging classification as a guidance signal for refining localization results. Specifically, by treating an object as a bag of instances, we introduce an Object-Aware Multiple Instance Learning approach (OA-MIL), featured with object-aware instance selection and object-aware instance extension. The former aims to select accurate instances for training, instead of directly using inaccurate box annotations. The latter focuses on generating high-quality instances for selection. Extensive experiments on synthetic noisy datasets (i.e., noisy PASCAL VOC and MS-COCO) and a real noisy wheat head dataset demonstrate the effectiveness of our OA-MIL. Code is available at https://github.com/cxliu0/OA-MIL.



### Resolving Copycat Problems in Visual Imitation Learning via Residual Action Prediction
- **Arxiv ID**: http://arxiv.org/abs/2207.09705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09705v1)
- **Published**: 2022-07-20 07:15:32+00:00
- **Updated**: 2022-07-20 07:15:32+00:00
- **Authors**: Chia-Chi Chuang, Donglin Yang, Chuan Wen, Yang Gao
- **Comment**: 27 pages, 10 figures, ECCV2022
- **Journal**: None
- **Summary**: Imitation learning is a widely used policy learning method that enables intelligent agents to acquire complex skills from expert demonstrations. The input to the imitation learning algorithm is usually composed of both the current observation and historical observations since the most recent observation might not contain enough information. This is especially the case with image observations, where a single image only includes one view of the scene, and it suffers from a lack of motion information and object occlusions. In theory, providing multiple observations to the imitation learning agent will lead to better performance. However, surprisingly people find that sometimes imitation from observation histories performs worse than imitation from the most recent observation. In this paper, we explain this phenomenon from the information flow within the neural network perspective. We also propose a novel imitation learning neural network architecture that does not suffer from this issue by design. Furthermore, our method scales to high-dimensional image observations. Finally, we benchmark our approach on two widely used simulators, CARLA and MuJoCo, and it successfully alleviates the copycat problem and surpasses the existing solutions.



### Learning Sequence Representations by Non-local Recurrent Neural Memory
- **Arxiv ID**: http://arxiv.org/abs/2207.09710v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.09710v1)
- **Published**: 2022-07-20 07:26:15+00:00
- **Updated**: 2022-07-20 07:26:15+00:00
- **Authors**: Wenjie Pei, Xin Feng, Canmiao Fu, Qiong Cao, Guangming Lu, Yu-Wing Tai
- **Comment**: To be appeared in International Journal of Computer Vision (IJCV).
  arXiv admin note: substantial text overlap with arXiv:1908.09535
- **Journal**: None
- **Summary**: The key challenge of sequence representation learning is to capture the long-range temporal dependencies. Typical methods for supervised sequence representation learning are built upon recurrent neural networks to capture temporal dependencies. One potential limitation of these methods is that they only model one-order information interactions explicitly between adjacent time steps in a sequence, hence the high-order interactions between nonadjacent time steps are not fully exploited. It greatly limits the capability of modeling the long-range temporal dependencies since the temporal features learned by one-order interactions cannot be maintained for a long term due to temporal information dilution and gradient vanishing. To tackle this limitation, we propose the Non-local Recurrent Neural Memory (NRNM) for supervised sequence representation learning, which performs non-local operations \MR{by means of self-attention mechanism} to learn full-order interactions within a sliding temporal memory block and models global interactions between memory blocks in a gated recurrent manner. Consequently, our model is able to capture long-range dependencies. Besides, the latent high-level features contained in high-order interactions can be distilled by our model. We validate the effectiveness and generalization of our NRNM on three types of sequence applications across different modalities, including sequence classification, step-wise sequential prediction and sequence similarity learning. Our model compares favorably against other state-of-the-art methods specifically designed for each of these sequence applications.



### Model Compression for Resource-Constrained Mobile Robots
- **Arxiv ID**: http://arxiv.org/abs/2207.10082v1
- **DOI**: 10.4204/EPTCS.362.7
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.10082v1)
- **Published**: 2022-07-20 07:26:36+00:00
- **Updated**: 2022-07-20 07:26:36+00:00
- **Authors**: Timotheos Souroulla, Alberto Hata, Ahmad Terra, Özer Özkahraman, Rafia Inam
- **Comment**: In Proceedings AREA 2022, arXiv:2207.09058
- **Journal**: EPTCS 362, 2022, pp. 54-64
- **Summary**: The number of mobile robots with constrained computing resources that need to execute complex machine learning models has been increasing during the past decade. Commonly, these robots rely on edge infrastructure accessible over wireless communication to execute heavy computational complex tasks. However, the edge might become unavailable and, consequently, oblige the execution of the tasks on the robot. This work focuses on making it possible to execute the tasks on the robots by reducing the complexity and the total number of parameters of pre-trained computer vision models. This is achieved by using model compression techniques such as Pruning and Knowledge Distillation. These compression techniques have strong theoretical and practical foundations, but their combined usage has not been widely explored in the literature. Therefore, this work especially focuses on investigating the effects of combining these two compression techniques. The results of this work reveal that up to 90% of the total number of parameters of a computer vision model can be removed without any considerable reduction in the model's accuracy.



### Multi-Task Learning for Emotion Descriptors Estimation at the fourth ABAW Challenge
- **Arxiv ID**: http://arxiv.org/abs/2207.09716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09716v1)
- **Published**: 2022-07-20 07:39:12+00:00
- **Updated**: 2022-07-20 07:39:12+00:00
- **Authors**: Yanan Chang, Yi Wu, Xiangyu Miao, Jiahe Wang, Shangfei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Facial valence/arousal, expression and action unit are related tasks in facial affective analysis. However, the tasks only have limited performance in the wild due to the various collected conditions. The 4th competition on affective behavior analysis in the wild (ABAW) provided images with valence/arousal, expression and action unit labels. In this paper, we introduce multi-task learning framework to enhance the performance of three related tasks in the wild. Feature sharing and label fusion are used to utilize their relations. We conduct experiments on the provided training and validating data.



### Feature Representation Learning for Unsupervised Cross-domain Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2207.09721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09721v1)
- **Published**: 2022-07-20 07:52:14+00:00
- **Updated**: 2022-07-20 07:52:14+00:00
- **Authors**: Conghui Hu, Gim Hee Lee
- **Comment**: ECCV2022
- **Journal**: None
- **Summary**: Current supervised cross-domain image retrieval methods can achieve excellent performance. However, the cost of data collection and labeling imposes an intractable barrier to practical deployment in real applications. In this paper, we investigate the unsupervised cross-domain image retrieval task, where class labels and pairing annotations are no longer a prerequisite for training. This is an extremely challenging task because there is no supervision for both in-domain feature representation learning and cross-domain alignment. We address both challenges by introducing: 1) a new cluster-wise contrastive learning mechanism to help extract class semantic-aware features, and 2) a novel distance-of-distance loss to effectively measure and minimize the domain discrepancy without any external supervision. Experiments on the Office-Home and DomainNet datasets consistently show the superior image retrieval accuracies of our framework over state-of-the-art approaches. Our source code can be found at https://github.com/conghuihu/UCDIR.



### OTPose: Occlusion-Aware Transformer for Pose Estimation in Sparsely-Labeled Videos
- **Arxiv ID**: http://arxiv.org/abs/2207.09725v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/2207.09725v2)
- **Published**: 2022-07-20 08:06:06+00:00
- **Updated**: 2022-07-28 01:43:19+00:00
- **Authors**: Kyung-Min Jin, Gun-Hee Lee, Seong-Whan Lee
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Although many approaches for multi-human pose estimation in videos have shown profound results, they require densely annotated data which entails excessive man labor. Furthermore, there exists occlusion and motion blur that inevitably lead to poor estimation performance. To address these problems, we propose a method that leverages an attention mask for occluded joints and encodes temporal dependency between frames using transformers. First, our framework composes different combinations of sparsely annotated frames that denote the track of the overall joint movement. We propose an occlusion attention mask from these combinations that enable encoding occlusion-aware heatmaps as a semi-supervised task. Second, the proposed temporal encoder employs transformer architecture to effectively aggregate the temporal relationship and keypoint-wise attention from each time step and accurately refines the target frame's final pose estimation. We achieve state-of-the-art pose estimation results for PoseTrack2017 and PoseTrack2018 datasets and demonstrate the robustness of our approach to occlusion and motion blur in sparsely annotated video data.



### Revisiting data augmentation for subspace clustering
- **Arxiv ID**: http://arxiv.org/abs/2207.09728v1
- **DOI**: 10.1016/j.knosys.2022.109974
- **Categories**: **cs.LG**, cs.AI, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2207.09728v1)
- **Published**: 2022-07-20 08:13:08+00:00
- **Updated**: 2022-07-20 08:13:08+00:00
- **Authors**: Maryam Abdolali, Nicolas Gillis
- **Comment**: 38 pages (including 10 of supplementary material)
- **Journal**: Knowledge-Based Systems 258, 109974, 2022
- **Summary**: Subspace clustering is the classical problem of clustering a collection of data samples that approximately lie around several low-dimensional subspaces. The current state-of-the-art approaches for this problem are based on the self-expressive model which represents the samples as linear combination of other samples. However, these approaches require sufficiently well-spread samples for accurate representation which might not be necessarily accessible in many applications. In this paper, we shed light on this commonly neglected issue and argue that data distribution within each subspace plays a critical role in the success of self-expressive models. Our proposed solution to tackle this issue is motivated by the central role of data augmentation in the generalization power of deep neural networks. We propose two subspace clustering frameworks for both unsupervised and semi-supervised settings that use augmented samples as an enlarged dictionary to improve the quality of the self-expressive representation. We present an automatic augmentation strategy using a few labeled samples for the semi-supervised problem relying on the fact that the data samples lie in the union of multiple linear subspaces. Experimental results confirm the effectiveness of data augmentation, as it significantly improves the performance of general self-expressive models.



### CrossHuman: Learning Cross-Guidance from Multi-Frame Images for Human Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2207.09735v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09735v1)
- **Published**: 2022-07-20 08:25:20+00:00
- **Updated**: 2022-07-20 08:25:20+00:00
- **Authors**: Liliang Chen, Jiaqi Li, Han Huang, Yandong Guo
- **Comment**: None
- **Journal**: None
- **Summary**: We propose CrossHuman, a novel method that learns cross-guidance from parametric human model and multi-frame RGB images to achieve high-quality 3D human reconstruction. To recover geometry details and texture even in invisible regions, we design a reconstruction pipeline combined with tracking-based methods and tracking-free methods. Given a monocular RGB sequence, we track the parametric human model in the whole sequence, the points (voxels) corresponding to the target frame are warped to reference frames by the parametric body motion. Guided by the geometry priors of the parametric body and spatially aligned features from RGB sequence, the robust implicit surface is fused. Moreover, a multi-frame transformer (MFT) and a self-supervised warp refinement module are integrated to the framework to relax the requirements of parametric body and help to deal with very loose cloth. Compared with previous works, our CrossHuman enables high-fidelity geometry details and texture in both visible and invisible regions and improves the accuracy of the human reconstruction even under estimated inaccurate parametric human models. The experiments demonstrate that our method achieves state-of-the-art (SOTA) performance.



### Interpreting Latent Spaces of Generative Models for Medical Images using Unsupervised Methods
- **Arxiv ID**: http://arxiv.org/abs/2207.09740v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.09740v1)
- **Published**: 2022-07-20 08:34:50+00:00
- **Updated**: 2022-07-20 08:34:50+00:00
- **Authors**: Julian Schön, Raghavendra Selvan, Jens Petersen
- **Comment**: Accepted for presentation at DGM4MICCAI 2022
- **Journal**: None
- **Summary**: Generative models such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) play an increasingly important role in medical image analysis. The latent spaces of these models often show semantically meaningful directions corresponding to human-interpretable image transformations. However, until now, their exploration for medical images has been limited due to the requirement of supervised data. Several methods for unsupervised discovery of interpretable directions in GAN latent spaces have shown interesting results on natural images. This work explores the potential of applying these techniques on medical images by training a GAN and a VAE on thoracic CT scans and using an unsupervised method to discover interpretable directions in the resulting latent space. We find several directions corresponding to non-trivial image transformations, such as rotation or breast size. Furthermore, the directions show that the generative models capture 3D structure despite being presented only with 2D data. The results show that unsupervised methods to discover interpretable directions in GANs generalize to VAEs and can be applied to medical images. This opens a wide array of future work using these methods in medical image analysis.



### Facial Affect Analysis: Learning from Synthetic Data & Multi-Task Learning Challenges
- **Arxiv ID**: http://arxiv.org/abs/2207.09748v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.09748v1)
- **Published**: 2022-07-20 08:46:18+00:00
- **Updated**: 2022-07-20 08:46:18+00:00
- **Authors**: Siyang Li, Yifan Xu, Huanyu Wu, Dongrui Wu, Yingjie Yin, Jiajiong Cao, Jingting Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Facial affect analysis remains a challenging task with its setting transitioned from lab-controlled to in-the-wild situations. In this paper, we present novel frameworks to handle the two challenges in the 4th Affective Behavior Analysis In-The-Wild (ABAW) competition: i) Multi-Task-Learning (MTL) Challenge and ii) Learning from Synthetic Data (LSD) Challenge. For MTL challenge, we adopt the SMM-EmotionNet with a better ensemble strategy of feature vectors. For LSD challenge, we propose respective methods to combat the problems of single labels, imbalanced distribution, fine-tuning limitations, and choice of model architectures. Experimental results on the official validation sets from the competition demonstrated that our proposed approaches outperformed baselines by a large margin. The code is available at https://github.com/sylyoung/ABAW4-HUST-ANT.



### Task-adaptive Spatial-Temporal Video Sampler for Few-shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.09759v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.09759v3)
- **Published**: 2022-07-20 09:04:12+00:00
- **Updated**: 2022-12-22 08:41:53+00:00
- **Authors**: Huabin Liu, Weixian Lv, John See, Weiyao Lin
- **Comment**: Accepted by ACM MM 2022
- **Journal**: None
- **Summary**: A primary challenge faced in few-shot action recognition is inadequate video data for training. To address this issue, current methods in this field mainly focus on devising algorithms at the feature level while little attention is paid to processing input video data. Moreover, existing frame sampling strategies may omit critical action information in temporal and spatial dimensions, which further impacts video utilization efficiency. In this paper, we propose a novel video frame sampler for few-shot action recognition to address this issue, where task-specific spatial-temporal frame sampling is achieved via a temporal selector (TS) and a spatial amplifier (SA). Specifically, our sampler first scans the whole video at a small computational cost to obtain a global perception of video frames. The TS plays its role in selecting top-T frames that contribute most significantly and subsequently. The SA emphasizes the discriminative information of each frame by amplifying critical regions with the guidance of saliency maps. We further adopt task-adaptive learning to dynamically adjust the sampling strategy according to the episode task at hand. Both the implementations of TS and SA are differentiable for end-to-end optimization, facilitating seamless integration of our proposed sampler with most few-shot action recognition methods. Extensive experiments show a significant boost in the performances on various benchmarks including long-term videos.The code is available at https://github.com/R00Kie-Liu/Sampler



### GIPSO: Geometrically Informed Propagation for Online Adaptation in 3D LiDAR Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.09763v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.09763v1)
- **Published**: 2022-07-20 09:06:07+00:00
- **Updated**: 2022-07-20 09:06:07+00:00
- **Authors**: Cristiano Saltori, Evgeny Krivosheev, Stéphane Lathuilière, Nicu Sebe, Fabio Galasso, Giuseppe Fiameni, Elisa Ricci, Fabio Poiesi
- **Comment**: Accepted at ECCV 2022
- **Journal**: None
- **Summary**: 3D point cloud semantic segmentation is fundamental for autonomous driving. Most approaches in the literature neglect an important aspect, i.e., how to deal with domain shift when handling dynamic scenes. This can significantly hinder the navigation capabilities of self-driving vehicles. This paper advances the state of the art in this research field. Our first contribution consists in analysing a new unexplored scenario in point cloud segmentation, namely Source-Free Online Unsupervised Domain Adaptation (SF-OUDA). We experimentally show that state-of-the-art methods have a rather limited ability to adapt pre-trained deep network models to unseen domains in an online manner. Our second contribution is an approach that relies on adaptive self-training and geometric-feature propagation to adapt a pre-trained source model online without requiring either source data or target labels. Our third contribution is to study SF-OUDA in a challenging setup where source data is synthetic and target data is point clouds captured in the real world. We use the recent SynLiDAR dataset as a synthetic source and introduce two new synthetic (source) datasets, which can stimulate future synthetic-to-real autonomous driving research. Our experiments show the effectiveness of our segmentation approach on thousands of real-world point clouds. Code and synthetic datasets are available at https://github.com/saltoricristiano/gipso-sfouda.



### Collaborating Domain-shared and Target-specific Feature Clustering for Cross-domain 3D Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.09767v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09767v1)
- **Published**: 2022-07-20 09:18:57+00:00
- **Updated**: 2022-07-20 09:18:57+00:00
- **Authors**: Qinying Liu, Zilei Wang
- **Comment**: Accepted by ECCV2022
- **Journal**: None
- **Summary**: In this work, we consider the problem of cross-domain 3D action recognition in the open-set setting, which has been rarely explored before. Specifically, there is a source domain and a target domain that contain the skeleton sequences with different styles and categories, and our purpose is to cluster the target data by utilizing the labeled source data and unlabeled target data. For such a challenging task, this paper presents a novel approach dubbed CoDT to collaboratively cluster the domain-shared features and target-specific features. CoDT consists of two parallel branches. One branch aims to learn domain-shared features with supervised learning in the source domain, while the other is to learn target-specific features using contrastive learning in the target domain. To cluster the features, we propose an online clustering algorithm that enables simultaneous promotion of robust pseudo label generation and feature clustering. Furthermore, to leverage the complementarity of domain-shared features and target-specific features, we propose a novel collaborative clustering strategy to enforce pair-wise relationship consistency between the two branches. We conduct extensive experiments on multiple cross-domain 3D action recognition datasets, and the results demonstrate the effectiveness of our method.



### A Hybrid Convolutional Neural Network with Meta Feature Learning for Abnormality Detection in Wireless Capsule Endoscopy Images
- **Arxiv ID**: http://arxiv.org/abs/2207.09769v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.09769v1)
- **Published**: 2022-07-20 09:25:57+00:00
- **Updated**: 2022-07-20 09:25:57+00:00
- **Authors**: Samir Jain, Ayan Seal, Aparajita Ojha
- **Comment**: None
- **Journal**: None
- **Summary**: Wireless Capsule Endoscopy is one of the most advanced non-invasive methods for the examination of gastrointestinal tracts. An intelligent computer-aided diagnostic system for detecting gastrointestinal abnormalities like polyp, bleeding, inflammation, etc. is highly exigent in wireless capsule endoscopy image analysis. Abnormalities greatly differ in their shape, size, color, and texture, and some appear to be visually similar to normal regions. This poses a challenge in designing a binary classifier due to intra-class variations. In this study, a hybrid convolutional neural network is proposed for abnormality detection that extracts a rich pool of meaningful features from wireless capsule endoscopy images using a variety of convolution operations. It consists of three parallel convolutional neural networks, each with a distinctive feature learning capability. The first network utilizes depthwise separable convolution, while the second employs cosine normalized convolution operation. A novel meta-feature extraction mechanism is introduced in the third network, to extract patterns from the statistical information drawn over the features generated from the first and second networks and its own previous layer. The network trio effectively handles intra-class variance and efficiently detects gastrointestinal abnormalities. The proposed hybrid convolutional neural network model is trained and tested on two widely used publicly available datasets. The test results demonstrate that the proposed model outperforms six state-of-the-art methods with 97\% and 98\% classification accuracy on KID and Kvasir-Capsule datasets respectively. Cross dataset evaluation results also demonstrate the generalization performance of the proposed model.



### Localization supervision of chest x-ray classifiers using label-specific eye-tracking annotation
- **Arxiv ID**: http://arxiv.org/abs/2207.09771v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.09771v2)
- **Published**: 2022-07-20 09:26:29+00:00
- **Updated**: 2022-12-14 23:24:55+00:00
- **Authors**: Ricardo Bigolin Lanfredi, Joyce D. Schroeder, Tolga Tasdizen
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been successfully applied to chest x-ray (CXR) images. Moreover, annotated bounding boxes have been shown to improve the interpretability of a CNN in terms of localizing abnormalities. However, only a few relatively small CXR datasets containing bounding boxes are available, and collecting them is very costly. Opportunely, eye-tracking (ET) data can be collected in a non-intrusive way during the clinical workflow of a radiologist. We use ET data recorded from radiologists while dictating CXR reports to train CNNs. We extract snippets from the ET data by associating them with the dictation of keywords and use them to supervise the localization of specific abnormalities. We show that this method improves a model's interpretability without impacting its image-level classification.



### Drivable Volumetric Avatars using Texel-Aligned Features
- **Arxiv ID**: http://arxiv.org/abs/2207.09774v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09774v1)
- **Published**: 2022-07-20 09:28:16+00:00
- **Updated**: 2022-07-20 09:28:16+00:00
- **Authors**: Edoardo Remelli, Timur Bagautdinov, Shunsuke Saito, Tomas Simon, Chenglei Wu, Shih-En Wei, Kaiwen Guo, Zhe Cao, Fabian Prada, Jason Saragih, Yaser Sheikh
- **Comment**: None
- **Journal**: SIGGRAPH 2022 Conference Proceedings
- **Summary**: Photorealistic telepresence requires both high-fidelity body modeling and faithful driving to enable dynamically synthesized appearance that is indistinguishable from reality. In this work, we propose an end-to-end framework that addresses two core challenges in modeling and driving full-body avatars of real people. One challenge is driving an avatar while staying faithful to details and dynamics that cannot be captured by a global low-dimensional parameterization such as body pose. Our approach supports driving of clothed avatars with wrinkles and motion that a real driving performer exhibits beyond the training corpus. Unlike existing global state representations or non-parametric screen-space approaches, we introduce texel-aligned features -- a localised representation which can leverage both the structural prior of a skeleton-based parametric model and observed sparse image signals at the same time. Another challenge is modeling a temporally coherent clothed avatar, which typically requires precise surface tracking. To circumvent this, we propose a novel volumetric avatar representation by extending mixtures of volumetric primitives to articulated objects. By explicitly incorporating articulation, our approach naturally generalizes to unseen poses. We also introduce a localized viewpoint conditioning, which leads to a large improvement in generalization of view-dependent appearance. The proposed volumetric representation does not require high-quality mesh tracking as a prerequisite and brings significant quality improvements compared to mesh-based counterparts. In our experiments, we carefully examine our design choices and demonstrate the efficacy of our approach, outperforming the state-of-the-art methods on challenging driving scenarios.



### Rectifying Open-set Object Detection: A Taxonomy, Practical Applications, and Proper Evaluation
- **Arxiv ID**: http://arxiv.org/abs/2207.09775v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09775v2)
- **Published**: 2022-07-20 09:28:51+00:00
- **Updated**: 2022-11-29 12:39:06+00:00
- **Authors**: Yusuke Hosoya, Masanori Suganuma, Takayuki Okatani
- **Comment**: 17 pages, 7 figures
- **Journal**: None
- **Summary**: Open-set object detection (OSOD) has recently gained attention. It is to detect unknown objects while correctly detecting known objects. In this paper, we first point out that the recent studies' formalization of OSOD, which generalizes open-set recognition (OSR) and thus considers an unlimited variety of unknown objects, has a fundamental issue. This issue emerges from the difference between image classification and object detection, making it hard to evaluate OSOD methods' performance properly. We then introduce a novel scenario of OSOD, which considers known and unknown classes within a specified super-class of object classes. This new scenario has practical applications and is free from the above issue, enabling proper evaluation of OSOD performance and probably making the problem more manageable. Finally, we experimentally evaluate existing OSOD methods with the new scenario using multiple datasets, showing that the current state-of-the-art OSOD methods attain limited performance similar to a simple baseline method. The paper also presents a taxonomy of OSOD that clarifies different problem formalizations. We hope our study helps the community reconsider OSOD problems and progress in the right direction.



### AU-Supervised Convolutional Vision Transformers for Synthetic Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.09777v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09777v2)
- **Published**: 2022-07-20 09:33:39+00:00
- **Updated**: 2022-07-22 04:24:52+00:00
- **Authors**: Shuyi Mao, Xinpeng Li, Junyao Chen, Xiaojiang Peng
- **Comment**: None
- **Journal**: None
- **Summary**: The paper describes our proposed methodology for the six basic expression classification track of Affective Behavior Analysis in-the-wild (ABAW) Competition 2022. In Learing from Synthetic Data(LSD) task, facial expression recognition (FER) methods aim to learn the representation of expression from the artificially generated data and generalise to real data. Because of the ambiguous of the synthetic data and the objectivity of the facial Action Unit (AU), we resort to the AU information for performance boosting, and make contributions as follows. First, to adapt the model to synthetic scenarios, we use the knowledge from pre-trained large-scale face recognition data. Second, we propose a conceptually-new framework, termed as AU-Supervised Convolutional Vision Transformers (AU-CVT), which clearly improves the performance of FER by jointly training auxiliary datasets with AU or pseudo AU labels. Our AU-CVT achieved F1 score as $0.6863$, accuracy as $0.7433$ on the validation set. The source code of our work is publicly available online: https://github.com/msy1412/ABAW4



### CoSMix: Compositional Semantic Mix for Domain Adaptation in 3D LiDAR Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.09778v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.09778v1)
- **Published**: 2022-07-20 09:33:42+00:00
- **Updated**: 2022-07-20 09:33:42+00:00
- **Authors**: Cristiano Saltori, Fabio Galasso, Giuseppe Fiameni, Nicu Sebe, Elisa Ricci, Fabio Poiesi
- **Comment**: Accepted at ECCV 2022
- **Journal**: None
- **Summary**: 3D LiDAR semantic segmentation is fundamental for autonomous driving. Several Unsupervised Domain Adaptation (UDA) methods for point cloud data have been recently proposed to improve model generalization for different sensors and environments. Researchers working on UDA problems in the image domain have shown that sample mixing can mitigate domain shift. We propose a new approach of sample mixing for point cloud UDA, namely Compositional Semantic Mix (CoSMix), the first UDA approach for point cloud segmentation based on sample mixing. CoSMix consists of a two-branch symmetric network that can process labelled synthetic data (source) and real-world unlabelled point clouds (target) concurrently. Each branch operates on one domain by mixing selected pieces of data from the other one, and by using the semantic information derived from source labels and target pseudo-labels. We evaluate CoSMix on two large-scale datasets, showing that it outperforms state-of-the-art methods by a large margin. Our code is available at https://github.com/saltoricristiano/cosmix-uda.



### Cross-Modal Contrastive Representation Learning for Audio-to-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2207.12121v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.GR, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2207.12121v1)
- **Published**: 2022-07-20 10:00:49+00:00
- **Updated**: 2022-07-20 10:00:49+00:00
- **Authors**: HaeChun Chung, JooYong Shim, Jong-Kook Kim
- **Comment**: 7 pages, 3 figures, Accepted to MUE 2022
- **Journal**: None
- **Summary**: Multiple modalities for certain information provide a variety of perspectives on that information, which can improve the understanding of the information. Thus, it may be crucial to generate data of different modality from the existing data to enhance the understanding. In this paper, we investigate the cross-modal audio-to-image generation problem and propose Cross-Modal Contrastive Representation Learning (CMCRL) to extract useful features from audios and use it in the generation phase. Experimental results show that CMCRL enhances quality of images generated than previous research.



### FaceFormer: Scale-aware Blind Face Restoration with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2207.09790v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09790v1)
- **Published**: 2022-07-20 10:08:34+00:00
- **Updated**: 2022-07-20 10:08:34+00:00
- **Authors**: Aijin Li, Gen Li, Lei Sun, Xintao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Blind face restoration usually encounters with diverse scale face inputs, especially in the real world. However, most of the current works support specific scale faces, which limits its application ability in real-world scenarios. In this work, we propose a novel scale-aware blind face restoration framework, named FaceFormer, which formulates facial feature restoration as scale-aware transformation. The proposed Facial Feature Up-sampling (FFUP) module dynamically generates upsampling filters based on the original scale-factor priors, which facilitate our network to adapt to arbitrary face scales. Moreover, we further propose the facial feature embedding (FFE) module which leverages transformer to hierarchically extract diversity and robustness of facial latent. Thus, our FaceFormer achieves fidelity and robustness restored faces, which possess realistic and symmetrical details of facial components. Extensive experiments demonstrate that our proposed method trained with synthetic dataset generalizes better to a natural low quality images than current state-of-the-arts.



### Unsupervised Industrial Anomaly Detection via Pattern Generative and Contrastive Networks
- **Arxiv ID**: http://arxiv.org/abs/2207.09792v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.09792v1)
- **Published**: 2022-07-20 10:09:53+00:00
- **Updated**: 2022-07-20 10:09:53+00:00
- **Authors**: Jianfeng Huang, Chenyang Li, Yimin Lin, Shiguo Lian
- **Comment**: None
- **Journal**: None
- **Summary**: It is hard to collect enough flaw images for training deep learning network in industrial production. Therefore, existing industrial anomaly detection methods prefer to use CNN-based unsupervised detection and localization network to achieve this task. However, these methods always fail when there are varieties happened in new signals since traditional end-to-end networks suffer barriers of fitting nonlinear model in high-dimensional space. Moreover, they have a memory library by clustering the feature of normal images essentially, which cause it is not robust to texture change. To this end, we propose the Vision Transformer based (VIT-based) unsupervised anomaly detection network. It utilizes a hierarchical task learning and human experience to enhance its interpretability. Our network consists of pattern generation and comparison networks. Pattern generation network uses two VIT-based encoder modules to extract the feature of two consecutive image patches, then uses VIT-based decoder module to learn the human designed style of these features and predict the third image patch. After this, we use the Siamese-based network to compute the similarity of the generation image patch and original image patch. Finally, we refine the anomaly localization by the bi-directional inference strategy. Comparison experiments on public dataset MVTec dataset show our method achieves 99.8% AUC, which surpasses previous state-of-the-art methods. In addition, we give a qualitative illustration on our own leather and cloth datasets. The accurate segment results strongly prove the accuracy of our method in anomaly detection.



### EASNet: Searching Elastic and Accurate Network Architecture for Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2207.09796v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.09796v1)
- **Published**: 2022-07-20 10:26:18+00:00
- **Updated**: 2022-07-20 10:26:18+00:00
- **Authors**: Qiang Wang, Shaohuai Shi, Kaiyong Zhao, Xiaowen Chu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advanced studies have spent considerable human efforts on optimizing network architectures for stereo matching but hardly achieved both high accuracy and fast inference speed. To ease the workload in network design, neural architecture search (NAS) has been applied with great success to various sparse prediction tasks, such as image classification and object detection. However, existing NAS studies on the dense prediction task, especially stereo matching, still cannot be efficiently and effectively deployed on devices of different computing capabilities. To this end, we propose to train an elastic and accurate network for stereo matching (EASNet) that supports various 3D architectural settings on devices with different computing capabilities. Given the deployment latency constraint on the target device, we can quickly extract a sub-network from the full EASNet without additional training while the accuracy of the sub-network can still be maintained. Extensive experiments show that our EASNet outperforms both state-of-the-art human-designed and NAS-based architectures on Scene Flow and MPI Sintel datasets in terms of model accuracy and inference speed. Particularly, deployed on an inference GPU, EASNet achieves a new SOTA 0.73 EPE on the Scene Flow dataset with 100 ms, which is 4.5$\times$ faster than LEAStereo with a better quality model.



### Multimodal Transformer for Automatic 3D Annotation and Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.09805v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09805v1)
- **Published**: 2022-07-20 10:38:29+00:00
- **Updated**: 2022-07-20 10:38:29+00:00
- **Authors**: Chang Liu, Xiaoyan Qian, Binxiao Huang, Xiaojuan Qi, Edmund Lam, Siew-Chong Tan, Ngai Wong
- **Comment**: 14 pages, 4 figures
- **Journal**: None
- **Summary**: Despite a growing number of datasets being collected for training 3D object detection models, significant human effort is still required to annotate 3D boxes on LiDAR scans. To automate the annotation and facilitate the production of various customized datasets, we propose an end-to-end multimodal transformer (MTrans) autolabeler, which leverages both LiDAR scans and images to generate precise 3D box annotations from weak 2D bounding boxes. To alleviate the pervasive sparsity problem that hinders existing autolabelers, MTrans densifies the sparse point clouds by generating new 3D points based on 2D image information. With a multi-task design, MTrans segments the foreground/background, densifies LiDAR point clouds, and regresses 3D boxes simultaneously. Experimental results verify the effectiveness of the MTrans for improving the quality of the generated labels. By enriching the sparse point clouds, our method achieves 4.48\% and 4.03\% better 3D AP on KITTI moderate and hard samples, respectively, versus the state-of-the-art autolabeler. MTrans can also be extended to improve the accuracy for 3D object detection, resulting in a remarkable 89.45\% AP on KITTI hard samples. Codes are at \url{https://github.com/Cliu2/MTrans}.



### The Anatomy of Video Editing: A Dataset and Benchmark Suite for AI-Assisted Video Editing
- **Arxiv ID**: http://arxiv.org/abs/2207.09812v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09812v2)
- **Published**: 2022-07-20 10:53:48+00:00
- **Updated**: 2022-07-21 06:53:02+00:00
- **Authors**: Dawit Mureja Argaw, Fabian Caba Heilbron, Joon-Young Lee, Markus Woodson, In So Kweon
- **Comment**: Code is available at: https://github.com/dawitmureja/AVE.git
- **Journal**: None
- **Summary**: Machine learning is transforming the video editing industry. Recent advances in computer vision have leveled-up video editing tasks such as intelligent reframing, rotoscoping, color grading, or applying digital makeups. However, most of the solutions have focused on video manipulation and VFX. This work introduces the Anatomy of Video Editing, a dataset, and benchmark, to foster research in AI-assisted video editing. Our benchmark suite focuses on video editing tasks, beyond visual effects, such as automatic footage organization and assisted video assembling. To enable research on these fronts, we annotate more than 1.5M tags, with relevant concepts to cinematography, from 196176 shots sampled from movie scenes. We establish competitive baseline methods and detailed analyses for each of the tasks. We hope our work sparks innovative research towards underexplored areas of AI-assisted video editing.



### NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2207.09814v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09814v2)
- **Published**: 2022-07-20 10:55:55+00:00
- **Updated**: 2022-08-12 04:41:05+00:00
- **Authors**: Chenfei Wu, Jian Liang, Xiaowei Hu, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zicheng Liu, Yuejian Fang, Nan Duan
- **Comment**: 24 pages, 19 figures
- **Journal**: None
- **Summary**: In this paper, we present NUWA-Infinity, a generative model for infinite visual synthesis, which is defined as the task of generating arbitrarily-sized high-resolution images or long-duration videos. An autoregressive over autoregressive generation mechanism is proposed to deal with this variable-size generation task, where a global patch-level autoregressive model considers the dependencies between patches, and a local token-level autoregressive model considers dependencies between visual tokens within each patch. A Nearby Context Pool (NCP) is introduced to cache-related patches already generated as the context for the current patch being generated, which can significantly save computation costs without sacrificing patch-level dependency modeling. An Arbitrary Direction Controller (ADC) is used to decide suitable generation orders for different visual synthesis tasks and learn order-aware positional embeddings. Compared to DALL-E, Imagen and Parti, NUWA-Infinity can generate high-resolution images with arbitrary sizes and support long-duration video generation additionally. Compared to NUWA, which also covers images and videos, NUWA-Infinity has superior visual synthesis capabilities in terms of resolution and variable-size generation. The GitHub link is https://github.com/microsoft/NUWA. The homepage link is https://nuwa-infinity.microsoft.com.



### UNIF: United Neural Implicit Functions for Clothed Human Reconstruction and Animation
- **Arxiv ID**: http://arxiv.org/abs/2207.09835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09835v1)
- **Published**: 2022-07-20 11:41:29+00:00
- **Updated**: 2022-07-20 11:41:29+00:00
- **Authors**: Shenhan Qian, Jiale Xu, Ziwei Liu, Liqian Ma, Shenghua Gao
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: We propose united implicit functions (UNIF), a part-based method for clothed human reconstruction and animation with raw scans and skeletons as the input. Previous part-based methods for human reconstruction rely on ground-truth part labels from SMPL and thus are limited to minimal-clothed humans. In contrast, our method learns to separate parts from body motions instead of part supervision, thus can be extended to clothed humans and other articulated objects. Our Partition-from-Motion is achieved by a bone-centered initialization, a bone limit loss, and a section normal loss that ensure stable part division even when the training poses are limited. We also present a minimal perimeter loss for SDF to suppress extra surfaces and part overlapping. Another core of our method is an adjacent part seaming algorithm that produces non-rigid deformations to maintain the connection between parts which significantly relieves the part-based artifacts. Under this algorithm, we further propose "Competing Parts", a method that defines blending weights by the relative position of a point to bones instead of the absolute position, avoiding the generalization problem of neural implicit functions with inverse LBS (linear blend skinning). We demonstrate the effectiveness of our method by clothed human body reconstruction and animation on the CAPE and the ClothSeq datasets.



### EleGANt: Exquisite and Locally Editable GAN for Makeup Transfer
- **Arxiv ID**: http://arxiv.org/abs/2207.09840v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09840v1)
- **Published**: 2022-07-20 11:52:07+00:00
- **Updated**: 2022-07-20 11:52:07+00:00
- **Authors**: Chenyu Yang, Wanrong He, Yingqing Xu, Yang Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing methods view makeup transfer as transferring color distributions of different facial regions and ignore details such as eye shadows and blushes. Besides, they only achieve controllable transfer within predefined fixed regions. This paper emphasizes the transfer of makeup details and steps towards more flexible controls. To this end, we propose Exquisite and locally editable GAN for makeup transfer (EleGANt). It encodes facial attributes into pyramidal feature maps to preserves high-frequency information. It uses attention to extract makeup features from the reference and adapt them to the source face, and we introduce a novel Sow-Attention Module that applies attention within shifted overlapped windows to reduce the computational cost. Moreover, EleGANt is the first to achieve customized local editing within arbitrary areas by corresponding editing on the feature maps. Extensive experiments demonstrate that EleGANt generates realistic makeup faces with exquisite details and achieves state-of-the-art performance. The code is available at https://github.com/Chenyu-Yang-2000/EleGANt.



### An Embedded Monocular Vision Approach for Ground-Aware Objects Detection and Position Estimation
- **Arxiv ID**: http://arxiv.org/abs/2207.09851v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.09851v1)
- **Published**: 2022-07-20 12:35:38+00:00
- **Updated**: 2022-07-20 12:35:38+00:00
- **Authors**: João G. Melo, Edna Barros
- **Comment**: 12 pages, 5 figures, submitted to RoboCup Symposium 2022
- **Journal**: None
- **Summary**: In the RoboCup Small Size League (SSL), teams are encouraged to propose solutions for executing basic soccer tasks inside the SSL field using only embedded sensing information. Thus, this work proposes an embedded monocular vision approach for detecting objects and estimating relative positions inside the soccer field. Prior knowledge from the environment is exploited by assuming objects lay on the ground, and the onboard camera has its position fixed on the robot. We implemented the proposed method on an NVIDIA Jetson Nano and employed SSD MobileNet v2 for 2D Object Detection with TensorRT optimization, detecting balls, robots, and goals with distances up to 3.5 meters. Ball localization evaluation shows that the proposed solution overcomes the currently used SSL vision system for positions closer than 1 meter to the onboard camera with a Root Mean Square Error of 14.37 millimeters. In addition, the proposed method achieves real-time performance with an average processing speed of 30 frames per second.



### Everything is There in Latent Space: Attribute Editing and Attribute Style Manipulation by StyleGAN Latent Space Exploration
- **Arxiv ID**: http://arxiv.org/abs/2207.09855v1
- **DOI**: 10.1145/3503161.3547972
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.09855v1)
- **Published**: 2022-07-20 12:40:32+00:00
- **Updated**: 2022-07-20 12:40:32+00:00
- **Authors**: Rishubh Parihar, Ankit Dhiman, Tejan Karmali, R. Venkatesh Babu
- **Comment**: Project page: https://sites.google.com/view/flamelatentediting
- **Journal**: None
- **Summary**: Unconstrained Image generation with high realism is now possible using recent Generative Adversarial Networks (GANs). However, it is quite challenging to generate images with a given set of attributes. Recent methods use style-based GAN models to perform image editing by leveraging the semantic hierarchy present in the layers of the generator. We present Few-shot Latent-based Attribute Manipulation and Editing (FLAME), a simple yet effective framework to perform highly controlled image editing by latent space manipulation. Specifically, we estimate linear directions in the latent space (of a pre-trained StyleGAN) that controls semantic attributes in the generated image. In contrast to previous methods that either rely on large-scale attribute labeled datasets or attribute classifiers, FLAME uses minimal supervision of a few curated image pairs to estimate disentangled edit directions. FLAME can perform both individual and sequential edits with high precision on a diverse set of images while preserving identity. Further, we propose a novel task of Attribute Style Manipulation to generate diverse styles for attributes such as eyeglass and hair. We first encode a set of synthetic images of the same identity but having different attribute styles in the latent space to estimate an attribute style manifold. Sampling a new latent from this manifold will result in a new attribute style in the generated image. We propose a novel sampling method to sample latent from the manifold, enabling us to generate a diverse set of attribute styles beyond the styles present in the training set. FLAME can generate diverse attribute styles in a disentangled manner. We illustrate the superior performance of FLAME against previous image editing methods by extensive qualitative and quantitative comparisons. FLAME also generalizes well on multiple datasets such as cars and churches.



### Evaluating the Stability of Deep Image Quality Assessment With Respect to Image Scaling
- **Arxiv ID**: http://arxiv.org/abs/2207.09856v1
- **DOI**: 10.1587/transinf.2022EDL8025
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.09856v1)
- **Published**: 2022-07-20 12:44:13+00:00
- **Updated**: 2022-07-20 12:44:13+00:00
- **Authors**: Koki Tsubota, Hiroaki Akutsu, Kiyoharu Aizawa
- **Comment**: IEICE Transactions on Information and Systems (Letter)
- **Journal**: None
- **Summary**: Image quality assessment (IQA) is a fundamental metric for image processing tasks (e.g., compression). With full-reference IQAs, traditional IQAs, such as PSNR and SSIM, have been used. Recently, IQAs based on deep neural networks (deep IQAs), such as LPIPS and DISTS, have also been used. It is known that image scaling is inconsistent among deep IQAs, as some perform down-scaling as pre-processing, whereas others instead use the original image size. In this paper, we show that the image scale is an influential factor that affects deep IQA performance. We comprehensively evaluate four deep IQAs on the same five datasets, and the experimental results show that image scale significantly influences IQA performance. We found that the most appropriate image scale is often neither the default nor the original size, and the choice differs depending on the methods and datasets used. We visualized the stability and found that PieAPP is the most stable among the four deep IQAs.



### Discrete-Constrained Regression for Local Counting Models
- **Arxiv ID**: http://arxiv.org/abs/2207.09865v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09865v1)
- **Published**: 2022-07-20 12:54:23+00:00
- **Updated**: 2022-07-20 12:54:23+00:00
- **Authors**: Haipeng Xiong, Angela Yao
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Local counts, or the number of objects in a local area, is a continuous value by nature. Yet recent state-of-the-art methods show that formulating counting as a classification task performs better than regression. Through a series of experiments on carefully controlled synthetic data, we show that this counter-intuitive result is caused by imprecise ground truth local counts. Factors such as biased dot annotations and incorrectly matched Gaussian kernels used to generate ground truth counts introduce deviations from the true local counts. Standard continuous regression is highly sensitive to these errors, explaining the performance gap between classification and regression. To mitigate the sensitivity, we loosen the regression formulation from a continuous scale to a discrete ordering and propose a novel discrete-constrained (DC) regression. Applied to crowd counting, DC-regression is more accurate than both classification and standard regression on three public benchmarks. A similar advantage also holds for the age estimation task, verifying the overall effectiveness of DC-regression.



### A System-driven Automatic Ground Truth Generation Method for DL Inner-City Driving Corridor Detectors
- **Arxiv ID**: http://arxiv.org/abs/2207.11234v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.11234v1)
- **Published**: 2022-07-20 12:55:16+00:00
- **Updated**: 2022-07-20 12:55:16+00:00
- **Authors**: Jona Ruthardt, Thomas Michalke
- **Comment**: 8 pages
- **Journal**: IEEE Intelligent Transportation Systems Conference 2022
- **Summary**: Data-driven perception approaches are well-established in automated driving systems. In many fields even super-human performance is reached. Unlike prediction and planning approaches, mainly supervised learning algorithms are used for the perception domain. Therefore, a major remaining challenge is the efficient generation of ground truth data. As perception modules are positioned close to the sensor, they typically run on raw sensor data of high bandwidth. Due to that, the generation of ground truth labels typically causes a significant manual effort, which leads to high costs for the labelling itself and the necessary quality control. In this contribution, we propose an automatic labeling approach for semantic segmentation of the drivable ego corridor that reduces the manual effort by a factor of 150 and more. The proposed holistic approach could be used in an automated data loop, allowing a continuous improvement of the depending perception modules.



### Adaptive Mixture of Experts Learning for Generalizable Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2207.09868v1
- **DOI**: 10.1145/3503161.3547769
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09868v1)
- **Published**: 2022-07-20 13:02:51+00:00
- **Updated**: 2022-07-20 13:02:51+00:00
- **Authors**: Qianyu Zhou, Ke-Yue Zhang, Taiping Yao, Ran Yi, Shouhong Ding, Lizhuang Ma
- **Comment**: Accepted to ACM MM 2022
- **Journal**: None
- **Summary**: With various face presentation attacks emerging continually, face anti-spoofing (FAS) approaches based on domain generalization (DG) have drawn growing attention. Existing DG-based FAS approaches always capture the domain-invariant features for generalizing on the various unseen domains. However, they neglect individual source domains' discriminative characteristics and diverse domain-specific information of the unseen domains, and the trained model is not sufficient to be adapted to various unseen domains. To address this issue, we propose an Adaptive Mixture of Experts Learning (AMEL) framework, which exploits the domain-specific information to adaptively establish the link among the seen source domains and unseen target domains to further improve the generalization. Concretely, Domain-Specific Experts (DSE) are designed to investigate discriminative and unique domain-specific features as a complement to common domain-invariant features. Moreover, Dynamic Expert Aggregation (DEA) is proposed to adaptively aggregate the complementary information of each source expert based on the domain relevance to the unseen target domain. And combined with meta-learning, these modules work collaboratively to adaptively aggregate meaningful domain-specific information for the various unseen target domains. Extensive experiments and visualizations demonstrate the effectiveness of our method against the state-of-the-art competitors.



### A Novel Neural Network Training Method for Autonomous Driving Using Semi-Pseudo-Labels and 3D Data Augmentations
- **Arxiv ID**: http://arxiv.org/abs/2207.09869v1
- **DOI**: 10.1007/978-3-031-21967-2_18
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.09869v1)
- **Published**: 2022-07-20 13:04:08+00:00
- **Updated**: 2022-07-20 13:04:08+00:00
- **Authors**: Tamas Matuszka, Daniel Kozma
- **Comment**: None
- **Journal**: None
- **Summary**: Training neural networks to perform 3D object detection for autonomous driving requires a large amount of diverse annotated data. However, obtaining training data with sufficient quality and quantity is expensive and sometimes impossible due to human and sensor constraints. Therefore, a novel solution is needed for extending current training methods to overcome this limitation and enable accurate 3D object detection. Our solution for the above-mentioned problem combines semi-pseudo-labeling and novel 3D augmentations. For demonstrating the applicability of the proposed method, we have designed a convolutional neural network for 3D object detection which can significantly increase the detection range in comparison with the training data distribution.



### Negative Samples are at Large: Leveraging Hard-distance Elastic Loss for Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2207.09884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09884v1)
- **Published**: 2022-07-20 13:30:16+00:00
- **Updated**: 2022-07-20 13:30:16+00:00
- **Authors**: Hyungtae Lee, Sungmin Eum, Heesung Kwon
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: We present a Momentum Re-identification (MoReID) framework that can leverage a very large number of negative samples in training for general re-identification task. The design of this framework is inspired by Momentum Contrast (MoCo), which uses a dictionary to store current and past batches to build a large set of encoded samples. As we find it less effective to use past positive samples which may be highly inconsistent to the encoded feature property formed with the current positive samples, MoReID is designed to use only a large number of negative samples stored in the dictionary. However, if we train the model using the widely used Triplet loss that uses only one sample to represent a set of positive/negative samples, it is hard to effectively leverage the enlarged set of negative samples acquired by the MoReID framework. To maximize the advantage of using the scaled-up negative sample set, we newly introduce Hard-distance Elastic loss (HE loss), which is capable of using more than one hard sample to represent a large number of samples. Our experiments demonstrate that a large number of negative samples provided by MoReID framework can be utilized at full capacity only with the HE loss, achieving the state-of-the-art accuracy on three re-ID benchmarks, VeRi-776, Market-1501, and VeRi-Wild.



### Face-to-Face Co-Located Human-Human Social Interaction Analysis using Nonverbal Cues: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2207.10574v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.10574v1)
- **Published**: 2022-07-20 13:37:57+00:00
- **Updated**: 2022-07-20 13:37:57+00:00
- **Authors**: Cigdem Beyan, Alessandro Vinciarelli, Alessio Del Bue
- **Comment**: Submitted to ACM Computing and Surveys
- **Journal**: None
- **Summary**: This work presents a systematic review of recent efforts (since 2010) aimed at automatic analysis of nonverbal cues displayed in face-to-face co-located human-human social interactions. The main reason for focusing on nonverbal cues is that these are the physical, machine detectable traces of social and psychological phenomena. Therefore, detecting and understanding nonverbal cues means, at least to a certain extent, to detect and understand social and psychological phenomena. The covered topics are categorized into three as: a) modeling social traits, such as leadership, dominance, personality traits, b) social role recognition and social relations detection and c) interaction dynamics analysis in terms of group cohesion, empathy, rapport and so forth. We target the co-located interactions, in which the interactants are always humans. The survey covers a wide spectrum of settings and scenarios, including free-standing interactions, meetings, indoor and outdoor social exchanges, dyadic conversations, and crowd dynamics. For each of them, the survey considers the three main elements of nonverbal cues analysis, namely data, sensing approaches and computational methodologies. The goal is to highlight the main advances of the last decade, to point out existing limitations, and to outline future directions.



### Labeling instructions matter in biomedical image analysis
- **Arxiv ID**: http://arxiv.org/abs/2207.09899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09899v1)
- **Published**: 2022-07-20 13:51:39+00:00
- **Updated**: 2022-07-20 13:51:39+00:00
- **Authors**: Tim Rädsch, Annika Reinke, Vivienn Weru, Minu D. Tizabi, Nicholas Schreck, A. Emre Kavur, Bünyamin Pekdemir, Tobias Roß, Annette Kopp-Schneider, Lena Maier-Hein
- **Comment**: None
- **Journal**: None
- **Summary**: Biomedical image analysis algorithm validation depends on high-quality annotation of reference datasets, for which labeling instructions are key. Despite their importance, their optimization remains largely unexplored. Here, we present the first systematic study of labeling instructions and their impact on annotation quality in the field. Through comprehensive examination of professional practice and international competitions registered at the MICCAI Society, we uncovered a discrepancy between annotators' needs for labeling instructions and their current quality and availability. Based on an analysis of 14,040 images annotated by 156 annotators from four professional companies and 708 Amazon Mechanical Turk (MTurk) crowdworkers using instructions with different information density levels, we further found that including exemplary images significantly boosts annotation performance compared to text-only descriptions, while solely extending text descriptions does not. Finally, professional annotators constantly outperform MTurk crowdworkers. Our study raises awareness for the need of quality standards in biomedical image analysis labeling instructions.



### A note on the variation of geometric functionals
- **Arxiv ID**: http://arxiv.org/abs/2207.09915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09915v1)
- **Published**: 2022-07-20 14:02:57+00:00
- **Updated**: 2022-07-20 14:02:57+00:00
- **Authors**: Nir Sochen
- **Comment**: None
- **Journal**: None
- **Summary**: Calculus of Variation combined with Differential Geometry as tools of modelling and solving problems in image processing and computer vision were introduced in the late 80's and the 90s of the 20th century. The beginning of an extensive work in these directions was marked by works such as Geodesic Active Contours (GAC), the Beltrami framework, level set method of Osher and Sethian the works of Charpiat et al. and the works by Chan and Vese to name just a few. In many cases the optimization of these functional are done by the gradient descent method via the calculation of the Euler-Lagrange equations. Straightforward use of the resulted EL equations in the gradient descent scheme leads to non-geometric and in some cases non sensical equations. It is costumary to modify these EL equations or even the functional itself in order to obtain geometric and/or sensical equations. The aim of this note is to point to the correct way to derive the EL and the gradient descent equations such that the resulted gradient descent equation is geometric and makes sense.



### An Efficient Framework for Few-shot Skeleton-based Temporal Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.09925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09925v1)
- **Published**: 2022-07-20 14:08:37+00:00
- **Updated**: 2022-07-20 14:08:37+00:00
- **Authors**: Leiyang Xu, Qiang Wang, Xiaotian Lin, Lin Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal action segmentation (TAS) aims to classify and locate actions in the long untrimmed action sequence. With the success of deep learning, many deep models for action segmentation have emerged. However, few-shot TAS is still a challenging problem. This study proposes an efficient framework for the few-shot skeleton-based TAS, including a data augmentation method and an improved model. The data augmentation approach based on motion interpolation is presented here to solve the problem of insufficient data, and can increase the number of samples significantly by synthesizing action sequences. Besides, we concatenate a Connectionist Temporal Classification (CTC) layer with a network designed for skeleton-based TAS to obtain an optimized model. Leveraging CTC can enhance the temporal alignment between prediction and ground truth and further improve the segment-wise metrics of segmentation results. Extensive experiments on both public and self-constructed datasets, including two small-scale datasets and one large-scale dataset, show the effectiveness of two proposed methods in improving the performance of the few-shot skeleton-based TAS task.



### ViGAT: Bottom-up event recognition and explanation in video using factorized graph attention network
- **Arxiv ID**: http://arxiv.org/abs/2207.09927v2
- **DOI**: 10.1109/ACCESS.2022.3213652
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.09927v2)
- **Published**: 2022-07-20 14:12:05+00:00
- **Updated**: 2022-10-31 12:44:11+00:00
- **Authors**: Nikolaos Gkalelis, Dimitrios Daskalakis, Vasileios Mezaris
- **Comment**: None
- **Journal**: IEEE Access, vol. 10, pp. 108797-108816, 2022
- **Summary**: In this paper a pure-attention bottom-up approach, called ViGAT, that utilizes an object detector together with a Vision Transformer (ViT) backbone network to derive object and frame features, and a head network to process these features for the task of event recognition and explanation in video, is proposed. The ViGAT head consists of graph attention network (GAT) blocks factorized along the spatial and temporal dimensions in order to capture effectively both local and long-term dependencies between objects or frames. Moreover, using the weighted in-degrees (WiDs) derived from the adjacency matrices at the various GAT blocks, we show that the proposed architecture can identify the most salient objects and frames that explain the decision of the network. A comprehensive evaluation study is performed, demonstrating that the proposed approach provides state-of-the-art results on three large, publicly available video datasets (FCVID, Mini-Kinetics, ActivityNet).



### Robust Landmark-based Stent Tracking in X-ray Fluoroscopy
- **Arxiv ID**: http://arxiv.org/abs/2207.09933v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.09933v3)
- **Published**: 2022-07-20 14:20:03+00:00
- **Updated**: 2022-07-22 01:07:48+00:00
- **Authors**: Luojie Huang, Yikang Liu, Li Chen, Eric Z. Chen, Xiao Chen, Shanhui Sun
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: In clinical procedures of angioplasty (i.e., open clogged coronary arteries), devices such as balloons and stents need to be placed and expanded in arteries under the guidance of X-ray fluoroscopy. Due to the limitation of X-ray dose, the resulting images are often noisy. To check the correct placement of these devices, typically multiple motion-compensated frames are averaged to enhance the view. Therefore, device tracking is a necessary procedure for this purpose. Even though angioplasty devices are designed to have radiopaque markers for the ease of tracking, current methods struggle to deliver satisfactory results due to the small marker size and complex scenes in angioplasty. In this paper, we propose an end-to-end deep learning framework for single stent tracking, which consists of three hierarchical modules: U-Net based landmark detection, ResNet based stent proposal and feature extraction, and graph convolutional neural network (GCN) based stent tracking that temporally aggregates both spatial information and appearance features. The experiments show that our method performs significantly better in detection compared with the state-of-the-art point-based tracking models. In addition, its fast inference speed satisfies clinical requirements.



### DeepIPC: Deeply Integrated Perception and Control for an Autonomous Vehicle in Real Environments
- **Arxiv ID**: http://arxiv.org/abs/2207.09934v5
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.09934v5)
- **Published**: 2022-07-20 14:20:35+00:00
- **Updated**: 2023-06-21 11:31:28+00:00
- **Authors**: Oskar Natan, Jun Miura
- **Comment**: Submitted to Robotics and Autonomous Systems
- **Journal**: None
- **Summary**: We propose DeepIPC, an end-to-end autonomous driving model that handles both perception and control tasks in driving a vehicle. The model consists of two main parts, perception and controller modules. The perception module takes an RGBD image to perform semantic segmentation and bird's eye view (BEV) semantic mapping along with providing their encoded features. Meanwhile, the controller module processes these features with the measurement of GNSS locations and angular speed to estimate waypoints that come with latent features. Then, two different agents are used to translate waypoints and latent features into a set of navigational controls to drive the vehicle. The model is evaluated by predicting driving records and performing automated driving under various conditions in real environments. The experimental results show that DeepIPC achieves the best drivability and multi-task performance even with fewer parameters compared to the other models. Codes will be published at https://github.com/oskarnatan/DeepIPC.



### Towards Efficient and Scale-Robust Ultra-High-Definition Image Demoireing
- **Arxiv ID**: http://arxiv.org/abs/2207.09935v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09935v1)
- **Published**: 2022-07-20 14:20:52+00:00
- **Updated**: 2022-07-20 14:20:52+00:00
- **Authors**: Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Jiajun Shen, Jia Li, Xiaojuan Qi
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: With the rapid development of mobile devices, modern widely-used mobile phones typically allow users to capture 4K resolution (i.e., ultra-high-definition) images. However, for image demoireing, a challenging task in low-level vision, existing works are generally carried out on low-resolution or synthetic images. Hence, the effectiveness of these methods on 4K resolution images is still unknown. In this paper, we explore moire pattern removal for ultra-high-definition images. To this end, we propose the first ultra-high-definition demoireing dataset (UHDM), which contains 5,000 real-world 4K resolution image pairs, and conduct a benchmark study on current state-of-the-art methods. Further, we present an efficient baseline model ESDNet for tackling 4K moire images, wherein we build a semantic-aligned scale-aware module to address the scale variation of moire patterns. Extensive experiments manifest the effectiveness of our approach, which outperforms state-of-the-art methods by a large margin while being much more lightweight. Code and dataset are available at https://xinyu-andy.github.io/uhdm-page.



### Probable Domain Generalization via Quantile Risk Minimization
- **Arxiv ID**: http://arxiv.org/abs/2207.09944v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.09944v4)
- **Published**: 2022-07-20 14:41:09+00:00
- **Updated**: 2023-08-22 09:31:35+00:00
- **Authors**: Cian Eastwood, Alexander Robey, Shashank Singh, Julius von Kügelgen, Hamed Hassani, George J. Pappas, Bernhard Schölkopf
- **Comment**: NeurIPS 2022 camera-ready (+ minor corrections)
- **Journal**: None
- **Summary**: Domain generalization (DG) seeks predictors which perform well on unseen test distributions by leveraging data drawn from multiple related training distributions or domains. To achieve this, DG is commonly formulated as an average- or worst-case problem over the set of possible domains. However, predictors that perform well on average lack robustness while predictors that perform well in the worst case tend to be overly-conservative. To address this, we propose a new probabilistic framework for DG where the goal is to learn predictors that perform well with high probability. Our key idea is that distribution shifts seen during training should inform us of probable shifts at test time, which we realize by explicitly relating training and test domains as draws from the same underlying meta-distribution. To achieve probable DG, we propose a new optimization problem called Quantile Risk Minimization (QRM). By minimizing the $\alpha$-quantile of predictor's risk distribution over domains, QRM seeks predictors that perform well with probability $\alpha$. To solve QRM in practice, we propose the Empirical QRM (EQRM) algorithm and provide: (i) a generalization bound for EQRM; and (ii) the conditions under which EQRM recovers the causal predictor as $\alpha \to 1$. In our experiments, we introduce a more holistic quantile-focused evaluation protocol for DG and demonstrate that EQRM outperforms state-of-the-art baselines on datasets from WILDS and DomainBed.



### VirtualPose: Learning Generalizable 3D Human Pose Models from Virtual Data
- **Arxiv ID**: http://arxiv.org/abs/2207.09949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09949v1)
- **Published**: 2022-07-20 14:47:28+00:00
- **Updated**: 2022-07-20 14:47:28+00:00
- **Authors**: Jiajun Su, Chunyu Wang, Xiaoxuan Ma, Wenjun Zeng, Yizhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: While monocular 3D pose estimation seems to have achieved very accurate results on the public datasets, their generalization ability is largely overlooked. In this work, we perform a systematic evaluation of the existing methods and find that they get notably larger errors when tested on different cameras, human poses and appearance. To address the problem, we introduce VirtualPose, a two-stage learning framework to exploit the hidden "free lunch" specific to this task, i.e. generating infinite number of poses and cameras for training models at no cost. To that end, the first stage transforms images to abstract geometry representations (AGR), and then the second maps them to 3D poses. It addresses the generalization issue from two aspects: (1) the first stage can be trained on diverse 2D datasets to reduce the risk of over-fitting to limited appearance; (2) the second stage can be trained on diverse AGR synthesized from a large number of virtual cameras and poses. It outperforms the SOTA methods without using any paired images and 3D poses from the benchmarks, which paves the way for practical applications. Code is available at https://github.com/wkom/VirtualPose.



### Learning Pedestrian Group Representations for Multi-modal Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/2207.09953v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.09953v1)
- **Published**: 2022-07-20 14:58:13+00:00
- **Updated**: 2022-07-20 14:58:13+00:00
- **Authors**: Inhwan Bae, Jin-Hwi Park, Hae-Gon Jeon
- **Comment**: Accepted at ECCV 2022
- **Journal**: None
- **Summary**: Modeling the dynamics of people walking is a problem of long-standing interest in computer vision. Many previous works involving pedestrian trajectory prediction define a particular set of individual actions to implicitly model group actions. In this paper, we present a novel architecture named GP-Graph which has collective group representations for effective pedestrian trajectory prediction in crowded environments, and is compatible with all types of existing approaches. A key idea of GP-Graph is to model both individual-wise and group-wise relations as graph representations. To do this, GP-Graph first learns to assign each pedestrian into the most likely behavior group. Using this assignment information, GP-Graph then forms both intra- and inter-group interactions as graphs, accounting for human-human relations within a group and group-group relations, respectively. To be specific, for the intra-group interaction, we mask pedestrian graph edges out of an associated group. We also propose group pooling&unpooling operations to represent a group with multiple pedestrians as one graph node. Lastly, GP-Graph infers a probability map for socially-acceptable future trajectories from the integrated features of both group interactions. Moreover, we introduce a group-level latent vector sampling to ensure collective inferences over a set of possible future trajectories. Extensive experiments are conducted to validate the effectiveness of our architecture, which demonstrates consistent performance improvements with publicly available benchmarks. Code is publicly available at https://github.com/inhwanbae/GPGraph.



### Telepresence Video Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2207.09956v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.09956v1)
- **Published**: 2022-07-20 15:02:55+00:00
- **Updated**: 2022-07-20 15:02:55+00:00
- **Authors**: Zhenqiang Ying, Deepti Ghadiyaram, Alan Bovik
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Video conferencing, which includes both video and audio content, has contributed to dramatic increases in Internet traffic, as the COVID-19 pandemic forced millions of people to work and learn from home. Global Internet traffic of video conferencing has dramatically increased Because of this, efficient and accurate video quality tools are needed to monitor and perceptually optimize telepresence traffic streamed via Zoom, Webex, Meet, etc. However, existing models are limited in their prediction capabilities on multi-modal, live streaming telepresence content. Here we address the significant challenges of Telepresence Video Quality Assessment (TVQA) in several ways. First, we mitigated the dearth of subjectively labeled data by collecting ~2k telepresence videos from different countries, on which we crowdsourced ~80k subjective quality labels. Using this new resource, we created a first-of-a-kind online video quality prediction framework for live streaming, using a multi-modal learning framework with separate pathways to compute visual and audio quality predictions. Our all-in-one model is able to provide accurate quality predictions at the patch, frame, clip, and audiovisual levels. Our model achieves state-of-the-art performance on both existing quality databases and our new TVQA database, at a considerably lower computational expense, making it an attractive solution for mobile and embedded systems.



### Estimating Model Performance under Domain Shifts with Class-Specific Confidence Scores
- **Arxiv ID**: http://arxiv.org/abs/2207.09957v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.09957v1)
- **Published**: 2022-07-20 15:04:32+00:00
- **Updated**: 2022-07-20 15:04:32+00:00
- **Authors**: Zeju Li, Konstantinos Kamnitsas, Mobarakol Islam, Chen Chen, Ben Glocker
- **Comment**: Accepted at MICCAI 2022
- **Journal**: None
- **Summary**: Machine learning models are typically deployed in a test setting that differs from the training setting, potentially leading to decreased model performance because of domain shift. If we could estimate the performance that a pre-trained model would achieve on data from a specific deployment setting, for example a certain clinic, we could judge whether the model could safely be deployed or if its performance degrades unacceptably on the specific data. Existing approaches estimate this based on the confidence of predictions made on unlabeled test data from the deployment's domain. We find existing methods struggle with data that present class imbalance, because the methods used to calibrate confidence do not account for bias induced by class imbalance, consequently failing to estimate class-wise accuracy. Here, we introduce class-wise calibration within the framework of performance estimation for imbalanced datasets. Specifically, we derive class-specific modifications of state-of-the-art confidence-based model evaluation methods including temperature scaling (TS), difference of confidences (DoC), and average thresholded confidence (ATC). We also extend the methods to estimate Dice similarity coefficient (DSC) in image segmentation. We conduct experiments on four tasks and find the proposed modifications consistently improve the estimation accuracy for imbalanced datasets. Our methods improve accuracy estimation by 18\% in classification under natural domain shifts, and double the estimation accuracy on segmentation tasks, when compared with prior methods.



### Rethinking Few-Shot Class-Incremental Learning with Open-Set Hypothesis in Hyperbolic Geometry
- **Arxiv ID**: http://arxiv.org/abs/2207.09963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09963v1)
- **Published**: 2022-07-20 15:13:48+00:00
- **Updated**: 2022-07-20 15:13:48+00:00
- **Authors**: Yawen Cui, Zitong Yu, Wei Peng, Li Liu
- **Comment**: submitted to IEEE Transactions on Cybernetics
- **Journal**: None
- **Summary**: Few-Shot Class-Incremental Learning (FSCIL) aims at incrementally learning novel classes from a few labeled samples by avoiding the overfitting and catastrophic forgetting simultaneously. The current protocol of FSCIL is built by mimicking the general class-incremental learning setting, while it is not totally appropriate due to the different data configuration, i.e., novel classes are all in the limited data regime. In this paper, we rethink the configuration of FSCIL with the open-set hypothesis by reserving the possibility in the first session for incoming categories. To assign better performances on both close-set and open-set recognition to the model, Hyperbolic Reciprocal Point Learning module (Hyper-RPL) is built on Reciprocal Point Learning (RPL) with hyperbolic neural networks. Besides, for learning novel categories from limited labeled data, we incorporate a hyperbolic metric learning (Hyper-Metric) module into the distillation-based framework to alleviate the overfitting issue and better handle the trade-off issue between the preservation of old knowledge and the acquisition of new knowledge. The comprehensive assessments of the proposed configuration and modules on three benchmark datasets are executed to validate the effectiveness concerning three evaluation indicators.



### M2-Net: Multi-stages Specular Highlight Detection and Removal in Multi-scenes
- **Arxiv ID**: http://arxiv.org/abs/2207.09965v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09965v1)
- **Published**: 2022-07-20 15:18:43+00:00
- **Updated**: 2022-07-20 15:18:43+00:00
- **Authors**: Zhaoyangfan Huang, Kun Hu, Xingjun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel uniformity framework for highlight detection and removal in multi-scenes, including synthetic images, face images, natural images, and text images. The framework consists of three main components, highlight feature extractor module, highlight coarse removal module, and highlight refine removal module. Firstly, the highlight feature extractor module can directly separate the highlight feature and non-highlight feature from the original highlight image. Then highlight removal image is obtained using a coarse highlight removal network. To further improve the highlight removal effect, the refined highlight removal image is finally obtained using refine highlight removal module based on contextual highlight attention mechanisms. Extensive experimental results in multiple scenes indicate that the proposed framework can obtain excellent visual effects of highlight removal and achieve state-of-the-art results in several quantitative evaluation metrics. Our algorithm is applied for the first time in video highlight removal with promising results.



### Temporal and cross-modal attention for audio-visual zero-shot learning
- **Arxiv ID**: http://arxiv.org/abs/2207.09966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09966v1)
- **Published**: 2022-07-20 15:19:30+00:00
- **Updated**: 2022-07-20 15:19:30+00:00
- **Authors**: Otniel-Bogdan Mercea, Thomas Hummel, A. Sophia Koepke, Zeynep Akata
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Audio-visual generalised zero-shot learning for video classification requires understanding the relations between the audio and visual information in order to be able to recognise samples from novel, previously unseen classes at test time. The natural semantic and temporal alignment between audio and visual data in video data can be exploited to learn powerful representations that generalise to unseen classes at test time. We propose a multi-modal and Temporal Cross-attention Framework (\modelName) for audio-visual generalised zero-shot learning. Its inputs are temporally aligned audio and visual features that are obtained from pre-trained networks. Encouraging the framework to focus on cross-modal correspondence across time instead of self-attention within the modalities boosts the performance significantly. We show that our proposed framework that ingests temporal features yields state-of-the-art performance on the \ucf, \vgg, and \activity benchmarks for (generalised) zero-shot learning. Code for reproducing all results is available at \url{https://github.com/ExplainableML/TCAF-GZSL}.



### NeuralBF: Neural Bilateral Filtering for Top-down Instance Segmentation on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2207.09978v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2207.09978v1)
- **Published**: 2022-07-20 15:37:32+00:00
- **Updated**: 2022-07-20 15:37:32+00:00
- **Authors**: Weiwei Sun, Daniel Rebain, Renjie Liao, Vladimir Tankovich, Soroosh Yazdani, Kwang Moo Yi, Andrea Tagliasacchi
- **Comment**: Project website: https://neuralbf.github.io
- **Journal**: None
- **Summary**: We introduce a method for instance proposal generation for 3D point clouds. Existing techniques typically directly regress proposals in a single feed-forward step, leading to inaccurate estimation. We show that this serves as a critical bottleneck, and propose a method based on iterative bilateral filtering with learned kernels. Following the spirit of bilateral filtering, we consider both the deep feature embeddings of each point, as well as their locations in the 3D space. We show via synthetic experiments that our method brings drastic improvements when generating instance proposals for a given point of interest. We further validate our method on the challenging ScanNet benchmark, achieving the best instance segmentation performance amongst the sub-category of top-down methods.



### DecoupleNet: Decoupled Network for Domain Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.09988v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.09988v1)
- **Published**: 2022-07-20 15:47:34+00:00
- **Updated**: 2022-07-20 15:47:34+00:00
- **Authors**: Xin Lai, Zhuotao Tian, Xiaogang Xu, Yingcong Chen, Shu Liu, Hengshuang Zhao, Liwei Wang, Jiaya Jia
- **Comment**: Accepted to ECCV 2022. Code is available at
  https://github.com/dvlab-research/DecoupleNet
- **Journal**: None
- **Summary**: Unsupervised domain adaptation in semantic segmentation has been raised to alleviate the reliance on expensive pixel-wise annotations. It leverages a labeled source domain dataset as well as unlabeled target domain images to learn a segmentation network. In this paper, we observe two main issues of the existing domain-invariant learning framework. (1) Being distracted by the feature distribution alignment, the network cannot focus on the segmentation task. (2) Fitting source domain data well would compromise the target domain performance. To address these issues, we propose DecoupleNet that alleviates source domain overfitting and enables the final model to focus more on the segmentation task. Furthermore, we put forward Self-Discrimination (SD) and introduce an auxiliary classifier to learn more discriminative target domain features with pseudo labels. Finally, we propose Online Enhanced Self-Training (OEST) to contextually enhance the quality of pseudo labels in an online manner. Experiments show our method outperforms existing state-of-the-art methods, and extensive ablation studies verify the effectiveness of each component. Code is available at https://github.com/dvlab-research/DecoupleNet.



### Overcoming Shortcut Learning in a Target Domain by Generalizing Basic Visual Factors from a Source Domain
- **Arxiv ID**: http://arxiv.org/abs/2207.10002v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.10002v1)
- **Published**: 2022-07-20 16:05:32+00:00
- **Updated**: 2022-07-20 16:05:32+00:00
- **Authors**: Piyapat Saranrittichai, Chaithanya Kumar Mummadi, Claudia Blaiotta, Mauricio Munoz, Volker Fischer
- **Comment**: Accepted for publication at European Conference on Computer Vision
  (ECCV) 2022
- **Journal**: None
- **Summary**: Shortcut learning occurs when a deep neural network overly relies on spurious correlations in the training dataset in order to solve downstream tasks. Prior works have shown how this impairs the compositional generalization capability of deep learning models. To address this problem, we propose a novel approach to mitigate shortcut learning in uncontrolled target domains. Our approach extends the training set with an additional dataset (the source domain), which is specifically designed to facilitate learning independent representations of basic visual factors. We benchmark our idea on synthetic target domains where we explicitly control shortcut opportunities as well as real-world target domains. Furthermore, we analyze the effect of different specifications of the source domain and the network architecture on compositional generalization. Our main finding is that leveraging data from a source domain is an effective way to mitigate shortcut learning. By promoting independence across different factors of variation in the learned representations, networks can learn to consider only predictive factors and ignore potential shortcut factors during inference.



### BYEL : Bootstrap Your Emotion Latent
- **Arxiv ID**: http://arxiv.org/abs/2207.10003v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.10003v2)
- **Published**: 2022-07-20 16:05:56+00:00
- **Updated**: 2022-08-22 01:59:29+00:00
- **Authors**: Hyungjun Lee, Hwangyu Lim, Sejoon Lim
- **Comment**: ECCVW 2022 Accepted
- **Journal**: None
- **Summary**: With the improved performance of deep learning, the number of studies trying to apply deep learning to human emotion analysis is increasing rapidly. But even with this trend going on, it is still difficult to obtain high-quality images and annotations. For this reason, the Learning from Synthetic Data (LSD) Challenge, which learns from synthetic images and infers from real images, is one of the most interesting areas. In general, Domain Adaptation methods are widely used to address LSD challenges, but there is a limitation that target domains (real images) are still needed. Focusing on these limitations, we propose a framework Bootstrap Your Emotion Latent (BYEL), which uses only synthetic images in training. BYEL is implemented by adding Emotion Classifiers and Emotion Vector Subtraction to the BYOL framework that performs well in Self-Supervised Representation Learning. We train our framework using synthetic images generated from the Aff-wild2 dataset and evaluate it using real images from the Aff-wild2 dataset. The result shows that our framework (0.3084) performs 2.8% higher than the baseline (0.3) on the macro F1 score metric.



### E-Graph: Minimal Solution for Rigid Rotation with Extensibility Graphs
- **Arxiv ID**: http://arxiv.org/abs/2207.10008v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.10008v1)
- **Published**: 2022-07-20 16:11:48+00:00
- **Updated**: 2022-07-20 16:11:48+00:00
- **Authors**: Yanyan Li, Federico Tombari
- **Comment**: None
- **Journal**: None
- **Summary**: Minimal solutions for relative rotation and translation estimation tasks have been explored in different scenarios, typically relying on the so-called co-visibility graph. However, how to build direct rotation relationships between two frames without overlap is still an open topic, which, if solved, could greatly improve the accuracy of visual odometry.   In this paper, a new minimal solution is proposed to solve relative rotation estimation between two images without overlapping areas by exploiting a new graph structure, which we call Extensibility Graph (E-Graph). Differently from a co-visibility graph, high-level landmarks, including vanishing directions and plane normals, are stored in our E-Graph, which are geometrically extensible. Based on E-Graph, the rotation estimation problem becomes simpler and more elegant, as it can deal with pure rotational motion and requires fewer assumptions, e.g. Manhattan/Atlanta World, planar/vertical motion. Finally, we embed our rotation estimation strategy into a complete camera tracking and mapping system which obtains 6-DoF camera poses and a dense 3D mesh model.   Extensive experiments on public benchmarks demonstrate that the proposed method achieves state-of-the-art tracking performance.



### Generative Domain Adaptation for Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/2207.10015v2
- **DOI**: 10.1007/978-3-031-20065-6_20
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10015v2)
- **Published**: 2022-07-20 16:24:57+00:00
- **Updated**: 2022-09-11 13:37:41+00:00
- **Authors**: Qianyu Zhou, Ke-Yue Zhang, Taiping Yao, Ran Yi, Kekai Sheng, Shouhong Ding, Lizhuang Ma
- **Comment**: Accepted to European Conference on Computer Vision (ECCV), 2022
- **Journal**: None
- **Summary**: Face anti-spoofing (FAS) approaches based on unsupervised domain adaption (UDA) have drawn growing attention due to promising performances for target scenarios. Most existing UDA FAS methods typically fit the trained models to the target domain via aligning the distribution of semantic high-level features. However, insufficient supervision of unlabeled target domains and neglect of low-level feature alignment degrade the performances of existing methods. To address these issues, we propose a novel perspective of UDA FAS that directly fits the target data to the models, i.e., stylizes the target data to the source-domain style via image translation, and further feeds the stylized data into the well-trained source model for classification. The proposed Generative Domain Adaptation (GDA) framework combines two carefully designed consistency constraints: 1) Inter-domain neural statistic consistency guides the generator in narrowing the inter-domain gap. 2) Dual-level semantic consistency ensures the semantic quality of stylized images. Besides, we propose intra-domain spectrum mixup to further expand target data distributions to ensure generalization and reduce the intra-domain gap. Extensive experiments and visualizations demonstrate the effectiveness of our method against the state-of-the-art methods.



### Secrets of Event-Based Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/2207.10022v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.10022v2)
- **Published**: 2022-07-20 16:40:38+00:00
- **Updated**: 2022-07-21 17:26:51+00:00
- **Authors**: Shintaro Shiba, Yoshimitsu Aoki, Guillermo Gallego
- **Comment**: 23 pages, 11 figures, 7 tables,
  https://github.com/tub-rip/event_based_optical_flow
- **Journal**: European Conference on Computer Vision (ECCV), Tel Aviv, 2022
- **Summary**: Event cameras respond to scene dynamics and offer advantages to estimate motion. Following recent image-based deep-learning achievements, optical flow estimation methods for event cameras have rushed to combine those image-based methods with event data. However, it requires several adaptations (data conversion, loss function, etc.) as they have very different properties. We develop a principled method to extend the Contrast Maximization framework to estimate optical flow from events alone. We investigate key elements: how to design the objective function to prevent overfitting, how to warp events to deal better with occlusions, and how to improve convergence with multi-scale raw events. With these key elements, our method ranks first among unsupervised methods on the MVSEC benchmark, and is competitive on the DSEC benchmark. Moreover, our method allows us to expose the issues of the ground truth flow in those benchmarks, and produces remarkable results when it is transferred to unsupervised learning settings. Our code is available at https://github.com/tub-rip/event_based_optical_flow



### Tailoring Self-Supervision for Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.10023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10023v1)
- **Published**: 2022-07-20 16:41:14+00:00
- **Updated**: 2022-07-20 16:41:14+00:00
- **Authors**: WonJun Moon, Ji-Hwan Kim, Jae-Pil Heo
- **Comment**: Accepted to ECCV 2022. Code is available at
  github.com/wjun0830/Localizable-Rotation
- **Journal**: None
- **Summary**: Recently, it is shown that deploying a proper self-supervision is a prospective way to enhance the performance of supervised learning. Yet, the benefits of self-supervision are not fully exploited as previous pretext tasks are specialized for unsupervised representation learning. To this end, we begin by presenting three desirable properties for such auxiliary tasks to assist the supervised objective. First, the tasks need to guide the model to learn rich features. Second, the transformations involved in the self-supervision should not significantly alter the training distribution. Third, the tasks are preferred to be light and generic for high applicability to prior arts. Subsequently, to show how existing pretext tasks can fulfill these and be tailored for supervised learning, we propose a simple auxiliary self-supervision task, predicting localizable rotation (LoRot). Our exhaustive experiments validate the merits of LoRot as a pretext task tailored for supervised learning in terms of robustness and generalization capability. Our code is available at https://github.com/wjun0830/Localizable-Rotation.



### Difficulty-Aware Simulator for Open Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.10024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10024v1)
- **Published**: 2022-07-20 16:41:36+00:00
- **Updated**: 2022-07-20 16:41:36+00:00
- **Authors**: WonJun Moon, Junho Park, Hyun Seok Seong, Cheol-Ho Cho, Jae-Pil Heo
- **Comment**: Accepted to ECCV 2022. Code is available at
  github.com/wjun0830/Difficulty-Aware-Simulator
- **Journal**: None
- **Summary**: Open set recognition (OSR) assumes unknown instances appear out of the blue at the inference time. The main challenge of OSR is that the response of models for unknowns is totally unpredictable. Furthermore, the diversity of open set makes it harder since instances have different difficulty levels. Therefore, we present a novel framework, DIfficulty-Aware Simulator (DIAS), that generates fakes with diverse difficulty levels to simulate the real world. We first investigate fakes from generative adversarial network (GAN) in the classifier's viewpoint and observe that these are not severely challenging. This leads us to define the criteria for difficulty by regarding samples generated with GANs having moderate-difficulty. To produce hard-difficulty examples, we introduce Copycat, imitating the behavior of the classifier. Furthermore, moderate- and easy-difficulty samples are also yielded by our modified GAN and Copycat, respectively. As a result, DIAS outperforms state-of-the-art methods with both metrics of AUROC and F-score. Our code is available at https://github.com/wjun0830/Difficulty-Aware-Simulator.



### Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks
- **Arxiv ID**: http://arxiv.org/abs/2207.10025v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10025v2)
- **Published**: 2022-07-20 16:41:37+00:00
- **Updated**: 2022-07-21 07:59:17+00:00
- **Authors**: Jae-Yeop Jeong, Yeong-Gi Hong, JiYeon Oh, Sumin Hong, Jin-Woo Jeong, Yuchul Jung
- **Comment**: Page 3, Added reference [2], [33]
- **Journal**: None
- **Summary**: Facial expression in-the-wild is essential for various interactive computing domains. Especially, "Learning from Synthetic Data" (LSD) is an important topic in the facial expression recognition task. In this paper, we propose a multi-task learning-based facial expression recognition approach which consists of emotion and appearance learning branches that can share all face information, and present preliminary results for the LSD challenge introduced in the 4th affective behavior analysis in-the-wild (ABAW) competition. Our method achieved the mean F1 score of 0.71.



### Locality Guidance for Improving Vision Transformers on Tiny Datasets
- **Arxiv ID**: http://arxiv.org/abs/2207.10026v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10026v1)
- **Published**: 2022-07-20 16:41:41+00:00
- **Updated**: 2022-07-20 16:41:41+00:00
- **Authors**: Kehan Li, Runyi Yu, Zhennan Wang, Li Yuan, Guoli Song, Jie Chen
- **Comment**: None
- **Journal**: None
- **Summary**: While the Vision Transformer (VT) architecture is becoming trendy in computer vision, pure VT models perform poorly on tiny datasets. To address this issue, this paper proposes the locality guidance for improving the performance of VTs on tiny datasets. We first analyze that the local information, which is of great importance for understanding images, is hard to be learned with limited data due to the high flexibility and intrinsic globality of the self-attention mechanism in VTs. To facilitate local information, we realize the locality guidance for VTs by imitating the features of an already trained convolutional neural network (CNN), inspired by the built-in local-to-global hierarchy of CNN. Under our dual-task learning paradigm, the locality guidance provided by a lightweight CNN trained on low-resolution images is adequate to accelerate the convergence and improve the performance of VTs to a large extent. Therefore, our locality guidance approach is very simple and efficient, and can serve as a basic performance enhancement method for VTs on tiny datasets. Extensive experiments demonstrate that our method can significantly improve VTs when training from scratch on tiny datasets and is compatible with different kinds of VTs and datasets. For example, our proposed method can boost the performance of various VTs on tiny datasets (e.g., 13.07% for DeiT, 8.98% for T2T and 7.85% for PVT), and enhance even stronger baseline PVTv2 by 1.86% to 79.30%, showing the potential of VTs on tiny datasets. The code is available at https://github.com/lkhl/tiny-transformers.



### MOTCOM: The Multi-Object Tracking Dataset Complexity Metric
- **Arxiv ID**: http://arxiv.org/abs/2207.10031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10031v1)
- **Published**: 2022-07-20 16:46:02+00:00
- **Updated**: 2022-07-20 16:46:02+00:00
- **Authors**: Malte Pedersen, Joakim Bruslund Haurum, Patrick Dendorfer, Thomas B. Moeslund
- **Comment**: ECCV 2022. Project webpage https://vap.aau.dk/motcom
- **Journal**: None
- **Summary**: There exists no comprehensive metric for describing the complexity of Multi-Object Tracking (MOT) sequences. This lack of metrics decreases explainability, complicates comparison of datasets, and reduces the conversation on tracker performance to a matter of leader board position. As a remedy, we present the novel MOT dataset complexity metric (MOTCOM), which is a combination of three sub-metrics inspired by key problems in MOT: occlusion, erratic motion, and visual similarity. The insights of MOTCOM can open nuanced discussions on tracker performance and may lead to a wider acknowledgement of novel contributions developed for either less known datasets or those aimed at solving sub-problems. We evaluate MOTCOM on the comprehensive MOT17, MOT20, and MOTSynth datasets and show that MOTCOM is far better at describing the complexity of MOT sequences compared to the conventional density and number of tracks. Project page at https://vap.aau.dk/motcom



### Action Quality Assessment using Transformers
- **Arxiv ID**: http://arxiv.org/abs/2207.12318v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.12318v1)
- **Published**: 2022-07-20 17:00:13+00:00
- **Updated**: 2022-07-20 17:00:13+00:00
- **Authors**: Abhay Iyer, Mohammad Alali, Hemanth Bodala, Sunit Vaidya
- **Comment**: 9 pages, 2 figures
- **Journal**: None
- **Summary**: Action quality assessment (AQA) is an active research problem in video-based applications that is a challenging task due to the score variance per frame. Existing methods address this problem via convolutional-based approaches but suffer from its limitation of effectively capturing long-range dependencies. With the recent advancements in Transformers, we show that they are a suitable alternative to the conventional convolutional-based architectures. Specifically, can transformer-based models solve the task of AQA by effectively capturing long-range dependencies, parallelizing computation, and providing a wider receptive field for diving videos? To demonstrate the effectiveness of our proposed architectures, we conducted comprehensive experiments and achieved a competitive Spearman correlation score of 0.9317. Additionally, we explore the hyperparameters effect on the model's performance and pave a new path for exploiting Transformers in AQA.



### Fully Sparse 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.10035v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2207.10035v2)
- **Published**: 2022-07-20 17:01:33+00:00
- **Updated**: 2022-10-03 15:31:24+00:00
- **Authors**: Lue Fan, Feng Wang, Naiyan Wang, Zhaoxiang Zhang
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: As the perception range of LiDAR increases, LiDAR-based 3D object detection becomes a dominant task in the long-range perception task of autonomous driving. The mainstream 3D object detectors usually build dense feature maps in the network backbone and prediction head. However, the computational and spatial costs on the dense feature map are quadratic to the perception range, which makes them hardly scale up to the long-range setting. To enable efficient long-range LiDAR-based object detection, we build a fully sparse 3D object detector (FSD). The computational and spatial cost of FSD is roughly linear to the number of points and independent of the perception range. FSD is built upon the general sparse voxel encoder and a novel sparse instance recognition (SIR) module. SIR first groups the points into instances and then applies instance-wise feature extraction and prediction. In this way, SIR resolves the issue of center feature missing, which hinders the design of the fully sparse architecture for all center-based or anchor-based detectors. Moreover, SIR avoids the time-consuming neighbor queries in previous point-based methods by grouping points into instances. We conduct extensive experiments on the large-scale Waymo Open Dataset to reveal the working mechanism of FSD, and state-of-the-art performance is reported. To demonstrate the superiority of FSD in long-range detection, we also conduct experiments on Argoverse 2 Dataset, which has a much larger perception range ($200m$) than Waymo Open Dataset ($75m$). On such a large perception range, FSD achieves state-of-the-art performance and is 2.4$\times$ faster than the dense counterpart. Codes will be released at https://github.com/TuSimple/SST.



### Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A New Physics-Inspired Transformer Model
- **Arxiv ID**: http://arxiv.org/abs/2207.10040v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.10040v2)
- **Published**: 2022-07-20 17:09:16+00:00
- **Updated**: 2022-07-24 22:54:09+00:00
- **Authors**: Zhiyuan Mao, Ajay Jaiswal, Zhangyang Wang, Stanley H. Chan
- **Comment**: This paper is accepted as a poster at ECCV 2022
- **Journal**: None
- **Summary**: Image restoration algorithms for atmospheric turbulence are known to be much more challenging to design than traditional ones such as blur or noise because the distortion caused by the turbulence is an entanglement of spatially varying blur, geometric distortion, and sensor noise. Existing CNN-based restoration methods built upon convolutional kernels with static weights are insufficient to handle the spatially dynamical atmospheric turbulence effect. To address this problem, in this paper, we propose a physics-inspired transformer model for imaging through atmospheric turbulence. The proposed network utilizes the power of transformer blocks to jointly extract a dynamical turbulence distortion map and restore a turbulence-free image. In addition, recognizing the lack of a comprehensive dataset, we collect and present two new real-world turbulence datasets that allow for evaluation with both classical objective metrics (e.g., PSNR and SSIM) and a new task-driven metric using text recognition accuracy. Both real testing sets and all related code will be made publicly available.



### Densely Constrained Depth Estimator for Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2207.10047v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10047v3)
- **Published**: 2022-07-20 17:24:22+00:00
- **Updated**: 2022-09-27 11:01:16+00:00
- **Authors**: Yingyan Li, Yuntao Chen, Jiawei He, Zhaoxiang Zhang
- **Comment**: Accepted by ECCV2022
- **Journal**: None
- **Summary**: Estimating accurate 3D locations of objects from monocular images is a challenging problem because of lacking depth. Previous work shows that utilizing the object's keypoint projection constraints to estimate multiple depth candidates boosts the detection performance. However, the existing methods can only utilize vertical edges as projection constraints for depth estimation. So these methods only use a small number of projection constraints and produce insufficient depth candidates, leading to inaccurate depth estimation. In this paper, we propose a method that utilizes dense projection constraints from edges of any direction. In this way, we employ much more projection constraints and produce considerable depth candidates. Besides, we present a graph matching weighting module to merge the depth candidates. The proposed method DCD (Densely Constrained Detector) achieves state-of-the-art performance on the KITTI and WOD benchmarks. Code is released at https://github.com/BraveGroup/DCD.



### Pretraining a Neural Network before Knowing Its Architecture
- **Arxiv ID**: http://arxiv.org/abs/2207.10049v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.10049v1)
- **Published**: 2022-07-20 17:27:50+00:00
- **Updated**: 2022-07-20 17:27:50+00:00
- **Authors**: Boris Knyazev
- **Comment**: Accepted at ICML 2022 Workshop on Pre-training: Perspectives,
  Pitfalls, and Paths Forward, source code is available at
  https://github.com/facebookresearch/ppuda
- **Journal**: None
- **Summary**: Training large neural networks is possible by training a smaller hypernetwork that predicts parameters for the large ones. A recently released Graph HyperNetwork (GHN) trained this way on one million smaller ImageNet architectures is able to predict parameters for large unseen networks such as ResNet-50. While networks with predicted parameters lose performance on the source task, the predicted parameters have been found useful for fine-tuning on other tasks. We study if fine-tuning based on the same GHN is still useful on novel strong architectures that were published after the GHN had been trained. We found that for recent architectures such as ConvNeXt, GHN initialization becomes less useful than for ResNet-50. One potential reason is the increased distribution shift of novel architectures from those used to train the GHN. We also found that the predicted parameters lack the diversity necessary to successfully fine-tune parameters with gradient descent. We alleviate this limitation by applying simple post-processing techniques to predicted parameters before fine-tuning them on a target task and improve fine-tuning of ResNet-50 and ConvNeXt.



### 3D Clothed Human Reconstruction in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2207.10053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10053v1)
- **Published**: 2022-07-20 17:33:19+00:00
- **Updated**: 2022-07-20 17:33:19+00:00
- **Authors**: Gyeongsik Moon, Hyeongjin Nam, Takaaki Shiratori, Kyoung Mu Lee
- **Comment**: Accepted to ECCV 2022, 25 pages including the supplementary material
- **Journal**: None
- **Summary**: Although much progress has been made in 3D clothed human reconstruction, most of the existing methods fail to produce robust results from in-the-wild images, which contain diverse human poses and appearances. This is mainly due to the large domain gap between training datasets and in-the-wild datasets. The training datasets are usually synthetic ones, which contain rendered images from GT 3D scans. However, such datasets contain simple human poses and less natural image appearances compared to those of real in-the-wild datasets, which makes generalization of it to in-the-wild images extremely challenging. To resolve this issue, in this work, we propose ClothWild, a 3D clothed human reconstruction framework that firstly addresses the robustness on in-thewild images. First, for the robustness to the domain gap, we propose a weakly supervised pipeline that is trainable with 2D supervision targets of in-the-wild datasets. Second, we design a DensePose-based loss function to reduce ambiguities of the weak supervision. Extensive empirical tests on several public in-the-wild datasets demonstrate that our proposed ClothWild produces much more accurate and robust results than the state-of-the-art methods. The codes are available in here: https://github.com/hygenie1228/ClothWild_RELEASE.



### Monocular 3D Object Reconstruction with GAN Inversion
- **Arxiv ID**: http://arxiv.org/abs/2207.10061v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10061v1)
- **Published**: 2022-07-20 17:47:22+00:00
- **Updated**: 2022-07-20 17:47:22+00:00
- **Authors**: Junzhe Zhang, Daxuan Ren, Zhongang Cai, Chai Kiat Yeo, Bo Dai, Chen Change Loy
- **Comment**: ECCV 2022. Project page:
  https://www.mmlab-ntu.com/project/meshinversion/
- **Journal**: None
- **Summary**: Recovering a textured 3D mesh from a monocular image is highly challenging, particularly for in-the-wild objects that lack 3D ground truths. In this work, we present MeshInversion, a novel framework to improve the reconstruction by exploiting the generative prior of a 3D GAN pre-trained for 3D textured mesh synthesis. Reconstruction is achieved by searching for a latent space in the 3D GAN that best resembles the target mesh in accordance with the single view observation. Since the pre-trained GAN encapsulates rich 3D semantics in terms of mesh geometry and texture, searching within the GAN manifold thus naturally regularizes the realness and fidelity of the reconstruction. Importantly, such regularization is directly applied in the 3D space, providing crucial guidance of mesh parts that are unobserved in the 2D space. Experiments on standard benchmarks show that our framework obtains faithful 3D reconstructions with consistent geometry and texture across both observed and unobserved parts. Moreover, it generalizes well to meshes that are less commonly seen, such as the extended articulation of deformable objects. Code is released at https://github.com/junzhezhang/mesh-inversion



### Semantic uncertainty intervals for disentangled latent spaces
- **Arxiv ID**: http://arxiv.org/abs/2207.10074v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2207.10074v2)
- **Published**: 2022-07-20 17:58:10+00:00
- **Updated**: 2022-11-30 05:13:21+00:00
- **Authors**: Swami Sankaranarayanan, Anastasios N. Angelopoulos, Stephen Bates, Yaniv Romano, Phillip Isola
- **Comment**: Accepted to NeurIPS 2022. Project page:
  https://swamiviv.github.io/semantic_uncertainty_intervals/
- **Journal**: None
- **Summary**: Meaningful uncertainty quantification in computer vision requires reasoning about semantic information -- say, the hair color of the person in a photo or the location of a car on the street. To this end, recent breakthroughs in generative modeling allow us to represent semantic information in disentangled latent spaces, but providing uncertainties on the semantic latent variables has remained challenging. In this work, we provide principled uncertainty intervals that are guaranteed to contain the true semantic factors for any underlying generative model. The method does the following: (1) it uses quantile regression to output a heuristic uncertainty interval for each element in the latent space (2) calibrates these uncertainties such that they contain the true value of the latent for a new, unseen input. The endpoints of these calibrated intervals can then be propagated through the generator to produce interpretable uncertainty visualizations for each semantic factor. This technique reliably communicates semantically meaningful, principled, and instance-adaptive uncertainty in inverse problems like image super-resolution and image completion.



### Is an Object-Centric Video Representation Beneficial for Transfer?
- **Arxiv ID**: http://arxiv.org/abs/2207.10075v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10075v2)
- **Published**: 2022-07-20 17:59:44+00:00
- **Updated**: 2022-10-08 18:19:42+00:00
- **Authors**: Chuhan Zhang, Ankush Gupta, Andrew Zisserman
- **Comment**: Accepted to ACCV 2022
- **Journal**: None
- **Summary**: The objective of this work is to learn an object-centric video representation, with the aim of improving transferability to novel tasks, i.e., tasks different from the pre-training task of action classification. To this end, we introduce a new object-centric video recognition model based on a transformer architecture. The model learns a set of object-centric summary vectors for the video, and uses these vectors to fuse the visual and spatio-temporal trajectory 'modalities' of the video clip. We also introduce a novel trajectory contrast loss to further enhance objectness in these summary vectors. With experiments on four datasets -- SomethingSomething-V2, SomethingElse, Action Genome and EpicKitchens -- we show that the object-centric model outperforms prior video representations (both object-agnostic and object-aware), when: (1) classifying actions on unseen objects and unseen environments; (2) low-shot learning of novel classes; (3) linear probe to other downstream tasks; as well as (4) for standard action classification.



### Discover and Mitigate Unknown Biases with Debiasing Alternate Networks
- **Arxiv ID**: http://arxiv.org/abs/2207.10077v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.10077v2)
- **Published**: 2022-07-20 17:59:51+00:00
- **Updated**: 2022-09-07 21:09:11+00:00
- **Authors**: Zhiheng Li, Anthony Hoogs, Chenliang Xu
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Deep image classifiers have been found to learn biases from datasets. To mitigate the biases, most previous methods require labels of protected attributes (e.g., age, skin tone) as full-supervision, which has two limitations: 1) it is infeasible when the labels are unavailable; 2) they are incapable of mitigating unknown biases -- biases that humans do not preconceive. To resolve those problems, we propose Debiasing Alternate Networks (DebiAN), which comprises two networks -- a Discoverer and a Classifier. By training in an alternate manner, the discoverer tries to find multiple unknown biases of the classifier without any annotations of biases, and the classifier aims at unlearning the biases identified by the discoverer. While previous works evaluate debiasing results in terms of a single bias, we create Multi-Color MNIST dataset to better benchmark mitigation of multiple biases in a multi-bias setting, which not only reveals the problems in previous methods but also demonstrates the advantage of DebiAN in identifying and mitigating multiple biases simultaneously. We further conduct extensive experiments on real-world datasets, showing that the discoverer in DebiAN can identify unknown biases that may be hard to be found by humans. Regarding debiasing, DebiAN achieves strong bias mitigation performance.



### World Robot Challenge 2020 -- Partner Robot: A Data-Driven Approach for Room Tidying with Mobile Manipulator
- **Arxiv ID**: http://arxiv.org/abs/2207.10106v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2207.10106v2)
- **Published**: 2022-07-20 18:00:20+00:00
- **Updated**: 2022-07-22 01:44:49+00:00
- **Authors**: Tatsuya Matsushima, Yuki Noguchi, Jumpei Arima, Toshiki Aoki, Yuki Okita, Yuya Ikeda, Koki Ishimoto, Shohei Taniguchi, Yuki Yamashita, Shoichi Seto, Shixiang Shane Gu, Yusuke Iwasawa, Yutaka Matsuo
- **Comment**: None
- **Journal**: None
- **Summary**: Tidying up a household environment using a mobile manipulator poses various challenges in robotics, such as adaptation to large real-world environmental variations, and safe and robust deployment in the presence of humans.The Partner Robot Challenge in World Robot Challenge (WRC) 2020, a global competition held in September 2021, benchmarked tidying tasks in the real home environments, and importantly, tested for full system performances.For this challenge, we developed an entire household service robot system, which leverages a data-driven approach to adapt to numerous edge cases that occur during the execution, instead of classical manual pre-programmed solutions. In this paper, we describe the core ingredients of the proposed robot system, including visual recognition, object manipulation, and motion planning. Our robot system won the second prize, verifying the effectiveness and potential of data-driven robot systems for mobile manipulation in home environments.



### BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2207.10120v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10120v2)
- **Published**: 2022-07-20 18:03:54+00:00
- **Updated**: 2022-07-22 13:02:35+00:00
- **Authors**: Davide Moltisanti, Jinyi Wu, Bo Dai, Chen Change Loy
- **Comment**: ECCV 2022. Dataset available at https://github.com/dmoltisanti/brace
- **Journal**: None
- **Summary**: Generative models for audio-conditioned dance motion synthesis map music features to dance movements. Models are trained to associate motion patterns to audio patterns, usually without an explicit knowledge of the human body. This approach relies on a few assumptions: strong music-dance correlation, controlled motion data and relatively simple poses and movements. These characteristics are found in all existing datasets for dance motion synthesis, and indeed recent methods can achieve good results.We introduce a new dataset aiming to challenge these common assumptions, compiling a set of dynamic dance sequences displaying complex human poses. We focus on breakdancing which features acrobatic moves and tangled postures. We source our data from the Red Bull BC One competition videos. Estimating human keypoints from these videos is difficult due to the complexity of the dance, as well as the multiple moving cameras recording setup. We adopt a hybrid labelling pipeline leveraging deep estimation models as well as manual annotations to obtain good quality keypoint sequences at a reduced cost. Our efforts produced the BRACE dataset, which contains over 3 hours and 30 minutes of densely annotated poses. We test state-of-the-art methods on BRACE, showing their limitations when evaluated on complex sequences. Our dataset can readily foster advance in dance motion synthesis. With intricate poses and swift movements, models are forced to go beyond learning a mapping between modalities and reason more effectively about body structure and movements.



### Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance
- **Arxiv ID**: http://arxiv.org/abs/2207.10123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10123v1)
- **Published**: 2022-07-20 18:05:53+00:00
- **Updated**: 2022-07-20 18:05:53+00:00
- **Authors**: Zhihang Zhong, Xiao Sun, Zhirong Wu, Yinqiang Zheng, Stephen Lin, Imari Sato
- **Comment**: ECCV2022
- **Journal**: None
- **Summary**: We study the challenging problem of recovering detailed motion from a single motion-blurred image. Existing solutions to this problem estimate a single image sequence without considering the motion ambiguity for each region. Therefore, the results tend to converge to the mean of the multi-modal possibilities. In this paper, we explicitly account for such motion ambiguity, allowing us to generate multiple plausible solutions all in sharp detail. The key idea is to introduce a motion guidance representation, which is a compact quantization of 2D optical flow with only four discrete motion directions. Conditioned on the motion guidance, the blur decomposition is led to a specific, unambiguous solution by using a novel two-stage decomposition network. We propose a unified framework for blur decomposition, which supports various interfaces for generating our motion guidance, including human input, motion information from adjacent video frames, and learning from a video dataset. Extensive experiments on synthesized datasets and real-world data show that the proposed framework is qualitatively and quantitatively superior to previous methods, and also offers the merit of producing physically plausible and diverse solutions. Code is available at https://github.com/zzh-tech/Animation-from-Blur.



### Latent Discriminant deterministic Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2207.10130v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.10130v1)
- **Published**: 2022-07-20 18:18:40+00:00
- **Updated**: 2022-07-20 18:18:40+00:00
- **Authors**: Gianni Franchi, Xuanlong Yu, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, David Filliat
- **Comment**: 24 pages. Accepted at ECCV 2022
- **Journal**: None
- **Summary**: Predictive uncertainty estimation is essential for deploying Deep Neural Networks in real-world autonomous systems. However, most successful approaches are computationally intensive. In this work, we attempt to address these challenges in the context of autonomous driving perception tasks. Recently proposed Deterministic Uncertainty Methods (DUM) can only partially meet such requirements as their scalability to complex computer vision tasks is not obvious. In this work we advance a scalable and effective DUM for high-resolution semantic segmentation, that relaxes the Lipschitz constraint typically hindering practicality of such architectures. We learn a discriminant latent space by leveraging a distinction maximization layer over an arbitrarily-sized set of trainable prototypes. Our approach achieves competitive results over Deep Ensembles, the state-of-the-art for uncertainty prediction, on image classification, segmentation and monocular depth estimation tasks. Our code is available at https://github.com/ENSTA-U2IS/LDU



### Continual Variational Autoencoder Learning via Online Cooperative Memorization
- **Arxiv ID**: http://arxiv.org/abs/2207.10131v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.10131v1)
- **Published**: 2022-07-20 18:19:27+00:00
- **Updated**: 2022-07-20 18:19:27+00:00
- **Authors**: Fei Ye, Adrian G. Bors
- **Comment**: Accepted by European Conference on Computer Vision 2022 (ECCV 2022)
- **Journal**: None
- **Summary**: Due to their inference, data representation and reconstruction properties, Variational Autoencoders (VAE) have been successfully used in continual learning classification tasks. However, their ability to generate images with specifications corresponding to the classes and databases learned during Continual Learning (CL) is not well understood and catastrophic forgetting remains a significant challenge. In this paper, we firstly analyze the forgetting behaviour of VAEs by developing a new theoretical framework that formulates CL as a dynamic optimal transport problem. This framework proves approximate bounds to the data likelihood without requiring the task information and explains how the prior knowledge is lost during the training process. We then propose a novel memory buffering approach, namely the Online Cooperative Memorization (OCM) framework, which consists of a Short-Term Memory (STM) that continually stores recent samples to provide future information for the model, and a Long-Term Memory (LTM) aiming to preserve a wide diversity of samples. The proposed OCM transfers certain samples from STM to LTM according to the information diversity selection criterion without requiring any supervised signals. The OCM framework is then combined with a dynamic VAE expansion mixture network for further enhancing its performance.



### A Generalized & Robust Framework For Timestamp Supervision in Temporal Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.10137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10137v1)
- **Published**: 2022-07-20 18:30:48+00:00
- **Updated**: 2022-07-20 18:30:48+00:00
- **Authors**: Rahul Rahaman, Dipika Singhania, Alexandre Thiery, Angela Yao
- **Comment**: None
- **Journal**: None
- **Summary**: In temporal action segmentation, Timestamp supervision requires only a handful of labelled frames per video sequence. For unlabelled frames, previous works rely on assigning hard labels, and performance rapidly collapses under subtle violations of the annotation assumptions. We propose a novel Expectation-Maximization (EM) based approach that leverages the label uncertainty of unlabelled frames and is robust enough to accommodate possible annotation errors. With accurate timestamp annotations, our proposed method produces SOTA results and even exceeds the fully-supervised setup in several metrics and datasets. When applied to timestamp annotations with missing action segments, our method presents stable performance. To further test our formulation's robustness, we introduce the new challenging annotation setup of Skip-tag supervision. This setup relaxes constraints and requires annotations of any fixed number of random frames in a video, making it more flexible than Timestamp supervision while remaining competitive.



### Learning to identify cracks on wind turbine blade surfaces using drone-based inspection images
- **Arxiv ID**: http://arxiv.org/abs/2207.11186v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.11186v1)
- **Published**: 2022-07-20 18:37:25+00:00
- **Updated**: 2022-07-20 18:37:25+00:00
- **Authors**: Akshay Iyer, Linh Nguyen, Shweta Khushu
- **Comment**: NeurIPS 2021 Workshop on Tackling Climate Change with Machine
  Learning
- **Journal**: None
- **Summary**: Wind energy is expected to be one of the leading ways to achieve the goals of the Paris Agreement but it in turn heavily depends on effective management of its operations and maintenance (O&M) costs. Blade failures account for one-third of all O&M costs thus making accurate detection of blade damages, especially cracks, very important for sustained operations and cost savings. Traditionally, damage inspection has been a completely manual process thus making it subjective, error-prone, and time-consuming. Hence in this work, we bring more objectivity, scalability, and repeatability in our damage inspection process, using deep learning, to miss fewer cracks. We build a deep learning model trained on a large dataset of blade damages, collected by our drone-based inspection, to correctly detect cracks. Our model is already in production and has processed more than a million damages with a recall of 0.96. We also focus on model interpretability using class activation maps to get a peek into the model workings. The model not only performs as good as human experts but also better in certain tricky cases. Thus, in this work, we aim to increase wind energy adoption by decreasing one of its major hurdles - the O\&M costs resulting from missing blade failures like cracks.



### AudioScopeV2: Audio-Visual Attention Architectures for Calibrated Open-Domain On-Screen Sound Separation
- **Arxiv ID**: http://arxiv.org/abs/2207.10141v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2207.10141v1)
- **Published**: 2022-07-20 18:44:01+00:00
- **Updated**: 2022-07-20 18:44:01+00:00
- **Authors**: Efthymios Tzinis, Scott Wisdom, Tal Remez, John R. Hershey
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: We introduce AudioScopeV2, a state-of-the-art universal audio-visual on-screen sound separation system which is capable of learning to separate sounds and associate them with on-screen objects by looking at in-the-wild videos. We identify several limitations of previous work on audio-visual on-screen sound separation, including the coarse resolution of spatio-temporal attention, poor convergence of the audio separation model, limited variety in training and evaluation data, and failure to account for the trade off between preservation of on-screen sounds and suppression of off-screen sounds. We provide solutions to all of these issues. Our proposed cross-modal and self-attention network architectures capture audio-visual dependencies at a finer resolution over time, and we also propose efficient separable variants that are capable of scaling to longer videos without sacrificing much performance. We also find that pre-training the separation model only on audio greatly improves results. For training and evaluation, we collected new human annotations of onscreen sounds from a large database of in-the-wild videos (YFCC100M). This new dataset is more diverse and challenging. Finally, we propose a calibration procedure that allows exact tuning of on-screen reconstruction versus off-screen suppression, which greatly simplifies comparing performance between models with different operating points. Overall, our experimental results show marked improvements in on-screen separation performance under much more general conditions than previous methods with minimal additional computational complexity.



### Tackling Long-Tailed Category Distribution Under Domain Shifts
- **Arxiv ID**: http://arxiv.org/abs/2207.10150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10150v1)
- **Published**: 2022-07-20 19:07:46+00:00
- **Updated**: 2022-07-20 19:07:46+00:00
- **Authors**: Xiao Gu, Yao Guo, Zeju Li, Jianing Qiu, Qi Dou, Yuxuan Liu, Benny Lo, Guang-Zhong Yang
- **Comment**: accepted to ECCV 2022
- **Journal**: None
- **Summary**: Machine learning models fail to perform well on real-world applications when 1) the category distribution P(Y) of the training dataset suffers from long-tailed distribution and 2) the test data is drawn from different conditional distributions P(X|Y). Existing approaches cannot handle the scenario where both issues exist, which however is common for real-world applications. In this study, we took a step forward and looked into the problem of long-tailed classification under domain shifts. We designed three novel core functional blocks including Distribution Calibrated Classification Loss, Visual-Semantic Mapping and Semantic-Similarity Guided Augmentation. Furthermore, we adopted a meta-learning framework which integrates these three blocks to improve domain generalization on unseen target domains. Two new datasets were proposed for this problem, named AWA2-LTS and ImageNet-LTS. We evaluated our method on the two datasets and extensive experimental results demonstrate that our proposed method can achieve superior performance over state-of-the-art long-tailed/domain generalization approaches and the combinations. Source codes and datasets can be found at our project page https://xiaogu.site/LTDS.



### Analysis of the Effect of Low-Overhead Lossy Image Compression on the Performance of Visual Crowd Counting for Smart City Applications
- **Arxiv ID**: http://arxiv.org/abs/2207.10155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10155v1)
- **Published**: 2022-07-20 19:20:03+00:00
- **Updated**: 2022-07-20 19:20:03+00:00
- **Authors**: Arian Bakhtiarnia, Błażej Leporowski, Lukas Esterle, Alexandros Iosifidis
- **Comment**: None
- **Journal**: None
- **Summary**: Images and video frames captured by cameras placed throughout smart cities are often transmitted over the network to a server to be processed by deep neural networks for various tasks. Transmission of raw images, i.e., without any form of compression, requires high bandwidth and can lead to congestion issues and delays in transmission. The use of lossy image compression techniques can reduce the quality of the images, leading to accuracy degradation. In this paper, we analyze the effect of applying low-overhead lossy image compression methods on the accuracy of visual crowd counting, and measure the trade-off between bandwidth reduction and the obtained accuracy.



### Structural Causal 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2207.10156v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.10156v1)
- **Published**: 2022-07-20 19:22:06+00:00
- **Updated**: 2022-07-20 19:22:06+00:00
- **Authors**: Weiyang Liu, Zhen Liu, Liam Paull, Adrian Weller, Bernhard Schölkopf
- **Comment**: ECCV 2022 (32 pages, 22 figures)
- **Journal**: None
- **Summary**: This paper considers the problem of unsupervised 3D object reconstruction from in-the-wild single-view images. Due to ambiguity and intrinsic ill-posedness, this problem is inherently difficult to solve and therefore requires strong regularization to achieve disentanglement of different latent factors. Unlike existing works that introduce explicit regularizations into objective functions, we look into a different space for implicit regularization -- the structure of latent space. Specifically, we restrict the structure of latent space to capture a topological causal ordering of latent factors (i.e., representing causal dependency as a directed acyclic graph). We first show that different causal orderings matter for 3D reconstruction, and then explore several approaches to find a task-dependent causal factor ordering. Our experiments demonstrate that the latent space structure indeed serves as an implicit regularization and introduces an inductive bias beneficial for reconstruction.



### Visual Knowledge Tracing
- **Arxiv ID**: http://arxiv.org/abs/2207.10157v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2207.10157v2)
- **Published**: 2022-07-20 19:24:57+00:00
- **Updated**: 2022-07-22 00:28:40+00:00
- **Authors**: Neehar Kondapaneni, Pietro Perona, Oisin Mac Aodha
- **Comment**: 14 pages, 4 figures, 14 supplemental pages, 11 supplemental figures,
  accepted to European Conference on Computer Vision (ECCV) 2022
- **Journal**: None
- **Summary**: Each year, thousands of people learn new visual categorization tasks -- radiologists learn to recognize tumors, birdwatchers learn to distinguish similar species, and crowd workers learn how to annotate valuable data for applications like autonomous driving. As humans learn, their brain updates the visual features it extracts and attend to, which ultimately informs their final classification decisions. In this work, we propose a novel task of tracing the evolving classification behavior of human learners as they engage in challenging visual classification tasks. We propose models that jointly extract the visual features used by learners as well as predicting the classification functions they utilize. We collect three challenging new datasets from real human learners in order to evaluate the performance of different visual knowledge tracing methods. Our results show that our recurrent models are able to predict the classification behavior of human learners on three challenging medical image and species identification tasks.



### GOCA: Guided Online Cluster Assignment for Self-Supervised Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.10158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10158v1)
- **Published**: 2022-07-20 19:26:55+00:00
- **Updated**: 2022-07-20 19:26:55+00:00
- **Authors**: Huseyin Coskun, Alireza Zareian, Joshua L. Moore, Federico Tombari, Chen Wang
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Clustering is a ubiquitous tool in unsupervised learning. Most of the existing self-supervised representation learning methods typically cluster samples based on visually dominant features. While this works well for image-based self-supervision, it often fails for videos, which require understanding motion rather than focusing on background. Using optical flow as complementary information to RGB can alleviate this problem. However, we observe that a naive combination of the two views does not provide meaningful gains. In this paper, we propose a principled way to combine two views. Specifically, we propose a novel clustering strategy where we use the initial cluster assignment of each view as prior to guide the final cluster assignment of the other view. This idea will enforce similar cluster structures for both views, and the formed clusters will be semantically abstract and robust to noisy inputs coming from each individual view. Additionally, we propose a novel regularization strategy to address the feature collapse problem, which is common in cluster-based self-supervised learning methods. Our extensive evaluation shows the effectiveness of our learned representations on downstream tasks, e.g., video retrieval and action recognition. Specifically, we outperform the state of the art by 7% on UCF and 4% on HMDB for video retrieval, and 5% on UCF and 6% on HMDB for video classification



### Liver Segmentation using Turbolift Learning for CT and Cone-beam C-arm Perfusion Imaging
- **Arxiv ID**: http://arxiv.org/abs/2207.10167v2
- **DOI**: 10.1016/j.compbiomed.2023.106539
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2207.10167v2)
- **Published**: 2022-07-20 19:38:50+00:00
- **Updated**: 2023-02-09 11:44:58+00:00
- **Authors**: Hana Haseljić, Soumick Chatterjee, Robert Frysch, Vojtěch Kulvait, Vladimir Semshchikov, Bennet Hensen, Frank Wacker, Inga Brüsch, Thomas Werncke, Oliver Speck, Andreas Nürnberger, Georg Rose
- **Comment**: None
- **Journal**: Computers in Biology and Medicine (2023) 106539
- **Summary**: Model-based reconstruction employing the time separation technique (TST) was found to improve dynamic perfusion imaging of the liver using C-arm cone-beam computed tomography (CBCT). To apply TST using prior knowledge extracted from CT perfusion data, the liver should be accurately segmented from the CT scans. Reconstructions of primary and model-based CBCT data need to be segmented for proper visualisation and interpretation of perfusion maps. This research proposes Turbolift learning, which trains a modified version of the multi-scale Attention UNet on different liver segmentation tasks serially, following the order of the trainings CT, CBCT, CBCT TST - making the previous trainings act as pre-training stages for the subsequent ones - addressing the problem of limited number of datasets for training. For the final task of liver segmentation from CBCT TST, the proposed method achieved an overall Dice scores of 0.874$\pm$0.031 and 0.905$\pm$0.007 in 6-fold and 4-fold cross-validation experiments, respectively - securing statistically significant improvements over the model, which was trained only for that task. Experiments revealed that Turbolift not only improves the overall performance of the model but also makes it robust against artefacts originating from the embolisation materials and truncation artefacts. Additionally, in-depth analyses confirmed the order of the segmentation tasks. This paper shows the potential of segmenting the liver from CT, CBCT, and CBCT TST, learning from the available limited training data, which can possibly be used in the future for the visualisation and evaluation of the perfusion maps for the treatment evaluation of liver diseases.



### Pediatric Bone Age Assessment using Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2207.10169v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.10169v1)
- **Published**: 2022-07-20 19:43:38+00:00
- **Updated**: 2022-07-20 19:43:38+00:00
- **Authors**: Aravinda Raman, Sameena Pathan, Tanweer Ali
- **Comment**: 18 pages, 28 figures, 1 table
- **Journal**: None
- **Summary**: Bone age assessment (BAA) is a standard method for determining the age difference between skeletal and chronological age. Manual processes are complicated and necessitate the expertise of experts. This is where deep learning comes into play. In this study, pre-trained models like VGG-16, InceptionV3, XceptionNet, and MobileNet are used to assess the bone age of the input data, and their mean average errors are compared and evaluated to see which model predicts the best.



### Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles
- **Arxiv ID**: http://arxiv.org/abs/2207.10172v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10172v2)
- **Published**: 2022-07-20 19:49:32+00:00
- **Updated**: 2022-07-22 03:28:41+00:00
- **Authors**: Guodong Wang, Yunhong Wang, Jie Qin, Dongming Zhang, Xiuguo Bao, Di Huang
- **Comment**: Accepted by ECCV'2022; Code is available at
  https://github.com/gdwang08/Jigsaw-VAD
- **Journal**: None
- **Summary**: Video Anomaly Detection (VAD) is an important topic in computer vision. Motivated by the recent advances in self-supervised learning, this paper addresses VAD by solving an intuitive yet challenging pretext task, i.e., spatio-temporal jigsaw puzzles, which is cast as a multi-label fine-grained classification problem. Our method exhibits several advantages over existing works: 1) the spatio-temporal jigsaw puzzles are decoupled in terms of spatial and temporal dimensions, responsible for capturing highly discriminative appearance and motion features, respectively; 2) full permutations are used to provide abundant jigsaw puzzles covering various difficulty levels, allowing the network to distinguish subtle spatio-temporal differences between normal and abnormal events; and 3) the pretext task is tackled in an end-to-end manner without relying on any pre-trained models. Our method outperforms state-of-the-art counterparts on three public benchmarks. Especially on ShanghaiTech Campus, the result is superior to reconstruction and prediction-based methods by a large margin.



### Scene Recognition with Objectness, Attribute and Category Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.10174v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.10174v1)
- **Published**: 2022-07-20 19:51:54+00:00
- **Updated**: 2022-07-20 19:51:54+00:00
- **Authors**: Ji Zhang, Jean-Paul Ainam, Li-hui Zhao, Wenai Song, Xin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Scene classification has established itself as a challenging research problem. Compared to images of individual objects, scene images could be much more semantically complex and abstract. Their difference mainly lies in the level of granularity of recognition. Yet, image recognition serves as a key pillar for the good performance of scene recognition as the knowledge attained from object images can be used for accurate recognition of scenes. The existing scene recognition methods only take the category label of the scene into consideration. However, we find that the contextual information that contains detailed local descriptions are also beneficial in allowing the scene recognition model to be more discriminative. In this paper, we aim to improve scene recognition using attribute and category label information encoded in objects. Based on the complementarity of attribute and category labels, we propose a Multi-task Attribute-Scene Recognition (MASR) network which learns a category embedding and at the same time predicts scene attributes. Attribute acquisition and object annotation are tedious and time consuming tasks. We tackle the problem by proposing a partially supervised annotation strategy in which human intervention is significantly reduced. The strategy provides a much more cost-effective solution to real world scenarios, and requires considerably less annotation efforts. Moreover, we re-weight the attribute predictions considering the level of importance indicated by the object detected scores. Using the proposed method, we efficiently annotate attribute labels for four large-scale datasets, and systematically investigate how scene and attribute recognition benefit from each other. The experimental results demonstrate that MASR learns a more discriminative representation and achieves competitive recognition performance compared to the state-of-the-art methods



### Controllable and Guided Face Synthesis for Unconstrained Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.10180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10180v1)
- **Published**: 2022-07-20 20:13:29+00:00
- **Updated**: 2022-07-20 20:13:29+00:00
- **Authors**: Feng Liu, Minchul Kim, Anil Jain, Xiaoming Liu
- **Comment**: to be published in ECCV 2022
- **Journal**: None
- **Summary**: Although significant advances have been made in face recognition (FR), FR in unconstrained environments remains challenging due to the domain gap between the semi-constrained training datasets and unconstrained testing scenarios. To address this problem, we propose a controllable face synthesis model (CFSM) that can mimic the distribution of target datasets in a style latent space. CFSM learns a linear subspace with orthogonal bases in the style latent space with precise control over the diversity and degree of synthesis. Furthermore, the pre-trained synthesis model can be guided by the FR model, making the resulting images more beneficial for FR model training. Besides, target dataset distributions are characterized by the learned orthogonal bases, which can be utilized to measure the distributional similarity among face datasets. Our approach yields significant performance gains on unconstrained benchmarks, such as IJB-B, IJB-C, TinyFace and IJB-S (+5.76% Rank1).



### Flow-based Visual Quality Enhancer for Super-resolution Magnetic Resonance Spectroscopic Imaging
- **Arxiv ID**: http://arxiv.org/abs/2207.10181v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.10181v1)
- **Published**: 2022-07-20 20:19:44+00:00
- **Updated**: 2022-07-20 20:19:44+00:00
- **Authors**: Siyuan Dong, Gilbert Hangel, Eric Z. Chen, Shanhui Sun, Wolfgang Bogner, Georg Widhalm, Chenyu You, John A. Onofrey, Robin de Graaf, James S. Duncan
- **Comment**: Accepted by DGM4MICCAI 2022
- **Journal**: None
- **Summary**: Magnetic Resonance Spectroscopic Imaging (MRSI) is an essential tool for quantifying metabolites in the body, but the low spatial resolution limits its clinical applications. Deep learning-based super-resolution methods provided promising results for improving the spatial resolution of MRSI, but the super-resolved images are often blurry compared to the experimentally-acquired high-resolution images. Attempts have been made with the generative adversarial networks to improve the image visual quality. In this work, we consider another type of generative model, the flow-based model, of which the training is more stable and interpretable compared to the adversarial networks. Specifically, we propose a flow-based enhancer network to improve the visual quality of super-resolution MRSI. Different from previous flow-based models, our enhancer network incorporates anatomical information from additional image modalities (MRI) and uses a learnable base distribution. In addition, we impose a guide loss and a data-consistency loss to encourage the network to generate images with high visual quality while maintaining high fidelity. Experiments on a 1H-MRSI dataset acquired from 25 high-grade glioma patients indicate that our enhancer network outperforms the adversarial networks and the baseline flow-based methods. Our method also allows visual quality adjustment and uncertainty estimation.



### Learning to estimate a surrogate respiratory signal from cardiac motion by signal-to-signal translation
- **Arxiv ID**: http://arxiv.org/abs/2208.01034v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2208.01034v1)
- **Published**: 2022-07-20 20:22:09+00:00
- **Updated**: 2022-07-20 20:22:09+00:00
- **Authors**: Akshay Iyer, Clifford Lindsay, Hendrik Pretorius, Michael King
- **Comment**: Medical Imaging Meets NeurIPS
- **Journal**: None
- **Summary**: In this work, we develop a neural network-based method to convert a noisy motion signal generated from segmenting rebinned list-mode cardiac SPECT images, to that of a high-quality surrogate signal, such as those seen from external motion tracking systems (EMTs). This synthetic surrogate will be used as input to our pre-existing motion correction technique developed for EMT surrogate signals. In our method, we test two families of neural networks to translate noisy internal motion to external surrogate: 1) fully connected networks and 2) convolutional neural networks. Our dataset consists of cardiac perfusion SPECT acquisitions for which cardiac motion was estimated (input: center-of-count-mass - COM signals) in conjunction with a respiratory surrogate motion signal acquired using a commercial Vicon Motion Tracking System (GT: EMT signals). We obtained an average R-score of 0.76 between the predicted surrogate and the EMT signal. Our goal is to lay a foundation to guide the optimization of neural networks for respiratory motion correction from SPECT without the need for an EMT.



### 2D GANs Meet Unsupervised Single-view 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2207.10183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10183v1)
- **Published**: 2022-07-20 20:24:07+00:00
- **Updated**: 2022-07-20 20:24:07+00:00
- **Authors**: Feng Liu, Xiaoming Liu
- **Comment**: to be published in ECCV 2022
- **Journal**: None
- **Summary**: Recent research has shown that controllable image generation based on pre-trained GANs can benefit a wide range of computer vision tasks. However, less attention has been devoted to 3D vision tasks. In light of this, we propose a novel image-conditioned neural implicit field, which can leverage 2D supervisions from GAN-generated multi-view images and perform the single-view reconstruction of generic objects. Firstly, a novel offline StyleGAN-based generator is presented to generate plausible pseudo images with full control over the viewpoint. Then, we propose to utilize a neural implicit function, along with a differentiable renderer to learn 3D geometry from pseudo images with object masks and rough pose initializations. To further detect the unreliable supervisions, we introduce a novel uncertainty module to predict uncertainty maps, which remedy the negative effect of uncertain regions in pseudo images, leading to a better reconstruction performance. The effectiveness of our approach is demonstrated through superior single-view 3D reconstruction results of generic objects.



### Bitwidth-Adaptive Quantization-Aware Neural Network Training: A Meta-Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/2207.10188v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.10188v1)
- **Published**: 2022-07-20 20:39:39+00:00
- **Updated**: 2022-07-20 20:39:39+00:00
- **Authors**: Jiseok Youn, Jaehun Song, Hyung-Sin Kim, Saewoong Bahk
- **Comment**: 14 pages (except references), 2 figures, to appear in ECCV 2022
- **Journal**: None
- **Summary**: Deep neural network quantization with adaptive bitwidths has gained increasing attention due to the ease of model deployment on various platforms with different resource budgets. In this paper, we propose a meta-learning approach to achieve this goal. Specifically, we propose MEBQAT, a simple yet effective way of bitwidth-adaptive quantization aware training (QAT) where meta-learning is effectively combined with QAT by redefining meta-learning tasks to incorporate bitwidths. After being deployed on a platform, MEBQAT allows the (meta-)trained model to be quantized to any candidate bitwidth then helps to conduct inference without much accuracy drop from quantization. Moreover, with a few-shot learning scenario, MEBQAT can also adapt a model to any bitwidth as well as any unseen target classes by adding conventional optimization or metric-based meta-learning. We design variants of MEBQAT to support both (1) a bitwidth-adaptive quantization scenario and (2) a new few-shot learning scenario where both quantization bitwidths and target classes are jointly adapted. We experimentally demonstrate their validity in multiple QAT schemes. By comparing their performance to (bitwidth-dedicated) QAT, existing bitwidth adaptive QAT and vanilla meta-learning, we find that merging bitwidths into meta-learning tasks achieves a higher level of robustness.



### Revisiting Hotels-50K and Hotel-ID
- **Arxiv ID**: http://arxiv.org/abs/2207.10200v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2207.10200v1)
- **Published**: 2022-07-20 21:38:19+00:00
- **Updated**: 2022-07-20 21:38:19+00:00
- **Authors**: Aarash Feizi, Arantxa Casanova, Adriana Romero-Soriano, Reihaneh Rabbany
- **Comment**: ICML 2022 DataPerf Workshop
- **Journal**: None
- **Summary**: In this paper, we propose revisited versions for two recent hotel recognition datasets: Hotels50K and Hotel-ID. The revisited versions provide evaluation setups with different levels of difficulty to better align with the intended real-world application, i.e. countering human trafficking. Real-world scenarios involve hotels and locations that are not captured in the current data sets, therefore it is important to consider evaluation settings where classes are truly unseen. We test this setup using multiple state-of-the-art image retrieval models and show that as expected, the models' performances decrease as the evaluation gets closer to the real-world unseen settings. The rankings of the best performing models also change across the different evaluation settings, which further motivates using the proposed revisited datasets.



### Hybrid CNN-Transformer Model For Facial Affect Recognition In the ABAW4 Challenge
- **Arxiv ID**: http://arxiv.org/abs/2207.10201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10201v1)
- **Published**: 2022-07-20 21:38:47+00:00
- **Updated**: 2022-07-20 21:38:47+00:00
- **Authors**: Lingfeng Wang, Haocheng Li, Chunyin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes our submission to the fourth Affective Behavior Analysis (ABAW) competition. We proposed a hybrid CNN-Transformer model for the Multi-Task-Learning (MTL) and Learning from Synthetic Data (LSD) task. Experimental results on validation dataset shows that our method achieves better performance than baseline model, which verifies that the effectiveness of proposed network.



### On the Robustness of 3D Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2207.10205v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.10205v1)
- **Published**: 2022-07-20 21:47:15+00:00
- **Updated**: 2022-07-20 21:47:15+00:00
- **Authors**: Fatima Albreiki, Sultan Abughazal, Jean Lahoud, Rao Anwer, Hisham Cholakkal, Fahad Khan
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, significant progress has been achieved for 3D object detection on point clouds thanks to the advances in 3D data collection and deep learning techniques. Nevertheless, 3D scenes exhibit a lot of variations and are prone to sensor inaccuracies as well as information loss during pre-processing. Thus, it is crucial to design techniques that are robust against these variations. This requires a detailed analysis and understanding of the effect of such variations. This work aims to analyze and benchmark popular point-based 3D object detectors against several data corruptions. To the best of our knowledge, we are the first to investigate the robustness of point-based 3D object detectors. To this end, we design and evaluate corruptions that involve data addition, reduction, and alteration. We further study the robustness of different modules against local and global variations. Our experimental results reveal several intriguing findings. For instance, we show that methods that integrate Transformers at a patch or object level lead to increased robustness, compared to using Transformers at the point level.



### Spotting Temporally Precise, Fine-Grained Events in Video
- **Arxiv ID**: http://arxiv.org/abs/2207.10213v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10213v1)
- **Published**: 2022-07-20 22:15:07+00:00
- **Updated**: 2022-07-20 22:15:07+00:00
- **Authors**: James Hong, Haotian Zhang, Michaël Gharbi, Matthew Fisher, Kayvon Fatahalian
- **Comment**: ECCV 2022; Website URL: https://jhong93.github.io/projects/spot.html
- **Journal**: None
- **Summary**: We introduce the task of spotting temporally precise, fine-grained events in video (detecting the precise moment in time events occur). Precise spotting requires models to reason globally about the full-time scale of actions and locally to identify subtle frame-to-frame appearance and motion differences that identify events during these actions. Surprisingly, we find that top performing solutions to prior video understanding tasks such as action detection and segmentation do not simultaneously meet both requirements. In response, we propose E2E-Spot, a compact, end-to-end model that performs well on the precise spotting task and can be trained quickly on a single GPU. We demonstrate that E2E-Spot significantly outperforms recent baselines adapted from the video action detection, segmentation, and spotting literature to the precise spotting task. Finally, we contribute new annotations and splits to several fine-grained sports action datasets to make these datasets suitable for future work on precise spotting.



### On Label Granularity and Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2207.10225v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.10225v1)
- **Published**: 2022-07-20 22:51:32+00:00
- **Updated**: 2022-07-20 22:51:32+00:00
- **Authors**: Elijah Cole, Kimberly Wilber, Grant Van Horn, Xuan Yang, Marco Fornoni, Pietro Perona, Serge Belongie, Andrew Howard, Oisin Mac Aodha
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Weakly supervised object localization (WSOL) aims to learn representations that encode object location using only image-level category labels. However, many objects can be labeled at different levels of granularity. Is it an animal, a bird, or a great horned owl? Which image-level labels should we use? In this paper we study the role of label granularity in WSOL. To facilitate this investigation we introduce iNatLoc500, a new large-scale fine-grained benchmark dataset for WSOL. Surprisingly, we find that choosing the right training label granularity provides a much larger performance boost than choosing the best WSOL algorithm. We also show that changing the label granularity can significantly improve data efficiency.



### MeshMAE: Masked Autoencoders for 3D Mesh Data Analysis
- **Arxiv ID**: http://arxiv.org/abs/2207.10228v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.10228v1)
- **Published**: 2022-07-20 23:33:22+00:00
- **Updated**: 2022-07-20 23:33:22+00:00
- **Authors**: Yaqian Liang, Shanshan Zhao, Baosheng Yu, Jing Zhang, Fazhi He
- **Comment**: accepted by ECCV
- **Journal**: None
- **Summary**: Recently, self-supervised pre-training has advanced Vision Transformers on various tasks w.r.t. different data modalities, e.g., image and 3D point cloud data. In this paper, we explore this learning paradigm for 3D mesh data analysis based on Transformers. Since applying Transformer architectures to new modalities is usually non-trivial, we first adapt Vision Transformer to 3D mesh data processing, i.e., Mesh Transformer. In specific, we divide a mesh into several non-overlapping local patches with each containing the same number of faces and use the 3D position of each patch's center point to form positional embeddings. Inspired by MAE, we explore how pre-training on 3D mesh data with the Transformer-based structure benefits downstream 3D mesh analysis tasks. We first randomly mask some patches of the mesh and feed the corrupted mesh into Mesh Transformers. Then, through reconstructing the information of masked patches, the network is capable of learning discriminative representations for mesh data. Therefore, we name our method MeshMAE, which can yield state-of-the-art or comparable performance on mesh analysis tasks, i.e., classification and segmentation. In addition, we also conduct comprehensive ablation studies to show the effectiveness of key designs in our method.



