# Arxiv Papers in cs.CV on 2022-07-30
### Neural Correspondence Field for Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.00113v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2208.00113v1)
- **Published**: 2022-07-30 01:48:23+00:00
- **Updated**: 2022-07-30 01:48:23+00:00
- **Authors**: Lin Huang, Tomas Hodan, Lingni Ma, Linguang Zhang, Luan Tran, Christopher Twigg, Po-Chen Wu, Junsong Yuan, Cem Keskin, Robert Wang
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: We propose a method for estimating the 6DoF pose of a rigid object with an available 3D model from a single RGB image. Unlike classical correspondence-based methods which predict 3D object coordinates at pixels of the input image, the proposed method predicts 3D object coordinates at 3D query points sampled in the camera frustum. The move from pixels to 3D points, which is inspired by recent PIFu-style methods for 3D reconstruction, enables reasoning about the whole object, including its (self-)occluded parts. For a 3D query point associated with a pixel-aligned image feature, we train a fully-connected neural network to predict: (i) the corresponding 3D object coordinates, and (ii) the signed distance to the object surface, with the first defined only for query points in the surface vicinity. We call the mapping realized by this network as Neural Correspondence Field. The object pose is then robustly estimated from the predicted 3D-3D correspondences by the Kabsch-RANSAC algorithm. The proposed method achieves state-of-the-art results on three BOP datasets and is shown superior especially in challenging cases with occlusion. The project website is at: linhuang17.github.io/NCF.



### Adaptive Feature Fusion for Cooperative Perception using LiDAR Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2208.00116v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.00116v3)
- **Published**: 2022-07-30 01:53:05+00:00
- **Updated**: 2023-01-12 22:20:46+00:00
- **Authors**: Donghao Qiao, Farhana Zulkernine
- **Comment**: Accepted by WACV2023
- **Journal**: None
- **Summary**: Cooperative perception allows a Connected Autonomous Vehicle (CAV) to interact with the other CAVs in the vicinity to enhance perception of surrounding objects to increase safety and reliability. It can compensate for the limitations of the conventional vehicular perception such as blind spots, low resolution, and weather effects. An effective feature fusion model for the intermediate fusion methods of cooperative perception can improve feature selection and information aggregation to further enhance the perception accuracy. We propose adaptive feature fusion models with trainable feature selection modules. One of our proposed models Spatial-wise Adaptive feature Fusion (S-AdaFusion) outperforms all other State-of-the-Arts (SOTAs) on two subsets of the OPV2V dataset: Default CARLA Towns for vehicle detection and the Culver City for domain adaptation. In addition, previous studies have only tested cooperative perception for vehicle detection. A pedestrian, however, is much more likely to be seriously injured in a traffic accident. We evaluate the performance of cooperative perception for both vehicle and pedestrian detection using the CODD dataset. Our architecture achieves higher Average Precision (AP) than other existing models for both vehicle and pedestrian detection on the CODD dataset. The experiments demonstrate that cooperative perception also improves the pedestrian detection accuracy compared to the conventional single vehicle perception process.



### DAS: Densely-Anchored Sampling for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.00119v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.00119v1)
- **Published**: 2022-07-30 02:07:46+00:00
- **Updated**: 2022-07-30 02:07:46+00:00
- **Authors**: Lizhao Liu, Shangxin Huang, Zhuangwei Zhuang, Ran Yang, Mingkui Tan, Yaowei Wang
- **Comment**: 32 pages. Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Deep Metric Learning (DML) serves to learn an embedding function to project semantically similar data into nearby embedding space and plays a vital role in many applications, such as image retrieval and face recognition. However, the performance of DML methods often highly depends on sampling methods to choose effective data from the embedding space in the training. In practice, the embeddings in the embedding space are obtained by some deep models, where the embedding space is often with barren area due to the absence of training points, resulting in so called "missing embedding" issue. This issue may impair the sample quality, which leads to degenerated DML performance. In this work, we investigate how to alleviate the "missing embedding" issue to improve the sampling quality and achieve effective DML. To this end, we propose a Densely-Anchored Sampling (DAS) scheme that considers the embedding with corresponding data point as "anchor" and exploits the anchor's nearby embedding space to densely produce embeddings without data points. Specifically, we propose to exploit the embedding space around single anchor with Discriminative Feature Scaling (DFS) and multiple anchors with Memorized Transformation Shifting (MTS). In this way, by combing the embeddings with and without data points, we are able to provide more embeddings to facilitate the sampling process thus boosting the performance of DML. Our method is effortlessly integrated into existing DML frameworks and improves them without bells and whistles. Extensive experiments on three benchmark datasets demonstrate the superiority of our method.



### Few-Shot Class-Incremental Learning from an Open-Set Perspective
- **Arxiv ID**: http://arxiv.org/abs/2208.00147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.00147v1)
- **Published**: 2022-07-30 05:42:48+00:00
- **Updated**: 2022-07-30 05:42:48+00:00
- **Authors**: Can Peng, Kun Zhao, Tianren Wang, Meng Li, Brian C. Lovell
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: The continual appearance of new objects in the visual world poses considerable challenges for current deep learning methods in real-world deployments. The challenge of new task learning is often exacerbated by the scarcity of data for the new categories due to rarity or cost. Here we explore the important task of Few-Shot Class-Incremental Learning (FSCIL) and its extreme data scarcity condition of one-shot. An ideal FSCIL model needs to perform well on all classes, regardless of their presentation order or paucity of data. It also needs to be robust to open-set real-world conditions and be easily adapted to the new tasks that always arise in the field. In this paper, we first reevaluate the current task setting and propose a more comprehensive and practical setting for the FSCIL task. Then, inspired by the similarity of the goals for FSCIL and modern face recognition systems, we propose our method -- Augmented Angular Loss Incremental Classification or ALICE. In ALICE, instead of the commonly used cross-entropy loss, we propose to use the angular penalty loss to obtain well-clustered features. As the obtained features not only need to be compactly clustered but also diverse enough to maintain generalization for future incremental classes, we further discuss how class augmentation, data augmentation, and data balancing affect classification performance. Experiments on benchmark datasets, including CIFAR100, miniImageNet, and CUB200, demonstrate the improved performance of ALICE over the state-of-the-art FSCIL methods.



### Learning Shadow Correspondence for Video Shadow Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.00150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.00150v1)
- **Published**: 2022-07-30 06:30:42+00:00
- **Updated**: 2022-07-30 06:30:42+00:00
- **Authors**: Xinpeng Ding, Jingweng Yang, Xiaowei Hu, Xiaomeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Video shadow detection aims to generate consistent shadow predictions among video frames. However, the current approaches suffer from inconsistent shadow predictions across frames, especially when the illumination and background textures change in a video. We make an observation that the inconsistent predictions are caused by the shadow feature inconsistency, i.e., the features of the same shadow regions show dissimilar proprieties among the nearby frames.In this paper, we present a novel Shadow-Consistent Correspondence method (SC-Cor) to enhance pixel-wise similarity of the specific shadow regions across frames for video shadow detection. Our proposed SC-Cor has three main advantages. Firstly, without requiring the dense pixel-to-pixel correspondence labels, SC-Cor can learn the pixel-wise correspondence across frames in a weakly-supervised manner. Secondly, SC-Cor considers intra-shadow separability, which is robust to the variant textures and illuminations in videos. Finally, SC-Cor is a plug-and-play module that can be easily integrated into existing shadow detectors with no extra computational cost. We further design a new evaluation metric to evaluate the temporal stability of the video shadow detection results. Experimental results show that SC-Cor outperforms the prior state-of-the-art method, by 6.51% on IoU and 3.35% on the newly introduced temporal stability metric.



### Learning Feature Decomposition for Domain Adaptive Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.00160v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.00160v1)
- **Published**: 2022-07-30 08:05:35+00:00
- **Updated**: 2022-07-30 08:05:35+00:00
- **Authors**: Shao-Yuan Lo, Wei Wang, Jim Thomas, Jingjing Zheng, Vishal M. Patel, Cheng-Hao Kuo
- **Comment**: Accepted at IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS) 2022
- **Journal**: None
- **Summary**: Monocular depth estimation (MDE) has attracted intense study due to its low cost and critical functions for robotic tasks such as localization, mapping and obstacle detection. Supervised approaches have led to great success with the advance of deep learning, but they rely on large quantities of ground-truth depth annotations that are expensive to acquire. Unsupervised domain adaptation (UDA) transfers knowledge from labeled source data to unlabeled target data, so as to relax the constraint of supervised learning. However, existing UDA approaches may not completely align the domain gap across different datasets because of the domain shift problem. We believe better domain alignment can be achieved via well-designed feature decomposition. In this paper, we propose a novel UDA method for MDE, referred to as Learning Feature Decomposition for Adaptation (LFDA), which learns to decompose the feature space into content and style components. LFDA only attempts to align the content component since it has a smaller domain gap. Meanwhile, it excludes the style component which is specific to the source domain from training the primary task. Furthermore, LFDA uses separate feature distribution estimations to further bridge the domain gap. Extensive experiments on three domain adaptative MDE scenarios show that the proposed method achieves superior accuracy and lower computational cost compared to the state-of-the-art approaches.



### Resolution enhancement of placenta histological images using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2208.00163v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.00163v1)
- **Published**: 2022-07-30 08:17:37+00:00
- **Updated**: 2022-07-30 08:17:37+00:00
- **Authors**: Arash Rabbani, Masoud Babaei
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, a method has been developed to improve the resolution of histological human placenta images. For this purpose, a paired series of high- and low-resolution images have been collected to train a deep neural network model that can predict image residuals required to improve the resolution of the input images. A modified version of the U-net neural network model has been tailored to find the relationship between the low resolution and residual images. After training for 900 epochs on an augmented dataset of 1000 images, the relative mean squared error of 0.003 is achieved for the prediction of 320 test images. The proposed method has not only improved the contrast of the low-resolution images at the edges of cells but added critical details and textures that mimic high-resolution images of placenta villous space.



### Distilled Low Rank Neural Radiance Field with Quantization for Light Field Compression
- **Arxiv ID**: http://arxiv.org/abs/2208.00164v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.00164v1)
- **Published**: 2022-07-30 08:19:29+00:00
- **Updated**: 2022-07-30 08:19:29+00:00
- **Authors**: Jinglei Shi, Christine Guillemot
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel light field compression method based on a Quantized Distilled Low Rank Neural Radiance Field (QDLR-NeRF) representation. While existing compression methods encode the set of light field sub-aperture images, our proposed method instead learns an implicit scene representation in the form of a Neural Radiance Field (NeRF), which also enables view synthesis. For reducing its size, the model is first learned under a Low Rank (LR) constraint using a Tensor Train (TT) decomposition in an Alternating Direction Method of Multipliers (ADMM) optimization framework. To further reduce the model size, the components of the tensor train decomposition need to be quantized. However, performing the optimization of the NeRF model by simultaneously taking the low rank constraint and the rate-constrained weight quantization into consideration is challenging. To deal with this difficulty, we introduce a network distillation operation that separates the low rank approximation and the weight quantization in the network training. The information from the initial LR constrained NeRF (LR-NeRF) is distilled to a model of a much smaller dimension (DLR-NeRF) based on the TT decomposition of the LR-NeRF. An optimized global codebook is then learned to quantize all TT components, producing the final QDLRNeRF. Experimental results show that our proposed method yields better compression efficiency compared with state-of-the-art methods, and it additionally has the advantage of allowing the synthesis of any light field view with a high quality.



### Temporal extrapolation of heart wall segmentation in cardiac magnetic resonance images via pixel tracking
- **Arxiv ID**: http://arxiv.org/abs/2208.00165v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.00165v1)
- **Published**: 2022-07-30 08:23:04+00:00
- **Updated**: 2022-07-30 08:23:04+00:00
- **Authors**: Arash Rabbani, Hao Gao, Dirk Husmeier
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we have tailored a pixel tracking method for temporal extrapolation of the ventricular segmentation masks in cardiac magnetic resonance images. The pixel tracking process starts from the end-diastolic frame of the heart cycle using the available manually segmented images to predict the end-systolic segmentation mask. The superpixels approach is used to divide the raw images into smaller cells and in each time frame, new labels are assigned to the image cells which leads to tracking the movement of the heart wall elements through different frames. The tracked masks at the end of systole are compared with the already available manually segmented masks and dice scores are found to be between 0.81 to 0.84. Considering the fact that the proposed method does not necessarily require a training dataset, it could be an attractive alternative approach to deep learning segmentation methods in scenarios where training data are limited.



### Virtual Reality Simulator for Fetoscopic Spina Bifida Repair Surgery
- **Arxiv ID**: http://arxiv.org/abs/2208.00169v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2208.00169v1)
- **Published**: 2022-07-30 08:51:11+00:00
- **Updated**: 2022-07-30 08:51:11+00:00
- **Authors**: Przemysław Korzeniowski, Szymon Płotka, Robert Brawura-Biskupski-Samaha, Arkadiusz Sitek
- **Comment**: Accepted for IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS) 2022, Kyoto, Japan
- **Journal**: None
- **Summary**: Spina Bifida (SB) is a birth defect developed during the early stage of pregnancy in which there is incomplete closing of the spine around the spinal cord. The growing interest in fetoscopic Spina-Bifida repair, which is performed in fetuses who are still in the pregnant uterus, prompts the need for appropriate training. The learning curve for such procedures is steep and requires excellent procedural skills. Computer-based virtual reality (VR) simulation systems offer a safe, cost-effective, and configurable training environment free from ethical and patient safety issues. However, to the best of our knowledge, there are currently no commercial or experimental VR training simulation systems available for fetoscopic SB-repair procedures. In this paper, we propose a novel VR simulator for core manual skills training for SB-repair. An initial simulation realism validation study was carried out by obtaining subjective feedback (face and content validity) from 14 clinicians. The overall simulation realism was on average marked 4.07 on a 5-point Likert scale (1 - very unrealistic, 5 - very realistic). Its usefulness as a training tool for SB-repair as well as in learning fundamental laparoscopic skills was marked 4.63 and 4.80, respectively. These results indicate that VR simulation of fetoscopic procedures may contribute to surgical training without putting fetuses and their mothers at risk. It could also facilitate wider adaptation of fetoscopic procedures in place of much more invasive open fetal surgeries.



### A Survey on Masked Autoencoder for Self-supervised Learning in Vision and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2208.00173v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.00173v1)
- **Published**: 2022-07-30 09:59:28+00:00
- **Updated**: 2022-07-30 09:59:28+00:00
- **Authors**: Chaoning Zhang, Chenshuang Zhang, Junha Song, John Seon Keun Yi, Kang Zhang, In So Kweon
- **Comment**: First survey on masked autoencoder (under progress)
- **Journal**: None
- **Summary**: Masked autoencoders are scalable vision learners, as the title of MAE \cite{he2022masked}, which suggests that self-supervised learning (SSL) in vision might undertake a similar trajectory as in NLP. Specifically, generative pretext tasks with the masked prediction (e.g., BERT) have become a de facto standard SSL practice in NLP. By contrast, early attempts at generative methods in vision have been buried by their discriminative counterparts (like contrastive learning); however, the success of mask image modeling has revived the masking autoencoder (often termed denoising autoencoder in the past). As a milestone to bridge the gap with BERT in NLP, masked autoencoder has attracted unprecedented attention for SSL in vision and beyond. This work conducts a comprehensive survey of masked autoencoders to shed insight on a promising direction of SSL. As the first to review SSL with masked autoencoders, this work focuses on its application in vision by discussing its historical developments, recent progress, and implications for diverse applications.



### Few-shot Single-view 3D Reconstruction with Memory Prior Contrastive Network
- **Arxiv ID**: http://arxiv.org/abs/2208.00183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.00183v1)
- **Published**: 2022-07-30 10:49:39+00:00
- **Updated**: 2022-07-30 10:49:39+00:00
- **Authors**: Zhen Xing, Yijiang Chen, Zhixin Ling, Xiangdong Zhou, Yu Xiang
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: 3D reconstruction of novel categories based on few-shot learning is appealing in real-world applications and attracts increasing research interests. Previous approaches mainly focus on how to design shape prior models for different categories. Their performance on unseen categories is not very competitive. In this paper, we present a Memory Prior Contrastive Network (MPCN) that can store shape prior knowledge in a few-shot learning based 3D reconstruction framework. With the shape memory, a multi-head attention module is proposed to capture different parts of a candidate shape prior and fuse these parts together to guide 3D reconstruction of novel categories. Besides, we introduce a 3D-aware contrastive learning method, which can not only complement the retrieval accuracy of memory network, but also better organize image features for downstream tasks. Compared with previous few-shot 3D reconstruction methods, MPCN can handle the inter-class variability without category annotations. Experimental results on a benchmark synthetic dataset and the Pascal3D+ real-world dataset show that our model outperforms the current state-of-the-art methods significantly.



### LRIP-Net: Low-Resolution Image Prior based Network for Limited-Angle CT Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2208.00207v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2208.00207v1)
- **Published**: 2022-07-30 13:03:20+00:00
- **Updated**: 2022-07-30 13:03:20+00:00
- **Authors**: Qifeng Gao, Rui Ding, Linyuan Wang, Bin Xue, Yuping Duan
- **Comment**: None
- **Journal**: None
- **Summary**: In the practical applications of computed tomography imaging, the projection data may be acquired within a limited-angle range and corrupted by noises due to the limitation of scanning conditions. The noisy incomplete projection data results in the ill-posedness of the inverse problems. In this work, we theoretically verify that the low-resolution reconstruction problem has better numerical stability than the high-resolution problem. In what follows, a novel low-resolution image prior based CT reconstruction model is proposed to make use of the low-resolution image to improve the reconstruction quality. More specifically, we build up a low-resolution reconstruction problem on the down-sampled projection data, and use the reconstructed low-resolution image as prior knowledge for the original limited-angle CT problem. We solve the constrained minimization problem by the alternating direction method with all subproblems approximated by the convolutional neural networks. Numerical experiments demonstrate that our double-resolution network outperforms both the variational method and popular learning-based reconstruction methods on noisy limited-angle reconstruction problems.



### Multiple Categories Of Visual Smoke Detection Database
- **Arxiv ID**: http://arxiv.org/abs/2208.00210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.00210v1)
- **Published**: 2022-07-30 13:14:06+00:00
- **Updated**: 2022-07-30 13:14:06+00:00
- **Authors**: Y. Gong, X. Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Smoke detection has become a significant task in associated industries due to the close relationship between the petrochemical industry's smoke emission and its safety production and environmental damage. There are several production situations in the real industrial production environment, including complete combustion of exhaust gas, inadequate combustion of exhaust gas, direct emission of exhaust gas, etc. We discovered that the datasets used in previous research work can only determine whether smoke is present or not, not its type. That is, the dataset's category does not map to the real-world production situations, which are not conducive to the precise regulation of the production system. As a result, we created a multi-categories smoke detection database that includes a total of 70196 images. We further employed multiple models to conduct the experiment on the proposed database, the results show that the performance of the current algorithms needs to be improved and demonstrate the effectiveness of the proposed database.



### Towards Privacy-Preserving, Real-Time and Lossless Feature Matching
- **Arxiv ID**: http://arxiv.org/abs/2208.00214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.00214v1)
- **Published**: 2022-07-30 13:25:59+00:00
- **Updated**: 2022-07-30 13:25:59+00:00
- **Authors**: Qiang Meng, Feng Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Most visual retrieval applications store feature vectors for downstream matching tasks. These vectors, from where user information can be spied out, will cause privacy leakage if not carefully protected. To mitigate privacy risks, current works primarily utilize non-invertible transformations or fully cryptographic algorithms. However, transformation-based methods usually fail to achieve satisfying matching performances while cryptosystems suffer from heavy computational overheads. In addition, secure levels of current methods should be improved to confront potential adversary attacks. To address these issues, this paper proposes a plug-in module called SecureVector that protects features by random permutations, 4L-DEC converting and existing homomorphic encryption techniques. For the first time, SecureVector achieves real-time and lossless feature matching among sanitized features, along with much higher security levels than current state-of-the-arts. Extensive experiments on face recognition, person re-identification, image retrieval, and privacy analyses demonstrate the effectiveness of our method. Given limited public projects in this field, codes of our method and implemented baselines are made open-source in https://github.com/IrvingMeng/SecureVector.



### Meta-DETR: Image-Level Few-Shot Detection with Inter-Class Correlation Exploitation
- **Arxiv ID**: http://arxiv.org/abs/2208.00219v1
- **DOI**: 10.1109/TPAMI.2022.3195735
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.00219v1)
- **Published**: 2022-07-30 13:46:07+00:00
- **Updated**: 2022-07-30 13:46:07+00:00
- **Authors**: Gongjie Zhang, Zhipeng Luo, Kaiwen Cui, Shijian Lu, Eric P. Xing
- **Comment**: Accepted by T-PAMI (IEEE Transactions on Pattern Analysis and Machine
  Intelligence). Codes: https://github.com/ZhangGongjie/Meta-DETR
- **Journal**: None
- **Summary**: Few-shot object detection has been extensively investigated by incorporating meta-learning into region-based detection frameworks. Despite its success, the said paradigm is still constrained by several factors, such as (i) low-quality region proposals for novel classes and (ii) negligence of the inter-class correlation among different classes. Such limitations hinder the generalization of base-class knowledge for the detection of novel-class objects. In this work, we design Meta-DETR, which (i) is the first image-level few-shot detector, and (ii) introduces a novel inter-class correlational meta-learning strategy to capture and leverage the correlation among different classes for robust and accurate few-shot object detection. Meta-DETR works entirely at image level without any region proposals, which circumvents the constraint of inaccurate proposals in prevalent few-shot detection frameworks. In addition, the introduced correlational meta-learning enables Meta-DETR to simultaneously attend to multiple support classes within a single feedforward, which allows to capture the inter-class correlation among different classes, thus significantly reducing the misclassification over similar classes and enhancing knowledge generalization to novel classes. Experiments over multiple few-shot object detection benchmarks show that the proposed Meta-DETR outperforms state-of-the-art methods by large margins. The implementation codes are available at https://github.com/ZhangGongjie/Meta-DETR.



### PolarMix: A General Data Augmentation Technique for LiDAR Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2208.00223v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.00223v1)
- **Published**: 2022-07-30 13:52:19+00:00
- **Updated**: 2022-07-30 13:52:19+00:00
- **Authors**: Aoran Xiao, Jiaxing Huang, Dayan Guan, Kaiwen Cui, Shijian Lu, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: LiDAR point clouds, which are usually scanned by rotating LiDAR sensors continuously, capture precise geometry of the surrounding environment and are crucial to many autonomous detection and navigation tasks. Though many 3D deep architectures have been developed, efficient collection and annotation of large amounts of point clouds remain one major challenge in the analytic and understanding of point cloud data. This paper presents PolarMix, a point cloud augmentation technique that is simple and generic but can mitigate the data constraint effectively across different perception tasks and scenarios. PolarMix enriches point cloud distributions and preserves point cloud fidelity via two cross-scan augmentation strategies that cut, edit, and mix point clouds along the scanning direction. The first is scene-level swapping which exchanges point cloud sectors of two LiDAR scans that are cut along the azimuth axis. The second is instance-level rotation and paste which crops point instances from one LiDAR scan, rotates them by multiple angles (to create multiple copies), and paste the rotated point instances into other scans. Extensive experiments show that PolarMix achieves superior performance consistently across different perception tasks and scenarios. In addition, it can work as plug-and-play for various 3D deep architectures and also performs well for unsupervised domain adaptation.



### Learning Pseudo Front Depth for 2D Forward-Looking Sonar-based Multi-view Stereo
- **Arxiv ID**: http://arxiv.org/abs/2208.00233v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2208.00233v1)
- **Published**: 2022-07-30 14:35:21+00:00
- **Updated**: 2022-07-30 14:35:21+00:00
- **Authors**: Yusheng Wang, Yonghoon Ji, Hiroshi Tsuchiya, Hajime Asama, Atsushi Yamashita
- **Comment**: Accepted at IROS 2022
- **Journal**: None
- **Summary**: Retrieving the missing dimension information in acoustic images from 2D forward-looking sonar is a well-known problem in the field of underwater robotics. There are works attempting to retrieve 3D information from a single image which allows the robot to generate 3D maps with fly-through motion. However, owing to the unique image formulation principle, estimating 3D information from a single image faces severe ambiguity problems. Classical methods of multi-view stereo can avoid the ambiguity problems, but may require a large number of viewpoints to generate an accurate model. In this work, we propose a novel learning-based multi-view stereo method to estimate 3D information. To better utilize the information from multiple frames, an elevation plane sweeping method is proposed to generate the depth-azimuth-elevation cost volume. The volume after regularization can be considered as a probabilistic volumetric representation of the target. Instead of performing regression on the elevation angles, we use pseudo front depth from the cost volume to represent the 3D information which can avoid the 2D-3D problem in acoustic imaging. High-accuracy results can be generated with only two or three images. Synthetic datasets were generated to simulate various underwater targets. We also built the first real dataset with accurate ground truth in a large scale water tank. Experimental results demonstrate the superiority of our method, compared to other state-of-the-art methods.



### RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.00237v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.00237v2)
- **Published**: 2022-07-30 14:45:20+00:00
- **Updated**: 2022-09-28 10:50:29+00:00
- **Authors**: Ruida Zhang, Yan Di, Zhiqiang Lou, Fabian Manhardt, Federico Tombari, Xiangyang Ji
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Category-level object pose estimation aims to predict the 6D pose as well as the 3D metric size of arbitrary objects from a known set of categories. Recent methods harness shape prior adaptation to map the observed point cloud into the canonical space and apply Umeyama algorithm to recover the pose and size. However, their shape prior integration strategy boosts pose estimation indirectly, which leads to insufficient pose-sensitive feature extraction and slow inference speed. To tackle this problem, in this paper, we propose a novel geometry-guided Residual Object Bounding Box Projection network RBP-Pose that jointly predicts object pose and residual vectors describing the displacements from the shape-prior-indicated object surface projections on the bounding box towards the real surface projections. Such definition of residual vectors is inherently zero-mean and relatively small, and explicitly encapsulates spatial cues of the 3D object for robust and accurate pose regression. We enforce geometry-aware consistency terms to align the predicted pose and residual vectors to further boost performance.



### Improving Fine-tuning of Self-supervised Models with Contrastive Initialization
- **Arxiv ID**: http://arxiv.org/abs/2208.00238v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.00238v1)
- **Published**: 2022-07-30 14:45:57+00:00
- **Updated**: 2022-07-30 14:45:57+00:00
- **Authors**: Haolin Pan, Yong Guo, Qinyi Deng, Haomin Yang, Yiqun Chen, Jian Chen
- **Comment**: 22 pages, 4 figures
- **Journal**: None
- **Summary**: Self-supervised learning (SSL) has achieved remarkable performance in pretraining the models that can be further used in downstream tasks via fine-tuning. However, these self-supervised models may not capture meaningful semantic information since the images belonging to the same class are always regarded as negative pairs in the contrastive loss. Consequently, the images of the same class are often located far away from each other in learned feature space, which would inevitably hamper the fine-tuning process. To address this issue, we seek to provide a better initialization for the self-supervised models by enhancing the semantic information. To this end, we propose a Contrastive Initialization (COIN) method that breaks the standard fine-tuning pipeline by introducing an extra initialization stage before fine-tuning. Extensive experiments show that, with the enriched semantics, our COIN significantly outperforms existing methods without introducing extra training cost and sets new state-of-the-arts on multiple downstream tasks.



### Revisiting the Critical Factors of Augmentation-Invariant Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.00275v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.00275v1)
- **Published**: 2022-07-30 17:07:13+00:00
- **Updated**: 2022-07-30 17:07:13+00:00
- **Authors**: Junqiang Huang, Xiangwen Kong, Xiangyu Zhang
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: We focus on better understanding the critical factors of augmentation-invariant representation learning. We revisit MoCo v2 and BYOL and try to prove the authenticity of the following assumption: different frameworks bring about representations of different characteristics even with the same pretext task. We establish the first benchmark for fair comparisons between MoCo v2 and BYOL, and observe: (i) sophisticated model configurations enable better adaptation to pre-training dataset; (ii) mismatched optimization strategies of pre-training and fine-tuning hinder model from achieving competitive transfer performances. Given the fair benchmark, we make further investigation and find asymmetry of network structure endows contrastive frameworks to work well under the linear evaluation protocol, while may hurt the transfer performances on long-tailed classification tasks. Moreover, negative samples do not make models more sensible to the choice of data augmentations, nor does the asymmetric network structure. We believe our findings provide useful information for future work.



### MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures
- **Arxiv ID**: http://arxiv.org/abs/2208.00277v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.00277v5)
- **Published**: 2022-07-30 17:14:14+00:00
- **Updated**: 2023-05-30 03:49:00+00:00
- **Authors**: Zhiqin Chen, Thomas Funkhouser, Peter Hedman, Andrea Tagliasacchi
- **Comment**: CVPR 2023. Project page: https://mobile-nerf.github.io, code:
  https://github.com/google-research/jax3d/tree/main/jax3d/projects/mobilenerf
- **Journal**: None
- **Summary**: Neural Radiance Fields (NeRFs) have demonstrated amazing ability to synthesize images of 3D scenes from novel views. However, they rely upon specialized volumetric rendering algorithms based on ray marching that are mismatched to the capabilities of widely deployed graphics hardware. This paper introduces a new NeRF representation based on textured polygons that can synthesize novel images efficiently with standard rendering pipelines. The NeRF is represented as a set of polygons with textures representing binary opacities and feature vectors. Traditional rendering of the polygons with a z-buffer yields an image with features at every pixel, which are interpreted by a small, view-dependent MLP running in a fragment shader to produce a final pixel color. This approach enables NeRFs to be rendered with the traditional polygon rasterization pipeline, which provides massive pixel-level parallelism, achieving interactive frame rates on a wide range of compute platforms, including mobile phones.



### Point Primitive Transformer for Long-Term 4D Point Cloud Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/2208.00281v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.00281v2)
- **Published**: 2022-07-30 17:41:55+00:00
- **Updated**: 2022-12-20 10:22:29+00:00
- **Authors**: Hao Wen, Yunze Liu, Jingwei Huang, Bo Duan, Li Yi
- **Comment**: None
- **Journal**: ECCV2022
- **Summary**: This paper proposes a 4D backbone for long-term point cloud video understanding. A typical way to capture spatial-temporal context is using 4Dconv or transformer without hierarchy. However, those methods are neither effective nor efficient enough due to camera motion, scene changes, sampling patterns, and the complexity of 4D data. To address those issues, we leverage the primitive plane as a mid-level representation to capture the long-term spatial-temporal context in 4D point cloud videos and propose a novel hierarchical backbone named Point Primitive Transformer(PPTr), which is mainly composed of intra-primitive point transformers and primitive transformers. Extensive experiments show that PPTr outperforms the previous state of the arts on different tasks.



### Simplex Clustering via sBeta with Applications to Online Adjustment of Black-Box Predictions
- **Arxiv ID**: http://arxiv.org/abs/2208.00287v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.00287v3)
- **Published**: 2022-07-30 18:29:11+00:00
- **Updated**: 2022-10-08 23:18:02+00:00
- **Authors**: Florent Chiaroni, Malik Boudiaf, Amar Mitiche, Ismail Ben Ayed
- **Comment**: None
- **Journal**: None
- **Summary**: We explore clustering the softmax predictions of deep neural networks and introduce a novel probabilistic clustering method, referred to as k-sBetas. In the general context of clustering discrete distributions, the existing methods focused on exploring distortion measures tailored to simplex data, such as the KL divergence, as alternatives to the standard Euclidean distance. We provide a general maximum a posteriori (MAP) perspective of clustering distributions, which emphasizes that the statistical models underlying the existing distortion-based methods may not be descriptive enough. Instead, we optimize a mixed-variable objective measuring the conformity of data within each cluster to the introduced sBeta density function, whose parameters are constrained and estimated jointly with binary assignment variables. Our versatile formulation approximates a variety of parametric densities for modeling simplex data, and enables to control the cluster-balance bias. This yields highly competitive performances for unsupervised adjustments of black-box model predictions in a variety of scenarios. Our code and comparisons with the existing simplex-clustering approaches along with our introduced softmax-prediction benchmarks are publicly available: https://github.com/fchiaroni/Clustering_Softmax_Predictions.



### Automated Detection of Acute Lymphoblastic Leukemia Subtypes from Microscopic Blood Smear Images using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.08992v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.08992v1)
- **Published**: 2022-07-30 20:31:59+00:00
- **Updated**: 2022-07-30 20:31:59+00:00
- **Authors**: Md. Taufiqul Haque Khan Tusar, Roban Khan Anik
- **Comment**: 25 pages, 20 figures, A project report submitted in partial
  fulfillment of the requirements for the Degree of Bachelor of Science in
  Computer Science and Engineering at City University, Dhaka 1216, Bangladesh
- **Journal**: None
- **Summary**: An estimated 300,000 new cases of leukemia are diagnosed each year which is 2.8 percent of all new cancer cases and the prevalence is rising day by day. The most dangerous and deadly type of leukemia is acute lymphoblastic leukemia (ALL), which affects people of all age groups, including children and adults. In this study, we propose an automated system to detect various-shaped ALL blast cells from microscopic blood smears images using Deep Neural Networks (DNN). The system can detect multiple subtypes of ALL cells with an accuracy of 98 percent. Moreover, we have developed a telediagnosis software to provide real-time support to diagnose ALL subtypes from microscopic blood smears images.



### Doubly Deformable Aggregation of Covariance Matrices for Few-shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.00306v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.00306v1)
- **Published**: 2022-07-30 20:41:38+00:00
- **Updated**: 2022-07-30 20:41:38+00:00
- **Authors**: Zhitong Xiong, Haopeng Li, Xiao Xiang Zhu
- **Comment**: ECCV 2022 accepted
- **Journal**: None
- **Summary**: Training semantic segmentation models with few annotated samples has great potential in various real-world applications. For the few-shot segmentation task, the main challenge is how to accurately measure the semantic correspondence between the support and query samples with limited training data. To address this problem, we propose to aggregate the learnable covariance matrices with a deformable 4D Transformer to effectively predict the segmentation map. Specifically, in this work, we first devise a novel hard example mining mechanism to learn covariance kernels for the Gaussian process. The learned covariance kernel functions have great advantages over existing cosine similarity-based methods in correspondence measurement. Based on the learned covariance kernels, an efficient doubly deformable 4D Transformer module is designed to adaptively aggregate feature similarity maps into segmentation results. By combining these two designs, the proposed method can not only set new state-of-the-art performance on public benchmarks, but also converge extremely faster than existing methods. Experiments on three public datasets have demonstrated the effectiveness of our method.



### Delving into Effective Gradient Matching for Dataset Condensation
- **Arxiv ID**: http://arxiv.org/abs/2208.00311v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.00311v1)
- **Published**: 2022-07-30 21:31:10+00:00
- **Updated**: 2022-07-30 21:31:10+00:00
- **Authors**: Zixuan Jiang, Jiaqi Gu, Mingjie Liu, David Z. Pan
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: As deep learning models and datasets rapidly scale up, network training is extremely time-consuming and resource-costly. Instead of training on the entire dataset, learning with a small synthetic dataset becomes an efficient solution. Extensive research has been explored in the direction of dataset condensation, among which gradient matching achieves state-of-the-art performance. The gradient matching method directly targets the training dynamics by matching the gradient when training on the original and synthetic datasets. However, there are limited deep investigations into the principle and effectiveness of this method. In this work, we delve into the gradient matching method from a comprehensive perspective and answer the critical questions of what, how, and where to match. We propose to match the multi-level gradients to involve both intra-class and inter-class gradient information. We demonstrate that the distance function should focus on the angle, considering the magnitude simultaneously to delay the overfitting. An overfitting-aware adaptive learning step strategy is also proposed to trim unnecessary optimization steps for algorithmic efficiency improvement. Ablation and comparison experiments demonstrate that our proposed methodology shows superior accuracy, efficiency, and generalization compared to prior work.



