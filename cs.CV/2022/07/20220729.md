# Arxiv Papers in cs.CV on 2022-07-29
### 3D Cartoon Face Generation with Controllable Expressions from a Single GAN Image
- **Arxiv ID**: http://arxiv.org/abs/2207.14425v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14425v1)
- **Published**: 2022-07-29 01:06:21+00:00
- **Updated**: 2022-07-29 01:06:21+00:00
- **Authors**: Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate an open research task of generating 3D cartoon face shapes from single 2D GAN generated human faces and without 3D supervision, where we can also manipulate the facial expressions of the 3D shapes. To this end, we discover the semantic meanings of StyleGAN latent space, such that we are able to produce face images of various expressions, poses, and lighting by controlling the latent codes. Specifically, we first finetune the pretrained StyleGAN face model on the cartoon datasets. By feeding the same latent codes to face and cartoon generation models, we aim to realize the translation from 2D human face images to cartoon styled avatars. We then discover semantic directions of the GAN latent space, in an attempt to change the facial expressions while preserving the original identity. As we do not have any 3D annotations for cartoon faces, we manipulate the latent codes to generate images with different poses and lighting, such that we can reconstruct the 3D cartoon face shapes. We validate the efficacy of our method on three cartoon datasets qualitatively and quantitatively.



### Paired Cross-Modal Data Augmentation for Fine-Grained Image-to-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2207.14428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14428v1)
- **Published**: 2022-07-29 01:21:54+00:00
- **Updated**: 2022-07-29 01:21:54+00:00
- **Authors**: Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao
- **Comment**: Accepted at ACM MM 2022
- **Journal**: None
- **Summary**: This paper investigates an open research problem of generating text-image pairs to improve the training of fine-grained image-to-text cross-modal retrieval task, and proposes a novel framework for paired data augmentation by uncovering the hidden semantic information of StyleGAN2 model. Specifically, we first train a StyleGAN2 model on the given dataset. We then project the real images back to the latent space of StyleGAN2 to obtain the latent codes. To make the generated images manipulatable, we further introduce a latent space alignment module to learn the alignment between StyleGAN2 latent codes and the corresponding textual caption features. When we do online paired data augmentation, we first generate augmented text through random token replacement, then pass the augmented text into the latent space alignment module to output the latent codes, which are finally fed to StyleGAN2 to generate the augmented images. We evaluate the efficacy of our augmented data approach on two public cross-modal retrieval datasets, in which the promising experimental results demonstrate the augmented text-image pair data can be trained together with the original data to boost the image-to-text cross-modal retrieval performance.



### Graph-Based Small Bowel Path Tracking with Cylindrical Constraints
- **Arxiv ID**: http://arxiv.org/abs/2207.14436v1
- **DOI**: 10.1109/ISBI52829.2022.9761423
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.14436v1)
- **Published**: 2022-07-29 02:17:56+00:00
- **Updated**: 2022-07-29 02:17:56+00:00
- **Authors**: Seung Yeon Shin, Sungwon Lee, Ronald M. Summers
- **Comment**: Published in: 2022 IEEE 19th International Symposium on Biomedical
  Imaging (ISBI)
- **Journal**: None
- **Summary**: We present a new graph-based method for small bowel path tracking based on cylindrical constraints. A distinctive characteristic of the small bowel compared to other organs is the contact between parts of itself along its course, which makes the path tracking difficult together with the indistinct appearance of the wall. It causes the tracked path to easily cross over the walls when relying on low-level features like the wall detection. To circumvent this, a series of cylinders that are fitted along the course of the small bowel are used to guide the tracking to more reliable directions. It is implemented as soft constraints using a new cost function. The proposed method is evaluated against ground-truth paths that are all connected from start to end of the small bowel for 10 abdominal CT scans. The proposed method showed clear improvements compared to the baseline method in tracking the path without making an error. Improvements of 6.6% and 17.0%, in terms of the tracked length, were observed for two different settings related to the small bowel segmentation.



### Dataset and Evaluation algorithm design for GOALS Challenge
- **Arxiv ID**: http://arxiv.org/abs/2207.14447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14447v1)
- **Published**: 2022-07-29 02:51:26+00:00
- **Updated**: 2022-07-29 02:51:26+00:00
- **Authors**: Huihui Fang, Fei Li, Huazhu Fu, Junde Wu, Xiulan Zhang, Yanwu Xu
- **Comment**: 8 pages, 3 figures, OMIA9 (MICCAI 2022) workshop
- **Journal**: None
- **Summary**: Glaucoma causes irreversible vision loss due to damage to the optic nerve, and there is no cure for glaucoma.OCT imaging modality is an essential technique for assessing glaucomatous damage since it aids in quantifying fundus structures. To promote the research of AI technology in the field of OCT-assisted diagnosis of glaucoma, we held a Glaucoma OCT Analysis and Layer Segmentation (GOALS) Challenge in conjunction with the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2022 to provide data and corresponding annotations for researchers studying layer segmentation from OCT images and the classification of glaucoma. This paper describes the released 300 circumpapillary OCT images, the baselines of the two sub-tasks, and the evaluation methodology. The GOALS Challenge is accessible at https://aistudio.baidu.com/aistudio/competition/detail/230.



### Testing Relational Understanding in Text-Guided Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2208.00005v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.00005v1)
- **Published**: 2022-07-29 02:59:38+00:00
- **Updated**: 2022-07-29 02:59:38+00:00
- **Authors**: Colin Conwell, Tomer Ullman
- **Comment**: None
- **Journal**: None
- **Summary**: Relations are basic building blocks of human cognition. Classic and recent work suggests that many relations are early developing, and quickly perceived. Machine models that aspire to human-level perception and reasoning should reflect the ability to recognize and reason generatively about relations. We report a systematic empirical examination of a recent text-guided image generation model (DALL-E 2), using a set of 15 basic physical and social relations studied or proposed in the literature, and judgements from human participants (N = 169). Overall, we find that only ~22% of images matched basic relation prompts. Based on a quantitative examination of people's judgments, we suggest that current image generation models do not yet have a grasp of even basic relations involving simple objects and agents. We examine reasons for model successes and failures, and suggest possible improvements based on computations observed in biological intelligence.



### PC-GANs: Progressive Compensation Generative Adversarial Networks for Pan-sharpening
- **Arxiv ID**: http://arxiv.org/abs/2207.14451v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.14451v1)
- **Published**: 2022-07-29 03:09:21+00:00
- **Updated**: 2022-07-29 03:09:21+00:00
- **Authors**: Yinghui Xing, Shuyuan Yang, Song Wang, Yan Zhang, Yanning Zhang
- **Comment**: 36 pages
- **Journal**: None
- **Summary**: The fusion of multispectral and panchromatic images is always dubbed pansharpening. Most of the available deep learning-based pan-sharpening methods sharpen the multispectral images through a one-step scheme, which strongly depends on the reconstruction ability of the network. However, remote sensing images always have large variations, as a result, these one-step methods are vulnerable to the error accumulation and thus incapable of preserving spatial details as well as the spectral information. In this paper, we propose a novel two-step model for pan-sharpening that sharpens the MS image through the progressive compensation of the spatial and spectral information. Firstly, a deep multiscale guided generative adversarial network is used to preliminarily enhance the spatial resolution of the MS image. Starting from the pre-sharpened MS image in the coarse domain, our approach then progressively refines the spatial and spectral residuals over a couple of generative adversarial networks (GANs) that have reverse architectures. The whole model is composed of triple GANs, and based on the specific architecture, a joint compensation loss function is designed to enable the triple GANs to be trained simultaneously. Moreover, the spatial-spectral residual compensation structure proposed in this paper can be extended to other pan-sharpening methods to further enhance their fusion results. Extensive experiments are performed on different datasets and the results demonstrate the effectiveness and efficiency of our proposed method.



### Deep Learning-based Occluded Person Re-identification: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2207.14452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14452v1)
- **Published**: 2022-07-29 03:10:18+00:00
- **Updated**: 2022-07-29 03:10:18+00:00
- **Authors**: Yunjie Peng, Saihui Hou, Chunshui Cao, Xu Liu, Yongzhen Huang, Zhiqiang He
- **Comment**: The paper is under consideration at IEEE Transactions on Circuits and
  Systems for Video Technology
- **Journal**: None
- **Summary**: Occluded person re-identification (Re-ID) aims at addressing the occlusion problem when retrieving the person of interest across multiple cameras. With the promotion of deep learning technology and the increasing demand for intelligent video surveillance, the frequent occlusion in real-world applications has made occluded person Re-ID draw considerable interest from researchers. A large number of occluded person Re-ID methods have been proposed while there are few surveys that focus on occlusion. To fill this gap and help boost future research, this paper provides a systematic survey of occluded person Re-ID. Through an in-depth analysis of the occlusion in person Re-ID, most existing methods are found to only consider part of the problems brought by occlusion. Therefore, we review occlusion-related person Re-ID methods from the perspective of issues and solutions. We summarize four issues caused by occlusion in person Re-ID, i.e., position misalignment, scale misalignment, noisy information, and missing information. The occlusion-related methods addressing different issues are then categorized and introduced accordingly. After that, we summarize and compare the performance of recent occluded person Re-ID methods on four popular datasets: Partial-ReID, Partial-iLIDS, Occluded-ReID, and Occluded-DukeMTMC. Finally, we provide insights on promising future research directions.



### Neural Density-Distance Fields
- **Arxiv ID**: http://arxiv.org/abs/2207.14455v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14455v1)
- **Published**: 2022-07-29 03:13:25+00:00
- **Updated**: 2022-07-29 03:13:25+00:00
- **Authors**: Itsuki Ueda, Yoshihiro Fukuhara, Hirokatsu Kataoka, Hiroaki Aizawa, Hidehiko Shishido, Itaru Kitahara
- **Comment**: ECCV 2022 (poster). project page: https://ueda0319.github.io/neddf/
- **Journal**: None
- **Summary**: The success of neural fields for 3D vision tasks is now indisputable. Following this trend, several methods aiming for visual localization (e.g., SLAM) have been proposed to estimate distance or density fields using neural fields. However, it is difficult to achieve high localization performance by only density fields-based methods such as Neural Radiance Field (NeRF) since they do not provide density gradient in most empty regions. On the other hand, distance field-based methods such as Neural Implicit Surface (NeuS) have limitations in objects' surface shapes. This paper proposes Neural Density-Distance Field (NeDDF), a novel 3D representation that reciprocally constrains the distance and density fields. We extend distance field formulation to shapes with no explicit boundary surface, such as fur or smoke, which enable explicit conversion from distance field to density field. Consistent distance and density fields realized by explicit conversion enable both robustness to initial values and high-quality registration. Furthermore, the consistency between fields allows fast convergence from sparse point clouds. Experiments show that NeDDF can achieve high localization performance while providing comparable results to NeRF on novel view synthesis. The code is available at https://github.com/ueda0319/neddf.



### Low-Complexity Loeffler DCT Approximations for Image and Video Coding
- **Arxiv ID**: http://arxiv.org/abs/2207.14463v1
- **DOI**: 10.3390/jlpea8040046
- **Categories**: **eess.IV**, cs.CV, cs.MM, eess.SP, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2207.14463v1)
- **Published**: 2022-07-29 03:56:18+00:00
- **Updated**: 2022-07-29 03:56:18+00:00
- **Authors**: D. F. G. Coelho, R. J. Cintra, F. M. Bayer, S. Kulasekera, A. Madanayake, P. A. C. Martinez, T. L. T. Silveira, R. S. Oliveira, V. S. Dimitrov
- **Comment**: 25 pages, 11 figures, 7 tables
- **Journal**: J. Low Power Electron. Appl. 2018, 8(4), 46
- **Summary**: This paper introduced a matrix parametrization method based on the Loeffler discrete cosine transform (DCT) algorithm. As a result, a new class of eight-point DCT approximations was proposed, capable of unifying the mathematical formalism of several eight-point DCT approximations archived in the literature. Pareto-efficient DCT approximations are obtained through multicriteria optimization, where computational complexity, proximity, and coding performance are considered. Efficient approximations and their scaled 16- and 32-point versions are embedded into image and video encoders, including a JPEG-like codec and H.264/AVC and H.265/HEVC standards. Results are compared to the unmodified standard codecs. Efficient approximations are mapped and implemented on a Xilinx VLX240T FPGA and evaluated for area, speed, and power consumption.



### Fine-grained Retrieval Prompt Tuning
- **Arxiv ID**: http://arxiv.org/abs/2207.14465v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14465v3)
- **Published**: 2022-07-29 04:10:04+00:00
- **Updated**: 2023-03-06 09:45:11+00:00
- **Authors**: Shijie Wang, Jianlong Chang, Zhihui Wang, Haojie Li, Wanli Ouyang, Qi Tian
- **Comment**: Accepted by AAAI 2023
- **Journal**: None
- **Summary**: Fine-grained object retrieval aims to learn discriminative representation to retrieve visually similar objects. However, existing top-performing works usually impose pairwise similarities on the semantic embedding spaces or design a localization sub-network to continually fine-tune the entire model in limited data scenarios, thus resulting in convergence to suboptimal solutions. In this paper, we develop Fine-grained Retrieval Prompt Tuning (FRPT), which steers a frozen pre-trained model to perform the fine-grained retrieval task from the perspectives of sample prompting and feature adaptation. Specifically, FRPT only needs to learn fewer parameters in the prompt and adaptation instead of fine-tuning the entire model, thus solving the issue of convergence to suboptimal solutions caused by fine-tuning the entire model. Technically, a discriminative perturbation prompt (DPP) is introduced and deemed as a sample prompting process, which amplifies and even exaggerates some discriminative elements contributing to category prediction via a content-aware inhomogeneous sampling operation. In this way, DPP can make the fine-grained retrieval task aided by the perturbation prompts close to the solved task during the original pre-training. Thereby, it preserves the generalization and discrimination of representation extracted from input samples. Besides, a category-specific awareness head is proposed and regarded as feature adaptation, which removes the species discrepancies in features extracted by the pre-trained model using category-guided instance normalization. And thus, it makes the optimized features only include the discrepancies among subcategories. Extensive experiments demonstrate that our FRPT with fewer learnable parameters achieves the state-of-the-art performance on three widely-used fine-grained datasets.



### Towards Domain-agnostic Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/2207.14466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14466v1)
- **Published**: 2022-07-29 04:10:22+00:00
- **Updated**: 2022-07-29 04:10:22+00:00
- **Authors**: Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Simon Chen, Chunhua Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Existing depth completion methods are often targeted at a specific sparse depth type, and generalize poorly across task domains. We present a method to complete sparse/semi-dense, noisy, and potentially low-resolution depth maps obtained by various range sensors, including those in modern mobile phones, or by multi-view reconstruction algorithms. Our method leverages a data driven prior in the form of a single image depth prediction network trained on large-scale datasets, the output of which is used as an input to our model. We propose an effective training scheme where we simulate various sparsity patterns in typical task domains. In addition, we design two new benchmarks to evaluate the generalizability and the robustness of depth completion methods. Our simple method shows superior cross-domain generalization ability against state-of-the-art depth completion methods, introducing a practical solution to high quality depth capture on a mobile device. Code is available at: https://github.com/YvanYin/FillDepth.



### Beyond CNNs: Exploiting Further Inherent Symmetries in Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.14472v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.14472v1)
- **Published**: 2022-07-29 04:28:20+00:00
- **Updated**: 2022-07-29 04:28:20+00:00
- **Authors**: Shuchao Pang, Anan Du, Mehmet A. Orgun, Yan Wang, Quan Z. Sheng, Shoujin Wang, Xiaoshui Huang, Zhenmei Yu
- **Comment**: this work was just accepted by IEEE Transactions on Cybernetics on 22
  July 2022. arXiv admin note: substantial text overlap with arXiv:2005.03924
- **Journal**: None
- **Summary**: Automatic tumor or lesion segmentation is a crucial step in medical image analysis for computer-aided diagnosis. Although the existing methods based on Convolutional Neural Networks (CNNs) have achieved the state-of-the-art performance, many challenges still remain in medical tumor segmentation. This is because, although the human visual system can detect symmetries in 2D images effectively, regular CNNs can only exploit translation invariance, overlooking further inherent symmetries existing in medical images such as rotations and reflections. To solve this problem, we propose a novel group equivariant segmentation framework by encoding those inherent symmetries for learning more precise representations. First, kernel-based equivariant operations are devised on each orientation, which allows it to effectively address the gaps of learning symmetries in existing approaches. Then, to keep segmentation networks globally equivariant, we design distinctive group layers with layer-wise symmetry constraints. Finally, based on our novel framework, extensive experiments conducted on real-world clinical data demonstrate that a Group Equivariant Res-UNet (named GER-UNet) outperforms its regular CNN-based counterpart and the state-of-the-art segmentation methods in the tasks of hepatic tumor segmentation, COVID-19 lung infection segmentation and retinal vessel detection. More importantly, the newly built GER-UNet also shows potential in reducing the sample complexity and the redundancy of filters, upgrading current segmentation CNNs and delineating organs on other medical imaging modalities.



### Centrality and Consistency: Two-Stage Clean Samples Identification for Learning with Instance-Dependent Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/2207.14476v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14476v1)
- **Published**: 2022-07-29 04:54:57+00:00
- **Updated**: 2022-07-29 04:54:57+00:00
- **Authors**: Ganlong Zhao, Guanbin Li, Yipeng Qin, Feng Liu, Yizhou Yu
- **Comment**: Accepted to ECCV2022
- **Journal**: None
- **Summary**: Deep models trained with noisy labels are prone to over-fitting and struggle in generalization. Most existing solutions are based on an ideal assumption that the label noise is class-conditional, i.e., instances of the same class share the same noise model, and are independent of features. While in practice, the real-world noise patterns are usually more fine-grained as instance-dependent ones, which poses a big challenge, especially in the presence of inter-class imbalance. In this paper, we propose a two-stage clean samples identification method to address the aforementioned challenge. First, we employ a class-level feature clustering procedure for the early identification of clean samples that are near the class-wise prediction centers. Notably, we address the class imbalance problem by aggregating rare classes according to their prediction entropy. Second, for the remaining clean samples that are close to the ground truth class boundary (usually mixed with the samples with instance-dependent noises), we propose a novel consistency-based classification method that identifies them using the consistency of two classifier heads: the higher the consistency, the larger the probability that a sample is clean. Extensive experiments on several challenging benchmarks demonstrate the superior performance of our method against the state-of-the-art.



### FCSN: Global Context Aware Segmentation by Learning the Fourier Coefficients of Objects in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2207.14477v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.14477v1)
- **Published**: 2022-07-29 04:56:36+00:00
- **Updated**: 2022-07-29 04:56:36+00:00
- **Authors**: Young Seok Jeon, Hongfei Yang, Mengling Feng
- **Comment**: None
- **Journal**: None
- **Summary**: The encoder-decoder model is a commonly used Deep Neural Network (DNN) model for medical image segmentation. Conventional encoder-decoder models make pixel-wise predictions focusing heavily on local patterns around the pixel. This makes it challenging to give segmentation that preserves the object's shape and topology, which often requires an understanding of the global context of the object. In this work, we propose a Fourier Coefficient Segmentation Network~(FCSN) -- a novel DNN-based model that segments an object by learning the complex Fourier coefficients of the object's masks. The Fourier coefficients are calculated by integrating over the whole contour. Therefore, for our model to make a precise estimation of the coefficients, the model is motivated to incorporate the global context of the object, leading to a more accurate segmentation of the object's shape. This global context awareness also makes our model robust to unseen local perturbations during inference, such as additive noise or motion blur that are prevalent in medical images. When FCSN is compared with other state-of-the-art models (UNet+, DeepLabV3+, UNETR) on 3 medical image segmentation tasks (ISIC\_2018, RIM\_CUP, RIM\_DISC), FCSN attains significantly lower Hausdorff scores of 19.14 (6\%), 17.42 (6\%), and 9.16 (14\%) on the 3 tasks, respectively. Moreover, FCSN is lightweight by discarding the decoder module, which incurs significant computational overhead. FCSN only requires 22.2M parameters, 82M and 10M fewer parameters than UNETR and DeepLabV3+. FCSN attains inference and training speeds of 1.6ms/img and 6.3ms/img, that is 8$\times$ and 3$\times$ faster than UNet and UNETR.



### StyleAM: Perception-Oriented Unsupervised Domain Adaption for Non-reference Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2207.14489v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14489v1)
- **Published**: 2022-07-29 05:51:18+00:00
- **Updated**: 2022-07-29 05:51:18+00:00
- **Authors**: Yiting Lu, Xin Li, Jianzhao Liu, Zhibo Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have shown great potential in non-reference image quality assessment (NR-IQA). However, the annotation of NR-IQA is labor-intensive and time-consuming, which severely limits their application especially for authentic images. To relieve the dependence on quality annotation, some works have applied unsupervised domain adaptation (UDA) to NR-IQA. However, the above methods ignore that the alignment space used in classification is sub-optimal, since the space is not elaborately designed for perception. To solve this challenge, we propose an effective perception-oriented unsupervised domain adaptation method StyleAM for NR-IQA, which transfers sufficient knowledge from label-rich source domain data to label-free target domain images via Style Alignment and Mixup. Specifically, we find a more compact and reliable space i.e., feature style space for perception-oriented UDA based on an interesting/amazing observation, that the feature style (i.e., the mean and variance) of the deep layer in DNNs is exactly associated with the quality score in NR-IQA. Therefore, we propose to align the source and target domains in a more perceptual-oriented space i.e., the feature style space, to reduce the intervention from other quality-irrelevant feature factors. Furthermore, to increase the consistency between quality score and its feature style, we also propose a novel feature augmentation strategy Style Mixup, which mixes the feature styles (i.e., the mean and variance) before the last layer of DNNs together with mixing their labels. Extensive experimental results on two typical cross-domain settings (i.e., synthetic to authentic, and multiple distortions to one distortion) have demonstrated the effectiveness of our proposed StyleAM on NR-IQA.



### Conservative Generator, Progressive Discriminator: Coordination of Adversaries in Few-shot Incremental Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2207.14491v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14491v1)
- **Published**: 2022-07-29 06:00:29+00:00
- **Updated**: 2022-07-29 06:00:29+00:00
- **Authors**: Chaerin Kong, Nojun Kwak
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: The capacity to learn incrementally from an online stream of data is an envied trait of human learners, as deep neural networks typically suffer from catastrophic forgetting and stability-plasticity dilemma. Several works have previously explored incremental few-shot learning, a task with greater challenges due to data constraint, mostly in classification setting with mild success. In this work, we study the underrepresented task of generative incremental few-shot learning. To effectively handle the inherent challenges of incremental learning and few-shot learning, we propose a novel framework named ConPro that leverages the two-player nature of GANs. Specifically, we design a conservative generator that preserves past knowledge in parameter and compute efficient manner, and a progressive discriminator that learns to reason semantic distances between past and present task samples, minimizing overfitting with few data points and pursuing good forward transfer. We present experiments to validate the effectiveness of ConPro.



### Reference-Guided Texture and Structure Inference for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2207.14498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14498v1)
- **Published**: 2022-07-29 06:26:03+00:00
- **Updated**: 2022-07-29 06:26:03+00:00
- **Authors**: Taorong Liu, Liang Liao, Zheng Wang, Shin'ichi Satoh
- **Comment**: IEEE International Conference on Image Processing(ICIP 2022)
- **Journal**: None
- **Summary**: Existing learning-based image inpainting methods are still in challenge when facing complex semantic environments and diverse hole patterns. The prior information learned from the large scale training data is still insufficient for these situations. Reference images captured covering the same scenes share similar texture and structure priors with the corrupted images, which offers new prospects for the image inpainting tasks. Inspired by this, we first build a benchmark dataset containing 10K pairs of input and reference images for reference-guided inpainting. Then we adopt an encoder-decoder structure to separately infer the texture and structure features of the input image considering their pattern discrepancy of texture and structure during inpainting. A feature alignment module is further designed to refine these features of the input image with the guidance of a reference image. Both quantitative and qualitative evaluations demonstrate the superiority of our method over the state-of-the-art methods in terms of completing complex holes.



### Class-Difficulty Based Methods for Long-Tailed Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.14499v2
- **DOI**: 10.1007/s11263-022-01643-3
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.14499v2)
- **Published**: 2022-07-29 06:33:22+00:00
- **Updated**: 2022-08-22 06:54:24+00:00
- **Authors**: Saptarshi Sinha, Hiroki Ohashi, Katsuyuki Nakamura
- **Comment**: Published in IJCV. Paper URL: https://rdcu.be/cTWem
- **Journal**: International Journal of Computer Vision (2022)
- **Summary**: Long-tailed datasets are very frequently encountered in real-world use cases where few classes or categories (known as majority or head classes) have higher number of data samples compared to the other classes (known as minority or tail classes). Training deep neural networks on such datasets gives results biased towards the head classes. So far, researchers have come up with multiple weighted loss and data re-sampling techniques in efforts to reduce the bias. However, most of such techniques assume that the tail classes are always the most difficult classes to learn and therefore need more weightage or attention. Here, we argue that the assumption might not always hold true. Therefore, we propose a novel approach to dynamically measure the instantaneous difficulty of each class during the training phase of the model. Further, we use the difficulty measures of each class to design a novel weighted loss technique called `class-wise difficulty based weighted (CDB-W) loss' and a novel data sampling technique called `class-wise difficulty based sampling (CDB-S)'. To verify the wide-scale usability of our CDB methods, we conducted extensive experiments on multiple tasks such as image classification, object detection, instance segmentation and video-action classification. Results verified that CDB-W loss and CDB-S could achieve state-of-the-art results on many class-imbalanced datasets such as ImageNet-LT, LVIS and EGTEA, that resemble real-world use cases.



### A Transfer Learning-Based Approach to Marine Vessel Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2207.14500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14500v1)
- **Published**: 2022-07-29 06:36:10+00:00
- **Updated**: 2022-07-29 06:36:10+00:00
- **Authors**: Guangmiao Zeng, Wanneng Yu, Rongjie Wang, Anhui Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Marine vessel re-identification technology is an important component of intelligent shipping systems and an important part of the visual perception tasks required for marine surveillance. However, unlike the situation on land, the maritime environment is complex and variable with fewer samples, and it is more difficult to perform vessel re-identification at sea. Therefore, this paper proposes a transfer dynamic alignment algorithm and simulates the swaying situation of vessels at sea, using a well-camouflaged and similar warship as the test target to improve the recognition difficulty and thus cope with the impact caused by complex sea conditions, and discusses the effect of different types of vessels as transfer objects. The experimental results show that the improved algorithm improves the mean average accuracy (mAP) by 10.2% and the first hit rate (Rank1) by 4.9% on average.



### GPU-accelerated SIFT-aided source identification of stabilized videos
- **Arxiv ID**: http://arxiv.org/abs/2207.14507v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.14507v1)
- **Published**: 2022-07-29 07:01:31+00:00
- **Updated**: 2022-07-29 07:01:31+00:00
- **Authors**: Andrea Montibeller, Cecilia Pasquini, Giulia Boato, Stefano Dell'Anna, Fernando Pérez-González
- **Comment**: None
- **Journal**: None
- **Summary**: Video stabilization is an in-camera processing commonly applied by modern acquisition devices. While significantly improving the visual quality of the resulting videos, it has been shown that such operation typically hinders the forensic analysis of video signals. In fact, the correct identification of the acquisition source usually based on Photo Response non-Uniformity (PRNU) is subject to the estimation of the transformation applied to each frame in the stabilization phase. A number of techniques have been proposed for dealing with this problem, which however typically suffer from a high computational burden due to the grid search in the space of inversion parameters. Our work attempts to alleviate these shortcomings by exploiting the parallelization capabilities of Graphics Processing Units (GPUs), typically used for deep learning applications, in the framework of stabilised frames inversion. Moreover, we propose to exploit SIFT features {to estimate the camera momentum and} %to identify less stabilized temporal segments, thus enabling a more accurate identification analysis, and to efficiently initialize the frame-wise parameter search of consecutive frames. Experiments on a consolidated benchmark dataset confirm the effectiveness of the proposed approach in reducing the required computational time and improving the source identification accuracy. {The code is available at \url{https://github.com/AMontiB/GPU-PRNU-SIFT}}.



### Transfer Learning for Segmentation Problems: Choose the Right Encoder and Skip the Decoder
- **Arxiv ID**: http://arxiv.org/abs/2207.14508v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.14508v1)
- **Published**: 2022-07-29 07:02:05+00:00
- **Updated**: 2022-07-29 07:02:05+00:00
- **Authors**: Jonas Dippel, Matthias Lenga, Thomas Goerttler, Klaus Obermayer, Johannes Höhne
- **Comment**: None
- **Journal**: None
- **Summary**: It is common practice to reuse models initially trained on different data to increase downstream task performance. Especially in the computer vision domain, ImageNet-pretrained weights have been successfully used for various tasks. In this work, we investigate the impact of transfer learning for segmentation problems, being pixel-wise classification problems that can be tackled with encoder-decoder architectures. We find that transfer learning the decoder does not help downstream segmentation tasks, while transfer learning the encoder is truly beneficial. We demonstrate that pretrained weights for a decoder may yield faster convergence, but they do not improve the overall model performance as one can obtain equivalent results with randomly initialized decoders. However, we show that it is more effective to reuse encoder weights trained on a segmentation or reconstruction task than reusing encoder weights trained on classification tasks. This finding implicates that using ImageNet-pretrained encoders for downstream segmentation problems is suboptimal. We also propose a contrastive self-supervised approach with multiple self-reconstruction tasks, which provides encoders that are suitable for transfer learning in segmentation problems in the absence of segmentation labels.



### Uncertainty-Driven Action Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2207.14513v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14513v1)
- **Published**: 2022-07-29 07:21:15+00:00
- **Updated**: 2022-07-29 07:21:15+00:00
- **Authors**: Caixia Zhou, Yaping Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic action quality assessment (AQA) has attracted more interests due to its wide applications. However, existing AQA methods usually employ the multi-branch models to generate multiple scores, which is not flexible for dealing with a variable number of judges. In this paper, we propose a novel Uncertainty-Driven AQA (UD-AQA) model to generate multiple predictions only using one single branch. Specifically, we design a CVAE (Conditional Variational Auto-Encoder) based module to encode the uncertainty, where multiple scores can be produced by sampling from the learned latent space multiple times. Moreover, we output the estimation of uncertainty and utilize the predicted uncertainty to re-weight AQA regression loss, which can reduce the contributions of uncertain samples for training. We further design an uncertainty-guided training strategy to dynamically adjust the learning order of the samples from low uncertainty to high uncertainty. The experiments show that our proposed method achieves new state-of-the-art results on the Olympic events MTL-AQA and surgical skill JIGSAWS datasets.



### Evaluating the Practicality of Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2207.14524v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.14524v1)
- **Published**: 2022-07-29 07:45:32+00:00
- **Updated**: 2022-07-29 07:45:32+00:00
- **Authors**: Hongjiu Yu, Qiancheng Sun, Jin Hu, Xingyuan Xue, Jixiang Luo, Dailan He, Yilong Li, Pengbo Wang, Yuanyuan Wang, Yaxu Dai, Yan Wang, Hongwei Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Learned image compression has achieved extraordinary rate-distortion performance in PSNR and MS-SSIM compared to traditional methods. However, it suffers from intensive computation, which is intolerable for real-world applications and leads to its limited industrial application for now. In this paper, we introduce neural architecture search (NAS) to designing more efficient networks with lower latency, and leverage quantization to accelerate the inference process. Meanwhile, efforts in engineering like multi-threading and SIMD have been made to improve efficiency. Optimized using a hybrid loss of PSNR and MS-SSIM for better visual quality, we obtain much higher MS-SSIM than JPEG, JPEG XL and AVIF over all bit rates, and PSNR between that of JPEG XL and AVIF. Our software implementation of LIC achieves comparable or even faster inference speed compared to jpeg-turbo while being multiple times faster than JPEG XL and AVIF. Besides, our implementation of LIC reaches stunning throughput of 145 fps for encoding and 208 fps for decoding on a Tesla T4 GPU for 1080p images. On CPU, the latency of our implementation is comparable with JPEG XL.



### Curriculum Learning for Data-Efficient Vision-Language Alignment
- **Arxiv ID**: http://arxiv.org/abs/2207.14525v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.14525v1)
- **Published**: 2022-07-29 07:45:56+00:00
- **Updated**: 2022-07-29 07:45:56+00:00
- **Authors**: Tejas Srinivasan, Xiang Ren, Jesse Thomason
- **Comment**: None
- **Journal**: None
- **Summary**: Aligning image and text encoders from scratch using contrastive learning requires large amounts of paired image-text data. We alleviate this need by aligning individually pre-trained language and vision representation models using a much smaller amount of paired data, augmented with a curriculum learning algorithm to learn fine-grained vision-language alignments. TOnICS (Training with Ontology-Informed Contrastive Sampling) initially samples minibatches whose image-text pairs contain a wide variety of objects to learn object-level alignment, and progressively samples minibatches where all image-text pairs contain the same object to learn finer-grained contextual alignment. Aligning pre-trained BERT and VinVL models to each other using TOnICS outperforms CLIP on downstream zero-shot image retrieval while using less than 1% as much training data.



### Contrastive Pre-training of Spatial-Temporal Trajectory Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2207.14539v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.14539v1)
- **Published**: 2022-07-29 08:16:20+00:00
- **Updated**: 2022-07-29 08:16:20+00:00
- **Authors**: Yan Lin, Huaiyu Wan, Shengnan Guo, Youfang Lin
- **Comment**: 13 pages, 11 figures, submitted to IEEE Trans. on Knowledge and Data
  Engineering
- **Journal**: TKDE-2022-07-1129
- **Summary**: Pre-training trajectory embeddings is a fundamental and critical procedure in spatial-temporal trajectory mining, and is beneficial for a wide range of downstream tasks. The key for generating effective trajectory embeddings is to extract high-level travel semantics from trajectories, including movement patterns and travel purposes, with consideration of the trajectories' long-term spatial-temporal correlations. Despite the existing efforts, there are still major challenges in pre-training trajectory embeddings. First, commonly used generative pretext tasks are not suitable for extracting high-level semantics from trajectories. Second, existing data augmentation methods fit badly on trajectory datasets. Third, current encoder designs fail to fully incorporate long-term spatial-temporal correlations hidden in trajectories. To tackle these challenges, we propose a novel Contrastive Spatial-Temporal Trajectory Embedding (CSTTE) model for learning comprehensive trajectory embeddings. CSTTE adopts the contrastive learning framework so that its pretext task is robust to noise. A specially designed data augmentation method for trajectories is coupled with the contrastive pretext task to preserve the high-level travel semantics. We also build an efficient spatial-temporal trajectory encoder to efficiently and comprehensively model the long-term spatial-temporal correlations in trajectories. Extensive experiments on two downstream tasks and three real-world datasets prove the superiority of our model compared with the existing trajectory embedding methods.



### A One-Shot Reparameterization Method for Reducing the Loss of Tile Pruning on DNNs
- **Arxiv ID**: http://arxiv.org/abs/2207.14545v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.14545v1)
- **Published**: 2022-07-29 08:27:15+00:00
- **Updated**: 2022-07-29 08:27:15+00:00
- **Authors**: Yanchen Li, Qingzhong Ai, Fumihiko Ino
- **Comment**: Presented at IJCNN 2022, oral
- **Journal**: None
- **Summary**: Recently, tile pruning has been widely studied to accelerate the inference of deep neural networks (DNNs). However, we found that the loss due to tile pruning, which can eliminate important elements together with unimportant elements, is large on trained DNNs. In this study, we propose a one-shot reparameterization method, called TileTrans, to reduce the loss of tile pruning. Specifically, we repermute the rows or columns of the weight matrix such that the model architecture can be kept unchanged after reparameterization. This repermutation realizes the reparameterization of the DNN model without any retraining. The proposed reparameterization method combines important elements into the same tile; thus, preserving the important elements after the tile pruning. Furthermore, TileTrans can be seamlessly integrated into existing tile pruning methods because it is a pre-processing method executed before pruning, which is orthogonal to most existing methods. The experimental results demonstrate that our method is essential in reducing the loss of tile pruning on DNNs. Specifically, the accuracy is improved by up to 17% for AlexNet while 5% for ResNet-34, where both models are pre-trained on ImageNet.



### ScaleFormer: Revisiting the Transformer-based Backbones from a Scale-wise Perspective for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2207.14552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14552v1)
- **Published**: 2022-07-29 08:55:00+00:00
- **Updated**: 2022-07-29 08:55:00+00:00
- **Authors**: Huimin Huang, Shiao Xie1, Lanfen Lin, Yutaro Iwamoto, Xianhua Han, Yen-Wei Chen, Ruofeng Tong
- **Comment**: Accepted to IJCAI 2022
- **Journal**: None
- **Summary**: Recently, a variety of vision transformers have been developed as their capability of modeling long-range dependency. In current transformer-based backbones for medical image segmentation, convolutional layers were replaced with pure transformers, or transformers were added to the deepest encoder to learn global context. However, there are mainly two challenges in a scale-wise perspective: (1) intra-scale problem: the existing methods lacked in extracting local-global cues in each scale, which may impact the signal propagation of small objects; (2) inter-scale problem: the existing methods failed to explore distinctive information from multiple scales, which may hinder the representation learning from objects with widely variable size, shape and location. To address these limitations, we propose a novel backbone, namely ScaleFormer, with two appealing designs: (1) A scale-wise intra-scale transformer is designed to couple the CNN-based local features with the transformer-based global cues in each scale, where the row-wise and column-wise global dependencies can be extracted by a lightweight Dual-Axis MSA. (2) A simple and effective spatial-aware inter-scale transformer is designed to interact among consensual regions in multiple scales, which can highlight the cross-scale dependency and resolve the complex scale variations. Experimental results on different benchmarks demonstrate that our Scale-Former outperforms the current state-of-the-art methods. The code is publicly available at: https://github.com/ZJUGiveLab/ScaleFormer.



### Prompting for Multi-Modal Tracking
- **Arxiv ID**: http://arxiv.org/abs/2207.14571v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14571v2)
- **Published**: 2022-07-29 09:35:02+00:00
- **Updated**: 2022-08-01 02:50:52+00:00
- **Authors**: Jinyu Yang, Zhe Li, Feng Zheng, Aleš Leonardis, Jingkuan Song
- **Comment**: Accepted at ACMMM 2022
- **Journal**: None
- **Summary**: Multi-modal tracking gains attention due to its ability to be more accurate and robust in complex scenarios compared to traditional RGB-based tracking. Its key lies in how to fuse multi-modal data and reduce the gap between modalities. However, multi-modal tracking still severely suffers from data deficiency, thus resulting in the insufficient learning of fusion modules. Instead of building such a fusion module, in this paper, we provide a new perspective on multi-modal tracking by attaching importance to the multi-modal visual prompts. We design a novel multi-modal prompt tracker (ProTrack), which can transfer the multi-modal inputs to a single modality by the prompt paradigm. By best employing the tracking ability of pre-trained RGB trackers learning at scale, our ProTrack can achieve high-performance multi-modal tracking by only altering the inputs, even without any extra training on multi-modal data. Extensive experiments on 5 benchmark datasets demonstrate the effectiveness of the proposed ProTrack.



### Image Augmentation for Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2207.14580v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.14580v1)
- **Published**: 2022-07-29 09:53:21+00:00
- **Updated**: 2022-07-29 09:53:21+00:00
- **Authors**: Oluwadara Adedeji, Peter Owoade, Opeyemi Ajayi, Olayiwola Arowolo
- **Comment**: 14 pages, 4 figures, 6 tables. Research project for Introduction to
  Deep Learning (11785) at Carnegie Mellon University
- **Journal**: None
- **Summary**: This study proposes the use of generative models (GANs) for augmenting the EuroSAT dataset for the Land Use and Land Cover (LULC) Classification task. We used DCGAN and WGAN-GP to generate images for each class in the dataset. We then explored the effect of augmenting the original dataset by about 10% in each case on model performance. The choice of GAN architecture seems to have no apparent effect on the model performance. However, a combination of geometric augmentation and GAN-generated images improved baseline results. Our study shows that GANs augmentation can improve the generalizability of deep classification models on satellite images.



### Learning Prototype via Placeholder for Zero-shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.14581v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14581v1)
- **Published**: 2022-07-29 09:56:44+00:00
- **Updated**: 2022-07-29 09:56:44+00:00
- **Authors**: Zaiquan Yang, Yang Liu, Wenjia Xu, Chong Huang, Lei Zhou, Chao Tong
- **Comment**: IJCAI2022
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) aims to recognize unseen classes by exploiting semantic descriptions shared between seen classes and unseen classes. Current methods show that it is effective to learn visual-semantic alignment by projecting semantic embeddings into the visual space as class prototypes. However, such a projection function is only concerned with seen classes. When applied to unseen classes, the prototypes often perform suboptimally due to domain shift. In this paper, we propose to learn prototypes via placeholders, termed LPL, to eliminate the domain shift between seen and unseen classes. Specifically, we combine seen classes to hallucinate new classes which play as placeholders of the unseen classes in the visual and semantic space. Placed between seen classes, the placeholders encourage prototypes of seen classes to be highly dispersed. And more space is spared for the insertion of well-separated unseen ones. Empirically, well-separated prototypes help counteract visual-semantic misalignment caused by domain shift. Furthermore, we exploit a novel semantic-oriented fine-tuning to guarantee the semantic reliability of placeholders. Extensive experiments on five benchmark datasets demonstrate the significant performance gain of LPL over the state-of-the-art methods. Code is available at https://github.com/zaiquanyang/LPL.



### Deep Deformable 3D Caricatures with Learned Shape Control
- **Arxiv ID**: http://arxiv.org/abs/2207.14593v1
- **DOI**: 10.1145/3528233.3530748
- **Categories**: **cs.CV**, cs.GR, I.3.4; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2207.14593v1)
- **Published**: 2022-07-29 10:21:27+00:00
- **Updated**: 2022-07-29 10:21:27+00:00
- **Authors**: Yucheol Jung, Wonjong Jang, Soongjin Kim, Jiaolong Yang, Xin Tong, Seungyong Lee
- **Comment**: ACM SIGGRAPH 2022. For the project page, see
  https://ycjungsubhuman.github.io/DeepDeformable3DCaricatures
- **Journal**: None
- **Summary**: A 3D caricature is an exaggerated 3D depiction of a human face. The goal of this paper is to model the variations of 3D caricatures in a compact parameter space so that we can provide a useful data-driven toolkit for handling 3D caricature deformations. To achieve the goal, we propose an MLP-based framework for building a deformable surface model, which takes a latent code and produces a 3D surface. In the framework, a SIREN MLP models a function that takes a 3D position on a fixed template surface and returns a 3D displacement vector for the input position. We create variations of 3D surfaces by learning a hypernetwork that takes a latent code and produces the parameters of the MLP. Once learned, our deformable model provides a nice editing space for 3D caricatures, supporting label-based semantic editing and point-handle-based deformation, both of which produce highly exaggerated and natural 3D caricature shapes. We also demonstrate other applications of our deformable model, such as automatic 3D caricature creation.



### WISE: Whitebox Image Stylization by Example-based Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.14606v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2207.14606v1)
- **Published**: 2022-07-29 10:59:54+00:00
- **Updated**: 2022-07-29 10:59:54+00:00
- **Authors**: Winfried Lötzsch, Max Reimann, Martin Büssemeyer, Amir Semmo, Jürgen Döllner, Matthias Trapp
- **Comment**: Accepted to ECCV
- **Journal**: None
- **Summary**: Image-based artistic rendering can synthesize a variety of expressive styles using algorithmic image filtering. In contrast to deep learning-based methods, these heuristics-based filtering techniques can operate on high-resolution images, are interpretable, and can be parameterized according to various design aspects. However, adapting or extending these techniques to produce new styles is often a tedious and error-prone task that requires expert knowledge. We propose a new paradigm to alleviate this problem: implementing algorithmic image filtering techniques as differentiable operations that can learn parametrizations aligned to certain reference styles. To this end, we present WISE, an example-based image-processing system that can handle a multitude of stylization techniques, such as watercolor, oil or cartoon stylization, within a common framework. By training parameter prediction networks for global and local filter parameterizations, we can simultaneously adapt effects to reference styles and image content, e.g., to enhance facial features. Our method can be optimized in a style-transfer framework or learned in a generative-adversarial setting for image-to-image translation. We demonstrate that jointly training an XDoG filter and a CNN for postprocessing can achieve comparable results to a state-of-the-art GAN-based method.



### Computational complexity reduction of deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/2207.14620v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CC, cs.CV, cs.NE, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2207.14620v1)
- **Published**: 2022-07-29 11:41:15+00:00
- **Updated**: 2022-07-29 11:41:15+00:00
- **Authors**: Mee Seong Im, Venkat R. Dasari
- **Comment**: 10 pages, 9 figures
- **Journal**: Mathematica Militaris, 25 (2022), no. 1, 1-10
- **Summary**: Deep neural networks (DNN) have been widely used and play a major role in the field of computer vision and autonomous navigation. However, these DNNs are computationally complex and their deployment over resource-constrained platforms is difficult without additional optimizations and customization.   In this manuscript, we describe an overview of DNN architecture and propose methods to reduce computational complexity in order to accelerate training and inference speeds to fit them on edge computing platforms with low computational resources.



### A Graph Theoretic Exploration of Coronary Vascular Trees
- **Arxiv ID**: http://arxiv.org/abs/2207.14624v1
- **DOI**: None
- **Categories**: **cs.DM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.14624v1)
- **Published**: 2022-07-29 11:50:35+00:00
- **Updated**: 2022-07-29 11:50:35+00:00
- **Authors**: Jay Aodh Mackenzie
- **Comment**: 9 pages, 15 figures
- **Journal**: None
- **Summary**: The aim of this study was to automate the generation of small coronary vascular networks from large point clouds that represent the coronary arterial network. Smaller networks that can be generated in a predictable manner can be used to assess the impact of network morphometry on, for example, blood flow in hemodynamic simulations. We develop a set of algorithms for generating coronary vascular networks from large point clouds. These algorithms sort the point cloud, simplify its network structure without information loss, and produce subgraphs based on given, physiologically meaningful parameters. The data were originally collected from optical fluorescence cryomicrotome images and processed before their use here.



### Content-Aware Differential Privacy with Conditional Invertible Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2207.14625v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG, J.3 I.4.0 J.3 I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2207.14625v1)
- **Published**: 2022-07-29 11:52:16+00:00
- **Updated**: 2022-07-29 11:52:16+00:00
- **Authors**: Malte Tölle, Ullrich Köthe, Florian André, Benjamin Meder, Sandy Engelhardt
- **Comment**: Accepted at 3rd DeCaF Workshop (MICCAI22)
- **Journal**: None
- **Summary**: Differential privacy (DP) has arisen as the gold standard in protecting an individual's privacy in datasets by adding calibrated noise to each data sample. While the application to categorical data is straightforward, its usability in the context of images has been limited. Contrary to categorical data the meaning of an image is inherent in the spatial correlation of neighboring pixels making the simple application of noise infeasible. Invertible Neural Networks (INN) have shown excellent generative performance while still providing the ability to quantify the exact likelihood. Their principle is based on transforming a complicated distribution into a simple one e.g. an image into a spherical Gaussian. We hypothesize that adding noise to the latent space of an INN can enable differentially private image modification. Manipulation of the latent space leads to a modified image while preserving important details. Further, by conditioning the INN on meta-data provided with the dataset we aim at leaving dimensions important for downstream tasks like classification untouched while altering other parts that potentially contain identifying information. We term our method content-aware differential privacy (CADP). We conduct experiments on publicly available benchmarking datasets as well as dedicated medical ones. In addition, we show the generalizability of our method to categorical data. The source code is publicly available at https://github.com/Cardio-AI/CADP.



### Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2207.14626v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.14626v2)
- **Published**: 2022-07-29 11:52:41+00:00
- **Updated**: 2022-11-21 08:21:27+00:00
- **Authors**: Ozan Özdenizci, Robert Legenstein
- **Comment**: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
- **Journal**: None
- **Summary**: Image restoration under adverse weather conditions has been of significant interest for various computer vision applications. Recent successful methods rely on the current progress in deep neural network architectural designs (e.g., with vision transformers). Motivated by the recent progress achieved with state-of-the-art conditional generative models, we present a novel patch-based image restoration algorithm based on denoising diffusion probabilistic models. Our patch-based diffusion modeling approach enables size-agnostic image restoration by using a guided denoising process with smoothed noise estimates across overlapping patches during inference. We empirically evaluate our model on benchmark datasets for image desnowing, combined deraining and dehazing, and raindrop removal. We demonstrate our approach to achieve state-of-the-art performances on both weather-specific and multi-weather image restoration, and experimentally show strong generalization to real-world test images.



### SYNTA: A novel approach for deep learning-based image analysis in muscle histopathology using photo-realistic synthetic data
- **Arxiv ID**: http://arxiv.org/abs/2207.14650v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.14650v2)
- **Published**: 2022-07-29 12:50:32+00:00
- **Updated**: 2022-08-03 14:52:20+00:00
- **Authors**: Leonid Mill, Oliver Aust, Jochen A. Ackermann, Philipp Burger, Monica Pascual, Katrin Palumbo-Zerr, Gerhard Krönke, Stefan Uderhardt, Georg Schett, Christoph S. Clemen, Rolf Schröder, Christian Holtzhausen, Samir Jabari, Andreas Maier, Anika Grüneboom
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial intelligence (AI), machine learning, and deep learning (DL) methods are becoming increasingly important in the field of biomedical image analysis. However, to exploit the full potential of such methods, a representative number of experimentally acquired images containing a significant number of manually annotated objects is needed as training data. Here we introduce SYNTA (synthetic data) as a novel approach for the generation of synthetic, photo-realistic, and highly complex biomedical images as training data for DL systems. We show the versatility of our approach in the context of muscle fiber and connective tissue analysis in histological sections. We demonstrate that it is possible to perform robust and expert-level segmentation tasks on previously unseen real-world data, without the need for manual annotations using synthetic training data alone. Being a fully parametric technique, our approach poses an interpretable and controllable alternative to Generative Adversarial Networks (GANs) and has the potential to significantly accelerate quantitative image analysis in a variety of biomedical applications in microscopy and beyond.



### Multimodal SuperCon: Classifier for Drivers of Deforestation in Indonesia
- **Arxiv ID**: http://arxiv.org/abs/2207.14656v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.14656v1)
- **Published**: 2022-07-29 13:03:31+00:00
- **Updated**: 2022-07-29 13:03:31+00:00
- **Authors**: Bella Septina Ika Hartanti, Valentino Vito, Aniati Murni Arymurthy, Andie Setiyoko
- **Comment**: None
- **Journal**: None
- **Summary**: Deforestation is one of the contributing factors to climate change. Climate change has a serious impact on human life, and it occurs due to emission of greenhouse gases, such as carbon dioxide, to the atmosphere. It is important to know the causes of deforestation for mitigation efforts, but there is a lack of data-driven research studies to predict these deforestation drivers. In this work, we propose a contrastive learning architecture, called Multimodal SuperCon, for classifying drivers of deforestation in Indonesia using satellite images obtained from Landsat 8. Multimodal SuperCon is an architecture which combines contrastive learning and multimodal fusion to handle the available deforestation dataset. Our proposed model outperforms previous work on driver classification, giving a 7% improvement in accuracy in comparison to a state-of-the-art rotation equivariant model for the same task.



### Matching with AffNet based rectifications
- **Arxiv ID**: http://arxiv.org/abs/2207.14660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14660v1)
- **Published**: 2022-07-29 13:06:18+00:00
- **Updated**: 2022-07-29 13:06:18+00:00
- **Authors**: Václav Vávra, Dmytro Mishkin, Jiří Matas
- **Comment**: 13 pages, 9 figures
- **Journal**: None
- **Summary**: We consider the problem of two-view matching under significant viewpoint changes with view synthesis. We propose two novel methods, minimizing the view synthesis overhead. The first one, named DenseAffNet, uses dense affine shapes estimates from AffNet, which allows it to partition the image, rectifying each partition with just a single affine map. The second one, named DepthAffNet, combines information from depth maps and affine shapes estimates to produce different sets of rectifying affine maps for different image partitions. DenseAffNet is faster than the state-of-the-art and more accurate on generic scenes. DepthAffNet is on par with the state of the art on scenes containing large planes. The evaluation is performed on 3 public datasets - EVD Dataset, Strong ViewPoint Changes Dataset and IMC Phototourism Dataset.



### Going Off-Grid: Continuous Implicit Neural Representations for 3D Vascular Modeling
- **Arxiv ID**: http://arxiv.org/abs/2207.14663v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.14663v2)
- **Published**: 2022-07-29 13:08:35+00:00
- **Updated**: 2022-09-16 12:01:33+00:00
- **Authors**: Dieuwertje Alblas, Christoph Brune, Kak Khee Yeung, Jelmer M. Wolterink
- **Comment**: MICCAI STACOM 2022
- **Journal**: None
- **Summary**: Personalised 3D vascular models are valuable for diagnosis, prognosis and treatment planning in patients with cardiovascular disease. Traditionally, such models have been constructed with explicit representations such as meshes and voxel masks, or implicit representations such as radial basis functions or atomic (tubular) shapes. Here, we propose to represent surfaces by the zero level set of their signed distance function (SDF) in a differentiable implicit neural representation (INR). This allows us to model complex vascular structures with a representation that is implicit, continuous, light-weight, and easy to integrate with deep learning algorithms. We here demonstrate the potential of this approach with three practical examples. First, we obtain an accurate and watertight surface for an abdominal aortic aneurysm (AAA) from CT images and show robust fitting from as little as 200 points on the surface. Second, we simultaneously fit nested vessel walls in a single INR without intersections. Third, we show how 3D models of individual arteries can be smoothly blended into a single watertight surface. Our results show that INRs are a flexible representation with potential for minimally interactive annotation and manipulation of complex vascular structures.



### High Dynamic Range and Super-Resolution from Raw Image Bursts
- **Arxiv ID**: http://arxiv.org/abs/2207.14671v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.14671v1)
- **Published**: 2022-07-29 13:31:28+00:00
- **Updated**: 2022-07-29 13:31:28+00:00
- **Authors**: Bruno Lecouat, Thomas Eboli, Jean Ponce, Julien Mairal
- **Comment**: Accepted to Siggraph 2022 Technical Papers program
- **Journal**: None
- **Summary**: Photographs captured by smartphones and mid-range cameras have limited spatial resolution and dynamic range, with noisy response in underexposed regions and color artefacts in saturated areas. This paper introduces the first approach (to the best of our knowledge) to the reconstruction of high-resolution, high-dynamic range color images from raw photographic bursts captured by a handheld camera with exposure bracketing. This method uses a physically-accurate model of image formation to combine an iterative optimization algorithm for solving the corresponding inverse problem with a learned image representation for robust alignment and a learned natural image prior. The proposed algorithm is fast, with low memory requirements compared to state-of-the-art learning-based approaches to image restoration, and features that are learned end to end from synthetic yet realistic data. Extensive experiments demonstrate its excellent performance with super-resolution factors of up to $\times 4$ on real photographs taken in the wild with hand-held cameras, and high robustness to low-light conditions, noise, camera shake, and moderate object motion.



### Enhanced Laser-Scan Matching with Online Error Estimation for Highway and Tunnel Driving
- **Arxiv ID**: http://arxiv.org/abs/2207.14674v1
- **DOI**: 10.33012/2022.18249
- **Categories**: **cs.RO**, cs.CV, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2207.14674v1)
- **Published**: 2022-07-29 13:42:32+00:00
- **Updated**: 2022-07-29 13:42:32+00:00
- **Authors**: Matthew McDermott, Jason Rife
- **Comment**: None
- **Journal**: Proceedings of the 2022 International Technical Meeting of The
  Institute of Navigation
- **Summary**: Lidar data can be used to generate point clouds for the navigation of autonomous vehicles or mobile robotics platforms. Scan matching, the process of estimating the rigid transformation that best aligns two point clouds, is the basis for lidar odometry, a form of dead reckoning. Lidar odometry is particularly useful when absolute sensors, like GPS, are not available. Here we propose the Iterative Closest Ellipsoidal Transform (ICET), a scan matching algorithm which provides two novel improvements over the current state-of-the-art Normal Distributions Transform (NDT). Like NDT, ICET decomposes lidar data into voxels and fits a Gaussian distribution to the points within each voxel. The first innovation of ICET reduces geometric ambiguity along large flat surfaces by suppressing the solution along those directions. The second innovation of ICET is to infer the output error covariance associated with the position and orientation transformation between successive point clouds; the error covariance is particularly useful when ICET is incorporated into a state-estimation routine such as an extended Kalman filter. We constructed a simulation to compare the performance of ICET and NDT in 2D space both with and without geometric ambiguity and found that ICET produces superior estimates while accurately predicting solution accuracy.



### Global-Local Self-Distillation for Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.14676v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14676v2)
- **Published**: 2022-07-29 13:50:09+00:00
- **Updated**: 2022-10-12 21:12:54+00:00
- **Authors**: Tim Lebailly, Tinne Tuytelaars
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: The downstream accuracy of self-supervised methods is tightly linked to the proxy task solved during training and the quality of the gradients extracted from it. Richer and more meaningful gradients updates are key to allow self-supervised methods to learn better and in a more efficient manner. In a typical self-distillation framework, the representation of two augmented images are enforced to be coherent at the global level. Nonetheless, incorporating local cues in the proxy task can be beneficial and improve the model accuracy on downstream tasks. This leads to a dual objective in which, on the one hand, coherence between global-representations is enforced and on the other, coherence between local-representations is enforced. Unfortunately, an exact correspondence mapping between two sets of local-representations does not exist making the task of matching local-representations from one augmentation to another non-trivial. We propose to leverage the spatial information in the input images to obtain geometric matchings and compare this geometric approach against previous methods based on similarity matchings. Our study shows that not only 1) geometric matchings perform better than similarity based matchings in low-data regimes but also 2) that similarity based matchings are highly hurtful in low-data regimes compared to the vanilla baseline without local self-distillation. The code is available at https://github.com/tileb1/global-local-self-distillation.



### AlphaVC: High-Performance and Efficient Learned Video Compression
- **Arxiv ID**: http://arxiv.org/abs/2207.14678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14678v1)
- **Published**: 2022-07-29 13:52:44+00:00
- **Updated**: 2022-07-29 13:52:44+00:00
- **Authors**: Yibo Shi, Yunying Ge, Jing Wang, Jue Mao
- **Comment**: ECCV2022
- **Journal**: None
- **Summary**: Recently, learned video compression has drawn lots of attention and show a rapid development trend with promising results. However, the previous works still suffer from some criticial issues and have a performance gap with traditional compression standards in terms of widely used PSNR metric. In this paper, we propose several techniques to effectively improve the performance. First, to address the problem of accumulative error, we introduce a conditional-I-frame as the first frame in the GoP, which stabilizes the reconstructed quality and saves the bit-rate. Second, to efficiently improve the accuracy of inter prediction without increasing the complexity of decoder, we propose a pixel-to-feature motion prediction method at encoder side that helps us to obtain high-quality motion information. Third, we propose a probability-based entropy skipping method, which not only brings performance gain, but also greatly reduces the runtime of entropy coding. With these powerful techniques, this paper proposes AlphaVC, a high-performance and efficient learned video compression scheme. To the best of our knowledge, AlphaVC is the first E2E AI codec that exceeds the latest compression standard VVC on all common test datasets for both PSNR (-28.2% BD-rate saving) and MSSSIM (-52.2% BD-rate saving), and has very fast encoding (0.001x VVC) and decoding (1.69x VVC) speeds.



### Towards Unconstrained Audio Splicing Detection and Localization with Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2207.14682v3
- **DOI**: None
- **Categories**: **cs.SD**, cs.AI, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2207.14682v3)
- **Published**: 2022-07-29 13:57:16+00:00
- **Updated**: 2023-03-31 09:56:52+00:00
- **Authors**: Denise Moussa, Germans Hirsch, Christian Riess
- **Comment**: Accepted at MMFORWILD 2022, ICPR Workshops - Code:
  https://faui1-gitlab.cs.fau.de/denise.moussa/audio-splicing-localization
- **Journal**: None
- **Summary**: Freely available and easy-to-use audio editing tools make it straightforward to perform audio splicing. Convincing forgeries can be created by combining various speech samples from the same person. Detection of such splices is important both in the public sector when considering misinformation, and in a legal context to verify the integrity of evidence. Unfortunately, most existing detection algorithms for audio splicing use handcrafted features and make specific assumptions. However, criminal investigators are often faced with audio samples from unconstrained sources with unknown characteristics, which raises the need for more generally applicable methods.   With this work, we aim to take a first step towards unconstrained audio splicing detection to address this need. We simulate various attack scenarios in the form of post-processing operations that may disguise splicing. We propose a Transformer sequence-to-sequence (seq2seq) network for splicing detection and localization. Our extensive evaluation shows that the proposed method outperforms existing dedicated approaches for splicing detection [3, 10] as well as the general-purpose networks EfficientNet [28] and RegNet [25].



### Forensic License Plate Recognition with Compression-Informed Transformers
- **Arxiv ID**: http://arxiv.org/abs/2207.14686v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.14686v2)
- **Published**: 2022-07-29 13:58:24+00:00
- **Updated**: 2022-09-16 13:45:56+00:00
- **Authors**: Denise Moussa, Anatol Maier, Andreas Spruck, Jürgen Seiler, Christian Riess
- **Comment**: Accepted at ICIP 2022, Code:
  https://faui1-gitlab.cs.fau.de/denise.moussa/forensic-license-plate-transformer/
- **Journal**: None
- **Summary**: Forensic license plate recognition (FLPR) remains an open challenge in legal contexts such as criminal investigations, where unreadable license plates (LPs) need to be deciphered from highly compressed and/or low resolution footage, e.g., from surveillance cameras. In this work, we propose a side-informed Transformer architecture that embeds knowledge on the input compression level to improve recognition under strong compression. We show the effectiveness of Transformers for license plate recognition (LPR) on a low-quality real-world dataset. We also provide a synthetic dataset that includes strongly degraded, illegible LP images and analyze the impact of knowledge embedding on it. The network outperforms existing FLPR methods and standard state-of-the art image recognition models while requiring less parameters. For the severest degraded images, we can improve recognition by up to 8.9 percent points.



### Can Shuffling Video Benefit Temporal Bias Problem: A Novel Training Framework for Temporal Grounding
- **Arxiv ID**: http://arxiv.org/abs/2207.14698v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14698v2)
- **Published**: 2022-07-29 14:11:48+00:00
- **Updated**: 2022-08-05 09:08:31+00:00
- **Authors**: Jiachang Hao, Haifeng Sun, Pengfei Ren, Jingyu Wang, Qi Qi, Jianxin Liao
- **Comment**: Accepted by ECCV2022
- **Journal**: None
- **Summary**: Temporal grounding aims to locate a target video moment that semantically corresponds to the given sentence query in an untrimmed video. However, recent works find that existing methods suffer a severe temporal bias problem. These methods do not reason the target moment locations based on the visual-textual semantic alignment but over-rely on the temporal biases of queries in training sets. To this end, this paper proposes a novel training framework for grounding models to use shuffled videos to address temporal bias problem without losing grounding accuracy. Our framework introduces two auxiliary tasks, cross-modal matching and temporal order discrimination, to promote the grounding model training. The cross-modal matching task leverages the content consistency between shuffled and original videos to force the grounding model to mine visual contents to semantically match queries. The temporal order discrimination task leverages the difference in temporal order to strengthen the understanding of long-term temporal contexts. Extensive experiments on Charades-STA and ActivityNet Captions demonstrate the effectiveness of our method for mitigating the reliance on temporal biases and strengthening the model's generalization ability against the different temporal distributions. Code is available at https://github.com/haojc/ShufflingVideosForTSG.



### Improving Small Lesion Segmentation in CT Scans using Intensity Distribution Supervision: Application to Small Bowel Carcinoid Tumor
- **Arxiv ID**: http://arxiv.org/abs/2207.14700v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.14700v1)
- **Published**: 2022-07-29 14:14:00+00:00
- **Updated**: 2022-07-29 14:14:00+00:00
- **Authors**: Seung Yeon Shin, Thomas C. Shen, Stephen A. Wank, Ronald M. Summers
- **Comment**: None
- **Journal**: None
- **Summary**: Finding small lesions is very challenging due to lack of noticeable features, severe class imbalance, as well as the size itself. One approach to improve small lesion segmentation is to reduce the region of interest and inspect it at a higher sensitivity rather than performing it for the entire region. It is usually implemented as sequential or joint segmentation of organ and lesion, which requires additional supervision on organ segmentation. Instead, we propose to utilize an intensity distribution of a target lesion at no additional labeling cost to effectively separate regions where the lesions are possibly located from the background. It is incorporated into network training as an auxiliary task. We applied the proposed method to segmentation of small bowel carcinoid tumors in CT scans. We observed improvements for all metrics (33.5% $\rightarrow$ 38.2%, 41.3% $\rightarrow$ 47.8%, 30.0% $\rightarrow$ 35.9% for the global, per case, and per tumor Dice scores, respectively.) compared to the baseline method, which proves the validity of our idea. Our method can be one option for explicitly incorporating intensity distribution information of a target in network training.



### Robust Quantitative Susceptibility Mapping via Approximate Message Passing with Parameter Estimation
- **Arxiv ID**: http://arxiv.org/abs/2207.14709v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.14709v3)
- **Published**: 2022-07-29 14:38:03+00:00
- **Updated**: 2023-05-30 21:37:57+00:00
- **Authors**: Shuai Huang, James J. Lah, Jason W. Allen, Deqiang Qiu
- **Comment**: Keywords: Approximate message passing, Compressive sensing, Outlier
  modelling, Parameter estimation, Quantitative susceptibility mapping
- **Journal**: None
- **Summary**: Purpose: For quantitative susceptibility mapping (QSM), the lack of ground-truth in clinical settings makes it challenging to determine suitable parameters for the dipole inversion. We propose a probabilistic Bayesian approach for QSM with built-in parameter estimation, and incorporate the nonlinear formulation of the dipole inversion to achieve a robust recovery of the susceptibility maps.   Theory: From a Bayesian perspective, the image wavelet coefficients are approximately sparse and modelled by the Laplace distribution. The measurement noise is modelled by a Gaussian-mixture distribution with two components, where the second component is used to model the noise outliers. Through probabilistic inference, the susceptibility map and distribution parameters can be jointly recovered using approximate message passing (AMP).   Methods: We compare our proposed AMP with built-in parameter estimation (AMP-PE) to the state-of-the-art L1-QSM, FANSI and MEDI approaches on the simulated and in vivo datasets, and perform experiments to explore the optimal settings of AMP-PE. Reproducible code is available at https://github.com/EmoryCN2L/QSM_AMP_PE   Results: On the simulated Sim2Snr1 dataset, AMP-PE achieved the lowest NRMSE, DFCM and the highest SSIM, while MEDI achieved the lowest HFEN. On the in vivo datasets, AMP-PE is robust and successfully recovers the susceptibility maps using the estimated parameters, whereas L1-QSM, FANSI and MEDI typically require additional visual fine-tuning to select or double-check working parameters.   Conclusion: AMP-PE provides automatic and adaptive parameter estimation for QSM and avoids the subjectivity from the visual fine-tuning step, making it an excellent choice for the clinical setting.



### End-to-end View Synthesis via NeRF Attention
- **Arxiv ID**: http://arxiv.org/abs/2207.14741v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.14741v3)
- **Published**: 2022-07-29 15:26:16+00:00
- **Updated**: 2022-09-15 03:04:10+00:00
- **Authors**: Zelin Zhao, Jiaya Jia
- **Comment**: Fixed reference formatting issues
- **Journal**: None
- **Summary**: In this paper, we present a simple seq2seq formulation for view synthesis where we take a set of ray points as input and output colors corresponding to the rays. Directly applying a standard transformer on this seq2seq formulation has two limitations. First, the standard attention cannot successfully fit the volumetric rendering procedure, and therefore high-frequency components are missing in the synthesized views. Second, applying global attention to all rays and pixels is extremely inefficient. Inspired by the neural radiance field (NeRF), we propose the NeRF attention (NeRFA) to address the above problems. On the one hand, NeRFA considers the volumetric rendering equation as a soft feature modulation procedure. In this way, the feature modulation enhances the transformers with the NeRF-like inductive bias. On the other hand, NeRFA performs multi-stage attention to reduce the computational overhead. Furthermore, the NeRFA model adopts the ray and pixel transformers to learn the interactions between rays and pixels. NeRFA demonstrates superior performance over NeRF and NerFormer on four datasets: DeepVoxels, Blender, LLFF, and CO3D. Besides, NeRFA establishes a new state-of-the-art under two settings: the single-scene view synthesis and the category-centric novel view synthesis.



### Computer Vision Methods for the Microstructural Analysis of Materials: The State-of-the-art and Future Perspectives
- **Arxiv ID**: http://arxiv.org/abs/2208.04149v1
- **DOI**: None
- **Categories**: **cond-mat.mtrl-sci**, cs.CV, physics.app-ph, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/2208.04149v1)
- **Published**: 2022-07-29 15:27:47+00:00
- **Updated**: 2022-07-29 15:27:47+00:00
- **Authors**: Khaled Alrfou, Amir Kordijazi, Tian Zhao
- **Comment**: 42 pages, 14 figures, 3 tables
- **Journal**: None
- **Summary**: Finding quantitative descriptors representing the microstructural features of a given material is an ongoing research area in the paradigm of Materials-by-Design. Historically, microstructural analysis mostly relies on qualitative descriptions. However, to build a robust and accurate process-structure-properties relationship, which is required for designing new advanced high-performance materials, the extraction of quantitative and meaningful statistical data from the microstructural analysis is a critical step. In recent years, computer vision (CV) methods, especially those which are centered around convolutional neural network (CNN) algorithms have shown promising results for this purpose. This review paper focuses on the state-of-the-art CNN-based techniques that have been applied to various multi-scale microstructural image analysis tasks, including classification, object detection, segmentation, feature extraction, and reconstruction. Additionally, we identified the main challenges with regard to the application of these methods to materials science research. Finally, we discussed some possible future directions of research in this area. In particular, we emphasized the application of transformer-based models and their capabilities to improve the microstructural analysis of materials.



### ALADIN: Distilling Fine-grained Alignment Scores for Efficient Image-Text Matching and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2207.14757v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.14757v1)
- **Published**: 2022-07-29 16:01:48+00:00
- **Updated**: 2022-07-29 16:01:48+00:00
- **Authors**: Nicola Messina, Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Fabrizio Falchi, Giuseppe Amato, Rita Cucchiara
- **Comment**: CBMI 2022
- **Journal**: None
- **Summary**: Image-text matching is gaining a leading role among tasks involving the joint understanding of vision and language. In literature, this task is often used as a pre-training objective to forge architectures able to jointly deal with images and texts. Nonetheless, it has a direct downstream application: cross-modal retrieval, which consists in finding images related to a given query text or vice-versa. Solving this task is of critical importance in cross-modal search engines. Many recent methods proposed effective solutions to the image-text matching problem, mostly using recent large vision-language (VL) Transformer networks. However, these models are often computationally expensive, especially at inference time. This prevents their adoption in large-scale cross-modal retrieval scenarios, where results should be provided to the user almost instantaneously. In this paper, we propose to fill in the gap between effectiveness and efficiency by proposing an ALign And DIstill Network (ALADIN). ALADIN first produces high-effective scores by aligning at fine-grained level images and texts. Then, it learns a shared embedding space - where an efficient kNN search can be performed - by distilling the relevance scores obtained from the fine-grained alignments. We obtained remarkable results on MS-COCO, showing that our method can compete with state-of-the-art VL Transformers while being almost 90 times faster. The code for reproducing our results is available at https://github.com/mesnico/ALADIN.



### Image Quality Assessment: Integrating Model-Centric and Data-Centric Approaches
- **Arxiv ID**: http://arxiv.org/abs/2207.14769v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14769v1)
- **Published**: 2022-07-29 16:23:57+00:00
- **Updated**: 2022-07-29 16:23:57+00:00
- **Authors**: Peibei Cao, Dingquan Li, Kede Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Learning-based image quality assessment (IQA) has made remarkable progress in the past decade, but nearly all consider the two key components - model and data - in relative isolation. Specifically, model-centric IQA focuses on developing "better" objective quality methods on fixed and extensively reused datasets, with a great danger of overfitting. Data-centric IQA involves conducting psychophysical experiments to construct "better" human-annotated datasets, which unfortunately ignores current IQA models during dataset creation. In this paper, we first design a series of experiments to probe computationally that such isolation of model and data impedes further progress of IQA. We then describe a computational framework that integrates model-centric and data-centric IQA. As a specific example, we design computational modules to quantify the sampling-worthiness of candidate images based on blind IQA (BIQA) model predictions and deep content-aware features. Experimental results show that the proposed sampling-worthiness module successfully spots diverse failures of the examined BIQA models, which are indeed worthy samples to be included in next-generation datasets.



### Open-radiomics: A Research Protocol to Make Radiomics-based Machine Learning Pipelines Reproducible
- **Arxiv ID**: http://arxiv.org/abs/2207.14776v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.14776v1)
- **Published**: 2022-07-29 16:37:46+00:00
- **Updated**: 2022-07-29 16:37:46+00:00
- **Authors**: Ernest, Namdar, Matthias W. Wagner, Birgit B. Ertl-Wagner, Farzad Khalvati
- **Comment**: None
- **Journal**: None
- **Summary**: The application of artificial intelligence (AI) techniques to medical imaging data has yielded promising results. As an important branch of AI pipelines in medical imaging, radiomics faces two major challenges namely reproducibility and accessibility. In this work, we introduce open-radiomics, a set of radiomics datasets, and a comprehensive radiomics pipeline that investigates the effects of radiomics feature extraction settings such as binWidth and image normalization on the reproducibility of the radiomics results performance. To make radiomics research more accessible and reproducible, we provide guidelines for building machine learning (ML) models on radiomics data, introduce Open-radiomics, an evolving collection of open-source radiomics datasets, and publish baseline models for the datasets.



### Using Multi-modal Data for Improving Generalizability and Explainability of Disease Classification in Radiology
- **Arxiv ID**: http://arxiv.org/abs/2207.14781v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2207.14781v1)
- **Published**: 2022-07-29 16:49:05+00:00
- **Updated**: 2022-07-29 16:49:05+00:00
- **Authors**: Pranav Agnihotri, Sara Ketabi, Khashayar, Namdar, Farzad Khalvati
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional datasets for the radiological diagnosis tend to only provide the radiology image alongside the radiology report. However, radiology reading as performed by radiologists is a complex process, and information such as the radiologist's eye-fixations over the course of the reading has the potential to be an invaluable data source to learn from. Nonetheless, the collection of such data is expensive and time-consuming. This leads to the question of whether such data is worth the investment to collect. This paper utilizes the recently published Eye-Gaze dataset to perform an exhaustive study on the impact on performance and explainability of deep learning (DL) classification in the face of varying levels of input features, namely: radiology images, radiology report text, and radiologist eye-gaze data. We find that the best classification performance of X-ray images is achieved with a combination of radiology report free-text and radiology image, with the eye-gaze data providing no performance boost. Nonetheless, eye-gaze data serving as secondary ground truth alongside the class label results in highly explainable models that generate better attention maps compared to models trained to do classification and attention map generation without eye-gaze data.



### Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion
- **Arxiv ID**: http://arxiv.org/abs/2207.14782v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2207.14782v1)
- **Published**: 2022-07-29 16:55:06+00:00
- **Updated**: 2022-07-29 16:55:06+00:00
- **Authors**: Weng Fei Low, Gim Hee Lee
- **Comment**: Accepted to ECCV 2022. Code is available at
  https://github.com/low5545/minimal-neural-atlas
- **Journal**: None
- **Summary**: Explicit neural surface representations allow for exact and efficient extraction of the encoded surface at arbitrary precision, as well as analytic derivation of differential geometric properties such as surface normal and curvature. Such desirable properties, which are absent in its implicit counterpart, makes it ideal for various applications in computer vision, graphics and robotics. However, SOTA works are limited in terms of the topology it can effectively describe, distortion it introduces to reconstruct complex surfaces and model efficiency. In this work, we present Minimal Neural Atlas, a novel atlas-based explicit neural surface representation. At its core is a fully learnable parametric domain, given by an implicit probabilistic occupancy field defined on an open square of the parametric space. In contrast, prior works generally predefine the parametric domain. The added flexibility enables charts to admit arbitrary topology and boundary. Thus, our representation can learn a minimal atlas of 3 charts with distortion-minimal parameterization for surfaces of arbitrary topology, including closed and open surfaces with arbitrary connected components. Our experiments support the hypotheses and show that our reconstructions are more accurate in terms of the overall geometry, due to the separation of concerns on topology and geometry.



### Recognition of Handwritten Chinese Text by Segmentation: A Segment-annotation-free Approach
- **Arxiv ID**: http://arxiv.org/abs/2207.14801v1
- **DOI**: 10.1109/TMM.2022.3146771
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14801v1)
- **Published**: 2022-07-29 17:30:43+00:00
- **Updated**: 2022-07-29 17:30:43+00:00
- **Authors**: Dezhi Peng, Lianwen Jin, Weihong Ma, Canyu Xie, Hesuo Zhang, Shenggao Zhu, Jing Li
- **Comment**: Accepted by IEEE Transactions on Multimedia (TMM)
- **Journal**: None
- **Summary**: Online and offline handwritten Chinese text recognition (HTCR) has been studied for decades. Early methods adopted oversegmentation-based strategies but suffered from low speed, insufficient accuracy, and high cost of character segmentation annotations. Recently, segmentation-free methods based on connectionist temporal classification (CTC) and attention mechanism, have dominated the field of HCTR. However, people actually read text character by character, especially for ideograms such as Chinese. This raises the question: are segmentation-free strategies really the best solution to HCTR? To explore this issue, we propose a new segmentation-based method for recognizing handwritten Chinese text that is implemented using a simple yet efficient fully convolutional network. A novel weakly supervised learning method is proposed to enable the network to be trained using only transcript annotations; thus, the expensive character segmentation annotations required by previous segmentation-based methods can be avoided. Owing to the lack of context modeling in fully convolutional networks, we propose a contextual regularization method to integrate contextual information into the network during the training stage, which can further improve the recognition performance. Extensive experiments conducted on four widely used benchmarks, namely CASIA-HWDB, CASIA-OLHWDB, ICDAR2013, and SCUT-HCCDoc, show that our method significantly surpasses existing methods on both online and offline HCTR, and exhibits a considerably higher inference speed than CTC/attention-based approaches.



### Artifact Identification in X-ray Diffraction Data using Machine Learning Methods
- **Arxiv ID**: http://arxiv.org/abs/2207.14804v1
- **DOI**: 10.1107/S1600577522011274
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.14804v1)
- **Published**: 2022-07-29 17:37:10+00:00
- **Updated**: 2022-07-29 17:37:10+00:00
- **Authors**: Howard Yanxon, James Weng, Hannah Parraga, Wenqian Xu, Uta Ruett, Nicholas Schwarz
- **Comment**: None
- **Journal**: (2023). J. Synchrotron Rad. 30,
- **Summary**: The in situ synchrotron high-energy X-ray powder diffraction (XRD) technique is highly utilized by researchers to analyze the crystallographic structures of materials in functional devices (e.g., battery materials) or in complex sample environments (e.g., diamond anvil cells or syntheses reactors). An atomic structure of a material can be identified by its diffraction pattern, along with detailed analysis such as Rietveld refinement which indicates how the measured structure deviates from the ideal structure (e.g., internal stresses or defects). For in situ experiments, a series of XRD images is usually collected on the same sample at different conditions (e.g., adiabatic conditions), yielding different states of matter, or simply collected continuously as a function of time to track the change of a sample over a chemical or physical process. In situ experiments are usually performed with area detectors, collecting 2D images composed of diffraction rings for ideal powders. Depending on the material's form, one may observe different characteristics other than the typical Debye Scherrer rings for a realistic sample and its environments, such as textures or preferred orientations and single crystal diffraction spots in the 2D XRD image. In this work, we present an investigation of machine learning methods for fast and reliable identification and separation of the single crystal diffraction spots in XRD images. The exclusion of artifacts during an XRD image integration process allows a precise analysis of the powder diffraction rings of interest. We observe that the gradient boosting method can consistently produce high accuracy results when it is trained with small subsets of highly diverse datasets. The method dramatically decreases the amount of time spent on identifying and separating single crystal spots in comparison to the conventional method.



### PageNet: Towards End-to-End Weakly Supervised Page-Level Handwritten Chinese Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.14807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14807v1)
- **Published**: 2022-07-29 17:47:45+00:00
- **Updated**: 2022-07-29 17:47:45+00:00
- **Authors**: Dezhi Peng, Lianwen Jin, Yuliang Liu, Canjie Luo, Songxuan Lai
- **Comment**: Accepted to appear in International Journal of Computer Vision (IJCV)
- **Journal**: None
- **Summary**: Handwritten Chinese text recognition (HCTR) has been an active research topic for decades. However, most previous studies solely focus on the recognition of cropped text line images, ignoring the error caused by text line detection in real-world applications. Although some approaches aimed at page-level text recognition have been proposed in recent years, they either are limited to simple layouts or require very detailed annotations including expensive line-level and even character-level bounding boxes. To this end, we propose PageNet for end-to-end weakly supervised page-level HCTR. PageNet detects and recognizes characters and predicts the reading order between them, which is more robust and flexible when dealing with complex layouts including multi-directional and curved text lines. Utilizing the proposed weakly supervised learning framework, PageNet requires only transcripts to be annotated for real data; however, it can still output detection and recognition results at both the character and line levels, avoiding the labor and cost of labeling bounding boxes of characters and text lines. Extensive experiments conducted on five datasets demonstrate the superiority of PageNet over existing weakly supervised and fully supervised page-level methods. These experimental results may spark further research beyond the realms of existing methods based on connectionist temporal classification or attention. The source code is available at https://github.com/shannanyinxiang/PageNet.



### StyleLight: HDR Panorama Generation for Lighting Estimation and Editing
- **Arxiv ID**: http://arxiv.org/abs/2207.14811v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.14811v1)
- **Published**: 2022-07-29 17:58:58+00:00
- **Updated**: 2022-07-29 17:58:58+00:00
- **Authors**: Guangcong Wang, Yinuo Yang, Chen Change Loy, Ziwei Liu
- **Comment**: ECCV 2022, Project Page: https://style-light.github.io/ , Code:
  https://github.com/Wanggcong/StyleLight
- **Journal**: None
- **Summary**: We present a new lighting estimation and editing framework to generate high-dynamic-range (HDR) indoor panorama lighting from a single limited field-of-view (LFOV) image captured by low-dynamic-range (LDR) cameras. Existing lighting estimation methods either directly regress lighting representation parameters or decompose this problem into LFOV-to-panorama and LDR-to-HDR lighting generation sub-tasks. However, due to the partial observation, the high-dynamic-range lighting, and the intrinsic ambiguity of a scene, lighting estimation remains a challenging task. To tackle this problem, we propose a coupled dual-StyleGAN panorama synthesis network (StyleLight) that integrates LDR and HDR panorama synthesis into a unified framework. The LDR and HDR panorama synthesis share a similar generator but have separate discriminators. During inference, given an LDR LFOV image, we propose a focal-masked GAN inversion method to find its latent code by the LDR panorama synthesis branch and then synthesize the HDR panorama by the HDR panorama synthesis branch. StyleLight takes LFOV-to-panorama and LDR-to-HDR lighting generation into a unified framework and thus greatly improves lighting estimation. Extensive experiments demonstrate that our framework achieves superior performance over state-of-the-art methods on indoor lighting estimation. Notably, StyleLight also enables intuitive lighting editing on indoor HDR panoramas, which is suitable for real-world applications. Code is available at https://style-light.github.io.



### GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2207.14812v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.14812v1)
- **Published**: 2022-07-29 17:59:01+00:00
- **Updated**: 2022-07-29 17:59:01+00:00
- **Authors**: Kelvin C. K. Chan, Xiangyu Xu, Xintao Wang, Jinwei Gu, Chen Change Loy
- **Comment**: Accepted to TPAMI. Extension of our CVPR 2021 version:
  https://openaccess.thecvf.com/content/CVPR2021/html/Chan_GLEAN_Generative_Latent_Bank_for_Large-Factor_Image_Super-Resolution_CVPR_2021_paper.html?ref=https://githubhelp.com.
  arXiv admin note: text overlap with arXiv:2012.00739
- **Journal**: None
- **Summary**: We show that pre-trained Generative Adversarial Networks (GANs) such as StyleGAN and BigGAN can be used as a latent bank to improve the performance of image super-resolution. While most existing perceptual-oriented approaches attempt to generate realistic outputs through learning with adversarial loss, our method, Generative LatEnt bANk (GLEAN), goes beyond existing practices by directly leveraging rich and diverse priors encapsulated in a pre-trained GAN. But unlike prevalent GAN inversion methods that require expensive image-specific optimization at runtime, our approach only needs a single forward pass for restoration. GLEAN can be easily incorporated in a simple encoder-bank-decoder architecture with multi-resolution skip connections. Employing priors from different generative models allows GLEAN to be applied to diverse categories (\eg~human faces, cats, buildings, and cars). We further present a lightweight version of GLEAN, named LightGLEAN, which retains only the critical components in GLEAN. Notably, LightGLEAN consists of only 21% of parameters and 35% of FLOPs while achieving comparable image quality. We extend our method to different tasks including image colorization and blind image restoration, and extensive experiments show that our proposed models perform favorably in comparison to existing methods. Codes and models are available at https://github.com/open-mmlab/mmediting.



### Paddy Leaf diseases identification on Infrared Images based on Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.00031v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.00031v2)
- **Published**: 2022-07-29 18:24:29+00:00
- **Updated**: 2022-08-06 12:06:23+00:00
- **Authors**: Petchiammal A, Briskline Kiruba S, D. Murugan
- **Comment**: Uploaded a different draft by mistake
- **Journal**: None
- **Summary**: Agriculture is the mainstay of human society because it is an essential need for every organism. Paddy cultivation is very significant so far as humans are concerned, largely in the Asian continent, and it is one of the staple foods. However, plant diseases in agriculture lead to depletion in productivity. Plant diseases are generally caused by pests, insects, and pathogens that decrease productivity to a large scale if not controlled within a particular time. Eventually, one cannot see an increase in paddy yield. Accurate and timely identification of plant diseases can help farmers mitigate losses due to pests and diseases. Recently, deep learning techniques have been used to identify paddy diseases and overcome these problems. This paper implements a convolutional neural network (CNN) based on a model and tests a public dataset consisting of 636 infrared image samples with five paddy disease classes and one healthy class. The proposed model proficiently identified and classified paddy diseases of five different types and achieved an accuracy of 88.28%



### A review of Deep learning Techniques for COVID-19 identification on Chest CT images
- **Arxiv ID**: http://arxiv.org/abs/2208.00032v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.00032v2)
- **Published**: 2022-07-29 18:27:20+00:00
- **Updated**: 2022-08-06 12:06:35+00:00
- **Authors**: Briskline Kiruba S, Petchiammal A, D. Murugan
- **Comment**: Uploaded a different draft by mistake
- **Journal**: None
- **Summary**: The current COVID-19 pandemic is a serious threat to humanity that directly affects the lungs. Automatic identification of COVID-19 is a challenge for health care officials. The standard gold method for diagnosing COVID-19 is Reverse Transcription Polymerase Chain Reaction (RT-PCR) to collect swabs from affected people. Some limitations encountered while collecting swabs are related to accuracy and longtime duration. Chest CT (Computed Tomography) is another test method that helps healthcare providers quickly identify the infected lung areas. It was used as a supporting tool for identifying COVID-19 in an earlier stage. With the help of deep learning, the CT imaging characteristics of COVID-19. Researchers have proven it to be highly effective for COVID-19 CT image classification. In this study, we review the recent deep learning techniques that can use to detect the COVID-19 disease. Relevant studies were collected by various databases such as Web of Science, Google Scholar, and PubMed. Finally, we compare the results of different deep learning models, and CT image analysis is discussed.



### MulViMotion: Shape-aware 3D Myocardial Motion Tracking from Multi-View Cardiac MRI
- **Arxiv ID**: http://arxiv.org/abs/2208.00034v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.00034v1)
- **Published**: 2022-07-29 18:29:52+00:00
- **Updated**: 2022-07-29 18:29:52+00:00
- **Authors**: Qingjie Meng, Chen Qin, Wenjia Bai, Tianrui Liu, Antonio de Marvao, Declan P O'Regan, Daniel Rueckert
- **Comment**: None
- **Journal**: None
- **Summary**: Recovering the 3D motion of the heart from cine cardiac magnetic resonance (CMR) imaging enables the assessment of regional myocardial function and is important for understanding and analyzing cardiovascular disease. However, 3D cardiac motion estimation is challenging because the acquired cine CMR images are usually 2D slices which limit the accurate estimation of through-plane motion. To address this problem, we propose a novel multi-view motion estimation network (MulViMotion), which integrates 2D cine CMR images acquired in short-axis and long-axis planes to learn a consistent 3D motion field of the heart. In the proposed method, a hybrid 2D/3D network is built to generate dense 3D motion fields by learning fused representations from multi-view images. To ensure that the motion estimation is consistent in 3D, a shape regularization module is introduced during training, where shape information from multi-view images is exploited to provide weak supervision to 3D motion estimation. We extensively evaluate the proposed method on 2D cine CMR images from 580 subjects of the UK Biobank study for 3D motion tracking of the left ventricular myocardium. Experimental results show that the proposed method quantitatively and qualitatively outperforms competing methods.



### Generating Multiple 4D Expression Transitions by Learning Face Landmark Trajectories
- **Arxiv ID**: http://arxiv.org/abs/2208.00050v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.00050v2)
- **Published**: 2022-07-29 19:33:56+00:00
- **Updated**: 2023-05-18 07:25:05+00:00
- **Authors**: Naima Otberdout, Claudio Ferrari, Mohamed Daoudi, Stefano Berretti, Alberto Del Bimbo
- **Comment**: This preprint is an extension of CVPR 2022 paper arXiv:2105.07463
- **Journal**: None
- **Summary**: In this work, we address the problem of 4D facial expressions generation. This is usually addressed by animating a neutral 3D face to reach an expression peak, and then get back to the neutral state. In the real world though, people show more complex expressions, and switch from one expression to another. We thus propose a new model that generates transitions between different expressions, and synthesizes long and composed 4D expressions. This involves three sub-problems: (i) modeling the temporal dynamics of expressions, (ii) learning transitions between them, and (iii) deforming a generic mesh. We propose to encode the temporal evolution of expressions using the motion of a set of 3D landmarks, that we learn to generate by training a manifold-valued GAN (Motion3DGAN). To allow the generation of composed expressions, this model accepts two labels encoding the starting and the ending expressions. The final sequence of meshes is generated by a Sparse2Dense mesh Decoder (S2D-Dec) that maps the landmark displacements to a dense, per-vertex displacement of a known mesh topology. By explicitly working with motion trajectories, the model is totally independent from the identity. Extensive experiments on five public datasets show that our proposed approach brings significant improvements with respect to previous solutions, while retaining good generalization to unseen data.



### UAVM: Towards Unifying Audio and Visual Models
- **Arxiv ID**: http://arxiv.org/abs/2208.00061v2
- **DOI**: 10.1109/LSP.2022.3224688
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2208.00061v2)
- **Published**: 2022-07-29 20:23:16+00:00
- **Updated**: 2023-02-16 04:57:13+00:00
- **Authors**: Yuan Gong, Alexander H. Liu, Andrew Rouditchenko, James Glass
- **Comment**: Published in Signal Processing Letters. Code at
  https://github.com/YuanGongND/uavm
- **Journal**: IEEE Signal Processing Letters, vol. 29, pp. 2437-2441, 2022
- **Summary**: Conventional audio-visual models have independent audio and video branches. In this work, we unify the audio and visual branches by designing a Unified Audio-Visual Model (UAVM). The UAVM achieves a new state-of-the-art audio-visual event classification accuracy of 65.8% on VGGSound. More interestingly, we also find a few intriguing properties of UAVM that the modality-independent counterparts do not have.



### Face-to-Face Contrastive Learning for Social Intelligence Question-Answering
- **Arxiv ID**: http://arxiv.org/abs/2208.01036v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.01036v3)
- **Published**: 2022-07-29 20:39:44+00:00
- **Updated**: 2022-10-27 17:10:11+00:00
- **Authors**: Alex Wilf, Martin Q. Ma, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency
- **Comment**: None
- **Journal**: None
- **Summary**: Creating artificial social intelligence - algorithms that can understand the nuances of multi-person interactions - is an exciting and emerging challenge in processing facial expressions and gestures from multimodal videos. Recent multimodal methods have set the state of the art on many tasks, but have difficulty modeling the complex face-to-face conversational dynamics across speaking turns in social interaction, particularly in a self-supervised setup. In this paper, we propose Face-to-Face Contrastive Learning (F2F-CL), a graph neural network designed to model social interactions using factorization nodes to contextualize the multimodal face-to-face interaction along the boundaries of the speaking turn. With the F2F-CL model, we propose to perform contrastive learning between the factorization nodes of different speaking turns within the same video. We experimentally evaluated the challenging Social-IQ dataset and show state-of-the-art results.



### Machine Learning and Computer Vision Techniques in Continuous Beehive Monitoring Applications: A survey
- **Arxiv ID**: http://arxiv.org/abs/2208.00085v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.00085v2)
- **Published**: 2022-07-29 21:56:59+00:00
- **Updated**: 2023-08-11 20:37:25+00:00
- **Authors**: Simon Bilik, Tomas Zemcik, Lukas Kratochvila, Dominik Ricanek, Karel Horak, Milos Richter
- **Comment**: None
- **Journal**: None
- **Summary**: Wide use and availability of the machine learning and computer vision techniques allows development of relatively complex monitoring systems in many domains. Besides the traditional industrial domain, new application appears also in biology and agriculture, where we could speak about the detection of infections, parasites and weeds, but also about automated monitoring and early warning systems. This is also connected with the introduction of the easily accessible hardware and development kits such as Arduino, or RaspberryPi family. In this paper, we survey 50 existing papers focusing on the methods of automated beehive monitoring methods using the computer vision techniques, particularly on the pollen and Varroa mite detection together with the bee traffic monitoring. Such systems could also be used for the monitoring of the honeybee colonies and for the inspection of their health state, which could identify potentially dangerous states before the situation is critical, or to better plan periodic bee colony inspections and therefore save significant costs. Later, we also include analysis of the research trends in this application field and we outline the possible direction of the new explorations. Our paper is aimed also at veterinary and apidology professionals and experts, who might not be familiar with machine learning to introduce them to its possibilities, therefore each family of applications is opened by a brief theoretical introduction and motivation related to its base method. We hope that this paper will inspire other scientists to use machine learning techniques for other applications in beehive monitoring.



### Low-complexity Approximate Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.00087v1
- **DOI**: 10.1109/TNNLS.2018.2815435
- **Categories**: **cs.LG**, cs.CC, cs.CV, eess.SP, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/2208.00087v1)
- **Published**: 2022-07-29 21:59:29+00:00
- **Updated**: 2022-07-29 21:59:29+00:00
- **Authors**: R. J. Cintra, S. Duffner, C. Garcia, A. Leite
- **Comment**: 13 pages, 4 figures, 8 tables
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems, v. 29,
  n. 12, Dec. 2018
- **Summary**: In this paper, we present an approach for minimizing the computational complexity of trained Convolutional Neural Networks (ConvNet). The idea is to approximate all elements of a given ConvNet and replace the original convolutional filters and parameters (pooling and bias coefficients; and activation function) with efficient approximations capable of extreme reductions in computational complexity. Low-complexity convolution filters are obtained through a binary (zero-one) linear programming scheme based on the Frobenius norm over sets of dyadic rationals. The resulting matrices allow for multiplication-free computations requiring only addition and bit-shifting operations. Such low-complexity structures pave the way for low-power, efficient hardware designs. We applied our approach on three use cases of different complexity: (i) a "light" but efficient ConvNet for face detection (with around 1000 parameters); (ii) another one for hand-written digit classification (with more than 180000 parameters); and (iii) a significantly larger ConvNet: AlexNet with $\approx$1.2 million matrices. We evaluated the overall performance on the respective tasks for different levels of approximations. In all considered applications, very low-complexity approximations have been derived maintaining an almost equal classification performance.



### Explicit Occlusion Reasoning for Multi-person 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.00090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.00090v1)
- **Published**: 2022-07-29 22:12:50+00:00
- **Updated**: 2022-07-29 22:12:50+00:00
- **Authors**: Qihao Liu, Yi Zhang, Song Bai, Alan Yuille
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Occlusion poses a great threat to monocular multi-person 3D human pose estimation due to large variability in terms of the shape, appearance, and position of occluders. While existing methods try to handle occlusion with pose priors/constraints, data augmentation, or implicit reasoning, they still fail to generalize to unseen poses or occlusion cases and may make large mistakes when multiple people are present. Inspired by the remarkable ability of humans to infer occluded joints from visible cues, we develop a method to explicitly model this process that significantly improves bottom-up multi-person human pose estimation with or without occlusions. First, we split the task into two subtasks: visible keypoints detection and occluded keypoints reasoning, and propose a Deeply Supervised Encoder Distillation (DSED) network to solve the second one. To train our model, we propose a Skeleton-guided human Shape Fitting (SSF) approach to generate pseudo occlusion labels on the existing datasets, enabling explicit occlusion reasoning. Experiments show that explicitly learning from occlusions improves human pose estimation. In addition, exploiting feature-level information of visible joints allows us to reason about occluded joints more accurately. Our method outperforms both the state-of-the-art top-down and bottom-up methods on several benchmarks.



### Robust Trajectory Prediction against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2208.00094v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.00094v1)
- **Published**: 2022-07-29 22:35:05+00:00
- **Updated**: 2022-07-29 22:35:05+00:00
- **Authors**: Yulong Cao, Danfei Xu, Xinshuo Weng, Zhuoqing Mao, Anima Anandkumar, Chaowei Xiao, Marco Pavone
- **Comment**: None
- **Journal**: None
- **Summary**: Trajectory prediction using deep neural networks (DNNs) is an essential component of autonomous driving (AD) systems. However, these methods are vulnerable to adversarial attacks, leading to serious consequences such as collisions. In this work, we identify two key ingredients to defend trajectory prediction models against adversarial attacks including (1) designing effective adversarial training methods and (2) adding domain-specific data augmentation to mitigate the performance degradation on clean data. We demonstrate that our method is able to improve the performance by 46% on adversarial data and at the cost of only 3% performance degradation on clean data, compared to the model trained with clean data. Additionally, compared to existing robust methods, our method can improve performance by 21% on adversarial examples and 9% on clean data. Our robust model is evaluated with a planner to study its downstream impacts. We demonstrate that our model can significantly reduce the severe accident rates (e.g., collisions and off-road driving).



### Weakly Supervised Deep Instance Nuclei Detection using Points Annotation in 3D Cardiovascular Immunofluorescent Images
- **Arxiv ID**: http://arxiv.org/abs/2208.00098v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.00098v1)
- **Published**: 2022-07-29 23:06:58+00:00
- **Updated**: 2022-07-29 23:06:58+00:00
- **Authors**: Nazanin Moradinasab, Yash Sharma, Laura S. Shankman, Gary K. Owens, Donald E. Brown
- **Comment**: None
- **Journal**: None
- **Summary**: Two major causes of death in the United States and worldwide are stroke and myocardial infarction. The underlying cause of both is thrombi released from ruptured or eroded unstable atherosclerotic plaques that occlude vessels in the heart (myocardial infarction) or the brain (stroke). Clinical studies show that plaque composition plays a more important role than lesion size in plaque rupture or erosion events. To determine the plaque composition, various cell types in 3D cardiovascular immunofluorescent images of plaque lesions are counted. However, counting these cells manually is expensive, time-consuming, and prone to human error. These challenges of manual counting motivate the need for an automated approach to localize and count the cells in images. The purpose of this study is to develop an automatic approach to accurately detect and count cells in 3D immunofluorescent images with minimal annotation effort. In this study, we used a weakly supervised learning approach to train the HoVer-Net segmentation model using point annotations to detect nuclei in fluorescent images. The advantage of using point annotations is that they require less effort as opposed to pixel-wise annotation. To train the HoVer-Net model using point annotations, we adopted a popularly used cluster labeling approach to transform point annotations into accurate binary masks of cell nuclei. Traditionally, these approaches have generated binary masks from point annotations, leaving a region around the object unlabeled (which is typically ignored during model training). However, these areas may contain important information that helps determine the boundary between cells. Therefore, we used the entropy minimization loss function in these areas to encourage the model to output more confident predictions on the unlabeled areas. Our comparison studies indicate that the HoVer-Net model trained using our weakly ...



