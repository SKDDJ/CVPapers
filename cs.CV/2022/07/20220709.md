# Arxiv Papers in cs.CV on 2022-07-09
### A Survey of Task-Based Machine Learning Content Extraction Services for VIDINT
- **Arxiv ID**: http://arxiv.org/abs/2207.04158v1
- **DOI**: None
- **Categories**: **cs.ET**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2207.04158v1)
- **Published**: 2022-07-09 00:02:08+00:00
- **Updated**: 2022-07-09 00:02:08+00:00
- **Authors**: Joshua Brunk, Nathan Jermann, Ryan Sharp, Carl D. Hoover
- **Comment**: None
- **Journal**: None
- **Summary**: This paper provides a comparison of current video content extraction tools with a focus on comparing commercial task-based machine learning services. Video intelligence (VIDINT) data has become a critical intelligence source in the past decade. The need for AI-based analytics and automation tools to extract and structure content from video has quickly become a priority for organizations needing to search, analyze and exploit video at scale. With rapid growth in machine learning technology, the maturity of machine transcription, machine translation, topic tagging, and object recognition tasks are improving at an exponential rate, breaking performance records in speed and accuracy as new applications evolve. Each section of this paper reviews and compares products, software resources and video analytics capabilities based on tasks relevant to extracting information from video with machine learning techniques.



### Few 'Zero Level Set'-Shot Learning of Shape Signed Distance Functions in Feature Space
- **Arxiv ID**: http://arxiv.org/abs/2207.04161v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.04161v1)
- **Published**: 2022-07-09 00:14:39+00:00
- **Updated**: 2022-07-09 00:14:39+00:00
- **Authors**: Amine Ouasfi, Adnane Boukhayma
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: We explore a new idea for learning based shape reconstruction from a point cloud, based on the recently popularized implicit neural shape representations. We cast the problem as a few-shot learning of implicit neural signed distance functions in feature space, that we approach using gradient based meta-learning. We use a convolutional encoder to build a feature space given the input point cloud. An implicit decoder learns to predict signed distance values given points represented in this feature space. Setting the input point cloud, i.e. samples from the target shape function's zero level set, as the support (i.e. context) in few-shot learning terms, we train the decoder such that it can adapt its weights to the underlying shape of this context with a few (5) tuning steps. We thus combine two types of implicit neural network conditioning mechanisms simultaneously for the first time, namely feature encoding and meta-learning. Our numerical and qualitative evaluation shows that in the context of implicit reconstruction from a sparse point cloud, our proposed strategy, i.e. meta-learning in feature space, outperforms existing alternatives, namely standard supervised learning in feature space, and meta-learning in euclidean space, while still providing fast inference.



### Towards Multimodal Vision-Language Models Generating Non-Generic Text
- **Arxiv ID**: http://arxiv.org/abs/2207.04174v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.04174v1)
- **Published**: 2022-07-09 01:56:35+00:00
- **Updated**: 2022-07-09 01:56:35+00:00
- **Authors**: Wes Robbins, Zanyar Zohourianshahzadi, Jugal Kalita
- **Comment**: None
- **Journal**: 2021 International Conference on Natural Language Processing
- **Summary**: Vision-language models can assess visual context in an image and generate descriptive text. While the generated text may be accurate and syntactically correct, it is often overly general. To address this, recent work has used optical character recognition to supplement visual information with text extracted from an image. In this work, we contend that vision-language models can benefit from additional information that can be extracted from an image, but are not used by current models. We modify previous multimodal frameworks to accept relevant information from any number of auxiliary classifiers. In particular, we focus on person names as an additional set of tokens and create a novel image-caption dataset to facilitate captioning with person names. The dataset, Politicians and Athletes in Captions (PAC), consists of captioned images of well-known people in context. By fine-tuning pretrained models with this dataset, we demonstrate a model that can naturally integrate facial recognition tokens into generated text by training on limited data. For the PAC dataset, we provide a discussion on collection and baseline benchmark scores.



### Direct Handheld Burst Imaging to Simulated Defocus
- **Arxiv ID**: http://arxiv.org/abs/2207.04175v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04175v2)
- **Published**: 2022-07-09 01:59:36+00:00
- **Updated**: 2022-07-13 23:44:04+00:00
- **Authors**: Meng-Lin Wu, Venkata Ravi Kiran Dayana, Hau Hwang
- **Comment**: ICIP 2022
- **Journal**: None
- **Summary**: A shallow depth-of-field image keeps the subject in focus, and the foreground and background contexts blurred. This effect requires much larger lens apertures than those of smartphone cameras. Conventional methods acquire RGB-D images and blur image regions based on their depth. However, this approach is not suitable for reflective or transparent surfaces, or finely detailed object silhouettes, where the depth value is inaccurate or ambiguous.   We present a learning-based method to synthesize the defocus blur in shallow depth-of-field images from handheld bursts acquired with a single small aperture lens. Our deep learning model directly produces the shallow depth-of-field image, avoiding explicit depth-based blurring. The simulated aperture diameter equals the camera translation during burst acquisition. Our method does not suffer from artifacts due to inaccurate or ambiguous depth estimation, and it is well-suited to portrait photography.



### Learning Robust Representation for Joint Grading of Ophthalmic Diseases via Adaptive Curriculum and Feature Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/2207.04183v2
- **DOI**: 10.1007/978-3-031-16437-8_50
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04183v2)
- **Published**: 2022-07-09 03:02:36+00:00
- **Updated**: 2023-03-26 18:05:48+00:00
- **Authors**: Haoxuan Che, Haibo Jin, Hao Chen
- **Comment**: Accepted by MICCAI22
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) and diabetic macular edema (DME) are leading causes of permanent blindness worldwide. Designing an automatic grading system with good generalization ability for DR and DME is vital in clinical practice. However, prior works either grade DR or DME independently, without considering internal correlations between them, or grade them jointly by shared feature representation, yet ignoring potential generalization issues caused by difficult samples and data bias. Aiming to address these problems, we propose a framework for joint grading with the dynamic difficulty-aware weighted loss (DAW) and the dual-stream disentangled learning architecture (DETACH). Inspired by curriculum learning, DAW learns from simple samples to difficult samples dynamically via measuring difficulty adaptively. DETACH separates features of grading tasks to avoid potential emphasis on the bias. With the addition of DAW and DETACH, the model learns robust disentangled feature representations to explore internal correlations between DR and DME and achieve better grading performance. Experiments on three benchmarks show the effectiveness and robustness of our framework under both the intra-dataset and cross-dataset tests.



### Domain Alignment Meets Fully Test-Time Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2207.04185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.04185v1)
- **Published**: 2022-07-09 03:17:19+00:00
- **Updated**: 2022-07-09 03:17:19+00:00
- **Authors**: Kowshik Thopalli, Pavan Turaga, Jayaraman J. Thiagarajan
- **Comment**: 16 Pages including references, 5 figures
- **Journal**: None
- **Summary**: A foundational requirement of a deployed ML model is to generalize to data drawn from a testing distribution that is different from training. A popular solution to this problem is to adapt a pre-trained model to novel domains using only unlabeled data. In this paper, we focus on a challenging variant of this problem, where access to the original source data is restricted. While fully test-time adaptation (FTTA) and unsupervised domain adaptation (UDA) are closely related, the advances in UDA are not readily applicable to TTA, since most UDA methods require access to the source data. Hence, we propose a new approach, CATTAn, that bridges UDA and FTTA, by relaxing the need to access entire source data, through a novel deep subspace alignment strategy. With a minimal overhead of storing the subspace basis set for the source data, CATTAn enables unsupervised alignment between source and target data during adaptation. Through extensive experimental evaluation on multiple 2D and 3D vision benchmarks (ImageNet-C, Office-31, OfficeHome, DomainNet, PointDA-10) and model architectures, we demonstrate significant gains in FTTA performance. Furthermore, we make a number of crucial findings on the utility of the alignment objective even with inherently robust models, pre-trained ViT representations and under low sample availability in the target domain.



### A Study on Self-Supervised Object Detection Pretraining
- **Arxiv ID**: http://arxiv.org/abs/2207.04186v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04186v2)
- **Published**: 2022-07-09 03:30:44+00:00
- **Updated**: 2022-08-10 20:11:15+00:00
- **Authors**: Trung Dang, Simon Kornblith, Huy Thong Nguyen, Peter Chin, Maryam Khademi
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we study different approaches to self-supervised pretraining of object detection models. We first design a general framework to learn a spatially consistent dense representation from an image, by randomly sampling and projecting boxes to each augmented view and maximizing the similarity between corresponding box features. We study existing design choices in the literature, such as box generation, feature extraction strategies, and using multiple views inspired by its success on instance-level image representation learning techniques. Our results suggest that the method is robust to different choices of hyperparameters, and using multiple views is not as effective as shown for instance-level image representation learning. We also design two auxiliary tasks to predict boxes in one view from their features in the other view, by (1) predicting boxes from the sampled set by using a contrastive loss, and (2) predicting box coordinates using a transformer, which potentially benefits downstream object detection tasks. We found that these tasks do not lead to better object detection performance when finetuning the pretrained model on labeled data.



### Learning Resolution-Adaptive Representations for Cross-Resolution Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2207.13037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.13037v1)
- **Published**: 2022-07-09 03:49:51+00:00
- **Updated**: 2022-07-09 03:49:51+00:00
- **Authors**: Lin Wu, Lingqiao Liu, Yang Wang, Zheng Zhang, Farid Boussaid, Mohammed Bennamoun
- **Comment**: Under review
- **Journal**: None
- **Summary**: The cross-resolution person re-identification (CRReID) problem aims to match low-resolution (LR) query identity images against high resolution (HR) gallery images. It is a challenging and practical problem since the query images often suffer from resolution degradation due to the different capturing conditions from real-world cameras. To address this problem, state-of-the-art (SOTA) solutions either learn the resolution-invariant representation or adopt super-resolution (SR) module to recover the missing information from the LR query. This paper explores an alternative SR-free paradigm to directly compare HR and LR images via a dynamic metric, which is adaptive to the resolution of a query image. We realize this idea by learning resolution-adaptive representations for cross-resolution comparison. Specifically, we propose two resolution-adaptive mechanisms. The first one disentangles the resolution-specific information into different sub-vectors in the penultimate layer of the deep neural networks, and thus creates a varying-length representation. To better extract resolution-dependent information, we further propose to learn resolution-adaptive masks for intermediate residual feature blocks. A novel progressive learning strategy is proposed to train those masks properly. These two mechanisms are combined to boost the performance of CRReID. Experimental results show that the proposed method is superior to existing approaches and achieves SOTA performance on multiple CRReID benchmarks.



### Pseudo-Pair based Self-Similarity Learning for Unsupervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2207.13035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.13035v1)
- **Published**: 2022-07-09 04:05:06+00:00
- **Updated**: 2022-07-09 04:05:06+00:00
- **Authors**: Lin Wu, Deyin Liu, Wenying Zhang, Dapeng Chen, Zongyuan Ge, Farid Boussaid, Mohammed Bennamoun, Jialie Shen
- **Comment**: Under review
- **Journal**: IEEE Transactions on Image Processing 2022
- **Summary**: Person re-identification (re-ID) is of great importance to video surveillance systems by estimating the similarity between a pair of cross-camera person shorts. Current methods for estimating such similarity require a large number of labeled samples for supervised training. In this paper, we present a pseudo-pair based self-similarity learning approach for unsupervised person re-ID without human annotations. Unlike conventional unsupervised re-ID methods that use pseudo labels based on global clustering, we construct patch surrogate classes as initial supervision, and propose to assign pseudo labels to images through the pairwise gradient-guided similarity separation. This can cluster images in pseudo pairs, and the pseudos can be updated during training. Based on pseudo pairs, we propose to improve the generalization of similarity function via a novel self-similarity learning:it learns local discriminative features from individual images via intra-similarity, and discovers the patch correspondence across images via inter-similarity. The intra-similarity learning is based on channel attention to detect diverse local features from an image. The inter-similarity learning employs a deformable convolution with a non-local block to align patches for cross-image similarity. Experimental results on several re-ID benchmark datasets demonstrate the superiority of the proposed method over the state-of-the-arts.



### Towards Highly Expressive Machine Learning Models of Non-Melanoma Skin Cancer
- **Arxiv ID**: http://arxiv.org/abs/2207.05749v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, eess.IV, I.2.7; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2207.05749v1)
- **Published**: 2022-07-09 04:53:25+00:00
- **Updated**: 2022-07-09 04:53:25+00:00
- **Authors**: Simon M. Thomas, James G. Lefevre, Glenn Baxter, Nicholas A. Hamilton
- **Comment**: 12 figures, 29 pages
- **Journal**: None
- **Summary**: Pathologists have a rich vocabulary with which they can describe all the nuances of cellular morphology. In their world, there is a natural pairing of images and words. Recent advances demonstrate that machine learning models can now be trained to learn high-quality image features and represent them as discrete units of information. This enables natural language, which is also discrete, to be jointly modelled alongside the imaging, resulting in a description of the contents of the imaging. Here we present experiments in applying discrete modelling techniques to the problem domain of non-melanoma skin cancer, specifically, histological images of Intraepidermal Carcinoma (IEC). Implementing a VQ-GAN model to reconstruct high-resolution (256x256) images of IEC images, we trained a sequence-to-sequence transformer to generate natural language descriptions using pathologist terminology. Combined with the idea of interactive concept vectors available by using continuous generative methods, we demonstrate an additional angle of interpretability. The result is a promising means of working towards highly expressive machine learning systems which are not only useful as predictive/classification tools, but also means to further our scientific understanding of disease.



### Learning Structured Representations of Visual Scenes
- **Arxiv ID**: http://arxiv.org/abs/2207.04200v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2207.04200v1)
- **Published**: 2022-07-09 05:40:08+00:00
- **Updated**: 2022-07-09 05:40:08+00:00
- **Authors**: Meng-Jiun Chiou
- **Comment**: Ph.D. thesis at the National University of Singapore
- **Journal**: None
- **Summary**: As the intermediate-level representations bridging the two levels, structured representations of visual scenes, such as visual relationships between pairwise objects, have been shown to not only benefit compositional models in learning to reason along with the structures but provide higher interpretability for model decisions. Nevertheless, these representations receive much less attention than traditional recognition tasks, leaving numerous open challenges unsolved. In the thesis, we study how machines can describe the content of the individual image or video with visual relationships as the structured representations. Specifically, we explore how structured representations of visual scenes can be effectively constructed and learned in both the static-image and video settings, with improvements resulting from external knowledge incorporation, bias-reducing mechanism, and enhanced representation models. At the end of this thesis, we also discuss some open challenges and limitations to shed light on future directions of structured representation learning for visual scenes.



### Smart Multi-tenant Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2207.04202v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2207.04202v1)
- **Published**: 2022-07-09 06:22:39+00:00
- **Updated**: 2022-07-09 06:22:39+00:00
- **Authors**: Weiming Zhuang, Yonggang Wen, Shuai Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Federated learning (FL) is an emerging distributed machine learning method that empowers in-situ model training on decentralized edge devices. However, multiple simultaneous training activities could overload resource-constrained devices. In this work, we propose a smart multi-tenant FL system, MuFL, to effectively coordinate and execute simultaneous training activities. We first formalize the problem of multi-tenant FL, define multi-tenant FL scenarios, and introduce a vanilla multi-tenant FL system that trains activities sequentially to form baselines. Then, we propose two approaches to optimize multi-tenant FL: 1) activity consolidation merges training activities into one activity with a multi-task architecture; 2) after training it for rounds, activity splitting divides it into groups by employing affinities among activities such that activities within a group have better synergy. Extensive experiments demonstrate that MuFL outperforms other methods while consuming 40% less energy. We hope this work will inspire the community to further study and optimize multi-tenant FL.



### Variational Approach for Intensity Domain Multi-exposure Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/2207.04204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04204v1)
- **Published**: 2022-07-09 06:31:34+00:00
- **Updated**: 2022-07-09 06:31:34+00:00
- **Authors**: Harbinder Singh, Dinesh Arora, Vinay Kumar
- **Comment**: 18 pages, 6 figures, Book Chapter
- **Journal**: None
- **Summary**: Recent innovations shows that blending of details captured by single Low Dynamic Range (LDR) sensor overcomes the limitations of standard digital cameras to capture details from high dynamic range scene. We present a method to produce well-exposed fused image that can be displayed directly on conventional display devices. The ambition is to preserve details in poorly illuminated and brightly illuminated regions. Proposed approach does not require true radiance reconstruction and tone manipulation steps. The aforesaid objective is achieved by taking into account local information measure that select well-exposed regions across input exposures. In addition, Contrast Limited Adaptive Histogram equalization (CLAHE) is introduced to improve uniformity of input multi-exposure image prior to fusion.



### BOSS: Bottom-up Cross-modal Semantic Composition with Hybrid Counterfactual Training for Robust Content-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2207.04211v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.IR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04211v1)
- **Published**: 2022-07-09 07:14:44+00:00
- **Updated**: 2022-07-09 07:14:44+00:00
- **Authors**: Wenqiao Zhang, Jiannan Guo, Mengze Li, Haochen Shi, Shengyu Zhang, Juncheng Li, Siliang Tang, Yueting Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: Content-Based Image Retrieval (CIR) aims to search for a target image by concurrently comprehending the composition of an example image and a complementary text, which potentially impacts a wide variety of real-world applications, such as internet search and fashion retrieval. In this scenario, the input image serves as an intuitive context and background for the search, while the corresponding language expressly requests new traits on how specific characteristics of the query image should be modified in order to get the intended target image. This task is challenging since it necessitates learning and understanding the composite image-text representation by incorporating cross-granular semantic updates. In this paper, we tackle this task by a novel \underline{\textbf{B}}ottom-up cr\underline{\textbf{O}}ss-modal \underline{\textbf{S}}emantic compo\underline{\textbf{S}}ition (\textbf{BOSS}) with Hybrid Counterfactual Training framework, which sheds new light on the CIR task by studying it from two previously overlooked perspectives: \emph{implicitly bottom-up composition of visiolinguistic representation} and \emph{explicitly fine-grained correspondence of query-target construction}. On the one hand, we leverage the implicit interaction and composition of cross-modal embeddings from the bottom local characteristics to the top global semantics, preserving and transforming the visual representation conditioned on language semantics in several continuous steps for effective target image search. On the other hand, we devise a hybrid counterfactual training strategy that can reduce the model's ambiguity for similar queries.



### COVID-19 Disease Identification on Chest-CT images using CNN and VGG16
- **Arxiv ID**: http://arxiv.org/abs/2207.04212v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.04212v1)
- **Published**: 2022-07-09 07:20:15+00:00
- **Updated**: 2022-07-09 07:20:15+00:00
- **Authors**: Briskline Kiruba S, Petchiammal A, D. Murugan
- **Comment**: None
- **Journal**: None
- **Summary**: A newly identified coronavirus disease called COVID-19 mainly affects the human respiratory system. COVID-19 is an infectious disease caused by a virus originating in Wuhan, China, in December 2019. Early diagnosis is the primary challenge of health care providers. In the earlier stage, medical organizations were dazzled because there were no proper health aids or medicine to detect a COVID-19. A new diagnostic tool RT-PCR (Reverse Transcription Polymerase Chain Reaction), was introduced. It collects swab specimens from the patient's nose or throat, where the COVID-19 virus gathers. This method has some limitations related to accuracy and testing time. Medical experts suggest an alternative approach called CT (Computed Tomography) that can quickly diagnose the infected lung areas and identify the COVID-19 in an earlier stage. Using chest CT images, computer researchers developed several deep learning models identifying the COVID-19 disease. This study presents a Convolutional Neural Network (CNN) and VGG16-based model for automated COVID-19 identification on chest CT images. The experimental results using a public dataset of 14320 CT images showed a classification accuracy of 96.34% and 96.99% for CNN and VGG16, respectively.



### Dual-Path Cross-Modal Attention for better Audio-Visual Speech Extraction
- **Arxiv ID**: http://arxiv.org/abs/2207.04213v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2207.04213v2)
- **Published**: 2022-07-09 07:27:46+00:00
- **Updated**: 2023-03-03 21:01:32+00:00
- **Authors**: Zhongweiyang Xu, Xulin Fan, Mark Hasegawa-Johnson
- **Comment**: Paper Accepted by ICASSP2023
- **Journal**: None
- **Summary**: Audio-visual target speech extraction, which aims to extract a certain speaker's speech from the noisy mixture by looking at lip movements, has made significant progress combining time-domain speech separation models and visual feature extractors (CNN). One problem of fusing audio and video information is that they have different time resolutions. Most current research upsamples the visual features along the time dimension so that audio and video features are able to align in time. However, we believe that lip movement should mostly contain long-term, or phone-level information. Based on this assumption, we propose a new way to fuse audio-visual features. We observe that for DPRNN \cite{dprnn}, the interchunk dimension's time resolution could be very close to the time resolution of video frames. Like \cite{sepformer}, the LSTM in DPRNN is replaced by intra-chunk and inter-chunk self-attention, but in the proposed algorithm, inter-chunk attention incorporates the visual features as an additional feature stream. This prevents the upsampling of visual cues, resulting in more efficient audio-visual fusion. The result shows we achieve superior results compared with other time-domain based audio-visual fusion models.



### Rethinking Persistent Homology for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2207.04220v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04220v3)
- **Published**: 2022-07-09 08:01:11+00:00
- **Updated**: 2023-03-06 00:22:57+00:00
- **Authors**: Ekaterina Khramtsova, Guido Zuccon, Xi Wang, Mahsa Baktashmotlagh
- **Comment**: ICML 2022 Workshop on Topology, Algebra, and Geometry in Machine
  Learning
- **Journal**: None
- **Summary**: Persistent topological properties of an image serve as an additional descriptor providing an insight that might not be discovered by traditional neural networks. The existing research in this area focuses primarily on efficiently integrating topological properties of the data in the learning process in order to enhance the performance. However, there is no existing study to demonstrate all possible scenarios where introducing topological properties can boost or harm the performance. This paper performs a detailed analysis of the effectiveness of topological properties for image classification in various training scenarios, defined by: the number of training samples, the complexity of the training data and the complexity of the backbone network. We identify the scenarios that benefit the most from topological features, e.g., training simple networks on small datasets. Additionally, we discuss the problem of topological consistency of the datasets which is one of the major bottlenecks for using topological features for classification. We further demonstrate how the topological inconsistency can harm the performance for certain scenarios.



### Learning to Register Unbalanced Point Pairs
- **Arxiv ID**: http://arxiv.org/abs/2207.04221v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04221v2)
- **Published**: 2022-07-09 08:03:59+00:00
- **Updated**: 2022-10-15 18:41:42+00:00
- **Authors**: Kanghee Lee, Junha Lee, Jaesik Park
- **Comment**: None
- **Journal**: None
- **Summary**: Point cloud registration methods can effectively handle large-scale, partially overlapping point cloud pairs. Despite its practicality, matching the unbalanced pairs in terms of spatial extent and density has been overlooked and rarely studied. We present a novel method, dubbed UPPNet, for Unbalanced Point cloud Pair registration. We propose to incorporate a hierarchical framework that effectively finds inlier correspondences by gradually reducing search space. The proposed method first predicts subregions within target point cloud that are likely to be overlapped with query. Then following super-point matching and fine-grained refinement modules predict accurate inlier correspondences between the target and query. Additional geometric constraints are applied to refine the correspondences that satisfy spatial compatibility. The proposed network can be trained in an end-to-end manner, predicting the accurate rigid transformation with a single forward pass. To validate the efficacy of the proposed method, we create a carefully designed benchmark, named KITTI-UPP dataset, by augmenting the KITTI odometry dataset. Extensive experiments reveal that the proposed method not only outperforms state-of-the-art point cloud registration methods by large margins on KITTI-UPP benchmark, but also achieves competitive results on the standard pairwise registration benchmark including 3DMatch, 3DLoMatch, ScanNet, and KITTI, thus showing the applicability of our method on various datasets. The source code and dataset will be publicly released.



### SiaTrans: Siamese Transformer Network for RGB-D Salient Object Detection with Depth Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2207.04224v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2207.04224v1)
- **Published**: 2022-07-09 08:22:12+00:00
- **Updated**: 2022-07-09 08:22:12+00:00
- **Authors**: Xingzhao Jia, Dongye Changlei, Yanjun Peng
- **Comment**: 21 pages, 7 figures
- **Journal**: None
- **Summary**: RGB-D SOD uses depth information to handle challenging scenes and obtain high-quality saliency maps. Existing state-of-the-art RGB-D saliency detection methods overwhelmingly rely on the strategy of directly fusing depth information. Although these methods improve the accuracy of saliency prediction through various cross-modality fusion strategies, misinformation provided by some poor-quality depth images can affect the saliency prediction result. To address this issue, a novel RGB-D salient object detection model (SiaTrans) is proposed in this paper, which allows training on depth image quality classification at the same time as training on SOD. In light of the common information between RGB and depth images on salient objects, SiaTrans uses a Siamese transformer network with shared weight parameters as the encoder and extracts RGB and depth features concatenated on the batch dimension, saving space resources without compromising performance. SiaTrans uses the Class token in the backbone network (T2T-ViT) to classify the quality of depth images without preventing the token sequence from going on with the saliency detection task. Transformer-based cross-modality fusion module (CMF) can effectively fuse RGB and depth information. And in the testing process, CMF can choose to fuse cross-modality information or enhance RGB information according to the quality classification signal of the depth image. The greatest benefit of our designed CMF and decoder is that they maintain the consistency of RGB and RGB-D information decoding: SiaTrans decodes RGB-D or RGB information under the same model parameters according to the classification signal during testing. Comprehensive experiments on nine RGB-D SOD benchmark datasets show that SiaTrans has the best overall performance and the least computation compared with recent state-of-the-art methods.



### Batch-efficient EigenDecomposition for Small and Medium Matrices
- **Arxiv ID**: http://arxiv.org/abs/2207.04228v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2207.04228v1)
- **Published**: 2022-07-09 09:14:12+00:00
- **Updated**: 2022-07-09 09:14:12+00:00
- **Authors**: Yue Song, Nicu Sebe, Wei Wang
- **Comment**: Accepted by ECCV22
- **Journal**: None
- **Summary**: EigenDecomposition (ED) is at the heart of many computer vision algorithms and applications. One crucial bottleneck limiting its usage is the expensive computation cost, particularly for a mini-batch of matrices in the deep neural networks. In this paper, we propose a QR-based ED method dedicated to the application scenarios of computer vision. Our proposed method performs the ED entirely by batched matrix/vector multiplication, which processes all the matrices simultaneously and thus fully utilizes the power of GPUs. Our technique is based on the explicit QR iterations by Givens rotation with double Wilkinson shifts. With several acceleration techniques, the time complexity of QR iterations is reduced from $O{(}n^5{)}$ to $O{(}n^3{)}$. The numerical test shows that for small and medium batched matrices (\emph{e.g.,} $dim{<}32$) our method can be much faster than the Pytorch SVD function. Experimental results on visual recognition and image generation demonstrate that our methods also achieve competitive performances.



### Sparse Ellipsometry: Portable Acquisition of Polarimetric SVBRDF and Shape with Unstructured Flash Photography
- **Arxiv ID**: http://arxiv.org/abs/2207.04236v2
- **DOI**: 10.1145/3528223.3530075
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04236v2)
- **Published**: 2022-07-09 09:42:59+00:00
- **Updated**: 2023-02-08 12:27:13+00:00
- **Authors**: Inseung Hwang, Daniel S. Jeon, Adolfo Muñoz, Diego Gutierrez, Xin Tong, Min H. Kim
- **Comment**: None
- **Journal**: ACM Transactions on Graphics 41, 4, Article 133 (July 2022)
- **Summary**: Ellipsometry techniques allow to measure polarization information of materials, requiring precise rotations of optical components with different configurations of lights and sensors. This results in cumbersome capture devices, carefully calibrated in lab conditions, and in very long acquisition times, usually in the order of a few days per object. Recent techniques allow to capture polarimetric spatially-varying reflectance information, but limited to a single view, or to cover all view directions, but limited to spherical objects made of a single homogeneous material. We present sparse ellipsometry, a portable polarimetric acquisition method that captures both polarimetric SVBRDF and 3D shape simultaneously. Our handheld device consists of off-the-shelf, fixed optical components. Instead of days, the total acquisition time varies between twenty and thirty minutes per object. We develop a complete polarimetric SVBRDF model that includes diffuse and specular components, as well as single scattering, and devise a novel polarimetric inverse rendering algorithm with data augmentation of specular reflection samples via generative modeling. Our results show a strong agreement with a recent ground-truth dataset of captured polarimetric BRDFs of real-world objects.



### PI-Trans: Parallel-ConvMLP and Implicit-Transformation Based GAN for Cross-View Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2207.04242v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04242v2)
- **Published**: 2022-07-09 10:35:44+00:00
- **Updated**: 2023-03-06 09:54:55+00:00
- **Authors**: Bin Ren, Hao Tang, Yiming Wang, Xia Li, Wei Wang, Nicu Sebe
- **Comment**: 5 pages, 5 figures
- **Journal**: 2023 IEEE International Conference on Acoustics, Speech and Signal
  Processing
- **Summary**: For semantic-guided cross-view image translation, it is crucial to learn where to sample pixels from the source view image and where to reallocate them guided by the target view semantic map, especially when there is little overlap or drastic view difference between the source and target images. Hence, one not only needs to encode the long-range dependencies among pixels in both the source view image and target view semantic map but also needs to translate these learned dependencies. To this end, we propose a novel generative adversarial network, PI-Trans, which mainly consists of a novel Parallel-ConvMLP module and an Implicit Transformation module at multiple semantic levels. Extensive experimental results show that PI-Trans achieves the best qualitative and quantitative performance by a large margin compared to the state-of-the-art methods on two challenging datasets. The source code is available at https://github.com/Amazingren/PI-Trans.



### Improving saliency models' predictions of the next fixation with humans' intrinsic cost of gaze shifts
- **Arxiv ID**: http://arxiv.org/abs/2207.04250v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04250v2)
- **Published**: 2022-07-09 11:21:13+00:00
- **Updated**: 2022-09-16 12:25:13+00:00
- **Authors**: Florian Kadner, Tobias Thomas, David Hoppe, Constantin A. Rothkopf
- **Comment**: None
- **Journal**: None
- **Summary**: The human prioritization of image regions can be modeled in a time invariant fashion with saliency maps or sequentially with scanpath models. However, while both types of models have steadily improved on several benchmarks and datasets, there is still a considerable gap in predicting human gaze. Here, we leverage two recent developments to reduce this gap: theoretical analyses establishing a principled framework for predicting the next gaze target and the empirical measurement of the human cost for gaze switches independently of image content. We introduce an algorithm in the framework of sequential decision making, which converts any static saliency map into a sequence of dynamic history-dependent value maps, which are recomputed after each gaze shift. These maps are based on 1) a saliency map provided by an arbitrary saliency model, 2) the recently measured human cost function quantifying preferences in magnitude and direction of eye movements, and 3) a sequential exploration bonus, which changes with each subsequent gaze shift. The parameters of the spatial extent and temporal decay of this exploration bonus are estimated from human gaze data. The relative contributions of these three components were optimized on the MIT1003 dataset for the NSS score and are sufficient to significantly outperform predictions of the next gaze target on NSS and AUC scores for five state of the art saliency models on three image data sets. Thus, we provide an implementation of human gaze preferences, which can be used to improve arbitrary saliency models' predictions of humans' next gaze targets.



### Rank-Enhanced Low-Dimensional Convolution Set for Hyperspectral Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/2207.04266v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04266v1)
- **Published**: 2022-07-09 13:35:12+00:00
- **Updated**: 2022-07-09 13:35:12+00:00
- **Authors**: Jinhui Hou, Zhiyu Zhu, Hui Liu, Junhui Hou
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: This paper tackles the challenging problem of hyperspectral (HS) image denoising. Unlike existing deep learning-based methods usually adopting complicated network architectures or empirically stacking off-the-shelf modules to pursue performance improvement, we focus on the efficient and effective feature extraction manner for capturing the high-dimensional characteristics of HS images. To be specific, based on the theoretical analysis that increasing the rank of the matrix formed by the unfolded convolutional kernels can promote feature diversity, we propose rank-enhanced low-dimensional convolution set (Re-ConvSet), which separately performs 1-D convolution along the three dimensions of an HS image side-by-side, and then aggregates the resulting spatial-spectral embeddings via a learnable compression layer. Re-ConvSet not only learns the diverse spatial-spectral features of HS images, but also reduces the parameters and complexity of the network. We then incorporate Re-ConvSet into the widely-used U-Net architecture to construct an HS image denoising method. Surprisingly, we observe such a concise framework outperforms the most recent method to a large extent in terms of quantitative metrics, visual results, and efficiency. We believe our work may shed light on deep learning-based HS image processing and analysis.



### Explainable AI (XAI) in Biomedical Signal and Image Processing: Promises and Challenges
- **Arxiv ID**: http://arxiv.org/abs/2207.04295v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04295v1)
- **Published**: 2022-07-09 16:27:41+00:00
- **Updated**: 2022-07-09 16:27:41+00:00
- **Authors**: Guang Yang, Arvind Rao, Christine Fernandez-Maloigne, Vince Calhoun, Gloria Menegaz
- **Comment**: IEEE ICIP 2022
- **Journal**: None
- **Summary**: Artificial intelligence has become pervasive across disciplines and fields, and biomedical image and signal processing is no exception. The growing and widespread interest on the topic has triggered a vast research activity that is reflected in an exponential research effort. Through study of massive and diverse biomedical data, machine and deep learning models have revolutionized various tasks such as modeling, segmentation, registration, classification and synthesis, outperforming traditional techniques. However, the difficulty in translating the results into biologically/clinically interpretable information is preventing their full exploitation in the field. Explainable AI (XAI) attempts to fill this translational gap by providing means to make the models interpretable and providing explanations. Different solutions have been proposed so far and are gaining increasing interest from the community. This paper aims at providing an overview on XAI in biomedical data processing and points to an upcoming Special Issue on Deep Learning in Biomedical Image and Signal Processing of the IEEE Signal Processing Magazine that is going to appear in March 2022.



### SHDM-NET: Heat Map Detail Guidance with Image Matting for Industrial Weld Semantic Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/2207.04297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04297v1)
- **Published**: 2022-07-09 16:38:33+00:00
- **Updated**: 2022-07-09 16:38:33+00:00
- **Authors**: Qi Wang, Jingwu Mei
- **Comment**: None
- **Journal**: None
- **Summary**: In actual industrial production, the assessment of the steel plate welding effect is an important task, and the segmentation of the weld section is the basis of the assessment. This paper proposes an industrial weld segmentation network based on a deep learning semantic segmentation algorithm fused with heatmap detail guidance and Image Matting to solve the automatic segmentation problem of weld regions. In the existing semantic segmentation networks, the boundary information can be preserved by fusing the features of both high-level and low-level layers. However, this method can lead to insufficient expression of the spatial information in the low-level layer, resulting in inaccurate segmentation boundary positioning. We propose a detailed guidance module based on heatmaps to fully express the segmented region boundary information in the low-level network to address this problem. Specifically, the expression of boundary information can be enhanced by adding a detailed branch to predict segmented boundary and then matching it with the boundary heat map generated by mask labels to calculate the mean square error loss. In addition, although deep learning has achieved great success in the field of semantic segmentation, the precision of the segmentation boundary region is not high due to the loss of detailed information caused by the classical segmentation network in the process of encoding and decoding process. This paper introduces a matting algorithm to calibrate the boundary of the segmentation region of the semantic segmentation network to solve this problem. Through many experiments on industrial weld data sets, the effectiveness of our method is demonstrated, and the MIOU reaches 97.93%. It is worth noting that this performance is comparable to human manual segmentation ( MIOU 97.96%).



### QKVA grid: Attention in Image Perspective and Stacked DETR
- **Arxiv ID**: http://arxiv.org/abs/2207.04313v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2207.04313v2)
- **Published**: 2022-07-09 18:05:43+00:00
- **Updated**: 2022-08-16 14:42:08+00:00
- **Authors**: Wenyuan Sheng
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new model named Stacked-DETR(SDETR), which inherits the main ideas in canonical DETR. We improve DETR in two directions: simplifying the cost of training and introducing the stacked architecture to enhance the performance. To the former, we focus on the inside of the Attention block and propose the QKVA grid, a new perspective to describe the process of attention. By this, we can step further on how Attention works for image problems and the effect of multi-head. These two ideas contribute the design of single-head encoder-layer. To the latter, SDETR reaches better performance(+0.6AP, +2.7APs) to DETR. Especially to the performance on small objects, SDETR achieves better results to the optimized Faster R-CNN baseline, which was a shortcoming in DETR. Our changes are based on the code of DETR. Training code and pretrained models are available at https://github.com/shengwenyuan/sdetr.



### Improving Diffusion Model Efficiency Through Patching
- **Arxiv ID**: http://arxiv.org/abs/2207.04316v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04316v1)
- **Published**: 2022-07-09 18:21:32+00:00
- **Updated**: 2022-07-09 18:21:32+00:00
- **Authors**: Troy Luhman, Eric Luhman
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion models are a powerful class of generative models that iteratively denoise samples to produce data. While many works have focused on the number of iterations in this sampling procedure, few have focused on the cost of each iteration. We find that adding a simple ViT-style patching transformation can considerably reduce a diffusion model's sampling time and memory usage. We justify our approach both through an analysis of the diffusion model objective, and through empirical experiments on LSUN Church, ImageNet 256, and FFHQ 1024. We provide implementations in Tensorflow and Pytorch.



### Snipper: A Spatiotemporal Transformer for Simultaneous Multi-Person 3D Pose Estimation Tracking and Forecasting on a Video Snippet
- **Arxiv ID**: http://arxiv.org/abs/2207.04320v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2207.04320v2)
- **Published**: 2022-07-09 18:42:14+00:00
- **Updated**: 2022-07-13 07:55:51+00:00
- **Authors**: Shihao Zou, Yuanlu Xu, Chao Li, Lingni Ma, Li Cheng, Minh Vo
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-person pose understanding from RGB videos includes three complex tasks: pose estimation, tracking and motion forecasting. Among these three tasks, pose estimation and tracking are correlated, and tracking is crucial to motion forecasting. Most existing works either focus on a single task or employ cascaded methods to solve each individual task separately. In this paper, we propose Snipper, a framework to perform multi-person 3D pose estimation, tracking and motion forecasting simultaneously in a single inference. Specifically, we first propose a deformable attention mechanism to aggregate spatiotemporal information from video snippets. Building upon this deformable attention, a visual transformer is learned to encode the spatiotemporal features from multi-frame images and to decode informative pose features to update multi-person pose queries. Last, these queries are regressed to predict multi-person pose trajectories and future motions in one forward pass. In the experiments, we show the effectiveness of Snipper on three challenging public datasets where a generic model rivals specialized state-of-art baselines for pose estimation, tracking, and forecasting. Code is available at https://github.com/JimmyZou/Snipper



### Video Coding Using Learned Latent GAN Compression
- **Arxiv ID**: http://arxiv.org/abs/2207.04324v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2207.04324v2)
- **Published**: 2022-07-09 19:07:43+00:00
- **Updated**: 2022-07-12 21:17:39+00:00
- **Authors**: Mustafa Shukor, Bharath Bhushan Damodaran, Xu Yao, Pierre Hellier
- **Comment**: Accepted at ACM Multimedia 2022
- **Journal**: None
- **Summary**: We propose in this paper a new paradigm for facial video compression. We leverage the generative capacity of GANs such as StyleGAN to represent and compress a video, including intra and inter compression. Each frame is inverted in the latent space of StyleGAN, from which the optimal compression is learned. To do so, a diffeomorphic latent representation is learned using a normalizing flows model, where an entropy model can be optimized for image coding. In addition, we propose a new perceptual loss that is more efficient than other counterparts. Finally, an entropy model for video inter coding with residual is also learned in the previously constructed latent representation. Our method (SGANC) is simple, faster to train, and achieves better results for image and video coding compared to state-of-the-art codecs such as VTM, AV1, and recent deep learning techniques. In particular, it drastically minimizes perceptual distortion at low bit rates.



### Unsupervised Joint Image Transfer and Uncertainty Quantification Using Patch Invariant Networks
- **Arxiv ID**: http://arxiv.org/abs/2207.04325v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2207.04325v2)
- **Published**: 2022-07-09 19:13:00+00:00
- **Updated**: 2022-09-02 08:16:15+00:00
- **Authors**: Christoph Angermann, Markus Haltmeier, Ahsan Raza Siyal
- **Comment**: Accepted to ECCV 2022 Workshop on Uncertainty Quantification for
  Computer Vision (UNCV 2022)
- **Journal**: None
- **Summary**: Unsupervised image transfer enables intra- and inter-modality image translation in applications where a large amount of paired training data is not abundant. To ensure a structure-preserving mapping from the input to the target domain, existing methods for unpaired image transfer are commonly based on cycle-consistency, causing additional computational resources and instability due to the learning of an inverse mapping. This paper presents a novel method for uni-directional domain mapping that does not rely on any paired training data. A proper transfer is achieved by using a GAN architecture and a novel generator loss based on patch invariance. To be more specific, the generator outputs are evaluated and compared at different scales, also leading to an increased focus on high-frequency details as well as an implicit data augmentation. This novel patch loss also offers the possibility to accurately predict aleatoric uncertainty by modeling an input-dependent scale map for the patch residuals. The proposed method is comprehensively evaluated on three well-established medical databases. As compared to four state-of-the-art methods, we observe significantly higher accuracy on these datasets, indicating great potential of the proposed method for unpaired image transfer with uncertainty taken into account. Implementation of the proposed framework is released here: \url{https://github.com/anger-man/unsupervised-image-transfer-and-uq}.



### Explaining Chest X-ray Pathologies in Natural Language
- **Arxiv ID**: http://arxiv.org/abs/2207.04343v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2207.04343v1)
- **Published**: 2022-07-09 22:09:37+00:00
- **Updated**: 2022-07-09 22:09:37+00:00
- **Authors**: Maxime Kayser, Cornelius Emde, Oana-Maria Camburu, Guy Parsons, Bartlomiej Papiez, Thomas Lukasiewicz
- **Comment**: None
- **Journal**: MICCAI 2022
- **Summary**: Most deep learning algorithms lack explanations for their predictions, which limits their deployment in clinical practice. Approaches to improve explainability, especially in medical imaging, have often been shown to convey limited information, be overly reassuring, or lack robustness. In this work, we introduce the task of generating natural language explanations (NLEs) to justify predictions made on medical images. NLEs are human-friendly and comprehensive, and enable the training of intrinsically explainable models. To this goal, we introduce MIMIC-NLE, the first, large-scale, medical imaging dataset with NLEs. It contains over 38,000 NLEs, which explain the presence of various thoracic pathologies and chest X-ray findings. We propose a general approach to solve the task and evaluate several architectures on this dataset, including via clinician assessment.



### Segmentation of Blood Vessels, Optic Disc Localization, Detection of Exudates and Diabetic Retinopathy Diagnosis from Digital Fundus Images
- **Arxiv ID**: http://arxiv.org/abs/2207.04345v1
- **DOI**: 10.1007/978-981-16-1543-6_16
- **Categories**: **eess.IV**, cs.CV, I.4.6; I.5.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2207.04345v1)
- **Published**: 2022-07-09 22:26:04+00:00
- **Updated**: 2022-07-09 22:26:04+00:00
- **Authors**: Soham Basu, Sayantan Mukherjee, Ankit Bhattacharya, Anindya Sen
- **Comment**: RAAI 2020, 11 pages, 12 figures, 2 tables
- **Journal**: Proceedings of Research and Applications in Artificial
  Intelligence. Advances in Intelligent Systems and Computing, vol. 1355 (2021)
  pp.173-184. Springer, Singapore
- **Summary**: Diabetic Retinopathy (DR) is a complication of long-standing, unchecked diabetes and one of the leading causes of blindness in the world. This paper focuses on improved and robust methods to extract some of the features of DR, viz. Blood Vessels and Exudates. Blood vessels are segmented using multiple morphological and thresholding operations. For the segmentation of exudates, k-means clustering and contour detection on the original images are used. Extensive noise reduction is performed to remove false positives from the vessel segmentation algorithm's results. The localization of Optic Disc using k-means clustering and template matching is also performed. Lastly, this paper presents a Deep Convolutional Neural Network (DCNN) model with 14 Convolutional Layers and 2 Fully Connected Layers, for the automatic, binary diagnosis of DR. The vessel segmentation, optic disc localization and DCNN achieve accuracies of 95.93%, 98.77% and 75.73% respectively. The source code and pre-trained model are available https://github.com/Sohambasu07/DR_2021



