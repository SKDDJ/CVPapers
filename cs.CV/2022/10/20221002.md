# Arxiv Papers in cs.CV on 2022-10-02
### Basic Binary Convolution Unit for Binarized Image Restoration Network
- **Arxiv ID**: http://arxiv.org/abs/2210.00405v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.00405v2)
- **Published**: 2022-10-02 01:54:40+00:00
- **Updated**: 2023-02-16 11:41:32+00:00
- **Authors**: Bin Xia, Yulun Zhang, Yitong Wang, Yapeng Tian, Wenming Yang, Radu Timofte, Luc Van Gool
- **Comment**: ICLR2023, code is available at https://github.com/Zj-BinXia/BBCU
- **Journal**: None
- **Summary**: Lighter and faster image restoration (IR) models are crucial for the deployment on resource-limited devices. Binary neural network (BNN), one of the most promising model compression methods, can dramatically reduce the computations and parameters of full-precision convolutional neural networks (CNN). However, there are different properties between BNN and full-precision CNN, and we can hardly use the experience of designing CNN to develop BNN. In this study, we reconsider components in binary convolution, such as residual connection, BatchNorm, activation function, and structure, for IR tasks. We conduct systematic analyses to explain each component's role in binary convolution and discuss the pitfalls. Specifically, we find that residual connection can reduce the information loss caused by binarization; BatchNorm can solve the value range gap between residual connection and binary convolution; The position of the activation function dramatically affects the performance of BNN. Based on our findings and analyses, we design a simple yet efficient basic binary convolution unit (BBCU). Furthermore, we divide IR networks into four parts and specially design variants of BBCU for each part to explore the benefit of binarizing these parts. We conduct experiments on different IR tasks, and our BBCU significantly outperforms other BNNs and lightweight models, which shows that BBCU can serve as a basic unit for binarized IR networks. All codes and models will be released.



### PCONet: A Convolutional Neural Network Architecture to Detect Polycystic Ovary Syndrome (PCOS) from Ovarian Ultrasound Images
- **Arxiv ID**: http://arxiv.org/abs/2210.00407v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.00407v1)
- **Published**: 2022-10-02 02:31:03+00:00
- **Updated**: 2022-10-02 02:31:03+00:00
- **Authors**: A. K. M. Salman Hosain, Md Humaion Kabir Mehedi, Irteza Enan Kabir
- **Comment**: None
- **Journal**: None
- **Summary**: Polycystic Ovary Syndrome (PCOS) is an endrocrinological dysfunction prevalent among women of reproductive age. PCOS is a combination of syndromes caused by an excess of androgens - a group of sex hormones - in women. Syndromes including acne, alopecia, hirsutism, hyperandrogenaemia, oligo-ovulation, etc. are caused by PCOS. It is also a major cause of female infertility. An estimated 15% of reproductive-aged women are affected by PCOS globally. The necessity of detecting PCOS early due to the severity of its deleterious effects cannot be overstated. In this paper, we have developed PCONet - a Convolutional Neural Network (CNN) - to detect polycistic ovary from ovarian ultrasound images. We have also fine tuned InceptionV3 - a pretrained convolutional neural network of 45 layers - by utilizing the transfer learning method to classify polcystic ovarian ultrasound images. We have compared these two models on various quantitative performance evaluation parameters and demonstrated that PCONet is the superior one among these two with an accuracy of 98.12%, whereas the fine tuned InceptionV3 showcased an accuracy of 96.56% on test images.



### Self-Supervised Monocular Depth Estimation: Solving the Edge-Fattening Problem
- **Arxiv ID**: http://arxiv.org/abs/2210.00411v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.00411v3)
- **Published**: 2022-10-02 03:08:59+00:00
- **Updated**: 2023-01-03 10:18:41+00:00
- **Authors**: Xingyu Chen, Ruonan Zhang, Ji Jiang, Yan Wang, Ge Li, Thomas H. Li
- **Comment**: 8 pages, 7 figures, published to WACV2023
- **Journal**: None
- **Summary**: Self-supervised monocular depth estimation (MDE) models universally suffer from the notorious edge-fattening issue. Triplet loss, as a widespread metric learning strategy, has largely succeeded in many computer vision applications. In this paper, we redesign the patch-based triplet loss in MDE to alleviate the ubiquitous edge-fattening issue. We show two drawbacks of the raw triplet loss in MDE and demonstrate our problem-driven redesigns. First, we present a min. operator based strategy applied to all negative samples, to prevent well-performing negatives sheltering the error of edge-fattening negatives. Second, we split the anchor-positive distance and anchor-negative distance from within the original triplet, which directly optimizes the positives without any mutual effect with the negatives. Extensive experiments show the combination of these two small redesigns can achieve unprecedented results: Our powerful and versatile triplet loss not only makes our model outperform all previous SoTA by a large margin, but also provides substantial performance boosts to a large number of existing models, while introducing no extra inference computation at all.



### Cross-identity Video Motion Retargeting with Joint Transformation and Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2210.01559v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01559v1)
- **Published**: 2022-10-02 03:09:12+00:00
- **Updated**: 2022-10-02 03:09:12+00:00
- **Authors**: Haomiao Ni, Yihao Liu, Sharon X. Huang, Yuan Xue
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: In this paper, we propose a novel dual-branch Transformation-Synthesis network (TS-Net), for video motion retargeting. Given one subject video and one driving video, TS-Net can produce a new plausible video with the subject appearance of the subject video and motion pattern of the driving video. TS-Net consists of a warp-based transformation branch and a warp-free synthesis branch. The novel design of dual branches combines the strengths of deformation-grid-based transformation and warp-free generation for better identity preservation and robustness to occlusion in the synthesized videos. A mask-aware similarity module is further introduced to the transformation branch to reduce computational overhead. Experimental results on face and dance datasets show that TS-Net achieves better performance in video motion retargeting than several state-of-the-art models as well as its single-branch variants. Our code is available at https://github.com/nihaomiao/WACV23_TSNet.



### Unsupervised Visual Odometry and Action Integration for PointGoal Navigation in Indoor Environment
- **Arxiv ID**: http://arxiv.org/abs/2210.00413v2
- **DOI**: 10.1109/TCSVT.2023.3263484
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00413v2)
- **Published**: 2022-10-02 03:12:03+00:00
- **Updated**: 2023-04-03 09:18:10+00:00
- **Authors**: Yijun Cao, Xianshi Zhang, Fuya Luo, Chuan Lin, Yongjie Li
- **Comment**: 12 pages, 6 figures
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology,
  2023
- **Summary**: PointGoal navigation in indoor environment is a fundamental task for personal robots to navigate to a specified point. Recent studies solved this PointGoal navigation task with near-perfect success rate in photo-realistically simulated environments, under the assumptions with noiseless actuation and most importantly, perfect localization with GPS and compass sensors. However, accurate GPS signalis difficult to be obtained in real indoor environment. To improve the PointGoal navigation accuracy without GPS signal, we use visual odometry (VO) and propose a novel action integration module (AIM) trained in unsupervised manner. Sepecifically, unsupervised VO computes the relative pose of the agent from the re-projection error of two adjacent frames, and then replaces the accurate GPS signal with the path integration. The pseudo position estimated by VO is used to train action integration which assists agent to update their internal perception of location and helps improve the success rate of navigation. The training and inference process only use RGB, depth, collision as well as self-action information. The experiments show that the proposed system achieves satisfactory results and outperforms the partially supervised learning algorithms on the popular Gibson dataset.



### ROSIA: Rotation-Search-Based Star Identification Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2210.00429v2
- **DOI**: 10.1109/TAES.2023.3279353
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00429v2)
- **Published**: 2022-10-02 05:34:19+00:00
- **Updated**: 2023-08-29 02:32:22+00:00
- **Authors**: Chee-Kheng Chng, Alvaro Parra Bustos, Benjamin McCarthy, Tat-Jun Chin
- **Comment**: 21 pages, 16 figures, Accepted to IEEE Transactions on Aerospace and
  Electronic Systems
- **Journal**: None
- **Summary**: This paper presents a rotation-search-based approach for addressing the star identification (Star-ID) problem. The proposed algorithm, ROSIA, is a heuristics-free algorithm that seeks the optimal rotation that maximally aligns the input and catalog stars in their respective coordinates. ROSIA searches the rotation space systematically with the Branch-and-Bound (BnB) method. Crucially affecting the runtime feasibility of ROSIA is the upper bound function that prioritizes the search space. In this paper, we make a theoretical contribution by proposing a tight (provable) upper bound function that enables a 400x speed-up compared to an existing formulation. Coupling the bounding function with an efficient evaluation scheme that leverages stereographic projection and the R-tree data structure, ROSIA achieves feasible operational speed on embedded processors with state-of-the-art performances under different sources of noise. The source code of ROSIA is available at https://github.com/ckchng/ROSIA.



### ManiCLIP: Multi-Attribute Face Manipulation from Text
- **Arxiv ID**: http://arxiv.org/abs/2210.00445v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00445v3)
- **Published**: 2022-10-02 07:22:55+00:00
- **Updated**: 2023-03-26 01:52:42+00:00
- **Authors**: Hao Wang, Guosheng Lin, Ana García del Molino, Anran Wang, Jiashi Feng, Zhiqi Shen
- **Comment**: Code link: https://github.com/hwang1996/ManiCLIP
- **Journal**: None
- **Summary**: In this paper we present a novel multi-attribute face manipulation method based on textual descriptions. Previous text-based image editing methods either require test-time optimization for each individual image or are restricted to single attribute editing. Extending these methods to multi-attribute face image editing scenarios will introduce undesired excessive attribute change, e.g., text-relevant attributes are overly manipulated and text-irrelevant attributes are also changed. In order to address these challenges and achieve natural editing over multiple face attributes, we propose a new decoupling training scheme where we use group sampling to get text segments from same attribute categories, instead of whole complex sentences. Further, to preserve other existing face attributes, we encourage the model to edit the latent code of each attribute separately via an entropy constraint. During the inference phase, our model is able to edit new face images without any test-time optimization, even from complex textual prompts. We show extensive experiments and analysis to demonstrate the efficacy of our method, which generates natural manipulated faces with minimal text-irrelevant attribute editing. Code and pre-trained model are available at https://github.com/hwang1996/ManiCLIP.



### Contrastive Audio-Visual Masked Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/2210.07839v4
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2210.07839v4)
- **Published**: 2022-10-02 07:29:57+00:00
- **Updated**: 2023-04-11 22:47:19+00:00
- **Authors**: Yuan Gong, Andrew Rouditchenko, Alexander H. Liu, David Harwath, Leonid Karlinsky, Hilde Kuehne, James Glass
- **Comment**: Accepted at ICLR 2023 as a notable top 25% paper. Code and pretrained
  models are at https://github.com/yuangongnd/cav-mae
- **Journal**: None
- **Summary**: In this paper, we first extend the recent Masked Auto-Encoder (MAE) model from a single modality to audio-visual multi-modalities. Subsequently, we propose the Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE) by combining contrastive learning and masked data modeling, two major self-supervised learning frameworks, to learn a joint and coordinated audio-visual representation. Our experiments show that the contrastive audio-visual correspondence learning objective not only enables the model to perform audio-visual retrieval tasks, but also helps the model learn a better joint representation. As a result, our fully self-supervised pretrained CAV-MAE achieves a new SOTA accuracy of 65.9% on VGGSound, and is comparable with the previous best supervised pretrained model on AudioSet in the audio-visual event classification task. Code and pretrained models are at https://github.com/yuangongnd/cav-mae.



### A Smart Recycling Bin Using Waste Image Classification At The Edge
- **Arxiv ID**: http://arxiv.org/abs/2210.00448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00448v1)
- **Published**: 2022-10-02 07:40:25+00:00
- **Updated**: 2022-10-02 07:40:25+00:00
- **Authors**: Xueying Li, Ryan Grammenos
- **Comment**: None
- **Journal**: None
- **Summary**: Rapid economic growth gives rise to the urgent demand for a more efficient waste recycling system. This work thereby developed an innovative recycling bin that automatically separates urban waste to increase the recycling rate. We collected 1800 recycling waste images and combined them with an existing public dataset to train classification models for two embedded systems, Jetson Nano and K210, targeting different markets. The model reached an accuracy of 95.98% on Jetson Nano and 96.64% on K210. A bin program was designed to collect feedback from users. On Jetson Nano, the overall power consumption of the application was reduced by 30% from the previous work to 4.7 W, while the second system, K210, only needed 0.89 W of power to operate. In summary, our work demonstrated a fully functional prototype of an energy-saving, high-accuracy smart recycling bin, which can be commercialized in the future to improve urban waste recycling.



### GaIA: Graphical Information Gain based Attention Network for Weakly Supervised Point Cloud Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.01558v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01558v1)
- **Published**: 2022-10-02 08:37:16+00:00
- **Updated**: 2022-10-02 08:37:16+00:00
- **Authors**: Min Seok Lee, Seok Woo Yang, Sung Won Han
- **Comment**: WACV 2023 accepted paper
- **Journal**: None
- **Summary**: While point cloud semantic segmentation is a significant task in 3D scene understanding, this task demands a time-consuming process of fully annotating labels. To address this problem, recent studies adopt a weakly supervised learning approach under the sparse annotation. Different from the existing studies, this study aims to reduce the epistemic uncertainty measured by the entropy for a precise semantic segmentation. We propose the graphical information gain based attention network called GaIA, which alleviates the entropy of each point based on the reliable information. The graphical information gain discriminates the reliable point by employing relative entropy between target point and its neighborhoods. We further introduce anchor-based additive angular margin loss, ArcPoint. The ArcPoint optimizes the unlabeled points containing high entropy towards semantically similar classes of the labeled points on hypersphere space. Experimental results on S3DIS and ScanNet-v2 datasets demonstrate our framework outperforms the existing weakly supervised methods. We have released GaIA at https://github.com/Karel911/GaIA.



### Fast OT for Latent Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2210.00479v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.00479v1)
- **Published**: 2022-10-02 10:25:12+00:00
- **Updated**: 2022-10-02 10:25:12+00:00
- **Authors**: Siddharth Roheda, Ashkan Panahi, Hamid Krim
- **Comment**: 6 PAGES
- **Journal**: None
- **Summary**: In this paper, we address the problem of unsupervised Domain Adaptation. The need for such an adaptation arises when the distribution of the target data differs from that which is used to develop the model and the ground truth information of the target data is unknown. We propose an algorithm that uses optimal transport theory with a verifiably efficient and implementable solution to learn the best latent feature representation. This is achieved by minimizing the cost of transporting the samples from the target domain to the distribution of the source domain.



### Compositional Generalization in Unsupervised Compositional Representation Learning: A Study on Disentanglement and Emergent Language
- **Arxiv ID**: http://arxiv.org/abs/2210.00482v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.00482v2)
- **Published**: 2022-10-02 10:35:53+00:00
- **Updated**: 2022-10-05 21:14:46+00:00
- **Authors**: Zhenlin Xu, Marc Niethammer, Colin Raffel
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: Deep learning models struggle with compositional generalization, i.e. the ability to recognize or generate novel combinations of observed elementary concepts. In hopes of enabling compositional generalization, various unsupervised learning algorithms have been proposed with inductive biases that aim to induce compositional structure in learned representations (e.g. disentangled representation and emergent language learning). In this work, we evaluate these unsupervised learning algorithms in terms of how well they enable compositional generalization. Specifically, our evaluation protocol focuses on whether or not it is easy to train a simple model on top of the learned representation that generalizes to new combinations of compositional factors. We systematically study three unsupervised representation learning algorithms - $\beta$-VAE, $\beta$-TCVAE, and emergent language (EL) autoencoders - on two datasets that allow directly testing compositional generalization. We find that directly using the bottleneck representation with simple models and few labels may lead to worse generalization than using representations from layers before or after the learned representation itself. In addition, we find that the previously proposed metrics for evaluating the levels of compositionality are not correlated with actual compositional generalization in our framework. Surprisingly, we find that increasing pressure to produce a disentangled representation produces representations with worse generalization, while representations from EL models show strong compositional generalization. Taken together, our results shed new light on the compositional generalization behavior of different unsupervised learning algorithms with a new setting to rigorously test this behavior, and suggest the potential benefits of delevoping EL learning algorithms for more generalizable representations.



### Unsupervised Multi-View Object Segmentation Using Radiance Field Propagation
- **Arxiv ID**: http://arxiv.org/abs/2210.00489v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00489v2)
- **Published**: 2022-10-02 11:14:23+00:00
- **Updated**: 2022-10-19 11:39:25+00:00
- **Authors**: Xinhang Liu, Jiaben Chen, Huai Yu, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: 23 pages, 14 figures, NeurIPS 2022
- **Journal**: None
- **Summary**: We present radiance field propagation (RFP), a novel approach to segmenting objects in 3D during reconstruction given only unlabeled multi-view images of a scene. RFP is derived from emerging neural radiance field-based techniques, which jointly encodes semantics with appearance and geometry. The core of our method is a novel propagation strategy for individual objects' radiance fields with a bidirectional photometric loss, enabling an unsupervised partitioning of a scene into salient or meaningful regions corresponding to different object instances. To better handle complex scenes with multiple objects and occlusions, we further propose an iterative expectation-maximization algorithm to refine object masks. RFP is one of the first unsupervised approach for tackling 3D real scene object segmentation for neural radiance field (NeRF) without any supervision, annotations, or other cues such as 3D bounding boxes and prior knowledge of object class. Experiments demonstrate that RFP achieves feasible segmentation results that are more accurate than previous unsupervised image/scene segmentation approaches, and are comparable to existing supervised NeRF-based methods. The segmented object representations enable individual 3D object editing operations.



### DARE: A large-scale handwritten date recognition system
- **Arxiv ID**: http://arxiv.org/abs/2210.00503v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00503v1)
- **Published**: 2022-10-02 12:47:36+00:00
- **Updated**: 2022-10-02 12:47:36+00:00
- **Authors**: Christian M. Dahl, Torben S. D. Johansen, Emil N. Sørensen, Christian E. Westermann, Simon F. Wittrock
- **Comment**: None
- **Journal**: None
- **Summary**: Handwritten text recognition for historical documents is an important task but it remains difficult due to a lack of sufficient training data in combination with a large variability of writing styles and degradation of historical documents. While recurrent neural network architectures are commonly used for handwritten text recognition, they are often computationally expensive to train and the benefit of recurrence drastically differs by task. For these reasons, it is important to consider non-recurrent architectures. In the context of handwritten date recognition, we propose an architecture based on the EfficientNetV2 class of models that is fast to train, robust to parameter choices, and accurately transcribes handwritten dates from a number of sources. For training, we introduce a database containing almost 10 million tokens, originating from more than 2.2 million handwritten dates which are segmented from different historical documents. As dates are some of the most common information on historical documents, and with historical archives containing millions of such documents, the efficient and automatic transcription of dates has the potential to lead to significant cost-savings over manual transcription. We show that training on handwritten text with high variability in writing styles result in robust models for general handwritten text recognition and that transfer learning from the DARE system increases transcription accuracy substantially, allowing one to obtain high accuracy even when using a relatively small training sample.



### Loc-VAE: Learning Structurally Localized Representation from 3D Brain MR Images for Content-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2210.00506v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.IR, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2210.00506v1)
- **Published**: 2022-10-02 13:00:17+00:00
- **Updated**: 2022-10-02 13:00:17+00:00
- **Authors**: Kei Nishimaki, Kumpei Ikuta, Yuto Onga, Hitoshi Iyatomi, Kenichi Oishi
- **Comment**: 6 pages, 6 figures. Accepted at the International Conference on
  Systems, Man, and Cybernetics (IEEE SMC '22)
- **Journal**: None
- **Summary**: Content-based image retrieval (CBIR) systems are an emerging technology that supports reading and interpreting medical images. Since 3D brain MR images are high dimensional, dimensionality reduction is necessary for CBIR using machine learning techniques. In addition, for a reliable CBIR system, each dimension in the resulting low-dimensional representation must be associated with a neurologically interpretable region. We propose a localized variational autoencoder (Loc-VAE) that provides neuroanatomically interpretable low-dimensional representation from 3D brain MR images for clinical CBIR. Loc-VAE is based on $\beta$-VAE with the additional constraint that each dimension of the low-dimensional representation corresponds to a local region of the brain. The proposed Loc-VAE is capable of acquiring representation that preserves disease features and is highly localized, even under high-dimensional compression ratios (4096:1). The low-dimensional representation obtained by Loc-VAE improved the locality measure of each dimension by 4.61 points compared to naive $\beta$-VAE, while maintaining comparable brain reconstruction capability and information about the diagnosis of Alzheimer's disease.



### Fast and Robust Video-Based Exercise Classification via Body Pose Tracking and Scalable Multivariate Time Series Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2210.00507v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.00507v1)
- **Published**: 2022-10-02 13:03:38+00:00
- **Updated**: 2022-10-02 13:03:38+00:00
- **Authors**: Ashish Singh, Antonio Bevilacqua, Thach Le Nguyen, Feiyan Hu, Kevin McGuinness, Martin OReilly, Darragh Whelan, Brian Caulfield, Georgiana Ifrim
- **Comment**: None
- **Journal**: None
- **Summary**: Technological advancements have spurred the usage of machine learning based applications in sports science. Physiotherapists, sports coaches and athletes actively look to incorporate the latest technologies in order to further improve performance and avoid injuries. While wearable sensors are very popular, their use is hindered by constraints on battery power and sensor calibration, especially for use cases which require multiple sensors to be placed on the body. Hence, there is renewed interest in video-based data capture and analysis for sports science. In this paper, we present the application of classifying S\&C exercises using video. We focus on the popular Military Press exercise, where the execution is captured with a video-camera using a mobile device, such as a mobile phone, and the goal is to classify the execution into different types. Since video recordings need a lot of storage and computation, this use case requires data reduction, while preserving the classification accuracy and enabling fast prediction. To this end, we propose an approach named BodyMTS to turn video into time series by employing body pose tracking, followed by training and prediction using multivariate time series classifiers. We analyze the accuracy and robustness of BodyMTS and show that it is robust to different types of noise caused by either video quality or pose estimation factors. We compare BodyMTS to state-of-the-art deep learning methods which classify human activity directly from videos and show that BodyMTS achieves similar accuracy, but with reduced running time and model engineering effort. Finally, we discuss some of the practical aspects of employing BodyMTS in this application in terms of accuracy and robustness under reduced data quality and size. We show that BodyMTS achieves an average accuracy of 87\%, which is significantly higher than the accuracy of human domain experts.



### Deep-OCTA: Ensemble Deep Learning Approaches for Diabetic Retinopathy Analysis on OCTA Images
- **Arxiv ID**: http://arxiv.org/abs/2210.00515v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.00515v1)
- **Published**: 2022-10-02 13:23:56+00:00
- **Updated**: 2022-10-02 13:23:56+00:00
- **Authors**: Junlin Hou, Fan Xiao, Jilan Xu, Yuejie Zhang, Haidong Zou, Rui Feng
- **Comment**: None
- **Journal**: None
- **Summary**: The ultra-wide optical coherence tomography angiography (OCTA) has become an important imaging modality in diabetic retinopathy (DR) diagnosis. However, there are few researches focusing on automatic DR analysis using ultra-wide OCTA. In this paper, we present novel and practical deep-learning solutions based on ultra-wide OCTA for the Diabetic Retinopathy Analysis Challenge (DRAC). In the segmentation of DR lesions task, we utilize UNet and UNet++ to segment three lesions with strong data augmentation and model ensemble. In the image quality assessment task, we create an ensemble of InceptionV3, SE-ResNeXt, and Vision Transformer models. Pre-training on the large dataset as well as the hybrid MixUp and CutMix strategy are both adopted to boost the generalization ability of our model. In the DR grading task, we build a Vision Transformer (ViT) and fnd that the ViT model pre-trained on color fundus images serves as a useful substrate for OCTA images. Our proposed methods ranked 4th, 3rd, and 5th on the three leaderboards of DRAC, respectively. The source code will be made available at https://github.com/FDU-VTS/DRAC.



### Neural Implicit Surface Reconstruction from Noisy Camera Observations
- **Arxiv ID**: http://arxiv.org/abs/2210.01548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01548v1)
- **Published**: 2022-10-02 13:35:51+00:00
- **Updated**: 2022-10-02 13:35:51+00:00
- **Authors**: Sarthak Gupta, Patrik Huber
- **Comment**: 4 pages - 2 for paper, 2 for supplementary
- **Journal**: None
- **Summary**: Representing 3D objects and scenes with neural radiance fields has become very popular over the last years. Recently, surface-based representations have been proposed, that allow to reconstruct 3D objects from simple photographs. However, most current techniques require an accurate camera calibration, i.e. camera parameters corresponding to each image, which is often a difficult task to do in real-life situations. To this end, we propose a method for learning 3D surfaces from noisy camera parameters. We show that we can learn camera parameters together with learning the surface representation, and demonstrate good quality 3D surface reconstruction even with noisy camera observations.



### Exploiting More Information in Sparse Point Cloud for 3D Single Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2210.00519v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00519v1)
- **Published**: 2022-10-02 13:38:30+00:00
- **Updated**: 2022-10-02 13:38:30+00:00
- **Authors**: Yubo Cui, Jiayao Shan, Zuoxu Gu, Zhiheng Li, Zheng Fang
- **Comment**: Accepted for publication at IEEE Robotics and Automation Letters
  (RAL)
- **Journal**: None
- **Summary**: 3D single object tracking is a key task in 3D computer vision. However, the sparsity of point clouds makes it difficult to compute the similarity and locate the object, posing big challenges to the 3D tracker. Previous works tried to solve the problem and improved the tracking performance in some common scenarios, but they usually failed in some extreme sparse scenarios, such as for tracking objects at long distances or partially occluded. To address the above problems, in this letter, we propose a sparse-to-dense and transformer-based framework for 3D single object tracking. First, we transform the 3D sparse points into 3D pillars and then compress them into 2D BEV features to have a dense representation. Then, we propose an attention-based encoder to achieve global similarity computation between template and search branches, which could alleviate the influence of sparsity. Meanwhile, the encoder applies the attention on multi-scale features to compensate for the lack of information caused by the sparsity of point cloud and the single scale of features. Finally, we use set-prediction to track the object through a two-stage decoder which also utilizes attention. Extensive experiments show that our method achieves very promising results on the KITTI and NuScenes datasets.



### Towards Learned Simulators for Cell Migration
- **Arxiv ID**: http://arxiv.org/abs/2210.01123v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2210.01123v2)
- **Published**: 2022-10-02 14:01:09+00:00
- **Updated**: 2022-11-02 19:36:18+00:00
- **Authors**: Koen Minartz, Yoeri Poels, Vlado Menkovski
- **Comment**: Accepted at NeurIPS 2022 AI for Science workshop
- **Journal**: None
- **Summary**: Simulators driven by deep learning are gaining popularity as a tool for efficiently emulating accurate but expensive numerical simulators. Successful applications of such neural simulators can be found in the domains of physics, chemistry, and structural biology, amongst others. Likewise, a neural simulator for cellular dynamics can augment lab experiments and traditional computational methods to enhance our understanding of a cell's interaction with its physical environment. In this work, we propose an autoregressive probabilistic model that can reproduce spatiotemporal dynamics of single cell migration, traditionally simulated with the Cellular Potts model. We observe that standard single-step training methods do not only lead to inconsistent rollout stability, but also fail to accurately capture the stochastic aspects of the dynamics, and we propose training strategies to mitigate these issues. Our evaluation on two proof-of-concept experimental scenarios shows that neural methods have the potential to faithfully simulate stochastic cellular dynamics at least an order of magnitude faster than a state-of-the-art implementation of the Cellular Potts model.



### Semi-autonomous Prosthesis Control Using Minimal Depth Information and Vibrotactile Feedback
- **Arxiv ID**: http://arxiv.org/abs/2210.00541v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.00541v1)
- **Published**: 2022-10-02 14:51:58+00:00
- **Updated**: 2022-10-02 14:51:58+00:00
- **Authors**: Miguel Nobre Castro, Strahinja Dosen
- **Comment**: None
- **Journal**: None
- **Summary**: A semi-autonomous prosthesis control based on computer vision can be used to improve performance while decreasing the cognitive burden, especially when using advanced systems with multiple functions. However, a drawback of this approach is that it relies on the complex processing of a significant amount of data (e.g., a point cloud provided by a depth sensor), which can be a challenge when deploying such a system onto an embedded prosthesis controller. In the present study, therefore, we propose a novel method to reconstruct the shape of the target object using minimal data. Specifically, four concurrent laser scanner lines provide partial contours of the object cross-section. Simple geometry is then used to reconstruct the dimensions and orientation of spherical, cylindrical and cuboid objects. The prototype system was implemented using depth sensor to simulate the scan lines and vibrotactile feedback to aid the user during aiming of the laser towards the target object. The prototype was tested on ten able-bodied volunteers who used the semi-autonomous prosthesis to grasp a set of ten objects of different shape, size and orientation. The novel prototype was compared against the benchmark system, which used the full depth data. The results showed that novel system could be used to successfully handle all the objects, and that the performance improved with training, although it was still somewhat worse compared to the benchmark. The present study is therefore an important step towards building a compact system for embedded depth sensing specialized for prosthesis grasping.



### Seeing Through the Noisy Dark: Towards Real-world Low-Light Image Enhancement and Denoising
- **Arxiv ID**: http://arxiv.org/abs/2210.00545v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00545v3)
- **Published**: 2022-10-02 14:57:23+00:00
- **Updated**: 2022-11-15 11:40:13+00:00
- **Authors**: Jiahuan Ren, Zhao Zhang, Richang Hong, Mingliang Xu, Yi Yang, Shuicheng Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Low-light image enhancement (LLIE) aims at improving the illumination and visibility of dark images with lighting noise. To handle the real-world low-light images often with heavy and complex noise, some efforts have been made for joint LLIE and denoising, which however only achieve inferior restoration performance. We attribute it to two challenges: 1) in real-world low-light images, noise is somewhat covered by low-lighting and the left noise after denoising would be inevitably amplified during enhancement; 2) conversion of raw data to sRGB would cause information loss and also more noise, and hence prior LLIE methods trained on raw data are unsuitable for more common sRGB images. In this work, we propose a novel Low-light Enhancement & Denoising Network for real-world low-light images (RLED-Net) in the sRGB color space. In RLED-Net, we apply a plug-and-play differentiable Latent Subspace Reconstruction Block (LSRB) to embed the real-world images into low-rank subspaces to suppress the noise and rectify the errors, such that the impact of noise during enhancement can be effectively shrunk. We then present an efficient Crossed-channel & Shift-window Transformer (CST) layer with two branches to calculate the window and channel attentions to resist the degradation (e.g., speckle noise and blur) caused by the noise in input images. Based on the CST layers, we further present a U-structure network CSTNet as backbone for deep feature recovery, and construct a feature refine block to refine the final features. Extensive experiments on both real noisy images and public image databases well verify the effectiveness of the proposed RLED-Net for RLLIE and denoising simultaneously.



### Siamese-NAS: Using Trained Samples Efficiently to Find Lightweight Neural Architecture by Prior Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2210.00546v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.00546v1)
- **Published**: 2022-10-02 15:04:08+00:00
- **Updated**: 2022-10-02 15:04:08+00:00
- **Authors**: Yu-Ming Zhang, Jun-Wei Hsieh, Chun-Chieh Lee, Kuo-Chin Fan
- **Comment**: None
- **Journal**: None
- **Summary**: In the past decade, many architectures of convolution neural networks were designed by handcraft, such as Vgg16, ResNet, DenseNet, etc. They all achieve state-of-the-art level on different tasks in their time. However, it still relies on human intuition and experience, and it also takes so much time consumption for trial and error. Neural Architecture Search (NAS) focused on this issue. In recent works, the Neural Predictor has significantly improved with few training architectures as training samples. However, the sampling efficiency is already considerable. In this paper, our proposed Siamese-Predictor is inspired by past works of predictor-based NAS. It is constructed with the proposed Estimation Code, which is the prior knowledge about the training procedure. The proposed Siamese-Predictor gets significant benefits from this idea. This idea causes it to surpass the current SOTA predictor on NASBench-201. In order to explore the impact of the Estimation Code, we analyze the relationship between it and accuracy. We also propose the search space Tiny-NanoBench for lightweight CNN architecture. This well-designed search space is easier to find better architecture with few FLOPs than NASBench-201. In summary, the proposed Siamese-Predictor is a predictor-based NAS. It achieves the SOTA level, especially with limited computation budgets. It applied to the proposed Tiny-NanoBench can just use a few trained samples to find extremely lightweight CNN architecture.



### Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2
- **Arxiv ID**: http://arxiv.org/abs/2210.00586v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00586v2)
- **Published**: 2022-10-02 17:53:08+00:00
- **Updated**: 2023-06-05 20:25:22+00:00
- **Authors**: Ali Borji
- **Comment**: dataset link udated!
- **Journal**: None
- **Summary**: The field of image synthesis has made great strides in the last couple of years. Recent models are capable of generating images with astonishing quality. Fine-grained evaluation of these models on some interesting categories such as faces is still missing. Here, we conduct a quantitative comparison of three popular systems including Stable Diffusion, Midjourney, and DALL-E 2 in their ability to generate photorealistic faces in the wild. We find that Stable Diffusion generates better faces than the other systems, according to the FID score. We also introduce a dataset of generated faces in the wild dubbed GFW, including a total of 15,076 faces. Furthermore, we hope that our study spurs follow-up research in assessing the generative models and improving them. Data and code are available at data and code, respectively.



### DFA: Dynamic Feature Aggregation for Efficient Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.00588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00588v1)
- **Published**: 2022-10-02 17:54:15+00:00
- **Updated**: 2022-10-02 17:54:15+00:00
- **Authors**: Yiming Cui
- **Comment**: None
- **Journal**: None
- **Summary**: Video object detection is a fundamental yet challenging task in computer vision. One practical solution is to take advantage of temporal information from the video and apply feature aggregation to enhance the object features in each frame. Though effective, those existing methods always suffer from low inference speeds because they use a fixed number of frames for feature aggregation regardless of the input frame. Therefore, this paper aims to improve the inference speed of the current feature aggregation-based video object detectors while maintaining their performance. To achieve this goal, we propose a vanilla dynamic aggregation module that adaptively selects the frames for feature enhancement. Then, we extend the vanilla dynamic aggregation module to a more effective and reconfigurable deformable version. Finally, we introduce inplace distillation loss to improve the representations of objects aggregated with fewer frames. Extensive experimental results validate the effectiveness and efficiency of our proposed methods: On the ImageNet VID benchmark, integrated with our proposed methods, FGFA and SELSA can improve the inference speed by 31% and 76% respectively while getting comparable performance on accuracy.



### "Help Me Help the AI": Understanding How Explainability Can Support Human-AI Interaction
- **Arxiv ID**: http://arxiv.org/abs/2210.03735v2
- **DOI**: 10.1145/3544548.3581001
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2210.03735v2)
- **Published**: 2022-10-02 20:17:11+00:00
- **Updated**: 2023-02-16 20:14:24+00:00
- **Authors**: Sunnie S. Y. Kim, Elizabeth Anne Watkins, Olga Russakovsky, Ruth Fong, Andrés Monroy-Hernández
- **Comment**: CHI 2023
- **Journal**: Proceedings of the 2023 CHI Conference on Human Factors in
  Computing Systems (CHI '23), April 23-28, 2023, Hamburg, Germany. ACM, New
  York, NY, USA
- **Summary**: Despite the proliferation of explainable AI (XAI) methods, little is understood about end-users' explainability needs and behaviors around XAI explanations. To address this gap and contribute to understanding how explainability can support human-AI interaction, we conducted a mixed-methods study with 20 end-users of a real-world AI application, the Merlin bird identification app, and inquired about their XAI needs, uses, and perceptions. We found that participants desire practically useful information that can improve their collaboration with the AI, more so than technical system details. Relatedly, participants intended to use XAI explanations for various purposes beyond understanding the AI's outputs: calibrating trust, improving their task skills, changing their behavior to supply better inputs to the AI, and giving constructive feedback to developers. Finally, among existing XAI approaches, participants preferred part-based explanations that resemble human reasoning and explanations. We discuss the implications of our findings and provide recommendations for future XAI design.



### iCTGAN--An Attack Mitigation Technique for Random-vector Attack on Accelerometer-based Gait Authentication Systems
- **Arxiv ID**: http://arxiv.org/abs/2210.00615v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, K.6.5
- **Links**: [PDF](http://arxiv.org/pdf/2210.00615v1)
- **Published**: 2022-10-02 20:34:25+00:00
- **Updated**: 2022-10-02 20:34:25+00:00
- **Authors**: Jun Hyung Mo, Rajesh Kumar
- **Comment**: 9 pages, 5 figures, IEEE International Joint Conference on Biometrics
  (IJCB 2022)
- **Journal**: None
- **Summary**: A recent study showed that commonly (vanilla) studied implementations of accelerometer-based gait authentication systems ($v$ABGait) are susceptible to random-vector attack. The same study proposed a beta noise-assisted implementation ($\beta$ABGait) to mitigate the attack. In this paper, we assess the effectiveness of the random-vector attack on both $v$ABGait and $\beta$ABGait using three accelerometer-based gait datasets. In addition, we propose $i$ABGait, an alternative implementation of ABGait, which uses a Conditional Tabular Generative Adversarial Network. Then we evaluate $i$ABGait's resilience against the traditional zero-effort and random-vector attacks. The results show that $i$ABGait mitigates the impact of the random-vector attack to a reasonable extent and outperforms $\beta$ABGait in most experimental settings.



### Optimization for Robustness Evaluation beyond $\ell_p$ Metrics
- **Arxiv ID**: http://arxiv.org/abs/2210.00621v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2210.00621v2)
- **Published**: 2022-10-02 20:48:05+00:00
- **Updated**: 2022-11-14 00:30:38+00:00
- **Authors**: Hengyue Liang, Buyun Liang, Ying Cui, Tim Mitchell, Ju Sun
- **Comment**: 5 pages, 1 figure, 3 tables, accepted by the 14th International OPT
  Workshop on Optimization for Machine Learning, and submitted to the 2023 IEEE
  International Conference on Acoustics, Speech, and Signal Processing (ICASSP
  2023)
- **Journal**: None
- **Summary**: Empirical evaluation of deep learning models against adversarial attacks entails solving nontrivial constrained optimization problems. Popular algorithms for solving these constrained problems rely on projected gradient descent (PGD) and require careful tuning of multiple hyperparameters. Moreover, PGD can only handle $\ell_1$, $\ell_2$, and $\ell_\infty$ attack models due to the use of analytical projectors. In this paper, we introduce a novel algorithmic framework that blends a general-purpose constrained-optimization solver PyGRANSO, With Constraint-Folding (PWCF), to add reliability and generality to robustness evaluation. PWCF 1) finds good-quality solutions without the need of delicate hyperparameter tuning, and 2) can handle general attack models, e.g., general $\ell_p$ ($p \geq 0$) and perceptual attacks, which are inaccessible to PGD-based algorithms.



### GANTouch: An Attack-Resilient Framework for Touch-based Continuous Authentication System
- **Arxiv ID**: http://arxiv.org/abs/2210.01594v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, K.6.5
- **Links**: [PDF](http://arxiv.org/pdf/2210.01594v1)
- **Published**: 2022-10-02 20:58:04+00:00
- **Updated**: 2022-10-02 20:58:04+00:00
- **Authors**: Mohit Agrawal, Pragyan Mehrotra, Rajesh Kumar, Rajiv Ratn Shah
- **Comment**: 11 pages, 7 figures, 2 tables, 3 algorithms, in IEEE TBIOM 2022
- **Journal**: None
- **Summary**: Previous studies have shown that commonly studied (vanilla) implementations of touch-based continuous authentication systems (V-TCAS) are susceptible to active adversarial attempts. This study presents a novel Generative Adversarial Network assisted TCAS (G-TCAS) framework and compares it to the V-TCAS under three active adversarial environments viz. Zero-effort, Population, and Random-vector. The Zero-effort environment was implemented in two variations viz. Zero-effort (same-dataset) and Zero-effort (cross-dataset). The first involved a Zero-effort attack from the same dataset, while the second used three different datasets. G-TCAS showed more resilience than V-TCAS under the Population and Random-vector, the more damaging adversarial scenarios than the Zero-effort. On average, the increase in the false accept rates (FARs) for V-TCAS was much higher (27.5% and 21.5%) than for G-TCAS (14% and 12.5%) for Population and Random-vector attacks, respectively. Moreover, we performed a fairness analysis of TCAS for different genders and found TCAS to be fair across genders. The findings suggest that we should evaluate TCAS under active adversarial environments and affirm the usefulness of GANs in the TCAS pipeline.



### MonoNHR: Monocular Neural Human Renderer
- **Arxiv ID**: http://arxiv.org/abs/2210.00627v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00627v1)
- **Published**: 2022-10-02 21:01:02+00:00
- **Updated**: 2022-10-02 21:01:02+00:00
- **Authors**: Hongsuk Choi, Gyeongsik Moon, Matthieu Armando, Vincent Leroy, Kyoung Mu Lee, Gregory Rogez
- **Comment**: Hongsuk Choi and Gyeongsik Moon contributed equally, 15 pages
  including the reference and supplementary material
- **Journal**: None
- **Summary**: Existing neural human rendering methods struggle with a single image input due to the lack of information in invisible areas and the depth ambiguity of pixels in visible areas. In this regard, we propose Monocular Neural Human Renderer (MonoNHR), a novel approach that renders robust free-viewpoint images of an arbitrary human given only a single image. MonoNHR is the first method that (i) renders human subjects never seen during training in a monocular setup, and (ii) is trained in a weakly-supervised manner without geometry supervision. First, we propose to disentangle 3D geometry and texture features and to condition the texture inference on the 3D geometry features. Second, we introduce a Mesh Inpainter module that inpaints the occluded parts exploiting human structural priors such as symmetry. Experiments on ZJU-MoCap, AIST, and HUMBI datasets show that our approach significantly outperforms the recent methods adapted to the monocular case.



### Pixel-global Self-supervised Learning with Uncertainty-aware Context Stabilizer
- **Arxiv ID**: http://arxiv.org/abs/2210.00646v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.00646v1)
- **Published**: 2022-10-02 22:25:01+00:00
- **Updated**: 2022-10-02 22:25:01+00:00
- **Authors**: Zhuangzhuang Zhang, Weixiong Zhang
- **Comment**: 18 pages, 5 figures
- **Journal**: None
- **Summary**: We developed a novel SSL approach to capture global consistency and pixel-level local consistencies between differently augmented views of the same images to accommodate downstream discriminative and dense predictive tasks. We adopted the teacher-student architecture used in previous contrastive SSL methods. In our method, the global consistency is enforced by aggregating the compressed representations of augmented views of the same image. The pixel-level consistency is enforced by pursuing similar representations for the same pixel in differently augmented views. Importantly, we introduced an uncertainty-aware context stabilizer to adaptively preserve the context gap created by the two views from different augmentations. Moreover, we used Monte Carlo dropout in the stabilizer to measure uncertainty and adaptively balance the discrepancy between the representations of the same pixels in different views.



### IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2210.00647v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2210.00647v3)
- **Published**: 2022-10-02 22:45:11+00:00
- **Updated**: 2023-08-29 08:34:40+00:00
- **Authors**: Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, Guofeng Zhang
- **Comment**: Accepted to ICCV2023, Project webpage:
  https://zju3dv.github.io/intrinsic_nerf/, code:
  https://github.com/zju3dv/IntrinsicNeRF
- **Journal**: None
- **Summary**: Existing inverse rendering combined with neural rendering methods can only perform editable novel view synthesis on object-specific scenes, while we present intrinsic neural radiance fields, dubbed IntrinsicNeRF, which introduce intrinsic decomposition into the NeRF-based neural rendering method and can extend its application to room-scale scenes. Since intrinsic decomposition is a fundamentally under-constrained inverse problem, we propose a novel distance-aware point sampling and adaptive reflectance iterative clustering optimization method, which enables IntrinsicNeRF with traditional intrinsic decomposition constraints to be trained in an unsupervised manner, resulting in multi-view consistent intrinsic decomposition results. To cope with the problem that different adjacent instances of similar reflectance in a scene are incorrectly clustered together, we further propose a hierarchical clustering method with coarse-to-fine optimization to obtain a fast hierarchical indexing representation. It supports compelling real-time augmented applications such as recoloring and illumination variation. Extensive experiments and editing samples on both object-specific/room-scale scenes and synthetic/real-word data demonstrate that we can obtain consistent intrinsic decomposition results and high-fidelity novel view synthesis even for challenging sequences.



