# Arxiv Papers in cs.CV on 2022-10-01
### Automated segmentation of microvessels in intravascular OCT images using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2210.00166v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.00166v2)
- **Published**: 2022-10-01 02:14:14+00:00
- **Updated**: 2022-11-03 17:10:40+00:00
- **Authors**: Juhwan Lee, Justin N. Kim, Lia Gomez-Perez, Yazan Gharaibeh, Issam Motairek, Ga-briel T. R. Pereira, Vladislav N. Zimin, Luis A. P. Dallan, Ammar Hoori, Sadeer Al-Kindi, Giulio Guagliumi, Hiram G. Bezerra, David L. Wilson
- **Comment**: 21 pages, 9 figures, 3 tables
- **Journal**: None
- **Summary**: To analyze this characteristic of vulnerability, we developed an automated deep learning method for detecting microvessels in intravascular optical coherence tomography (IVOCT) images. A total of 8,403 IVOCT image frames from 85 lesions and 37 normal segments were analyzed. Manual annotation was done using a dedicated software (OCTOPUS) previously developed by our group. Data augmentation in the polar (r,{\theta}) domain was applied to raw IVOCT images to ensure that microvessels appear at all possible angles. Pre-processing methods included guidewire/shadow detection, lumen segmentation, pixel shifting, and noise reduction. DeepLab v3+ was used to segment microvessel candidates. A bounding box on each candidate was classified as either microvessel or non-microvessel using a shallow convolutional neural network. For better classification, we used data augmentation (i.e., angle rotation) on bounding boxes with a microvessel during network training. Data augmentation and pre-processing steps improved microvessel segmentation performance significantly, yielding a method with Dice of 0.71+/-0.10 and pixel-wise sensitivity/specificity of 87.7+/-6.6%/99.8+/-0.1%. The network for classifying microvessels from candidates performed exceptionally well, with sensitivity of 99.5+/-0.3%, specificity of 98.8+/-1.0%, and accuracy of 99.1+/-0.5%. The classification step eliminated the majority of residual false positives, and the Dice coefficient increased from 0.71 to 0.73. In addition, our method produced 698 image frames with microvessels present, compared to 730 from manual analysis, representing a 4.4% difference. When compared to the manual method, the automated method improved microvessel continuity, implying improved segmentation performance. The method will be useful for research purposes as well as potential future treatment planning.



### Improving ProtoNet for Few-Shot Video Object Recognition: Winner of ORBIT Challenge 2022
- **Arxiv ID**: http://arxiv.org/abs/2210.00174v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.00174v1)
- **Published**: 2022-10-01 03:03:20+00:00
- **Updated**: 2022-10-01 03:03:20+00:00
- **Authors**: Li Gu, Zhixiang Chi, Huan Liu, Yuanhao Yu, Yang Wang
- **Comment**: Winner of ORBIT Challenge 2022
- **Journal**: None
- **Summary**: In this work, we present the winning solution for ORBIT Few-Shot Video Object Recognition Challenge 2022. Built upon the ProtoNet baseline, the performance of our method is improved with three effective techniques. These techniques include the embedding adaptation, the uniform video clip sampler and the invalid frame detection. In addition, we re-factor and re-implement the official codebase to encourage modularity, compatibility and improved performance. Our implementation accelerates the data loading in both training and testing.



### EAPruning: Evolutionary Pruning for Vision Transformers and CNNs
- **Arxiv ID**: http://arxiv.org/abs/2210.00181v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00181v1)
- **Published**: 2022-10-01 03:38:56+00:00
- **Updated**: 2022-10-01 03:38:56+00:00
- **Authors**: Qingyuan Li, Bo Zhang, Xiangxiang Chu
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Structured pruning greatly eases the deployment of large neural networks in resource-constrained environments. However, current methods either involve strong domain expertise, require extra hyperparameter tuning, or are restricted only to a specific type of network, which prevents pervasive industrial applications. In this paper, we undertake a simple and effective approach that can be easily applied to both vision transformers and convolutional neural networks. Specifically, we consider pruning as an evolution process of sub-network structures that inherit weights through reconstruction techniques. We achieve a 50% FLOPS reduction for ResNet50 and MobileNetV1, leading to 1.37x and 1.34x speedup respectively. For DeiT-Base, we reach nearly 40% FLOPs reduction and 1.4x speedup. Our code will be made available.



### Structure-Aware NeRF without Posed Camera via Epipolar Constraint
- **Arxiv ID**: http://arxiv.org/abs/2210.00183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00183v1)
- **Published**: 2022-10-01 03:57:39+00:00
- **Updated**: 2022-10-01 03:57:39+00:00
- **Authors**: Shu Chen, Yang Zhang, Yaxin Xu, Beiji Zou
- **Comment**: None
- **Journal**: None
- **Summary**: The neural radiance field (NeRF) for realistic novel view synthesis requires camera poses to be pre-acquired by a structure-from-motion (SfM) approach. This two-stage strategy is not convenient to use and degrades the performance because the error in the pose extraction can propagate to the view synthesis. We integrate the pose extraction and view synthesis into a single end-to-end procedure so they can benefit from each other. For training NeRF models, only RGB images are given, without pre-known camera poses. The camera poses are obtained by the epipolar constraint in which the identical feature in different views has the same world coordinates transformed from the local camera coordinates according to the extracted poses. The epipolar constraint is jointly optimized with pixel color constraint. The poses are represented by a CNN-based deep network, whose input is the related frames. This joint optimization enables NeRF to be aware of the scene's structure that has an improved generalization performance. Extensive experiments on a variety of scenes demonstrate the effectiveness of the proposed approach. Code is available at https://github.com/XTU-PR-LAB/SaNerf.



### Cut-Paste Consistency Learning for Semi-Supervised Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.00191v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.00191v1)
- **Published**: 2022-10-01 04:43:54+00:00
- **Updated**: 2022-10-01 04:43:54+00:00
- **Authors**: Boon Peng Yap, Beng Koon Ng
- **Comment**: Accepted to appear in WACV 2023
- **Journal**: None
- **Summary**: Semi-supervised learning has the potential to improve the data-efficiency of training data-hungry deep neural networks, which is especially important for medical image analysis tasks where labeled data is scarce. In this work, we present a simple semi-supervised learning method for lesion segmentation tasks based on the ideas of cut-paste augmentation and consistency regularization. By exploiting the mask information available in the labeled data, we synthesize partially labeled samples from the unlabeled images so that the usual supervised learning objective (e.g., binary cross entropy) can be applied. Additionally, we introduce a background consistency term to regularize the training on the unlabeled background regions of the synthetic images. We empirically verify the effectiveness of the proposed method on two public lesion segmentation datasets, including an eye fundus photograph dataset and a brain CT scan dataset. The experiment results indicate that our method achieves consistent and superior performance over other self-training and consistency-based methods without introducing sophisticated network components.



### Differentiable Parsing and Visual Grounding of Natural Language Instructions for Object Placement
- **Arxiv ID**: http://arxiv.org/abs/2210.00215v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.00215v4)
- **Published**: 2022-10-01 07:36:51+00:00
- **Updated**: 2023-03-13 07:09:20+00:00
- **Authors**: Zirui Zhao, Wee Sun Lee, David Hsu
- **Comment**: To appear in ICRA 2023
- **Journal**: None
- **Summary**: We present a new method, PARsing And visual GrOuNding (ParaGon), for grounding natural language in object placement tasks. Natural language generally describes objects and spatial relations with compositionality and ambiguity, two major obstacles to effective language grounding. For compositionality, ParaGon parses a language instruction into an object-centric graph representation to ground objects individually. For ambiguity, ParaGon uses a novel particle-based graph neural network to reason about object placements with uncertainty. Essentially, ParaGon integrates a parsing algorithm into a probabilistic, data-driven learning framework. It is fully differentiable and trained end-to-end from data for robustness against complex, ambiguous language input.



### A Dual-Attention Learning Network with Word and Sentence Embedding for Medical Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2210.00220v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00220v2)
- **Published**: 2022-10-01 08:32:40+00:00
- **Updated**: 2022-11-12 01:21:33+00:00
- **Authors**: Xiaofei Huang, Hongfang Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Research in medical visual question answering (MVQA) can contribute to the development of computeraided diagnosis. MVQA is a task that aims to predict accurate and convincing answers based on given medical images and associated natural language questions. This task requires extracting medical knowledge-rich feature content and making fine-grained understandings of them. Therefore, constructing an effective feature extraction and understanding scheme are keys to modeling. Existing MVQA question extraction schemes mainly focus on word information, ignoring medical information in the text. Meanwhile, some visual and textual feature understanding schemes cannot effectively capture the correlation between regions and keywords for reasonable visual reasoning. In this study, a dual-attention learning network with word and sentence embedding (WSDAN) is proposed. We design a module, transformer with sentence embedding (TSE), to extract a double embedding representation of questions containing keywords and medical information. A dualattention learning (DAL) module consisting of self-attention and guided attention is proposed to model intensive intramodal and intermodal interactions. With multiple DAL modules (DALs), learning visual and textual co-attention can increase the granularity of understanding and improve visual reasoning. Experimental results on the ImageCLEF 2019 VQA-MED (VQA-MED 2019) and VQA-RAD datasets demonstrate that our proposed method outperforms previous state-of-the-art methods. According to the ablation studies and Grad-CAM maps, WSDAN can extract rich textual information and has strong visual reasoning ability.



### Motion-inductive Self-supervised Object Discovery in Videos
- **Arxiv ID**: http://arxiv.org/abs/2210.00221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00221v1)
- **Published**: 2022-10-01 08:38:28+00:00
- **Updated**: 2022-10-01 08:38:28+00:00
- **Authors**: Shuangrui Ding, Weidi Xie, Yabo Chen, Rui Qian, Xiaopeng Zhang, Hongkai Xiong, Qi Tian
- **Comment**: Technical report
- **Journal**: None
- **Summary**: In this paper, we consider the task of unsupervised object discovery in videos. Previous works have shown promising results via processing optical flows to segment objects. However, taking flow as input brings about two drawbacks. First, flow cannot capture sufficient cues when objects remain static or partially occluded. Second, it is challenging to establish temporal coherency from flow-only input, due to the missing texture information. To tackle these limitations, we propose a model for directly processing consecutive RGB frames, and infer the optical flow between any pair of frames using a layered representation, with the opacity channels being treated as the segmentation. Additionally, to enforce object permanence, we apply temporal consistency loss on the inferred masks from randomly-paired frames, which refer to the motions at different paces, and encourage the model to segment the objects even if they may not move at the current time point. Experimentally, we demonstrate superior performance over previous state-of-the-art methods on three public video segmentation datasets (DAVIS2016, SegTrackv2, and FBMS-59), while being computationally efficient by avoiding the overhead of computing optical flow as input.



### Contour-Aware Equipotential Learning for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.00223v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00223v1)
- **Published**: 2022-10-01 08:45:44+00:00
- **Updated**: 2022-10-01 08:45:44+00:00
- **Authors**: Xu Yin, Dongbo Min, Yuchi Huo, Sung-Eui Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: With increasing demands for high-quality semantic segmentation in the industry, hard-distinguishing semantic boundaries have posed a significant threat to existing solutions. Inspired by real-life experience, i.e., combining varied observations contributes to higher visual recognition confidence, we present the equipotential learning (EPL) method. This novel module transfers the predicted/ground-truth semantic labels to a self-defined potential domain to learn and infer decision boundaries along customized directions. The conversion to the potential domain is implemented via a lightweight differentiable anisotropic convolution without incurring any parameter overhead. Besides, the designed two loss functions, the point loss and the equipotential line loss implement anisotropic field regression and category-level contour learning, respectively, enhancing prediction consistencies in the inter/intra-class boundary areas. More importantly, EPL is agnostic to network architectures, and thus it can be plugged into most existing segmentation models. This paper is the first attempt to address the boundary segmentation problem with field regression and contour learning. Meaningful performance improvements on Pascal Voc 2012 and Cityscapes demonstrate that the proposed EPL module can benefit the off-the-shelf fully convolutional network models when recognizing semantic boundary areas. Besides, intensive comparisons and analysis show the favorable merits of EPL for distinguishing semantically-similar and irregular-shaped categories.



### Attention Augmented ConvNeXt UNet For Rectal Tumour Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.00227v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.00227v2)
- **Published**: 2022-10-01 09:08:43+00:00
- **Updated**: 2022-10-27 02:20:38+00:00
- **Authors**: Hongwei Wu, Junlin Wang, Xin Wang, Hui Nan, Yaxin Wang, Haonan Jing, Kaixuan Shi
- **Comment**: I plan to replace this article, and supplement and confirm the
  structure and experimental content of this article
- **Journal**: None
- **Summary**: It is a challenge to segment the location and size of rectal cancer tumours through deep learning. In this paper, in order to improve the ability of extracting suffi-cient feature information in rectal tumour segmentation, attention enlarged ConvNeXt UNet (AACN-UNet), is proposed. The network mainly includes two improvements: 1) the encoder stage of UNet is changed to ConvNeXt structure for encoding operation, which can not only integrate multi-scale semantic information on a large scale, but al-so reduce information loss and extract more feature information from CT images; 2) CBAM attention mechanism is added to improve the connection of each feature in channel and space, which is conducive to extracting the effective feature of the target and improving the segmentation accuracy.The experiment with UNet and its variant network shows that AACN-UNet is 0.9% ,1.1% and 1.4% higher than the current best results in P, F1 and Miou.Compared with the training time, the number of parameters in UNet network is less. This shows that our proposed AACN-UNet has achieved ex-cellent results in CT image segmentation of rectal cancer.



### T2CI-GAN: Text to Compressed Image generation using Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/2210.03734v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.03734v1)
- **Published**: 2022-10-01 09:26:25+00:00
- **Updated**: 2022-10-01 09:26:25+00:00
- **Authors**: Bulla Rajesh, Nandakishore Dusa, Mohammed Javed, Shiv Ram Dubey, P. Nagabhushan
- **Comment**: Accepted for publication at IAPR's 6th CVIP 2022
- **Journal**: None
- **Summary**: The problem of generating textual descriptions for the visual data has gained research attention in the recent years. In contrast to that the problem of generating visual data from textual descriptions is still very challenging, because it requires the combination of both Natural Language Processing (NLP) and Computer Vision techniques. The existing methods utilize the Generative Adversarial Networks (GANs) and generate the uncompressed images from textual description. However, in practice, most of the visual data are processed and transmitted in the compressed representation. Hence, the proposed work attempts to generate the visual data directly in the compressed representation form using Deep Convolutional GANs (DCGANs) to achieve the storage and computational efficiency. We propose GAN models for compressed image generation from text. The first model is directly trained with JPEG compressed DCT images (compressed domain) to generate the compressed images from text descriptions. The second model is trained with RGB images (pixel domain) to generate JPEG compressed DCT representation from text descriptions. The proposed models are tested on an open source benchmark dataset Oxford-102 Flower images using both RGB and JPEG compressed versions, and accomplished the state-of-the-art performance in the JPEG compressed domain. The code will be publicly released at GitHub after acceptance of paper.



### Learnable Distribution Calibration for Few-Shot Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.00232v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00232v1)
- **Published**: 2022-10-01 09:40:26+00:00
- **Updated**: 2022-10-01 09:40:26+00:00
- **Authors**: Binghao Liu, Boyu Yang, Lingxi Xie, Ren Wang, Qi Tian, Qixiang Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot class-incremental learning (FSCIL) faces challenges of memorizing old class distributions and estimating new class distributions given few training samples. In this study, we propose a learnable distribution calibration (LDC) approach, with the aim to systematically solve these two challenges using a unified framework. LDC is built upon a parameterized calibration unit (PCU), which initializes biased distributions for all classes based on classifier vectors (memory-free) and a single covariance matrix. The covariance matrix is shared by all classes, so that the memory costs are fixed. During base training, PCU is endowed with the ability to calibrate biased distributions by recurrently updating sampled features under the supervision of real distributions. During incremental learning, PCU recovers distributions for old classes to avoid `forgetting', as well as estimating distributions and augmenting samples for new classes to alleviate `over-fitting' caused by the biased distributions of few-shot samples. LDC is theoretically plausible by formatting a variational inference procedure. It improves FSCIL's flexibility as the training procedure requires no class similarity priori. Experiments on CUB200, CIFAR100, and mini-ImageNet datasets show that LDC outperforms the state-of-the-arts by 4.64%, 1.98%, and 3.97%, respectively. LDC's effectiveness is also validated on few-shot learning scenarios.



### Blindly Deconvolving Super-noisy Blurry Image Sequences
- **Arxiv ID**: http://arxiv.org/abs/2210.00252v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10 (Primary) 68W99 (Secondary), I.4
- **Links**: [PDF](http://arxiv.org/pdf/2210.00252v1)
- **Published**: 2022-10-01 11:17:17+00:00
- **Updated**: 2022-10-01 11:17:17+00:00
- **Authors**: Leonid Kostrykin, Stefan Harmeling
- **Comment**: 19 pages, 9 figures
- **Journal**: None
- **Summary**: Image blur and image noise are imaging artifacts intrinsically arising in image acquisition. In this paper, we consider multi-frame blind deconvolution (MFBD), where image blur is described by the convolution of an unobservable, undeteriorated image and an unknown filter, and the objective is to recover the undeteriorated image from a sequence of its blurry and noisy observations. We present two new methods for MFBD, which, in contrast to previous work, do not require the estimation of the unknown filters. The first method is based on likelihood maximization and requires careful initialization to cope with the non-convexity of the loss function. The second method circumvents this requirement and exploits that the solution of likelihood maximization emerges as an eigenvector of a specifically constructed matrix, if the signal subspace spanned by the observations has a sufficiently large dimension. We describe a pre-processing step, which increases the dimension of the signal subspace by artificially generating additional observations. We also propose an extension of the eigenvector method, which copes with insufficient dimensions of the signal subspace by estimating a footprint of the unknown filters (that is a vector of the size of the filters, only one is required for the whole image sequence). We have applied the eigenvector method to synthetically generated image sequences and performed a quantitative comparison with a previous method, obtaining strongly improved results.



### Cascaded Multi-Modal Mixing Transformers for Alzheimer's Disease Classification with Incomplete Data
- **Arxiv ID**: http://arxiv.org/abs/2210.00255v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.00255v2)
- **Published**: 2022-10-01 11:31:02+00:00
- **Updated**: 2023-07-16 08:15:41+00:00
- **Authors**: Linfeng Liu, Siyu Liu, Lu Zhang, Xuan Vinh To, Fatima Nasrallah, Shekhar S. Chandra
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate medical classification requires a large number of multi-modal data, and in many cases, different feature types. Previous studies have shown promising results when using multi-modal data, outperforming single-modality models when classifying diseases such as Alzheimer's Disease (AD). However, those models are usually not flexible enough to handle missing modalities. Currently, the most common workaround is discarding samples with missing modalities which leads to considerable data under-utilization. Adding to the fact that labeled medical images are already scarce, the performance of data-driven methods like deep learning can be severely hampered. Therefore, a multi-modal method that can handle missing data in various clinical settings is highly desirable. In this paper, we present Multi-Modal Mixing Transformer (3MAT), a disease classification transformer that not only leverages multi-modal data but also handles missing data scenarios. In this work, we test 3MT for AD and Cognitively normal (CN) classification and mild cognitive impairment (MCI) conversion prediction to progressive MCI (pMCI) or stable MCI (sMCI) using clinical and neuroimaging data. The model uses a novel Cascaded Modality Transformer architecture with cross-attention to incorporate multi-modal information for more informed predictions. We propose a novel modality dropout mechanism to ensure an unprecedented level of modality independence and robustness to handle missing data scenarios. The result is a versatile network that enables the mixing of arbitrary numbers of modalities with different feature types and also ensures full data utilization missing data scenarios. The model is trained and evaluated on the ADNI dataset with the SOTRA performance and further evaluated with the AIBL dataset with missing data.



### Long-Tailed Class Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.00266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00266v1)
- **Published**: 2022-10-01 12:41:48+00:00
- **Updated**: 2022-10-01 12:41:48+00:00
- **Authors**: Xialei Liu, Yu-Song Hu, Xu-Sheng Cao, Andrew D. Bagdanov, Ke Li, Ming-Ming Cheng
- **Comment**: Accepted at ECCV 2022
- **Journal**: None
- **Summary**: In class incremental learning (CIL) a model must learn new classes in a sequential manner without forgetting old ones. However, conventional CIL methods consider a balanced distribution for each new task, which ignores the prevalence of long-tailed distributions in the real world. In this work we propose two long-tailed CIL scenarios, which we term ordered and shuffled LT-CIL. Ordered LT-CIL considers the scenario where we learn from head classes collected with more samples than tail classes which have few. Shuffled LT-CIL, on the other hand, assumes a completely random long-tailed distribution for each task. We systematically evaluate existing methods in both LT-CIL scenarios and demonstrate very different behaviors compared to conventional CIL scenarios. Additionally, we propose a two-stage learning baseline with a learnable weight scaling layer for reducing the bias caused by long-tailed distribution in LT-CIL and which in turn also improves the performance of conventional CIL due to the limited exemplars. Our results demonstrate the superior performance (up to 6.44 points in average incremental accuracy) of our approach on CIFAR-100 and ImageNet-Subset. The code is available at https://github.com/xialeiliu/Long-Tailed-CIL



### Offline Handwritten Amharic Character Recognition Using Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.00275v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00275v1)
- **Published**: 2022-10-01 13:16:18+00:00
- **Updated**: 2022-10-01 13:16:18+00:00
- **Authors**: Mesay Samuel, Lars Schmidt-Thieme, DP Sharma, Abiot Sinamo, Abey Bruck
- **Comment**: PanAfriCon AI 2022 virtual conference paper
- **Journal**: None
- **Summary**: Few-shot learning is an important, but challenging problem of machine learning aimed at learning from only fewer labeled training examples. It has become an active area of research due to deep learning requiring huge amounts of labeled dataset, which is not feasible in the real world. Learning from a few examples is also an important attempt towards learning like humans. Few-shot learning has proven a very good promise in different areas of machine learning applications, particularly in image classification. As it is a recent technique, most researchers focus on understanding and solving the issues related to its concept by focusing only on common image datasets like Mini-ImageNet and Omniglot. Few-shot learning also opens an opportunity to address low resource languages like Amharic. In this study, offline handwritten Amharic character recognition using few-shot learning is addressed. Particularly, prototypical networks, the popular and simpler type of few-shot learning, is implemented as a baseline. Using the opportunities explored in the nature of Amharic alphabet having row-wise and column-wise similarities, a novel way of augmenting the training episodes is proposed. The experimental results show that the proposed method outperformed the baseline method. This study has implemented few-shot learning for Amharic characters for the first time. More importantly, the findings of the study open new ways of examining the influence of training episodes in few-shot learning, which is one of the important issues that needs exploration. The datasets used for this study are collected from native Amharic language writers using an Android App developed as a part of this study.



### Det-SLAM: A semantic visual SLAM for highly dynamic scenes using Detectron2
- **Arxiv ID**: http://arxiv.org/abs/2210.00278v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.00278v1)
- **Published**: 2022-10-01 13:25:11+00:00
- **Updated**: 2022-10-01 13:25:11+00:00
- **Authors**: Ali Eslamian, Mohammad R. Ahmadzadeh
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: According to experts, Simultaneous Localization and Mapping (SLAM) is an intrinsic part of autonomous robotic systems. Several SLAM systems with impressive performance have been invented and used during the last several decades. However, there are still unresolved issues, such as how to deal with moving objects in dynamic situations. Classic SLAM systems depend on the assumption of a static environment, which becomes unworkable in highly dynamic situations. Several methods have been presented to tackle this issue in recent years, but each has its limitations. This research combines the visual SLAM systems ORB-SLAM3 and Detectron2 to present the Det-SLAM system, which employs depth information and semantic segmentation to identify and eradicate dynamic spots to accomplish semantic SLAM for dynamic situations. Evaluation of public TUM datasets indicates that Det-SLAM is more resilient than previous dynamic SLAM systems and can lower the estimated error of camera posture in dynamic indoor scenarios.



### Gait-based Age Group Classification with Adaptive Graph Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2210.00294v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.00294v1)
- **Published**: 2022-10-01 14:54:15+00:00
- **Updated**: 2022-10-01 14:54:15+00:00
- **Authors**: Timilehin B. Aderinola, Tee Connie, Thian Song Ong, Andrew Beng Jin Teoh, Michael Kah Ong Goh
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning techniques have recently been utilized for model-free age-associated gait feature extraction. However, acquiring model-free gait demands accurate pre-processing such as background subtraction, which is non-trivial in unconstrained environments. On the other hand, model-based gait can be obtained without background subtraction and is less affected by covariates. For model-based gait-based age group classification problems, present works rely solely on handcrafted features, where feature extraction is tedious and requires domain expertise. This paper proposes a deep learning approach to extract age-associated features from model-based gait for age group classification. Specifically, we first develop an unconstrained gait dataset called Multimedia University Gait Age and Gender dataset (MMU GAG). Next, the body joint coordinates are determined via pose estimation algorithms and represented as compact gait graphs via a novel part aggregation scheme. Then, a Part-AdaptIve Residual Graph Convolutional Neural Network (PairGCN) is designed for age-associated feature learning. Experiments suggest that PairGCN features are far more informative than handcrafted features, yielding up to 99% accuracy for classifying subjects as a child, adult, or senior in the MMU GAG dataset.



### An Ensemble of Convolutional Neural Networks to Detect Foliar Diseases in Apple Plants
- **Arxiv ID**: http://arxiv.org/abs/2210.00298v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.00298v1)
- **Published**: 2022-10-01 15:40:04+00:00
- **Updated**: 2022-10-01 15:40:04+00:00
- **Authors**: Kush Vora, Dishant Padalia
- **Comment**: 6 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: Apple diseases, if not diagnosed early, can lead to massive resource loss and pose a serious threat to humans and animals who consume the infected apples. Hence, it is critical to diagnose these diseases early in order to manage plant health and minimize the risks associated with them. However, the conventional approach of monitoring plant diseases entails manual scouting and analyzing the features, texture, color, and shape of the plant leaves, resulting in delayed diagnosis and misjudgments. Our work proposes an ensembled system of Xception, InceptionResNet, and MobileNet architectures to detect 5 different types of apple plant diseases. The model has been trained on the publicly available Plant Pathology 2021 dataset and can classify multiple diseases in a given plant leaf. The system has achieved outstanding results in multi-class and multi-label classification and can be used in a real-time setting to monitor large apple plantations to aid the farmers manage their yields effectively.



### Multimodal Analogical Reasoning over Knowledge Graphs
- **Arxiv ID**: http://arxiv.org/abs/2210.00312v4
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.00312v4)
- **Published**: 2022-10-01 16:24:15+00:00
- **Updated**: 2023-03-01 02:51:12+00:00
- **Authors**: Ningyu Zhang, Lei Li, Xiang Chen, Xiaozhuan Liang, Shumin Deng, Huajun Chen
- **Comment**: Accepted by ICLR 2023. The project website is
  https://zjunlp.github.io/project/MKG_Analogy/introduction.html
- **Journal**: None
- **Summary**: Analogical reasoning is fundamental to human cognition and holds an important place in various fields. However, previous studies mainly focus on single-modal analogical reasoning and ignore taking advantage of structure knowledge. Notably, the research in cognitive psychology has demonstrated that information from multimodal sources always brings more powerful cognitive transfer than single modality sources. To this end, we introduce the new task of multimodal analogical reasoning over knowledge graphs, which requires multimodal reasoning ability with the help of background knowledge. Specifically, we construct a Multimodal Analogical Reasoning dataSet (MARS) and a multimodal knowledge graph MarKG. We evaluate with multimodal knowledge graph embedding and pre-trained Transformer baselines, illustrating the potential challenges of the proposed task. We further propose a novel model-agnostic Multimodal analogical reasoning framework with Transformer (MarT) motivated by the structure mapping theory, which can obtain better performance. Code and datasets are available in https://github.com/zjunlp/MKG_Analogy.



### CAST: Concurrent Recognition and Segmentation with Adaptive Segment Tokens
- **Arxiv ID**: http://arxiv.org/abs/2210.00314v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.4.6; I.4.10; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/2210.00314v3)
- **Published**: 2022-10-01 16:31:44+00:00
- **Updated**: 2022-10-24 16:52:08+00:00
- **Authors**: Tsung-Wei Ke, Stella X. Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing an image and segmenting it into coherent regions are often treated as separate tasks. Human vision, however, has a general sense of segmentation hierarchy before recognition occurs. We are thus inspired to learn image recognition with hierarchical image segmentation based entirely on unlabeled images. Our insight is to learn fine-to-coarse features concurrently at superpixels, segments, and full image levels, enforcing consistency and goodness of feature induced segmentations while maximizing discrimination among image instances.   Our model innovates vision transformers on three aspects. 1) We use adaptive segment tokens instead of fixed-shape patch tokens. 2) We create a token hierarchy by inserting graph pooling between transformer blocks, naturally producing consistent multi-scale segmentations while increasing the segment size and reducing the number of tokens. 3) We produce hierarchical image segmentation for free while training for recognition by maximizing image-wise discrimination.   Our work delivers the first concurrent recognition and hierarchical segmentation model without any supervision. Validated on ImageNet and PASCAL VOC, it achieves better recognition and segmentation with higher computational efficiency.



### Longitudinal Sentiment Analyses for Radicalization Research: Intertemporal Dynamics on Social Media Platforms and their Implications
- **Arxiv ID**: http://arxiv.org/abs/2210.00339v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, stat.AP, D.1.5; D.3.0; J.4; K.4.1
- **Links**: [PDF](http://arxiv.org/pdf/2210.00339v1)
- **Published**: 2022-10-01 18:30:00+00:00
- **Updated**: 2022-10-01 18:30:00+00:00
- **Authors**: Dennis Klinkhammer
- **Comment**: 11 pages, 2 figures
- **Journal**: None
- **Summary**: This discussion paper demonstrates how longitudinal sentiment analyses can depict intertemporal dynamics on social media platforms, what challenges are inherent and how further research could benefit from a longitudinal perspective. Furthermore and since tools for sentiment analyses shall simplify and accelerate the analytical process regarding qualitative data at acceptable inter-rater reliability, their applicability in the context of radicalization research will be examined regarding the Tweets collected on January 6th 2021, the day of the storming of the U.S. Capitol in Washington. Therefore, a total of 49,350 Tweets will be analyzed evenly distributed within three different sequences: before, during and after the U.S. Capitol in Washington was stormed. These sequences highlight the intertemporal dynamics within comments on social media platforms as well as the possible benefits of a longitudinal perspective when using conditional means and conditional variances. Limitations regarding the identification of supporters of such events and associated hate speech as well as common application errors will be demonstrated as well. As a result, only under certain conditions a longitudinal sentiment analysis can increase the accuracy of evidence based predictions in the context of radicalization research.



### Evaluation of Pre-Trained CNN Models for Geographic Fake Image Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.00361v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00361v1)
- **Published**: 2022-10-01 20:37:24+00:00
- **Updated**: 2022-10-01 20:37:24+00:00
- **Authors**: Sid Ahmed Fezza, Mohammed Yasser Ouis, Bachir Kaddar, Wassim Hamidouche, Abdenour Hadid
- **Comment**: IEEE International Workshop on Multimedia Signal Processing
  (MMSP'2022)
- **Journal**: None
- **Summary**: Thanks to the remarkable advances in generative adversarial networks (GANs), it is becoming increasingly easy to generate/manipulate images. The existing works have mainly focused on deepfake in face images and videos. However, we are currently witnessing the emergence of fake satellite images, which can be misleading or even threatening to national security. Consequently, there is an urgent need to develop detection methods capable of distinguishing between real and fake satellite images. To advance the field, in this paper, we explore the suitability of several convolutional neural network (CNN) architectures for fake satellite image detection. Specifically, we benchmark four CNN models by conducting extensive experiments to evaluate their performance and robustness against various image distortions. This work allows the establishment of new baselines and may be useful for the development of CNN-based methods for fake satellite image detection.



### DCI-ES: An Extended Disentanglement Framework with Connections to Identifiability
- **Arxiv ID**: http://arxiv.org/abs/2210.00364v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2210.00364v2)
- **Published**: 2022-10-01 20:43:54+00:00
- **Updated**: 2023-02-16 20:43:19+00:00
- **Authors**: Cian Eastwood, Andrei Liviu Nicolicioiu, Julius von Kügelgen, Armin Kekić, Frederik Träuble, Andrea Dittadi, Bernhard Schölkopf
- **Comment**: Accepted to ICLR 2023
- **Journal**: None
- **Summary**: In representation learning, a common approach is to seek representations which disentangle the underlying factors of variation. Eastwood & Williams (2018) proposed three metrics for quantifying the quality of such disentangled representations: disentanglement (D), completeness (C) and informativeness (I). In this work, we first connect this DCI framework to two common notions of linear and nonlinear identifiability, thereby establishing a formal link between disentanglement and the closely-related field of independent component analysis. We then propose an extended DCI-ES framework with two new measures of representation quality - explicitness (E) and size (S) - and point out how D and C can be computed for black-box predictors. Our main idea is that the functional capacity required to use a representation is an important but thus-far neglected aspect of representation quality, which we quantify using explicitness or ease-of-use (E). We illustrate the relevance of our extensions on the MPI3D and Cars3D datasets.



### NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review
- **Arxiv ID**: http://arxiv.org/abs/2210.00379v4
- **DOI**: None
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2210.00379v4)
- **Published**: 2022-10-01 21:35:11+00:00
- **Updated**: 2023-05-10 22:13:47+00:00
- **Authors**: Kyle Gao, Yina Gao, Hongjie He, Dening Lu, Linlin Xu, Jonathan Li
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Radiance Field (NeRF), a new novel view synthesis with implicit scene representation has taken the field of Computer Vision by storm. As a novel view synthesis and 3D reconstruction method, NeRF models find applications in robotics, urban mapping, autonomous navigation, virtual reality/augmented reality, and more. Since the original paper by Mildenhall et al., more than 250 preprints were published, with more than 100 eventually being accepted in tier one Computer Vision Conferences. Given NeRF popularity and the current interest in this research area, we believe it necessary to compile a comprehensive survey of NeRF papers from the past two years, which we organized into both architecture, and application based taxonomies. We also provide an introduction to the theory of NeRF based novel view synthesis, and a benchmark comparison of the performance and speed of key NeRF models. By creating this survey, we hope to introduce new researchers to NeRF, provide a helpful reference for influential works in this field, as well as motivate future research directions with our discussion section.



