# Arxiv Papers in cs.CV on 2022-10-29
### Robust Boosting Forests with Richer Deep Feature Hierarchy
- **Arxiv ID**: http://arxiv.org/abs/2210.16451v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2210.16451v1)
- **Published**: 2022-10-29 00:40:17+00:00
- **Updated**: 2022-10-29 00:40:17+00:00
- **Authors**: Jianqiao Wangni
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a robust variant of boosting forest to the various adversarial defense methods, and apply it to enhance the robustness of the deep neural network. We retain the deep network architecture, weights, and middle layer features, then install gradient boosting forest to select the features from each layer of the deep network, and predict the target. For training each decision tree, we propose a novel conservative and greedy trade-off, with consideration for less misprediction instead of pure gain functions, therefore being suboptimal and conservative. We actively increase tree depth to remedy the accuracy with splits in more features, being more greedy in growing tree depth. We propose a new task on 3D face model, whose robustness has not been carefully studied, despite the great security and privacy concerns related to face analytics. We tried a simple attack method on a pure convolutional neural network (CNN) face shape estimator, making it degenerate to only output average face shape with invisible perturbation. Our conservative-greedy boosting forest (CGBF) on face landmark datasets showed a great improvement over original pure deep learning methods under the adversarial attacks.



### Joint Sub-component Level Segmentation and Classification for Anomaly Detection within Dual-Energy X-Ray Security Imagery
- **Arxiv ID**: http://arxiv.org/abs/2210.16453v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16453v1)
- **Published**: 2022-10-29 00:44:50+00:00
- **Updated**: 2022-10-29 00:44:50+00:00
- **Authors**: Neelanjan Bhowmik, Toby P. Breckon
- **Comment**: None
- **Journal**: None
- **Summary**: X-ray baggage security screening is in widespread use and crucial to maintaining transport security for threat/anomaly detection tasks. The automatic detection of anomaly, which is concealed within cluttered and complex electronics/electrical items, using 2D X-ray imagery is of primary interest in recent years. We address this task by introducing joint object sub-component level segmentation and classification strategy using deep Convolution Neural Network architecture. The performance is evaluated over a dataset of cluttered X-ray baggage security imagery, consisting of consumer electrical and electronics items using variants of dual-energy X-ray imagery (pseudo-colour, high, low, and effective-Z). The proposed joint sub-component level segmentation and classification approach achieve ~99% true positive and ~5% false positive for anomaly detection task.



### Region of Interest Detection in Melanocytic Skin Tumor Whole Slide Images
- **Arxiv ID**: http://arxiv.org/abs/2210.16457v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16457v1)
- **Published**: 2022-10-29 01:12:08+00:00
- **Updated**: 2022-10-29 01:12:08+00:00
- **Authors**: Yi Cui, Yao Li, Jayson R. Miedema, Sherif Farag, J. S. Marron, Nancy E. Thomas
- **Comment**: Accepted to MedNeurIPS 2022
- **Journal**: None
- **Summary**: Automated region of interest detection in histopathological image analysis is a challenging and important topic with tremendous potential impact on clinical practice. The deep-learning methods used in computational pathology help us to reduce costs and increase the speed and accuracy of regions of interest detection and cancer diagnosis. In this work, we propose a patch-based region of interest detection method for melanocytic skin tumor whole-slide images. We work with a dataset that contains 165 primary melanomas and nevi Hematoxylin and Eosin whole-slide images and build a deep-learning method. The proposed method performs well on a hold-out test data set including five TCGA-SKCM slides (accuracy of 93.94\% in slide classification task and intersection over union rate of 41.27\% in the region of interest detection task), showing the outstanding performance of our model on melanocytic skin tumor. Even though we test the experiments on the skin tumor dataset, our work could also be extended to other medical image detection problems, such as various tumors' classification and prediction, to help and benefit the clinical evaluation and diagnosis of different tumors.



### ImplantFormer: Vision Transformer based Implant Position Regression Using Dental CBCT Data
- **Arxiv ID**: http://arxiv.org/abs/2210.16467v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16467v3)
- **Published**: 2022-10-29 02:31:27+00:00
- **Updated**: 2023-06-07 07:12:45+00:00
- **Authors**: Xinquan Yang, Xuguang Li, Xuechen Li, Peixi Wu, Linlin Shen, Yongqiang Deng
- **Comment**: None
- **Journal**: None
- **Summary**: Implant prosthesis is the most appropriate treatment for dentition defect or dentition loss, which usually involves a surgical guide design process to decide the implant position. However, such design heavily relies on the subjective experiences of dentists. In this paper, a transformer-based Implant Position Regression Network, ImplantFormer, is proposed to automatically predict the implant position based on the oral CBCT data. We creatively propose to predict the implant position using the 2D axial view of the tooth crown area and fit a centerline of the implant to obtain the actual implant position at the tooth root. Convolutional stem and decoder are designed to coarsely extract image features before the operation of patch embedding and integrate multi-level feature maps for robust prediction, respectively. As both long-range relationship and local features are involved, our approach can better represent global information and achieves better location performance. Extensive experiments on a dental implant dataset through five-fold cross-validation demonstrated that the proposed ImplantFormer achieves superior performance than existing methods.



### Learning Audio-Visual Dynamics Using Scene Graphs for Audio Source Separation
- **Arxiv ID**: http://arxiv.org/abs/2210.16472v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2210.16472v1)
- **Published**: 2022-10-29 02:55:39+00:00
- **Updated**: 2022-10-29 02:55:39+00:00
- **Authors**: Moitreya Chatterjee, Narendra Ahuja, Anoop Cherian
- **Comment**: Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: There exists an unequivocal distinction between the sound produced by a static source and that produced by a moving one, especially when the source moves towards or away from the microphone. In this paper, we propose to use this connection between audio and visual dynamics for solving two challenging tasks simultaneously, namely: (i) separating audio sources from a mixture using visual cues, and (ii) predicting the 3D visual motion of a sounding source using its separated audio. Towards this end, we present Audio Separator and Motion Predictor (ASMP) -- a deep learning framework that leverages the 3D structure of the scene and the motion of sound sources for better audio source separation. At the heart of ASMP is a 2.5D scene graph capturing various objects in the video and their pseudo-3D spatial proximities. This graph is constructed by registering together 2.5D monocular depth predictions from the 2D video frames and associating the 2.5D scene regions with the outputs of an object detector applied on those frames. The ASMP task is then mathematically modeled as the joint problem of: (i) recursively segmenting the 2.5D scene graph into several sub-graphs, each associated with a constituent sound in the input audio mixture (which is then separated) and (ii) predicting the 3D motions of the corresponding sound sources from the separated audio. To empirically evaluate ASMP, we present experiments on two challenging audio-visual datasets, viz. Audio Separation in the Wild (ASIW) and Audio Visual Event (AVE). Our results demonstrate that ASMP achieves a clear improvement in source separation quality, outperforming prior works on both datasets, while also estimating the direction of motion of the sound sources better than other methods.



### Pair DETR: Contrastive Learning Speeds Up DETR Training
- **Arxiv ID**: http://arxiv.org/abs/2210.16476v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16476v2)
- **Published**: 2022-10-29 03:02:49+00:00
- **Updated**: 2022-11-11 17:32:38+00:00
- **Authors**: Seyed Mehdi Iranmanesh, Xiaotong Chen, Kuo-Chin Lien
- **Comment**: arXiv admin note: text overlap with arXiv:2108.06152
- **Journal**: None
- **Summary**: The DETR object detection approach applies the transformer encoder and decoder architecture to detect objects and achieves promising performance. In this paper, we present a simple approach to address the main problem of DETR, the slow convergence, by using representation learning technique. In this approach, we detect an object bounding box as a pair of keypoints, the top-left corner and the center, using two decoders. By detecting objects as paired keypoints, the model builds up a joint classification and pair association on the output queries from two decoders. For the pair association we propose utilizing contrastive self-supervised learning algorithm without requiring specialized architecture. Experimental results on MS COCO dataset show that Pair DETR can converge at least 10x faster than original DETR and 1.5x faster than Conditional DETR during training, while having consistently higher Average Precision scores.



### GPA-Net:No-Reference Point Cloud Quality Assessment with Multi-task Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2210.16478v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.16478v3)
- **Published**: 2022-10-29 03:06:55+00:00
- **Updated**: 2023-06-01 14:42:23+00:00
- **Authors**: Ziyu Shan, Qi Yang, Rui Ye, Yujie Zhang, Yiling Xu, Xiaozhong Xu, Shan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid development of 3D vision, point cloud has become an increasingly popular 3D visual media content. Due to the irregular structure, point cloud has posed novel challenges to the related research, such as compression, transmission, rendering and quality assessment. In these latest researches, point cloud quality assessment (PCQA) has attracted wide attention due to its significant role in guiding practical applications, especially in many cases where the reference point cloud is unavailable. However, current no-reference metrics which based on prevalent deep neural network have apparent disadvantages. For example, to adapt to the irregular structure of point cloud, they require preprocessing such as voxelization and projection that introduce extra distortions, and the applied grid-kernel networks, such as Convolutional Neural Networks, fail to extract effective distortion-related features. Besides, they rarely consider the various distortion patterns and the philosophy that PCQA should exhibit shifting, scaling, and rotational invariance. In this paper, we propose a novel no-reference PCQA metric named the Graph convolutional PCQA network (GPA-Net). To extract effective features for PCQA, we propose a new graph convolution kernel, i.e., GPAConv, which attentively captures the perturbation of structure and texture. Then, we propose the multi-task framework consisting of one main task (quality regression) and two auxiliary tasks (distortion type and degree predictions). Finally, we propose a coordinate normalization module to stabilize the results of GPAConv under shift, scale and rotation transformations. Experimental results on two independent databases show that GPA-Net achieves the best performance compared to the state-of-the-art no-reference PCQA metrics, even better than some full-reference metrics in some cases.



### IM: An R-Package for Computation of Image Moments and Moment Invariants
- **Arxiv ID**: http://arxiv.org/abs/2210.16485v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MS, I.4.7
- **Links**: [PDF](http://arxiv.org/pdf/2210.16485v1)
- **Published**: 2022-10-29 03:54:37+00:00
- **Updated**: 2022-10-29 03:54:37+00:00
- **Authors**: Allison Irvine, Tan Dang, M. Murat Dundar, Bartek Rajwa
- **Comment**: Apr 2014, technical report
- **Journal**: None
- **Summary**: Moment invariants are well-established and effective shape descriptors for image classification. In this report, we introduce a package for R-language, named IM, that implements the calculation of moments for images and allows the reconstruction of images from moments within an object-oriented framework. Several types of moments may be computed using the IM library, including discrete and continuous Chebyshev, Gegenbauer, Legendre, Krawtchouk, dual Hahn, generalized pseudo-Zernike, Fourier-Mellin, and radial harmonic Fourier moments. In addition, custom bivariate types of moments can be calculated using combinations of two different types of polynomials. A method of polar transformation of pixel coordinates is used to provide an approximate invariance to rotation for moments that are orthogonal over a rectangle. The different types of polynomials used to calculate moments are discussed in this report, as well as comparisons of reconstruction and running time. Examples of image classification using image moments are provided.



### Learning Probabilistic Models from Generator Latent Spaces with Hat EBM
- **Arxiv ID**: http://arxiv.org/abs/2210.16486v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2210.16486v2)
- **Published**: 2022-10-29 03:55:34+00:00
- **Updated**: 2023-01-13 02:45:19+00:00
- **Authors**: Mitch Hill, Erik Nijkamp, Jonathan Mitchell, Bo Pang, Song-Chun Zhu
- **Comment**: NeurIPS 2022 camera ready
- **Journal**: None
- **Summary**: This work proposes a method for using any generator network as the foundation of an Energy-Based Model (EBM). Our formulation posits that observed images are the sum of unobserved latent variables passed through the generator network and a residual random variable that spans the gap between the generator output and the image manifold. One can then define an EBM that includes the generator as part of its forward pass, which we call the Hat EBM. The model can be trained without inferring the latent variables of the observed data or calculating the generator Jacobian determinant. This enables explicit probabilistic modeling of the output distribution of any type of generator network. Experiments show strong performance of the proposed method on (1) unconditional ImageNet synthesis at 128x128 resolution, (2) refining the output of existing generators, and (3) learning EBMs that incorporate non-probabilistic generators. Code and pretrained models to reproduce our results are available at https://github.com/point0bar1/hat-ebm.



### Hybridization of filter and wrapper approaches for the dimensionality reduction and classification of hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/2210.16496v1
- **DOI**: 10.1109/ATSIP.2017.8075549
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16496v1)
- **Published**: 2022-10-29 05:25:10+00:00
- **Updated**: 2022-10-29 05:25:10+00:00
- **Authors**: Asma Elmaizi, Maria Merzouqi, Elkebir Sarhrouni, Ahmed hammouch, Chafik Nacir
- **Comment**: None
- **Journal**: Proceedings - 3rd International Conference on Advanced
  Technologies for Signal and Image Processing, ATSIP 2017, 2017, 8075549 -
  http://www.scopus.com/inward/record.url?eid=2-s2.0-85035329769&partnerID=MN8TOARS
- **Summary**: The high dimensionality of hyperspectral images often imposes a heavy computational burden for image processing. Therefore, dimensionality reduction is often an essential step in order to remove the irrelevant, noisy and redundant bands. And consequently, increase the classification accuracy. However, identification of useful bands from hundreds or even thousands of related bands is a nontrivial task. This paper aims at identifying a small set of bands, for improving computational speed and prediction accuracy. Hence, we have proposed a hybrid algorithm through band selection for dimensionality reduction of hyperspectral images. The proposed approach combines mutual information gain (MIG), Minimum Redundancy Maximum Relevance (mRMR) and Error probability of Fano with Support Vector Machine Bands Elimination (SVM-PF). The proposed approach is compared to an effective reproduced filters approach based on mutual information. Experimental results on HSI AVIRIS 92AV3C have shown that the proposed approach outperforms the reproduced filters.   Keywords - Hyperspectral images, Classification, band Selection, filter, wrapper, mutual information, information gain.



### A pruning method based on the dissimilarity of angle among channels and filters
- **Arxiv ID**: http://arxiv.org/abs/2210.16504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16504v1)
- **Published**: 2022-10-29 05:47:57+00:00
- **Updated**: 2022-10-29 05:47:57+00:00
- **Authors**: Jiayi Yao, Ping Li, Xiatao Kang, Yuzhe Wang
- **Comment**: Accepted by ICTAI 2022
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN) is more and more widely used in various fileds, and its computation and memory-demand are also increasing significantly. In order to make it applicable to limited conditions such as embedded application, network compression comes out. Among them, researchers pay more attention to network pruning. In this paper, we encode the convolution network to obtain the similarity of different encoding nodes, and evaluate the connectivity-power among convolutional kernels on the basis of the similarity. Then impose different level of penalty according to different connectivity-power. Meanwhile, we propose Channel Pruning base on the Dissimilarity of Angle (DACP). Firstly, we train a sparse model by GL penalty, and impose an angle dissimilarity constraint on the channels and filters of convolutional network to obtain a more sparse structure. Eventually, the effectiveness of our method is demonstrated in the section of experiment. On CIFAR-10, we reduce 66.86% FLOPs on VGG-16 with 93.31% accuracy after pruning, where FLOPs represents the number of floating-point operations per second of the model. Moreover, on ResNet-32, we reduce FLOPs by 58.46%, which makes the accuracy after pruning reach 91.76%.



### Few-shot Image Generation via Adaptation-Aware Kernel Modulation
- **Arxiv ID**: http://arxiv.org/abs/2210.16559v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16559v3)
- **Published**: 2022-10-29 10:26:40+00:00
- **Updated**: 2023-05-09 16:42:00+00:00
- **Authors**: Yunqing Zhao, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, Ngai-Man Cheung
- **Comment**: The Thirty-Sixth Annual Conference on Neural Information Processing
  Systems (NeurIPS 2022), 14 pages
- **Journal**: None
- **Summary**: Few-shot image generation (FSIG) aims to learn to generate new and diverse samples given an extremely limited number of samples from a domain, e.g., 10 training samples. Recent work has addressed the problem using transfer learning approach, leveraging a GAN pretrained on a large-scale source domain dataset and adapting that model to the target domain based on very limited target domain samples. Central to recent FSIG methods are knowledge preserving criteria, which aim to select a subset of source model's knowledge to be preserved into the adapted model. However, a major limitation of existing methods is that their knowledge preserving criteria consider only source domain/source task, and they fail to consider target domain/adaptation task in selecting source model's knowledge, casting doubt on their suitability for setups of different proximity between source and target domain. Our work makes two contributions. As our first contribution, we re-visit recent FSIG works and their experiments. Our important finding is that, under setups which assumption of close proximity between source and target domains is relaxed, existing state-of-the-art (SOTA) methods which consider only source domain/source task in knowledge preserving perform no better than a baseline fine-tuning method. To address the limitation of existing methods, as our second contribution, we propose Adaptation-Aware kernel Modulation (AdAM) to address general FSIG of different source-target domain proximity. Extensive experimental results show that the proposed method consistently achieves SOTA performance across source/target domains of different proximity, including challenging setups when source and target domains are more apart. Project Page: https://yunqing-me.github.io/AdAM/



### iSmallNet: Densely Nested Network with Label Decoupling for Infrared Small Target Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.16561v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.16561v2)
- **Published**: 2022-10-29 10:27:54+00:00
- **Updated**: 2023-06-29 10:51:43+00:00
- **Authors**: Zhiheng Hu, Yongzhen Wang, Peng Li, Jie Qin, Haoran Xie, Mingqiang Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Small targets are often submerged in cluttered backgrounds of infrared images. Conventional detectors tend to generate false alarms, while CNN-based detectors lose small targets in deep layers. To this end, we propose iSmallNet, a multi-stream densely nested network with label decoupling for infrared small object detection. On the one hand, to fully exploit the shape information of small targets, we decouple the original labeled ground-truth (GT) map into an interior map and a boundary one. The GT map, in collaboration with the two additional maps, tackles the unbalanced distribution of small object boundaries. On the other hand, two key modules are delicately designed and incorporated into the proposed network to boost the overall performance. First, to maintain small targets in deep layers, we develop a multi-scale nested interaction module to explore a wide range of context information. Second, we develop an interior-boundary fusion module to integrate multi-granularity information. Experiments on NUAA-SIRST and NUDT-SIRST clearly show the superiority of iSmallNet over 11 state-of-the-art detectors.



### DeFIX: Detecting and Fixing Failure Scenarios with Reinforcement Learning in Imitation Learning Based Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2210.16567v1
- **DOI**: 10.1109/ITSC55140.2022.9922209
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16567v1)
- **Published**: 2022-10-29 10:58:43+00:00
- **Updated**: 2022-10-29 10:58:43+00:00
- **Authors**: Resul Dagdanov, Feyza Eksen, Halil Durmus, Ferhat Yurdakul, Nazim Kemal Ure
- **Comment**: 6 pages, 4 figures, 2 tables, published in IEEE International
  Conference on Intelligent Transportation Systems (ITSC), October 12, 2022,
  Macau, China
- **Journal**: 2022 IEEE 25th International Conference on Intelligent
  Transportation Systems (ITSC), 2022, pp. 4215-4220
- **Summary**: Safely navigating through an urban environment without violating any traffic rules is a crucial performance target for reliable autonomous driving. In this paper, we present a Reinforcement Learning (RL) based methodology to DEtect and FIX (DeFIX) failures of an Imitation Learning (IL) agent by extracting infraction spots and re-constructing mini-scenarios on these infraction areas to train an RL agent for fixing the shortcomings of the IL approach. DeFIX is a continuous learning framework, where extraction of failure scenarios and training of RL agents are executed in an infinite loop. After each new policy is trained and added to the library of policies, a policy classifier method effectively decides on which policy to activate at each step during the evaluation. It is demonstrated that even with only one RL agent trained on failure scenario of an IL agent, DeFIX method is either competitive or does outperform state-of-the-art IL and RL based autonomous urban driving benchmarks. We trained and validated our approach on the most challenging map (Town05) of CARLA simulator which involves complex, realistic, and adversarial driving scenarios. The source code is publicly available at https://github.com/data-and-decision-lab/DeFIX



### SearchTrack: Multiple Object Tracking with Object-Customized Search and Motion-Aware Features
- **Arxiv ID**: http://arxiv.org/abs/2210.16572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16572v1)
- **Published**: 2022-10-29 11:17:53+00:00
- **Updated**: 2022-10-29 11:17:53+00:00
- **Authors**: Zhong-Min Tsai, Yu-Ju Tsai, Chien-Yao Wang, Hong-Yuan Liao, Youn-Long Lin, Yung-Yu Chuang
- **Comment**: BMVC 2022. Code: https://github.com/qa276390/SearchTrack
- **Journal**: None
- **Summary**: The paper presents a new method, SearchTrack, for multiple object tracking and segmentation (MOTS). To address the association problem between detected objects, SearchTrack proposes object-customized search and motion-aware features. By maintaining a Kalman filter for each object, we encode the predicted motion into the motion-aware feature, which includes both motion and appearance cues. For each object, a customized fully convolutional search engine is created by SearchTrack by learning a set of weights for dynamic convolutions specific to the object. Experiments demonstrate that our SearchTrack method outperforms competitive methods on both MOTS and MOT tasks, particularly in terms of association accuracy. Our method achieves 71.5 HOTA (car) and 57.6 HOTA (pedestrian) on the KITTI MOTS and 53.4 HOTA on MOT17. In terms of association accuracy, our method achieves state-of-the-art performance among 2D online methods on the KITTI MOTS. Our code is available at https://github.com/qa276390/SearchTrack.



### Boosting Monocular 3D Object Detection with Object-Centric Auxiliary Depth Supervision
- **Arxiv ID**: http://arxiv.org/abs/2210.16574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16574v1)
- **Published**: 2022-10-29 11:32:28+00:00
- **Updated**: 2022-10-29 11:32:28+00:00
- **Authors**: Youngseok Kim, Sanmin Kim, Sangmin Sim, Jun Won Choi, Dongsuk Kum
- **Comment**: Accepted by IEEE Transaction on Intelligent Transportation System
  (T-ITS)
- **Journal**: None
- **Summary**: Recent advances in monocular 3D detection leverage a depth estimation network explicitly as an intermediate stage of the 3D detection network. Depth map approaches yield more accurate depth to objects than other methods thanks to the depth estimation network trained on a large-scale dataset. However, depth map approaches can be limited by the accuracy of the depth map, and sequentially using two separated networks for depth estimation and 3D detection significantly increases computation cost and inference time. In this work, we propose a method to boost the RGB image-based 3D detector by jointly training the detection network with a depth prediction loss analogous to the depth estimation task. In this way, our 3D detection network can be supervised by more depth supervision from raw LiDAR points, which does not require any human annotation cost, to estimate accurate depth without explicitly predicting the depth map. Our novel object-centric depth prediction loss focuses on depth around foreground objects, which is important for 3D object detection, to leverage pixel-wise depth supervision in an object-centric manner. Our depth regression model is further trained to predict the uncertainty of depth to represent the 3D confidence of objects. To effectively train the 3D detector with raw LiDAR points and to enable end-to-end training, we revisit the regression target of 3D objects and design a network architecture. Extensive experiments on KITTI and nuScenes benchmarks show that our method can significantly boost the monocular image-based 3D detector to outperform depth map approaches while maintaining the real-time inference speed.



### INR-V: A Continuous Representation Space for Video-based Generative Tasks
- **Arxiv ID**: http://arxiv.org/abs/2210.16579v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16579v2)
- **Published**: 2022-10-29 11:54:58+00:00
- **Updated**: 2023-04-03 02:58:58+00:00
- **Authors**: Bipasha Sen, Aditya Agarwal, Vinay P Namboodiri, C. V. Jawahar
- **Comment**: Published in Transactions on Machine Learning Research (10/2022);
  https://openreview.net/forum?id=aIoEkwc2oB
- **Journal**: None
- **Summary**: Generating videos is a complex task that is accomplished by generating a set of temporally coherent images frame-by-frame. This limits the expressivity of videos to only image-based operations on the individual video frames needing network designs to obtain temporally coherent trajectories in the underlying image space. We propose INR-V, a video representation network that learns a continuous space for video-based generative tasks. INR-V parameterizes videos using implicit neural representations (INRs), a multi-layered perceptron that predicts an RGB value for each input pixel location of the video. The INR is predicted using a meta-network which is a hypernetwork trained on neural representations of multiple video instances. Later, the meta-network can be sampled to generate diverse novel videos enabling many downstream video-based generative tasks. Interestingly, we find that conditional regularization and progressive weight initialization play a crucial role in obtaining INR-V. The representation space learned by INR-V is more expressive than an image space showcasing many interesting properties not possible with the existing works. For instance, INR-V can smoothly interpolate intermediate videos between known video instances (such as intermediate identities, expressions, and poses in face videos). It can also in-paint missing portions in videos to recover temporally coherent full videos. In this work, we evaluate the space learned by INR-V on diverse generative tasks such as video interpolation, novel video generation, video inversion, and video inpainting against the existing baselines. INR-V significantly outperforms the baselines on several of these demonstrated tasks, clearly showcasing the potential of the proposed representation space.



### CMT: Interpretable Model for Rapid Recognition Pneumonia from Chest X-Ray Images by Fusing Low Complexity Multilevel Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2210.16584v1
- **DOI**: 10.1109/JBHI.2023.3247949
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16584v1)
- **Published**: 2022-10-29 12:12:03+00:00
- **Updated**: 2022-10-29 12:12:03+00:00
- **Authors**: Shengchao Chen, Sufen Ren, Guanjun Wang, Mengxing Huang, Chenyang Xue
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: Chest imaging plays an essential role in diagnosing and predicting patients with COVID-19 with evidence of worsening respiratory status. Many deep learning-based diagnostic models for pneumonia have been developed to enable computer-aided diagnosis. However, the long training and inference time make them inflexible. In addition, the lack of interpretability reduces their credibility in clinical medical practice. This paper presents CMT, a model with interpretability and rapid recognition of pneumonia, especially COVID-19 positive. Multiple convolutional layers in CMT are first used to extract features in CXR images, and then Transformer is applied to calculate the possibility of each symptom. To improve the model's generalization performance and to address the problem of sparse medical image data, we propose Feature Fusion Augmentation (FFA), a plug-and-play method for image augmentation. It fuses the features of the two images to varying degrees to produce a new image that does not deviate from the original distribution. Furthermore, to reduce the computational complexity and accelerate the convergence, we propose Multilevel Multi-Head Self-Attention (MMSA), which computes attention on different levels to establish the relationship between global and local features. It significantly improves the model performance while substantially reducing its training and inference time. Experimental results on the largest COVID-19 dataset show the proposed CMT has state-of-the-art performance. The effectiveness of FFA and MMSA is demonstrated in the ablation experiments. In addition, the weights and feature activation maps of the model inference process are visualized to show the CMT's interpretability.



### TFormer: 3D Tooth Segmentation in Mesh Scans with Geometry Guided Transformer
- **Arxiv ID**: http://arxiv.org/abs/2210.16627v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16627v1)
- **Published**: 2022-10-29 15:20:54+00:00
- **Updated**: 2022-10-29 15:20:54+00:00
- **Authors**: Huimin Xiong, Kunle Li, Kaiyuan Tan, Yang Feng, Joey Tianyi Zhou, Jin Hao, Zuozhu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Optical Intra-oral Scanners (IOS) are widely used in digital dentistry, providing 3-Dimensional (3D) and high-resolution geometrical information of dental crowns and the gingiva. Accurate 3D tooth segmentation, which aims to precisely delineate the tooth and gingiva instances in IOS, plays a critical role in a variety of dental applications. However, segmentation performance of previous methods are error-prone in complicated tooth-tooth or tooth-gingiva boundaries, and usually exhibit unsatisfactory results across various patients, yet the clinically applicability is not verified with large-scale dataset. In this paper, we propose a novel method based on 3D transformer architectures that is evaluated with large-scale and high-resolution 3D IOS datasets. Our method, termed TFormer, captures both local and global dependencies among different teeth to distinguish various types of teeth with divergent anatomical structures and confusing boundaries. Moreover, we design a geometry guided loss based on a novel point curvature to exploit boundary geometric features, which helps refine the boundary predictions for more accurate and smooth segmentation. We further employ a multi-task learning scheme, where an additional teeth-gingiva segmentation head is introduced to improve the performance. Extensive experimental results in a large-scale dataset with 16,000 IOS, the largest IOS dataset to our best knowledge, demonstrate that our TFormer can surpass existing state-of-the-art baselines with a large margin, with its utility in real-world scenarios verified by a clinical applicability test.



### 2D and 3D CT Radiomic Features Performance Comparison in Characterization of Gastric Cancer: A Multi-center Study
- **Arxiv ID**: http://arxiv.org/abs/2210.16640v1
- **DOI**: 10.1109/JBHI.2020.3002805
- **Categories**: **eess.IV**, cs.CV, eess.SP, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2210.16640v1)
- **Published**: 2022-10-29 16:09:07+00:00
- **Updated**: 2022-10-29 16:09:07+00:00
- **Authors**: Lingwei Meng, Di Dong, Xin Chen, Mengjie Fang, Rongpin Wang, Jing Li, Zaiyi Liu, Jie Tian
- **Comment**: Published in IEEE Journal of Biomedical and Health Informatics
- **Journal**: IEEE.J.Biomed.Health.Inf. 25 (2021) 755-763
- **Summary**: Objective: Radiomics, an emerging tool for medical image analysis, is potential towards precisely characterizing gastric cancer (GC). Whether using one-slice 2D annotation or whole-volume 3D annotation remains a long-time debate, especially for heterogeneous GC. We comprehensively compared 2D and 3D radiomic features' representation and discrimination capacity regarding GC, via three tasks.   Methods: Four-center 539 GC patients were retrospectively enrolled and divided into the training and validation cohorts. From 2D or 3D regions of interest (ROIs) annotated by radiologists, radiomic features were extracted respectively. Feature selection and model construction procedures were customed for each combination of two modalities (2D or 3D) and three tasks. Subsequently, six machine learning models (Model_2D^LNM, Model_3D^LNM; Model_2D^LVI, Model_3D^LVI; Model_2D^pT, Model_3D^pT) were derived and evaluated to reflect modalities' performances in characterizing GC. Furthermore, we performed an auxiliary experiment to assess modalities' performances when resampling spacing is different.   Results: Regarding three tasks, the yielded areas under the curve (AUCs) were: Model_2D^LNM's 0.712 (95% confidence interval, 0.613-0.811), Model_3D^LNM's 0.680 (0.584-0.775); Model_2D^LVI's 0.677 (0.595-0.761), Model_3D^LVI's 0.615 (0.528-0.703); Model_2D^pT's 0.840 (0.779-0.901), Model_3D^pT's 0.813 (0.747-0.879). Moreover, the auxiliary experiment indicated that Models_2D are statistically more advantageous than Models3D with different resampling spacings.   Conclusion: Models constructed with 2D radiomic features revealed comparable performances with those constructed with 3D features in characterizing GC.   Significance: Our work indicated that time-saving 2D annotation would be the better choice in GC, and provided a related reference to further radiomics-based researches.



### Unsupervised Audio-Visual Lecture Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.16644v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16644v1)
- **Published**: 2022-10-29 16:26:34+00:00
- **Updated**: 2022-10-29 16:26:34+00:00
- **Authors**: Darshan Singh S, Anchit Gupta, C. V. Jawahar, Makarand Tapaswi
- **Comment**: 17 pages, 14 figures, 14 tables, Accepted to WACV 2023. Project page:
  https://cvit.iiit.ac.in/research/projects/cvit-projects/avlectures
- **Journal**: None
- **Summary**: Over the last decade, online lecture videos have become increasingly popular and have experienced a meteoric rise during the pandemic. However, video-language research has primarily focused on instructional videos or movies, and tools to help students navigate the growing online lectures are lacking. Our first contribution is to facilitate research in the educational domain, by introducing AVLectures, a large-scale dataset consisting of 86 courses with over 2,350 lectures covering various STEM subjects. Each course contains video lectures, transcripts, OCR outputs for lecture frames, and optionally lecture notes, slides, assignments, and related educational content that can inspire a variety of tasks. Our second contribution is introducing video lecture segmentation that splits lectures into bite-sized topics that show promise in improving learner engagement. We formulate lecture segmentation as an unsupervised task that leverages visual, textual, and OCR cues from the lecture, while clip representations are fine-tuned on a pretext self-supervised task of matching the narration with the temporally aligned visual content. We use these representations to generate segments using a temporally consistent 1-nearest neighbor algorithm, TW-FINCH. We evaluate our method on 15 courses and compare it against various visual and textual baselines, outperforming all of them. Our comprehensive ablation studies also identify the key factors driving the success of our approach.



### Breaking the Symmetry: Resolving Symmetry Ambiguities in Equivariant Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.16646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16646v1)
- **Published**: 2022-10-29 16:28:59+00:00
- **Updated**: 2022-10-29 16:28:59+00:00
- **Authors**: Sidhika Balachandar, Adrien Poulenard, Congyue Deng, Leonidas Guibas
- **Comment**: 13 pages, 8 figures, NeurIPS Symmetry and Geometry in Neural
  Representations Conference
- **Journal**: None
- **Summary**: Equivariant networks have been adopted in many 3-D learning areas. Here we identify a fundamental limitation of these networks: their ambiguity to symmetries. Equivariant networks cannot complete symmetry-dependent tasks like segmenting a left-right symmetric object into its left and right sides. We tackle this problem by adding components that resolve symmetry ambiguities while preserving rotational equivariance. We present OAVNN: Orientation Aware Vector Neuron Network, an extension of the Vector Neuron Network. OAVNN is a rotation equivariant network that is robust to planar symmetric inputs. Our network consists of three key components. 1) We introduce an algorithm to calculate symmetry detecting features. 2) We create a symmetry-sensitive orientation aware linear layer. 3) We construct an attention mechanism that relates directional information across points. We evaluate the network using left-right segmentation and find that the network quickly obtains accurate segmentations. We hope this work motivates investigations on the expressivity of equivariant networks on symmetric objects.



### Rare Wildlife Recognition with Self-Supervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2211.05636v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2211.05636v1)
- **Published**: 2022-10-29 17:57:38+00:00
- **Updated**: 2022-10-29 17:57:38+00:00
- **Authors**: Xiaochen Zheng
- **Comment**: A dissertation submitted to attain the degree of Master of Sciences
  of ETH Zurich in August 2021. Code is available at:
  https://github.com/xcvil/self-rare-wildlife
- **Journal**: None
- **Summary**: Automated animal censuses with aerial imagery are a vital ingredient towards wildlife conservation. Recent models are generally based on supervised learning and thus require vast amounts of training data. Due to their scarcity and minuscule size, annotating animals in aerial imagery is a highly tedious process. In this project, we present a methodology to reduce the amount of required training data by resorting to self-supervised pretraining. In detail, we examine a combination of recent contrastive learning methodologies like Momentum Contrast (MoCo) and Cross-Level Instance-Group Discrimination (CLD) to condition our model on the aerial images without the requirement for labels. We show that a combination of MoCo, CLD, and geometric augmentations outperforms conventional models pretrained on ImageNet by a large margin. Meanwhile, strategies for smoothing label or prediction distribution in supervised learning have been proven useful in preventing the model from overfitting. We combine the self-supervised contrastive models with image mixup strategies and find that it is useful for learning more robust visual representations. Crucially, our methods still yield favorable results even if we reduce the number of training animals to just 10%, at which point our best model scores double the recall of the baseline at similar precision. This effectively allows reducing the number of required annotations to a fraction while still being able to train high-accuracy models in such highly challenging settings.



### A Comparative Study of Graph Neural Networks for Shape Classification in Neuroimaging
- **Arxiv ID**: http://arxiv.org/abs/2210.16670v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16670v1)
- **Published**: 2022-10-29 19:03:01+00:00
- **Updated**: 2022-10-29 19:03:01+00:00
- **Authors**: Nairouz Shehata, Wulfie Bain, Ben Glocker
- **Comment**: Accepted at GeoMedIA Workshop 2022 (Proceedings of Machine Learning
  Research). Code available on https://github.com/biomedia-mira/medmesh
- **Journal**: None
- **Summary**: Graph neural networks have emerged as a promising approach for the analysis of non-Euclidean data such as meshes. In medical imaging, mesh-like data plays an important role for modelling anatomical structures, and shape classification can be used in computer aided diagnosis and disease detection. However, with a plethora of options, the best architectural choices for medical shape analysis using GNNs remain unclear. We conduct a comparative analysis to provide practitioners with an overview of the current state-of-the-art in geometric deep learning for shape classification in neuroimaging. Using biological sex classification as a proof-of-concept task, we find that using FPFH as node features substantially improves GNN performance and generalisation to out-of-distribution data; we compare the performance of three alternative convolutional layers; and we reinforce the importance of data augmentation for graph based learning. We then confirm these results hold for a clinically relevant task, using the classification of Alzheimer's disease.



### Semantic-SuPer: A Semantic-aware Surgical Perception Framework for Endoscopic Tissue Identification, Reconstruction, and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2210.16674v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16674v2)
- **Published**: 2022-10-29 19:33:21+00:00
- **Updated**: 2023-02-20 07:51:54+00:00
- **Authors**: Shan Lin, Albert J. Miao, Jingpei Lu, Shunkai Yu, Zih-Yun Chiu, Florian Richter, Michael C. Yip
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA) 2023
- **Journal**: None
- **Summary**: Accurate and robust tracking and reconstruction of the surgical scene is a critical enabling technology toward autonomous robotic surgery. Existing algorithms for 3D perception in surgery mainly rely on geometric information, while we propose to also leverage semantic information inferred from the endoscopic video using image segmentation algorithms. In this paper, we present a novel, comprehensive surgical perception framework, Semantic-SuPer, that integrates geometric and semantic information to facilitate data association, 3D reconstruction, and tracking of endoscopic scenes, benefiting downstream tasks like surgical navigation. The proposed framework is demonstrated on challenging endoscopic data with deforming tissue, showing its advantages over our baseline and several other state-of the-art approaches. Our code and dataset are available at https://github.com/ucsdarclab/Python-SuPer.



### Single-Shot Domain Adaptation via Target-Aware Generative Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.16692v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2210.16692v1)
- **Published**: 2022-10-29 20:53:57+00:00
- **Updated**: 2022-10-29 20:53:57+00:00
- **Authors**: Rakshith Subramanyam, Kowshik Thopalli, Spring Berman, Pavan Turaga, Jayaraman J. Thiagarajan
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of adapting models from a source domain using data from any target domain of interest has gained prominence, thanks to the brittle generalization in deep neural networks. While several test-time adaptation techniques have emerged, they typically rely on synthetic data augmentations in cases of limited target data availability. In this paper, we consider the challenging setting of single-shot adaptation and explore the design of augmentation strategies. We argue that augmentations utilized by existing methods are insufficient to handle large distribution shifts, and hence propose a new approach SiSTA (Single-Shot Target Augmentations), which first fine-tunes a generative model from the source domain using a single-shot target, and then employs novel sampling strategies for curating synthetic target data. Using experiments with a state-of-the-art domain adaptation method, we find that SiSTA produces improvements as high as 20\% over existing baselines under challenging shifts in face attribute detection, and that it performs competitively to oracle models obtained by training on a larger target dataset.



### Multi-Scale Fusion Methodologies for Head and Neck Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.16704v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16704v1)
- **Published**: 2022-10-29 22:44:26+00:00
- **Updated**: 2022-10-29 22:44:26+00:00
- **Authors**: Abhishek Srivastava, Debesh Jha, Bulent Aydogan, Mohamed E. Abazeed, Ulas Bagci
- **Comment**: None
- **Journal**: None
- **Summary**: Head and Neck (H\&N) organ-at-risk (OAR) and tumor segmentations are essential components of radiation therapy planning. The varying anatomic locations and dimensions of H\&N nodal Gross Tumor Volumes (GTVn) and H\&N primary gross tumor volume (GTVp) are difficult to obtain due to lack of accurate and reliable delineation methods. The downstream effect of incorrect segmentation can result in unnecessary irradiation of normal organs. Towards a fully automated radiation therapy planning algorithm, we explore the efficacy of multi-scale fusion based deep learning architectures for accurately segmenting H\&N tumors from medical scans.



