# Arxiv Papers in cs.CV on 2022-10-08
### Revisiting Self-Supervised Contrastive Learning for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.03853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03853v1)
- **Published**: 2022-10-08 00:04:27+00:00
- **Updated**: 2022-10-08 00:04:27+00:00
- **Authors**: Yuxuan Shu, Xiao Gu, Guang-Zhong Yang, Benny Lo
- **Comment**: Accepted to BMVC 2022
- **Journal**: None
- **Summary**: The success of most advanced facial expression recognition works relies heavily on large-scale annotated datasets. However, it poses great challenges in acquiring clean and consistent annotations for facial expression datasets. On the other hand, self-supervised contrastive learning has gained great popularity due to its simple yet effective instance discrimination training strategy, which can potentially circumvent the annotation issue. Nevertheless, there remain inherent disadvantages of instance-level discrimination, which are even more challenging when faced with complicated facial representations. In this paper, we revisit the use of self-supervised contrastive learning and explore three core strategies to enforce expression-specific representations and to minimize the interference from other facial attributes, such as identity and face styling. Experimental results show that our proposed method outperforms the current state-of-the-art self-supervised learning methods, in terms of both categorical and dimensional facial expression recognition tasks.



### Towards Light Weight Object Detection System
- **Arxiv ID**: http://arxiv.org/abs/2210.03861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03861v1)
- **Published**: 2022-10-08 00:55:15+00:00
- **Updated**: 2022-10-08 00:55:15+00:00
- **Authors**: Dharma KC, Venkata Ravi Kiran Dayana, Meng-Lin Wu, Venkateswara Rao Cherukuri, Hau Hwang
- **Comment**: None
- **Journal**: None
- **Summary**: Transformers are a popular choice for classification tasks and as backbones for object detection tasks. However, their high latency brings challenges in their adaptation to lightweight object detection systems. We present an approximation of the self-attention layers used in the transformer architecture. This approximation reduces the latency of the classification system while incurring minimal loss in accuracy. We also present a method that uses a transformer encoder layer for multi-resolution feature fusion. This feature fusion improves the accuracy of the state-of-the-art lightweight object detection system without significantly increasing the number of parameters. Finally, we provide an abstraction for the transformer architecture called Generalized Transformer (gFormer) that can guide the design of novel transformer-like architectures.



### Improving Data-Efficient Fossil Segmentation via Model Editing
- **Arxiv ID**: http://arxiv.org/abs/2210.03879v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03879v2)
- **Published**: 2022-10-08 02:12:38+00:00
- **Updated**: 2023-04-10 03:43:57+00:00
- **Authors**: Indu Panigrahi, Ryan Manzuk, Adam Maloof, Ruth Fong
- **Comment**: CVPR 2023 Workshop on Learning with Limited Labelled Data for Image
  and Video Understanding
- **Journal**: None
- **Summary**: Most computer vision research focuses on datasets containing thousands of images of commonplace objects. However, many high-impact datasets, such as those in medicine and the geosciences, contain fine-grain objects that require domain-expert knowledge to recognize and are time-consuming to collect and annotate. As a result, these datasets contain few labeled images, and current machine vision models cannot train intensively on them. Originally introduced to correct large-language models, model-editing techniques in machine learning have been shown to improve model performance using only small amounts of data and additional training. Using a Mask R-CNN to segment ancient reef fossils in rock sample images, we present a two-part paradigm to improve fossil segmentation with few labeled images: we first identify model weaknesses using image perturbations and then mitigate those weaknesses using model editing.   Specifically, we apply domain-informed image perturbations to expose the Mask R-CNN's inability to distinguish between different classes of fossils and its inconsistency in segmenting fossils with different textures. To address these shortcomings, we extend an existing model-editing method for correcting systematic mistakes in image classification to image segmentation with no additional labeled data needed and show its effectiveness in decreasing confusion between different kinds of fossils. We also highlight the best settings for model editing in our situation: making a single edit using all relevant pixels in one image (vs. using multiple images, multiple edits, or fewer pixels). Though we focus on fossil segmentation, our approach may be useful in other similar fine-grain segmentation problems where data is limited.



### Rethinking the Detection Head Configuration for Traffic Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.03883v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.03883v1)
- **Published**: 2022-10-08 02:23:57+00:00
- **Updated**: 2022-10-08 02:23:57+00:00
- **Authors**: Yi Shi, Jiang Wu, Shixuan Zhao, Gangyao Gao, Tao Deng, Hongmei Yan
- **Comment**: 26 pages, 4 figures, 7 tables
- **Journal**: None
- **Summary**: Multi-scale detection plays an important role in object detection models. However, researchers usually feel blank on how to reasonably configure detection heads combining multi-scale features at different input resolutions. We find that there are different matching relationships between the object distribution and the detection head at different input resolutions. Based on the instructive findings, we propose a lightweight traffic object detection network based on matching between detection head and object distribution, termed as MHD-Net. It consists of three main parts. The first is the detection head and object distribution matching strategy, which guides the rational configuration of detection head, so as to leverage multi-scale features to effectively detect objects at vastly different scales. The second is the cross-scale detection head configuration guideline, which instructs to replace multiple detection heads with only two detection heads possessing of rich feature representations to achieve an excellent balance between detection accuracy, model parameters, FLOPs and detection speed. The third is the receptive field enlargement method, which combines the dilated convolution module with shallow features of backbone to further improve the detection accuracy at the cost of increasing model parameters very slightly. The proposed model achieves more competitive performance than other models on BDD100K dataset and our proposed ETFOD-v2 dataset. The code will be available.



### Meta-DMoE: Adapting to Domain Shift by Meta-Distillation from Mixture-of-Experts
- **Arxiv ID**: http://arxiv.org/abs/2210.03885v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.03885v2)
- **Published**: 2022-10-08 02:28:10+00:00
- **Updated**: 2023-01-11 22:35:58+00:00
- **Authors**: Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu, Jin Tang
- **Comment**: Accepted at NeurIPS2022
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of domain shift. Most existing methods perform training on multiple source domains using a single model, and the same trained model is used on all unseen target domains. Such solutions are sub-optimal as each target domain exhibits its own specialty, which is not adapted. Furthermore, expecting single-model training to learn extensive knowledge from multiple source domains is counterintuitive. The model is more biased toward learning only domain-invariant features and may result in negative knowledge transfer. In this work, we propose a novel framework for unsupervised test-time adaptation, which is formulated as a knowledge distillation process to address domain shift. Specifically, we incorporate Mixture-of-Experts (MoE) as teachers, where each expert is separately trained on different source domains to maximize their specialty. Given a test-time target domain, a small set of unlabeled data is sampled to query the knowledge from MoE. As the source domains are correlated to the target domains, a transformer-based aggregator then combines the domain knowledge by examining the interconnection among them. The output is treated as a supervision signal to adapt a student prediction network toward the target domain. We further employ meta-learning to enforce the aggregator to distill positive knowledge and the student network to achieve fast adaptation. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art and validates the effectiveness of each proposed component. Our code is available at https://github.com/n3il666/Meta-DMoE.



### ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints
- **Arxiv ID**: http://arxiv.org/abs/2210.03895v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2210.03895v1)
- **Published**: 2022-10-08 03:06:49+00:00
- **Updated**: 2022-10-08 03:06:49+00:00
- **Authors**: Yinpeng Dong, Shouwei Ruan, Hang Su, Caixin Kang, Xingxing Wei, Jun Zhu
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: Recent studies have demonstrated that visual recognition models lack robustness to distribution shift. However, current work mainly considers model robustness to 2D image transformations, leaving viewpoint changes in the 3D world less explored. In general, viewpoint changes are prevalent in various real-world applications (e.g., autonomous driving), making it imperative to evaluate viewpoint robustness. In this paper, we propose a novel method called ViewFool to find adversarial viewpoints that mislead visual recognition models. By encoding real-world objects as neural radiance fields (NeRF), ViewFool characterizes a distribution of diverse adversarial viewpoints under an entropic regularizer, which helps to handle the fluctuations of the real camera pose and mitigate the reality gap between the real objects and their neural representations. Experiments validate that the common image classifiers are extremely vulnerable to the generated adversarial viewpoints, which also exhibit high cross-model transferability. Based on ViewFool, we introduce ImageNet-V, a new out-of-distribution dataset for benchmarking viewpoint robustness of image classifiers. Evaluation results on 40 classifiers with diverse architectures, objective functions, and data augmentations reveal a significant drop in model performance when tested on ImageNet-V, which provides a possibility to leverage ViewFool as an effective data augmentation strategy to improve viewpoint robustness.



### Multi-Scale Wavelet Transformer for Face Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.03899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03899v1)
- **Published**: 2022-10-08 03:39:36+00:00
- **Updated**: 2022-10-08 03:39:36+00:00
- **Authors**: Jie Liu, Jingjing Wang, Peng Zhang, Chunmao Wang, Di Xie, Shiliang Pu
- **Comment**: The first two authors contributed equally to this work. Accepted to
  ACCV 2022 as oral presentation
- **Journal**: None
- **Summary**: Currently, many face forgery detection methods aggregate spatial and frequency features to enhance the generalization ability and gain promising performance under the cross-dataset scenario. However, these methods only leverage one level frequency information which limits their expressive ability. To overcome these limitations, we propose a multi-scale wavelet transformer framework for face forgery detection. Specifically, to take full advantage of the multi-scale and multi-frequency wavelet representation, we gradually aggregate the multi-scale wavelet representation at different stages of the backbone network. To better fuse the frequency feature with the spatial features, frequency-based spatial attention is designed to guide the spatial feature extractor to concentrate more on forgery traces. Meanwhile, cross-modality attention is proposed to fuse the frequency features with the spatial features. These two attention modules are calculated through a unified transformer block for efficiency. A wide variety of experiments demonstrate that the proposed method is efficient and effective for both within and cross datasets.



### LW-ISP: A Lightweight Model with ISP and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.03904v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.03904v1)
- **Published**: 2022-10-08 04:00:03+00:00
- **Updated**: 2022-10-08 04:00:03+00:00
- **Authors**: Hongyang Chen, Kaisheng Ma
- **Comment**: 16 PAGES, ACCEPTED AS A CONFERENCE PAPER AT: BMVC 2022
- **Journal**: None
- **Summary**: The deep learning (DL)-based methods of low-level tasks have many advantages over the traditional camera in terms of hardware prospects, error accumulation and imaging effects. Recently, the application of deep learning to replace the image signal processing (ISP) pipeline has appeared one after another; however, there is still a long way to go towards real landing. In this paper, we show the possibility of learning-based method to achieve real-time high-performance processing in the ISP pipeline. We propose LW-ISP, a novel architecture designed to implicitly learn the image mapping from RAW data to RGB image. Based on U-Net architecture, we propose the fine-grained attention module and a plug-and-play upsampling block suitable for low-level tasks. In particular, we design a heterogeneous distillation algorithm to distill the implicit features and reconstruction information of the clean image, so as to guide the learning of the student model. Our experiments demonstrate that LW-ISP has achieved a 0.38 dB improvement in PSNR compared to the previous best method, while the model parameters and calculation have been reduced by 23 times and 81 times. The inference efficiency has been accelerated by at least 15 times. Without bells and whistles, LW-ISP has achieved quite competitive results in ISP subtasks including image denoising and enhancement.



### A Higher Purpose: Measuring Electricity Access Using High-Resolution Daytime Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2210.03909v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.03909v1)
- **Published**: 2022-10-08 04:18:58+00:00
- **Updated**: 2022-10-08 04:18:58+00:00
- **Authors**: Zeal Shah, Simone Fobi, Gabriel Cadamuro, Jay Taneja
- **Comment**: None
- **Journal**: None
- **Summary**: Governments and international organizations the world over are investing towards the goal of achieving universal energy access for improving socio-economic development. However, in developing settings, monitoring electrification efforts is typically inaccurate, infrequent, and expensive. In this work, we develop and present techniques for high-resolution monitoring of electrification progress at scale. Specifically, our 3 unique contributions are: (i) identifying areas with(out) electricity access, (ii) quantifying the extent of electrification in electrified areas (percentage/number of electrified structures), and (iii) differentiating between customer types in electrified regions (estimating the percentage/number of residential/non-residential electrified structures). We combine high-resolution 50 cm daytime satellite images with Convolutional Neural Networks (CNNs) to train a series of classification and regression models. We evaluate our models using unique ground truth datasets on building locations, building types (residential/non-residential), and building electrification status. Our classification models show a 92% accuracy in identifying electrified regions, 85% accuracy in estimating percent of (low/high) electrified buildings within the region, and 69% accuracy in differentiating between (low/high) percentage of electrified residential buildings. Our regressions show $R^2$ scores of 78% and 80% in estimating the number of electrified buildings and number of residential electrified building in images respectively. We also demonstrate the generalizability of our models in never-before-seen regions to assess their potential for consistent and high-resolution measurements of electrification in emerging economies, and conclude by highlighting opportunities for improvement.



### CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Face Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2210.03919v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.03919v4)
- **Published**: 2022-10-08 05:12:25+00:00
- **Updated**: 2023-05-07 20:26:50+00:00
- **Authors**: Chenliang Zhou, Fangcheng Zhong, Cengiz Oztireli
- **Comment**: None
- **Journal**: None
- **Summary**: Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridges images and text by embedding them into a joint latent space. This opens the door to ample literature that aims to manipulate an input image by providing a textual explanation. However, due to the discrepancy between image and text embeddings in the joint space, using text embeddings as the optimization target often introduces undesired artifacts in the resulting images. Disentanglement, interpretability, and controllability are also hard to guarantee for manipulation. To alleviate these problems, we propose to define corpus subspaces spanned by relevant prompts to capture specific image characteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) as an optimization target to improve the performance of text-guided image manipulation. Our method is a simple and general paradigm that can be easily computed and adapted, and smoothly incorporated into any CLIP-based image manipulation algorithm. To demonstrate the effectiveness of our method, we conduct several theoretical and empirical studies. As a case study, we utilize the method for text-guided semantic face editing. We quantitatively and qualitatively demonstrate that PAE facilitates a more disentangled, interpretable, and controllable image manipulation with state-of-the-art quality and accuracy.



### Contextual Modeling for 3D Dense Captioning on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2210.03925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03925v1)
- **Published**: 2022-10-08 05:33:00+00:00
- **Updated**: 2022-10-08 05:33:00+00:00
- **Authors**: Yufeng Zhong, Long Xu, Jiebo Luo, Lin Ma
- **Comment**: None
- **Journal**: None
- **Summary**: 3D dense captioning, as an emerging vision-language task, aims to identify and locate each object from a set of point clouds and generate a distinctive natural language sentence for describing each located object. However, the existing methods mainly focus on mining inter-object relationship, while ignoring contextual information, especially the non-object details and background environment within the point clouds, thus leading to low-quality descriptions, such as inaccurate relative position information. In this paper, we make the first attempt to utilize the point clouds clustering features as the contextual information to supply the non-object details and background environment of the point clouds and incorporate them into the 3D dense captioning task. We propose two separate modules, namely the Global Context Modeling (GCM) and Local Context Modeling (LCM), in a coarse-to-fine manner to perform the contextual modeling of the point clouds. Specifically, the GCM module captures the inter-object relationship among all objects with global contextual information to obtain more complete scene information of the whole point clouds. The LCM module exploits the influence of the neighboring objects of the target object and local contextual information to enrich the object representations. With such global and local contextual modeling strategies, our proposed model can effectively characterize the object representations and contextual information and thereby generate comprehensive and detailed descriptions of the located objects. Extensive experiments on the ScanRefer and Nr3D datasets demonstrate that our proposed method sets a new record on the 3D dense captioning task, and verify the effectiveness of our raised contextual modeling of point clouds.



### EgoTaskQA: Understanding Human Tasks in Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/2210.03929v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.03929v1)
- **Published**: 2022-10-08 05:49:05+00:00
- **Updated**: 2022-10-08 05:49:05+00:00
- **Authors**: Baoxiong Jia, Ting Lei, Song-Chun Zhu, Siyuan Huang
- **Comment**: Published at NeurIPS Track on Datasets and Benchmarks 2022
- **Journal**: None
- **Summary**: Understanding human tasks through video observations is an essential capability of intelligent agents. The challenges of such capability lie in the difficulty of generating a detailed understanding of situated actions, their effects on object states (i.e., state changes), and their causal dependencies. These challenges are further aggravated by the natural parallelism from multi-tasking and partial observations in multi-agent collaboration. Most prior works leverage action localization or future prediction as an indirect metric for evaluating such task understanding from videos. To make a direct evaluation, we introduce the EgoTaskQA benchmark that provides a single home for the crucial dimensions of task understanding through question-answering on real-world egocentric videos. We meticulously design questions that target the understanding of (1) action dependencies and effects, (2) intents and goals, and (3) agents' beliefs about others. These questions are divided into four types, including descriptive (what status?), predictive (what will?), explanatory (what caused?), and counterfactual (what if?) to provide diagnostic analyses on spatial, temporal, and causal understandings of goal-oriented tasks. We evaluate state-of-the-art video reasoning models on our benchmark and show their significant gaps between humans in understanding complex goal-oriented egocentric videos. We hope this effort will drive the vision community to move onward with goal-oriented video understanding and reasoning.



### Cloud Native Robotic Applications with GPU Sharing on Kubernetes
- **Arxiv ID**: http://arxiv.org/abs/2210.03936v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.DC, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/2210.03936v2)
- **Published**: 2022-10-08 06:41:14+00:00
- **Updated**: 2022-10-31 09:52:23+00:00
- **Authors**: Giovanni Toffetti, Leonardo Militano, Seán Murphy, Remo Maurer, Mark Straub
- **Comment**: Submission accepted at the IROS'22 Cloud Robotics Workshop
- **Journal**: None
- **Summary**: In this paper we discuss our experience in teaching the Robotic Applications Programming course at ZHAW combining the use of a Kubernetes (k8s) cluster and real, heterogeneous, robotic hardware. We discuss the main advantages of our solutions in terms of seamless simulation-to-real experience for students and the main shortcomings we encountered with networking and sharing GPUs to support deep learning workloads. We describe the current and foreseen alternatives to avoid these drawbacks in future course editions and propose a more cloud-native approach to deploying multiple robotics applications on a k8s cluster.



### Hierarchical Few-Shot Object Detection: Problem, Benchmark and Method
- **Arxiv ID**: http://arxiv.org/abs/2210.03940v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03940v1)
- **Published**: 2022-10-08 07:02:44+00:00
- **Updated**: 2022-10-08 07:02:44+00:00
- **Authors**: Lu Zhang, Yang Wang, Jiaogen Zhou, Chenbo Zhang, Yinglu Zhang, Jihong Guan, Yatao Bian, Shuigeng Zhou
- **Comment**: Accepted by ACM MM 2022
- **Journal**: None
- **Summary**: Few-shot object detection (FSOD) is to detect objects with a few examples. However, existing FSOD methods do not consider hierarchical fine-grained category structures of objects that exist widely in real life. For example, animals are taxonomically classified into orders, families, genera and species etc. In this paper, we propose and solve a new problem called hierarchical few-shot object detection (Hi-FSOD), which aims to detect objects with hierarchical categories in the FSOD paradigm. To this end, on the one hand, we build the first large-scale and high-quality Hi-FSOD benchmark dataset HiFSOD-Bird, which contains 176,350 wild-bird images falling to 1,432 categories. All the categories are organized into a 4-level taxonomy, consisting of 32 orders, 132 families, 572 genera and 1,432 species. On the other hand, we propose the first Hi-FSOD method HiCLPL, where a hierarchical contrastive learning approach is developed to constrain the feature space so that the feature distribution of objects is consistent with the hierarchical taxonomy and the model's generalization power is strengthened. Meanwhile, a probabilistic loss is designed to enable the child nodes to correct the classification errors of their parent nodes in the taxonomy. Extensive experiments on the benchmark dataset HiFSOD-Bird show that our method HiCLPL outperforms the existing FSOD methods.



### Learning Fine-Grained Visual Understanding for Video Question Answering via Decoupling Spatial-Temporal Modeling
- **Arxiv ID**: http://arxiv.org/abs/2210.03941v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2210.03941v1)
- **Published**: 2022-10-08 07:03:31+00:00
- **Updated**: 2022-10-08 07:03:31+00:00
- **Authors**: Hsin-Ying Lee, Hung-Ting Su, Bing-Chen Tsai, Tsung-Han Wu, Jia-Fong Yeh, Winston H. Hsu
- **Comment**: BMVC 2022. Code is available at https://github.com/shinying/dest
- **Journal**: None
- **Summary**: While recent large-scale video-language pre-training made great progress in video question answering, the design of spatial modeling of video-language models is less fine-grained than that of image-language models; existing practices of temporal modeling also suffer from weak and noisy alignment between modalities. To learn fine-grained visual understanding, we decouple spatial-temporal modeling and propose a hybrid pipeline, Decoupled Spatial-Temporal Encoders, integrating an image- and a video-language encoder. The former encodes spatial semantics from larger but sparsely sampled frames independently of time, while the latter models temporal dynamics at lower spatial but higher temporal resolution. To help the video-language model learn temporal relations for video QA, we propose a novel pre-training objective, Temporal Referring Modeling, which requires the model to identify temporal positions of events in video sequences. Extensive experiments demonstrate that our model outperforms previous work pre-trained on orders of magnitude larger datasets.



### Point Cloud Upsampling via Cascaded Refinement Network
- **Arxiv ID**: http://arxiv.org/abs/2210.03942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03942v1)
- **Published**: 2022-10-08 07:09:37+00:00
- **Updated**: 2022-10-08 07:09:37+00:00
- **Authors**: Hang Du, Xuejun Yan, Jingjing Wang, Di Xie, Shiliang Pu
- **Comment**: The first two authors contributed equally to this work. The code is
  publicly available at https://github.com/hikvision-research/3DVision.
  Accepted to ACCV 2022 as oral presentation
- **Journal**: None
- **Summary**: Point cloud upsampling focuses on generating a dense, uniform and proximity-to-surface point set. Most previous approaches accomplish these objectives by carefully designing a single-stage network, which makes it still challenging to generate a high-fidelity point distribution. Instead, upsampling point cloud in a coarse-to-fine manner is a decent solution. However, existing coarse-to-fine upsampling methods require extra training strategies, which are complicated and time-consuming during the training. In this paper, we propose a simple yet effective cascaded refinement network, consisting of three generation stages that have the same network architecture but achieve different objectives. Specifically, the first two upsampling stages generate the dense but coarse points progressively, while the last refinement stage further adjust the coarse points to a better position. To mitigate the learning conflicts between multiple stages and decrease the difficulty of regressing new points, we encourage each stage to predict the point offsets with respect to the input shape. In this manner, the proposed cascaded refinement network can be easily optimized without extra learning strategies. Moreover, we design a transformer-based feature extraction module to learn the informative global and local shape context. In inference phase, we can dynamically adjust the model efficiency and effectiveness, depending on the available computational resources. Extensive experiments on both synthetic and real-scanned datasets demonstrate that the proposed approach outperforms the existing state-of-the-art methods.



### ArabSign: A Multi-modality Dataset and Benchmark for Continuous Arabic Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.03951v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03951v1)
- **Published**: 2022-10-08 07:36:20+00:00
- **Updated**: 2022-10-08 07:36:20+00:00
- **Authors**: Hamzah Luqman
- **Comment**: 8
- **Journal**: None
- **Summary**: Sign language recognition has attracted the interest of researchers in recent years. While numerous approaches have been proposed for European and Asian sign languages recognition, very limited attempts have been made to develop similar systems for the Arabic sign language (ArSL). This can be attributed partly to the lack of a dataset at the sentence level. In this paper, we aim to make a significant contribution by proposing ArabSign, a continuous ArSL dataset. The proposed dataset consists of 9,335 samples performed by 6 signers. The total time of the recorded sentences is around 10 hours and the average sentence's length is 3.1 signs. ArabSign dataset was recorded using a Kinect V2 camera that provides three types of information (color, depth, and skeleton joint points) recorded simultaneously for each sentence. In addition, we provide the annotation of the dataset according to ArSL and Arabic language structures that can help in studying the linguistic characteristics of ArSL. To benchmark this dataset, we propose an encoder-decoder model for Continuous ArSL recognition. The model has been evaluated on the proposed dataset, and the obtained results show that the encoder-decoder model outperformed the attention mechanism with an average word error rate (WER) of 0.50 compared with 0.62 with the attention mechanism. The data and code are available at github.com/Hamzah-Luqman/ArabSign



### Detaching and Boosting: Dual Engine for Scale-Invariant Self-Supervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2210.03952v2
- **DOI**: 10.1109/LRA.2022.3210877
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03952v2)
- **Published**: 2022-10-08 07:38:11+00:00
- **Updated**: 2023-02-20 08:11:34+00:00
- **Authors**: Peizhe Jiang, Wei Yang, Xiaoqing Ye, Xiao Tan, Meng Wu
- **Comment**: Accepted by ICLR and IEEE Robotics and Automation Letters (RAL)
- **Journal**: IEEE Robotics and Automation Letters 7.4 (2022): 12094-12101
- **Summary**: Monocular depth estimation (MDE) in the self-supervised scenario has emerged as a promising method as it refrains from the requirement of ground truth depth. Despite continuous efforts, MDE is still sensitive to scale changes especially when all the training samples are from one single camera. Meanwhile, it deteriorates further since camera movement results in heavy coupling between the predicted depth and the scale change. In this paper, we present a scale-invariant approach for self-supervised MDE, in which scale-sensitive features (SSFs) are detached away while scale-invariant features (SIFs) are boosted further. To be specific, a simple but effective data augmentation by imitating the camera zooming process is proposed to detach SSFs, making the model robust to scale changes. Besides, a dynamic cross-attention module is designed to boost SIFs by fusing multi-scale cross-attention features adaptively. Extensive experiments on the KITTI dataset demonstrate that the detaching and boosting strategies are mutually complementary in MDE and our approach achieves new State-of-The-Art performance against existing works from 0.097 to 0.090 w.r.t absolute relative error. The code will be made public soon.



### Contact-aware Human Motion Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2210.03954v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03954v1)
- **Published**: 2022-10-08 07:53:19+00:00
- **Updated**: 2022-10-08 07:53:19+00:00
- **Authors**: Wei Mao, Miaomiao Liu, Richard Hartley, Mathieu Salzmann
- **Comment**: Accepted to NeurIPS2022
- **Journal**: None
- **Summary**: In this paper, we tackle the task of scene-aware 3D human motion forecasting, which consists of predicting future human poses given a 3D scene and a past human motion. A key challenge of this task is to ensure consistency between the human and the scene, accounting for human-scene interactions. Previous attempts to do so model such interactions only implicitly, and thus tend to produce artifacts such as "ghost motion" because of the lack of explicit constraints between the local poses and the global motion. Here, by contrast, we propose to explicitly model the human-scene contacts. To this end, we introduce distance-based contact maps that capture the contact relationships between every joint and every 3D scene point at each time instant. We then develop a two-stage pipeline that first predicts the future contact maps from the past ones and the scene point cloud, and then forecasts the future human poses by conditioning them on the predicted contact maps. During training, we explicitly encourage consistency between the global motion and the local poses via a prior defined using the contact maps and future poses. Our approach outperforms the state-of-the-art human motion forecasting and human synthesis methods on both synthetic and real datasets. Our code is available at https://github.com/wei-mao-2019/ContAwareMotionPred.



### Robust Graph Structure Learning via Multiple Statistical Tests
- **Arxiv ID**: http://arxiv.org/abs/2210.03956v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03956v4)
- **Published**: 2022-10-08 07:56:13+00:00
- **Updated**: 2022-12-23 06:24:59+00:00
- **Authors**: Yaohua Wang, FangYi Zhang, Ming Lin, Senzhang Wang, Xiuyu Sun, Rong Jin
- **Comment**: Accepted by the NeurIPS 2022. Homepage: https://thomas-wyh.github.io/
- **Journal**: None
- **Summary**: Graph structure learning aims to learn connectivity in a graph from data. It is particularly important for many computer vision related tasks since no explicit graph structure is available for images for most cases. A natural way to construct a graph among images is to treat each image as a node and assign pairwise image similarities as weights to corresponding edges. It is well known that pairwise similarities between images are sensitive to the noise in feature representations, leading to unreliable graph structures. We address this problem from the viewpoint of statistical tests. By viewing the feature vector of each node as an independent sample, the decision of whether creating an edge between two nodes based on their similarity in feature representation can be thought as a ${\it single}$ statistical test. To improve the robustness in the decision of creating an edge, multiple samples are drawn and integrated by ${\it multiple}$ statistical tests to generate a more reliable similarity measure, consequentially more reliable graph structure. The corresponding elegant matrix form named $\mathcal{B}\textbf{-Attention}$ is designed for efficiency. The effectiveness of multiple tests for graph structure learning is verified both theoretically and empirically on multiple clustering and ReID benchmark datasets. Source codes are available at https://github.com/Thomas-wyh/B-Attention.



### FBNet: Feedback Network for Point Cloud Completion
- **Arxiv ID**: http://arxiv.org/abs/2210.03974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03974v1)
- **Published**: 2022-10-08 09:12:37+00:00
- **Updated**: 2022-10-08 09:12:37+00:00
- **Authors**: Xuejun Yan, Hongyu Yan, Jingjing Wang, Hang Du, Zhihong Wu, Di Xie, Shiliang Pu, Li Lu
- **Comment**: The first two authors contributed equally to this work. The source
  code and model are available at
  https://github.com/hikvision-research/3DVision/. Accepted to ECCV 2022 as
  oral presentation
- **Journal**: None
- **Summary**: The rapid development of point cloud learning has driven point cloud completion into a new era. However, the information flows of most existing completion methods are solely feedforward, and high-level information is rarely reused to improve low-level feature learning. To this end, we propose a novel Feedback Network (FBNet) for point cloud completion, in which present features are efficiently refined by rerouting subsequent fine-grained ones. Firstly, partial inputs are fed to a Hierarchical Graph-based Network (HGNet) to generate coarse shapes. Then, we cascade several Feedback-Aware Completion (FBAC) Blocks and unfold them across time recurrently. Feedback connections between two adjacent time steps exploit fine-grained features to improve present shape generations. The main challenge of building feedback connections is the dimension mismatching between present and subsequent features. To address this, the elaborately designed point Cross Transformer exploits efficient information from feedback features via cross attention strategy and then refines present features with the enhanced feedback features. Quantitative and qualitative experiments on several datasets demonstrate the superiority of proposed FBNet compared to state-of-the-art methods on point completion task.



### (Fusionformer):Exploiting the Joint Motion Synergy with Fusion Network Based On Transformer for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2210.04006v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.04006v2)
- **Published**: 2022-10-08 12:22:10+00:00
- **Updated**: 2022-10-30 14:07:28+00:00
- **Authors**: Xinwei Yu, Xiaohua Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: For the current 3D human pose estimation task, a group of methods mainly learn the rules of 2D-3D projection from spatial and temporal correlation. However, earlier methods model the global features of the entire body joint in the time domain, but ignore the motion trajectory of individual joint. The recent work [29] considers that there are differences in motion between different joints and deals with the temporal relationship of each joint separately. However, we found that different joints show the same movement trends under some specific actions. Therefore, our proposed Fusionformer method introduces a self-trajectory module and a mutual-trajectory module based on the spatio-temporal module .After that, the global spatio-temporal features and local joint trajectory features are fused through a linear network in a parallel manner. To eliminate the influence of bad 2D poses on 3D projections, finally we also introduce a pose refinement network to balance the consistency of 3D projections. In addition, we evaluate the proposed method on two benchmark datasets (Human3.6M, MPI-INF-3DHP). Comparing our method with the baseline method poseformer, the results show an improvement of 2.4% MPJPE and 4.3% P-MPJPE on the Human3.6M dataset, respectively.



### AdaptivePose++: A Powerful Single-Stage Network for Multi-Person Pose Regression
- **Arxiv ID**: http://arxiv.org/abs/2210.04014v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.04014v1)
- **Published**: 2022-10-08 12:54:20+00:00
- **Updated**: 2022-10-08 12:54:20+00:00
- **Authors**: Yabo Xiao, Xiaojuan Wang, Dongdong Yu, Kai Su, Lei Jin, Mei Song, Shuicheng Yan, Jian Zhao
- **Comment**: Submit to IEEE TCSVT; 11 pages. arXiv admin note: text overlap with
  arXiv:2112.13635
- **Journal**: None
- **Summary**: Multi-person pose estimation generally follows top-down and bottom-up paradigms. Both of them use an extra stage ($\boldsymbol{e.g.,}$ human detection in top-down paradigm or grouping process in bottom-up paradigm) to build the relationship between the human instance and corresponding keypoints, thus leading to the high computation cost and redundant two-stage pipeline. To address the above issue, we propose to represent the human parts as adaptive points and introduce a fine-grained body representation method. The novel body representation is able to sufficiently encode the diverse pose information and effectively model the relationship between the human instance and corresponding keypoints in a single-forward pass. With the proposed body representation, we further deliver a compact single-stage multi-person pose regression network, termed as AdaptivePose. During inference, our proposed network only needs a single-step decode operation to form the multi-person pose without complex post-processes and refinements. We employ AdaptivePose for both 2D/3D multi-person pose estimation tasks to verify the effectiveness of AdaptivePose. Without any bells and whistles, we achieve the most competitive performance on MS COCO and CrowdPose in terms of accuracy and speed. Furthermore, the outstanding performance on MuCo-3DHP and MuPoTS-3D further demonstrates the effectiveness and generalizability on 3D scenes. Code is available at https://github.com/buptxyb666/AdaptivePose.



### Fast-ParC: Position Aware Global Kernel for ConvNets and ViTs
- **Arxiv ID**: http://arxiv.org/abs/2210.04020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.04020v1)
- **Published**: 2022-10-08 13:14:02+00:00
- **Updated**: 2022-10-08 13:14:02+00:00
- **Authors**: Tao Yang, Haokui Zhang, Wenze Hu, Changwen Chen, Xiaoyu Wang
- **Comment**: 19 pages, 8 figures, 11 tables. A preliminary version of this paper
  has been published in ECCV 2022 and it can be find in arXiv:2203.03952
- **Journal**: None
- **Summary**: Transformer models have made tremendous progress in various fields in recent years. In the field of computer vision, vision transformers (ViTs) also become strong alternatives to convolutional neural networks (ConvNets), yet they have not been able to replace ConvNets since both have their own merits. For instance, ViTs are good at extracting global features with attention mechanisms while ConvNets are more efficient in modeling local relationships due to their strong inductive bias. A natural idea that arises is to combine the strengths of both ConvNets and ViTs to design new structures. In this paper, we propose a new basic neural network operator named position-aware circular convolution (ParC) and its accelerated version Fast-ParC. The ParC operator can capture global features by using a global kernel and circular convolution while keeping location sensitiveness by employing position embeddings. Our Fast-ParC further reduces the O(n2) time complexity of ParC to O(n log n) using Fast Fourier Transform. This acceleration makes it possible to use global convolution in the early stages of models with large feature maps, yet still maintains the overall computational cost comparable with using 3x3 or 7x7 kernels. The proposed operation can be used in a plug-and-play manner to 1) convert ViTs to pure-ConvNet architecture to enjoy wider hardware support and achieve higher inference speed; 2) replacing traditional convolutions in the deep stage of ConvNets to improve accuracy by enlarging the effective receptive field. Experiment results show that our ParC op can effectively enlarge the receptive field of traditional ConvNets, and adopting the proposed op benefits both ViTs and ConvNet models on all three popular vision tasks, image classification, object



### Enhancing Generalizable 6D Pose Tracking of an In-Hand Object with Tactile Sensing
- **Arxiv ID**: http://arxiv.org/abs/2210.04026v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.04026v1)
- **Published**: 2022-10-08 13:47:03+00:00
- **Updated**: 2022-10-08 13:47:03+00:00
- **Authors**: Xiaomeng Xu, Yun Liu, Weihang Chen, Haocheng Yuan, He Wang, Jing Xu, Rui Chen, Li Yi
- **Comment**: None
- **Journal**: None
- **Summary**: While holding and manipulating an object, humans track the object states through vision and touch so as to achieve complex tasks. However, nowadays the majority of robot research perceives object states just from visual signals, hugely limiting the robotic manipulation abilities. This work presents a tactile-enhanced generalizable 6D pose tracking design named TEG-Track to track previously unseen in-hand objects. TEG-Track extracts tactile kinematic cues of an in-hand object from consecutive tactile sensing signals. Such cues are incorporated into a geometric-kinematic optimization scheme to enhance existing generalizable visual trackers. To test our method in real scenarios and enable future studies on generalizable visual-tactile tracking, we collect a real visual-tactile in-hand object pose tracking dataset. Experiments show that TEG-Track significantly improves state-of-the-art generalizable 6D pose trackers in both synthetic and real cases.



### Multi-Modal Human Authentication Using Silhouettes, Gait and RGB
- **Arxiv ID**: http://arxiv.org/abs/2210.04050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.04050v1)
- **Published**: 2022-10-08 15:17:32+00:00
- **Updated**: 2022-10-08 15:17:32+00:00
- **Authors**: Yuxiang Guo, Cheng Peng, Chun Pong Lau, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: Whole-body-based human authentication is a promising approach for remote biometrics scenarios. Current literature focuses on either body recognition based on RGB images or gait recognition based on body shapes and walking patterns; both have their advantages and drawbacks. In this work, we propose Dual-Modal Ensemble (DME), which combines both RGB and silhouette data to achieve more robust performances for indoor and outdoor whole-body based recognition. Within DME, we propose GaitPattern, which is inspired by the double helical gait pattern used in traditional gait analysis. The GaitPattern contributes to robust identification performance over a large range of viewing angles. Extensive experimental results on the CASIA-B dataset demonstrate that the proposed method outperforms state-of-the-art recognition systems. We also provide experimental results using the newly collected BRIAR dataset.



### A deep learning network with differentiable dynamic programming for retina OCT surface segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.06335v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.06335v1)
- **Published**: 2022-10-08 16:26:09+00:00
- **Updated**: 2022-10-08 16:26:09+00:00
- **Authors**: Hui Xie, Weiyu Xu, Xiaodong Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple-surface segmentation in Optical Coherence Tomography (OCT) images is a challenge problem, further complicated by the frequent presence of weak image boundaries. Recently, many deep learning (DL) based methods have been developed for this task and yield remarkable performance. Unfortunately, due to the scarcity of training data in medical imaging, it is challenging for DL networks to learn the global structure of the target surfaces, including surface smoothness. To bridge this gap, this study proposes to seamlessly unify a U-Net for feature learning with a constrained differentiable dynamic programming module to achieve an end-to-end learning for retina OCT surface segmentation to explicitly enforce surface smoothness. It effectively utilizes the feedback from the downstream model optimization module to guide feature learning, yielding a better enforcement of global structures of the target surfaces. Experiments on Duke AMD (age-related macular degeneration) and JHU MS (multiple sclerosis) OCT datasets for retinal layer segmentation demonstrated very promising segmentation accuracy.



### Flow-based GAN for 3D Point Cloud Generation from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/2210.04072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.04072v1)
- **Published**: 2022-10-08 17:58:20+00:00
- **Updated**: 2022-10-08 17:58:20+00:00
- **Authors**: Yao Wei, George Vosselman, Michael Ying Yang
- **Comment**: 13 pages, 5 figures, accepted to BMVC2022
- **Journal**: None
- **Summary**: Generating a 3D point cloud from a single 2D image is of great importance for 3D scene understanding applications. To reconstruct the whole 3D shape of the object shown in the image, the existing deep learning based approaches use either explicit or implicit generative modeling of point clouds, which, however, suffer from limited quality. In this work, we aim to alleviate this issue by introducing a hybrid explicit-implicit generative modeling scheme, which inherits the flow-based explicit generative models for sampling point clouds with arbitrary resolutions while improving the detailed 3D structures of point clouds by leveraging the implicit generative adversarial networks (GANs). We evaluate on the large-scale synthetic dataset ShapeNet, with the experimental results demonstrating the superior performance of the proposed method. In addition, the generalization ability of our method is demonstrated by performing on cross-category synthetic images as well as by testing on real images from PASCAL3D+ dataset.



### Dual Pyramid Generative Adversarial Networks for Semantic Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2210.04085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.04085v1)
- **Published**: 2022-10-08 18:45:44+00:00
- **Updated**: 2022-10-08 18:45:44+00:00
- **Authors**: Shijie Li, Ming-Ming Cheng, Juergen Gall
- **Comment**: BMVC2022
- **Journal**: None
- **Summary**: The goal of semantic image synthesis is to generate photo-realistic images from semantic label maps. It is highly relevant for tasks like content generation and image editing. Current state-of-the-art approaches, however, still struggle to generate realistic objects in images at various scales. In particular, small objects tend to fade away and large objects are often generated as collages of patches. In order to address this issue, we propose a Dual Pyramid Generative Adversarial Network (DP-GAN) that learns the conditioning of spatially-adaptive normalization blocks at all scales jointly, such that scale information is bi-directionally used, and it unifies supervision at different scales. Our qualitative and quantitative results show that the proposed approach generates images where small and large objects look more realistic compared to images generated by state-of-the-art methods.



### Symmetry Defense Against CNN Adversarial Perturbation Attacks
- **Arxiv ID**: http://arxiv.org/abs/2210.04087v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.04087v3)
- **Published**: 2022-10-08 18:49:58+00:00
- **Updated**: 2023-08-10 12:42:06+00:00
- **Authors**: Blerta Lindqvist
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: This paper uses symmetry to make Convolutional Neural Network classifiers (CNNs) robust against adversarial perturbation attacks. Such attacks add perturbation to original images to generate adversarial images that fool classifiers such as road sign classifiers of autonomous vehicles. Although symmetry is a pervasive aspect of the natural world, CNNs are unable to handle symmetry well. For example, a CNN can classify an image differently from its mirror image. For an adversarial image that misclassifies with a wrong label $l_w$, CNN inability to handle symmetry means that a symmetric adversarial image can classify differently from the wrong label $l_w$. Further than that, we find that the classification of a symmetric adversarial image reverts to the correct label. To classify an image when adversaries are unaware of the defense, we apply symmetry to the image and use the classification label of the symmetric image. To classify an image when adversaries are aware of the defense, we use mirror symmetry and pixel inversion symmetry to form a symmetry group. We apply all the group symmetries to the image and decide on the output label based on the agreement of any two of the classification labels of the symmetry images. Adaptive attacks fail because they need to rely on loss functions that use conflicting CNN output values for symmetric images. Without attack knowledge, the proposed symmetry defense succeeds against both gradient-based and random-search attacks, with up to near-default accuracies for ImageNet. The defense even improves the classification accuracy of original images.



### Training Deep Learning Algorithms on Synthetic Forest Images for Tree Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.04104v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.04104v1)
- **Published**: 2022-10-08 20:49:40+00:00
- **Updated**: 2022-10-08 20:49:40+00:00
- **Authors**: Vincent Grondin, François Pomerleau, Philippe Giguère
- **Comment**: Work presented at ICRA 2022 Workshop in Innovation in Forestry
  Robotics: Research and Industry Adoption
- **Journal**: None
- **Summary**: Vision-based segmentation in forested environments is a key functionality for autonomous forestry operations such as tree felling and forwarding. Deep learning algorithms demonstrate promising results to perform visual tasks such as object detection. However, the supervised learning process of these algorithms requires annotations from a large diversity of images. In this work, we propose to use simulated forest environments to automatically generate 43 k realistic synthetic images with pixel-level annotations, and use it to train deep learning algorithms for tree detection. This allows us to address the following questions: i) what kind of performance should we expect from deep learning in harsh synthetic forest environments, ii) which annotations are the most important for training, and iii) what modality should be used between RGB and depth. We also report the promising transfer learning capability of features learned on our synthetic dataset by directly predicting bounding box, segmentation masks and keypoints on real images. Code available on GitHub (https://github.com/norlab-ulaval/PercepTreeV1).



### The effect of variable labels on deep learning models trained to predict breast density
- **Arxiv ID**: http://arxiv.org/abs/2210.04106v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.04106v1)
- **Published**: 2022-10-08 21:18:05+00:00
- **Updated**: 2022-10-08 21:18:05+00:00
- **Authors**: Steven Squires, Elaine F. Harkness, D. Gareth Evans, Susan M. Astley
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: High breast density is associated with reduced efficacy of mammographic screening and increased risk of developing breast cancer. Accurate and reliable automated density estimates can be used for direct risk prediction and passing density related information to further predictive models. Expert reader assessments of density show a strong relationship to cancer risk but also inter-reader variation. The effect of label variability on model performance is important when considering how to utilise automated methods for both research and clinical purposes. Methods: We utilise subsets of images with density labels to train a deep transfer learning model which is used to assess how label variability affects the mapping from representation to prediction. We then create two end-to-end deep learning models which allow us to investigate the effect of label variability on the model representation formed. Results: We show that the trained mappings from representations to labels are altered considerably by the variability of reader scores. Training on labels with distribution variation removed causes the Spearman rank correlation coefficients to rise from $0.751\pm0.002$ to either $0.815\pm0.006$ when averaging across readers or $0.844\pm0.002$ when averaging across images. However, when we train different models to investigate the representation effect we see little difference, with Spearman rank correlation coefficients of $0.846\pm0.006$ and $0.850\pm0.006$ showing no statistically significant difference in the quality of the model representation with regard to density prediction. Conclusions: We show that the mapping between representation and mammographic density prediction is significantly affected by label variability. However, the effect of the label variability on the model representation is limited.



### Visual Looming from Motion Field and Surface Normals
- **Arxiv ID**: http://arxiv.org/abs/2210.04108v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.04108v2)
- **Published**: 2022-10-08 21:36:49+00:00
- **Updated**: 2022-10-20 15:53:44+00:00
- **Authors**: Juan Yepes, Daniel Raviv
- **Comment**: Includes supplemental material
- **Journal**: None
- **Summary**: Looming, traditionally defined as the relative expansion of objects in the observer's retina, is a fundamental visual cue for perception of threat and can be used to accomplish collision free navigation. In this paper we derive novel solutions for obtaining visual looming quantitatively from the 2D motion field resulting from a six-degree-of-freedom motion of an observer relative to a local surface in 3D. We also show the relationship between visual looming and surface normals. We present novel methods to estimate visual looming from spatial derivatives of optical flow without the need for knowing range. Simulation results show that estimations of looming are very close to ground truth looming under some assumptions of surface orientations. In addition, we present results of visual looming using real data from the KITTI dataset. Advantages and limitations of the methods are discussed as well.



### Leveraging progressive model and overfitting for efficient learned image compression
- **Arxiv ID**: http://arxiv.org/abs/2210.04112v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.04112v1)
- **Published**: 2022-10-08 21:54:58+00:00
- **Updated**: 2022-10-08 21:54:58+00:00
- **Authors**: Honglei Zhang, Francesco Cricri, Hamed Rezazadegan Tavakoli, Emre Aksu, Miska M. Hannuksela
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning is overwhelmingly dominant in the field of computer vision and image/video processing for the last decade. However, for image and video compression, it lags behind the traditional techniques based on discrete cosine transform (DCT) and linear filters. Built on top of an autoencoder architecture, learned image compression (LIC) systems have drawn enormous attention in recent years. Nevertheless, the proposed LIC systems are still inferior to the state-of-the-art traditional techniques, for example, the Versatile Video Coding (VVC/H.266) standard, due to either their compression performance or decoding complexity. Although claimed to outperform the VVC/H.266 on a limited bit rate range, some proposed LIC systems take over 40 seconds to decode a 2K image on a GPU system. In this paper, we introduce a powerful and flexible LIC framework with multi-scale progressive (MSP) probability model and latent representation overfitting (LOF) technique. With different predefined profiles, the proposed framework can achieve various balance points between compression efficiency and computational complexity. Experiments show that the proposed framework achieves 2.5%, 1.0%, and 1.3% Bjontegaard delta bit rate (BD-rate) reduction over the VVC/H.266 standard on three benchmark datasets on a wide bit rate range. More importantly, the decoding complexity is reduced from O(n) to O(1) compared to many other LIC systems, resulting in over 20 times speedup when decoding 2K images.



### Sequential Ensembling for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.05387v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.05387v1)
- **Published**: 2022-10-08 22:13:59+00:00
- **Updated**: 2022-10-08 22:13:59+00:00
- **Authors**: Rawal Khirodkar, Brandon Smith, Siddhartha Chandra, Amit Agrawal, Antonio Criminisi
- **Comment**: None
- **Journal**: None
- **Summary**: Ensemble approaches for deep-learning-based semantic segmentation remain insufficiently explored despite the proliferation of competitive benchmarks and downstream applications. In this work, we explore and benchmark the popular ensembling approach of combining predictions of multiple, independently-trained, state-of-the-art models at test time on popular datasets. Furthermore, we propose a novel method inspired by boosting to sequentially ensemble networks that significantly outperforms the naive ensemble baseline. Our approach trains a cascade of models conditioned on class probabilities predicted by the previous model as an additional input. A key benefit of this approach is that it allows for dynamic computation offloading, which helps deploy models on mobile devices. Our proposed novel ADaptive modulatiON (ADON) block allows spatial feature modulation at various layers using previous-stage probabilities. Our approach does not require sophisticated sample selection strategies during training and works with multiple neural architectures. We significantly improve over the naive ensemble baseline on challenging datasets such as Cityscapes, ADE-20K, COCO-Stuff, and PASCAL-Context and set a new state-of-the-art.



### Convolutional Neural Network-Based Image Watermarking using Discrete Wavelet Transform
- **Arxiv ID**: http://arxiv.org/abs/2210.06179v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.06179v2)
- **Published**: 2022-10-08 22:54:46+00:00
- **Updated**: 2022-10-29 12:52:47+00:00
- **Authors**: Alireza Tavakoli, Zahra Honjani, Hedieh Sajedi
- **Comment**: None
- **Journal**: None
- **Summary**: With the growing popularity of the Internet, digital images are used and transferred more frequently. Although this phenomenon facilitates easy access to information, it also creates security concerns and violates intellectual property rights by allowing illegal use, copying, and digital content theft. Using watermarks in digital images is one of the most common ways to maintain security. Watermarking is proving and declaring ownership of an image by adding a digital watermark to the original image. Watermarks can be either text or an image placed overtly or covertly in an image and are expected to be challenging to remove. This paper proposes a combination of convolutional neural networks (CNNs) and wavelet transforms to obtain a watermarking network for embedding and extracting watermarks. The network is independent of the host image resolution, can accept all kinds of watermarks, and has only 11 layers while keeping performance. Performance is measured by two terms; the similarity between the extracted watermark and the original one and the similarity between the host image and the watermarked one.



### MultiStyleGAN: Multiple One-shot Image Stylizations using a Single GAN
- **Arxiv ID**: http://arxiv.org/abs/2210.04120v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.04120v2)
- **Published**: 2022-10-08 23:05:29+00:00
- **Updated**: 2023-04-20 23:48:10+00:00
- **Authors**: Viraj Shah, Ayush Sarkar, Sudharsan Krishnakumar Anitha, Svetlana Lazebnik
- **Comment**: Project webpage available at https://virajshah.com/multistyle
- **Journal**: None
- **Summary**: Image stylization aims at applying a reference style to arbitrary input images. A common scenario is one-shot stylization, where only one example is available for each reference style. Recent approaches for one-shot stylization such as JoJoGAN fine-tune a pre-trained StyleGAN2 generator on a single style reference image. However, such methods cannot generate multiple stylizations without fine-tuning a new model for each style separately. In this work, we present a MultiStyleGAN method that is capable of producing multiple different stylizations at once by fine-tuning a single generator. The key component of our method is a learnable transformation module called Style Transformation Network. It takes latent codes as input, and learns linear mappings to different regions of the latent space to produce distinct codes for each style, resulting in a multistyle space. Our model inherently mitigates overfitting since it is trained on multiple styles, hence improving the quality of stylizations. Our method can learn upwards of $120$ image stylizations at once, bringing $8\times$ to $60\times$ improvement in training time over recent competing methods. We support our results through user studies and quantitative results that indicate meaningful improvements over existing methods.



