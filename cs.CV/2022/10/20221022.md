# Arxiv Papers in cs.CV on 2022-10-22
### Learning a Grammar Inducer from Massive Uncurated Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/2210.12309v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.12309v1)
- **Published**: 2022-10-22 00:22:55+00:00
- **Updated**: 2022-10-22 00:22:55+00:00
- **Authors**: Songyang Zhang, Linfeng Song, Lifeng Jin, Haitao Mi, Kun Xu, Dong Yu, Jiebo Luo
- **Comment**: Accepted by EMNLP 2022
- **Journal**: None
- **Summary**: Video-aided grammar induction aims to leverage video information for finding more accurate syntactic grammars for accompanying text. While previous work focuses on building systems for inducing grammars on text that are well-aligned with video content, we investigate the scenario, in which text and video are only in loose correspondence. Such data can be found in abundance online, and the weak correspondence is similar to the indeterminacy problem studied in language acquisition. Furthermore, we build a new model that can better learn video-span correlation without manually designed features adopted by previous work. Experiments show that our model trained only on large-scale YouTube data with no text-video alignment reports strong and robust performances across three unseen datasets, despite domain shift and noisy label issues. Furthermore our model yields higher F1 scores than the previous state-of-the-art systems trained on in-domain data.



### Tools for Extracting Spatio-Temporal Patterns in Meteorological Image Sequences: From Feature Engineering to Attention-Based Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.12310v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12310v2)
- **Published**: 2022-10-22 00:26:54+00:00
- **Updated**: 2022-10-25 03:59:28+00:00
- **Authors**: Akansha Singh Bansal, Yoonjin Lee, Kyle Hilburn, Imme Ebert-Uphoff
- **Comment**: The paper is submitted for review to the EDS Journal
- **Journal**: None
- **Summary**: Atmospheric processes involve both space and time. This is why human analysis of atmospheric imagery can often extract more information from animated loops of image sequences than from individual images. Automating such an analysis requires the ability to identify spatio-temporal patterns in image sequences which is a very challenging task, because of the endless possibilities of patterns in both space and time. In this paper we review different concepts and techniques that are useful to extract spatio-temporal context specifically for meteorological applications. In this survey we first motivate the need for these approaches in meteorology using two applications, solar forecasting and detecting convection from satellite imagery. Then we provide an overview of many different concepts and techniques that are helpful for the interpretation of meteorological image sequences, such as (1) feature engineering methods to strengthen the desired signal in the input, using meteorological knowledge, classic image processing, harmonic analysis and topological data analysis (2) explain how different convolution filters (2D/3D/LSTM-convolution) can be utilized strategically in convolutional neural network architectures to find patterns in both space and time (3) discuss the powerful new concept of 'attention' in neural networks and the powerful abilities it brings to the interpretation of image sequences (4) briefly survey strategies from unsupervised, self-supervised and transfer learning to reduce the need for large labeled datasets. We hope that presenting an overview of these tools - many of which are underutilized - will help accelerate progress in this area.



### Diffusion Motion: Generate Text-Guided 3D Human Motion by Diffusion Model
- **Arxiv ID**: http://arxiv.org/abs/2210.12315v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12315v2)
- **Published**: 2022-10-22 00:41:17+00:00
- **Updated**: 2023-04-14 14:39:54+00:00
- **Authors**: Zhiyuan Ren, Zhihong Pan, Xin Zhou, Le Kang
- **Comment**: Accepted by ICASSP 2023
- **Journal**: None
- **Summary**: We propose a simple and novel method for generating 3D human motion from complex natural language sentences, which describe different velocity, direction and composition of all kinds of actions. Different from existing methods that use classical generative architecture, we apply the Denoising Diffusion Probabilistic Model to this task, synthesizing diverse motion results under the guidance of texts. The diffusion model converts white noise into structured 3D motion by a Markov process with a series of denoising steps and is efficiently trained by optimizing a variational lower bound. To achieve the goal of text-conditioned image synthesis, we use the classifier-free guidance strategy to fuse text embedding into the model during training. Our experiments demonstrate that our model achieves competitive results on HumanML3D test set quantitatively and can generate more visually natural and diverse examples. We also show with experiments that our model is capable of zero-shot generation of motions for unseen text guidance.



### Deep Multi-Branch CNN Architecture for Early Alzheimer's Detection from Brain MRIs
- **Arxiv ID**: http://arxiv.org/abs/2210.12331v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, 68Txx, I.2; I.4; I.5; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2210.12331v3)
- **Published**: 2022-10-22 02:25:07+00:00
- **Updated**: 2023-06-17 23:24:53+00:00
- **Authors**: Paul K. Mandal, Rakesh Mahto
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Alzheimer's disease (AD) is a neuro-degenerative disease that can cause dementia and result severe reduction in brain function inhibiting simple tasks especially if no preventative care is taken. Over 1 in 9 Americans suffer from AD induced dementia and unpaid care for people with AD related dementia is valued at $271.6 billion. Hence, various approaches have been developed for early AD diagnosis to prevent its further progression. In this paper, we first review other approaches that could be used for early detection of AD. We then give an overview of our dataset that was from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and propose a deep Convolutional Neural Network (CNN) architecture consisting of 7,866,819 parameters. This model has three different convolutional branches with each having a different length. Each branch is comprised of different kernel sizes. This model can predict whether a patient is non-demented, mild-demented, or moderately demented with a 99.05% three class accuracy.



### Accumulated Trivial Attention Matters in Vision Transformers on Small Datasets
- **Arxiv ID**: http://arxiv.org/abs/2210.12333v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12333v1)
- **Published**: 2022-10-22 02:34:17+00:00
- **Updated**: 2022-10-22 02:34:17+00:00
- **Authors**: Xiangyu Chen, Qinghao Hu, Kaidong Li, Cuncong Zhong, Guanghui Wang
- **Comment**: Camera-Ready Version for WACV 2023
- **Journal**: None
- **Summary**: Vision Transformers has demonstrated competitive performance on computer vision tasks benefiting from their ability to capture long-range dependencies with multi-head self-attention modules and multi-layer perceptron. However, calculating global attention brings another disadvantage compared with convolutional neural networks, i.e. requiring much more data and computations to converge, which makes it difficult to generalize well on small datasets, which is common in practical applications. Previous works are either focusing on transferring knowledge from large datasets or adjusting the structure for small datasets. After carefully examining the self-attention modules, we discover that the number of trivial attention weights is far greater than the important ones and the accumulated trivial weights are dominating the attention in Vision Transformers due to their large quantity, which is not handled by the attention itself. This will cover useful non-trivial attention and harm the performance when trivial attention includes more noise, e.g. in shallow layers for some backbones. To solve this issue, we proposed to divide attention weights into trivial and non-trivial ones by thresholds, then Suppressing Accumulated Trivial Attention (SATA) weights by proposed Trivial WeIghts Suppression Transformation (TWIST) to reduce attention noise. Extensive experiments on CIFAR-100 and Tiny-ImageNet datasets show that our suppressing method boosts the accuracy of Vision Transformers by up to 2.3%. Code is available at https://github.com/xiangyu8/SATA.



### An Algorithm and Heuristic based on Normalized Mutual Information for Dimensionality Reduction and Classification of Hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/2210.13456v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, 68R05, I.4.7; I.4.8; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2210.13456v1)
- **Published**: 2022-10-22 02:58:04+00:00
- **Updated**: 2022-10-22 02:58:04+00:00
- **Authors**: Elkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1211.0613. text
  overlap with arXiv:2210.12296
- **Journal**: International Journal of Tomography and Statistics, Vol. 25, 2014,
  No.1, 41-54. ISSN: 2319-3336
  http://www.scopus.com/inward/record.url?eid=2-s2.0-84901443439&partnerID=MN8TOARS
- **Summary**: In the feature classification domain, the choice of data affects widely the results. The Hyperspectral image (HSI), is a set of more than a hundred bidirectional measures (called bands), of the same region (called ground truth map: GT). The HSI is modelized at a set of N vectors. So we have N features (or attributes) expressing N vectors of measures for C substances (called classes). The problematic is that it's pratically impossible to investgate all possible subsets. So we must find K vectors among N, such as relevant and no redundant ones; in order to classify substances. Here we introduce an algorithm based on Normalized Mutual Information to select relevant and no redundant bands, necessary to increase classification accuracy of HSI.   Keywords: Feature Selection, Normalized Mutual information, Hyperspectral images, Classification, Redundancy.



### Band selection and classification of hyperspectral images by minimizing normalized mutual information
- **Arxiv ID**: http://arxiv.org/abs/2210.14326v1
- **DOI**: 10.1109/ICMCS.2012.6320192.
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14326v1)
- **Published**: 2022-10-22 04:10:10+00:00
- **Updated**: 2022-10-22 04:10:10+00:00
- **Authors**: E. Sarhrouni, A. Hammouch, D. Aboutajdine
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2210.13456,
  arXiv:2210.12296, arXiv:1211.0613, arXiv:1210.0528; text overlap with
  arXiv:1210.0052, arXiv:1211.0055
- **Journal**: Multimedia Computing and Systems (ICMCS), 2012 International
  Conference on. May 10-12, 2012, Tangier, Morocco. Publication Year: 2012 ,
  Page(s): 155 - 159.
  http://www.scopus.com/inward/record.url?eid=2-s2.0-84869854259&partnerID=MN8TOARS
- **Summary**: Hyperspectral images (HSI) classification is a high technical remote sensing tool. The main goal is to classify the point of a region. The HIS contains more than a hundred bidirectional measures, called bands (or simply images), of the same region called Ground Truth Map (GT). Unfortunately, some bands contain redundant information, others are affected by the noise, and the high dimensionalities of features make the accuracy of classification lower. All these bands can be important for some applications, but for the classification a small subset of these is relevant. In this paper we use mutual information (MI) to select the relevant bands; and the Normalized Mutual Information coefficient to avoid and control redundant ones. This is a feature selection scheme and a Filter strategy. We establish this study on HSI AVIRIS 92AV3C. This is effectiveness, and fast scheme to control redundancy. Index Terms: Hyperspectral images, Classification, Feature Selection, Normalized Mutual Information, Redundancy.



### Mixed Precision Quantization to Tackle Gradient Leakage Attacks in Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.13457v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.13457v1)
- **Published**: 2022-10-22 04:24:32+00:00
- **Updated**: 2022-10-22 04:24:32+00:00
- **Authors**: Pretom Roy Ovi, Emon Dey, Nirmalya Roy, Aryya Gangopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Federated Learning (FL) enables collaborative model building among a large number of participants without the need for explicit data sharing. But this approach shows vulnerabilities when privacy inference attacks are applied to it. In particular, in the event of a gradient leakage attack, which has a higher success rate in retrieving sensitive data from the model gradients, FL models are at higher risk due to the presence of communication in their inherent architecture. The most alarming thing about this gradient leakage attack is that it can be performed in such a covert way that it does not hamper the training performance while the attackers backtrack from the gradients to get information about the raw data. Two of the most common approaches proposed as solutions to this issue are homomorphic encryption and adding noise with differential privacy parameters. These two approaches suffer from two major drawbacks. They are: the key generation process becomes tedious with the increasing number of clients, and noise-based differential privacy suffers from a significant drop in global model accuracy. As a countermeasure, we propose a mixed-precision quantized FL scheme, and we empirically show that both of the issues addressed above can be resolved. In addition, our approach can ensure more robustness as different layers of the deep model are quantized with different precision and quantization modes. We empirically proved the validity of our method with three benchmark datasets and found a minimal accuracy drop in the global model after applying quantization.



### A Task-aware Dual Similarity Network for Fine-grained Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.12348v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12348v1)
- **Published**: 2022-10-22 04:24:55+00:00
- **Updated**: 2022-10-22 04:24:55+00:00
- **Authors**: Yan Qi, Han Sun, Ningzhong Liu, Huiyu Zhou
- **Comment**: accepted by PRICAI2022, codes: https://github.com/qiyanhero/TDSNet
- **Journal**: None
- **Summary**: The goal of fine-grained few-shot learning is to recognize sub-categories under the same super-category by learning few labeled samples. Most of the recent approaches adopt a single similarity measure, that is, global or local measure alone. However, for fine-grained images with high intra-class variance and low inter-class variance, exploring global invariant features and discriminative local details is quite essential. In this paper, we propose a Task-aware Dual Similarity Network(TDSNet), which applies global features and local patches to achieve better performance. Specifically, a local feature enhancement module is adopted to activate the features with strong discriminability. Besides, task-aware attention exploits the important patches among the entire task. Finally, both the class prototypes obtained by global features and discriminative local patches are employed for prediction. Extensive experiments on three fine-grained datasets demonstrate that the proposed TDSNet achieves competitive performance by comparing with other state-of-the-art algorithms.



### Instance-Aware Image Completion
- **Arxiv ID**: http://arxiv.org/abs/2210.12350v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.12350v3)
- **Published**: 2022-10-22 04:38:00+00:00
- **Updated**: 2023-05-26 09:06:06+00:00
- **Authors**: Jinoh Cho, Minguk Kang, Vibhav Vineet, Jaesik Park
- **Comment**: AI for Content Creation (AI4CC) CVPR workshop, 2023
- **Journal**: None
- **Summary**: Image completion is a task that aims to fill in the missing region of a masked image with plausible contents. However, existing image completion methods tend to fill in the missing region with the surrounding texture instead of hallucinating a visual instance that is suitable in accordance with the context of the scene. In this work, we propose a novel image completion model, dubbed ImComplete, that hallucinates the missing instance that harmonizes well with - and thus preserves - the original context. ImComplete first adopts a transformer architecture that considers the visible instances and the location of the missing region. Then, ImComplete completes the semantic segmentation masks within the missing region, providing pixel-level semantic and structural guidance. Finally, the image synthesis blocks generate photo-realistic content. We perform a comprehensive evaluation of the results in terms of visual quality (LPIPS and FID) and contextual preservation scores (CLIPscore and object detection accuracy) with COCO-panoptic and Visual Genome datasets. Experimental results show the superiority of ImComplete on various natural images.



### NeuPhysics: Editable Neural Geometry and Physics from Monocular Videos
- **Arxiv ID**: http://arxiv.org/abs/2210.12352v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.12352v1)
- **Published**: 2022-10-22 04:57:55+00:00
- **Updated**: 2022-10-22 04:57:55+00:00
- **Authors**: Yi-Ling Qiao, Alexander Gao, Ming C. Lin
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: We present a method for learning 3D geometry and physics parameters of a dynamic scene from only a monocular RGB video input. To decouple the learning of underlying scene geometry from dynamic motion, we represent the scene as a time-invariant signed distance function (SDF) which serves as a reference frame, along with a time-conditioned deformation field. We further bridge this neural geometry representation with a differentiable physics simulator by designing a two-way conversion between the neural field and its corresponding hexahedral mesh, enabling us to estimate physics parameters from the source video by minimizing a cycle consistency loss. Our method also allows a user to interactively edit 3D objects from the source video by modifying the recovered hexahedral mesh, and propagating the operation back to the neural field representation. Experiments show that our method achieves superior mesh and video reconstruction of dynamic scenes compared to competing Neural Field approaches, and we provide extensive examples which demonstrate its ability to extract useful 3D representations from videos captured with consumer-grade cameras.



### MS-DCANet: A Novel Segmentation Network For Multi-Modality COVID-19 Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2210.12361v4
- **DOI**: 10.2147/JMDH.S417068
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12361v4)
- **Published**: 2022-10-22 05:51:52+00:00
- **Updated**: 2023-07-19 06:54:25+00:00
- **Authors**: Xiaoyu Pan, Huazheng Zhu, Jinglong Du, Guangtao Hu, Baoru Han, Yuanyuan Jia
- **Comment**: 21pages,13 figures,9 tables
- **Journal**: J Multidiscip Healthc. 2023;16:2023-2043
- **Summary**: The Coronavirus Disease 2019 (COVID-19) pandemic has increased the public health burden and brought profound disaster to humans. For the particularity of the COVID-19 medical images with blurred boundaries, low contrast and different infection sites, some researchers have improved the accuracy by adding more complexity. Also, they overlook the complexity of lesions, which hinder their ability to capture the relationship between segmentation sites and the background, as well as the edge contours and global context. However, increasing the computational complexity, parameters and inference speed is unfavorable for model transfer from laboratory to clinic. A perfect segmentation network needs to balance the above three factors completely. To solve the above issues, this paper propose a symmetric automatic segmentation framework named MS-DCANet. We introduce Tokenized MLP block, a novel attention scheme that use a shift-window mechanism to conditionally fuse local and global features to get more continuous boundaries and spatial positioning capabilities. It has greater understanding of irregular lesions contours. MS-DCANet also uses several Dual Channel blocks and a Res-ASPP block to improve the ability to recognize small targets. On multi-modality COVID-19 tasks, MS-DCANet achieved state-of-the-art performance compared with other baselines. It can well trade off the accuracy and complexity. To prove the strong generalization ability of our proposed model, we apply it to other tasks (ISIC 2018 and BAA) and achieve satisfactory results.



### S2WAT: Image Style Transfer via Hierarchical Vision Transformer using Strips Window Attention
- **Arxiv ID**: http://arxiv.org/abs/2210.12381v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12381v2)
- **Published**: 2022-10-22 07:56:13+00:00
- **Updated**: 2022-11-07 07:41:39+00:00
- **Authors**: Chiyu Zhang, Jun Yang, Lei Wang, Zaiyan Dai
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new hierarchical vision Transformer for image style transfer, called Strips Window Attention Transformer (S2WAT), which serves as an encoder of encoder-transfer-decoder architecture. With hierarchical features, S2WAT can leverage proven techniques in other fields of computer vision, such as feature pyramid networks (FPN) or U-Net, to image style transfer in future works. However, the existing window-based Transformers will cause a problem that the stylized images will be grid-like when introduced into image style transfer directly. To solve this problem, we propose S2WAT whose representation is computed with Strips Window Attention (SpW Attention). The SpW Attention can integrate both local information and long-range dependencies in horizontal and vertical directions by a novel feature fusion scheme named Attn Merge. Qualitative and quantitative experiments demonstrate that S2WAT achieves comparable performance to state-of-the-art CNN-based, Flow-based, and Transformer-based approaches. The code and models are available at https://github.com/AlienZhang1996/S2WAT.



### Diversity-Promoting Ensemble for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.12388v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.12388v2)
- **Published**: 2022-10-22 08:47:25+00:00
- **Updated**: 2022-12-21 15:08:28+00:00
- **Authors**: Mariana-Iuliana Georgescu, Radu Tudor Ionescu, Andreea-Iuliana Miron
- **Comment**: Accepted at SAC 2023
- **Journal**: None
- **Summary**: Medical image segmentation is an actively studied task in medical imaging, where the precision of the annotations is of utter importance towards accurate diagnosis and treatment. In recent years, the task has been approached with various deep learning systems, among the most popular models being U-Net. In this work, we propose a novel strategy to generate ensembles of different architectures for medical image segmentation, by leveraging the diversity (decorrelation) of the models forming the ensemble. More specifically, we utilize the Dice score among model pairs to estimate the correlation between the outputs of the two models forming each pair. To promote diversity, we select models with low Dice scores among each other. We carry out gastro-intestinal tract image segmentation experiments to compare our diversity-promoting ensemble (DiPE) with another strategy to create ensembles based on selecting the top scoring U-Net models. Our empirical results show that DiPE surpasses both individual models as well as the ensemble creation strategy based on selecting the top scoring models.



### Neural Distortion Fields for Spatial Calibration of Wide Field-of-View Near-Eye Displays
- **Arxiv ID**: http://arxiv.org/abs/2210.12389v1
- **DOI**: 10.1364/OE.472288
- **Categories**: **cs.CV**, cs.HC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12389v1)
- **Published**: 2022-10-22 08:48:31+00:00
- **Updated**: 2022-10-22 08:48:31+00:00
- **Authors**: Yuichi Hiroi, Kiyosato Someya, Yuta Itoh
- **Comment**: 17 pages. This is a preprint of a publication at OSA Optics Express
  30(22) pp.40628-40644, 2022
- **Journal**: Opt. Express 30(22) pp.40628-40644 (2022)
- **Summary**: We propose a spatial calibration method for wide Field-of-View (FoV) Near-Eye Displays (NEDs) with complex image distortions. Image distortions in NEDs can destroy the reality of the virtual object and cause sickness. To achieve distortion-free images in NEDs, it is necessary to establish a pixel-by-pixel correspondence between the viewpoint and the displayed image. Designing compact and wide-FoV NEDs requires complex optical designs. In such designs, the displayed images are subject to gaze-contingent, non-linear geometric distortions, which explicit geometric models can be difficult to represent or computationally intensive to optimize.   To solve these problems, we propose Neural Distortion Field (NDF), a fully-connected deep neural network that implicitly represents display surfaces complexly distorted in spaces. NDF takes spatial position and gaze direction as input and outputs the display pixel coordinate and its intensity as perceived in the input gaze direction. We synthesize the distortion map from a novel viewpoint by querying points on the ray from the viewpoint and computing a weighted sum to project output display coordinates into an image. Experiments showed that NDF calibrates an augmented reality NED with 90$^{\circ}$ FoV with about 3.23 pixel (5.8 arcmin) median error using only 8 training viewpoints. Additionally, we confirmed that NDF calibrates more accurately than the non-linear polynomial fitting, especially around the center of the FoV.



### SLAMs: Semantic Learning based Activation Map for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.12417v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12417v2)
- **Published**: 2022-10-22 11:17:30+00:00
- **Updated**: 2022-11-10 09:06:27+00:00
- **Authors**: Junliang Chen, Xiaodong Zhao, Minmin Liu, Linlin Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Recent mainstream weakly-supervised semantic segmentation (WSSS) approaches mainly relies on image-level classification learning, which has limited representation capacity. In this paper, we propose a novel semantic learning based framework, named SLAMs (Semantic Learning based Activation Map), for WSSS.



### Weakly-Supervised Temporal Article Grounding
- **Arxiv ID**: http://arxiv.org/abs/2210.12444v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.12444v2)
- **Published**: 2022-10-22 13:23:02+00:00
- **Updated**: 2023-02-24 02:53:39+00:00
- **Authors**: Long Chen, Yulei Niu, Brian Chen, Xudong Lin, Guangxing Han, Christopher Thomas, Hammad Ayyubi, Heng Ji, Shih-Fu Chang
- **Comment**: EMNLP 2022, https://github.com/zjuchenlong/WSAG
- **Journal**: None
- **Summary**: Given a long untrimmed video and natural language queries, video grounding (VG) aims to temporally localize the semantically-aligned video segments. Almost all existing VG work holds two simple but unrealistic assumptions: 1) All query sentences can be grounded in the corresponding video. 2) All query sentences for the same video are always at the same semantic scale. Unfortunately, both assumptions make today's VG models fail to work in practice. For example, in real-world multimodal assets (eg, news articles), most of the sentences in the article can not be grounded in their affiliated videos, and they typically have rich hierarchical relations (ie, at different semantic scales). To this end, we propose a new challenging grounding task: Weakly-Supervised temporal Article Grounding (WSAG). Specifically, given an article and a relevant video, WSAG aims to localize all ``groundable'' sentences to the video, and these sentences are possibly at different semantic scales. Accordingly, we collect the first WSAG dataset to facilitate this task: YouwikiHow, which borrows the inherent multi-scale descriptions in wikiHow articles and plentiful YouTube videos. In addition, we propose a simple but effective method DualMIL for WSAG, which consists of a two-level MIL loss and a single-/cross- sentence constraint loss. These training objectives are carefully designed for these relaxed assumptions. Extensive ablations have verified the effectiveness of DualMIL.



### Sub-network Multi-objective Evolutionary Algorithm for Filter Pruning
- **Arxiv ID**: http://arxiv.org/abs/2211.01957v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.01957v1)
- **Published**: 2022-10-22 13:34:14+00:00
- **Updated**: 2022-10-22 13:34:14+00:00
- **Authors**: Xuhua Li, Weize Sun, Lei Huang, Shaowu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Filter pruning is a common method to achieve model compression and acceleration in deep neural networks (DNNs).Some research regarded filter pruning as a combinatorial optimization problem and thus used evolutionary algorithms (EA) to prune filters of DNNs. However, it is difficult to find a satisfactory compromise solution in a reasonable time due to the complexity of solution space searching. To solve this problem, we first formulate a multi-objective optimization problem based on a sub-network of the full model and propose a Sub-network Multiobjective Evolutionary Algorithm (SMOEA) for filter pruning. By progressively pruning the convolutional layers in groups, SMOEA can obtain a lightweight pruned result with better performance.Experiments on VGG-14 model for CIFAR-10 verify the effectiveness of the proposed SMOEA. Specifically, the accuracy of the pruned model with 16.56% parameters decreases by 0.28% only, which is better than the widely used popular filter pruning criteria.



### A Flexible-Frame-Rate Vision-Aided Inertial Object Tracking System for Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2210.12476v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.12476v1)
- **Published**: 2022-10-22 15:26:50+00:00
- **Updated**: 2022-10-22 15:26:50+00:00
- **Authors**: Yo-Chung Lau, Kuan-Wei Tseng, I-Ju Hsieh, Hsiao-Ching Tseng, Yi-Ping Hung
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time object pose estimation and tracking is challenging but essential for emerging augmented reality (AR) applications. In general, state-of-the-art methods address this problem using deep neural networks which indeed yield satisfactory results. Nevertheless, the high computational cost of these methods makes them unsuitable for mobile devices where real-world applications usually take place. In addition, head-mounted displays such as AR glasses require at least 90~FPS to avoid motion sickness, which further complicates the problem. We propose a flexible-frame-rate object pose estimation and tracking system for mobile devices. It is a monocular visual-inertial-based system with a client-server architecture. Inertial measurement unit (IMU) pose propagation is performed on the client side for high speed tracking, and RGB image-based 3D pose estimation is performed on the server side to obtain accurate poses, after which the pose is sent to the client side for visual-inertial fusion, where we propose a bias self-correction mechanism to reduce drift. We also propose a pose inspection algorithm to detect tracking failures and incorrect pose estimation. Connected by high-speed networking, our system supports flexible frame rates up to 120 FPS and guarantees high precision and real-time tracking on low-end devices. Both simulations and real world experiments show that our method achieves accurate and robust object tracking.



### Offset-Guided Attention Network for Room-Level Aware Floor Plan Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.17411v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.17411v1)
- **Published**: 2022-10-22 16:53:49+00:00
- **Updated**: 2022-10-22 16:53:49+00:00
- **Authors**: Zhangyu Wang, Ningyuan Sun
- **Comment**: Under review of IEEE Access(3 accepts and 1 reject)
- **Journal**: None
- **Summary**: Recognition of floor plans has been a challenging and popular task. Despite that many recent approaches have been proposed for this task, they typically fail to make the room-level unified prediction. Specifically, multiple semantic categories can be assigned in a single room, which seriously limits their visual quality and applicability. In this paper, we propose a novel approach to recognize the floor plan layouts with a newly proposed Offset-Guided Attention mechanism to improve the semantic consistency within a room. In addition, we present a Feature Fusion Attention module that leverages the channel-wise attention to encourage the consistency of the room, wall, and door predictions, further enhancing the room-level semantic consistency. Experimental results manifest our approach is able to improve the room-level semantic consistency and outperforms the existing works both qualitatively and quantitatively.



### Generative Modeling of High-resolution Global Precipitation Forecasts
- **Arxiv ID**: http://arxiv.org/abs/2210.12504v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, physics.ao-ph
- **Links**: [PDF](http://arxiv.org/pdf/2210.12504v1)
- **Published**: 2022-10-22 17:21:16+00:00
- **Updated**: 2022-10-22 17:21:16+00:00
- **Authors**: James Duncan, Shashank Subramanian, Peter Harrington
- **Comment**: Accepted to NeurIPS 2022 Tackling Climate Change with Machine
  Learning Workshop
- **Journal**: None
- **Summary**: Forecasting global precipitation patterns and, in particular, extreme precipitation events is of critical importance to preparing for and adapting to climate change. Making accurate high-resolution precipitation forecasts using traditional physical models remains a major challenge in operational weather forecasting as they incur substantial computational costs and struggle to achieve sufficient forecast skill. Recently, deep-learning-based models have shown great promise in closing the gap with numerical weather prediction (NWP) models in terms of precipitation forecast skill, opening up exciting new avenues for precipitation modeling. However, it is challenging for these deep learning models to fully resolve the fine-scale structures of precipitation phenomena and adequately characterize the extremes of the long-tailed precipitation distribution. In this work, we present several improvements to the architecture and training process of a current state-of-the art deep learning precipitation model (FourCastNet) using a novel generative adversarial network (GAN) to better capture fine scales and extremes. Our improvements achieve superior performance in capturing the extreme percentiles of global precipitation, while comparable to state-of-the-art NWP models in terms of forecast skill at 1--2 day lead times. Together, these improvements set a new state-of-the-art in global precipitation forecasting.



### Cut-and-Approximate: 3D Shape Reconstruction from Planar Cross-sections with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.12509v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.12509v1)
- **Published**: 2022-10-22 17:48:12+00:00
- **Updated**: 2022-10-22 17:48:12+00:00
- **Authors**: Azimkhon Ostonov
- **Comment**: 13 pages; NeurIPS paper
- **Journal**: None
- **Summary**: Current methods for 3D object reconstruction from a set of planar cross-sections still struggle to capture detailed topology or require a considerable number of cross-sections. In this paper, we present, to the best of our knowledge the first 3D shape reconstruction network to solve this task which additionally uses orthographic projections of the shape. Our method is based on applying a Reinforcement Learning algorithm to learn how to effectively parse the shape using a trial-and-error scheme relying on scalar rewards. This method cuts a part of a 3D shape in each step which is then approximated as a polygon mesh. The agent aims to maximize the reward that depends on the accuracy of surface reconstruction for the approximated parts. We also consider pre-training of the network for faster learning using demonstrations generated by a heuristic approach. Experiments show that our training algorithm which benefits from both imitation learning and also self exploration, learns efficient policies faster, which results the agent to produce visually compelling results.



### DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents
- **Arxiv ID**: http://arxiv.org/abs/2210.12511v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.12511v1)
- **Published**: 2022-10-22 17:52:46+00:00
- **Updated**: 2022-10-22 17:52:46+00:00
- **Authors**: Ziqiao Ma, Ben VanDerPloeg, Cristian-Paul Bara, Huang Yidong, Eui-In Kim, Felix Gervits, Matthew Marge, Joyce Chai
- **Comment**: Findings of EMNLP, 2022
- **Journal**: None
- **Summary**: In the real world, autonomous driving agents navigate in highly dynamic environments full of unexpected situations where pre-trained models are unreliable. In these situations, what is immediately available to vehicles is often only human operators. Empowering autonomous driving agents with the ability to navigate in a continuous and dynamic environment and to communicate with humans through sensorimotor-grounded dialogue becomes critical. To this end, we introduce Dialogue On the ROad To Handle Irregular Events (DOROTHIE), a novel interactive simulation platform that enables the creation of unexpected situations on the fly to support empirical studies on situated communication with autonomous driving agents. Based on this platform, we created the Situated Dialogue Navigation (SDN), a navigation benchmark of 183 trials with a total of 8415 utterances, around 18.7 hours of control streams, and 2.9 hours of trimmed audio. SDN is developed to evaluate the agent's ability to predict dialogue moves from humans as well as generate its own dialogue moves and physical navigation actions. We further developed a transformer-based baseline model for these SDN tasks. Our empirical results indicate that language guided-navigation in a highly dynamic environment is an extremely difficult task for end-to-end models. These results will provide insight towards future work on robust autonomous driving agents. The DOROTHIE platform, SDN benchmark, and code for the baseline model are available at https://github.com/sled-group/DOROTHIE.



### Learning Point-Language Hierarchical Alignment for 3D Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/2210.12513v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12513v4)
- **Published**: 2022-10-22 18:02:10+00:00
- **Updated**: 2023-06-09 04:06:39+00:00
- **Authors**: Jiaming Chen, Weixin Luo, Ran Song, Xiaolin Wei, Lin Ma, Wei Zhang
- **Comment**: Champion on ECCV 2022 ScanRefer Challenge
- **Journal**: None
- **Summary**: This paper presents a novel hierarchical alignment model (HAM) that learns multi-granularity visual and linguistic representations in an end-to-end manner. We extract key points and proposal points to model 3D contexts and instances, and propose point-language alignment with context modulation (PLACM) mechanism, which learns to gradually align word-level and sentence-level linguistic embeddings with visual representations, while the modulation with the visual context captures latent informative relationships. To further capture both global and local relationships, we propose a spatially multi-granular modeling scheme that applies PLACM to both global and local fields. Experimental results demonstrate the superiority of HAM, with visualized results showing that it can dynamically model fine-grained visual and linguistic representations. HAM outperforms existing methods by a significant margin and achieves state-of-the-art performance on two publicly available datasets, and won the championship in ECCV 2022 ScanRefer challenge. Code is available at~\url{https://github.com/PPjmchen/HAM}.



### H-SAUR: Hypothesize, Simulate, Act, Update, and Repeat for Understanding Object Articulations from Interactions
- **Arxiv ID**: http://arxiv.org/abs/2210.12521v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12521v1)
- **Published**: 2022-10-22 18:39:33+00:00
- **Updated**: 2022-10-22 18:39:33+00:00
- **Authors**: Kei Ota, Hsiao-Yu Tung, Kevin A. Smith, Anoop Cherian, Tim K. Marks, Alan Sullivan, Asako Kanezaki, Joshua B. Tenenbaum
- **Comment**: None
- **Journal**: None
- **Summary**: The world is filled with articulated objects that are difficult to determine how to use from vision alone, e.g., a door might open inwards or outwards. Humans handle these objects with strategic trial-and-error: first pushing a door then pulling if that doesn't work. We enable these capabilities in autonomous agents by proposing "Hypothesize, Simulate, Act, Update, and Repeat" (H-SAUR), a probabilistic generative framework that simultaneously generates a distribution of hypotheses about how objects articulate given input observations, captures certainty over hypotheses over time, and infer plausible actions for exploration and goal-conditioned manipulation. We compare our model with existing work in manipulating objects after a handful of exploration actions, on the PartNet-Mobility dataset. We further propose a novel PuzzleBoxes benchmark that contains locked boxes that require multiple steps to solve. We show that the proposed model significantly outperforms the current state-of-the-art articulated object manipulation framework, despite using zero training data. We further improve the test-time efficiency of H-SAUR by integrating a learned prior from learning-based vision models.



### How Real is Real: Evaluating the Robustness of Real-World Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/2210.12523v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2210.12523v1)
- **Published**: 2022-10-22 18:53:45+00:00
- **Updated**: 2022-10-22 18:53:45+00:00
- **Authors**: Athiya Deviyani, Efe Sinan Hoplamaz, Alan Savio Paul
- **Comment**: Machine Learning Practical Final Report, The University of Edinburgh
- **Journal**: None
- **Summary**: Image super-resolution (SR) is a field in computer vision that focuses on reconstructing high-resolution images from the respective low-resolution image. However, super-resolution is a well-known ill-posed problem as most methods rely on the downsampling method performed on the high-resolution image to form the low-resolution image to be known. Unfortunately, this is not something that is available in real-life super-resolution applications such as increasing the quality of a photo taken on a mobile phone. In this paper we will evaluate multiple state-of-the-art super-resolution methods and gauge their performance when presented with various types of real-life images and discuss the benefits and drawbacks of each method. We also introduce a novel dataset, WideRealSR, containing real images from a wide variety of sources. Finally, through careful experimentation and evaluation, we will present a potential solution to alleviate the generalization problem which is imminent in most state-of-the-art super-resolution models.



### Efficient Hair Style Transfer with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.12524v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12524v1)
- **Published**: 2022-10-22 18:56:16+00:00
- **Updated**: 2022-10-22 18:56:16+00:00
- **Authors**: Muhammed Pektas, Baris Gecer, Aybars Ugur
- **Comment**: arXiv admin note: text overlap with arXiv:2010.16417 by other authors
- **Journal**: None
- **Summary**: Despite the recent success of image generation and style transfer with Generative Adversarial Networks (GANs), hair synthesis and style transfer remain challenging due to the shape and style variability of human hair in in-the-wild conditions. The current state-of-the-art hair synthesis approaches struggle to maintain global composition of the target style and cannot be used in real-time applications due to their high running costs on high-resolution portrait images. Therefore, We propose a novel hairstyle transfer method, called EHGAN, which reduces computational costs to enable real-time processing while improving the transfer of hairstyle with better global structure compared to the other state-of-the-art hair synthesis methods. To achieve this goal, we train an encoder and a low-resolution generator to transfer hairstyle and then, increase the resolution of results with a pre-trained super-resolution model. We utilize Adaptive Instance Normalization (AdaIN) and design our novel Hair Blending Block (HBB) to obtain the best performance of the generator. EHGAN needs around 2.7 times and over 10,000 times less time consumption than the state-of-the-art MichiGAN and LOHO methods respectively while obtaining better photorealism and structural similarity to the desired style than its competitors.



### Baby Physical Safety Monitoring in Smart Home Using Action Recognition System
- **Arxiv ID**: http://arxiv.org/abs/2210.12527v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.12527v2)
- **Published**: 2022-10-22 19:00:14+00:00
- **Updated**: 2023-04-30 01:17:01+00:00
- **Authors**: Victor Adewopo, Nelly Elsayed, Kelly Anderson
- **Comment**: None
- **Journal**: None
- **Summary**: Humans are able to intuitively deduce actions that took place between two states in observations via deductive reasoning. This is because the brain operates on a bidirectional communication model, which has radically improved the accuracy of recognition and prediction based on features connected to previous experiences. During the past decade, deep learning models for action recognition have significantly improved. However, deep neural networks struggle with these tasks on a smaller dataset for specific Action Recognition (AR) tasks. As with most action recognition tasks, the ambiguity of accurately describing activities in spatial-temporal data is a drawback that can be overcome by curating suitable datasets, including careful annotations and preprocessing of video data for analyzing various recognition tasks. In this study, we present a novel lightweight framework combining transfer learning techniques with a Conv2D LSTM layer to extract features from the pre-trained I3D model on the Kinetics dataset for a new AR task (Smart Baby Care) that requires a smaller dataset and less computational resources. Furthermore, we developed a benchmark dataset and an automated model that uses LSTM convolution with I3D (ConvLSTM-I3D) for recognizing and predicting baby activities in a smart baby room. Finally, we implemented video augmentation to improve model performance on the smart baby care task. Compared to other benchmark models, our experimental framework achieved better performance with less computational resources.



### JoJoNet: Joint-contrast and Joint-sampling-and-reconstruction Network for Multi-contrast MRI
- **Arxiv ID**: http://arxiv.org/abs/2210.12548v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12548v2)
- **Published**: 2022-10-22 20:46:56+00:00
- **Updated**: 2022-10-27 03:52:33+00:00
- **Authors**: Lin Zhao, Xiao Chen, Eric Z. Chen, Yikang Liu, Dinggang Shen, Terrence Chen, Shanhui Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-contrast Magnetic Resonance Imaging (MRI) generates multiple medical images with rich and complementary information for routine clinical use; however, it suffers from a long acquisition time. Recent works for accelerating MRI, mainly designed for single contrast, may not be optimal for multi-contrast scenario since the inherent correlations among the multi-contrast images are not exploited. In addition, independent reconstruction of each contrast usually does not translate to optimal performance of downstream tasks. Motivated by these aspects, in this paper we design an end-to-end framework for accelerating multi-contrast MRI which simultaneously optimizes the entire MR imaging workflow including sampling, reconstruction and downstream tasks to achieve the best overall outcomes. The proposed framework consists of a sampling mask generator for each image contrast and a reconstructor exploiting the inter-contrast correlations with a recurrent structure which enables the information sharing in a holistic way. The sampling mask generator and the reconstructor are trained jointly across the multiple image contrasts. The acceleration ratio of each image contrast is also learnable and can be driven by a downstream task performance. We validate our approach on a multi-contrast brain dataset and a multi-contrast knee dataset. Experiments show that (1) our framework consistently outperforms the baselines designed for single contrast on both datasets; (2) our newly designed recurrent reconstruction network effectively improves the reconstruction quality for multi-contrast images; (3) the learnable acceleration ratio improves the downstream task performance significantly. Overall, this work has potentials to open up new avenues for optimizing the entire multi-contrast MR imaging workflow.



### HuPR: A Benchmark for Human Pose Estimation Using Millimeter Wave Radar
- **Arxiv ID**: http://arxiv.org/abs/2210.12564v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.12564v1)
- **Published**: 2022-10-22 22:28:40+00:00
- **Updated**: 2022-10-22 22:28:40+00:00
- **Authors**: Shih-Po Lee, Niraj Prakash Kini, Wen-Hsiao Peng, Ching-Wen Ma, Jenq-Neng Hwang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a novel human pose estimation benchmark, Human Pose with Millimeter Wave Radar (HuPR), that includes synchronized vision and radio signal components. This dataset is created using cross-calibrated mmWave radar sensors and a monocular RGB camera for cross-modality training of radar-based human pose estimation. There are two advantages of using mmWave radar to perform human pose estimation. First, it is robust to dark and low-light conditions. Second, it is not visually perceivable by humans and thus, can be widely applied to applications with privacy concerns, e.g., surveillance systems in patient rooms. In addition to the benchmark, we propose a cross-modality training framework that leverages the ground-truth 2D keypoints representing human body joints for training, which are systematically generated from the pre-trained 2D pose estimation network based on a monocular camera input image, avoiding laborious manual label annotation efforts. The framework consists of a new radar pre-processing method that better extracts the velocity information from radar data, Cross- and Self-Attention Module (CSAM), to fuse multi-scale radar features, and Pose Refinement Graph Convolutional Networks (PRGCN), to refine the predicted keypoint confidence heatmaps. Our intensive experiments on the HuPR benchmark show that the proposed scheme achieves better human pose estimation performance with only radar data, as compared to traditional pre-processing solutions and previous radio-frequency-based methods.



