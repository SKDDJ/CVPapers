# Arxiv Papers in cs.CV on 2022-10-07
### Polyhistor: Parameter-Efficient Multi-Task Adaptation for Dense Vision Tasks
- **Arxiv ID**: http://arxiv.org/abs/2210.03265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03265v1)
- **Published**: 2022-10-07 00:25:02+00:00
- **Updated**: 2022-10-07 00:25:02+00:00
- **Authors**: Yen-Cheng Liu, Chih-Yao Ma, Junjiao Tian, Zijian He, Zsolt Kira
- **Comment**: Accepted to NeurIPS 2022; Project Page is at
  https://ycliu93.github.io/projects/polyhistor.html
- **Journal**: None
- **Summary**: Adapting large-scale pretrained models to various downstream tasks via fine-tuning is a standard method in machine learning. Recently, parameter-efficient fine-tuning methods show promise in adapting a pretrained model to different tasks while training only a few parameters. Despite their success, most existing methods are proposed in Natural Language Processing tasks with language Transformers, and adaptation to Computer Vision tasks with Vision Transformers remains under-explored, especially for dense vision tasks. Further, in multi-task settings, individually fine-tuning and storing separate models for different tasks is inefficient. In this work, we provide an extensive multi-task parameter-efficient benchmark and examine existing parameter-efficient fine-tuning NLP methods for vision tasks. Our results on four different dense vision tasks showed that existing methods cannot be efficiently integrated due to the hierarchical nature of the Hierarchical Vision Transformers. To overcome this issue, we propose Polyhistor and Polyhistor-Lite, consisting of Decomposed HyperNetworks and Layer-wise Scaling Kernels, to share information across different tasks with a few trainable parameters. This leads to favorable performance improvements against existing parameter-efficient methods while using fewer trainable parameters. Specifically, Polyhistor achieves competitive accuracy compared to the state-of-the-art while only using ~10% of their trainable parameters. Furthermore, our methods show larger performance gains when large networks and more pretraining data are used.



### TRADE: Object Tracking with 3D Trajectory and Ground Depth Estimates for UAVs
- **Arxiv ID**: http://arxiv.org/abs/2210.03270v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.03270v1)
- **Published**: 2022-10-07 00:52:21+00:00
- **Updated**: 2022-10-07 00:52:21+00:00
- **Authors**: Pedro F. Proen√ßa, Patrick Spieler, Robert A. Hewitt, Jeff Delaune
- **Comment**: None
- **Journal**: None
- **Summary**: We propose TRADE for robust tracking and 3D localization of a moving target in cluttered environments, from UAVs equipped with a single camera. Ultimately TRADE enables 3d-aware target following.   Tracking-by-detection approaches are vulnerable to target switching, especially between similar objects. Thus, TRADE predicts and incorporates the target 3D trajectory to select the right target from the tracker's response map. Unlike static environments, depth estimation of a moving target from a single camera is a ill-posed problem. Therefore we propose a novel 3D localization method for ground targets on complex terrain. It reasons about scene geometry by combining ground plane segmentation, depth-from-motion and single-image depth estimation. The benefits of using TRADE are demonstrated as tracking robustness and depth accuracy on several dynamic scenes simulated in this work. Additionally, we demonstrate autonomous target following using a thermal camera by running TRADE on a quadcopter's board computer.



### Scalable Self-Supervised Representation Learning from Spatiotemporal Motion Trajectories for Multimodal Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2210.03289v1
- **DOI**: 10.1109/MDM55031.2022.00028
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.03289v1)
- **Published**: 2022-10-07 02:41:02+00:00
- **Updated**: 2022-10-07 02:41:02+00:00
- **Authors**: Swetava Ganguli, C. V. Krishnakumar Iyer, Vipul Pandey
- **Comment**: Extended abstract accepted for presentation at BayLearn 2022. 3
  pages, 2 figures, 1 table. Abstract based on IEEE MDM 2022 research track
  paper: arXiv:2110.12521
- **Journal**: None
- **Summary**: Self-supervised representation learning techniques utilize large datasets without semantic annotations to learn meaningful, universal features that can be conveniently transferred to solve a wide variety of downstream supervised tasks. In this work, we propose a self-supervised method for learning representations of geographic locations from unlabeled GPS trajectories to solve downstream geospatial computer vision tasks. Tiles resulting from a raster representation of the earth's surface are modeled as nodes on a graph or pixels of an image. GPS trajectories are modeled as allowed Markovian paths on these nodes. A scalable and distributed algorithm is presented to compute image-like representations, called reachability summaries, of the spatial connectivity patterns between tiles and their neighbors implied by the observed Markovian paths. A convolutional, contractive autoencoder is trained to learn compressed representations, called reachability embeddings, of reachability summaries for every tile. Reachability embeddings serve as task-agnostic, feature representations of geographic locations. Using reachability embeddings as pixel representations for five different downstream geospatial tasks, cast as supervised semantic segmentation problems, we quantitatively demonstrate that reachability embeddings are semantically meaningful representations and result in 4-23% gain in performance, as measured using area under the precision-recall curve (AUPRC) metric, when compared to baseline models that use pixel representations that do not account for the spatial connectivity between tiles. Reachability embeddings transform sequential, spatiotemporal mobility data into semantically meaningful tensor representations that can be combined with other sources of imagery and are designed to facilitate multimodal learning in geospatial computer vision.



### EmbryosFormer: Deformable Transformer and Collaborative Encoding-Decoding for Embryos Stage Development Classification
- **Arxiv ID**: http://arxiv.org/abs/2210.04615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.04615v1)
- **Published**: 2022-10-07 02:54:34+00:00
- **Updated**: 2022-10-07 02:54:34+00:00
- **Authors**: Tien-Phat Nguyen, Trong-Thang Pham, Tri Nguyen, Hieu Le, Dung Nguyen, Hau Lam, Phong Nguyen, Jennifer Fowler, Minh-Triet Tran, Ngan Le
- **Comment**: Accepted at WACV 2023
- **Journal**: None
- **Summary**: The timing of cell divisions in early embryos during the In-Vitro Fertilization (IVF) process is a key predictor of embryo viability. However, observing cell divisions in Time-Lapse Monitoring (TLM) is a time-consuming process and highly depends on experts. In this paper, we propose EmbryosFormer, a computational model to automatically detect and classify cell divisions from original time-lapse images. Our proposed network is designed as an encoder-decoder deformable transformer with collaborative heads. The transformer contracting path predicts per-image labels and is optimized by a classification head. The transformer expanding path models the temporal coherency between embryo images to ensure monotonic non-decreasing constraint and is optimized by a segmentation head. Both contracting and expanding paths are synergetically learned by a collaboration head. We have benchmarked our proposed EmbryosFormer on two datasets: a public dataset with mouse embryos with 8-cell stage and an in-house dataset with human embryos with 4-cell stage. Source code: https://github.com/UARK-AICV/Embryos.



### GMA3D: Local-Global Attention Learning to Estimate Occluded Motions of Scene Flow
- **Arxiv ID**: http://arxiv.org/abs/2210.03296v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.03296v2)
- **Published**: 2022-10-07 03:09:00+00:00
- **Updated**: 2023-07-23 04:28:18+00:00
- **Authors**: Zhiyang Lu, Ming Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Scene flow represents the motion information of each point in the 3D point clouds. It is a vital downstream method applied to many tasks, such as motion segmentation and object tracking. However, there are always occlusion points between two consecutive point clouds, whether from the sparsity data sampling or real-world occlusion. In this paper, we focus on addressing occlusion issues in scene flow by the semantic self-similarity and motion consistency of the moving objects. We propose a GMA3D module based on the transformer framework, which utilizes local and global semantic similarity to infer the motion information of occluded points from the motion information of local and global non-occluded points respectively, and then uses an offset aggregator to aggregate them. Our module is the first to apply the transformer-based architecture to gauge the scene flow occlusion problem on point clouds. Experiments show that our GMA3D can solve the occlusion problem in the scene flow, especially in the real scene. We evaluated the proposed method on the occluded version of point cloud datasets and get state-of-the-art results on the real scene KITTI dataset. To testify that GMA3D is still beneficial to non-occluded scene flow, we also conducted experiments on non-occluded version datasets and achieved promising performance on FlyThings3D and KITTI. The code is available at https://anonymous.4open.science/r/GMA3D-E100.



### Preprocessors Matter! Realistic Decision-Based Attacks on Machine Learning Systems
- **Arxiv ID**: http://arxiv.org/abs/2210.03297v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.03297v2)
- **Published**: 2022-10-07 03:10:34+00:00
- **Updated**: 2023-07-20 19:28:22+00:00
- **Authors**: Chawin Sitawarin, Florian Tram√®r, Nicholas Carlini
- **Comment**: ICML 2023. Code can be found at
  https://github.com/google-research/preprocessor-aware-black-box-attack
- **Journal**: None
- **Summary**: Decision-based attacks construct adversarial examples against a machine learning (ML) model by making only hard-label queries. These attacks have mainly been applied directly to standalone neural networks. However, in practice, ML models are just one component of a larger learning system. We find that by adding a single preprocessor in front of a classifier, state-of-the-art query-based attacks are up to 7$\times$ less effective at attacking a prediction pipeline than at attacking the model alone. We explain this discrepancy by the fact that most preprocessors introduce some notion of invariance to the input space. Hence, attacks that are unaware of this invariance inevitably waste a large number of queries to re-discover or overcome it. We, therefore, develop techniques to (i) reverse-engineer the preprocessor and then (ii) use this extracted information to attack the end-to-end system. Our preprocessors extraction method requires only a few hundred queries, and our preprocessor-aware attacks recover the same efficacy as when attacking the model alone. The code can be found at https://github.com/google-research/preprocessor-aware-black-box-attack.



### Topology-Preserving Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/2210.03299v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03299v2)
- **Published**: 2022-10-07 03:13:35+00:00
- **Updated**: 2022-10-19 09:29:50+00:00
- **Authors**: Han Zhang, Lok Ming Lui
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation aims to automatically extract anatomical or pathological structures in the human body. Most objects or regions of interest are of similar patterns. For example, the relative location and the relative size of the lung and the kidney differ little among subjects. Incorporating these morphology rules as prior knowledge into the segmentation model is believed to be an effective way to enhance the accuracy of the segmentation results. Motivated by this, we propose in this work the Topology-Preserving Segmentation Network (TPSN) which can predict segmentation masks with the same topology prescribed for specific tasks. TPSN is a deformation-based model that yields a deformation map through an encoder-decoder architecture to warp the template masks into a target shape approximating the region to segment. Comparing to the segmentation framework based on pixel-wise classification, deformation-based segmentation models that warp a template to enclose the regions are more convenient to enforce geometric constraints. In our framework, we carefully design the ReLU Jacobian regularization term to enforce the bijectivity of the deformation map. As such, the predicted mask by TPSN has the same topology as that of the template prior mask.



### GOLLIC: Learning Global Context beyond Patches for Lossless High-Resolution Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2210.03301v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.03301v1)
- **Published**: 2022-10-07 03:15:02+00:00
- **Updated**: 2022-10-07 03:15:02+00:00
- **Authors**: Yuan Lan, Liang Qin, Zhaoyi Sun, Yang Xiang, Jie Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Neural-network-based approaches recently emerged in the field of data compression and have already led to significant progress in image compression, especially in achieving a higher compression ratio. In the lossless image compression scenario, however, existing methods often struggle to learn a probability model of full-size high-resolution images due to the limitation of the computation source. The current strategy is to crop high-resolution images into multiple non-overlapping patches and process them independently. This strategy ignores long-term dependencies beyond patches, thus limiting modeling performance. To address this problem, we propose a hierarchical latent variable model with a global context to capture the long-term dependencies of high-resolution images. Besides the latent variable unique to each patch, we introduce shared latent variables between patches to construct the global context. The shared latent variables are extracted by a self-supervised clustering module inside the model's encoder. This clustering module assigns each patch the confidence that it belongs to any cluster. Later, shared latent variables are learned according to latent variables of patches and their confidence, which reflects the similarity of patches in the same cluster and benefits the global context modeling. Experimental results show that our global context model improves compression ratio compared to the engineered codecs and deep learning models on three benchmark high-resolution image datasets, DIV2K, CLIC.pro, and CLIC.mobile.



### Scaling Forward Gradient With Local Losses
- **Arxiv ID**: http://arxiv.org/abs/2210.03310v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2210.03310v3)
- **Published**: 2022-10-07 03:52:27+00:00
- **Updated**: 2023-03-02 03:08:10+00:00
- **Authors**: Mengye Ren, Simon Kornblith, Renjie Liao, Geoffrey Hinton
- **Comment**: 31 pages, ICLR 2023
- **Journal**: None
- **Summary**: Forward gradient learning computes a noisy directional gradient and is a biologically plausible alternative to backprop for learning deep neural networks. However, the standard forward gradient algorithm, when applied naively, suffers from high variance when the number of parameters to be learned is large. In this paper, we propose a series of architectural and algorithmic modifications that together make forward gradient learning practical for standard deep learning benchmark tasks. We show that it is possible to substantially reduce the variance of the forward gradient estimator by applying perturbations to activations rather than weights. We further improve the scalability of forward gradient by introducing a large number of local greedy loss functions, each of which involves only a small number of learnable parameters, and a new MLPMixer-inspired architecture, LocalMixer, that is more suitable for local learning. Our approach matches backprop on MNIST and CIFAR-10 and significantly outperforms previously proposed backprop-free algorithms on ImageNet.



### Resolving Class Imbalance for LiDAR-based Object Detector by Dynamic Weight Average and Contextual Ground Truth Sampling
- **Arxiv ID**: http://arxiv.org/abs/2210.03331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03331v1)
- **Published**: 2022-10-07 05:23:25+00:00
- **Updated**: 2022-10-07 05:23:25+00:00
- **Authors**: Daeun Lee, Jongwon Park, Jinkyu Kim
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: An autonomous driving system requires a 3D object detector, which must perceive all present road agents reliably to navigate an environment safely. However, real-world driving datasets often suffer from the problem of data imbalance, which causes difficulties in training a model that works well across all classes, resulting in an undesired imbalanced sub-optimal performance. In this work, we propose a method to address this data imbalance problem. Our method consists of two main components: (i) a LiDAR-based 3D object detector with per-class multiple detection heads where losses from each head are modified by dynamic weight average to be balanced. (ii) Contextual ground truth (GT) sampling, where we improve conventional GT sampling techniques by leveraging semantic information to augment point cloud with sampled ground truth GT objects. Our experiment with KITTI and nuScenes datasets confirms our proposed method's effectiveness in dealing with the data imbalance problem, producing better detection accuracy compared to existing approaches.



### Explainable AI based Glaucoma Detection using Transfer Learning and LIME
- **Arxiv ID**: http://arxiv.org/abs/2210.03332v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.03332v1)
- **Published**: 2022-10-07 05:36:33+00:00
- **Updated**: 2022-10-07 05:36:33+00:00
- **Authors**: Touhidul Islam Chayan, Anita Islam, Eftykhar Rahman, Md. Tanzim Reza, Tasnim Sakib Apon, MD. Golam Rabiul Alam
- **Comment**: None
- **Journal**: None
- **Summary**: Glaucoma is the second driving reason for partial or complete blindness among all the visual deficiencies which mainly occurs because of excessive pressure in the eye due to anxiety or depression which damages the optic nerve and creates complications in vision. Traditional glaucoma screening is a time-consuming process that necessitates the medical professionals' constant attention, and even so time to time due to the time constrains and pressure they fail to classify correctly that leads to wrong treatment. Numerous efforts have been made to automate the entire glaucoma classification procedure however, these existing models in general have a black box characteristics that prevents users from understanding the key reasons behind the prediction and thus medical practitioners generally can not rely on these system. In this article after comparing with various pre-trained models, we propose a transfer learning model that is able to classify Glaucoma with 94.71\% accuracy. In addition, we have utilized Local Interpretable Model-Agnostic Explanations(LIME) that introduces explainability in our system. This improvement enables medical professionals obtain important and comprehensive information that aid them in making judgments. It also lessen the opacity and fragility of the traditional deep learning models.



### A Keypoint Based Enhancement Method for Audio Driven Free View Talking Head Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2210.03335v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03335v1)
- **Published**: 2022-10-07 05:44:10+00:00
- **Updated**: 2022-10-07 05:44:10+00:00
- **Authors**: Yichen Han, Ya Li, Yingming Gao, Jinlong Xue, Songpo Wang, Lei Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Audio driven talking head synthesis is a challenging task that attracts increasing attention in recent years. Although existing methods based on 2D landmarks or 3D face models can synthesize accurate lip synchronization and rhythmic head pose for arbitrary identity, they still have limitations, such as the cut feeling in the mouth mapping and the lack of skin highlights. The morphed region is blurry compared to the surrounding face. A Keypoint Based Enhancement (KPBE) method is proposed for audio driven free view talking head synthesis to improve the naturalness of the generated video. Firstly, existing methods were used as the backend to synthesize intermediate results. Then we used keypoint decomposition to extract video synthesis controlling parameters from the backend output and the source image. After that, the controlling parameters were composited to the source keypoints and the driving keypoints. A motion field based method was used to generate the final image from the keypoint representation. With keypoint representation, we overcame the cut feeling in the mouth mapping and the lack of skin highlights. Experiments show that our proposed enhancement method improved the quality of talking-head videos in terms of mean opinion score.



### Dual Clustering Co-teaching with Consistent Sample Mining for Unsupervised Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2210.03339v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03339v1)
- **Published**: 2022-10-07 06:04:04+00:00
- **Updated**: 2022-10-07 06:04:04+00:00
- **Authors**: Zeqi Chen, Zhichao Cui, Chi Zhang, Jiahuan Zhou, Yuehu Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In unsupervised person Re-ID, peer-teaching strategy leveraging two networks to facilitate training has been proven to be an effective method to deal with the pseudo label noise. However, training two networks with a set of noisy pseudo labels reduces the complementarity of the two networks and results in label noise accumulation. To handle this issue, this paper proposes a novel Dual Clustering Co-teaching (DCCT) approach. DCCT mainly exploits the features extracted by two networks to generate two sets of pseudo labels separately by clustering with different parameters. Each network is trained with the pseudo labels generated by its peer network, which can increase the complementarity of the two networks to reduce the impact of noises. Furthermore, we propose dual clustering with dynamic parameters (DCDP) to make the network adaptive and robust to dynamically changing clustering parameters. Moreover, Consistent Sample Mining (CSM) is proposed to find the samples with unchanged pseudo labels during training for potential noisy sample removal. Extensive experiments demonstrate the effectiveness of the proposed method, which outperforms the state-of-the-art unsupervised person Re-ID methods by a considerable margin and surpasses most methods utilizing camera information.



### Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding
- **Arxiv ID**: http://arxiv.org/abs/2210.03347v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.03347v2)
- **Published**: 2022-10-07 06:42:06+00:00
- **Updated**: 2023-06-15 21:34:23+00:00
- **Authors**: Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova
- **Comment**: Accepted at ICML
- **Journal**: None
- **Summary**: Visually-situated language is ubiquitous -- sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.



### Efficient Diffusion Models for Vision: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2210.09292v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.09292v2)
- **Published**: 2022-10-07 06:46:13+00:00
- **Updated**: 2022-10-20 12:29:30+00:00
- **Authors**: Anwaar Ulhaq, Naveed Akhtar, Ganna Pogrebna
- **Comment**: 14 Pages, 5 Figures (in progress)
- **Journal**: None
- **Summary**: Diffusion Models (DMs) have demonstrated state-of-the-art performance in content generation without requiring adversarial training. These models are trained using a two-step process. First, a forward - diffusion - process gradually adds noise to a datum (usually an image). Then, a backward - reverse diffusion - process gradually removes the noise to turn it into a sample of the target distribution being modelled. DMs are inspired by non-equilibrium thermodynamics and have inherent high computational complexity. Due to the frequent function evaluations and gradient calculations in high-dimensional spaces, these models incur considerable computational overhead during both training and inference stages. This can not only preclude the democratization of diffusion-based modelling, but also hinder the adaption of diffusion models in real-life applications. Not to mention, the efficiency of computational models is fast becoming a significant concern due to excessive energy consumption and environmental scares. These factors have led to multiple contributions in the literature that focus on devising computationally efficient DMs. In this review, we present the most recent advances in diffusion models for vision, specifically focusing on the important design aspects that affect the computational efficiency of DMs. In particular, we emphasize the recently proposed design choices that have led to more efficient DMs. Unlike the other recent reviews, which discuss diffusion models from a broad perspective, this survey is aimed at pushing this research direction forward by highlighting the design strategies in the literature that are resulting in practicable models for the broader research community. We also provide a future outlook of diffusion models in vision from their computational efficiency viewpoint.



### Game-Theoretic Understanding of Misclassification
- **Arxiv ID**: http://arxiv.org/abs/2210.03349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03349v1)
- **Published**: 2022-10-07 06:50:02+00:00
- **Updated**: 2022-10-07 06:50:02+00:00
- **Authors**: Kosuke Sumiyasu, Kazuhiko Kawamoto, Hiroshi Kera
- **Comment**: 15 pages, 8 figures
- **Journal**: None
- **Summary**: This paper analyzes various types of image misclassification from a game-theoretic view. Particularly, we consider the misclassification of clean, adversarial, and corrupted images and characterize it through the distribution of multi-order interactions. We discover that the distribution of multi-order interactions varies across the types of misclassification. For example, misclassified adversarial images have a higher strength of high-order interactions than correctly classified clean images, which indicates that adversarial perturbations create spurious features that arise from complex cooperation between pixels. By contrast, misclassified corrupted images have a lower strength of low-order interactions than correctly classified clean images, which indicates that corruptions break the local cooperation between pixels. We also provide the first analysis of Vision Transformers using interactions. We found that Vision Transformers show a different tendency in the distribution of interactions from that in CNNs, and this implies that they exploit the features that CNNs do not use for the prediction. Our study demonstrates that the recent game-theoretic analysis of deep learning models can be broadened to analyze various malfunctions of deep learning models including Vision Transformers by using the distribution, order, and sign of interactions.



### Multiple Object Tracking from appearance by hierarchically clustering tracklets
- **Arxiv ID**: http://arxiv.org/abs/2210.03355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03355v1)
- **Published**: 2022-10-07 07:04:15+00:00
- **Updated**: 2022-10-07 07:04:15+00:00
- **Authors**: Andreu Girbau, Ferran Marqu√©s, Shin'ichi Satoh
- **Comment**: To be published in BMVC 2022
- **Journal**: None
- **Summary**: Current approaches in Multiple Object Tracking (MOT) rely on the spatio-temporal coherence between detections combined with object appearance to match objects from consecutive frames. In this work, we explore MOT using object appearances as the main source of association between objects in a video, using spatial and temporal priors as weighting factors. We form initial tracklets by leveraging on the idea that instances of an object that are close in time should be similar in appearance, and build the final object tracks by fusing the tracklets in a hierarchical fashion. We conduct extensive experiments that show the effectiveness of our method over three different MOT benchmarks, MOT17, MOT20, and DanceTrack, being competitive in MOT17 and MOT20 and establishing state-of-the-art results in DanceTrack.



### Pre-trained Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2210.03372v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.03372v2)
- **Published**: 2022-10-07 07:28:03+00:00
- **Updated**: 2022-10-14 12:37:24+00:00
- **Authors**: Yuanhao Ban, Yinpeng Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised pre-training has drawn increasing attention in recent years due to its superior performance on numerous downstream tasks after fine-tuning. However, it is well-known that deep learning models lack the robustness to adversarial examples, which can also invoke security issues to pre-trained models, despite being less explored. In this paper, we delve into the robustness of pre-trained models by introducing Pre-trained Adversarial Perturbations (PAPs), which are universal perturbations crafted for the pre-trained models to maintain the effectiveness when attacking fine-tuned ones without any knowledge of the downstream tasks. To this end, we propose a Low-Level Layer Lifting Attack (L4A) method to generate effective PAPs by lifting the neuron activations of low-level layers of the pre-trained models. Equipped with an enhanced noise augmentation strategy, L4A is effective at generating more transferable PAPs against fine-tuned models. Extensive experiments on typical pre-trained vision models and ten downstream tasks demonstrate that our method improves the attack success rate by a large margin compared with state-of-the-art methods.



### Temporal Feature Alignment in Contrastive Self-Supervised Learning for Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.03382v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.03382v1)
- **Published**: 2022-10-07 07:51:01+00:00
- **Updated**: 2022-10-07 07:51:01+00:00
- **Authors**: Bulat Khaertdinov, Stylianos Asteriadis
- **Comment**: Accepted to IJCB 2022
- **Journal**: None
- **Summary**: Automated Human Activity Recognition has long been a problem of great interest in human-centered and ubiquitous computing. In the last years, a plethora of supervised learning algorithms based on deep neural networks has been suggested to address this problem using various modalities. While every modality has its own limitations, there is one common challenge. Namely, supervised learning requires vast amounts of annotated data which is practically hard to collect. In this paper, we benefit from the self-supervised learning paradigm (SSL) that is typically used to learn deep feature representations from unlabeled data. Moreover, we upgrade a contrastive SSL framework, namely SimCLR, widely used in various applications by introducing a temporal feature alignment procedure for Human Activity Recognition. Specifically, we propose integrating a dynamic time warping (DTW) algorithm in a latent space to force features to be aligned in a temporal dimension. Extensive experiments have been conducted for the unimodal scenario with inertial modality as well as in multimodal settings using inertial and skeleton data. According to the obtained results, the proposed approach has a great potential in learning robust feature representations compared to the recent SSL baselines, and clearly outperforms supervised models in semi-supervised learning. The code for the unimodal case is available via the following link: https://github.com/bulatkh/csshar_tfa.



### Mars Rover Localization Based on A2G Obstacle Distribution Pattern Matching
- **Arxiv ID**: http://arxiv.org/abs/2210.03398v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03398v1)
- **Published**: 2022-10-07 08:29:48+00:00
- **Updated**: 2022-10-07 08:29:48+00:00
- **Authors**: Lang Zhou, Zhitai Zhang, Hongliang Wang
- **Comment**: 8 pages, in Chinese language, 9 figures
- **Journal**: None
- **Summary**: Rover localization is one of the perquisites for large scale rover exploration. In NASA's Mars 2020 mission, the Ingenuity helicopter is carried together with the rover, which is capable of obtaining high-resolution imagery of Mars terrain, and it is possible to perform localization based on aerial-to-ground (A2G) imagery correspondence. However, considering the low-texture nature of the Mars terrain, and large perspective changes between UAV and rover imagery, traditional image matching methods will struggle to obtain valid image correspondence. In this paper we propose a novel pipeline for Mars rover localization. An algorithm combing image-based rock detection and rock distribution pattern matching is used to acquire A2G imagery correspondence, thus establishing the rover position in a UAV-generated ground map. Feasibility of this method is evaluated on sample data from a Mars analogue environment. The proposed method can serve as a reliable assist in future Mars missions.



### Computational imaging with the human brain
- **Arxiv ID**: http://arxiv.org/abs/2210.03400v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2210.03400v1)
- **Published**: 2022-10-07 08:40:18+00:00
- **Updated**: 2022-10-07 08:40:18+00:00
- **Authors**: Gao Wang, Daniele Faccio
- **Comment**: None
- **Journal**: None
- **Summary**: Brain-computer interfaces (BCIs) are enabling a range of new possibilities and routes for augmenting human capability. Here, we propose BCIs as a route towards forms of computation, i.e. computational imaging, that blend the brain with external silicon processing. We demonstrate ghost imaging of a hidden scene using the human visual system that is combined with an adaptive computational imaging scheme. This is achieved through a projection pattern `carving' technique that relies on real-time feedback from the brain to modify patterns at the light projector, thus enabling more efficient and higher resolution imaging. This brain-computer connectivity demonstrates a form of augmented human computation that could in the future extend the sensing range of human vision and provide new approaches to the study of the neurophysics of human perception. As an example, we illustrate a simple experiment whereby image reconstruction quality is affected by simultaneous conscious processing and readout of the perceived light intensities.



### Detailed Annotations of Chest X-Rays via CT Projection for Report Understanding
- **Arxiv ID**: http://arxiv.org/abs/2210.03416v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6; I.4.8; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2210.03416v1)
- **Published**: 2022-10-07 09:21:48+00:00
- **Updated**: 2022-10-07 09:21:48+00:00
- **Authors**: Constantin Seibold, Simon Rei√ü, Saquib Sarfraz, Matthias A. Fink, Victoria Mayer, Jan Sellner, Moon Sung Kim, Klaus H. Maier-Hein, Jens Kleesiek, Rainer Stiefelhagen
- **Comment**: 33rd British Machine Vision Conference (BMVC 2022)
- **Journal**: None
- **Summary**: In clinical radiology reports, doctors capture important information about the patient's health status. They convey their observations from raw medical imaging data about the inner structures of a patient. As such, formulating reports requires medical experts to possess wide-ranging knowledge about anatomical regions with their normal, healthy appearance as well as the ability to recognize abnormalities. This explicit grasp on both the patient's anatomy and their appearance is missing in current medical image-processing systems as annotations are especially difficult to gather. This renders the models to be narrow experts e.g. for identifying specific diseases. In this work, we recover this missing link by adding human anatomy into the mix and enable the association of content in medical reports to their occurrence in associated imagery (medical phrase grounding). To exploit anatomical structures in this scenario, we present a sophisticated automatic pipeline to gather and integrate human bodily structures from computed tomography datasets, which we incorporate in our PAXRay: A Projected dataset for the segmentation of Anatomical structures in X-Ray data. Our evaluation shows that methods that take advantage of anatomical information benefit heavily in visually grounding radiologists' findings, as our anatomical segmentations allow for up to absolute 50% better grounding results on the OpenI dataset as compared to commonly used region proposals. The PAXRay dataset is available at https://constantinseibold.github.io/paxray/.



### A Simple Plugin for Transforming Images to Arbitrary Scales
- **Arxiv ID**: http://arxiv.org/abs/2210.03417v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03417v1)
- **Published**: 2022-10-07 09:24:38+00:00
- **Updated**: 2022-10-07 09:24:38+00:00
- **Authors**: Qinye Zhou, Ziyi Li, Weidi Xie, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Existing models on super-resolution often specialized for one scale, fundamentally limiting their use in practical scenarios. In this paper, we aim to develop a general plugin that can be inserted into existing super-resolution models, conveniently augmenting their ability towards Arbitrary Resolution Image Scaling, thus termed ARIS. We make the following contributions: (i) we propose a transformer-based plugin module, which uses spatial coordinates as query, iteratively attend the low-resolution image feature through cross-attention, and output visual feature for the queried spatial location, resembling an implicit representation for images; (ii) we introduce a novel self-supervised training scheme, that exploits consistency constraints to effectively augment the model's ability for upsampling images towards unseen scales, i.e. ground-truth high-resolution images are not available; (iii) without loss of generality, we inject the proposed ARIS plugin module into several existing models, namely, IPT, SwinIR, and HAT, showing that the resulting models can not only maintain their original performance on fixed scale factor but also extrapolate to unseen scales, substantially outperforming existing any-scale super-resolution models on standard benchmarks, e.g. Urban100, DIV2K, etc.



### Missing Modality meets Meta Sampling (M3S): An Efficient Universal Approach for Multimodal Sentiment Analysis with Missing Modality
- **Arxiv ID**: http://arxiv.org/abs/2210.03428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03428v1)
- **Published**: 2022-10-07 09:54:05+00:00
- **Updated**: 2022-10-07 09:54:05+00:00
- **Authors**: Haozhe Chi, Minghua Yang, Junhao Zhu, Guanhong Wang, Gaoang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal sentiment analysis (MSA) is an important way of observing mental activities with the help of data captured from multiple modalities. However, due to the recording or transmission error, some modalities may include incomplete data. Most existing works that address missing modalities usually assume a particular modality is completely missing and seldom consider a mixture of missing across multiple modalities. In this paper, we propose a simple yet effective meta-sampling approach for multimodal sentiment analysis with missing modalities, namely Missing Modality-based Meta Sampling (M3S). To be specific, M3S formulates a missing modality sampling strategy into the modal agnostic meta-learning (MAML) framework. M3S can be treated as an efficient add-on training component on existing models and significantly improve their performances on multimodal data with a mixture of missing modalities. We conduct experiments on IEMOCAP, SIMS and CMU-MOSI datasets, and superior performance is achieved compared with recent state-of-the-art methods.



### Adversarially Robust Prototypical Few-shot Segmentation with Neural-ODEs
- **Arxiv ID**: http://arxiv.org/abs/2210.03429v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03429v1)
- **Published**: 2022-10-07 10:00:45+00:00
- **Updated**: 2022-10-07 10:00:45+00:00
- **Authors**: Prashant Pandey, Aleti Vardhan, Mustafa Chasmai, Tanuj Sur, Brejesh Lall
- **Comment**: MICCAI 2022. arXiv admin note: substantial text overlap with
  arXiv:2208.12428
- **Journal**: None
- **Summary**: Few-shot Learning (FSL) methods are being adopted in settings where data is not abundantly available. This is especially seen in medical domains where the annotations are expensive to obtain. Deep Neural Networks have been shown to be vulnerable to adversarial attacks. This is even more severe in the case of FSL due to the lack of a large number of training examples. In this paper, we provide a framework to make few-shot segmentation models adversarially robust in the medical domain where such attacks can severely impact the decisions made by clinicians who use them. We propose a novel robust few-shot segmentation framework, Prototypical Neural Ordinary Differential Equation (PNODE), that provides defense against gradient-based adversarial attacks. We show that our framework is more robust compared to traditional adversarial defense mechanisms such as adversarial training. Adversarial training involves increased training time and shows robustness to limited types of attacks depending on the type of adversarial examples seen during training. Our proposed framework generalises well to common adversarial attacks like FGSM, PGD and SMIA while having the model parameters comparable to the existing few-shot segmentation models. We show the effectiveness of our proposed approach on three publicly available multi-organ segmentation datasets in both in-domain and cross-domain settings by attacking the support and query sets without the need for ad-hoc adversarial training.



### PS-ARM: An End-to-End Attention-aware Relation Mixer Network for Person Search
- **Arxiv ID**: http://arxiv.org/abs/2210.03433v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03433v1)
- **Published**: 2022-10-07 10:04:12+00:00
- **Updated**: 2022-10-07 10:04:12+00:00
- **Authors**: Mustansar Fiaz, Hisham Cholakkal, Sanath Narayan, Rao Muhammad Anwer, Fahad Shahbaz Khan
- **Comment**: Paper accepted in ACCV 2022
- **Journal**: None
- **Summary**: Person search is a challenging problem with various real-world applications, that aims at joint person detection and re-identification of a query person from uncropped gallery images. Although, the previous study focuses on rich feature information learning, it is still hard to retrieve the query person due to the occurrence of appearance deformations and background distractors. In this paper, we propose a novel attention-aware relation mixer (ARM) module for person search, which exploits the global relation between different local regions within RoI of a person and make it robust against various appearance deformations and occlusion. The proposed ARM is composed of a relation mixer block and a spatio-channel attention layer. The relation mixer block introduces a spatially attended spatial mixing and a channel-wise attended channel mixing for effectively capturing discriminative relation features within an RoI. These discriminative relation features are further enriched by introducing a spatio-channel attention where the foreground and background discriminability is empowered in a joint spatio-channel space. Our ARM module is generic and it does not rely on fine-grained supervision or topological assumptions, hence being easily integrated into any Faster R-CNN based person search methods. Comprehensive experiments are performed on two challenging benchmark datasets: CUHKSYSU and PRW. Our PS-ARM achieves state-of-the-art performance on both datasets. On the challenging PRW dataset, our PS-ARM achieves an absolute gain of 5 in the mAP score over SeqNet, while operating at a comparable speed.



### IDPL: Intra-subdomain adaptation adversarial learning segmentation method based on Dynamic Pseudo Labels
- **Arxiv ID**: http://arxiv.org/abs/2210.03435v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03435v2)
- **Published**: 2022-10-07 10:06:57+00:00
- **Updated**: 2022-10-20 16:19:50+00:00
- **Authors**: Xuewei Li, Weilun Zhang, Jie Gao, Xuzhou Fu, Jian Yu
- **Comment**: Accepted at The 29th International Conference on Neural Information
  Processing (ICONIP 2022)
- **Journal**: Lecture Notes in Computer Science (LNCS) proceedings of ICONIP
  2022
- **Summary**: Unsupervised domain adaptation(UDA) has been applied to image semantic segmentation to solve the problem of domain offset. However, in some difficult categories with poor recognition accuracy, the segmentation effects are still not ideal. To this end, in this paper, Intra-subdomain adaptation adversarial learning segmentation method based on Dynamic Pseudo Labels(IDPL) is proposed. The whole process consists of 3 steps: Firstly, the instance-level pseudo label dynamic generation module is proposed, which fuses the class matching information in global classes and local instances, thus adaptively generating the optimal threshold for each class, obtaining high-quality pseudo labels. Secondly, the subdomain classifier module based on instance confidence is constructed, which can dynamically divide the target domain into easy and difficult subdomains according to the relative proportion of easy and difficult instances. Finally, the subdomain adversarial learning module based on self-attention is proposed. It uses multi-head self-attention to confront the easy and difficult subdomains at the class level with the help of generated high-quality pseudo labels, so as to focus on mining the features of difficult categories in the high-entropy region of target domain images, which promotes class-level conditional distribution alignment between the subdomains, improving the segmentation performance of difficult categories. For the difficult categories, the experimental results show that the performance of IDPL is significantly improved compared with other latest mainstream methods.



### Trans2k: Unlocking the Power of Deep Models for Transparent Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2210.03436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03436v1)
- **Published**: 2022-10-07 10:08:13+00:00
- **Updated**: 2022-10-07 10:08:13+00:00
- **Authors**: Alan Lukezic, Ziga Trojer, Jiri Matas, Matej Kristan
- **Comment**: Accepted to BMVC 2022. Project page:
  https://github.com/trojerz/Trans2k
- **Journal**: None
- **Summary**: Visual object tracking has focused predominantly on opaque objects, while transparent object tracking received very little attention. Motivated by the uniqueness of transparent objects in that their appearance is directly affected by the background, the first dedicated evaluation dataset has emerged recently. We contribute to this effort by proposing the first transparent object tracking training dataset Trans2k that consists of over 2k sequences with 104,343 images overall, annotated by bounding boxes and segmentation masks. Noting that transparent objects can be realistically rendered by modern renderers, we quantify domain-specific attributes and render the dataset containing visual attributes and tracking situations not covered in the existing object training datasets. We observe a consistent performance boost (up to 16%) across a diverse set of modern tracking architectures when trained using Trans2k, and show insights not previously possible due to the lack of appropriate training sets. The dataset and the rendering engine will be publicly released to unlock the power of modern learning-based trackers and foster new designs in transparent object tracking.



### KRF: Keypoint Refinement with Fusion Network for 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2210.03437v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.03437v1)
- **Published**: 2022-10-07 10:13:30+00:00
- **Updated**: 2022-10-07 10:13:30+00:00
- **Authors**: Irvin Haozhe Zhan, Yiheng Han, Yu-Ping Wang, Long Zeng, Yong-Jin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Existing refinement methods gradually lose their ability to further improve pose estimation methods' accuracy. In this paper, we propose a new refinement pipeline, Keypoint Refinement with Fusion Network (KRF), for 6D pose estimation, especially for objects with serious occlusion. The pipeline consists of two steps. It first completes the input point clouds via a novel point completion network. The network uses both local and global features, considering the pose information during point completion. Then, it registers the completed object point cloud with corresponding target point cloud by Color supported Iterative KeyPoint (CIKP). The CIKP method introduces color information into registration and registers point cloud around each keypoint to increase stability. The KRF pipeline can be integrated with existing popular 6D pose estimation methods, e.g. the full flow bidirectional fusion network, to further improved their pose estimation accuracy. Experiments show that our method outperforms the state-of-the-art method from 93.9\% to 94.4\% on YCB-Video dataset and from 64.4\% to 66.8\% on Occlusion LineMOD dataset. Our source code is available at https://github.com/zhanhz/KRF.



### Key Information Extraction in Purchase Documents using Deep Learning and Rule-based Corrections
- **Arxiv ID**: http://arxiv.org/abs/2210.03453v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03453v1)
- **Published**: 2022-10-07 10:51:38+00:00
- **Updated**: 2022-10-07 10:51:38+00:00
- **Authors**: Roberto Arroyo, Javier Yebes, Elena Mart√≠nez, H√©ctor Corrales, Javier Lorenzo
- **Comment**: Conference on Computational Linguistics (COLING 2022). PAN-DL
  Workshop
- **Journal**: None
- **Summary**: Deep Learning (DL) is dominating the fields of Natural Language Processing (NLP) and Computer Vision (CV) in the recent times. However, DL commonly relies on the availability of large data annotations, so other alternative or complementary pattern-based techniques can help to improve results. In this paper, we build upon Key Information Extraction (KIE) in purchase documents using both DL and rule-based corrections. Our system initially trusts on Optical Character Recognition (OCR) and text understanding based on entity tagging to identify purchase facts of interest (e.g., product codes, descriptions, quantities, or prices). These facts are then linked to a same product group, which is recognized by means of line detection and some grouping heuristics. Once these DL approaches are processed, we contribute several mechanisms consisting of rule-based corrections for improving the baseline DL predictions. We prove the enhancements provided by these rule-based corrections over the baseline DL results in the presented experiments for purchase documents from public and NielsenIQ datasets.



### Flexible Alignment Super-Resolution Network for Multi-Contrast MRI
- **Arxiv ID**: http://arxiv.org/abs/2210.03460v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.03460v2)
- **Published**: 2022-10-07 11:07:20+00:00
- **Updated**: 2023-01-08 06:57:55+00:00
- **Authors**: Yiming Liu, Mengxi Zhang, Weiqin Zhang, Bo Jiang, Bo Hou, Dan Liu, Jie Chen, Heqing Lian
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic resonance imaging plays an essential role in clinical diagnosis by acquiring the structural information of biological tissue. Recently, many multi-contrast MRI super-resolution networks achieve good effects. However, most studies ignore the impact of the inappropriate foreground scale and patch size of multi-contrast MRI, which probably leads to inappropriate feature alignment. To tackle this problem, we propose the Flexible Alignment Super-Resolution Network (FASR-Net) for multi-contrast MRI Super-Resolution. The Flexible Alignment module of FASR-Net consists of two modules for feature alignment. (1) The Single-Multi Pyramid Alignment(S-A) module solves the situation where low-resolution (LR) images and reference (Ref) images have different scales. (2) The Multi-Multi Pyramid Alignment(M-A) module solves the situation where LR and Ref images have the same scale. Besides, we propose the Cross-Hierarchical Progressive Fusion (CHPF) module aiming at fusing the features effectively, further improving the image quality. Compared with other state-of-the-art methods, FASR-net achieves the most competitive results on FastMRI and IXI datasets. Our code will be available at \href{https://github.com/yimingliu123/FASR-Net}{https://github.com/yimingliu123/FASR-Net}.



### FastCLIPstyler: Optimisation-free Text-based Image Style Transfer Using Style Representations
- **Arxiv ID**: http://arxiv.org/abs/2210.03461v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.03461v2)
- **Published**: 2022-10-07 11:16:36+00:00
- **Updated**: 2022-11-14 12:46:31+00:00
- **Authors**: Ananda Padhmanabhan Suresh, Sanjana Jain, Pavit Noinongyao, Ankush Ganguly
- **Comment**: None
- **Journal**: None
- **Summary**: Artistic style transfer is usually performed between two images, a style image and a content image. Recently, a model named CLIPstyler demonstrated that a natural language description of style could replace the necessity of a reference style image. However, their technique requires a lengthy optimisation procedure at run-time for each query, requiring multiple forward and backward passes through a network as well as expensive loss computations. In this work, we create a generalised text-based style transfer network capable of stylising images in a single forward pass for an arbitrary text input making the image stylisation process around 1000 times more efficient than CLIPstyler. We also demonstrate how our technique eliminates the issue of leakage of unwanted artefacts into some of the generated images from CLIPstyler, making them unusable. We also propose an optional fine-tuning step to improve the quality of the generated image. We qualitatively evaluate the performance of our framework and show that it can generate images of comparable quality to state-of-the-art techniques.



### IDa-Det: An Information Discrepancy-aware Distillation for 1-bit Detectors
- **Arxiv ID**: http://arxiv.org/abs/2210.03477v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03477v1)
- **Published**: 2022-10-07 12:04:14+00:00
- **Updated**: 2022-10-07 12:04:14+00:00
- **Authors**: Sheng Xu, Yanjing Li, Bohan Zeng, Teli ma, Baochang Zhang, Xianbin Cao, Peng Gao, Jinhu Lv
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge distillation (KD) has been proven to be useful for training compact object detection models. However, we observe that KD is often effective when the teacher model and student counterpart share similar proposal information. This explains why existing KD methods are less effective for 1-bit detectors, caused by a significant information discrepancy between the real-valued teacher and the 1-bit student. This paper presents an Information Discrepancy-aware strategy (IDa-Det) to distill 1-bit detectors that can effectively eliminate information discrepancies and significantly reduce the performance gap between a 1-bit detector and its real-valued counterpart. We formulate the distillation process as a bi-level optimization formulation. At the inner level, we select the representative proposals with maximum information discrepancy. We then introduce a novel entropy distillation loss to reduce the disparity based on the selected proposals. Extensive experiments demonstrate IDa-Det's superiority over state-of-the-art 1-bit detectors and KD methods on both PASCAL VOC and COCO datasets. IDa-Det achieves a 76.9% mAP for a 1-bit Faster-RCNN with ResNet-18 backbone. Our code is open-sourced on https://github.com/SteveTsui/IDa-Det.



### Neighbor Regularized Bayesian Optimization for Hyperparameter Optimization
- **Arxiv ID**: http://arxiv.org/abs/2210.03481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03481v1)
- **Published**: 2022-10-07 12:08:01+00:00
- **Updated**: 2022-10-07 12:08:01+00:00
- **Authors**: Lei Cui, Yangguang Li, Xin Lu, Dong An, Fenggang Liu
- **Comment**: Accepted by BMVC 2022
- **Journal**: None
- **Summary**: Bayesian Optimization (BO) is a common solution to search optimal hyperparameters based on sample observations of a machine learning model. Existing BO algorithms could converge slowly even collapse when the potential observation noise misdirects the optimization. In this paper, we propose a novel BO algorithm called Neighbor Regularized Bayesian Optimization (NRBO) to solve the problem. We first propose a neighbor-based regularization to smooth each sample observation, which could reduce the observation noise efficiently without any extra training cost. Since the neighbor regularization highly depends on the sample density of a neighbor area, we further design a density-based acquisition function to adjust the acquisition reward and obtain more stable statistics. In addition, we design a adjustment mechanism to ensure the framework maintains a reasonable regularization strength and density reward conditioned on remaining computation resources. We conduct experiments on the bayesmark benchmark and important computer vision benchmarks such as ImageNet and COCO. Extensive experiments demonstrate the effectiveness of NRBO and it consistently outperforms other state-of-the-art methods.



### CLAD: A realistic Continual Learning benchmark for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2210.03482v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.03482v1)
- **Published**: 2022-10-07 12:08:25+00:00
- **Updated**: 2022-10-07 12:08:25+00:00
- **Authors**: Eli Verwimp, Kuo Yang, Sarah Parisot, Hong Lanqing, Steven McDonagh, Eduardo P√©rez-Pellitero, Matthias De Lange, Tinne Tuytelaars
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we describe the design and the ideas motivating a new Continual Learning benchmark for Autonomous Driving (CLAD), that focuses on the problems of object classification and object detection. The benchmark utilises SODA10M, a recently released large-scale dataset that concerns autonomous driving related problems. First, we review and discuss existing continual learning benchmarks, how they are related, and show that most are extreme cases of continual learning. To this end, we survey the benchmarks used in continual learning papers at three highly ranked computer vision conferences. Next, we introduce CLAD-C, an online classification benchmark realised through a chronological data stream that poses both class and domain incremental challenges; and CLAD-D, a domain incremental continual object detection benchmark. We examine the inherent difficulties and challenges posed by the benchmark, through a survey of the techniques and methods used by the top-3 participants in a CLAD-challenge workshop at ICCV 2021. We conclude with possible pathways to improve the current continual learning state of the art, and which directions we deem promising for future research.



### De-risking geological carbon storage from high resolution time-lapse seismic to explainable leakage detection
- **Arxiv ID**: http://arxiv.org/abs/2211.03527v1
- **DOI**: None
- **Categories**: **physics.geo-ph**, cs.CV, cs.NA, eess.IV, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2211.03527v1)
- **Published**: 2022-10-07 12:27:18+00:00
- **Updated**: 2022-10-07 12:27:18+00:00
- **Authors**: Ziyi Yin, Huseyin Tuna Erdinc, Abhinav Prakash Gahlot, Mathias Louboutin, Felix J. Herrmann
- **Comment**: None
- **Journal**: None
- **Summary**: Geological carbon storage represents one of the few truly scalable technologies capable of reducing the CO2 concentration in the atmosphere. While this technology has the potential to scale, its success hinges on our ability to mitigate its risks. An important aspect of risk mitigation concerns assurances that the injected CO2 remains within the storage complex. Amongst the different monitoring modalities, seismic imaging stands out with its ability to attain high resolution and high fidelity images. However, these superior features come, unfortunately, at prohibitive costs and time-intensive efforts potentially rendering extensive seismic monitoring undesirable. To overcome this shortcoming, we present a methodology where time-lapse images are created by inverting non-replicated time-lapse monitoring data jointly. By no longer insisting on replication of the surveys to obtain high fidelity time-lapse images and differences, extreme costs and time-consuming labor are averted. To demonstrate our approach, hundreds of noisy time-lapse seismic datasets are simulated that contain imprints of regular CO2 plumes and irregular plumes that leak. These time-lapse datasets are subsequently inverted to produce time-lapse difference images used to train a deep neural classifier. The testing results show that the classifier is capable of detecting CO2 leakage automatically on unseen data and with a reasonable accuracy.



### Learning to Learn and Sample BRDFs
- **Arxiv ID**: http://arxiv.org/abs/2210.03510v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.03510v2)
- **Published**: 2022-10-07 12:55:24+00:00
- **Updated**: 2023-05-04 14:42:04+00:00
- **Authors**: Chen Liu, Michael Fischer, Tobias Ritschel
- **Comment**: Accepted to Eurographics 2023; Project Page at
  https://ryushinn.github.io/metasampling
- **Journal**: None
- **Summary**: We propose a method to accelerate the joint process of physically acquiring and learning neural Bi-directional Reflectance Distribution Function (BRDF) models. While BRDF learning alone can be accelerated by meta-learning, acquisition remains slow as it relies on a mechanical process. We show that meta-learning can be extended to optimize the physical sampling pattern, too. After our method has been meta-trained for a set of fully-sampled BRDFs, it is able to quickly train on new BRDFs with up to five orders of magnitude fewer physical acquisition samples at similar quality. Our approach also extends to other linear and non-linear BRDF models, which we show in an extensive evaluation.



### Simulating single-photon detector array sensors for depth imaging
- **Arxiv ID**: http://arxiv.org/abs/2210.05644v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2210.05644v1)
- **Published**: 2022-10-07 13:23:34+00:00
- **Updated**: 2022-10-07 13:23:34+00:00
- **Authors**: Stirling Scholes, Germ√°n Mora-Mart√≠n, Feng Zhu, Istvan Gyongy, Phil Soan, Jonathan Leach
- **Comment**: None
- **Journal**: None
- **Summary**: Single-Photon Avalanche Detector (SPAD) arrays are a rapidly emerging technology. These multi-pixel sensors have single-photon sensitivities and pico-second temporal resolutions thus they can rapidly generate depth images with millimeter precision. Such sensors are a key enabling technology for future autonomous systems as they provide guidance and situational awareness. However, to fully exploit the capabilities of SPAD array sensors, it is crucial to establish the quality of depth images they are able to generate in a wide range of scenarios. Given a particular optical system and a finite image acquisition time, what is the best-case depth resolution and what are realistic images generated by SPAD arrays? In this work, we establish a robust yet simple numerical procedure that rapidly establishes the fundamental limits to depth imaging with SPAD arrays under real world conditions. Our approach accurately generates realistic depth images in a wide range of scenarios, allowing the performance of an optical depth imaging system to be established without the need for costly and laborious field testing. This procedure has applications in object detection and tracking for autonomous systems and could be easily extended to systems for underwater imaging or for imaging around corners.



### A2: Efficient Automated Attacker for Boosting Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2210.03543v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03543v2)
- **Published**: 2022-10-07 13:28:00+00:00
- **Updated**: 2022-10-17 02:24:12+00:00
- **Authors**: Zhuoer Xu, Guanghui Zhu, Changhua Meng, Shiwen Cui, Zhenzhe Ying, Weiqiang Wang, Ming GU, Yihua Huang
- **Comment**: Accepted by NeurIPS2022
- **Journal**: None
- **Summary**: Based on the significant improvement of model robustness by AT (Adversarial Training), various variants have been proposed to further boost the performance. Well-recognized methods have focused on different components of AT (e.g., designing loss functions and leveraging additional unlabeled data). It is generally accepted that stronger perturbations yield more robust models. However, how to generate stronger perturbations efficiently is still missed. In this paper, we propose an efficient automated attacker called A2 to boost AT by generating the optimal perturbations on-the-fly during training. A2 is a parameterized automated attacker to search in the attacker space for the best attacker against the defense model and examples. Extensive experiments across different datasets demonstrate that A2 generates stronger perturbations with low extra cost and reliably improves the robustness of various AT methods against different attacks.



### Time-Space Transformers for Video Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.03546v1
- **DOI**: 10.1109/WACV51458.2022.00270
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03546v1)
- **Published**: 2022-10-07 13:30:11+00:00
- **Updated**: 2022-10-07 13:30:11+00:00
- **Authors**: Andra Petrovai, Sergiu Nedevschi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel solution for the task of video panoptic segmentation, that simultaneously predicts pixel-level semantic and instance segmentation and generates clip-level instance tracks. Our network, named VPS-Transformer, with a hybrid architecture based on the state-of-the-art panoptic segmentation network Panoptic-DeepLab, combines a convolutional architecture for single-frame panoptic segmentation and a novel video module based on an instantiation of the pure Transformer block. The Transformer, equipped with attention mechanisms, models spatio-temporal relations between backbone output features of current and past frames for more accurate and consistent panoptic estimates. As the pure Transformer block introduces large computation overhead when processing high resolution images, we propose a few design changes for a more efficient compute. We study how to aggregate information more effectively over the space-time volume and we compare several variants of the Transformer block with different attention schemes. Extensive experiments on the Cityscapes-VPS dataset demonstrate that our best model improves the temporal consistency and video panoptic quality by a margin of 2.2%, with little extra computation.



### Instance Segmentation of Dense and Overlapping Objects via Layering
- **Arxiv ID**: http://arxiv.org/abs/2210.03551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03551v1)
- **Published**: 2022-10-07 13:37:56+00:00
- **Updated**: 2022-10-07 13:37:56+00:00
- **Authors**: Long Chen, Yuli Wu, Dorit Merhof
- **Comment**: None
- **Journal**: None
- **Summary**: Instance segmentation aims to delineate each individual object of interest in an image. State-of-the-art approaches achieve this goal by either partitioning semantic segmentations or refining coarse representations of detected objects. In this work, we propose a novel approach to solve the problem via object layering, i.e. by distributing crowded, even overlapping objects into different layers. By grouping spatially separated objects in the same layer, instances can be effortlessly isolated by extracting connected components in each layer. In comparison to previous methods, our approach is not affected by complex object shapes or object overlaps. With minimal post-processing, our method yields very competitive results on a diverse line of datasets: C. elegans (BBBC), Overlapping Cervical Cells (OCC) and cultured neuroblastoma cells (CCDB). The source code is publicly available.



### A deep learning approach for detection and localization of leaf anomalies
- **Arxiv ID**: http://arxiv.org/abs/2210.03558v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68T10, 68T45, 68U10, I.2.5; I.2.6; I.2.10; I.3.6; I.3.8; I.4.2; I.4.5; I.4.9; I.5.0;
  I.5.4; J.2; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2210.03558v1)
- **Published**: 2022-10-07 13:45:18+00:00
- **Updated**: 2022-10-07 13:45:18+00:00
- **Authors**: Davide Calabr√≤, Massimiliano Lupo Pasini, Nicola Ferro, Simona Perotto
- **Comment**: 23 pages, 8 figures
- **Journal**: None
- **Summary**: The detection and localization of possible diseases in crops are usually automated by resorting to supervised deep learning approaches. In this work, we tackle these goals with unsupervised models, by applying three different types of autoencoders to a specific open-source dataset of healthy and unhealthy pepper and cherry leaf images. CAE, CVAE and VQ-VAE autoencoders are deployed to screen unlabeled images of such a dataset, and compared in terms of image reconstruction, anomaly removal, detection and localization. The vector-quantized variational architecture turns out to be the best performing one with respect to all these targets.



### Automated segmentation and morphological characterization of placental histology images based on a single labeled image
- **Arxiv ID**: http://arxiv.org/abs/2210.03566v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.03566v1)
- **Published**: 2022-10-07 14:00:10+00:00
- **Updated**: 2022-10-07 14:00:10+00:00
- **Authors**: Arash Rabbani, Masoud Babaei, Masoumeh Gharib
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, a novel method of data augmentation has been presented for the segmentation of placental histological images when the labeled data are scarce. This method generates new realizations of the placenta intervillous morphology while maintaining the general textures and orientations. As a result, a diversified artificial dataset of images is generated that can be used for training deep learning segmentation models. We have observed that on average the presented method of data augmentation led to a 42% decrease in the binary cross-entropy loss of the validation dataset compared to the common approach in the literature. Additionally, the morphology of the intervillous space is studied under the effect of the proposed image reconstruction technique, and the diversity of the artificially generated population is quantified. Due to the high resemblance of the generated images to the real ones, the applications of the proposed method may not be limited to placental histological images, and it is recommended that other types of tissues be investigated in future studies.



### AI-Driven Road Maintenance Inspection v2: Reducing Data Dependency & Quantifying Road Damage
- **Arxiv ID**: http://arxiv.org/abs/2210.03570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03570v1)
- **Published**: 2022-10-07 14:11:27+00:00
- **Updated**: 2022-10-07 14:11:27+00:00
- **Authors**: Haris Iqbal, Hemang Chawla, Arnav Varma, Terence Brouns, Ahmed Badar, Elahe Arani, Bahram Zonooz
- **Comment**: Accepted at IRF Global R2T Conference & Exhibition 2022
- **Journal**: None
- **Summary**: Road infrastructure maintenance inspection is typically a labor-intensive and critical task to ensure the safety of all road users. Existing state-of-the-art techniques in Artificial Intelligence (AI) for object detection and segmentation help automate a huge chunk of this task given adequate annotated data. However, annotating videos from scratch is cost-prohibitive. For instance, it can take an annotator several days to annotate a 5-minute video recorded at 30 FPS. Hence, we propose an automated labelling pipeline by leveraging techniques like few-shot learning and out-of-distribution detection to generate labels for road damage detection. In addition, our pipeline includes a risk factor assessment for each damage by instance quantification to prioritize locations for repairs which can lead to optimal deployment of road maintenance machinery. We show that the AI models trained with these techniques can not only generalize better to unseen real-world data with reduced requirement for human annotation but also provide an estimate of maintenance urgency, thereby leading to safer roads.



### An Investigation into Whitening Loss for Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.03586v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.03586v1)
- **Published**: 2022-10-07 14:43:29+00:00
- **Updated**: 2022-10-07 14:43:29+00:00
- **Authors**: Xi Weng, Lei Huang, Lei Zhao, Rao Muhammad Anwer, Salman Khan, Fahad Shahbaz Khan
- **Comment**: Accepted at NeurIPS 2022. The Code is available at:
  https://github.com/winci-ai/CW-RGP
- **Journal**: None
- **Summary**: A desirable objective in self-supervised learning (SSL) is to avoid feature collapse. Whitening loss guarantees collapse avoidance by minimizing the distance between embeddings of positive pairs under the conditioning that the embeddings from different views are whitened. In this paper, we propose a framework with an informative indicator to analyze whitening loss, which provides a clue to demystify several interesting phenomena as well as a pivoting point connecting to other SSL methods. We reveal that batch whitening (BW) based methods do not impose whitening constraints on the embedding, but they only require the embedding to be full-rank. This full-rank constraint is also sufficient to avoid dimensional collapse. Based on our analysis, we propose channel whitening with random group partition (CW-RGP), which exploits the advantages of BW-based methods in preventing collapse and avoids their disadvantages requiring large batch size. Experimental results on ImageNet classification and COCO object detection reveal that the proposed CW-RGP possesses a promising potential for learning good representations. The code is available at https://github.com/winci-ai/CW-RGP.



### Modeling Inter-Class and Intra-Class Constraints in Novel Class Discovery
- **Arxiv ID**: http://arxiv.org/abs/2210.03591v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03591v3)
- **Published**: 2022-10-07 14:46:32+00:00
- **Updated**: 2023-03-23 13:15:27+00:00
- **Authors**: Wenbin Li, Zhichen Fan, Jing Huo, Yang Gao
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: Novel class discovery (NCD) aims at learning a model that transfers the common knowledge from a class-disjoint labelled dataset to another unlabelled dataset and discovers new classes (clusters) within it. Many methods, as well as elaborate training pipelines and appropriate objectives, have been proposed and considerably boosted performance on NCD tasks. Despite all this, we find that the existing methods do not sufficiently take advantage of the essence of the NCD setting. To this end, in this paper, we propose to model both inter-class and intra-class constraints in NCD based on the symmetric Kullback-Leibler divergence (sKLD). Specifically, we propose an inter-class sKLD constraint to effectively exploit the disjoint relationship between labelled and unlabelled classes, enforcing the separability for different classes in the embedding space. In addition, we present an intra-class sKLD constraint to explicitly constrain the intra-relationship between a sample and its augmentations and ensure the stability of the training process at the same time. We conduct extensive experiments on the popular CIFAR10, CIFAR100 and ImageNet benchmarks and successfully demonstrate that our method can establish a new state of the art and can achieve significant performance improvements, e.g., 3.5%/3.7% clustering accuracy improvements on CIFAR100-50 dataset split under the task-aware/-agnostic evaluation protocol, over previous state-of-the-art methods. Code is available at https://github.com/FanZhichen/NCD-IIC.



### Specialized Re-Ranking: A Novel Retrieval-Verification Framework for Cloth Changing Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2210.03592v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03592v1)
- **Published**: 2022-10-07 14:47:28+00:00
- **Updated**: 2022-10-07 14:47:28+00:00
- **Authors**: Renjie Zhang, Yu Fang, Huaxin Song, Fangbin Wan, Yanwei Fu, Hirokazu Kato, Yang Wu
- **Comment**: Accepted by Pattern Recognition
- **Journal**: None
- **Summary**: Cloth changing person re-identification(Re-ID) can work under more complicated scenarios with higher security than normal Re-ID and biometric techniques and is therefore extremely valuable in applications. Meanwhile, higher flexibility in appearance always leads to more similar-looking confusing images, which is the weakness of the widely used retrieval methods. In this work, we shed light on how to handle these similar images. Specifically, we propose a novel retrieval-verification framework. Given an image, the retrieval module can search for similar images quickly. Our proposed verification network will then compare the input image and the candidate images by contrasting those local details and give a similarity score. An innovative ranking strategy is also introduced to take a good balance between retrieval and verification results. Comprehensive experiments are conducted to show the effectiveness of our framework and its capability in improving the state-of-the-art methods remarkably on both synthetic and realistic datasets.



### BlanketSet -- A clinical real-world in-bed action recognition and qualitative semi-synchronised MoCap dataset
- **Arxiv ID**: http://arxiv.org/abs/2210.03600v3
- **DOI**: 10.1109/ENBENG58165.2023.10175335
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2210.03600v3)
- **Published**: 2022-10-07 14:58:27+00:00
- **Updated**: 2023-03-19 19:13:23+00:00
- **Authors**: Jo√£o Carmona, Tam√°s Kar√°csony, Jo√£o Paulo Silva Cunha
- **Comment**: 4 pages, Dataset available at:
  https://rdm.inesctec.pt/dataset/nis-2022-004
- **Journal**: 2023 IEEE 7th Portuguese Meeting on Bioengineering (ENBENG),
  Porto, Portugal, 2023, pp. 116-119
- **Summary**: Clinical in-bed video-based human motion analysis is a very relevant computer vision topic for several relevant biomedical applications. Nevertheless, the main public large datasets (e.g. ImageNet or 3DPW) used for deep learning approaches lack annotated examples for these clinical scenarios. To address this issue, we introduce BlanketSet, an RGB-IR-D action recognition dataset of sequences performed in a hospital bed. This dataset has the potential to help bridge the improvements attained in more general large datasets to these clinical scenarios. Information on how to access the dataset is available at https://rdm.inesctec.pt/dataset/nis-2022-004.



### 1st ICLR International Workshop on Privacy, Accountability, Interpretability, Robustness, Reasoning on Structured Data (PAIR^2Struct)
- **Arxiv ID**: http://arxiv.org/abs/2210.03612v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.03612v1)
- **Published**: 2022-10-07 15:12:03+00:00
- **Updated**: 2022-10-07 15:12:03+00:00
- **Authors**: Hao Wang, Wanyu Lin, Hao He, Di Wang, Chengzhi Mao, Muhan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have seen advances on principles and guidance relating to accountable and ethical use of artificial intelligence (AI) spring up around the globe. Specifically, Data Privacy, Accountability, Interpretability, Robustness, and Reasoning have been broadly recognized as fundamental principles of using machine learning (ML) technologies on decision-critical and/or privacy-sensitive applications. On the other hand, in tremendous real-world applications, data itself can be well represented as various structured formalisms, such as graph-structured data (e.g., networks), grid-structured data (e.g., images), sequential data (e.g., text), etc. By exploiting the inherently structured knowledge, one can design plausible approaches to identify and use more relevant variables to make reliable decisions, thereby facilitating real-world deployments.



### C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2210.03625v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.03625v2)
- **Published**: 2022-10-07 15:30:24+00:00
- **Updated**: 2023-05-09 19:58:59+00:00
- **Authors**: Andrew Rouditchenko, Yung-Sung Chuang, Nina Shvetsova, Samuel Thomas, Rogerio Feris, Brian Kingsbury, Leonid Karlinsky, David Harwath, Hilde Kuehne, James Glass
- **Comment**: Accepted at ICASSP 2023. The code, models, and dataset are available
  at https://github.com/roudimit/c2kd
- **Journal**: None
- **Summary**: Multilingual text-video retrieval methods have improved significantly in recent years, but the performance for other languages lags behind English. We propose a Cross-Lingual Cross-Modal Knowledge Distillation method to improve multilingual text-video retrieval. Inspired by the fact that English text-video retrieval outperforms other languages, we train a student model using input text in different languages to match the cross-modal predictions from teacher models using input text in English. We propose a cross entropy based objective which forces the distribution over the student's text-video similarity scores to be similar to those of the teacher models. We introduce a new multilingual video dataset, Multi-YouCook2, by translating the English captions in the YouCook2 video dataset to 8 other languages. Our method improves multilingual text-video retrieval performance on Multi-YouCook2 and several other datasets such as Multi-MSRVTT and VATEX. We also conducted an analysis on the effectiveness of different multilingual text models as teachers. The code, models, and dataset are available at https://github.com/roudimit/c2kd.



### Pose Guided Human Image Synthesis with Partially Decoupled GAN
- **Arxiv ID**: http://arxiv.org/abs/2210.03627v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.03627v1)
- **Published**: 2022-10-07 15:31:37+00:00
- **Updated**: 2022-10-07 15:31:37+00:00
- **Authors**: Jianhan Wu, Jianzong Wang, Shijing Si, Xiaoyang Qu, Jing Xiao
- **Comment**: 16 pages, 14th Asian Conference on Machine Learning conference
- **Journal**: None
- **Summary**: Pose Guided Human Image Synthesis (PGHIS) is a challenging task of transforming a human image from the reference pose to a target pose while preserving its style. Most existing methods encode the texture of the whole reference human image into a latent space, and then utilize a decoder to synthesize the image texture of the target pose. However, it is difficult to recover the detailed texture of the whole human image. To alleviate this problem, we propose a method by decoupling the human body into several parts (\eg, hair, face, hands, feet, \etc) and then using each of these parts to guide the synthesis of a realistic image of the person, which preserves the detailed information of the generated images. In addition, we design a multi-head attention-based module for PGHIS. Because most convolutional neural network-based methods have difficulty in modeling long-range dependency due to the convolutional operation, the long-range modeling capability of attention mechanism is more suitable than convolutional neural networks for pose transfer task, especially for sharp pose deformation. Extensive experiments on Market-1501 and DeepFashion datasets reveal that our method almost outperforms other existing state-of-the-art methods in terms of both qualitative and quantitative metrics.



### Leveraging Structure from Motion to Localize Inaccessible Bus Stops
- **Arxiv ID**: http://arxiv.org/abs/2210.03646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03646v1)
- **Published**: 2022-10-07 15:55:34+00:00
- **Updated**: 2022-10-07 15:55:34+00:00
- **Authors**: Indu Panigrahi, Tom Bu, Christoph Mertz
- **Comment**: None
- **Journal**: None
- **Summary**: The detection of hazardous conditions near public transit stations is necessary for ensuring the safety and accessibility of public transit. Smart city infrastructures aim to facilitate this task among many others through the use of computer vision. However, most state-of-the-art computer vision models require thousands of images in order to perform accurate detection, and there exist few images of hazardous conditions as they are generally rare. In this paper, we examine the detection of snow-covered sidewalks along bus routes. Previous work has focused on detecting other vehicles in heavy snowfall or simply detecting the presence of snow. However, our application has an added complication of determining if the snow covers areas of importance and can cause falls or other accidents (e.g. snow covering a sidewalk) or simply covers some background area (e.g. snow on a neighboring field). This problem involves localizing the positions of the areas of importance when they are not necessarily visible.   We introduce a method that utilizes Structure from Motion (SfM) rather than additional annotated data to address this issue. Specifically, our method learns the locations of sidewalks in a given scene by applying a segmentation model and SfM to images from bus cameras during clear weather. Then, we use the learned locations to detect if and where the sidewalks become obscured with snow. After evaluating across various threshold parameters, we identify an optimal range at which our method consistently classifies different categories of sidewalk images correctly. Although we demonstrate an application for snow coverage along bus routes, this method can extend to other hazardous conditions as well. Code for this project is available at https://github.com/ind1010/SfM_for_BusEdge.



### Understanding the Covariance Structure of Convolutional Filters
- **Arxiv ID**: http://arxiv.org/abs/2210.03651v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.03651v1)
- **Published**: 2022-10-07 15:59:13+00:00
- **Updated**: 2022-10-07 15:59:13+00:00
- **Authors**: Asher Trockman, Devin Willmott, J. Zico Kolter
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network weights are typically initialized at random from univariate distributions, controlling just the variance of individual weights even in highly-structured operations like convolutions. Recent ViT-inspired convolutional networks such as ConvMixer and ConvNeXt use large-kernel depthwise convolutions whose learned filters have notable structure; this presents an opportunity to study their empirical covariances. In this work, we first observe that such learned filters have highly-structured covariance matrices, and moreover, we find that covariances calculated from small networks may be used to effectively initialize a variety of larger networks of different depths, widths, patch sizes, and kernel sizes, indicating a degree of model-independence to the covariance structure. Motivated by these findings, we then propose a learning-free multivariate initialization scheme for convolutional filters using a simple, closed-form construction of their covariance. Models using our initialization outperform those using traditional univariate initializations, and typically meet or exceed the performance of those initialized from the covariances of learned filters; in some cases, this improvement can be achieved without training the depthwise convolutional filters at all.



### Spatio-temporal Tendency Reasoning for Human Body Pose and Shape Estimation from Videos
- **Arxiv ID**: http://arxiv.org/abs/2210.03659v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.03659v2)
- **Published**: 2022-10-07 16:09:07+00:00
- **Updated**: 2022-10-10 03:24:48+00:00
- **Authors**: Boyang Zhang, SuPing Wu, Hu Cao, Kehua Ma, Pan Li, Lei Lin
- **Comment**: Accepted by BMVC2022
- **Journal**: None
- **Summary**: In this paper, we present a spatio-temporal tendency reasoning (STR) network for recovering human body pose and shape from videos. Previous approaches have focused on how to extend 3D human datasets and temporal-based learning to promote accuracy and temporal smoothing. Different from them, our STR aims to learn accurate and natural motion sequences in an unconstrained environment through temporal and spatial tendency and to fully excavate the spatio-temporal features of existing video data. To this end, our STR learns the representation of features in the temporal and spatial dimensions respectively, to concentrate on a more robust representation of spatio-temporal features. More specifically, for efficient temporal modeling, we first propose a temporal tendency reasoning (TTR) module. TTR constructs a time-dimensional hierarchical residual connection representation within a video sequence to effectively reason temporal sequences' tendencies and retain effective dissemination of human information. Meanwhile, for enhancing the spatial representation, we design a spatial tendency enhancing (STE) module to further learns to excite spatially time-frequency domain sensitive features in human motion information representations. Finally, we introduce integration strategies to integrate and refine the spatio-temporal feature representations. Extensive experimental findings on large-scale publically available datasets reveal that our STR remains competitive with the state-of-the-art on three datasets. Our code are available at https://github.com/Changboyang/STR.git.



### Bi-directional Weakly Supervised Knowledge Distillation for Whole Slide Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2210.03664v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03664v2)
- **Published**: 2022-10-07 16:12:04+00:00
- **Updated**: 2022-10-10 10:13:06+00:00
- **Authors**: Linhao Qu, Xiaoyuan Luo, Manning Wang, Zhijian Song
- **Comment**: Accepted by NeurIPS 2022
- **Journal**: None
- **Summary**: Computer-aided pathology diagnosis based on the classification of Whole Slide Image (WSI) plays an important role in clinical practice, and it is often formulated as a weakly-supervised Multiple Instance Learning (MIL) problem. Existing methods solve this problem from either a bag classification or an instance classification perspective. In this paper, we propose an end-to-end weakly supervised knowledge distillation framework (WENO) for WSI classification, which integrates a bag classifier and an instance classifier in a knowledge distillation framework to mutually improve the performance of both classifiers. Specifically, an attention-based bag classifier is used as the teacher network, which is trained with weak bag labels, and an instance classifier is used as the student network, which is trained using the normalized attention scores obtained from the teacher network as soft pseudo labels for the instances in positive bags. An instance feature extractor is shared between the teacher and the student to further enhance the knowledge exchange between them. In addition, we propose a hard positive instance mining strategy based on the output of the student network to force the teacher network to keep mining hard positive instances. WENO is a plug-and-play framework that can be easily applied to any existing attention-based bag classification methods. Extensive experiments on five datasets demonstrate the efficiency of WENO. Code is available at https://github.com/miccaiif/WENO.



### IronDepth: Iterative Refinement of Single-View Depth using Surface Normal and its Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2210.03676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03676v1)
- **Published**: 2022-10-07 16:34:20+00:00
- **Updated**: 2022-10-07 16:34:20+00:00
- **Authors**: Gwangbin Bae, Ignas Budvytis, Roberto Cipolla
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Single image surface normal estimation and depth estimation are closely related problems as the former can be calculated from the latter. However, the surface normals computed from the output of depth estimation methods are significantly less accurate than the surface normals directly estimated by networks. To reduce such discrepancy, we introduce a novel framework that uses surface normal and its uncertainty to recurrently refine the predicted depth-map. The depth of each pixel can be propagated to a query pixel, using the predicted surface normal as guidance. We thus formulate depth refinement as a classification of choosing the neighboring pixel to propagate from. Then, by propagating to sub-pixel points, we upsample the refined, low-resolution output. The proposed method shows state-of-the-art performance on NYUv2 and iBims-1 - both in terms of depth and normal. Our refinement module can also be attached to the existing depth estimation methods to improve their accuracy. We also show that our framework, only trained for depth estimation, can also be used for depth completion. The code is available at https://github.com/baegwangbin/IronDepth.



### Quantitative Metrics for Evaluating Explanations of Video DeepFake Detectors
- **Arxiv ID**: http://arxiv.org/abs/2210.03683v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.03683v1)
- **Published**: 2022-10-07 16:41:46+00:00
- **Updated**: 2022-10-07 16:41:46+00:00
- **Authors**: Federico Baldassarre, Quentin Debard, Gonzalo Fiz Pontiveros, Tri Kurniawan Wijaya
- **Comment**: Accepted at BMVC 2022, code repository at
  https://github.com/baldassarreFe/deepfake-detection
- **Journal**: None
- **Summary**: The proliferation of DeepFake technology is a rising challenge in today's society, owing to more powerful and accessible generation methods. To counter this, the research community has developed detectors of ever-increasing accuracy. However, the ability to explain the decisions of such models to users is lacking behind and is considered an accessory in large-scale benchmarks, despite being a crucial requirement for the correct deployment of automated tools for content moderation. We attribute the issue to the reliance on qualitative comparisons and the lack of established metrics. We describe a simple set of metrics to evaluate the visual quality and informativeness of explanations of video DeepFake classifiers from a human-centric perspective. With these metrics, we compare common approaches to improve explanation quality and discuss their effect on both classification and explanation performance on the recent DFDC and DFD datasets.



### Humans need not label more humans: Occlusion Copy & Paste for Occluded Human Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.03686v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03686v1)
- **Published**: 2022-10-07 16:44:05+00:00
- **Updated**: 2022-10-07 16:44:05+00:00
- **Authors**: Evan Ling, Dezhao Huang, Minhoe Hur
- **Comment**: 13 pages, 5 figures, BMVC 2022
- **Journal**: None
- **Summary**: Modern object detection and instance segmentation networks stumble when picking out humans in crowded or highly occluded scenes. Yet, these are often scenarios where we require our detectors to work well. Many works have approached this problem with model-centric improvements. While they have been shown to work to some extent, these supervised methods still need sufficient relevant examples (i.e. occluded humans) during training for the improvements to be maximised. In our work, we propose a simple yet effective data-centric approach, Occlusion Copy & Paste, to introduce occluded examples to models during training - we tailor the general copy & paste augmentation approach to tackle the difficult problem of same-class occlusion. It improves instance segmentation performance on occluded scenarios for "free" just by leveraging on existing large-scale datasets, without additional data or manual labelling needed. In a principled study, we show whether various proposed add-ons to the copy & paste augmentation indeed contribute to better performance. Our Occlusion Copy & Paste augmentation is easily interoperable with any models: by simply applying it to a recent generic instance segmentation model without explicit model architectural design to tackle occlusion, we achieve state-of-the-art instance segmentation performance on the very challenging OCHuman dataset. Source code is available at https://github.com/levan92/occlusion-copy-paste.



### GENHOP: An Image Generation Method Based on Successive Subspace Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.03689v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.03689v1)
- **Published**: 2022-10-07 16:51:24+00:00
- **Updated**: 2022-10-07 16:51:24+00:00
- **Authors**: Xuejing Lei, Wei Wang, C. -C. Jay Kuo
- **Comment**: 10 pages, 5 figures, accepted by ISCAS 2022
- **Journal**: None
- **Summary**: Being different from deep-learning-based (DL-based) image generation methods, a new image generative model built upon successive subspace learning principle is proposed and named GenHop (an acronym of Generative PixelHop) in this work. GenHop consists of three modules: 1) high-to-low dimension reduction, 2) seed image generation, and 3) low-to-high dimension expansion. In the first module, it builds a sequence of high-to-low dimensional subspaces through a sequence of whitening processes, each of which contains samples of joint-spatial-spectral representation. In the second module, it generates samples in the lowest dimensional subspace. In the third module, it finds a proper high-dimensional sample for a seed image by adding details back via locally linear embedding (LLE) and a sequence of coloring processes. Experiments show that GenHop can generate visually pleasant images whose FID scores are comparable or even better than those of DL-based generative models for MNIST, Fashion-MNIST and CelebA datasets.



### Compressing Video Calls using Synthetic Talking Heads
- **Arxiv ID**: http://arxiv.org/abs/2210.03692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03692v1)
- **Published**: 2022-10-07 16:52:40+00:00
- **Updated**: 2022-10-07 16:52:40+00:00
- **Authors**: Madhav Agarwal, Anchit Gupta, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, C V Jawahar
- **Comment**: British Machine Vision Conference (BMVC), 2022
- **Journal**: None
- **Summary**: We leverage the modern advancements in talking head generation to propose an end-to-end system for talking head video compression. Our algorithm transmits pivot frames intermittently while the rest of the talking head video is generated by animating them. We use a state-of-the-art face reenactment network to detect key points in the non-pivot frames and transmit them to the receiver. A dense flow is then calculated to warp a pivot frame to reconstruct the non-pivot ones. Transmitting key points instead of full frames leads to significant compression. We propose a novel algorithm to adaptively select the best-suited pivot frames at regular intervals to provide a smooth experience. We also propose a frame-interpolater at the receiver's end to improve the compression levels further. Finally, a face enhancement network improves reconstruction quality, significantly improving several aspects like the sharpness of the generations. We evaluate our method both qualitatively and quantitatively on benchmark datasets and compare it with multiple compression techniques. We release a demo video and additional information at https://cvit.iiit.ac.in/research/projects/cvit-projects/talking-video-compression.



### Multi-Frequency-Aware Patch Adversarial Learning for Neural Point Cloud Rendering
- **Arxiv ID**: http://arxiv.org/abs/2210.03693v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.03693v1)
- **Published**: 2022-10-07 16:54:15+00:00
- **Updated**: 2022-10-07 16:54:15+00:00
- **Authors**: Jay Karhade, Haiyue Zhu, Ka-Shing Chung, Rajesh Tripathy, Wei Lin, Marcelo H. Ang Jr
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: We present a neural point cloud rendering pipeline through a novel multi-frequency-aware patch adversarial learning framework. The proposed approach aims to improve the rendering realness by minimizing the spectrum discrepancy between real and synthesized images, especially on the high-frequency localized sharpness information which causes image blur visually. Specifically, a patch multi-discriminator scheme is proposed for the adversarial learning, which combines both spectral domain (Fourier Transform and Discrete Wavelet Transform) discriminators as well as the spatial (RGB) domain discriminator to force the generator to capture global and local spectral distributions of the real images. The proposed multi-discriminator scheme not only helps to improve rendering realness, but also enhance the convergence speed and stability of adversarial learning. Moreover, we introduce a noise-resistant voxelisation approach by utilizing both the appearance distance and spatial distance to exclude the spatial outlier points caused by depth noise. Our entire architecture is fully differentiable and can be learned in an end-to-end fashion. Extensive experiments show that our method produces state-of-the-art results for neural point cloud rendering by a significant margin. Our source code will be made public at a later date.



### Images as Weight Matrices: Sequential Image Generation Through Synaptic Learning Rules
- **Arxiv ID**: http://arxiv.org/abs/2210.06184v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2210.06184v2)
- **Published**: 2022-10-07 17:27:50+00:00
- **Updated**: 2023-02-28 18:40:52+00:00
- **Authors**: Kazuki Irie, J√ºrgen Schmidhuber
- **Comment**: Accepted to ICLR 2023
- **Journal**: None
- **Summary**: Work on fast weight programmers has demonstrated the effectiveness of key/value outer product-based learning rules for sequentially generating a weight matrix (WM) of a neural net (NN) by another NN or itself. However, the weight generation steps are typically not visually interpretable by humans, because the contents stored in the WM of an NN are not. Here we apply the same principle to generate natural images. The resulting fast weight painters (FPAs) learn to execute sequences of delta learning rules to sequentially generate images as sums of outer products of self-invented keys and values, one rank at a time, as if each image was a WM of an NN. We train our FPAs in the generative adversarial networks framework, and evaluate on various image datasets. We show how these generic learning rules can generate images with respectable visual quality without any explicit inductive bias for images. While the performance largely lags behind the one of specialised state-of-the-art image generators, our approach allows for visualising how synaptic learning rules iteratively produce complex connection patterns, yielding human-interpretable meaningful images. Finally, we also show that an additional convolutional U-Net (now popular in diffusion models) at the output of an FPA can learn one-step "denoising" of FPA-generated images to enhance their quality. Our code is public.



### In What Ways Are Deep Neural Networks Invariant and How Should We Measure This?
- **Arxiv ID**: http://arxiv.org/abs/2210.03773v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.03773v1)
- **Published**: 2022-10-07 18:43:21+00:00
- **Updated**: 2022-10-07 18:43:21+00:00
- **Authors**: Henry Kvinge, Tegan H. Emerson, Grayson Jorgenson, Scott Vasquez, Timothy Doster, Jesse D. Lew
- **Comment**: To appear at NeurIPS 2022
- **Journal**: None
- **Summary**: It is often said that a deep learning model is "invariant" to some specific type of transformation. However, what is meant by this statement strongly depends on the context in which it is made. In this paper we explore the nature of invariance and equivariance of deep learning models with the goal of better understanding the ways in which they actually capture these concepts on a formal level. We introduce a family of invariance and equivariance metrics that allows us to quantify these properties in a way that disentangles them from other metrics such as loss or accuracy. We use our metrics to better understand the two most popular methods used to build invariance into networks: data augmentation and equivariant layers. We draw a range of conclusions about invariance and equivariance in deep learning models, ranging from whether initializing a model with pretrained weights has an effect on a trained model's invariance, to the extent to which invariance learned via training can generalize to out-of-distribution data.



### MRI-based classification of IDH mutation and 1p/19q codeletion status of gliomas using a 2.5D hybrid multi-task convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/2210.03779v1
- **DOI**: 10.1093/noajnl/vdad023
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2210.03779v1)
- **Published**: 2022-10-07 18:46:39+00:00
- **Updated**: 2022-10-07 18:46:39+00:00
- **Authors**: Satrajit Chakrabarty, Pamela LaMontagne, Joshua Shimony, Daniel S. Marcus, Aristeidis Sotiras
- **Comment**: None
- **Journal**: None
- **Summary**: Isocitrate dehydrogenase (IDH) mutation and 1p/19q codeletion status are important prognostic markers for glioma. Currently, they are determined using invasive procedures. Our goal was to develop artificial intelligence-based methods to non-invasively determine these molecular alterations from MRI. For this purpose, pre-operative MRI scans of 2648 patients with gliomas (grade II-IV) were collected from Washington University School of Medicine (WUSM; n = 835) and publicly available datasets viz. Brain Tumor Segmentation (BraTS; n = 378), LGG 1p/19q (n = 159), Ivy Glioblastoma Atlas Project (Ivy GAP; n = 41), The Cancer Genome Atlas (TCGA; n = 461), and the Erasmus Glioma Database (EGD; n = 774). A 2.5D hybrid convolutional neural network was proposed to simultaneously localize the tumor and classify its molecular status by leveraging imaging features from MR scans and prior knowledge features from clinical records and tumor location. The models were tested on one internal (TCGA) and two external (WUSM and EGD) test sets. For IDH, the best-performing model achieved areas under the receiver operating characteristic (AUROC) of 0.925, 0.874, 0.933 and areas under the precision-recall curves (AUPRC) of 0.899, 0.702, 0.853 on the internal, WUSM, and EGD test sets, respectively. For 1p/19q, the best model achieved AUROCs of 0.782, 0.754, 0.842, and AUPRCs of 0.588, 0.713, 0.782, on those three data-splits, respectively. The high accuracy of the model on unseen data showcases its generalization capabilities and suggests its potential to perform a 'virtual biopsy' for tailoring treatment planning and overall clinical management of gliomas.



### LOCL: Learning Object-Attribute Composition using Localization
- **Arxiv ID**: http://arxiv.org/abs/2210.03780v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2; I.4; I.5; I.7; I.m
- **Links**: [PDF](http://arxiv.org/pdf/2210.03780v1)
- **Published**: 2022-10-07 18:48:45+00:00
- **Updated**: 2022-10-07 18:48:45+00:00
- **Authors**: Satish Kumar, ASM Iftekhar, Ekta Prashnani, B. S. Manjunath
- **Comment**: 20 pages, 7 figures, 11 tables, Accepted in British Machine Vision
  Conference 2022
- **Journal**: None
- **Summary**: This paper describes LOCL (Learning Object Attribute Composition using Localization) that generalizes composition zero shot learning to objects in cluttered and more realistic settings. The problem of unseen Object Attribute (OA) associations has been well studied in the field, however, the performance of existing methods is limited in challenging scenes. In this context, our key contribution is a modular approach to localizing objects and attributes of interest in a weakly supervised context that generalizes robustly to unseen configurations. Localization coupled with a composition classifier significantly outperforms state of the art (SOTA) methods, with an improvement of about 12% on currently available challenging datasets. Further, the modularity enables the use of localized feature extractor to be used with existing OA compositional learning methods to improve their overall performance.



### Evaluating the Performance of StyleGAN2-ADA on Medical Images
- **Arxiv ID**: http://arxiv.org/abs/2210.03786v1
- **DOI**: 10.1007/978-3-031-16980-9_14
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.03786v1)
- **Published**: 2022-10-07 19:18:55+00:00
- **Updated**: 2022-10-07 19:18:55+00:00
- **Authors**: McKell Woodland, John Wood, Brian M. Anderson, Suprateek Kundu, Ethan Lin, Eugene Koay, Bruno Odisio, Caroline Chung, Hyunseon Christine Kang, Aradhana M. Venkatesan, Sireesha Yedururi, Brian De, Yuan-Mao Lin, Ankit B. Patel, Kristy K. Brock
- **Comment**: This preprint has not undergone post-submission improvements or
  corrections. The Version of Record of this contribution is published in LNCS,
  volume 13570, and is available online at
  https://doi.org/10.1007/978-3-031-16980-9_14
- **Journal**: Lecture Notes in Computer Science 13570 (2022)
- **Summary**: Although generative adversarial networks (GANs) have shown promise in medical imaging, they have four main limitations that impeded their utility: computational cost, data requirements, reliable evaluation measures, and training complexity. Our work investigates each of these obstacles in a novel application of StyleGAN2-ADA to high-resolution medical imaging datasets. Our dataset is comprised of liver-containing axial slices from non-contrast and contrast-enhanced computed tomography (CT) scans. Additionally, we utilized four public datasets composed of various imaging modalities. We trained a StyleGAN2 network with transfer learning (from the Flickr-Faces-HQ dataset) and data augmentation (horizontal flipping and adaptive discriminator augmentation). The network's generative quality was measured quantitatively with the Fr\'echet Inception Distance (FID) and qualitatively with a visual Turing test given to seven radiologists and radiation oncologists.   The StyleGAN2-ADA network achieved a FID of 5.22 ($\pm$ 0.17) on our liver CT dataset. It also set new record FIDs of 10.78, 3.52, 21.17, and 5.39 on the publicly available SLIVER07, ChestX-ray14, ACDC, and Medical Segmentation Decathlon (brain tumors) datasets. In the visual Turing test, the clinicians rated generated images as real 42% of the time, approaching random guessing. Our computational ablation study revealed that transfer learning and data augmentation stabilize training and improve the perceptual quality of the generated images. We observed the FID to be consistent with human perceptual evaluation of medical images. Finally, our work found that StyleGAN2-ADA consistently produces high-quality results without hyperparameter searches or retraining.



### Learning a Visually Grounded Memory Assistant
- **Arxiv ID**: http://arxiv.org/abs/2210.03787v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2210.03787v1)
- **Published**: 2022-10-07 19:19:01+00:00
- **Updated**: 2022-10-07 19:19:01+00:00
- **Authors**: Meera Hahn, Kevin Carlberg, Ruta Desai, James Hillis
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel interface for large scale collection of human memory and assistance. Using the 3D Matterport simulator we create a realistic indoor environments in which we have people perform specific embodied memory tasks that mimic household daily activities. This interface was then deployed on Amazon Mechanical Turk allowing us to test and record human memory, navigation and needs for assistance at a large scale that was previously impossible. Using the interface we collect the `The Visually Grounded Memory Assistant Dataset' which is aimed at developing our understanding of (1) the information people encode during navigation of 3D environments and (2) conditions under which people ask for memory assistance. Additionally we experiment with with predicting when people will ask for assistance using models trained on hand-selected visual and semantic features. This provides an opportunity to build stronger ties between the machine-learning and cognitive-science communities through learned models of human perception, memory, and cognition.



### Self-Aligned Concave Curve: Illumination Enhancement for Unsupervised Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2210.03792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03792v1)
- **Published**: 2022-10-07 19:32:55+00:00
- **Updated**: 2022-10-07 19:32:55+00:00
- **Authors**: Wenjing Wang, Zhengbo Xu, Haofeng Huang, Jiaying Liu
- **Comment**: This paper has been accepted by ACM Multimedia 2022
- **Journal**: None
- **Summary**: Low light conditions not only degrade human visual experience, but also reduce the performance of downstream machine analytics. Although many works have been designed for low-light enhancement or domain adaptive machine analytics, the former considers less on high-level vision, while the latter neglects the potential of image-level signal adjustment. How to restore underexposed images/videos from the perspective of machine vision has long been overlooked. In this paper, we are the first to propose a learnable illumination enhancement model for high-level vision. Inspired by real camera response functions, we assume that the illumination enhancement function should be a concave curve, and propose to satisfy this concavity through discrete integral. With the intention of adapting illumination from the perspective of machine vision without task-specific annotated data, we design an asymmetric cross-domain self-supervised training strategy. Our model architecture and training designs mutually benefit each other, forming a powerful unsupervised normal-to-low light adaptation framework. Comprehensive experiments demonstrate that our method surpasses existing low-light enhancement and adaptation methods and shows superior generalization on various low-light vision tasks, including classification, detection, action recognition, and optical flow estimation. Project website: https://daooshee.github.io/SACC-Website/



### SVL-Adapter: Self-Supervised Adapter for Vision-Language Pretrained Models
- **Arxiv ID**: http://arxiv.org/abs/2210.03794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03794v1)
- **Published**: 2022-10-07 19:35:08+00:00
- **Updated**: 2022-10-07 19:35:08+00:00
- **Authors**: Omiros Pantazis, Gabriel Brostow, Kate Jones, Oisin Mac Aodha
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Vision-language models such as CLIP are pretrained on large volumes of internet sourced image and text pairs, and have been shown to sometimes exhibit impressive zero- and low-shot image classification performance. However, due to their size, fine-tuning these models on new datasets can be prohibitively expensive, both in terms of the supervision and compute required. To combat this, a series of light-weight adaptation methods have been proposed to efficiently adapt such models when limited supervision is available. In this work, we show that while effective on internet-style datasets, even those remedies under-deliver on classification tasks with images that differ significantly from those commonly found online. To address this issue, we present a new approach called SVL-Adapter that combines the complementary strengths of both vision-language pretraining and self-supervised representation learning. We report an average classification accuracy improvement of 10% in the low-shot setting when compared to existing methods, on a set of challenging visual classification tasks. Further, we present a fully automatic way of selecting an important blending hyperparameter for our model that does not require any held-out labeled validation data. Code for our project is available here: https://github.com/omipan/svl_adapter.



### Can Artificial Intelligence Reconstruct Ancient Mosaics?
- **Arxiv ID**: http://arxiv.org/abs/2210.06145v1
- **DOI**: 10.1080/00393630.2023.2227798
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.06145v1)
- **Published**: 2022-10-07 19:42:09+00:00
- **Updated**: 2022-10-07 19:42:09+00:00
- **Authors**: Fernando Moral-Andr√©s, Elena Merino-G√≥mez, Pedro Reviriego, Fabrizio Lombardi
- **Comment**: None
- **Journal**: Studies in Conservation (Taylor and Francis), 2023
- **Summary**: A large number of ancient mosaics have not reached us because they have been destroyed by erosion, earthquakes, looting or even used as materials in newer construction. To make things worse, among the small fraction of mosaics that we have been able to recover, many are damaged or incomplete. Therefore, restoration and reconstruction of mosaics play a fundamental role to preserve cultural heritage and to understand the role of mosaics in ancient cultures. This reconstruction has traditionally been done manually and more recently using computer graphics programs but always by humans. In the last years, Artificial Intelligence (AI) has made impressive progress in the generation of images from text descriptions and reference images. State of the art AI tools such as DALL-E2 can generate high quality images from text prompts and can take a reference image to guide the process. In august 2022, DALL-E2 launched a new feature called outpainting that takes as input an incomplete image and a text prompt and then generates a complete image filling the missing parts. In this paper, we explore whether this innovative technology can be used to reconstruct mosaics with missing parts. Hence a set of ancient mosaics have been used and reconstructed using DALL-E2; results are promising showing that AI is able to interpret the key features of the mosaics and is able to produce reconstructions that capture the essence of the scene. However, in some cases AI fails to reproduce some details, geometric forms or introduces elements that are not consistent with the rest of the mosaic. This suggests that as AI image generation technology matures in the next few years, it could be a valuable tool for mosaic reconstruction going forward.



### Scene-level Tracking and Reconstruction without Object Priors
- **Arxiv ID**: http://arxiv.org/abs/2210.03815v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2210.03815v1)
- **Published**: 2022-10-07 20:56:14+00:00
- **Updated**: 2022-10-07 20:56:14+00:00
- **Authors**: Haonan Chang, Abdeslam Boularias
- **Comment**: Accepted by IROS2022
- **Journal**: None
- **Summary**: We present the first real-time system capable of tracking and reconstructing, individually, every visible object in a given scene, without any form of prior on the rigidness of the objects, texture existence, or object category. In contrast with previous methods such as Co-Fusion and MaskFusion that first segment the scene into individual objects and then process each object independently, the proposed method dynamically segments the non-rigid scene as part of the tracking and reconstruction process. When new measurements indicate topology change, reconstructed models are updated in real-time to reflect that change. Our proposed system can provide the live geometry and deformation of all visible objects in a novel scene in real-time, which makes it possible to be integrated seamlessly into numerous existing robotics applications that rely on object models for grasping and manipulation. The capabilities of the proposed system are demonstrated in challenging scenes that contain multiple rigid and non-rigid objects.



### Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review
- **Arxiv ID**: http://arxiv.org/abs/2210.03829v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.03829v2)
- **Published**: 2022-10-07 21:49:26+00:00
- **Updated**: 2023-07-26 16:26:52+00:00
- **Authors**: Seyed Mojtaba Marvasti-Zadeh, Devin Goodsman, Nilanjan Ray, Nadir Erbilgin
- **Comment**: Under review
- **Journal**: None
- **Summary**: This paper provides a comprehensive review of past and current advances in the early detection of bark beetle-induced tree mortality from three primary perspectives: bark beetle & host interactions, RS, and ML/DL. In contrast to prior efforts, this review encompasses all RS systems and emphasizes ML/DL methods to investigate their strengths and weaknesses. We parse existing literature based on multi- or hyper-spectral analyses and distill their knowledge based on: bark beetle species & attack phases with a primary emphasis on early stages of attacks, host trees, study regions, RS platforms & sensors, spectral/spatial/temporal resolutions, spectral signatures, spectral vegetation indices (SVIs), ML approaches, learning schemes, task categories, models, algorithms, classes/clusters, features, and DL networks & architectures. Although DL-based methods and the random forest (RF) algorithm showed promising results, highlighting their potential to detect subtle changes across visible, thermal, and short-wave infrared (SWIR) spectral regions, they still have limited effectiveness and high uncertainties. To inspire novel solutions to these shortcomings, we delve into the principal challenges & opportunities from different perspectives, enabling a deeper understanding of the current state of research and guiding future research directions.



### Self-Supervised Deep Equilibrium Models for Inverse Problems with Theoretical Guarantees
- **Arxiv ID**: http://arxiv.org/abs/2210.03837v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.03837v1)
- **Published**: 2022-10-07 22:19:26+00:00
- **Updated**: 2022-10-07 22:19:26+00:00
- **Authors**: Weijie Gan, Chunwei Ying, Parna Eshraghi, Tongyao Wang, Cihat Eldeniz, Yuyang Hu, Jiaming Liu, Yasheng Chen, Hongyu An, Ulugbek S. Kamilov
- **Comment**: None
- **Journal**: None
- **Summary**: Deep equilibrium models (DEQ) have emerged as a powerful alternative to deep unfolding (DU) for image reconstruction. DEQ models-implicit neural networks with effectively infinite number of layers-were shown to achieve state-of-the-art image reconstruction without the memory complexity associated with DU. While the performance of DEQ has been widely investigated, the existing work has primarily focused on the settings where groundtruth data is available for training. We present self-supervised deep equilibrium model (SelfDEQ) as the first self-supervised reconstruction framework for training model-based implicit networks from undersampled and noisy MRI measurements. Our theoretical results show that SelfDEQ can compensate for unbalanced sampling across multiple acquisitions and match the performance of fully supervised DEQ. Our numerical results on in-vivo MRI data show that SelfDEQ leads to state-of-the-art performance using only undersampled and noisy training data.



### Learning to embed semantic similarity for joint image-text retrieval
- **Arxiv ID**: http://arxiv.org/abs/2210.03838v1
- **DOI**: 10.1109/TPAMI.2021.3132163
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03838v1)
- **Published**: 2022-10-07 22:20:28+00:00
- **Updated**: 2022-10-07 22:20:28+00:00
- **Authors**: Noam Malali, Yosi Keller
- **Comment**: in IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2023
- **Journal**: None
- **Summary**: We present a deep learning approach for learning the joint semantic embeddings of images and captions in a Euclidean space, such that the semantic similarity is approximated by the L2 distances in the embedding space. For that, we introduce a metric learning scheme that utilizes multitask learning to learn the embedding of identical semantic concepts using a center loss. By introducing a differentiable quantization scheme into the end-to-end trainable network, we derive a semantic embedding of semantically similar concepts in Euclidean space. We also propose a novel metric learning formulation using an adaptive margin hinge loss, that is refined during the training phase. The proposed scheme was applied to the MS-COCO, Flicke30K and Flickr8K datasets, and was shown to compare favorably with contemporary state-of-the-art approaches.



### Toward an Over-parameterized Direct-Fit Model of Visual Perception
- **Arxiv ID**: http://arxiv.org/abs/2210.03850v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.03850v2)
- **Published**: 2022-10-07 23:54:30+00:00
- **Updated**: 2022-10-11 15:37:43+00:00
- **Authors**: Xin Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we revisit the problem of computational modeling of simple and complex cells for an over-parameterized and direct-fit model of visual perception. Unlike conventional wisdom, we highlight the difference in parallel and sequential binding mechanisms between simple and complex cells. A new proposal for abstracting them into space partitioning and composition is developed as the foundation of our new hierarchical construction. Our construction can be interpreted as a product topology-based generalization of the existing k-d tree, making it suitable for brute-force direct-fit in a high-dimensional space. The constructed model has been applied to several classical experiments in neuroscience and psychology. We provide an anti-sparse coding interpretation of the constructed vision model and show how it leads to a dynamic programming (DP)-like approximate nearest-neighbor search based on $\ell_{\infty}$-optimization. We also briefly discuss two possible implementations based on asymmetrical (decoder matters more) auto-encoder and spiking neural networks (SNN), respectively.



