# Arxiv Papers in cs.CV on 2022-10-25
### ConnectedUNets++: Mass Segmentation from Whole Mammographic Images
- **Arxiv ID**: http://arxiv.org/abs/2210.13668v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.13668v3)
- **Published**: 2022-10-25 00:04:29+00:00
- **Updated**: 2022-11-04 07:08:51+00:00
- **Authors**: Prithul Sarker, Sushmita Sarker, George Bebis, Alireza Tavakkoli
- **Comment**: Results are to be updated
- **Journal**: None
- **Summary**: Deep learning has made a breakthrough in medical image segmentation in recent years due to its ability to extract high-level features without the need for prior knowledge. In this context, U-Net is one of the most advanced medical image segmentation models, with promising results in mammography. Despite its excellent overall performance in segmenting multimodal medical images, the traditional U-Net structure appears to be inadequate in various ways. There are certain U-Net design modifications, such as MultiResUNet, Connected-UNets, and AU-Net, that have improved overall performance in areas where the conventional U-Net architecture appears to be deficient. Following the success of UNet and its variants, we have presented two enhanced versions of the Connected-UNets architecture: ConnectedUNets+ and ConnectedUNets++. In ConnectedUNets+, we have replaced the simple skip connections of Connected-UNets architecture with residual skip connections, while in ConnectedUNets++, we have modified the encoder-decoder structure along with employing residual skip connections. We have evaluated our proposed architectures on two publicly available datasets, the Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) and INbreast.



### Adversarial Domain Adaptation for Action Recognition Around the Clock
- **Arxiv ID**: http://arxiv.org/abs/2210.17412v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.17412v1)
- **Published**: 2022-10-25 01:08:27+00:00
- **Updated**: 2022-10-25 01:08:27+00:00
- **Authors**: Anwaar Ulhaq
- **Comment**: 6 Pages, 6 Figures
- **Journal**: None
- **Summary**: Due to the numerous potential applications in visual surveillance and nighttime driving, recognizing human action in low-light conditions remains a difficult problem in computer vision. Existing methods separate action recognition and dark enhancement into two distinct steps to accomplish this task. However, isolating the recognition and enhancement impedes end-to-end learning of the space-time representation for video action classification. This paper presents a domain adaptation-based action recognition approach that uses adversarial learning in cross-domain settings to learn cross-domain action recognition. Supervised learning can train it on a large amount of labeled data from the source domain (daytime action sequences). However, it uses deep domain invariant features to perform unsupervised learning on many unlabelled data from the target domain (night-time action sequences). The resulting augmented model, named 3D-DiNet can be trained using standard backpropagation with an additional layer. It achieves SOTA performance on InFAR and XD145 actions datasets.



### Geo-SIC: Learning Deformable Geometric Shapes in Deep Image Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2210.13704v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13704v1)
- **Published**: 2022-10-25 01:55:17+00:00
- **Updated**: 2022-10-25 01:55:17+00:00
- **Authors**: Jian Wang, Miaomiao Zhang
- **Comment**: 10 pages, 6 figures, 36th Conference on Neural Information Processing
  Systems (NeurIPS 2022)
- **Journal**: None
- **Summary**: Deformable shapes provide important and complex geometric features of objects presented in images. However, such information is oftentimes missing or underutilized as implicit knowledge in many image analysis tasks. This paper presents Geo-SIC, the first deep learning model to learn deformable shapes in a deformation space for an improved performance of image classification. We introduce a newly designed framework that (i) simultaneously derives features from both image and latent shape spaces with large intra-class variations; and (ii) gains increased model interpretability by allowing direct access to the underlying geometric features of image data. In particular, we develop a boosted classification network, equipped with an unsupervised learning of geometric shape representations characterized by diffeomorphic transformations within each class. In contrast to previous approaches using pre-extracted shapes, our model provides a more fundamental approach by naturally learning the most relevant shape features jointly with an image classifier. We demonstrate the effectiveness of our method on both simulated 2D images and real 3D brain magnetic resonance (MR) images. Experimental results show that our model substantially improves the image classification accuracy with an additional benefit of increased model interpretability. Our code is publicly available at https://github.com/jw4hv/Geo-SIC



### An Effective Deep Network for Head Pose Estimation without Keypoints
- **Arxiv ID**: http://arxiv.org/abs/2210.13705v1
- **DOI**: 10.5220/0010870900003122
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13705v1)
- **Published**: 2022-10-25 01:57:04+00:00
- **Updated**: 2022-10-25 01:57:04+00:00
- **Authors**: Chien Thai, Viet Tran, Minh Bui, Huong Ninh, Hai Tran
- **Comment**: None
- **Journal**: In Proceedings of the 11th International Conference on Pattern
  Recognition Applications and Methods - ICPRAM 2022, ISBN 978-989-758-549-4;
  ISSN 2184-4313, pages 90-98
- **Summary**: Human head pose estimation is an essential problem in facial analysis in recent years that has a lot of computer vision applications such as gaze estimation, virtual reality, and driver assistance. Because of the importance of the head pose estimation problem, it is necessary to design a compact model to resolve this task in order to reduce the computational cost when deploying on facial analysis-based applications such as large camera surveillance systems, AI cameras while maintaining accuracy. In this work, we propose a lightweight model that effectively addresses the head pose estimation problem. Our approach has two main steps. 1) We first train many teacher models on the synthesis dataset - 300W-LPA to get the head pose pseudo labels. 2) We design an architecture with the ResNet18 backbone and train our proposed model with the ensemble of these pseudo labels via the knowledge distillation process. To evaluate the effectiveness of our model, we use AFLW-2000 and BIWI - two real-world head pose datasets. Experimental results show that our proposed model significantly improves the accuracy in comparison with the state-of-the-art head pose estimation methods. Furthermore, our model has the real-time speed of $\sim$300 FPS when inferring on Tesla V100.



### ASD: Towards Attribute Spatial Decomposition for Prior-Free Facial Attribute Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.13716v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13716v1)
- **Published**: 2022-10-25 02:25:05+00:00
- **Updated**: 2022-10-25 02:25:05+00:00
- **Authors**: Chuanfei Hu, Hang Shao, Bo Dong, Zhe Wang, Yongxiong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Representing the spatial properties of facial attributes is a vital challenge for facial attribute recognition (FAR). Recent advances have achieved the reliable performances for FAR, benefiting from the description of spatial properties via extra prior information. However, the extra prior information might not be always available, resulting in the restricted application scenario of the prior-based methods. Meanwhile, the spatial ambiguity of facial attributes caused by inherent spatial diversities of facial parts is ignored. To address these issues, we propose a prior-free method for attribute spatial decomposition (ASD), mitigating the spatial ambiguity of facial attributes without any extra prior information. Specifically, assignment-embedding module (AEM) is proposed to enable the procedure of ASD, which consists of two operations: attribute-to-location assignment and location-to-attribute embedding. The attribute-to-location assignment first decomposes the feature map based on latent factors, assigning the magnitude of attribute components on each spatial location. Then, the assigned attribute components from all locations to represent the global-level attribute embeddings. Furthermore, correlation matrix minimization (CMM) is introduced to enlarge the discriminability of attribute embeddings. Experimental results demonstrate the superiority of ASD compared with state-of-the-art prior-based methods, while the reliable performance of ASD for the case of limited training data is further validated.



### Facial Action Units Detection Aided by Global-Local Expression Embedding
- **Arxiv ID**: http://arxiv.org/abs/2210.13718v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.13718v1)
- **Published**: 2022-10-25 02:35:32+00:00
- **Updated**: 2022-10-25 02:35:32+00:00
- **Authors**: Zhipeng Hu, Wei Zhang, Lincheng Li, Yu Ding, Wei Chen, Zhigang Deng, Xin Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Since Facial Action Unit (AU) annotations require domain expertise, common AU datasets only contain a limited number of subjects. As a result, a crucial challenge for AU detection is addressing identity overfitting. We find that AUs and facial expressions are highly associated, and existing facial expression datasets often contain a large number of identities. In this paper, we aim to utilize the expression datasets without AU labels to facilitate AU detection. Specifically, we develop a novel AU detection framework aided by the Global-Local facial Expressions Embedding, dubbed GLEE-Net. Our GLEE-Net consists of three branches to extract identity-independent expression features for AU detection. We introduce a global branch for modeling the overall facial expression while eliminating the impacts of identities. We also design a local branch focusing on specific local face regions. The combined output of global and local branches is firstly pre-trained on an expression dataset as an identity-independent expression embedding, and then finetuned on AU datasets. Therefore, we significantly alleviate the issue of limited identities. Furthermore, we introduce a 3D global branch that extracts expression coefficients through 3D face reconstruction to consolidate 2D AU descriptions. Finally, a Transformer-based multi-label classifier is employed to fuse all the representations for AU detection. Extensive experiments demonstrate that our method significantly outperforms the state-of-the-art on the widely-used DISFA, BP4D and BP4D+ datasets.



### Multi-modal Dynamic Graph Network: Coupling Structural and Functional Connectome for Disease Diagnosis and Classification
- **Arxiv ID**: http://arxiv.org/abs/2210.13721v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.13721v1)
- **Published**: 2022-10-25 02:41:32+00:00
- **Updated**: 2022-10-25 02:41:32+00:00
- **Authors**: Yanwu Yang, Xutao Guo, Zhikai Chang, Chenfei Ye, Yang Xiang, Ting Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modal neuroimaging technology has greatlly facilitated the efficiency and diagnosis accuracy, which provides complementary information in discovering objective disease biomarkers. Conventional deep learning methods, e.g. convolutional neural networks, overlook relationships between nodes and fail to capture topological properties in graphs. Graph neural networks have been proven to be of great importance in modeling brain connectome networks and relating disease-specific patterns. However, most existing graph methods explicitly require known graph structures, which are not available in the sophisticated brain system. Especially in heterogeneous multi-modal brain networks, there exists a great challenge to model interactions among brain regions in consideration of inter-modal dependencies. In this study, we propose a Multi-modal Dynamic Graph Convolution Network (MDGCN) for structural and functional brain network learning. Our method benefits from modeling inter-modal representations and relating attentive multi-model associations into dynamic graphs with a compositional correspondence matrix. Moreover, a bilateral graph convolution layer is proposed to aggregate multi-modal representations in terms of multi-modal associations. Extensive experiments on three datasets demonstrate the superiority of our proposed method in terms of disease classification, with the accuracy of 90.4%, 85.9% and 98.3% in predicting Mild Cognitive Impairment (MCI), Parkinson's disease (PD), and schizophrenia (SCHZ) respectively. Furthermore, our statistical evaluations on the correspondence matrix exhibit a high correspondence with previous evidence of biomarkers.



### S3E: A Large-scale Multimodal Dataset for Collaborative SLAM
- **Arxiv ID**: http://arxiv.org/abs/2210.13723v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.13723v3)
- **Published**: 2022-10-25 02:42:49+00:00
- **Updated**: 2022-12-28 13:19:37+00:00
- **Authors**: Dapeng Feng, Yuhua Qi, Shipeng Zhong, Zhiqiang Chen, Yudu Jiao, Qiming Chen, Tao Jiang, Hongbo Chen
- **Comment**: None
- **Journal**: None
- **Summary**: With the advanced request to employ a team of robots to perform a task collaboratively, the research community has become increasingly interested in collaborative simultaneous localization and mapping. Unfortunately, existing datasets are limited in the scale and variation of the collaborative trajectories, even though generalization between inter-trajectories among different agents is crucial to the overall viability of collaborative tasks. To help align the research community's contributions with realistic multiagent ordinated SLAM problems, we propose S3E, a large-scale multimodal dataset captured by a fleet of unmanned ground vehicles along four designed collaborative trajectory paradigms. S3E consists of 7 outdoor and 5 indoor sequences that each exceed 200 seconds, consisting of well temporal synchronized and spatial calibrated high-frequency IMU, high-quality stereo camera, and 360 degree LiDAR data. Crucially, our effort exceeds previous attempts regarding dataset size, scene variability, and complexity. It has 4x as much average recording time as the pioneering EuRoC dataset. We also provide careful dataset analysis as well as baselines for collaborative SLAM and single counterparts. Data and more up-to-date details are found at https://github.com/PengYu-Team/S3E.



### GLIF: A Unified Gated Leaky Integrate-and-Fire Neuron for Spiking Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.13768v4
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.13768v4)
- **Published**: 2022-10-25 05:07:48+00:00
- **Updated**: 2023-02-13 16:52:10+00:00
- **Authors**: Xingting Yao, Fanrong Li, Zitao Mo, Jian Cheng
- **Comment**: Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) have been studied over decades to incorporate their biological plausibility and leverage their promising energy efficiency. Throughout existing SNNs, the leaky integrate-and-fire (LIF) model is commonly adopted to formulate the spiking neuron and evolves into numerous variants with different biological features. However, most LIF-based neurons support only single biological feature in different neuronal behaviors, limiting their expressiveness and neuronal dynamic diversity. In this paper, we propose GLIF, a unified spiking neuron, to fuse different bio-features in different neuronal behaviors, enlarging the representation space of spiking neurons. In GLIF, gating factors, which are exploited to determine the proportion of the fused bio-features, are learnable during training. Combining all learnable membrane-related parameters, our method can make spiking neurons different and constantly changing, thus increasing the heterogeneity and adaptivity of spiking neurons. Extensive experiments on a variety of datasets demonstrate that our method obtains superior performance compared with other SNNs by simply changing their neuronal formulations to GLIF. In particular, we train a spiking ResNet-19 with GLIF and achieve $77.35\%$ top-1 accuracy with six time steps on CIFAR-100, which has advanced the state-of-the-art. Codes are available at \url{https://github.com/Ikarosy/Gated-LIF}.



### GlobalFlowNet: Video Stabilization using Deep Distilled Global Motion Estimates
- **Arxiv ID**: http://arxiv.org/abs/2210.13769v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.13769v3)
- **Published**: 2022-10-25 05:09:18+00:00
- **Updated**: 2022-11-04 15:03:05+00:00
- **Authors**: Jerin Geo James, Devansh Jain, Ajit Rajwade
- **Comment**: Accepted in WACV 2023
- **Journal**: None
- **Summary**: Videos shot by laymen using hand-held cameras contain undesirable shaky motion. Estimating the global motion between successive frames, in a manner not influenced by moving objects, is central to many video stabilization techniques, but poses significant challenges. A large body of work uses 2D affine transformations or homography for the global motion. However, in this work, we introduce a more general representation scheme, which adapts any existing optical flow network to ignore the moving objects and obtain a spatially smooth approximation of the global motion between video frames. We achieve this by a knowledge distillation approach, where we first introduce a low pass filter module into the optical flow network to constrain the predicted optical flow to be spatially smooth. This becomes our student network, named as \textsc{GlobalFlowNet}. Then, using the original optical flow network as the teacher network, we train the student network using a robust loss function. Given a trained \textsc{GlobalFlowNet}, we stabilize videos using a two stage process. In the first stage, we correct the instability in affine parameters using a quadratic programming approach constrained by a user-specified cropping limit to control loss of field of view. In the second stage, we stabilize the video further by smoothing global motion parameters, expressed using a small number of discrete cosine transform coefficients. In extensive experiments on a variety of different videos, our technique outperforms state of the art techniques in terms of subjective quality and different quantitative measures of video stability. The source code is publicly available at \href{https://github.com/GlobalFlowNet/GlobalFlowNet}{https://github.com/GlobalFlowNet/GlobalFlowNet}



### Towards Trustworthy Multi-label Sewer Defect Classification via Evidential Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.13782v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.13782v1)
- **Published**: 2022-10-25 06:02:22+00:00
- **Updated**: 2022-10-25 06:02:22+00:00
- **Authors**: Chenyang Zhao, Chuanfei Hu, Hang Shao, Zhe Wang, Yongxiong Wang
- **Comment**: Chenyang Zhao and Chuanfei Hu contributed equally to this work.
  Corresponding author: Chuanfei Hu
- **Journal**: None
- **Summary**: An automatic vision-based sewer inspection plays a key role of sewage system in a modern city. Recent advances focus on utilizing deep learning model to realize the sewer inspection system, benefiting from the capability of data-driven feature representation. However, the inherent uncertainty of sewer defects is ignored, resulting in the missed detection of serious unknown sewer defect categories. In this paper, we propose a trustworthy multi-label sewer defect classification (TMSDC) method, which can quantify the uncertainty of sewer defect prediction via evidential deep learning. Meanwhile, a novel expert base rate assignment (EBRA) is proposed to introduce the expert knowledge for describing reliable evidences in practical situations. Experimental results demonstrate the effectiveness of TMSDC and the superior capability of uncertainty estimation is achieved on the latest public benchmark.



### An Industrial Workplace Alerting and Monitoring Platform to Prevent Workplace Injury and Accidents
- **Arxiv ID**: http://arxiv.org/abs/2210.17414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.17414v1)
- **Published**: 2022-10-25 06:35:00+00:00
- **Updated**: 2022-10-25 06:35:00+00:00
- **Authors**: Sanjay Adhikesaven
- **Comment**: None
- **Journal**: None
- **Summary**: Workplace accidents are a critical problem that causes many deaths, injuries, and financial losses. Climate change has a severe impact on industrial workers, partially caused by global warming. To reduce such casualties, it is important to proactively find unsafe environments where injuries could occur by detecting the use of personal protective equipment (PPE) and identifying unsafe activities. Thus, we propose an industrial workplace alerting and monitoring platform to detect PPE use and classify unsafe activity in group settings involving multiple humans and objects over a long period of time. Our proposed method is the first to analyze prolonged actions involving multiple people or objects. It benefits from combining pose estimation with PPE detection in one platform. Additionally, we propose the first open source annotated data set with video data from industrial workplaces annotated with the action classifications and detected PPE. The proposed system can be implemented within the surveillance cameras already present in industrial settings, making it a practical and effective solution.



### Does Medical Imaging learn different Convolution Filters?
- **Arxiv ID**: http://arxiv.org/abs/2210.13799v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.13799v1)
- **Published**: 2022-10-25 07:05:46+00:00
- **Updated**: 2022-10-25 07:05:46+00:00
- **Authors**: Paul Gavrikov, Janis Keuper
- **Comment**: Accepted at MedNeurIPS 2022
- **Journal**: None
- **Summary**: Recent work has investigated the distributions of learned convolution filters through a large-scale study containing hundreds of heterogeneous image models. Surprisingly, on average, the distributions only show minor drifts in comparisons of various studied dimensions including the learned task, image domain, or dataset. However, among the studied image domains, medical imaging models appeared to show significant outliers through "spikey" distributions, and, therefore, learn clusters of highly specific filters different from other domains. Following this observation, we study the collected medical imaging models in more detail. We show that instead of fundamental differences, the outliers are due to specific processing in some architectures. Quite the contrary, for standardized architectures, we find that models trained on medical data do not significantly differ in their filter distributions from similar architectures trained on data from other domains. Our conclusions reinforce previous hypotheses stating that pre-training of imaging models can be done with any kind of diverse image data.



### Deep Boosting Robustness of DNN-based Image Watermarking via DBMark
- **Arxiv ID**: http://arxiv.org/abs/2210.13801v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2210.13801v3)
- **Published**: 2022-10-25 07:09:49+00:00
- **Updated**: 2022-11-16 09:44:10+00:00
- **Authors**: Guanhui Ye, Jiashi Gao, Wei Xie, Bo Yin, Xuetao Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Image watermarking is a technique for hiding information into images that can withstand distortions while requiring the encoded image to be perceptually identical to the original image. Recent work based on deep neural networks (DNN) has achieved impressive progression in digital watermarking. Higher robustness under various distortions is the eternal pursuit of digital image watermarking approaches. In this paper, we propose DBMARK, a novel end-to-end digital image watermarking framework to deep boost the robustness of DNN-based image watermarking. The key novelty is the synergy of invertible neural networks (INN) and effective watermark features generation. The framework generates watermark features with redundancy and error correction ability through the effective neural network based message processor, synergized with the powerful information embedding and extraction abilities of INN to achieve higher robustness and invisibility. The powerful learning ability of neural networks enables the message processor to adapt to various distortions. In addition, we propose to embed the watermark information in the discrete wavelet transform (DWT) domain and design low-low (LL) sub-band loss to enhance invisibility. Extensive experiment results demonstrate the superiority of the proposed framework compared with the state-of-the-art ones under various distortions such as dropout, cropout, crop, Gaussian filter, and JPEG compression.



### Salient Object Detection via Dynamic Scale Routing
- **Arxiv ID**: http://arxiv.org/abs/2210.13821v1
- **DOI**: 10.1109/TIP.2022.3214332
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13821v1)
- **Published**: 2022-10-25 08:01:27+00:00
- **Updated**: 2022-10-25 08:01:27+00:00
- **Authors**: Zhenyu Wu, Shuai Li, Chenglizhao Chen, Hong Qin, Aimin Hao
- **Comment**: 15 pages, 15 figures
- **Journal**: IEEE TIP, 2022
- **Summary**: Recent research advances in salient object detection (SOD) could largely be attributed to ever-stronger multi-scale feature representation empowered by the deep learning technologies. The existing SOD deep models extract multi-scale features via the off-the-shelf encoders and combine them smartly via various delicate decoders. However, the kernel sizes in this commonly-used thread are usually "fixed". In our new experiments, we have observed that kernels of small size are preferable in scenarios containing tiny salient objects. In contrast, large kernel sizes could perform better for images with large salient objects. Inspired by this observation, we advocate the "dynamic" scale routing (as a brand-new idea) in this paper. It will result in a generic plug-in that could directly fit the existing feature backbone. This paper's key technical innovations are two-fold. First, instead of using the vanilla convolution with fixed kernel sizes for the encoder design, we propose the dynamic pyramid convolution (DPConv), which dynamically selects the best-suited kernel sizes w.r.t. the given input. Second, we provide a self-adaptive bidirectional decoder design to accommodate the DPConv-based encoder best. The most significant highlight is its capability of routing between feature scales and their dynamic collection, making the inference process scale-aware. As a result, this paper continues to enhance the current SOTA performance. Both the code and dataset are publicly available at https://github.com/wuzhenyubuaa/DPNet.



### Instance Segmentation for Chinese Character Stroke Extraction, Datasets and Benchmarks
- **Arxiv ID**: http://arxiv.org/abs/2210.13826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13826v1)
- **Published**: 2022-10-25 08:09:14+00:00
- **Updated**: 2022-10-25 08:09:14+00:00
- **Authors**: Lizhao Liu, Kunyang Lin, Shangxin Huang, Zhongli Li, Chao Li, Yunbo Cao, Qingyu Zhou
- **Comment**: 12 pages, 8 pages for the main paper, 4 pages for the supplementary
- **Journal**: None
- **Summary**: Stroke is the basic element of Chinese character and stroke extraction has been an important and long-standing endeavor. Existing stroke extraction methods are often handcrafted and highly depend on domain expertise due to the limited training data. Moreover, there are no standardized benchmarks to provide a fair comparison between different stroke extraction methods, which, we believe, is a major impediment to the development of Chinese character stroke understanding and related tasks. In this work, we present the first public available Chinese Character Stroke Extraction (CCSE) benchmark, with two new large-scale datasets: Kaiti CCSE (CCSE-Kai) and Handwritten CCSE (CCSE-HW). With the large-scale datasets, we hope to leverage the representation power of deep models such as CNNs to solve the stroke extraction task, which, however, remains an open question. To this end, we turn the stroke extraction problem into a stroke instance segmentation problem. Using the proposed datasets to train a stroke instance segmentation model, we surpass previous methods by a large margin. Moreover, the models trained with the proposed datasets benefit the downstream font generation and handwritten aesthetic assessment tasks. We hope these benchmark results can facilitate further research. The source code and datasets are publicly available at: https://github.com/lizhaoliu-Lec/CCSE.



### End-to-end Transformer for Compressed Video Quality Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2210.13827v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.13827v1)
- **Published**: 2022-10-25 08:12:05+00:00
- **Updated**: 2022-10-25 08:12:05+00:00
- **Authors**: Li Yu, Wenshuai Chang, Shiyu Wu, Moncef Gabbouj
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks have achieved excellent results in compressed video quality enhancement task in recent years. State-of-the-art methods explore the spatiotemporal information of adjacent frames mainly by deformable convolution. However, offset fields in deformable convolution are difficult to train, and its instability in training often leads to offset overflow, which reduce the efficiency of correlation modeling. In this work, we propose a transformer-based compressed video quality enhancement (TVQE) method, consisting of Swin-AutoEncoder based Spatio-Temporal feature Fusion (SSTF) module and Channel-wise Attention based Quality Enhancement (CAQE) module. The proposed SSTF module learns both local and global features with the help of Swin-AutoEncoder, which improves the ability of correlation modeling. Meanwhile, the window mechanism-based Swin Transformer and the encoderdecoder structure greatly improve the execution efficiency. On the other hand, the proposed CAQE module calculates the channel attention, which aggregates the temporal information between channels in the feature map, and finally achieves the efficient fusion of inter-frame information. Extensive experimental results on the JCT-VT test sequences show that the proposed method achieves better performance in average for both subjective and objective quality. Meanwhile, our proposed method outperforms existing ones in terms of both inference speed and GPU consumption.



### Stable Deep MRI Reconstruction using Generative Priors
- **Arxiv ID**: http://arxiv.org/abs/2210.13834v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.13834v3)
- **Published**: 2022-10-25 08:34:29+00:00
- **Updated**: 2023-06-15 17:10:10+00:00
- **Authors**: Martin Zach, Florian Knoll, Thomas Pock
- **Comment**: None
- **Journal**: None
- **Summary**: Data-driven approaches recently achieved remarkable success in magnetic resonance imaging (MRI) reconstruction, but integration into clinical routine remains challenging due to a lack of generalizability and interpretability. In this paper, we address these challenges in a unified framework based on generative image priors. We propose a novel deep neural network based regularizer which is trained in a generative setting on reference magnitude images only. After training, the regularizer encodes higher-level domain statistics which we demonstrate by synthesizing images without data. Embedding the trained model in a classical variational approach yields high-quality reconstructions irrespective of the sub-sampling pattern. In addition, the model shows stable behavior when confronted with out-of-distribution data in the form of contrast variation. Furthermore, a probabilistic interpretation provides a distribution of reconstructions and hence allows uncertainty quantification. To reconstruct parallel MRI, we propose a fast algorithm to jointly estimate the image and the sensitivity maps. The results demonstrate competitive performance, on par with state-of-the-art end-to-end deep learning methods, while preserving the flexibility with respect to sub-sampling patterns and allowing for uncertainty quantification.



### Synthetic Data Supervised Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.13835v1
- **DOI**: 10.1145/3503161.3547930
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13835v1)
- **Published**: 2022-10-25 08:36:29+00:00
- **Updated**: 2022-10-25 08:36:29+00:00
- **Authors**: Zhenyu Wu, Lin Wang, Wei Wang, Tengfei Shi, Chenglizhao Chen, Aimin Hao, Shuo Li
- **Comment**: 9 pages, 8 figures
- **Journal**: ACM MM, 2022
- **Summary**: Although deep salient object detection (SOD) has achieved remarkable progress, deep SOD models are extremely data-hungry, requiring large-scale pixel-wise annotations to deliver such promising results. In this paper, we propose a novel yet effective method for SOD, coined SODGAN, which can generate infinite high-quality image-mask pairs requiring only a few labeled data, and these synthesized pairs can replace the human-labeled DUTS-TR to train any off-the-shelf SOD model. Its contribution is three-fold. 1) Our proposed diffusion embedding network can address the manifold mismatch and is tractable for the latent code generation, better matching with the ImageNet latent space. 2) For the first time, our proposed few-shot saliency mask generator can synthesize infinite accurate image synchronized saliency masks with a few labeled data. 3) Our proposed quality-aware discriminator can select highquality synthesized image-mask pairs from noisy synthetic data pool, improving the quality of synthetic data. For the first time, our SODGAN tackles SOD with synthetic data directly generated from the generative model, which opens up a new research paradigm for SOD. Extensive experimental results show that the saliency model trained on synthetic data can achieve $98.4\%$ F-measure of the saliency model trained on the DUTS-TR. Moreover, our approach achieves a new SOTA performance in semi/weakly-supervised methods, and even outperforms several fully-supervised SOTA methods. Code is available at https://github.com/wuzhenyubuaa/SODGAN



### THOR-Net: End-to-end Graformer-based Realistic Two Hands and Object Reconstruction with Self-supervision
- **Arxiv ID**: http://arxiv.org/abs/2210.13853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13853v1)
- **Published**: 2022-10-25 09:18:50+00:00
- **Updated**: 2022-10-25 09:18:50+00:00
- **Authors**: Ahmed Tawfik Aboukhadra, Jameel Malik, Ahmed Elhayek, Nadia Robertini, Didier Stricker
- **Comment**: To be published in WACV2023
- **Journal**: None
- **Summary**: Realistic reconstruction of two hands interacting with objects is a new and challenging problem that is essential for building personalized Virtual and Augmented Reality environments. Graph Convolutional networks (GCNs) allow for the preservation of the topologies of hands poses and shapes by modeling them as a graph. In this work, we propose the THOR-Net which combines the power of GCNs, Transformer, and self-supervision to realistically reconstruct two hands and an object from a single RGB image. Our network comprises two stages; namely the features extraction stage and the reconstruction stage. In the features extraction stage, a Keypoint RCNN is used to extract 2D poses, features maps, heatmaps, and bounding boxes from a monocular RGB image. Thereafter, this 2D information is modeled as two graphs and passed to the two branches of the reconstruction stage. The shape reconstruction branch estimates meshes of two hands and an object using our novel coarse-to-fine GraFormer shape network. The 3D poses of the hands and objects are reconstructed by the other branch using a GraFormer network. Finally, a self-supervised photometric loss is used to directly regress the realistic textured of each vertex in the hands' meshes. Our approach achieves State-of-the-art results in Hand shape estimation on the HO-3D dataset (10.0mm) exceeding ArtiBoost (10.8mm). It also surpasses other methods in hand pose estimation on the challenging two hands and object (H2O) dataset by 5mm on the left-hand pose and 1 mm on the right-hand pose.



### LAB: Learnable Activation Binarizer for Binary Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.13858v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.13858v1)
- **Published**: 2022-10-25 09:22:31+00:00
- **Updated**: 2022-10-25 09:22:31+00:00
- **Authors**: Sieger Falkena, Hadi Jamali-Rad, Jan van Gemert
- **Comment**: This paper is accepted to appear in the proceedings of WACV 2023
- **Journal**: None
- **Summary**: Binary Neural Networks (BNNs) are receiving an upsurge of attention for bringing power-hungry deep learning towards edge devices. The traditional wisdom in this space is to employ sign() for binarizing featuremaps. We argue and illustrate that sign() is a uniqueness bottleneck, limiting information propagation throughout the network. To alleviate this, we propose to dispense sign(), replacing it with a learnable activation binarizer (LAB), allowing the network to learn a fine-grained binarization kernel per layer - as opposed to global thresholding. LAB is a novel universal module that can seamlessly be integrated into existing architectures. To confirm this, we plug it into four seminal BNNs and show a considerable performance boost at the cost of tolerable increase in delay and complexity. Finally, we build an end-to-end BNN (coined as LAB-BNN) around LAB, and demonstrate that it achieves competitive performance on par with the state-of-the-art on ImageNet.



### SUPR: A Sparse Unified Part-Based Human Representation
- **Arxiv ID**: http://arxiv.org/abs/2210.13861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13861v1)
- **Published**: 2022-10-25 09:32:34+00:00
- **Updated**: 2022-10-25 09:32:34+00:00
- **Authors**: Ahmed A. A. Osman, Timo Bolkart, Dimitrios Tzionas, Michael J. Black
- **Comment**: Accepted in ECCV 2022
- **Journal**: None
- **Summary**: Statistical 3D shape models of the head, hands, and fullbody are widely used in computer vision and graphics. Despite their wide use, we show that existing models of the head and hands fail to capture the full range of motion for these parts. Moreover, existing work largely ignores the feet, which are crucial for modeling human movement and have applications in biomechanics, animation, and the footwear industry. The problem is that previous body part models are trained using 3D scans that are isolated to the individual parts. Such data does not capture the full range of motion for such parts, e.g. the motion of head relative to the neck. Our observation is that full-body scans provide important information about the motion of the body parts. Consequently, we propose a new learning scheme that jointly trains a full-body model and specific part models using a federated dataset of full-body and body-part scans. Specifically, we train an expressive human body model called SUPR (Sparse Unified Part-Based Human Representation), where each joint strictly influences a sparse set of model vertices. The factorized representation enables separating SUPR into an entire suite of body part models. Note that the feet have received little attention and existing 3D body models have highly under-actuated feet. Using novel 4D scans of feet, we train a model with an extended kinematic tree that captures the range of motion of the toes. Additionally, feet deform due to ground contact. To model this, we include a novel non-linear deformation function that predicts foot deformation conditioned on the foot pose, shape, and ground contact. We train SUPR on an unprecedented number of scans: 1.2 million body, head, hand and foot scans. We quantitatively compare SUPR and the separated body parts and find that our suite of models generalizes better than existing models. SUPR is available at http://supr.is.tue.mpg.de



### A jet tagging algorithm of graph network with HaarPooling message passing
- **Arxiv ID**: http://arxiv.org/abs/2210.13869v4
- **DOI**: None
- **Categories**: **hep-ex**, cs.CV, cs.LG, hep-ph
- **Links**: [PDF](http://arxiv.org/pdf/2210.13869v4)
- **Published**: 2022-10-25 09:45:49+00:00
- **Updated**: 2023-08-14 05:33:48+00:00
- **Authors**: Fei Ma, Feiyi Liu, Wei Li
- **Comment**: None
- **Journal**: None
- **Summary**: Recently methods of graph neural networks (GNNs) have been applied to solving the problems in high energy physics (HEP) and have shown its great potential for quark-gluon tagging with graph representation of jet events. In this paper, we introduce an approach of GNNs combined with a HaarPooling operation to analyze the events, called HaarPooling Message Passing neural network (HMPNet). In HMPNet, HaarPooling not only extracts the features of graph, but embeds additional information obtained by clustering of k-means of different particle features. We construct Haarpooling from five different features: absolute energy $\log E$, transverse momentum $\log p_T$, relative coordinates $(\Delta\eta,\Delta\phi)$, the mixed ones $(\log E, \log p_T)$ and $(\log E, \log p_T, \Delta\eta,\Delta\phi)$. The results show that an appropriate selection of information for HaarPooling enhances the accuracy of quark-gluon tagging, as adding extra information of $\log P_T$ to the HMPNet outperforms all the others, whereas adding relative coordinates information $(\Delta\eta,\Delta\phi)$ is not very effective. This implies that by adding effective particle features from HaarPooling can achieve much better results than solely pure message passing neutral network (MPNN) can do, which demonstrates significant improvement of feature extraction via the pooling process. Finally we compare the HMPNet study, ordering by $p_T$, with other studies and prove that the HMPNet is also a good choice of GNN algorithms for jet tagging.



### Towards emotion recognition for virtual environments: an evaluation of EEG features on benchmark dataset
- **Arxiv ID**: http://arxiv.org/abs/2210.13876v1
- **DOI**: 10.1007/s00779-017-1072-7
- **Categories**: **cs.HC**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2210.13876v1)
- **Published**: 2022-10-25 10:02:55+00:00
- **Updated**: 2022-10-25 10:02:55+00:00
- **Authors**: M. L. Menezes, A. Samara, L. Galway, A. Sant'anna, A. Verikas, F. Alonso-Fernandez, H. Wang, R. Bond
- **Comment**: Published at Personal and Ubiquitous Computing Journal (PUC)
- **Journal**: None
- **Summary**: One of the challenges in virtual environments is the difficulty users have in interacting with these increasingly complex systems. Ultimately, endowing machines with the ability to perceive users emotions will enable a more intuitive and reliable interaction. Consequently, using the electroencephalogram as a bio-signal sensor, the affective state of a user can be modelled and subsequently utilised in order to achieve a system that can recognise and react to the user's emotions. This paper investigates features extracted from electroencephalogram signals for the purpose of affective state modelling based on Russell's Circumplex Model. Investigations are presented that aim to provide the foundation for future work in modelling user affect to enhance interaction experience in virtual environments. The DEAP dataset was used within this work, along with a Support Vector Machine and Random Forest, which yielded reasonable classification accuracies for Valence and Arousal using feature vectors based on statistical measurements and band power from the \'z, \b{eta}, \'z, and \'z\'z waves and High Order Crossing of the EEG signal.



### A deep learning approach for brain tumor detection using magnetic resonance imaging
- **Arxiv ID**: http://arxiv.org/abs/2210.13882v1
- **DOI**: 10.11591/ijece.v13i1.pp1039-1047
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.13882v1)
- **Published**: 2022-10-25 10:13:29+00:00
- **Updated**: 2022-10-25 10:13:29+00:00
- **Authors**: Al-Akhir Nayan, Ahamad Nokib Mozumder, Md. Rakibul Haque, Fahim Hossain Sifat, Khan Raqib Mahmud, Abul Kalam Al Azad, Muhammad Golam Kibria
- **Comment**: None
- **Journal**: International Journal of Electrical and Computer Engineering
  (IJECE), 2023
- **Summary**: The growth of abnormal cells in the brain's tissue causes brain tumors. Brain tumors are considered one of the most dangerous disorders in children and adults. It develops quickly, and the patient's survival prospects are slim if not appropriately treated. Proper treatment planning and precise diagnoses are essential to improving a patient's life expectancy. Brain tumors are mainly diagnosed using magnetic resonance imaging (MRI). As part of a convolution neural network (CNN)-based illustration, an architecture containing five convolution layers, five max-pooling layers, a Flatten layer, and two dense layers has been proposed for detecting brain tumors from MRI images. The proposed model includes an automatic feature extractor, modified hidden layer architecture, and activation function. Several test cases were performed, and the proposed model achieved 98.6% accuracy and 97.8% precision score with a low cross-entropy rate. Compared with other approaches such as adjacent feature propagation network (AFPNet), mask region-based CNN (mask RCNN), YOLOv5, and Fourier CNN (FCNN), the proposed model has performed better in detecting brain tumors.



### Real-time AdaBoost cascade face tracker based on likelihood map and optical flow
- **Arxiv ID**: http://arxiv.org/abs/2210.13885v1
- **DOI**: 10.1049/iet-bmt.2016.0202
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13885v1)
- **Published**: 2022-10-25 10:15:07+00:00
- **Updated**: 2022-10-25 10:15:07+00:00
- **Authors**: Andreas Ranftl, Fernando Alonso-Fernandez, Stefan Karlsson, Josef Bigun
- **Comment**: Published at IET Biometrics Journal
- **Journal**: None
- **Summary**: The authors present a novel face tracking approach where optical flow information is incorporated into a modified version of the Viola Jones detection algorithm. In the original algorithm, detection is static, as information from previous frames is not considered. In addition, candidate windows have to pass all stages of the classification cascade, otherwise they are discarded as containing no face. In contrast, the proposed tracker preserves information about the number of classification stages passed by each window. Such information is used to build a likelihood map, which represents the probability of having a face located at that position. Tracking capabilities are provided by extrapolating the position of the likelihood map to the next frame by optical flow computation. The proposed algorithm works in real time on a standard laptop. The system is verified on the Boston Head Tracking Database, showing that the proposed algorithm outperforms the standard Viola Jones detector in terms of detection rate and stability of the output bounding box, as well as including the capability to deal with occlusions. The authors also evaluate two recently published face detectors based on convolutional networks and deformable part models with their algorithm showing a comparable accuracy at a fraction of the computation time.



### Clinically-Inspired Multi-Agent Transformers for Disease Trajectory Forecasting from Multimodal Data
- **Arxiv ID**: http://arxiv.org/abs/2210.13889v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.13889v1)
- **Published**: 2022-10-25 10:16:42+00:00
- **Updated**: 2022-10-25 10:16:42+00:00
- **Authors**: Huy Hoang Nguyen, Matthew B. Blaschko, Simo Saarakkala, Aleksei Tiulpin
- **Comment**: 10 pages, under review
- **Journal**: None
- **Summary**: Deep neural networks are often applied to medical images to automate the problem of medical diagnosis. However, a more clinically relevant question that practitioners usually face is how to predict the future trajectory of a disease. Current methods for prognosis or disease trajectory forecasting often require domain knowledge and are complicated to apply. In this paper, we formulate the prognosis prediction problem as a one-to-many prediction problem. Inspired by a clinical decision-making process with two agents -- a radiologist and a general practitioner -- we predict prognosis with two transformer-based components that share information with each other. The first transformer in this framework aims to analyze the imaging data, and the second one leverages its internal states as inputs, also fusing them with auxiliary clinical data. The temporal nature of the problem is modeled within the transformer states, allowing us to treat the forecasting problem as a multi-task classification, for which we propose a novel loss. We show the effectiveness of our approach in predicting the development of structural knee osteoarthritis changes and forecasting Alzheimer's disease clinical status directly from raw multi-modal data. The proposed method outperforms multiple state-of-the-art baselines with respect to performance and calibration, both of which are needed for real-world applications. An open-source implementation of our method is made publicly available at \url{https://github.com/Oulu-IMEDS/CLIMATv2}.



### A Novel Approach for Dimensionality Reduction and Classification of Hyperspectral Images based on Normalized Synergy
- **Arxiv ID**: http://arxiv.org/abs/2210.13901v1
- **DOI**: 10.14569/ijacsa.2019.0100831
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13901v1)
- **Published**: 2022-10-25 10:36:26+00:00
- **Updated**: 2022-10-25 10:36:26+00:00
- **Authors**: Asma Elmaizi, Hasna Nhaila, Elkebir Sarhrouni, Ahmed Hammouch, Nacir Chafik
- **Comment**: None
- **Journal**: (IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 10, No. 8, 2019 -
  https://www.scopus.com/record/display.uri?eid=2-s2.0-85072285978&origin=inward&txGid=c9d92a087d23b9e6061fdb47b0ec3a96
- **Summary**: During the last decade, hyperspectral images have attracted increasing interest from researchers worldwide. They provide more detailed information about an observed area and allow an accurate target detection and precise discrimination of objects compared to classical RGB and multispectral images. Despite the great potentialities of hyperspectral technology, the analysis and exploitation of the large volume data remain a challenging task. The existence of irrelevant redundant and noisy images decreases the classification accuracy. As a result, dimensionality reduction is a mandatory step in order to select a minimal and effective images subset. In this paper, a new filter approach normalized mutual synergy (NMS) is proposed in order to detect relevant bands that are complementary in the class prediction better than the original hyperspectral cube data. The algorithm consists of two steps: images selection through normalized synergy information and pixel classification. The proposed approach measures the discriminative power of the selected bands based on a combination of their maximal normalized synergic information, minimum redundancy and maximal mutual information with the ground truth. A comparative study using the support vector machine (SVM) and k-nearest neighbor (KNN) classifiers is conducted to evaluate the proposed approach compared to the state of art band selection methods. Experimental results on three benchmark hyperspectral images proposed by the NASA "Aviris Indiana Pine", "Salinas" and "Pavia University" demonstrated the robustness, effectiveness and the discriminative power of the proposed approach over the literature approaches.   Keywords: Hyperspectral images; target detection; pixel classification; dimensionality reduction; band selection; information theory; mutual information; normalized synergy



### Confidence-Calibrated Face and Kinship Verification
- **Arxiv ID**: http://arxiv.org/abs/2210.13905v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13905v3)
- **Published**: 2022-10-25 10:43:46+00:00
- **Updated**: 2023-06-16 06:49:38+00:00
- **Authors**: Min Xu, Ximiao Zhang, Xiuzhuang Zhou
- **Comment**: 14 pages, 10 figures, and 9 tables
- **Journal**: None
- **Summary**: In this paper, we investigate the problem of prediction confidence in face and kinship verification. Most existing face and kinship verification methods focus on accuracy performance while ignoring confidence estimation for their prediction results. However, confidence estimation is essential for modeling reliability and trustworthiness in such high-risk tasks. To address this, we introduce an effective confidence measure that allows verification models to convert a similarity score into a confidence score for any given face pair. We further propose a confidence-calibrated approach, termed Angular Scaling Calibration (ASC). ASC is easy to implement and can be readily applied to existing verification models without model modifications, yielding accuracy-preserving and confidence-calibrated probabilistic verification models. In addition, we introduce the uncertainty in the calibrated confidence to boost the reliability and trustworthiness of the verification models in the presence of noisy data. To the best of our knowledge, our work presents the first comprehensive confidence-calibrated solution for modern face and kinship verification tasks. We conduct extensive experiments on four widely used face and kinship verification datasets, and the results demonstrate the effectiveness of our proposed approach. Code and models are available at https://github.com/cnulab/ASC.



### 'A net for everyone': fully personalized and unsupervised neural networks trained with longitudinal data from a single patient
- **Arxiv ID**: http://arxiv.org/abs/2210.14228v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14228v1)
- **Published**: 2022-10-25 11:07:24+00:00
- **Updated**: 2022-10-25 11:07:24+00:00
- **Authors**: Christian Strack, Kelsey L. Pomykala, Heinz-Peter Schlemmer, Jan Egger, Jens Kleesiek
- **Comment**: None
- **Journal**: None
- **Summary**: With the rise in importance of personalized medicine, we trained personalized neural networks to detect tumor progression in longitudinal datasets. The model was evaluated on two datasets with a total of 64 scans from 32 patients diagnosed with glioblastoma multiforme (GBM). Contrast-enhanced T1w sequences of brain magnetic resonance imaging (MRI) images were used in this study. For each patient, we trained their own neural network using just two images from different timepoints. Our approach uses a Wasserstein-GAN (generative adversarial network), an unsupervised network architecture, to map the differences between the two images. Using this map, the change in tumor volume can be evaluated. Due to the combination of data augmentation and the network architecture, co-registration of the two images is not needed. Furthermore, we do not rely on any additional training data, (manual) annotations or pre-training neural networks. The model received an AUC-score of 0.87 for tumor change. We also introduced a modified RANO criteria, for which an accuracy of 66% can be achieved. We show that using data from just one patient can be used to train deep neural networks to monitor tumor change.



### Connective Reconstruction-based Novelty Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.13917v1
- **DOI**: 10.48550/arXiv.2210.13917
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.13917v1)
- **Published**: 2022-10-25 11:09:39+00:00
- **Updated**: 2022-10-25 11:09:39+00:00
- **Authors**: Seyyed Morteza Hashemi, Parvaneh Aliniya, Parvin Razzaghi
- **Comment**: None
- **Journal**: None
- **Summary**: Detection of out-of-distribution samples is one of the critical tasks for real-world applications of computer vision. The advancement of deep learning has enabled us to analyze real-world data which contain unexplained samples, accentuating the need to detect out-of-distribution instances more than before. GAN-based approaches have been widely used to address this problem due to their ability to perform distribution fitting; however, they are accompanied by training instability and mode collapse. We propose a simple yet efficient reconstruction-based method that avoids adding complexities to compensate for the limitations of GAN models while outperforming them. Unlike previous reconstruction-based works that only utilize reconstruction error or generated samples, our proposed method simultaneously incorporates both of them in the detection task. Our model, which we call "Connective Novelty Detection" has two subnetworks, an autoencoder, and a binary classifier. The autoencoder learns the representation of the positive class by reconstructing them. Then, the model creates negative and connected positive examples using real and generated samples. Negative instances are generated via manipulating the real data, so their distribution is close to the positive class to achieve a more accurate boundary for the classifier. To boost the robustness of the detection to reconstruction error, connected positive samples are created by combining the real and generated samples. Finally, the binary classifier is trained using connected positive and negative examples. We demonstrate a considerable improvement in novelty detection over state-of-the-art methods on MNIST and Caltech-256 datasets.



### A Comparative Attention Framework for Better Few-Shot Object Detection on Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2210.13923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13923v1)
- **Published**: 2022-10-25 11:20:31+00:00
- **Updated**: 2022-10-25 11:20:31+00:00
- **Authors**: Pierre Le Jeune, Anissa Mokraoui
- **Comment**: None
- **Journal**: None
- **Summary**: Few-Shot Object Detection (FSOD) methods are mainly designed and evaluated on natural image datasets such as Pascal VOC and MS COCO. However, it is not clear whether the best methods for natural images are also the best for aerial images. Furthermore, direct comparison of performance between FSOD methods is difficult due to the wide variety of detection frameworks and training strategies. Therefore, we propose a benchmarking framework that provides a flexible environment to implement and compare attention-based FSOD methods. The proposed framework focuses on attention mechanisms and is divided into three modules: spatial alignment, global attention, and fusion layer. To remain competitive with existing methods, which often leverage complex training, we propose new augmentation techniques designed for object detection. Using this framework, several FSOD methods are reimplemented and compared. This comparison highlights two distinct performance regimes on aerial and natural images: FSOD performs worse on aerial images. Our experiments suggest that small objects, which are harder to detect in the few-shot setting, account for the poor performance. Finally, we develop a novel multiscale alignment method, Cross-Scales Query-Support Alignment (XQSA) for FSOD, to improve the detection of small objects. XQSA outperforms the state-of-the-art significantly on DOTA and DIOR.



### Deep Crowd Anomaly Detection: State-of-the-Art, Challenges, and Future Research Directions
- **Arxiv ID**: http://arxiv.org/abs/2210.13927v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.13927v1)
- **Published**: 2022-10-25 11:24:50+00:00
- **Updated**: 2022-10-25 11:24:50+00:00
- **Authors**: Md. Haidar Sharif, Lei Jiao, Christian W. Omlin
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd anomaly detection is one of the most popular topics in computer vision in the context of smart cities. A plethora of deep learning methods have been proposed that generally outperform other machine learning solutions. Our review primarily discusses algorithms that were published in mainstream conferences and journals between 2020 and 2022. We present datasets that are typically used for benchmarking, produce a taxonomy of the developed algorithms, and discuss and compare their performances. Our main findings are that the heterogeneities of pre-trained convolutional models have a negligible impact on crowd video anomaly detection performance. We conclude our discussion with fruitful directions for future research.



### Control and Evaluation of Event Cameras Output Sharpness via Bias
- **Arxiv ID**: http://arxiv.org/abs/2210.13929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13929v1)
- **Published**: 2022-10-25 11:31:37+00:00
- **Updated**: 2022-10-25 11:31:37+00:00
- **Authors**: Mehdi Sefidgar Dilmaghani, Waseem Shariff, Cian Ryan, Joe Lemley, Peter Corcoran
- **Comment**: None
- **Journal**: None
- **Summary**: Event cameras also known as neuromorphic sensors are relatively a new technology with some privilege over the RGB cameras. The most important one is their difference in capturing the light changes in the environment, each pixel changes independently from the others when it captures a change in the environment light. To increase the users degree of freedom in controlling the output of these cameras, such as changing the sensitivity of the sensor to light changes, controlling the number of generated events and other similar operations, the camera manufacturers usually introduce some tools to make sensor level changes in camera settings. The contribution of this research is to examine and document the effects of changing the sensor settings on the sharpness as an indicator of quality of the generated stream of event data. To have a qualitative understanding this stream of event is converted to frames, then the average image gradient magnitude as an index of the number of edges and accordingly sharpness is calculated for these frames. Five different bias settings are explained and the effect of their change in the event output is surveyed and analyzed. In addition, the operation of the event camera sensing array is explained with an analogue circuit model and the functions of the bias foundations are linked with this model.



### Pointly-Supervised Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.13950v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13950v1)
- **Published**: 2022-10-25 12:03:51+00:00
- **Updated**: 2022-10-25 12:03:51+00:00
- **Authors**: Junsong Fan, Zhaoxiang Zhang, Tieniu Tan
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: In this paper, we propose a new approach to applying point-level annotations for weakly-supervised panoptic segmentation. Instead of the dense pixel-level labels used by fully supervised methods, point-level labels only provide a single point for each target as supervision, significantly reducing the annotation burden. We formulate the problem in an end-to-end framework by simultaneously generating panoptic pseudo-masks from point-level labels and learning from them. To tackle the core challenge, i.e., panoptic pseudo-mask generation, we propose a principled approach to parsing pixels by minimizing pixel-to-point traversing costs, which model semantic similarity, low-level texture cues, and high-level manifold knowledge to discriminate panoptic targets. We conduct experiments on the Pascal VOC and the MS COCO datasets to demonstrate the approach's effectiveness and show state-of-the-art performance in the weakly-supervised panoptic segmentation problem. Codes are available at https://github.com/BraveGroup/PSPS.git.



### Minutiae-Guided Fingerprint Embeddings via Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2210.13994v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13994v2)
- **Published**: 2022-10-25 13:08:32+00:00
- **Updated**: 2022-10-26 03:26:53+00:00
- **Authors**: Steven A. Grosz, Joshua J. Engelsma, Rajeev Ranjan, Naveen Ramakrishnan, Manoj Aggarwal, Gerard G. Medioni, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Minutiae matching has long dominated the field of fingerprint recognition. However, deep networks can be used to extract fixed-length embeddings from fingerprints. To date, the few studies that have explored the use of CNN architectures to extract such embeddings have shown extreme promise. Inspired by these early works, we propose the first use of a Vision Transformer (ViT) to learn a discriminative fixed-length fingerprint embedding. We further demonstrate that by guiding the ViT to focus in on local, minutiae related features, we can boost the recognition performance. Finally, we show that by fusing embeddings learned by CNNs and ViTs we can reach near parity with a commercial state-of-the-art (SOTA) matcher. In particular, we obtain a TAR=94.23% @ FAR=0.1% on the NIST SD 302 public-domain dataset, compared to a SOTA commercial matcher which obtains TAR=96.71% @ FAR=0.1%. Additionally, our fixed-length embeddings can be matched orders of magnitude faster than the commercial system (2.5 million matches/second compared to 50K matches/second). We make our code and models publicly available to encourage further research on this topic: https://github.com/tba.



### Unsupervised domain-adaptive person re-identification with multi-camera constraints
- **Arxiv ID**: http://arxiv.org/abs/2210.13999v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.13999v1)
- **Published**: 2022-10-25 13:12:28+00:00
- **Updated**: 2022-10-25 13:12:28+00:00
- **Authors**: S. Takeuchi, F. Li, S. Iwasaki, J. Ning, G. Suzuki
- **Comment**: ICIP 2022
- **Journal**: None
- **Summary**: Person re-identification is a key technology for analyzing video-based human behavior; however, its application is still challenging in practical situations due to the performance degradation for domains different from those in the training data. Here, we propose an environment-constrained adaptive network for reducing the domain gap. This network refines pseudo-labels estimated via a self-training scheme by imposing multi-camera constraints. The proposed method incorporates person-pair information without person identity labels obtained from the environment into the model training. In addition, we develop a method that appropriately selects a person from the pair that contributes to the performance improvement. We evaluate the performance of the network using public and private datasets and confirm the performance surpasses state-of-the-art methods in domains with overlapping camera views. To the best of our knowledge, this is the first study on domain-adaptive learning with multi-camera constraints that can be obtained in real environments.



### Multi-task Video Enhancement for Dental Interventions
- **Arxiv ID**: http://arxiv.org/abs/2210.16236v1
- **DOI**: 10.1007/978-3-031-16449-1_18
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16236v1)
- **Published**: 2022-10-25 13:17:59+00:00
- **Updated**: 2022-10-25 13:17:59+00:00
- **Authors**: Efklidis Katsaros, Piotr K. Ostrowski, Krzysztof Włódarczak, Emilia Lewandowska, Jacek Ruminski, Damian Siupka-Mróz, Łukasz Lassmann, Anna Jezierska, Daniel Węsierski
- **Comment**: Accepted at MICCAI 2022:
  https://link.springer.com/chapter/10.1007/978-3-031-16449-1_18
- **Journal**: In: Wang, L., Dou, Q., Fletcher, P.T., Speidel, S., Li, S. (eds)
  Medical Image Computing and Computer Assisted Intervention - MICCAI 2022.
  MICCAI 2022. Lecture Notes in Computer Science, vol 13437. Springer, Cham
- **Summary**: A microcamera firmly attached to a dental handpiece allows dentists to continuously monitor the progress of conservative dental procedures. Video enhancement in video-assisted dental interventions alleviates low-light, noise, blur, and camera handshakes that collectively degrade visual comfort. To this end, we introduce a novel deep network for multi-task video enhancement that enables macro-visualization of dental scenes. In particular, the proposed network jointly leverages video restoration and temporal alignment in a multi-scale manner for effective video enhancement. Our experiments on videos of natural teeth in phantom scenes demonstrate that the proposed network achieves state-of-the-art results in multiple tasks with near real-time processing. We release Vident-lab at https://doi.org/10.34808/1jby-ay90, the first dataset of dental videos with multi-task labels to facilitate further research in relevant video processing applications.



### MEW-UNet: Multi-axis representation learning in frequency domain for medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.14007v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14007v1)
- **Published**: 2022-10-25 13:22:41+00:00
- **Updated**: 2022-10-25 13:22:41+00:00
- **Authors**: Jiacheng Ruan, Mingye Xie, Suncheng Xiang, Ting Liu, Yuzhuo Fu
- **Comment**: 5 pages, 3 figures, 4 tables
- **Journal**: None
- **Summary**: Recently, Visual Transformer (ViT) has been widely used in various fields of computer vision due to applying self-attention mechanism in the spatial domain to modeling global knowledge. Especially in medical image segmentation (MIS), many works are devoted to combining ViT and CNN, and even some works directly utilize pure ViT-based models. However, recent works improved models in the aspect of spatial domain while ignoring the importance of frequency domain information. Therefore, we propose Multi-axis External Weights UNet (MEW-UNet) for MIS based on the U-shape architecture by replacing self-attention in ViT with our Multi-axis External Weights block. Specifically, our block performs a Fourier transform on the three axes of the input feature and assigns the external weight in the frequency domain, which is generated by our Weights Generator. Then, an inverse Fourier transform is performed to change the features back to the spatial domain. We evaluate our model on four datasets and achieve state-of-the-art performances. In particular, on the Synapse dataset, our method outperforms MT-UNet by 10.15mm in terms of HD95. Code is available at https://github.com/JCruan519/MEW-UNet.



### A Comparative Study on Deep-Learning Methods for Dense Image Matching of Multi-angle and Multi-date Remote Sensing Stereo Images
- **Arxiv ID**: http://arxiv.org/abs/2210.14031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14031v1)
- **Published**: 2022-10-25 14:10:04+00:00
- **Updated**: 2022-10-25 14:10:04+00:00
- **Authors**: Hessah Albanwan, Rongjun Qin
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: Deep learning (DL) stereo matching methods gained great attention in remote sensing satellite datasets. However, most of these existing studies conclude assessments based only on a few/single stereo images lacking a systematic evaluation on how robust DL methods are on satellite stereo images with varying radiometric and geometric configurations. This paper provides an evaluation of four DL stereo matching methods through hundreds of multi-date multi-site satellite stereo pairs with varying geometric configurations, against the traditional well-practiced Census-SGM (Semi-global matching), to comprehensively understand their accuracy, robustness, generalization capabilities, and their practical potential. The DL methods include a learning-based cost metric through convolutional neural networks (MC-CNN) followed by SGM, and three end-to-end (E2E) learning models using Geometry and Context Network (GCNet), Pyramid Stereo Matching Network (PSMNet), and LEAStereo. Our experiments show that E2E algorithms can achieve upper limits of geometric accuracies, while may not generalize well for unseen data. The learning-based cost metric and Census-SGM are rather robust and can consistently achieve acceptable results. All DL algorithms are robust to geometric configurations of stereo pairs and are less sensitive in comparison to the Census-SGM, while learning-based cost metrics can generalize on satellite images when trained on different datasets (airborne or ground-view).



### On Fine-Tuned Deep Features for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2210.14083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14083v1)
- **Published**: 2022-10-25 15:07:04+00:00
- **Updated**: 2022-10-25 15:07:04+00:00
- **Authors**: Qian Wang, Toby P. Breckon
- **Comment**: None
- **Journal**: None
- **Summary**: Prior feature transformation based approaches to Unsupervised Domain Adaptation (UDA) employ the deep features extracted by pre-trained deep models without fine-tuning them on the specific source or target domain data for a particular domain adaptation task. In contrast, end-to-end learning based approaches optimise the pre-trained backbones and the customised adaptation modules simultaneously to learn domain-invariant features for UDA. In this work, we explore the potential of combining fine-tuned features and feature transformation based UDA methods for improved domain adaptation performance. Specifically, we integrate the prevalent progressive pseudo-labelling techniques into the fine-tuning framework to extract fine-tuned features which are subsequently used in a state-of-the-art feature transformation based domain adaptation method SPL (Selective Pseudo-Labeling). Thorough experiments with multiple deep models including ResNet-50/101 and DeiT-small/base are conducted to demonstrate the combination of fine-tuned features and SPL can achieve state-of-the-art performance on several benchmark datasets.



### Search for Concepts: Discovering Visual Concepts Using Direct Optimization
- **Arxiv ID**: http://arxiv.org/abs/2210.14808v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14808v1)
- **Published**: 2022-10-25 15:55:24+00:00
- **Updated**: 2022-10-25 15:55:24+00:00
- **Authors**: Pradyumna Reddy, Paul Guerrero, Niloy J. Mitra
- **Comment**: None
- **Journal**: None
- **Summary**: Finding an unsupervised decomposition of an image into individual objects is a key step to leverage compositionality and to perform symbolic reasoning. Traditionally, this problem is solved using amortized inference, which does not generalize beyond the scope of the training data, may sometimes miss correct decompositions, and requires large amounts of training data. We propose finding a decomposition using direct, unamortized optimization, via a combination of a gradient-based optimization for differentiable object properties and global search for non-differentiable properties. We show that using direct optimization is more generalizable, misses fewer correct decompositions, and typically requires less data than methods based on amortized inference. This highlights a weakness of the current prevalent practice of using amortized inference that can potentially be improved by integrating more direct optimization elements.



### Lafite2: Few-shot Text-to-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2210.14124v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.14124v1)
- **Published**: 2022-10-25 16:22:23+00:00
- **Updated**: 2022-10-25 16:22:23+00:00
- **Authors**: Yufan Zhou, Chunyuan Li, Changyou Chen, Jianfeng Gao, Jinhui Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Text-to-image generation models have progressed considerably in recent years, which can now generate impressive realistic images from arbitrary text. Most of such models are trained on web-scale image-text paired datasets, which may not be affordable for many researchers. In this paper, we propose a novel method for pre-training text-to-image generation model on image-only datasets. It considers a retrieval-then-optimization procedure to synthesize pseudo text features: for a given image, relevant pseudo text features are first retrieved, then optimized for better alignment. The low requirement of the proposed method yields high flexibility and usability: it can be beneficial to a wide range of settings, including the few-shot, semi-supervised and fully-supervised learning; it can be applied on different models including generative adversarial networks (GANs) and diffusion models. Extensive experiments illustrate the effectiveness of the proposed method. On MS-COCO dataset, our GAN model obtains Fr\'echet Inception Distance (FID) of 6.78 which is the new state-of-the-art (SoTA) of GANs under fully-supervised setting. Our diffusion model obtains FID of 8.42 and 4.28 on zero-shot and supervised setting respectively, which are competitive to SoTA diffusion models with a much smaller model size.



### Learning Explicit Object-Centric Representations with Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2210.14139v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.14139v1)
- **Published**: 2022-10-25 16:39:49+00:00
- **Updated**: 2022-10-25 16:39:49+00:00
- **Authors**: Oscar Vikström, Alexander Ilin
- **Comment**: None
- **Journal**: None
- **Summary**: With the recent successful adaptation of transformers to the vision domain, particularly when trained in a self-supervised fashion, it has been shown that vision transformers can learn impressive object-reasoning-like behaviour and features expressive for the task of object segmentation in images. In this paper, we build on the self-supervision task of masked autoencoding and explore its effectiveness for explicitly learning object-centric representations with transformers. To this end, we design an object-centric autoencoder using transformers only and train it end-to-end to reconstruct full images from unmasked patches. We show that the model efficiently learns to decompose simple scenes as measured by segmentation metrics on several multi-object benchmarks.



### From colouring-in to pointillism: revisiting semantic segmentation supervision
- **Arxiv ID**: http://arxiv.org/abs/2210.14142v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14142v2)
- **Published**: 2022-10-25 16:42:03+00:00
- **Updated**: 2022-11-17 16:20:21+00:00
- **Authors**: Rodrigo Benenson, Vittorio Ferrari
- **Comment**: Open Images V7 available at https://g.co/dataset/open-images
- **Journal**: None
- **Summary**: The prevailing paradigm for producing semantic segmentation training data relies on densely labelling each pixel of each image in the training set, akin to colouring-in books. This approach becomes a bottleneck when scaling up in the number of images, classes, and annotators. Here we propose instead a pointillist approach for semantic segmentation annotation, where only point-wise yes/no questions are answered. We explore design alternatives for such an active learning approach, measure the speed and consistency of human annotators on this task, show that this strategy enables training good segmentation models, and that it is suitable for evaluating models at test time. As concrete proof of the scalability of our method, we collected and released 22.6M point labels over 4,171 classes on the Open Images dataset. Our results enable to rethink the semantic segmentation pipeline of annotation, training, and evaluation from a pointillism point of view.



### Redistributor: Transforming Empirical Data Distributions
- **Arxiv ID**: http://arxiv.org/abs/2210.14219v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MS
- **Links**: [PDF](http://arxiv.org/pdf/2210.14219v1)
- **Published**: 2022-10-25 17:59:03+00:00
- **Updated**: 2022-10-25 17:59:03+00:00
- **Authors**: Pavol Harar, Dennis Elbrächter, Monika Dörfler, Kory D. Johnson
- **Comment**: 19 pages, 8 figures
- **Journal**: None
- **Summary**: We present an algorithm and package, Redistributor, which forces a collection of scalar samples to follow a desired distribution. When given independent and identically distributed samples of some random variable $S$ and the continuous cumulative distribution function of some desired target $T$, it provably produces a consistent estimator of the transformation $R$ which satisfies $R(S)=T$ in distribution. As the distribution of $S$ or $T$ may be unknown, we also include algorithms for efficiently estimating these distributions from samples. This allows for various interesting use cases in image processing, where Redistributor serves as a remarkably simple and easy-to-use tool that is capable of producing visually appealing results. The package is implemented in Python and is optimized to efficiently handle large data sets, making it also suitable as a preprocessing step in machine learning. The source code is available at https://gitlab.com/paloha/redistributor.



### PlanT: Explainable Planning Transformers via Object-Level Representations
- **Arxiv ID**: http://arxiv.org/abs/2210.14222v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.14222v1)
- **Published**: 2022-10-25 17:59:46+00:00
- **Updated**: 2022-10-25 17:59:46+00:00
- **Authors**: Katrin Renz, Kashyap Chitta, Otniel-Bogdan Mercea, A. Sophia Koepke, Zeynep Akata, Andreas Geiger
- **Comment**: CoRL 2022. Project Page: https://www.katrinrenz.de/plant/
- **Journal**: None
- **Summary**: Planning an optimal route in a complex environment requires efficient reasoning about the surrounding scene. While human drivers prioritize important objects and ignore details not relevant to the decision, learning-based planners typically extract features from dense, high-dimensional grid representations containing all vehicle and road context information. In this paper, we propose PlanT, a novel approach for planning in the context of self-driving that uses a standard transformer architecture. PlanT is based on imitation learning with a compact object-level input representation. On the Longest6 benchmark for CARLA, PlanT outperforms all prior methods (matching the driving score of the expert) while being 5.3x faster than equivalent pixel-based planning baselines during inference. Combining PlanT with an off-the-shelf perception module provides a sensor-based driving system that is more than 10 points better in terms of driving score than the existing state of the art. Furthermore, we propose an evaluation protocol to quantify the ability of planners to identify relevant objects, providing insights regarding their decision-making. Our results indicate that PlanT can focus on the most relevant object in the scene, even when this object is geometrically distant.



### A Survey on 3D-aware Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2210.14267v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, A.1; I.4; I.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2210.14267v2)
- **Published**: 2022-10-25 18:45:08+00:00
- **Updated**: 2022-10-30 08:35:21+00:00
- **Authors**: Weihao Xia, Jing-Hao Xue
- **Comment**: Project: https://weihaox.github.io/awesome-3D-aware-synthesis
- **Journal**: None
- **Summary**: Recent years have seen remarkable progress in deep learning powered visual content creation. This includes 3D-aware generative image synthesis, which produces high-fidelity images in a 3D-consistent manner while simultaneously capturing compact surfaces of objects from pure image collections without the need for any 3D supervision, thus bridging the gap between 2D imagery and 3D reality. The 3D-aware generative models have shown that the introduction of 3D information can lead to more controllable image generation. The task of 3D-aware image synthesis has taken the field of computer vision by storm, with hundreds of papers accepted to top-tier journals and conferences in recent year (mainly the past two years), but there lacks a comprehensive survey of this remarkable and swift progress. Our survey aims to introduce new researchers to this topic, provide a useful reference for related works, and stimulate future research directions through our discussion section. Apart from the presented papers, we aim to constantly update the latest relevant papers along with corresponding implementations at https://weihaox.github.io/awesome-3D-aware-synthesis.



### Guiding Users to Where to Give Color Hints for Efficient Interactive Sketch Colorization via Unsupervised Region Prioritization
- **Arxiv ID**: http://arxiv.org/abs/2210.14270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14270v1)
- **Published**: 2022-10-25 18:50:09+00:00
- **Updated**: 2022-10-25 18:50:09+00:00
- **Authors**: Youngin Cho, Junsoo Lee, Soyoung Yang, Juntae Kim, Yeojeong Park, Haneol Lee, Mohammad Azam Khan, Daesik Kim, Jaegul Choo
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: Existing deep interactive colorization models have focused on ways to utilize various types of interactions, such as point-wise color hints, scribbles, or natural-language texts, as methods to reflect a user's intent at runtime. However, another approach, which actively informs the user of the most effective regions to give hints for sketch image colorization, has been under-explored. This paper proposes a novel model-guided deep interactive colorization framework that reduces the required amount of user interactions, by prioritizing the regions in a colorization model. Our method, called GuidingPainter, prioritizes these regions where the model most needs a color hint, rather than just relying on the user's manual decision on where to give a color hint. In our extensive experiments, we show that our approach outperforms existing interactive colorization methods in terms of the conventional metrics, such as PSNR and FID, and reduces required amount of interactions.



### Learning to Augment via Implicit Differentiation for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2210.14271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14271v1)
- **Published**: 2022-10-25 18:51:51+00:00
- **Updated**: 2022-10-25 18:51:51+00:00
- **Authors**: Tingwei Wang, Da Li, Kaiyang Zhou, Tao Xiang, Yi-Zhe Song
- **Comment**: BMVC 2022 (Oral)
- **Journal**: None
- **Summary**: Machine learning models are intrinsically vulnerable to domain shift between training and testing data, resulting in poor performance in novel domains. Domain generalization (DG) aims to overcome the problem by leveraging multiple source domains to learn a domain-generalizable model. In this paper, we propose a novel augmentation-based DG approach, dubbed AugLearn. Different from existing data augmentation methods, our AugLearn views a data augmentation module as hyper-parameters of a classification model and optimizes the module together with the model via meta-learning. Specifically, at each training step, AugLearn (i) divides source domains into a pseudo source and a pseudo target set, and (ii) trains the augmentation module in such a way that the augmented (synthetic) images can make the model generalize well on the pseudo target set. Moreover, to overcome the expensive second-order gradient computation during meta-learning, we formulate an efficient joint training algorithm, for both the augmentation module and the classification model, based on the implicit function theorem. With the flexibility of augmenting data in both time and frequency spaces, AugLearn shows effectiveness on three standard DG benchmarks, PACS, Office-Home and Digits-DG.



### Accelerating Certified Robustness Training via Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/2210.14283v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14283v1)
- **Published**: 2022-10-25 19:12:28+00:00
- **Updated**: 2022-10-25 19:12:28+00:00
- **Authors**: Pratik Vaishnavi, Kevin Eykholt, Amir Rahmati
- **Comment**: NeurIPS '22 Camera Ready version (with appendix)
- **Journal**: None
- **Summary**: Training deep neural network classifiers that are certifiably robust against adversarial attacks is critical to ensuring the security and reliability of AI-controlled systems. Although numerous state-of-the-art certified training methods have been developed, they are computationally expensive and scale poorly with respect to both dataset and network complexity. Widespread usage of certified training is further hindered by the fact that periodic retraining is necessary to incorporate new data and network improvements. In this paper, we propose Certified Robustness Transfer (CRT), a general-purpose framework for reducing the computational overhead of any certifiably robust training method through knowledge transfer. Given a robust teacher, our framework uses a novel training loss to transfer the teacher's robustness to the student. We provide theoretical and empirical validation of CRT. Our experiments on CIFAR-10 show that CRT speeds up certified robustness training by $8 \times$ on average across three different architecture generations while achieving comparable robustness to state-of-the-art methods. We also show that CRT can scale to large-scale datasets like ImageNet.



### Refining Action Boundaries for One-stage Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.14284v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14284v1)
- **Published**: 2022-10-25 19:14:07+00:00
- **Updated**: 2022-10-25 19:14:07+00:00
- **Authors**: Hanyuan Wang, Majid Mirmehdi, Dima Damen, Toby Perrett
- **Comment**: Accepted to AVSS 2022. Our code is available at
  https://github.com/hanielwang/Refining_Boundary_Head.git
- **Journal**: None
- **Summary**: Current one-stage action detection methods, which simultaneously predict action boundaries and the corresponding class, do not estimate or use a measure of confidence in their boundary predictions, which can lead to inaccurate boundaries. We incorporate the estimation of boundary confidence into one-stage anchor-free detection, through an additional prediction head that predicts the refined boundaries with higher confidence. We obtain state-of-the-art performance on the challenging EPIC-KITCHENS-100 action detection as well as the standard THUMOS14 action detection benchmarks, and achieve improvement on the ActivityNet-1.3 benchmark.



### Cross-View Image Sequence Geo-localization
- **Arxiv ID**: http://arxiv.org/abs/2210.14295v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14295v2)
- **Published**: 2022-10-25 19:46:18+00:00
- **Updated**: 2022-11-02 05:40:52+00:00
- **Authors**: Xiaohan Zhang, Waqas Sultani, Safwan Wshah
- **Comment**: None
- **Journal**: None
- **Summary**: Cross-view geo-localization aims to estimate the GPS location of a query ground-view image by matching it to images from a reference database of geo-tagged aerial images. To address this challenging problem, recent approaches use panoramic ground-view images to increase the range of visibility. Although appealing, panoramic images are not readily available compared to the videos of limited Field-Of-View (FOV) images. In this paper, we present the first cross-view geo-localization method that works on a sequence of limited FOV images. Our model is trained end-to-end to capture the temporal structure that lies within the frames using the attention-based temporal feature aggregation module. To robustly tackle different sequences length and GPS noises during inference, we propose to use a sequential dropout scheme to simulate variant length sequences. To evaluate the proposed approach in realistic settings, we present a new large-scale dataset containing ground-view sequences along with the corresponding aerial-view images. Extensive experiments and comparisons demonstrate the superiority of the proposed approach compared to several competitive baselines.



### Progressively refined deep joint registration segmentation (ProRSeg) of gastrointestinal organs at risk: Application to MRI and cone-beam CT
- **Arxiv ID**: http://arxiv.org/abs/2210.14297v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14297v1)
- **Published**: 2022-10-25 19:47:30+00:00
- **Updated**: 2022-10-25 19:47:30+00:00
- **Authors**: Jue Jiang, Jun Hong, Kathryn Tringale, Marsha Reyngold, Christopher Crane, Neelam Tyagi, Harini Veeraraghavan
- **Comment**: This manuscript is currently under review at Medical Physics
- **Journal**: None
- **Summary**: Method: ProRSeg was trained using 5-fold cross-validation with 110 T2-weighted MRI acquired at 5 treatment fractions from 10 different patients, taking care that same patient scans were not placed in training and testing folds. Segmentation accuracy was measured using Dice similarity coefficient (DSC) and Hausdorff distance at 95th percentile (HD95). Registration consistency was measured using coefficient of variation (CV) in displacement of OARs. Ablation tests and accuracy comparisons against multiple methods were done. Finally, applicability of ProRSeg to segment cone-beam CT (CBCT) scans was evaluated on 80 scans using 5-fold cross-validation. Results: ProRSeg processed 3D volumes (128 $\times$ 192 $\times$ 128) in 3 secs on a NVIDIA Tesla V100 GPU. It's segmentations were significantly more accurate ($p<0.001$) than compared methods, achieving a DSC of 0.94 $\pm$0.02 for liver, 0.88$\pm$0.04 for large bowel, 0.78$\pm$0.03 for small bowel and 0.82$\pm$0.04 for stomach-duodenum from MRI. ProRSeg achieved a DSC of 0.72$\pm$0.01 for small bowel and 0.76$\pm$0.03 for stomach-duodenum from CBCT. ProRSeg registrations resulted in the lowest CV in displacement (stomach-duodenum $CV_{x}$: 0.75\%, $CV_{y}$: 0.73\%, and $CV_{z}$: 0.81\%; small bowel $CV_{x}$: 0.80\%, $CV_{y}$: 0.80\%, and $CV_{z}$: 0.68\%; large bowel $CV_{x}$: 0.71\%, $CV_{y}$ : 0.81\%, and $CV_{z}$: 0.75\%). ProRSeg based dose accumulation accounting for intra-fraction (pre-treatment to post-treatment MRI scan) and inter-fraction motion showed that the organ dose constraints were violated in 4 patients for stomach-duodenum and for 3 patients for small bowel. Study limitations include lack of independent testing and ground truth phantom datasets to measure dose accumulation accuracy.



### Object recognition in atmospheric turbulence scenes
- **Arxiv ID**: http://arxiv.org/abs/2210.14318v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14318v2)
- **Published**: 2022-10-25 20:21:25+00:00
- **Updated**: 2023-05-29 18:55:03+00:00
- **Authors**: Disen Hu, Nantheera Anantrasirichai
- **Comment**: None
- **Journal**: None
- **Summary**: The influence of atmospheric turbulence on acquired surveillance imagery poses significant challenges in image interpretation and scene analysis. Conventional approaches for target classification and tracking are less effective under such conditions. While deep-learning-based object detection methods have shown great success in normal conditions, they cannot be directly applied to atmospheric turbulence sequences. In this paper, we propose a novel framework that learns distorted features to detect and classify object types in turbulent environments. Specifically, we utilise deformable convolutions to handle spatial turbulent displacement. Features are extracted using a feature pyramid network, and Faster R-CNN is employed as the object detector. Experimental results on a synthetic VOC dataset demonstrate that the proposed framework outperforms the benchmark with a mean Average Precision (mAP) score exceeding 30%. Additionally, subjective results on real data show significant improvement in performance.



### Shared Manifold Learning Using a Triplet Network for Multiple Sensor Translation and Fusion with Missing Data
- **Arxiv ID**: http://arxiv.org/abs/2210.17311v1
- **DOI**: 10.1109/JSTARS.2022.3217485
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.17311v1)
- **Published**: 2022-10-25 20:22:09+00:00
- **Updated**: 2022-10-25 20:22:09+00:00
- **Authors**: Aditya Dutt, Alina Zare, Paul Gader
- **Comment**: 19 pages, 16 figures; Accepted to IEEE Journal of Selected Topics in
  Applied Earth Observations and Remote Sensing
- **Journal**: None
- **Summary**: Heterogeneous data fusion can enhance the robustness and accuracy of an algorithm on a given task. However, due to the difference in various modalities, aligning the sensors and embedding their information into discriminative and compact representations is challenging. In this paper, we propose a Contrastive learning based MultiModal Alignment Network (CoMMANet) to align data from different sensors into a shared and discriminative manifold where class information is preserved. The proposed architecture uses a multimodal triplet autoencoder to cluster the latent space in such a way that samples of the same classes from each heterogeneous modality are mapped close to each other. Since all the modalities exist in a shared manifold, a unified classification framework is proposed. The resulting latent space representations are fused to perform more robust and accurate classification. In a missing sensor scenario, the latent space of one sensor is easily and efficiently predicted using another sensor's latent space, thereby allowing sensor translation. We conducted extensive experiments on a manually labeled multimodal dataset containing hyperspectral data from AVIRIS-NG and NEON, and LiDAR (light detection and ranging) data from NEON. Lastly, the model is validated on two benchmark datasets: Berlin Dataset (hyperspectral and synthetic aperture radar) and MUUFL Gulfport Dataset (hyperspectral and LiDAR). A comparison made with other methods demonstrates the superiority of this method. We achieved a mean overall accuracy of 94.3% on the MUUFL dataset and the best overall accuracy of 71.26% on the Berlin dataset, which is better than other state-of-the-art approaches.



### Explicitly Increasing Input Information Density for Vision Transformers on Small Datasets
- **Arxiv ID**: http://arxiv.org/abs/2210.14319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14319v1)
- **Published**: 2022-10-25 20:24:53+00:00
- **Updated**: 2022-10-25 20:24:53+00:00
- **Authors**: Xiangyu Chen, Ying Qin, Wenju Xu, Andrés M. Bur, Cuncong Zhong, Guanghui Wang
- **Comment**: Accepted to NeurIPS workshop (VTTA) 2022
- **Journal**: None
- **Summary**: Vision Transformers have attracted a lot of attention recently since the successful implementation of Vision Transformer (ViT) on vision tasks. With vision Transformers, specifically the multi-head self-attention modules, networks can capture long-term dependencies inherently. However, these attention modules normally need to be trained on large datasets, and vision Transformers show inferior performance on small datasets when training from scratch compared with widely dominant backbones like ResNets. Note that the Transformer model was first proposed for natural language processing, which carries denser information than natural images. To boost the performance of vision Transformers on small datasets, this paper proposes to explicitly increase the input information density in the frequency domain. Specifically, we introduce selecting channels by calculating the channel-wise heatmaps in the frequency domain using Discrete Cosine Transform (DCT), reducing the size of input while keeping most information and hence increasing the information density. As a result, 25% fewer channels are kept while better performance is achieved compared with previous work. Extensive experiments demonstrate the effectiveness of the proposed approach on five small-scale datasets, including CIFAR-10/100, SVHN, Flowers-102, and Tiny ImageNet. The accuracy has been boosted up to 17.05% with Swin and Focal Transformers. Codes are available at https://github.com/xiangyu8/DenseVT.



### New wrapper method based on normalized mutual information for dimension reduction and classification of hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/2210.14346v1
- **DOI**: 10.1109/ICOA.2018.8370546
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14346v1)
- **Published**: 2022-10-25 21:17:11+00:00
- **Updated**: 2022-10-25 21:17:11+00:00
- **Authors**: Hasna Nhaila, Asma Elmaizi, Elkebir Sarhrouni, Ahmed Hammouch
- **Comment**: None
- **Journal**: Proceedings of the 2018 International Conference on Optimization
  and Applications, ICOA 2018, 2018, pp. 1-7
  http://www.scopus.com/inward/record.url?eid=2-s2.0-85048829863&partnerID=MN8TOARS
- **Summary**: Feature selection is one of the most important problems in hyperspectral images classification. It consists to choose the most informative bands from the entire set of input datasets and discard the noisy, redundant and irrelevant ones. In this context, we propose a new wrapper method based on normalized mutual information (NMI) and error probability (PE) using support vector machine (SVM) to reduce the dimensionality of the used hyperspectral images and increase the classification efficiency. The experiments have been performed on two challenging hyperspectral benchmarks datasets captured by the NASA's Airborne Visible/Infrared Imaging Spectrometer Sensor (AVIRIS). Several metrics had been calculated to evaluate the performance of the proposed algorithm. The obtained results prove that our method can increase the classification performance and provide an accurate thematic map in comparison with other reproduced algorithms. This method may be improved for more classification efficiency. Keywords-Feature selection, hyperspectral images, classification, wrapper, normalized mutual information, support vector machine.



### Multi-Domain Long-Tailed Learning by Augmenting Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/2210.14358v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14358v2)
- **Published**: 2022-10-25 21:54:26+00:00
- **Updated**: 2023-02-06 08:25:47+00:00
- **Authors**: Xinyu Yang, Huaxiu Yao, Allan Zhou, Chelsea Finn
- **Comment**: None
- **Journal**: None
- **Summary**: There is an inescapable long-tailed class-imbalance issue in many real-world classification problems. Existing long-tailed classification methods focus on the single-domain setting, where all examples are drawn from the same distribution. However, real-world scenarios often involve multiple domains with distinct imbalanced class distributions. We study this multi-domain long-tailed learning problem and aim to produce a model that generalizes well across all classes and domains. Towards that goal, we introduce TALLY, which produces invariant predictors by balanced augmenting hidden representations over domains and classes. Built upon a proposed selective balanced sampling strategy, TALLY achieves this by mixing the semantic representation of one example with the domain-associated nuisances of another, producing a new representation for use as data augmentation. To improve the disentanglement of semantic representations, TALLY further utilizes a domain-invariant class prototype that averages out domain-specific effects. We evaluate TALLY on four long-tailed variants of classical domain generalization benchmarks and two real-world imbalanced multi-domain datasets. The results indicate that TALLY consistently outperforms other state-of-the-art methods in both subpopulation shift and domain shift.



### Fusing Modalities by Multiplexed Graph Neural Networks for Outcome Prediction in Tuberculosis
- **Arxiv ID**: http://arxiv.org/abs/2210.14377v1
- **DOI**: 10.1007/978-3-031-16449-1_28
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.14377v1)
- **Published**: 2022-10-25 23:03:05+00:00
- **Updated**: 2022-10-25 23:03:05+00:00
- **Authors**: Niharika S. D'Souza, Hongzhi Wang, Andrea Giovannini, Antonio Foncubierta-Rodriguez, Kristen L. Beck, Orest Boyko, Tanveer Syeda-Mahmood
- **Comment**: Accepted into MICCAI 2022
- **Journal**: None
- **Summary**: In a complex disease such as tuberculosis, the evidence for the disease and its evolution may be present in multiple modalities such as clinical, genomic, or imaging data. Effective patient-tailored outcome prediction and therapeutic guidance will require fusing evidence from these modalities. Such multimodal fusion is difficult since the evidence for the disease may not be uniform across all modalities, not all modality features may be relevant, or not all modalities may be present for all patients. All these nuances make simple methods of early, late, or intermediate fusion of features inadequate for outcome prediction. In this paper, we present a novel fusion framework using multiplexed graphs and derive a new graph neural network for learning from such graphs. Specifically, the framework allows modalities to be represented through their targeted encodings, and models their relationship explicitly via multiplexed graphs derived from salient features in a combined latent space. We present results that show that our proposed method outperforms state-of-the-art methods of fusing modalities for multi-outcome prediction on a large Tuberculosis (TB) dataset.



### CLIP-FLow: Contrastive Learning by semi-supervised Iterative Pseudo labeling for Optical Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/2210.14383v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14383v3)
- **Published**: 2022-10-25 23:22:25+00:00
- **Updated**: 2022-12-03 02:12:15+00:00
- **Authors**: Zhiqi Zhang, Nitin Bansal, Changjiang Cai, Pan Ji, Qingan Yan, Xiangyu Xu, Yi Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Synthetic datasets are often used to pretrain end-to-end optical flow networks, due to the lack of a large amount of labeled, real-scene data. But major drops in accuracy occur when moving from synthetic to real scenes. How do we better transfer the knowledge learned from synthetic to real domains? To this end, we propose CLIP-FLow, a semi-supervised iterative pseudo-labeling framework to transfer the pretraining knowledge to the target real domain. We leverage large-scale, unlabeled real data to facilitate transfer learning with the supervision of iteratively updated pseudo-ground truth labels, bridging the domain gap between the synthetic and the real. In addition, we propose a contrastive flow loss on reference features and the warped features by pseudo ground truth flows, to further boost the accurate matching and dampen the mismatching due to motion, occlusion, or noisy pseudo labels. We adopt RAFT as the backbone and obtain an F1-all error of 4.11%, i.e. a 19% error reduction from RAFT (5.10%) and ranking 2$^{nd}$ place at submission on the KITTI 2015 benchmark. Our framework can also be extended to other models, e.g. CRAFT, reducing the F1-all error from 4.79% to 4.66% on KITTI 2015 benchmark.



### A Survey on Fundamental Concepts and Practical Challenges of Hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/2210.16237v1
- **DOI**: 10.1109/ICoCS.2014.7060990
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16237v1)
- **Published**: 2022-10-25 23:52:14+00:00
- **Updated**: 2022-10-25 23:52:14+00:00
- **Authors**: Hasna Nhaila, Elkebir Sarhrouni, Ahmed Hammouch
- **Comment**: None
- **Journal**: 2014 2nd World Conference on Complex Systems, WCCS 2014, 2015, pp.
  659-664, 7060990.
  http://www.scopus.com/inward/record.url?eid=2-s2.0-84929207477&partnerID=MN8TOARS
- **Summary**: The Remote sensing provides a synoptic view of land by detecting the energy reflected from Earth's surface. The Hyperspectral images (HSI) use perfect sensors that extract more than a hundred of images, with more detailed information than using traditional Multispectral data. In this paper, we aim to study this aspect of communication in the case of passive reception. First, a brief overview of acquisition process and treatment of Hyperspectral images is provided. Then, we explain representation spaces and the various analysis methods of these images. Furthermore, the factors influencing this analysis are investigated and some applications, in this area, are presented. Finally, we explain the relationship between Hyperspectral images and Datamining and we outline the open issues related to this area. So we consider the case study: HSI AVIRIS 92AV3C. This study serves as map of route for integrating classification methods in the higher dimensionality data.   Keywords-component: Hyperspectral images, Passive Sensing,Classification, Data mining.



### Hyperspectral images classification and Dimensionality Reduction using Homogeneity feature and mutual information
- **Arxiv ID**: http://arxiv.org/abs/2210.16239v1
- **DOI**: 10.1109/ISACV.2015.7106167
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16239v1)
- **Published**: 2022-10-25 23:55:04+00:00
- **Updated**: 2022-10-25 23:55:04+00:00
- **Authors**: Hasna Nhaila, Maria Merzouqi, Elkebir Sarhrouni, Ahmed Hammouch
- **Comment**: None
- **Journal**: 2015 Intelligent Systems and Computer Vision, ISCV 2015, 2015,
  7106167 -
  http://www.scopus.com/inward/record.url?eid=2-s2.0-84934343941&partnerID=MN8TOARS
- **Summary**: The Hyperspectral image (HSI) contains several hundred bands of the same region called the Ground Truth (GT). The bands are taken in juxtaposed frequencies, but some of them are noisily measured or contain no information. For the classification, the selection of bands, affects significantly the results of classification, in fact, using a subset of relevant bands, these results can be better than those obtained using all bands, from which the need to reduce the dimensionality of the HSI. In this paper, a categorization of dimensionality reduction methods, according to the generation process, is presented. Furthermore, we reproduce an algorithm based on mutual information (MI) to reduce dimensionality by features selection and we introduce an algorithm using mutual information and homogeneity. The two schemas are a filter strategy. Finally, to validate this, we consider the case study AVIRIS HSI 92AV3C.   Keywords: Hyperspectrale images; classification; features selection; mutual information; homogeneity



