# Arxiv Papers in cs.CV on 2022-10-20
### Does Learning from Decentralized Non-IID Unlabeled Data Benefit from Self Supervision?
- **Arxiv ID**: http://arxiv.org/abs/2210.10947v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.10947v2)
- **Published**: 2022-10-20 01:32:41+00:00
- **Updated**: 2023-02-28 16:54:02+00:00
- **Authors**: Lirui Wang, Kaiqing Zhang, Yunzhu Li, Yonglong Tian, Russ Tedrake
- **Comment**: None
- **Journal**: None
- **Summary**: Decentralized learning has been advocated and widely deployed to make efficient use of distributed datasets, with an extensive focus on supervised learning (SL) problems. Unfortunately, the majority of real-world data are unlabeled and can be highly heterogeneous across sources. In this work, we carefully study decentralized learning with unlabeled data through the lens of self-supervised learning (SSL), specifically contrastive visual representation learning. We study the effectiveness of a range of contrastive learning algorithms under decentralized learning settings, on relatively large-scale datasets including ImageNet-100, MS-COCO, and a new real-world robotic warehouse dataset. Our experiments show that the decentralized SSL (Dec-SSL) approach is robust to the heterogeneity of decentralized datasets, and learns useful representation for object classification, detection, and segmentation tasks. This robustness makes it possible to significantly reduce communication and reduce the participation ratio of data sources with only minimal drops in performance. Interestingly, using the same amount of data, the representation learned by Dec-SSL can not only perform on par with that learned by centralized SSL which requires communication and excessive data storage costs, but also sometimes outperform representations extracted from decentralized SL which requires extra knowledge about the data labels. Finally, we provide theoretical insights into understanding why data heterogeneity is less of a concern for Dec-SSL objectives, and introduce feature alignment and clustering techniques to develop a new Dec-SSL algorithm that further improves the performance, in the face of highly non-IID data. Our study presents positive evidence to embrace unlabeled data in decentralized learning, and we hope to provide new insights into whether and why decentralized SSL is effective.



### Non-Iterative Scribble-Supervised Learning with Pacing Pseudo-Masks for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.10956v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10956v1)
- **Published**: 2022-10-20 01:57:44+00:00
- **Updated**: 2022-10-20 01:57:44+00:00
- **Authors**: Zefan Yang, Di Lin, Dong Ni, Yi Wang
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: Scribble-supervised medical image segmentation tackles the limitation of sparse masks. Conventional approaches alternate between: labeling pseudo-masks and optimizing network parameters. However, such iterative two-stage paradigm is unwieldy and could be trapped in poor local optima since the networks undesirably regress to the erroneous pseudo-masks. To address these issues, we propose a non-iterative method where a stream of varying (pacing) pseudo-masks teach a network via consistency training, named PacingPseudo. Our motivation lies first in a non-iterative process. Interestingly, it can be achieved gracefully by a siamese architecture, wherein a stream of pseudo-masks naturally assimilate a stream of predicted masks during training. Second, we make the consistency training effective with two necessary designs: (i) entropy regularization to obtain high-confidence pseudo-masks for effective teaching; and (ii) distorted augmentations to create discrepancy between the pseudo-mask and predicted-mask streams for consistency regularization. Third, we devise a new memory bank mechanism that provides an extra source of ensemble features to complement scarce labeled pixels. The efficacy of the proposed PacingPseudo is validated on three public medical image datasets, including the segmentation tasks of abdominal multi-organs, cardiac structures, and myocardium. Extensive experiments demonstrate our PacingPseudo improves the baseline by large margins and consistently outcompetes several previous methods. In some cases, our PacingPseudo achieves comparable performance with its fully-supervised counterparts, showing the feasibility of our method for the challenging scribble-supervised segmentation applications. The code and scribble annotations will be publicly available.



### 3D Human Mesh Construction Leveraging Wi-Fi
- **Arxiv ID**: http://arxiv.org/abs/2210.10957v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10957v1)
- **Published**: 2022-10-20 01:58:27+00:00
- **Updated**: 2022-10-20 01:58:27+00:00
- **Authors**: Yichao Wang, Jie Yang
- **Comment**: SenSys 2022
- **Journal**: None
- **Summary**: In this paper, we present, Wi-Mesh, a WiFi vision-based 3D human mesh construction system. Our system leverages the advances of WiFi to visualize the shape and deformations of the human body for 3D mesh construction. In particular, it leverages multiple transmitting and receiving antennas on WiFi devices to estimate the two-dimensional angle of arrival (2D AoA) of the WiFi signal reflections to enable WiFi devices to see the physical environment as we humans do. It then extracts only the images of the human body from the physical environment and leverages deep learning models to digitize the extracted human body into a 3D mesh representation. Experimental evaluation under various indoor environments shows that Wi-Mesh achieves an average vertices location error of 2.81cm and joint position error of 2.4cm, which is comparable to the systems that utilize specialized and dedicated hardware. The proposed system has the advantage of re-using the WiFi devices that already exist in the environment for potential mass adoption. It can also work in non-line of sight (NLoS), poor lighting conditions, and baggy clothes, where the camera-based systems do not work well.



### Geo6D: Geometric Constraints Learning for 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2210.10959v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10959v6)
- **Published**: 2022-10-20 02:00:58+00:00
- **Updated**: 2023-08-22 01:31:55+00:00
- **Authors**: Jianqiu Chen, Mingshan Sun, Ye Zheng, Tianpeng Bao, Zhenyu He, Donghai Li, Guoqiang Jin, Rui Zhao, Liwei Wu, Xiaoke Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Numerous 6D pose estimation methods have been proposed that employ end-to-end regression to directly estimate the target pose parameters. Since the visible features of objects are implicitly influenced by their poses, the network allows inferring the pose by analyzing the differences in features in the visible region. However, due to the unpredictable and unrestricted range of pose variations, the implicitly learned visible feature-pose constraints are insufficiently covered by the training samples, making the network vulnerable to unseen object poses. To tackle these challenges, we proposed a novel geometric constraints learning approach called Geo6D for direct regression 6D pose estimation methods. It introduces a pose transformation formula expressed in relative offset representation, which is leveraged as geometric constraints to reconstruct the input and output targets of the network. These reconstructed data enable the network to estimate the pose based on explicit geometric constraints and relative offset representation mitigates the issue of the pose distribution gap. Extensive experimental results show that when equipped with Geo6D, the direct 6D methods achieve state-of-the-art performance on multiple datasets and demonstrate significant effectiveness, even with only 10% amount of data.



### Diffusion Models already have a Semantic Latent Space
- **Arxiv ID**: http://arxiv.org/abs/2210.10960v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10960v2)
- **Published**: 2022-10-20 02:07:23+00:00
- **Updated**: 2023-03-29 06:39:50+00:00
- **Authors**: Mingi Kwon, Jaeseok Jeong, Youngjung Uh
- **Comment**: ICLR2023 (Notable - Top 25%)
- **Journal**: None
- **Summary**: Diffusion models achieve outstanding generative performance in various domains. Despite their great success, they lack semantic latent space which is essential for controlling the generative process. To address the problem, we propose asymmetric reverse process (Asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. Our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. In addition, we introduce a principled design of the generative process for versatile editing and quality boost ing by quantifiable measures: editing strength of an interval and quality deficiency at a timestep. Our method is applicable to various architectures (DDPM++, iD- DPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN- bedroom, and METFACES). Project page: https://kwonminki.github.io/Asyrp/



### SSiT: Saliency-guided Self-supervised Image Transformer for Diabetic Retinopathy Grading
- **Arxiv ID**: http://arxiv.org/abs/2210.10969v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10969v4)
- **Published**: 2022-10-20 02:35:26+00:00
- **Updated**: 2023-07-03 06:18:21+00:00
- **Authors**: Yijin Huang, Junyan Lyu, Pujin Cheng, Roger Tam, Xiaoying Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised learning (SSL) has been widely applied to learn image representations through exploiting unlabeled images. However, it has not been fully explored in the medical image analysis field. In this work, we propose Saliency-guided Self-Supervised image Transformer (SSiT) for diabetic retinopathy (DR) grading from fundus images. We novelly introduce saliency maps into SSL, with a goal of guiding self-supervised pre-training with domain-specific prior knowledge. Specifically, two saliency-guided learning tasks are employed in SSiT: (1) We conduct saliency-guided contrastive learning based on the momentum contrast, wherein we utilize fundus images' saliency maps to remove trivial patches from the input sequences of the momentum-updated key encoder. And thus, the key encoder is constrained to provide target representations focusing on salient regions, guiding the query encoder to capture salient features. (2) We train the query encoder to predict the saliency segmentation, encouraging preservation of fine-grained information in the learned representations. Extensive experiments are conducted on four publicly-accessible fundus image datasets. The proposed SSiT significantly outperforms other representative state-of-the-art SSL methods on all datasets and under various evaluation settings, establishing the effectiveness of the learned representations from SSiT. The source code is available at https://github.com/YijinHuang/SSiT.



### A Multimodal Sensor Fusion Framework Robust to Missing Modalities for Person Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.10972v2
- **DOI**: 10.1145/3551626.3564965
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10972v2)
- **Published**: 2022-10-20 02:39:48+00:00
- **Updated**: 2022-10-22 04:51:51+00:00
- **Authors**: Vijay John, Yasutomo Kawanishi
- **Comment**: Accepted for ACM Multimedia Asia, 2022
- **Journal**: None
- **Summary**: Utilizing the sensor characteristics of the audio, visible camera, and thermal camera, the robustness of person recognition can be enhanced. Existing multimodal person recognition frameworks are primarily formulated assuming that multimodal data is always available. In this paper, we propose a novel trimodal sensor fusion framework using the audio, visible, and thermal camera, which addresses the missing modality problem. In the framework, a novel deep latent embedding framework, termed the AVTNet, is proposed to learn multiple latent embeddings. Also, a novel loss function, termed missing modality loss, accounts for possible missing modalities based on the triplet loss calculation while learning the individual latent embeddings. Additionally, a joint latent embedding utilizing the trimodal data is learnt using the multi-head attention transformer, which assigns attention weights to the different modalities. The different latent embeddings are subsequently used to train a deep neural network. The proposed framework is validated on the Speaking Faces dataset. A comparative analysis with baseline algorithms shows that the proposed framework significantly increases the person recognition accuracy while accounting for missing modalities.



### Improving Segmentation of Breast Ultrasound Images: Semi Automatic Two Pointers Histogram Splitting Technique
- **Arxiv ID**: http://arxiv.org/abs/2210.10975v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.10975v1)
- **Published**: 2022-10-20 02:46:05+00:00
- **Updated**: 2022-10-20 02:46:05+00:00
- **Authors**: Rasheed Abid, S. Kaisar Alam
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically segmenting lesion area in breast ultrasound (BUS) images is a challenging one due to its noise, speckle and artifacts. Edge-map of BUS images also does not help because in most cases the edge-map gives no information whatsoever. Almost all segmentation technique takes the edge-map of the image as its first step, though there are a few algorithms that try to avoid edge-maps as well. Improving the edge-map of breast ultrasound images theoretically improves the chances of automatic segmentation to be more precise. In this paper, we propose a semi-automatic technique of histogram splitting using two pointers. Here the user only has to select two initially guessed points denoting a circle on the region of interest (ROI). The method will automatically study the internal histogram and split it using two pointers. The output BUS image has improved edge-map and ultimately the segmentation on it is better compared to regular segmentation using same algorithm and same initialization. Also, we further processed the edge-map to have less edge-pixels to area ratio, improving the homogeneity and the chances of easy segmentation in the future.



### MGTUNet: An new UNet for colon nuclei instance segmentation and quantification
- **Arxiv ID**: http://arxiv.org/abs/2210.10981v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GT
- **Links**: [PDF](http://arxiv.org/pdf/2210.10981v1)
- **Published**: 2022-10-20 03:00:40+00:00
- **Updated**: 2022-10-20 03:00:40+00:00
- **Authors**: Liangrui Pan, Lian Wang, Mingting Liu, Zhujun Xu, Liwen Xu, Shaoliang Peng
- **Comment**: Accepted in BIBM2022(regular paper)
- **Journal**: None
- **Summary**: Colorectal cancer (CRC) is among the top three malignant tumor types in terms of morbidity and mortality. Histopathological images are the gold standard for diagnosing colon cancer. Cellular nuclei instance segmentation and classification, and nuclear component regression tasks can aid in the analysis of the tumor microenvironment in colon tissue. Traditional methods are still unable to handle both types of tasks end-to-end at the same time, and have poor prediction accuracy and high application costs. This paper proposes a new UNet model for handling nuclei based on the UNet framework, called MGTUNet, which uses Mish, Group normalization and transposed convolution layer to improve the segmentation model, and a ranger optimizer to adjust the SmoothL1Loss values. Secondly, it uses different channels to segment and classify different types of nucleus, ultimately completing the nuclei instance segmentation and classification task, and the nuclei component regression task simultaneously. Finally, we did extensive comparison experiments using eight segmentation models. By comparing the three evaluation metrics and the parameter sizes of the models, MGTUNet obtained 0.6254 on PQ, 0.6359 on mPQ, and 0.8695 on R2. Thus, the experiments demonstrated that MGTUNet is now a state-of-the-art method for quantifying histopathological images of colon cancer.



### PSA-Det3D: Pillar Set Abstraction for 3D object Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.10983v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.10983v2)
- **Published**: 2022-10-20 03:05:34+00:00
- **Updated**: 2022-10-26 09:36:39+00:00
- **Authors**: Zhicong Huang, Jingwen Zhao, Zhijie Zheng, Dihu Chena, Haifeng Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Small object detection for 3D point cloud is a challenging problem because of two limitations: (1) Perceiving small objects is much more diffcult than normal objects due to the lack of valid points. (2) Small objects are easily blocked which breaks the shape of their meshes in 3D point cloud. In this paper, we propose a pillar set abstraction (PSA) and foreground point compensation (FPC) and design a point-based detection network, PSA-Det3D, to improve the detection performance for small object. The PSA embeds a pillar query operation on the basis of set abstraction (SA) to expand its receptive field of the network, which can aggregate point-wise features effectively. To locate more occluded objects, we persent a proposal generation layer consisting of a foreground point segmentation and a FPC module. Both the foreground points and the estimated centers are finally fused together to generate the detection result. The experiments on the KITTI 3D detection benchmark show that our proposed PSA-Det3D outperforms other algorithms with high accuracy for small object detection.



### RAIS: Robust and Accurate Interactive Segmentation via Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.10984v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.10984v1)
- **Published**: 2022-10-20 03:05:44+00:00
- **Updated**: 2022-10-20 03:05:44+00:00
- **Authors**: Yuying Hao, Yi Liu, Juncai Peng, Haoyi Xiong, Guowei Chen, Shiyu Tang, Zeyu Chen, Baohua Lai
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Interactive image segmentation aims at segmenting a target region through a way of human-computer interaction. Recent works based on deep learning have achieved excellent performance, while most of them focus on improving the accuracy of the training set and ignore potential improvement on the test set. In the inference phase, they tend to have a good performance on similar domains to the training set, and lack adaptability to domain shift, so they require more user efforts to obtain satisfactory results. In this work, we propose RAIS, a robust and accurate architecture for interactive segmentation with continuous learning, where the model can learn from both train and test data sets. For efficient learning on the test set, we propose a novel optimization strategy to update global and local parameters with a basic segmentation module and adaptation module, respectively. Moreover, we perform extensive experiments on several benchmarks that show our method can handle data distribution shifts and achieves SOTA performance compared with recent interactive segmentation methods. Besides, our method also shows its robustness in the datasets of remote sensing and medical imaging where the data domains are completely different between training and testing.



### NIFT: Neural Interaction Field and Template for Object Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2210.10992v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2210.10992v3)
- **Published**: 2022-10-20 03:35:05+00:00
- **Updated**: 2023-03-01 01:30:41+00:00
- **Authors**: Zeyu Huang, Juzhan Xu, Sisi Dai, Kai Xu, Hao Zhang, Hui Huang, Ruizhen Hu
- **Comment**: ICRA 2023
- **Journal**: None
- **Summary**: We introduce NIFT, Neural Interaction Field and Template, a descriptive and robust interaction representation of object manipulations to facilitate imitation learning. Given a few object manipulation demos, NIFT guides the generation of the interaction imitation for a new object instance by matching the Neural Interaction Template (NIT) extracted from the demos in the target Neural Interaction Field (NIF) defined for the new object. Specifically, the NIF is a neural field that encodes the relationship between each spatial point and a given object, where the relative position is defined by a spherical distance function rather than occupancies or signed distances, which are commonly adopted by conventional neural fields but less informative. For a given demo interaction, the corresponding NIT is defined by a set of spatial points sampled in the demo NIF with associated neural features. To better capture the interaction, the points are sampled on the Interaction Bisector Surface (IBS), which consists of points that are equidistant to the two interacting objects and has been used extensively for interaction representation. With both point selection and pointwise features defined for better interaction encoding, NIT effectively guides the feature matching in the NIFs of the new object instances such that the relative poses are optimized to realize the manipulation while imitating the demo interactions. Experiments show that our NIFT solution outperforms state-of-the-art imitation learning methods for object manipulation and generalizes better to objects from new categories.



### Semi-supervised object detection based on single-stage detector for thighbone fracture localization
- **Arxiv ID**: http://arxiv.org/abs/2210.10998v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.10998v1)
- **Published**: 2022-10-20 03:47:23+00:00
- **Updated**: 2022-10-20 03:47:23+00:00
- **Authors**: Jinman Wei, Jinkun Yao, Guoshan Zhanga, Bin Guan, Yueming Zhang, Shaoquan Wang
- **Comment**: Preprint submitted to Applied Soft Computing
- **Journal**: None
- **Summary**: The thighbone is the largest bone supporting the lower body. If the thighbone fracture is not treated in time, it will lead to lifelong inability to walk. Correct diagnosis of thighbone disease is very important in orthopedic medicine. Deep learning is promoting the development of fracture detection technology. However, the existing computer aided diagnosis (CAD) methods baesd on deep learning rely on a large number of manually labeled data, and labeling these data costs a lot of time and energy. Therefore, we develop a object detection method with limited labeled image quantity and apply it to the thighbone fracture localization. In this work, we build a semi-supervised object detection(SSOD) framework based on single-stage detector, which including three modules: adaptive difficult sample oriented (ADSO) module, Fusion Box and deformable expand encoder (Dex encoder). ADSO module takes the classification score as the label reliability evaluation criterion by weighting, Fusion Box is designed to merge similar pseudo boxes into a reliable box for box regression and Dex encoder is proposed to enhance the adaptability of image augmentation. The experiment is conducted on the thighbone fracture dataset, which includes 3484 training thigh fracture images and 358 testing thigh fracture images. The experimental results show that the proposed method achieves the state-of-the-art AP in thighbone fracture detection at different labeled data rates, i.e. 1%, 5% and 10%. Besides, we use full data to achieve knowledge distillation, our method achieves 86.2% AP50 and 52.6% AP75.



### Visual-Semantic Contrastive Alignment for Few-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2210.11000v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11000v1)
- **Published**: 2022-10-20 03:59:40+00:00
- **Updated**: 2022-10-20 03:59:40+00:00
- **Authors**: Mohamed Afham, Ranga Rodrigo
- **Comment**: ECCV 2022 Workshop on Computer Vision in the Wild
- **Journal**: None
- **Summary**: Few-Shot learning aims to train and optimize a model that can adapt to unseen visual classes with only a few labeled examples. The existing few-shot learning (FSL) methods, heavily rely only on visual data, thus fail to capture the semantic attributes to learn a more generalized version of the visual concept from very few examples. However, it is a known fact that human visual learning benefits immensely from inputs from multiple modalities such as vision, language, and audio. Inspired by the human learning nature of encapsulating the existing knowledge of a visual category which is in the form of language, we introduce a contrastive alignment mechanism for visual and semantic feature vectors to learn much more generalized visual concepts for few-shot learning. Our method simply adds an auxiliary contrastive learning objective which captures the contextual knowledge of a visual category from a strong textual encoder in addition to the existing training mechanism. Hence, the approach is more generalized and can be plugged into any existing FSL method. The pre-trained semantic feature extractor (learned from a large-scale text corpora) we use in our approach provides a strong contextual prior knowledge to assist FSL. The experimental results done in popular FSL datasets show that our approach is generic in nature and provides a strong boost to the existing FSL baselines.



### SimpleClick: Interactive Image Segmentation with Simple Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2210.11006v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11006v3)
- **Published**: 2022-10-20 04:20:48+00:00
- **Updated**: 2023-03-11 19:36:34+00:00
- **Authors**: Qin Liu, Zhenlin Xu, Gedas Bertasius, Marc Niethammer
- **Comment**: Tech report. Update 03/11/2023: Add results on a tiny model and
  append supplementary materials
- **Journal**: None
- **Summary**: Click-based interactive image segmentation aims at extracting objects with a limited user clicking. A hierarchical backbone is the de-facto architecture for current methods. Recently, the plain, non-hierarchical Vision Transformer (ViT) has emerged as a competitive backbone for dense prediction tasks. This design allows the original ViT to be a foundation model that can be finetuned for downstream tasks without redesigning a hierarchical backbone for pretraining. Although this design is simple and has been proven effective, it has not yet been explored for interactive image segmentation. To fill this gap, we propose SimpleClick, the first interactive segmentation method that leverages a plain backbone. Based on the plain backbone, we introduce a symmetric patch embedding layer that encodes clicks into the backbone with minor modifications to the backbone itself. With the plain backbone pretrained as a masked autoencoder (MAE), SimpleClick achieves state-of-the-art performance. Remarkably, our method achieves 4.15 NoC@90 on SBD, improving 21.8% over the previous best result. Extensive evaluation on medical images demonstrates the generalizability of our method. We further develop an extremely tiny ViT backbone for SimpleClick and provide a detailed computational analysis, highlighting its suitability as a practical annotation tool.



### Towards Sustainable Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.11016v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11016v1)
- **Published**: 2022-10-20 04:49:56+00:00
- **Updated**: 2022-10-20 04:49:56+00:00
- **Authors**: Shanghua Gao, Pan Zhou, Ming-Ming Cheng, Shuicheng Yan
- **Comment**: Extension of NeurIPS 2022 workshop
- **Journal**: None
- **Summary**: Although increasingly training-expensive, most self-supervised learning (SSL) models have repeatedly been trained from scratch but not fully utilized, since only a few SOTAs are employed for downstream tasks. In this work, we explore a sustainable SSL framework with two major challenges: i) learning a stronger new SSL model based on the existing pretrained SSL model, also called as "base" model, in a cost-friendly manner, ii) allowing the training of the new model to be compatible with various base models. We propose a Target-Enhanced Conditional (TEC) scheme which introduces two components to the existing mask-reconstruction based SSL. Firstly, we propose patch-relation enhanced targets which enhances the target given by base model and encourages the new model to learn semantic-relation knowledge from the base model by using incomplete inputs. This hardening and target-enhancing help the new model surpass the base model, since they enforce additional patch relation modeling to handle incomplete input. Secondly, we introduce a conditional adapter that adaptively adjusts new model prediction to align with the target of different base models. Extensive experimental results show that our TEC scheme can accelerate the learning speed, and also improve SOTA SSL base models, e.g., MAE and iBOT, taking an explorative step towards sustainable SSL.



### An Attention-Guided and Wavelet-Constrained Generative Adversarial Network for Infrared and Visible Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/2210.11018v2
- **DOI**: 10.1016/j.infrared.2023.104570
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11018v2)
- **Published**: 2022-10-20 05:01:20+00:00
- **Updated**: 2022-10-24 07:00:00+00:00
- **Authors**: Xiaowen Liu, Renhua Wang, Hongtao Huo, Xin Yang, Jing Li
- **Comment**: None
- **Journal**: None
- **Summary**: The GAN-based infrared and visible image fusion methods have gained ever-increasing attention due to its effectiveness and superiority. However, the existing methods adopt the global pixel distribution of source images as the basis for discrimination, which fails to focus on the key modality information. Moreover, the dual-discriminator based methods suffer from the confrontation between the discriminators. To this end, we propose an attention-guided and wavelet-constrained GAN for infrared and visible image fusion (AWFGAN). In this method, two unique discrimination strategies are designed to improve the fusion performance. Specifically, we introduce the spatial attention modules (SAM) into the generator to obtain the spatial attention maps, and then the attention maps are utilized to force the discrimination of infrared images to focus on the target regions. In addition, we extend the discrimination range of visible information to the wavelet subspace, which can force the generator to restore the high-frequency details of visible images. Ablation experiments demonstrate the effectiveness of our method in eliminating the confrontation between discriminators. And the comparison experiments on public datasets demonstrate the effectiveness and superiority of the proposed method.



### Single Image Super-Resolution Using Lightweight Networks Based on Swin Transformer
- **Arxiv ID**: http://arxiv.org/abs/2210.11019v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11019v1)
- **Published**: 2022-10-20 05:03:16+00:00
- **Updated**: 2022-10-20 05:03:16+00:00
- **Authors**: Bolong Zhang, Juan Chen, Quan Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Image super-resolution reconstruction is an important task in the field of image processing technology, which can restore low resolution image to high quality image with high resolution. In recent years, deep learning has been applied in the field of image super-resolution reconstruction. With the continuous development of deep neural network, the quality of the reconstructed images has been greatly improved, but the model complexity has also been increased. In this paper, we propose two lightweight models named as MSwinSR and UGSwinSR based on Swin Transformer. The most important structure in MSwinSR is called Multi-size Swin Transformer Block (MSTB), which mainly contains four parallel multi-head self-attention (MSA) blocks. UGSwinSR combines U-Net and GAN with Swin Transformer. Both of them can reduce the model complexity, but MSwinSR can reach a higher objective quality, while UGSwinSR can reach a higher perceptual quality. The experimental results demonstrate that MSwinSR increases PSNR by $\mathbf{0.07dB}$ compared with the state-of-the-art model SwinIR, while the number of parameters can reduced by $\mathbf{30.68\%}$, and the calculation cost can reduced by $\mathbf{9.936\%}$. UGSwinSR can effectively reduce the amount of calculation of the network, which can reduced by $\mathbf{90.92\%}$ compared with SwinIR.



### A survey on Self Supervised learning approaches for improving Multimodal representation learning
- **Arxiv ID**: http://arxiv.org/abs/2210.11024v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11024v1)
- **Published**: 2022-10-20 05:19:49+00:00
- **Updated**: 2022-10-20 05:19:49+00:00
- **Authors**: Naman Goyal
- **Comment**: None
- **Journal**: None
- **Summary**: Recently self supervised learning has seen explosive growth and use in variety of machine learning tasks because of its ability to avoid the cost of annotating large-scale datasets.   This paper gives an overview for best self supervised learning approaches for multimodal learning. The presented approaches have been aggregated by extensive study of the literature and tackle the application of self supervised learning in different ways. The approaches discussed are cross modal generation, cross modal pretraining, cyclic translation, and generating unimodal labels in self supervised fashion.



### DeepRING: Learning Roto-translation Invariant Representation for LiDAR based Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.11029v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.11029v1)
- **Published**: 2022-10-20 05:35:30+00:00
- **Updated**: 2022-10-20 05:35:30+00:00
- **Authors**: Sha Lu, Xuecheng Xu, Li Tang, Rong Xiong, Yue Wang
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: LiDAR based place recognition is popular for loop closure detection and re-localization. In recent years, deep learning brings improvements to place recognition by learnable feature extraction. However, these methods degenerate when the robot re-visits previous places with large perspective difference. To address the challenge, we propose DeepRING to learn the roto-translation invariant representation from LiDAR scan, so that robot visits the same place with different perspective can have similar representations. There are two keys in DeepRING: the feature is extracted from sinogram, and the feature is aggregated by magnitude spectrum. The two steps keeps the final representation with both discrimination and roto-translation invariance. Moreover, we state the place recognition as a one-shot learning problem with each place being a class, leveraging relation learning to build representation similarity. Substantial experiments are carried out on public datasets, validating the effectiveness of each proposed component, and showing that DeepRING outperforms the comparative methods, especially in dataset level generalization.



### PointTAD: Multi-Label Temporal Action Detection with Learnable Query Points
- **Arxiv ID**: http://arxiv.org/abs/2210.11035v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11035v3)
- **Published**: 2022-10-20 06:08:03+00:00
- **Updated**: 2023-03-21 16:03:50+00:00
- **Authors**: Jing Tan, Xiaotong Zhao, Xintian Shi, Bin Kang, Limin Wang
- **Comment**: NeurIPS 2022 camera ready version
- **Journal**: None
- **Summary**: Traditional temporal action detection (TAD) usually handles untrimmed videos with small number of action instances from a single label (e.g., ActivityNet, THUMOS). However, this setting might be unrealistic as different classes of actions often co-occur in practice. In this paper, we focus on the task of multi-label temporal action detection that aims to localize all action instances from a multi-label untrimmed video. Multi-label TAD is more challenging as it requires for fine-grained class discrimination within a single video and precise localization of the co-occurring instances. To mitigate this issue, we extend the sparse query-based detection paradigm from the traditional TAD and propose the multi-label TAD framework of PointTAD. Specifically, our PointTAD introduces a small set of learnable query points to represent the important frames of each action instance. This point-based representation provides a flexible mechanism to localize the discriminative frames at boundaries and as well the important frames inside the action. Moreover, we perform the action decoding process with the Multi-level Interactive Module to capture both point-level and instance-level action semantics. Finally, our PointTAD employs an end-to-end trainable framework simply based on RGB input for easy deployment. We evaluate our proposed method on two popular benchmarks and introduce the new metric of detection-mAP for multi-label TAD. Our model outperforms all previous methods by a large margin under the detection-mAP metric, and also achieves promising results under the segmentation-mAP metric. Code is available at https://github.com/MCG-NJU/PointTAD.



### Robust Image Registration with Absent Correspondences in Pre-operative and Follow-up Brain MRI Scans of Diffuse Glioma Patients
- **Arxiv ID**: http://arxiv.org/abs/2210.11045v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11045v2)
- **Published**: 2022-10-20 06:37:40+00:00
- **Updated**: 2022-11-12 18:50:08+00:00
- **Authors**: Tony C. W. Mok, Albert C. S. Chung
- **Comment**: BraTS-Reg workshop
- **Journal**: None
- **Summary**: Registration of pre-operative and follow-up brain MRI scans is challenging due to the large variation of tissue appearance and missing correspondences in tumour recurrence regions caused by tumour mass effect. Although recent deep learning-based deformable registration methods have achieved remarkable success in various medical applications, most of them are not capable of registering images with pathologies. In this paper, we propose a 3-step registration pipeline for pre-operative and follow-up brain MRI scans that consists of 1) a multi-level affine registration, 2) a conditional deep Laplacian pyramid image registration network (cLapIRN) with forward-backward consistency constraint, and 3) a non-linear instance optimization method. We apply the method to the Brain Tumor Sequence Registration (BraTS-Reg) Challenge. Our method achieves accurate and robust registration of brain MRI scans with pathologies, which achieves a median absolute error of 1.64 mm and 88\% of successful registration rate in the validation set of BraTS-Reg challenge. Our method ranks 1st place in the 2022 MICCAI BraTS-Reg challenge.



### JRDB-Pose: A Large-scale Dataset for Multi-Person Pose Estimation and Tracking
- **Arxiv ID**: http://arxiv.org/abs/2210.11940v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.11940v2)
- **Published**: 2022-10-20 07:14:37+00:00
- **Updated**: 2023-03-12 00:07:12+00:00
- **Authors**: Edward Vendrow, Duy Tho Le, Jianfei Cai, Hamid Rezatofighi
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: Autonomous robotic systems operating in human environments must understand their surroundings to make accurate and safe decisions. In crowded human scenes with close-up human-robot interaction and robot navigation, a deep understanding requires reasoning about human motion and body dynamics over time with human body pose estimation and tracking. However, existing datasets either do not provide pose annotations or include scene types unrelated to robotic applications. Many datasets also lack the diversity of poses and occlusions found in crowded human scenes. To address this limitation we introduce JRDB-Pose, a large-scale dataset and benchmark for multi-person pose estimation and tracking using videos captured from a social navigation robot. The dataset contains challenge scenes with crowded indoor and outdoor locations and a diverse range of scales and occlusion types. JRDB-Pose provides human pose annotations with per-keypoint occlusion labels and track IDs consistent across the scene. A public evaluation server is made available for fair evaluation on a held-out test set. JRDB-Pose is available at https://jrdb.erc.monash.edu/ .



### Representation Learning with Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2210.11058v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11058v1)
- **Published**: 2022-10-20 07:26:47+00:00
- **Updated**: 2022-10-20 07:26:47+00:00
- **Authors**: Jeremias Traub
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion models (DMs) have achieved state-of-the-art results for image synthesis tasks as well as density estimation. Applied in the latent space of a powerful pretrained autoencoder (LDM), their immense computational requirements can be significantly reduced without sacrificing sampling quality. However, DMs and LDMs lack a semantically meaningful representation space as the diffusion process gradually destroys information in the latent variables. We introduce a framework for learning such representations with diffusion models (LRDM). To that end, a LDM is conditioned on the representation extracted from the clean image by a separate encoder. In particular, the DM and the representation encoder are trained jointly in order to learn rich representations specific to the generative denoising process. By introducing a tractable representation prior, we can efficiently sample from the representation distribution for unconditional image synthesis without training of any additional model. We demonstrate that i) competitive image generation results can be achieved with image-parameterized LDMs, ii) LRDMs are capable of learning semantically meaningful representations, allowing for faithful image reconstructions and semantic interpolations. Our implementation is available at https://github.com/jeremiastraub/diffusion.



### End-to-End Context-Aided Unicity Matching for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2210.12008v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12008v2)
- **Published**: 2022-10-20 07:33:57+00:00
- **Updated**: 2022-12-06 02:22:11+00:00
- **Authors**: Min Cao, Cong Ding, Chen Chen, Junchi Yan, Qi Tian
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: Most existing person re-identification methods compute the matching relations between person images across camera views based on the ranking of the pairwise similarities. This matching strategy with the lack of the global viewpoint and the context's consideration inevitably leads to ambiguous matching results and sub-optimal performance. Based on a natural assumption that images belonging to the same person identity should not match with images belonging to multiple different person identities across views, called the unicity of person matching on the identity level, we propose an end-to-end person unicity matching architecture for learning and refining the person matching relations. First, we adopt the image samples' contextual information in feature space to generate the initial soft matching results by using graph neural networks. Secondly, we utilize the samples' global context relationship to refine the soft matching results and reach the matching unicity through bipartite graph matching. Given full consideration to real-world person re-identification applications, we achieve the unicity matching in both one-shot and multi-shot settings of person re-identification and further develop a fast version of the unicity matching without losing the performance. The proposed method is evaluated on five public benchmarks, including four multi-shot datasets MSMT17, DukeMTMC, Market1501, CUHK03, and a one-shot dataset VIPeR. Experimental results show the superiority of the proposed method on performance and efficiency.



### MovieCLIP: Visual Scene Recognition in Movies
- **Arxiv ID**: http://arxiv.org/abs/2210.11065v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.11065v2)
- **Published**: 2022-10-20 07:38:56+00:00
- **Updated**: 2022-10-23 01:25:13+00:00
- **Authors**: Digbalay Bose, Rajat Hebbar, Krishna Somandepalli, Haoyang Zhang, Yin Cui, Kree Cole-McLaughlin, Huisheng Wang, Shrikanth Narayanan
- **Comment**: Accepted to 2023 IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV 2023). Project website with supplemental material:
  https://sail.usc.edu/~mica/MovieCLIP/. Revised version with updated author
  affiliations
- **Journal**: None
- **Summary**: Longform media such as movies have complex narrative structures, with events spanning a rich variety of ambient visual scenes. Domain specific challenges associated with visual scenes in movies include transitions, person coverage, and a wide array of real-life and fictional scenarios. Existing visual scene datasets in movies have limited taxonomies and don't consider the visual scene transition within movie clips. In this work, we address the problem of visual scene recognition in movies by first automatically curating a new and extensive movie-centric taxonomy of 179 scene labels derived from movie scripts and auxiliary web-based video datasets. Instead of manual annotations which can be expensive, we use CLIP to weakly label 1.12 million shots from 32K movie clips based on our proposed taxonomy. We provide baseline visual models trained on the weakly labeled dataset called MovieCLIP and evaluate them on an independent dataset verified by human raters. We show that leveraging features from models pretrained on MovieCLIP benefits downstream tasks such as multi-label scene and genre classification of web videos and movie trailers.



### Frequency of Interest-based Noise Attenuation Method to Improve Anomaly Detection Performance
- **Arxiv ID**: http://arxiv.org/abs/2210.11068v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2210.11068v3)
- **Published**: 2022-10-20 07:42:33+00:00
- **Updated**: 2022-12-02 07:43:48+00:00
- **Authors**: YeongHyeon Park, Myung Jin Kim, Won Seok Park
- **Comment**: 5 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: Accurately extracting driving events is the way to maximize computational efficiency and anomaly detection performance in the tire frictional nose-based anomaly detection task. This study proposes a concise and highly useful method for improving the precision of the event extraction that is hindered by extra noise such as wind noise, which is difficult to characterize clearly due to its randomness. The core of the proposed method is based on the identification of the road friction sound corresponding to the frequency of interest and removing the opposite characteristics with several frequency filters. Our method enables precision maximization of driving event extraction while improving anomaly detection performance by an average of 8.506%. Therefore, we conclude our method is a practical solution suitable for road surface anomaly detection purposes in outdoor edge computing environments.



### Large-batch Optimization for Dense Visual Predictions
- **Arxiv ID**: http://arxiv.org/abs/2210.11078v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11078v1)
- **Published**: 2022-10-20 08:11:52+00:00
- **Updated**: 2022-10-20 08:11:52+00:00
- **Authors**: Zeyue Xue, Jianming Liang, Guanglu Song, Zhuofan Zong, Liang Chen, Yu Liu, Ping Luo
- **Comment**: 23 pages, 6 figures
- **Journal**: NeurIPS 2022
- **Summary**: Training a large-scale deep neural network in a large-scale dataset is challenging and time-consuming. The recent breakthrough of large-batch optimization is a promising way to tackle this challenge. However, although the current advanced algorithms such as LARS and LAMB succeed in classification models, the complicated pipelines of dense visual predictions such as object detection and segmentation still suffer from the heavy performance drop in the large-batch training regime. To address this challenge, we propose a simple yet effective algorithm, named Adaptive Gradient Variance Modulator (AGVM), which can train dense visual predictors with very large batch size, enabling several benefits more appealing than prior arts. Firstly, AGVM can align the gradient variances between different modules in the dense visual predictors, such as backbone, feature pyramid network (FPN), detection, and segmentation heads. We show that training with a large batch size can fail with the gradient variances misaligned among them, which is a phenomenon primarily overlooked in previous work. Secondly, AGVM is a plug-and-play module that generalizes well to many different architectures (e.g., CNNs and Transformers) and different tasks (e.g., object detection, instance segmentation, semantic segmentation, and panoptic segmentation). It is also compatible with different optimizers (e.g., SGD and AdamW). Thirdly, a theoretical analysis of AGVM is provided. Extensive experiments on the COCO and ADE20K datasets demonstrate the superiority of AGVM. For example, it can train Faster R-CNN+ResNet50 in 4 minutes without losing performance. AGVM enables training an object detector with one billion parameters in just 3.5 hours, reducing the training time by 20.9x, whilst achieving 62.2 mAP on COCO. The deliverables are released at https://github.com/Sense-X/AGVM.



### Standardized Medical Image Classification across Medical Disciplines
- **Arxiv ID**: http://arxiv.org/abs/2210.11091v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11091v1)
- **Published**: 2022-10-20 08:38:31+00:00
- **Updated**: 2022-10-20 08:38:31+00:00
- **Authors**: Simone Mayer, Dominik Müller, Frank Kramer
- **Comment**: https://frankkramer-lab.github.io/aucmedi/
- **Journal**: None
- **Summary**: AUCMEDI is a Python-based framework for medical image classification. In this paper, we evaluate the capabilities of AUCMEDI, by applying it to multiple datasets. Datasets were specifically chosen to cover a variety of medical disciplines and imaging modalities. We designed a simple pipeline using Jupyter notebooks and applied it to all datasets. Results show that AUCMEDI was able to train a model with accurate classification capabilities for each dataset: Averaged AUC per dataset range between 0.82 and 1.0, averaged F1 scores range between 0.61 and 1.0. With its high adaptability and strong performance, AUCMEDI proves to be a powerful instrument to build widely applicable neural networks. The notebooks serve as application examples for AUCMEDI.



### Robustcaps: a transformation-robust capsule network for image classification
- **Arxiv ID**: http://arxiv.org/abs/2210.11092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11092v1)
- **Published**: 2022-10-20 08:42:33+00:00
- **Updated**: 2022-10-20 08:42:33+00:00
- **Authors**: Sai Raam Venkataraman, S. Balasubramanian, R. Raghunatha Sarma
- **Comment**: None
- **Journal**: None
- **Summary**: Geometric transformations of the training data as well as the test data present challenges to the use of deep neural networks to vision-based learning tasks. In order to address this issue, we present a deep neural network model that exhibits the desirable property of transformation-robustness. Our model, termed RobustCaps, uses group-equivariant convolutions in an improved capsule network model. RobustCaps uses a global context-normalised procedure in its routing algorithm to learn transformation-invariant part-whole relationships within image data. This learning of such relationships allows our model to outperform both capsule and convolutional neural network baselines on transformation-robust classification tasks. Specifically, RobustCaps achieves state-of-the-art accuracies on CIFAR-10, FashionMNIST, and CIFAR-100 when the images in these datasets are subjected to train and test-time rotations and translations.



### Iterative collaborative routing among equivariant capsules for transformation-robust capsule networks
- **Arxiv ID**: http://arxiv.org/abs/2210.11095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11095v1)
- **Published**: 2022-10-20 08:47:18+00:00
- **Updated**: 2022-10-20 08:47:18+00:00
- **Authors**: Sai Raam Venkataraman, S. Balasubramanian, R. Raghunatha Sarma
- **Comment**: None
- **Journal**: None
- **Summary**: Transformation-robustness is an important feature for machine learning models that perform image classification. Many methods aim to bestow this property to models by the use of data augmentation strategies, while more formal guarantees are obtained via the use of equivariant models. We recognise that compositional, or part-whole structure is also an important aspect of images that has to be considered for building transformation-robust models. Thus, we propose a capsule network model that is, at once, equivariant and compositionality-aware. Equivariance of our capsule network model comes from the use of equivariant convolutions in a carefully-chosen novel architecture. The awareness of compositionality comes from the use of our proposed novel, iterative, graph-based routing algorithm, termed Iterative collaborative routing (ICR). ICR, the core of our contribution, weights the predictions made for capsules based on an iteratively averaged score of the degree-centralities of its nearest neighbours. Experiments on transformed image classification on FashionMNIST, CIFAR-10, and CIFAR-100 show that our model that uses ICR outperforms convolutional and capsule baselines to achieve state-of-the-art performance.



### Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation
- **Arxiv ID**: http://arxiv.org/abs/2210.11109v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2210.11109v2)
- **Published**: 2022-10-20 09:10:17+00:00
- **Updated**: 2022-10-26 11:29:01+00:00
- **Authors**: Yu Zhao, Jianguo Wei, Zhichao Lin, Yueheng Sun, Meishan Zhang, Min Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Image-to-text tasks, such as open-ended image captioning and controllable image description, have received extensive attention for decades. Here, we further advance this line of work by presenting Visual Spatial Description (VSD), a new perspective for image-to-text toward spatial semantics. Given an image and two objects inside it, VSD aims to produce one description focusing on the spatial perspective between the two objects. Accordingly, we manually annotate a dataset to facilitate the investigation of the newly-introduced task and build several benchmark encoder-decoder models by using VL-BART and VL-T5 as backbones. In addition, we investigate pipeline and joint end-to-end architectures for incorporating visual spatial relationship classification (VSRC) information into our model. Finally, we conduct experiments on our benchmark dataset to evaluate all our models. Results show that our models are impressive, providing accurate and human-like spatial-oriented text descriptions. Meanwhile, VSRC has great potential for VSD, and the joint end-to-end architecture is the better choice for their integration. We make the dataset and codes public for research purposes.



### Pruning by Active Attention Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2210.11114v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2210.11114v1)
- **Published**: 2022-10-20 09:17:02+00:00
- **Updated**: 2022-10-20 09:17:02+00:00
- **Authors**: Zahra Babaiee, Lucas Liebenwein, Ramin Hasani, Daniela Rus, Radu Grosu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2204.07412
- **Journal**: None
- **Summary**: Filter pruning of a CNN is typically achieved by applying discrete masks on the CNN's filter weights or activation maps, post-training. Here, we present a new filter-importance-scoring concept named pruning by active attention manipulation (PAAM), that sparsifies the CNN's set of filters through a particular attention mechanism, during-training. PAAM learns analog filter scores from the filter weights by optimizing a cost function regularized by an additive term in the scores. As the filters are not independent, we use attention to dynamically learn their correlations. Moreover, by training the pruning scores of all layers simultaneously, PAAM can account for layer inter-dependencies, which is essential to finding a performant sparse sub-network. PAAM can also train and generate a pruned network from scratch in a straightforward, one-stage training process without requiring a pre-trained network. Finally, PAAM does not need layer-specific hyperparameters and pre-defined layer budgets, since it can implicitly determine the appropriate number of filters in each layer. Our experimental results on different network architectures suggest that PAAM outperforms state-of-the-art structured-pruning methods (SOTA). On CIFAR-10 dataset, without requiring a pre-trained baseline network, we obtain 1.02% and 1.19% accuracy gain and 52.3% and 54% parameters reduction, on ResNet56 and ResNet110, respectively. Similarly, on the ImageNet dataset, PAAM achieves 1.06% accuracy gain while pruning 51.1% of the parameters on ResNet50. For Cifar-10, this is better than the SOTA with a margin of 9.5% and 6.6%, respectively, and on ImageNet with a margin of 11%.



### Super-Resolution and Image Re-projection for Iris Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.11129v1
- **DOI**: 10.1109/ISBA.2019.8778581
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11129v1)
- **Published**: 2022-10-20 09:46:23+00:00
- **Updated**: 2022-10-20 09:46:23+00:00
- **Authors**: Eduardo Ribeiro, Andreas Uhl, Fernando Alonso-Fernandez
- **Comment**: Published at IEEE International Conference on Identity, Security and
  Behavior Analysis, ISBA 2019
- **Journal**: None
- **Summary**: Several recent works have addressed the ability of deep learning to disclose rich, hierarchical and discriminative models for the most diverse purposes. Specifically in the super-resolution field, Convolutional Neural Networks (CNNs) using different deep learning approaches attempt to recover realistic texture and fine grained details from low resolution images. In this work we explore the viability of these approaches for iris Super-Resolution (SR) in an iris recognition environment. For this, we test different architectures with and without a so called image re-projection to reduce artifacts applying it to different iris databases to verify the viability of the different CNNs for iris super-resolution. Results show that CNNs and image re-projection can improve the results specially for the accuracy of recognition systems using a complete different training database performing the transfer learning successfully.



### On-line signature verification using Tablet PC
- **Arxiv ID**: http://arxiv.org/abs/2210.11135v1
- **DOI**: 10.1109/ISPA.2005.195417
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11135v1)
- **Published**: 2022-10-20 09:59:28+00:00
- **Updated**: 2022-10-20 09:59:28+00:00
- **Authors**: Fernando Alonso-Fernandez, Julian Fierrez-Aguilar, Francisco del-Valle, Javier Ortega-Garcia
- **Comment**: Published at 4th International Symposium on Image and Signal
  Processing and Analysis, ISPA 2005
- **Journal**: None
- **Summary**: On-line signature verification for Tablet PC devices is studied. The on-line signature verification algorithm presented by the authors at the First International Signature Verification Competition (SVC 2004) is adapted to work in Tablet PC environments. An example prototype of securing access and securing document application using this Tablet PC system is also reported. Two different commercial Tablet PCs are evaluated, including information of interest for signature verification systems such as sampling and pressure statistics. Authentication performance experiments are reported considering both random and skilled forgeries by using a new database with over 3000 signatures.



### Sensor interoperability and fusion in signature verification: A case study using tablet PC
- **Arxiv ID**: http://arxiv.org/abs/2210.11139v1
- **DOI**: 10.1007/11569947_23
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11139v1)
- **Published**: 2022-10-20 10:06:36+00:00
- **Updated**: 2022-10-20 10:06:36+00:00
- **Authors**: Fernando Alonso-Fernandez, Julian Fierrez-Aguilar, Javier Ortega-Garcia
- **Comment**: Published at Intl. Workshop on Biometric Recognition Systems, IWBRS
  2005
- **Journal**: None
- **Summary**: Several works related to information fusion for signature verification have been presented. However, few works have focused on sensor fusion and sensor interoperability. In this paper, these two topics are evaluated for signature verification using two different commercial Tablet PCs. An enrolment strategy using signatures from the two Tablet PCs is also proposed. Authentication performance experiments are reported by using a database with over 3000 signatures.



### General Image Descriptors for Open World Image Retrieval using ViT CLIP
- **Arxiv ID**: http://arxiv.org/abs/2210.11141v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.11141v1)
- **Published**: 2022-10-20 10:10:15+00:00
- **Updated**: 2022-10-20 10:10:15+00:00
- **Authors**: Marcos V. Conde, Ivan Aerlic, Simon Jégou
- **Comment**: ECCV 2022 Instance-Level Recognition Workshop
- **Journal**: None
- **Summary**: The Google Universal Image Embedding (GUIE) Challenge is one of the first competitions in multi-domain image representations in the wild, covering a wide distribution of objects: landmarks, artwork, food, etc. This is a fundamental computer vision problem with notable applications in image retrieval, search engines and e-commerce. In this work, we explain our 4th place solution to the GUIE Challenge, and our "bag of tricks" to fine-tune zero-shot Vision Transformers (ViT) pre-trained using CLIP.



### Reproducibility of the Methods in Medical Imaging with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.11146v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11146v1)
- **Published**: 2022-10-20 10:23:33+00:00
- **Updated**: 2022-10-20 10:23:33+00:00
- **Authors**: Attila Simko, Anders Garpebring, Joakim Jonsson, Tufve Nyholm, Tommy Löfstedt
- **Comment**: None
- **Journal**: None
- **Summary**: Concerns about the reproducibility of deep learning research are more prominent than ever, with no clear solution in sight. The relevance of machine learning research can only be improved if we also employ empirical rigor that incorporates reproducibility guidelines, especially so in the medical imaging field. The Medical Imaging with Deep Learning (MIDL) conference has made advancements in this direction by advocating open access, and recently also recommending authors to make their code public - both aspects being adopted by the majority of the conference submissions. This helps the reproducibility of the methods, however, there is currently little or no support for further evaluation of these supplementary material, making them vulnerable to poor quality, which affects the impact of the entire submission. We have evaluated all accepted full paper submissions to MIDL between 2018 and 2022 using established, but slightly adjusted guidelines on reproducibility and the quality of the public repositories. The evaluations show that publishing repositories and using public datasets are becoming more popular, which helps traceability, but the quality of the repositories has not improved over the years, leaving room for improvement in every aspect of designing repositories. Merely 22% of all submissions contain a repository that were deemed repeatable using our evaluations. From the commonly encountered issues during the evaluations, we propose a set of guidelines for machine learning-related research for medical imaging applications, adjusted specifically for future submissions to MIDL.



### Reversed Image Signal Processing and RAW Reconstruction. AIM 2022 Challenge Report
- **Arxiv ID**: http://arxiv.org/abs/2210.11153v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11153v1)
- **Published**: 2022-10-20 10:43:53+00:00
- **Updated**: 2022-10-20 10:43:53+00:00
- **Authors**: Marcos V. Conde, Radu Timofte, Yibin Huang, Jingyang Peng, Chang Chen, Cheng Li, Eduardo Pérez-Pellitero, Fenglong Song, Furui Bai, Shuai Liu, Chaoyu Feng, Xiaotao Wang, Lei Lei, Yu Zhu, Chenghua Li, Yingying Jiang, Yong A, Peisong Wang, Cong Leng, Jian Cheng, Xiaoyu Liu, Zhicun Yin, Zhilu Zhang, Junyi Li, Ming Liu, Wangmeng Zuo, Jun Jiang, Jinha Kim, Yue Zhang, Beiji Zou, Zhikai Zong, Xiaoxiao Liu, Juan Marín Vega, Michael Sloth, Peter Schneider-Kamp, Richard Röttger, Furkan Kınlı, Barış Özcan, Furkan Kıraç, Li Leyi, SM Nadim Uddin, Dipon Kumar Ghosh, Yong Ju Jung
- **Comment**: ECCV 2022 Advances in Image Manipulation (AIM) workshop
- **Journal**: None
- **Summary**: Cameras capture sensor RAW images and transform them into pleasant RGB images, suitable for the human eyes, using their integrated Image Signal Processor (ISP). Numerous low-level vision tasks operate in the RAW domain (e.g. image denoising, white balance) due to its linear relationship with the scene irradiance, wide-range of information at 12bits, and sensor designs. Despite this, RAW image datasets are scarce and more expensive to collect than the already large and public RGB datasets.   This paper introduces the AIM 2022 Challenge on Reversed Image Signal Processing and RAW Reconstruction. We aim to recover raw sensor images from the corresponding RGBs without metadata and, by doing this, "reverse" the ISP transformation. The proposed methods and benchmark establish the state-of-the-art for this low-level vision inverse problem, and generating realistic raw sensor readings can potentially benefit other tasks such as denoising and super-resolution.



### VideoPipe 2022 Challenge: Real-World Video Understanding for Urban Pipe Inspection
- **Arxiv ID**: http://arxiv.org/abs/2210.11158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11158v1)
- **Published**: 2022-10-20 10:52:49+00:00
- **Updated**: 2022-10-20 10:52:49+00:00
- **Authors**: Yi Liu, Xuan Zhang, Ying Li, Guixin Liang, Yabing Jiang, Lixia Qiu, Haiping Tang, Fei Xie, Wei Yao, Yi Dai, Yu Qiao, Yali Wang
- **Comment**: VideoPipe Challenge @ ICPR2022. Homepage:
  https://videopipe.github.io/
- **Journal**: None
- **Summary**: Video understanding is an important problem in computer vision. Currently, the well-studied task in this research is human action recognition, where the clips are manually trimmed from the long videos, and a single class of human action is assumed for each clip. However, we may face more complicated scenarios in the industrial applications. For example, in the real-world urban pipe system, anomaly defects are fine-grained, multi-labeled, domain-relevant. To recognize them correctly, we need to understand the detailed video content. For this reason, we propose to advance research areas of video understanding, with a shift from traditional action recognition to industrial anomaly analysis. In particular, we introduce two high-quality video benchmarks, namely QV-Pipe and CCTV-Pipe, for anomaly inspection in the real-world urban pipe systems. Based on these new datasets, we will host two competitions including (1) Video Defect Classification on QV-Pipe and (2) Temporal Defect Localization on CCTV-Pipe. In this report, we describe the details of these benchmarks, the problem definitions of competition tracks, the evaluation metric, and the result summary. We expect that, this competition would bring new opportunities and challenges for video understanding in smart city and beyond. The details of our VideoPipe challenge can be found in https://videopipe.github.io.



### Content-based Graph Privacy Advisor
- **Arxiv ID**: http://arxiv.org/abs/2210.11169v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11169v2)
- **Published**: 2022-10-20 11:12:42+00:00
- **Updated**: 2022-11-13 11:38:22+00:00
- **Authors**: Dimitrios Stoidis, Andrea Cavallaro
- **Comment**: 8 pages, 3 figures, in Proceedings of IEEE BigMM 2022
- **Journal**: None
- **Summary**: People may be unaware of the privacy risks of uploading an image online. In this paper, we present Graph Privacy Advisor, an image privacy classifier that uses scene information and object cardinality as cues to predict whether an image is private. Graph Privacy Advisor simplifies a state-of-the-art graph model and improves its performance by refining the relevance of the information extracted from the image. We determine the most informative visual features to be used for the privacy classification task and reduce the complexity of the model by replacing high-dimensional image feature vectors with lower-dimensional, more effective features. We also address the problem of biased prior information by modelling object co-occurrences instead of the frequency of object occurrences in each class.



### Coordinates Are NOT Lonely -- Codebook Prior Helps Implicit Neural 3D Representations
- **Arxiv ID**: http://arxiv.org/abs/2210.11170v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11170v2)
- **Published**: 2022-10-20 11:13:50+00:00
- **Updated**: 2022-10-21 04:51:01+00:00
- **Authors**: Fukun Yin, Wen Liu, Zilong Huang, Pei Cheng, Tao Chen, Gang YU
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: Implicit neural 3D representation has achieved impressive results in surface or scene reconstruction and novel view synthesis, which typically uses the coordinate-based multi-layer perceptrons (MLPs) to learn a continuous scene representation. However, existing approaches, such as Neural Radiance Field (NeRF) and its variants, usually require dense input views (i.e. 50-150) to obtain decent results. To relive the over-dependence on massive calibrated images and enrich the coordinate-based feature representation, we explore injecting the prior information into the coordinate-based network and introduce a novel coordinate-based model, CoCo-INR, for implicit neural 3D representation. The cores of our method are two attention modules: codebook attention and coordinate attention. The former extracts the useful prototypes containing rich geometry and appearance information from the prior codebook, and the latter propagates such prior information into each coordinate and enriches its feature representation for a scene or object surface. With the help of the prior information, our method can render 3D views with more photo-realistic appearance and geometries than the current methods using fewer calibrated images available. Experiments on various scene reconstruction datasets, including DTU and BlendedMVS, and the full 3D head reconstruction dataset, H3DS, demonstrate the robustness under fewer input views and fine detail-preserving capability of our proposed method.



### Towards Better Guided Attention and Human Knowledge Insertion in Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.11177v1
- **DOI**: 10.1007/978-3-031-25069-9_29
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11177v1)
- **Published**: 2022-10-20 11:32:38+00:00
- **Updated**: 2022-10-20 11:32:38+00:00
- **Authors**: Ankit Gupta, Ida-Maria Sintorn
- **Comment**: None
- **Journal**: None
- **Summary**: Attention Branch Networks (ABNs) have been shown to simultaneously provide visual explanation and improve the performance of deep convolutional neural networks (CNNs). In this work, we introduce Multi-Scale Attention Branch Networks (MSABN), which enhance the resolution of the generated attention maps, and improve the performance. We evaluate MSABN on benchmark image recognition and fine-grained recognition datasets where we observe MSABN outperforms ABN and baseline models. We also introduce a new data augmentation strategy utilizing the attention maps to incorporate human knowledge in the form of bounding box annotations of the objects of interest. We show that even with a limited number of edited samples, a significant performance gain can be achieved with this strategy.



### Multi-hypothesis 3D human pose estimation metrics favor miscalibrated distributions
- **Arxiv ID**: http://arxiv.org/abs/2210.11179v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11179v1)
- **Published**: 2022-10-20 11:47:07+00:00
- **Updated**: 2022-10-20 11:47:07+00:00
- **Authors**: Paweł A. Pierzchlewicz, R. James Cotton, Mohammad Bashiri, Fabian H. Sinz
- **Comment**: Under review
- **Journal**: None
- **Summary**: Due to depth ambiguities and occlusions, lifting 2D poses to 3D is a highly ill-posed problem. Well-calibrated distributions of possible poses can make these ambiguities explicit and preserve the resulting uncertainty for downstream tasks. This study shows that previous attempts, which account for these ambiguities via multiple hypotheses generation, produce miscalibrated distributions. We identify that miscalibration can be attributed to the use of sample-based metrics such as minMPJPE. In a series of simulations, we show that minimizing minMPJPE, as commonly done, should converge to the correct mean prediction. However, it fails to correctly capture the uncertainty, thus resulting in a miscalibrated distribution. To mitigate this problem, we propose an accurate and well-calibrated model called Conditional Graph Normalizing Flow (cGNFs). Our model is structured such that a single cGNF can estimate both conditional and marginal densities within the same model - effectively solving a zero-shot density estimation problem. We evaluate cGNF on the Human~3.6M dataset and show that cGNF provides a well-calibrated distribution estimate while being close to state-of-the-art in terms of overall minMPJPE. Furthermore, cGNF outperforms previous methods on occluded joints while it remains well-calibrated.



### Facial Expression Video Generation Based-On Spatio-temporal Convolutional GAN: FEV-GAN
- **Arxiv ID**: http://arxiv.org/abs/2210.11182v1
- **DOI**: 10.1016/j.iswa.2022.200139
- **Categories**: **cs.CV**, 41A05, 41A10, 65D05, 65D17
- **Links**: [PDF](http://arxiv.org/pdf/2210.11182v1)
- **Published**: 2022-10-20 11:54:32+00:00
- **Updated**: 2022-10-20 11:54:32+00:00
- **Authors**: Hamza Bouzid, Lahoucine Ballihi
- **Comment**: 13 pages, 8 figures, accepted in ISWA journal
- **Journal**: Intelligent Systems with Applications. 19 (2022)
- **Summary**: Facial expression generation has always been an intriguing task for scientists and researchers all over the globe. In this context, we present our novel approach for generating videos of the six basic facial expressions. Starting from a single neutral facial image and a label indicating the desired facial expression, we aim to synthesize a video of the given identity performing the specified facial expression. Our approach, referred to as FEV-GAN (Facial Expression Video GAN), is based on Spatio-temporal Convolutional GANs, that are known to model both content and motion in the same network. Previous methods based on such a network have shown a good ability to generate coherent videos with smooth temporal evolution. However, they still suffer from low image quality and low identity preservation capability. In this work, we address this problem by using a generator composed of two image encoders. The first one is pre-trained for facial identity feature extraction and the second for spatial feature extraction. We have qualitatively and quantitatively evaluated our model on two international facial expression benchmark databases: MUG and Oulu-CASIA NIR&VIS. The experimental results analysis demonstrates the effectiveness of our approach in generating videos of the six basic facial expressions while preserving the input identity. The analysis also proves that the use of both identity and spatial features enhances the decoder ability to better preserve the identity and generate high-quality videos. The code and the pre-trained model will soon be made publicly available.



### PalGAN: Image Colorization with Palette Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.11204v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11204v1)
- **Published**: 2022-10-20 12:28:31+00:00
- **Updated**: 2022-10-20 12:28:31+00:00
- **Authors**: Yi Wang, Menghan Xia, Lu Qi, Jing Shao, Yu Qiao
- **Comment**: Accepted at ECCV 2022
- **Journal**: None
- **Summary**: Multimodal ambiguity and color bleeding remain challenging in colorization. To tackle these problems, we propose a new GAN-based colorization approach PalGAN, integrated with palette estimation and chromatic attention. To circumvent the multimodality issue, we present a new colorization formulation that estimates a probabilistic palette from the input gray image first, then conducts color assignment conditioned on the palette through a generative model. Further, we handle color bleeding with chromatic attention. It studies color affinities by considering both semantic and intensity correlation. In extensive experiments, PalGAN outperforms state-of-the-arts in quantitative evaluation and visual comparison, delivering notable diverse, contrastive, and edge-preserving appearances. With the palette design, our method enables color transfer between images even with irrelevant contexts.



### YOWO-Plus: An Incremental Improvement
- **Arxiv ID**: http://arxiv.org/abs/2210.11219v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11219v1)
- **Published**: 2022-10-20 12:51:39+00:00
- **Updated**: 2022-10-20 12:51:39+00:00
- **Authors**: Jianhua Yang
- **Comment**: 4 pages, 1 figure
- **Journal**: None
- **Summary**: In this technical report, we would like to introduce our updates to YOWO, a real-time method for spatio-temporal action detection. We make a bunch of little design changes to make it better. For network structure, we use the same ones of official implemented YOWO, including 3D-ResNext-101 and YOLOv2, but we use a better pretrained weight of our reimplemented YOLOv2, which is better than the official YOLOv2. We also optimize the label assignment used in YOWO. To accurately detection action instances, we deploy GIoU loss for box regression. After our incremental improvement, YOWO achieves 84.9\% frame mAP and 50.5\% video mAP on the UCF101-24, significantly higher than the official YOWO. On the AVA, our optimized YOWO achieves 20.6\% frame mAP with 16 frames, also exceeding the official YOWO. With 32 frames, our YOWO achieves 21.6 frame mAP with 25 FPS on an RTX 3090 GPU. We name the optimized YOWO as YOWO-Plus. Moreover, we replace the 3D-ResNext-101 with the efficient 3D-ShuffleNet-v2 to design a lightweight action detector, YOWO-Nano. YOWO-Nano achieves 81.0 \% frame mAP and 49.7\% video frame mAP with over 90 FPS on the UCF101-24. It also achieves 18.4 \% frame mAP with about 90 FPS on the AVA. As far as we know, YOWO-Nano is the fastest state-of-the-art action detector. Our code is available on https://github.com/yjh0410/PyTorch_YOWO.



### Comparing Machine Learning Techniques for Alfalfa Biomass Yield Prediction
- **Arxiv ID**: http://arxiv.org/abs/2210.11226v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11226v1)
- **Published**: 2022-10-20 13:00:33+00:00
- **Updated**: 2022-10-20 13:00:33+00:00
- **Authors**: Jonathan Vance, Khaled Rasheed, Ali Missaoui, Frederick Maier, Christian Adkins, Chris Whitmire
- **Comment**: None
- **Journal**: None
- **Summary**: The alfalfa crop is globally important as livestock feed, so highly efficient planting and harvesting could benefit many industries, especially as the global climate changes and traditional methods become less accurate. Recent work using machine learning (ML) to predict yields for alfalfa and other crops has shown promise. Previous efforts used remote sensing, weather, planting, and soil data to train machine learning models for yield prediction. However, while remote sensing works well, the models require large amounts of data and cannot make predictions until the harvesting season begins. Using weather and planting data from alfalfa variety trials in Kentucky and Georgia, our previous work compared feature selection techniques to find the best technique and best feature set. In this work, we trained a variety of machine learning models, using cross validation for hyperparameter optimization, to predict biomass yields, and we showed better accuracy than similar work that employed more complex techniques. Our best individual model was a random forest with a mean absolute error of 0.081 tons/acre and R{$^2$} of 0.941. Next, we expanded this dataset to include Wisconsin and Mississippi, and we repeated our experiments, obtaining a higher best R{$^2$} of 0.982 with a regression tree. We then isolated our testing datasets by state to explore this problem's eligibility for domain adaptation (DA), as we trained on multiple source states and tested on one target state. This Trivial DA (TDA) approach leaves plenty of room for improvement through exploring more complex DA techniques in forthcoming work.



### Context-driven Visual Object Recognition based on Knowledge Graphs
- **Arxiv ID**: http://arxiv.org/abs/2210.11233v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LG, cs.SC
- **Links**: [PDF](http://arxiv.org/pdf/2210.11233v1)
- **Published**: 2022-10-20 13:09:00+00:00
- **Updated**: 2022-10-20 13:09:00+00:00
- **Authors**: Sebastian Monka, Lavdim Halilaj, Achim Rettinger
- **Comment**: ISWC 2022
- **Journal**: None
- **Summary**: Current deep learning methods for object recognition are purely data-driven and require a large number of training samples to achieve good results. Due to their sole dependence on image data, these methods tend to fail when confronted with new environments where even small deviations occur. Human perception, however, has proven to be significantly more robust to such distribution shifts. It is assumed that their ability to deal with unknown scenarios is based on extensive incorporation of contextual knowledge. Context can be based either on object co-occurrences in a scene or on memory of experience. In accordance with the human visual cortex which uses context to form different object representations for a seen image, we propose an approach that enhances deep learning methods by using external contextual knowledge encoded in a knowledge graph. Therefore, we extract different contextual views from a generic knowledge graph, transform the views into vector space and infuse it into a DNN. We conduct a series of experiments to investigate the impact of different contextual views on the learned object representations for the same image dataset. The experimental results provide evidence that the contextual views influence the image representations in the DNN differently and therefore lead to different predictions for the same images. We also show that context helps to strengthen the robustness of object recognition models for out-of-distribution images, usually occurring in transfer learning tasks or real-world scenarios.



### Attacking Motion Estimation with Adversarial Snow
- **Arxiv ID**: http://arxiv.org/abs/2210.11242v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11242v1)
- **Published**: 2022-10-20 13:14:19+00:00
- **Updated**: 2022-10-20 13:14:19+00:00
- **Authors**: Jenny Schmalfuss, Lukas Mehl, Andrés Bruhn
- **Comment**: None
- **Journal**: None
- **Summary**: Current adversarial attacks for motion estimation (optical flow) optimize small per-pixel perturbations, which are unlikely to appear in the real world. In contrast, we exploit a real-world weather phenomenon for a novel attack with adversarially optimized snow. At the core of our attack is a differentiable renderer that consistently integrates photorealistic snowflakes with realistic motion into the 3D scene. Through optimization we obtain adversarial snow that significantly impacts the optical flow while being indistinguishable from ordinary snow. Surprisingly, the impact of our novel attack is largest on methods that previously showed a high robustness to small L_p perturbations.



### TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition
- **Arxiv ID**: http://arxiv.org/abs/2210.11277v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.11277v2)
- **Published**: 2022-10-20 13:52:18+00:00
- **Updated**: 2022-11-03 05:09:34+00:00
- **Authors**: Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, Kui Jia
- **Comment**: Accepted by NeurIPS 2022
- **Journal**: None
- **Summary**: Creation of 3D content by stylization is a promising yet challenging problem in computer vision and graphics research. In this work, we focus on stylizing photorealistic appearance renderings of a given surface mesh of arbitrary topology. Motivated by the recent surge of cross-modal supervision of the Contrastive Language-Image Pre-training (CLIP) model, we propose TANGO, which transfers the appearance style of a given 3D shape according to a text prompt in a photorealistic manner. Technically, we propose to disentangle the appearance style as the spatially varying bidirectional reflectance distribution function, the local geometric variation, and the lighting condition, which are jointly optimized, via supervision of the CLIP loss, by a spherical Gaussians based differentiable renderer. As such, TANGO enables photorealistic 3D style transfer by automatically predicting reflectance effects even for bare, low-quality meshes, without training on a task-specific dataset. Extensive experiments show that TANGO outperforms existing methods of text-driven 3D style transfer in terms of photorealistic quality, consistency of 3D geometry, and robustness when stylizing low-quality meshes. Our codes and results are available at our project webpage https://cyw-3d.github.io/tango/.



### Cyclical Self-Supervision for Semi-Supervised Ejection Fraction Prediction from Echocardiogram Videos
- **Arxiv ID**: http://arxiv.org/abs/2210.11291v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11291v2)
- **Published**: 2022-10-20 14:23:40+00:00
- **Updated**: 2022-12-16 07:48:13+00:00
- **Authors**: Weihang Dai, Xiaomeng Li, Xinpeng Ding, Kwang-Ting Cheng
- **Comment**: Accepted in IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Left-ventricular ejection fraction (LVEF) is an important indicator of heart failure. Existing methods for LVEF estimation from video require large amounts of annotated data to achieve high performance, e.g. using 10,030 labeled echocardiogram videos to achieve mean absolute error (MAE) of 4.10. Labeling these videos is time-consuming however and limits potential downstream applications to other heart diseases. This paper presents the first semi-supervised approach for LVEF prediction. Unlike general video prediction tasks, LVEF prediction is specifically related to changes in the left ventricle (LV) in echocardiogram videos. By incorporating knowledge learned from predicting LV segmentations into LVEF regression, we can provide additional context to the model for better predictions. To this end, we propose a novel Cyclical Self-Supervision (CSS) method for learning video-based LV segmentation, which is motivated by the observation that the heartbeat is a cyclical process with temporal repetition. Prediction masks from our segmentation model can then be used as additional input for LVEF regression to provide spatial context for the LV region. We also introduce teacher-student distillation to distill the information from LV segmentation masks into an end-to-end LVEF regression model that only requires video inputs. Results show our method outperforms alternative semi-supervised methods and can achieve MAE of 4.17, which is competitive with state-of-the-art supervised performance, using half the number of labels. Validation on an external dataset also shows improved generalization ability from using our method. Our code is available at https://github.com/xmed-lab/CSS-SemiVideo.



### A Survey of Computer Vision Technologies In Urban and Controlled-environment Agriculture
- **Arxiv ID**: http://arxiv.org/abs/2210.11318v1
- **DOI**: None
- **Categories**: **cs.CV**, A.1; I.2.10; J.m
- **Links**: [PDF](http://arxiv.org/pdf/2210.11318v1)
- **Published**: 2022-10-20 14:51:01+00:00
- **Updated**: 2022-10-20 14:51:01+00:00
- **Authors**: Jiayun Luo, Boyang Li, Cyril Leung
- **Comment**: 18 pages, 2 tables, submitted to ACM Computing Surveys
- **Journal**: None
- **Summary**: In the evolution of agriculture to its next stage, Agriculture 5.0, artificial intelligence will play a central role. Controlled-environment agriculture, or CEA, is a special form of urban and suburban agricultural practice that offers numerous economic, environmental, and social benefits, including shorter transportation routes to population centers, reduced environmental impact, and increased productivity. Due to its ability to control environmental factors, CEA couples well with computer vision (CV) in the adoption of real-time monitoring of the plant conditions and autonomous cultivation and harvesting. The objective of this paper is to familiarize CV researchers with agricultural applications and agricultural practitioners with the solutions offered by CV. We identify five major CV applications in CEA, analyze their requirements and motivation, and survey the state of the art as reflected in 68 technical papers using deep learning methods. In addition, we discuss five key subareas of computer vision and how they related to these CEA problems, as well as nine vision-based CEA datasets. We hope the survey will help researchers quickly gain a bird-eye view of the striving research area and will spark inspiration for new research and development.



### Image-Text Retrieval with Binary and Continuous Label Supervision
- **Arxiv ID**: http://arxiv.org/abs/2210.11319v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.11319v1)
- **Published**: 2022-10-20 14:52:34+00:00
- **Updated**: 2022-10-20 14:52:34+00:00
- **Authors**: Zheng Li, Caili Guo, Zerun Feng, Jenq-Neng Hwang, Ying Jin, Yufeng Zhang
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: Most image-text retrieval work adopts binary labels indicating whether a pair of image and text matches or not. Such a binary indicator covers only a limited subset of image-text semantic relations, which is insufficient to represent relevance degrees between images and texts described by continuous labels such as image captions. The visual-semantic embedding space obtained by learning binary labels is incoherent and cannot fully characterize the relevance degrees. In addition to the use of binary labels, this paper further incorporates continuous pseudo labels (generally approximated by text similarity between captions) to indicate the relevance degrees. To learn a coherent embedding space, we propose an image-text retrieval framework with Binary and Continuous Label Supervision (BCLS), where binary labels are used to guide the retrieval model to learn limited binary correlations, and continuous labels are complementary to the learning of image-text semantic relations. For the learning of binary labels, we improve the common Triplet ranking loss with Soft Negative mining (Triplet-SN) to improve convergence. For the learning of continuous labels, we design Kendall ranking loss inspired by Kendall rank correlation coefficient (Kendall), which improves the correlation between the similarity scores predicted by the retrieval model and the continuous labels. To mitigate the noise introduced by the continuous pseudo labels, we further design Sliding Window sampling and Hard Sample mining strategy (SW-HS) to alleviate the impact of noise and reduce the complexity of our framework to the same order of magnitude as the triplet ranking loss. Extensive experiments on two image-text retrieval benchmarks demonstrate that our method can improve the performance of state-of-the-art image-text retrieval models.



### Deep-Learning-Based Precipitation Nowcasting with Ground Weather Station Data and Radar Data
- **Arxiv ID**: http://arxiv.org/abs/2210.12853v1
- **DOI**: None
- **Categories**: **physics.ao-ph**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.12853v1)
- **Published**: 2022-10-20 14:59:58+00:00
- **Updated**: 2022-10-20 14:59:58+00:00
- **Authors**: Jihoon Ko, Kyuhan Lee, Hyunjin Hwang, Kijung Shin
- **Comment**: to appear at the 17th International Workshop on Spatial and
  Spatiotemporal Data Mining (SSTDM-22)
- **Journal**: None
- **Summary**: Recently, many deep-learning techniques have been applied to various weather-related prediction tasks, including precipitation nowcasting (i.e., predicting precipitation levels and locations in the near future). Most existing deep-learning-based approaches for precipitation nowcasting, however, consider only radar and/or satellite images as inputs, and meteorological observations collected from ground weather stations, which are sparsely located, are relatively unexplored. In this paper, we propose ASOC, a novel attentive method for effectively exploiting ground-based meteorological observations from multiple weather stations. ASOC is designed to capture temporal dynamics of the observations and also contextual relationships between them. ASOC is easily combined with existing image-based precipitation nowcasting models without changing their architectures. We show that such a combination improves the average critical success index (CSI) of predicting heavy (at least 10 mm/hr) and light (at least 1 mm/hr) rainfall events at 1-6 hr lead times by 5.7%, compared to the original image-based model, using the radar images and ground-based observations around South Korea collected from 2014 to 2020.



### Play It Back: Iterative Attention for Audio Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.11328v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2210.11328v2)
- **Published**: 2022-10-20 15:03:22+00:00
- **Updated**: 2023-03-12 12:03:04+00:00
- **Authors**: Alexandros Stergiou, Dima Damen
- **Comment**: Accepted at IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP) 2023
- **Journal**: None
- **Summary**: A key function of auditory cognition is the association of characteristic sounds with their corresponding semantics over time. Humans attempting to discriminate between fine-grained audio categories, often replay the same discriminative sounds to increase their prediction confidence. We propose an end-to-end attention-based architecture that through selective repetition attends over the most discriminative sounds across the audio sequence. Our model initially uses the full audio sequence and iteratively refines the temporal segments replayed based on slot attention. At each playback, the selected segments are replayed using a smaller hop length which represents higher resolution features within these segments. We show that our method can consistently achieve state-of-the-art performance across three audio-classification benchmarks: AudioSet, VGG-Sound, and EPIC-KITCHENS-100.



### SS-VAERR: Self-Supervised Apparent Emotional Reaction Recognition from Video
- **Arxiv ID**: http://arxiv.org/abs/2210.11341v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11341v1)
- **Published**: 2022-10-20 15:21:51+00:00
- **Updated**: 2022-10-20 15:21:51+00:00
- **Authors**: Marija Jegorova, Stavros Petridis, Maja Pantic
- **Comment**: None
- **Journal**: None
- **Summary**: This work focuses on the apparent emotional reaction recognition (AERR) from the video-only input, conducted in a self-supervised fashion. The network is first pre-trained on different self-supervised pretext tasks and later fine-tuned on the downstream target task. Self-supervised learning facilitates the use of pre-trained architectures and larger datasets that might be deemed unfit for the target task and yet might be useful to learn informative representations and hence provide useful initializations for further fine-tuning on smaller more suitable data. Our presented contribution is two-fold: (1) an analysis of different state-of-the-art (SOTA) pretext tasks for the video-only apparent emotional reaction recognition architecture, and (2) an analysis of various combinations of the regression and classification losses that are likely to improve the performance further. Together these two contributions result in the current state-of-the-art performance for the video-only spontaneous apparent emotional reaction recognition with continuous annotations.



### Deep conditional transformation models for survival analysis
- **Arxiv ID**: http://arxiv.org/abs/2210.11366v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11366v2)
- **Published**: 2022-10-20 16:01:05+00:00
- **Updated**: 2022-10-21 13:46:28+00:00
- **Authors**: Gabriele Campanella, Lucas Kook, Ida Häggström, Torsten Hothorn, Thomas J. Fuchs
- **Comment**: None
- **Journal**: None
- **Summary**: An every increasing number of clinical trials features a time-to-event outcome and records non-tabular patient data, such as magnetic resonance imaging or text data in the form of electronic health records. Recently, several neural-network based solutions have been proposed, some of which are binary classifiers. Parametric, distribution-free approaches which make full use of survival time and censoring status have not received much attention. We present deep conditional transformation models (DCTMs) for survival outcomes as a unifying approach to parametric and semiparametric survival analysis. DCTMs allow the specification of non-linear and non-proportional hazards for both tabular and non-tabular data and extend to all types of censoring and truncation. On real and semi-synthetic data, we show that DCTMs compete with state-of-the-art DL approaches to survival analysis.



### On Feature Learning in the Presence of Spurious Correlations
- **Arxiv ID**: http://arxiv.org/abs/2210.11369v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2210.11369v1)
- **Published**: 2022-10-20 16:10:28+00:00
- **Updated**: 2022-10-20 16:10:28+00:00
- **Authors**: Pavel Izmailov, Polina Kirichenko, Nate Gruver, Andrew Gordon Wilson
- **Comment**: NeurIPS 2022. Code available at
  https://github.com/izmailovpavel/spurious_feature_learning
- **Journal**: None
- **Summary**: Deep classifiers are known to rely on spurious features $\unicode{x2013}$ patterns which are correlated with the target on the training data but not inherently relevant to the learning problem, such as the image backgrounds when classifying the foregrounds. In this paper we evaluate the amount of information about the core (non-spurious) features that can be decoded from the representations learned by standard empirical risk minimization (ERM) and specialized group robustness training. Following recent work on Deep Feature Reweighting (DFR), we evaluate the feature representations by re-training the last layer of the model on a held-out set where the spurious correlation is broken. On multiple vision and NLP problems, we show that the features learned by simple ERM are highly competitive with the features learned by specialized group robustness methods targeted at reducing the effect of spurious correlations. Moreover, we show that the quality of learned feature representations is greatly affected by the design decisions beyond the training method, such as the model architecture and pre-training strategy. On the other hand, we find that strong regularization is not necessary for learning high quality feature representations. Finally, using insights from our analysis, we significantly improve upon the best results reported in the literature on the popular Waterbirds, CelebA hair color prediction and WILDS-FMOW problems, achieving 97%, 92% and 50% worst-group accuracies, respectively.



### Transformer-based Global 3D Hand Pose Estimation in Two Hands Manipulating Objects Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2210.11384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11384v1)
- **Published**: 2022-10-20 16:24:47+00:00
- **Updated**: 2022-10-20 16:24:47+00:00
- **Authors**: Hoseong Cho, Donguk Kim, Chanwoo Kim, Seongyeong Lee, Seungryul Baek
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: This report describes our 1st place solution to ECCV 2022 challenge on Human Body, Hands, and Activities (HBHA) from Egocentric and Multi-view Cameras (hand pose estimation). In this challenge, we aim to estimate global 3D hand poses from the input image where two hands and an object are interacting on the egocentric viewpoint. Our proposed method performs end-to-end multi-hand pose estimation via transformer architecture. In particular, our method robustly estimates hand poses in a scenario where two hands interact. Additionally, we propose an algorithm that considers hand scales to robustly estimate the absolute depth. The proposed algorithm works well even when the hand sizes are various for each person. Our method attains 14.4 mm (left) and 15.9 mm (right) errors for each hand in the test set.



### Transformer-based Action recognition in hand-object interacting scenarios
- **Arxiv ID**: http://arxiv.org/abs/2210.11387v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11387v1)
- **Published**: 2022-10-20 16:27:37+00:00
- **Updated**: 2022-10-20 16:27:37+00:00
- **Authors**: Hoseong Cho, Seungryul Baek
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: This report describes the 2nd place solution to the ECCV 2022 Human Body, Hands, and Activities (HBHA) from Egocentric and Multi-view Cameras Challenge: Action Recognition. This challenge aims to recognize hand-object interaction in an egocentric view. We propose a framework that estimates keypoints of two hands and an object with a Transformer-based keypoint estimator and recognizes actions based on the estimated keypoints. We achieved a top-1 accuracy of 87.19% on the testset.



### Physics-informed deep diffusion MRI reconstruction: break the bottleneck of training data in artificial intelligence
- **Arxiv ID**: http://arxiv.org/abs/2210.11388v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11388v1)
- **Published**: 2022-10-20 16:27:54+00:00
- **Updated**: 2022-10-20 16:27:54+00:00
- **Authors**: Chen Qian, Zi Wang, Xinlin Zhang, Qingrui Cai, Taishan Kang, Boyu Jiang, Ran Tao, Zhigang Wu, Di Guo, Xiaobo Qu
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: In this work, we propose a Physics-Informed Deep Diffusion magnetic resonance imaging (DWI) reconstruction method (PIDD). PIDD contains two main components: The multi-shot DWI data synthesis and a deep learning reconstruction network. For data synthesis, we first mathematically analyze the motion during the multi-shot data acquisition and approach it by a simplified physical motion model. The motion model inspires a polynomial model for motion-induced phase synthesis. Then, lots of synthetic phases are combined with a few real data to generate a large amount of training data. For reconstruction network, we exploit the smoothness property of each shot image phase as learnable convolution kernels in the k-space and complementary sparsity in the image domain. Results on both synthetic and in vivo brain data show that, the proposed PIDD trained on synthetic data enables sub-second ultra-fast, high-quality, and robust reconstruction with different b-values and undersampling patterns.



### TTTFlow: Unsupervised Test-Time Training with Normalizing Flow
- **Arxiv ID**: http://arxiv.org/abs/2210.11389v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11389v1)
- **Published**: 2022-10-20 16:32:06+00:00
- **Updated**: 2022-10-20 16:32:06+00:00
- **Authors**: David Osowiechi, Gustavo A. Vargas Hakim, Mehrdad Noori, Milad Cheraghalikhani, Ismail Ben Ayed, Christian Desrosiers
- **Comment**: None
- **Journal**: None
- **Summary**: A major problem of deep neural networks for image classification is their vulnerability to domain changes at test-time. Recent methods have proposed to address this problem with test-time training (TTT), where a two-branch model is trained to learn a main classification task and also a self-supervised task used to perform test-time adaptation. However, these techniques require defining a proxy task specific to the target application. To tackle this limitation, we propose TTTFlow: a Y-shaped architecture using an unsupervised head based on Normalizing Flows to learn the normal distribution of latent features and detect domain shifts in test examples. At inference, keeping the unsupervised head fixed, we adapt the model to domain-shifted examples by maximizing the log likelihood of the Normalizing Flow. Our results show that our method can significantly improve the accuracy with respect to previous works.



### Self-Supervised Learning with Masked Image Modeling for Teeth Numbering, Detection of Dental Restorations, and Instance Segmentation in Dental Panoramic Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2210.11404v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11404v1)
- **Published**: 2022-10-20 16:50:07+00:00
- **Updated**: 2022-10-20 16:50:07+00:00
- **Authors**: Amani Almalki, Longin Jan Latecki
- **Comment**: None
- **Journal**: None
- **Summary**: The computer-assisted radiologic informative report is currently emerging in dental practice to facilitate dental care and reduce time consumption in manual panoramic radiographic interpretation. However, the amount of dental radiographs for training is very limited, particularly from the point of view of deep learning. This study aims to utilize recent self-supervised learning methods like SimMIM and UM-MAE to increase the model efficiency and understanding of the limited number of dental radiographs. We use the Swin Transformer for teeth numbering, detection of dental restorations, and instance segmentation tasks. To the best of our knowledge, this is the first study that applied self-supervised learning methods to Swin Transformer on dental panoramic radiographs. Our results show that the SimMIM method obtained the highest performance of 90.4% and 88.9% on detecting teeth and dental restorations and instance segmentation, respectively, increasing the average precision by 13.4 and 12.8 over the random initialization baseline. Moreover, we augment and correct the existing dataset of panoramic radiographs. The code and the dataset are available at https://github.com/AmaniHAlmalki/DentalMIM.



### Similarity of Neural Architectures Based on Input Gradient Transferability
- **Arxiv ID**: http://arxiv.org/abs/2210.11407v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11407v2)
- **Published**: 2022-10-20 16:56:47+00:00
- **Updated**: 2023-03-15 08:06:48+00:00
- **Authors**: Jaehui Hwang, Dongyoon Han, Byeongho Heo, Song Park, Sanghyuk Chun, Jong-Seok Lee
- **Comment**: 21pages, 10 figures, 1.5MB
- **Journal**: None
- **Summary**: In recent years, a huge amount of deep neural architectures have been developed for image classification. It remains curious whether these models are similar or different and what factors contribute to their similarities or differences. To address this question, we aim to design a quantitative and scalable similarity function between neural architectures. We utilize adversarial attack transferability, which has information related to input gradients and decision boundaries that are widely used to understand model behaviors. We conduct a large-scale analysis on 69 state-of-the-art ImageNet classifiers using our proposed similarity function to answer the question. Moreover, we observe neural architecture-related phenomena using model similarity that model diversity can lead to better performance on model ensembles and knowledge distillation under specific conditions. Our results provide insights into why the development of diverse neural architectures with distinct components is necessary.



### GPR-Net: Multi-view Layout Estimation via a Geometry-aware Panorama Registration Network
- **Arxiv ID**: http://arxiv.org/abs/2210.11419v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11419v2)
- **Published**: 2022-10-20 17:10:41+00:00
- **Updated**: 2022-10-21 14:26:37+00:00
- **Authors**: Jheng-Wei Su, Chi-Han Peng, Peter Wonka, Hung-Kuo Chu
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstructing 3D layouts from multiple $360^{\circ}$ panoramas has received increasing attention recently as estimating a complete layout of a large-scale and complex room from a single panorama is very difficult. The state-of-the-art method, called PSMNet, introduces the first learning-based framework that jointly estimates the room layout and registration given a pair of panoramas. However, PSMNet relies on an approximate (i.e., "noisy") registration as input. Obtaining this input requires a solution for wide baseline registration which is a challenging problem. In this work, we present a complete multi-view panoramic layout estimation framework that jointly learns panorama registration and layout estimation given a pair of panoramas without relying on a pose prior. The major improvement over PSMNet comes from a novel Geometry-aware Panorama Registration Network or GPR-Net that effectively tackles the wide baseline registration problem by exploiting the layout geometry and computing fine-grained correspondences on the layout boundaries, instead of the global pixel-space. Our architecture consists of two parts. First, given two panoramas, we adopt a vision transformer to learn a set of 1D horizon features sampled on the panorama. These 1D horizon features encode the depths of individual layout boundary samples and the correspondence and covisibility maps between layout boundaries. We then exploit a non-linear registration module to convert these 1D horizon features into a set of corresponding 2D boundary points on the layout. Finally, we estimate the final relative camera pose via RANSAC and obtain the complete layout simply by taking the union of registered layouts. Experimental results indicate that our method achieves state-of-the-art performance in both panorama registration and layout estimation on a large-scale indoor panorama dataset ZInD.



### DiffEdit: Diffusion-based semantic image editing with mask guidance
- **Arxiv ID**: http://arxiv.org/abs/2210.11427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11427v1)
- **Published**: 2022-10-20 17:16:37+00:00
- **Updated**: 2022-10-20 17:16:37+00:00
- **Authors**: Guillaume Couairon, Jakob Verbeek, Holger Schwenk, Matthieu Cord
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Image generation has recently seen tremendous advances, with diffusion models allowing to synthesize convincing images for a large variety of text prompts. In this article, we propose DiffEdit, a method to take advantage of text-conditioned diffusion models for the task of semantic image editing, where the goal is to edit an image based on a text query. Semantic image editing is an extension of image generation, with the additional constraint that the generated image should be as similar as possible to a given input image. Current editing methods based on diffusion models usually require to provide a mask, making the task much easier by treating it as a conditional inpainting task. In contrast, our main contribution is able to automatically generate a mask highlighting regions of the input image that need to be edited, by contrasting predictions of a diffusion model conditioned on different text prompts. Moreover, we rely on latent inference to preserve content in those regions of interest and show excellent synergies with mask-based diffusion. DiffEdit achieves state-of-the-art editing performance on ImageNet. In addition, we evaluate semantic image editing in more challenging settings, using images from the COCO dataset as well as text-based generated images.



### Cell tracking for live-cell microscopy using an activity-prioritized assignment strategy
- **Arxiv ID**: http://arxiv.org/abs/2210.11441v1
- **DOI**: 10.1109/IPAS55744.2022.10053036
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2210.11441v1)
- **Published**: 2022-10-20 17:40:31+00:00
- **Updated**: 2022-10-20 17:40:31+00:00
- **Authors**: Karina Ruzaeva, Jan-Christopher Cohrs, Keitaro Kasahara, Dietrich Kohlheyer, Katharina Nöh, Benjamin Berkels
- **Comment**: Accepted for publication at the 5th IEEE International Conference on
  Image Processing Applications and Systems 2022 (IPAS)
- **Journal**: None
- **Summary**: Cell tracking is an essential tool in live-cell imaging to determine single-cell features, such as division patterns or elongation rates. Unlike in common multiple object tracking, in microbial live-cell experiments cells are growing, moving, and dividing over time, to form cell colonies that are densely packed in mono-layer structures. With increasing cell numbers, following the precise cell-cell associations correctly over many generations becomes more and more challenging, due to the massively increasing number of possible associations.   To tackle this challenge, we propose a fast parameter-free cell tracking approach, which consists of activity-prioritized nearest neighbor assignment of growing cells and a combinatorial solver that assigns splitting mother cells to their daughters. As input for the tracking, Omnipose is utilized for instance segmentation. Unlike conventional nearest-neighbor-based tracking approaches, the assignment steps of our proposed method are based on a Gaussian activity-based metric, predicting the cell-specific migration probability, thereby limiting the number of erroneous assignments. In addition to being a building block for cell tracking, the proposed activity map is a standalone tracking-free metric for indicating cell activity. Finally, we perform a quantitative analysis of the tracking accuracy for different frame rates, to inform life scientists about a suitable (in terms of tracking performance) choice of the frame rate for their cultivation experiments, when cell tracks are the desired key outcome.



### Snapshot of Algebraic Vision
- **Arxiv ID**: http://arxiv.org/abs/2210.11443v1
- **DOI**: None
- **Categories**: **math.AG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11443v1)
- **Published**: 2022-10-20 17:45:22+00:00
- **Updated**: 2022-10-20 17:45:22+00:00
- **Authors**: Joe Kileel, Kathlén Kohn
- **Comment**: None
- **Journal**: None
- **Summary**: In this survey article, we present interactions between algebraic geometry and computer vision, which have recently come under the header of Algebraic Vision. The subject has given new insights in multiple view geometry and its application to 3D scene reconstruction, and carried a host of novel problems and ideas back into algebraic geometry.



### MixMask: Revisiting Masking Strategy for Siamese ConvNets
- **Arxiv ID**: http://arxiv.org/abs/2210.11456v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11456v3)
- **Published**: 2022-10-20 17:54:03+00:00
- **Updated**: 2023-03-21 16:57:57+00:00
- **Authors**: Kirill Vishniakov, Eric Xing, Zhiqiang Shen
- **Comment**: Technical report. Code is available at
  https://github.com/LightnessOfBeing/MixMask
- **Journal**: None
- **Summary**: Recent advances in self-supervised learning have integrated Masked Image Modeling (MIM) and Siamese Networks into a unified framework that leverages the benefits of both techniques. However, several issues remain unaddressed when applying conventional erase-based masking with Siamese ConvNets. These include (I) the inability to drop uninformative masked regions in ConvNets as they process data continuously, resulting in low training efficiency compared to ViT models; and (II) the mismatch between erase-based masking and the contrastive-based objective in Siamese ConvNets, which differs from the MIM approach. In this paper, we propose a filling-based masking strategy called MixMask to prevent information incompleteness caused by the randomly erased regions in an image in the vanilla masking method. Furthermore, we introduce a flexible loss function design that considers the semantic distance change between two different mixed views to adapt the integrated architecture and prevent mismatches between the transformed input and objective in Masked Siamese ConvNets (MSCN). We conducted extensive experiments on various datasets, including CIFAR-100, Tiny-ImageNet, and ImageNet-1K. The results demonstrate that our proposed framework achieves superior accuracy on linear probing, semi-supervised, and supervised finetuning, outperforming the state-of-the-art MSCN by a significant margin. Additionally, we demonstrate the superiority of our approach in object detection and segmentation tasks. Our source code is available at https://github.com/LightnessOfBeing/MixMask.



### Breaking Bad: A Dataset for Geometric Fracture and Reassembly
- **Arxiv ID**: http://arxiv.org/abs/2210.11463v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.11463v1)
- **Published**: 2022-10-20 17:57:19+00:00
- **Updated**: 2022-10-20 17:57:19+00:00
- **Authors**: Silvia Sellán, Yun-Chun Chen, Ziyi Wu, Animesh Garg, Alec Jacobson
- **Comment**: NeurIPS 2022 Track on Datasets and Benchmarks. The first three
  authors contributed equally to this work. Project page:
  https://breaking-bad-dataset.github.io/ Code:
  https://github.com/Wuziyi616/multi_part_assembly Dataset:
  https://borealisdata.ca/dataset.xhtml?persistentId=doi:10.5683/SP3/LZNPKB
- **Journal**: None
- **Summary**: We introduce Breaking Bad, a large-scale dataset of fractured objects. Our dataset consists of over one million fractured objects simulated from ten thousand base models. The fracture simulation is powered by a recent physically based algorithm that efficiently generates a variety of fracture modes of an object. Existing shape assembly datasets decompose objects according to semantically meaningful parts, effectively modeling the construction process. In contrast, Breaking Bad models the destruction process of how a geometric object naturally breaks into fragments. Our dataset serves as a benchmark that enables the study of fractured object reassembly and presents new challenges for geometric shape understanding. We analyze our dataset with several geometry measurements and benchmark three state-of-the-art shape assembly deep learning methods under various settings. Extensive experimental results demonstrate the difficulty of our dataset, calling on future research in model designs specifically for the geometric shape assembly task. We host our dataset at https://breaking-bad-dataset.github.io/.



### Self-Supervised Learning via Maximum Entropy Coding
- **Arxiv ID**: http://arxiv.org/abs/2210.11464v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.11464v1)
- **Published**: 2022-10-20 17:58:30+00:00
- **Updated**: 2022-10-20 17:58:30+00:00
- **Authors**: Xin Liu, Zhongdao Wang, Yali Li, Shengjin Wang
- **Comment**: Accepted by NeurIPS 2022
- **Journal**: None
- **Summary**: A mainstream type of current self-supervised learning methods pursues a general-purpose representation that can be well transferred to downstream tasks, typically by optimizing on a given pretext task such as instance discrimination. In this work, we argue that existing pretext tasks inevitably introduce biases into the learned representation, which in turn leads to biased transfer performance on various downstream tasks. To cope with this issue, we propose Maximum Entropy Coding (MEC), a more principled objective that explicitly optimizes on the structure of the representation, so that the learned representation is less biased and thus generalizes better to unseen downstream tasks. Inspired by the principle of maximum entropy in information theory, we hypothesize that a generalizable representation should be the one that admits the maximum entropy among all plausible representations. To make the objective end-to-end trainable, we propose to leverage the minimal coding length in lossy data coding as a computationally tractable surrogate for the entropy, and further derive a scalable reformulation of the objective that allows fast computation. Extensive experiments demonstrate that MEC learns a more generalizable representation than previous methods based on specific pretext tasks. It achieves state-of-the-art performance consistently on various downstream tasks, including not only ImageNet linear probe, but also semi-supervised classification, object detection, instance segmentation, and object tracking. Interestingly, we show that existing batch-wise and feature-wise self-supervised objectives could be seen equivalent to low-order approximations of MEC. Code and pre-trained models are available at https://github.com/xinliu20/MEC.



### Multi-View Guided Multi-View Stereo
- **Arxiv ID**: http://arxiv.org/abs/2210.11467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11467v1)
- **Published**: 2022-10-20 17:59:18+00:00
- **Updated**: 2022-10-20 17:59:18+00:00
- **Authors**: Matteo Poggi, Andrea Conti, Stefano Mattoccia
- **Comment**: IROS 2022. First two authors contributed equally. Project page:
  https://github.com/andreaconti/multi-view-guided-multi-view-stereo
- **Journal**: None
- **Summary**: This paper introduces a novel deep framework for dense 3D reconstruction from multiple image frames, leveraging a sparse set of depth measurements gathered jointly with image acquisition. Given a deep multi-view stereo network, our framework uses sparse depth hints to guide the neural network by modulating the plane-sweep cost volume built during the forward step, enabling us to infer constantly much more accurate depth maps. Moreover, since multiple viewpoints can provide additional depth measurements, we propose a multi-view guidance strategy that increases the density of the sparse points used to guide the network, thus leading to even more accurate results. We evaluate our Multi-View Guided framework within a variety of state-of-the-art deep multi-view stereo networks, demonstrating its effectiveness at improving the results achieved by each of them on BlendedMVG and DTU datasets.



### G2NetPL: Generic Game-Theoretic Network for Partial-Label Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2210.11469v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11469v1)
- **Published**: 2022-10-20 17:59:21+00:00
- **Updated**: 2022-10-20 17:59:21+00:00
- **Authors**: Rabab Abdelfattah, Xin Zhang, Mostafa M. Fouda, Xiaofeng Wang, Song Wang
- **Comment**: Accepted by BMVC2022
- **Journal**: None
- **Summary**: Multi-label image classification aims to predict all possible labels in an image. It is usually formulated as a partial-label learning problem, since it could be expensive in practice to annotate all the labels in every training image. Existing works on partial-label learning focus on the case where each training image is labeled with only a subset of its positive/negative labels. To effectively address partial-label classification, this paper proposes an end-to-end Generic Game-theoretic Network (G2NetPL) for partial-label learning, which can be applied to most partial-label settings, including a very challenging, but annotation-efficient case where only a subset of the training images are labeled, each with only one positive label, while the rest of the training images remain unlabeled. In G2NetPL, each unobserved label is associated with a soft pseudo label, which, together with the network, formulates a two-player non-zero-sum non-cooperative game. The objective of the network is to minimize the loss function with given pseudo labels, while the pseudo labels will seek convergence to 1 (positive) or 0 (negative) with a penalty of deviating from the predicted labels determined by the network. In addition, we introduce a confidence-aware scheduler into the loss of the network to adaptively perform easy-to-hard learning for different labels. Extensive experiments demonstrate that our proposed G2NetPL outperforms many state-of-the-art multi-label classification methods under various partial-label settings on three different datasets.



### i-MAE: Are Latent Representations in Masked Autoencoders Linearly Separable?
- **Arxiv ID**: http://arxiv.org/abs/2210.11470v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11470v1)
- **Published**: 2022-10-20 17:59:54+00:00
- **Updated**: 2022-10-20 17:59:54+00:00
- **Authors**: Kevin Zhang, Zhiqiang Shen
- **Comment**: Technical report. Project page:
  https://zhiqiangshen.com/projects/i-mae/
- **Journal**: None
- **Summary**: Masked image modeling (MIM) has been recognized as a strong and popular self-supervised pre-training approach in the vision domain. However, the interpretability of the mechanism and properties of the learned representations by such a scheme are so far not well-explored. In this work, through comprehensive experiments and empirical studies on Masked Autoencoders (MAE), we address two critical questions to explore the behaviors of the learned representations: (i) Are the latent representations in Masked Autoencoders linearly separable if the input is a mixture of two images instead of one? This can be concrete evidence used to explain why MAE-learned representations have superior performance on downstream tasks, as proven by many literature impressively. (ii) What is the degree of semantics encoded in the latent feature space by Masked Autoencoders? To explore these two problems, we propose a simple yet effective Interpretable MAE (i-MAE) framework with a two-way image reconstruction and a latent feature reconstruction with distillation loss to help us understand the behaviors inside MAE's structure. Extensive experiments are conducted on CIFAR-10/100, Tiny-ImageNet and ImageNet-1K datasets to verify the observations we discovered. Furthermore, in addition to qualitatively analyzing the characteristics of the latent representations, we examine the existence of linear separability and the degree of semantics in the latent space by proposing two novel metrics. The surprising and consistent results across the qualitative and quantitative experiments demonstrate that i-MAE is a superior framework design for interpretability research of MAE frameworks, as well as achieving better representational ability. Code is available at https://github.com/vision-learning-acceleration-lab/i-mae.



### VIBUS: Data-efficient 3D Scene Parsing with VIewpoint Bottleneck and Uncertainty-Spectrum Modeling
- **Arxiv ID**: http://arxiv.org/abs/2210.11472v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.11472v1)
- **Published**: 2022-10-20 17:59:57+00:00
- **Updated**: 2022-10-20 17:59:57+00:00
- **Authors**: Beiwen Tian, Liyi Luo, Hao Zhao, Guyue Zhou
- **Comment**: Accepted to ISPRS Journal of Photogrammetry and Remote Sensing, Code:
  https://github.com/AIR-DISCOVER/VIBUS
- **Journal**: None
- **Summary**: Recently, 3D scenes parsing with deep learning approaches has been a heating topic. However, current methods with fully-supervised models require manually annotated point-wise supervision which is extremely user-unfriendly and time-consuming to obtain. As such, training 3D scene parsing models with sparse supervision is an intriguing alternative. We term this task as data-efficient 3D scene parsing and propose an effective two-stage framework named VIBUS to resolve it by exploiting the enormous unlabeled points. In the first stage, we perform self-supervised representation learning on unlabeled points with the proposed Viewpoint Bottleneck loss function. The loss function is derived from an information bottleneck objective imposed on scenes under different viewpoints, making the process of representation learning free of degradation and sampling. In the second stage, pseudo labels are harvested from the sparse labels based on uncertainty-spectrum modeling. By combining data-driven uncertainty measures and 3D mesh spectrum measures (derived from normal directions and geodesic distances), a robust local affinity metric is obtained. Finite gamma/beta mixture models are used to decompose category-wise distributions of these measures, leading to automatic selection of thresholds. We evaluate VIBUS on the public benchmark ScanNet and achieve state-of-the-art results on both validation set and online test server. Ablation studies show that both Viewpoint Bottleneck and uncertainty-spectrum modeling bring significant improvements. Codes and models are publicly available at https://github.com/AIR-DISCOVER/VIBUS.



### Overexposure Mask Fusion: Generalizable Reverse ISP Multi-Step Refinement
- **Arxiv ID**: http://arxiv.org/abs/2210.11511v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11511v1)
- **Published**: 2022-10-20 18:21:41+00:00
- **Updated**: 2022-10-20 18:21:41+00:00
- **Authors**: Jinha Kim, Jun Jiang, Jinwei Gu
- **Comment**: 15 pages, 8 figures, ECCV
- **Journal**: None
- **Summary**: With the advent of deep learning methods replacing the ISP in transforming sensor RAW readings into RGB images, numerous methodologies solidified into real-life applications. Equally potent is the task of inverting this process which will have applications in enhancing computational photography tasks that are conducted in the RAW domain, addressing lack of available RAW data while reaping from the benefits of performing tasks directly on sensor readings. This paper's proposed methodology is a state-of-the-art solution to the task of RAW reconstruction, and the multi-step refinement process integrating an overexposure mask is novel in three ways: instead of from RGB to bayer, the pipeline trains from RGB to demosaiced RAW allowing use of perceptual loss functions; the multi-step processes has greatly enhanced the performance of the baseline U-Net from start to end; the pipeline is a generalizable process of refinement that can enhance other high performance methodologies that support end-to-end learning.



### Composing Ensembles of Pre-trained Models via Iterative Consensus
- **Arxiv ID**: http://arxiv.org/abs/2210.11522v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11522v1)
- **Published**: 2022-10-20 18:46:31+00:00
- **Updated**: 2022-10-20 18:46:31+00:00
- **Authors**: Shuang Li, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, Igor Mordatch
- **Comment**: None
- **Journal**: None
- **Summary**: Large pre-trained models exhibit distinct and complementary capabilities dependent on the data they are trained on. Language models such as GPT-3 are capable of textual reasoning but cannot understand visual information, while vision models such as DALL-E can generate photorealistic photos but fail to understand complex language descriptions. In this work, we propose a unified framework for composing ensembles of different pre-trained models -- combining the strengths of each individual model to solve various multimodal problems in a zero-shot manner. We use pre-trained models as "generators" or "scorers" and compose them via closed-loop iterative consensus optimization. The generator constructs proposals and the scorers iteratively provide feedback to refine the generated result. Such closed-loop communication enables models to correct errors caused by other models, significantly boosting performance on downstream tasks, e.g. improving accuracy on grade school math problems by 7.5%, without requiring any model finetuning. We demonstrate that consensus achieved by an ensemble of scorers outperforms the feedback of a single scorer, by leveraging the strengths of each expert model. Results show that the proposed method can be used as a general purpose framework for a wide range of zero-shot multimodal tasks, such as image generation, video question answering, mathematical reasoning, and robotic manipulation. Project page: https://energy-based-model.github.io/composing-pretrained-models.



### ConfMix: Unsupervised Domain Adaptation for Object Detection via Confidence-based Mixing
- **Arxiv ID**: http://arxiv.org/abs/2210.11539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11539v1)
- **Published**: 2022-10-20 19:16:39+00:00
- **Updated**: 2022-10-20 19:16:39+00:00
- **Authors**: Giulio Mattolin, Luca Zanella, Elisa Ricci, Yiming Wang
- **Comment**: Accepted in WACV 2023
- **Journal**: None
- **Summary**: Unsupervised Domain Adaptation (UDA) for object detection aims to adapt a model trained on a source domain to detect instances from a new target domain for which annotations are not available. Different from traditional approaches, we propose ConfMix, the first method that introduces a sample mixing strategy based on region-level detection confidence for adaptive object detector learning. We mix the local region of the target sample that corresponds to the most confident pseudo detections with a source image, and apply an additional consistency loss term to gradually adapt towards the target data distribution. In order to robustly define a confidence score for a region, we exploit the confidence score per pseudo detection that accounts for both the detector-dependent confidence and the bounding box uncertainty. Moreover, we propose a novel pseudo labelling scheme that progressively filters the pseudo target detections using the confidence metric that varies from a loose to strict manner along the training. We perform extensive experiments with three datasets, achieving state-of-the-art performance in two of them and approaching the supervised target model performance in the other. Code is available at: https://github.com/giuliomattolin/ConfMix.



### Transferring learned patterns from ground-based field imagery to predict UAV-based imagery for crop and weed semantic segmentation in precision crop farming
- **Arxiv ID**: http://arxiv.org/abs/2210.11545v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11545v1)
- **Published**: 2022-10-20 19:25:06+00:00
- **Updated**: 2022-10-20 19:25:06+00:00
- **Authors**: Junfeng Gao, Wenzhi Liao, David Nuyttens, Peter Lootens, Erik Alexandersson, Jan Pieters
- **Comment**: None
- **Journal**: None
- **Summary**: Weed and crop segmentation is becoming an increasingly integral part of precision farming that leverages the current computer vision and deep learning technologies. Research has been extensively carried out based on images captured with a camera from various platforms. Unmanned aerial vehicles (UAVs) and ground-based vehicles including agricultural robots are the two popular platforms for data collection in fields. They all contribute to site-specific weed management (SSWM) to maintain crop yield. Currently, the data from these two platforms is processed separately, though sharing the same semantic objects (weed and crop). In our paper, we have developed a deep convolutional network that enables to predict both field and aerial images from UAVs for weed segmentation and mapping with only field images provided in the training phase. The network learning process is visualized by feature maps at shallow and deep layers. The results show that the mean intersection of union (IOU) values of the segmentation for the crop (maize), weeds, and soil background in the developed model for the field dataset are 0.744, 0.577, 0.979, respectively, and the performance of aerial images from an UAV with the same model, the IOU values of the segmentation for the crop (maize), weeds and soil background are 0.596, 0.407, and 0.875, respectively. To estimate the effect on the use of plant protection agents, we quantify the relationship between herbicide spraying saving rate and grid size (spraying resolution) based on the predicted weed map. The spraying saving rate is up to 90% when the spraying resolution is at 1.78 x 1.78 cm2. The study shows that the developed deep convolutional neural network could be used to classify weeds from both field and aerial images and delivers satisfactory results.



### H4VDM: H.264 Video Device Matching
- **Arxiv ID**: http://arxiv.org/abs/2210.11549v3
- **DOI**: 10.1007/978-3-031-37742-6_24
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.11549v3)
- **Published**: 2022-10-20 19:31:23+00:00
- **Updated**: 2023-08-22 16:15:26+00:00
- **Authors**: Ziyue Xiang, Paolo Bestagini, Stefano Tubaro, Edward J. Delp
- **Comment**: None
- **Journal**: None
- **Summary**: Methods that can determine if two given video sequences are captured by the same device (e.g., mobile telephone or digital camera) can be used in many forensics tasks. In this paper we refer to this as "video device matching". In open-set video forensics scenarios it is easier to determine if two video sequences were captured with the same device than identifying the specific device. In this paper, we propose a technique for open-set video device matching. Given two H.264 compressed video sequences, our method can determine if they are captured by the same device, even if our method has never encountered the device in training. We denote our proposed technique as H.264 Video Device Matching (H4VDM). H4VDM uses H.264 compression information extracted from video sequences to make decisions. It is more robust against artifacts that alter camera sensor fingerprints, and it can be used to analyze relatively small fragments of the H.264 sequence. We trained and tested our method on a publicly available video forensics dataset consisting of 35 devices, where our proposed method demonstrated good performance.



### Learning Attention Propagation for Compositional Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.11557v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11557v1)
- **Published**: 2022-10-20 19:44:11+00:00
- **Updated**: 2022-10-20 19:44:11+00:00
- **Authors**: Muhammad Gul Zain Ali Khan, Muhammad Ferjad Naeem, Luc Van Gool, Alain Pagani, Didier Stricker, Muhammad Zeshan Afzal
- **Comment**: None
- **Journal**: None
- **Summary**: Compositional zero-shot learning aims to recognize unseen compositions of seen visual primitives of object classes and their states. While all primitives (states and objects) are observable during training in some combination, their complex interaction makes this task especially hard. For example, wet changes the visual appearance of a dog very differently from a bicycle. Furthermore, we argue that relationships between compositions go beyond shared states or objects. A cluttered office can contain a busy table; even though these compositions don't share a state or object, the presence of a busy table can guide the presence of a cluttered office. We propose a novel method called Compositional Attention Propagated Embedding (CAPE) as a solution. The key intuition to our method is that a rich dependency structure exists between compositions arising from complex interactions of primitives in addition to other dependencies between compositions. CAPE learns to identify this structure and propagates knowledge between them to learn class embedding for all seen and unseen compositions. In the challenging generalized compositional zero-shot setting, we show that our method outperforms previous baselines to set a new state-of-the-art on three publicly available benchmarks.



### Rethinking Learning Approaches for Long-Term Action Anticipation
- **Arxiv ID**: http://arxiv.org/abs/2210.11566v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11566v1)
- **Published**: 2022-10-20 20:07:30+00:00
- **Updated**: 2022-10-20 20:07:30+00:00
- **Authors**: Megha Nawhal, Akash Abdu Jyothi, Greg Mori
- **Comment**: Accepted at ECCV'22. Project page:
  http://meghanawhal.github.io/projects/anticipatr.html
- **Journal**: None
- **Summary**: Action anticipation involves predicting future actions having observed the initial portion of a video. Typically, the observed video is processed as a whole to obtain a video-level representation of the ongoing activity in the video, which is then used for future prediction. We introduce ANTICIPATR which performs long-term action anticipation leveraging segment-level representations learned using individual segments from different activities, in addition to a video-level representation. We propose a two-stage learning approach to train a novel transformer-based model that uses these two types of representations to directly predict a set of future action instances over any given anticipation duration. Results on Breakfast, 50Salads, Epic-Kitchens-55, and EGTEA Gaze+ datasets demonstrate the effectiveness of our approach.



### Deep Learning for Diagonal Earlobe Crease Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.11582v4
- **DOI**: 10.5220/0011644400003411
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11582v4)
- **Published**: 2022-10-20 20:51:48+00:00
- **Updated**: 2023-02-07 00:37:49+00:00
- **Authors**: Sara L. Almonacid-Uribe, Oliverio J. Santana, Daniel Hernández-Sosa, David Freire-Obregón
- **Comment**: Accepted at 12th International Conference on Pattern Recognition
  Applications (ICPRAM 2023)
- **Journal**: None
- **Summary**: An article published on Medical News Today in June 2022 presented a fundamental question in its title: Can an earlobe crease predict heart attacks? The author explained that end arteries supply the heart and ears. In other words, if they lose blood supply, no other arteries can take over, resulting in tissue damage. Consequently, some earlobes have a diagonal crease, line, or deep fold that resembles a wrinkle. In this paper, we take a step toward detecting this specific marker, commonly known as DELC or Frank's Sign. For this reason, we have made the first DELC dataset available to the public. In addition, we have investigated the performance of numerous cutting-edge backbones on annotated photos. Experimentally, we demonstrate that it is possible to solve this challenge by combining pre-trained encoders with a customized classifier to achieve 97.7% accuracy. Moreover, we have analyzed the backbone trade-off between performance and size, estimating MobileNet as the most promising encoder.



### XC: Exploring Quantitative Use Cases for Explanations in 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.11590v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11590v1)
- **Published**: 2022-10-20 21:02:55+00:00
- **Updated**: 2022-10-20 21:02:55+00:00
- **Authors**: Sunsheng Gu, Vahdat Abdelzad, Krzysztof Czarnecki
- **Comment**: Accepted at 1st Workshop on eXplainable AI approaches for debugging
  and diagnosis (XAI4Debugging@NeurIPS2021)
- **Journal**: None
- **Summary**: Explainable AI (XAI) methods are frequently applied to obtain qualitative insights about deep models' predictions. However, such insights need to be interpreted by a human observer to be useful. In this paper, we aim to use explanations directly to make decisions without human observers. We adopt two gradient-based explanation methods, Integrated Gradients (IG) and backprop, for the task of 3D object detection. Then, we propose a set of quantitative measures, named Explanation Concentration (XC) scores, that can be used for downstream tasks. These scores quantify the concentration of attributions within the boundaries of detected objects. We evaluate the effectiveness of XC scores via the task of distinguishing true positive (TP) and false positive (FP) detected objects in the KITTI and Waymo datasets. The results demonstrate an improvement of more than 100\% on both datasets compared to other heuristics such as random guesses and the number of LiDAR points in the bounding box, raising confidence in XC's potential for application in more use cases. Our results also indicate that computationally expensive XAI methods like IG may not be more valuable when used quantitatively compare to simpler methods.



### Photo-realistic 360 Head Avatars in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2210.11594v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11594v1)
- **Published**: 2022-10-20 21:05:19+00:00
- **Updated**: 2022-10-20 21:05:19+00:00
- **Authors**: Stanislaw Szymanowicz, Virginia Estellers, Tadas Baltrusaitis, Matthew Johnson
- **Comment**: ECCV 2022 Workshop on Computer Vision for Metaverse
- **Journal**: None
- **Summary**: Delivering immersive, 3D experiences for human communication requires a method to obtain 360 degree photo-realistic avatars of humans. To make these experiences accessible to all, only commodity hardware, like mobile phone cameras, should be necessary to capture the data needed for avatar creation. For avatars to be rendered realistically from any viewpoint, we require training images and camera poses from all angles. However, we cannot rely on there being trackable features in the foreground or background of all images for use in estimating poses, especially from the side or back of the head. To overcome this, we propose a novel landmark detector trained on synthetic data to estimate camera poses from 360 degree mobile phone videos of a human head for use in a multi-stage optimization process which creates a photo-realistic avatar. We perform validation experiments with synthetic data and showcase our method on 360 degree avatars trained from mobile phone videos.



### Slippage-robust Gaze Tracking for Near-eye Display
- **Arxiv ID**: http://arxiv.org/abs/2210.11637v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2210.11637v2)
- **Published**: 2022-10-20 23:47:56+00:00
- **Updated**: 2022-11-01 17:52:05+00:00
- **Authors**: Wei Zhang, Jiaxi Cao, Xiang Wang, Enqi Tian, Bin Li
- **Comment**: 7 pages, 8 figures
- **Journal**: None
- **Summary**: In recent years, head-mounted near-eye display devices have become the key hardware foundation for virtual reality and augmented reality. Thus head-mounted gaze tracking technology has received attention as an essential part of human-computer interaction. However, unavoidable slippage of head-mounted devices (HMD) often results higher gaze tracking errors and hinders the practical usage of HMD. To tackle this problem, we propose a slippage-robust gaze tracking for near-eye display method based on the aspheric eyeball model and accurately compute the eyeball optical axis and rotation center. We tested several methods on datasets with slippage and the experimental results show that the proposed method significantly outperforms the previous method (almost double the suboptimal method).



