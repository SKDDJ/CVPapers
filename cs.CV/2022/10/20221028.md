# Arxiv Papers in cs.CV on 2022-10-28
### Hyper-Connected Transformer Network for Multi-Modality PET-CT Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.15808v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15808v2)
- **Published**: 2022-10-28 00:03:43+00:00
- **Updated**: 2023-08-07 10:33:34+00:00
- **Authors**: Lei Bi, Michael Fulham, Shaoli Song, David Dagan Feng, Jinman Kim
- **Comment**: EMBC 2023
- **Journal**: None
- **Summary**: [18F]-Fluorodeoxyglucose (FDG) positron emission tomography - computed tomography (PET-CT) has become the imaging modality of choice for diagnosing many cancers. Co-learning complementary PET-CT imaging features is a fundamental requirement for automatic tumor segmentation and for developing computer aided cancer diagnosis systems. In this study, we propose a hyper-connected transformer (HCT) network that integrates a transformer network (TN) with a hyper connected fusion for multi-modality PET-CT images. The TN was leveraged for its ability to provide global dependencies in image feature learning, which was achieved by using image patch embeddings with a self-attention mechanism to capture image-wide contextual information. We extended the single-modality definition of TN with multiple TN based branches to separately extract image features. We also introduced a hyper connected fusion to fuse the contextual and complementary image features across multiple transformers in an iterative manner. Our results with two clinical datasets show that HCT achieved better performance in segmentation accuracy when compared to the existing methods.



### FUSSL: Fuzzy Uncertain Self Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.15818v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15818v1)
- **Published**: 2022-10-28 01:06:10+00:00
- **Updated**: 2022-10-28 01:06:10+00:00
- **Authors**: Salman Mohamadi, Gianfranco Doretto, Donald A. Adjeroh
- **Comment**: Accepted to WACV 2023
- **Journal**: None
- **Summary**: Self supervised learning (SSL) has become a very successful technique to harness the power of unlabeled data, with no annotation effort. A number of developed approaches are evolving with the goal of outperforming supervised alternatives, which have been relatively successful. One main issue in SSL is robustness of the approaches under different settings. In this paper, for the first time, we recognize the fundamental limits of SSL coming from the use of a single-supervisory signal. To address this limitation, we leverage the power of uncertainty representation to devise a robust and general standard hierarchical learning/training protocol for any SSL baseline, regardless of their assumptions and approaches. Essentially, using the information bottleneck principle, we decompose feature learning into a two-stage training procedure, each with a distinct supervision signal. This double supervision approach is captured in two key steps: 1) invariance enforcement to data augmentation, and 2) fuzzy pseudo labeling (both hard and soft annotation). This simple, yet, effective protocol which enables cross-class/cluster feature learning, is instantiated via an initial training of an ensemble of models through invariance enforcement to data augmentation as first training phase, and then assigning fuzzy labels to the original samples for the second training phase. We consider multiple alternative scenarios with double supervision and evaluate the effectiveness of our approach on recent baselines, covering four different SSL paradigms, including geometrical, contrastive, non-contrastive, and hard/soft whitening (redundancy reduction) baselines. Extensive experiments under multiple settings show that the proposed training protocol consistently improves the performance of the former baselines, independent of their respective underlying principles.



### Teacher-Student Architecture for Knowledge Learning: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2210.17332v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.17332v1)
- **Published**: 2022-10-28 01:11:52+00:00
- **Updated**: 2022-10-28 01:11:52+00:00
- **Authors**: Chengming Hu, Xuan Li, Dan Liu, Xi Chen, Ju Wang, Xue Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Although Deep Neural Networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs with voluminous parameters are hard to be deployed in a real-time system. To tackle this issue, Teacher-Student architectures were first utilized in knowledge distillation, where simple student networks can achieve comparable performance to deep teacher networks. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge learning objectives, including knowledge distillation, knowledge expansion, knowledge adaption, and multi-task learning. With the help of Teacher-Student architectures, current studies are able to achieve multiple knowledge-learning objectives through lightweight and effective student networks. Different from the existing knowledge distillation surveys, this survey detailedly discusses Teacher-Student architectures with multiple knowledge learning objectives. In addition, we systematically introduce the knowledge construction and optimization process during the knowledge learning and then analyze various Teacher-Student architectures and effective learning schemes that have been leveraged to learn representative and robust knowledge. This paper also summarizes the latest applications of Teacher-Student architectures based on different purposes (i.e., classification, recognition, and generation). Finally, the potential research directions of knowledge learning are investigated on the Teacher-Student architecture design, the quality of knowledge, and the theoretical studies of regression-based learning, respectively. With this comprehensive survey, both industry practitioners and the academic community can learn insightful guidelines about Teacher-Student architectures on multiple knowledge learning objectives.



### Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation
- **Arxiv ID**: http://arxiv.org/abs/2210.15858v3
- **DOI**: 10.1109/ISMAR55827.2022.00066
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.15858v3)
- **Published**: 2022-10-28 02:56:47+00:00
- **Updated**: 2023-03-06 05:00:57+00:00
- **Authors**: Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, Guofeng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a dense tracking and mapping system named Vox-Fusion, which seamlessly fuses neural implicit representations with traditional volumetric fusion methods. Our approach is inspired by the recently developed implicit mapping and positioning system and further extends the idea so that it can be freely applied to practical scenarios. Specifically, we leverage a voxel-based neural implicit surface representation to encode and optimize the scene inside each voxel. Furthermore, we adopt an octree-based structure to divide the scene and support dynamic expansion, enabling our system to track and map arbitrary scenes without knowing the environment like in previous works. Moreover, we proposed a high-performance multi-process framework to speed up the method, thus supporting some applications that require real-time performance. The evaluation results show that our methods can achieve better accuracy and completeness than previous methods. We also show that our Vox-Fusion can be used in augmented reality and virtual reality applications. Our source code is publicly available at https://github.com/zju3dv/Vox-Fusion.



### Fashion-Specific Attributes Interpretation via Dual Gaussian Visual-Semantic Embedding
- **Arxiv ID**: http://arxiv.org/abs/2210.17417v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.17417v2)
- **Published**: 2022-10-28 03:34:51+00:00
- **Updated**: 2022-11-07 22:43:29+00:00
- **Authors**: Ryotaro Shimizu, Masanari Kimura, Masayuki Goto
- **Comment**: None
- **Journal**: None
- **Summary**: Several techniques to map various types of components, such as words, attributes, and images, into the embedded space have been studied. Most of them estimate the embedded representation of target entity as a point in the projective space. Some models, such as Word2Gauss, assume a probability distribution behind the embedded representation, which enables the spread or variance of the meaning of embedded target components to be captured and considered in more detail. We examine the method of estimating embedded representations as probability distributions for the interpretation of fashion-specific abstract and difficult-to-understand terms. Terms, such as "casual," "adult-casual,'' "beauty-casual," and "formal," are extremely subjective and abstract and are difficult for both experts and non-experts to understand, which discourages users from trying new fashion. We propose an end-to-end model called dual Gaussian visual-semantic embedding, which maps images and attributes in the same projective space and enables the interpretation of the meaning of these terms by its broad applications. We demonstrate the effectiveness of the proposed method through multifaceted experiments involving image and attribute mapping, image retrieval and re-ordering techniques, and a detailed theoretical/analytical discussion of the distance measure included in the loss function.



### VLT: Vision-Language Transformer and Query Generation for Referring Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.15871v1
- **DOI**: 10.1109/TPAMI.2022.3217852
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15871v1)
- **Published**: 2022-10-28 03:36:07+00:00
- **Updated**: 2022-10-28 03:36:07+00:00
- **Authors**: Henghui Ding, Chang Liu, Suchen Wang, Xudong Jiang
- **Comment**: TPAMI
- **Journal**: None
- **Summary**: We propose a Vision-Language Transformer (VLT) framework for referring segmentation to facilitate deep interactions among multi-modal information and enhance the holistic understanding to vision-language features. There are different ways to understand the dynamic emphasis of a language expression, especially when interacting with the image. However, the learned queries in existing transformer works are fixed after training, which cannot cope with the randomness and huge diversity of the language expressions. To address this issue, we propose a Query Generation Module, which dynamically produces multiple sets of input-specific queries to represent the diverse comprehensions of language expression. To find the best among these diverse comprehensions, so as to generate a better mask, we propose a Query Balance Module to selectively fuse the corresponding responses of the set of queries. Furthermore, to enhance the model's ability in dealing with diverse language expressions, we consider inter-sample learning to explicitly endow the model with knowledge of understanding different language expressions to the same object. We introduce masked contrastive learning to narrow down the features of different expressions for the same target object while distinguishing the features of different objects. The proposed approach is lightweight and achieves new state-of-the-art referring segmentation results consistently on five datasets.



### Exploring Spatial-Temporal Features for Deepfake Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/2210.15872v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.15872v1)
- **Published**: 2022-10-28 03:38:49+00:00
- **Updated**: 2022-10-28 03:38:49+00:00
- **Authors**: Wu Haiwei, Zhou Jiantao, Zhang Shile, Tian Jinyu
- **Comment**: None
- **Journal**: None
- **Summary**: With the continuous research on Deepfake forensics, recent studies have attempted to provide the fine-grained localization of forgeries, in addition to the coarse classification at the video-level. However, the detection and localization performance of existing Deepfake forensic methods still have plenty of room for further improvement. In this work, we propose a Spatial-Temporal Deepfake Detection and Localization (ST-DDL) network that simultaneously explores spatial and temporal features for detecting and localizing forged regions. Specifically, we design a new Anchor-Mesh Motion (AMM) algorithm to extract temporal (motion) features by modeling the precise geometric movements of the facial micro-expression. Compared with traditional motion extraction methods (e.g., optical flow) designed to simulate large-moving objects, our proposed AMM could better capture the small-displacement facial features. The temporal features and the spatial features are then fused in a Fusion Attention (FA) module based on a Transformer architecture for the eventual Deepfake forensic tasks. The superiority of our ST-DDL network is verified by experimental comparisons with several state-of-the-art competitors, in terms of both video- and pixel-level detection and localization performance. Furthermore, to impel the future development of Deepfake forensics, we build a public forgery dataset consisting of 6000 videos, with many new features such as using widely-used commercial software (e.g., After Effects) for the production, providing online social networks transmitted versions, and splicing multi-source videos. The source code and dataset are available at https://github.com/HighwayWu/ST-DDL.



### Facial Action Unit Detection and Intensity Estimation from Self-supervised Representation
- **Arxiv ID**: http://arxiv.org/abs/2210.15878v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.15878v1)
- **Published**: 2022-10-28 03:55:09+00:00
- **Updated**: 2022-10-28 03:55:09+00:00
- **Authors**: Bowen Ma, Rudong An, Wei Zhang, Yu Ding, Zeng Zhao, Rongsheng Zhang, Tangjie Lv, Changjie Fan, Zhipeng Hu
- **Comment**: None
- **Journal**: None
- **Summary**: As a fine-grained and local expression behavior measurement, facial action unit (FAU) analysis (e.g., detection and intensity estimation) has been documented for its time-consuming, labor-intensive, and error-prone annotation. Thus a long-standing challenge of FAU analysis arises from the data scarcity of manual annotations, limiting the generalization ability of trained models to a large extent. Amounts of previous works have made efforts to alleviate this issue via semi/weakly supervised methods and extra auxiliary information. However, these methods still require domain knowledge and have not yet avoided the high dependency on data annotation. This paper introduces a robust facial representation model MAE-Face for AU analysis. Using masked autoencoding as the self-supervised pre-training approach, MAE-Face first learns a high-capacity model from a feasible collection of face images without additional data annotations. Then after being fine-tuned on AU datasets, MAE-Face exhibits convincing performance for both AU detection and AU intensity estimation, achieving a new state-of-the-art on nearly all the evaluation results. Further investigation shows that MAE-Face achieves decent performance even when fine-tuned on only 1\% of the AU training set, strongly proving its robustness and generalization performance.



### Complex Handwriting Trajectory Recovery: Evaluation Metrics and Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2210.15879v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.15879v1)
- **Published**: 2022-10-28 03:57:07+00:00
- **Updated**: 2022-10-28 03:57:07+00:00
- **Authors**: Zhounan Chen, Daihui Yang, Jinglin Liang, Xinwu Liu, Yuyi Wang, Zhenghua Peng, Shuangping Huang
- **Comment**: Accepted by Asian Conference on Computer Vision 2022(ACCV2022)
- **Journal**: None
- **Summary**: Many important tasks such as forensic signature verification, calligraphy synthesis, etc, rely on handwriting trajectory recovery of which, however, even an appropriate evaluation metric is still missing. Indeed, existing metrics only focus on the writing orders but overlook the fidelity of glyphs. Taking both facets into account, we come up with two new metrics, the adaptive intersection on union (AIoU) which eliminates the influence of various stroke widths, and the length-independent dynamic time warping (LDTW) which solves the trajectory-point alignment problem. After that, we then propose a novel handwriting trajectory recovery model named Parsing-and-tracing ENcoder-decoder Network (PEN-Net), in particular for characters with both complex glyph and long trajectory, which was believed very challenging. In the PEN-Net, a carefully designed double-stream parsing encoder parses the glyph structure, and a global tracing decoder overcomes the memory difficulty of long trajectory prediction. Our experiments demonstrate that the two new metrics AIoU and LDTW together can truly assess the quality of handwriting trajectory recovery and the proposed PEN-Net exhibits satisfactory performance in various complex-glyph languages including Chinese, Japanese and Indic.



### Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing
- **Arxiv ID**: http://arxiv.org/abs/2210.15889v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15889v3)
- **Published**: 2022-10-28 04:38:10+00:00
- **Updated**: 2022-12-07 23:56:12+00:00
- **Authors**: Wenguan Wang, Yi Yang, Fei Wu
- **Comment**: Ongoing project
- **Journal**: None
- **Summary**: Neural-symbolic computing (NeSy), which pursues the integration of the symbolic and statistical paradigms of cognition, has been an active research area of Artificial Intelligence (AI) for many years. As NeSy shows promise of reconciling the advantages of reasoning and interpretability of symbolic representation and robust learning in neural networks, it may serve as a catalyst for the next generation of AI. In the present paper, we provide a systematic overview of the important and recent developments of research on NeSy AI. Firstly, we introduce study history of this area, covering early work and foundations. We further discuss background concepts and identify key driving factors behind the development of NeSy. Afterward, we categorize recent landmark approaches along several main characteristics that underline this research paradigm, including neural-symbolic integration, knowledge representation, knowledge embedding, and functionality. Then, we briefly discuss the successful application of modern NeSy approaches in several domains. Finally, we identify the open problems together with potential future research directions. This survey is expected to help new researchers enter this rapidly-developing field and accelerate progress towards data-and knowledge-driven AI.



### Single-Image HDR Reconstruction by Multi-Exposure Generation
- **Arxiv ID**: http://arxiv.org/abs/2210.15897v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2210.15897v1)
- **Published**: 2022-10-28 05:12:56+00:00
- **Updated**: 2022-10-28 05:12:56+00:00
- **Authors**: Phuoc-Hieu Le, Quynh Le, Rang Nguyen, Binh-Son Hua
- **Comment**: WACV 2023 paper. 8 pages of content, 2 pages of references, 8 pages
  of supplementary material
- **Journal**: None
- **Summary**: High dynamic range (HDR) imaging is an indispensable technique in modern photography. Traditional methods focus on HDR reconstruction from multiple images, solving the core problems of image alignment, fusion, and tone mapping, yet having a perfect solution due to ghosting and other visual artifacts in the reconstruction. Recent attempts at single-image HDR reconstruction show a promising alternative: by learning to map pixel values to their irradiance using a neural network, one can bypass the align-and-merge pipeline completely yet still obtain a high-quality HDR image. In this work, we propose a weakly supervised learning method that inverts the physical image formation process for HDR reconstruction via learning to generate multiple exposures from a single image. Our neural network can invert the camera response to reconstruct pixel irradiance before synthesizing multiple exposures and hallucinating details in under- and over-exposed regions from a single input image. To train the network, we propose a representation loss, a reconstruction loss, and a perceptual loss applied on pairs of under- and over-exposure images and thus do not require HDR images for training. Our experiments show that our proposed model can effectively reconstruct HDR images. Our qualitative and quantitative results show that our method achieves state-of-the-art performance on the DrTMO dataset. Our code is available at https://github.com/VinAIResearch/single_image_hdr.



### Learning to Immunize Images for Tamper Localization and Self-Recovery
- **Arxiv ID**: http://arxiv.org/abs/2210.15902v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15902v1)
- **Published**: 2022-10-28 05:16:56+00:00
- **Updated**: 2022-10-28 05:16:56+00:00
- **Authors**: Qichao Ying, Hang Zhou, Zhenxing Qian, Sheng Li, Xinpeng Zhang
- **Comment**: Under Review. Extended version of our ACMMM 2021 paper
- **Journal**: None
- **Summary**: Digital images are vulnerable to nefarious tampering attacks such as content addition or removal that severely alter the original meaning. It is somehow like a person without protection that is open to various kinds of viruses. Image immunization (Imuge) is a technology of protecting the images by introducing trivial perturbation, so that the protected images are immune to the viruses in that the tampered contents can be auto-recovered. This paper presents Imuge+, an enhanced scheme for image immunization. By observing the invertible relationship between image immunization and the corresponding self-recovery, we employ an invertible neural network to jointly learn image immunization and recovery respectively in the forward and backward pass. We also introduce an efficient attack layer that involves both malicious tamper and benign image post-processing, where a novel distillation-based JPEG simulator is proposed for improved JPEG robustness. Our method achieves promising results in real-world tests where experiments show accurate tamper localization as well as high-fidelity content recovery. Additionally, we show superior performance on tamper localization compared to state-of-the-art schemes based on passive forensics.



### Self-Supervised Learning with Multi-View Rendering for 3D Point Cloud Analysis
- **Arxiv ID**: http://arxiv.org/abs/2210.15904v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2210.15904v1)
- **Published**: 2022-10-28 05:23:03+00:00
- **Updated**: 2022-10-28 05:23:03+00:00
- **Authors**: Bach Tran, Binh-Son Hua, Anh Tuan Tran, Minh Hoai
- **Comment**: ACCV 2022 paper. 14 pages of content, 4 pages of references, 6 pages
  of supplementary material
- **Journal**: None
- **Summary**: Recently, great progress has been made in 3D deep learning with the emergence of deep neural networks specifically designed for 3D point clouds. These networks are often trained from scratch or from pre-trained models learned purely from point cloud data. Inspired by the success of deep learning in the image domain, we devise a novel pre-training technique for better model initialization by utilizing the multi-view rendering of the 3D data. Our pre-training is self-supervised by a local pixel/point level correspondence loss computed from perspective projection and a global image/point cloud level loss based on knowledge distillation, thus effectively improving upon popular point cloud networks, including PointNet, DGCNN and SR-UNet. These improved models outperform existing state-of-the-art methods on various datasets and downstream tasks. We also analyze the benefits of synthetic and real data for pre-training, and observe that pre-training on synthetic data is also useful for high-level downstream tasks. Code and pre-trained models are available at https://github.com/VinAIResearch/selfsup_pcd.



### Long-HOT: A Modular Hierarchical Approach for Long-Horizon Object Transport
- **Arxiv ID**: http://arxiv.org/abs/2210.15908v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.15908v1)
- **Published**: 2022-10-28 05:30:49+00:00
- **Updated**: 2022-10-28 05:30:49+00:00
- **Authors**: Sriram Narayanan, Dinesh Jayaraman, Manmohan Chandraker
- **Comment**: None
- **Journal**: None
- **Summary**: We address key challenges in long-horizon embodied exploration and navigation by proposing a new object transport task and a novel modular framework for temporally extended navigation. Our first contribution is the design of a novel Long-HOT environment focused on deep exploration and long-horizon planning where the agent is required to efficiently find and pick up target objects to be carried and dropped at a goal location, with load constraints and optional access to a container if it finds one. Further, we propose a modular hierarchical transport policy (HTP) that builds a topological graph of the scene to perform exploration with the help of weighted frontiers. Our hierarchical approach uses a combination of motion planning algorithms to reach point goals within explored locations and object navigation policies for moving towards semantic targets at unknown locations. Experiments on both our proposed Habitat transport task and on MultiOn benchmarks show that our method significantly outperforms baselines and prior works. Further, we validate the effectiveness of our modular approach for long-horizon transport by demonstrating meaningful generalization to much harder transport scenes with training only on simpler versions of the task.



### Subsidiary Prototype Alignment for Universal Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2210.15909v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15909v1)
- **Published**: 2022-10-28 05:32:14+00:00
- **Updated**: 2022-10-28 05:32:14+00:00
- **Authors**: Jogendra Nath Kundu, Suvaansh Bhambri, Akshay Kulkarni, Hiran Sarkar, Varun Jampani, R. Venkatesh Babu
- **Comment**: NeurIPS 2022. Project page: https://sites.google.com/view/spa-unida
- **Journal**: None
- **Summary**: Universal Domain Adaptation (UniDA) deals with the problem of knowledge transfer between two datasets with domain-shift as well as category-shift. The goal is to categorize unlabeled target samples, either into one of the "known" categories or into a single "unknown" category. A major problem in UniDA is negative transfer, i.e. misalignment of "known" and "unknown" classes. To this end, we first uncover an intriguing tradeoff between negative-transfer-risk and domain-invariance exhibited at different layers of a deep network. It turns out we can strike a balance between these two metrics at a mid-level layer. Towards designing an effective framework based on this insight, we draw motivation from Bag-of-visual-Words (BoW). Word-prototypes in a BoW-like representation of a mid-level layer would represent lower-level visual primitives that are likely to be unaffected by the category-shift in the high-level features. We develop modifications that encourage learning of word-prototypes followed by word-histogram based classification. Following this, subsidiary prototype-space alignment (SPA) can be seen as a closed-set alignment problem, thereby avoiding negative transfer. We realize this with a novel word-histogram-related pretext task to enable closed-set SPA, operating in conjunction with goal task UniDA. We demonstrate the efficacy of our approach on top of existing UniDA techniques, yielding state-of-the-art performance across three standard UniDA and Open-Set DA object recognition benchmarks.



### GeoGCN: Geometric Dual-domain Graph Convolution Network for Point Cloud Denoising
- **Arxiv ID**: http://arxiv.org/abs/2210.15913v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15913v1)
- **Published**: 2022-10-28 05:48:57+00:00
- **Updated**: 2022-10-28 05:48:57+00:00
- **Authors**: Zhaowei Chen, Peng Li, Zeyong Wei, Honghua Chen, Haoran Xie, Mingqiang Wei, Fu Lee Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose GeoGCN, a novel geometric dual-domain graph convolution network for point cloud denoising (PCD). Beyond the traditional wisdom of PCD, to fully exploit the geometric information of point clouds, we define two kinds of surface normals, one is called Real Normal (RN), and the other is Virtual Normal (VN). RN preserves the local details of noisy point clouds while VN avoids the global shape shrinkage during denoising. GeoGCN is a new PCD paradigm that, 1) first regresses point positions by spatialbased GCN with the help of VNs, 2) then estimates initial RNs by performing Principal Component Analysis on the regressed points, and 3) finally regresses fine RNs by normalbased GCN. Unlike existing PCD methods, GeoGCN not only exploits two kinds of geometry expertise (i.e., RN and VN) but also benefits from training data. Experiments validate that GeoGCN outperforms SOTAs in terms of both noise-robustness and local-and-global feature preservation.



### Comparison of Stereo Matching Algorithms for the Development of Disparity Map
- **Arxiv ID**: http://arxiv.org/abs/2210.15926v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15926v1)
- **Published**: 2022-10-28 06:14:14+00:00
- **Updated**: 2022-10-28 06:14:14+00:00
- **Authors**: Hamid Fsian, Vahid Mohammadi, Pierre Gouton, Saeid Minaei
- **Comment**: None
- **Journal**: None
- **Summary**: Stereo Matching is one of the classical problems in computer vision for the extraction of 3D information but still controversial for accuracy and processing costs. The use of matching techniques and cost functions is crucial in the development of the disparity map. This paper presents a comparative study of six different stereo matching algorithms including Block Matching (BM), Block Matching with Dynamic Programming (BMDP), Belief Propagation (BP), Gradient Feature Matching (GF), Histogram of Oriented Gradient (HOG), and the proposed method. Also three cost functions namely Mean Squared Error (MSE), Sum of Absolute Differences (SAD), Normalized Cross-Correlation (NCC) were used and compared. The stereo images used in this study were from the Middlebury Stereo Datasets provided with perfect and imperfect calibrations. Results show that the selection of matching function is quite important and also depends on the images properties. Results showed that the BP algorithm in most cases provided better results getting accuracies over 95%.



### Being Comes from Not-being: Open-vocabulary Text-to-Motion Generation with Wordless Training
- **Arxiv ID**: http://arxiv.org/abs/2210.15929v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.15929v3)
- **Published**: 2022-10-28 06:20:55+00:00
- **Updated**: 2023-03-24 08:43:35+00:00
- **Authors**: Junfan Lin, Jianlong Chang, Lingbo Liu, Guanbin Li, Liang Lin, Qi Tian, Chang Wen Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Text-to-motion generation is an emerging and challenging problem, which aims to synthesize motion with the same semantics as the input text. However, due to the lack of diverse labeled training data, most approaches either limit to specific types of text annotations or require online optimizations to cater to the texts during inference at the cost of efficiency and stability. In this paper, we investigate offline open-vocabulary text-to-motion generation in a zero-shot learning manner that neither requires paired training data nor extra online optimization to adapt for unseen texts. Inspired by the prompt learning in NLP, we pretrain a motion generator that learns to reconstruct the full motion from the masked motion. During inference, instead of changing the motion generator, our method reformulates the input text into a masked motion as the prompt for the motion generator to ``reconstruct'' the motion. In constructing the prompt, the unmasked poses of the prompt are synthesized by a text-to-pose generator. To supervise the optimization of the text-to-pose generator, we propose the first text-pose alignment model for measuring the alignment between texts and 3D poses. And to prevent the pose generator from overfitting to limited training texts, we further propose a novel wordless training mechanism that optimizes the text-to-pose generator without any training texts. The comprehensive experimental results show that our method obtains a significant improvement against the baseline methods. The code is available at https://github.com/junfanlin/oohmg.



### PSFormer: Point Transformer for 3D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.15933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15933v1)
- **Published**: 2022-10-28 06:34:28+00:00
- **Updated**: 2022-10-28 06:34:28+00:00
- **Authors**: Baian Chen, Lipeng Gu, Xin Zhuang, Yiyang Shen, Weiming Wang, Mingqiang Wei
- **Comment**: None
- **Journal**: None
- **Summary**: We propose PSFormer, an effective point transformer model for 3D salient object detection. PSFormer is an encoder-decoder network that takes full advantage of transformers to model the contextual information in both multi-scale point- and scene-wise manners. In the encoder, we develop a Point Context Transformer (PCT) module to capture region contextual features at the point level; PCT contains two different transformers to excavate the relationship among points. In the decoder, we develop a Scene Context Transformer (SCT) module to learn context representations at the scene level; SCT contains both Upsampling-and-Transformer blocks and Multi-context Aggregation units to integrate the global semantic and multi-level features from the encoder into the global scene context. Experiments show clear improvements of PSFormer over its competitors and validate that PSFormer is more robust to challenging cases such as small objects, multiple objects, and objects with complex structures.



### Grafting Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2210.15943v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15943v2)
- **Published**: 2022-10-28 07:07:13+00:00
- **Updated**: 2023-04-03 14:16:14+00:00
- **Authors**: Jongwoo Park, Kumara Kahatapitiya, Donghyun Kim, Shivchander Sudalairaj, Quanfu Fan, Michael S. Ryoo
- **Comment**: None
- **Journal**: None
- **Summary**: Vision Transformers (ViTs) have recently become the state-of-the-art across many computer vision tasks. In contrast to convolutional networks (CNNs), ViTs enable global information sharing even within shallow layers of a network, i.e., among high-resolution features. However, this perk was later overlooked with the success of pyramid architectures such as Swin Transformer, which show better performance-complexity trade-offs. In this paper, we present a simple and efficient add-on component (termed GrafT) that considers global dependencies and multi-scale information throughout the network, in both high- and low-resolution features alike. It has the flexibility of branching out at arbitrary depths and shares most of the parameters and computations of the backbone. GrafT shows consistent gains over various well-known models which includes both hybrid and pure Transformer types, both homogeneous and pyramid structures, and various self-attention methods. In particular, it largely benefits mobile-size models by providing high-level semantics. On the ImageNet-1k dataset, GrafT delivers +3.9%, +1.4%, and +1.9% top-1 accuracy improvement to DeiT-T, Swin-T, and MobileViT-XXS, respectively. Our code and models will be made available.



### NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2210.15947v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.15947v2)
- **Published**: 2022-10-28 07:11:05+00:00
- **Updated**: 2023-02-18 07:00:29+00:00
- **Authors**: Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu, Andreas Geiger
- **Comment**: Project page: https://lsongx.github.io/projects/nerfplayer.html
- **Journal**: None
- **Summary**: Visually exploring in a real-world 4D spatiotemporal space freely in VR has been a long-term quest. The task is especially appealing when only a few or even single RGB cameras are used for capturing the dynamic scene. To this end, we present an efficient framework capable of fast reconstruction, compact modeling, and streamable rendering. First, we propose to decompose the 4D spatiotemporal space according to temporal characteristics. Points in the 4D space are associated with probabilities of belonging to three categories: static, deforming, and new areas. Each area is represented and regularized by a separate neural field. Second, we propose a hybrid representations based feature streaming scheme for efficiently modeling the neural fields. Our approach, coined NeRFPlayer, is evaluated on dynamic scenes captured by single hand-held cameras and multi-camera arrays, achieving comparable or superior rendering performance in terms of quality and speed comparable to recent state-of-the-art methods, achieving reconstruction in 10 seconds per frame and interactive rendering.



### Matching entropy based disparity estimation from light field
- **Arxiv ID**: http://arxiv.org/abs/2210.15948v2
- **DOI**: 10.1364/OE.479741
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15948v2)
- **Published**: 2022-10-28 07:12:00+00:00
- **Updated**: 2023-02-01 13:16:22+00:00
- **Authors**: Ligen Shi, Chang Liu, Di He, Xing Zhao, Jun Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: A major challenge for matching-based depth estimation is to prevent mismatches in occlusion and smooth regions. An effective matching window satisfying three characteristics: texture richness, disparity consistency and anti-occlusion should be able to prevent mismatches to some extent. According to these characteristics, we propose matching entropy in the spatial domain of light field to measure the amount of correct information in a matching window, which provides the criterion for matching window selection. Based on matching entropy regularization, we establish an optimization model for depth estimation with a matching cost fidelity term. To find the optimum, we propose a two-step adaptive matching algorithm. First, the region type is adaptively determined to identify occluding, occluded, smooth and textured regions. Then, the matching entropy criterion is used to adaptively select the size and shape of matching windows, as well as the visible viewpoints. The two-step process can reduce mismatches and redundant calculations by selecting effective matching windows. The experimental results on synthetic and real data show that the proposed method can effectively improve the accuracy of depth estimation in occlusion and smooth regions and has strong robustness for different noise levels. Therefore, high-precision depth estimation from 4D light field data is achieved.



### IB-U-Nets: Improving medical image segmentation tasks with 3D Inductive Biased kernels
- **Arxiv ID**: http://arxiv.org/abs/2210.15949v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.15949v1)
- **Published**: 2022-10-28 07:12:15+00:00
- **Updated**: 2022-10-28 07:12:15+00:00
- **Authors**: Shrajan Bhandary, Zahra Babaiee, Dejan Kostyszyn, Tobias Fechter, Constantinos Zamboglou, Anca-Ligia Grosu, Radu Grosu
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Despite the success of convolutional neural networks for 3D medical-image segmentation, the architectures currently used are still not robust enough to the protocols of different scanners, and the variety of image properties they produce. Moreover, access to large-scale datasets with annotated regions of interest is scarce, and obtaining good results is thus difficult. To overcome these challenges, we introduce IB-U-Nets, a novel architecture with inductive bias, inspired by the visual processing in vertebrates. With the 3D U-Net as the base, we add two 3D residual components to the second encoder blocks. They provide an inductive bias, helping U-Nets to segment anatomical structures from 3D images with increased robustness and accuracy. We compared IB-U-Nets with state-of-the-art 3D U-Nets on multiple modalities and organs, such as the prostate and spleen, using the same training and testing pipeline, including data processing, augmentation and cross-validation. Our results demonstrate the superior robustness and accuracy of IB-U-Nets, especially on small datasets, as is typically the case in medical-image analysis. IB-U-Nets source code and models are publicly available.



### LBF:Learnable Bilateral Filter For Point Cloud Denoising
- **Arxiv ID**: http://arxiv.org/abs/2210.15950v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15950v1)
- **Published**: 2022-10-28 07:13:15+00:00
- **Updated**: 2022-10-28 07:13:15+00:00
- **Authors**: Huajian Si, Zeyong Wei, Zhe Zhu, Honghua Chen, Dong Liang, Weiming Wang, Mingqiang Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Bilateral filter (BF) is a fast, lightweight and effective tool for image denoising and well extended to point cloud denoising. However, it often involves continual yet manual parameter adjustment; this inconvenience discounts the efficiency and user experience to obtain satisfied denoising results. We propose LBF, an end-to-end learnable bilateral filtering network for point cloud denoising; to our knowledge, this is the first time. Unlike the conventional BF and its variants that receive the same parameters for a whole point cloud, LBF learns adaptive parameters for each point according its geometric characteristic (e.g., corner, edge, plane), avoiding remnant noise, wrongly-removed geometric details, and distorted shapes. Besides the learnable paradigm of BF, we have two cores to facilitate LBF. First, different from the local BF, LBF possesses a global-scale feature perception ability by exploiting multi-scale patches of each point. Second, LBF formulates a geometry-aware bi-directional projection loss, leading the denoising results to being faithful to their underlying surfaces. Users can apply our LBF without any laborious parameter tuning to achieve the optimal denoising results. Experiments show clear improvements of LBF over its competitors on both synthetic and real-scanned datasets.



### Contextual Learning in Fourier Complex Field for VHR Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2210.15972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15972v1)
- **Published**: 2022-10-28 08:13:33+00:00
- **Updated**: 2022-10-28 08:13:33+00:00
- **Authors**: Yan Zhang, Xiyuan Gao, Qingyan Duan, Jiaxu Leng, Xiao Pu, Xinbo Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Very high-resolution (VHR) remote sensing (RS) image classification is the fundamental task for RS image analysis and understanding. Recently, transformer-based models demonstrated outstanding potential for learning high-order contextual relationships from natural images with general resolution (224x224 pixels) and achieved remarkable results on general image classification tasks. However, the complexity of the naive transformer grows quadratically with the increase in image size, which prevents transformer-based models from VHR RS image (500x500 pixels) classification and other computationally expensive downstream tasks. To this end, we propose to decompose the expensive self-attention (SA) into real and imaginary parts via discrete Fourier transform (DFT) and therefore propose an efficient complex self-attention (CSA) mechanism. Benefiting from the conjugated symmetric property of DFT, CSA is capable to model the high-order contextual information with less than half computations of naive SA. To overcome the gradient explosion in Fourier complex field, we replace the Softmax function with the carefully designed Logmax function to normalize the attention map of CSA and stabilize the gradient propagation. By stacking various layers of CSA blocks, we propose the Fourier Complex Transformer (FCT) model to learn global contextual information from VHR aerial images following the hierarchical manners. Universal experiments conducted on commonly used RS classification data sets demonstrate the effectiveness and efficiency of FCT, especially on very high-resolution RS images.



### FedVMR: A New Federated Learning method for Video Moment Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2210.15977v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.15977v1)
- **Published**: 2022-10-28 08:17:57+00:00
- **Updated**: 2022-10-28 08:17:57+00:00
- **Authors**: Yan Wang, Xin Luo, Zhen-Duo Chen, Peng-Fei Zhang, Meng Liu, Xin-Shun Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the great success achieved, existing video moment retrieval (VMR) methods are developed under the assumption that data are centralizedly stored. However, in real-world applications, due to the inherent nature of data generation and privacy concerns, data are often distributed on different silos, bringing huge challenges to effective large-scale training. In this work, we try to overcome above limitation by leveraging the recent success of federated learning. As the first that is explored in VMR field, the new task is defined as video moment retrieval with distributed data. Then, a novel federated learning method named FedVMR is proposed to facilitate large-scale and secure training of VMR models in decentralized environment. Experiments on benchmark datasets demonstrate its effectiveness. This work is the very first attempt to enable safe and efficient VMR training in decentralized scene, which is hoped to pave the way for further study in the related research field.



### Differentially Private CutMix for Split Learning with Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2210.15986v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.15986v1)
- **Published**: 2022-10-28 08:33:29+00:00
- **Updated**: 2022-10-28 08:33:29+00:00
- **Authors**: Seungeun Oh, Jihong Park, Sihun Baek, Hyelin Nam, Praneeth Vepakomma, Ramesh Raskar, Mehdi Bennis, Seong-Lyun Kim
- **Comment**: to be presented at the 36nd Conference on Neural Information
  Processing Systems (NeurIPS 2022), First Workshop on Interpolation
  Regularizers and Beyond (INTERPOLATE), New Orleans, United States
- **Journal**: None
- **Summary**: Recently, vision transformer (ViT) has started to outpace the conventional CNN in computer vision tasks. Considering privacy-preserving distributed learning with ViT, federated learning (FL) communicates models, which becomes ill-suited due to ViT' s large model size and computing costs. Split learning (SL) detours this by communicating smashed data at a cut-layer, yet suffers from data privacy leakage and large communication costs caused by high similarity between ViT' s smashed data and input data. Motivated by this problem, we propose DP-CutMixSL, a differentially private (DP) SL framework by developing DP patch-level randomized CutMix (DP-CutMix), a novel privacy-preserving inter-client interpolation scheme that replaces randomly selected patches in smashed data. By experiment, we show that DP-CutMixSL not only boosts privacy guarantees and communication efficiency, but also achieves higher accuracy than its Vanilla SL counterpart. Theoretically, we analyze that DP-CutMix amplifies R\'enyi DP (RDP), which is upper-bounded by its Vanilla Mixup counterpart.



### Towards Few-Shot Open-Set Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.15996v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.15996v2)
- **Published**: 2022-10-28 09:02:32+00:00
- **Updated**: 2022-12-09 12:32:47+00:00
- **Authors**: Binyi Su, Hua Zhang, Jingzhi Li, Zhong Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Open-set object detection (OSOD) aims to detect the known categories and identify unknown objects in a dynamic world, which has achieved significant attentions. However, previous approaches only consider this problem in data-abundant conditions, while neglecting the few-shot scenes. In this paper, we seek a solution for the few-shot open-set object detection (FSOSOD), which aims to quickly train a detector based on few samples while detecting all known classes and identifying unknown classes. The main challenge for this task is that few training samples induce the model to overfit on the known classes, resulting in a poor open-set performance. We propose a new FSOSOD algorithm to tackle this issue, named Few-shOt Open-set Detector (FOOD), which contains a novel class weight sparsification classifier (CWSC) and a novel unknown decoupling learner (UDL). To prevent over-fitting, CWSC randomly sparses parts of the normalized weights for the logit prediction of all classes, and then decreases the co-adaptability between the class and its neighbors. Alongside, UDL decouples training the unknown class and enables the model to form a compact unknown decision boundary. Thus, the unknown objects can be identified with a confidence probability without any pseudo-unknown samples for training. We compare our method with several state-of-the-art OSOD methods in few-shot scenes and observe that our method improves the recall of unknown classes by 5%-9% across all shots in VOC-COCO dataset setting.



### Benchmarking performance of object detection under image distortions in an uncontrolled environment
- **Arxiv ID**: http://arxiv.org/abs/2210.15999v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/2210.15999v1)
- **Published**: 2022-10-28 09:06:52+00:00
- **Updated**: 2022-10-28 09:06:52+00:00
- **Authors**: Ayman Beghdadi, Malik Mallem, Lotfi Beji
- **Comment**: None
- **Journal**: None
- **Summary**: The robustness of object detection algorithms plays a prominent role in real-world applications, especially in uncontrolled environments due to distortions during image acquisition. It has been proven that the performance of object detection methods suffers from in-capture distortions. In this study, we present a performance evaluation framework for the state-of-the-art object detection methods using a dedicated dataset containing images with various distortions at different levels of severity. Furthermore, we propose an original strategy of image distortion generation applied to the MS-COCO dataset that combines some local and global distortions to reach much better performances. We have shown that training using the proposed dataset improves the robustness of object detection by 31.5\%. Finally, we provide a custom dataset including natural images distorted from MS-COCO to perform a more reliable evaluation of the robustness against common distortions. The database and the generation source codes of the different distortions are made publicly available



### Thermal Infrared Image Inpainting via Edge-Aware Guidance
- **Arxiv ID**: http://arxiv.org/abs/2210.16000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.16000v1)
- **Published**: 2022-10-28 09:06:54+00:00
- **Updated**: 2022-10-28 09:06:54+00:00
- **Authors**: Zeyu Wang, Haibin Shen, Changyou Men, Quan Sun, Kejie Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Image inpainting has achieved fundamental advances with deep learning. However, almost all existing inpainting methods aim to process natural images, while few target Thermal Infrared (TIR) images, which have widespread applications. When applied to TIR images, conventional inpainting methods usually generate distorted or blurry content. In this paper, we propose a novel task -- Thermal Infrared Image Inpainting, which aims to reconstruct missing regions of TIR images. Crucially, we propose a novel deep-learning-based model TIR-Fill. We adopt the edge generator to complete the canny edges of broken TIR images. The completed edges are projected to the normalization weights and biases to enhance edge awareness of the model. In addition, a refinement network based on gated convolution is employed to improve TIR image consistency. The experiments demonstrate that our method outperforms state-of-the-art image inpainting approaches on FLIR thermal dataset.



### Addressing Bias in Face Detectors using Decentralised Data collection with incentives
- **Arxiv ID**: http://arxiv.org/abs/2210.16024v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16024v1)
- **Published**: 2022-10-28 09:54:40+00:00
- **Updated**: 2022-10-28 09:54:40+00:00
- **Authors**: M. R. Ahan, Robin Lehmann, Richard Blythman
- **Comment**: 8 pages. Accepted at NeurIPS 2022 Workshop on Decentralization &
  Trustworthy Machine Learning in Web3
- **Journal**: Advances in Neural Information Processing Systems (2022)
- **Summary**: Recent developments in machine learning have shown that successful models do not rely only on huge amounts of data but the right kind of data. We show in this paper how this data-centric approach can be facilitated in a decentralized manner to enable efficient data collection for algorithms. Face detectors are a class of models that suffer heavily from bias issues as they have to work on a large variety of different data. We also propose a face detection and anonymization approach using a hybrid MultiTask Cascaded CNN with FaceNet Embeddings to benchmark multiple datasets to describe and evaluate the bias in the models towards different ethnicities, gender, and age groups along with ways to enrich fairness in a decentralized system of data labeling, correction, and verification by users to create a robust pipeline for model retraining.



### UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance
- **Arxiv ID**: http://arxiv.org/abs/2210.16031v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2210.16031v3)
- **Published**: 2022-10-28 10:07:25+00:00
- **Updated**: 2022-11-03 01:57:47+00:00
- **Authors**: Wei Li, Xue Xu, Xinyan Xiao, Jiachen Liu, Hu Yang, Guohao Li, Zhanpeng Wang, Zhifan Feng, Qiaoqiao She, Yajuan Lyu, Hua Wu
- **Comment**: First Version, 16 pages
- **Journal**: None
- **Summary**: Diffusion generative models have recently greatly improved the power of text-conditioned image generation. Existing image generation models mainly include text conditional diffusion model and cross-modal guided diffusion model, which are good at small scene image generation and complex scene image generation respectively. In this work, we propose a simple yet effective approach, namely UPainting, to unify simple and complex scene image generation, as shown in Figure 1. Based on architecture improvements and diverse guidance schedules, UPainting effectively integrates cross-modal guidance from a pretrained image-text matching model into a text conditional diffusion model that utilizes a pretrained Transformer language model as the text encoder. Our key findings is that combining the power of large-scale Transformer language model in understanding language and image-text matching model in capturing cross-modal semantics and style, is effective to improve sample fidelity and image-text alignment of image generation. In this way, UPainting has a more general image generation capability, which can generate images of both simple and complex scenes more effectively. To comprehensively compare text-to-image models, we further create a more general benchmark, UniBench, with well-written Chinese and English prompts in both simple and complex scenes. We compare UPainting with recent models and find that UPainting greatly outperforms other models in terms of caption similarity and image fidelity in both simple and complex scenes. UPainting project page \url{https://upainting.github.io/}.



### A Survey on Causal Representation Learning and Future Work for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2210.16034v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16034v1)
- **Published**: 2022-10-28 10:15:36+00:00
- **Updated**: 2022-10-28 10:15:36+00:00
- **Authors**: Changjie Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Statistical machine learning algorithms have achieved state-of-the-art results on benchmark datasets, outperforming humans in many tasks. However, the out-of-distribution data and confounder, which have an unpredictable causal relationship, significantly degrade the performance of the existing models. Causal Representation Learning (CRL) has recently been a promising direction to address the causal relationship problem in vision understanding. This survey presents recent advances in CRL in vision. Firstly, we introduce the basic concept of causal inference. Secondly, we analyze the CRL theoretical work, especially in invariant risk minimization, and the practical work in feature understanding and transfer learning. Finally, we propose a future research direction in medical image analysis and CRL general theory.



### Deep Learning-Based Anomaly Detection in Synthetic Aperture Radar Imaging
- **Arxiv ID**: http://arxiv.org/abs/2210.16038v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2210.16038v1)
- **Published**: 2022-10-28 10:22:29+00:00
- **Updated**: 2022-10-28 10:22:29+00:00
- **Authors**: Max Muzeau, Chengfang Ren, Sébastien Angelliaume, Mihai Datcu, Jean-Philippe Ovarlez
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: In this paper, we proposed to investigate unsupervised anomaly detection in Synthetic Aperture Radar (SAR) images. Our approach considers anomalies as abnormal patterns that deviate from their surroundings but without any prior knowledge of their characteristics. In the literature, most model-based algorithms face three main issues. First, the speckle noise corrupts the image and potentially leads to numerous false detections. Second, statistical approaches may exhibit deficiencies in modeling spatial correlation in SAR images. Finally, neural networks based on supervised learning approaches are not recommended due to the lack of annotated SAR data, notably for the class of abnormal patterns. Our proposed method aims to address these issues through a self-supervised algorithm. The speckle is first removed through the deep learning SAR2SAR algorithm. Then, an adversarial autoencoder is trained to reconstruct an anomaly-free SAR image. Finally, a change detection processing step is applied between the input and the output to detect anomalies. Experiments are performed to show the advantages of our method compared to the conventional Reed-Xiaoli algorithm, highlighting the importance of an efficient despeckling pre-processing step.



### Rawgment: Noise-Accounted RAW Augmentation Enables Recognition in a Wide Variety of Environments
- **Arxiv ID**: http://arxiv.org/abs/2210.16046v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16046v2)
- **Published**: 2022-10-28 10:33:45+00:00
- **Updated**: 2023-03-27 06:17:13+00:00
- **Authors**: Masakazu Yoshimura, Junji Otsuka, Atsushi Irie, Takeshi Ohashi
- **Comment**: Accepted to CVPR2023
- **Journal**: None
- **Summary**: Image recognition models that work in challenging environments (e.g., extremely dark, blurry, or high dynamic range conditions) must be useful. However, creating training datasets for such environments is expensive and hard due to the difficulties of data collection and annotation. It is desirable if we could get a robust model without the need for hard-to-obtain datasets. One simple approach is to apply data augmentation such as color jitter and blur to standard RGB (sRGB) images in simple scenes. Unfortunately, this approach struggles to yield realistic images in terms of pixel intensity and noise distribution due to not considering the non-linearity of Image Signal Processors (ISPs) and noise characteristics of image sensors. Instead, we propose a noise-accounted RAW image augmentation method. In essence, color jitter and blur augmentation are applied to a RAW image before applying non-linear ISP, resulting in realistic intensity. Furthermore, we introduce a noise amount alignment method that calibrates the domain gap in the noise property caused by the augmentation. We show that our proposed noise-accounted RAW augmentation method doubles the image recognition accuracy in challenging environments only with simple training data.



### Automated analysis of diabetic retinopathy using vessel segmentation maps as inductive bias
- **Arxiv ID**: http://arxiv.org/abs/2210.16053v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16053v3)
- **Published**: 2022-10-28 10:58:53+00:00
- **Updated**: 2022-12-21 14:20:40+00:00
- **Authors**: Linus Kreitner, Ivan Ezhov, Daniel Rueckert, Johannes C. Paetzold, Martin J. Menten
- **Comment**: Submission for MICCAI 2022 Diabetic Retinopathy Analysis Challenge
  (DRAC) Proceedings, DOI: 10.5281/zenodo.6362349
- **Journal**: None
- **Summary**: Recent studies suggest that early stages of diabetic retinopathy (DR) can be diagnosed by monitoring vascular changes in the deep vascular complex. In this work, we investigate a novel method for automated DR grading based on optical coherence tomography angiography (OCTA) images. Our work combines OCTA scans with their vessel segmentations, which then serve as inputs to task specific networks for lesion segmentation, image quality assessment and DR grading. For this, we generate synthetic OCTA images to train a segmentation network that can be directly applied on real OCTA data. We test our approach on MICCAI 2022's DR analysis challenge (DRAC). In our experiments, the proposed method performs equally well as the baseline model.



### MagicMix: Semantic Mixing with Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2210.16056v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16056v1)
- **Published**: 2022-10-28 11:07:48+00:00
- **Updated**: 2022-10-28 11:07:48+00:00
- **Authors**: Jun Hao Liew, Hanshu Yan, Daquan Zhou, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Have you ever imagined what a corgi-alike coffee machine or a tiger-alike rabbit would look like? In this work, we attempt to answer these questions by exploring a new task called semantic mixing, aiming at blending two different semantics to create a new concept (e.g., corgi + coffee machine -- > corgi-alike coffee machine). Unlike style transfer, where an image is stylized according to the reference style without changing the image content, semantic blending mixes two different concepts in a semantic manner to synthesize a novel concept while preserving the spatial layout and geometry. To this end, we present MagicMix, a simple yet effective solution based on pre-trained text-conditioned diffusion models. Motivated by the progressive generation property of diffusion models where layout/shape emerges at early denoising steps while semantically meaningful details appear at later steps during the denoising process, our method first obtains a coarse layout (either by corrupting an image or denoising from a pure Gaussian noise given a text prompt), followed by injection of conditional prompt for semantic mixing. Our method does not require any spatial mask or re-training, yet is able to synthesize novel objects with high fidelity. To improve the mixing quality, we further devise two simple strategies to provide better control and flexibility over the synthesized content. With our method, we present our results over diverse downstream applications, including semantic style transfer, novel object synthesis, breed mixing, and concept removal, demonstrating the flexibility of our method. More results can be found on the project page https://magicmix.github.io



### Semi-UFormer: Semi-supervised Uncertainty-aware Transformer for Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/2210.16057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16057v1)
- **Published**: 2022-10-28 11:08:41+00:00
- **Updated**: 2022-10-28 11:08:41+00:00
- **Authors**: Ming Tong, Yongzhen Wang, Peng Cui, Xuefeng Yan, Mingqiang Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Image dehazing is fundamental yet not well-solved in computer vision. Most cutting-edge models are trained in synthetic data, leading to the poor performance on real-world hazy scenarios. Besides, they commonly give deterministic dehazed images while neglecting to mine their uncertainty. To bridge the domain gap and enhance the dehazing performance, we propose a novel semi-supervised uncertainty-aware transformer network, called Semi-UFormer. Semi-UFormer can well leverage both the real-world hazy images and their uncertainty guidance information. Specifically, Semi-UFormer builds itself on the knowledge distillation framework. Such teacher-student networks effectively absorb real-world haze information for quality dehazing. Furthermore, an uncertainty estimation block is introduced into the model to estimate the pixel uncertainty representations, which is then used as a guidance signal to help the student network produce haze-free images more accurately. Extensive experiments demonstrate that Semi-UFormer generalizes well from synthetic to real-world images.



### Improving Chest X-Ray Classification by RNN-based Patient Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2210.16074v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16074v1)
- **Published**: 2022-10-28 11:47:15+00:00
- **Updated**: 2022-10-28 11:47:15+00:00
- **Authors**: David Biesner, Helen Schneider, Benjamin Wulff, Ulrike Attenberger, Rafet Sifa
- **Comment**: To be published in proceedings of IEEE International Conference on
  Machine Learning Applications IEEE ICMLA 2022
- **Journal**: None
- **Summary**: Chest X-Ray imaging is one of the most common radiological tools for detection of various pathologies related to the chest area and lung function. In a clinical setting, automated assessment of chest radiographs has the potential of assisting physicians in their decision making process and optimize clinical workflows, for example by prioritizing emergency patients.   Most work analyzing the potential of machine learning models to classify chest X-ray images focuses on vision methods processing and predicting pathologies for one image at a time. However, many patients undergo such a procedure multiple times during course of a treatment or during a single hospital stay. The patient history, that is previous images and especially the corresponding diagnosis contain useful information that can aid a classification system in its prediction.   In this study, we analyze how information about diagnosis can improve CNN-based image classification models by constructing a novel dataset from the well studied CheXpert dataset of chest X-rays. We show that a model trained on additional patient history information outperforms a model trained without the information by a significant margin.   We provide code to replicate the dataset creation and model training.



### Adaptive Mask-based Pyramid Network for Realistic Bokeh Rendering
- **Arxiv ID**: http://arxiv.org/abs/2210.16078v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16078v1)
- **Published**: 2022-10-28 11:57:53+00:00
- **Updated**: 2022-10-28 11:57:53+00:00
- **Authors**: Konstantinos Georgiadis, Albert Saà-Garriga, Mehmet Kerim Yucel, Anastasios Drosou, Bruno Manganelli
- **Comment**: ECCV 2022 Advances in Image Manipulation Workshop. See the workshop
  website for posters and recordings
- **Journal**: None
- **Summary**: Bokeh effect highlights an object (or any part of the image) while blurring the rest of the image, and creates a visually pleasant artistic effect. Due to the sensor-based limitations on mobile devices, machine learning (ML) based bokeh rendering has gained attention as a reliable alternative. In this paper, we focus on several improvements in ML-based bokeh rendering; i) on-device performance with high-resolution images, ii) ability to guide bokeh generation with user-editable masks and iii) ability to produce varying blur strength. To this end, we propose Adaptive Mask-based Pyramid Network (AMPN), which is formed of a Mask-Guided Bokeh Generator (MGBG) block and a Laplacian Pyramid Refinement (LPR) block. MGBG consists of two lightweight networks stacked to each other to generate the bokeh effect, and LPR refines and upsamples the output of MGBG to produce the high-resolution bokeh image. We achieve i) via our lightweight, mobile-friendly design choices, ii) via the stacked-network design of MGBG and the weakly-supervised mask prediction scheme and iii) via manually or automatically editing the intensity values of the mask that guide the bokeh generation. In addition to these features, our results show that AMPN produces competitive or better results compared to existing methods on the EBB! dataset, while being faster and smaller than the alternatives.



### Object Segmentation of Cluttered Airborne LiDAR Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2210.16081v2
- **DOI**: 10.3233/FAIA220347
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.16081v2)
- **Published**: 2022-10-28 11:58:22+00:00
- **Updated**: 2023-02-06 15:42:14+00:00
- **Authors**: Mariona Caros, Ariadna Just, Santi Segui, Jordi Vitria
- **Comment**: proceedings of the 24th International Conference of the Catalan
  Association for Artificial Intelligence (CCIA 2022)
- **Journal**: Artificial Intelligence Research and Development. 356 (2022)
  259-268
- **Summary**: Airborne topographic LiDAR is an active remote sensing technology that emits near-infrared light to map objects on the Earth's surface. Derived products of LiDAR are suitable to service a wide range of applications because of their rich three-dimensional spatial information and their capacity to obtain multiple returns. However, processing point cloud data still requires a significant effort in manual editing. Certain human-made objects are difficult to detect because of their variety of shapes, irregularly-distributed point clouds, and low number of class samples. In this work, we propose an efficient end-to-end deep learning framework to automatize the detection and segmentation of objects defined by an arbitrary number of LiDAR points surrounded by clutter. Our method is based on a light version of PointNet that achieves good performance on both object recognition and segmentation tasks. The results are tested against manually delineated power transmission towers and show promising accuracy.



### ROMA: Run-Time Object Detection To Maximize Real-Time Accuracy
- **Arxiv ID**: http://arxiv.org/abs/2210.16083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16083v1)
- **Published**: 2022-10-28 12:06:29+00:00
- **Updated**: 2022-10-28 12:06:29+00:00
- **Authors**: JunKyu Lee, Blesson Varghese, Hans Vandierendonck
- **Comment**: Accepted at the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV) 2023
- **Journal**: None
- **Summary**: This paper analyzes the effects of dynamically varying video contents and detection latency on the real-time detection accuracy of a detector and proposes a new run-time accuracy variation model, ROMA, based on the findings from the analysis. ROMA is designed to select an optimal detector out of a set of detectors in real time without label information to maximize real-time object detection accuracy. ROMA utilizing four YOLOv4 detectors on an NVIDIA Jetson Nano shows real-time accuracy improvements by 4 to 37% for a scenario of dynamically varying video contents and detection latency consisting of MOT17Det and MOT20Det datasets, compared to individual YOLOv4 detectors and two state-of-the-art runtime techniques.



### A CNN-LSTM Combination Network for Cataract Detection using Eye Fundus Images
- **Arxiv ID**: http://arxiv.org/abs/2210.16093v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16093v1)
- **Published**: 2022-10-28 12:35:15+00:00
- **Updated**: 2022-10-28 12:35:15+00:00
- **Authors**: Dishant Padalia, Abhishek Mazumdar, Bharati Singh
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: According to multiple authoritative authorities, including the World Health Organization, vision-related impairments and disorders are becoming a significant issue. According to a recent report, one of the leading causes of irreversible blindness in persons over the age of 50 is delayed cataract treatment. A cataract is a cloudy spot in the eye's lens that causes visual loss. Cataracts often develop slowly and consequently result in difficulty in driving, reading, and even recognizing faces. This necessitates the development of rapid and dependable diagnosis and treatment solutions for ocular illnesses. Previously, such visual illness diagnosis were done manually, which was time-consuming and prone to human mistake. However, as technology advances, automated, computer-based methods that decrease both time and human labor while producing trustworthy results are now accessible. In this study, we developed a CNN-LSTM-based model architecture with the goal of creating a low-cost diagnostic system that can classify normal and cataractous cases of ocular disease from fundus images. The proposed model was trained on the publicly available ODIR dataset, which included fundus images of patients' left and right eyes. The suggested architecture outperformed previous systems with a state-of-the-art 97.53% accuracy.



### cRedAnno+: Annotation Exploitation in Self-Explanatory Lung Nodule Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2210.16097v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16097v2)
- **Published**: 2022-10-28 12:44:31+00:00
- **Updated**: 2022-11-07 21:38:58+00:00
- **Authors**: Jiahao Lu, Chong Yin, Kenny Erleben, Michael Bachmann Nielsen, Sune Darkner
- **Comment**: 5 pages, 5 figures, 2 tables. arXiv admin note: text overlap with
  arXiv:2206.13608
- **Journal**: None
- **Summary**: Recently, attempts have been made to reduce annotation requirements in feature-based self-explanatory models for lung nodule diagnosis. As a representative, cRedAnno achieves competitive performance with considerably reduced annotation needs by introducing self-supervised contrastive learning to do unsupervised feature extraction. However, it exhibits unstable performance under scarce annotation conditions. To improve the accuracy and robustness of cRedAnno, we propose an annotation exploitation mechanism by conducting semi-supervised active learning with sparse seeding and training quenching in the learned semantically meaningful reasoning space to jointly utilise the extracted features, annotations, and unlabelled data. The proposed approach achieves comparable or even higher malignancy prediction accuracy with 10x fewer annotations, meanwhile showing better robustness and nodule attribute prediction accuracy under the condition of 1% annotations. Our complete code is open-source available: https://github.com/diku-dk/credanno.



### Impact of PolSAR pre-processing and balancing methods on complex-valued neural networks segmentation tasks
- **Arxiv ID**: http://arxiv.org/abs/2210.17419v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.17419v1)
- **Published**: 2022-10-28 12:49:43+00:00
- **Updated**: 2022-10-28 12:49:43+00:00
- **Authors**: José Agustin Barrachina, Chengfang Ren, Christèle Morisseau, Gilles Vieillard, Jean-Philippe Ovarlez
- **Comment**: 9 pages, 4 figures, 5 tables
- **Journal**: None
- **Summary**: In this paper, we investigated the semantic segmentation of Polarimetric Synthetic Aperture Radar (PolSAR) using Complex-Valued Neural Network (CVNN). Although the coherency matrix is more widely used as the input of CVNN, the Pauli vector has recently been shown to be a valid alternative. We exhaustively compare both methods for six model architectures, three complex-valued, and their respective real-equivalent models. We are comparing, therefore, not only the input representation impact but also the complex- against the real-valued models. We then argue that the dataset splitting produces a high correlation between training and validation sets, saturating the task and thus achieving very high performance. We, therefore, use a different data pre-processing technique designed to reduce this effect and reproduce the results with the same configurations as before (input representation and model architectures). After seeing that the performance per class is highly different according to class occurrences, we propose two methods for reducing this gap and performing the results for all input representations, models, and dataset pre-processing.



### Improving the Transferability of Adversarial Attacks on Face Recognition with Beneficial Perturbation Feature Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.16117v4
- **DOI**: 10.1109/TCSS.2023.3291565
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16117v4)
- **Published**: 2022-10-28 13:25:59+00:00
- **Updated**: 2023-07-19 07:34:37+00:00
- **Authors**: Fengfan Zhou, Hefei Ling, Yuxuan Shi, Jiazhong Chen, Zongyi Li, Ping Li
- **Comment**: \c{opyright} 2023 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: Face recognition (FR) models can be easily fooled by adversarial examples, which are crafted by adding imperceptible perturbations on benign face images. The existence of adversarial face examples poses a great threat to the security of society. In order to build a more sustainable digital nation, in this paper, we improve the transferability of adversarial face examples to expose more blind spots of existing FR models. Though generating hard samples has shown its effectiveness in improving the generalization of models in training tasks, the effectiveness of utilizing this idea to improve the transferability of adversarial face examples remains unexplored. To this end, based on the property of hard samples and the symmetry between training tasks and adversarial attack tasks, we propose the concept of hard models, which have similar effects as hard samples for adversarial attack tasks. Utilizing the concept of hard models, we propose a novel attack method called Beneficial Perturbation Feature Augmentation Attack (BPFA), which reduces the overfitting of adversarial examples to surrogate FR models by constantly generating new hard models to craft the adversarial examples. Specifically, in the backpropagation, BPFA records the gradients on pre-selected feature maps and uses the gradient on the input image to craft the adversarial example. In the next forward propagation, BPFA leverages the recorded gradients to add beneficial perturbations on their corresponding feature maps to increase the loss. Extensive experiments demonstrate that BPFA can significantly boost the transferability of adversarial attacks on FR.



### Localized Randomized Smoothing for Collective Robustness Certification
- **Arxiv ID**: http://arxiv.org/abs/2210.16140v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16140v2)
- **Published**: 2022-10-28 14:10:24+00:00
- **Updated**: 2023-03-03 17:19:40+00:00
- **Authors**: Jan Schuchardt, Tom Wollschläger, Aleksandar Bojchevski, Stephan Günnemann
- **Comment**: Accepted at ICLR 2023
- **Journal**: None
- **Summary**: Models for image segmentation, node classification and many other tasks map a single input to multiple labels. By perturbing this single shared input (e.g. the image) an adversary can manipulate several predictions (e.g. misclassify several pixels). Collective robustness certification is the task of provably bounding the number of robust predictions under this threat model. The only dedicated method that goes beyond certifying each output independently is limited to strictly local models, where each prediction is associated with a small receptive field. We propose a more general collective robustness certificate for all types of models. We further show that this approach is beneficial for the larger class of softly local models, where each output is dependent on the entire input but assigns different levels of importance to different input regions (e.g. based on their proximity in the image). The certificate is based on our novel localized randomized smoothing approach, where the random perturbation strength for different input regions is proportional to their importance for the outputs. Localized smoothing Pareto-dominates existing certificates on both image segmentation and node classification tasks, simultaneously offering higher accuracy and stronger certificates.



### Federated Learning for Chronic Obstructive Pulmonary Disease Classification with Partial Personalized Attention Mechanism
- **Arxiv ID**: http://arxiv.org/abs/2210.16142v1
- **DOI**: 10.1109/BIBM55620.2022.9995355
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16142v1)
- **Published**: 2022-10-28 14:12:42+00:00
- **Updated**: 2022-10-28 14:12:42+00:00
- **Authors**: Yiqing Shen, Baiyun Liu, Ruize Yu, Yudong Wang, Shaokang Wang, Jiangfen Wu, Weidao Chen
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Chronic Obstructive Pulmonary Disease (COPD) is the fourth leading cause of death worldwide. Yet, COPD diagnosis heavily relies on spirometric examination as well as functional airway limitation, which may cause a considerable portion of COPD patients underdiagnosed especially at the early stage. Recent advance in deep learning (DL) has shown their promising potential in COPD identification from CT images. However, with heterogeneous syndromes and distinct phenotypes, DL models trained with CTs from one data center fail to generalize on images from another center. Due to privacy regularizations, a collaboration of distributed CT images into one centralized center is not feasible. Federated learning (FL) approaches enable us to train with distributed private data. Yet, routine FL solutions suffer from performance degradation in the case where COPD CTs are not independent and identically distributed (Non-IID). To address this issue, we propose a novel personalized federated learning (PFL) method based on vision transformer (ViT) for distributed and heterogeneous COPD CTs. To be more specific, we partially personalize some heads in multiheaded self-attention layers to learn the personalized attention for local data and retain the other heads shared to extract the common attention. To the best of our knowledge, this is the first proposal of a PFL framework specifically for ViT to identify COPD. Our evaluation of a dataset set curated from six medical centers shows our method outperforms the PFL approaches for convolutional neural networks.



### Reliability of CKA as a Similarity Measure in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.16156v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16156v2)
- **Published**: 2022-10-28 14:32:52+00:00
- **Updated**: 2022-11-16 14:01:09+00:00
- **Authors**: MohammadReza Davari, Stefan Horoi, Amine Natik, Guillaume Lajoie, Guy Wolf, Eugene Belilovsky
- **Comment**: None
- **Journal**: None
- **Summary**: Comparing learned neural representations in neural networks is a challenging but important problem, which has been approached in different ways. The Centered Kernel Alignment (CKA) similarity metric, particularly its linear variant, has recently become a popular approach and has been widely used to compare representations of a network's different layers, of architecturally similar networks trained differently, or of models with different architectures trained on the same data. A wide variety of conclusions about similarity and dissimilarity of these various representations have been made using CKA. In this work we present analysis that formally characterizes CKA sensitivity to a large class of simple transformations, which can naturally occur in the context of modern machine learning. This provides a concrete explanation of CKA sensitivity to outliers, which has been observed in past works, and to transformations that preserve the linear separability of the data, an important generalization attribute. We empirically investigate several weaknesses of the CKA similarity metric, demonstrating situations in which it gives unexpected or counter-intuitive results. Finally we study approaches for modifying representations to maintain functional behaviour while changing the CKA value. Our results illustrate that, in many cases, the CKA value can be easily manipulated without substantial changes to the functional behaviour of the models, and call for caution when leveraging activation alignment metrics.



### Multimodal Transformer for Parallel Concatenated Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2210.16174v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16174v1)
- **Published**: 2022-10-28 14:45:32+00:00
- **Updated**: 2022-10-28 14:45:32+00:00
- **Authors**: Stephen D. Liang, Jerry M. Mendel
- **Comment**: NeurIPS 2022 Workshop on Vision Transformers: Theory and Application,
  New Orleans, LA, December 2022
- **Journal**: None
- **Summary**: In this paper, we propose a multimodal transformer using parallel concatenated architecture. Instead of using patches, we use column stripes for images in R, G, B channels as the transformer input. The column stripes keep the spatial relations of original image. We incorporate the multimodal transformer with variational autoencoder for synthetic cross-modal data generation. The multimodal transformer is designed using multiple compression matrices, and it serves as encoders for Parallel Concatenated Variational AutoEncoders (PC-VAE). The PC-VAE consists of multiple encoders, one latent space, and two decoders. The encoders are based on random Gaussian matrices and don't need any training. We propose a new loss function based on the interaction information from partial information decomposition. The interaction information evaluates the input cross-modal information and decoder output. The PC-VAE are trained via minimizing the loss function. Experiments are performed to validate the proposed multimodal transformer for PC-VAE.



### Digital twins of physical printing-imaging channel
- **Arxiv ID**: http://arxiv.org/abs/2210.17420v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.17420v1)
- **Published**: 2022-10-28 14:48:03+00:00
- **Updated**: 2022-10-28 14:48:03+00:00
- **Authors**: Yury Belousov, Brian Pulfer, Roman Chaban, Joakim Tutt, Olga Taran, Taras Holotyak, Slava Voloshynovskiy
- **Comment**: Paper accepted at the IEEE International Workshop on Information
  Forensics and Security (WIFS) 2022
- **Journal**: None
- **Summary**: In this paper, we address the problem of modeling a printing-imaging channel built on a machine learning approach a.k.a. digital twin for anti-counterfeiting applications based on copy detection patterns (CDP). The digital twin is formulated on an information-theoretic framework called Turbo that uses variational approximations of mutual information developed for both encoder and decoder in a two-directional information passage. The proposed model generalizes several state-of-the-art architectures such as adversarial autoencoder (AAE), CycleGAN and adversarial latent space autoencoder (ALAE). This model can be applied to any type of printing and imaging and it only requires training data consisting of digital templates or artworks that are sent to a printing device and data acquired by an imaging device. Moreover, these data can be paired, unpaired or hybrid paired-unpaired which makes the proposed architecture very flexible and scalable to many practical setups. We demonstrate the impact of various architectural factors, metrics and discriminators on the overall system performance in the task of generation/prediction of printed CDP from their digital counterparts and vice versa. We also compare the proposed system with several state-of-the-art methods used for image-to-image translation applications.



### TripletTrack: 3D Object Tracking using Triplet Embeddings and LSTM
- **Arxiv ID**: http://arxiv.org/abs/2210.16204v1
- **DOI**: 10.1109/CVPRW56347.2022.00496
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16204v1)
- **Published**: 2022-10-28 15:23:50+00:00
- **Updated**: 2022-10-28 15:23:50+00:00
- **Authors**: Nicola Marinello, Marc Proesmans, Luc Van Gool
- **Comment**: Accepted to CVPR 2022 Workshop on Autonomous Driving
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops June 2022 4500-4510
- **Summary**: 3D object tracking is a critical task in autonomous driving systems. It plays an essential role for the system's awareness about the surrounding environment. At the same time there is an increasing interest in algorithms for autonomous cars that solely rely on inexpensive sensors, such as cameras. In this paper we investigate the use of triplet embeddings in combination with motion representations for 3D object tracking. We start from an off-the-shelf 3D object detector, and apply a tracking mechanism where objects are matched by an affinity score computed on local object feature embeddings and motion descriptors. The feature embeddings are trained to include information about the visual appearance and monocular 3D object characteristics, while motion descriptors provide a strong representation of object trajectories. We will show that our approach effectively re-identifies objects, and also behaves reliably and accurately in case of occlusions, missed detections and can detect re-appearance across different field of views. Experimental evaluation shows that our approach outperforms state-of-the-art on nuScenes by a large margin. We also obtain competitive results on KITTI.



### Design of Convolutional Extreme Learning Machines for Vision-Based Navigation Around Small Bodies
- **Arxiv ID**: http://arxiv.org/abs/2210.16244v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.16244v1)
- **Published**: 2022-10-28 16:24:21+00:00
- **Updated**: 2022-10-28 16:24:21+00:00
- **Authors**: Mattia Pugliatti, Francesco Topputo
- **Comment**: 27 pages, 12 figures, journal contribution submitted to JGCD
- **Journal**: None
- **Summary**: Deep learning architectures such as convolutional neural networks are the standard in computer vision for image processing tasks. Their accuracy however often comes at the cost of long and computationally expensive training, the need for large annotated datasets, and extensive hyper-parameter searches. On the other hand, a different method known as convolutional extreme learning machine has shown the potential to perform equally with a dramatic decrease in training time. Space imagery, especially about small bodies, could be well suited for this method. In this work, convolutional extreme learning machine architectures are designed and tested against their deep-learning counterparts. Because of the relatively fast training time of the former, convolutional extreme learning machine architectures enable efficient exploration of the architecture design space, which would have been impractical with the latter, introducing a methodology for an efficient design of a neural network architecture for computer vision tasks. Also, the coupling between the image processing method and labeling strategy is investigated and demonstrated to play a major role when considering vision-based navigation around small bodies.



### I am Only Happy When There is Light: The Impact of Environmental Changes on Affective Facial Expressions Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.17421v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.17421v1)
- **Published**: 2022-10-28 16:28:26+00:00
- **Updated**: 2022-10-28 16:28:26+00:00
- **Authors**: Doreen Jirak, Alessandra Sciutti, Pablo Barros, Francesco Rea
- **Comment**: Paper contribution to the Social and Cognitive Interactions for
  Assistive Robotics Workshop (SCIAR) In Conjunction with IROS 2022
- **Journal**: None
- **Summary**: Human-robot interaction (HRI) benefits greatly from advances in the machine learning field as it allows researchers to employ high-performance models for perceptual tasks like detection and recognition. Especially deep learning models, either pre-trained for feature extraction or used for classification, are now established methods to characterize human behaviors in HRI scenarios and to have social robots that understand better those behaviors. As HRI experiments are usually small-scale and constrained to particular lab environments, the questions are how well can deep learning models generalize to specific interaction scenarios, and further, how good is their robustness towards environmental changes? These questions are important to address if the HRI field wishes to put social robotic companions into real environments acting consistently, i.e. changing lighting conditions or moving people should still produce the same recognition results. In this paper, we study the impact of different image conditions on the recognition of arousal and valence from human facial expressions using the FaceChannel framework \cite{Barro20}. Our results show how the interpretation of human affective states can differ greatly in either the positive or negative direction even when changing only slightly the image properties. We conclude the paper with important points to consider when employing deep learning models to ensure sound interpretation of HRI experiments.



### Latent Space is Feature Space: Regularization Term for GANs Training on Limited Dataset
- **Arxiv ID**: http://arxiv.org/abs/2210.16251v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16251v2)
- **Published**: 2022-10-28 16:34:48+00:00
- **Updated**: 2022-11-15 07:56:42+00:00
- **Authors**: Pengwei Wang
- **Comment**: 8 pages, 6 figures, 2 tables
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GAN) is currently widely used as an unsupervised image generation method. Current state-of-the-art GANs can generate photorealistic images with high resolution. However, a large amount of data is required, or the model would prone to generate images with similar patterns (mode collapse) and bad quality. I proposed an additional structure and loss function for GANs called LFM, trained to maximize the feature diversity between the different dimensions of the latent space to avoid mode collapse without affecting the image quality. Orthogonal latent vector pairs are created, and feature vector pairs extracted by discriminator are examined by dot product, with which discriminator and generator are in a novel adversarial relationship. In experiments, this system has been built upon DCGAN and proved to have improvement on Frechet Inception Distance (FID) training from scratch on CelebA Dataset. This system requires mild extra performance and can work with data augmentation methods. The code is available on github.com/penway/LFM.



### DOORS: Dataset fOr bOuldeRs Segmentation. Statistical properties and Blender setup
- **Arxiv ID**: http://arxiv.org/abs/2210.16253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.DB, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16253v1)
- **Published**: 2022-10-28 16:39:06+00:00
- **Updated**: 2022-10-28 16:39:06+00:00
- **Authors**: Mattia Pugliatti, Francesco Topputo
- **Comment**: 16 pages, 19 figures, summary paper of a dataset
- **Journal**: None
- **Summary**: The capability to detect boulders on the surface of small bodies is beneficial for vision-based applications such as hazard detection during critical operations and navigation. This task is challenging due to the wide assortment of irregular shapes, the characteristics of the boulders population, and the rapid variability in the illumination conditions. Moreover, the lack of publicly available labeled datasets for these applications damps the research about data-driven algorithms. In this work, the authors provide a statistical characterization and setup used for the generation of two datasets about boulders on small bodies that are made publicly available.



### SEMPAI: a Self-Enhancing Multi-Photon Artificial Intelligence for prior-informed assessment of muscle function and pathology
- **Arxiv ID**: http://arxiv.org/abs/2210.16273v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM, I.2.6; I.4; I.5; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2210.16273v1)
- **Published**: 2022-10-28 17:03:04+00:00
- **Updated**: 2022-10-28 17:03:04+00:00
- **Authors**: Alexander Mühlberg, Paul Ritter, Simon Langer, Chloë Goossens, Stefanie Nübler, Dominik Schneidereit, Oliver Taubmann, Felix Denzinger, Dominik Nörenberg, Michael Haug, Wolfgang H. Goldmann, Andreas K. Maier, Oliver Friedrich, Lucas Kreiss
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning (DL) shows notable success in biomedical studies. However, most DL algorithms work as a black box, exclude biomedical experts, and need extensive data. We introduce the Self-Enhancing Multi-Photon Artificial Intelligence (SEMPAI), that integrates hypothesis-driven priors in a data-driven DL approach for research on multiphoton microscopy (MPM) of muscle fibers. SEMPAI utilizes meta-learning to optimize prior integration, data representation, and neural network architecture simultaneously. This allows hypothesis testing and provides interpretable feedback about the origin of biological information in MPM images. SEMPAI performs joint learning of several tasks to enable prediction for small datasets. The method is applied on an extensive multi-study dataset resulting in the largest joint analysis of pathologies and function for single muscle fibers. SEMPAI outperforms state-of-the-art biomarkers in six of seven predictive tasks, including those with scarce data. SEMPAI's DL models with integrated priors are superior to those without priors and to prior-only machine learning approaches.



### Boulders Identification on Small Bodies Under Varying Illumination Conditions
- **Arxiv ID**: http://arxiv.org/abs/2210.16283v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16283v1)
- **Published**: 2022-10-28 17:22:46+00:00
- **Updated**: 2022-10-28 17:22:46+00:00
- **Authors**: Mattia Pugliatti, Francesco Topputo
- **Comment**: 12 pages, 15 figures, 3rd Space Imaging Workshop, Atlanta, Georgia
  Tech
- **Journal**: None
- **Summary**: The capability to detect boulders on the surface of small bodies is beneficial for vision-based applications such as navigation and hazard detection during critical operations. This task is challenging due to the wide assortment of irregular shapes, the characteristics of the boulders population, and the rapid variability in the illumination conditions. The authors address this challenge by designing a multi-step training approach to develop a data-driven image processing pipeline to robustly detect and segment boulders scattered over the surface of a small body. Due to the limited availability of labeled image-mask pairs, the developed methodology is supported by two artificial environments designed in Blender specifically for this work. These are used to generate a large amount of synthetic image-label sets, which are made publicly available to the image processing community. The methodology presented addresses the challenges of varying illumination conditions, irregular shapes, fast training time, extensive exploration of the architecture design space, and domain gap between synthetic and real images from previously flown missions. The performance of the developed image processing pipeline is tested both on synthetic and real images, exhibiting good performances, and high generalization capabilities



### Improving Hyperspectral Adversarial Robustness Under Multiple Attacks
- **Arxiv ID**: http://arxiv.org/abs/2210.16346v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16346v4)
- **Published**: 2022-10-28 18:21:45+00:00
- **Updated**: 2023-05-11 15:44:32+00:00
- **Authors**: Nicholas Soucy, Salimeh Yasaei Sekeh
- **Comment**: 6 pages, 2 figures, 1 table, 1 algorithm
- **Journal**: None
- **Summary**: Semantic segmentation models classifying hyperspectral images (HSI) are vulnerable to adversarial examples. Traditional approaches to adversarial robustness focus on training or retraining a single network on attacked data, however, in the presence of multiple attacks these approaches decrease in performance compared to networks trained individually on each attack. To combat this issue we propose an Adversarial Discriminator Ensemble Network (ADE-Net) which focuses on attack type detection and adversarial robustness under a unified model to preserve per data-type weight optimally while robustifiying the overall network. In the proposed method, a discriminator network is used to separate data by attack type into their specific attack-expert ensemble network.



### An Approach for Noisy, Crowdsourced Datasets Utilizing Ensemble Modeling, Normalized Distributions of Annotations, and Entropic Measures of Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/2210.16380v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16380v2)
- **Published**: 2022-10-28 19:39:14+00:00
- **Updated**: 2023-03-17 01:11:23+00:00
- **Authors**: Graham West, Matthew I. Swindall, Ben Keener, Timothy Player, Alex C. Williams, James H. Brusuelas, John F. Wallin
- **Comment**: None
- **Journal**: None
- **Summary**: Performing classification on noisy, crowdsourced image datasets can prove challenging even for the best neural networks. Two issues which complicate the problem on such datasets are class imbalance and ground-truth uncertainty in labeling. The AL-ALL and AL-PUB datasets -- consisting of tightly cropped, individual characters from images of ancient Greek papyri -- are strongly affected by both issues. The application of ensemble modeling to such datasets can help identify images where the ground-truth is questionable and quantify the trustworthiness of those samples. As such, we apply stacked generalization consisting of nearly identical ResNets with different loss functions: one utilizing sparse cross-entropy (CXE) and the other Kullback-Liebler Divergence (KLD). Both networks use labels drawn from the crowdsourced consensus. For the second network, the KLD is calculated with respect to the proposed Normalized Distribution of Annotations (NDA). For our ensemble model, we apply a k-nearest neighbors model to the outputs of the CXE and KLD networks. Individually, the ResNet models have approximately 93% accuracy, while the ensemble model achieves an accuracy of > 95%. We also perform an analysis of the Shannon entropy of the various models' output distributions to measure classification uncertainty. Our results suggest that entropy is useful for predicting model misclassifications.



### U-Net-based Models for Skin Lesion Segmentation: More Attention and Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.16399v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16399v1)
- **Published**: 2022-10-28 20:39:09+00:00
- **Updated**: 2022-10-28 20:39:09+00:00
- **Authors**: Pooya Mohammadi Kazaj, MohammadHossein Koosheshi, Ali Shahedi, Alireza Vafaei Sadr
- **Comment**: 27 pages, 12 figures
- **Journal**: None
- **Summary**: According to WHO[1], since the 1970s, diagnosis of melanoma skin cancer has been more frequent. However, if detected early, the 5-year survival rate for melanoma can increase to 99 percent. In this regard, skin lesion segmentation can be pivotal in monitoring and treatment planning. In this work, ten models and four augmentation configurations are trained on the ISIC 2016 dataset. The performance and overfitting are compared utilizing five metrics. Our results show that the U-Net-Resnet50 and the R2U-Net have the highest metrics value, along with two data augmentation scenarios. We also investigate CBAM and AG blocks in the U-Net architecture, which enhances segmentation performance at a meager computational cost. In addition, we propose using pyramid, AG, and CBAM blocks in a sequence, which significantly surpasses the results of using the two individually. Finally, our experiments show that models that have exploited attention modules successfully overcome common skin lesion segmentation problems. Lastly, in the spirit of reproducible research, we implement models and codes publicly available.



### DiMBERT: Learning Vision-Language Grounded Representations with Disentangled Multimodal-Attention
- **Arxiv ID**: http://arxiv.org/abs/2210.16431v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2210.16431v1)
- **Published**: 2022-10-28 23:00:40+00:00
- **Updated**: 2022-10-28 23:00:40+00:00
- **Authors**: Fenglin Liu, Xian Wu, Shen Ge, Xuancheng Ren, Wei Fan, Xu Sun, Yuexian Zou
- **Comment**: Published in ACM TKDD2022 (ACM Transactions on Knowledge Discovery
  from Data)
- **Journal**: None
- **Summary**: Vision-and-language (V-L) tasks require the system to understand both vision content and natural language, thus learning fine-grained joint representations of vision and language (a.k.a. V-L representations) is of paramount importance. Recently, various pre-trained V-L models are proposed to learn V-L representations and achieve improved results in many tasks. However, the mainstream models process both vision and language inputs with the same set of attention matrices. As a result, the generated V-L representations are entangled in one common latent space. To tackle this problem, we propose DiMBERT (short for Disentangled Multimodal-Attention BERT), which is a novel framework that applies separated attention spaces for vision and language, and the representations of multi-modalities can thus be disentangled explicitly. To enhance the correlation between vision and language in disentangled spaces, we introduce the visual concepts to DiMBERT which represent visual information in textual format. In this manner, visual concepts help to bridge the gap between the two modalities. We pre-train DiMBERT on a large amount of image-sentence pairs on two tasks: bidirectional language modeling and sequence-to-sequence language modeling. After pre-train, DiMBERT is further fine-tuned for the downstream tasks. Experiments show that DiMBERT sets new state-of-the-art performance on three tasks (over four datasets), including both generation tasks (image captioning and visual storytelling) and classification tasks (referring expressions). The proposed DiM (short for Disentangled Multimodal-Attention) module can be easily incorporated into existing pre-trained V-L models to boost their performance, up to a 5% increase on the representative task. Finally, we conduct a systematic analysis and demonstrate the effectiveness of our DiM and the introduced visual concepts.



