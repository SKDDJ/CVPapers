# Arxiv Papers in cs.CV on 2022-10-04
### Centroid Distance Keypoint Detector for Colored Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2210.01298v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.01298v2)
- **Published**: 2022-10-04 00:55:51+00:00
- **Updated**: 2023-06-15 04:43:24+00:00
- **Authors**: Hanzhe Teng, Dimitrios Chatziparaschis, Xinyue Kan, Amit K. Roy-Chowdhury, Konstantinos Karydis
- **Comment**: Accepted to IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV) 2023; copyright will be transferred to IEEE upon publication
- **Journal**: None
- **Summary**: Keypoint detection serves as the basis for many computer vision and robotics applications. Despite the fact that colored point clouds can be readily obtained, most existing keypoint detectors extract only geometry-salient keypoints, which can impede the overall performance of systems that intend to (or have the potential to) leverage color information. To promote advances in such systems, we propose an efficient multi-modal keypoint detector that can extract both geometry-salient and color-salient keypoints in colored point clouds. The proposed CEntroid Distance (CED) keypoint detector comprises an intuitive and effective saliency measure, the centroid distance, that can be used in both 3D space and color space, and a multi-modal non-maximum suppression algorithm that can select keypoints with high saliency in two or more modalities. The proposed saliency measure leverages directly the distribution of points in a local neighborhood and does not require normal estimation or eigenvalue decomposition. We evaluate the proposed method in terms of repeatability and computational efficiency (i.e. running time) against state-of-the-art keypoint detectors on both synthetic and real-world datasets. Results demonstrate that our proposed CED keypoint detector requires minimal computational time while attaining high repeatability. To showcase one of the potential applications of the proposed method, we further investigate the task of colored point cloud registration. Results suggest that our proposed CED detector outperforms state-of-the-art handcrafted and learning-based keypoint detectors in the evaluated scenes. The C++ implementation of the proposed method is made publicly available at https://github.com/UCR-Robotics/CED_Detector.



### Nuisances via Negativa: Adjusting for Spurious Correlations via Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.01302v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.01302v2)
- **Published**: 2022-10-04 01:40:31+00:00
- **Updated**: 2023-03-01 06:00:47+00:00
- **Authors**: Aahlad Puli, Nitish Joshi, He He, Rajesh Ranganath
- **Comment**: None
- **Journal**: None
- **Summary**: In prediction tasks, there exist features that are related to the label in the same way across different settings for that task; these are semantic features or semantics. Features with varying relationships to the label are nuisances. For example, in detecting cows from natural images, the shape of the head is a semantic but because images of cows often have grass backgrounds but not always, the background is a nuisance. Relationships between a nuisance and the label are unstable across settings and, consequently, models that exploit nuisance-label relationships face performance degradation when these relationships change. Direct knowledge of a nuisance helps build models that are robust to such changes, but requires extra annotations beyond labels and covariates. In this paper, we develop an alternative way to produce robust models by data augmentation. These data augmentations corrupt semantic information to produce models that identify and adjust for where nuisances drive predictions. We study semantic corruptions in powering different spurious-correlation avoiding methods on multiple out-of distribution (OOD) tasks like classifying waterbirds, natural language inference (NLI), and detecting cardiomegaly in chest X-rays.



### Representing Spatial Trajectories as Distributions
- **Arxiv ID**: http://arxiv.org/abs/2210.01322v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.01322v1)
- **Published**: 2022-10-04 02:35:50+00:00
- **Updated**: 2022-10-04 02:35:50+00:00
- **Authors**: Dídac Surís, Carl Vondrick
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: We introduce a representation learning framework for spatial trajectories. We represent partial observations of trajectories as probability distributions in a learned latent space, which characterize the uncertainty about unobserved parts of the trajectory. Our framework allows us to obtain samples from a trajectory for any continuous point in time, both interpolating and extrapolating. Our flexible approach supports directly modifying specific attributes of a trajectory, such as its pace, as well as combining different partial observations into single representations. Experiments show our method's advantage over baselines in prediction tasks.



### ASAP: Accurate semantic segmentation for real time performance
- **Arxiv ID**: http://arxiv.org/abs/2210.01323v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.01323v1)
- **Published**: 2022-10-04 02:35:53+00:00
- **Updated**: 2022-10-04 02:35:53+00:00
- **Authors**: Jaehyun Park, Subin Lee, Eon Kim, Byeongjun Moon, Dabeen Yu, Yeonseung Yu, Junghwan Kim
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Feature fusion modules from encoder and self-attention module have been adopted in semantic segmentation. However, the computation of these modules is costly and has operational limitations in real-time environments. In addition, segmentation performance is limited in autonomous driving environments with a lot of contextual information perpendicular to the road surface, such as people, buildings, and general objects. In this paper, we propose an efficient feature fusion method, Feature Fusion with Different Norms (FFDN) that utilizes rich global context of multi-level scale and vertical pooling module before self-attention that preserves most contextual information while reducing the complexity of global context encoding in the vertical direction. By doing this, we could handle the properties of representation in global space and reduce additional computational cost. In addition, we analyze low performance in challenging cases including small and vertically featured objects. We achieve the mean Interaction of-union(mIoU) of 73.1 and the Frame Per Second(FPS) of 191, which are comparable results with state-of-the-arts on Cityscapes test datasets.



### Automated Medical Device Display Reading Using Deep Learning Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.01325v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4.9; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2210.01325v1)
- **Published**: 2022-10-04 02:39:45+00:00
- **Updated**: 2022-10-04 02:39:45+00:00
- **Authors**: Lucas P. Moreira
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Telemedicine and mobile health applications, especially during the quarantine imposed by the covid-19 pandemic, led to an increase on the need of transferring health monitor readings from patients to specialists. Considering that most home medical devices use seven-segment displays, an automatic display reading algorithm should provide a more reliable tool for remote health care. This work proposes an end-to-end method for detection and reading seven-segment displays from medical devices based on deep learning object detection models. Two state of the art model families, EfficientDet and EfficientDet-lite, previously trained with the MS-COCO dataset, were fine-tuned on a dataset comprised by medical devices photos taken with mobile digital cameras, to simulate real case applications. Evaluation of the trained model show high efficiency, where all models achieved more than 98% of detection precision and more than 98% classification accuracy, with model EfficientDet-lite1 showing 100% detection precision and 100% correct digit classification for a test set of 104 images and 438 digits.



### Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2210.01338v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2210.01338v2)
- **Published**: 2022-10-04 03:09:50+00:00
- **Updated**: 2023-04-24 02:27:07+00:00
- **Authors**: Xu Yang, Hanwang Zhang, Chongyang Gao, Jianfei Cai
- **Comment**: Accepted to IJCV. Codes are available at
  https://github.com/GCYZSL/CVLMN
- **Journal**: None
- **Summary**: Humans tend to decompose a sentence into different parts like \textsc{sth do sth at someplace} and then fill each part with certain content. Inspired by this, we follow the \textit{principle of modular design} to propose a novel image captioner: learning to Collocate Visual-Linguistic Neural Modules (CVLNM). Unlike the \re{widely used} neural module networks in VQA, where the language (\ie, question) is fully observable, \re{the task of collocating visual-linguistic modules is more challenging.} This is because the language is only partially observable, for which we need to dynamically collocate the modules during the process of image captioning. To sum up, we make the following technical contributions to design and train our CVLNM: 1) \textit{distinguishable module design} -- \re{four modules in the encoder} including one linguistic module for function words and three visual modules for different content words (\ie, noun, adjective, and verb) and another linguistic one in the decoder for commonsense reasoning, 2) a self-attention based \textit{module controller} for robustifying the visual reasoning, 3) a part-of-speech based \textit{syntax loss} imposed on the module controller for further regularizing the training of our CVLNM. Extensive experiments on the MS-COCO dataset show that our CVLNM is more effective, \eg, achieving a new state-of-the-art 129.5 CIDEr-D, and more robust, \eg, being less likely to overfit to dataset bias and suffering less when fewer training samples are available. Codes are available at \url{https://github.com/GCYZSL/CVLMN}



### ImmFusion: Robust mmWave-RGB Fusion for 3D Human Body Reconstruction in All Weather Conditions
- **Arxiv ID**: http://arxiv.org/abs/2210.01346v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01346v2)
- **Published**: 2022-10-04 03:30:18+00:00
- **Updated**: 2023-07-10 03:36:39+00:00
- **Authors**: Anjun Chen, Xiangyu Wang, Kun Shi, Shaohao Zhu, Bin Fang, Yingfeng Chen, Jiming Chen, Yuchi Huo, Qi Ye
- **Comment**: Accepted to ICRA2023, Project Page:
  https://chen3110.github.io/ImmFusion/index.html
- **Journal**: None
- **Summary**: 3D human reconstruction from RGB images achieves decent results in good weather conditions but degrades dramatically in rough weather. Complementary, mmWave radars have been employed to reconstruct 3D human joints and meshes in rough weather. However, combining RGB and mmWave signals for robust all-weather 3D human reconstruction is still an open challenge, given the sparse nature of mmWave and the vulnerability of RGB images. In this paper, we present ImmFusion, the first mmWave-RGB fusion solution to reconstruct 3D human bodies in all weather conditions robustly. Specifically, our ImmFusion consists of image and point backbones for token feature extraction and a Transformer module for token fusion. The image and point backbones refine global and local features from original data, and the Fusion Transformer Module aims for effective information fusion of two modalities by dynamically selecting informative tokens. Extensive experiments on a large-scale dataset, mmBody, captured in various environments demonstrate that ImmFusion can efficiently utilize the information of two modalities to achieve a robust 3D human body reconstruction in all weather conditions. In addition, our method's accuracy is significantly superior to that of state-of-the-art Transformer-based LiDAR-camera fusion methods.



### Distance Based Image Classification: A solution to generative classification's conundrum?
- **Arxiv ID**: http://arxiv.org/abs/2210.01349v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.01349v1)
- **Published**: 2022-10-04 03:35:13+00:00
- **Updated**: 2022-10-04 03:35:13+00:00
- **Authors**: Wen-Yan Lin, Siying Liu, Bing Tian Dai, Hongdong Li
- **Comment**: accepted by IJCV
- **Journal**: None
- **Summary**: Most classifiers rely on discriminative boundaries that separate instances of each class from everything else. We argue that discriminative boundaries are counter-intuitive as they define semantics by what-they-are-not; and should be replaced by generative classifiers which define semantics by what-they-are. Unfortunately, generative classifiers are significantly less accurate. This may be caused by the tendency of generative models to focus on easy to model semantic generative factors and ignore non-semantic factors that are important but difficult to model. We propose a new generative model in which semantic factors are accommodated by shell theory's hierarchical generative process and non-semantic factors by an instance specific noise term. We use the model to develop a classification scheme which suppresses the impact of noise while preserving semantic cues. The result is a surprisingly accurate generative classifier, that takes the form of a modified nearest-neighbor algorithm; we term it distance classification. Unlike discriminative classifiers, a distance classifier: defines semantics by what-they-are; is amenable to incremental updates; and scales well with the number of classes.



### Uncertainty-Aware Lidar Place Recognition in Novel Environments
- **Arxiv ID**: http://arxiv.org/abs/2210.01361v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01361v3)
- **Published**: 2022-10-04 04:06:44+00:00
- **Updated**: 2023-07-12 03:44:59+00:00
- **Authors**: Keita Mason, Joshua Knights, Milad Ramezani, Peyman Moghadam, Dimity Miller
- **Comment**: 8 pages, 4 figures. Accepted for publication at IEEE IROS 2023
- **Journal**: None
- **Summary**: State-of-the-art lidar place recognition models exhibit unreliable performance when tested on environments different from their training dataset, which limits their use in complex and evolving environments. To address this issue, we investigate the task of uncertainty-aware lidar place recognition, where each predicted place must have an associated uncertainty that can be used to identify and reject incorrect predictions. We introduce a novel evaluation protocol and present the first comprehensive benchmark for this task, testing across five uncertainty estimation techniques and three large-scale datasets. Our results show that an Ensembles approach is the highest performing technique, consistently improving the performance of lidar place recognition and uncertainty estimation in novel environments, though it incurs a computational cost. Code is publicly available at https://github.com/csiro-robotics/Uncertainty-LPR.



### A Generalizable Artificial Intelligence Model for COVID-19 Classification Task Using Chest X-ray Radiographs: Evaluated Over Four Clinical Datasets with 15,097 Patients
- **Arxiv ID**: http://arxiv.org/abs/2210.02189v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.02189v1)
- **Published**: 2022-10-04 04:12:13+00:00
- **Updated**: 2022-10-04 04:12:13+00:00
- **Authors**: Ran Zhang, Xin Tie, John W. Garrett, Dalton Griner, Zhihua Qi, Nicholas B. Bevins, Scott B. Reeder, Guang-Hong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To answer the long-standing question of whether a model trained from a single clinical site can be generalized to external sites.   Materials and Methods: 17,537 chest x-ray radiographs (CXRs) from 3,264 COVID-19-positive patients and 4,802 COVID-19-negative patients were collected from a single site for AI model development. The generalizability of the trained model was retrospectively evaluated using four different real-world clinical datasets with a total of 26,633 CXRs from 15,097 patients (3,277 COVID-19-positive patients). The area under the receiver operating characteristic curve (AUC) was used to assess diagnostic performance.   Results: The AI model trained using a single-source clinical dataset achieved an AUC of 0.82 (95% CI: 0.80, 0.84) when applied to the internal temporal test set. When applied to datasets from two external clinical sites, an AUC of 0.81 (95% CI: 0.80, 0.82) and 0.82 (95% CI: 0.80, 0.84) were achieved. An AUC of 0.79 (95% CI: 0.77, 0.81) was achieved when applied to a multi-institutional COVID-19 dataset collected by the Medical Imaging and Data Resource Center (MIDRC). A power-law dependence, N^(k )(k is empirically found to be -0.21 to -0.25), indicates a relatively weak performance dependence on the training data sizes.   Conclusion: COVID-19 classification AI model trained using well-curated data from a single clinical site is generalizable to external clinical sites without a significant drop in performance.



### Towards Flexible Inductive Bias via Progressive Reparameterization Scheduling
- **Arxiv ID**: http://arxiv.org/abs/2210.01370v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01370v1)
- **Published**: 2022-10-04 04:20:20+00:00
- **Updated**: 2022-10-04 04:20:20+00:00
- **Authors**: Yunsung Lee, Gyuseong Lee, Kwangrok Ryoo, Hyojun Go, Jihye Park, Seungryong Kim
- **Comment**: Accepted at VIPriors ECCVW 2022, camera-ready version
- **Journal**: None
- **Summary**: There are two de facto standard architectures in recent computer vision: Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Strong inductive biases of convolutions help the model learn sample effectively, but such strong biases also limit the upper bound of CNNs when sufficient data are available. On the contrary, ViT is inferior to CNNs for small data but superior for sufficient data. Recent approaches attempt to combine the strengths of these two architectures. However, we show these approaches overlook that the optimal inductive bias also changes according to the target data scale changes by comparing various models' accuracy on subsets of sampled ImageNet at different ratios. In addition, through Fourier analysis of feature maps, the model's response patterns according to signal frequency changes, we observe which inductive bias is advantageous for each data scale. The more convolution-like inductive bias is included in the model, the smaller the data scale is required where the ViT-like model outperforms the ResNet performance. To obtain a model with flexible inductive bias on the data scale, we show reparameterization can interpolate inductive bias between convolution and self-attention. By adjusting the number of epochs the model stays in the convolution, we show that reparameterization from convolution to self-attention interpolates the Fourier analysis pattern between CNNs and ViTs. Adapting these findings, we propose Progressive Reparameterization Scheduling (PRS), in which reparameterization adjusts the required amount of convolution-like or self-attention-like inductive bias per layer. For small-scale datasets, our PRS performs reparameterization from convolution to self-attention linearly faster at the late stage layer. PRS outperformed previous studies on the small-scale dataset, e.g., CIFAR-100.



### Toward Edge-Efficient Dense Predictions with Synergistic Multi-Task Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/2210.01384v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.01384v1)
- **Published**: 2022-10-04 04:49:08+00:00
- **Updated**: 2022-10-04 04:49:08+00:00
- **Authors**: Thanh Vu, Yanqi Zhou, Chunfeng Wen, Yueqi Li, Jan-Michael Frahm
- **Comment**: WACV 2023. 14 pages, 5 figures
- **Journal**: None
- **Summary**: In this work, we propose a novel and scalable solution to address the challenges of developing efficient dense predictions on edge platforms. Our first key insight is that MultiTask Learning (MTL) and hardware-aware Neural Architecture Search (NAS) can work in synergy to greatly benefit on-device Dense Predictions (DP). Empirical results reveal that the joint learning of the two paradigms is surprisingly effective at improving DP accuracy, achieving superior performance over both the transfer learning of single-task NAS and prior state-of-the-art approaches in MTL, all with just 1/10th of the computation. To the best of our knowledge, our framework, named EDNAS, is the first to successfully leverage the synergistic relationship of NAS and MTL for DP. Our second key insight is that the standard depth training for multi-task DP can cause significant instability and noise to MTL evaluation. Instead, we propose JAReD, an improved, easy-to-adopt Joint Absolute-Relative Depth loss, that reduces up to 88% of the undesired noise while simultaneously boosting accuracy. We conduct extensive evaluations on standard datasets, benchmark against strong baselines and state-of-the-art approaches, as well as provide an analysis of the discovered optimal architectures.



### Bridged Transformer for Vision and Point Cloud 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.01391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01391v1)
- **Published**: 2022-10-04 05:44:22+00:00
- **Updated**: 2022-10-04 05:44:22+00:00
- **Authors**: Yikai Wang, TengQi Ye, Lele Cao, Wenbing Huang, Fuchun Sun, Fengxiang He, Dacheng Tao
- **Comment**: CVPR 2022
- **Journal**: None
- **Summary**: 3D object detection is a crucial research topic in computer vision, which usually uses 3D point clouds as input in conventional setups. Recently, there is a trend of leveraging multiple sources of input data, such as complementing the 3D point cloud with 2D images that often have richer color and fewer noises. However, due to the heterogeneous geometrics of the 2D and 3D representations, it prevents us from applying off-the-shelf neural networks to achieve multimodal fusion. To that end, we propose Bridged Transformer (BrT), an end-to-end architecture for 3D object detection. BrT is simple and effective, which learns to identify 3D and 2D object bounding boxes from both points and image patches. A key element of BrT lies in the utilization of object queries for bridging 3D and 2D spaces, which unifies different sources of data representations in Transformer. We adopt a form of feature aggregation realized by point-to-patch projections which further strengthen the correlations between images and points. Moreover, BrT works seamlessly for fusing the point cloud with multi-view images. We experimentally show that BrT surpasses state-of-the-art methods on SUN RGB-D and ScanNetV2 datasets.



### Streaming Video Analytics On The Edge With Asynchronous Cloud Support
- **Arxiv ID**: http://arxiv.org/abs/2210.01402v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.01402v1)
- **Published**: 2022-10-04 06:22:13+00:00
- **Updated**: 2022-10-04 06:22:13+00:00
- **Authors**: Anurag Ghosh, Srinivasan Iyengar, Stephen Lee, Anuj Rathore, Venkat N Padmanabhan
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Emerging Internet of Things (IoT) and mobile computing applications are expected to support latency-sensitive deep neural network (DNN) workloads. To realize this vision, the Internet is evolving towards an edge-computing architecture, where computing infrastructure is located closer to the end device to help achieve low latency. However, edge computing may have limited resources compared to cloud environments and thus, cannot run large DNN models that often have high accuracy. In this work, we develop REACT, a framework that leverages cloud resources to execute large DNN models with higher accuracy to improve the accuracy of models running on edge devices. To do so, we propose a novel edge-cloud fusion algorithm that fuses edge and cloud predictions, achieving low latency and high accuracy. We extensively evaluate our approach and show that our approach can significantly improve the accuracy compared to baseline approaches. We focus specifically on object detection in videos (applicable in many video analytics scenarios) and show that the fused edge-cloud predictions can outperform the accuracy of edge-only and cloud-only scenarios by as much as 50%. We also show that REACT can achieve good performance across tradeoff points by choosing a wide range of system parameters to satisfy use-case specific constraints, such as limited network bandwidth or GPU cycles.



### Accurate Image Restoration with Attention Retractable Transformer
- **Arxiv ID**: http://arxiv.org/abs/2210.01427v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01427v4)
- **Published**: 2022-10-04 07:35:01+00:00
- **Updated**: 2023-02-03 15:35:33+00:00
- **Authors**: Jiale Zhang, Yulun Zhang, Jinjin Gu, Yongbing Zhang, Linghe Kong, Xin Yuan
- **Comment**: Accepted to ICLR 2023. Code and models are available at
  https://github.com/gladzhang/ART
- **Journal**: None
- **Summary**: Recently, Transformer-based image restoration networks have achieved promising improvements over convolutional neural networks due to parameter-independent global interactions. To lower computational cost, existing works generally limit self-attention computation within non-overlapping windows. However, each group of tokens are always from a dense area of the image. This is considered as a dense attention strategy since the interactions of tokens are restrained in dense regions. Obviously, this strategy could result in restricted receptive fields. To address this issue, we propose Attention Retractable Transformer (ART) for image restoration, which presents both dense and sparse attention modules in the network. The sparse attention module allows tokens from sparse areas to interact and thus provides a wider receptive field. Furthermore, the alternating application of dense and sparse attention modules greatly enhances representation ability of Transformer while providing retractable attention on the input image.We conduct extensive experiments on image super-resolution, denoising, and JPEG compression artifact reduction tasks. Experimental results validate that our proposed ART outperforms state-of-the-art methods on various benchmark datasets both quantitatively and visually. We also provide code and models at https://github.com/gladzhang/ART.



### Non-learning Stereo-aided Depth Completion under Mis-projection via Selective Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/2210.01436v1
- **DOI**: 10.1109/ACCESS.2021.3117710
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01436v1)
- **Published**: 2022-10-04 07:46:56+00:00
- **Updated**: 2022-10-04 07:46:56+00:00
- **Authors**: Yasuhiro Yao, Ryoichi Ishikawa, Shingo Ando, Kana Kurata, Naoki Ito, Jun Shimamura, Takeshi Oishi
- **Comment**: 15 pages, 13 figures
- **Journal**: in IEEE Access, vol. 9, pp. 136674-136686, 2021
- **Summary**: We propose a non-learning depth completion method for a sparse depth map captured using a light detection and ranging (LiDAR) sensor guided by a pair of stereo images. Generally, conventional stereo-aided depth completion methods have two limiations. (i) They assume the given sparse depth map is accurately aligned to the input image, whereas the alignment is difficult to achieve in practice. (ii) They have limited accuracy in the long range because the depth is estimated by pixel disparity. To solve the abovementioned limitations, we propose selective stereo matching (SSM) that searches the most appropriate depth value for each image pixel from its neighborly projected LiDAR points based on an energy minimization framework. This depth selection approach can handle any type of mis-projection. Moreover, SSM has an advantage in terms of long-range depth accuracy because it directly uses the LiDAR measurement rather than the depth acquired from the stereo. SSM is a discrete process; thus, we apply variational smoothing with binary anisotropic diffusion tensor (B-ADT) to generate a continuous depth map while preserving depth discontinuity across object boundaries. Experimentally, compared with the previous state-of-the-art stereo-aided depth completion, the proposed method reduced the mean absolute error (MAE) of the depth estimation to 0.65 times and demonstrated approximately twice more accurate estimation in the long range. Moreover, under various LiDAR-camera calibration errors, the proposed method reduced the depth estimation MAE to 0.34-0.93 times from previous depth completion methods.



### Complementary consistency semi-supervised learning for 3D left atrial image segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.01438v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.01438v5)
- **Published**: 2022-10-04 07:50:28+00:00
- **Updated**: 2023-04-04 13:09:22+00:00
- **Authors**: Hejun Huang, Zuguo Chen, Chaoyang Chen, Ming Lu, Ying Zou
- **Comment**: None
- **Journal**: None
- **Summary**: A network based on complementary consistency training, called CC-Net, has been proposed for semi-supervised left atrium image segmentation. CC-Net efficiently utilizes unlabeled data from the perspective of complementary information to address the problem of limited ability of existing semi-supervised segmentation algorithms to extract information from unlabeled data. The complementary symmetric structure of CC-Net includes a main model and two auxiliary models. The complementary model inter-perturbations between the main and auxiliary models force consistency to form complementary consistency. The complementary information obtained by the two auxiliary models helps the main model to effectively focus on ambiguous areas, while enforcing consistency between the models is advantageous in obtaining decision boundaries with low uncertainty. CC-Net has been validated on two public datasets. In the case of specific proportions of labeled data, compared with current advanced algorithms, CC-Net has the best semi-supervised segmentation performance. Our code is publicly available at https://github.com/Cuthbert-Huang/CC-Net.



### Boosting Few-shot Fine-grained Recognition with Background Suppression and Foreground Alignment
- **Arxiv ID**: http://arxiv.org/abs/2210.01439v2
- **DOI**: 10.1109/TCSVT.2023.3236636
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01439v2)
- **Published**: 2022-10-04 07:54:40+00:00
- **Updated**: 2023-01-19 03:26:18+00:00
- **Authors**: Zican Zha, Hao Tang, Yunlian Sun, Jinhui Tang
- **Comment**: Accepted to IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT) 2023
- **Journal**: None
- **Summary**: Few-shot fine-grained recognition (FS-FGR) aims to recognize novel fine-grained categories with the help of limited available samples. Undoubtedly, this task inherits the main challenges from both few-shot learning and fine-grained recognition. First, the lack of labeled samples makes the learned model easy to overfit. Second, it also suffers from high intra-class variance and low inter-class differences in the datasets. To address this challenging task, we propose a two-stage background suppression and foreground alignment framework, which is composed of a background activation suppression (BAS) module, a foreground object alignment (FOA) module, and a local-to-local (L2L) similarity metric. Specifically, the BAS is introduced to generate a foreground mask for localization to weaken background disturbance and enhance dominative foreground objects. The FOA then reconstructs the feature map of each support sample according to its correction to the query ones, which addresses the problem of misalignment between support-query image pairs. To enable the proposed method to have the ability to capture subtle differences in confused samples, we present a novel L2L similarity metric to further measure the local similarity between a pair of aligned spatial features in the embedding space. What's more, considering that background interference brings poor robustness, we infer the pairwise similarity of feature maps using both the raw image and the refined image. Extensive experiments conducted on multiple popular fine-grained benchmarks demonstrate that our method outperforms the existing state of the art by a large margin. The source codes are available at: https://github.com/CSer-Tang-hao/BSFA-FSFG.



### Low-Light Image Restoration Based on Retina Model using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.01806v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.01806v1)
- **Published**: 2022-10-04 08:14:49+00:00
- **Updated**: 2022-10-04 08:14:49+00:00
- **Authors**: Yurui Ming, Yuanyuan Liang
- **Comment**: None
- **Journal**: None
- **Summary**: We report the possibility of using a simple neural network for effortless restoration of low-light images inspired by the retina model, which mimics the neurophysiological principles and dynamics of various types of optical neurons. The proposed neural network model saves the cost of computational overhead in contrast with traditional signal-processing models, and generates results comparable with complicated deep learning models from the subjective perceptual perspective. This work shows that to directly simulate the functionalities of retinal neurons using neural networks not only avoids the manually seeking for the optimal parameters, but also paves the way to build corresponding artificial versions for certain neurobiological organizations.



### A Novel Light Field Coding Scheme Based on Deep Belief Network & Weighted Binary Images for Additive Layered Displays
- **Arxiv ID**: http://arxiv.org/abs/2210.01447v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.01447v2)
- **Published**: 2022-10-04 08:18:06+00:00
- **Updated**: 2023-04-21 14:59:26+00:00
- **Authors**: Sally Khaidem, Mansi Sharma
- **Comment**: The paper is under consideration at Pattern Recognition Letters
- **Journal**: None
- **Summary**: Light-field displays create an immersive experience by providing binocular depth sensation and motion parallax. Stacking light attenuating layers is one approach to implement a light field display with a broader depth of field, wide viewing angles and high resolution. Due to the transparent holographic optical element (HOE) layers, additive layered displays can be integrated into augmented reality (AR) wearables to overlay virtual objects onto the real world, creating a seamless mixed reality (XR) experience. This paper proposes a novel framework for light field representation and coding that utilizes Deep Belief Network (DBN) and weighted binary images suitable for additive layered displays. The weighted binary representation of layers makes the framework more flexible for adaptive bitrate encoding. The framework effectively captures intrinsic redundancies in the light field data, and thus provides a scalable solution for light field coding suitable for XR display applications. The latent code is encoded by H.265 codec generating a rate-scalable bit-stream. We achieve adaptive bitrate decoding by varying the number of weighted binary images and the H.265 quantization parameter, while maintaining an optimal reconstruction quality. The framework is tested on real and synthetic benchmark datasets, and the results validate the rate-scalable property of the proposed scheme.



### Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2210.01448v3
- **DOI**: 10.1145/3550454.3555435
- **Categories**: **cs.SD**, cs.AI, cs.CV, cs.GR, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2210.01448v3)
- **Published**: 2022-10-04 08:19:06+00:00
- **Updated**: 2023-05-04 12:13:11+00:00
- **Authors**: Tenglong Ao, Qingzhe Gao, Yuke Lou, Baoquan Chen, Libin Liu
- **Comment**: SIGGRAPH Asia 2022 (Journal Track); Project Page:
  https://pku-mocca.github.io/Rhythmic-Gesticulator-Page/
- **Journal**: None
- **Summary**: Automatic synthesis of realistic co-speech gestures is an increasingly important yet challenging task in artificial embodied agent creation. Previous systems mainly focus on generating gestures in an end-to-end manner, which leads to difficulties in mining the clear rhythm and semantics due to the complex yet subtle harmony between speech and gestures. We present a novel co-speech gesture synthesis method that achieves convincing results both on the rhythm and semantics. For the rhythm, our system contains a robust rhythm-based segmentation pipeline to ensure the temporal coherence between the vocalization and gestures explicitly. For the gesture semantics, we devise a mechanism to effectively disentangle both low- and high-level neural embeddings of speech and motion based on linguistic theory. The high-level embedding corresponds to semantics, while the low-level embedding relates to subtle variations. Lastly, we build correspondence between the hierarchical embeddings of the speech and the motion, resulting in rhythm- and semantics-aware gesture synthesis. Evaluations with existing objective metrics, a newly proposed rhythmic metric, and human feedback show that our method outperforms state-of-the-art systems by a clear margin.



### Geo-imagery management and statistical processing in a regional context using Open Data Cube
- **Arxiv ID**: http://arxiv.org/abs/2210.01470v1
- **DOI**: 10.1109/IGARSS47720.2021.9553940
- **Categories**: **cs.CV**, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/2210.01470v1)
- **Published**: 2022-10-04 08:45:56+00:00
- **Updated**: 2022-10-04 08:45:56+00:00
- **Authors**: U. Otamendi, I. Azpiroz, M. Quartulli, I. Olaizola, F. J. Perez, D. Alda, X. Garitano
- **Comment**: 4 pages, 4 figures, Published in 2021 IEEE International Geoscience
  and Remote Sensing Symposium IGARSS
- **Journal**: 2021 IEEE International Geoscience and Remote Sensing Symposium
  IGARSS
- **Summary**: We propose a methodology to manage and process remote sensing and geo-imagery data for non-expert users. The proposed system provides automated data ingestion and manipulation capability for analytical data-driven purposes. In this paper, we describe the technological basis of the proposed method in addition to describing the tool architecture, the inherent data flow, and its operation in a specific use case to provide statistical summaries of Sentinel-2 regions of interest corresponding to the cultivation of polygonal areas located in the Basque Country (ES).



### Double Attention-based Lightweight Network for Plant Pest Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.09956v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.09956v1)
- **Published**: 2022-10-04 09:25:09+00:00
- **Updated**: 2022-10-04 09:25:09+00:00
- **Authors**: Sivasubramaniam Janarthan, Selvarajah Thuseethan, Sutharshan Rajasegarar, John Yearwood
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Timely recognition of plant pests from field images is significant to avoid potential losses of crop yields. Traditional convolutional neural network-based deep learning models demand high computational capability and require large labelled samples for each pest type for training. On the other hand, the existing lightweight network-based approaches suffer in correctly classifying the pests because of common characteristics and high similarity between multiple plant pests. In this work, a novel double attention-based lightweight deep learning architecture is proposed to automatically recognize different plant pests. The lightweight network facilitates faster and small data training while the double attention module increases performance by focusing on the most pertinent information. The proposed approach achieves 96.61%, 99.08% and 91.60% on three variants of two publicly available datasets with 5869, 545 and 500 samples, respectively. Moreover, the comparison results reveal that the proposed approach outperforms existing approaches on both small and large datasets consistently.



### APAUNet: Axis Projection Attention UNet for Small Target in 3D Medical Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.01485v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01485v1)
- **Published**: 2022-10-04 09:28:58+00:00
- **Updated**: 2022-10-04 09:28:58+00:00
- **Authors**: Yuncheng Jiang, Zixun Zhang, Shixi Qin, Yao Guo, Zhen Li, Shuguang Cui
- **Comment**: Accepted by ACCV2022
- **Journal**: None
- **Summary**: In 3D medical image segmentation, small targets segmentation is crucial for diagnosis but still faces challenges. In this paper, we propose the Axis Projection Attention UNet, named APAUNet, for 3D medical image segmentation, especially for small targets. Considering the large proportion of the background in the 3D feature space, we introduce a projection strategy to project the 3D features into three orthogonal 2D planes to capture the contextual attention from different views. In this way, we can filter out the redundant feature information and mitigate the loss of critical information for small lesions in 3D scans. Then we utilize a dimension hybridization strategy to fuse the 3D features with attention from different axes and merge them by a weighted summation to adaptively learn the importance of different perspectives. Finally, in the APA Decoder, we concatenate both high and low resolution features in the 2D projection process, thereby obtaining more precise multi-scale information, which is vital for small lesion segmentation. Quantitative and qualitative experimental results on two public datasets (BTCV and MSD) demonstrate that our proposed APAUNet outperforms the other methods. Concretely, our APAUNet achieves an average dice score of 87.84 on BTCV, 84.48 on MSD-Liver and 69.13 on MSD-Pancreas, and significantly surpass the previous SOTA methods on small targets.



### Enhancing Spatiotemporal Prediction Model using Modular Design and Beyond
- **Arxiv ID**: http://arxiv.org/abs/2210.01500v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.01500v1)
- **Published**: 2022-10-04 10:09:35+00:00
- **Updated**: 2022-10-04 10:09:35+00:00
- **Authors**: Haoyu Pan, Hao Wu, Tan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Predictive learning uses a known state to generate a future state over a period of time. It is a challenging task to predict spatiotemporal sequence because the spatiotemporal sequence varies both in time and space. The mainstream method is to model spatial and temporal structures at the same time using RNN-based or transformer-based architecture, and then generates future data by using learned experience in the way of auto-regressive. The method of learning spatial and temporal features simultaneously brings a lot of parameters to the model, which makes the model difficult to be convergent. In this paper, a modular design is proposed, which decomposes spatiotemporal sequence model into two modules: a spatial encoder-decoder and a predictor. These two modules can extract spatial features and predict future data respectively. The spatial encoder-decoder maps the data into a latent embedding space and generates data from the latent space while the predictor forecasts future embedding from past. By applying the design to the current research and performing experiments on KTH-Action and MovingMNIST datasets, we both improve computational performance and obtain state-of-the-art results.



### How deep convolutional neural networks lose spatial information with training
- **Arxiv ID**: http://arxiv.org/abs/2210.01506v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.01506v2)
- **Published**: 2022-10-04 10:21:03+00:00
- **Updated**: 2022-11-23 11:34:18+00:00
- **Authors**: Umberto M. Tomasini, Leonardo Petrini, Francesco Cagnetta, Matthieu Wyart
- **Comment**: None
- **Journal**: None
- **Summary**: A central question of machine learning is how deep nets manage to learn tasks in high dimensions. An appealing hypothesis is that they achieve this feat by building a representation of the data where information irrelevant to the task is lost. For image datasets, this view is supported by the observation that after (and not before) training, the neural representation becomes less and less sensitive to diffeomorphisms acting on images as the signal propagates through the net. This loss of sensitivity correlates with performance, and surprisingly correlates with a gain of sensitivity to white noise acquired during training. These facts are unexplained, and as we demonstrate still hold when white noise is added to the images of the training set. Here, we (i) show empirically for various architectures that stability to image diffeomorphisms is achieved by both spatial and channel pooling, (ii) introduce a model scale-detection task which reproduces our empirical observations on spatial pooling and (iii) compute analitically how the sensitivity to diffeomorphisms and noise scales with depth due to spatial pooling. The scalings are found to depend on the presence of strides in the net architecture. We find that the increased sensitivity to noise is due to the perturbing noise piling up during pooling, after being rectified by ReLU units.



### CLINICAL: Targeted Active Learning for Imbalanced Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2210.01520v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01520v1)
- **Published**: 2022-10-04 10:57:05+00:00
- **Updated**: 2022-10-04 10:57:05+00:00
- **Authors**: Suraj Kothawade, Atharv Savarkar, Venkat Iyer, Lakshman Tamil, Ganesh Ramakrishnan, Rishabh Iyer
- **Comment**: Accepted to MICCAI 2022 MILLanD Workshop
- **Journal**: None
- **Summary**: Training deep learning models on medical datasets that perform well for all classes is a challenging task. It is often the case that a suboptimal performance is obtained on some classes due to the natural class imbalance issue that comes with medical data. An effective way to tackle this problem is by using targeted active learning, where we iteratively add data points to the training data that belong to the rare classes. However, existing active learning methods are ineffective in targeting rare classes in medical datasets. In this work, we propose Clinical (targeted aCtive Learning for ImbalaNced medICal imAge cLassification) a framework that uses submodular mutual information functions as acquisition functions to mine critical data points from rare classes. We apply our framework to a wide-array of medical imaging datasets on a variety of real-world class imbalance scenarios - namely, binary imbalance and long-tail imbalance. We show that Clinical outperforms the state-of-the-art active learning methods by acquiring a diverse set of data points that belong to the rare classes.



### DIAGNOSE: Avoiding Out-of-distribution Data using Submodular Information Measures
- **Arxiv ID**: http://arxiv.org/abs/2210.01526v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01526v1)
- **Published**: 2022-10-04 11:07:48+00:00
- **Updated**: 2022-10-04 11:07:48+00:00
- **Authors**: Suraj Kothawade, Akshit Srivastava, Venkat Iyer, Ganesh Ramakrishnan, Rishabh Iyer
- **Comment**: Accepted to MICCAI 2022 MILLanD Workshop
- **Journal**: None
- **Summary**: Avoiding out-of-distribution (OOD) data is critical for training supervised machine learning models in the medical imaging domain. Furthermore, obtaining labeled medical data is difficult and expensive since it requires expert annotators like doctors, radiologists, etc. Active learning (AL) is a well-known method to mitigate labeling costs by selecting the most diverse or uncertain samples. However, current AL methods do not work well in the medical imaging domain with OOD data. We propose Diagnose (avoiDing out-of-dIstribution dAta usinG submodular iNfOrmation meaSurEs), a novel active learning framework that can jointly model similarity and dissimilarity, which is crucial in mining in-distribution data and avoiding OOD data at the same time. Particularly, we use a small number of data points as exemplars that represent a query set of in-distribution data points and a private set of OOD data points. We illustrate the generalizability of our framework by evaluating it on a wide variety of real-world OOD scenarios. Our experiments verify the superiority of Diagnose over the state-of-the-art AL methods across multiple domains of medical imaging.



### Integrating pre-processing pipelines in ODC based framework
- **Arxiv ID**: http://arxiv.org/abs/2210.01528v1
- **DOI**: 10.1109/IGARSS46834.2022.9884209
- **Categories**: **cs.CV**, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/2210.01528v1)
- **Published**: 2022-10-04 11:12:09+00:00
- **Updated**: 2022-10-04 11:12:09+00:00
- **Authors**: U. Otamendi, I. Azpiroz, M. Quartulli, I. Olaizola
- **Comment**: 4 pages, 5 figures, IGARSS 2022 - 2022 IEEE International Geoscience
  and Remote Sensing Symposium
- **Journal**: 2022 IEEE International Geoscience and Remote Sensing Symposium
- **Summary**: Using on-demand processing pipelines to generate virtual geospatial products is beneficial to optimizing resource management and decreasing processing requirements and data storage space. Additionally, pre-processed products improve data quality for data-driven analytical algorithms, such as machine learning or deep learning models. This paper proposes a method to integrate virtual products based on integrating open-source processing pipelines. In order to validate and evaluate the functioning of this approach, we have integrated it into a geo-imagery management framework based on Open Data Cube (ODC). To validate the methodology, we have performed three experiments developing on-demand processing pipelines using multi-sensor remote sensing data, for instance, Sentinel-1 and Sentinel-2. These pipelines are integrated using open-source processing frameworks.



### Analysis of the performance of U-Net neural networks for the segmentation of living cells
- **Arxiv ID**: http://arxiv.org/abs/2210.01538v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.01538v1)
- **Published**: 2022-10-04 11:48:59+00:00
- **Updated**: 2022-10-04 11:48:59+00:00
- **Authors**: André O. Françani
- **Comment**: None
- **Journal**: None
- **Summary**: The automated analysis of microscopy images is a challenge in the context of single-cell tracking and quantification. This work has as goals the study of the performance of deep learning for segmenting microscopy images and the improvement of the previously available pipeline for tracking single cells. Deep learning techniques, mainly convolutional neural networks, have been applied to cell segmentation problems and have shown high accuracy and fast performance. To perform the image segmentation, an analysis of hyperparameters was done in order to implement a convolutional neural network with U-Net architecture. Furthermore, different models were built in order to optimize the size of the network and the number of learnable parameters. The trained network is then used in the pipeline that localizes the traps in a microfluidic device, performs the image segmentation on trap images, and evaluates the fluorescence intensity and the area of single cells over time. The tracking of the cells during an experiment is performed by image processing algorithms, such as centroid estimation and watershed. Finally, with all improvements in the neural network to segment single cells and in the pipeline, quasi-real-time image analysis was enabled, where 6.20GB of data was processed in 4 minutes.



### VICRegL: Self-Supervised Learning of Local Visual Features
- **Arxiv ID**: http://arxiv.org/abs/2210.01571v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.01571v1)
- **Published**: 2022-10-04 12:54:25+00:00
- **Updated**: 2022-10-04 12:54:25+00:00
- **Authors**: Adrien Bardes, Jean Ponce, Yann LeCun
- **Comment**: Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: Most recent self-supervised methods for learning image representations focus on either producing a global feature with invariance properties, or producing a set of local features. The former works best for classification tasks while the latter is best for detection and segmentation tasks. This paper explores the fundamental trade-off between learning local and global features. A new method called VICRegL is proposed that learns good global and local features simultaneously, yielding excellent performance on detection and segmentation tasks while maintaining good performance on classification tasks. Concretely, two identical branches of a standard convolutional net architecture are fed two differently distorted versions of the same image. The VICReg criterion is applied to pairs of global feature vectors. Simultaneously, the VICReg criterion is applied to pairs of local feature vectors occurring before the last pooling layer. Two local feature vectors are attracted to each other if their l2-distance is below a threshold or if their relative locations are consistent with a known geometric transformation between the two input images. We demonstrate strong performance on linear classification and segmentation transfer tasks. Code and pretrained models are publicly available at: https://github.com/facebookresearch/VICRegL



### Cooperative Self-Training for Multi-Target Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.01578v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01578v1)
- **Published**: 2022-10-04 13:03:17+00:00
- **Updated**: 2022-10-04 13:03:17+00:00
- **Authors**: Yangsong Zhang, Subhankar Roy, Hongtao Lu, Elisa Ricci, Stéphane Lathuilière
- **Comment**: Accepted at WACV 2023
- **Journal**: None
- **Summary**: In this work we address multi-target domain adaptation (MTDA) in semantic segmentation, which consists in adapting a single model from an annotated source dataset to multiple unannotated target datasets that differ in their underlying data distributions. To address MTDA, we propose a self-training strategy that employs pseudo-labels to induce cooperation among multiple domain-specific classifiers. We employ feature stylization as an efficient way to generate image views that forms an integral part of self-training. Additionally, to prevent the network from overfitting to noisy pseudo-labels, we devise a rectification strategy that leverages the predictions from different classifiers to estimate the quality of pseudo-labels. Our extensive experiments on numerous settings, based on four different semantic segmentation datasets, validate the effectiveness of the proposed self-training strategy and show that our method outperforms state-of-the-art MTDA approaches. Code available at: https://github.com/Mael-zys/CoaST



### FRIDA: Fisheye Re-Identification Dataset with Annotations
- **Arxiv ID**: http://arxiv.org/abs/2210.01582v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01582v2)
- **Published**: 2022-10-04 13:08:47+00:00
- **Updated**: 2022-10-19 22:19:00+00:00
- **Authors**: Mertcan Cokbas, John Bolognino, Janusz Konrad, Prakash Ishwar
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Person re-identification (PRID) from side-mounted rectilinear-lens cameras is a well-studied problem. On the other hand, PRID from overhead fisheye cameras is new and largely unstudied, primarily due to the lack of suitable image datasets. To fill this void, we introduce the "Fisheye Re-IDentification Dataset with Annotations" (FRIDA), with 240k+ bounding-box annotations of people, captured by 3 time-synchronized, ceiling-mounted fisheye cameras in a large indoor space. Due to a field-of-view overlap, PRID in this case differs from a typical PRID problem, which we discuss in depth. We also evaluate the performance of 10 state-of-the-art PRID algorithms on FRIDA. We show that for 6 CNN-based algorithms, training on FRIDA boosts the performance by up to 11.64% points in mAP compared to training on a common rectilinear-camera PRID dataset.



### How Image Generation Helps Visible-to-Infrared Person Re-Identification?
- **Arxiv ID**: http://arxiv.org/abs/2210.01585v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01585v2)
- **Published**: 2022-10-04 13:09:29+00:00
- **Updated**: 2022-10-25 01:33:14+00:00
- **Authors**: Honghu Pan, Yongyong Chen, Yunqi He, Xin Li, Zhenyu He
- **Comment**: Submitted to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Compared to visible-to-visible (V2V) person re-identification (ReID), the visible-to-infrared (V2I) person ReID task is more challenging due to the lack of sufficient training samples and the large cross-modality discrepancy.   To this end, we propose Flow2Flow, a unified framework that could jointly achieve training sample expansion and cross-modality image generation for V2I person ReID.   Specifically, Flow2Flow learns bijective transformations from both the visible image domain and the infrared domain to a shared isotropic Gaussian domain with an invertible visible flow-based generator and an infrared one, respectively.   With Flow2Flow, we are able to generate pseudo training samples by the transformation from latent Gaussian noises to visible or infrared images, and generate cross-modality images by transformations from existing-modality images to latent Gaussian noises to missing-modality images.   For the purpose of identity alignment and modality alignment of generated images, we develop adversarial training strategies to train Flow2Flow.   Specifically, we design an image encoder and a modality discriminator for each modality.   The image encoder encourages the generated images to be similar to real images of the same identity via identity adversarial training, and the modality discriminator makes the generated images modal-indistinguishable from real images via modality adversarial training.   Experimental results on SYSU-MM01 and RegDB demonstrate that both training sample expansion and cross-modality image generation can significantly improve V2I ReID accuracy.



### Cross-Geography Generalization of Machine Learning Methods for Classification of Flooded Regions in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2210.01588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01588v1)
- **Published**: 2022-10-04 13:11:44+00:00
- **Updated**: 2022-10-04 13:11:44+00:00
- **Authors**: Sushant Lenka, Pratyush Kerhalkar, Pranav Shetty, Harsh Gupta, Bhavam Vidyarthi, Ujjwal Verma
- **Comment**: None
- **Journal**: None
- **Summary**: Identification of regions affected by floods is a crucial piece of information required for better planning and management of post-disaster relief and rescue efforts. Traditionally, remote sensing images are analysed to identify the extent of damage caused by flooding. The data acquired from sensors onboard earth observation satellites are analyzed to detect the flooded regions, which can be affected by low spatial and temporal resolution. However, in recent years, the images acquired from Unmanned Aerial Vehicles (UAVs) have also been utilized to assess post-disaster damage. Indeed, a UAV based platform can be rapidly deployed with a customized flight plan and minimum dependence on the ground infrastructure. This work proposes two approaches for identifying flooded regions in UAV aerial images. The first approach utilizes texture-based unsupervised segmentation to detect flooded areas, while the second uses an artificial neural network on the texture features to classify images as flooded and non-flooded. Unlike the existing works where the models are trained and tested on images of the same geographical regions, this work studies the performance of the proposed model in identifying flooded regions across geographical regions. An F1-score of 0.89 is obtained using the proposed segmentation-based approach which is higher than existing classifiers. The robustness of the proposed approach demonstrates that it can be utilized to identify flooded regions of any region with minimum or no user intervention.



### FreDSNet: Joint Monocular Depth and Semantic Segmentation with Fast Fourier Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2210.01595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01595v1)
- **Published**: 2022-10-04 13:18:15+00:00
- **Updated**: 2022-10-04 13:18:15+00:00
- **Authors**: Bruno Berenguel-Baeta, Jesus Bermudez-Cameo, Jose J. Guerrero
- **Comment**: 7 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: In this work we present FreDSNet, a deep learning solution which obtains semantic 3D understanding of indoor environments from single panoramas. Omnidirectional images reveal task-specific advantages when addressing scene understanding problems due to the 360-degree contextual information about the entire environment they provide. However, the inherent characteristics of the omnidirectional images add additional problems to obtain an accurate detection and segmentation of objects or a good depth estimation. To overcome these problems, we exploit convolutions in the frequential domain obtaining a wider receptive field in each convolutional layer. These convolutions allow to leverage the whole context information from omnidirectional images. FreDSNet is the first network that jointly provides monocular depth estimation and semantic segmentation from a single panoramic image exploiting fast Fourier convolutions. Our experiments show that FreDSNet has similar performance as specific state of the art methods for semantic segmentation and depth estimation. FreDSNet code is publicly available in https://github.com/Sbrunoberenguel/FreDSNet



### ROAD-R: The Autonomous Driving Dataset with Logical Requirements
- **Arxiv ID**: http://arxiv.org/abs/2210.01597v2
- **DOI**: 10.1007/s10994-023-06322-z
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.01597v2)
- **Published**: 2022-10-04 13:22:19+00:00
- **Updated**: 2022-10-05 11:42:42+00:00
- **Authors**: Eleonora Giunchiglia, Mihaela Cătălina Stoian, Salman Khan, Fabio Cuzzolin, Thomas Lukasiewicz
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks have proven to be very powerful at computer vision tasks. However, they often exhibit unexpected behaviours, violating known requirements expressing background knowledge. This calls for models (i) able to learn from the requirements, and (ii) guaranteed to be compliant with the requirements themselves. Unfortunately, the development of such models is hampered by the lack of datasets equipped with formally specified requirements. In this paper, we introduce the ROad event Awareness Dataset with logical Requirements (ROAD-R), the first publicly available dataset for autonomous driving with requirements expressed as logical constraints. Given ROAD-R, we show that current state-of-the-art models often violate its logical constraints, and that it is possible to exploit them to create models that (i) have a better performance, and (ii) are guaranteed to be compliant with the requirements themselves.



### Positive Pair Distillation Considered Harmful: Continual Meta Metric Learning for Lifelong Object Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2210.01600v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01600v1)
- **Published**: 2022-10-04 13:26:37+00:00
- **Updated**: 2022-10-04 13:26:37+00:00
- **Authors**: Kai Wang, Chenshen Wu, Andy Bagdanov, Xialei Liu, Shiqi Yang, Shangling Jui, Joost van de Weijer
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: Lifelong object re-identification incrementally learns from a stream of re-identification tasks. The objective is to learn a representation that can be applied to all tasks and that generalizes to previously unseen re-identification tasks. The main challenge is that at inference time the representation must generalize to previously unseen identities. To address this problem, we apply continual meta metric learning to lifelong object re-identification. To prevent forgetting of previous tasks, we use knowledge distillation and explore the roles of positive and negative pairs. Based on our observation that the distillation and metric losses are antagonistic, we propose to remove positive pairs from distillation to robustify model updates. Our method, called Distillation without Positive Pairs (DwoPP), is evaluated on extensive intra-domain experiments on person and vehicle re-identification datasets, as well as inter-domain experiments on the LReID benchmark. Our experiments demonstrate that DwoPP significantly outperforms the state-of-the-art. The code is here: https://github.com/wangkai930418/DwoPP_code



### Self-improving Multiplane-to-layer Images for Novel View Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2210.01602v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2210.01602v1)
- **Published**: 2022-10-04 13:27:14+00:00
- **Updated**: 2022-10-04 13:27:14+00:00
- **Authors**: Pavel Solovev, Taras Khakhulin, Denis Korzhenkov
- **Comment**: Accepted for WACV 2023
- **Journal**: None
- **Summary**: We present a new method for lightweight novel-view synthesis that generalizes to an arbitrary forward-facing scene. Recent approaches are computationally expensive, require per-scene optimization, or produce a memory-expensive representation. We start by representing the scene with a set of fronto-parallel semitransparent planes and afterward convert them to deformable layers in an end-to-end manner. Additionally, we employ a feed-forward refinement procedure that corrects the estimated representation by aggregating information from input views. Our method does not require fine-tuning when a new scene is processed and can handle an arbitrary number of views without restrictions. Experimental results show that our approach surpasses recent models in terms of common metrics and human evaluation, with the noticeable advantage in inference speed and compactness of the inferred layered geometry, see https://samsunglabs.github.io/MLI



### Neural-Symbolic Recursive Machine for Systematic Generalization
- **Arxiv ID**: http://arxiv.org/abs/2210.01603v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.01603v1)
- **Published**: 2022-10-04 13:27:38+00:00
- **Updated**: 2022-10-04 13:27:38+00:00
- **Authors**: Qing Li, Yixin Zhu, Yitao Liang, Ying Nian Wu, Song-Chun Zhu, Siyuan Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the tremendous success, existing machine learning models still fall short of human-like systematic generalization -- learning compositional rules from limited data and applying them to unseen combinations in various domains. We propose Neural-Symbolic Recursive Machine (NSR) to tackle this deficiency. The core representation of NSR is a Grounded Symbol System (GSS) with combinatorial syntax and semantics, which entirely emerges from training data. Akin to the neuroscience studies suggesting separate brain systems for perceptual, syntactic, and semantic processing, NSR implements analogous separate modules of neural perception, syntactic parsing, and semantic reasoning, which are jointly learned by a deduction-abduction algorithm. We prove that NSR is expressive enough to model various sequence-to-sequence tasks. Superior systematic generalization is achieved via the inductive biases of equivariance and recursiveness embedded in NSR. In experiments, NSR achieves state-of-the-art performance in three benchmarks from different domains: SCAN for semantic parsing, PCFG for string manipulation, and HINT for arithmetic reasoning. Specifically, NSR achieves 100% generalization accuracy on SCAN and PCFG and outperforms state-of-the-art models on HINT by about 23%. Our NSR demonstrates stronger generalization than pure neural networks due to its symbolic representation and inductive biases. NSR also demonstrates better transferability than existing neural-symbolic approaches due to less domain-specific knowledge required.



### A Generative Shape Compositional Framework: Towards Representative Populations of Virtual Heart Chimaeras
- **Arxiv ID**: http://arxiv.org/abs/2210.01607v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.01607v1)
- **Published**: 2022-10-04 13:36:52+00:00
- **Updated**: 2022-10-04 13:36:52+00:00
- **Authors**: Haoran Dou, Seppo Virtanen, Nishant Ravikumar, Alejandro F. Frangi
- **Comment**: 15 pages, 4 figures
- **Journal**: None
- **Summary**: Generating virtual populations of anatomy that capture sufficient variability while remaining plausible is essential for conducting in-silico trials of medical devices. However, not all anatomical shapes of interest are always available for each individual in a population. Hence, missing/partially-overlapping anatomical information is often available across individuals in a population. We introduce a generative shape model for complex anatomical structures, learnable from datasets of unpaired datasets. The proposed generative model can synthesise complete whole complex shape assemblies coined virtual chimaeras, as opposed to natural human chimaeras. We applied this framework to build virtual chimaeras from databases of whole-heart shape assemblies that each contribute samples for heart substructures. Specifically, we propose a generative shape compositional framework which comprises two components - a part-aware generative shape model which captures the variability in shape observed for each structure of interest in the training population; and a spatial composition network which assembles/composes the structures synthesised by the former into multi-part shape assemblies (viz. virtual chimaeras). We also propose a novel self supervised learning scheme that enables the spatial composition network to be trained with partially overlapping data and weak labels. We trained and validated our approach using shapes of cardiac structures derived from cardiac magnetic resonance images available in the UK Biobank. Our approach significantly outperforms a PCA-based shape model (trained with complete data) in terms of generalisability and specificity. This demonstrates the superiority of the proposed approach as the synthesised cardiac virtual populations are more plausible and capture a greater degree of variability in shape than those generated by the PCA-based shape model.



### PlaneDepth: Self-supervised Depth Estimation via Orthogonal Planes
- **Arxiv ID**: http://arxiv.org/abs/2210.01612v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01612v3)
- **Published**: 2022-10-04 13:51:59+00:00
- **Updated**: 2023-03-28 05:06:59+00:00
- **Authors**: Ruoyu Wang, Zehao Yu, Shenghua Gao
- **Comment**: Accepted by CVPR 2023. Code and models are available at:
  https://github.com/svip-lab/PlaneDepth
- **Journal**: None
- **Summary**: Multiple near frontal-parallel planes based depth representation demonstrated impressive results in self-supervised monocular depth estimation (MDE). Whereas, such a representation would cause the discontinuity of the ground as it is perpendicular to the frontal-parallel planes, which is detrimental to the identification of drivable space in autonomous driving. In this paper, we propose the PlaneDepth, a novel orthogonal planes based presentation, including vertical planes and ground planes. PlaneDepth estimates the depth distribution using a Laplacian Mixture Model based on orthogonal planes for an input image. These planes are used to synthesize a reference view to provide the self-supervision signal. Further, we find that the widely used resizing and cropping data augmentation breaks the orthogonality assumptions, leading to inferior plane predictions. We address this problem by explicitly constructing the resizing cropping transformation to rectify the predefined planes and predicted camera pose. Moreover, we propose an augmented self-distillation loss supervised with a bilateral occlusion mask to boost the robustness of orthogonal planes representation for occlusions. Thanks to our orthogonal planes representation, we can extract the ground plane in an unsupervised manner, which is important for autonomous driving. Extensive experiments on the KITTI dataset demonstrate the effectiveness and efficiency of our method. The code is available at https://github.com/svip-lab/PlaneDepth.



### On Background Bias in Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.01615v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2210.01615v1)
- **Published**: 2022-10-04 13:57:39+00:00
- **Updated**: 2022-10-04 13:57:39+00:00
- **Authors**: Konstantin Kobs, Andreas Hotho
- **Comment**: To be published at ICMV 2022
- **Journal**: None
- **Summary**: Deep Metric Learning trains a neural network to map input images to a lower-dimensional embedding space such that similar images are closer together than dissimilar images. When used for item retrieval, a query image is embedded using the trained model and the closest items from a database storing their respective embeddings are returned as the most similar items for the query. Especially in product retrieval, where a user searches for a certain product by taking a photo of it, the image background is usually not important and thus should not influence the embedding process. Ideally, the retrieval process always returns fitting items for the photographed object, regardless of the environment the photo was taken in. In this paper, we analyze the influence of the image background on Deep Metric Learning models by utilizing five common loss functions and three common datasets. We find that Deep Metric Learning networks are prone to so-called background bias, which can lead to a severe decrease in retrieval performance when changing the image background during inference. We also show that replacing the background of images during training with random background images alleviates this issue. Since we use an automatic background removal method to do this background replacement, no additional manual labeling work and model changes are required while inference time stays the same. Qualitative and quantitative analyses, for which we introduce a new evaluation metric, confirm that models trained with replaced backgrounds attend more to the main object in the image, benefitting item retrieval systems.



### SelfNeRF: Fast Training NeRF for Human from Monocular Self-rotating Video
- **Arxiv ID**: http://arxiv.org/abs/2210.01651v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2210.01651v1)
- **Published**: 2022-10-04 14:54:40+00:00
- **Updated**: 2022-10-04 14:54:40+00:00
- **Authors**: Bo Peng, Jun Hu, Jingtao Zhou, Juyong Zhang
- **Comment**: Project page: https://ustc3dv.github.io/SelfNeRF
- **Journal**: None
- **Summary**: In this paper, we propose SelfNeRF, an efficient neural radiance field based novel view synthesis method for human performance. Given monocular self-rotating videos of human performers, SelfNeRF can train from scratch and achieve high-fidelity results in about twenty minutes. Some recent works have utilized the neural radiance field for dynamic human reconstruction. However, most of these methods need multi-view inputs and require hours of training, making it still difficult for practical use. To address this challenging problem, we introduce a surface-relative representation based on multi-resolution hash encoding that can greatly improve the training speed and aggregate inter-frame information. Extensive experimental results on several different datasets demonstrate the effectiveness and efficiency of SelfNeRF to challenging monocular videos.



### Robust Target Training for Multi-Source Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2210.01676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01676v1)
- **Published**: 2022-10-04 15:20:01+00:00
- **Updated**: 2022-10-04 15:20:01+00:00
- **Authors**: Zhongying Deng, Da Li, Yi-Zhe Song, Tao Xiang
- **Comment**: Accepted to BMVC 2022
- **Journal**: None
- **Summary**: Given multiple labeled source domains and a single target domain, most existing multi-source domain adaptation (MSDA) models are trained on data from all domains jointly in one step. Such an one-step approach limits their ability to adapt to the target domain. This is because the training set is dominated by the more numerous and labeled source domain data. The source-domain-bias can potentially be alleviated by introducing a second training step, where the model is fine-tuned with the unlabeled target domain data only using pseudo labels as supervision. However, the pseudo labels are inevitably noisy and when used unchecked can negatively impact the model performance. To address this problem, we propose a novel Bi-level Optimization based Robust Target Training (BORT$^2$) method for MSDA. Given any existing fully-trained one-step MSDA model, BORT$^2$ turns it to a labeling function to generate pseudo-labels for the target data and trains a target model using pseudo-labeled target data only. Crucially, the target model is a stochastic CNN which is designed to be intrinsically robust against label noise generated by the labeling function. Such a stochastic CNN models each target instance feature as a Gaussian distribution with an entropy maximization regularizer deployed to measure the label uncertainty, which is further exploited to alleviate the negative impact of noisy pseudo labels. Training the labeling function and the target model poses a nested bi-level optimization problem, for which we formulate an elegant solution based on implicit differentiation. Extensive experiments demonstrate that our proposed method achieves the state of the art performance on three MSDA benchmarks, including the large-scale DomainNet dataset. Our code will be available at \url{https://github.com/Zhongying-Deng/BORT2}



### CFL-Net: Image Forgery Localization Using Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.02182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.02182v1)
- **Published**: 2022-10-04 15:31:30+00:00
- **Updated**: 2022-10-04 15:31:30+00:00
- **Authors**: Fahim Faisal Niloy, Kishor Kumar Bhaumik, Simon S. Woo
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: Conventional forgery localizing methods usually rely on different forgery footprints such as JPEG artifacts, edge inconsistency, camera noise, etc., with cross-entropy loss to locate manipulated regions. However, these methods have the disadvantage of over-fitting and focusing on only a few specific forgery footprints. On the other hand, real-life manipulated images are generated via a wide variety of forgery operations and thus, leave behind a wide variety of forgery footprints. Therefore, we need a more general approach for image forgery localization that can work well on a variety of forgery conditions. A key assumption in underlying forged region localization is that there remains a difference of feature distribution between untampered and manipulated regions in each forged image sample, irrespective of the forgery type. In this paper, we aim to leverage this difference of feature distribution to aid in image forgery localization. Specifically, we use contrastive loss to learn mapping into a feature space where the features between untampered and manipulated regions are well-separated for each image. Also, our method has the advantage of localizing manipulated region without requiring any prior knowledge or assumption about the forgery type. We demonstrate that our work outperforms several existing methods on three benchmark image manipulation datasets. Code is available at https://github.com/niloy193/CFLNet.



### Robustness Certification of Visual Perception Models via Camera Motion Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2210.04625v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.04625v2)
- **Published**: 2022-10-04 15:31:57+00:00
- **Updated**: 2022-11-13 05:01:27+00:00
- **Authors**: Hanjiang Hu, Zuxin Liu, Linyi Li, Jiacheng Zhu, Ding Zhao
- **Comment**: CoRL 2022 camera-ready version, 21 pages, 7 figures, 8 tables
- **Journal**: None
- **Summary**: A vast literature shows that the learning-based visual perception model is sensitive to adversarial noises, but few works consider the robustness of robotic perception models under widely-existing camera motion perturbations. To this end, we study the robustness of the visual perception model under camera motion perturbations to investigate the influence of camera motion on robotic perception. Specifically, we propose a motion smoothing technique for arbitrary image classification models, whose robustness under camera motion perturbations could be certified. The proposed robustness certification framework based on camera motion smoothing provides tight and scalable robustness guarantees for visual perception modules so that they are applicable to wide robotic applications. As far as we are aware, this is the first work to provide robustness certification for the deep perception module against camera motions, which improves the trustworthiness of robotic perception. A realistic indoor robotic dataset with a dense point cloud map for the entire room, MetaRoom, is introduced for the challenging certifiable robust perception task. We conduct extensive experiments to validate the certification approach via motion smoothing against camera motion perturbations. Our framework guarantees the certified accuracy of 81.7% against camera translation perturbation along depth direction within -0.1m ~ 0.1m. We also validate the effectiveness of our method on the real-world robot by conducting hardware experiments on the robotic arm with an eye-in-hand camera. The code is available at https://github.com/HanjiangHu/camera-motion-smoothing.



### Deep Learning-based Facial Appearance Simulation Driven by Surgically Planned Craniomaxillofacial Bony Movement
- **Arxiv ID**: http://arxiv.org/abs/2210.01685v1
- **DOI**: 10.1007/978-3-031-16449-1_54
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01685v1)
- **Published**: 2022-10-04 15:33:01+00:00
- **Updated**: 2022-10-04 15:33:01+00:00
- **Authors**: Xi Fang, Daeseung Kim, Xuanang Xu, Tianshu Kuang, Hannah H. Deng, Joshua C. Barber, Nathan Lampen, Jaime Gateno, Michael A. K. Liebschner, James J. Xia, Pingkun Yan
- **Comment**: MICCAI 2022 Young Scientist Publication Award
- **Journal**: None
- **Summary**: Simulating facial appearance change following bony movement is a critical step in orthognathic surgical planning for patients with jaw deformities. Conventional biomechanics-based methods such as the finite-element method (FEM) are labor intensive and computationally inefficient. Deep learning-based approaches can be promising alternatives due to their high computational efficiency and strong modeling capability. However, the existing deep learning-based method ignores the physical correspondence between facial soft tissue and bony segments and thus is significantly less accurate compared to FEM. In this work, we propose an Attentive Correspondence assisted Movement Transformation network (ACMT-Net) to estimate the facial appearance by transforming the bony movement to facial soft tissue through a point-to-point attentive correspondence matrix. Experimental results on patients with jaw deformity show that our proposed method can achieve comparable facial change prediction accuracy compared with the state-of-the-art FEM-based approach with significantly improved computational efficiency.



### Vision-based Warning System for Maintenance Personnel on Short-Term Roadwork Site
- **Arxiv ID**: http://arxiv.org/abs/2210.01689v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01689v2)
- **Published**: 2022-10-04 15:37:51+00:00
- **Updated**: 2022-10-20 11:37:49+00:00
- **Authors**: Xiao Ni, Walpola Layantha Perera, Carsten Kühnel, Christian Vollrath
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a vision-based warning system for the maintenance personnel working on short-term construction sites. Traditional solutions use passive protection, like setting up traffic cones, safety beacons, or even nothing. However, such methods cannot function as physical safety barriers to separate working areas from used lanes. In contrast, our system provides active protection, leveraging acoustic and visual warning signals to help road workers be cautious of approaching vehicles before they pass the working area. To decrease too many warnings to relieve a disturbance of road workers, we implemented our traffic flow check algorithm, by which about 80% of the useless notices can be filtered. We conduct the evaluations in laboratory conditions and the real world, proving our system's applicability and reliability.



### HandFlow: Quantifying View-Dependent 3D Ambiguity in Two-Hand Reconstruction with Normalizing Flow
- **Arxiv ID**: http://arxiv.org/abs/2210.01692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01692v1)
- **Published**: 2022-10-04 15:42:22+00:00
- **Updated**: 2022-10-04 15:42:22+00:00
- **Authors**: Jiayi Wang, Diogo Luvizon, Franziska Mueller, Florian Bernard, Adam Kortylewski, Dan Casas, Christian Theobalt
- **Comment**: VMV 2022 - Symposium on Vision, Modeling, and Visualization
- **Journal**: None
- **Summary**: Reconstructing two-hand interactions from a single image is a challenging problem due to ambiguities that stem from projective geometry and heavy occlusions. Existing methods are designed to estimate only a single pose, despite the fact that there exist other valid reconstructions that fit the image evidence equally well. In this paper we propose to address this issue by explicitly modeling the distribution of plausible reconstructions in a conditional normalizing flow framework. This allows us to directly supervise the posterior distribution through a novel determinant magnitude regularization, which is key to varied 3D hand pose samples that project well into the input image. We also demonstrate that metrics commonly used to assess reconstruction quality are insufficient to evaluate pose predictions under such severe ambiguity. To address this, we release the first dataset with multiple plausible annotations per image called MultiHands. The additional annotations enable us to evaluate the estimated distribution using the maximum mean discrepancy metric. Through this, we demonstrate the quality of our probabilistic reconstruction and show that explicit ambiguity modeling is better-suited for this challenging problem.



### Conquering the Communication Constraints to Enable Large Pre-Trained Models in Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.01708v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.01708v2)
- **Published**: 2022-10-04 16:08:54+00:00
- **Updated**: 2022-11-23 03:57:02+00:00
- **Authors**: Guangyu Sun, Matias Mendieta, Taojiannan Yang, Chen Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Federated learning (FL) has emerged as a promising paradigm for enabling the collaborative training of models without centralized access to the raw data on local devices. In the typical FL paradigm (e.g., FedAvg), model weights are sent to and from the server each round to participating clients. Recently, the use of small pre-trained models has been shown effective in federated learning optimization and improving convergence. However, recent state-of-the-art pre-trained models are getting more capable but also have more parameters. In conventional FL, sharing the enormous model weights can quickly put a massive communication burden on the system, especially if more capable models are employed. Can we find a solution to enable those strong and readily-available pre-trained models in FL to achieve excellent performance while simultaneously reducing the communication burden? To this end, we investigate the use of parameter-efficient fine-tuning in federated learning and thus introduce a new framework: FedPEFT. Specifically, we systemically evaluate the performance of FedPEFT across a variety of client stability, data distribution, and differential privacy settings. By only locally tuning and globally sharing a small portion of the model weights, significant reductions in the total communication overhead can be achieved while maintaining competitive or even better performance in a wide range of federated learning scenarios, providing insight into a new paradigm for practical and effective federated systems.



### An Improved Multi-State Constraint Kalman Filter for Visual-Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/2210.08117v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.08117v1)
- **Published**: 2022-10-04 16:10:39+00:00
- **Updated**: 2022-10-04 16:10:39+00:00
- **Authors**: M. R. Abdollahi, Seid H. Pourtakdoust, M. H. Yoosefian Nooshabadi, H. N. Pishkenari
- **Comment**: None
- **Journal**: None
- **Summary**: Fast pose estimation (PE) is of vital importance for successful mission performance of agile autonomous robots. Global Positioning Systems such as GPS and GNSS have been typically used in fusion with Inertial Navigation Systems (INS) for PE. However, the low update rate and lack of proper signals make their utility impractical for indoor and urban applications. On the other hand, Visual-Inertial Odometry (VIO) is gaining popularity as a practical alternative for GNSS/INS systems in GPS-denied environments. Among the many VIO-based methods, the Multi-State Constraint Kalman Filter (MSCKF) has received a greater attention due to its robustness, speed and accuracy. To this end, the high computational cost associated with image processing for real-time implementation of MSCKF on resource-constrained vehicles is still a challenging ongoing research. In this paper, an enhanced version of the MSCKF is proposed. To this aim, different feature marginalization and state pruning strategies are suggested that result in a much faster algorithm. The proposed algorithm is tested both on an open-source dataset and in real-world experiments for validation. It is demonstrated that the proposed Fast-MSCKF (FMSCKF) is about six times faster and at least 20% more accurate in final position estimation than the standard MSCKF algorithm.



### Anatomically constrained CT image translation for heterogeneous blood vessel segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.01713v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.01713v1)
- **Published**: 2022-10-04 16:14:49+00:00
- **Updated**: 2022-10-04 16:14:49+00:00
- **Authors**: Giammarco La Barbera, Haithem Boussaid, Francesco Maso, Sabine Sarnacki, Laurence Rouet, Pietro Gori, Isabelle Bloch
- **Comment**: Accepted at BMVC 2022
- **Journal**: None
- **Summary**: Anatomical structures such as blood vessels in contrast-enhanced CT (ceCT) images can be challenging to segment due to the variability in contrast medium diffusion. The combined use of ceCT and contrast-free (CT) CT images can improve the segmentation performances, but at the cost of a double radiation exposure. To limit the radiation dose, generative models could be used to synthesize one modality, instead of acquiring it. The CycleGAN approach has recently attracted particular attention because it alleviates the need for paired data that are difficult to obtain. Despite the great performances demonstrated in the literature, limitations still remain when dealing with 3D volumes generated slice by slice from unpaired datasets with different fields of view. We present an extension of CycleGAN to generate high fidelity images, with good structural consistency, in this context. We leverage anatomical constraints and automatic region of interest selection by adapting the Self-Supervised Body Regressor. These constraints enforce anatomical consistency and allow feeding anatomically-paired input images to the algorithm. Results show qualitative and quantitative improvements, compared to stateof-the-art methods, on the translation task between ceCT and CT images (and vice versa).



### MBW: Multi-view Bootstrapping in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2210.01721v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.01721v1)
- **Published**: 2022-10-04 16:27:54+00:00
- **Updated**: 2022-10-04 16:27:54+00:00
- **Authors**: Mosam Dabhi, Chaoyang Wang, Tim Clifford, Laszlo Attila Jeni, Ian R. Fasel, Simon Lucey
- **Comment**: NeurIPS 2022 conference. Project webpage and code:
  https://github.com/mosamdabhi/MBW
- **Journal**: None
- **Summary**: Labeling articulated objects in unconstrained settings have a wide variety of applications including entertainment, neuroscience, psychology, ethology, and many fields of medicine. Large offline labeled datasets do not exist for all but the most common articulated object categories (e.g., humans). Hand labeling these landmarks within a video sequence is a laborious task. Learned landmark detectors can help, but can be error-prone when trained from only a few examples. Multi-camera systems that train fine-grained detectors have shown significant promise in detecting such errors, allowing for self-supervised solutions that only need a small percentage of the video sequence to be hand-labeled. The approach, however, is based on calibrated cameras and rigid geometry, making it expensive, difficult to manage, and impractical in real-world scenarios. In this paper, we address these bottlenecks by combining a non-rigid 3D neural prior with deep flow to obtain high-fidelity landmark estimates from videos with only two or three uncalibrated, handheld cameras. With just a few annotations (representing 1-2% of the frames), we are able to produce 2D results comparable to state-of-the-art fully supervised methods, along with 3D reconstructions that are impossible with other existing approaches. Our Multi-view Bootstrapping in the Wild (MBW) approach demonstrates impressive results on standard human datasets, as well as tigers, cheetahs, fish, colobus monkeys, chimpanzees, and flamingos from videos captured casually in a zoo. We release the codebase for MBW as well as this challenging zoo dataset consisting image frames of tail-end distribution categories with their corresponding 2D, 3D labels generated from minimal human intervention.



### Dense Prediction Transformer for Scale Estimation in Monocular Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/2210.01723v1
- **DOI**: 10.1109/LARS/SBR/WRE56824.2022.9995735
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.01723v1)
- **Published**: 2022-10-04 16:29:21+00:00
- **Updated**: 2022-10-04 16:29:21+00:00
- **Authors**: André O. Françani, Marcos R. O. A. Maximo
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular visual odometry consists of the estimation of the position of an agent through images of a single camera, and it is applied in autonomous vehicles, medical robots, and augmented reality. However, monocular systems suffer from the scale ambiguity problem due to the lack of depth information in 2D frames. This paper contributes by showing an application of the dense prediction transformer model for scale estimation in monocular visual odometry systems. Experimental results show that the scale drift problem of monocular systems can be reduced through the accurate estimation of the depth map by this model, achieving competitive state-of-the-art performance on a visual odometry benchmark.



### ASIF: Coupled Data Turns Unimodal Models to Multimodal Without Training
- **Arxiv ID**: http://arxiv.org/abs/2210.01738v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.01738v2)
- **Published**: 2022-10-04 16:56:22+00:00
- **Updated**: 2023-02-10 18:38:19+00:00
- **Authors**: Antonio Norelli, Marco Fumero, Valentino Maiorca, Luca Moschella, Emanuele Rodolà, Francesco Locatello
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: CLIP proved that aligning visual and language spaces is key to solving many vision tasks without explicit training, but required to train image and text encoders from scratch on a huge dataset. LiT improved this by only training the text encoder and using a pre-trained vision network. In this paper, we show that a common space can be created without any training at all, using single-domain encoders (trained with or without supervision) and a much smaller amount of image-text pairs. Furthermore, our model has unique properties. Most notably, deploying a new version with updated training samples can be done in a matter of seconds. Additionally, the representations in the common space are easily interpretable as every dimension corresponds to the similarity of the input to a unique entry in the multimodal dataset. Experiments on standard zero-shot visual benchmarks demonstrate the typical transfer ability of image-text models. Overall, our method represents a simple yet surprisingly strong baseline for foundation multi-modal models, raising important questions on their data efficiency and on the role of retrieval in machine learning.



### CADet: Fully Self-Supervised Out-Of-Distribution Detection With Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.01742v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.01742v3)
- **Published**: 2022-10-04 17:02:37+00:00
- **Updated**: 2023-06-27 13:58:46+00:00
- **Authors**: Charles Guille-Escuret, Pau Rodriguez, David Vazquez, Ioannis Mitliagkas, Joao Monteiro
- **Comment**: None
- **Journal**: None
- **Summary**: Handling out-of-distribution (OOD) samples has become a major stake in the real-world deployment of machine learning systems. This work explores the use of self-supervised contrastive learning to the simultaneous detection of two types of OOD samples: unseen classes and adversarial perturbations. First, we pair self-supervised contrastive learning with the maximum mean discrepancy (MMD) two-sample test. This approach enables us to robustly test whether two independent sets of samples originate from the same distribution, and we demonstrate its effectiveness by discriminating between CIFAR-10 and CIFAR-10.1 with higher confidence than previous work. Motivated by this success, we introduce CADet (Contrastive Anomaly Detection), a novel method for OOD detection of single samples. CADet draws inspiration from MMD, but leverages the similarity between contrastive transformations of a same sample. CADet outperforms existing adversarial detection methods in identifying adversarially perturbed samples on ImageNet and achieves comparable performance to unseen label detection methods on two challenging benchmarks: ImageNet-O and iNaturalist. Significantly, CADet is fully self-supervised and requires neither labels for in-distribution samples nor access to OOD examples.



### Perspective Aware Road Obstacle Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.01779v2
- **DOI**: 10.1109/LRA.2023.3245410
- **Categories**: **cs.CV**, cs.RO, I.4.6; I.4.8; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2210.01779v2)
- **Published**: 2022-10-04 17:48:42+00:00
- **Updated**: 2023-06-19 18:29:05+00:00
- **Authors**: Krzysztof Lis, Sina Honari, Pascal Fua, Mathieu Salzmann
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters ( Volume: 8, Issue: 4, April
  2023, Pages: 2150-2157)
- **Summary**: While road obstacle detection techniques have become increasingly effective, they typically ignore the fact that, in practice, the apparent size of the obstacles decreases as their distance to the vehicle increases. In this paper, we account for this by computing a scale map encoding the apparent size of a hypothetical object at every image location. We then leverage this perspective map to (i) generate training data by injecting onto the road synthetic objects whose size corresponds to the perspective foreshortening; and (ii) incorporate perspective information in the decoding part of the detection network to guide the obstacle detector. Our results on standard benchmarks show that, together, these two strategies significantly boost the obstacle detection performance, allowing our approach to consistently outperform state-of-the-art methods in terms of instance-level obstacle detection.



### COPILOT: Human-Environment Collision Prediction and Localization from Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/2210.01781v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.01781v2)
- **Published**: 2022-10-04 17:49:23+00:00
- **Updated**: 2023-03-26 05:27:31+00:00
- **Authors**: Boxiao Pan, Bokui Shen, Davis Rempe, Despoina Paschalidou, Kaichun Mo, Yanchao Yang, Leonidas J. Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to forecast human-environment collisions from egocentric observations is vital to enable collision avoidance in applications such as VR, AR, and wearable assistive robotics. In this work, we introduce the challenging problem of predicting collisions in diverse environments from multi-view egocentric videos captured from body-mounted cameras. Solving this problem requires a generalizable perception system that can classify which human body joints will collide and estimate a collision region heatmap to localize collisions in the environment. To achieve this, we propose a transformer-based model called COPILOT to perform collision prediction and localization simultaneously, which accumulates information across multi-view inputs through a novel 4D space-time-viewpoint attention mechanism. To train our model and enable future research on this task, we develop a synthetic data generation framework that produces egocentric videos of virtual humans moving and colliding within diverse 3D environments. This framework is then used to establish a large-scale dataset consisting of 8.6M egocentric RGBD frames. Extensive experiments show that COPILOT generalizes to unseen synthetic as well as real-world scenes. We further demonstrate COPILOT outputs are useful for downstream collision avoidance through simple closed-loop control. Please visit our project webpage at https://sites.google.com/stanford.edu/copilot.



### COARSE3D: Class-Prototypes for Contrastive Learning in Weakly-Supervised 3D Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.01784v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01784v2)
- **Published**: 2022-10-04 17:54:53+00:00
- **Updated**: 2022-10-08 03:42:10+00:00
- **Authors**: Rong Li, Anh-Quan Cao, Raoul de Charette
- **Comment**: None
- **Journal**: None
- **Summary**: Annotation of large-scale 3D data is notoriously cumbersome and costly. As an alternative, weakly-supervised learning alleviates such a need by reducing the annotation by several order of magnitudes. We propose COARSE3D, a novel architecture-agnostic contrastive learning strategy for 3D segmentation. Since contrastive learning requires rich and diverse examples as keys and anchors, we leverage a prototype memory bank capturing class-wise global dataset information efficiently into a small number of prototypes acting as keys. An entropy-driven sampling technique then allows us to select good pixels from predictions as anchors. Experiments on three projection-based backbones show we outperform baselines on three challenging real-world outdoor datasets, working with as low as 0.001% annotations.



### Real-Time Monitoring of User Stress, Heart Rate and Heart Rate Variability on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/2210.01791v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2210.01791v1)
- **Published**: 2022-10-04 17:58:37+00:00
- **Updated**: 2022-10-04 17:58:37+00:00
- **Authors**: Peyman Bateni, Leonid Sigal
- **Comment**: None
- **Journal**: None
- **Summary**: Stress is considered to be the epidemic of the 21st-century. Yet, mobile apps cannot directly evaluate the impact of their content and services on user stress. We introduce the Beam AI SDK to address this issue. Using our SDK, apps can monitor user stress through the selfie camera in real-time. Our technology extracts the user's pulse wave by analyzing subtle color variations across the skin regions of the user's face. The user's pulse wave is then used to determine stress (according to the Baevsky Stress Index), heart rate, and heart rate variability. We evaluate our technology on the UBFC dataset, the MMSE-HR dataset, and Beam AI's internal data. Our technology achieves 99.2%, 97.8% and 98.5% accuracy for heart rate estimation on each benchmark respectively, a nearly twice lower error rate than competing methods. We further demonstrate an average Pearson correlation of 0.801 in determining stress and heart rate variability, thus producing commercially useful readings to derive content decisions in apps. Our SDK is available for use at www.beamhealth.ai.



### Implicit Warping for Animation with Image Sets
- **Arxiv ID**: http://arxiv.org/abs/2210.01794v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01794v1)
- **Published**: 2022-10-04 17:59:56+00:00
- **Updated**: 2022-10-04 17:59:56+00:00
- **Authors**: Arun Mallya, Ting-Chun Wang, Ming-Yu Liu
- **Comment**: To be published at NeurIPS 2022
- **Journal**: None
- **Summary**: We present a new implicit warping framework for image animation using sets of source images through the transfer of the motion of a driving video. A single cross- modal attention layer is used to find correspondences between the source images and the driving image, choose the most appropriate features from different source images, and warp the selected features. This is in contrast to the existing methods that use explicit flow-based warping, which is designed for animation using a single source and does not extend well to multiple sources. The pick-and-choose capability of our framework helps it achieve state-of-the-art results on multiple datasets for image animation using both single and multiple source images. The project website is available at https://deepimagination.cc/implicit warping/



### MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models
- **Arxiv ID**: http://arxiv.org/abs/2210.01820v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01820v2)
- **Published**: 2022-10-04 18:00:06+00:00
- **Updated**: 2023-01-30 22:12:11+00:00
- **Authors**: Chenglin Yang, Siyuan Qiao, Qihang Yu, Xiaoding Yuan, Yukun Zhu, Alan Yuille, Hartwig Adam, Liang-Chieh Chen
- **Comment**: ICLR 2023. arXiv v2: add ImageNet-1K-V2, tiny-MOAT on COCO detection
  and ADE20K segmentation
- **Journal**: None
- **Summary**: This paper presents MOAT, a family of neural networks that build on top of MObile convolution (i.e., inverted residual blocks) and ATtention. Unlike the current works that stack separate mobile convolution and transformer blocks, we effectively merge them into a MOAT block. Starting with a standard Transformer block, we replace its multi-layer perceptron with a mobile convolution block, and further reorder it before the self-attention operation. The mobile convolution block not only enhances the network representation capacity, but also produces better downsampled features. Our conceptually simple MOAT networks are surprisingly effective, achieving 89.1% / 81.5% top-1 accuracy on ImageNet-1K / ImageNet-1K-V2 with ImageNet22K pretraining. Additionally, MOAT can be seamlessly applied to downstream tasks that require large resolution inputs by simply converting the global attention to window attention. Thanks to the mobile convolution that effectively exchanges local information between pixels (and thus cross-windows), MOAT does not need the extra window-shifting mechanism. As a result, on COCO object detection, MOAT achieves 59.2% box AP with 227M model parameters (single-scale inference, and hard NMS), and on ADE20K semantic segmentation, MOAT attains 57.6% mIoU with 496M model parameters (single-scale inference). Finally, the tiny-MOAT family, obtained by simply reducing the channel sizes, also surprisingly outperforms several mobile-specific transformer-based models on ImageNet. The tiny-MOAT family is also benchmarked on downstream tasks, serving as a baseline for the community. We hope our simple yet effective MOAT will inspire more seamless integration of convolution and self-attention. Code is publicly available.



### Centerpoints Are All You Need in Overhead Imagery
- **Arxiv ID**: http://arxiv.org/abs/2210.01857v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.01857v1)
- **Published**: 2022-10-04 18:57:43+00:00
- **Updated**: 2022-10-04 18:57:43+00:00
- **Authors**: James Mason Inder, Mark Lowell, Andrew J. Maltenfort
- **Comment**: None
- **Journal**: None
- **Summary**: Labeling data to use for training object detectors is expensive and time consuming. Publicly available overhead datasets for object detection are labeled with image-aligned bounding boxes, object-aligned bounding boxes, or object masks, but it is not clear whether such detailed labeling is necessary. To test the idea, we developed novel single- and two-stage network architectures that use centerpoints for labeling. In this paper we show that these architectures achieve nearly equivalent performance to approaches using more detailed labeling on three overhead object detection datasets.



### Capturing and Animation of Body and Clothing from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/2210.01868v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2210.01868v1)
- **Published**: 2022-10-04 19:34:05+00:00
- **Updated**: 2022-10-04 19:34:05+00:00
- **Authors**: Yao Feng, Jinlong Yang, Marc Pollefeys, Michael J. Black, Timo Bolkart
- **Comment**: 7 pages main paper, 2 pages supp. mat
- **Journal**: None
- **Summary**: While recent work has shown progress on extracting clothed 3D human avatars from a single image, video, or a set of 3D scans, several limitations remain. Most methods use a holistic representation to jointly model the body and clothing, which means that the clothing and body cannot be separated for applications like virtual try-on. Other methods separately model the body and clothing, but they require training from a large set of 3D clothed human meshes obtained from 3D/4D scanners or physics simulations. Our insight is that the body and clothing have different modeling requirements. While the body is well represented by a mesh-based parametric 3D model, implicit representations and neural radiance fields are better suited to capturing the large variety in shape and appearance present in clothing. Building on this insight, we propose SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a mesh-based body with a neural radiance field. Integrating the mesh into the volumetric rendering in combination with a differentiable rasterizer enables us to optimize SCARF directly from monocular videos, without any 3D supervision. The hybrid modeling enables SCARF to (i) animate the clothed body avatar by changing body poses (including hand articulation and facial expressions), (ii) synthesize novel views of the avatar, and (iii) transfer clothing between avatars in virtual try-on applications. We demonstrate that SCARF reconstructs clothing with higher visual quality than existing methods, that the clothing deforms with changing body pose and body shape, and that clothing can be successfully transferred between avatars of different subjects. The code and models are available at https://github.com/YadiraF/SCARF.



### A Perceptual Quality Metric for Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2210.01879v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.01879v1)
- **Published**: 2022-10-04 19:56:10+00:00
- **Updated**: 2022-10-04 19:56:10+00:00
- **Authors**: Qiqi Hou, Abhijay Ghildyal, Feng Liu
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Research on video frame interpolation has made significant progress in recent years. However, existing methods mostly use off-the-shelf metrics to measure the quality of interpolation results with the exception of a few methods that employ user studies, which is time-consuming. As video frame interpolation results often exhibit unique artifacts, existing quality metrics sometimes are not consistent with human perception when measuring the interpolation results. Some recent deep learning-based perceptual quality metrics are shown more consistent with human judgments, but their performance on videos is compromised since they do not consider temporal information. In this paper, we present a dedicated perceptual quality metric for measuring video frame interpolation results. Our method learns perceptual features directly from videos instead of individual frames. It compares pyramid features extracted from video frames and employs Swin Transformer blocks-based spatio-temporal modules to extract spatio-temporal information. To train our metric, we collected a new video frame interpolation quality assessment dataset. Our experiments show that our dedicated quality metric outperforms state-of-the-art methods when measuring video frame interpolation results. Our code and model are made publicly available at \url{https://github.com/hqqxyy/VFIPS}.



### Self-supervised Pre-training for Semantic Segmentation in an Indoor Scene
- **Arxiv ID**: http://arxiv.org/abs/2210.01884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01884v1)
- **Published**: 2022-10-04 20:10:14+00:00
- **Updated**: 2022-10-04 20:10:14+00:00
- **Authors**: Sulabh Shrestha, Yimeng Li, Jana Kosecka
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to endow maps of indoor scenes with semantic information is an integral part of robotic agents which perform different tasks such as target driven navigation, object search or object rearrangement. The state-of-the-art methods use Deep Convolutional Neural Networks (DCNNs) for predicting semantic segmentation of an image as useful representation for these tasks. The accuracy of semantic segmentation depends on the availability and the amount of labeled data from the target environment or the ability to bridge the domain gap between test and training environment. We propose RegConsist, a method for self-supervised pre-training of a semantic segmentation model, exploiting the ability of the agent to move and register multiple views in the novel environment. Given the spatial and temporal consistency cues used for pixel level data association, we use a variant of contrastive learning to train a DCNN model for predicting semantic segmentation from RGB views in the target environment. The proposed method outperforms models pre-trained on ImageNet and achieves competitive performance when using models that are trained for exactly the same task but on a different dataset. We also perform various ablation studies to analyze and demonstrate the efficacy of our proposed method.



### Multi-view Human Body Mesh Translator
- **Arxiv ID**: http://arxiv.org/abs/2210.01886v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.01886v1)
- **Published**: 2022-10-04 20:10:59+00:00
- **Updated**: 2022-10-04 20:10:59+00:00
- **Authors**: Xiangjian Jiang, Xuecheng Nie, Zitian Wang, Luoqi Liu, Si Liu
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Existing methods for human mesh recovery mainly focus on single-view frameworks, but they often fail to produce accurate results due to the ill-posed setup. Considering the maturity of the multi-view motion capture system, in this paper, we propose to solve the prior ill-posed problem by leveraging multiple images from different views, thus significantly enhancing the quality of recovered meshes. In particular, we present a novel \textbf{M}ulti-view human body \textbf{M}esh \textbf{T}ranslator (MMT) model for estimating human body mesh with the help of vision transformer. Specifically, MMT takes multi-view images as input and translates them to targeted meshes in a single-forward manner. MMT fuses features of different views in both encoding and decoding phases, leading to representations embedded with global information. Additionally, to ensure the tokens are intensively focused on the human pose and shape, MMT conducts cross-view alignment at the feature level by projecting 3D keypoint positions to each view and enforcing their consistency in geometry constraints. Comprehensive experiments demonstrate that MMT outperforms existing single or multi-view models by a large margin for human mesh recovery task, notably, 28.8\% improvement in MPVE over the current state-of-the-art method on the challenging HUMBI dataset. Qualitative evaluation also verifies the effectiveness of MMT in reconstructing high-quality human mesh. Codes will be made available upon acceptance.



### Collecting The Puzzle Pieces: Disentangled Self-Driven Human Pose Transfer by Permuting Textures
- **Arxiv ID**: http://arxiv.org/abs/2210.01887v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.01887v3)
- **Published**: 2022-10-04 20:14:47+00:00
- **Updated**: 2023-08-30 21:17:43+00:00
- **Authors**: Nannan Li, Kevin J. Shih, Bryan A. Plummer
- **Comment**: Accepted to ICCV 2023
- **Journal**: None
- **Summary**: Human pose transfer synthesizes new view(s) of a person for a given pose. Recent work achieves this via self-reconstruction, which disentangles a person's pose and texture information by breaking the person down into parts, then recombines them for reconstruction. However, part-level disentanglement preserves some pose information that can create unwanted artifacts. In this paper, we propose Pose Transfer by Permuting Textures (PT$^2$), an approach for self-driven human pose transfer that disentangles pose from texture at the patch-level. Specifically, we remove pose from an input image by permuting image patches so only texture information remains. Then we reconstruct the input image by sampling from the permuted textures for patch-level disentanglement. To reduce noise and recover clothing shape information from the permuted patches, we employ encoders with multiple kernel sizes in a triple branch network. On DeepFashion and Market-1501, PT$^2$ reports significant gains on automatic metrics over other self-driven methods, and even outperforms some fully-supervised methods. A user study also reports images generated by our method are preferred in 68% of cases over self-driven approaches from prior work. Code is available at https://github.com/NannanLi999/pt_square.



### Adaptively Weighted Data Augmentation Consistency Regularization for Robust Optimization under Concept Shift
- **Arxiv ID**: http://arxiv.org/abs/2210.01891v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.01891v2)
- **Published**: 2022-10-04 20:28:38+00:00
- **Updated**: 2023-01-31 03:21:13+00:00
- **Authors**: Yijun Dong, Yuege Xie, Rachel Ward
- **Comment**: None
- **Journal**: None
- **Summary**: Concept shift is a prevailing problem in natural tasks like medical image segmentation where samples usually come from different subpopulations with variant correlations between features and labels. One common type of concept shift in medical image segmentation is the "information imbalance" between label-sparse samples with few (if any) segmentation labels and label-dense samples with plentiful labeled pixels. Existing distributionally robust algorithms have focused on adaptively truncating/down-weighting the "less informative" (i.e., label-sparse in our context) samples. To exploit data features of label-sparse samples more efficiently, we propose an adaptively weighted online optimization algorithm -- AdaWAC -- to incorporate data augmentation consistency regularization in sample reweighting. Our method introduces a set of trainable weights to balance the supervised loss and unsupervised consistency regularization of each sample separately. At the saddle point of the underlying objective, the weights assign label-dense samples to the supervised loss and label-sparse samples to the unsupervised consistency regularization. We provide a convergence guarantee by recasting the optimization as online mirror descent on a saddle point problem. Our empirical results demonstrate that AdaWAC not only enhances the segmentation performance and sample efficiency but also improves the robustness to concept shift on various medical image segmentation tasks with different UNet-style backbones.



### Supervised Metric Learning to Rank for Retrieval via Contextual Similarity Optimization
- **Arxiv ID**: http://arxiv.org/abs/2210.01908v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.01908v3)
- **Published**: 2022-10-04 21:08:27+00:00
- **Updated**: 2023-06-02 15:25:04+00:00
- **Authors**: Christopher Liao, Theodoros Tsiligkaridis, Brian Kulis
- **Comment**: None
- **Journal**: None
- **Summary**: There is extensive interest in metric learning methods for image retrieval. Many metric learning loss functions focus on learning a correct ranking of training samples, but strongly overfit semantically inconsistent labels and require a large amount of data. To address these shortcomings, we propose a new metric learning method, called contextual loss, which optimizes contextual similarity in addition to cosine similarity. Our contextual loss implicitly enforces semantic consistency among neighbors while converging to the correct ranking. We empirically show that the proposed loss is more robust to label noise, and is less prone to overfitting even when a large portion of train data is withheld. Extensive experiments demonstrate that our method achieves a new state-of-the-art across four image retrieval benchmarks and multiple different evaluation settings. Code is available at: https://github.com/Chris210634/metric-learning-using-contextual-similarity



### Grounding Language with Visual Affordances over Unstructured Data
- **Arxiv ID**: http://arxiv.org/abs/2210.01911v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.01911v3)
- **Published**: 2022-10-04 21:16:48+00:00
- **Updated**: 2023-03-08 11:00:55+00:00
- **Authors**: Oier Mees, Jessica Borja-Diaz, Wolfram Burgard
- **Comment**: Accepted at the 2023 IEEE International Conference on Robotics and
  Automation (ICRA). Project website: http://hulc2.cs.uni-freiburg.de
- **Journal**: None
- **Summary**: Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills. However, in practice, learning multi-task, language-conditioned robotic skills typically requires large-scale data collection and frequent human intervention to reset the environment or help correcting the current policies. In this work, we propose a novel approach to efficiently learn general-purpose language-conditioned robot skills from unstructured, offline and reset-free data in the real world by exploiting a self-supervised visuo-lingual affordance model, which requires annotating as little as 1% of the total data with language. We evaluate our method in extensive experiments both in simulated and real-world robotic tasks, achieving state-of-the-art performance on the challenging CALVIN benchmark and learning over 25 distinct visuomotor manipulation tasks with a single policy in the real world. We find that when paired with LLMs to break down abstract natural language instructions into subgoals via few-shot prompting, our method is capable of completing long-horizon, multi-tier tasks in the real world, while requiring an order of magnitude less data than previous approaches. Code and videos are available at http://hulc2.cs.uni-freiburg.de



### Differentiable Raycasting for Self-supervised Occupancy Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2210.01917v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.01917v2)
- **Published**: 2022-10-04 21:35:21+00:00
- **Updated**: 2022-10-18 14:29:45+00:00
- **Authors**: Tarasha Khurana, Peiyun Hu, Achal Dave, Jason Ziglar, David Held, Deva Ramanan
- **Comment**: ECCV 2022. Code available at
  https://github.com/tarashakhurana/emergent-occ-forecasting
- **Journal**: None
- **Summary**: Motion planning for safe autonomous driving requires learning how the environment around an ego-vehicle evolves with time. Ego-centric perception of driveable regions in a scene not only changes with the motion of actors in the environment, but also with the movement of the ego-vehicle itself. Self-supervised representations proposed for large-scale planning, such as ego-centric freespace, confound these two motions, making the representation difficult to use for downstream motion planners. In this paper, we use geometric occupancy as a natural alternative to view-dependent representations such as freespace. Occupancy maps naturally disentangle the motion of the environment from the motion of the ego-vehicle. However, one cannot directly observe the full 3D occupancy of a scene (due to occlusion), making it difficult to use as a signal for learning. Our key insight is to use differentiable raycasting to "render" future occupancy predictions into future LiDAR sweep predictions, which can be compared with ground-truth sweeps for self-supervised learning. The use of differentiable raycasting allows occupancy to emerge as an internal representation within the forecasting network. In the absence of groundtruth occupancy, we quantitatively evaluate the forecasting of raycasted LiDAR sweeps and show improvements of upto 15 F1 points. For downstream motion planners, where emergent occupancy can be directly used to guide non-driveable regions, this representation relatively reduces the number of collisions with objects by up to 17% as compared to freespace-centric motion planners.



### When and why vision-language models behave like bags-of-words, and what to do about it?
- **Arxiv ID**: http://arxiv.org/abs/2210.01936v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.01936v3)
- **Published**: 2022-10-04 22:13:25+00:00
- **Updated**: 2023-03-23 23:21:38+00:00
- **Authors**: Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, James Zou
- **Comment**: ICLR 2023 Oral (notable-top-5%)
- **Journal**: None
- **Summary**: Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode compositional information. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order. ARO consists of Visual Genome Attribution, to test the understanding of objects' properties; Visual Genome Relation, to test for relational understanding; and COCO & Flickr30k-Order, to test for order sensitivity. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We show where state-of-the-art VLMs have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large datasets with rich compositional structure in the images and captions. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deficiency. To understand why these limitations emerge and are not represented in the standard tests, we zoom into the evaluation and training procedures. We demonstrate that it is possible to perform well on retrieval over existing datasets without using the composition and order information. Given that contrastive pretraining optimizes for retrieval on datasets with similar shortcuts, we hypothesize that this can explain why the models do not need to learn to represent compositional information. This finding suggests a natural solution: composition-aware hard negative mining. We show that a simple-to-implement modification of contrastive learning significantly improves the performance on tasks requiring understanding of order and compositionality.



### Affection: Learning Affective Explanations for Real-World Visual Data
- **Arxiv ID**: http://arxiv.org/abs/2210.01946v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2210.01946v1)
- **Published**: 2022-10-04 22:44:17+00:00
- **Updated**: 2022-10-04 22:44:17+00:00
- **Authors**: Panos Achlioptas, Maks Ovsjanikov, Leonidas Guibas, Sergey Tulyakov
- **Comment**: https://affective-explanations.org
- **Journal**: None
- **Summary**: In this work, we explore the emotional reactions that real-world images tend to induce by using natural language as the medium to express the rationale behind an affective response to a given visual stimulus. To embark on this journey, we introduce and share with the research community a large-scale dataset that contains emotional reactions and free-form textual explanations for 85,007 publicly available images, analyzed by 6,283 annotators who were asked to indicate and explain how and why they felt in a particular way when observing a specific image, producing a total of 526,749 responses. Even though emotional reactions are subjective and sensitive to context (personal mood, social status, past experiences) - we show that there is significant common ground to capture potentially plausible emotional responses with a large support in the subject population. In light of this crucial observation, we ask the following questions: i) Can we develop multi-modal neural networks that provide reasonable affective responses to real-world visual data, explained with language? ii) Can we steer such methods towards producing explanations with varying degrees of pragmatic language or justifying different emotional reactions while adapting to the underlying visual stimulus? Finally, iii) How can we evaluate the performance of such methods for this novel task? With this work, we take the first steps in addressing all of these questions, thus paving the way for richer, more human-centric, and emotionally-aware image analysis systems. Our introduced dataset and all developed methods are available on https://affective-explanations.org



