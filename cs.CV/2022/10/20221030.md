# Arxiv Papers in cs.CV on 2022-10-30
### Spatio-Temporal Attention in Multi-Granular Brain Chronnectomes for Detection of Autism Spectrum Disorder
- **Arxiv ID**: http://arxiv.org/abs/2211.07360v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.07360v1)
- **Published**: 2022-10-30 01:43:17+00:00
- **Updated**: 2022-10-30 01:43:17+00:00
- **Authors**: James Orme-Rogers, Ajitesh Srivastava
- **Comment**: 6 pages, 2 figures
- **Journal**: None
- **Summary**: The traditional methods for detecting autism spectrum disorder (ASD) are expensive, subjective, and time-consuming, often taking years for a diagnosis, with many children growing well into adolescence and even adulthood before finally confirming the disorder. Recently, graph-based learning techniques have demonstrated impressive results on resting-state functional magnetic resonance imaging (rs-fMRI) data from the Autism Brain Imaging Data Exchange (ABIDE). We introduce IMAGIN, a multI-granular, Multi-Atlas spatio-temporal attention Graph Isomorphism Network, which we use to learn graph representations of dynamic functional brain connectivity (chronnectome), as opposed to static connectivity (connectome). The experimental results demonstrate that IMAGIN achieves a 5-fold cross-validation accuracy of 79.25%, which surpasses the current state-of-the-art by 1.5%. In addition, analysis of the spatial and temporal attention scores provides further validation for the neural basis of autism.



### Multi-view Multi-label Anomaly Network Traffic Classification based on MLP-Mixer Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2210.16719v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16719v2)
- **Published**: 2022-10-30 01:52:05+00:00
- **Updated**: 2022-11-08 03:25:28+00:00
- **Authors**: Yu Zheng, Zhangxuan Dang, Chunlei Peng, Chao Yang, Xinbo Gao
- **Comment**: 15 pages,6 figures
- **Journal**: None
- **Summary**: Network traffic classification is the basis of many network security applications and has attracted enough attention in the field of cyberspace security. Existing network traffic classification based on convolutional neural networks (CNNs) often emphasizes local patterns of traffic data while ignoring global information associations. In this paper, we propose a MLP-Mixer based multi-view multi-label neural network for network traffic classification. Compared with the existing CNN-based methods, our method adopts the MLP-Mixer structure, which is more in line with the structure of the packet than the conventional convolution operation. In our method, the packet is divided into the packet header and the packet body, together with the flow features of the packet as input from different views. We utilize a multi-label setting to learn different scenarios simultaneously to improve the classification performance by exploiting the correlations between different scenarios. Taking advantage of the above characteristics, we propose an end-to-end network traffic classification method. We conduct experiments on three public datasets, and the experimental results show that our method can achieve superior performance.



### Exemplar Guided Deep Neural Network for Spatial Transcriptomics Analysis of Gene Expression Prediction
- **Arxiv ID**: http://arxiv.org/abs/2210.16721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16721v1)
- **Published**: 2022-10-30 02:22:20+00:00
- **Updated**: 2022-10-30 02:22:20+00:00
- **Authors**: Yan Yang, Md Zakir Hossain, Eric A Stone, Shafin Rahman
- **Comment**: None
- **Journal**: None
- **Summary**: Spatial transcriptomics (ST) is essential for understanding diseases and developing novel treatments. It measures gene expression of each fine-grained area (i.e., different windows) in the tissue slide with low throughput. This paper proposes an Exemplar Guided Network (EGN) to accurately and efficiently predict gene expression directly from each window of a tissue slide image. We apply exemplar learning to dynamically boost gene expression prediction from nearest/similar exemplars of a given tissue slide image window. Our EGN framework composes of three main components: 1) an extractor to structure a representation space for unsupervised exemplar retrievals; 2) a vision transformer (ViT) backbone to progressively extract representations of the input window; and 3) an Exemplar Bridging (EB) block to adaptively revise the intermediate ViT representations by using the nearest exemplars. Finally, we complete the gene expression prediction task with a simple attention-based prediction block. Experiments on standard benchmark datasets indicate the superiority of our approach when comparing with the past state-of-the-art (SOTA) methods.



### A Self-Supervised Approach to Reconstruction in Sparse X-Ray Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2211.00002v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/2211.00002v1)
- **Published**: 2022-10-30 02:33:45+00:00
- **Updated**: 2022-10-30 02:33:45+00:00
- **Authors**: Rey Mendoza, Minh Nguyen, Judith Weng Zhu, Vincent Dumont, Talita Perciano, Juliane Mueller, Vidya Ganapati
- **Comment**: NeurIPS 2022 Machine Learning and the Physical Sciences Workshop.
  arXiv admin note: text overlap with arXiv:2210.16709
- **Journal**: None
- **Summary**: Computed tomography has propelled scientific advances in fields from biology to materials science. This technology allows for the elucidation of 3-dimensional internal structure by the attenuation of x-rays through an object at different rotations relative to the beam. By imaging 2-dimensional projections, a 3-dimensional object can be reconstructed through a computational algorithm. Imaging at a greater number of rotation angles allows for improved reconstruction. However, taking more measurements increases the x-ray dose and may cause sample damage. Deep neural networks have been used to transform sparse 2-D projection measurements to a 3-D reconstruction by training on a dataset of known similar objects. However, obtaining high-quality object reconstructions for the training dataset requires high x-ray dose measurements that can destroy or alter the specimen before imaging is complete. This becomes a chicken-and-egg problem: high-quality reconstructions cannot be generated without deep learning, and the deep neural network cannot be learned without the reconstructions. This work develops and validates a self-supervised probabilistic deep learning technique, the physics-informed variational autoencoder, to solve this problem. A dataset consisting solely of sparse projection measurements from each object is used to jointly reconstruct all objects of the set. This approach has the potential to allow visualization of fragile samples with x-ray computed tomography. We release our code for reproducing our results at: https://github.com/vganapati/CT_PVAE .



### ISG: I can See Your Gene Expression
- **Arxiv ID**: http://arxiv.org/abs/2210.16728v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16728v1)
- **Published**: 2022-10-30 02:49:37+00:00
- **Updated**: 2022-10-30 02:49:37+00:00
- **Authors**: Yan Yang, LiYuan Pan, Liu Liu, Eric A Stone
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims to predict gene expression from a histology slide image precisely. Such a slide image has a large resolution and sparsely distributed textures. These obstruct extracting and interpreting discriminative features from the slide image for diverse gene types prediction. Existing gene expression methods mainly use general components to filter textureless regions, extract features, and aggregate features uniformly across regions. However, they ignore gaps and interactions between different image regions and are therefore inferior in the gene expression task. Instead, we present ISG framework that harnesses interactions among discriminative features from texture-abundant regions by three new modules: 1) a Shannon Selection module, based on the Shannon information content and Solomonoff's theory, to filter out textureless image regions; 2) a Feature Extraction network to extract expressive low-dimensional feature representations for efficient region interactions among a high-resolution image; 3) a Dual Attention network attends to regions with desired gene expression features and aggregates them for the prediction task. Extensive experiments on standard benchmark datasets show that the proposed ISG framework outperforms state-of-the-art methods significantly.



### On-the-fly Object Detection using StyleGAN with CLIP Guidance
- **Arxiv ID**: http://arxiv.org/abs/2210.16742v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16742v1)
- **Published**: 2022-10-30 04:43:01+00:00
- **Updated**: 2022-10-30 04:43:01+00:00
- **Authors**: Yuzhe Lu, Shusen Liu, Jayaraman J. Thiagarajan, Wesam Sakla, Rushil Anirudh
- **Comment**: None
- **Journal**: None
- **Summary**: We present a fully automated framework for building object detectors on satellite imagery without requiring any human annotation or intervention. We achieve this by leveraging the combined power of modern generative models (e.g., StyleGAN) and recent advances in multi-modal learning (e.g., CLIP). While deep generative models effectively encode the key semantics pertinent to a data distribution, this information is not immediately accessible for downstream tasks, such as object detection. In this work, we exploit CLIP's ability to associate image features with text descriptions to identify neurons in the generator network, which are subsequently used to build detectors on-the-fly.



### MEDS-Net: Self-Distilled Multi-Encoders Network with Bi-Direction Maximum Intensity projections for Lung Nodule Detection
- **Arxiv ID**: http://arxiv.org/abs/2211.00003v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2211.00003v2)
- **Published**: 2022-10-30 05:55:45+00:00
- **Updated**: 2022-12-26 10:25:13+00:00
- **Authors**: Muhammad Usman, Azka Rehman, Abdullah Shahid, Siddique Latif, Shi Sub Byon, Byoung Dai Lee, Sung Hyun Kim, Byung il Lee, Yeong Gil Shin
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we propose a lung nodule detection scheme which fully incorporates the clinic workflow of radiologists. Particularly, we exploit Bi-Directional Maximum intensity projection (MIP) images of various thicknesses (i.e., 3, 5 and 10mm) along with a 3D patch of CT scan, consisting of 10 adjacent slices to feed into self-distillation-based Multi-Encoders Network (MEDS-Net). The proposed architecture first condenses 3D patch input to three channels by using a dense block which consists of dense units which effectively examine the nodule presence from 2D axial slices. This condensed information, along with the forward and backward MIP images, is fed to three different encoders to learn the most meaningful representation, which is forwarded into the decoded block at various levels. At the decoder block, we employ a self-distillation mechanism by connecting the distillation block, which contains five lung nodule detectors. It helps to expedite the convergence and improves the learning ability of the proposed architecture. Finally, the proposed scheme reduces the false positives by complementing the main detector with auxiliary detectors. The proposed scheme has been rigorously evaluated on 888 scans of LUNA16 dataset and obtained a CPM score of 93.6\%. The results demonstrate that incorporating of bi-direction MIP images enables MEDS-Net to effectively distinguish nodules from surroundings which help to achieve the sensitivity of 91.5% and 92.8% with false positives rate of 0.25 and 0.5 per scan, respectively.



### Benchmarking Adversarial Patch Against Aerial Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.16765v1
- **DOI**: 10.1109/TGRS.2022.3225306
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16765v1)
- **Published**: 2022-10-30 07:55:59+00:00
- **Updated**: 2022-10-30 07:55:59+00:00
- **Authors**: Jiawei Lian, Shaohui Mei, Shun Zhang, Mingyang Ma
- **Comment**: 14 pages, 14 figures
- **Journal**: None
- **Summary**: DNNs are vulnerable to adversarial examples, which poses great security concerns for security-critical systems. In this paper, a novel adaptive-patch-based physical attack (AP-PA) framework is proposed, which aims to generate adversarial patches that are adaptive in both physical dynamics and varying scales, and by which the particular targets can be hidden from being detected. Furthermore, the adversarial patch is also gifted with attack effectiveness against all targets of the same class with a patch outside the target (No need to smear targeted objects) and robust enough in the physical world. In addition, a new loss is devised to consider more available information of detected objects to optimize the adversarial patch, which can significantly improve the patch's attack efficacy (Average precision drop up to 87.86% and 85.48% in white-box and black-box settings, respectively) and optimizing efficiency. We also establish one of the first comprehensive, coherent, and rigorous benchmarks to evaluate the attack efficacy of adversarial patches on aerial detection tasks. Finally, several proportionally scaled experiments are performed physically to demonstrate that the elaborated adversarial patches can successfully deceive aerial detection algorithms in dynamic physical circumstances. The code is available at https://github.com/JiaweiLian/AP-PA.



### Dataset Distillation via Factorization
- **Arxiv ID**: http://arxiv.org/abs/2210.16774v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16774v1)
- **Published**: 2022-10-30 08:36:19+00:00
- **Updated**: 2022-10-30 08:36:19+00:00
- **Authors**: Songhua Liu, Kai Wang, Xingyi Yang, Jingwen Ye, Xinchao Wang
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: In this paper, we study \xw{dataset distillation (DD)}, from a novel perspective and introduce a \emph{dataset factorization} approach, termed \emph{HaBa}, which is a plug-and-play strategy portable to any existing DD baseline. Unlike conventional DD approaches that aim to produce distilled and representative samples, \emph{HaBa} explores decomposing a dataset into two components: data \emph{Ha}llucination networks and \emph{Ba}ses, where the latter is fed into the former to reconstruct image samples. The flexible combinations between bases and hallucination networks, therefore, equip the distilled data with exponential informativeness gain, which largely increase the representation capability of distilled datasets. To furthermore increase the data efficiency of compression results, we further introduce a pair of adversarial contrastive constraints on the resultant hallucination networks and bases, which increase the diversity of generated images and inject more discriminant information into the factorization. Extensive comparisons and experiments demonstrate that our method can yield significant improvement on downstream classification tasks compared with previous state of the arts, while reducing the total number of compressed parameters by up to 65\%. Moreover, distilled datasets by our approach also achieve \textasciitilde10\% higher accuracy than baseline methods in cross-architecture generalization. Our code is available \href{https://github.com/Huage001/DatasetFactorization}{here}.



### Saliency Can Be All You Need In Contrastive Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.16776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16776v1)
- **Published**: 2022-10-30 08:47:53+00:00
- **Updated**: 2022-10-30 08:47:53+00:00
- **Authors**: Veysel Kocaman, Ofer M. Shir, Thomas Bäck, Ahmed Nabil Belbachir
- **Comment**: Accepted for the 17th International Symposium on Visual Computing
  (ISVC 2022)
- **Journal**: None
- **Summary**: We propose an augmentation policy for Contrastive Self-Supervised Learning (SSL) in the form of an already established Salient Image Segmentation technique entitled Global Contrast based Salient Region Detection. This detection technique, which had been devised for unrelated Computer Vision tasks, was empirically observed to play the role of an augmentation facilitator within the SSL protocol. This observation is rooted in our practical attempts to learn, by SSL-fashion, aerial imagery of solar panels, which exhibit challenging boundary patterns. Upon the successful integration of this technique on our problem domain, we formulated a generalized procedure and conducted a comprehensive, systematic performance assessment with various Contrastive SSL algorithms subject to standard augmentation techniques. This evaluation, which was conducted across multiple datasets, indicated that the proposed technique indeed contributes to SSL. We hypothesize whether salient image segmentation may suffice as the only augmentation policy in Contrastive SSL when treating downstream segmentation tasks.



### Recognizing Handwriting Styles in a Historical Scanned Document Using Unsupervised Fuzzy Clustering
- **Arxiv ID**: http://arxiv.org/abs/2210.16780v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16780v2)
- **Published**: 2022-10-30 09:07:51+00:00
- **Updated**: 2023-06-28 21:41:39+00:00
- **Authors**: Sriparna Majumdar, Aaron Brick
- **Comment**: 26 pages in total, 5 figures and 2 tables
- **Journal**: None
- **Summary**: The forensic attribution of the handwriting in a digitized document to multiple scribes is a challenging problem of high dimensionality. Unique handwriting styles may be dissimilar in a blend of several factors including character size, stroke width, loops, ductus, slant angles, and cursive ligatures. Previous work on labeled data with Hidden Markov models, support vector machines, and semi-supervised recurrent neural networks have provided moderate to high success. In this study, we successfully detect hand shifts in a historical manuscript through fuzzy soft clustering in combination with linear principal component analysis. This advance demonstrates the successful deployment of unsupervised methods for writer attribution of historical documents and forensic document analysis.



### Unsupervised Learning of Structured Representations via Closed-Loop Transcription
- **Arxiv ID**: http://arxiv.org/abs/2210.16782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16782v1)
- **Published**: 2022-10-30 09:09:05+00:00
- **Updated**: 2022-10-30 09:09:05+00:00
- **Authors**: Shengbang Tong, Xili Dai, Yubei Chen, Mingyang Li, Zengyi Li, Brent Yi, Yann LeCun, Yi Ma
- **Comment**: 17 pages
- **Journal**: None
- **Summary**: This paper proposes an unsupervised method for learning a unified representation that serves both discriminative and generative purposes. While most existing unsupervised learning approaches focus on a representation for only one of these two goals, we show that a unified representation can enjoy the mutual benefits of having both. Such a representation is attainable by generalizing the recently proposed \textit{closed-loop transcription} framework, known as CTRL, to the unsupervised setting. This entails solving a constrained maximin game over a rate reduction objective that expands features of all samples while compressing features of augmentations of each sample. Through this process, we see discriminative low-dimensional structures emerge in the resulting representations. Under comparable experimental conditions and network complexities, we demonstrate that these structured representations enable classification performance close to state-of-the-art unsupervised discriminative representations, and conditionally generated image quality significantly higher than that of state-of-the-art unsupervised generative models. Source code can be found at https://github.com/Delay-Xili/uCTRL.



### Image-free Domain Generalization via CLIP for 3D Hand Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2210.16788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16788v1)
- **Published**: 2022-10-30 09:32:37+00:00
- **Updated**: 2022-10-30 09:32:37+00:00
- **Authors**: Seongyeong Lee, Hansoo Park, Dong Uk Kim, Jihyeon Kim, Muhammadjon Boboev, Seungryul Baek
- **Comment**: None
- **Journal**: None
- **Summary**: RGB-based 3D hand pose estimation has been successful for decades thanks to large-scale databases and deep learning. However, the hand pose estimation network does not operate well for hand pose images whose characteristics are far different from the training data. This is caused by various factors such as illuminations, camera angles, diverse backgrounds in the input images, etc. Many existing methods tried to solve it by supplying additional large-scale unconstrained/target domain images to augment data space; however collecting such large-scale images takes a lot of labors. In this paper, we present a simple image-free domain generalization approach for the hand pose estimation framework that uses only source domain data. We try to manipulate the image features of the hand pose estimation network by adding the features from text descriptions using the CLIP (Contrastive Language-Image Pre-training) model. The manipulated image features are then exploited to train the hand pose estimation network via the contrastive learning framework. In experiments with STB and RHD datasets, our algorithm shows improved performance over the state-of-the-art domain generalization approaches.



### Two-Level Temporal Relation Model for Online Video Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.16795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16795v1)
- **Published**: 2022-10-30 10:01:01+00:00
- **Updated**: 2022-10-30 10:01:01+00:00
- **Authors**: Çağan Selim Çoban, Oğuzhan Keskin, Jordi Pont-Tuset, Fatma Güney
- **Comment**: None
- **Journal**: None
- **Summary**: In Video Instance Segmentation (VIS), current approaches either focus on the quality of the results, by taking the whole video as input and processing it offline; or on speed, by handling it frame by frame at the cost of competitive performance. In this work, we propose an online method that is on par with the performance of the offline counterparts. We introduce a message-passing graph neural network that encodes objects and relates them through time. We additionally propose a novel module to fuse features from the feature pyramid network with residual connections. Our model, trained end-to-end, achieves state-of-the-art performance on the YouTube-VIS dataset within the online methods. Further experiments on DAVIS demonstrate the generalization capability of our model to the video object segmentation task. Code is available at: \url{https://github.com/caganselim/TLTM}



### The Florence 4D Facial Expression Dataset
- **Arxiv ID**: http://arxiv.org/abs/2210.16807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16807v1)
- **Published**: 2022-10-30 10:45:21+00:00
- **Updated**: 2022-10-30 10:45:21+00:00
- **Authors**: F. Principi, S. Berretti, C. Ferrari, N. Otberdout, M. Daoudi, A. Del Bimbo
- **Comment**: None
- **Journal**: None
- **Summary**: Human facial expressions change dynamically, so their recognition / analysis should be conducted by accounting for the temporal evolution of face deformations either in 2D or 3D. While abundant 2D video data do exist, this is not the case in 3D, where few 3D dynamic (4D) datasets were released for public use. The negative consequence of this scarcity of data is amplified by current deep learning based-methods for facial expression analysis that require large quantities of variegate samples to be effectively trained. With the aim of smoothing such limitations, in this paper we propose a large dataset, named Florence 4D, composed of dynamic sequences of 3D face models, where a combination of synthetic and real identities exhibit an unprecedented variety of 4D facial expressions, with variations that include the classical neutral-apex transition, but generalize to expression-to-expression. All these characteristics are not exposed by any of the existing 4D datasets and they cannot even be obtained by combining more than one dataset. We strongly believe that making such a data corpora publicly available to the community will allow designing and experimenting new applications that were not possible to investigate till now. To show at some extent the difficulty of our data in terms of different identities and varying expressions, we also report a baseline experimentation on the proposed dataset that can be used as baseline.



### SL3D: Self-supervised-Self-labeled 3D Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.16810v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16810v3)
- **Published**: 2022-10-30 11:08:25+00:00
- **Updated**: 2022-12-16 08:57:41+00:00
- **Authors**: Fernando Julio Cendra, Lan Ma, Jiajun Shen, Xiaojuan Qi
- **Comment**: This paper has already been accepted by Neural Information Processing
  Systems (NeurIPS 2022) Workshop on Self-Supervised Learning: Theory and
  Practice
- **Journal**: None
- **Summary**: Deep learning has attained remarkable success in many 3D visual recognition tasks, including shape classification, object detection, and semantic segmentation. However, many of these results rely on manually collecting densely annotated real-world 3D data, which is highly time-consuming and expensive to obtain, limiting the scalability of 3D recognition tasks. Thus, we study unsupervised 3D recognition and propose a Self-supervised-Self-Labeled 3D Recognition (SL3D) framework. SL3D simultaneously solves two coupled objectives, i.e., clustering and learning feature representation to generate pseudo-labeled data for unsupervised 3D recognition. SL3D is a generic framework and can be applied to solve different 3D recognition tasks, including classification, object detection, and semantic segmentation. Extensive experiments demonstrate its effectiveness. Code is available at https://github.com/fcendra/sl3d.



### CAD 3D Model classification by Graph Neural Networks: A new approach based on STEP format
- **Arxiv ID**: http://arxiv.org/abs/2210.16815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16815v1)
- **Published**: 2022-10-30 11:27:58+00:00
- **Updated**: 2022-10-30 11:27:58+00:00
- **Authors**: L. Mandelli, S. Berretti
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a new approach for retrieval and classification of 3D models that directly performs in the Computer-Aided Design (CAD) format without any conversion to other representations like point clouds or meshes, thus avoiding any loss of information. Among the various CAD formats, we consider the widely used STEP extension, which represents a standard for product manufacturing information. This particular format represents a 3D model as a set of primitive elements such as surfaces and vertices linked together. In our approach, we exploit the linked structure of STEP files to create a graph in which the nodes are the primitive elements and the arcs are the connections between them. We then use Graph Neural Networks (GNNs) to solve the problem of model classification. Finally, we created two datasets of 3D models in native CAD format, respectively, by collecting data from the Traceparts model library and from the Configurators software modeling company. We used these datasets to test and compare our approach with respect to state-of-the-art methods that consider other 3D formats. Our code is available at https://github.com/divanoLetto/3D_STEP_Classification



### Temporal-Viewpoint Transportation Plan for Skeletal Few-shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.16820v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16820v1)
- **Published**: 2022-10-30 11:46:38+00:00
- **Updated**: 2022-10-30 11:46:38+00:00
- **Authors**: Lei Wang, Piotr Koniusz
- **Comment**: Accepted as an oral paper at the 16th Asian Conference on Computer
  Vision (ACCV 2022). It extends our arXiv preprint arXiv:2112.12668 (2021)
- **Journal**: None
- **Summary**: We propose a Few-shot Learning pipeline for 3D skeleton-based action recognition by Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE). To factor out misalignment between query and support sequences of 3D body joints, we propose an advanced variant of Dynamic Time Warping which jointly models each smooth path between the query and support frames to achieve simultaneously the best alignment in the temporal and simulated camera viewpoint spaces for end-to-end learning under the limited few-shot training data. Sequences are encoded with a temporal block encoder based on Simple Spectral Graph Convolution, a lightweight linear Graph Neural Network backbone. We also include a setting with a transformer. Finally, we propose a similarity-based loss which encourages the alignment of sequences of the same class while preventing the alignment of unrelated sequences. We show state-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D Multiview Activity II.



### Towards Versatile Embodied Navigation
- **Arxiv ID**: http://arxiv.org/abs/2210.16822v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16822v1)
- **Published**: 2022-10-30 11:53:49+00:00
- **Updated**: 2022-10-30 11:53:49+00:00
- **Authors**: Hanqing Wang, Wei Liang, Luc Van Gool, Wenguan Wang
- **Comment**: Accepted to NeurIPS 2022; Code: https://github.com/hanqingwangai/VXN
- **Journal**: None
- **Summary**: With the emergence of varied visual navigation tasks (e.g, image-/object-/audio-goal and vision-language navigation) that specify the target in different ways, the community has made appealing advances in training specialized agents capable of handling individual navigation tasks well. Given plenty of embodied navigation tasks and task-specific solutions, we address a more fundamental question: can we learn a single powerful agent that masters not one but multiple navigation tasks concurrently? First, we propose VXN, a large-scale 3D dataset that instantiates four classic navigation tasks in standardized, continuous, and audiovisual-rich environments. Second, we propose Vienna, a versatile embodied navigation agent that simultaneously learns to perform the four navigation tasks with one model. Building upon a full-attentive architecture, Vienna formulates various navigation tasks as a unified, parse-and-query procedure: the target description, augmented with four task embeddings, is comprehensively interpreted into a set of diversified goal vectors, which are refined as the navigation progresses, and used as queries to retrieve supportive context from episodic history for decision making. This enables the reuse of knowledge across navigation tasks with varying input domains/modalities. We empirically demonstrate that, compared with learning each visual navigation task individually, our multitask agent achieves comparable or even better performance with reduced complexity.



### Self-Regularized Prototypical Network for Few-Shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.16829v1
- **DOI**: 10.1016/j.patcog.2022.109018
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16829v1)
- **Published**: 2022-10-30 12:43:07+00:00
- **Updated**: 2022-10-30 12:43:07+00:00
- **Authors**: Henghui Ding, Hui Zhang, Xudong Jiang
- **Comment**: Pattern Recognition (PR)
- **Journal**: Pattern Recognition 133 (2023): 109018
- **Summary**: The deep CNNs in image semantic segmentation typically require a large number of densely-annotated images for training and have difficulties in generalizing to unseen object categories. Therefore, few-shot segmentation has been developed to perform segmentation with just a few annotated examples. In this work, we tackle the few-shot segmentation using a self-regularized prototypical network (SRPNet) based on prototype extraction for better utilization of the support information. The proposed SRPNet extracts class-specific prototype representations from support images and generates segmentation masks for query images by a distance metric - the fidelity. A direct yet effective prototype regularization on support set is proposed in SRPNet, in which the generated prototypes are evaluated and regularized on the support set itself. The extent to which the generated prototypes restore the support mask imposes an upper limit on performance. The performance on the query set should never exceed the upper limit no matter how complete the knowledge is generalized from support set to query set. With the specific prototype regularization, SRPNet fully exploits knowledge from the support and offers high-quality prototypes that are representative for each semantic class and meanwhile discriminative for different classes. The query performance is further improved by an iterative query inference (IQI) module that combines a set of regularized prototypes. Our proposed SRPNet achieves new state-of-art performance on 1-shot and 5-shot segmentation benchmarks.



### Alleviating the Sample Selection Bias in Few-shot Learning by Removing Projection to the Centroid
- **Arxiv ID**: http://arxiv.org/abs/2210.16834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16834v1)
- **Published**: 2022-10-30 13:03:13+00:00
- **Updated**: 2022-10-30 13:03:13+00:00
- **Authors**: Jing Xu, Xu Luo, Xinglin Pan, Wenjie Pei, Yanan Li, Zenglin Xu
- **Comment**: Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: Few-shot learning (FSL) targets at generalization of vision models towards unseen tasks without sufficient annotations. Despite the emergence of a number of few-shot learning methods, the sample selection bias problem, i.e., the sensitivity to the limited amount of support data, has not been well understood. In this paper, we find that this problem usually occurs when the positions of support samples are in the vicinity of task centroid -- the mean of all class centroids in the task. This motivates us to propose an extremely simple feature transformation to alleviate this problem, dubbed Task Centroid Projection Removing (TCPR). TCPR is applied directly to all image features in a given task, aiming at removing the dimension of features along the direction of the task centroid. While the exact task centroid cannot be accurately obtained from limited data, we estimate it using base features that are each similar to one of the support features. Our method effectively prevents features from being too close to the task centroid. Extensive experiments over ten datasets from different domains show that TCPR can reliably improve classification accuracy across various feature extractors, training algorithms and datasets. The code has been made available at https://github.com/KikimorMay/FSL-TCBR.



### Combining Attention Module and Pixel Shuffle for License Plate Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2210.16836v1
- **DOI**: 10.1109/SIBGRAPI55357.2022.9991753
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16836v1)
- **Published**: 2022-10-30 13:05:07+00:00
- **Updated**: 2022-10-30 13:05:07+00:00
- **Authors**: Valfride Nascimento, Rayson Laroca, Jorge de A. Lambert, William Robson Schwartz, David Menotti
- **Comment**: Accepted for presentation at the Conference on Graphics, Patterns and
  Images (SIBGRAPI) 2022
- **Journal**: None
- **Summary**: The License Plate Recognition (LPR) field has made impressive advances in the last decade due to novel deep learning approaches combined with the increased availability of training data. However, it still has some open issues, especially when the data come from low-resolution (LR) and low-quality images/videos, as in surveillance systems. This work focuses on license plate (LP) reconstruction in LR and low-quality images. We present a Single-Image Super-Resolution (SISR) approach that extends the attention/transformer module concept by exploiting the capabilities of PixelShuffle layers and that has an improved loss function based on LPR predictions. For training the proposed architecture, we use synthetic images generated by applying heavy Gaussian noise in terms of Structural Similarity Index Measure (SSIM) to the original high-resolution (HR) images. In our experiments, the proposed method outperformed the baselines both quantitatively and qualitatively. The datasets we created for this work are publicly available to the research community at https://github.com/valfride/lpr-rsr/



### 1st Place Solutions for UG2+ Challenge 2022 ATMOSPHERIC TURBULENCE MITIGATION
- **Arxiv ID**: http://arxiv.org/abs/2210.16847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16847v1)
- **Published**: 2022-10-30 14:11:36+00:00
- **Updated**: 2022-10-30 14:11:36+00:00
- **Authors**: Zhuang Liu, Zhichao Zhao, Ye Yuan, Zhi Qiao, Jinfeng Bai, Zhilong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: In this technical report, we briefly introduce the solution of our team ''summer'' for Atomospheric Turbulence Mitigation in UG$^2$+ Challenge in CVPR 2022. In this task, we propose a unified end-to-end framework to reconstruct a high quality image from distorted frames, which is mainly consists of a Restormer-based image reconstruction module and a NIMA-based image quality assessment module. Our framework is efficient and generic, which is adapted to both hot-air image and text pattern. Moreover, we elaborately synthesize more than 10 thousands of images to simulate atmospheric turbulence. And these images improve the robustness of the model. Finally, we achieve the average accuracy of 98.53\% on the reconstruction result of the text patterns, ranking 1st on the final leaderboard.



### A simple, efficient and scalable contrastive masked autoencoder for learning visual representations
- **Arxiv ID**: http://arxiv.org/abs/2210.16870v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16870v1)
- **Published**: 2022-10-30 16:21:22+00:00
- **Updated**: 2022-10-30 16:21:22+00:00
- **Authors**: Shlok Mishra, Joshua Robinson, Huiwen Chang, David Jacobs, Aaron Sarna, Aaron Maschinot, Dilip Krishnan
- **Comment**: Mishra and Robinson contributed equally
- **Journal**: None
- **Summary**: We introduce CAN, a simple, efficient and scalable method for self-supervised learning of visual representations. Our framework is a minimal and conceptually clean synthesis of (C) contrastive learning, (A) masked autoencoders, and (N) the noise prediction approach used in diffusion models. The learning mechanisms are complementary to one another: contrastive learning shapes the embedding space across a batch of image samples; masked autoencoders focus on reconstruction of the low-frequency spatial correlations in a single image sample; and noise prediction encourages the reconstruction of the high-frequency components of an image. The combined approach results in a robust, scalable and simple-to-implement algorithm. The training process is symmetric, with 50% of patches in both views being masked at random, yielding a considerable efficiency improvement over prior contrastive learning methods. Extensive empirical studies demonstrate that CAN achieves strong downstream performance under both linear and finetuning evaluations on transfer learning and robustness tasks. CAN outperforms MAE and SimCLR when pre-training on ImageNet, but is especially useful for pre-training on larger uncurated datasets such as JFT-300M: for linear probe on ImageNet, CAN achieves 75.4% compared to 73.4% for SimCLR and 64.1% for MAE. The finetuned performance on ImageNet of our ViT-L model is 86.1%, compared to 85.5% for SimCLR, and 85.4% for MAE. The overall FLOPs load of SimCLR is 70% higher than CAN for ViT-L models.



### Uncertainty-DTW for Time Series and Sequences
- **Arxiv ID**: http://arxiv.org/abs/2211.00005v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2211.00005v1)
- **Published**: 2022-10-30 17:06:55+00:00
- **Updated**: 2022-10-30 17:06:55+00:00
- **Authors**: Lei Wang, Piotr Koniusz
- **Comment**: Accepted as an oral paper at the 17th European Conference on Computer
  Vision (ECCV 2022). arXiv admin note: text overlap with arXiv:2210.16820
- **Journal**: None
- **Summary**: Dynamic Time Warping (DTW) is used for matching pairs of sequences and celebrated in applications such as forecasting the evolution of time series, clustering time series or even matching sequence pairs in few-shot action recognition. The transportation plan of DTW contains a set of paths; each path matches frames between two sequences under a varying degree of time warping, to account for varying temporal intra-class dynamics of actions. However, as DTW is the smallest distance among all paths, it may be affected by the feature uncertainty which varies across time steps/frames. Thus, in this paper, we propose to model the so-called aleatoric uncertainty of a differentiable (soft) version of DTW. To this end, we model the heteroscedastic aleatoric uncertainty of each path by the product of likelihoods from Normal distributions, each capturing variance of pair of frames. (The path distance is the sum of base distances between features of pairs of frames of the path.) The Maximum Likelihood Estimation (MLE) applied to a path yields two terms: (i) a sum of Euclidean distances weighted by the variance inverse, and (ii) a sum of log-variance regularization terms. Thus, our uncertainty-DTW is the smallest weighted path distance among all paths, and the regularization term (penalty for the high uncertainty) is the aggregate of log-variances along the path. The distance and the regularization term can be used in various objectives. We showcase forecasting the evolution of time series, estimating the Fr\'echet mean of time series, and supervised/unsupervised few-shot action recognition of the articulated human 3D body joints.



### Time-rEversed diffusioN tEnsor Transformer: A new TENET of Few-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.16897v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16897v1)
- **Published**: 2022-10-30 17:40:12+00:00
- **Updated**: 2022-10-30 17:40:12+00:00
- **Authors**: Shan Zhang, Naila Murray, Lei Wang, Piotr Koniusz
- **Comment**: Accepted at the 17th European Conference on Computer Vision (ECCV
  2022)
- **Journal**: None
- **Summary**: In this paper, we tackle the challenging problem of Few-shot Object Detection. Existing FSOD pipelines (i) use average-pooled representations that result in information loss; and/or (ii) discard position information that can help detect object instances. Consequently, such pipelines are sensitive to large intra-class appearance and geometric variations between support and query images. To address these drawbacks, we propose a Time-rEversed diffusioN tEnsor Transformer (TENET), which i) forms high-order tensor representations that capture multi-way feature occurrences that are highly discriminative, and ii) uses a transformer that dynamically extracts correlations between the query image and the entire support set, instead of a single average-pooled support embedding. We also propose a Transformer Relation Head (TRH), equipped with higher-order representations, which encodes correlations between query regions and the entire support set, while being sensitive to the positional variability of object instances. Our model achieves state-of-the-art results on PASCAL VOC, FSOD, and COCO.



### Attention Swin U-Net: Cross-Contextual Attention Mechanism for Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.16898v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16898v1)
- **Published**: 2022-10-30 17:41:35+00:00
- **Updated**: 2022-10-30 17:41:35+00:00
- **Authors**: Ehsan Khodapanah Aghdam, Reza Azad, Maral Zarvani, Dorit Merhof
- **Comment**: None
- **Journal**: None
- **Summary**: Melanoma is caused by the abnormal growth of melanocytes in human skin. Like other cancers, this life-threatening skin cancer can be treated with early diagnosis. To support a diagnosis by automatic skin lesion segmentation, several Fully Convolutional Network (FCN) approaches, specifically the U-Net architecture, have been proposed. The U-Net model with a symmetrical architecture has exhibited superior performance in the segmentation task. However, the locality restriction of the convolutional operation incorporated in the U-Net architecture limits its performance in capturing long-range dependency, which is crucial for the segmentation task in medical images. To address this limitation, recently a Transformer based U-Net architecture that replaces the CNN blocks with the Swin Transformer module has been proposed to capture both local and global representation. In this paper, we propose Att-SwinU-Net, an attention-based Swin U-Net extension, for medical image segmentation. In our design, we seek to enhance the feature re-usability of the network by carefully designing the skip connection path. We argue that the classical concatenation operation utilized in the skip connection path can be further improved by incorporating an attention mechanism. By performing a comprehensive ablation study on several skin lesion segmentation datasets, we demonstrate the effectiveness of our proposed attention mechanism.



### High Resolution Multi-Scale RAFT (Robust Vision Challenge 2022)
- **Arxiv ID**: http://arxiv.org/abs/2210.16900v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16900v1)
- **Published**: 2022-10-30 17:48:11+00:00
- **Updated**: 2022-10-30 17:48:11+00:00
- **Authors**: Azin Jahedi, Maximilian Luz, Lukas Mehl, Marc Rivinius, Andrés Bruhn
- **Comment**: Technical report for the Robust Vision Challenge 2022
- **Journal**: None
- **Summary**: In this report, we present our optical flow approach, MS-RAFT+, that won the Robust Vision Challenge 2022. It is based on the MS-RAFT method, which successfully integrates several multi-scale concepts into single-scale RAFT. Our approach extends this method by exploiting an additional finer scale for estimating the flow, which is made feasible by on-demand cost computation. This way, it can not only operate at half the original resolution, but also use MS-RAFT's shared convex upsampler to obtain full resolution flow. Moreover, our approach relies on an adjusted fine-tuning scheme during training. This in turn aims at improving the generalization across benchmarks. Among all participating methods in the Robust Vision Challenge, our approach ranks first on VIPER and second on KITTI, Sintel, and Middlebury, resulting in the first place of the overall ranking.



### Foreign Object Debris Detection for Airport Pavement Images based on Self-supervised Localization and Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2210.16901v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16901v1)
- **Published**: 2022-10-30 17:48:57+00:00
- **Updated**: 2022-10-30 17:48:57+00:00
- **Authors**: Travis Munyer, Daniel Brinkman, Xin Zhong, Chenyu Huang, Iason Konstantzos
- **Comment**: This paper has been accepted for publication by the 2022
  International Conference on Computational Science & Computational
  Intelligence (CSCI'22), Research Track on Signal & Image Processing, Computer
  Vision & Pattern Recognition
- **Journal**: None
- **Summary**: Supervised object detection methods provide subpar performance when applied to Foreign Object Debris (FOD) detection because FOD could be arbitrary objects according to the Federal Aviation Administration (FAA) specification. Current supervised object detection algorithms require datasets that contain annotated examples of every to-be-detected object. While a large and expensive dataset could be developed to include common FOD examples, it is infeasible to collect all possible FOD examples in the dataset representation because of the open-ended nature of FOD. Limitations of the dataset could cause FOD detection systems driven by those supervised algorithms to miss certain FOD, which can become dangerous to airport operations. To this end, this paper presents a self-supervised FOD localization by learning to predict the runway images, which avoids the enumeration of FOD annotation examples. The localization method utilizes the Vision Transformer (ViT) to improve localization performance. The experiments show that the method successfully detects arbitrary FOD in real-world runway situations. The paper also provides an extension to the localization result to perform classification; a feature that can be useful to downstream tasks. To train the localization, this paper also presents a simple and realistic dataset creation framework that only collects clean runway images. The training and testing data for this method are collected at a local airport using unmanned aircraft systems (UAS). Additionally, the developed dataset is provided for public use and further studies.



### FatNet: High Resolution Kernels for Classification Using Fully Convolutional Optical Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.16914v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16914v1)
- **Published**: 2022-10-30 18:31:46+00:00
- **Updated**: 2022-10-30 18:31:46+00:00
- **Authors**: Riad Ibadulla, Thomas M. Chen, Constantino Carlos Reyes-Aldasoro
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes the transformation of a traditional in-silico classification network into an optical fully convolutional neural network with high-resolution feature maps and kernels. When using the free-space 4f system to accelerate the inference speed of neural networks, higher resolutions of feature maps and kernels can be used without the loss in frame rate. We present FatNet for the classification of images, which is more compatible with free-space acceleration than standard convolutional classifiers. It neglects the standard combination of convolutional feature extraction and classifier dense layers by performing both in one fully convolutional network. This approach takes full advantage of the parallelism in the 4f free-space system and performs fewer conversions between electronics and optics by reducing the number of channels and increasing the resolution, making the network faster in optics than off-the-shelf networks. To demonstrate the capabilities of FatNet, it trained with the CIFAR100 dataset on GPU and the simulator of the 4f system, then compared the results against ResNet-18. The results show 8.2 times fewer convolution operations at the cost of only 6% lower accuracy compared to the original network. These are promising results for the approach of training deep learning with high-resolution kernels in the direction towards the upcoming optics era.



### See as a Bee: UV Sensor for Aerial Strawberry Crop Monitoring
- **Arxiv ID**: http://arxiv.org/abs/2210.16923v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16923v1)
- **Published**: 2022-10-30 18:56:24+00:00
- **Updated**: 2022-10-30 18:56:24+00:00
- **Authors**: Megan Heath, Ali Imran, David St-Onge
- **Comment**: The video reference is: https://www.youtube.com/watch?v=ZSasfgOsjAY.
  This paper has been submitted to ICRA 2023
- **Journal**: None
- **Summary**: Precision agriculture aims to use technological tools for the agro-food sector to increase productivity, cut labor costs, and reduce the use of resources. This work takes inspiration from bees vision to design a remote sensing system tailored to incorporate UV-reflectance into a flower detector. We demonstrate how this approach can provide feature-rich images for deep learning strawberry flower detection and we apply it to a scalable, yet cost effective aerial monitoring robotic system in the field. We also compare the performance of our UV-G-B image detector with a similar work that utilizes RGB images.



### OGInfra: Geolocating Oil & Gas Infrastructure using Remote Sensing based Active Fire Data
- **Arxiv ID**: http://arxiv.org/abs/2210.16924v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.16924v1)
- **Published**: 2022-10-30 18:58:15+00:00
- **Updated**: 2022-10-30 18:58:15+00:00
- **Authors**: Samyak Prajapati, Amrit Raj, Yash Chaudhari, Akhilesh Nandwal, Japman Singh Monga
- **Comment**: None
- **Journal**: None
- **Summary**: Remote sensing has become a crucial part of our daily lives, whether it be from triangulating our location using GPS or providing us with a weather forecast. It has multiple applications in domains such as military, socio-economical, commercial, and even in supporting humanitarian efforts. This work proposes a novel technique for the automated geo-location of Oil & Gas infrastructure with the use of Active Fire Data from the NASA FIRMS data repository & Deep Learning techniques; achieving a top accuracy of 90.68% with the use of ResNet101.



### Context-empowered Visual Attention Prediction in Pedestrian Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2210.16933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.16933v1)
- **Published**: 2022-10-30 19:38:17+00:00
- **Updated**: 2022-10-30 19:38:17+00:00
- **Authors**: Igor Vozniak, Philipp Mueller, Lorena Hell, Nils Lipp, Ahmed Abouelazm, Christian Mueller
- **Comment**: None
- **Journal**: None
- **Summary**: Effective and flexible allocation of visual attention is key for pedestrians who have to navigate to a desired goal under different conditions of urgency and safety preferences. While automatic modelling of pedestrian attention holds great promise to improve simulations of pedestrian behavior, current saliency prediction approaches mostly focus on generic free-viewing scenarios and do not reflect the specific challenges present in pedestrian attention prediction. In this paper, we present Context-SalNET, a novel encoder-decoder architecture that explicitly addresses three key challenges of visual attention prediction in pedestrians: First, Context-SalNET explicitly models the context factors urgency and safety preference in the latent space of the encoder-decoder model. Second, we propose the exponentially weighted mean squared error loss (ew-MSE) that is able to better cope with the fact that only a small part of the ground truth saliency maps consist of non-zero entries. Third, we explicitly model epistemic uncertainty to account for the fact that training data for pedestrian attention prediction is limited. To evaluate Context-SalNET, we recorded the first dataset of pedestrian visual attention in VR that includes explicit variation of the context factors urgency and safety preference. Context-SalNET achieves clear improvements over state-of-the-art saliency prediction approaches as well as over ablations. Our novel dataset will be made fully available and can serve as a valuable resource for further research on pedestrian attention prediction.



### ViTASD: Robust Vision Transformer Baselines for Autism Spectrum Disorder Facial Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2210.16943v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.16943v2)
- **Published**: 2022-10-30 20:38:56+00:00
- **Updated**: 2023-03-11 05:22:12+00:00
- **Authors**: Xu Cao, Wenqian Ye, Elena Sizikova, Xue Bai, Megan Coffee, Hongwu Zeng, Jianguo Cao
- **Comment**: 5 pages, 3 figures, Accepted by the ICASSP 2023
- **Journal**: None
- **Summary**: Autism spectrum disorder (ASD) is a lifelong neurodevelopmental disorder with very high prevalence around the world. Research progress in the field of ASD facial analysis in pediatric patients has been hindered due to a lack of well-established baselines. In this paper, we propose the use of the Vision Transformer (ViT) for the computational analysis of pediatric ASD. The presented model, known as ViTASD, distills knowledge from large facial expression datasets and offers model structure transferability. Specifically, ViTASD employs a vanilla ViT to extract features from patients' face images and adopts a lightweight decoder with a Gaussian Process layer to enhance the robustness for ASD analysis. Extensive experiments conducted on standard ASD facial analysis benchmarks show that our method outperforms all of the representative approaches in ASD facial analysis, while the ViTASD-L achieves a new state-of-the-art. Our code and pretrained models are available at https://github.com/IrohXu/ViTASD.



### Few-Shot Classification of Skin Lesions from Dermoscopic Images by Meta-Learning Representative Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2210.16954v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.16954v1)
- **Published**: 2022-10-30 21:27:15+00:00
- **Updated**: 2022-10-30 21:27:15+00:00
- **Authors**: Karthik Desingu, Mirunalini P., Aravindan Chandrabose
- **Comment**: 10 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: Annotated images and ground truth for the diagnosis of rare and novel diseases are scarce. This is expected to prevail, considering the small number of affected patient population and limited clinical expertise to annotate images. Further, the frequently occurring long-tailed class distributions in skin lesion and other disease classification datasets cause conventional training approaches to lead to poor generalization due to biased class priors. Few-shot learning, and meta-learning in general, aim to overcome these issues by aiming to perform well in low data regimes. This paper focuses on improving meta-learning for the classification of dermoscopic images. Specifically, we propose a baseline supervised method on the meta-training set that allows a network to learn highly representative and generalizable feature embeddings for images, that are readily transferable to new few-shot learning tasks. We follow some of the previous work in literature that posit that a representative feature embedding can be more effective than complex meta-learning algorithms. We empirically prove the efficacy of the proposed meta-training method on dermoscopic images for learning embeddings, and show that even simple linear classifiers trained atop these representations suffice to outperform some of the usual meta-learning methods.



### Gravitational Dimensionality Reduction Using Newtonian Gravity and Einstein's General Relativity
- **Arxiv ID**: http://arxiv.org/abs/2211.01369v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, gr-qc, physics.class-ph
- **Links**: [PDF](http://arxiv.org/pdf/2211.01369v1)
- **Published**: 2022-10-30 23:40:06+00:00
- **Updated**: 2022-10-30 23:40:06+00:00
- **Authors**: Benyamin Ghojogh, Smriti Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the effectiveness of using machine learning in physics, it has been widely received increased attention in the literature. However, the notion of applying physics in machine learning has not been given much awareness to. This work is a hybrid of physics and machine learning where concepts of physics are used in machine learning. We propose the supervised Gravitational Dimensionality Reduction (GDR) algorithm where the data points of every class are moved to each other for reduction of intra-class variances and better separation of classes. For every data point, the other points are considered to be gravitational particles, such as stars, where the point is attracted to the points of its class by gravity. The data points are first projected onto a spacetime manifold using principal component analysis. We propose two variants of GDR -- one with the Newtonian gravity and one with the Einstein's general relativity. The former uses Newtonian gravity in a straight line between points but the latter moves data points along the geodesics of spacetime manifold. For GDR with relativity gravitation, we use both Schwarzschild and Minkowski metric tensors to cover both general relativity and special relativity. Our simulations show the effectiveness of GDR in discrimination of classes.



