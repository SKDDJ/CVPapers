# Arxiv Papers in cs.CV on 2022-10-21
### InfraRed Investigation in Singapore (IRIS) Observatory: Urban heat island contributors and mitigators analysis using neighborhood-scale thermal imaging
- **Arxiv ID**: http://arxiv.org/abs/2210.11663v1
- **DOI**: None
- **Categories**: **physics.ao-ph**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2210.11663v1)
- **Published**: 2022-10-21 01:21:50+00:00
- **Updated**: 2022-10-21 01:21:50+00:00
- **Authors**: Miguel Martin, Vasantha Ramani, Clayton Miller
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies heat fluxes from contributors and mitigators of urban heat islands using thermal images and weather data. Thermal images were collected from an observatory operating on the rooftop of a building between November 2021 and April 2022. Over the same period, an automatic weather station network was used to measure weather conditions at several locations on a university campus in Singapore. From data collected by the observatory and the automatic weather station network, a method was developed to estimate the heat emitted by building facades, vegetation, and traffic. Before performing the analysis of urban heat fluxes, it was observed that the surface temperature collected from the observatory is sensitive to some variables. After the sensitivity analysis, thermal images were calibrated against measurements of the surface temperature in an outdoor environment. Finally, several contributors and mitigators of urban heat islands were analyzed from heat fluxes assessed with thermal images and weather data. According to thermal images collected by the rooftop observatory, concrete walls are an important contributor to urban heat islands due to the longwave radiation they emit at night. Vegetation, on the other hand, seems to be an effective mitigator because of latent heat fluxes generated by evapotranspiration. Traffic looks to be a negligible source of heat if considered over a small portion of a road. In the future, more efforts can be made to estimate the magnitude of the heat released by an air-conditioning system from thermal images.



### Doctors Handwritten Prescription Recognition System In Multi Language Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.11666v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2210.11666v1)
- **Published**: 2022-10-21 01:32:49+00:00
- **Updated**: 2022-10-21 01:32:49+00:00
- **Authors**: Pavithiran G, Sharan Padmanabhan, Nuvvuru Divya, Aswathy V, Irene Jerusha P, Chandar B
- **Comment**: 5 pages, 7 figures
- **Journal**: None
- **Summary**: Doctors typically write in incomprehensible handwriting, making it difficult for both the general public and some pharmacists to understand the medications they have prescribed. It is not ideal for them to write the prescription quietly and methodically because they will be dealing with dozens of patients every day and will be swamped with work.As a result, their handwriting is illegible. This may result in reports or prescriptions consisting of short forms and cursive writing that a typical person or pharmacist won't be able to read properly, which will cause prescribed medications to be misspelled. However, some individuals are accustomed to writing prescriptions in regional languages because we all live in an area with a diversity of regional languages. It makes analyzing the content much more challenging. So, in this project, we'll use a recognition system to build a tool that can translate the handwriting of physicians in any language. This system will be made into an application which is fully autonomous in functioning. As the user uploads the prescription image the program will pre-process the image by performing image pre-processing, and word segmentations initially before processing the image for training. And it will be done for every language we require the model to detect. And as of the deduction model will be made using deep learning techniques including CNN, RNN, and LSTM, which are utilized to train the model. To match words from various languages that will be written in the system, Unicode will be used. Furthermore, fuzzy search and market basket analysis are employed to offer an end result that will be optimized from the pharmaceutical database and displayed to the user as a structured output.



### RGB-Only Reconstruction of Tabletop Scenes for Collision-Free Manipulator Control
- **Arxiv ID**: http://arxiv.org/abs/2210.11668v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11668v2)
- **Published**: 2022-10-21 01:45:08+00:00
- **Updated**: 2023-03-10 06:13:13+00:00
- **Authors**: Zhenggang Tang, Balakumar Sundaralingam, Jonathan Tremblay, Bowen Wen, Ye Yuan, Stephen Tyree, Charles Loop, Alexander Schwing, Stan Birchfield
- **Comment**: ICRA 2023. Project page at https://ngp-mpc.github.io/
- **Journal**: None
- **Summary**: We present a system for collision-free control of a robot manipulator that uses only RGB views of the world. Perceptual input of a tabletop scene is provided by multiple images of an RGB camera (without depth) that is either handheld or mounted on the robot end effector. A NeRF-like process is used to reconstruct the 3D geometry of the scene, from which the Euclidean full signed distance function (ESDF) is computed. A model predictive control algorithm is then used to control the manipulator to reach a desired pose while avoiding obstacles in the ESDF. We show results on a real dataset collected and annotated in our lab.



### Error-Covariance Analysis of Monocular Pose Estimation Using Total Least Squares
- **Arxiv ID**: http://arxiv.org/abs/2210.12157v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2210.12157v1)
- **Published**: 2022-10-21 01:46:18+00:00
- **Updated**: 2022-10-21 01:46:18+00:00
- **Authors**: Saeed Maleki, John Crassidis, Yang Cheng, Matthias Schmid
- **Comment**: arXiv admin note: text overlap with arXiv:2106.11522. text overlap
  with arXiv:2210.11697
- **Journal**: None
- **Summary**: This study presents a theoretical structure for the monocular pose estimation problem using the total least squares. The unit-vector line-of-sight observations of the features are extracted from the monocular camera images. First, the optimization framework is formulated for the pose estimation problem with observation vectors extracted from unit vectors from the camera center-of-projection, pointing towards the image features. The attitude and position solutions obtained via the derived optimization framework are proven to reach the Cram\'er-Rao lower bound under the small angle approximation of the attitude errors. Specifically, The Fisher Information Matrix and the Cram\'er-Rao bounds are evaluated and compared to the analytical derivations of the error-covariance expressions to rigorously prove the optimality of the estimates. The sensor data for the measurement model is provided through a series of vector observations, and two fully populated noise-covariance matrices are assumed for the body and reference observation data. The inverse of the former matrices appear in terms of a series of weight matrices in the cost function. The proposed solution is simulated in a Monte-Carlo framework with 10,000 samples to validate the error-covariance analysis.



### Meta Input: How to Leverage Off-the-Shelf Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.13186v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.13186v1)
- **Published**: 2022-10-21 02:11:38+00:00
- **Updated**: 2022-10-21 02:11:38+00:00
- **Authors**: Minsu Kim, Youngjoon Yu, Sungjune Park, Yong Man Ro
- **Comment**: None
- **Journal**: None
- **Summary**: These days, although deep neural networks (DNNs) have achieved a noticeable progress in a wide range of research area, it lacks the adaptability to be employed in the real-world applications because of the environment discrepancy problem. Such a problem originates from the difference between training and testing environments, and it is widely known that it causes serious performance degradation, when a pretrained DNN model is applied to a new testing environment. Therefore, in this paper, we introduce a novel approach that allows end-users to exploit pretrained DNN models in their own testing environment without modifying the models. To this end, we present a \textit{meta input} which is an additional input transforming the distribution of testing data to be aligned with that of training data. The proposed meta input can be optimized with a small number of testing data only by considering the relation between testing input data and its output prediction. Also, it does not require any knowledge of the network's internal architecture and modification of its weight parameters. Then, the obtained meta input is added to testing data in order to shift the distribution of testing data to that of originally used training data. As a result, end-users can exploit well-trained models in their own testing environment which can differ from the training environment. We validate the effectiveness and versatility of the proposed meta input by showing the robustness against the environment discrepancy through the comprehensive experiments with various tasks.



### MEEV: Body Mesh Estimation On Egocentric Video
- **Arxiv ID**: http://arxiv.org/abs/2210.14165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14165v1)
- **Published**: 2022-10-21 02:20:50+00:00
- **Updated**: 2022-10-21 02:20:50+00:00
- **Authors**: Nicolas Monet, Dongyoon Wee
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: This technical report introduces our solution, MEEV, proposed to the EgoBody Challenge at ECCV 2022. Captured from head-mounted devices, the dataset consists of human body shape and motion of interacting people. The EgoBody dataset has challenges such as occluded body or blurry image. In order to overcome the challenges, MEEV is designed to exploit multiscale features for rich spatial information. Besides, to overcome the limited size of dataset, the model is pre-trained with the dataset aggregated 2D and 3D pose estimation datasets. Achieving 82.30 for MPJPE and 92.93 for MPVPE, MEEV has won the EgoBody Challenge at ECCV 2022, which shows the effectiveness of the proposed method. The code is available at https://github.com/clovaai/meev



### Video Summarization Overview
- **Arxiv ID**: http://arxiv.org/abs/2210.11707v1
- **DOI**: 10.1561/0600000099
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11707v1)
- **Published**: 2022-10-21 03:29:31+00:00
- **Updated**: 2022-10-21 03:29:31+00:00
- **Authors**: Mayu Otani, Yale Song, Yang Wang
- **Comment**: 53 pages
- **Journal**: Foundations and Trends in Computer Graphics and Vision: Vol. 13:
  No. 4, pp 284-335 (2022)
- **Summary**: With the broad growth of video capturing devices and applications on the web, it is more demanding to provide desired video content for users efficiently. Video summarization facilitates quickly grasping video content by creating a compact summary of videos. Much effort has been devoted to automatic video summarization, and various problem settings and approaches have been proposed. Our goal is to provide an overview of this field. This survey covers early studies as well as recent approaches which take advantage of deep learning techniques. We describe video summarization approaches and their underlying concepts. We also discuss benchmarks and evaluations. We overview how prior work addressed evaluation and detail the pros and cons of the evaluation protocols. Last but not least, we discuss open challenges in this field.



### A Survey of Data Optimization for Problems in Computer Vision Datasets
- **Arxiv ID**: http://arxiv.org/abs/2210.11717v1
- **DOI**: None
- **Categories**: **cs.CV**, A.1
- **Links**: [PDF](http://arxiv.org/pdf/2210.11717v1)
- **Published**: 2022-10-21 03:58:43+00:00
- **Updated**: 2022-10-21 03:58:43+00:00
- **Authors**: Zhijing Wan, Zhixiang Wang, CheukTing Chung, Zheng Wang
- **Comment**: 30 pages, 7 figures
- **Journal**: None
- **Summary**: Recent years have witnessed remarkable progress in artificial intelligence (AI) thanks to refined deep network structures, powerful computing devices, and large-scale labeled datasets. However, researchers have mainly invested in the optimization of models and computational devices, leading to the fact that good models and powerful computing devices are currently readily available, while datasets are still stuck at the initial stage of large-scale but low quality. Data becomes a major obstacle to AI development. Taking note of this, we dig deeper and find that there has been some but unstructured work on data optimization. They focus on various problems in datasets and attempt to improve dataset quality by optimizing its structure to facilitate AI development. In this paper, we present the first review of recent advances in this area. First, we summarize and analyze various problems that exist in large-scale computer vision datasets. We then define data optimization and classify data optimization algorithms into three directions according to the optimization form: data sampling, data subset selection, and active learning. Next, we organize these data optimization works according to data problems addressed, and provide a systematic and comparative description. Finally, we summarize the existing literature and propose some potential future research topics.



### CRT-6D: Fast 6D Object Pose Estimation with Cascaded Refinement Transformers
- **Arxiv ID**: http://arxiv.org/abs/2210.11718v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11718v1)
- **Published**: 2022-10-21 04:06:52+00:00
- **Updated**: 2022-10-21 04:06:52+00:00
- **Authors**: Pedro Castro, Tae-Kyun Kim
- **Comment**: Accepted at WACV2023
- **Journal**: None
- **Summary**: Learning based 6D object pose estimation methods rely on computing large intermediate pose representations and/or iteratively refining an initial estimation with a slow render-compare pipeline. This paper introduces a novel method we call Cascaded Pose Refinement Transformers, or CRT-6D. We replace the commonly used dense intermediate representation with a sparse set of features sampled from the feature pyramid we call OSKFs(Object Surface Keypoint Features) where each element corresponds to an object keypoint. We employ lightweight deformable transformers and chain them together to iteratively refine proposed poses over the sampled OSKFs. We achieve inference runtimes 2x faster than the closest real-time state of the art methods while supporting up to 21 objects on a single model. We demonstrate the effectiveness of CRT-6D by performing extensive experiments on the LM-O and YCBV datasets. Compared to real-time methods, we achieve state of the art on LM-O and YCB-V, falling slightly behind methods with inference runtimes one order of magnitude higher. The source code is available at: https://github.com/PedroCastro/CRT-6D



### Context-Enhanced Stereo Transformer
- **Arxiv ID**: http://arxiv.org/abs/2210.11719v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.11719v1)
- **Published**: 2022-10-21 04:10:47+00:00
- **Updated**: 2022-10-21 04:10:47+00:00
- **Authors**: Weiyu Guo, Zhaoshuo Li, Yongkui Yang, Zheng Wang, Russell H. Taylor, Mathias Unberath, Alan Yuille, Yingwei Li
- **Comment**: Accepted by ECCV2022
- **Journal**: None
- **Summary**: Stereo depth estimation is of great interest for computer vision research. However, existing methods struggles to generalize and predict reliably in hazardous regions, such as large uniform regions. To overcome these limitations, we propose Context Enhanced Path (CEP). CEP improves the generalization and robustness against common failure cases in existing solutions by capturing the long-range global information. We construct our stereo depth estimation model, Context Enhanced Stereo Transformer (CSTR), by plugging CEP into the state-of-the-art stereo depth estimation method Stereo Transformer. CSTR is examined on distinct public datasets, such as Scene Flow, Middlebury-2014, KITTI-2015, and MPI-Sintel. We find CSTR outperforms prior approaches by a large margin. For example, in the zero-shot synthetic-to-real setting, CSTR outperforms the best competing approaches on Middlebury-2014 dataset by 11%. Our extensive experiments demonstrate that the long-range information is critical for stereo matching task and CEP successfully captures such information.



### AROS: Affordance Recognition with One-Shot Human Stances
- **Arxiv ID**: http://arxiv.org/abs/2210.11725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11725v1)
- **Published**: 2022-10-21 04:29:21+00:00
- **Updated**: 2022-10-21 04:29:21+00:00
- **Authors**: Abel Pacheco-Ortega, Walterio Mayol-Cuevas
- **Comment**: 8 figures, 5 tables, supplementary material
- **Journal**: None
- **Summary**: We present AROS, a one-shot learning approach that uses an explicit representation of interactions between highly-articulated human poses and 3D scenes. The approach is one-shot as the method does not require re-training to add new affordance instances. Furthermore, only one or a small handful of examples of the target pose are needed to describe the interaction. Given a 3D mesh of a previously unseen scene, we can predict affordance locations that support the interactions and generate corresponding articulated 3D human bodies around them. We evaluate on three public datasets of scans of real environments with varied degrees of noise. Via rigorous statistical analysis of crowdsourced evaluations, results show that our one-shot approach outperforms data-intensive baselines by up to 80\%.



### Distilling the Undistillable: Learning from a Nasty Teacher
- **Arxiv ID**: http://arxiv.org/abs/2210.11728v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11728v1)
- **Published**: 2022-10-21 04:35:44+00:00
- **Updated**: 2022-10-21 04:35:44+00:00
- **Authors**: Surgan Jandial, Yash Khasbage, Arghya Pal, Vineeth N Balasubramanian, Balaji Krishnamurthy
- **Comment**: Published in main track of ECCV 2022, 17 pages with references, 5
  figures, 6 tables
- **Journal**: ECCV 2022
- **Summary**: The inadvertent stealing of private/sensitive information using Knowledge Distillation (KD) has been getting significant attention recently and has guided subsequent defense efforts considering its critical nature. Recent work Nasty Teacher proposed to develop teachers which can not be distilled or imitated by models attacking it. However, the promise of confidentiality offered by a nasty teacher is not well studied, and as a further step to strengthen against such loopholes, we attempt to bypass its defense and steal (or extract) information in its presence successfully. Specifically, we analyze Nasty Teacher from two different directions and subsequently leverage them carefully to develop simple yet efficient methodologies, named as HTC and SCM, which increase the learning from Nasty Teacher by upto 68.63% on standard datasets. Additionally, we also explore an improvised defense method based on our insights of stealing. Our detailed set of experiments and ablations on diverse models/settings demonstrate the efficacy of our approach.



### Generative Range Imaging for Learning Scene Priors of 3D LiDAR Data
- **Arxiv ID**: http://arxiv.org/abs/2210.11750v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11750v1)
- **Published**: 2022-10-21 06:08:39+00:00
- **Updated**: 2022-10-21 06:08:39+00:00
- **Authors**: Kazuto Nakashima, Yumi Iwashita, Ryo Kurazume
- **Comment**: Accepted to WACV 2023
- **Journal**: None
- **Summary**: 3D LiDAR sensors are indispensable for the robust vision of autonomous mobile robots. However, deploying LiDAR-based perception algorithms often fails due to a domain gap from the training environment, such as inconsistent angular resolution and missing properties. Existing studies have tackled the issue by learning inter-domain mapping, while the transferability is constrained by the training configuration and the training is susceptible to peculiar lossy noises called ray-drop. To address the issue, this paper proposes a generative model of LiDAR range images applicable to the data-level domain transfer. Motivated by the fact that LiDAR measurement is based on point-by-point range imaging, we train an implicit image representation-based generative adversarial networks along with a differentiable ray-drop effect. We demonstrate the fidelity and diversity of our model in comparison with the point-based and image-based state-of-the-art generative models. We also showcase upsampling and restoration applications. Furthermore, we introduce a Sim2Real application for LiDAR semantic segmentation. We demonstrate that our method is effective as a realistic ray-drop simulator and outperforms state-of-the-art methods.



### Dissecting Deep Metric Learning Losses for Image-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2210.13188v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.13188v1)
- **Published**: 2022-10-21 06:48:27+00:00
- **Updated**: 2022-10-21 06:48:27+00:00
- **Authors**: Hong Xuan, Xi Chen
- **Comment**: arXiv admin note: text overlap with arXiv:2201.11307
- **Journal**: WACV2023
- **Summary**: Visual-Semantic Embedding (VSE) is a prevalent approach in image-text retrieval by learning a joint embedding space between the image and language modalities where semantic similarities would be preserved. The triplet loss with hard-negative mining has become the de-facto objective for most VSE methods. Inspired by recent progress in deep metric learning (DML) in the image domain which gives rise to new loss functions that outperform triplet loss, in this paper, we revisit the problem of finding better objectives for VSE in image-text matching. Despite some attempts in designing losses based on gradient movement, most DML losses are defined empirically in the embedding space. Instead of directly applying these loss functions which may lead to sub-optimal gradient updates in model parameters, in this paper we present a novel Gradient-based Objective AnaLysis framework, or \textit{GOAL}, to systematically analyze the combinations and reweighting of the gradients in existing DML functions. With the help of this analysis framework, we further propose a new family of objectives in the gradient space exploring different gradient combinations. In the event that the gradients are not integrable to a valid loss function, we implement our proposed objectives such that they would directly operate in the gradient space instead of on the losses in the embedding space. Comprehensive experiments have demonstrated that our novel objectives have consistently improved performance over baselines across different visual/text features and model frameworks. We also showed the generalizability of the GOAL framework by extending it to other models using triplet family losses including vision-language model with heavy cross-modal interactions and have achieved state-of-the-art results on the image-text retrieval tasks on COCO and Flick30K.



### PoseScript: 3D Human Poses from Natural Language
- **Arxiv ID**: http://arxiv.org/abs/2210.11795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11795v1)
- **Published**: 2022-10-21 08:18:49+00:00
- **Updated**: 2022-10-21 08:18:49+00:00
- **Authors**: Ginger Delmas, Philippe Weinzaepfel, Thomas Lucas, Francesc Moreno-Noguer, Grégory Rogez
- **Comment**: Published in ECCV 2022
- **Journal**: None
- **Summary**: Natural language is leveraged in many computer vision tasks such as image captioning, cross-modal retrieval or visual question answering, to provide fine-grained semantic information. While human pose is key to human understanding, current 3D human pose datasets lack detailed language descriptions. In this work, we introduce the PoseScript dataset, which pairs a few thousand 3D human poses from AMASS with rich human-annotated descriptions of the body parts and their spatial relationships. To increase the size of this dataset to a scale compatible with typical data hungry learning algorithms, we propose an elaborate captioning process that generates automatic synthetic descriptions in natural language from given 3D keypoints. This process extracts low-level pose information -- the posecodes -- using a set of simple but generic rules on the 3D keypoints. The posecodes are then combined into higher level textual descriptions using syntactic rules. Automatic annotations substantially increase the amount of available data, and make it possible to effectively pretrain deep models for finetuning on human captions. To demonstrate the potential of annotated poses, we show applications of the PoseScript dataset to retrieval of relevant poses from large-scale datasets and to synthetic pose generation, both based on a textual pose description.



### Unsupervised Image Semantic Segmentation through Superpixels and Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.11810v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11810v1)
- **Published**: 2022-10-21 08:35:18+00:00
- **Updated**: 2022-10-21 08:35:18+00:00
- **Authors**: Moshe Eliasof, Nir Ben Zikri, Eran Treister
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised image segmentation is an important task in many real-world scenarios where labelled data is of scarce availability. In this paper we propose a novel approach that harnesses recent advances in unsupervised learning using a combination of Mutual Information Maximization (MIM), Neural Superpixel Segmentation and Graph Neural Networks (GNNs) in an end-to-end manner, an approach that has not been explored yet. We take advantage of the compact representation of superpixels and combine it with GNNs in order to learn strong and semantically meaningful representations of images. Specifically, we show that our GNN based approach allows to model interactions between distant pixels in the image and serves as a strong prior to existing CNNs for an improved accuracy. Our experiments reveal both the qualitative and quantitative advantages of our approach compared to current state-of-the-art methods over four popular datasets.



### Isomorphic mesh generation from point clouds with multilayer perceptrons
- **Arxiv ID**: http://arxiv.org/abs/2210.14157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.14157v1)
- **Published**: 2022-10-21 08:37:17+00:00
- **Updated**: 2022-10-21 08:37:17+00:00
- **Authors**: Shoko Miyauchi, Ken'ichi Morooka, Ryo Kurazume
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new neural network, called isomorphic mesh generator (iMG), which generates isomorphic meshes from point clouds containing noise and missing parts. Isomorphic meshes of arbitrary objects have a unified mesh structure even though the objects belong to different classes. This unified representation enables surface models to be handled by DNNs. Moreover, the unified mesh structure of isomorphic meshes enables the same process to be applied to all isomorphic meshes; although in the case of general mesh models, we need to consider the processes depending on their mesh structures. Therefore, the use of isomorphic meshes leads to efficient memory usage and calculation time compared with general mesh models. As iMG is a data-free method, preparing any point clouds as training data in advance is unnecessary, except a point cloud of the target object used as the input data of iMG. Additionally, iMG outputs an isomorphic mesh obtained by mapping a reference mesh to a given input point cloud. To estimate the mapping function stably, we introduce a step-by-step mapping strategy. This strategy achieves a flexible deformation while maintaining the structure of the reference mesh. From simulation and experiments using a mobile phone, we confirmed that iMG can generate isomorphic meshes of given objects reliably even when the input point cloud includes noise and missing parts.



### Self-Supervised Pretraining on Satellite Imagery: a Case Study on Label-Efficient Vehicle Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.11815v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11815v1)
- **Published**: 2022-10-21 08:41:22+00:00
- **Updated**: 2022-10-21 08:41:22+00:00
- **Authors**: Jules BOURCIER, Thomas Floquet, Gohar Dashyan, Tugdual Ceillier, Karteek Alahari, Jocelyn Chanussot
- **Comment**: None
- **Journal**: None
- **Summary**: In defense-related remote sensing applications, such as vehicle detection on satellite imagery, supervised learning requires a huge number of labeled examples to reach operational performances. Such data are challenging to obtain as it requires military experts, and some observables are intrinsically rare. This limited labeling capability, as well as the large number of unlabeled images available due to the growing number of sensors, make object detection on remote sensing imagery highly relevant for self-supervised learning. We study in-domain self-supervised representation learning for object detection on very high resolution optical satellite imagery, that is yet poorly explored. For the first time to our knowledge, we study the problem of label efficiency on this task. We use the large land use classification dataset Functional Map of the World to pretrain representations with an extension of the Momentum Contrast framework. We then investigate this model's transferability on a real-world task of fine-grained vehicle detection and classification on Preligens proprietary data, which is designed to be representative of an operational use case of strategic site surveillance. We show that our in-domain self-supervised learning model is competitive with ImageNet pretraining, and outperforms it in the low-label regime.



### Motion Matters: A Novel Motion Modeling For Cross-View Gait Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.11817v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11817v2)
- **Published**: 2022-10-21 08:42:00+00:00
- **Updated**: 2023-01-19 14:11:52+00:00
- **Authors**: Jingqi Li, Jiaqi Gao, Yuzhen Zhang, Hongming Shan, Junping Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: As a unique biometric that can be perceived at a distance, gait has broad applications in person authentication, social security, and so on. Existing gait recognition methods suffer from changes in viewpoint and clothing and barely consider extracting diverse motion features, a fundamental characteristic in gaits, from gait sequences. This paper proposes a novel motion modeling method to extract the discriminative and robust representation. Specifically, we first extract the motion features from the encoded motion sequences in the shallow layer. Then we continuously enhance the motion feature in deep layers. This motion modeling approach is independent of mainstream work in building network architectures. As a result, one can apply this motion modeling method to any backbone to improve gait recognition performance. In this paper, we combine motion modeling with one commonly used backbone~(GaitGL) as GaitGL-M to illustrate motion modeling. Extensive experimental results on two commonly-used cross-view gait datasets demonstrate the superior performance of GaitGL-M over existing state-of-the-art methods.



### Valuing Vicinity: Memory attention framework for context-based semantic segmentation in histopathology
- **Arxiv ID**: http://arxiv.org/abs/2210.11822v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11822v1)
- **Published**: 2022-10-21 08:49:30+00:00
- **Updated**: 2022-10-21 08:49:30+00:00
- **Authors**: Oliver Ester, Fabian Hörst, Constantin Seibold, Julius Keyl, Saskia Ting, Nikolaos Vasileiadis, Jessica Schmitz, Philipp Ivanyi, Viktor Grünwald, Jan Hinrich Bräsen, Jan Egger, Jens Kleesiek
- **Comment**: None
- **Journal**: None
- **Summary**: The segmentation of histopathological whole slide images into tumourous and non-tumourous types of tissue is a challenging task that requires the consideration of both local and global spatial contexts to classify tumourous regions precisely. The identification of subtypes of tumour tissue complicates the issue as the sharpness of separation decreases and the pathologist's reasoning is even more guided by spatial context. However, the identification of detailed types of tissue is crucial for providing personalized cancer therapies. Due to the high resolution of whole slide images, existing semantic segmentation methods, restricted to isolated image sections, are incapable of processing context information beyond. To take a step towards better context comprehension, we propose a patch neighbour attention mechanism to query the neighbouring tissue context from a patch embedding memory bank and infuse context embeddings into bottleneck hidden feature maps. Our memory attention framework (MAF) mimics a pathologist's annotation procedure -- zooming out and considering surrounding tissue context. The framework can be integrated into any encoder-decoder segmentation method. We evaluate the MAF on a public breast cancer and an internal kidney cancer data set using famous segmentation models (U-Net, DeeplabV3) and demonstrate the superiority over other context-integrating algorithms -- achieving a substantial improvement of up to $17\%$ on Dice score. The code is publicly available at: https://github.com/tio-ikim/valuing-vicinity



### 3D Human Pose Estimation in Multi-View Operating Room Videos Using Differentiable Camera Projections
- **Arxiv ID**: http://arxiv.org/abs/2210.11826v1
- **DOI**: 10.1080/21681163.2022.2155580
- **Categories**: **cs.CV**, I.4.8; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2210.11826v1)
- **Published**: 2022-10-21 09:00:02+00:00
- **Updated**: 2022-10-21 09:00:02+00:00
- **Authors**: Beerend G. A. Gerats, Jelmer M. Wolterink, Ivo A. M. J. Broeders
- **Comment**: 12 pages, 4 figures, submitted to the 2022 AE-CAI Special Issue of
  Computer Methods in Biomechanics and Biomedical Engineering: Imaging &
  Visualization
- **Journal**: Computer Methods in Biomechanics and Biomedical Engineering:
  Imaging & Visualization. 11 (2023) 1197-1205
- **Summary**: 3D human pose estimation in multi-view operating room (OR) videos is a relevant asset for person tracking and action recognition. However, the surgical environment makes it challenging to find poses due to sterile clothing, frequent occlusions, and limited public data. Methods specifically designed for the OR are generally based on the fusion of detected poses in multiple camera views. Typically, a 2D pose estimator such as a convolutional neural network (CNN) detects joint locations. Then, the detected joint locations are projected to 3D and fused over all camera views. However, accurate detection in 2D does not guarantee accurate localisation in 3D space. In this work, we propose to directly optimise for localisation in 3D by training 2D CNNs end-to-end based on a 3D loss that is backpropagated through each camera's projection parameters. Using videos from the MVOR dataset, we show that this end-to-end approach outperforms optimisation in 2D space.



### Improving the Anomaly Detection in GPR Images by Fine-Tuning CNNs with Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/2210.11833v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.11833v2)
- **Published**: 2022-10-21 09:25:15+00:00
- **Updated**: 2022-11-22 01:47:19+00:00
- **Authors**: Xiren Zhou, Shikang Liu, Ao Chen, Yizhan Fan, Huanhuan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Ground Penetrating Radar (GPR) has been widely used to estimate the healthy operation of some urban roads and underground facilities. When identifying subsurface anomalies by GPR in an area, the obtained data could be unbalanced, and the numbers and types of possible underground anomalies could not be acknowledged in advance. In this paper, a novel method is proposed to improve the subsurface anomaly detection from GPR B-scan images. A normal (i.e. without subsurface objects) GPR image section is firstly collected in the detected area. Concerning that the GPR image is essentially the representation of electromagnetic (EM) wave and propagation time, and to preserve both the subsurface background and objects' details, the normal GPR image is segmented and then fused with simulated GPR images that contain different kinds of objects to generate the synthetic data for the detection area based on the wavelet decompositions. Pre-trained CNNs could then be fine-tuned with the synthetic data, and utilized to extract features of segmented GPR images subsequently obtained in the detection area. The extracted features could be classified by the one-class learning algorithm in the feature space without pre-set anomaly types or numbers. The conducted experiments demonstrate that fine-tuning the pre-trained CNN with the proposed synthetic data could effectively improve the feature extraction of the network for the objects in the detection area. Besides, the proposed method requires only a section of normal data that could be easily obtained in the detection area, and could also meet the timeliness requirements in practical applications.



### Diffusion Visual Counterfactual Explanations
- **Arxiv ID**: http://arxiv.org/abs/2210.11841v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11841v1)
- **Published**: 2022-10-21 09:35:47+00:00
- **Updated**: 2022-10-21 09:35:47+00:00
- **Authors**: Maximilian Augustin, Valentyn Boreiko, Francesco Croce, Matthias Hein
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: Visual Counterfactual Explanations (VCEs) are an important tool to understand the decisions of an image classifier. They are 'small' but 'realistic' semantic changes of the image changing the classifier decision. Current approaches for the generation of VCEs are restricted to adversarially robust models and often contain non-realistic artefacts, or are limited to image classification problems with few classes. In this paper, we overcome this by generating Diffusion Visual Counterfactual Explanations (DVCEs) for arbitrary ImageNet classifiers via a diffusion process. Two modifications to the diffusion process are key for our DVCEs: first, an adaptive parameterization, whose hyperparameters generalize across images and models, together with distance regularization and late start of the diffusion process, allow us to generate images with minimal semantic changes to the original ones but different classification. Second, our cone regularization via an adversarially robust model ensures that the diffusion process does not converge to trivial non-semantic changes, but instead produces realistic images of the target class which achieve high confidence by the classifier.



### Can Visual Context Improve Automatic Speech Recognition for an Embodied Agent?
- **Arxiv ID**: http://arxiv.org/abs/2210.13189v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.AI, cs.CL, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.13189v1)
- **Published**: 2022-10-21 11:16:05+00:00
- **Updated**: 2022-10-21 11:16:05+00:00
- **Authors**: Pradip Pramanick, Chayan Sarkar
- **Comment**: Accepted in EMNLP '22
- **Journal**: None
- **Summary**: The usage of automatic speech recognition (ASR) systems are becoming omnipresent ranging from personal assistant to chatbots, home, and industrial automation systems, etc. Modern robots are also equipped with ASR capabilities for interacting with humans as speech is the most natural interaction modality. However, ASR in robots faces additional challenges as compared to a personal assistant. Being an embodied agent, a robot must recognize the physical entities around it and therefore reliably recognize the speech containing the description of such entities. However, current ASR systems are often unable to do so due to limitations in ASR training, such as generic datasets and open-vocabulary modeling. Also, adverse conditions during inference, such as noise, accented, and far-field speech makes the transcription inaccurate. In this work, we present a method to incorporate a robot's visual information into an ASR system and improve the recognition of a spoken utterance containing a visible entity. Specifically, we propose a new decoder biasing technique to incorporate the visual context while ensuring the ASR output does not degrade for incorrect context. We achieve a 59% relative reduction in WER from an unmodified ASR system.



### Collaborative Image Understanding
- **Arxiv ID**: http://arxiv.org/abs/2210.11907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11907v1)
- **Published**: 2022-10-21 12:13:08+00:00
- **Updated**: 2022-10-21 12:13:08+00:00
- **Authors**: Koby Bibas, Oren Sar Shalom, Dietmar Jannach
- **Comment**: CIKM 2022
- **Journal**: None
- **Summary**: Automatically understanding the contents of an image is a highly relevant problem in practice. In e-commerce and social media settings, for example, a common problem is to automatically categorize user-provided pictures. Nowadays, a standard approach is to fine-tune pre-trained image models with application-specific data. Besides images, organizations however often also collect collaborative signals in the context of their application, in particular how users interacted with the provided online content, e.g., in forms of viewing, rating, or tagging. Such signals are commonly used for item recommendation, typically by deriving latent user and item representations from the data. In this work, we show that such collaborative information can be leveraged to improve the classification process of new images. Specifically, we propose a multitask learning framework, where the auxiliary task is to reconstruct collaborative latent item representations. A series of experiments on datasets from e-commerce and social media demonstrates that considering collaborative signals helps to significantly improve the performance of the main task of image classification by up to 9.1%.



### Boosting vision transformers for image retrieval
- **Arxiv ID**: http://arxiv.org/abs/2210.11909v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11909v1)
- **Published**: 2022-10-21 12:17:12+00:00
- **Updated**: 2022-10-21 12:17:12+00:00
- **Authors**: Chull Hwan Song, Jooyoung Yoon, Shunghyun Choi, Yannis Avrithis
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: Vision transformers have achieved remarkable progress in vision tasks such as image classification and detection. However, in instance-level image retrieval, transformers have not yet shown good performance compared to convolutional networks. We propose a number of improvements that make transformers outperform the state of the art for the first time. (1) We show that a hybrid architecture is more effective than plain transformers, by a large margin. (2) We introduce two branches collecting global (classification token) and local (patch tokens) information, from which we form a global image representation. (3) In each branch, we collect multi-layer features from the transformer encoder, corresponding to skip connections across distant layers. (4) We enhance locality of interactions at the deeper layers of the encoder, which is the relative weakness of vision transformers. We train our model on all commonly used training sets and, for the first time, we make fair comparisons separately per training set. In all cases, we outperform previous models based on global representation. Public code is available at https://github.com/dealicious-inc/DToP.



### Data reconstruction of turbulent flows with Gappy POD, Extended POD and Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2210.11921v1
- **DOI**: None
- **Categories**: **physics.flu-dyn**, cs.CV, cs.LG, nlin.CD, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/2210.11921v1)
- **Published**: 2022-10-21 12:44:39+00:00
- **Updated**: 2022-10-21 12:44:39+00:00
- **Authors**: Tianyi Li, Michele Buzzicotti, Luca Biferale, Fabio Bonaccorso, Shiyi Chen, Minping Wan
- **Comment**: None
- **Journal**: None
- **Summary**: Three methods are used to reconstruct two-dimensional instantaneous velocity fields in a turbulent flow under rotation. The first two methods both use the linear proper orthogonal decomposition (POD), which are Gappy POD (GPOD) and Extended POD (EPOD), while the third one reconstructs the flow using a fully non-linear Convolutional Neural Network embedded in a Generative Adversarial Network (GAN). First, we show that there is always an optimal number of modes regarding a specific gap for the GPOD with dimension reduction. Moreover, adopting a Lasso regularizer for GPOD provides comparable reconstruction results. In order to systematically compare the applicability of the three tools, we consider a square gap at changing the size. Results show that compared with POD-based methods, GAN reconstruction not only has a smaller $L_2$ error, but also better turbulent statistics of both the velocity module and the velocity module gradient. This can be attributed to the ability of nonlinearity expression of the network and the presence of adversarial loss during the GAN training. We also investigate effects of the adversarial ratio, which controls the compromising between the $L_2$ error and the statistical properties. Finally, we assess the reconstruction on random gappiness. All methods perform well for small- and medium-size gaps, while GAN works better when the gappiness is large.



### Men Also Do Laundry: Multi-Attribute Bias Amplification
- **Arxiv ID**: http://arxiv.org/abs/2210.11924v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.11924v3)
- **Published**: 2022-10-21 12:50:15+00:00
- **Updated**: 2023-05-30 15:38:36+00:00
- **Authors**: Dora Zhao, Jerone T. A. Andrews, Alice Xiang
- **Comment**: Accepted at ICML 2023
- **Journal**: None
- **Summary**: As computer vision systems become more widely deployed, there is increasing concern from both the research community and the public that these systems are not only reproducing but amplifying harmful social biases. The phenomenon of bias amplification, which is the focus of this work, refers to models amplifying inherent training set biases at test time. Existing metrics measure bias amplification with respect to single annotated attributes (e.g., $\texttt{computer}$). However, several visual datasets consist of images with multiple attribute annotations. We show models can learn to exploit correlations with respect to multiple attributes (e.g., {$\texttt{computer}$, $\texttt{keyboard}$}), which are not accounted for by current metrics. In addition, we show current metrics can give the erroneous impression that minimal or no bias amplification has occurred as they involve aggregating over positive and negative values. Further, these metrics lack a clear desired value, making them difficult to interpret. To address these shortcomings, we propose a new metric: Multi-Attribute Bias Amplification. We validate our proposed metric through an analysis of gender bias amplification on the COCO and imSitu datasets. Finally, we benchmark bias mitigation methods using our proposed metric, suggesting possible avenues for future bias mitigation



### LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling
- **Arxiv ID**: http://arxiv.org/abs/2210.11929v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2210.11929v1)
- **Published**: 2022-10-21 13:03:49+00:00
- **Updated**: 2022-10-21 13:03:49+00:00
- **Authors**: Dongsheng Chen, Chaofan Tao, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu
- **Comment**: 13 pages, 6 figures, accepted by EMNLP 2022 main conference
- **Journal**: None
- **Summary**: Recent large-scale video-language pre-trained models have shown appealing performance on various downstream tasks. However, the pre-training process is computationally expensive due to the requirement of millions of video-text pairs and the redundant data structure of each video. To mitigate these problems, we propose LiteVL, which adapts a pre-trained image-language model BLIP into a video-text model directly on downstream tasks, without heavy pre-training. To enhance the temporal modeling lacking in the image-language model, we propose to add temporal attention modules in the image encoder of BLIP with dynamic temporal scaling. Besides the model-wise adaptation, we also propose a non-parametric pooling mechanism to adaptively reweight the fine-grained video embedding conditioned on the text. Experimental results on text-video retrieval and video question answering show that the proposed LiteVL even outperforms previous video-language pre-trained models by a clear margin, though without any video-language pre-training.



### Fine-grained Semantic Alignment Network for Weakly Supervised Temporal Language Grounding
- **Arxiv ID**: http://arxiv.org/abs/2210.11933v1
- **DOI**: 10.18653/v1/2021.findings-emnlp.9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11933v1)
- **Published**: 2022-10-21 13:10:27+00:00
- **Updated**: 2022-10-21 13:10:27+00:00
- **Authors**: Yuechen Wang, Wengang Zhou, Houqiang Li
- **Comment**: 11 pages, 4 figures, accepted by Findings of EMNLP 2021
- **Journal**: None
- **Summary**: Temporal language grounding (TLG) aims to localize a video segment in an untrimmed video based on a natural language description. To alleviate the expensive cost of manual annotations for temporal boundary labels, we are dedicated to the weakly supervised setting, where only video-level descriptions are provided for training. Most of the existing weakly supervised methods generate a candidate segment set and learn cross-modal alignment through a MIL-based framework. However, the temporal structure of the video as well as the complicated semantics in the sentence are lost during the learning. In this work, we propose a novel candidate-free framework: Fine-grained Semantic Alignment Network (FSAN), for weakly supervised TLG. Instead of view the sentence and candidate moments as a whole, FSAN learns token-by-clip cross-modal semantic alignment by an iterative cross-modal interaction module, generates a fine-grained cross-modal semantic alignment map, and performs grounding directly on top of the map. Extensive experiments are conducted on two widely-used benchmarks: ActivityNet-Captions, and DiDeMo, where our FSAN achieves state-of-the-art performance.



### Automatic Cattle Identification using YOLOv5 and Mosaic Augmentation: A Comparative Analysis
- **Arxiv ID**: http://arxiv.org/abs/2210.11939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11939v1)
- **Published**: 2022-10-21 13:13:40+00:00
- **Updated**: 2022-10-21 13:13:40+00:00
- **Authors**: Rabin Dulal, Lihong Zheng, Muhammad Ashad Kabir, Shawn McGrath, Jonathan Medway, Dave Swain, Will Swain
- **Comment**: Accepted in the Digital Image Computing: Techniques and Applications,
  2022 (DICTA 2022)
- **Journal**: None
- **Summary**: You Only Look Once (YOLO) is a single-stage object detection model popular for real-time object detection, accuracy, and speed. This paper investigates the YOLOv5 model to identify cattle in the yards. The current solution to cattle identification includes radio-frequency identification (RFID) tags. The problem occurs when the RFID tag is lost or damaged. A biometric solution identifies the cattle and helps to assign the lost or damaged tag or replace the RFID-based system. Muzzle patterns in cattle are unique biometric solutions like a fingerprint in humans. This paper aims to present our recent research in utilizing five popular object detection models, looking at the architecture of YOLOv5, investigating the performance of eight backbones with the YOLOv5 model, and the influence of mosaic augmentation in YOLOv5 by experimental results on the available cattle muzzle images. Finally, we concluded with the excellent potential of using YOLOv5 in automatic cattle identification. Our experiments show YOLOv5 with transformer performed best with mean Average Precision (mAP) 0.5 (the average of AP when the IoU is greater than 50%) of 0.995, and mAP 0.5:0.95 (the average of AP from 50% to 95% IoU with an interval of 5%) of 0.9366. In addition, our experiments show the increase in accuracy of the model by using mosaic augmentation in all backbones used in our experiments. Moreover, we can also detect cattle with partial muzzle images.



### CobNet: Cross Attention on Object and Background for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.11968v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11968v2)
- **Published**: 2022-10-21 13:49:46+00:00
- **Updated**: 2022-10-30 10:06:50+00:00
- **Authors**: Haoyan Guan, Michael Spratling
- **Comment**: Accepted to ICPR2022
- **Journal**: None
- **Summary**: Few-shot segmentation aims to segment images containing objects from previously unseen classes using only a few annotated samples. Most current methods focus on using object information extracted, with the aid of human annotations, from support images to identify the same objects in new query images. However, background information can also be useful to distinguish objects from their surroundings. Hence, some previous methods also extract background information from the support images. In this paper, we argue that such information is of limited utility, as the background in different images can vary widely. To overcome this issue, we propose CobNet which utilises information about the background that is extracted from the query images without annotations of those images. Experiments show that our method achieves a mean Intersection-over-Union score of 61.4% and 37.8% for 1-shot segmentation on PASCAL-5i and COCO-20i respectively, outperforming previous methods. It is also shown to produce state-of-the-art performances of 53.7% for weakly-supervised few-shot segmentation, where no annotations are provided for the support images.



### Real-Time Constrained 6D Object-Pose Tracking of An In-Hand Suture Needle for Minimally Invasive Robotic Surgery
- **Arxiv ID**: http://arxiv.org/abs/2210.11973v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.11973v1)
- **Published**: 2022-10-21 14:02:10+00:00
- **Updated**: 2022-10-21 14:02:10+00:00
- **Authors**: Zih-Yun Chiu, Florian Richter, Michael C. Yip
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous suturing has been a long-sought-after goal for surgical robotics. Outside of staged environments, accurate localization of suture needles is a critical foundation for automating various suture needle manipulation tasks in the real world. When localizing a needle held by a gripper, previous work usually tracks them separately without considering their relationship. Because of the significant errors that can arise in the stereo-triangulation of objects and instruments, their reconstructions may often not be consistent. This can lead to unrealistic tool-needle grasp reconstructions that are infeasible. Instead, an obvious strategy to improve localization would be to leverage constraints that arise from contact, thereby constraining reconstructions of objects and instruments into a jointly feasible space. In this work, we consider feasible grasping constraints when tracking the 6D pose of an in-hand suture needle. We propose a reparameterization trick to define a new state space for describing a needle pose, where grasp constraints can be easily defined and satisfied. Our proposed state space and feasible grasping constraints are then incorporated into Bayesian filters for real-time needle localization. In the experiments, we show that our constrained methods outperform previous unconstrained/constrained tracking approaches and demonstrate the importance of incorporating feasible grasping constraints into automating suture needle manipulation tasks.



### Face Pyramid Vision Transformer
- **Arxiv ID**: http://arxiv.org/abs/2210.11974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11974v1)
- **Published**: 2022-10-21 14:03:40+00:00
- **Updated**: 2022-10-21 14:03:40+00:00
- **Authors**: Khawar Islam, Muhammad Zaigham Zaheer, Arif Mahmood
- **Comment**: Accepted in BMVC 2022
- **Journal**: None
- **Summary**: A novel Face Pyramid Vision Transformer (FPVT) is proposed to learn a discriminative multi-scale facial representations for face recognition and verification. In FPVT, Face Spatial Reduction Attention (FSRA) and Dimensionality Reduction (FDR) layers are employed to make the feature maps compact, thus reducing the computations. An Improved Patch Embedding (IPE) algorithm is proposed to exploit the benefits of CNNs in ViTs (e.g., shared weights, local context, and receptive fields) to model lower-level edges to higher-level semantic primitives. Within FPVT framework, a Convolutional Feed-Forward Network (CFFN) is proposed that extracts locality information to learn low level facial information. The proposed FPVT is evaluated on seven benchmark datasets and compared with ten existing state-of-the-art methods, including CNNs, pure ViTs, and Convolutional ViTs. Despite fewer parameters, FPVT has demonstrated excellent performance over the compared methods. Project page is available at https://khawar-islam.github.io/fpvt/



### Real-time Detection of 2D Tool Landmarks with Synthetic Training Data
- **Arxiv ID**: http://arxiv.org/abs/2210.11991v1
- **DOI**: 10.5220/0010689900003061
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.11991v1)
- **Published**: 2022-10-21 14:31:43+00:00
- **Updated**: 2022-10-21 14:31:43+00:00
- **Authors**: Bram Vanherle, Jeroen Put, Nick Michiels, Frank Van Reeth
- **Comment**: None
- **Journal**: ROBOVIS, 2021, 40-47
- **Summary**: In this paper a deep learning architecture is presented that can, in real time, detect the 2D locations of certain landmarks of physical tools, such as a hammer or screwdriver. To avoid the labor of manual labeling, the network is trained on synthetically generated data. Training computer vision models on computer generated images, while still achieving good accuracy on real images, is a challenge due to the difference in domain. The proposed method uses an advanced rendering method in combination with transfer learning and an intermediate supervision architecture to address this problem. It is shown that the model presented in this paper, named Intermediate Heatmap Model (IHM), generalizes to real images when trained on synthetic data. To avoid the need for an exact textured 3D model of the tool in question, it is shown that the model will generalize to an unseen tool when trained on a set of different 3D models of the same type of tool. IHM is compared to two existing approaches to keypoint detection and it is shown that it outperforms those at detecting tool landmarks, trained on synthetic data.



### On-Board Pedestrian Trajectory Prediction Using Behavioral Features
- **Arxiv ID**: http://arxiv.org/abs/2210.11999v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.11999v1)
- **Published**: 2022-10-21 14:40:51+00:00
- **Updated**: 2022-10-21 14:40:51+00:00
- **Authors**: Phillip Czech, Markus Braun, Ulrich Kreßel, Bin Yang
- **Comment**: Accepted at ICMLA 2022, 7 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: This paper presents a novel approach to pedestrian trajectory prediction for on-board camera systems, which utilizes behavioral features of pedestrians that can be inferred from visual observations. Our proposed method, called Behavior-Aware Pedestrian Trajectory Prediction (BA-PTP), processes multiple input modalities, i.e. bounding boxes, body and head orientation of pedestrians as well as their pose, with independent encoding streams. The encodings of each stream are fused using a modality attention mechanism, resulting in a final embedding that is used to predict future bounding boxes in the image.   In experiments on two datasets for pedestrian behavior prediction, we demonstrate the benefit of using behavioral features for pedestrian trajectory prediction and evaluate the effectiveness of the proposed encoding strategy. Additionally, we investigate the relevance of different behavioral features on the prediction performance based on an ablation study.



### HDHumans: A Hybrid Approach for High-fidelity Digital Humans
- **Arxiv ID**: http://arxiv.org/abs/2210.12003v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12003v2)
- **Published**: 2022-10-21 14:42:11+00:00
- **Updated**: 2023-07-14 07:07:07+00:00
- **Authors**: Marc Habermann, Lingjie Liu, Weipeng Xu, Gerard Pons-Moll, Michael Zollhoefer, Christian Theobalt
- **Comment**: None
- **Journal**: None
- **Summary**: Photo-real digital human avatars are of enormous importance in graphics, as they enable immersive communication over the globe, improve gaming and entertainment experiences, and can be particularly beneficial for AR and VR settings. However, current avatar generation approaches either fall short in high-fidelity novel view synthesis, generalization to novel motions, reproduction of loose clothing, or they cannot render characters at the high resolution offered by modern displays. To this end, we propose HDHumans, which is the first method for HD human character synthesis that jointly produces an accurate and temporally coherent 3D deforming surface and highly photo-realistic images of arbitrary novel views and of motions not seen at training time. At the technical core, our method tightly integrates a classical deforming character template with neural radiance fields (NeRF). Our method is carefully designed to achieve a synergy between classical surface deformation and NeRF. First, the template guides the NeRF, which allows synthesizing novel views of a highly dynamic and articulated character and even enables the synthesis of novel motions. Second, we also leverage the dense pointclouds resulting from NeRF to further improve the deforming surface via 3D-to-3D supervision. We outperform the state of the art quantitatively and qualitatively in terms of synthesis quality and resolution, as well as the quality of 3D surface reconstruction.



### Adversarial Transformer for Repairing Human Airway Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.12029v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12029v1)
- **Published**: 2022-10-21 15:20:08+00:00
- **Updated**: 2022-10-21 15:20:08+00:00
- **Authors**: Zeyu Tang, Nan Yang, Simon Walsh, Guang Yang
- **Comment**: 8 Pages, 7 figures
- **Journal**: None
- **Summary**: Discontinuity in the delineation of peripheral bronchioles hinders the potential clinical application of automated airway segmentation models. Moreover, the deployment of such models is limited by the data heterogeneity across different centres, and pathological abnormalities also make achieving accurate robust segmentation in distal small airways difficult. Meanwhile, the diagnosis and prognosis of lung diseases often rely on evaluating structural changes in those anatomical regions. To address this gap, this paper presents a patch-scale adversarial-based refinement network that takes in preliminary segmentation along with original CT images and outputs a refined mask of the airway structure. The method is validated on three different datasets encompassing healthy cases, cases with cystic fibrosis and cases with COVID-19. The results are quantitatively evaluated by seven metrics and achieved more than a 15% rise in detected length ratio and detected branch ratio, showing promising performance compared to previously proposed models. The visual illustration also proves our refinement guided by a patch-scale discriminator and centreline objective functions is effective in detecting discontinuities and missing bronchioles. Furthermore, the generalizability of our refinement pipeline is tested on three previous models and improves their segmentation completeness significantly.



### BlanketGen - A synthetic blanket occlusion augmentation pipeline for MoCap datasets
- **Arxiv ID**: http://arxiv.org/abs/2210.12035v2
- **DOI**: 10.1109/ENBENG58165.2023.10175320
- **Categories**: **cs.CV**, I.2.10; I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/2210.12035v2)
- **Published**: 2022-10-21 15:27:58+00:00
- **Updated**: 2023-03-19 19:14:01+00:00
- **Authors**: João Carmona, Tamás Karácsony, João Paulo Silva Cunha
- **Comment**: 4 pages, Code and further information to generate the dataset is
  available at:
  https://gitlab.inesctec.pt/brain-lab/brain-lab-public/blanket-gen-releases
- **Journal**: 2023 IEEE 7th Portuguese Meeting on Bioengineering (ENBENG),
  Porto, Portugal, 2023, pp. 112-115
- **Summary**: Human motion analysis has seen drastic improvements recently, however, due to the lack of representative datasets, for clinical in-bed scenarios it is still lagging behind. To address this issue, we implemented BlanketGen, a pipeline that augments videos with synthetic blanket occlusions. With this pipeline, we generated an augmented version of the pose estimation dataset 3DPW called BlanketGen-3DPW. We then used this new dataset to fine-tune a Deep Learning model to improve its performance in these scenarios with promising results. Code and further information are available at https://gitlab.inesctec.pt/brain-lab/brain-lab-public/blanket-gen-releases.



### Query Semantic Reconstruction for Background in Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.12055v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12055v2)
- **Published**: 2022-10-21 15:49:16+00:00
- **Updated**: 2022-12-21 12:52:12+00:00
- **Authors**: Haoyan Guan, Michael Spratling
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot segmentation (FSS) aims to segment unseen classes using a few annotated samples. Typically, a prototype representing the foreground class is extracted from annotated support image(s) and is matched to features representing each pixel in the query image. However, models learnt in this way are insufficiently discriminatory, and often produce false positives: misclassifying background pixels as foreground. Some FSS methods try to address this issue by using the background in the support image(s) to help identify the background in the query image. However, the backgrounds of theses images is often quite distinct, and hence, the support image background information is uninformative. This article proposes a method, QSR, that extracts the background from the query image itself, and as a result is better able to discriminate between foreground and background features in the query image. This is achieved by modifying the training process to associate prototypes with class labels including known classes from the training data and latent classes representing unknown background objects. This class information is then used to extract a background prototype from the query image. To successfully associate prototypes with class labels and extract a background prototype that is capable of predicting a mask for the background regions of the image, the machinery for extracting and using foreground prototypes is induced to become more discriminative between different classes. Experiments for both 1-shot and 5-shot FSS on both the PASCAL-5i and COCO-20i datasets demonstrate that the proposed method results in a significant improvement in performance for the baseline methods it is applied to. As QSR operates only during training, these improved results are produced with no extra computational complexity during testing.



### Do Vision-and-Language Transformers Learn Grounded Predicate-Noun Dependencies?
- **Arxiv ID**: http://arxiv.org/abs/2210.12079v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12079v1)
- **Published**: 2022-10-21 16:07:00+00:00
- **Updated**: 2022-10-21 16:07:00+00:00
- **Authors**: Mitja Nikolaus, Emmanuelle Salin, Stephane Ayache, Abdellah Fourtassi, Benoit Favre
- **Comment**: To appear at EMNLP 2022
- **Journal**: None
- **Summary**: Recent advances in vision-and-language modeling have seen the development of Transformer architectures that achieve remarkable performance on multimodal reasoning tasks. Yet, the exact capabilities of these black-box models are still poorly understood. While much of previous work has focused on studying their ability to learn meaning at the word-level, their ability to track syntactic dependencies between words has received less attention. We take a first step in closing this gap by creating a new multimodal task targeted at evaluating understanding of predicate-noun dependencies in a controlled setup. We evaluate a range of state-of-the-art models and find that their performance on the task varies considerably, with some models performing relatively well and others at chance level. In an effort to explain this variability, our analyses indicate that the quality (and not only sheer quantity) of pretraining data is essential. Additionally, the best performing models leverage fine-grained multimodal pretraining objectives in addition to the standard image-text matching objectives. This study highlights that targeted and controlled evaluations are a crucial step for a precise and rigorous test of the multimodal knowledge of vision-and-language models.



### Learning shape distributions from large databases of healthy organs: applications to zero-shot and few-shot abnormal pancreas detection
- **Arxiv ID**: http://arxiv.org/abs/2210.12095v1
- **DOI**: 10.1007/978-3-031-16434-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12095v1)
- **Published**: 2022-10-21 16:39:59+00:00
- **Updated**: 2022-10-21 16:39:59+00:00
- **Authors**: Rebeca Vétil, Clément Abi Nader, Alexandre Bône, Marie-Pierre Vullierme, Marc-Michel Roheé, Pietro Gori, Isabelle Bloch
- **Comment**: 10 pages, 3 figures
- **Journal**: Medical Image Computing and Computer Assisted Intervention 2022,
  Lecture Notes in Computer Science volume 13432, pp 464-473
- **Summary**: We propose a scalable and data-driven approach to learn shape distributions from large databases of healthy organs. To do so, volumetric segmentation masks are embedded into a common probabilistic shape space that is learned with a variational auto-encoding network. The resulting latent shape representations are leveraged to derive zeroshot and few-shot methods for abnormal shape detection. The proposed distribution learning approach is illustrated on a large database of 1200 healthy pancreas shapes. Downstream qualitative and quantitative experiments are conducted on a separate test set of 224 pancreas from patients with mixed conditions. The abnormal pancreas detection AUC reached up to 65.41% in the zero-shot configuration, and 78.97% in the few-shot configuration with as few as 15 abnormal examples, outperforming a baseline approach based on the sole volume.



### Boomerang: Local sampling on image manifolds using diffusion models
- **Arxiv ID**: http://arxiv.org/abs/2210.12100v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2210.12100v1)
- **Published**: 2022-10-21 16:52:16+00:00
- **Updated**: 2022-10-21 16:52:16+00:00
- **Authors**: Lorenzo Luzi, Ali Siahkoohi, Paul M Mayer, Josue Casco-Rodriguez, Richard Baraniuk
- **Comment**: None
- **Journal**: None
- **Summary**: Diffusion models can be viewed as mapping points in a high-dimensional latent space onto a low-dimensional learned manifold, typically an image manifold. The intermediate values between the latent space and image manifold can be interpreted as noisy images which are determined by the noise scheduling scheme employed during pre-training. We exploit this interpretation to introduce Boomerang, a local image manifold sampling approach using the dynamics of diffusion models. We call it Boomerang because we first add noise to an input image, moving it closer to the latent space, then bring it back to the image space through diffusion dynamics. We use this method to generate images which are similar, but nonidentical, to the original input images on the image manifold. We are able to set how close the generated image is to the original based on how much noise we add. Additionally, the generated images have a degree of stochasticity, allowing us to locally sample as many times as we want without repetition. We show three applications for which Boomerang can be used. First, we provide a framework for constructing privacy-preserving datasets having controllable degrees of anonymity. Second, we show how to use Boomerang for data augmentation while staying on the image manifold. Third, we introduce a framework for image super-resolution with 8x upsampling. Boomerang does not require any modification to the training of diffusion models and can be used with pretrained models on a single, inexpensive GPU.



### Describing Sets of Images with Textual-PCA
- **Arxiv ID**: http://arxiv.org/abs/2210.12112v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2210.12112v1)
- **Published**: 2022-10-21 17:10:49+00:00
- **Updated**: 2022-10-21 17:10:49+00:00
- **Authors**: Oded Hupert, Idan Schwartz, Lior Wolf
- **Comment**: Accepted to Findings of EMNLP'22
- **Journal**: None
- **Summary**: We seek to semantically describe a set of images, capturing both the attributes of single images and the variations within the set. Our procedure is analogous to Principle Component Analysis, in which the role of projection vectors is replaced with generated phrases. First, a centroid phrase that has the largest average semantic similarity to the images in the set is generated, where both the computation of the similarity and the generation are based on pretrained vision-language models. Then, the phrase that generates the highest variation among the similarity scores is generated, using the same models. The next phrase maximizes the variance subject to being orthogonal, in the latent space, to the highest-variance phrase, and the process continues. Our experiments show that our method is able to convincingly capture the essence of image sets and describe the individual elements in a semantically meaningful way within the context of the entire set. Our code is available at: https://github.com/OdedH/textual-pca.



### Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological Report
- **Arxiv ID**: http://arxiv.org/abs/2210.12113v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.12113v2)
- **Published**: 2022-10-21 17:13:14+00:00
- **Updated**: 2023-03-30 18:36:27+00:00
- **Authors**: Pouria Rouzrokh, Bardia Khosravi, Shahriar Faghani, Mana Moassefi, Sanaz Vahdati, Bradley J. Erickson
- **Comment**: 17 pages, 7 figures
- **Journal**: None
- **Summary**: Despite the ever-increasing interest in applying deep learning (DL) models to medical imaging, the typical scarcity and imbalance of medical datasets can severely impact the performance of DL models. The generation of synthetic data that might be freely shared without compromising patient privacy is a well-known technique for addressing these difficulties. Inpainting algorithms are a subset of DL generative models that can alter one or more regions of an input image while matching its surrounding context and, in certain cases, non-imaging input conditions. Although the majority of inpainting techniques for medical imaging data use generative adversarial networks (GANs), the performance of these algorithms is frequently suboptimal due to their limited output variety, a problem that is already well-known for GANs. Denoising diffusion probabilistic models (DDPMs) are a recently introduced family of generative networks that can generate results of comparable quality to GANs, but with diverse outputs. In this paper, we describe a DDPM to execute multiple inpainting tasks on 2D axial slices of brain MRI with various sequences, and present proof-of-concept examples of its performance in a variety of evaluation scenarios. Our model and a public online interface to try our tool are available at: https://github.com/Mayo-Radiology-Informatics-Lab/MBTI



### One-Shot Neural Fields for 3D Object Understanding
- **Arxiv ID**: http://arxiv.org/abs/2210.12126v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.12126v3)
- **Published**: 2022-10-21 17:33:14+00:00
- **Updated**: 2023-08-09 00:30:08+00:00
- **Authors**: Valts Blukis, Taeyeop Lee, Jonathan Tremblay, Bowen Wen, In So Kweon, Kuk-Jin Yoon, Dieter Fox, Stan Birchfield
- **Comment**: IEEE/CVF Conference on Computer Vision and Pattern Recognition
  Workshop (CVPRW) on XRNeRF: Advances in NeRF for the Metaverse 2023
- **Journal**: None
- **Summary**: We present a unified and compact scene representation for robotics, where each object in the scene is depicted by a latent code capturing geometry and appearance. This representation can be decoded for various tasks such as novel view rendering, 3D reconstruction (e.g. recovering depth, point clouds, or voxel maps), collision checking, and stable grasp prediction. We build our representation from a single RGB input image at test time by leveraging recent advances in Neural Radiance Fields (NeRF) that learn category-level priors on large multiview datasets, then fine-tune on novel objects from one or few views. We expand the NeRF model for additional grasp outputs and explore ways to leverage this representation for robotics. At test-time, we build the representation from a single RGB input image observing the scene from only one viewpoint. We find that the recovered representation allows rendering from novel views, including of occluded object parts, and also for predicting successful stable grasps. Grasp poses can be directly decoded from our latent representation with an implicit grasp decoder. We experimented in both simulation and real world and demonstrated the capability for robust robotic grasping using such compact representation. Website: https://nerfgrasp.github.io



### Geometric Sparse Coding in Wasserstein Space
- **Arxiv ID**: http://arxiv.org/abs/2210.12135v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP, math.OC, math.PR, math.ST, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/2210.12135v1)
- **Published**: 2022-10-21 17:46:31+00:00
- **Updated**: 2022-10-21 17:46:31+00:00
- **Authors**: Marshall Mueller, Shuchin Aeron, James M. Murphy, Abiy Tasissa
- **Comment**: 24 pages
- **Journal**: None
- **Summary**: Wasserstein dictionary learning is an unsupervised approach to learning a collection of probability distributions that generate observed distributions as Wasserstein barycentric combinations. Existing methods for Wasserstein dictionary learning optimize an objective that seeks a dictionary with sufficient representation capacity via barycentric interpolation to approximate the observed training data, but without imposing additional structural properties on the coefficients associated to the dictionary. This leads to dictionaries that densely represent the observed data, which makes interpretation of the coefficients challenging and may also lead to poor empirical performance when using the learned coefficients in downstream tasks. In contrast and motivated by sparse dictionary learning in Euclidean spaces, we propose a geometrically sparse regularizer for Wasserstein space that promotes representations of a data point using only nearby dictionary elements. We show this approach leads to sparse representations in Wasserstein space and addresses the problem of non-uniqueness of barycentric representation. Moreover, when data is generated as Wasserstein barycenters of fixed distributions, this regularizer facilitates the recovery of the generating distributions in cases that are ill-posed for unregularized Wasserstein dictionary learning. Through experimentation on synthetic and real data, we show that our geometrically regularized approach yields sparser and more interpretable dictionaries in Wasserstein space, which perform better in downstream applications.



### Target Aware Poisson-Gaussian Noise Parameters Estimation from Noisy Images
- **Arxiv ID**: http://arxiv.org/abs/2210.12142v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12142v2)
- **Published**: 2022-10-21 17:53:56+00:00
- **Updated**: 2022-12-20 10:11:30+00:00
- **Authors**: Étienne Objois, Kaan Okumuş, Nicolas Bähler
- **Comment**: 11 pages, 14 figures and 4 tables
- **Journal**: None
- **Summary**: Digital sensors can lead to noisy results under many circumstances. To be able to remove the undesired noise from images, proper noise modeling and an accurate noise parameter estimation is crucial. In this project, we use a Poisson-Gaussian noise model for the raw-images captured by the sensor, as it fits the physical characteristics of the sensor closely. Moreover, we limit ourselves to the case where observed (noisy), and ground-truth (noise-free) image pairs are available. Using such pairs is beneficial for the noise estimation and is not widely studied in literature. Based on this model, we derive the theoretical maximum likelihood solution, discuss its practical implementation and optimization. Further, we propose two algorithms based on variance and cumulant statistics. Finally, we compare the results of our methods with two different approaches, a CNN we trained ourselves, and another one taken from literature. The comparison between all these methods shows that our algorithms outperform the others in terms of MSE and have good additional properties.



### Unsupervised Multi-object Segmentation by Predicting Probable Motion Patterns
- **Arxiv ID**: http://arxiv.org/abs/2210.12148v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.12148v1)
- **Published**: 2022-10-21 17:57:05+00:00
- **Updated**: 2022-10-21 17:57:05+00:00
- **Authors**: Laurynas Karazija, Subhabrata Choudhury, Iro Laina, Christian Rupprecht, Andrea Vedaldi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new approach to learn to segment multiple image objects without manual supervision. The method can extract objects form still images, but uses videos for supervision. While prior works have considered motion for segmentation, a key insight is that, while motion can be used to identify objects, not all objects are necessarily in motion: the absence of motion does not imply the absence of objects. Hence, our model learns to predict image regions that are likely to contain motion patterns characteristic of objects moving rigidly. It does not predict specific motion, which cannot be done unambiguously from a still image, but a distribution of possible motions, which includes the possibility that an object does not move at all. We demonstrate the advantage of this approach over its deterministic counterpart and show state-of-the-art unsupervised object segmentation performance on simulated and real-world benchmarks, surpassing methods that use motion even at test time. As our approach is applicable to variety of network architectures that segment the scenes, we also apply it to existing image reconstruction-based models showing drastic improvement. Project page and code: https://www.robots.ox.ac.uk/~vgg/research/ppmp .



### High-Fidelity Visual Structural Inspections through Transformers and Learnable Resizers
- **Arxiv ID**: http://arxiv.org/abs/2210.12175v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12175v1)
- **Published**: 2022-10-21 18:08:26+00:00
- **Updated**: 2022-10-21 18:08:26+00:00
- **Authors**: Kareem Eltouny, Seyedomid Sajedi, Xiao Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Visual inspection is the predominant technique for evaluating the condition of civil infrastructure. The recent advances in unmanned aerial vehicles (UAVs) and artificial intelligence have made the visual inspections faster, safer, and more reliable. Camera-equipped UAVs are becoming the new standard in the industry by collecting massive amounts of visual data for human inspectors. Meanwhile, there has been significant research on autonomous visual inspections using deep learning algorithms, including semantic segmentation. While UAVs can capture high-resolution images of buildings' fa\c{c}ades, high-resolution segmentation is extremely challenging due to the high computational memory demands. Typically, images are uniformly downsized at the price of losing fine local details. Contrarily, breaking the images into multiple smaller patches can cause a loss of global contextual in-formation. We propose a hybrid strategy that can adapt to different inspections tasks by managing the global and local semantics trade-off. The framework comprises a compound, high-resolution deep learning architecture equipped with an attention-based segmentation model and learnable downsampler-upsampler modules designed for optimal efficiency and in-formation retention. The framework also utilizes vision transformers on a grid of image crops aiming for high precision learning without downsizing. An augmented inference technique is used to boost the performance and re-duce the possible loss of context due to grid cropping. Comprehensive experiments have been performed on 3D physics-based graphics models synthetic environments in the Quake City dataset. The proposed framework is evaluated using several metrics on three segmentation tasks: component type, component damage state, and global damage (crack, rebar, spalling).



### A New Perspective for Understanding Generalization Gap of Deep Neural Networks Trained with Large Batch Sizes
- **Arxiv ID**: http://arxiv.org/abs/2210.12184v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12184v1)
- **Published**: 2022-10-21 18:23:12+00:00
- **Updated**: 2022-10-21 18:23:12+00:00
- **Authors**: Oyebade K. Oyedotun, Konstantinos Papadopoulos, Djamila Aouada
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are typically optimized using various forms of mini-batch gradient descent algorithm. A major motivation for mini-batch gradient descent is that with a suitably chosen batch size, available computing resources can be optimally utilized (including parallelization) for fast model training. However, many works report the progressive loss of model generalization when the training batch size is increased beyond some limits. This is a scenario commonly referred to as generalization gap. Although several works have proposed different methods for alleviating the generalization gap problem, a unanimous account for understanding generalization gap is still lacking in the literature. This is especially important given that recent works have observed that several proposed solutions for generalization gap problem such learning rate scaling and increased training budget do not indeed resolve it. As such, our main exposition in this paper is to investigate and provide new perspectives for the source of generalization loss for DNNs trained with a large batch size. Our analysis suggests that large training batch size results in increased near-rank loss of units' activation (i.e. output) tensors, which consequently impacts model optimization and generalization. Extensive experiments are performed for validation on popular DNN models such as VGG-16, residual network (ResNet-56) and LeNet-5 using CIFAR-10, CIFAR-100, Fashion-MNIST and MNIST datasets.



### Attention-Based Scattering Network for Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2210.12185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.12185v1)
- **Published**: 2022-10-21 18:25:34+00:00
- **Updated**: 2022-10-21 18:25:34+00:00
- **Authors**: Jason Stock, Chuck Anderson
- **Comment**: NeurIPS 2022 Workshop - Tackling Climate Change with Machine
  Learning, 4 page limit w/ appendix
- **Journal**: None
- **Summary**: Multi-channel satellite imagery, from stacked spectral bands or spatiotemporal data, have meaningful representations for various atmospheric properties. Combining these features in an effective manner to create a performant and trustworthy model is of utmost importance to forecasters. Neural networks show promise, yet suffer from unintuitive computations, fusion of high-level features, and may be limited by the quantity of available data. In this work, we leverage the scattering transform to extract high-level features without additional trainable parameters and introduce a separation scheme to bring attention to independent input channels. Experiments show promising results on estimating tropical cyclone intensity and predicting the occurrence of lightning from satellite imagery.



### Augmentation by Counterfactual Explanation -- Fixing an Overconfident Classifier
- **Arxiv ID**: http://arxiv.org/abs/2210.12196v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12196v1)
- **Published**: 2022-10-21 18:53:16+00:00
- **Updated**: 2022-10-21 18:53:16+00:00
- **Authors**: Sumedha Singla, Nihal Murali, Forough Arabshahi, Sofia Triantafyllou, Kayhan Batmanghelich
- **Comment**: Accepted in WACV 2023
- **Journal**: None
- **Summary**: A highly accurate but overconfident model is ill-suited for deployment in critical applications such as healthcare and autonomous driving. The classification outcome should reflect a high uncertainty on ambiguous in-distribution samples that lie close to the decision boundary. The model should also refrain from making overconfident decisions on samples that lie far outside its training distribution, far-out-of-distribution (far-OOD), or on unseen samples from novel classes that lie near its training distribution (near-OOD). This paper proposes an application of counterfactual explanations in fixing an over-confident classifier. Specifically, we propose to fine-tune a given pre-trained classifier using augmentations from a counterfactual explainer (ACE) to fix its uncertainty characteristics while retaining its predictive performance. We perform extensive experiments with detecting far-OOD, near-OOD, and ambiguous samples. Our empirical results show that the revised model have improved uncertainty measures, and its performance is competitive to the state-of-the-art methods.



### High-Quality RGB-D Reconstruction via Multi-View Uncalibrated Photometric Stereo and Gradient-SDF
- **Arxiv ID**: http://arxiv.org/abs/2210.12202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12202v1)
- **Published**: 2022-10-21 19:09:08+00:00
- **Updated**: 2022-10-21 19:09:08+00:00
- **Authors**: Lu Sang, Bjoern Haefner, Xingxing Zuo, Daniel Cremers
- **Comment**: WACV 2023
- **Journal**: None
- **Summary**: Fine-detailed reconstructions are in high demand in many applications. However, most of the existing RGB-D reconstruction methods rely on pre-calculated accurate camera poses to recover the detailed surface geometry, where the representation of a surface needs to be adapted when optimizing different quantities. In this paper, we present a novel multi-view RGB-D based reconstruction method that tackles camera pose, lighting, albedo, and surface normal estimation via the utilization of a gradient signed distance field (gradient-SDF). The proposed method formulates the image rendering process using specific physically-based model(s) and optimizes the surface's quantities on the actual surface using its volumetric representation, as opposed to other works which estimate surface quantities only near the actual surface. To validate our method, we investigate two physically-based image formation models for natural light and point light source applications. The experimental results on synthetic and real-world datasets demonstrate that the proposed method can recover high-quality geometry of the surface more faithfully than the state-of-the-art and further improves the accuracy of estimated camera poses.



### Imbalanced Classification in Medical Imaging via Regrouping
- **Arxiv ID**: http://arxiv.org/abs/2210.12234v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.12234v2)
- **Published**: 2022-10-21 20:25:20+00:00
- **Updated**: 2022-10-26 23:32:46+00:00
- **Authors**: Le Peng, Yash Travadi, Rui Zhang, Ying Cui, Ju Sun
- **Comment**: None
- **Journal**: None
- **Summary**: We propose performing imbalanced classification by regrouping majority classes into small classes so that we turn the problem into balanced multiclass classification. This new idea is dramatically different from popular loss reweighting and class resampling methods. Our preliminary result on imbalanced medical image classification shows that this natural idea can substantially boost the classification performance as measured by average precision (approximately area-under-the-precision-recall-curve, or AUPRC), which is more appropriate for evaluating imbalanced classification than other metrics such as balanced accuracy.



### FIND: An Unsupervised Implicit 3D Model of Articulated Human Feet
- **Arxiv ID**: http://arxiv.org/abs/2210.12241v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12241v2)
- **Published**: 2022-10-21 20:47:16+00:00
- **Updated**: 2022-11-21 19:51:35+00:00
- **Authors**: Oliver Boyne, James Charles, Roberto Cipolla
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: In this paper we present a high fidelity and articulated 3D human foot model. The model is parameterised by a disentangled latent code in terms of shape, texture and articulated pose. While high fidelity models are typically created with strong supervision such as 3D keypoint correspondences or pre-registration, we focus on the difficult case of little to no annotation. To this end, we make the following contributions: (i) we develop a Foot Implicit Neural Deformation field model, named FIND, capable of tailoring explicit meshes at any resolution i.e. for low or high powered devices; (ii) an approach for training our model in various modes of weak supervision with progressively better disentanglement as more labels, such as pose categories, are provided; (iii) a novel unsupervised part-based loss for fitting our model to 2D images which is better than traditional photometric or silhouette losses; (iv) finally, we release a new dataset of high resolution 3D human foot scans, Foot3D. On this dataset, we show our model outperforms a strong PCA implementation trained on the same data in terms of shape quality and part correspondences, and that our novel unsupervised part-based loss improves inference on images.



### Score-based Denoising Diffusion with Non-Isotropic Gaussian Noise Models
- **Arxiv ID**: http://arxiv.org/abs/2210.12254v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12254v2)
- **Published**: 2022-10-21 21:16:46+00:00
- **Updated**: 2022-11-23 00:40:58+00:00
- **Authors**: Vikram Voleti, Christopher Pal, Adam Oberman
- **Comment**: NeurIPS 2022 Workshop ; 4 pages, 1 page of references, 18 pages of
  appendix, 2 figures
- **Journal**: NeurIPS 2022 Workshop on Score-Based Methods
- **Summary**: Generative models based on denoising diffusion techniques have led to an unprecedented increase in the quality and diversity of imagery that is now possible to create with neural generative models. However, most contemporary state-of-the-art methods are derived from a standard isotropic Gaussian formulation. In this work we examine the situation where non-isotropic Gaussian distributions are used. We present the key mathematical derivations for creating denoising diffusion models using an underlying non-isotropic Gaussian noise model. We also provide initial experiments with the CIFAR-10 dataset to help verify empirically that this more general modeling approach can also yield high-quality samples.



### Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination
- **Arxiv ID**: http://arxiv.org/abs/2210.12261v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12261v1)
- **Published**: 2022-10-21 21:33:10+00:00
- **Updated**: 2022-10-21 21:33:10+00:00
- **Authors**: Yue Yang, Wenlin Yao, Hongming Zhang, Xiaoyang Wang, Dong Yu, Jianshu Chen
- **Comment**: EMNLP 2022
- **Journal**: None
- **Summary**: Large-scale pretrained language models have made significant advances in solving downstream language understanding tasks. However, they generally suffer from reporting bias, the phenomenon describing the lack of explicit commonsense knowledge in written text, e.g., ''an orange is orange''. To overcome this limitation, we develop a novel approach, Z-LaVI, to endow language models with visual imagination capabilities. Specifically, we leverage two complementary types of ''imaginations'': (i) recalling existing images through retrieval and (ii) synthesizing nonexistent images via text-to-image generation. Jointly exploiting the language inputs and the imagination, a pretrained vision-language model (e.g., CLIP) eventually composes a zero-shot solution to the original language tasks. Notably, fueling language models with imagination can effectively leverage visual knowledge to solve plain language tasks. In consequence, Z-LaVI consistently improves the zero-shot performance of existing language models across a diverse set of language tasks.



### An Exploration of Neural Radiance Field Scene Reconstruction: Synthetic, Real-world and Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2210.12268v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.12268v1)
- **Published**: 2022-10-21 21:51:17+00:00
- **Updated**: 2022-10-21 21:51:17+00:00
- **Authors**: Benedict Quartey, Tuluhan Akbulut, Wasiwasi Mgonzo, Zheng Xin Yong
- **Comment**: None
- **Journal**: None
- **Summary**: This project presents an exploration into 3D scene reconstruction of synthetic and real-world scenes using Neural Radiance Field (NeRF) approaches. We primarily take advantage of the reduction in training and rendering time of neural graphic primitives multi-resolution hash encoding, to reconstruct static video game scenes and real-world scenes, comparing and observing reconstruction detail and limitations. Additionally, we explore dynamic scene reconstruction using Neural Radiance Fields for Dynamic Scenes(D-NeRF). Finally, we extend the implementation of D-NeRF, originally constrained to handle synthetic scenes to also handle real-world dynamic scenes.



### Feature selection intelligent algorithm with mutual information and steepest ascent strategy
- **Arxiv ID**: http://arxiv.org/abs/2210.12296v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.12296v1)
- **Published**: 2022-10-21 23:15:42+00:00
- **Updated**: 2022-10-21 23:15:42+00:00
- **Authors**: Elkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine
- **Comment**: arXiv admin note: text overlap with arXiv:1211.0613
- **Journal**: Int. J. Advanced Intelligence Paradigms, Vol. 5, No. 4, 2013 -
  http://www.scopus.com/inward/record.url?eid=2-s2.0-84890828902&partnerID=MN8TOARS
- **Summary**: Remote sensing is a higher technology to produce knowledge for data mining applications. In principle hyperspectral images (HSIs) is a remote sensing tool that provides precise classification of regions. The HSI contains more than a hundred of images of the ground truth (GT) map. Some images are carrying relevant information, but others describe redundant information, or they are affected by atmospheric noise. The aim is to reduce dimensionality of HSI. Many studies use mutual information (MI) or normalised forms of MI to select appropriate bands. In this paper we design an algorithm based also on MI, and we combine MI with steepest ascent algorithm, to improve a symmetric uncertainty coefficient-based strategy to select relevant bands for classification of HSI. This algorithm is a feature selection tool and a wrapper strategy. We perform our study on HSI AVIRIS 92AV3C. This is an artificial intelligent system to control redundancy; we had to clear the difference of the result's algorithm and the human decision, and this can be viewed as case study which human decision is perhaps different to an intelligent algorithm. Index Terms - Hyperspectral images, Classification, Fea-ture selection, Mutual Information, Redundancy, Steepest Ascent. Artificial Intelligence



### Detection of Real-time DeepFakes in Video Conferencing with Active Probing and Corneal Reflection
- **Arxiv ID**: http://arxiv.org/abs/2210.14153v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.14153v1)
- **Published**: 2022-10-21 23:31:17+00:00
- **Updated**: 2022-10-21 23:31:17+00:00
- **Authors**: Hui Guo, Xin Wang, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: The COVID pandemic has led to the wide adoption of online video calls in recent years. However, the increasing reliance on video calls provides opportunities for new impersonation attacks by fraudsters using the advanced real-time DeepFakes. Real-time DeepFakes pose new challenges to detection methods, which have to run in real-time as a video call is ongoing. In this paper, we describe a new active forensic method to detect real-time DeepFakes. Specifically, we authenticate video calls by displaying a distinct pattern on the screen and using the corneal reflection extracted from the images of the call participant's face. This pattern can be induced by a call participant displaying on a shared screen or directly integrated into the video-call client. In either case, no specialized imaging or lighting hardware is required. Through large-scale simulations, we evaluate the reliability of this approach under a range in a variety of real-world imaging scenarios.



