# Arxiv Papers in cs.CV on 2022-10-23
### Outsourcing Training without Uploading Data via Efficient Collaborative Open-Source Sampling
- **Arxiv ID**: http://arxiv.org/abs/2210.12575v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/2210.12575v1)
- **Published**: 2022-10-23 00:12:18+00:00
- **Updated**: 2022-10-23 00:12:18+00:00
- **Authors**: Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, Michael Spranger
- **Comment**: Accepted to NeurIPS'22
- **Journal**: None
- **Summary**: As deep learning blooms with growing demand for computation and data resources, outsourcing model training to a powerful cloud server becomes an attractive alternative to training at a low-power and cost-effective end device. Traditional outsourcing requires uploading device data to the cloud server, which can be infeasible in many real-world applications due to the often sensitive nature of the collected data and the limited communication bandwidth. To tackle these challenges, we propose to leverage widely available open-source data, which is a massive dataset collected from public and heterogeneous sources (e.g., Internet images). We develop a novel strategy called Efficient Collaborative Open-source Sampling (ECOS) to construct a proximal proxy dataset from open-source data for cloud training, in lieu of client data. ECOS probes open-source data on the cloud server to sense the distribution of client data via a communication- and computation-efficient sampling process, which only communicates a few compressed public features and client scalar responses. Extensive empirical studies show that the proposed ECOS improves the quality of automated client labeling, model compression, and label outsourcing when applied in various learning scenarios.



### Feedback Assisted Adversarial Learning to Improve the Quality of Cone-beam CT Images
- **Arxiv ID**: http://arxiv.org/abs/2210.12578v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12578v1)
- **Published**: 2022-10-23 00:31:51+00:00
- **Updated**: 2022-10-23 00:31:51+00:00
- **Authors**: Takumi Hase, Megumi Nakao, Mitsuhiro Nakamura, Tetsuya Matsuda
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised image translation using adversarial learning has been attracting attention to improve the image quality of medical images. However, adversarial training based on the global evaluation values of discriminators does not provide sufficient translation performance for locally different image features. We propose adversarial learning with a feedback mechanism from a discriminator to improve the quality of CBCT images. This framework employs U-net as the discriminator and outputs a probability map representing the local discrimination results. The probability map is fed back to the generator and used for training to improve the image translation. Our experiments using 76 corresponding CT-CBCT images confirmed that the proposed framework could capture more diverse image features than conventional adversarial learning frameworks and produced synthetic images with pixel values close to the reference image and a correlation coefficient of 0.93.



### Single Image Super-Resolution via a Dual Interactive Implicit Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2210.12593v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.12593v1)
- **Published**: 2022-10-23 02:05:19+00:00
- **Updated**: 2022-10-23 02:05:19+00:00
- **Authors**: Quan H. Nguyen, William J. Beksi
- **Comment**: To be published in the 2023 IEEE/CVF Winter Conference on
  Applications of Computer Vision (WACV)
- **Journal**: None
- **Summary**: In this paper, we introduce a novel implicit neural network for the task of single image super-resolution at arbitrary scale factors. To do this, we represent an image as a decoding function that maps locations in the image along with their associated features to their reciprocal pixel attributes. Since the pixel locations are continuous in this representation, our method can refer to any location in an image of varying resolution. To retrieve an image of a particular resolution, we apply a decoding function to a grid of locations each of which refers to the center of a pixel in the output image. In contrast to other techniques, our dual interactive neural network decouples content and positional features. As a result, we obtain a fully implicit representation of the image that solves the super-resolution problem at (real-valued) elective scales using a single model. We demonstrate the efficacy and flexibility of our approach against the state of the art on publicly available benchmark datasets.



### DMODE: Differential Monocular Object Distance Estimation Module without Class Specific Information
- **Arxiv ID**: http://arxiv.org/abs/2210.12596v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.12596v1)
- **Published**: 2022-10-23 02:06:56+00:00
- **Updated**: 2022-10-23 02:06:56+00:00
- **Authors**: Pedram Agand, Michael Chang, Mo Chen
- **Comment**: 9 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: Using a single camera to estimate the distances of objects reduces costs compared to stereo-vision and LiDAR. Although monocular distance estimation has been studied in the literature, previous methods mostly rely on knowing an object's class in some way. This can result in deteriorated performance for dataset with multi-class objects and objects with an undefined class. In this paper, we aim to overcome the potential downsides of class-specific approaches, and provide an alternative technique called DMODE that does not require any information relating to its class. Using differential approaches, we combine the changes in an object's size over time together with the camera's motion to estimate the object's distance. Since DMODE is class agnostic method, it is easily adaptable to new environments. Therefore, it is able to maintain performance across different object detectors, and be easily adapted to new object classes. We tested our model across different scenarios of training and testing on the KITTI MOTS dataset's ground-truth bounding box annotations, and bounding box outputs of TrackRCNN and EagerMOT. The instantaneous change of bounding box sizes and camera position are then used to obtain an object's position in 3D without measuring its detection source or class properties. Our results show that we are able to outperform traditional alternatives methods e.g. IPM \cite{TuohyIPM}, SVR \cite{svr}, and \cite{zhu2019learning} in test environments with multi-class object distance detections.



### Transformers For Recognition In Overhead Imagery: A Reality Check
- **Arxiv ID**: http://arxiv.org/abs/2210.12599v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12599v2)
- **Published**: 2022-10-23 02:17:31+00:00
- **Updated**: 2022-10-31 20:14:54+00:00
- **Authors**: Francesco Luzi, Aneesh Gupta, Leslie Collins, Kyle Bradbury, Jordan Malof
- **Comment**: This paper has been accepted to WACV 2023, but this is not the final
  version
- **Journal**: None
- **Summary**: There is evidence that transformers offer state-of-the-art recognition performance on tasks involving overhead imagery (e.g., satellite imagery). However, it is difficult to make unbiased empirical comparisons between competing deep learning models, making it unclear whether, and to what extent, transformer-based models are beneficial. In this paper we systematically compare the impact of adding transformer structures into state-of-the-art segmentation models for overhead imagery. Each model is given a similar budget of free parameters, and their hyperparameters are optimized using Bayesian Optimization with a fixed quantity of data and computation time. We conduct our experiments with a large and diverse dataset comprising two large public benchmarks: Inria and DeepGlobe. We perform additional ablation studies to explore the impact of specific transformer-based modeling choices. Our results suggest that transformers provide consistent, but modest, performance improvements. We only observe this advantage however in hybrid models that combine convolutional and transformer-based structures, while fully transformer-based models achieve relatively poor performance.



### Facial De-occlusion Network for Virtual Telepresence Systems
- **Arxiv ID**: http://arxiv.org/abs/2210.12622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12622v1)
- **Published**: 2022-10-23 05:34:17+00:00
- **Updated**: 2022-10-23 05:34:17+00:00
- **Authors**: Surabhi Gupta, Ashwath Shetty, Avinash Sharma
- **Comment**: This workshop paper is presented in CVPR Workshop on Computer Vision
  for Augmented and Virtual Reality, New Orleans, LA, 2022. Link:
  https://xr.cornell.edu/workshop/2022/papers
- **Journal**: None
- **Summary**: To see what is not in the image is one of the broader missions of computer vision. Technology to inpaint images has made significant progress with the coming of deep learning. This paper proposes a method to tackle occlusion specific to human faces. Virtual presence is a promising direction in communication and recreation for the future. However, Virtual Reality (VR) headsets occlude a significant portion of the face, hindering the photo-realistic appearance of the face in the virtual world. State-of-the-art image inpainting methods for de-occluding the eye region does not give usable results. To this end, we propose a working solution that gives usable results to tackle this problem enabling the use of the real-time photo-realistic de-occluded face of the user in VR settings.



### Active Predictive Coding: A Unified Neural Framework for Learning Hierarchical World Models for Perception and Planning
- **Arxiv ID**: http://arxiv.org/abs/2210.13461v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2210.13461v1)
- **Published**: 2022-10-23 05:44:22+00:00
- **Updated**: 2022-10-23 05:44:22+00:00
- **Authors**: Rajesh P. N. Rao, Dimitrios C. Gklezakos, Vishwas Sathish
- **Comment**: 15 pages, 10 figures, 2 supplementary figures
- **Journal**: None
- **Summary**: Predictive coding has emerged as a prominent model of how the brain learns through predictions, anticipating the importance accorded to predictive learning in recent AI architectures such as transformers. Here we propose a new framework for predictive coding called active predictive coding which can learn hierarchical world models and solve two radically different open problems in AI: (1) how do we learn compositional representations, e.g., part-whole hierarchies, for equivariant vision? and (2) how do we solve large-scale planning problems, which are hard for traditional reinforcement learning, by composing complex action sequences from primitive policies? Our approach exploits hypernetworks, self-supervised learning and reinforcement learning to learn hierarchical world models that combine task-invariant state transition networks and task-dependent policy networks at multiple abstraction levels. We demonstrate the viability of our approach on a variety of vision datasets (MNIST, FashionMNIST, Omniglot) as well as on a scalable hierarchical planning problem. Our results represent, to our knowledge, the first demonstration of a unified solution to the part-whole learning problem posed by Hinton, the nested reference frames problem posed by Hawkins, and the integrated state-action hierarchy learning problem in reinforcement learning.



### RSVG: Exploring Data and Models for Visual Grounding on Remote Sensing Data
- **Arxiv ID**: http://arxiv.org/abs/2210.12634v1
- **DOI**: 10.1109/TGRS.2023.3250471
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12634v1)
- **Published**: 2022-10-23 07:08:22+00:00
- **Updated**: 2022-10-23 07:08:22+00:00
- **Authors**: Yang Zhan, Zhitong Xiong, Yuan Yuan
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: In this paper, we introduce the task of visual grounding for remote sensing data (RSVG). RSVG aims to localize the referred objects in remote sensing (RS) images with the guidance of natural language. To retrieve rich information from RS imagery using natural language, many research tasks, like RS image visual question answering, RS image captioning, and RS image-text retrieval have been investigated a lot. However, the object-level visual grounding on RS images is still under-explored. Thus, in this work, we propose to construct the dataset and explore deep learning models for the RSVG task. Specifically, our contributions can be summarized as follows. 1) We build the new large-scale benchmark dataset of RSVG, termed RSVGD, to fully advance the research of RSVG. This new dataset includes image/expression/box triplets for training and evaluating visual grounding models. 2) We benchmark extensive state-of-the-art (SOTA) natural image visual grounding methods on the constructed RSVGD dataset, and some insightful analyses are provided based on the results. 3) A novel transformer-based Multi-Level Cross-Modal feature learning (MLCM) module is proposed. Remotely-sensed images are usually with large scale variations and cluttered backgrounds. To deal with the scale-variation problem, the MLCM module takes advantage of multi-scale visual features and multi-granularity textual embeddings to learn more discriminative representations. To cope with the cluttered background problem, MLCM adaptively filters irrelevant noise and enhances salient features. In this way, our proposed model can incorporate more effective multi-level and multi-modal features to boost performance. Furthermore, this work also provides useful insights for developing better RSVG models. The dataset and code will be publicly available at https://github.com/ZhanYang-nwpu/RSVG-pytorch.



### Artificial Intelligence-Based Methods for Fusion of Electronic Health Records and Imaging Data
- **Arxiv ID**: http://arxiv.org/abs/2210.13462v1
- **DOI**: 10.1038/s41598-022-22514-4
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.13462v1)
- **Published**: 2022-10-23 07:13:37+00:00
- **Updated**: 2022-10-23 07:13:37+00:00
- **Authors**: Farida Mohsen, Hazrat Ali, Nady El Hajj, Zubair Shah
- **Comment**: Accepted in Nature Scientific Reports. 20 pages
- **Journal**: Sci Rep 12, 17981 (2022)
- **Summary**: Healthcare data are inherently multimodal, including electronic health records (EHR), medical images, and multi-omics data. Combining these multimodal data sources contributes to a better understanding of human health and provides optimal personalized healthcare. Advances in artificial intelligence (AI) technologies, particularly machine learning (ML), enable the fusion of these different data modalities to provide multimodal insights. To this end, in this scoping review, we focus on synthesizing and analyzing the literature that uses AI techniques to fuse multimodal medical data for different clinical applications. More specifically, we focus on studies that only fused EHR with medical imaging data to develop various AI methods for clinical applications. We present a comprehensive analysis of the various fusion strategies, the diseases and clinical outcomes for which multimodal fusion was used, the ML algorithms used to perform multimodal fusion for each clinical application, and the available multimodal medical datasets. We followed the PRISMA-ScR guidelines. We searched Embase, PubMed, Scopus, and Google Scholar to retrieve relevant studies. We extracted data from 34 studies that fulfilled the inclusion criteria. In our analysis, a typical workflow was observed: feeding raw data, fusing different data modalities by applying conventional machine learning (ML) or deep learning (DL) algorithms, and finally, evaluating the multimodal fusion through clinical outcome predictions. Specifically, early fusion was the most used technique in most applications for multimodal learning (22 out of 34 studies). We found that multimodality fusion models outperformed traditional single-modality models for the same task. Disease diagnosis and prediction were the most common clinical outcomes (reported in 20 and 10 studies, respectively) from a clinical outcome perspective.



### Anticipative Feature Fusion Transformer for Multi-Modal Action Anticipation
- **Arxiv ID**: http://arxiv.org/abs/2210.12649v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.12649v1)
- **Published**: 2022-10-23 08:11:03+00:00
- **Updated**: 2022-10-23 08:11:03+00:00
- **Authors**: Zeyun Zhong, David Schneider, Michael Voit, Rainer Stiefelhagen, Jürgen Beyerer
- **Comment**: Accepted to WACV 2023
- **Journal**: None
- **Summary**: Although human action anticipation is a task which is inherently multi-modal, state-of-the-art methods on well known action anticipation datasets leverage this data by applying ensemble methods and averaging scores of unimodal anticipation networks. In this work we introduce transformer based modality fusion techniques, which unify multi-modal data at an early stage. Our Anticipative Feature Fusion Transformer (AFFT) proves to be superior to popular score fusion approaches and presents state-of-the-art results outperforming previous methods on EpicKitchens-100 and EGTEA Gaze+. Our model is easily extensible and allows for adding new modalities without architectural changes. Consequently, we extracted audio features on EpicKitchens-100 which we add to the set of commonly used features in the community.



### Extending Phrase Grounding with Pronouns in Visual Dialogues
- **Arxiv ID**: http://arxiv.org/abs/2210.12658v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12658v1)
- **Published**: 2022-10-23 08:32:25+00:00
- **Updated**: 2022-10-23 08:32:25+00:00
- **Authors**: Panzhong Lu, Xin Zhang, Meishan Zhang, Min Zhang
- **Comment**: Accepted by EMNLP 2022
- **Journal**: None
- **Summary**: Conventional phrase grounding aims to localize noun phrases mentioned in a given caption to their corresponding image regions, which has achieved great success recently. Apparently, sole noun phrase grounding is not enough for cross-modal visual language understanding. Here we extend the task by considering pronouns as well. First, we construct a dataset of phrase grounding with both noun phrases and pronouns to image regions. Based on the dataset, we test the performance of phrase grounding by using a state-of-the-art literature model of this line. Then, we enhance the baseline grounding model with coreference information which should help our task potentially, modeling the coreference structures with graph convolutional networks. Experiments on our dataset, interestingly, show that pronouns are easier to ground than noun phrases, where the possible reason might be that these pronouns are much less ambiguous. Additionally, our final model with coreference information can significantly boost the grounding performance of both noun phrases and pronouns.



### Rethinking Rotation in Self-Supervised Contrastive Learning: Adaptive Positive or Negative Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.12681v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12681v2)
- **Published**: 2022-10-23 09:37:47+00:00
- **Updated**: 2022-11-24 05:57:56+00:00
- **Authors**: Atsuyuki Miyai, Qing Yu, Daiki Ikami, Go Irie, Kiyoharu Aizawa
- **Comment**: Accepted at the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV) 2023
- **Journal**: None
- **Summary**: Rotation is frequently listed as a candidate for data augmentation in contrastive learning but seldom provides satisfactory improvements. We argue that this is because the rotated image is always treated as either positive or negative. The semantics of an image can be rotation-invariant or rotation-variant, so whether the rotated image is treated as positive or negative should be determined based on the content of the image. Therefore, we propose a novel augmentation strategy, adaptive Positive or Negative Data Augmentation (PNDA), in which an original and its rotated image are a positive pair if they are semantically close and a negative pair if they are semantically different. To achieve PNDA, we first determine whether rotation is positive or negative on an image-by-image basis in an unsupervised way. Then, we apply PNDA to contrastive learning frameworks. Our experiments showed that PNDA improves the performance of contrastive learning. The code is available at \url{ https://github.com/AtsuMiyai/rethinking_rotation}.



### Photo-realistic Neural Domain Randomization
- **Arxiv ID**: http://arxiv.org/abs/2210.12682v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2210.12682v1)
- **Published**: 2022-10-23 09:45:27+00:00
- **Updated**: 2022-10-23 09:45:27+00:00
- **Authors**: Sergey Zakharov, Rares Ambrus, Vitor Guizilini, Wadim Kehl, Adrien Gaidon
- **Comment**: Accepted to European Conference on Computer Vision (ECCV), 2022
- **Journal**: None
- **Summary**: Synthetic data is a scalable alternative to manual supervision, but it requires overcoming the sim-to-real domain gap. This discrepancy between virtual and real worlds is addressed by two seemingly opposed approaches: improving the realism of simulation or foregoing realism entirely via domain randomization. In this paper, we show that the recent progress in neural rendering enables a new unified approach we call Photo-realistic Neural Domain Randomization (PNDR). We propose to learn a composition of neural networks that acts as a physics-based ray tracer generating high-quality renderings from scene geometry alone. Our approach is modular, composed of different neural networks for materials, lighting, and rendering, thus enabling randomization of different key image generation components in a differentiable pipeline. Once trained, our method can be combined with other methods and used to generate photo-realistic image augmentations online and significantly more efficiently than via traditional ray-tracing. We demonstrate the usefulness of PNDR through two downstream tasks: 6D object detection and monocular depth estimation. Our experiments show that training with PNDR enables generalization to novel scenes and significantly outperforms the state of the art in terms of real-world transfer.



### GAN-based Facial Attribute Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2210.12683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12683v1)
- **Published**: 2022-10-23 09:49:08+00:00
- **Updated**: 2022-10-23 09:49:08+00:00
- **Authors**: Yunfan Liu, Qi Li, Qiyao Deng, Zhenan Sun, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Facial Attribute Manipulation (FAM) aims to aesthetically modify a given face image to render desired attributes, which has received significant attention due to its broad practical applications ranging from digital entertainment to biometric forensics. In the last decade, with the remarkable success of Generative Adversarial Networks (GANs) in synthesizing realistic images, numerous GAN-based models have been proposed to solve FAM with various problem formulation approaches and guiding information representations. This paper presents a comprehensive survey of GAN-based FAM methods with a focus on summarizing their principal motivations and technical details. The main contents of this survey include: (i) an introduction to the research background and basic concepts related to FAM, (ii) a systematic review of GAN-based FAM methods in three main categories, and (iii) an in-depth discussion of important properties of FAM methods, open issues, and future research directions. This survey not only builds a good starting point for researchers new to this field but also serves as a reference for the vision community.



### Holistic Interaction Transformer Network for Action Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.12686v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.12686v2)
- **Published**: 2022-10-23 10:19:37+00:00
- **Updated**: 2022-11-18 05:28:40+00:00
- **Authors**: Gueter Josmy Faure, Min-Hung Chen, Shang-Hong Lai
- **Comment**: Accepted for WACV 2023. Code: https://github.com/joslefaure/HIT
- **Journal**: None
- **Summary**: Actions are about how we interact with the environment, including other people, objects, and ourselves. In this paper, we propose a novel multi-modal Holistic Interaction Transformer Network (HIT) that leverages the largely ignored, but critical hand and pose information essential to most human actions. The proposed "HIT" network is a comprehensive bi-modal framework that comprises an RGB stream and a pose stream. Each of them separately models person, object, and hand interactions. Within each sub-network, an Intra-Modality Aggregation module (IMA) is introduced that selectively merges individual interaction units. The resulting features from each modality are then glued using an Attentive Fusion Mechanism (AFM). Finally, we extract cues from the temporal context to better classify the occurring actions using cached memory. Our method significantly outperforms previous approaches on the J-HMDB, UCF101-24, and MultiSports datasets. We also achieve competitive results on AVA. The code will be available at https://github.com/joslefaure/HIT.



### Face Emotion Recognization Using Dataset Augmentation Based on Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2210.12689v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12689v2)
- **Published**: 2022-10-23 10:21:45+00:00
- **Updated**: 2022-11-21 14:55:36+00:00
- **Authors**: Mengyu Rao, Ruyi Bao, Liangshun Dong
- **Comment**: 5 pages, 8 figures, 3 tables
- **Journal**: None
- **Summary**: Facial expression is one of the most external indications of a person's feelings and emotions. In daily conversation, according to the psychologist, only 7% and 38% of information is communicated through words and sounds respective, while up to 55% is through facial expression. It plays an important role in coordinating interpersonal relationships. Ekman and Friesen recognized six essential emotions in the nineteenth century depending on a cross-cultural study, which indicated that people feel each basic emotion in the same fashion despite culture. As a branch of the field of analyzing sentiment, facial expression recognition offers broad application prospects in a variety of domains, including the interaction between humans and computers, healthcare, and behavior monitoring. Therefore, many researchers have devoted themselves to facial expression recognition. In this paper, an effective hybrid data augmentation method is used. This approach is operated on two public datasets, and four benchmark models see some remarkable results.



### Attention Based Relation Network for Facial Action Units Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.13988v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.13988v1)
- **Published**: 2022-10-23 11:26:53+00:00
- **Updated**: 2022-10-23 11:26:53+00:00
- **Authors**: Yao Wei, Haoxiang Wang, Mingze Sun, Jiawang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Facial action unit (AU) recognition is essential to facial expression analysis. Since there are highly positive or negative correlations between AUs, some existing AU recognition works have focused on modeling AU relations. However, previous relationship-based approaches typically embed predefined rules into their models and ignore the impact of various AU relations in different crowds. In this paper, we propose a novel Attention Based Relation Network (ABRNet) for AU recognition, which can automatically capture AU relations without unnecessary or even disturbing predefined rules. ABRNet uses several relation learning layers to automatically capture different AU relations. The learned AU relation features are then fed into a self-attention fusion module, which aims to refine individual AU features with attention weights to enhance the feature robustness. Furthermore, we propose an AU relation dropout strategy and AU relation loss (AUR-Loss) to better model AU relations, which can further improve AU recognition. Extensive experiments show that our approach achieves state-of-the-art performance on the DISFA and DISFA+ datasets.



### Few-Shot Meta Learning for Recognizing Facial Phenotypes of Genetic Disorders
- **Arxiv ID**: http://arxiv.org/abs/2210.12705v2
- **DOI**: 10.3233/SHTI230312
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2210.12705v2)
- **Published**: 2022-10-23 11:52:57+00:00
- **Updated**: 2023-05-24 09:37:42+00:00
- **Authors**: Ömer Sümer, Fabio Hellmann, Alexander Hustinx, Tzung-Chien Hsieh, Elisabeth André, Peter Krawitz
- **Comment**: This paper is accepted for publication at MIE 2023 Conference
- **Journal**: None
- **Summary**: Computer vision-based methods have valuable use cases in precision medicine, and recognizing facial phenotypes of genetic disorders is one of them. Many genetic disorders are known to affect faces' visual appearance and geometry. Automated classification and similarity retrieval aid physicians in decision-making to diagnose possible genetic conditions as early as possible. Previous work has addressed the problem as a classification problem and used deep learning methods. The challenging issue in practice is the sparse label distribution and huge class imbalances across categories. Furthermore, most disorders have few labeled samples in training sets, making representation learning and generalization essential to acquiring a reliable feature descriptor. In this study, we used a facial recognition model trained on a large corpus of healthy individuals as a pre-task and transferred it to facial phenotype recognition. Furthermore, we created simple baselines of few-shot meta-learning methods to improve our base feature descriptor. Our quantitative results on GestaltMatcher Database show that our CNN baseline surpasses previous works, including GestaltMatcher, and few-shot meta-learning strategies improve retrieval performance in frequent and rare classes.



### Adversarial Pretraining of Self-Supervised Deep Networks: Past, Present and Future
- **Arxiv ID**: http://arxiv.org/abs/2210.13463v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.13463v1)
- **Published**: 2022-10-23 13:14:06+00:00
- **Updated**: 2022-10-23 13:14:06+00:00
- **Authors**: Guo-Jun Qi, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we review adversarial pretraining of self-supervised deep networks including both convolutional neural networks and vision transformers. Unlike the adversarial training with access to labeled examples, adversarial pretraining is complicated as it only has access to unlabeled examples. To incorporate adversaries into pretraining models on either input or feature level, we find that existing approaches are largely categorized into two groups: memory-free instance-wise attacks imposing worst-case perturbations on individual examples, and memory-based adversaries shared across examples over iterations. In particular, we review several representative adversarial pretraining models based on Contrastive Learning (CL) and Masked Image Modeling (MIM), respectively, two popular self-supervised pretraining methods in literature. We also review miscellaneous issues about computing overheads, input-/feature-level adversaries, as well as other adversarial pretraining approaches beyond the above two groups. Finally, we discuss emerging trends and future directions about the relations between adversarial and cooperative pretraining, unifying adversarial CL and MIM pretraining, and the trade-off between accuracy and robustness in adversarial pretraining.



### Joint Rigid Motion Correction and Sparse-View CT via Self-Calibrating Neural Field
- **Arxiv ID**: http://arxiv.org/abs/2210.12731v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12731v2)
- **Published**: 2022-10-23 13:55:07+00:00
- **Updated**: 2022-11-06 05:45:22+00:00
- **Authors**: Qing Wu, Xin Li, Hongjiang Wei, Jingyi Yu, Yuyao Zhang
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Neural Radiance Field (NeRF) has widely received attention in Sparse-View Computed Tomography (SVCT) reconstruction tasks as a self-supervised deep learning framework. NeRF-based SVCT methods represent the desired CT image as a continuous function of spatial coordinates and train a Multi-Layer Perceptron (MLP) to learn the function by minimizing loss on the SV sinogram. Benefiting from the continuous representation provided by NeRF, the high-quality CT image can be reconstructed. However, existing NeRF-based SVCT methods strictly suppose there is completely no relative motion during the CT acquisition because they require \textit{accurate} projection poses to model the X-rays that scan the SV sinogram. Therefore, these methods suffer from severe performance drops for real SVCT imaging with motion. In this work, we propose a self-calibrating neural field to recover the artifacts-free image from the rigid motion-corrupted SV sinogram without using any external data. Specifically, we parametrize the inaccurate projection poses caused by rigid motion as trainable variables and then jointly optimize these pose variables and the MLP. We conduct numerical experiments on a public CT image dataset. The results indicate our model significantly outperforms two representative NeRF-based methods for SVCT reconstruction tasks with four different levels of rigid motion.



### Self-supervised Amodal Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2210.12733v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12733v1)
- **Published**: 2022-10-23 14:09:35+00:00
- **Updated**: 2022-10-23 14:09:35+00:00
- **Authors**: Jian Yao, Yuxin Hong, Chiyu Wang, Tianjun Xiao, Tong He, Francesco Locatello, David Wipf, Yanwei Fu, Zheng Zhang
- **Comment**: accepted in Neurips2022
- **Journal**: None
- **Summary**: Amodal perception requires inferring the full shape of an object that is partially occluded. This task is particularly challenging on two levels: (1) it requires more information than what is contained in the instant retina or imaging sensor, (2) it is difficult to obtain enough well-annotated amodal labels for supervision. To this end, this paper develops a new framework of Self-supervised amodal Video object segmentation (SaVos). Our method efficiently leverages the visual information of video temporal sequences to infer the amodal mask of objects. The key intuition is that the occluded part of an object can be explained away if that part is visible in other frames, possibly deformed as long as the deformation can be reasonably learned. Accordingly, we derive a novel self-supervised learning paradigm that efficiently utilizes the visible object parts as the supervision to guide the training on videos. In addition to learning type prior to complete masks for known types, SaVos also learns the spatiotemporal prior, which is also useful for the amodal task and could generalize to unseen types. The proposed framework achieves the state-of-the-art performance on the synthetic amodal segmentation benchmark FISHBOWL and the real world benchmark KINS-Video-Car. Further, it lends itself well to being transferred to novel distributions using test-time adaptation, outperforming existing models even after the transfer to a new distribution.



### Functional Indirection Neural Estimator for Better Out-of-distribution Generalization
- **Arxiv ID**: http://arxiv.org/abs/2210.12739v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12739v1)
- **Published**: 2022-10-23 14:43:02+00:00
- **Updated**: 2022-10-23 14:43:02+00:00
- **Authors**: Kha Pham, Hung Le, Man Ngo, Truyen Tran
- **Comment**: Accepted paper at NeurIPS 2022
- **Journal**: None
- **Summary**: The capacity to achieve out-of-distribution (OOD) generalization is a hallmark of human intelligence and yet remains out of reach for machines. This remarkable capability has been attributed to our abilities to make conceptual abstraction and analogy, and to a mechanism known as indirection, which binds two representations and uses one representation to refer to the other. Inspired by these mechanisms, we hypothesize that OOD generalization may be achieved by performing analogy-making and indirection in the functional space instead of the data space as in current methods. To realize this, we design FINE (Functional Indirection Neural Estimator), a neural framework that learns to compose functions that map data input to output on-the-fly. FINE consists of a backbone network and a trainable semantic memory of basis weight matrices. Upon seeing a new input-output data pair, FINE dynamically constructs the backbone weights by mixing the basis weights. The mixing coefficients are indirectly computed through querying a separate corresponding semantic memory using the data pair. We demonstrate empirically that FINE can strongly improve out-of-distribution generalization on IQ tasks that involve geometric transformations. In particular, we train FINE and competing models on IQ tasks using images from the MNIST, Omniglot and CIFAR100 datasets and test on tasks with unseen image classes from one or different datasets and unseen transformation rules. FINE not only achieves the best performance on all tasks but also is able to adapt to small-scale data scenarios.



### Principal Component Classification
- **Arxiv ID**: http://arxiv.org/abs/2210.12746v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2210.12746v2)
- **Published**: 2022-10-23 15:05:14+00:00
- **Updated**: 2022-10-26 17:23:29+00:00
- **Authors**: Rozenn Dahyot
- **Comment**: 5 pages; 5 figures; 1 table
- **Journal**: None
- **Summary**: We propose to directly compute classification estimates by learning features encoded with their class scores using PCA. Our resulting model has a encoder-decoder structure suitable for supervised learning, it is computationally efficient and performs well for classification on several datasets.



### SC-wLS: Towards Interpretable Feed-forward Camera Re-localization
- **Arxiv ID**: http://arxiv.org/abs/2210.12748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12748v1)
- **Published**: 2022-10-23 15:15:48+00:00
- **Updated**: 2022-10-23 15:15:48+00:00
- **Authors**: Xin Wu, Hao Zhao, Shunkai Li, Yingdian Cao, Hongbin Zha
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Visual re-localization aims to recover camera poses in a known environment, which is vital for applications like robotics or augmented reality. Feed-forward absolute camera pose regression methods directly output poses by a network, but suffer from low accuracy. Meanwhile, scene coordinate based methods are accurate, but need iterative RANSAC post-processing, which brings challenges to efficient end-to-end training and inference. In order to have the best of both worlds, we propose a feed-forward method termed SC-wLS that exploits all scene coordinate estimates for weighted least squares pose regression. This differentiable formulation exploits a weight network imposed on 2D-3D correspondences, and requires pose supervision only. Qualitative results demonstrate the interpretability of learned weights. Evaluations on 7Scenes and Cambridge datasets show significantly promoted performance when compared with former feed-forward counterparts. Moreover, our SC-wLS method enables a new capability: self-supervised test-time adaptation on the weight network. Codes and models are publicly available.



### UIA-ViT: Unsupervised Inconsistency-Aware Method based on Vision Transformer for Face Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2210.12752v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12752v1)
- **Published**: 2022-10-23 15:24:47+00:00
- **Updated**: 2022-10-23 15:24:47+00:00
- **Authors**: Wanyi Zhuang, Qi Chu, Zhentao Tan, Qiankun Liu, Haojie Yuan, Changtao Miao, Zixiang Luo, Nenghai Yu
- **Comment**: accepted by ECCV 2022 (oral)
- **Journal**: None
- **Summary**: Intra-frame inconsistency has been proved to be effective for the generalization of face forgery detection. However, learning to focus on these inconsistency requires extra pixel-level forged location annotations. Acquiring such annotations is non-trivial. Some existing methods generate large-scale synthesized data with location annotations, which is only composed of real images and cannot capture the properties of forgery regions. Others generate forgery location labels by subtracting paired real and fake images, yet such paired data is difficult to collected and the generated label is usually discontinuous. To overcome these limitations, we propose a novel Unsupervised Inconsistency-Aware method based on Vision Transformer, called UIA-ViT, which only makes use of video-level labels and can learn inconsistency-aware feature without pixel-level annotations. Due to the self-attention mechanism, the attention map among patch embeddings naturally represents the consistency relation, making the vision Transformer suitable for the consistency representation learning. Based on vision Transformer, we propose two key components: Unsupervised Patch Consistency Learning (UPCL) and Progressive Consistency Weighted Assemble (PCWA). UPCL is designed for learning the consistency-related representation with progressive optimized pseudo annotations. PCWA enhances the final classification embedding with previous patch embeddings optimized by UPCL to further improve the detection performance. Extensive experiments demonstrate the effectiveness of the proposed method.



### LCPFormer: Towards Effective 3D Point Cloud Analysis via Local Context Propagation in Transformers
- **Arxiv ID**: http://arxiv.org/abs/2210.12755v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12755v2)
- **Published**: 2022-10-23 15:43:01+00:00
- **Updated**: 2023-02-19 14:44:11+00:00
- **Authors**: Zhuoxu Huang, Zhiyou Zhao, Banghuai Li, Jungong Han
- **Comment**: Accepted by TCSVT
- **Journal**: None
- **Summary**: Transformer with its underlying attention mechanism and the ability to capture long-range dependencies makes it become a natural choice for unordered point cloud data. However, separated local regions from the general sampling architecture corrupt the structural information of the instances, and the inherent relationships between adjacent local regions lack exploration, while local structural information is crucial in a transformer-based 3D point cloud model. Therefore, in this paper, we propose a novel module named Local Context Propagation (LCP) to exploit the message passing between neighboring local regions and make their representations more informative and discriminative. More specifically, we use the overlap points of adjacent local regions (which statistically show to be prevalent) as intermediaries, then re-weight the features of these shared points from different local regions before passing them to the next layers. Inserting the LCP module between two transformer layers results in a significant improvement in network expressiveness. Finally, we design a flexible LCPFormer architecture equipped with the LCP module. The proposed method is applicable to different tasks and outperforms various transformer-based methods in benchmarks including 3D shape classification and dense prediction tasks such as 3D object detection and semantic segmentation. Code will be released for reproduction.



### VP-SLAM: A Monocular Real-time Visual SLAM with Points, Lines and Vanishing Points
- **Arxiv ID**: http://arxiv.org/abs/2210.12756v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12756v2)
- **Published**: 2022-10-23 15:54:26+00:00
- **Updated**: 2022-10-28 10:29:20+00:00
- **Authors**: Andreas Georgis, Panagiotis Mermigkas, Petros Maragos
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional monocular Visual Simultaneous Localization and Mapping (vSLAM) systems can be divided into three categories: those that use features, those that rely on the image itself, and hybrid models. In the case of feature-based methods, new research has evolved to incorporate more information from their environment using geometric primitives beyond points, such as lines and planes. This is because in many environments, which are man-made environments, characterized as Manhattan world, geometric primitives such as lines and planes occupy most of the space in the environment. The exploitation of these schemes can lead to the introduction of algorithms capable of optimizing the trajectory of a Visual SLAM system and also helping to construct an exuberant map. Thus, we present a real-time monocular Visual SLAM system that incorporates real-time methods for line and VP extraction, as well as two strategies that exploit vanishing points to estimate the robot's translation and improve its rotation.Particularly, we build on ORB-SLAM2, which is considered the current state-of-the-art solution in terms of both accuracy and efficiency, and extend its formulation to handle lines and VPs to create two strategies the first optimize the rotation and the second refine the translation part from the known rotation. First, we extract VPs using a real-time method and use them for a global rotation optimization strategy. Second, we present a translation estimation method that takes advantage of last-stage rotation optimization to model a linear system. Finally, we evaluate our system on the TUM RGB-D benchmark and demonstrate that the proposed system achieves state-of-the-art results and runs in real time, and its performance remains close to the original ORB-SLAM2 system



### Beta R-CNN: Looking into Pedestrian Detection from Another Perspective
- **Arxiv ID**: http://arxiv.org/abs/2210.12758v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12758v1)
- **Published**: 2022-10-23 15:56:54+00:00
- **Updated**: 2022-10-23 15:56:54+00:00
- **Authors**: Zixuan Xu, Banghuai Li, Ye Yuan, Anhong Dang
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Recently significant progress has been made in pedestrian detection, but it remains challenging to achieve high performance in occluded and crowded scenes. It could be attributed mostly to the widely used representation of pedestrians, i.e., 2D axis-aligned bounding box, which just describes the approximate location and size of the object. Bounding box models the object as a uniform distribution within the boundary, making pedestrians indistinguishable in occluded and crowded scenes due to much noise. To eliminate the problem, we propose a novel representation based on 2D beta distribution, named Beta Representation. It pictures a pedestrian by explicitly constructing the relationship between full-body and visible boxes, and emphasizes the center of visual mass by assigning different probability values to pixels. As a result, Beta Representation is much better for distinguishing highly-overlapped instances in crowded scenes with a new NMS strategy named BetaNMS. What's more, to fully exploit Beta Representation, a novel pipeline Beta R-CNN equipped with BetaHead and BetaMask is proposed, leading to high detection performance in occluded and crowded scenes.



### Compressing Explicit Voxel Grid Representations: fast NeRFs become also small
- **Arxiv ID**: http://arxiv.org/abs/2210.12782v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.12782v1)
- **Published**: 2022-10-23 16:42:29+00:00
- **Updated**: 2022-10-23 16:42:29+00:00
- **Authors**: Chenxi Lola Deng, Enzo Tartaglione
- **Comment**: None
- **Journal**: None
- **Summary**: NeRFs have revolutionized the world of per-scene radiance field reconstruction because of their intrinsic compactness. One of the main limitations of NeRFs is their slow rendering speed, both at training and inference time. Recent research focuses on the optimization of an explicit voxel grid (EVG) that represents the scene, which can be paired with neural networks to learn radiance fields. This approach significantly enhances the speed both at train and inference time, but at the cost of large memory occupation. In this work we propose Re:NeRF, an approach that specifically targets EVG-NeRFs compressibility, aiming to reduce memory storage of NeRF models while maintaining comparable performance. We benchmark our approach with three different EVG-NeRF architectures on four popular benchmarks, showing Re:NeRF's broad usability and effectiveness.



### An Improved RaftStereo Trained with A Mixed Dataset for the Robust Vision Challenge 2022
- **Arxiv ID**: http://arxiv.org/abs/2210.12785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12785v1)
- **Published**: 2022-10-23 17:01:34+00:00
- **Updated**: 2022-10-23 17:01:34+00:00
- **Authors**: Hualie Jiang, Rui Xu, Wenjie Jiang
- **Comment**: Technical report; Ranking at 2nd on the stereo track of Robust Vision
  Challenge 2022
- **Journal**: None
- **Summary**: Stereo-matching is a fundamental problem in computer vision. Despite recent progress by deep learning, improving the robustness is ineluctable when deploying stereo-matching models to real-world applications. Different from the common practices, i.e., developing an elaborate model to achieve robustness, we argue that collecting multiple available datasets for training is a cheaper way to increase generalization ability. Specifically, this report presents an improved RaftStereo trained with a mixed dataset of seven public datasets for the robust vision challenge (denoted as iRaftStereo_RVC). When evaluated on the training sets of Middlebury, KITTI-2015, and ETH3D, the model outperforms its counterparts trained with only one dataset, such as the popular Sceneflow. After fine-tuning the pre-trained model on the three datasets of the challenge, it ranks at 2nd place on the stereo leaderboard, demonstrating the benefits of mixed dataset pre-training.



### Respecting Transfer Gap in Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2210.12787v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.12787v1)
- **Published**: 2022-10-23 17:05:32+00:00
- **Updated**: 2022-10-23 17:05:32+00:00
- **Authors**: Yulei Niu, Long Chen, Chang Zhou, Hanwang Zhang
- **Comment**: Accepted by NeurIPS 2022
- **Journal**: None
- **Summary**: Knowledge distillation (KD) is essentially a process of transferring a teacher model's behavior, e.g., network response, to a student model. The network response serves as additional supervision to formulate the machine domain, which uses the data collected from the human domain as a transfer set. Traditional KD methods hold an underlying assumption that the data collected in both human domain and machine domain are both independent and identically distributed (IID). We point out that this naive assumption is unrealistic and there is indeed a transfer gap between the two domains. Although the gap offers the student model external knowledge from the machine domain, the imbalanced teacher knowledge would make us incorrectly estimate how much to transfer from teacher to student per sample on the non-IID transfer set. To tackle this challenge, we propose Inverse Probability Weighting Distillation (IPWD) that estimates the propensity score of a training sample belonging to the machine domain, and assigns its inverse amount to compensate for under-represented samples. Experiments on CIFAR-100 and ImageNet demonstrate the effectiveness of IPWD for both two-stage distillation and one-stage self-distillation.



### Drastically Reducing the Number of Trainable Parameters in Deep CNNs by Inter-layer Kernel-sharing
- **Arxiv ID**: http://arxiv.org/abs/2210.14151v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.14151v1)
- **Published**: 2022-10-23 18:14:30+00:00
- **Updated**: 2022-10-23 18:14:30+00:00
- **Authors**: Alireza Azadbakht, Saeed Reza Kheradpisheh, Ismail Khalfaoui-Hassani, Timothée Masquelier
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) have become the state-of-the-art (SOTA) approach for many computer vision tasks: image classification, object detection, semantic segmentation, etc. However, most SOTA networks are too large for edge computing. Here, we suggest a simple way to reduce the number of trainable parameters and thus the memory footprint: sharing kernels between multiple convolutional layers. Kernel-sharing is only possible between ``isomorphic" layers, i.e.layers having the same kernel size, input and output channels. This is typically the case inside each stage of a DCNN. Our experiments on CIFAR-10 and CIFAR-100, using the ConvMixer and SE-ResNet architectures show that the number of parameters of these models can drastically be reduced with minimal cost on accuracy. The resulting networks are appealing for certain edge computing applications that are subject to severe memory constraints, and even more interesting if leveraging "frozen weights" hardware accelerators. Kernel-sharing is also an efficient regularization method, which can reduce overfitting. The codes are publicly available at https://github.com/AlirezaAzadbakht/kernel-sharing.



### Pushing the Efficiency Limit Using Structured Sparse Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2210.12818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12818v1)
- **Published**: 2022-10-23 18:37:22+00:00
- **Updated**: 2022-10-23 18:37:22+00:00
- **Authors**: Vinay Kumar Verma, Nikhil Mehta, Shijing Si, Ricardo Henao, Lawrence Carin
- **Comment**: Accepted at the IEEE Winter Conference on Applications of Computer
  Vision, WACV 2023
- **Journal**: None
- **Summary**: Weight pruning is among the most popular approaches for compressing deep convolutional neural networks. Recent work suggests that in a randomly initialized deep neural network, there exist sparse subnetworks that achieve performance comparable to the original network. Unfortunately, finding these subnetworks involves iterative stages of training and pruning, which can be computationally expensive. We propose Structured Sparse Convolution (SSC), which leverages the inherent structure in images to reduce the parameters in the convolutional filter. This leads to improved efficiency of convolutional architectures compared to existing methods that perform pruning at initialization. We show that SSC is a generalization of commonly used layers (depthwise, groupwise and pointwise convolution) in ``efficient architectures.'' Extensive experiments on well-known CNN models and datasets show the effectiveness of the proposed method. Architectures based on SSC achieve state-of-the-art performance compared to baselines on CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet classification benchmarks.



### An Interpretable Deep Semantic Segmentation Method for Earth Observation
- **Arxiv ID**: http://arxiv.org/abs/2210.12820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12820v1)
- **Published**: 2022-10-23 18:46:44+00:00
- **Updated**: 2022-10-23 18:46:44+00:00
- **Authors**: Ziyang Zhang, Plamen Angelov, Eduardo Soares, Nicolas Longepe, Pierre Philippe Mathieu
- **Comment**: None
- **Journal**: None
- **Summary**: Earth observation is fundamental for a range of human activities including flood response as it offers vital information to decision makers. Semantic segmentation plays a key role in mapping the raw hyper-spectral data coming from the satellites into a human understandable form assigning class labels to each pixel. In this paper, we introduce a prototype-based interpretable deep semantic segmentation (IDSS) method, which is highly accurate as well as interpretable. Its parameters are in orders of magnitude less than the number of parameters used by deep networks such as U-Net and are clearly interpretable by humans. The proposed here IDSS offers a transparent structure that allows users to inspect and audit the algorithm's decision. Results have demonstrated that IDSS could surpass other algorithms, including U-Net, in terms of IoU (Intersection over Union) total water and Recall total water. We used WorldFloods data set for our experiments and plan to use the semantic segmentation results combined with masks for permanent water to detect flood events.



### Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization
- **Arxiv ID**: http://arxiv.org/abs/2210.12826v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.12826v1)
- **Published**: 2022-10-23 19:14:50+00:00
- **Updated**: 2022-10-23 19:14:50+00:00
- **Authors**: Peter Schaldenbrand, Zhixuan Liu, Jean Oh
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an approach to generating videos based on a series of given language descriptions. Frames of the video are generated sequentially and optimized by guidance from the CLIP image-text encoder; iterating through language descriptions, weighting the current description higher than others. As opposed to optimizing through an image generator model itself, which tends to be computationally heavy, the proposed approach computes the CLIP loss directly at the pixel level, achieving general content at a speed suitable for near real-time systems. The approach can generate videos in up to 720p resolution, variable frame-rates, and arbitrary aspect ratios at a rate of 1-2 frames per second. Please visit our website to view videos and access our open-source code: https://pschaldenbrand.github.io/text2video/ .



### Delving into Masked Autoencoders for Multi-Label Thorax Disease Classification
- **Arxiv ID**: http://arxiv.org/abs/2210.12843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12843v1)
- **Published**: 2022-10-23 20:14:57+00:00
- **Updated**: 2022-10-23 20:14:57+00:00
- **Authors**: Junfei Xiao, Yutong Bai, Alan Yuille, Zongwei Zhou
- **Comment**: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV
  2023)
- **Journal**: None
- **Summary**: Vision Transformer (ViT) has become one of the most popular neural architectures due to its great scalability, computational efficiency, and compelling performance in many vision tasks. However, ViT has shown inferior performance to Convolutional Neural Network (CNN) on medical tasks due to its data-hungry nature and the lack of annotated medical data. In this paper, we pre-train ViTs on 266,340 chest X-rays using Masked Autoencoders (MAE) which reconstruct missing pixels from a small part of each image. For comparison, CNNs are also pre-trained on the same 266,340 X-rays using advanced self-supervised methods (e.g., MoCo v2). The results show that our pre-trained ViT performs comparably (sometimes better) to the state-of-the-art CNN (DenseNet-121) for multi-label thorax disease classification. This performance is attributed to the strong recipes extracted from our empirical studies for pre-training and fine-tuning ViT. The pre-training recipe signifies that medical reconstruction requires a much smaller proportion of an image (10% vs. 25%) and a more moderate random resized crop range (0.5~1.0 vs. 0.2~1.0) compared with natural imaging. Furthermore, we remark that in-domain transfer learning is preferred whenever possible. The fine-tuning recipe discloses that layer-wise LR decay, RandAug magnitude, and DropPath rate are significant factors to consider. We hope that this study can direct future research on the application of Transformers to a larger variety of medical imaging tasks.



### 1st Place Solution of The Robust Vision Challenge 2022 Semantic Segmentation Track
- **Arxiv ID**: http://arxiv.org/abs/2210.12852v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12852v3)
- **Published**: 2022-10-23 20:52:22+00:00
- **Updated**: 2022-11-07 08:44:17+00:00
- **Authors**: Junfei Xiao, Zhichao Xu, Shiyi Lan, Zhiding Yu, Alan Yuille, Anima Anandkumar
- **Comment**: The Winning Solution to The Robust Vision Challenge 2022 Semantic
  Segmentation Track
- **Journal**: None
- **Summary**: This report describes the winning solution to the Robust Vision Challenge (RVC) semantic segmentation track at ECCV 2022. Our method adopts the FAN-B-Hybrid model as the encoder and uses SegFormer as the segmentation framework. The model is trained on a composite dataset consisting of images from 9 datasets (ADE20K, Cityscapes, Mapillary Vistas, ScanNet, VIPER, WildDash 2, IDD, BDD, and COCO) with a simple dataset balancing strategy. All the original labels are projected to a 256-class unified label space, and the model is trained using a cross-entropy loss. Without significant hyperparameter tuning or any specific loss weighting, our solution ranks the first place on all the testing semantic segmentation benchmarks from multiple domains (ADE20K, Cityscapes, Mapillary Vistas, ScanNet, VIPER, and WildDash 2). The proposed method can serve as a strong baseline for the multi-domain segmentation task and benefit future works. Code will be available at https://github.com/lambert-x/RVC_Segmentation.



### Symmetry and Variance: Generative Parametric Modelling of Historical Brick Wall Patterns
- **Arxiv ID**: http://arxiv.org/abs/2210.12856v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2210.12856v1)
- **Published**: 2022-10-23 21:14:36+00:00
- **Updated**: 2022-10-23 21:14:36+00:00
- **Authors**: Sevgi Altun, Mustafa Cem Gunes, Yusuf H. Sahin, Alican Mertan, Gozde Unal, Mine Ozkar
- **Comment**: 10 pages, 7 Figures. This paper is published at "Symmetry: Art and
  Science | 12th SIS-Symmetry Congress"
- **Journal**: Porto, Portugal: SIS-Symmetry (2022) pp. 96-104
- **Summary**: This study integrates artificial intelligence and computational design tools to extract information from architectural heritage. Photogrammetry-based point cloud models of brick walls from the Anatolian Seljuk period are analysed in terms of the interrelated units of construction, simultaneously considering both the inherent symmetries and irregularities. The real-world data is used as input for acquiring the stochastic parameters of spatial relations and a set of parametric shape rules to recreate designs of existing and hypothetical brick walls within the style. The motivation is to be able to generate large data sets for machine learning of the style and to devise procedures for robotic production of such designs with repetitive units.



### K-SAM: Sharpness-Aware Minimization at the Speed of SGD
- **Arxiv ID**: http://arxiv.org/abs/2210.12864v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12864v1)
- **Published**: 2022-10-23 21:49:58+00:00
- **Updated**: 2022-10-23 21:49:58+00:00
- **Authors**: Renkun Ni, Ping-yeh Chiang, Jonas Geiping, Micah Goldblum, Andrew Gordon Wilson, Tom Goldstein
- **Comment**: 13 pages, 2 figures
- **Journal**: None
- **Summary**: Sharpness-Aware Minimization (SAM) has recently emerged as a robust technique for improving the accuracy of deep neural networks. However, SAM incurs a high computational cost in practice, requiring up to twice as much computation as vanilla SGD. The computational challenge posed by SAM arises because each iteration requires both ascent and descent steps and thus double the gradient computations. To address this challenge, we propose to compute gradients in both stages of SAM on only the top-k samples with highest loss. K-SAM is simple and extremely easy-to-implement while providing significant generalization boosts over vanilla SGD at little to no additional cost.



### Deep Equilibrium Approaches to Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2210.12867v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12867v1)
- **Published**: 2022-10-23 22:02:19+00:00
- **Updated**: 2022-10-23 22:02:19+00:00
- **Authors**: Ashwini Pokle, Zhengyang Geng, Zico Kolter
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: Diffusion-based generative models are extremely effective in generating high-quality images, with generated samples often surpassing the quality of those produced by other models under several metrics. One distinguishing feature of these models, however, is that they typically require long sampling chains to produce high-fidelity images. This presents a challenge not only from the lenses of sampling time, but also from the inherent difficulty in backpropagating through these chains in order to accomplish tasks such as model inversion, i.e. approximately finding latent states that generate known images. In this paper, we look at diffusion models through a different perspective, that of a (deep) equilibrium (DEQ) fixed point model. Specifically, we extend the recent denoising diffusion implicit model (DDIM; Song et al. 2020), and model the entire sampling chain as a joint, multivariate fixed point system. This setup provides an elegant unification of diffusion and equilibrium models, and shows benefits in 1) single image sampling, as it replaces the fully-serial typical sampling process with a parallel one; and 2) model inversion, where we can leverage fast gradients in the DEQ setting to much more quickly find the noise that generates a given image. The approach is also orthogonal and thus complementary to other methods used to reduce the sampling time, or improve model inversion. We demonstrate our method's strong performance across several datasets, including CIFAR10, CelebA, and LSUN Bedrooms and Churches.



### IDD-3D: Indian Driving Dataset for 3D Unstructured Road Scenes
- **Arxiv ID**: http://arxiv.org/abs/2210.12878v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.12878v1)
- **Published**: 2022-10-23 23:03:17+00:00
- **Updated**: 2022-10-23 23:03:17+00:00
- **Authors**: Shubham Dokania, A. H. Abdul Hafez, Anbumani Subramanian, Manmohan Chandraker, C. V. Jawahar
- **Comment**: 10 pages, 8 figures, 5 tables, Accepted in Winter Conference on
  Applications of Computer Vision (WACV 2023)
- **Journal**: None
- **Summary**: Autonomous driving and assistance systems rely on annotated data from traffic and road scenarios to model and learn the various object relations in complex real-world scenarios. Preparation and training of deploy-able deep learning architectures require the models to be suited to different traffic scenarios and adapt to different situations. Currently, existing datasets, while large-scale, lack such diversities and are geographically biased towards mainly developed cities. An unstructured and complex driving layout found in several developing countries such as India poses a challenge to these models due to the sheer degree of variations in the object types, densities, and locations. To facilitate better research toward accommodating such scenarios, we build a new dataset, IDD-3D, which consists of multi-modal data from multiple cameras and LiDAR sensors with 12k annotated driving LiDAR frames across various traffic scenarios. We discuss the need for this dataset through statistical comparisons with existing datasets and highlight benchmarks on standard 3D object detection and tracking tasks in complex layouts. Code and data available at https://github.com/shubham1810/idd3d_kit.git



### DALL-E 2 Fails to Reliably Capture Common Syntactic Processes
- **Arxiv ID**: http://arxiv.org/abs/2210.12889v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.12889v2)
- **Published**: 2022-10-23 23:56:54+00:00
- **Updated**: 2022-10-25 05:16:50+00:00
- **Authors**: Evelina Leivada, Elliot Murphy, Gary Marcus
- **Comment**: None
- **Journal**: None
- **Summary**: Machine intelligence is increasingly being linked to claims about sentience, language processing, and an ability to comprehend and transform natural language into a range of stimuli. We systematically analyze the ability of DALL-E 2 to capture 8 grammatical phenomena pertaining to compositionality that are widely discussed in linguistics and pervasive in human language: binding principles and coreference, passives, word order, coordination, comparatives, negation, ellipsis, and structural ambiguity. Whereas young children routinely master these phenomena, learning systematic mappings between syntax and semantics, DALL-E 2 is unable to reliably infer meanings that are consistent with the syntax. These results challenge recent claims concerning the capacity of such systems to understand of human language. We make available the full set of test materials as a benchmark for future testing.



