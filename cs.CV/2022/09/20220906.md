# Arxiv Papers in cs.CV on 2022-09-06
### Impact analysis of recovery cases due to COVID19 using LSTM deep learning model
- **Arxiv ID**: http://arxiv.org/abs/2209.02173v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02173v1)
- **Published**: 2022-09-06 00:51:30+00:00
- **Updated**: 2022-09-06 00:51:30+00:00
- **Authors**: Md Ershadul Haque, Samiul Hoque
- **Comment**: None
- **Journal**: None
- **Summary**: The present world is badly affected by novel coronavirus (COVID-19). Using medical kits to identify the coronavirus affected persons are very slow. What happens in the next, nobody knows. The world is facing erratic problem and do not know what will happen in near future. This paper is trying to make prognosis of the coronavirus recovery cases using LSTM (Long Short Term Memory). This work exploited data of 258 regions, their latitude and longitude and the number of death of 403 days ranging from 22-01-2020 to 27-02-2021. Specifically, advanced deep learning-based algorithms known as the LSTM, play a great effect on extracting highly essential features for time series data (TSD) analysis.There are lots of methods which already use to analyze propagation prediction. The main task of this paper culminates in analyzing the spreading of Coronavirus across worldwide recovery cases using LSTM deep learning-based architectures.



### CNSNet: A Cleanness-Navigated-Shadow Network for Shadow Removal
- **Arxiv ID**: http://arxiv.org/abs/2209.02174v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02174v1)
- **Published**: 2022-09-06 01:33:38+00:00
- **Updated**: 2022-09-06 01:33:38+00:00
- **Authors**: Qianhao Yu, Naishan Zheng, Jie Huang, Feng Zhao
- **Comment**: Accepted in ECCVW 2022
- **Journal**: None
- **Summary**: The key to shadow removal is recovering the contents of the shadow regions with the guidance of the non-shadow regions. Due to the inadequate long-range modeling, the CNN-based approaches cannot thoroughly investigate the information from the non-shadow regions. To solve this problem, we propose a novel cleanness-navigated-shadow network (CNSNet), with a shadow-oriented adaptive normalization (SOAN) module and a shadow-aware aggregation with transformer (SAAT) module based on the shadow mask. Under the guidance of the shadow mask, the SOAN module formulates the statistics from the non-shadow region and adaptively applies them to the shadow region for region-wise restoration. The SAAT module utilizes the shadow mask to precisely guide the restoration of each shadowed pixel by considering the highly relevant pixels from the shadow-free regions for global pixel-wise restoration. Extensive experiments on three benchmark datasets (ISTD, ISTD+, and SRD) show that our method achieves superior de-shadowing performance.



### Transformer-CNN Cohort: Semi-supervised Semantic Segmentation by the Best of Both Students
- **Arxiv ID**: http://arxiv.org/abs/2209.02178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02178v1)
- **Published**: 2022-09-06 02:11:08+00:00
- **Updated**: 2022-09-06 02:11:08+00:00
- **Authors**: Xu Zheng, Yunhao Luo, Hao Wang, Chong Fu, Lin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The popular methods for semi-supervised semantic segmentation mostly adopt a unitary network model using convolutional neural networks (CNNs) and enforce consistency of the model predictions over small perturbations applied to the inputs or model. However, such a learning paradigm suffers from a) limited learning capability of the CNN-based model; b) limited capacity of learning the discriminative features for the unlabeled data; c) limited learning for both global and local information from the whole image. In this paper, we propose a novel Semi-supervised Learning approach, called Transformer-CNN Cohort (TCC), that consists of two students with one based on the vision transformer (ViT) and the other based on the CNN. Our method subtly incorporates the multi-level consistency regularization on the predictions and the heterogeneous feature spaces via pseudo labeling for the unlabeled data. First, as the inputs of the ViT student are image patches, the feature maps extracted encode crucial class-wise statistics. To this end, we propose class-aware feature consistency distillation (CFCD) that first leverages the outputs of each student as the pseudo labels and generates class-aware feature (CF) maps. It then transfers knowledge via the CF maps between the students. Second, as the ViT student has more uniform representations for all layers, we propose consistency-aware cross distillation to transfer knowledge between the pixel-wise predictions from the cohort. We validate the TCC framework on Cityscapes and Pascal VOC 2012 datasets, which significantly outperforms existing semi-supervised methods by a large margin.



### A Multitask Deep Learning Model for Parsing Bridge Elements and Segmenting Defect in Bridge Inspection Images
- **Arxiv ID**: http://arxiv.org/abs/2209.02190v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02190v2)
- **Published**: 2022-09-06 02:48:15+00:00
- **Updated**: 2022-10-28 15:12:44+00:00
- **Authors**: Chenyu Zhang, Muhammad Monjurul Karim, Ruwen Qin
- **Comment**: Accepted for presentation at the 2023 TRB Annual Meeting and
  publication in the Transportation Research Record: Journal of the
  Transportation Research Board (TRR)
- **Journal**: None
- **Summary**: The vast network of bridges in the United States raises a high requirement for maintenance and rehabilitation. The massive cost of manual visual inspection to assess bridge conditions is a burden to some extent. Advanced robots have been leveraged to automate inspection data collection. Automating the segmentations of multiclass elements and surface defects on the elements in the large volume of inspection image data would facilitate an efficient and effective assessment of the bridge condition. Training separate single-task networks for element parsing (i.e., semantic segmentation of multiclass elements) and defect segmentation fails to incorporate the close connection between these two tasks. Both recognizable structural elements and apparent surface defects are present in the inspection images. This paper is motivated to develop a multitask deep learning model that fully utilizes such interdependence between bridge elements and defects to boost the model's task performance and generalization. Furthermore, the study investigated the effectiveness of the proposed model designs for improving task performance, including feature decomposition, cross-talk sharing, and multi-objective loss function. A dataset with pixel-level labels of bridge elements and corrosion was developed for model training and testing. Quantitative and qualitative results from evaluating the developed multitask deep model demonstrate its advantages over the single-task-based model not only in performance (2.59% higher mIoU on bridge parsing and 1.65% on corrosion segmentation) but also in computational time and implementation capability.



### LRT: An Efficient Low-Light Restoration Transformer for Dark Light Field Images
- **Arxiv ID**: http://arxiv.org/abs/2209.02197v2
- **DOI**: 10.1109/TIP.2023.3297412
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02197v2)
- **Published**: 2022-09-06 03:23:58+00:00
- **Updated**: 2023-03-16 03:04:09+00:00
- **Authors**: Shansi Zhang, Nan Meng, Edmund Y. Lam
- **Comment**: None
- **Journal**: None
- **Summary**: Light field (LF) images containing information for multiple views have numerous applications, which can be severely affected by low-light imaging. Recent learning-based methods for low-light enhancement have some disadvantages, such as a lack of noise suppression, complex training process and poor performance in extremely low-light conditions. To tackle these deficiencies while fully utilizing the multi-view information, we propose an efficient Low-light Restoration Transformer (LRT) for LF images, with multiple heads to perform intermediate tasks within a single network, including denoising, luminance adjustment, refinement and detail enhancement, achieving progressive restoration from small scale to full scale. Moreover, we design an angular transformer block with an efficient view-token scheme to model the global angular dependencies, and a multi-scale spatial transformer block to encode the multi-scale local and global information within each view. To address the issue of insufficient training data, we formulate a synthesis pipeline by simulating the major noise sources with the estimated noise parameters of LF camera. Experimental results demonstrate that our method achieves the state-of-the-art performance on low-light LF restoration with high efficiency.



### Task-wise Sampling Convolutions for Arbitrary-Oriented Object Detection in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2209.02200v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02200v1)
- **Published**: 2022-09-06 03:42:18+00:00
- **Updated**: 2022-09-06 03:42:18+00:00
- **Authors**: Zhanchao Huang, Wei Li, Xiang-Gen Xia, Hao Wang, Ran Tao
- **Comment**: 16 pages, 14 figures, 13 tables
- **Journal**: None
- **Summary**: Arbitrary-oriented object detection (AOOD) has been widely applied to locate and classify objects with diverse orientations in remote sensing images. However, the inconsistent features for the localization and classification tasks in AOOD models may lead to ambiguity and low-quality object predictions, which constrains the detection performance. In this paper, an AOOD method called task-wise sampling convolutions (TS-Conv) is proposed. TS-Conv adaptively samples task-wise features from respective sensitive regions and maps these features together in alignment to guide a dynamic label assignment for better predictions. Specifically, sampling positions of the localization convolution in TS-Conv is supervised by the oriented bounding box (OBB) prediction associated with spatial coordinates. While sampling positions and convolutional kernel of the classification convolution are designed to be adaptively adjusted according to different orientations for improving the orientation robustness of features. Furthermore, a dynamic task-aware label assignment (DTLA) strategy is developed to select optimal candidate positions and assign labels dynamicly according to ranked task-aware scores obtained from TS-Conv. Extensive experiments on several public datasets covering multiple scenes, multimodal images, and multiple categories of objects demonstrate the effectiveness, scalability and superior performance of the proposed TS-Conv.



### High Speed Rotation Estimation with Dynamic Vision Sensors
- **Arxiv ID**: http://arxiv.org/abs/2209.02205v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2209.02205v1)
- **Published**: 2022-09-06 04:00:46+00:00
- **Updated**: 2022-09-06 04:00:46+00:00
- **Authors**: Guangrong Zhao, Yiran Shen, Ning Chen, Pengfei Hu, Lei Liu, Hongkai Wen
- **Comment**: 10 pages,13 figures
- **Journal**: None
- **Summary**: Rotational speed is one of the important metrics to be measured for calibrating the electric motors in manufacturing, monitoring engine during car repairing, faults detection on electrical appliance and etc. However, existing measurement techniques either require prohibitive hardware (e.g., high-speed camera) or are inconvenient to use in real-world application scenarios. In this paper, we propose, EV-Tach, an event-based tachometer via efficient dynamic vision sensing on mobile devices. EV-Tach is designed as a high-fidelity and convenient tachometer by introducing dynamic vision sensor as a new sensing modality to capture the high-speed rotation precisely under various real-world scenarios. By designing a series of signal processing algorithms bespoke for dynamic vision sensing on mobile devices, EV-Tach is able to extract the rotational speed accurately from the event stream produced by dynamic vision sensing on rotary targets. According to our extensive evaluations, the Relative Mean Absolute Error (RMAE) of EV-Tach is as low as 0.03% which is comparable to the state-of-the-art laser tachometer under fixed measurement mode. Moreover, EV-Tach is robust to subtle movement of user's hand, therefore, can be used as a handheld device, where the laser tachometer fails to produce reasonable results.



### USLN: A statistically guided lightweight network for underwater image enhancement via dual-statistic white balance and multi-color space stretch
- **Arxiv ID**: http://arxiv.org/abs/2209.02221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02221v1)
- **Published**: 2022-09-06 05:05:44+00:00
- **Updated**: 2022-09-06 05:05:44+00:00
- **Authors**: Ziyuan Xiao, Yina Han, Susanto Rahardja, Yuanliang Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Underwater images are inevitably affected by color distortion and reduced contrast. Traditional statistic-based methods such as white balance and histogram stretching attempted to adjust the imbalance of color channels and narrow distribution of intensities a priori thus with limited performance. Recently, deep-learning-based methods have achieved encouraging results. However, the involved complicate architecture and high computational costs may hinder their deployment in practical constrained platforms. Inspired by above works, we propose a statistically guided lightweight underwater image enhancement network (USLN). Concretely, we first develop a dual-statistic white balance module which can learn to use both average and maximum of images to compensate the color distortion for each specific pixel. Then this is followed by a multi-color space stretch module to adjust the histogram distribution in RGB, HSI, and Lab color spaces adaptively. Extensive experiments show that, with the guidance of statistics, USLN significantly reduces the required network capacity (over98%) and achieves state-of-the-art performance. The code and relevant resources are available at https://github.com/deepxzy/USLN.



### Real-Time Cattle Interaction Recognition via Triple-stream Network
- **Arxiv ID**: http://arxiv.org/abs/2209.02241v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.02241v1)
- **Published**: 2022-09-06 06:31:09+00:00
- **Updated**: 2022-09-06 06:31:09+00:00
- **Authors**: Yang Yang, Mizuka Komatsu, Kenji Oyama, Takenao Ohkawa
- **Comment**: Accepted in ICMLA2022
- **Journal**: None
- **Summary**: In stockbreeding of beef cattle, computer vision-based approaches have been widely employed to monitor cattle conditions (e.g. the physical, physiology, and health). To this end, the accurate and effective recognition of cattle action is a prerequisite. Generally, most existing models are confined to individual behavior that uses video-based methods to extract spatial-temporal features for recognizing the individual actions of each cattle. However, there is sociality among cattle and their interaction usually reflects important conditions, e.g. estrus, and also video-based method neglects the real-time capability of the model. Based on this, we tackle the challenging task of real-time recognizing interactions between cattle in a single frame in this paper. The pipeline of our method includes two main modules: Cattle Localization Network and Interaction Recognition Network. At every moment, cattle localization network outputs high-quality interaction proposals from every detected cattle and feeds them into the interaction recognition network with a triple-stream architecture. Such a triple-stream network allows us to fuse different features relevant to recognizing interactions. Specifically, the three kinds of features are a visual feature that extracts the appearance representation of interaction proposals, a geometric feature that reflects the spatial relationship between cattle, and a semantic feature that captures our prior knowledge of the relationship between the individual action and interaction of cattle. In addition, to solve the problem of insufficient quantity of labeled data, we pre-train the model based on self-supervised learning. Qualitative and quantitative evaluation evidences the performance of our framework as an effective method to recognize cattle interaction in real time.



### PTSEFormer: Progressive Temporal-Spatial Enhanced TransFormer Towards Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.02242v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.02242v1)
- **Published**: 2022-09-06 06:32:57+00:00
- **Updated**: 2022-09-06 06:32:57+00:00
- **Authors**: Han Wang, Jun Tang, Xiaodong Liu, Shanyan Guan, Rong Xie, Li Song
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have witnessed a trend of applying context frames to boost the performance of object detection as video object detection. Existing methods usually aggregate features at one stroke to enhance the feature. These methods, however, usually lack spatial information from neighboring frames and suffer from insufficient feature aggregation. To address the issues, we perform a progressive way to introduce both temporal information and spatial information for an integrated enhancement. The temporal information is introduced by the temporal feature aggregation model (TFAM), by conducting an attention mechanism between the context frames and the target frame (i.e., the frame to be detected). Meanwhile, we employ a Spatial Transition Awareness Model (STAM) to convey the location transition information between each context frame and target frame. Built upon a transformer-based detector DETR, our PTSEFormer also follows an end-to-end fashion to avoid heavy post-processing procedures while achieving 88.1% mAP on the ImageNet VID dataset. Codes are available at https://github.com/Hon-Wong/PTSEFormer.



### An evaluation of U-Net in Renal Structure Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.02247v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.02247v1)
- **Published**: 2022-09-06 06:53:41+00:00
- **Updated**: 2022-09-06 06:53:41+00:00
- **Authors**: Haoyu Wang, Ziyan Huang, Jin Ye, Can Tu, Yuncheng Yang, Shiyi Du, Zhongying Deng, Chenglong Ma, Jingqi Niu, Junjun He
- **Comment**: None
- **Journal**: None
- **Summary**: Renal structure segmentation from computed tomography angiography~(CTA) is essential for many computer-assisted renal cancer treatment applications. Kidney PArsing~(KiPA 2022) Challenge aims to build a fine-grained multi-structure dataset and improve the segmentation of multiple renal structures. Recently, U-Net has dominated the medical image segmentation. In the KiPA challenge, we evaluated several U-Net variants and selected the best models for the final submission.



### Spatio-Temporal Action Detection Under Large Motion
- **Arxiv ID**: http://arxiv.org/abs/2209.02250v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02250v2)
- **Published**: 2022-09-06 06:55:26+00:00
- **Updated**: 2022-10-25 08:36:29+00:00
- **Authors**: Gurkirt Singh, Vasileios Choutas, Suman Saha, Fisher Yu, Luc Van Gool
- **Comment**: 10 pages, 5 figures, 5 tables
- **Journal**: None
- **Summary**: Current methods for spatiotemporal action tube detection often extend a bounding box proposal at a given keyframe into a 3D temporal cuboid and pool features from nearby frames. However, such pooling fails to accumulate meaningful spatiotemporal features if the position or shape of the actor shows large 2D motion and variability through the frames, due to large camera motion, large actor shape deformation, fast actor action and so on. In this work, we aim to study the performance of cuboid-aware feature aggregation in action detection under large action. Further, we propose to enhance actor feature representation under large motion by tracking actors and performing temporal feature aggregation along the respective tracks. We define the actor motion with intersection-over-union (IoU) between the boxes of action tubes/tracks at various fixed time scales. The action having a large motion would result in lower IoU over time, and slower actions would maintain higher IoU. We find that track-aware feature aggregation consistently achieves a large improvement in action detection performance, especially for actions under large motion compared to the cuboid-aware baseline. As a result, we also report state-of-the-art on the large-scale MultiSports dataset. The Code is available at https://github.com/gurkirt/ActionTrackDetectron.



### An Indoor Localization Dataset and Data Collection Framework with High Precision Position Annotation
- **Arxiv ID**: http://arxiv.org/abs/2209.02270v1
- **DOI**: 10.1016/j.pmcj.2022.101554
- **Categories**: **cs.LG**, cs.CV, cs.NI, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2209.02270v1)
- **Published**: 2022-09-06 07:41:11+00:00
- **Updated**: 2022-09-06 07:41:11+00:00
- **Authors**: F. Serhan Daniş, A. Teoman Naskali, A. Taylan Cemgil, Cem Ersoy
- **Comment**: 30 pages
- **Journal**: F. Serhan Dani\c{s}, A. Teoman Naskali, A. Taylan Cemgil, Cem
  Ersoy, "An indoor localization dataset and data collection framework with
  high precision position annotation", Pervasive and Mobile Computing, Volume
  81, 101554, 2022
- **Summary**: We introduce a novel technique and an associated high resolution dataset that aims to precisely evaluate wireless signal based indoor positioning algorithms. The technique implements an augmented reality (AR) based positioning system that is used to annotate the wireless signal parameter data samples with high precision position data. We track the position of a practical and low cost navigable setup of cameras and a Bluetooth Low Energy (BLE) beacon in an area decorated with AR markers. We maximize the performance of the AR-based localization by using a redundant number of markers. Video streams captured by the cameras are subjected to a series of marker recognition, subset selection and filtering operations to yield highly precise pose estimations. Our results show that we can reduce the positional error of the AR localization system to a rate under 0.05 meters. The position data are then used to annotate the BLE data that are captured simultaneously by the sensors stationed in the environment, hence, constructing a wireless signal data set with the ground truth, which allows a wireless signal based localization system to be evaluated accurately.



### Automated Defect Recognition of Castings defects using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.02279v1
- **DOI**: 10.1007/s10921-021-00842-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02279v1)
- **Published**: 2022-09-06 08:10:48+00:00
- **Updated**: 2022-09-06 08:10:48+00:00
- **Authors**: Alberto García-Pérez, María José Gómez-Silva, Arturo de la Escalera
- **Comment**: This preprint has not undergone peer review (when applicable) or any
  post-submission improvements or corrections. The Version of Record of this
  article is published in Journal of Nondestructive Evaluation, and is
  available online at https://doi.org/10.1007/s10921-021-00842-1
- **Journal**: Journal of Nondestructive Evaluation (2022) 41:11
- **Summary**: Industrial X-ray analysis is common in aerospace, automotive or nuclear industries where structural integrity of some parts needs to be guaranteed. However, the interpretation of radiographic images is sometimes difficult and may lead to two experts disagree on defect classification. The Automated Defect Recognition (ADR) system presented herein will reduce the analysis time and will also help reducing the subjective interpretation of the defects while increasing the reliability of the human inspector. Our Convolutional Neural Network (CNN) model achieves 94.2\% accuracy (mAP@IoU=50\%), which is considered as similar to expected human performance, when applied to an automotive aluminium castings dataset (GDXray), exceeding current state of the art for this dataset. On an industrial environment, its inference time is less than 400 ms per DICOM image, so it can be installed on production facilities with no impact on delivery time. In addition, an ablation study of the main hyper-parameters to optimise model accuracy from the initial baseline result of 75\% mAP up to 94.2\% mAP, was also conducted.



### Progressive Glass Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.02280v1
- **DOI**: 10.1109/TIP.2022.3162709
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02280v1)
- **Published**: 2022-09-06 08:11:17+00:00
- **Updated**: 2022-09-06 08:11:17+00:00
- **Authors**: Letian Yu, Haiyang Mei, Wen Dong, Ziqi Wei, Li Zhu, Yuxin Wang, Xin Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Glass is very common in the real world. Influenced by the uncertainty about the glass region and the varying complex scenes behind the glass, the existence of glass poses severe challenges to many computer vision tasks, making glass segmentation as an important computer vision task. Glass does not have its own visual appearances but only transmit/reflect the appearances of its surroundings, making it fundamentally different from other common objects. To address such a challenging task, existing methods typically explore and combine useful cues from different levels of features in the deep network. As there exists a characteristic gap between level-different features, i.e., deep layer features embed more high-level semantics and are better at locating the target objects while shallow layer features have larger spatial sizes and keep richer and more detailed low-level information, fusing these features naively thus would lead to a sub-optimal solution. In this paper, we approach the effective features fusion towards accurate glass segmentation in two steps. First, we attempt to bridge the characteristic gap between different levels of features by developing a Discriminability Enhancement (DE) module which enables level-specific features to be a more discriminative representation, alleviating the features incompatibility for fusion. Second, we design a Focus-and-Exploration Based Fusion (FEBF) module to richly excavate useful information in the fusion process by highlighting the common and exploring the difference between level-different features.



### High Dynamic Range Image Quality Assessment Based on Frequency Disparity
- **Arxiv ID**: http://arxiv.org/abs/2209.02285v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02285v1)
- **Published**: 2022-09-06 08:22:13+00:00
- **Updated**: 2022-09-06 08:22:13+00:00
- **Authors**: Yue Liu, Zhangkai Ni, Shiqi Wang, Hanli Wang, Sam Kwong
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a novel and effective image quality assessment (IQA) algorithm based on frequency disparity for high dynamic range (HDR) images is proposed, termed as local-global frequency feature-based model (LGFM). Motivated by the assumption that the human visual system is highly adapted for extracting structural information and partial frequencies when perceiving the visual scene, the Gabor and the Butterworth filters are applied to the luminance of the HDR image to extract local and global frequency features, respectively. The similarity measurement and feature pooling are sequentially performed on the frequency features to obtain the predicted quality score. The experiments evaluated on four widely used benchmarks demonstrate that the proposed LGFM can provide a higher consistency with the subjective perception compared with the state-of-the-art HDR IQA methods. Our code is available at: \url{https://github.com/eezkni/LGFM}.



### SIND: A Drone Dataset at Signalized Intersection in China
- **Arxiv ID**: http://arxiv.org/abs/2209.02297v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GL
- **Links**: [PDF](http://arxiv.org/pdf/2209.02297v1)
- **Published**: 2022-09-06 08:49:44+00:00
- **Updated**: 2022-09-06 08:49:44+00:00
- **Authors**: Yanchao Xu, Wenbo Shao, Jun Li, Kai Yang, Weida Wang, Hua Huang, Chen Lv, Hong Wang
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Intersection is one of the most challenging scenarios for autonomous driving tasks. Due to the complexity and stochasticity, essential applications (e.g., behavior modeling, motion prediction, safety validation, etc.) at intersections rely heavily on data-driven techniques. Thus, there is an intense demand for trajectory datasets of traffic participants (TPs) in intersections. Currently, most intersections in urban areas are equipped with traffic lights. However, there is not yet a large-scale, high-quality, publicly available trajectory dataset for signalized intersections. Therefore, in this paper, a typical two-phase signalized intersection is selected in Tianjin, China. Besides, a pipeline is designed to construct a Signalized INtersection Dataset (SIND), which contains 7 hours of recording including over 13,000 TPs with 7 types. Then, the behaviors of traffic light violations in SIND are recorded. Furthermore, the SIND is also compared with other similar works. The features of the SIND can be summarized as follows: 1) SIND provides more comprehensive information, including traffic light states, motion parameters, High Definition (HD) map, etc. 2) The category of TPs is diverse and characteristic, where the proportion of vulnerable road users (VRUs) is up to 62.6% 3) Multiple traffic light violations of non-motor vehicles are shown. We believe that SIND would be an effective supplement to existing datasets and can promote related research on autonomous driving.The dataset is available online via: https://github.com/SOTIF-AVLab/SinD



### Multimodal contrastive learning for remote sensing tasks
- **Arxiv ID**: http://arxiv.org/abs/2209.02329v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.02329v1)
- **Published**: 2022-09-06 09:31:45+00:00
- **Updated**: 2022-09-06 09:31:45+00:00
- **Authors**: Umangi Jain, Alex Wilson, Varun Gulshan
- **Comment**: None
- **Journal**: None
- **Summary**: Self-supervised methods have shown tremendous success in the field of computer vision, including applications in remote sensing and medical imaging. Most popular contrastive-loss based methods like SimCLR, MoCo, MoCo-v2 use multiple views of the same image by applying contrived augmentations on the image to create positive pairs and contrast them with negative examples. Although these techniques work well, most of these techniques have been tuned on ImageNet (and similar computer vision datasets). While there have been some attempts to capture a richer set of deformations in the positive samples, in this work, we explore a promising alternative to generating positive examples for remote sensing data within the contrastive learning framework. Images captured from different sensors at the same location and nearby timestamps can be thought of as strongly augmented instances of the same scene, thus removing the need to explore and tune a set of hand crafted strong augmentations. In this paper, we propose a simple dual-encoder framework, which is pre-trained on a large unlabeled dataset (~1M) of Sentinel-1 and Sentinel-2 image pairs. We test the embeddings on two remote sensing downstream tasks: flood segmentation and land cover mapping, and empirically show that embeddings learnt from this technique outperform the conventional technique of collecting positive examples via aggressive data augmentations.



### MACAB: Model-Agnostic Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World
- **Arxiv ID**: http://arxiv.org/abs/2209.02339v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2209.02339v1)
- **Published**: 2022-09-06 09:56:33+00:00
- **Updated**: 2022-09-06 09:56:33+00:00
- **Authors**: Hua Ma, Yinshan Li, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Anmin Fu, Said F. Al-Sarawi, Nepal Surya, Derek Abbott
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is the foundation of various critical computer-vision tasks such as segmentation, object tracking, and event detection. To train an object detector with satisfactory accuracy, a large amount of data is required. However, due to the intensive workforce involved with annotating large datasets, such a data curation task is often outsourced to a third party or relied on volunteers. This work reveals severe vulnerabilities of such data curation pipeline. We propose MACAB that crafts clean-annotated images to stealthily implant the backdoor into the object detectors trained on them even when the data curator can manually audit the images. We observe that the backdoor effect of both misclassification and the cloaking are robustly achieved in the wild when the backdoor is activated with inconspicuously natural physical triggers. Backdooring non-classification object detection with clean-annotation is challenging compared to backdooring existing image classification tasks with clean-label, owing to the complexity of having multiple objects within each frame, including victim and non-victim objects. The efficacy of the MACAB is ensured by constructively i abusing the image-scaling function used by the deep learning framework, ii incorporating the proposed adversarial clean image replica technique, and iii combining poison data selection criteria given constrained attacking budget. Extensive experiments demonstrate that MACAB exhibits more than 90% attack success rate under various real-world scenes. This includes both cloaking and misclassification backdoor effect even restricted with a small attack budget. The poisoned samples cannot be effectively identified by state-of-the-art detection techniques.The comprehensive video demo is at https://youtu.be/MA7L_LpXkp4, which is based on a poison rate of 0.14% for YOLOv4 cloaking backdoor and Faster R-CNN misclassification backdoor.



### Language-aware Domain Generalization Network for Cross-Scene Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.02700v1
- **DOI**: 10.1109/TGRS.2022.3233885
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02700v1)
- **Published**: 2022-09-06 10:06:10+00:00
- **Updated**: 2022-09-06 10:06:10+00:00
- **Authors**: Yuxiang Zhang, Mengmeng Zhang, Wei Li, Shuai Wang, Ran Tao
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2209.01634
- **Journal**: None
- **Summary**: Text information including extensive prior knowledge about land cover classes has been ignored in hyperspectral image classification (HSI) tasks. It is necessary to explore the effectiveness of linguistic mode in assisting HSI classification. In addition, the large-scale pre-training image-text foundation models have demonstrated great performance in a variety of downstream applications, including zero-shot transfer. However, most domain generalization methods have never addressed mining linguistic modal knowledge to improve the generalization performance of model. To compensate for the inadequacies listed above, a Language-aware Domain Generalization Network (LDGnet) is proposed to learn cross-domain invariant representation from cross-domain shared prior knowledge. The proposed method only trains on the source domain (SD) and then transfers the model to the target domain (TD). The dual-stream architecture including image encoder and text encoder is used to extract visual and linguistic features, in which coarse-grained and fine-grained text representations are designed to extract two levels of linguistic features. Furthermore, linguistic features are used as cross-domain shared semantic space, and visual-linguistic alignment is completed by supervised contrastive learning in semantic space. Extensive experiments on three datasets demonstrate the superiority of the proposed method when compared with state-of-the-art techniques.



### Finger Multimodal Feature Fusion and Recognition Based on Channel Spatial Attention
- **Arxiv ID**: http://arxiv.org/abs/2209.02368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02368v1)
- **Published**: 2022-09-06 10:48:30+00:00
- **Updated**: 2022-09-06 10:48:30+00:00
- **Authors**: Jian Guo, Jiaxiang Tu, Hengyi Ren, Chong Han, Lijuan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the instability and limitations of unimodal biometric systems, multimodal systems have attracted more and more attention from researchers. However, how to exploit the independent and complementary information between different modalities remains a key and challenging problem. In this paper, we propose a multimodal biometric fusion recognition algorithm based on fingerprints and finger veins (Fingerprint Finger Veins-Channel Spatial Attention Fusion Module, FPV-CSAFM). Specifically, for each pair of fingerprint and finger vein images, we first propose a simple and effective Convolutional Neural Network (CNN) to extract features. Then, we build a multimodal feature fusion module (Channel Spatial Attention Fusion Module, CSAFM) to fully fuse the complementary information between fingerprints and finger veins. Different from existing fusion strategies, our fusion method can dynamically adjust the fusion weights according to the importance of different modalities in channel and spatial dimensions, so as to better combine the information between different modalities and improve the overall recognition performance. To evaluate the performance of our method, we conduct a series of experiments on multiple public datasets. Experimental results show that the proposed FPV-CSAFM achieves excellent recognition performance on three multimodal datasets based on fingerprints and finger veins.



### Improving Robustness to Out-of-Distribution Data by Frequency-based Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.02369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02369v1)
- **Published**: 2022-09-06 10:48:37+00:00
- **Updated**: 2022-09-06 10:48:37+00:00
- **Authors**: Koki Mukai, Soichiro Kumano, Toshihiko Yamasaki
- **Comment**: ICIP 2022
- **Journal**: None
- **Summary**: Although Convolutional Neural Networks (CNNs) have high accuracy in image recognition, they are vulnerable to adversarial examples and out-of-distribution data, and the difference from human recognition has been pointed out. In order to improve the robustness against out-of-distribution data, we present a frequency-based data augmentation technique that replaces the frequency components with other images of the same class. When the training data are CIFAR10 and the out-of-distribution data are SVHN, the Area Under Receiver Operating Characteristic (AUROC) curve of the model trained with the proposed method increases from 89.22\% to 98.15\%, and further increased to 98.59\% when combined with another data augmentation method. Furthermore, we experimentally demonstrate that the robust model for out-of-distribution data uses a lot of high-frequency components of the image.



### Continual Learning, Fast and Slow
- **Arxiv ID**: http://arxiv.org/abs/2209.02370v3
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.02370v3)
- **Published**: 2022-09-06 10:48:45+00:00
- **Updated**: 2023-07-09 10:02:41+00:00
- **Authors**: Quang Pham, Chenghao Liu, Steven C. H. Hoi
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2110.00175
- **Journal**: None
- **Summary**: According to the Complementary Learning Systems (CLS) theory~\cite{mcclelland1995there} in neuroscience, humans do effective \emph{continual learning} through two complementary systems: a fast learning system centered on the hippocampus for rapid learning of the specifics, individual experiences; and a slow learning system located in the neocortex for the gradual acquisition of structured knowledge about the environment. Motivated by this theory, we propose \emph{DualNets} (for Dual Networks), a general continual learning framework comprising a fast learning system for supervised learning of pattern-separated representation from specific tasks and a slow learning system for representation learning of task-agnostic general representation via Self-Supervised Learning (SSL). DualNets can seamlessly incorporate both representation types into a holistic framework to facilitate better continual learning in deep neural networks. Via extensive experiments, we demonstrate the promising results of DualNets on a wide range of continual learning protocols, ranging from the standard offline, task-aware setting to the challenging online, task-free scenario. Notably, on the CTrL~\cite{veniat2020efficient} benchmark that has unrelated tasks with vastly different visual images, DualNets can achieve competitive performance with existing state-of-the-art dynamic architecture strategies~\cite{ostapenko2021continual}. Furthermore, we conduct comprehensive ablation studies to validate DualNets efficacy, robustness, and scalability. Code will be made available at \url{https://github.com/phquang/DualNet}.



### Understanding and Reducing Crater Counting Errors in Citizen Science Data and the Need for Standardisation
- **Arxiv ID**: http://arxiv.org/abs/2209.02375v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2209.02375v1)
- **Published**: 2022-09-06 10:54:08+00:00
- **Updated**: 2022-09-06 10:54:08+00:00
- **Authors**: P. D. Tar, N. A. Thacker
- **Comment**: None
- **Journal**: None
- **Summary**: Citizen science has become a popular tool for preliminary data processing tasks, such as identifying and counting Lunar impact craters in modern high-resolution imagery. However, use of such data requires that citizen science products are understandable and reliable. Contamination and missing data can reduce the usefulness of datasets so it is important that such effects are quantified. This paper presents a method, based upon a newly developed quantitative pattern recognition system (Linear Poisson Models) for estimating levels of contamination within MoonZoo citizen science crater data. Evidence will show that it is possible to remove the effects of contamination, with reference to some agreed upon ground truth, resulting in estimated crater counts which are highly repeatable. However, it will also be shown that correcting for missing data is currently more difficult to achieve. The techniques are tested on MoonZoo citizen science crater annotations from the Apollo 17 site and also undergraduate and expert results from the same region.



### A Scene-Text Synthesis Engine Achieved Through Learning from Decomposed Real-World Data
- **Arxiv ID**: http://arxiv.org/abs/2209.02397v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02397v1)
- **Published**: 2022-09-06 11:15:58+00:00
- **Updated**: 2022-09-06 11:15:58+00:00
- **Authors**: Zhengmi Tang, Tomo Miyazaki, Shinichiro Omachi
- **Comment**: None
- **Journal**: None
- **Summary**: Scene-text image synthesis techniques aimed at naturally composing text instances on background scene images are very appealing for training deep neural networks because they can provide accurate and comprehensive annotation information. Prior studies have explored generating synthetic text images on two-dimensional and three-dimensional surfaces based on rules derived from real-world observations. Some of these studies have proposed generating scene-text images from learning; however, owing to the absence of a suitable training dataset, unsupervised frameworks have been explored to learn from existing real-world data, which may not result in a robust performance. To ease this dilemma and facilitate research on learning-based scene text synthesis, we propose DecompST, a real-world dataset prepared using public benchmarks, with three types of annotations: quadrilateral-level BBoxes, stroke-level text masks, and text-erased images. Using the DecompST dataset, we propose an image synthesis engine that includes a text location proposal network (TLPNet) and a text appearance adaptation network (TAANet). TLPNet first predicts the suitable regions for text embedding. TAANet then adaptively changes the geometry and color of the text instance according to the context of the background. Our comprehensive experiments verified the effectiveness of the proposed method for generating pretraining data for scene text detectors.



### ViTKD: Practical Guidelines for ViT feature knowledge distillation
- **Arxiv ID**: http://arxiv.org/abs/2209.02432v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02432v1)
- **Published**: 2022-09-06 11:52:46+00:00
- **Updated**: 2022-09-06 11:52:46+00:00
- **Authors**: Zhendong Yang, Zhe Li, Ailing Zeng, Zexian Li, Chun Yuan, Yu Li
- **Comment**: 5 figures; 9 tables
- **Journal**: None
- **Summary**: Knowledge Distillation (KD) for Convolutional Neural Network (CNN) is extensively studied as a way to boost the performance of a small model. Recently, Vision Transformer (ViT) has achieved great success on many computer vision tasks and KD for ViT is also desired. However, besides the output logit-based KD, other feature-based KD methods for CNNs cannot be directly applied to ViT due to the huge structure gap. In this paper, we explore the way of feature-based distillation for ViT. Based on the nature of feature maps in ViT, we design a series of controlled experiments and derive three practical guidelines for ViT's feature distillation. Some of our findings are even opposite to the practices in the CNN era. Based on the three guidelines, we propose our feature-based method ViTKD which brings consistent and considerable improvement to the student. On ImageNet-1k, we boost DeiT-Tiny from 74.42% to 76.06%, DeiT-Small from 80.55% to 81.95%, and DeiT-Base from 81.76% to 83.46%. Moreover, ViTKD and the logit-based KD method are complementary and can be applied together directly. This combination can further improve the performance of the student. Specifically, the student DeiT-Tiny, Small, and Base achieve 77.78%, 83.59%, and 85.41%, respectively. The code is available at https://github.com/yzd-v/cls_KD.



### Threat Detection In Self-Driving Vehicles Using Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2209.02438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02438v1)
- **Published**: 2022-09-06 12:01:07+00:00
- **Updated**: 2022-09-06 12:01:07+00:00
- **Authors**: Umang Goenka, Aaryan Jagetia, Param Patil, Akshay Singh, Taresh Sharma, Poonam Saini
- **Comment**: Presented in 3rd International Conference on Machine Learning, Image
  Processing, Network Security and Data Sciences MIND-2021
- **Journal**: None
- **Summary**: On-road obstacle detection is an important field of research that falls in the scope of intelligent transportation infrastructure systems. The use of vision-based approaches results in an accurate and cost-effective solution to such systems. In this research paper, we propose a threat detection mechanism for autonomous self-driving cars using dashcam videos to ensure the presence of any unwanted obstacle on the road that falls within its visual range. This information can assist the vehicle's program to en route safely. There are four major components, namely, YOLO to identify the objects, advanced lane detection algorithm, multi regression model to measure the distance of the object from the camera, the two-second rule for measuring the safety, and limiting speed. In addition, we have used the Car Crash Dataset(CCD) for calculating the accuracy of the model. The YOLO algorithm gives an accuracy of around 93%. The final accuracy of our proposed Threat Detection Model (TDM) is 82.65%.



### The HoloLens in Medicine: A systematic Review and Taxonomy
- **Arxiv ID**: http://arxiv.org/abs/2209.03245v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.CY, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.03245v1)
- **Published**: 2022-09-06 13:05:29+00:00
- **Updated**: 2022-09-06 13:05:29+00:00
- **Authors**: Christina Gsaxner, Jianning Li, Antonio Pepe, Yuan Jin, Jens Kleesiek, Dieter Schmalstieg, Jan Egger
- **Comment**: 35 pages, 11 figures
- **Journal**: None
- **Summary**: The HoloLens (Microsoft Corp., Redmond, WA), a head-worn, optically see-through augmented reality display, is the main player in the recent boost in medical augmented reality research. In medical settings, the HoloLens enables the physician to obtain immediate insight into patient information, directly overlaid with their view of the clinical scenario, the medical student to gain a better understanding of complex anatomies or procedures, and even the patient to execute therapeutic tasks with improved, immersive guidance. In this systematic review, we provide a comprehensive overview of the usage of the first-generation HoloLens within the medical domain, from its release in March 2016, until the year of 2021, were attention is shifting towards it's successor, the HoloLens 2. We identified 171 relevant publications through a systematic search of the PubMed and Scopus databases. We analyze these publications in regard to their intended use case, technical methodology for registration and tracking, data sources, visualization as well as validation and evaluation. We find that, although the feasibility of using the HoloLens in various medical scenarios has been shown, increased efforts in the areas of precision, reliability, usability, workflow and perception are necessary to establish AR in clinical practice.



### Multi-task Swin Transformer for Motion Artifacts Classification and Cardiac Magnetic Resonance Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.02470v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02470v1)
- **Published**: 2022-09-06 13:14:44+00:00
- **Updated**: 2022-09-06 13:14:44+00:00
- **Authors**: Michal K. Grzeszczyk, Szymon Płotka, Arkadiusz Sitek
- **Comment**: Accepted for Statistical Atlases and Computational Modeling of the
  Heart (STACOM) workshop, The Extreme Cardiac MRI Analysis Challenge under
  Respiratory Motion (CMRxMotion) challenge
- **Journal**: None
- **Summary**: Cardiac Magnetic Resonance Imaging is commonly used for the assessment of the cardiac anatomy and function. The delineations of left and right ventricle blood pools and left ventricular myocardium are important for the diagnosis of cardiac diseases. Unfortunately, the movement of a patient during the CMR acquisition procedure may result in motion artifacts appearing in the final image. Such artifacts decrease the diagnostic quality of CMR images and force redoing of the procedure. In this paper, we present a Multi-task Swin UNEt TRansformer network for simultaneous solving of two tasks in the CMRxMotion challenge: CMR segmentation and motion artifacts classification. We utilize both segmentation and classification as a multi-task learning approach which allows us to determine the diagnostic quality of CMR and generate masks at the same time. CMR images are classified into three diagnostic quality classes, whereas, all samples with non-severe motion artifacts are being segmented. Ensemble of five networks trained using 5-Fold Cross-validation achieves segmentation performance of DICE coefficient of 0.871 and classification accuracy of 0.595.



### Segment Augmentation and Differentiable Ranking for Logo Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2209.02482v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02482v2)
- **Published**: 2022-09-06 13:30:21+00:00
- **Updated**: 2022-09-13 12:52:35+00:00
- **Authors**: Feyza Yavuz, Sinan Kalkan
- **Comment**: ICPR2022, Poster Presentation
- **Journal**: None
- **Summary**: Logo retrieval is a challenging problem since the definition of similarity is more subjective compared to image retrieval tasks and the set of known similarities is very scarce. To tackle this challenge, in this paper, we propose a simple but effective segment-based augmentation strategy to introduce artificially similar logos for training deep networks for logo retrieval. In this novel augmentation strategy, we first find segments in a logo and apply transformations such as rotation, scaling, and color change, on the segments, unlike the conventional image-level augmentation strategies. Moreover, we evaluate whether the recently introduced ranking-based loss function, Smooth-AP, is a better approach for learning similarity for logo retrieval. On the large scale METU Trademark Dataset, we show that (i) our segment-based augmentation strategy improves retrieval performance compared to the baseline model or image-level augmentation strategies, and (ii) Smooth-AP indeed performs better than conventional losses for logo retrieval.



### Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors
- **Arxiv ID**: http://arxiv.org/abs/2209.02485v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2209.02485v1)
- **Published**: 2022-09-06 13:32:55+00:00
- **Updated**: 2022-09-06 13:32:55+00:00
- **Authors**: Xi Wang, Gen Li, Yen-Ling Kuo, Muhammed Kocabas, Emre Aksan, Otmar Hilliges
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for inferring diverse 3D models of human-object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities arising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements of humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on a large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories.



### Surya Namaskar: real-time advanced yoga pose recognition and correction for smart healthcare
- **Arxiv ID**: http://arxiv.org/abs/2209.02492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02492v1)
- **Published**: 2022-09-06 13:37:25+00:00
- **Updated**: 2022-09-06 13:37:25+00:00
- **Authors**: Abhishek Sharma, Pranjal Sharma, Darshan Pincha, Prateek Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, yoga has gained worldwide attention because of increasing levels of stress in the modern way of life, and there are many ways or resources to learn yoga. The word yoga means a deep connection between the mind and body. Today there is substantial Medical and scientific evidence to show that the very fundamentals of the activity of our brain, our chemistry even our genetic content can be changed by practicing different systems of yoga. Suryanamaskar, also known as salute to the sun, is a yoga practice that combines eight different forms and 12 asanas(4 asana get repeated) devoted to the Hindu Sun God, Surya. Suryanamaskar offers a number of health benefits such as strengthening muscles and helping to control blood sugar levels. Here the Mediapipe Library is used to analyze Surya namaskar situations. Standing is detected in real time with advanced software, as one performs Surya namaskar in front of the camera. The class divider identifies the form as one of the following: Pranamasana, Hasta Padasana, Hasta Uttanasana, Ashwa - Sanchalan asana, Ashtanga Namaskar, Dandasana, or Bhujangasana and Svanasana. Deep learning-based techniques(CNN) are used to develop this model with model accuracy of 98.68 percent and an accuracy score of 0.75 to detect correct yoga (Surya Namaskar ) posture. With this method, the users can practice the desired pose and can check if the pose that the person is doing is correct or not. It will help in doing all the different poses of surya namaskar correctly and increase the efficiency of the yoga practitioner. This paper describes the whole framework which is to be implemented in the model.



### MMV_Im2Im: An Open Source Microscopy Machine Vision Toolbox for Image-to-Image Transformation
- **Arxiv ID**: http://arxiv.org/abs/2209.02498v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02498v2)
- **Published**: 2022-09-06 13:42:17+00:00
- **Updated**: 2023-03-17 07:28:18+00:00
- **Authors**: Justin Sonneck, Jianxu Chen
- **Comment**: 26 pages, 10 figures
- **Journal**: None
- **Summary**: Over the past decade, deep learning (DL) research in computer vision has been growing rapidly, with many advances in DL-based image analysis methods for biomedical problems. In this work, we introduce MMV_Im2Im, a new open-source python package for image-to-image transformation in bioimaging applications. MMV_Im2Im is designed with a generic image-to-image transformation framework that can be used for a wide range of tasks, including semantic segmentation, instance segmentation, image restoration, and image generation, etc.. Our implementation takes advantage of state-of-the-art machine learning engineering techniques, allowing researchers to focus on their research without worrying about engineering details. We demonstrate the effectiveness of MMV_Im2Im on more than ten different biomedical problems, showcasing its general potentials and applicabilities. For computational biomedical researchers, MMV_Im2Im provides a starting point for developing new biomedical image analysis or machine learning algorithms, where they can either reuse the code in this package or fork and extend this package to facilitate the development of new methods. Experimental biomedical researchers can benefit from this work by gaining a comprehensive view of the image-to-image transformation concept through diversified examples and use cases. We hope this work can give the community inspirations on how DL-based image-to-image transformation can be integrated into the assay development process, enabling new biomedical studies that cannot be done only with traditional experimental assays. To help researchers get started, we have provided source code, documentation, and tutorials for MMV_Im2Im at https://github.com/MMV-Lab/mmv_im2im under MIT license.



### Learned Distributed Image Compression with Multi-Scale Patch Matching in Feature Domain
- **Arxiv ID**: http://arxiv.org/abs/2209.02514v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2209.02514v2)
- **Published**: 2022-09-06 14:06:46+00:00
- **Updated**: 2022-11-28 04:55:49+00:00
- **Authors**: Yujun Huang, Bin Chen, Shiyu Qin, Jiawei Li, Yaowei Wang, Tao Dai, Shu-Tao Xia
- **Comment**: This work is accepted by the AAAI 2023
- **Journal**: None
- **Summary**: Beyond achieving higher compression efficiency over classical image compression codecs, deep image compression is expected to be improved with additional side information, e.g., another image from a different perspective of the same scene. To better utilize the side information under the distributed compression scenario, the existing method (Ayzik and Avidan 2020) only implements patch matching at the image domain to solve the parallax problem caused by the difference in viewing points. However, the patch matching at the image domain is not robust to the variance of scale, shape, and illumination caused by the different viewing angles, and can not make full use of the rich texture information of the side information image. To resolve this issue, we propose Multi-Scale Feature Domain Patch Matching (MSFDPM) to fully utilizes side information at the decoder of the distributed image compression model. Specifically, MSFDPM consists of a side information feature extractor, a multi-scale feature domain patch matching module, and a multi-scale feature fusion network. Furthermore, we reuse inter-patch correlation from the shallow layer to accelerate the patch matching of the deep layer. Finally, we nd that our patch matching in a multi-scale feature domain further improves compression rate by about 20% compared with the patch matching method at image domain (Ayzik and Avidan 2020).



### Sequential Cross Attention Based Multi-task Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.02518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02518v1)
- **Published**: 2022-09-06 14:17:33+00:00
- **Updated**: 2022-09-06 14:17:33+00:00
- **Authors**: Sunkyung Kim, Hyesong Choi, Dongbo Min
- **Comment**: ICIP 2022
- **Journal**: None
- **Summary**: In multi-task learning (MTL) for visual scene understanding, it is crucial to transfer useful information between multiple tasks with minimal interferences. In this paper, we propose a novel architecture that effectively transfers informative features by applying the attention mechanism to the multi-scale features of the tasks. Since applying the attention module directly to all possible features in terms of scale and task requires a high complexity, we propose to apply the attention module sequentially for the task and scale. The cross-task attention module (CTAM) is first applied to facilitate the exchange of relevant information between the multiple task features of the same scale. The cross-scale attention module (CSAM) then aggregates useful information from feature maps at different resolutions in the same task. Also, we attempt to capture long range dependencies through the self-attention module in the feature extraction network. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the NYUD-v2 and PASCAL-Context dataset.



### UPAR: Unified Pedestrian Attribute Recognition and Person Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2209.02522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02522v1)
- **Published**: 2022-09-06 14:20:56+00:00
- **Updated**: 2022-09-06 14:20:56+00:00
- **Authors**: Andreas Specker, Mickael Cormier, Jürgen Beyerer
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing soft-biometric pedestrian attributes is essential in video surveillance and fashion retrieval. Recent works show promising results on single datasets. Nevertheless, the generalization ability of these methods under different attribute distributions, viewpoints, varying illumination, and low resolutions remains rarely understood due to strong biases and varying attributes in current datasets. To close this gap and support a systematic investigation, we present UPAR, the Unified Person Attribute Recognition Dataset. It is based on four well-known person attribute recognition datasets: PA100K, PETA, RAPv2, and Market1501. We unify those datasets by providing 3,3M additional annotations to harmonize 40 important binary attributes over 12 attribute categories across the datasets. We thus enable research on generalizable pedestrian attribute recognition as well as attribute-based person retrieval for the first time. Due to the vast variance of the image distribution, pedestrian pose, scale, and occlusion, existing approaches are greatly challenged both in terms of accuracy and efficiency. Furthermore, we develop a strong baseline for PAR and attribute-based person retrieval based on a thorough analysis of regularization methods. Our models achieve state-of-the-art performance in cross-domain and specialization settings on PA100k, PETA, RAPv2, Market1501-Attributes, and UPAR. We believe UPAR and our strong baseline will contribute to the artificial intelligence community and promote research on large-scale, generalizable attribute recognition systems.



### Semantic Image Synthesis with Semantically Coupled VQ-Model
- **Arxiv ID**: http://arxiv.org/abs/2209.02536v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.02536v1)
- **Published**: 2022-09-06 14:37:01+00:00
- **Updated**: 2022-09-06 14:37:01+00:00
- **Authors**: Stephan Alaniz, Thomas Hummel, Zeynep Akata
- **Comment**: ICLR 2022 DGM4HSD
- **Journal**: None
- **Summary**: Semantic image synthesis enables control over unconditional image generation by allowing guidance on what is being generated. We conditionally synthesize the latent space from a vector quantized model (VQ-model) pre-trained to autoencode images. Instead of training an autoregressive Transformer on separately learned conditioning latents and image latents, we find that jointly learning the conditioning and image latents significantly improves the modeling capabilities of the Transformer model. While our jointly trained VQ-model achieves a similar reconstruction performance to a vanilla VQ-model for both semantic and image latents, tying the two modalities at the autoencoding stage proves to be an important ingredient to improve autoregressive modeling performance. We show that our model improves semantic image synthesis using autoregressive models on popular semantic image datasets ADE20k, Cityscapes and COCO-Stuff.



### CAMO-MOT: Combined Appearance-Motion Optimization for 3D Multi-Object Tracking with Camera-LiDAR Fusion
- **Arxiv ID**: http://arxiv.org/abs/2209.02540v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02540v3)
- **Published**: 2022-09-06 14:41:38+00:00
- **Updated**: 2022-09-12 05:42:38+00:00
- **Authors**: Li Wang, Xinyu Zhang, Wenyuan Qin, Xiaoyu Li, Lei Yang, Zhiwei Li, Lei Zhu, Hong Wang, Jun Li, Huaping Liu
- **Comment**: Based on this work, we achieved 1st place on the nuScenes tracking
  leaderboard
- **Journal**: None
- **Summary**: 3D Multi-object tracking (MOT) ensures consistency during continuous dynamic detection, conducive to subsequent motion planning and navigation tasks in autonomous driving. However, camera-based methods suffer in the case of occlusions and it can be challenging to accurately track the irregular motion of objects for LiDAR-based methods. Some fusion methods work well but do not consider the untrustworthy issue of appearance features under occlusion. At the same time, the false detection problem also significantly affects tracking. As such, we propose a novel camera-LiDAR fusion 3D MOT framework based on the Combined Appearance-Motion Optimization (CAMO-MOT), which uses both camera and LiDAR data and significantly reduces tracking failures caused by occlusion and false detection. For occlusion problems, we are the first to propose an occlusion head to select the best object appearance features multiple times effectively, reducing the influence of occlusions. To decrease the impact of false detection in tracking, we design a motion cost matrix based on confidence scores which improve the positioning and object prediction accuracy in 3D space. As existing multi-object tracking methods only consider a single category, we also propose to build a multi-category loss to implement multi-object tracking in multi-category scenes. A series of validation experiments are conducted on the KITTI and nuScenes tracking benchmarks. Our proposed method achieves state-of-the-art performance and the lowest identity switches (IDS) value (23 for Car and 137 for Pedestrian) among all multi-modal MOT methods on the KITTI test dataset. And our proposed method achieves state-of-the-art performance among all algorithms on the nuScenes test dataset with 75.3% AMOTA.



### The Outcome of the 2022 Landslide4Sense Competition: Advanced Landslide Detection from Multi-Source Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2209.02556v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02556v2)
- **Published**: 2022-09-06 15:05:12+00:00
- **Updated**: 2022-09-12 13:05:03+00:00
- **Authors**: Omid Ghorbanzadeh, Yonghao Xu, Hengwei Zhao, Junjue Wang, Yanfei Zhong, Dong Zhao, Qi Zang, Shuang Wang, Fahong Zhang, Yilei Shi, Xiao Xiang Zhu, Lin Bai, Weile Li, Weihang Peng, Pedram Ghamisi
- **Comment**: None
- **Journal**: None
- **Summary**: The scientific outcomes of the 2022 Landslide4Sense (L4S) competition organized by the Institute of Advanced Research in Artificial Intelligence (IARAI) are presented here. The objective of the competition is to automatically detect landslides based on large-scale multiple sources of satellite imagery collected globally. The 2022 L4S aims to foster interdisciplinary research on recent developments in deep learning (DL) models for the semantic segmentation task using satellite imagery. In the past few years, DL-based models have achieved performance that meets expectations on image interpretation, due to the development of convolutional neural networks (CNNs). The main objective of this article is to present the details and the best-performing algorithms featured in this competition. The winning solutions are elaborated with state-of-the-art models like the Swin Transformer, SegFormer, and U-Net. Advanced machine learning techniques and strategies such as hard example mining, self-training, and mix-up data augmentation are also considered. Moreover, we describe the L4S benchmark data set in order to facilitate further comparisons, and report the results of the accuracy assessment online. The data is accessible on \textit{Future Development Leaderboard} for future evaluation at \url{https://www.iarai.ac.at/landslide4sense/challenge/}, and researchers are invited to submit more prediction results, evaluate the accuracy of their methods, compare them with those of other users, and, ideally, improve the landslide detection results reported in this article.



### Progressive Domain Adaptation with Contrastive Learning for Object Detection in the Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2209.02564v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2209.02564v2)
- **Published**: 2022-09-06 15:16:35+00:00
- **Updated**: 2023-01-16 20:51:54+00:00
- **Authors**: Debojyoti Biswas, Jelena Tešić
- **Comment**: None
- **Journal**: None
- **Summary**: Images in aerial datasets are very large in resolution, and each frame contains many dense and small objects. State-of-the-art detection methods fail to capture small objects, local features, and region proposals for densely overlapped objects in aerial imagery due to the high variation of object sizes in satellite imagery with respect to the image size and high variation of content. Aerial imagery content varies greatly within the dataset due to the large change in lighting conditions, and the type of ground imagery captures from high altitudes. The variation is even higher between different datasets as object sizes, class distributions, image acquisition, and weather conditions can vary even more drastically. Thus, Domain Adaptation (DA) has been introduced as a band-aid to alleviate the degradation of object identification in previously unseen datasets. In this paper, we propose a small object detection pipeline that improves the feature extraction process by spatial pyramid pooling, cross-stage partial networks, heat-map-based region proposal network, and objects localization and identification through a novel image difficulty score that adapts the overall focal loss measure based on the image difficulty. Next, we propose novel contrastive learning with progressive domain adaptation to produce domain-invariant features across aerial datasets using local and global features. Effective analysis and illustration of different performance metrics and challenges show that our proposed method is comparable to the current State-of-Art models and creates a first-ever Domain Adaptation benchmark for the object detection task in highly imbalanced satellite datasets with large domain gaps and dominant small objects.



### Cross Modal Compression: Towards Human-comprehensible Semantic Compression
- **Arxiv ID**: http://arxiv.org/abs/2209.02574v1
- **DOI**: 10.1145/3474085.3475558
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2209.02574v1)
- **Published**: 2022-09-06 15:31:11+00:00
- **Updated**: 2022-09-06 15:31:11+00:00
- **Authors**: Jiguo Li, Chuanmin Jia, Xinfeng Zhang, Siwei Ma, Wen Gao
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Traditional image/video compression aims to reduce the transmission/storage cost with signal fidelity as high as possible. However, with the increasing demand for machine analysis and semantic monitoring in recent years, semantic fidelity rather than signal fidelity is becoming another emerging concern in image/video compression. With the recent advances in cross modal translation and generation, in this paper, we propose the cross modal compression~(CMC), a semantic compression framework for visual data, to transform the high redundant visual data~(such as image, video, etc.) into a compact, human-comprehensible domain~(such as text, sketch, semantic map, attributions, etc.), while preserving the semantic. Specifically, we first formulate the CMC problem as a rate-distortion optimization problem. Secondly, we investigate the relationship with the traditional image/video compression and the recent feature compression frameworks, showing the difference between our CMC and these prior frameworks. Then we propose a novel paradigm for CMC to demonstrate its effectiveness. The qualitative and quantitative results show that our proposed CMC can achieve encouraging reconstructed results with an ultrahigh compression ratio, showing better compression performance than the widely used JPEG baseline.



### Improving the Accuracy and Robustness of CNNs Using a Deep CCA Neural Data Regularizer
- **Arxiv ID**: http://arxiv.org/abs/2209.02582v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2209.02582v1)
- **Published**: 2022-09-06 15:40:39+00:00
- **Updated**: 2022-09-06 15:40:39+00:00
- **Authors**: Cassidy Pirlot, Richard C. Gerum, Cory Efird, Joel Zylberberg, Alona Fyshe
- **Comment**: None
- **Journal**: None
- **Summary**: As convolutional neural networks (CNNs) become more accurate at object recognition, their representations become more similar to the primate visual system. This finding has inspired us and other researchers to ask if the implication also runs the other way: If CNN representations become more brain-like, does the network become more accurate? Previous attempts to address this question showed very modest gains in accuracy, owing in part to limitations of the regularization method. To overcome these limitations, we developed a new neural data regularizer for CNNs that uses Deep Canonical Correlation Analysis (DCCA) to optimize the resemblance of the CNN's image representations to that of the monkey visual cortex. Using this new neural data regularizer, we see much larger performance gains in both classification accuracy and within-super-class accuracy, as compared to the previous state-of-the-art neural data regularizers. These networks are also more robust to adversarial attacks than their unregularized counterparts. Together, these results confirm that neural data regularization can push CNN performance higher, and introduces a new method that obtains a larger performance boost.



### Domain Engineering for Applied Monocular Reconstruction of Parametric Faces
- **Arxiv ID**: http://arxiv.org/abs/2209.02600v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02600v1)
- **Published**: 2022-09-06 15:51:20+00:00
- **Updated**: 2022-09-06 15:51:20+00:00
- **Authors**: Igor Borovikov, Karine Levonyan, Jon Rein, Pawel Wrotek, Nitish Victor
- **Comment**: An extended SIPP 2022 conference paper. arXiv admin note: substantial
  text overlap with arXiv:2208.02935
- **Journal**: Signal and Image Processing: An International Journal, August
  2022, Volume 13, No 2/3/4, pages 33-51
- **Summary**: Many modern online 3D applications and video games rely on parametric models of human faces for creating believable avatars. However, manually reproducing someone's facial likeness with a parametric model is difficult and time-consuming. Machine Learning solution for that task is highly desirable but is also challenging. The paper proposes a novel approach to the so-called Face-to-Parameters problem (F2P for short), aiming to reconstruct a parametric face from a single image. The proposed method utilizes synthetic data, domain decomposition, and domain adaptation to address multifaceted challenges in solving the F2P. The open-sourced codebase illustrates our key observations and provides means for quantitative evaluation. The presented approach proves practical in an industrial application; it improves accuracy and allows for more efficient models training. The techniques have the potential to extend to other types of parametric models.



### Statistical Shape Modeling of Biventricular Anatomy with Shared Boundaries
- **Arxiv ID**: http://arxiv.org/abs/2209.02706v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02706v2)
- **Published**: 2022-09-06 15:54:37+00:00
- **Updated**: 2022-09-12 21:07:38+00:00
- **Authors**: Krithika Iyer, Alan Morris, Brian Zenger, Karthik Karanth, Benjamin A Orkild, Oleksandre Korshak, Shireen Elhabian
- **Comment**: None
- **Journal**: None
- **Summary**: Statistical shape modeling (SSM) is a valuable and powerful tool to generate a detailed representation of complex anatomy that enables quantitative analysis and the comparison of shapes and their variations. SSM applies mathematics, statistics, and computing to parse the shape into a quantitative representation (such as correspondence points or landmarks) that will help answer various questions about the anatomical variations across the population. Complex anatomical structures have many diverse parts with varying interactions or intricate architecture. For example, the heart is four-chambered anatomy with several shared boundaries between chambers. Coordinated and efficient contraction of the chambers of the heart is necessary to adequately perfuse end organs throughout the body. Subtle shape changes within these shared boundaries of the heart can indicate potential pathological changes that lead to uncoordinated contraction and poor end-organ perfusion. Early detection and robust quantification could provide insight into ideal treatment techniques and intervention timing. However, existing SSM approaches fall short of explicitly modeling the statistics of shared boundaries. This paper presents a general and flexible data-driven approach for building statistical shape models of multi-organ anatomies with shared boundaries that capture morphological and alignment changes of individual anatomies and their shared boundary surfaces throughout the population. We demonstrate the effectiveness of the proposed methods using a biventricular heart dataset by developing shape models that consistently parameterize the cardiac biventricular structure and the interventricular septum (shared boundary surface) across the population data.



### Automatic counting of mounds on UAV images: combining instance segmentation and patch-level correction
- **Arxiv ID**: http://arxiv.org/abs/2209.02608v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.6; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2209.02608v1)
- **Published**: 2022-09-06 16:02:38+00:00
- **Updated**: 2022-09-06 16:02:38+00:00
- **Authors**: Majid Nikougoftar Nategh, Ahmed Zgaren, Wassim Bouachir, Nizar Bouguila
- **Comment**: 8 pages; IEEE International Conference on Machine Learning and
  Applications
- **Journal**: None
- **Summary**: Site preparation by mounding is a commonly used silvicultural treatment that improves tree growth conditions by mechanically creating planting microsites called mounds. Following site preparation, the next critical step is to count the number of mounds, which provides forest managers with a precise estimate of the number of seedlings required for a given plantation block. Counting the number of mounds is generally conducted through manual field surveys by forestry workers, which is costly and prone to errors, especially for large areas. To address this issue, we present a novel framework exploiting advances in Unmanned Aerial Vehicle (UAV) imaging and computer vision to accurately estimate the number of mounds on a planting block. The proposed framework comprises two main components. First, we exploit a visual recognition method based on a deep learning algorithm for multiple object detection by pixel-based segmentation. This enables a preliminary count of visible mounds, as well as other frequently seen objects (e.g. trees, debris, accumulation of water), to be used to characterize the planting block. Second, since visual recognition could limited by several perturbation factors (e.g. mound erosion, occlusion), we employ a machine learning estimation function that predicts the final number of mounds based on the local block properties extracted in the first stage. We evaluate the proposed framework on a new UAV dataset representing numerous planting blocks with varying features. The proposed method outperformed manual counting methods in terms of relative counting precision, indicating that it has the potential to be advantageous and efficient in difficult situations.



### Deep filter bank regression for super-resolution of anisotropic MR brain images
- **Arxiv ID**: http://arxiv.org/abs/2209.02611v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02611v1)
- **Published**: 2022-09-06 16:05:19+00:00
- **Updated**: 2022-09-06 16:05:19+00:00
- **Authors**: Samuel W. Remedios, Shuo Han, Yuan Xue, Aaron Carass, Trac D. Tran, Dzung L. Pham, Jerry L. Prince
- **Comment**: None
- **Journal**: None
- **Summary**: In 2D multi-slice magnetic resonance (MR) acquisition, the through-plane signals are typically of lower resolution than the in-plane signals. While contemporary super-resolution (SR) methods aim to recover the underlying high-resolution volume, the estimated high-frequency information is implicit via end-to-end data-driven training rather than being explicitly stated and sought. To address this, we reframe the SR problem statement in terms of perfect reconstruction filter banks, enabling us to identify and directly estimate the missing information. In this work, we propose a two-stage approach to approximate the completion of a perfect reconstruction filter bank corresponding to the anisotropic acquisition of a particular scan. In stage 1, we estimate the missing filters using gradient descent and in stage 2, we use deep networks to learn the mapping from coarse coefficients to detail coefficients. In addition, the proposed formulation does not rely on external training data, circumventing the need for domain shift correction. Under our approach, SR performance is improved particularly in "slice gap" scenarios, likely due to the constrained solution space imposed by the framework.



### Single-Stage Broad Multi-Instance Multi-Label Learning (BMIML) with Diverse Inter-Correlations and its application to medical image classification
- **Arxiv ID**: http://arxiv.org/abs/2209.02625v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.02625v2)
- **Published**: 2022-09-06 16:28:54+00:00
- **Updated**: 2023-06-14 15:40:52+00:00
- **Authors**: Qi Lai, Jianhang Zhou, Yanfen Gan, Chi-Man Vong, Deshuang Huang
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: described by multiple instances (e.g., image patches) and simultaneously associated with multiple labels. Existing MIML methods are useful in many applications but most of which suffer from relatively low accuracy and training efficiency due to several issues: i) the inter-label correlations(i.e., the probabilistic correlations between the multiple labels corresponding to an object) are neglected; ii) the inter-instance correlations (i.e., the probabilistic correlations of different instances in predicting the object label) cannot be learned directly (or jointly) with other types of correlations due to the missing instance labels; iii) diverse inter-correlations (e.g., inter-label correlations, inter-instance correlations) can only be learned in multiple stages. To resolve these issues, a new single-stage framework called broad multi-instance multi-label learning (BMIML) is proposed. In BMIML, there are three innovative modules: i) an auto-weighted label enhancement learning (AWLEL) based on broad learning system (BLS) is designed, which simultaneously and efficiently captures the inter-label correlations while traditional BLS cannot; ii) A specific MIML neural network called scalable multi-instance probabilistic regression (SMIPR) is constructed to effectively estimate the inter-instance correlations using the object label only, which can provide additional probabilistic information for learning; iii) Finally, an interactive decision optimization (IDO) is designed to combine and optimize the results from AWLEL and SMIPR and form a single-stage framework. Experiments show that BMIML is highly competitive to (or even better than) existing methods in accuracy and much faster than most MIML methods even for large medical image data sets (> 90K images).



### Bag of Tricks for FGSM Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2209.02684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02684v1)
- **Published**: 2022-09-06 17:53:21+00:00
- **Updated**: 2022-09-06 17:53:21+00:00
- **Authors**: Zichao Li, Li Liu, Zeyu Wang, Yuyin Zhou, Cihang Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial training (AT) with samples generated by Fast Gradient Sign Method (FGSM), also known as FGSM-AT, is a computationally simple method to train robust networks. However, during its training procedure, an unstable mode of "catastrophic overfitting" has been identified in arXiv:2001.03994 [cs.LG], where the robust accuracy abruptly drops to zero within a single training step. Existing methods use gradient regularizers or random initialization tricks to attenuate this issue, whereas they either take high computational cost or lead to lower robust accuracy. In this work, we provide the first study, which thoroughly examines a collection of tricks from three perspectives: Data Initialization, Network Structure, and Optimization, to overcome the catastrophic overfitting in FGSM-AT.   Surprisingly, we find that simple tricks, i.e., a) masking partial pixels (even without randomness), b) setting a large convolution stride and smooth activation functions, or c) regularizing the weights of the first convolutional layer, can effectively tackle the overfitting issue. Extensive results on a range of network architectures validate the effectiveness of each proposed trick, and the combinations of tricks are also investigated. For example, trained with PreActResNet-18 on CIFAR-10, our method attains 49.8% accuracy against PGD-50 attacker and 46.4% accuracy against AutoAttack, demonstrating that pure FGSM-AT is capable of enabling robust learners. The code and models are publicly available at https://github.com/UCSC-VLAA/Bag-of-Tricks-for-FGSM-AT.



### Unpaired Image Translation via Vector Symbolic Architectures
- **Arxiv ID**: http://arxiv.org/abs/2209.02686v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02686v1)
- **Published**: 2022-09-06 17:53:57+00:00
- **Updated**: 2022-09-06 17:53:57+00:00
- **Authors**: Justin Theiss, Jay Leverett, Daeil Kim, Aayush Prakash
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Image-to-image translation has played an important role in enabling synthetic data for computer vision. However, if the source and target domains have a large semantic mismatch, existing techniques often suffer from source content corruption aka semantic flipping. To address this problem, we propose a new paradigm for image-to-image translation using Vector Symbolic Architectures (VSA), a theoretical framework which defines algebraic operations in a high-dimensional vector (hypervector) space. We introduce VSA-based constraints on adversarial learning for source-to-target translations by learning a hypervector mapping that inverts the translation to ensure consistency with source content. We show both qualitatively and quantitatively that our method improves over other state-of-the-art techniques.



### Statistical Foundation Behind Machine Learning and Its Impact on Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2209.02691v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02691v1)
- **Published**: 2022-09-06 17:59:04+00:00
- **Updated**: 2022-09-06 17:59:04+00:00
- **Authors**: Lei Zhang, Heung-Yeung Shum
- **Comment**: None
- **Journal**: None
- **Summary**: This paper revisits the principle of uniform convergence in statistical learning, discusses how it acts as the foundation behind machine learning, and attempts to gain a better understanding of the essential problem that current deep learning algorithms are solving. Using computer vision as an example domain in machine learning, the discussion shows that recent research trends in leveraging increasingly large-scale data to perform pre-training for representation learning are largely to reduce the discrepancy between a practically tractable empirical loss and its ultimately desired but intractable expected loss. Furthermore, this paper suggests a few future research directions, predicts the continued increase of data, and argues that more fundamental research is needed on robustness, interpretability, and reasoning capabilities of machine learning by incorporating structure and knowledge.



### Deep Learning Assisted Optimization for 3D Reconstruction from Single 2D Line Drawings
- **Arxiv ID**: http://arxiv.org/abs/2209.02692v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02692v3)
- **Published**: 2022-09-06 17:59:11+00:00
- **Updated**: 2022-11-02 12:55:07+00:00
- **Authors**: Jia Zheng, Yifan Zhu, Kehan Wang, Qiang Zou, Zihan Zhou
- **Comment**: Project page is at https://manycore-research.github.io/cstr
- **Journal**: None
- **Summary**: In this paper, we revisit the long-standing problem of automatic reconstruction of 3D objects from single line drawings. Previous optimization-based methods can generate compact and accurate 3D models, but their success rates depend heavily on the ability to (i) identifying a sufficient set of true geometric constraints, and (ii) choosing a good initial value for the numerical optimization. In view of these challenges, we propose to train deep neural networks to detect pairwise relationships among geometric entities (i.e., edges) in the 3D object, and to predict initial depth value of the vertices. Our experiments on a large dataset of CAD models show that, by leveraging deep learning in a geometric constraint solving pipeline, the success rate of optimization-based 3D reconstruction can be significantly improved.



### Spatiotemporal Cardiac Statistical Shape Modeling: A Data-Driven Approach
- **Arxiv ID**: http://arxiv.org/abs/2209.02736v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02736v1)
- **Published**: 2022-09-06 18:00:45+00:00
- **Updated**: 2022-09-06 18:00:45+00:00
- **Authors**: Jadie Adams, Nawazish Khan, Alan Morris, Shireen Elhabian
- **Comment**: Accepted in the Statistical Atlases and Computational Modeling of the
  Heart (STACOM) workshop, part of the 25th International Conference on Medical
  Image Computing and Computer Assisted Intervention, MICCAI 2022. To be
  published in a Lecture Notes in Computer Science proceeding published by
  Springer
- **Journal**: None
- **Summary**: Clinical investigations of anatomy's structural changes over time could greatly benefit from population-level quantification of shape, or spatiotemporal statistic shape modeling (SSM). Such a tool enables characterizing patient organ cycles or disease progression in relation to a cohort of interest. Constructing shape models requires establishing a quantitative shape representation (e.g., corresponding landmarks). Particle-based shape modeling (PSM) is a data-driven SSM approach that captures population-level shape variations by optimizing landmark placement. However, it assumes cross-sectional study designs and hence has limited statistical power in representing shape changes over time. Existing methods for modeling spatiotemporal or longitudinal shape changes require predefined shape atlases and pre-built shape models that are typically constructed cross-sectionally. This paper proposes a data-driven approach inspired by the PSM method to learn population-level spatiotemporal shape changes directly from shape data. We introduce a novel SSM optimization scheme that produces landmarks that are in correspondence both across the population (inter-subject) and across time-series (intra-subject). We apply the proposed method to 4D cardiac data from atrial-fibrillation patients and demonstrate its efficacy in representing the dynamic change of the left atrium. Furthermore, we show that our method outperforms an image-based approach for spatiotemporal SSM with respect to a generative time-series model, the Linear Dynamical System (LDS). LDS fit using a spatiotemporal shape model optimized via our approach provides better generalization and specificity, indicating it accurately captures the underlying time-dependency.



### Handcrafted Feature Selection Techniques for Pattern Recognition: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2209.02746v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02746v1)
- **Published**: 2022-09-06 18:05:35+00:00
- **Updated**: 2022-09-06 18:05:35+00:00
- **Authors**: Alysson Ribeiro da Silva, Camila Guedes Silveira
- **Comment**: Technical Report on Handcraft Feature Selection Techniques
- **Journal**: None
- **Summary**: The accuracy of a classifier, when performing Pattern recognition, is mostly tied to the quality and representativeness of the input feature vector. Feature Selection is a process that allows for representing information properly and may increase the accuracy of a classifier. This process is responsible for finding the best possible features, thus allowing us to identify to which class a pattern belongs. Feature selection methods can be categorized as Filters, Wrappers, and Embed. This paper presents a survey on some Filters and Wrapper methods for handcrafted feature selection. Some discussions, with regard to the data structure, processing time, and ability to well represent a feature vector, are also provided in order to explicitly show how appropriate some methods are in order to perform feature selection. Therefore, the presented feature selection methods can be accurate and efficient if applied considering their positives and negatives, finding which one fits best the problem's domain may be the hardest task.



### Scalable Regularization of Scene Graph Generation Models using Symbolic Theories
- **Arxiv ID**: http://arxiv.org/abs/2209.02749v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02749v1)
- **Published**: 2022-09-06 18:08:21+00:00
- **Updated**: 2022-09-06 18:08:21+00:00
- **Authors**: Davide Buffelli, Efthymia Tsamoura
- **Comment**: None
- **Journal**: None
- **Summary**: Several techniques have recently aimed to improve the performance of deep learning models for Scene Graph Generation (SGG) by incorporating background knowledge. State-of-the-art techniques can be divided into two families: one where the background knowledge is incorporated into the model in a subsymbolic fashion, and another in which the background knowledge is maintained in symbolic form. Despite promising results, both families of techniques face several shortcomings: the first one requires ad-hoc, more complex neural architectures increasing the training or inference cost; the second one suffers from limited scalability w.r.t. the size of the background knowledge. Our work introduces a regularization technique for injecting symbolic background knowledge into neural SGG models that overcomes the limitations of prior art. Our technique is model-agnostic, does not incur any cost at inference time, and scales to previously unmanageable background knowledge sizes. We demonstrate that our technique can improve the accuracy of state-of-the-art SGG models, by up to 33%.



### Crowdsourced-based Deep Convolutional Networks for Urban Flood Depth Mapping
- **Arxiv ID**: http://arxiv.org/abs/2209.09200v1
- **DOI**: 10.35490/EC3.2022.145
- **Categories**: **cs.CV**, cs.AI, cs.HC, 94-08, 68T45, 68T07, I.2.10; I.4.8; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2209.09200v1)
- **Published**: 2022-09-06 18:16:12+00:00
- **Updated**: 2022-09-06 18:16:12+00:00
- **Authors**: Bahareh Alizadeh, Amir H. Behzadan
- **Comment**: 2022 European Conference on Computing in Construction
- **Journal**: None
- **Summary**: Successful flood recovery and evacuation require access to reliable flood depth information. Most existing flood mapping tools do not provide real-time flood maps of inundated streets in and around residential areas. In this paper, a deep convolutional network is used to determine flood depth with high spatial resolution by analyzing crowdsourced images of submerged traffic signs. Testing the model on photos from a recent flood in the U.S. and Canada yields a mean absolute error of 6.978 in., which is on par with previous studies, thus demonstrating the applicability of this approach to low-cost, accurate, and real-time flood risk mapping.



### A Masked Bounding-Box Selection Based ResNet Predictor for Text Rotation Prediction
- **Arxiv ID**: http://arxiv.org/abs/2209.09198v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09198v1)
- **Published**: 2022-09-06 19:02:21+00:00
- **Updated**: 2022-09-06 19:02:21+00:00
- **Authors**: Michael Yang, Yuan Lin, ChiuMan Ho
- **Comment**: None
- **Journal**: None
- **Summary**: The existing Optical Character Recognition (OCR) systems are capable of recognizing images with horizontal texts. However, when the rotation of the texts increases, it becomes harder to recognizing these texts. The performance of the OCR systems decreases. Thus predicting the rotations of the texts and correcting the images are important. Previous work mainly uses traditional Computer Vision methods like Hough Transform and Deep Learning methods like Convolutional Neural Network. However, all of these methods are prone to background noises commonly existing in general images with texts. To tackle this problem, in this work, we introduce a new masked bounding-box selection method, that incorporating the bounding box information into the system. By training a ResNet predictor to focus on the bounding box as the region of interest (ROI), the predictor learns to overlook the background noises. Evaluations on the text rotation prediction tasks show that our method improves the performance by a large margin.



### Fusion of Satellite Images and Weather Data with Transformer Networks for Downy Mildew Disease Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.02797v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.02797v1)
- **Published**: 2022-09-06 19:55:16+00:00
- **Updated**: 2022-09-06 19:55:16+00:00
- **Authors**: William Maillet, Maryam Ouhami, Adel Hafiane
- **Comment**: None
- **Journal**: None
- **Summary**: Crop diseases significantly affect the quantity and quality of agricultural production. In a context where the goal of precision agriculture is to minimize or even avoid the use of pesticides, weather and remote sensing data with deep learning can play a pivotal role in detecting crop diseases, allowing localized treatment of crops. However, combining heterogeneous data such as weather and images remains a hot topic and challenging task. Recent developments in transformer architectures have shown the possibility of fusion of data from different domains, for instance text-image. The current trend is to custom only one transformer to create a multimodal fusion model. Conversely, we propose a new approach to realize data fusion using three transformers. In this paper, we first solved the missing satellite images problem, by interpolating them with a ConvLSTM model. Then, proposed a multimodal fusion architecture that jointly learns to process visual and weather information. The architecture is built from three main components, a Vision Transformer and two transformer-encoders, allowing to fuse both image and weather modalities. The results of the proposed method are promising achieving 97\% overall accuracy.



### Use and Misuse of Machine Learning in Anthropology
- **Arxiv ID**: http://arxiv.org/abs/2209.02811v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, J.5; I.2
- **Links**: [PDF](http://arxiv.org/pdf/2209.02811v1)
- **Published**: 2022-09-06 20:32:24+00:00
- **Updated**: 2022-09-06 20:32:24+00:00
- **Authors**: Jeff Calder, Reed Coil, Annie Melton, Peter J. Olver, Gilbert Tostevin, Katrina Yezzi-Woodley
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning (ML), being now widely accessible to the research community at large, has fostered a proliferation of new and striking applications of these emergent mathematical techniques across a wide range of disciplines. In this paper, we will focus on a particular case study: the field of paleoanthropology, which seeks to understand the evolution of the human species based on biological and cultural evidence. As we will show, the easy availability of ML algorithms and lack of expertise on their proper use among the anthropological research community has led to foundational misapplications that have appeared throughout the literature. The resulting unreliable results not only undermine efforts to legitimately incorporate ML into anthropological research, but produce potentially faulty understandings about our human evolutionary and behavioral past.   The aim of this paper is to provide a brief introduction to some of the ways in which ML has been applied within paleoanthropology; we also include a survey of some basic ML algorithms for those who are not fully conversant with the field, which remains under active development. We discuss a series of missteps, errors, and violations of correct protocols of ML methods that appear disconcertingly often within the accumulating body of anthropological literature. These mistakes include use of outdated algorithms and practices; inappropriate train/test splits, sample composition, and textual explanations; as well as an absence of transparency due to the lack of data/code sharing, and the subsequent limitations imposed on independent replication. We assert that expanding samples, sharing data and code, re-evaluating approaches to peer review, and, most importantly, developing interdisciplinary teams that include experts in ML are all necessary for progress in future research incorporating ML within anthropology.



### CP-AGCN: Pytorch-based Attention Informed Graph Convolutional Network for Identifying Infants at Risk of Cerebral Palsy
- **Arxiv ID**: http://arxiv.org/abs/2209.02824v1
- **DOI**: 10.1016/j.simpa.2022.100419
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.02824v1)
- **Published**: 2022-09-06 21:28:12+00:00
- **Updated**: 2022-09-06 21:28:12+00:00
- **Authors**: Haozheng Zhang, Edmond S. L. Ho, Hubert P. H. Shum
- **Comment**: None
- **Journal**: None
- **Summary**: Early prediction is clinically considered one of the essential parts of cerebral palsy (CP) treatment. We propose to implement a low-cost and interpretable classification system for supporting CP prediction based on General Movement Assessment (GMA). We design a Pytorch-based attention-informed graph convolutional network to early identify infants at risk of CP from skeletal data extracted from RGB videos. We also design a frequency-binning module for learning the CP movements in the frequency domain while filtering noise. Our system only requires consumer-grade RGB videos for training to support interactive-time CP prediction by providing an interpretable CP classification result.



### Low-Energy Convolutional Neural Networks (CNNs) using Hadamard Method
- **Arxiv ID**: http://arxiv.org/abs/2209.09106v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09106v1)
- **Published**: 2022-09-06 21:36:57+00:00
- **Updated**: 2022-09-06 21:36:57+00:00
- **Authors**: Varun Mannam
- **Comment**: None
- **Journal**: None
- **Summary**: The growing demand for the internet of things (IoT) makes it necessary to implement computer vision tasks such as object recognition in low-power devices. Convolutional neural networks (CNNs) are a potential approach for object recognition and detection. However, the convolutional layer in CNN consumes significant energy compared to the fully connected layers. To mitigate this problem, a new approach based on the Hadamard transformation as an alternative to the convolution operation is demonstrated using two fundamental datasets, MNIST and CIFAR10. The mathematical expression of the Hadamard method shows the clear potential to save energy consumption compared to convolutional layers, which are helpful with BigData applications. In addition, to the test accuracy of the MNIST dataset, the Hadamard method performs similarly to the convolution method. In contrast, with the CIFAR10 dataset, test data accuracy is dropped (due to complex data and multiple channels) compared to the convolution method. Finally, the demonstrated method is helpful for other computer vision tasks when the kernel size is smaller than the input image size.



### Unsupervised Scene Sketch to Photo Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2209.02834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.02834v1)
- **Published**: 2022-09-06 22:25:06+00:00
- **Updated**: 2022-09-06 22:25:06+00:00
- **Authors**: Jiayun Wang, Sangryul Jeon, Stella X. Yu, Xi Zhang, Himanshu Arora, Yu Lou
- **Comment**: None
- **Journal**: ECCVW 2022
- **Summary**: Sketches make an intuitive and powerful visual expression as they are fast executed freehand drawings. We present a method for synthesizing realistic photos from scene sketches. Without the need for sketch and photo pairs, our framework directly learns from readily available large-scale photo datasets in an unsupervised manner. To this end, we introduce a standardization module that provides pseudo sketch-photo pairs during training by converting photos and sketches to a standardized domain, i.e. the edge map. The reduced domain gap between sketch and photo also allows us to disentangle them into two components: holistic scene structures and low-level visual styles such as color and texture. Taking this advantage, we synthesize a photo-realistic image by combining the structure of a sketch and the visual style of a reference photo. Extensive experimental results on perceptual similarity metrics and human perceptual studies show the proposed method could generate realistic photos with high fidelity from scene sketches and outperform state-of-the-art photo synthesis baselines. We also demonstrate that our framework facilitates a controllable manipulation of photo synthesis by editing strokes of corresponding sketches, delivering more fine-grained details than previous approaches that rely on region-level editing.



### Studying Bias in GANs through the Lens of Race
- **Arxiv ID**: http://arxiv.org/abs/2209.02836v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2209.02836v2)
- **Published**: 2022-09-06 22:25:56+00:00
- **Updated**: 2022-09-15 01:39:39+00:00
- **Authors**: Vongani H. Maluleke, Neerja Thakkar, Tim Brooks, Ethan Weber, Trevor Darrell, Alexei A. Efros, Angjoo Kanazawa, Devin Guillory
- **Comment**: ECCV 2022. Project Page: https://neerja.me/bias-gans/
- **Journal**: None
- **Summary**: In this work, we study how the performance and evaluation of generative image models are impacted by the racial composition of their training datasets. By examining and controlling the racial distributions in various training datasets, we are able to observe the impacts of different training distributions on generated image quality and the racial distributions of the generated images. Our results show that the racial compositions of generated images successfully preserve that of the training data. However, we observe that truncation, a technique used to generate higher quality images during inference, exacerbates racial imbalances in the data. Lastly, when examining the relationship between image quality and race, we find that the highest perceived visual quality images of a given race come from a distribution where that race is well-represented, and that annotators consistently prefer generated images of white people over those of Black people.



### DC-Art-GAN: Stable Procedural Content Generation using DC-GANs for Digital Art
- **Arxiv ID**: http://arxiv.org/abs/2209.02847v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.02847v2)
- **Published**: 2022-09-06 23:06:46+00:00
- **Updated**: 2023-03-13 18:23:32+00:00
- **Authors**: Rohit Gandikota, Nik Bear Brown
- **Comment**: the project is done as an undergrad report. On the hind sight, it
  does not contain full and exhaustive analysis
- **Journal**: None
- **Summary**: Art is an artistic method of using digital technologies as a part of the generative or creative process. With the advent of digital currency and NFTs (Non-Fungible Token), the demand for digital art is growing aggressively. In this manuscript, we advocate the concept of using deep generative networks with adversarial training for a stable and variant art generation. The work mainly focuses on using the Deep Convolutional Generative Adversarial Network (DC-GAN) and explores the techniques to address the common pitfalls in GAN training. We compare various architectures and designs of DC-GANs to arrive at a recommendable design choice for a stable and realistic generation. The main focus of the work is to generate realistic images that do not exist in reality but are synthesised from random noise by the proposed model. We provide visual results of generated animal face images (some pieces of evidence showing a blend of species) along with recommendations for training, architecture and design choices. We also show how training image preprocessing plays a massive role in GAN training.



### Monkeypox virus detection using pre-trained deep learning-based approaches
- **Arxiv ID**: http://arxiv.org/abs/2209.04444v2
- **DOI**: 10.1007/s10916-022-01868-2
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.04444v2)
- **Published**: 2022-09-06 23:17:34+00:00
- **Updated**: 2022-09-17 10:46:51+00:00
- **Authors**: Chiranjibi Sitaula, Tej Bahadur Shahi
- **Comment**: Under consideration in Journal of Medical Systems
- **Journal**: Journal of Medical Systems, 2022
- **Summary**: Monkeypox virus is emerging slowly with the decline of COVID-19 virus infections around the world. People are afraid of it, thinking that it would appear as a pandemic like COVID-19. As such, it is crucial to detect them earlier before widespread community transmission. AI-based detection could help identify them at the early stage. In this paper, we aim to compare 13 different pre-trained deep learning (DL) models for the Monkeypox virus detection. For this, we initially fine-tune them with the addition of universal custom layers for all of them and analyse the results using four well-established measures: Precision, Recall, F1-score, and Accuracy. After the identification of the best-performing DL models, we ensemble them to improve the overall performance using a majority voting over the probabilistic outputs obtained from them. We perform our experiments on a publicly available dataset, which results in average Precision, Recall, F1-score, and Accuracy of 85.44\%, 85.47\%, 85.40\%, and 87.13\%, respectively with the help of our proposed ensemble approach. These encouraging results, which outperform the state-of-the-art methods, suggest that the proposed approach is applicable to health practitioners for mass screening.



### Video Restoration with a Deep Plug-and-Play Prior
- **Arxiv ID**: http://arxiv.org/abs/2209.02854v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, math.OC, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2209.02854v2)
- **Published**: 2022-09-06 23:31:20+00:00
- **Updated**: 2022-09-15 16:17:08+00:00
- **Authors**: Antoine Monod, Julie Delon, Matias Tassano, Andrés Almansa
- **Comment**: 10 pages + 4 pages supplementary; code at github.com/amonod/pnp-video
- **Journal**: None
- **Summary**: This paper presents a novel method for restoring digital videos via a Deep Plug-and-Play (PnP) approach. Under a Bayesian formalism, the method consists in using a deep convolutional denoising network in place of the proximal operator of the prior in an alternating optimization scheme. We distinguish ourselves from prior PnP work by directly applying that method to restore a digital video from a degraded video observation. This way, a network trained once for denoising can be repurposed for other video restoration tasks. Our experiments in video deblurring, super-resolution, and interpolation of random missing pixels all show a clear benefit to using a network specifically designed for video denoising, as it yields better restoration performance and better temporal stability than a single image network with similar denoising performance using the same PnP formulation. Moreover, our method compares favorably to applying a different state-of-the-art PnP scheme separately on each frame of the sequence. This opens new perspectives in the field of video restoration.



