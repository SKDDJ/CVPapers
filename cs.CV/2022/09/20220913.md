# Arxiv Papers in cs.CV on 2022-09-13
### Semantic2Graph: Graph-based Multi-modal Feature Fusion for Action Segmentation in Videos
- **Arxiv ID**: http://arxiv.org/abs/2209.05653v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, 68T01, 68T30, 68T45, I.2.10; I.4.8; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2209.05653v4)
- **Published**: 2022-09-13 00:01:23+00:00
- **Updated**: 2023-03-20 00:53:24+00:00
- **Authors**: Junbin Zhang, Pei-Hsuan Tsai, Meng-Hsun Tsai
- **Comment**: 13 pages, 3 figures, 9 tables. This paper was submitted to Springer
- **Journal**: None
- **Summary**: Video action segmentation and recognition tasks have been widely applied in many fields. Most previous studies employ large-scale, high computational visual models to understand videos comprehensively. However, few studies directly employ the graph model to reason about the video. The graph model provides the benefits of fewer parameters, low computational cost, a large receptive field, and flexible neighborhood message aggregation. In this paper, we present a graph-based method named Semantic2Graph, to turn the video action segmentation and recognition problem into node classification of graphs. To preserve fine-grained relations in videos, we construct the graph structure of videos at the frame-level and design three types of edges: temporal, semantic, and self-loop. We combine visual, structural, and semantic features as node attributes. Semantic edges are used to model long-term spatio-temporal relations, while the semantic features are the embedding of the label-text based on the textual prompt. A Graph Neural Networks (GNNs) model is used to learn multi-modal feature fusion. Experimental results show that Semantic2Graph achieves improvement on GTEA and 50Salads, compared to the state-of-the-art results. Multiple ablation experiments further confirm the effectiveness of semantic features in improving model performance, and semantic edges enable Semantic2Graph to capture long-term dependencies at a low cost.



### ComplETR: Reducing the cost of annotations for object detection in dense scenes with vision transformers
- **Arxiv ID**: http://arxiv.org/abs/2209.05654v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.05654v1)
- **Published**: 2022-09-13 00:11:16+00:00
- **Updated**: 2022-09-13 00:11:16+00:00
- **Authors**: Achin Jain, Kibok Lee, Gurumurthy Swaminathan, Hao Yang, Bernt Schiele, Avinash Ravichandran, Onkar Dabeer
- **Comment**: None
- **Journal**: None
- **Summary**: Annotating bounding boxes for object detection is expensive, time-consuming, and error-prone. In this work, we propose a DETR based framework called ComplETR that is designed to explicitly complete missing annotations in partially annotated dense scene datasets. This reduces the need to annotate every object instance in the scene thereby reducing annotation cost. ComplETR augments object queries in DETR decoder with patch information of objects in the image. Combined with a matching loss, it can effectively find objects that are similar to the input patch and complete the missing annotations. We show that our framework outperforms the state-of-the-art methods such as Soft Sampling and Unbiased Teacher by itself, while at the same time can be used in conjunction with these methods to further improve their performance. Our framework is also agnostic to the choice of the downstream object detectors; we show performance improvement for several popular detectors such as Faster R-CNN, Cascade R-CNN, CenterNet2, and Deformable DETR on multiple dense scene datasets.



### Switchable Self-attention Module
- **Arxiv ID**: http://arxiv.org/abs/2209.05680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.05680v1)
- **Published**: 2022-09-13 01:19:38+00:00
- **Updated**: 2022-09-13 01:19:38+00:00
- **Authors**: Shanshan Zhong, Wushao Wen, Jinghui Qin
- **Comment**: Work in progress
- **Journal**: None
- **Summary**: Attention mechanism has gained great success in vision recognition. Many works are devoted to improving the effectiveness of attention mechanism, which finely design the structure of the attention operator. These works need lots of experiments to pick out the optimal settings when scenarios change, which consumes a lot of time and computational resources. In addition, a neural network often contains many network layers, and most studies often use the same attention module to enhance different network layers, which hinders the further improvement of the performance of the self-attention mechanism. To address the above problems, we propose a self-attention module SEM. Based on the input information of the attention module and alternative attention operators, SEM can automatically decide to select and integrate attention operators to compute attention maps. The effectiveness of SEM is demonstrated by extensive experiments on widely used benchmark datasets and popular self-attention networks.



### One-shot Network Pruning at Initialization with Discriminative Image Patches
- **Arxiv ID**: http://arxiv.org/abs/2209.05683v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.05683v2)
- **Published**: 2022-09-13 01:41:37+00:00
- **Updated**: 2022-10-04 03:01:47+00:00
- **Authors**: Yinan Yang, Yu Wang, Ying Ji, Heng Qi, Jien Kato
- **Comment**: BMVC 2022
- **Journal**: None
- **Summary**: One-shot Network Pruning at Initialization (OPaI) is an effective method to decrease network pruning costs. Recently, there is a growing belief that data is unnecessary in OPaI. However, we obtain an opposite conclusion by ablation experiments in two representative OPaI methods, SNIP and GraSP. Specifically, we find that informative data is crucial to enhancing pruning performance. In this paper, we propose two novel methods, Discriminative One-shot Network Pruning (DOP) and Super Stitching, to prune the network by high-level visual discriminative image patches. Our contributions are as follows. (1) Extensive experiments reveal that OPaI is data-dependent. (2) Super Stitching performs significantly better than the original OPaI method on benchmark ImageNet, especially in a highly compressed model.



### PSAQ-ViT V2: Towards Accurate and General Data-Free Quantization for Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2209.05687v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.05687v2)
- **Published**: 2022-09-13 01:55:53+00:00
- **Updated**: 2023-07-31 03:14:23+00:00
- **Authors**: Zhikai Li, Mengjuan Chen, Junrui Xiao, Qingyi Gu
- **Comment**: Accepted by TNNLS 2023
- **Journal**: None
- **Summary**: Data-free quantization can potentially address data privacy and security concerns in model compression, and thus has been widely investigated. Recently, PSAQ-ViT designs a relative value metric, patch similarity, to generate data from pre-trained vision transformers (ViTs), achieving the first attempt at data-free quantization for ViTs. In this paper, we propose PSAQ-ViT V2, a more accurate and general data-free quantization framework for ViTs, built on top of PSAQ-ViT. More specifically, following the patch similarity metric in PSAQ-ViT, we introduce an adaptive teacher-student strategy, which facilitates the constant cyclic evolution of the generated samples and the quantized model (student) in a competitive and interactive fashion under the supervision of the full-precision model (teacher), thus significantly improving the accuracy of the quantized model. Moreover, without the auxiliary category guidance, we employ the task- and model-independent prior information, making the general-purpose scheme compatible with a broad range of vision tasks and models. Extensive experiments are conducted on various models on image classification, object detection, and semantic segmentation tasks, and PSAQ-ViT V2, with the naive quantization strategy and without access to real-world data, consistently achieves competitive results, showing potential as a powerful baseline on data-free quantization for ViTs. For instance, with Swin-S as the (backbone) model, 8-bit quantization reaches 82.13 top-1 accuracy on ImageNet, 50.9 box AP and 44.1 mask AP on COCO, and 47.2 mIoU on ADE20K. We hope that accurate and general PSAQ-ViT V2 can serve as a potential and practice solution in real-world applications involving sensitive data. Code is released and merged at: https://github.com/zkkli/PSAQ-ViT.



### Concept-Based Explanations for Tabular Data
- **Arxiv ID**: http://arxiv.org/abs/2209.05690v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.05690v1)
- **Published**: 2022-09-13 02:19:29+00:00
- **Updated**: 2022-09-13 02:19:29+00:00
- **Authors**: Varsha Pendyala, Jihye Choi
- **Comment**: None
- **Journal**: None
- **Summary**: The interpretability of machine learning models has been an essential area of research for the safe deployment of machine learning systems. One particular approach is to attribute model decisions to high-level concepts that humans can understand. However, such concept-based explainability for Deep Neural Networks (DNNs) has been studied mostly on image domain. In this paper, we extend TCAV, the concept attribution approach, to tabular learning, by providing an idea on how to define concepts over tabular data. On a synthetic dataset with ground-truth concept explanations and a real-world dataset, we show the validity of our method in generating interpretability results that match the human-level intuitions. On top of this, we propose a notion of fairness based on TCAV that quantifies what layer of DNN has learned representations that lead to biased predictions of the model. Also, we empirically demonstrate the relation of TCAV-based fairness to a group fairness notion, Demographic Parity.



### Vision Transformers for Action Recognition: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2209.05700v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.05700v1)
- **Published**: 2022-09-13 02:57:05+00:00
- **Updated**: 2022-09-13 02:57:05+00:00
- **Authors**: Anwaar Ulhaq, Naveed Akhtar, Ganna Pogrebna, Ajmal Mian
- **Comment**: 15 Figures and 5 Tables
- **Journal**: None
- **Summary**: Vision transformers are emerging as a powerful tool to solve computer vision problems. Recent techniques have also proven the efficacy of transformers beyond the image domain to solve numerous video-related tasks. Among those, human action recognition is receiving special attention from the research community due to its widespread applications. This article provides the first comprehensive survey of vision transformer techniques for action recognition. We analyze and summarize the existing and emerging literature in this direction while highlighting the popular trends in adapting transformers for action recognition. Due to their specialized application, we collectively refer to these methods as ``action transformers''. Our literature review provides suitable taxonomies for action transformers based on their architecture, modality, and intended objective. Within the context of action transformers, we explore the techniques to encode spatio-temporal data, dimensionality reduction, frame patch and spatio-temporal cube construction, and various representation methods. We also investigate the optimization of spatio-temporal attention in transformer layers to handle longer sequences, typically by reducing the number of tokens in a single attention operation. Moreover, we also investigate different network learning strategies, such as self-supervised and zero-shot learning, along with their associated losses for transformer-based action recognition. This survey also summarizes the progress towards gaining grounds on evaluation metric scores on important benchmarks with action transformers. Finally, it provides a discussion on the challenges, outlook, and future avenues for this research direction.



### A Capsule Network for Hierarchical Multi-Label Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.05723v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T07 (Primary), 68T10 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/2209.05723v1)
- **Published**: 2022-09-13 04:17:08+00:00
- **Updated**: 2022-09-13 04:17:08+00:00
- **Authors**: Khondaker Tasrif Noor, Antonio Robles-Kelly, Brano Kusy
- **Comment**: This paper is to be published in S+SSPR 2022 proceedings
- **Journal**: None
- **Summary**: Image classification is one of the most important areas in computer vision. Hierarchical multi-label classification applies when a multi-class image classification problem is arranged into smaller ones based upon a hierarchy or taxonomy. Thus, hierarchical classification modes generally provide multiple class predictions on each instance, whereby these are expected to reflect the structure of image classes as related to one another. In this paper, we propose a multi-label capsule network (ML-CapsNet) for hierarchical classification. Our ML-CapsNet predicts multiple image classes based on a hierarchical class-label tree structure. To this end, we present a loss function that takes into account the multi-label predictions of the network. As a result, the training approach for our ML-CapsNet uses a coarse to fine paradigm while maintaining consistency with the structure in the classification levels in the label-hierarchy. We also perform experiments using widely available datasets and compare the model with alternatives elsewhere in the literature. In our experiments, our ML-CapsNet yields a margin of improvement with respect to these alternative methods.



### Defense against Privacy Leakage in Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.05724v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.05724v1)
- **Published**: 2022-09-13 04:19:35+00:00
- **Updated**: 2022-09-13 04:19:35+00:00
- **Authors**: Jing Wu, Munawar Hayat, Mingyi Zhou, Mehrtash Harandi
- **Comment**: Defence against model inversion attack in federated learning
- **Journal**: None
- **Summary**: Federated Learning (FL) provides a promising distributed learning paradigm, since it seeks to protect users privacy by not sharing their private training data. Recent research has demonstrated, however, that FL is susceptible to model inversion attacks, which can reconstruct users' private data by eavesdropping on shared gradients. Existing defense solutions cannot survive stronger attacks and exhibit a poor trade-off between privacy and performance. In this paper, we present a straightforward yet effective defense strategy based on obfuscating the gradients of sensitive data with concealing data. Specifically, we alter a few samples within a mini batch to mimic the sensitive data at the gradient levels. Using a gradient projection technique, our method seeks to obscure sensitive data without sacrificing FL performance. Our extensive evaluations demonstrate that, compared to other defenses, our technique offers the highest level of protection while preserving FL performance. Our source code is located in the repository.



### A Guide to Employ Hyperspectral Imaging for Assessing Wheat Quality at Different Stages of Supply Chain in Australia: A Review
- **Arxiv ID**: http://arxiv.org/abs/2209.05727v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.05727v1)
- **Published**: 2022-09-13 04:30:40+00:00
- **Updated**: 2022-09-13 04:30:40+00:00
- **Authors**: Priyabrata Karmakar, Shyh Wei Teng. Manzur Murshed, Paul Pang, Cuong Van Bui
- **Comment**: None
- **Journal**: None
- **Summary**: Wheat is one of the major staple crops across the globe. Therefore, it is mandatory to measure, maintain and improve the wheat quality for human consumption. Traditional wheat quality measurement methods are mostly invasive, destructive and limited to small samples of wheat. In a typical supply chain of wheat, there are many receival points where bulk wheat arrives, gets stored and forwarded as per the requirements. In this receival points, the application of traditional quality measurement methods is difficult and often very expensive. Therefore, there is a need for non-invasive, non-destructive real-time methods for wheat quality assessments. One such method that fulfils the above-mentioned criteria is hyperspectral imaging (HSI) for food quality measurement and it can also be applied to bulk samples. In this paper, we have investigated how HSI has been used in the literature for assessing stored wheat quality. So that the required information to implement real-time digital quality assessment methods at the different stages of Australian supply chain can be made available in a single and compact document.



### CMR3D: Contextualized Multi-Stage Refinement for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.06641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.06641v1)
- **Published**: 2022-09-13 05:26:09+00:00
- **Updated**: 2022-09-13 05:26:09+00:00
- **Authors**: Dhanalaxmi Gaddam, Jean Lahoud, Fahad Shahbaz Khan, Rao Muhammad Anwer, Hisham Cholakkal
- **Comment**: 5 figures, 10 pages including references
- **Journal**: None
- **Summary**: Existing deep learning-based 3D object detectors typically rely on the appearance of individual objects and do not explicitly pay attention to the rich contextual information of the scene. In this work, we propose Contextualized Multi-Stage Refinement for 3D Object Detection (CMR3D) framework, which takes a 3D scene as input and strives to explicitly integrate useful contextual information of the scene at multiple levels to predict a set of object bounding-boxes along with their corresponding semantic labels. To this end, we propose to utilize a context enhancement network that captures the contextual information at different levels of granularity followed by a multi-stage refinement module to progressively refine the box positions and class predictions. Extensive experiments on the large-scale ScanNetV2 benchmark reveal the benefits of our proposed method, leading to an absolute improvement of 2.0% over the baseline. In addition to 3D object detection, we investigate the effectiveness of our CMR3D framework for the problem of 3D object counting. Our source code will be publicly released.



### OCR for TIFF Compressed Document Images Directly in Compressed Domain Using Text segmentation and Hidden Markov Model
- **Arxiv ID**: http://arxiv.org/abs/2209.09118v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.09118v1)
- **Published**: 2022-09-13 06:34:26+00:00
- **Updated**: 2022-09-13 06:34:26+00:00
- **Authors**: Dikshit Sharma, Mohammed Javed
- **Comment**: The paper has 14 figures and 1 table
- **Journal**: None
- **Summary**: In today's technological era, document images play an important and integral part in our day to day life, and specifically with the surge of Covid-19, digitally scanned documents have become key source of communication, thus avoiding any sort of infection through physical contact. Storage and transmission of scanned document images is a very memory intensive task, hence compression techniques are being used to reduce the image size before archival and transmission. To extract information or to operate on the compressed images, we have two ways of doing it. The first way is to decompress the image and operate on it and subsequently compress it again for the efficiency of storage and transmission. The other way is to use the characteristics of the underlying compression algorithm to directly process the images in their compressed form without involving decompression and re-compression. In this paper, we propose a novel idea of developing an OCR for CCITT (The International Telegraph and Telephone Consultative Committee) compressed machine printed TIFF document images directly in the compressed domain. After segmenting text regions into lines and words, HMM is applied for recognition using three coding modes of CCITT- horizontal, vertical and the pass mode. Experimental results show that OCR on pass modes give a promising results.



### Moving from 2D to 3D: volumetric medical image classification for rectal cancer staging
- **Arxiv ID**: http://arxiv.org/abs/2209.05771v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.05771v1)
- **Published**: 2022-09-13 07:10:14+00:00
- **Updated**: 2022-09-13 07:10:14+00:00
- **Authors**: Joohyung Lee, Jieun Oh, Inkyu Shin, You-sung Kim, Dae Kyung Sohn, Tae-sung Kim, In So Kweon
- **Comment**: 11 pages, 2 figures, accepted to MICCAI 2022
- **Journal**: None
- **Summary**: Volumetric images from Magnetic Resonance Imaging (MRI) provide invaluable information in preoperative staging of rectal cancer. Above all, accurate preoperative discrimination between T2 and T3 stages is arguably both the most challenging and clinically significant task for rectal cancer treatment, as chemo-radiotherapy is usually recommended to patients with T3 (or greater) stage cancer. In this study, we present a volumetric convolutional neural network to accurately discriminate T2 from T3 stage rectal cancer with rectal MR volumes. Specifically, we propose 1) a custom ResNet-based volume encoder that models the inter-slice relationship with late fusion (i.e., 3D convolution at the last layer), 2) a bilinear computation that aggregates the resulting features from the encoder to create a volume-wise feature, and 3) a joint minimization of triplet loss and focal loss. With MR volumes of pathologically confirmed T2/T3 rectal cancer, we perform extensive experiments to compare various designs within the framework of residual learning. As a result, our network achieves an AUC of 0.831, which is higher than the reported accuracy of the professional radiologist groups. We believe this method can be extended to other volume analysis tasks



### CAIBC: Capturing All-round Information Beyond Color for Text-based Person Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2209.05773v1
- **DOI**: 10.1145/3503161.3548057
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2209.05773v1)
- **Published**: 2022-09-13 07:10:58+00:00
- **Updated**: 2022-09-13 07:10:58+00:00
- **Authors**: Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu, Tian Wang, Yifeng Li
- **Comment**: Accepted on ACM MM '22
- **Journal**: None
- **Summary**: Given a natural language description, text-based person retrieval aims to identify images of a target person from a large-scale person image database. Existing methods generally face a \textbf{color over-reliance problem}, which means that the models rely heavily on color information when matching cross-modal data. Indeed, color information is an important decision-making accordance for retrieval, but the over-reliance on color would distract the model from other key clues (e.g. texture information, structural information, etc.), and thereby lead to a sub-optimal retrieval performance. To solve this problem, in this paper, we propose to \textbf{C}apture \textbf{A}ll-round \textbf{I}nformation \textbf{B}eyond \textbf{C}olor (\textbf{CAIBC}) via a jointly optimized multi-branch architecture for text-based person retrieval. CAIBC contains three branches including an RGB branch, a grayscale (GRS) branch and a color (CLR) branch. Besides, with the aim of making full use of all-round information in a balanced and effective way, a mutual learning mechanism is employed to enable the three branches which attend to varied aspects of information to communicate with and learn from each other. Extensive experimental analysis is carried out to evaluate our proposed CAIBC method on the CUHK-PEDES and RSTPReid datasets in both \textbf{supervised} and \textbf{weakly supervised} text-based person retrieval settings, which demonstrates that CAIBC significantly outperforms existing methods and achieves the state-of-the-art performance on all the three tasks.



### PointScatter: Point Set Representation for Tubular Structure Extraction
- **Arxiv ID**: http://arxiv.org/abs/2209.05774v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.05774v1)
- **Published**: 2022-09-13 07:14:30+00:00
- **Updated**: 2022-09-13 07:14:30+00:00
- **Authors**: Dong Wang, Zhao Zhang, Ziwei Zhao, Yuhang Liu, Yihong Chen, Liwei Wang
- **Comment**: ECCV2022 (Oral)
- **Journal**: None
- **Summary**: This paper explores the point set representation for tubular structure extraction tasks. Compared with the traditional mask representation, the point set representation enjoys its flexibility and representation ability, which would not be restricted by the fixed grid as the mask. Inspired by this, we propose PointScatter, an alternative to the segmentation models for the tubular structure extraction task. PointScatter splits the image into scatter regions and parallelly predicts points for each scatter region. We further propose the greedy-based region-wise bipartite matching algorithm to train the network end-to-end and efficiently. We benchmark the PointScatter on four public tubular datasets, and the extensive experiments on tubular structure segmentation and centerline extraction task demonstrate the effectiveness of our approach. Code is available at https://github.com/zhangzhao2022/pointscatter.



### Exemplar-Based Image Colorization with A Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/2209.05775v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.05775v1)
- **Published**: 2022-09-13 07:15:25+00:00
- **Updated**: 2022-09-13 07:15:25+00:00
- **Authors**: Zhenfeng Xue, Jiandang Yang, Jie Ren, Yong Liu
- **Comment**: 17 pages, 12 figures
- **Journal**: None
- **Summary**: Image learning and colorization are hot spots in multimedia domain. Inspired by the learning capability of humans, in this paper, we propose an automatic colorization method with a learning framework. This method can be viewed as a hybrid of exemplar-based and learning-based method, and it decouples the colorization process and learning process so as to generate various color styles for the same gray image. The matching process in the exemplar-based colorization method can be regarded as a parameterized function, and we employ a large amount of color images as the training samples to fit the parameters. During the training process, the color images are the ground truths, and we learn the optimal parameters for the matching process by minimizing the errors in terms of the parameters for the matching function. To deal with images with various compositions, a global feature is introduced, which can be used to classify the images with respect to their compositions, and then learn the optimal matching parameters for each image category individually. What's more, a spatial consistency based post-processing is design to smooth the extracted color information from the reference image to remove matching errors. Extensive experiments are conducted to verify the effectiveness of the method, and it achieves comparable performance against the state-of-the-art colorization algorithms.



### A lightweight Transformer-based model for fish landmark detection
- **Arxiv ID**: http://arxiv.org/abs/2209.05777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.05777v1)
- **Published**: 2022-09-13 07:18:57+00:00
- **Updated**: 2022-09-13 07:18:57+00:00
- **Authors**: Alzayat Saleh, David Jones, Dean Jerry, Mostafa Rahimi Azghadi
- **Comment**: 9 pages, 8 figures. Submitted to the Computers and Electronics in
  Agriculture journal
- **Journal**: None
- **Summary**: Transformer-based models, such as the Vision Transformer (ViT), can outperform onvolutional Neural Networks (CNNs) in some vision tasks when there is sufficient training data. However, (CNNs) have a strong and useful inductive bias for vision tasks (i.e. translation equivariance and locality). In this work, we developed a novel model architecture that we call a Mobile fish landmark detection network (MFLD-net). We have made this model using convolution operations based on ViT (i.e. Patch embeddings, Multi-Layer Perceptrons). MFLD-net can achieve competitive or better results in low data regimes while being lightweight and therefore suitable for embedded and mobile devices. Furthermore, we show that MFLD-net can achieve keypoint (landmark) estimation accuracies on-par or even better than some of the state-of-the-art (CNNs) on a fish image dataset. Additionally, unlike ViT, MFLD-net does not need a pre-trained model and can generalise well when trained on a small dataset. We provide quantitative and qualitative results that demonstrate the model's generalisation capabilities. This work will provide a foundation for future efforts in developing mobile, but efficient fish monitoring systems and devices.



### Look Before You Leap: Improving Text-based Person Retrieval by Learning A Consistent Cross-modal Common Manifold
- **Arxiv ID**: http://arxiv.org/abs/2209.06209v1
- **DOI**: 10.1145/3503161.3548166
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2209.06209v1)
- **Published**: 2022-09-13 07:21:21+00:00
- **Updated**: 2022-09-13 07:21:21+00:00
- **Authors**: Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu, Tian Wang, Yifeng Li
- **Comment**: Accepted on ACM MM '22. arXiv admin note: text overlap with
  arXiv:2209.05773
- **Journal**: None
- **Summary**: The core problem of text-based person retrieval is how to bridge the heterogeneous gap between multi-modal data. Many previous approaches contrive to learning a latent common manifold mapping paradigm following a \textbf{cross-modal distribution consensus prediction (CDCP)} manner. When mapping features from distribution of one certain modality into the common manifold, feature distribution of the opposite modality is completely invisible. That is to say, how to achieve a cross-modal distribution consensus so as to embed and align the multi-modal features in a constructed cross-modal common manifold all depends on the experience of the model itself, instead of the actual situation. With such methods, it is inevitable that the multi-modal data can not be well aligned in the common manifold, which finally leads to a sub-optimal retrieval performance. To overcome this \textbf{CDCP dilemma}, we propose a novel algorithm termed LBUL to learn a Consistent Cross-modal Common Manifold (C$^{3}$M) for text-based person retrieval. The core idea of our method, just as a Chinese saying goes, is to `\textit{san si er hou xing}', namely, to \textbf{Look Before yoU Leap (LBUL)}. The common manifold mapping mechanism of LBUL contains a looking step and a leaping step. Compared to CDCP-based methods, LBUL considers distribution characteristics of both the visual and textual modalities before embedding data from one certain modality into C$^{3}$M to achieve a more solid cross-modal distribution consensus, and hence achieve a superior retrieval accuracy. We evaluate our proposed method on two text-based person retrieval datasets CUHK-PEDES and RSTPReid. Experimental results demonstrate that the proposed LBUL outperforms previous methods and achieves the state-of-the-art performance.



### Self-supervised motion descriptor for cardiac phase detection in 4D CMR based on discrete vector field estimations
- **Arxiv ID**: http://arxiv.org/abs/2209.05778v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.05778v2)
- **Published**: 2022-09-13 07:23:17+00:00
- **Updated**: 2022-09-18 07:17:37+00:00
- **Authors**: Sven Koehler, Tarique Hussain, Hamza Hussain, Daniel Young, Samir Sarikouch, Thomas Pickhardt, Gerald Greil, Sandy Engelhardt
- **Comment**: accepted for the STACOM2022 workshop @ MICCAI2022
- **Journal**: None
- **Summary**: Cardiac magnetic resonance (CMR) sequences visualise the cardiac function voxel-wise over time. Simultaneously, deep learning-based deformable image registration is able to estimate discrete vector fields which warp one time step of a CMR sequence to the following in a self-supervised manner. However, despite the rich source of information included in these 3D+t vector fields, a standardised interpretation is challenging and the clinical applications remain limited so far. In this work, we show how to efficiently use a deformable vector field to describe the underlying dynamic process of a cardiac cycle in form of a derived 1D motion descriptor. Additionally, based on the expected cardiovascular physiological properties of a contracting or relaxing ventricle, we define a set of rules that enables the identification of five cardiovascular phases including the end-systole (ES) and end-diastole (ED) without the usage of labels. We evaluate the plausibility of the motion descriptor on two challenging multi-disease, -center, -scanner short-axis CMR datasets. First, by reporting quantitative measures such as the periodic frame difference for the extracted phases. Second, by comparing qualitatively the general pattern when we temporally resample and align the motion descriptors of all instances across both datasets. The average periodic frame difference for the ED, ES key phases of our approach is $0.80\pm{0.85}$, $0.69\pm{0.79}$ which is slightly better than the inter-observer variability ($1.07\pm{0.86}$, $0.91\pm{1.6}$) and the supervised baseline method ($1.18\pm{1.91}$, $1.21\pm{1.78}$). Code and labels will be made available on our GitHub repository. https://github.com/Cardio-AI/cmr-phase-detection



### Test-Time Adaptation with Principal Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/2209.05779v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.05779v1)
- **Published**: 2022-09-13 07:24:40+00:00
- **Updated**: 2022-09-13 07:24:40+00:00
- **Authors**: Thomas Cordier, Victor Bouvier, Gilles Hénaff, Céline Hudelot
- **Comment**: 7 pages, 2 figures, 2 tables, accepted at Workshop on Trustworthy
  Artificial Intelligence in conjunction with ECML/PKDD 22
- **Journal**: None
- **Summary**: Machine Learning models are prone to fail when test data are different from training data, a situation often encountered in real applications known as distribution shift. While still valid, the training-time knowledge becomes less effective, requiring a test-time adaptation to maintain high performance. Following approaches that assume batch-norm layer and use their statistics for adaptation, we propose a Test-Time Adaptation with Principal Component Analysis (TTAwPCA), which presumes a fitted PCA and adapts at test time a spectral filter based on the singular values of the PCA for robustness to corruptions. TTAwPCA combines three components: the output of a given layer is decomposed using a Principal Component Analysis (PCA), filtered by a penalization of its singular values, and reconstructed with the PCA inverse transform. This generic enhancement adds fewer parameters than current methods. Experiments on CIFAR-10-C and CIFAR- 100-C demonstrate the effectiveness and limits of our method using a unique filter of 2000 parameters.



### Adversarial Coreset Selection for Efficient Robust Training
- **Arxiv ID**: http://arxiv.org/abs/2209.05785v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.05785v2)
- **Published**: 2022-09-13 07:37:53+00:00
- **Updated**: 2023-08-08 06:06:35+00:00
- **Authors**: Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie
- **Comment**: Accepted to the International Journal of Computer Vision (IJCV).
  Extended version of the ECCV2022 paper: arXiv:2112.00378. arXiv admin note:
  substantial text overlap with arXiv:2112.00378
- **Journal**: None
- **Summary**: Neural networks are vulnerable to adversarial attacks: adding well-crafted, imperceptible perturbations to their input can modify their output. Adversarial training is one of the most effective approaches to training robust models against such attacks. Unfortunately, this method is much slower than vanilla training of neural networks since it needs to construct adversarial examples for the entire training data at every iteration. By leveraging the theory of coreset selection, we show how selecting a small subset of training data provides a principled approach to reducing the time complexity of robust training. To this end, we first provide convergence guarantees for adversarial coreset selection. In particular, we show that the convergence bound is directly related to how well our coresets can approximate the gradient computed over the entire training data. Motivated by our theoretical analysis, we propose using this gradient approximation error as our adversarial coreset selection objective to reduce the training set size effectively. Once built, we run adversarial training over this subset of the training data. Unlike existing methods, our approach can be adapted to a wide variety of training objectives, including TRADES, $\ell_p$-PGD, and Perceptual Adversarial Training. We conduct extensive experiments to demonstrate that our approach speeds up adversarial training by 2-3 times while experiencing a slight degradation in the clean and robust accuracy.



### SFS-A68: a dataset for the segmentation of space functions in apartment buildings
- **Arxiv ID**: http://arxiv.org/abs/2209.09094v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.09094v1)
- **Published**: 2022-09-13 07:49:54+00:00
- **Updated**: 2022-09-13 07:49:54+00:00
- **Authors**: Amir Ziaee, Georg Suter
- **Comment**: Published in proceedings of the 29th International Workshop on
  Intelligent Computing in Engineering, EG-ICE 2022, Aarhus, Denmark.
  https://doi.org/10.7146/aul.455.c222
- **Journal**: Teizer, Jochen & Schultz, Carl. (2022). Proceedings of the 29th
  EG-ICE International Workshop on Intelligent Computing in Engineering:
  Frontmatter and Backmatter. 1-8. 10.7146/aul.455.c191
- **Summary**: Analyzing building models for usable area, building safety, or energy analysis requires function classification data of spaces and related objects. Automated space function classification is desirable to reduce input model preparation effort and errors. Existing space function classifiers use space feature vectors or space connectivity graphs as input. The application of deep learning (DL) image segmentation methods to space function classification has not been studied. As an initial step towards addressing this gap, we present a dataset, SFS-A68, that consists of input and ground truth images generated from 68 digital 3D models of space layouts of apartment buildings. The dataset is suitable for developing DL models for space function segmentation. We use the dataset to train and evaluate an experimental space function segmentation network based on transfer learning and training from scratch. Test results confirm the applicability of DL image segmentation for space function classification. The code and the dataset of the experiments are publicly available online (https://github.com/A2Amir/SFS-A68).



### Time-of-Day Neural Style Transfer for Architectural Photographs
- **Arxiv ID**: http://arxiv.org/abs/2209.05800v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2209.05800v2)
- **Published**: 2022-09-13 08:00:33+00:00
- **Updated**: 2022-10-28 03:00:14+00:00
- **Authors**: Yingshu Chen, Tuan-Anh Vu, Ka-Chun Shum, Binh-Son Hua, Sai-Kit Yeung
- **Comment**: Updated version with corrected equations. Paper published at the
  International Conference on Computational Photography (ICCP) 2022. 12 pages
  of content with 6 pages of supplementary materials
- **Journal**: None
- **Summary**: Architectural photography is a genre of photography that focuses on capturing a building or structure in the foreground with dramatic lighting in the background. Inspired by recent successes in image-to-image translation methods, we aim to perform style transfer for architectural photographs. However, the special composition in architectural photography poses great challenges for style transfer in this type of photographs. Existing neural style transfer methods treat the architectural images as a single entity, which would generate mismatched chrominance and destroy geometric features of the original architecture, yielding unrealistic lighting, wrong color rendition, and visual artifacts such as ghosting, appearance distortion, or color mismatching. In this paper, we specialize a neural style transfer method for architectural photography. Our method addresses the composition of the foreground and background in an architectural photograph in a two-branch neural network that separately considers the style transfer of the foreground and the background, respectively. Our method comprises a segmentation module, a learning-based image-to-image translation module, and an image blending optimization module. We trained our image-to-image translation neural network with a new dataset of unconstrained outdoor architectural photographs captured at different magic times of a day, utilizing additional semantic information for better chrominance matching and geometry preservation. Our experiments show that our method can produce photorealistic lighting and color rendition on both the foreground and background, and outperforms general image-to-image translation and arbitrary style transfer baselines quantitatively and qualitatively. Our code and data are available at https://github.com/hkust-vgd/architectural_style_transfer.



### Analyzing the Impact of Varied Window Hyper-parameters on Deep CNN for sEMG based Motion Intent Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.05804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.05804v1)
- **Published**: 2022-09-13 08:14:49+00:00
- **Updated**: 2022-09-13 08:14:49+00:00
- **Authors**: Frank Kulwa, Oluwarotimi Williams Samuel, Mojisola Grace Asogbon, Olumide Olayinka Obe, Guanglin Li
- **Comment**: None
- **Journal**: None
- **Summary**: The use of deep neural networks in electromyogram (EMG) based prostheses control provides a promising alternative to the hand-crafted features by automatically learning muscle activation patterns from the EMG signals. Meanwhile, the use of raw EMG signals as input to convolution neural networks (CNN) offers a simple, fast, and ideal scheme for effective control of prostheses. Therefore, this study investigates the relationship between window length and overlap, which may influence the generation of robust raw EMG 2-dimensional (2D) signals for application in CNN. And a rule of thumb for a proper combination of these parameters that could guarantee optimal network performance was derived. Moreover, we investigate the relationship between the CNN receptive window size and the raw EMG signal size. Experimental results show that the performance of the CNN increases with the increase in overlap within the generated signals, with the highest improvement of 9.49% accuracy and 23.33% F1-score realized when the overlap is 75% of the window length. Similarly, the network performance increases with the increase in receptive window (kernel) size. Findings from this study suggest that a combination of 75% overlap in 2D EMG signals and wider network kernels may provide ideal motor intents classification for adequate EMG-CNN based prostheses control scheme.



### Check and Link: Pairwise Lesion Correspondence Guides Mammogram Mass Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.05809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.05809v1)
- **Published**: 2022-09-13 08:26:07+00:00
- **Updated**: 2022-09-13 08:26:07+00:00
- **Authors**: Ziwei Zhao, Dong Wang, Yihong Chen, Ziteng Wang, Liwei Wang
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Detecting mass in mammogram is significant due to the high occurrence and mortality of breast cancer. In mammogram mass detection, modeling pairwise lesion correspondence explicitly is particularly important. However, most of the existing methods build relatively coarse correspondence and have not utilized correspondence supervision. In this paper, we propose a new transformer-based framework CL-Net to learn lesion detection and pairwise correspondence in an end-to-end manner. In CL-Net, View-Interactive Lesion Detector is proposed to achieve dynamic interaction across candidates of cross views, while Lesion Linker employs the correspondence supervision to guide the interaction process more accurately. The combination of these two designs accomplishes precise understanding of pairwise lesion correspondence for mammograms. Experiments show that CL-Net yields state-of-the-art performance on the public DDSM dataset and our in-house dataset. Moreover, it outperforms previous methods by a large margin in low FPI regime.



### CPnP: Consistent Pose Estimator for Perspective-n-Point Problem with Bias Elimination
- **Arxiv ID**: http://arxiv.org/abs/2209.05824v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.05824v1)
- **Published**: 2022-09-13 09:00:58+00:00
- **Updated**: 2022-09-13 09:00:58+00:00
- **Authors**: Guangyang Zeng, Shiyu Chen, Biqiang Mu, Guodong Shi, Junfeng Wu
- **Comment**: None
- **Journal**: None
- **Summary**: The Perspective-n-Point (PnP) problem has been widely studied in both computer vision and photogrammetry societies. With the development of feature extraction techniques, a large number of feature points might be available in a single shot. It is promising to devise a consistent estimator, i.e., the estimate can converge to the true camera pose as the number of points increases. To this end, we propose a consistent PnP solver, named \emph{CPnP}, with bias elimination. Specifically, linear equations are constructed from the original projection model via measurement model modification and variable elimination, based on which a closed-form least-squares solution is obtained. We then analyze and subtract the asymptotic bias of this solution, resulting in a consistent estimate. Additionally, Gauss-Newton (GN) iterations are executed to refine the consistent solution. Our proposed estimator is efficient in terms of computations -- it has $O(n)$ computational complexity. Experimental tests on both synthetic data and real images show that our proposed estimator is superior to some well-known ones for images with dense visual features, in terms of estimation precision and computing time.



### Computer vision system to count crustacean larvae
- **Arxiv ID**: http://arxiv.org/abs/2209.05834v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.05834v1)
- **Published**: 2022-09-13 09:18:13+00:00
- **Updated**: 2022-09-13 09:18:13+00:00
- **Authors**: Chen Rothschild
- **Comment**: None
- **Journal**: None
- **Summary**: Fish products account for about 16 percent of the human diet worldwide, as of 2017. The counting action is a significant component in growing and producing these products. Growers must count the fish accurately, to do so technological solutions are needed. Two computer vision systems to automatically count crustacean larvae grown in industrial ponds were developed. The first system included an iPhone 11 camera with 3024X4032 resolution which acquired images from an industrial pond in indoor conditions. Two experiments were performed with this system, the first one included 200 images acquired in one day on growth stages 9,10 with an iPhone 11 camera on specific illumination condition. In the second experiment, a larvae industrial pond was photographed for 11 days with two devices an iPhone 11 and a SONY DSCHX90V cameras. With the first device (iPhone 11) two illumination conditions were tested. In each condition, 110 images were acquired. That system resulted in an accuracy of 88.4 percent image detection. The second system included a DSLR Nikon D510 camera with a 2000X2000 resolution with which seven experiments were performed outside the industrial pond. Images were acquired on day 1 of larvae growing stage resulting in the acquisition of a total of 700 images. That system resulted in an accuracy of 86 percent for a density of 50. An algorithm that automatically counts the number of larvae was developed for both cases based on the YOLOv5 CNN model. In addition, in this study, a larvae growth function was developed. Daily, several larvae were taken manually from the industrial pond and analyzed under a microscope. Once the growth stage was determined, images of the larva were acquired. Each larva's length was measured manually from the images. The most suitable model was the Gompertz model with a goodness of fit index of R squared of 0.983.



### Skin Lesion Recognition with Class-Hierarchy Regularized Hyperbolic Embeddings
- **Arxiv ID**: http://arxiv.org/abs/2209.05842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.05842v1)
- **Published**: 2022-09-13 09:39:37+00:00
- **Updated**: 2022-09-13 09:39:37+00:00
- **Authors**: Zhen Yu, Toan Nguyen, Yaniv Gal, Lie Ju, Shekhar S. Chandra, Lei Zhang, Paul Bonnington, Victoria Mar, Zhiyong Wang, Zongyuan Ge
- **Comment**: in The 25th International Conference on Medical Image Computing and
  Computer Assisted Intervention
- **Journal**: None
- **Summary**: In practice, many medical datasets have an underlying taxonomy defined over the disease label space. However, existing classification algorithms for medical diagnoses often assume semantically independent labels. In this study, we aim to leverage class hierarchy with deep learning algorithms for more accurate and reliable skin lesion recognition. We propose a hyperbolic network to learn image embeddings and class prototypes jointly. The hyperbola provably provides a space for modeling hierarchical relations better than Euclidean geometry. Meanwhile, we restrict the distribution of hyperbolic prototypes with a distance matrix that is encoded from the class hierarchy. Accordingly, the learned prototypes preserve the semantic class relations in the embedding space and we can predict the label of an image by assigning its feature to the nearest hyperbolic class prototype. We use an in-house skin lesion dataset which consists of around 230k dermoscopic images on 65 skin diseases to verify our method. Extensive experiments provide evidence that our model can achieve higher accuracy with less severe classification errors than models without considering class relations.



### Just Noticeable Difference Modeling for Face Recognition System
- **Arxiv ID**: http://arxiv.org/abs/2209.05856v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.05856v1)
- **Published**: 2022-09-13 10:06:36+00:00
- **Updated**: 2022-09-13 10:06:36+00:00
- **Authors**: Yu Tian, Zhangkai Ni, Baoliang Chen, Shurun Wang, Shiqi Wang, Hanli Wang, Sam Kwong
- **Comment**: None
- **Journal**: None
- **Summary**: High-quality face images are required to guarantee the stability and reliability of automatic face recognition (FR) systems in surveillance and security scenarios. However, a massive amount of face data is usually compressed before being analyzed due to limitations on transmission or storage. The compressed images may lose the powerful identity information, resulting in the performance degradation of the FR system. Herein, we make the first attempt to study just noticeable difference (JND) for the FR system, which can be defined as the maximum distortion that the FR system cannot notice. More specifically, we establish a JND dataset including 3530 original images and 137,670 compressed images generated by advanced reference encoding/decoding software based on the Versatile Video Coding (VVC) standard (VTM-15.0). Subsequently, we develop a novel JND prediction model to directly infer JND images for the FR system. In particular, in order to maximum redundancy removal without impairment of robust identity information, we apply the encoder with multiple feature extraction and attention-based feature decomposition modules to progressively decompose face features into two uncorrelated components, i.e., identity and residual features, via self-supervised learning. Then, the residual feature is fed into the decoder to generate the residual map. Finally, the predicted JND map is obtained by subtracting the residual map from the original image. Experimental results have demonstrated that the proposed model achieves higher accuracy of JND map prediction compared with the state-of-the-art JND models, and is capable of saving more bits while maintaining the performance of the FR system compared with VTM-15.0.



### Binaural Signal Representations for Joint Sound Event Detection and Acoustic Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.05900v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2209.05900v1)
- **Published**: 2022-09-13 11:29:00+00:00
- **Updated**: 2022-09-13 11:29:00+00:00
- **Authors**: Daniel Aleksander Krause, Annamaria Mesaros
- **Comment**: None
- **Journal**: None
- **Summary**: Sound event detection (SED) and Acoustic scene classification (ASC) are two widely researched audio tasks that constitute an important part of research on acoustic scene analysis. Considering shared information between sound events and acoustic scenes, performing both tasks jointly is a natural part of a complex machine listening system. In this paper, we investigate the usefulness of several spatial audio features in training a joint deep neural network (DNN) model performing SED and ASC. Experiments are performed for two different datasets containing binaural recordings and synchronous sound event and acoustic scene labels to analyse the differences between performing SED and ASC separately or jointly. The presented results show that the use of specific binaural features, mainly the Generalized Cross Correlation with Phase Transform (GCC-phat) and sines and cosines of phase differences, result in a better performing model in both separate and joint tasks as compared with baseline methods based on logmel energies only.



### Realistic Hair Synthesis with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.12875v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.12875v1)
- **Published**: 2022-09-13 11:48:26+00:00
- **Updated**: 2022-09-13 11:48:26+00:00
- **Authors**: Muhammed Pektas, Aybars Ugur
- **Comment**: Master Thesis, in Turkish language
- **Journal**: None
- **Summary**: Recent successes in generative modeling have accelerated studies on this subject and attracted the attention of researchers. One of the most important methods used to achieve this success is Generative Adversarial Networks (GANs). It has many application areas such as; virtual reality (VR), augmented reality (AR), super resolution, image enhancement. Despite the recent advances in hair synthesis and style transfer using deep learning and generative modelling, due to the complex nature of hair still contains unsolved challenges. The methods proposed in the literature to solve this problem generally focus on making high-quality hair edits on images. In this thesis, a generative adversarial network method is proposed to solve the hair synthesis problem. While developing this method, it is aimed to achieve real-time hair synthesis while achieving visual outputs that compete with the best methods in the literature. The proposed method was trained with the FFHQ dataset and then its results in hair style transfer and hair reconstruction tasks were evaluated. The results obtained in these tasks and the operating time of the method were compared with MichiGAN, one of the best methods in the literature. The comparison was made at a resolution of 128x128. As a result of the comparison, it has been shown that the proposed method achieves competitive results with MichiGAN in terms of realistic hair synthesis, and performs better in terms of operating time.



### Computer vision based vehicle tracking as a complementary and scalable approach to RFID tagging
- **Arxiv ID**: http://arxiv.org/abs/2209.05911v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.05911v1)
- **Published**: 2022-09-13 11:49:38+00:00
- **Updated**: 2022-09-13 11:49:38+00:00
- **Authors**: Pranav Kant Gaur, Abhilash Bhardwaj, Pritam Shete, Mohini Laghate, Dinesh M Sarode
- **Comment**: None
- **Journal**: None
- **Summary**: Logging of incoming/outgoing vehicles serves as a piece of critical information for root-cause analysis to combat security breach incidents in various sensitive organizations. RFID tagging hampers the scalability of vehicle tracking solutions on both logistics as well as technical fronts. For instance, requiring each incoming vehicle(departmental or private) to be RFID tagged is a severe constraint and coupling video analytics with RFID to detect abnormal vehicle movement is non-trivial. We leverage publicly available implementations of computer vision algorithms to develop an interpretable vehicle tracking algorithm using finite-state machine formalism. The state-machine consumes input from the cascaded object detection and optical character recognition(OCR) models for state transitions. We evaluated the proposed method on 75 video clips of 285 vehicles from our system deployment site. We observed that the detection rate is most affected by the speed and the type of vehicle. The highest detection rate is achieved when the vehicle movement is restricted to follow a movement restrictions(SOP) at the checkpoint similar to RFID tagging. We further analyzed 700 vehicle tracking predictions on live-data and identified that the majority of vehicle number prediction errors are due to illegible-text, image-blur, text occlusion and out-of-vocab letters in vehicle numbers. Towards system deployment and performance enhancement, we expect our ongoing system monitoring to provide evidences to establish a higher vehicle-throughput SOP at the security checkpoint as well as to drive the fine-tuning of the deployed computer-vision models and the state-machine to establish the proposed approach as a promising alternative to RFID-tagging.



### Dual-Scale Single Image Dehazing Via Neural Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.05913v1
- **DOI**: 10.1109/TIP.2022.3207571
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.05913v1)
- **Published**: 2022-09-13 11:56:03+00:00
- **Updated**: 2022-09-13 11:56:03+00:00
- **Authors**: Zhengguo Li, Chaobing Zheng, Haiyan Shu, Shiqian Wu
- **Comment**: Single image dehazing, dual-scale, neural augmentation, haze line
  averaging, generative adversarial network. arXiv admin note: substantial text
  overlap with arXiv:2111.10943
- **Journal**: None
- **Summary**: Model-based single image dehazing algorithms restore haze-free images with sharp edges and rich details for real-world hazy images at the expense of low PSNR and SSIM values for synthetic hazy images. Data-driven ones restore haze-free images with high PSNR and SSIM values for synthetic hazy images but with low contrast, and even some remaining haze for real world hazy images. In this paper, a novel single image dehazing algorithm is introduced by combining model-based and data-driven approaches. Both transmission map and atmospheric light are first estimated by the model-based methods, and then refined by dual-scale generative adversarial networks (GANs) based approaches. The resultant algorithm forms a neural augmentation which converges very fast while the corresponding data-driven approach might not converge. Haze-free images are restored by using the estimated transmission map and atmospheric light as well as the Koschmiederlaw. Experimental results indicate that the proposed algorithm can remove haze well from real-world and synthetic hazy images.



### Document Image Binarization in JPEG Compressed Domain using Dual Discriminator Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.05921v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.05921v1)
- **Published**: 2022-09-13 12:07:32+00:00
- **Updated**: 2022-09-13 12:07:32+00:00
- **Authors**: Bulla Rajesh, Manav Kamlesh Agrawal, Milan Bhuva, Kisalaya Kishore, Mohammed Javed
- **Comment**: Accepted in IAPR endorsed first International Conference on Computer
  Vision and Machine Intelligence (CVMI2022), held at IIIT Allahabad
- **Journal**: None
- **Summary**: Image binarization techniques are being popularly used in enhancement of noisy and/or degraded images catering different Document Image Anlaysis (DIA) applications like word spotting, document retrieval, and OCR. Most of the existing techniques focus on feeding pixel images into the Convolution Neural Networks to accomplish document binarization, which may not produce effective results when working with compressed images that need to be processed without full decompression. Therefore in this research paper, the idea of document image binarization directly using JPEG compressed stream of document images is proposed by employing Dual Discriminator Generative Adversarial Networks (DD-GANs). Here the two discriminator networks - Global and Local work on different image ratios and use focal loss as generator loss. The proposed model has been thoroughly tested with different versions of DIBCO dataset having challenges like holes, erased or smudged ink, dust, and misplaced fibres. The model proved to be highly robust, efficient both in terms of time and space complexities, and also resulted in state-of-the-art performance in JPEG compressed domain.



### SVNet: Where SO(3) Equivariance Meets Binarization on Point Cloud Representation
- **Arxiv ID**: http://arxiv.org/abs/2209.05924v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.05924v2)
- **Published**: 2022-09-13 12:12:19+00:00
- **Updated**: 2022-09-20 20:58:17+00:00
- **Authors**: Zhuo Su, Max Welling, Matti Pietikäinen, Li Liu
- **Comment**: Accepted in 3DV 2022. 11 pages including the appendix
- **Journal**: None
- **Summary**: Efficiency and robustness are increasingly needed for applications on 3D point clouds, with the ubiquitous use of edge devices in scenarios like autonomous driving and robotics, which often demand real-time and reliable responses. The paper tackles the challenge by designing a general framework to construct 3D learning architectures with SO(3) equivariance and network binarization. However, a naive combination of equivariant networks and binarization either causes sub-optimal computational efficiency or geometric ambiguity. We propose to locate both scalar and vector features in our networks to avoid both cases. Precisely, the presence of scalar features makes the major part of the network binarizable, while vector features serve to retain rich structural information and ensure SO(3) equivariance. The proposed approach can be applied to general backbones like PointNet and DGCNN. Meanwhile, experiments on ModelNet40, ShapeNet, and the real-world dataset ScanObjectNN, demonstrated that the method achieves a great trade-off between efficiency, rotation robustness, and accuracy. The codes are available at https://github.com/zhuoinoulu/svnet.



### Weakly-Supervised Stitching Network for Real-World Panoramic Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2209.05968v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.05968v1)
- **Published**: 2022-09-13 13:01:47+00:00
- **Updated**: 2022-09-13 13:01:47+00:00
- **Authors**: Dae-Young Song, Geonsoo Lee, HeeKyung Lee, Gi-Mun Um, Donghyeon Cho
- **Comment**: Accepted by ECCV2022 (poster)
- **Journal**: None
- **Summary**: Recently, there has been growing attention on an end-to-end deep learning-based stitching model. However, the most challenging point in deep learning-based stitching is to obtain pairs of input images with a narrow field of view and ground truth images with a wide field of view captured from real-world scenes. To overcome this difficulty, we develop a weakly-supervised learning mechanism to train the stitching model without requiring genuine ground truth images. In addition, we propose a stitching model that takes multiple real-world fisheye images as inputs and creates a 360 output image in an equirectangular projection format. In particular, our model consists of color consistency corrections, warping, and blending, and is trained by perceptual and SSIM losses. The effectiveness of the proposed algorithm is verified on two real-world stitching datasets.



### Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.05980v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.05980v2)
- **Published**: 2022-09-13 13:24:22+00:00
- **Updated**: 2023-02-21 10:06:35+00:00
- **Authors**: Maksym Yatsura, Kaspar Sakmann, N. Grace Hua, Matthias Hein, Jan Hendrik Metzen
- **Comment**: accepted at ICLR 2023
- **Journal**: None
- **Summary**: Adversarial patch attacks are an emerging security threat for real world deep learning applications. We present Demasked Smoothing, the first approach (up to our knowledge) to certify the robustness of semantic segmentation models against this threat model. Previous work on certifiably defending against patch attacks has mostly focused on image classification task and often required changes in the model architecture and additional training which is undesirable and computationally expensive. In Demasked Smoothing, any segmentation model can be applied without particular training, fine-tuning, or restriction of the architecture. Using different masking strategies, Demasked Smoothing can be applied both for certified detection and certified recovery. In extensive experiments we show that Demasked Smoothing can on average certify 64% of the pixel predictions for a 1% patch in the detection task and 48% against a 0.5% patch for the recovery task on the ADE20K dataset.



### M$^2$-3DLaneNet: Exploring Multi-Modal 3D Lane Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.05996v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.05996v3)
- **Published**: 2022-09-13 13:45:18+00:00
- **Updated**: 2023-08-08 20:52:26+00:00
- **Authors**: Yueru Luo, Xu Yan, Chaoda Zheng, Chao Zheng, Shuqi Mei, Tang Kun, Shuguang Cui, Zhen Li
- **Comment**: update
- **Journal**: None
- **Summary**: Estimating accurate lane lines in 3D space remains challenging due to their sparse and slim nature. Previous works mainly focused on using images for 3D lane detection, leading to inherent projection error and loss of geometry information. To address these issues, we explore the potential of leveraging LiDAR for 3D lane detection, either as a standalone method or in combination with existing monocular approaches. In this paper, we propose M$^2$-3DLaneNet to integrate complementary information from multiple sensors. Specifically, M$^2$-3DLaneNet lifts 2D features into 3D space by incorporating geometry information from LiDAR data through depth completion. Subsequently, the lifted 2D features are further enhanced with LiDAR features through cross-modality BEV fusion. Extensive experiments on the large-scale OpenLane dataset demonstrate the effectiveness of M$^2$-3DLaneNet, regardless of the range (75m or 100m).



### Virtual Underwater Datasets for Autonomous Inspections
- **Arxiv ID**: http://arxiv.org/abs/2209.06013v2
- **DOI**: 10.3390/jmse10091289
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.06013v2)
- **Published**: 2022-09-13 14:06:36+00:00
- **Updated**: 2022-09-14 11:50:54+00:00
- **Authors**: Ioannis Polymenis, Maryam Haroutunian, Rose Norman, David Trodden
- **Comment**: 20 pages, 10 figures
- **Journal**: J. Mar. Sci. Eng. 2022, 10(9), 1289;
- **Summary**: Underwater Vehicles have become more sophisticated, driven by the off-shore sector and the scientific community's rapid advancements in underwater operations. Notably, many underwater tasks, including the assessment of subsea infrastructure, are performed with the assistance of Autonomous Underwater Vehicles (AUVs). There have been recent breakthroughs in Artificial Intelligence (AI) and, notably, Deep Learning (DL) models and applications, which have widespread usage in a variety of fields, including aerial unmanned vehicles, autonomous car navigation, and other applications. However, they are not as prevalent in underwater applications due to the difficulty of obtaining underwater datasets for a specific application. In this sense, the current study utilises recent advancements in the area of DL to construct a bespoke dataset generated from photographs of items captured in a laboratory environment. Generative Adversarial Networks (GANs) were utilised to translate the laboratory object dataset into the underwater domain by combining the collected images with photographs containing the underwater environment. The findings demonstrated the feasibility of creating such a dataset, since the resulting images closely resembled the real underwater environment when compared with real-world underwater ship hull images. Therefore, the artificial datasets of the underwater environment can overcome the difficulties arising from the limited access to real-world underwater images and are used to enhance underwater operations through underwater object image classification and detection.



### Two-Step Color-Polarization Demosaicking Network
- **Arxiv ID**: http://arxiv.org/abs/2209.06027v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.06027v1)
- **Published**: 2022-09-13 14:28:18+00:00
- **Updated**: 2022-09-13 14:28:18+00:00
- **Authors**: Vy Nguyen, Masayuki Tanaka, Yusuke Monno, Masatoshi Okutomi
- **Comment**: Accepted in ICIP2022. Project page:
  http://www.ok.sc.e.titech.ac.jp/res/PolarDem/TCPDNet.html
- **Journal**: None
- **Summary**: Polarization information of light in a scene is valuable for various image processing and computer vision tasks. A division-of-focal-plane polarimeter is a promising approach to capture the polarization images of different orientations in one shot, while it requires color-polarization demosaicking. In this paper, we propose a two-step color-polarization demosaicking network~(TCPDNet), which consists of two sub-tasks of color demosaicking and polarization demosaicking. We also introduce a reconstruction loss in the YCbCr color space to improve the performance of TCPDNet. Experimental comparisons demonstrate that TCPDNet outperforms existing methods in terms of the image quality of polarization images and the accuracy of Stokes parameters.



### DMTNet: Dynamic Multi-scale Network for Dual-pixel Images Defocus Deblurring with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2209.06040v1
- **DOI**: 10.1109/ICME52920.2022.9859631
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.06040v1)
- **Published**: 2022-09-13 14:47:09+00:00
- **Updated**: 2022-09-13 14:47:09+00:00
- **Authors**: Dafeng Zhang, Xiaobing Wang
- **Comment**: None
- **Journal**: 2022 IEEE International Conference on Multimedia and Expo (ICME),
  2022, pp. 1-6
- **Summary**: Recent works achieve excellent results in defocus deblurring task based on dual-pixel data using convolutional neural network (CNN), while the scarcity of data limits the exploration and attempt of vision transformer in this task. In addition, the existing works use fixed parameters and network architecture to deblur images with different distribution and content information, which also affects the generalization ability of the model. In this paper, we propose a dynamic multi-scale network, named DMTNet, for dual-pixel images defocus deblurring. DMTNet mainly contains two modules: feature extraction module and reconstruction module. The feature extraction module is composed of several vision transformer blocks, which uses its powerful feature extraction capability to obtain richer features and improve the robustness of the model. The reconstruction module is composed of several Dynamic Multi-scale Sub-reconstruction Module (DMSSRM). DMSSRM can restore images by adaptively assigning weights to features from different scales according to the blur distribution and content information of the input images. DMTNet combines the advantages of transformer and CNN, in which the vision transformer improves the performance ceiling of CNN, and the inductive bias of CNN enables transformer to extract more robust features without relying on a large amount of data. DMTNet might be the first attempt to use vision transformer to restore the blurring images to clarity. By combining with CNN, the vision transformer may achieve better performance on small datasets. Experimental results on the popular benchmarks demonstrate that our DMTNet significantly outperforms state-of-the-art methods.



### Generalised Automatic Anatomy Finder (GAAF): A general framework for 3D location-finding in CT scans
- **Arxiv ID**: http://arxiv.org/abs/2209.06042v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.06042v1)
- **Published**: 2022-09-13 14:50:16+00:00
- **Updated**: 2022-09-13 14:50:16+00:00
- **Authors**: Edward G. A. Henderson, Eliana M. Vasquez Osorio, Marcel van Herk, Andrew F. Green
- **Comment**: None
- **Journal**: None
- **Summary**: We present GAAF, a Generalised Automatic Anatomy Finder, for the identification of generic anatomical locations in 3D CT scans. GAAF is an end-to-end pipeline, with dedicated modules for data pre-processing, model training, and inference. At it's core, GAAF uses a custom a localisation convolutional neural network (CNN). The CNN model is small, lightweight and can be adjusted to suit the particular application. The GAAF framework has so far been tested in the head and neck, and is able to find anatomical locations such as the centre-of-mass of the brainstem. GAAF was evaluated in an open-access dataset and is capable of accurate and robust localisation performance. All our code is open source and available at https://github.com/rrr-uom-projects/GAAF.



### SeRP: Self-Supervised Representation Learning Using Perturbed Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2209.06067v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.06067v1)
- **Published**: 2022-09-13 15:22:36+00:00
- **Updated**: 2022-09-13 15:22:36+00:00
- **Authors**: Siddhant Garg, Mudit Chaudhary
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: We present SeRP, a framework for Self-Supervised Learning of 3D point clouds. SeRP consists of encoder-decoder architecture that takes perturbed or corrupted point clouds as inputs and aims to reconstruct the original point cloud without corruption. The encoder learns the high-level latent representations of the points clouds in a low-dimensional subspace and recovers the original structure. In this work, we have used Transformers and PointNet-based Autoencoders. The proposed framework also addresses some of the limitations of Transformers-based Masked Autoencoders which are prone to leakage of location information and uneven information density. We trained our models on the complete ShapeNet dataset and evaluated them on ModelNet40 as a downstream classification task. We have shown that the pretrained models achieved 0.5-1% higher classification accuracies than the networks trained from scratch. Furthermore, we also proposed VASP: Vector-Quantized Autoencoder for Self-supervised Representation Learning for Point Clouds that employs Vector-Quantization for discrete representation learning for Transformer-based autoencoders.



### DOMINO: Domain-aware Model Calibration in Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.06077v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.06077v1)
- **Published**: 2022-09-13 15:31:52+00:00
- **Updated**: 2022-09-13 15:31:52+00:00
- **Authors**: Skylar E. Stolte, Kyle Volle, Aprinda Indahlastari, Alejandro Albizu, Adam J. Woods, Kevin Brink, Matthew Hale, Ruogu Fang
- **Comment**: 10 pages, 6 figures, 3 tables. Accepted by International Conference
  on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2022
  Oral Talk
- **Journal**: None
- **Summary**: Model calibration measures the agreement between the predicted probability estimates and the true correctness likelihood. Proper model calibration is vital for high-risk applications. Unfortunately, modern deep neural networks are poorly calibrated, compromising trustworthiness and reliability. Medical image segmentation particularly suffers from this due to the natural uncertainty of tissue boundaries. This is exasperated by their loss functions, which favor overconfidence in the majority classes. We address these challenges with DOMINO, a domain-aware model calibration method that leverages the semantic confusability and hierarchical similarity between class labels. Our experiments demonstrate that our DOMINO-calibrated deep neural networks outperform non-calibrated models and state-of-the-art morphometric methods in head image segmentation. Our results show that our method can consistently achieve better calibration, higher accuracy, and faster inference times than these methods, especially on rarer classes. This performance is attributed to our domain-aware regularization to inform semantic model calibration. These findings show the importance of semantic ties between class labels in building confidence in deep learning models. The framework has the potential to improve the trustworthiness and reliability of generic medical image segmentation models. The code for this article is available at: https://github.com/lab-smile/DOMINO.



### On the Optimal Combination of Cross-Entropy and Soft Dice Losses for Lesion Segmentation with Out-of-Distribution Robustness
- **Arxiv ID**: http://arxiv.org/abs/2209.06078v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.06078v2)
- **Published**: 2022-09-13 15:32:32+00:00
- **Updated**: 2022-09-14 16:27:05+00:00
- **Authors**: Adrian Galdran, Gustavo Carneiro, Miguel Ángel González Ballester
- **Comment**: Accepted at the DFU Challenge Proceedings, part of MICCAI 2022
- **Journal**: None
- **Summary**: We study the impact of different loss functions on lesion segmentation from medical images. Although the Cross-Entropy (CE) loss is the most popular option when dealing with natural images, for biomedical image segmentation the soft Dice loss is often preferred due to its ability to handle imbalanced scenarios. On the other hand, the combination of both functions has also been successfully applied in this kind of tasks. A much less studied problem is the generalization ability of all these losses in the presence of Out-of-Distribution (OoD) data. This refers to samples appearing in test time that are drawn from a different distribution than training images. In our case, we train our models on images that always contain lesions, but in test time we also have lesion-free samples. We analyze the impact of the minimization of different loss functions on in-distribution performance, but also its ability to generalize to OoD data, via comprehensive experiments on polyp segmentation from endoscopic images and ulcer segmentation from diabetic feet images. Our findings are surprising: CE-Dice loss combinations that excel in segmenting in-distribution images have a poor performance when dealing with OoD data, which leads us to recommend the adoption of the CE loss for this kind of problems, due to its robustness and ability to generalize to OoD samples. Code associated to our experiments can be found at https://github.com/agaldran/lesion_losses_ood .



### PET image denoising based on denoising diffusion probabilistic models
- **Arxiv ID**: http://arxiv.org/abs/2209.06167v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2209.06167v2)
- **Published**: 2022-09-13 17:13:44+00:00
- **Updated**: 2022-09-14 16:07:53+00:00
- **Authors**: Kuang Gong, Keith A. Johnson, Georges El Fakhri, Quanzheng Li, Tinsu Pan
- **Comment**: 8 figures
- **Journal**: None
- **Summary**: Due to various physical degradation factors and limited counts received, PET image quality needs further improvements. The denoising diffusion probabilistic models (DDPM) are distribution learning-based models, which try to transform a normal distribution into a specific data distribution based on iterative refinements. In this work, we proposed and evaluated different DDPM-based methods for PET image denoising. Under the DDPM framework, one way to perform PET image denoising is to provide the PET image and/or the prior image as the network input. Another way is to supply the prior image as the input with the PET image included in the refinement steps, which can fit for scenarios of different noise levels. 120 18F-FDG datasets and 140 18F-MK-6240 datasets were utilized to evaluate the proposed DDPM-based methods. Quantification show that the DDPM-based frameworks with PET information included can generate better results than the nonlocal mean and Unet-based denoising methods. Adding additional MR prior in the model can help achieve better performance and further reduce the uncertainty during image denoising. Solely relying on MR prior while ignoring the PET information can result in large bias. Regional and surface quantification shows that employing MR prior as the network input while embedding PET image as a data-consistency constraint during inference can achieve the best performance. In summary, DDPM-based PET image denoising is a flexible framework, which can efficiently utilize prior information and achieve better performance than the nonlocal mean and Unet-based denoising methods.



### Comparative analysis of segmentation and generative models for fingerprint retrieval task
- **Arxiv ID**: http://arxiv.org/abs/2209.06172v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.06172v2)
- **Published**: 2022-09-13 17:21:14+00:00
- **Updated**: 2022-11-02 12:13:35+00:00
- **Authors**: Megh Patel, Devarsh Patel, Sarthak Patel
- **Comment**: This is a working draft and not indented for publication
- **Journal**: None
- **Summary**: Biometric Authentication like Fingerprints has become an integral part of the modern technology for authentication and verification of users. It is pervasive in more ways than most of us are aware of. However, these fingerprint images deteriorate in quality if the fingers are dirty, wet, injured or when sensors malfunction. Therefore, extricating the original fingerprint by removing the noise and inpainting it to restructure the image is crucial for its authentication. Hence, this paper proposes a deep learning approach to address these issues using Generative (GAN) and Segmentation models. Qualitative and Quantitative comparison has been done between pix2pixGAN and cycleGAN (generative models) as well as U-net (segmentation model). To train the model, we created our own dataset NFD - Noisy Fingerprint Dataset meticulously with different backgrounds along with scratches in some images to make it more realistic and robust. In our research, the u-net model performed better than the GAN networks



### HistoPerm: A Permutation-Based View Generation Approach for Improving Histopathologic Feature Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.06185v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.06185v2)
- **Published**: 2022-09-13 17:35:08+00:00
- **Updated**: 2023-04-05 21:33:36+00:00
- **Authors**: Joseph DiPalma, Lorenzo Torresani, Saeed Hassanpour
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been effective for histology image analysis in digital pathology. However, many current deep learning approaches require large, strongly- or weakly-labeled images and regions of interest, which can be time-consuming and resource-intensive to obtain. To address this challenge, we present HistoPerm, a view generation method for representation learning using joint embedding architectures that enhances representation learning for histology images. HistoPerm permutes augmented views of patches extracted from whole-slide histology images to improve classification performance. We evaluated the effectiveness of HistoPerm on two histology image datasets for Celiac disease and Renal Cell Carcinoma, using three widely used joint embedding architecture-based representation learning methods: BYOL, SimCLR, and VICReg. Our results show that HistoPerm consistently improves patch- and slide-level classification performance in terms of accuracy, F1-score, and AUC. Specifically, for patch-level classification accuracy on the Celiac disease dataset, HistoPerm boosts BYOL and VICReg by 8% and SimCLR by 3%. On the Renal Cell Carcinoma dataset, patch-level classification accuracy is increased by 2% for BYOL and VICReg, and by 1% for SimCLR. In addition, on the Celiac disease dataset, models with HistoPerm outperform the fully-supervised baseline model by 6%, 5%, and 2% for BYOL, SimCLR, and VICReg, respectively. For the Renal Cell Carcinoma dataset, HistoPerm lowers the classification accuracy gap for the models up to 10% relative to the fully-supervised baseline. These findings suggest that HistoPerm can be a valuable tool for improving representation learning of histopathology features when access to labeled data is limited and can lead to whole-slide classification results that are comparable to or superior to fully-supervised methods.



### A Benchmark and a Baseline for Robust Multi-view Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2209.06681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.06681v1)
- **Published**: 2022-09-13 17:44:16+00:00
- **Updated**: 2022-09-13 17:44:16+00:00
- **Authors**: Philipp Schröppel, Jan Bechtold, Artemij Amiranashvili, Thomas Brox
- **Comment**: Accepted at 3DV 2022
- **Journal**: None
- **Summary**: Recent deep learning approaches for multi-view depth estimation are employed either in a depth-from-video or a multi-view stereo setting. Despite different settings, these approaches are technically similar: they correlate multiple source views with a keyview to estimate a depth map for the keyview. In this work, we introduce the Robust Multi-View Depth Benchmark that is built upon a set of public datasets and allows evaluation in both settings on data from different domains. We evaluate recent approaches and find imbalanced performances across domains. Further, we consider a third setting, where camera poses are available and the objective is to estimate the corresponding depth maps with their correct scale. We show that recent approaches do not generalize across datasets in this setting. This is because their cost volume output runs out of distribution. To resolve this, we present the Robust MVD Baseline model for multi-view depth estimation, which is built upon existing components but employs a novel scale augmentation procedure. It can be applied for robust multi-view depth estimation, independent of the target data. We provide code for the proposed benchmark and baseline model at https://github.com/lmb-freiburg/robustmvd.



### StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation
- **Arxiv ID**: http://arxiv.org/abs/2209.06192v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2209.06192v1)
- **Published**: 2022-09-13 17:47:39+00:00
- **Updated**: 2022-09-13 17:47:39+00:00
- **Authors**: Adyasha Maharana, Darryl Hannan, Mohit Bansal
- **Comment**: ECCV 2022 (33 pages; code, data, demo, model card available at
  https://github.com/adymaharana/storydalle)
- **Journal**: None
- **Summary**: Recent advances in text-to-image synthesis have led to large pretrained transformers with excellent capabilities to generate visualizations from a given text. However, these models are ill-suited for specialized tasks like story visualization, which requires an agent to produce a sequence of images given a corresponding sequence of captions, forming a narrative. Moreover, we find that the story visualization task fails to accommodate generalization to unseen plots and characters in new narratives. Hence, we first propose the task of story continuation, where the generated visual story is conditioned on a source image, allowing for better generalization to narratives with new characters. Then, we enhance or 'retro-fit' the pretrained text-to-image synthesis models with task-specific modules for (a) sequential image generation and (b) copying relevant elements from an initial frame. Then, we explore full-model finetuning, as well as prompt-based tuning for parameter-efficient adaptation, of the pre-trained model. We evaluate our approach StoryDALL-E on two existing datasets, PororoSV and FlintstonesSV, and introduce a new dataset DiDeMoSV collected from a video-captioning dataset. We also develop a model StoryGANc based on Generative Adversarial Networks (GAN) for story continuation, and compare it with the StoryDALL-E model to demonstrate the advantages of our approach. We show that our retro-fitting approach outperforms GAN-based models for story continuation and facilitates copying of visual elements from the source image, thereby improving continuity in the generated visual story. Finally, our analysis suggests that pretrained transformers struggle to comprehend narratives containing several characters. Overall, our work demonstrates that pretrained text-to-image synthesis models can be adapted for complex and low-resource tasks like story continuation.



### High-resolution semantically-consistent image-to-image translation
- **Arxiv ID**: http://arxiv.org/abs/2209.06264v1
- **DOI**: 10.1109/JSTARS.2022.3226705
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.06264v1)
- **Published**: 2022-09-13 19:08:30+00:00
- **Updated**: 2022-09-13 19:08:30+00:00
- **Authors**: Mikhail Sokolov, Christopher Henry, Joni Storie, Christopher Storie, Victor Alhassan, Mathieu Turgeon-Pelchat
- **Comment**: 25 pages, 7 figures
- **Journal**: None
- **Summary**: Deep learning has become one of remote sensing scientists' most efficient computer vision tools in recent years. However, the lack of training labels for the remote sensing datasets means that scientists need to solve the domain adaptation problem to narrow the discrepancy between satellite image datasets. As a result, image segmentation models that are then trained, could better generalize and use an existing set of labels instead of requiring new ones. This work proposes an unsupervised domain adaptation model that preserves semantic consistency and per-pixel quality for the images during the style-transferring phase. This paper's major contribution is proposing the improved architecture of the SemI2I model, which significantly boosts the proposed model's performance and makes it competitive with the state-of-the-art CyCADA model. A second contribution is testing the CyCADA model on the remote sensing multi-band datasets such as WorldView-2 and SPOT-6. The proposed model preserves semantic consistency and per-pixel quality for the images during the style-transferring phase. Thus, the semantic segmentation model, trained on the adapted images, shows substantial performance gain compared to the SemI2I model and reaches similar results as the state-of-the-art CyCADA model. The future development of the proposed method could include ecological domain transfer, {\em a priori} evaluation of dataset quality in terms of data distribution, or exploration of the inner architecture of the domain adaptation model.



### Warm Start Active Learning with Proxy Labels \& Selection via Semi-Supervised Fine-Tuning
- **Arxiv ID**: http://arxiv.org/abs/2209.06285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.06285v1)
- **Published**: 2022-09-13 20:21:40+00:00
- **Updated**: 2022-09-13 20:21:40+00:00
- **Authors**: Vishwesh Nath, Dong Yang, Holger R. Roth, Daguang Xu
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Which volume to annotate next is a challenging problem in building medical imaging datasets for deep learning. One of the promising methods to approach this question is active learning (AL). However, AL has been a hard nut to crack in terms of which AL algorithm and acquisition functions are most useful for which datasets. Also, the problem is exacerbated with which volumes to label first when there is zero labeled data to start with. This is known as the cold start problem in AL. We propose two novel strategies for AL specifically for 3D image segmentation. First, we tackle the cold start problem by proposing a proxy task and then utilizing uncertainty generated from the proxy task to rank the unlabeled data to be annotated. Second, we craft a two-stage learning framework for each active iteration where the unlabeled data is also used in the second stage as a semi-supervised fine-tuning strategy. We show the promise of our approach on two well-known large public datasets from medical segmentation decathlon. The results indicate that the initial selection of data and semi-supervised framework both showed significant improvement for several AL strategies.



### Multiple View Performers for Shape Completion
- **Arxiv ID**: http://arxiv.org/abs/2209.06291v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.06291v1)
- **Published**: 2022-09-13 20:45:24+00:00
- **Updated**: 2022-09-13 20:45:24+00:00
- **Authors**: David Watkins, Peter Allen, Krzysztof Choromanski, Jacob Varley, Nicholas Waytowich
- **Comment**: 6 pages, 2 pages of references, 6 figures, 3 tables
- **Journal**: None
- **Summary**: We propose the Multiple View Performer (MVP) - a new architecture for 3D shape completion from a series of temporally sequential views. MVP accomplishes this task by using linear-attention Transformers called Performers. Our model allows the current observation of the scene to attend to the previous ones for more accurate infilling. The history of past observations is compressed via the compact associative memory approximating modern continuous Hopfield memory, but crucially of size independent from the history length. We compare our model with several baselines for shape completion over time, demonstrating the generalization gains that MVP provides. To the best of our knowledge, MVP is the first multiple view voxel reconstruction method that does not require registration of multiple depth views and the first causal Transformer based model for 3D shape completion.



### Do Androids Laugh at Electric Sheep? Humor "Understanding" Benchmarks from The New Yorker Caption Contest
- **Arxiv ID**: http://arxiv.org/abs/2209.06293v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.06293v2)
- **Published**: 2022-09-13 20:54:00+00:00
- **Updated**: 2023-07-06 06:20:00+00:00
- **Authors**: Jack Hessel, Ana Marasović, Jena D. Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, Yejin Choi
- **Comment**: None
- **Journal**: ACL 2023
- **Summary**: Large neural networks can now generate jokes, but do they really "understand" humor? We challenge AI models with three tasks derived from the New Yorker Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning caption, and explaining why a winning caption is funny. These tasks encapsulate progressively more sophisticated aspects of "understanding" a cartoon; key elements are the complex, often surprising relationships between images and captions and the frequent inclusion of indirect and playful allusions to human experience and culture. We investigate both multimodal and language-only models: the former are challenged with the cartoon images directly, while the latter are given multifaceted descriptions of the visual scene to simulate human-level visual understanding. We find that both types of models struggle at all three tasks. For example, our best multimodal models fall 30 accuracy points behind human performance on the matching task, and, even when provided ground-truth visual scene descriptors, human-authored explanations are preferred head-to-head over the best machine-authored ones (few-shot GPT-4) in more than 2/3 of cases. We release models, code, leaderboard, and corpus, which includes newly-gathered annotations describing the image's locations/entities, what's unusual in the scene, and an explanation of the joke.



### Placing Human Animations into 3D Scenes by Learning Interaction- and Geometry-Driven Keyframes
- **Arxiv ID**: http://arxiv.org/abs/2209.06314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.06314v1)
- **Published**: 2022-09-13 21:46:00+00:00
- **Updated**: 2022-09-13 21:46:00+00:00
- **Authors**: James F. Mullen Jr, Divya Kothandaraman, Aniket Bera, Dinesh Manocha
- **Comment**: WACV 2023. Our project website is available at
  https://gamma.umd.edu/paak/
- **Journal**: None
- **Summary**: We present a novel method for placing a 3D human animation into a 3D scene while maintaining any human-scene interactions in the animation. We use the notion of computing the most important meshes in the animation for the interaction with the scene, which we call "keyframes." These keyframes allow us to better optimize the placement of the animation into the scene such that interactions in the animations (standing, laying, sitting, etc.) match the affordances of the scene (e.g., standing on the floor or laying in a bed). We compare our method, which we call PAAK, with prior approaches, including POSA, PROX ground truth, and a motion synthesis method, and highlight the benefits of our method with a perceptual study. Human raters preferred our PAAK method over the PROX ground truth data 64.6\% of the time. Additionally, in direct comparisons, the raters preferred PAAK over competing methods including 61.5\% compared to POSA.



### Optimizing SLAM Evaluation Footprint Through Dynamic Range Coverage Analysis of Datasets
- **Arxiv ID**: http://arxiv.org/abs/2209.06316v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.06316v2)
- **Published**: 2022-09-13 21:47:18+00:00
- **Updated**: 2023-03-01 16:10:00+00:00
- **Authors**: Islam Ali, Hong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Simultaneous Localization and Mapping (SLAM) is considered an ever-evolving problem due to its usage in many applications. Evaluation of SLAM is done typically using publicly available datasets which are increasing in number and the level of difficulty. Each dataset provides a certain level of dynamic range coverage that is a key aspect of measuring the robustness and resilience of SLAM. In this paper, we provide a systematic analysis of the dynamic range coverage of datasets based on a number of characterization metrics, and our analysis shows a huge level of redundancy within and between datasets. Subsequently, we propose a dynamic programming (DP) algorithm for eliminating the redundancy in the evaluation process of SLAM by selecting a subset of sequences that matches a single or multiple dynamic range coverage objectives. It is shown that, with the help of dataset characterization and DP selection algorithm, a reduction in the evaluation effort can be achieved while maintaining the same level of coverage. We also study how the evaluation process of a real-world SLAM system can be optimized utilizing the method proposed.



### FaceTopoNet: Facial Expression Recognition using Face Topology Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.06322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.06322v1)
- **Published**: 2022-09-13 22:02:54+00:00
- **Updated**: 2022-09-13 22:02:54+00:00
- **Authors**: Mojtaba Kolahdouzi, Alireza Sepas-Moghaddam, Ali Etemad
- **Comment**: None
- **Journal**: None
- **Summary**: Prior work has shown that the order in which different components of the face are learned using a sequential learner can play an important role in the performance of facial expression recognition systems. We propose FaceTopoNet, an end-to-end deep model for facial expression recognition, which is capable of learning an effective tree topology of the face. Our model then traverses the learned tree to generate a sequence, which is then used to form an embedding to feed a sequential learner. The devised model adopts one stream for learning structure and one stream for learning texture. The structure stream focuses on the positions of the facial landmarks, while the main focus of the texture stream is on the patches around the landmarks to learn textural information. We then fuse the outputs of the two streams by utilizing an effective attention-based fusion strategy. We perform extensive experiments on four large-scale in-the-wild facial expression datasets - namely AffectNet, FER2013, ExpW, and RAF-DB - and one lab-controlled dataset (CK+) to evaluate our approach. FaceTopoNet achieves state-of-the-art performance on three of the five datasets and obtains competitive results on the other two datasets. We also perform rigorous ablation and sensitivity experiments to evaluate the impact of different components and parameters in our model. Lastly, we perform robustness experiments and demonstrate that FaceTopoNet is more robust against occlusions in comparison to other leading methods in the area.



### DMMGAN: Diverse Multi Motion Prediction of 3D Human Joints using Attention-Based Generative Adverserial Network
- **Arxiv ID**: http://arxiv.org/abs/2209.09124v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.09124v2)
- **Published**: 2022-09-13 23:22:33+00:00
- **Updated**: 2022-10-02 23:19:32+00:00
- **Authors**: Payam Nikdel, Mohammad Mahdavian, Mo Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Human motion prediction is a fundamental part of many human-robot applications. Despite the recent progress in human motion prediction, most studies simplify the problem by predicting the human motion relative to a fixed joint and/or only limit their model to predict one possible future motion. While due to the complex nature of human motion, a single output cannot reflect all the possible actions one can do. Also, for any robotics application, we need the full human motion including the user trajectory not a 3d pose relative to the hip joint.   In this paper, we try to address these two issues by proposing a transformer-based generative model for forecasting multiple diverse human motions. Our model generates \textit{N} future possible motion by querying a history of human motion. Our model first predicts the pose of the body relative to the hip joint. Then the \textit{Hip Prediction Module} predicts the trajectory of the hip movement for each predicted pose frame. To emphasize on the diverse future motions we introduce a similarity loss that penalizes the pairwise sample distance. We show that our system outperforms the state-of-the-art in human motion prediction while it can predict diverse multi-motion future trajectories with hip movements



