# Arxiv Papers in cs.CV on 2022-09-18
### Evolution of a Web-Scale Near Duplicate Image Detection System
- **Arxiv ID**: http://arxiv.org/abs/2209.08433v1
- **DOI**: 10.1145/3366423.3380031
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08433v1)
- **Published**: 2022-09-18 00:57:50+00:00
- **Updated**: 2022-09-18 00:57:50+00:00
- **Authors**: Andrey Gusev, Jiajing Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting near duplicate images is fundamental to the content ecosystem of photo sharing web applications. However, such a task is challenging when involving a web-scale image corpus containing billions of images. In this paper, we present an efficient system for detecting near duplicate images across 8 billion images. Our system consists of three stages: candidate generation, candidate selection, and clustering. We also demonstrate that this system can be used to greatly improve the quality of recommendations and search results across a number of real-world applications.   In addition, we include the evolution of the system over the course of six years, bringing out experiences and lessons on how new systems are designed to accommodate organic content growth as well as the latest technology. Finally, we are releasing a human-labeled dataset of ~53,000 pairs of images introduced in this paper.



### SDFE-LV: A Large-Scale, Multi-Source, and Unconstrained Database for Spotting Dynamic Facial Expressions in Long Videos
- **Arxiv ID**: http://arxiv.org/abs/2209.08445v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08445v1)
- **Published**: 2022-09-18 01:59:12+00:00
- **Updated**: 2022-09-18 01:59:12+00:00
- **Authors**: Xiaolin Xu, Yuan Zong, Wenming Zheng, Yang Li, Chuangao Tang, Xingxun Jiang, Haolin Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a large-scale, multi-source, and unconstrained database called SDFE-LV for spotting the onset and offset frames of a complete dynamic facial expression from long videos, which is known as the topic of dynamic facial expression spotting (DFES) and a vital prior step for lots of facial expression analysis tasks. Specifically, SDFE-LV consists of 1,191 long videos, each of which contains one or more complete dynamic facial expressions. Moreover, each complete dynamic facial expression in its corresponding long video was independently labeled for five times by 10 well-trained annotators. To the best of our knowledge, SDFE-LV is the first unconstrained large-scale database for the DFES task whose long videos are collected from multiple real-world/closely real-world media sources, e.g., TV interviews, documentaries, movies, and we-media short videos. Therefore, DFES tasks on SDFE-LV database will encounter numerous difficulties in practice such as head posture changes, occlusions, and illumination. We also provided a comprehensive benchmark evaluation from different angles by using lots of recent state-of-the-art deep spotting methods and hence researchers interested in DFES can quickly and easily get started. Finally, with the deep discussions on the experimental evaluation results, we attempt to point out several meaningful directions to deal with DFES tasks and hope that DFES can be better advanced in the future. In addition, SDFE-LV will be freely released for academic use only as soon as possible.



### MetaDIP: Accelerating Deep Image Prior with Meta Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.08452v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08452v1)
- **Published**: 2022-09-18 02:41:58+00:00
- **Updated**: 2022-09-18 02:41:58+00:00
- **Authors**: Kevin Zhang, Mingyang Xie, Maharshi Gor, Yi-Ting Chen, Yvonne Zhou, Christopher A. Metzler
- **Comment**: None
- **Journal**: None
- **Summary**: Deep image prior (DIP) is a recently proposed technique for solving imaging inverse problems by fitting the reconstructed images to the output of an untrained convolutional neural network. Unlike pretrained feedforward neural networks, the same DIP can generalize to arbitrary inverse problems, from denoising to phase retrieval, while offering competitive performance at each task. The central disadvantage of DIP is that, while feedforward neural networks can reconstruct an image in a single pass, DIP must gradually update its weights over hundreds to thousands of iterations, at a significant computational cost. In this work we use meta-learning to massively accelerate DIP-based reconstructions. By learning a proper initialization for the DIP weights, we demonstrate a 10x improvement in runtimes across a range of inverse imaging tasks. Moreover, we demonstrate that a network trained to quickly reconstruct faces also generalizes to reconstructing natural image patches.



### TODE-Trans: Transparent Object Depth Estimation with Transformer
- **Arxiv ID**: http://arxiv.org/abs/2209.08455v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.08455v1)
- **Published**: 2022-09-18 03:04:01+00:00
- **Updated**: 2022-09-18 03:04:01+00:00
- **Authors**: Kang Chen, Shaochen Wang, Beihao Xia, Dongxu Li, Zhen Kan, Bin Li
- **Comment**: Submitted to ICRA2023
- **Journal**: None
- **Summary**: Transparent objects are widely used in industrial automation and daily life. However, robust visual recognition and perception of transparent objects have always been a major challenge. Currently, most commercial-grade depth cameras are still not good at sensing the surfaces of transparent objects due to the refraction and reflection of light. In this work, we present a transformer-based transparent object depth estimation approach from a single RGB-D input. We observe that the global characteristics of the transformer make it easier to extract contextual information to perform depth estimation of transparent areas. In addition, to better enhance the fine-grained features, a feature fusion module (FFM) is designed to assist coherent prediction. Our empirical evidence demonstrates that our model delivers significant improvements in recent popular datasets, e.g., 25% gain on RMSE and 21% gain on REL compared to previous state-of-the-art convolutional-based counterparts in ClearGrasp dataset. Extensive results show that our transformer-based model enables better aggregation of the object's RGB and inaccurate depth information to obtain a better depth representation. Our code and the pre-trained model will be available at https://github.com/yuchendoudou/TODE.



### StereoVoxelNet: Real-Time Obstacle Detection Based on Occupancy Voxels from a Stereo Camera Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.08459v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08459v2)
- **Published**: 2022-09-18 03:32:38+00:00
- **Updated**: 2023-03-04 18:18:54+00:00
- **Authors**: Hongyu Li, Zhengang Li, Neset Unver Akmandor, Huaizu Jiang, Yanzhi Wang, Taskin Padir
- **Comment**: Accepted by ICRA 2023
- **Journal**: None
- **Summary**: Obstacle detection is a safety-critical problem in robot navigation, where stereo matching is a popular vision-based approach. While deep neural networks have shown impressive results in computer vision, most of the previous obstacle detection works only leverage traditional stereo matching techniques to meet the computational constraints for real-time feedback. This paper proposes a computationally efficient method that employs a deep neural network to detect occupancy from stereo images directly. Instead of learning the point cloud correspondence from the stereo data, our approach extracts the compact obstacle distribution based on volumetric representations. In addition, we prune the computation of safety irrelevant spaces in a coarse-to-fine manner based on octrees generated by the decoder. As a result, we achieve real-time performance on the onboard computer (NVIDIA Jetson TX2). Our approach detects obstacles accurately in the range of 32 meters and achieves better IoU (Intersection over Union) and CD (Chamfer Distance) scores with only 2% of the computation cost of the state-of-the-art stereo model. Furthermore, we validate our method's robustness and real-world feasibility through autonomous navigation experiments with a real robot. Hence, our work contributes toward closing the gap between the stereo-based system in robot perception and state-of-the-art stereo models in computer vision. To counter the scarcity of high-quality real-world indoor stereo datasets, we collect a 1.36 hours stereo dataset with a mobile robot which is used to fine-tune our model. The dataset, the code, and further details including additional visualizations are available at https://lhy.xyz/stereovoxelnet



### Human Performance Modeling and Rendering via Neural Animated Mesh
- **Arxiv ID**: http://arxiv.org/abs/2209.08468v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08468v1)
- **Published**: 2022-09-18 03:58:00+00:00
- **Updated**: 2022-09-18 03:58:00+00:00
- **Authors**: Fuqiang Zhao, Yuheng Jiang, Kaixin Yao, Jiakai Zhang, Liao Wang, Haizhao Dai, Yuhui Zhong, Yingliang Zhang, Minye Wu, Lan Xu, Jingyi Yu
- **Comment**: 18 pages, 17 figures
- **Journal**: None
- **Summary**: We have recently seen tremendous progress in the neural advances for photo-real human modeling and rendering. However, it's still challenging to integrate them into an existing mesh-based pipeline for downstream applications. In this paper, we present a comprehensive neural approach for high-quality reconstruction, compression, and rendering of human performances from dense multi-view videos. Our core intuition is to bridge the traditional animated mesh workflow with a new class of highly efficient neural techniques. We first introduce a neural surface reconstructor for high-quality surface generation in minutes. It marries the implicit volumetric rendering of the truncated signed distance field (TSDF) with multi-resolution hash encoding. We further propose a hybrid neural tracker to generate animated meshes, which combines explicit non-rigid tracking with implicit dynamic deformation in a self-supervised framework. The former provides the coarse warping back into the canonical space, while the latter implicit one further predicts the displacements using the 4D hash encoding as in our reconstructor. Then, we discuss the rendering schemes using the obtained animated meshes, ranging from dynamic texturing to lumigraph rendering under various bandwidth settings. To strike an intricate balance between quality and bandwidth, we propose a hierarchical solution by first rendering 6 virtual views covering the performer and then conducting occlusion-aware neural texture blending. We demonstrate the efficacy of our approach in a variety of mesh-based applications and photo-realistic free-view experiences on various platforms, i.e., inserting virtual human performances into real environments through mobile AR or immersively watching talent shows with VR headsets.



### GaitMM: Multi-Granularity Motion Sequence Learning for Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.08470v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08470v2)
- **Published**: 2022-09-18 04:07:33+00:00
- **Updated**: 2023-06-24 04:48:05+00:00
- **Authors**: Lei Wang, Bo Liu, Bincheng Wang, Fuqiang Yu
- **Comment**: Accepted to ICIP2023
- **Journal**: None
- **Summary**: Gait recognition aims to identify individual-specific walking patterns by observing the different periodic movements of each body part. However, most existing methods treat each part equally and fail to account for the data redundancy caused by the different step frequencies and sampling rates of gait sequences. In this study, we propose a multi-granularity motion representation network (GaitMM) for gait sequence learning. In GaitMM, we design a combined full-body and fine-grained sequence learning module (FFSL) to explore part-independent spatio-temporal representations. Moreover, we utilize a frame-wise compression strategy, referred to as multi-scale motion aggregation (MSMA), to capture discriminative information in the gait sequence. Experiments on two public datasets, CASIA-B and OUMVLP, show that our approach reaches state-of-the-art performances.



### Bootstrap Generalization Ability from Loss Landscape Perspective
- **Arxiv ID**: http://arxiv.org/abs/2209.08473v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08473v2)
- **Published**: 2022-09-18 04:40:32+00:00
- **Updated**: 2023-04-21 12:45:27+00:00
- **Authors**: Huanran Chen, Shitong Shao, Ziyi Wang, Zirui Shang, Jin Chen, Xiaofeng Ji, Xinxiao Wu
- **Comment**: 18 pages, 4 figures, Accepted by ECCV Workshop2022
- **Journal**: None
- **Summary**: Domain generalization aims to learn a model that can generalize well on the unseen test dataset, i.e., out-of-distribution data, which has different distribution from the training dataset. To address domain generalization in computer vision, we introduce the loss landscape theory into this field. Specifically, we bootstrap the generalization ability of the deep learning model from the loss landscape perspective in four aspects, including backbone, regularization, training paradigm, and learning rate. We verify the proposed theory on the NICO++, PACS, and VLCS datasets by doing extensive ablation studies as well as visualizations. In addition, we apply this theory in the ECCV 2022 NICO Challenge1 and achieve the 3rd place without using any domain invariant methods.



### EMA-VIO: Deep Visual-Inertial Odometry with External Memory Attention
- **Arxiv ID**: http://arxiv.org/abs/2209.08490v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.08490v1)
- **Published**: 2022-09-18 07:05:36+00:00
- **Updated**: 2022-09-18 07:05:36+00:00
- **Authors**: Zheming Tu, Changhao Chen, Xianfei Pan, Ruochen Liu, Jiarui Cui, Jun Mao
- **Comment**: Accepted by IEEE Sensors Journal
- **Journal**: None
- **Summary**: Accurate and robust localization is a fundamental need for mobile agents. Visual-inertial odometry (VIO) algorithms exploit the information from camera and inertial sensors to estimate position and translation. Recent deep learning based VIO models attract attentions as they provide pose information in a data-driven way, without the need of designing hand-crafted algorithms. Existing learning based VIO models rely on recurrent models to fuse multimodal data and process sensor signal, which are hard to train and not efficient enough. We propose a novel learning based VIO framework with external memory attention that effectively and efficiently combines visual and inertial features for states estimation. Our proposed model is able to estimate pose accurately and robustly, even in challenging scenarios, e.g., on overcast days and water-filled ground , which are difficult for traditional VIO algorithms to extract visual features. Experiments validate that it outperforms both traditional and learning based VIO baselines in different scenes.



### LATITUDE: Robotic Global Localization with Truncated Dynamic Low-pass Filter in City-scale NeRF
- **Arxiv ID**: http://arxiv.org/abs/2209.08498v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.08498v2)
- **Published**: 2022-09-18 07:56:06+00:00
- **Updated**: 2023-02-27 09:06:19+00:00
- **Authors**: Zhenxin Zhu, Yuantao Chen, Zirui Wu, Chao Hou, Yongliang Shi, Chuxuan Li, Pengfei Li, Hao Zhao, Guyue Zhou
- **Comment**: 7 pages, 6 figures, ICRA 2023
- **Journal**: None
- **Summary**: Neural Radiance Fields (NeRFs) have made great success in representing complex 3D scenes with high-resolution details and efficient memory. Nevertheless, current NeRF-based pose estimators have no initial pose prediction and are prone to local optima during optimization. In this paper, we present LATITUDE: Global Localization with Truncated Dynamic Low-pass Filter, which introduces a two-stage localization mechanism in city-scale NeRF. In place recognition stage, we train a regressor through images generated from trained NeRFs, which provides an initial value for global localization. In pose optimization stage, we minimize the residual between the observed image and rendered image by directly optimizing the pose on tangent plane. To avoid convergence to local optimum, we introduce a Truncated Dynamic Low-pass Filter (TDLF) for coarse-to-fine pose registration. We evaluate our method on both synthetic and real-world data and show its potential applications for high-precision navigation in large-scale city scenes. Codes and data will be publicly available at https://github.com/jike5/LATITUDE.



### Revisiting Rolling Shutter Bundle Adjustment: Toward Accurate and Fast Solution
- **Arxiv ID**: http://arxiv.org/abs/2209.08503v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08503v3)
- **Published**: 2022-09-18 08:21:07+00:00
- **Updated**: 2023-04-18 14:16:24+00:00
- **Authors**: Bangyan Liao, Delin Qu, Yifei Xue, Huiqing Zhang, Yizhen Lao
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: We propose a robust and fast bundle adjustment solution that estimates the 6-DoF pose of the camera and the geometry of the environment based on measurements from a rolling shutter (RS) camera. This tackles the challenges in the existing works, namely relying on additional sensors, high frame rate video as input, restrictive assumptions on camera motion, readout direction, and poor efficiency. To this end, we first investigate the influence of normalization to the image point on RSBA performance and show its better approximation in modelling the real 6-DoF camera motion. Then we present a novel analytical model for the visual residual covariance, which can be used to standardize the reprojection error during the optimization, consequently improving the overall accuracy. More importantly, the combination of normalization and covariance standardization weighting in RSBA (NW-RSBA) can avoid common planar degeneracy without needing to constrain the filming manner. Besides, we propose an acceleration strategy for NW-RSBA based on the sparsity of its Jacobian matrix and Schur complement. The extensive synthetic and real data experiments verify the effectiveness and efficiency of the proposed solution over the state-of-the-art works. We also demonstrate the proposed method can be easily implemented and plug-in famous GSSfM and GSSLAM systems as completed RSSfM and RSSLAM solutions.



### Imbalanced Node Processing Method in Graph Neural Network Classification Task
- **Arxiv ID**: http://arxiv.org/abs/2209.08514v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08514v3)
- **Published**: 2022-09-18 09:22:32+00:00
- **Updated**: 2022-10-12 03:36:19+00:00
- **Authors**: Min Liu, Siwen Jin, Luo Jin, Shuohan Wang, Yu Fang, Yuliang Shi
- **Comment**: Insufficient experiments, will continue to study in depth
- **Journal**: None
- **Summary**: In recent years, the node classification task in graph neural networks(GNNs) has developed rapidly, driving the development of research in various fields. However, there are a large number of class imbalances in the graph data, and there is a large gap between the number of different classes, resulting in suboptimal results in classification. Proposing a solution to the imbalance problem has become indispensable for the successful advancement of our downstream missions. Therefore, we start with the loss function and try to find a loss function that can effectively solve the imbalance of graph nodes to participate in the node classification task. thence, we introduce GHMC Loss into the graph neural networks to deal with difficult samples that are not marginal. Attenuate the loss contribution of marginal samples and simple samples. Experiments on multiple benchmarks show that our method can effectively deal with the class imbalance problem, and our method improves the accuracy by 3% compared to the traditional loss function.



### VisTaNet: Attention Guided Deep Fusion for Surface Roughness Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.08516v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.08516v1)
- **Published**: 2022-09-18 09:37:06+00:00
- **Updated**: 2022-09-18 09:37:06+00:00
- **Authors**: Prasanna Kumar Routray, Aditya Sanjiv Kanade, Jay Bhanushali, Manivannan Muniyandi
- **Comment**: None
- **Journal**: None
- **Summary**: Human texture perception is a weighted average of multi-sensory inputs: visual and tactile. While the visual sensing mechanism extracts global features, the tactile mechanism complements it by extracting local features. The lack of coupled visuotactile datasets in the literature is a challenge for studying multimodal fusion strategies analogous to human texture perception. This paper presents a visual dataset that augments an existing tactile dataset. We propose a novel deep fusion architecture that fuses visual and tactile data using four types of fusion strategies: summation, concatenation, max-pooling, and attention. Our model shows significant performance improvements (97.22%) in surface roughness classification accuracy over tactile only (SVM - 92.60%) and visual only (FENet-50 - 85.01%) architectures. Among the several fusion techniques, attention-guided architecture results in better classification accuracy. Our study shows that analogous to human texture perception, the proposed model chooses a weighted combination of the two modalities (visual and tactile), thus resulting in higher surface roughness classification accuracy; and it chooses to maximize the weightage of the tactile modality where the visual modality fails and vice-versa.



### Adaptive Dimension Reduction and Variational Inference for Transductive Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.08527v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08527v1)
- **Published**: 2022-09-18 10:29:02+00:00
- **Updated**: 2022-09-18 10:29:02+00:00
- **Authors**: Yuqing Hu, Stéphane Pateux, Vincent Gripon
- **Comment**: None
- **Journal**: None
- **Summary**: Transductive Few-Shot learning has gained increased attention nowadays considering the cost of data annotations along with the increased accuracy provided by unlabelled samples in the domain of few shot. Especially in Few-Shot Classification (FSC), recent works explore the feature distributions aiming at maximizing likelihoods or posteriors with respect to the unknown parameters. Following this vein, and considering the parallel between FSC and clustering, we seek for better taking into account the uncertainty in estimation due to lack of data, as well as better statistical properties of the clusters associated with each class. Therefore in this paper we propose a new clustering method based on Variational Bayesian inference, further improved by Adaptive Dimension Reduction based on Probabilistic Linear Discriminant Analysis. Our proposed method significantly improves accuracy in the realistic unbalanced transductive setting on various Few-Shot benchmarks when applied to features used in previous studies, with a gain of up to $6\%$ in accuracy. In addition, when applied to balanced setting, we obtain very competitive results without making use of the class-balance artefact which is disputable for practical use cases. We also provide the performance of our method on a high performing pretrained backbone, with the reported results further surpassing the current state-of-the-art accuracy, suggesting the genericity of the proposed method.



### Overcoming Language Priors in Visual Question Answering via Distinguishing Superficially Similar Instances
- **Arxiv ID**: http://arxiv.org/abs/2209.08529v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2209.08529v1)
- **Published**: 2022-09-18 10:30:44+00:00
- **Updated**: 2022-09-18 10:30:44+00:00
- **Authors**: Yike Wu, Yu Zhao, Shiwan Zhao, Ying Zhang, Xiaojie Yuan, Guoqing Zhao, Ning Jiang
- **Comment**: Published in COLING 2022
- **Journal**: None
- **Summary**: Despite the great progress of Visual Question Answering (VQA), current VQA models heavily rely on the superficial correlation between the question type and its corresponding frequent answers (i.e., language priors) to make predictions, without really understanding the input. In this work, we define the training instances with the same question type but different answers as \textit{superficially similar instances}, and attribute the language priors to the confusion of VQA model on such instances. To solve this problem, we propose a novel training framework that explicitly encourages the VQA model to distinguish between the superficially similar instances. Specifically, for each training instance, we first construct a set that contains its superficially similar counterparts. Then we exploit the proposed distinguishing module to increase the distance between the instance and its counterparts in the answer space. In this way, the VQA model is forced to further focus on the other parts of the input beyond the question type, which helps to overcome the language priors. Experimental results show that our method achieves the state-of-the-art performance on VQA-CP v2. Codes are available at \href{https://github.com/wyk-nku/Distinguishing-VQA.git}{Distinguishing-VQA}.



### SF2SE3: Clustering Scene Flow into SE(3)-Motions via Proposal and Selection
- **Arxiv ID**: http://arxiv.org/abs/2209.08532v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08532v2)
- **Published**: 2022-09-18 10:54:50+00:00
- **Updated**: 2022-09-26 21:24:59+00:00
- **Authors**: Leonhard Sommer, Philipp Schröppel, Thomas Brox
- **Comment**: German Conference on Pattern Recognition 2022, Konstanz, Germany
- **Journal**: None
- **Summary**: We propose SF2SE3, a novel approach to estimate scene dynamics in form of a segmentation into independently moving rigid objects and their SE(3)-motions. SF2SE3 operates on two consecutive stereo or RGB-D images. First, noisy scene flow is obtained by application of existing optical flow and depth estimation algorithms. SF2SE3 then iteratively (1) samples pixel sets to compute SE(3)-motion proposals, and (2) selects the best SE(3)-motion proposal with respect to a maximum coverage formulation. Finally, objects are formed by assigning pixels uniquely to the selected SE(3)-motions based on consistency with the input scene flow and spatial proximity. The main novelties are a more informed strategy for the sampling of motion proposals and a maximum coverage formulation for the proposal selection. We conduct evaluations on multiple datasets regarding application of SF2SE3 for scene flow estimation, object segmentation and visual odometry. SF2SE3 performs on par with the state of the art for scene flow estimation and is more accurate for segmentation and odometry.



### RDD2022: A multi-national image dataset for automatic Road Damage Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.08538v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, E.0; J.0
- **Links**: [PDF](http://arxiv.org/pdf/2209.08538v1)
- **Published**: 2022-09-18 11:29:49+00:00
- **Updated**: 2022-09-18 11:29:49+00:00
- **Authors**: Deeksha Arya, Hiroya Maeda, Sanjay Kumar Ghosh, Durga Toshniwal, Yoshihide Sekimoto
- **Comment**: 16 pages, 20 figures, IEEE BigData Cup - Crowdsensing-based Road
  damage detection challenge (CRDDC'2022)
- **Journal**: None
- **Summary**: The data article describes the Road Damage Dataset, RDD2022, which comprises 47,420 road images from six countries, Japan, India, the Czech Republic, Norway, the United States, and China. The images have been annotated with more than 55,000 instances of road damage. Four types of road damage, namely longitudinal cracks, transverse cracks, alligator cracks, and potholes, are captured in the dataset. The annotated dataset is envisioned for developing deep learning-based methods to detect and classify road damage automatically. The dataset has been released as a part of the Crowd sensing-based Road Damage Detection Challenge (CRDDC2022). The challenge CRDDC2022 invites researchers from across the globe to propose solutions for automatic road damage detection in multiple countries. The municipalities and road agencies may utilize the RDD2022 dataset, and the models trained using RDD2022 for low-cost automatic monitoring of road conditions. Further, computer vision and machine learning researchers may use the dataset to benchmark the performance of different algorithms for other image-based applications of the same type (classification, object detection, etc.).



### ActiveNeRF: Learning where to See with Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/2209.08546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08546v1)
- **Published**: 2022-09-18 12:09:15+00:00
- **Updated**: 2022-09-18 12:09:15+00:00
- **Authors**: Xuran Pan, Zihang Lai, Shiji Song, Gao Huang
- **Comment**: Accepted by ECCV2022
- **Journal**: None
- **Summary**: Recently, Neural Radiance Fields (NeRF) has shown promising performances on reconstructing 3D scenes and synthesizing novel views from a sparse set of 2D images. Albeit effective, the performance of NeRF is highly influenced by the quality of training samples. With limited posed images from the scene, NeRF fails to generalize well to novel views and may collapse to trivial solutions in unobserved regions. This makes NeRF impractical under resource-constrained scenarios. In this paper, we present a novel learning framework, ActiveNeRF, aiming to model a 3D scene with a constrained input budget. Specifically, we first incorporate uncertainty estimation into a NeRF model, which ensures robustness under few observations and provides an interpretation of how NeRF understands the scene. On this basis, we propose to supplement the existing training set with newly captured samples based on an active learning scheme. By evaluating the reduction of uncertainty given new inputs, we select the samples that bring the most information gain. In this way, the quality of novel view synthesis can be improved with minimal additional resources. Extensive experiments validate the performance of our model on both realistic and synthetic scenes, especially with scarcer training data. Code will be released at \url{https://github.com/LeapLabTHU/ActiveNeRF}.



### Perception-Distortion Trade-off in the SR Space Spanned by Flow Models
- **Arxiv ID**: http://arxiv.org/abs/2209.08564v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2209.08564v1)
- **Published**: 2022-09-18 13:12:21+00:00
- **Updated**: 2022-09-18 13:12:21+00:00
- **Authors**: Cansu Korkmaz, A. Murat Tekalp, Zafer Dogan, Erkut Erdem, Aykut Erdem
- **Comment**: 5 pages, 4 figures, accepted for publication in IEEE ICIP 2022
  Conference
- **Journal**: None
- **Summary**: Flow-based generative super-resolution (SR) models learn to produce a diverse set of feasible SR solutions, called the SR space. Diversity of SR solutions increases with the temperature ($\tau$) of latent variables, which introduces random variations of texture among sample solutions, resulting in visual artifacts and low fidelity. In this paper, we present a simple but effective image ensembling/fusion approach to obtain a single SR image eliminating random artifacts and improving fidelity without significantly compromising perceptual quality. We achieve this by benefiting from a diverse set of feasible photo-realistic solutions in the SR space spanned by flow models. We propose different image ensembling and fusion strategies which offer multiple paths to move sample solutions in the SR space to more desired destinations in the perception-distortion plane in a controllable manner depending on the fidelity vs. perceptual quality requirements of the task at hand. Experimental results demonstrate that our image ensembling/fusion strategy achieves more promising perception-distortion trade-off compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.



### MMSR: Multiple-Model Learned Image Super-Resolution Benefiting From Class-Specific Image Priors
- **Arxiv ID**: http://arxiv.org/abs/2209.08568v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2209.08568v1)
- **Published**: 2022-09-18 13:28:56+00:00
- **Updated**: 2022-09-18 13:28:56+00:00
- **Authors**: Cansu Korkmaz, A. Murat Tekalp, Zafer Dogan
- **Comment**: 5 pages, 4 figures, accepted for publication in IEEE ICIP 2022
  Conference
- **Journal**: None
- **Summary**: Assuming a known degradation model, the performance of a learned image super-resolution (SR) model depends on how well the variety of image characteristics within the training set matches those in the test set. As a result, the performance of an SR model varies noticeably from image to image over a test set depending on whether characteristics of specific images are similar to those in the training set or not. Hence, in general, a single SR model cannot generalize well enough for all types of image content. In this work, we show that training multiple SR models for different classes of images (e.g., for text, texture, etc.) to exploit class-specific image priors and employing a post-processing network that learns how to best fuse the outputs produced by these multiple SR models surpasses the performance of state-of-the-art generic SR models. Experimental results clearly demonstrate that the proposed multiple-model SR (MMSR) approach significantly outperforms a single pre-trained state-of-the-art SR model both quantitatively and visually. It even exceeds the performance of the best single class-specific SR model trained on similar text or texture images.



### ERNIE-mmLayout: Multi-grained MultiModal Transformer for Document Understanding
- **Arxiv ID**: http://arxiv.org/abs/2209.08569v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2209.08569v1)
- **Published**: 2022-09-18 13:46:56+00:00
- **Updated**: 2022-09-18 13:46:56+00:00
- **Authors**: Wenjin Wang, Zhengjie Huang, Bin Luo, Qianglong Chen, Qiming Peng, Yinxu Pan, Weichong Yin, Shikun Feng, Yu Sun, Dianhai Yu, Yin Zhang
- **Comment**: Accepted by ACM Multimedia 2022
- **Journal**: None
- **Summary**: Recent efforts of multimodal Transformers have improved Visually Rich Document Understanding (VrDU) tasks via incorporating visual and textual information. However, existing approaches mainly focus on fine-grained elements such as words and document image patches, making it hard for them to learn from coarse-grained elements, including natural lexical units like phrases and salient visual regions like prominent image regions. In this paper, we attach more importance to coarse-grained elements containing high-density information and consistent semantics, which are valuable for document understanding. At first, a document graph is proposed to model complex relationships among multi-grained multimodal elements, in which salient visual regions are detected by a cluster-based method. Then, a multi-grained multimodal Transformer called mmLayout is proposed to incorporate coarse-grained information into existing pre-trained fine-grained multimodal Transformers based on the graph. In mmLayout, coarse-grained information is aggregated from fine-grained, and then, after further processing, is fused back into fine-grained for final prediction. Furthermore, common sense enhancement is introduced to exploit the semantic information of natural lexical units. Experimental results on four tasks, including information extraction and document question answering, show that our method can improve the performance of multimodal Transformers based on fine-grained elements and achieve better performance with fewer parameters. Qualitative analyses show that our method can capture consistent semantics in coarse-grained elements.



### SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.08575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08575v1)
- **Published**: 2022-09-18 14:33:49+00:00
- **Updated**: 2022-09-18 14:33:49+00:00
- **Authors**: Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng, Shi-Min Hu
- **Comment**: SegNeXt, a simple CNN for semantic segmentation. Code is available
- **Journal**: None
- **Summary**: We present SegNeXt, a simple convolutional network architecture for semantic segmentation. Recent transformer-based models have dominated the field of semantic segmentation due to the efficiency of self-attention in encoding spatial information. In this paper, we show that convolutional attention is a more efficient and effective way to encode contextual information than the self-attention mechanism in transformers. By re-examining the characteristics owned by successful segmentation models, we discover several key components leading to the performance improvement of segmentation models. This motivates us to design a novel convolutional attention network that uses cheap convolutional operations. Without bells and whistles, our SegNeXt significantly improves the performance of previous state-of-the-art methods on popular benchmarks, including ADE20K, Cityscapes, COCO-Stuff, Pascal VOC, Pascal Context, and iSAID. Notably, SegNeXt outperforms EfficientNet-L2 w/ NAS-FPN and achieves 90.6% mIoU on the Pascal VOC 2012 test leaderboard using only 1/10 parameters of it. On average, SegNeXt achieves about 2.0% mIoU improvements compared to the state-of-the-art methods on the ADE20K datasets with the same or fewer computations. Code is available at https://github.com/uyzhang/JSeg (Jittor) and https://github.com/Visual-Attention-Network/SegNeXt (Pytorch).



### RankFeat: Rank-1 Feature Removal for Out-of-distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.08590v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08590v1)
- **Published**: 2022-09-18 16:01:31+00:00
- **Updated**: 2022-09-18 16:01:31+00:00
- **Authors**: Yue Song, Nicu Sebe, Wei Wang
- **Comment**: NeurIPS22
- **Journal**: None
- **Summary**: The task of out-of-distribution (OOD) detection is crucial for deploying machine learning models in real-world settings. In this paper, we observe that the singular value distributions of the in-distribution (ID) and OOD features are quite different: the OOD feature matrix tends to have a larger dominant singular value than the ID feature, and the class predictions of OOD samples are largely determined by it. This observation motivates us to propose \texttt{RankFeat}, a simple yet effective \texttt{post hoc} approach for OOD detection by removing the rank-1 matrix composed of the largest singular value and the associated singular vectors from the high-level feature (\emph{i.e.,} $\mathbf{X}{-} \mathbf{s}_{1}\mathbf{u}_{1}\mathbf{v}_{1}^{T}$). \texttt{RankFeat} achieves the \emph{state-of-the-art} performance and reduces the average false positive rate (FPR95) by 17.90\% compared with the previous best method. Extensive ablation studies and comprehensive theoretical analyses are presented to support the empirical results.



### Siamese Network-based Lightweight Framework for Tomato Leaf Disease Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.11214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11214v1)
- **Published**: 2022-09-18 16:08:07+00:00
- **Updated**: 2022-09-18 16:08:07+00:00
- **Authors**: Selvarajah Thuseethan, Palanisamy Vigneshwaran, Joseph Charles, Chathrie Wimalasooriya
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Automatic tomato disease recognition from leaf images is vital to avoid crop losses by applying control measures on time. Even though recent deep learning-based tomato disease recognition methods with classical training procedures showed promising recognition results, they demand large labelled data and involve expensive training. The traditional deep learning models proposed for tomato disease recognition also consume high memory and storage because of a high number of parameters. While lightweight networks overcome some of these issues to a certain extent, they continue to show low performance and struggle to handle imbalanced data. In this paper, a novel Siamese network-based lightweight framework is proposed for automatic tomato leaf disease recognition. This framework achieves the highest accuracy of 96.97% on the tomato subset obtained from the PlantVillage dataset and 95.48% on the Taiwan tomato leaf disease dataset. Experimental results further confirm that the proposed framework is effective with imbalanced and small data. The backbone deep network integrated with this framework is lightweight with approximately 2.9629 million trainable parameters, which is way lower than existing lightweight deep networks.



### ASAP: Adaptive Scheme for Asynchronous Processing of Event-based Vision Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2209.08597v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.08597v1)
- **Published**: 2022-09-18 16:28:29+00:00
- **Updated**: 2022-09-18 16:28:29+00:00
- **Authors**: Raul Tapia, Augusto Gómez Eguíluz, José Ramiro Martínez-de Dios, Anibal Ollero
- **Comment**: None
- **Journal**: None
- **Summary**: Event cameras can capture pixel-level illumination changes with very high temporal resolution and dynamic range. They have received increasing research interest due to their robustness to lighting conditions and motion blur. Two main approaches exist in the literature to feed the event-based processing algorithms: packaging the triggered events in event packages and sending them one-by-one as single events. These approaches suffer limitations from either processing overflow or lack of responsivity. Processing overflow is caused by high event generation rates when the algorithm cannot process all the events in real-time. Conversely, lack of responsivity happens in cases of low event generation rates when the event packages are sent at too low frequencies. This paper presents ASAP, an adaptive scheme to manage the event stream through variable-size packages that accommodate to the event package processing times. The experimental results show that ASAP is capable of feeding an asynchronous event-by-event clustering algorithm in a responsive and efficient manner and at the same time prevents overflow.



### Deep Adaptation of Adult-Child Facial Expressions by Fusing Landmark Features
- **Arxiv ID**: http://arxiv.org/abs/2209.08614v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.08614v1)
- **Published**: 2022-09-18 17:29:36+00:00
- **Updated**: 2022-09-18 17:29:36+00:00
- **Authors**: Megan A. Witherow, Manar D. Samad, Norou Diawara, Haim Y. Bar, Khan M. Iftekharuddin
- **Comment**: None
- **Journal**: None
- **Summary**: Imaging of facial affects may be used to measure psychophysiological attributes of children through their adulthood, especially for monitoring lifelong conditions like Autism Spectrum Disorder. Deep convolutional neural networks have shown promising results in classifying facial expressions of adults. However, classifier models trained with adult benchmark data are unsuitable for learning child expressions due to discrepancies in psychophysical development. Similarly, models trained with child data perform poorly in adult expression classification. We propose domain adaptation to concurrently align distributions of adult and child expressions in a shared latent space to ensure robust classification of either domain. Furthermore, age variations in facial images are studied in age-invariant face recognition yet remain unleveraged in adult-child expression classification. We take inspiration from multiple fields and propose deep adaptive FACial Expressions fusing BEtaMix SElected Landmark Features (FACE-BE-SELF) for adult-child facial expression classification. For the first time in the literature, a mixture of Beta distributions is used to decompose and select facial features based on correlations with expression, domain, and identity factors. We evaluate FACE-BE-SELF on two pairs of adult-child data sets. Our proposed FACE-BE-SELF approach outperforms adult-child transfer learning and other baseline domain adaptation methods in aligning latent representations of adult and child expressions.



### PIM-QAT: Neural Network Quantization for Processing-In-Memory (PIM) Systems
- **Arxiv ID**: http://arxiv.org/abs/2209.08617v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.AR, cs.CV, 68T01, 68T07, 68T45, B.2; B.3; C.1; C.3; G.3; I.2; I.5
- **Links**: [PDF](http://arxiv.org/pdf/2209.08617v1)
- **Published**: 2022-09-18 17:51:55+00:00
- **Updated**: 2022-09-18 17:51:55+00:00
- **Authors**: Qing Jin, Zhiyu Chen, Jian Ren, Yanyu Li, Yanzhi Wang, Kaiyuan Yang
- **Comment**: 25 pages, 12 figures, 8 tables
- **Journal**: None
- **Summary**: Processing-in-memory (PIM), an increasingly studied neuromorphic hardware, promises orders of energy and throughput improvements for deep learning inference. Leveraging the massively parallel and efficient analog computing inside memories, PIM circumvents the bottlenecks of data movements in conventional digital hardware. However, an extra quantization step (i.e. PIM quantization), typically with limited resolution due to hardware constraints, is required to convert the analog computing results into digital domain. Meanwhile, non-ideal effects extensively exist in PIM quantization because of the imperfect analog-to-digital interface, which further compromises the inference accuracy.   In this paper, we propose a method for training quantized networks to incorporate PIM quantization, which is ubiquitous to all PIM systems. Specifically, we propose a PIM quantization aware training (PIM-QAT) algorithm, and introduce rescaling techniques during backward and forward propagation by analyzing the training dynamics to facilitate training convergence. We also propose two techniques, namely batch normalization (BN) calibration and adjusted precision training, to suppress the adverse effects of non-ideal linearity and stochastic thermal noise involved in real PIM chips. Our method is validated on three mainstream PIM decomposition schemes, and physically on a prototype chip. Comparing with directly deploying conventionally trained quantized model on PIM systems, which does not take into account this extra quantization step and thus fails, our method provides significant improvement. It also achieves comparable inference accuracy on PIM systems as that of conventionally quantized models on digital hardware, across CIFAR10 and CIFAR100 datasets using various network depths for the most popular network topology.



### RVSL: Robust Vehicle Similarity Learning in Real Hazy Scenes Based on Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.08630v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY, cs.GT, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08630v1)
- **Published**: 2022-09-18 18:45:06+00:00
- **Updated**: 2022-09-18 18:45:06+00:00
- **Authors**: Wei-Ting Chen, I-Hsiang Chen, Chih-Yuan Yeh, Hao-Hsiang Yang, Hua-En Chang, Jian-Jiun Ding, Sy-Yen Kuo
- **Comment**: Accepted by ECCV 2022
- **Journal**: None
- **Summary**: Recently, vehicle similarity learning, also called re-identification (ReID), has attracted significant attention in computer vision. Several algorithms have been developed and obtained considerable success. However, most existing methods have unpleasant performance in the hazy scenario due to poor visibility. Though some strategies are possible to resolve this problem, they still have room to be improved due to the limited performance in real-world scenarios and the lack of real-world clear ground truth. Thus, to resolve this problem, inspired by CycleGAN, we construct a training paradigm called \textbf{RVSL} which integrates ReID and domain transformation techniques. The network is trained on semi-supervised fashion and does not require to employ the ID labels and the corresponding clear ground truths to learn hazy vehicle ReID mission in the real-world haze scenes. To further constrain the unsupervised learning process effectively, several losses are developed. Experimental results on synthetic and real-world datasets indicate that the proposed method can achieve state-of-the-art performance on hazy vehicle ReID problems. It is worth mentioning that although the proposed method is trained without real-world label information, it can achieve competitive performance compared to existing supervised methods trained on complete label information.



### CNN based Intelligent Streetlight Management Using Smart CCTV Camera and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.08633v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.08633v3)
- **Published**: 2022-09-18 19:12:43+00:00
- **Updated**: 2023-03-27 11:56:44+00:00
- **Authors**: Md Sakib Ullah Sourav, Huidong Wang, Mohammad Raziuddin Chowdhury, Rejwan Bin Sulaiman
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most neglected sources of energy loss is streetlights which generate too much light in areas where it is not required. Energy waste has enormous economic and environmental effects. In addition, due to the conventional manual nature of the operation, streetlights are frequently seen being turned ON during the day and OFF in the evening, which is regrettable even in the twenty-first century. These issues require automated streetlight control in order to be resolved. This study aims to develop a novel streetlight controlling method by combining a smart transport monitoring system powered by computer vision technology with a closed circuit television (CCTV) camera that allows the light-emitting diode (LED) streetlight to automatically light up with the appropriate brightness by detecting the presence of pedestrians or vehicles and dimming the streetlight in their absence using semantic image segmentation from the CCTV video streaming. Consequently, our model distinguishes daylight and nighttime, which made it feasible to automate the process of turning the streetlight 'ON' and 'OFF' to save energy consumption costs. According to the aforementioned approach, geolocation sensor data could be utilized to make more informed streetlight management decisions. To complete the tasks, we consider training the U-net model with ResNet-34 as its backbone. The validity of the models is guaranteed with the use of assessment matrices. The suggested concept is straightforward, economical, energy-efficient, long-lasting, and more resilient than conventional alternatives.



### Why Deep Surgical Models Fail?: Revisiting Surgical Action Triplet Recognition through the Lens of Robustness
- **Arxiv ID**: http://arxiv.org/abs/2209.08647v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08647v2)
- **Published**: 2022-09-18 20:24:32+00:00
- **Updated**: 2023-02-20 17:05:30+00:00
- **Authors**: Yanqi Cheng, Lihao Liu, Shujun Wang, Yueming Jin, Carola-Bibiane Schönlieb, Angelica I. Aviles-Rivero
- **Comment**: None
- **Journal**: None
- **Summary**: Surgical action triplet recognition provides a better understanding of the surgical scene. This task is of high relevance as it provides the surgeon with context-aware support and safety. The current go-to strategy for improving performance is the development of new network mechanisms. However, the performance of current state-of-the-art techniques is substantially lower than other surgical tasks. Why is this happening? This is the question that we address in this work. We present the first study to understand the failure of existing deep learning models through the lens of robustness and explainability. Firstly, we study current existing models under weak and strong $\delta-$perturbations via an adversarial optimisation scheme. We then analyse the failure modes via feature based explanations. Our study reveals that the key to improving performance and increasing reliability is in the core and spurious attributes. Our work opens the door to more trustworthy and reliable deep learning models in surgical data science.



### Through a fair looking-glass: mitigating bias in image datasets
- **Arxiv ID**: http://arxiv.org/abs/2209.08648v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.08648v1)
- **Published**: 2022-09-18 20:28:36+00:00
- **Updated**: 2022-09-18 20:28:36+00:00
- **Authors**: Amirarsalan Rajabi, Mehdi Yazdani-Jahromi, Ozlem Ozmen Garibay, Gita Sukthankar
- **Comment**: None
- **Journal**: None
- **Summary**: With the recent growth in computer vision applications, the question of how fair and unbiased they are has yet to be explored. There is abundant evidence that the bias present in training data is reflected in the models, or even amplified. Many previous methods for image dataset de-biasing, including models based on augmenting datasets, are computationally expensive to implement. In this study, we present a fast and effective model to de-bias an image dataset through reconstruction and minimizing the statistical dependence between intended variables. Our architecture includes a U-net to reconstruct images, combined with a pre-trained classifier which penalizes the statistical dependence between target attribute and the protected attribute. We evaluate our proposed model on CelebA dataset, compare the results with a state-of-the-art de-biasing method, and show that the model achieves a promising fairness-accuracy combination.



### Learn the Time to Learn: Replay Scheduling in Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.08660v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08660v1)
- **Published**: 2022-09-18 21:29:58+00:00
- **Updated**: 2022-09-18 21:29:58+00:00
- **Authors**: Marcus Klasson, Hedvig Kjellström, Cheng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Replay methods have shown to be successful in mitigating catastrophic forgetting in continual learning scenarios despite having limited access to historical data. However, storing historical data is cheap in many real-world applications, yet replaying all historical data would be prohibited due to processing time constraints. In such settings, we propose learning the time to learn for a continual learning system, in which we learn replay schedules over which tasks to replay at different time steps. To demonstrate the importance of learning the time to learn, we first use Monte Carlo tree search to find the proper replay schedule and show that it can outperform fixed scheduling policies in terms of continual learning performance. Moreover, to improve the scheduling efficiency itself, we propose to use reinforcement learning to learn the replay scheduling policies that can generalize to new continual learning scenarios without added computational cost. In our experiments, we show the advantages of learning the time to learn, which brings current continual learning research closer to real-world needs.



### Semantic Segmentation using Neural Ordinary Differential Equations
- **Arxiv ID**: http://arxiv.org/abs/2209.08667v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08667v1)
- **Published**: 2022-09-18 22:13:55+00:00
- **Updated**: 2022-09-18 22:13:55+00:00
- **Authors**: Seyedalireza Khoshsirat, Chandra Kambhamettu
- **Comment**: None
- **Journal**: None
- **Summary**: The idea of neural Ordinary Differential Equations (ODE) is to approximate the derivative of a function (data model) instead of the function itself. In residual networks, instead of having a discrete sequence of hidden layers, the derivative of the continuous dynamics of hidden state can be parameterized by an ODE. It has been shown that this type of neural network is able to produce the same results as an equivalent residual network for image classification. In this paper, we design a novel neural ODE for the semantic segmentation task. We start by a baseline network that consists of residual modules, then we use the modules to build our neural ODE network. We show that our neural ODE is able to achieve the state-of-the-art results using 57% less memory for training, 42% less memory for testing, and 68% less number of parameters. We evaluate our model on the Cityscapes, CamVid, LIP, and PASCAL-Context datasets.



