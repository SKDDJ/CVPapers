# Arxiv Papers in cs.CV on 2022-09-10
### Self-supervised Human Mesh Recovery with Cross-Representation Alignment
- **Arxiv ID**: http://arxiv.org/abs/2209.04596v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.04596v1)
- **Published**: 2022-09-10 04:47:20+00:00
- **Updated**: 2022-09-10 04:47:20+00:00
- **Authors**: Xuan Gong, Meng Zheng, Benjamin Planche, Srikrishna Karanam, Terrence Chen, David Doermann, Ziyan Wu
- **Comment**: Accepted ECCV2022
- **Journal**: None
- **Summary**: Fully supervised human mesh recovery methods are data-hungry and have poor generalizability due to the limited availability and diversity of 3D-annotated benchmark datasets. Recent progress in self-supervised human mesh recovery has been made using synthetic-data-driven training paradigms where the model is trained from synthetic paired 2D representation (e.g., 2D keypoints and segmentation masks) and 3D mesh. However, on synthetic dense correspondence maps (i.e., IUV) few have been explored since the domain gap between synthetic training data and real testing data is hard to address for 2D dense representation. To alleviate this domain gap on IUV, we propose cross-representation alignment utilizing the complementary information from the robust but sparse representation (2D keypoints). Specifically, the alignment errors between initial mesh estimation and both 2D representations are forwarded into regressor and dynamically corrected in the following mesh regression. This adaptive cross-representation alignment explicitly learns from the deviations and captures complementary information: robustness from sparse representation and richness from dense representation. We conduct extensive experiments on multiple standard benchmark datasets and demonstrate competitive results, helping take a step towards reducing the annotation effort needed to produce state-of-the-art models in human mesh estimation.



### Preserving Privacy in Federated Learning with Ensemble Cross-Domain Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2209.04599v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.04599v1)
- **Published**: 2022-09-10 05:20:31+00:00
- **Updated**: 2022-09-10 05:20:31+00:00
- **Authors**: Xuan Gong, Abhishek Sharma, Srikrishna Karanam, Ziyan Wu, Terrence Chen, David Doermann, Arun Innanje
- **Comment**: Accepted by AAAI2022
- **Journal**: None
- **Summary**: Federated Learning (FL) is a machine learning paradigm where local nodes collaboratively train a central model while the training data remains decentralized. Existing FL methods typically share model parameters or employ co-distillation to address the issue of unbalanced data distribution. However, they suffer from communication bottlenecks. More importantly, they risk privacy leakage. In this work, we develop a privacy preserving and communication efficient method in a FL framework with one-shot offline knowledge distillation using unlabeled, cross-domain public data. We propose a quantized and noisy ensemble of local predictions from completely trained local models for stronger privacy guarantees without sacrificing accuracy. Based on extensive experiments on image classification and text classification tasks, we show that our privacy-preserving method outperforms baseline FL algorithms with superior performance in both accuracy and communication efficiency.



### Self-supervised Learning for Panoptic Segmentation of Multiple Fruit Flower Species
- **Arxiv ID**: http://arxiv.org/abs/2209.04618v1
- **DOI**: 10.1109/LRA.2022.3217000
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04618v1)
- **Published**: 2022-09-10 08:27:35+00:00
- **Updated**: 2022-09-10 08:27:35+00:00
- **Authors**: Abubakar Siddique, Amy Tabb, Henry Medeiros
- **Comment**: 8 pages, 7 figures
- **Journal**: IEEE Robotics and Automation Letters ( Volume: 7, Issue: 4,
  October 2022)
- **Summary**: Convolutional neural networks trained using manually generated labels are commonly used for semantic or instance segmentation. In precision agriculture, automated flower detection methods use supervised models and post-processing techniques that may not perform consistently as the appearance of the flowers and the data acquisition conditions vary. We propose a self-supervised learning strategy to enhance the sensitivity of segmentation models to different flower species using automatically generated pseudo-labels. We employ a data augmentation and refinement approach to improve the accuracy of the model predictions. The augmented semantic predictions are then converted to panoptic pseudo-labels to iteratively train a multi-task model. The self-supervised model predictions can be refined with existing post-processing approaches to further improve their accuracy. An evaluation on a multi-species fruit tree flower dataset demonstrates that our method outperforms state-of-the-art models without computationally expensive post-processing steps, providing a new baseline for flower detection applications.



### Real-time event simulation with frame-based cameras
- **Arxiv ID**: http://arxiv.org/abs/2209.04634v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.04634v2)
- **Published**: 2022-09-10 10:35:53+00:00
- **Updated**: 2023-03-23 17:31:25+00:00
- **Authors**: Andreas Ziegler, Daniel Teigland, Jonas Tebbe, Thomas Gossard, Andreas Zell
- **Comment**: Accepted for 2023 IEEE International Conference on Robotics and
  Automation (ICRA 2023). Project web page:
  https://cogsys-tuebingen.github.io/realtime_event_simulator/
- **Journal**: None
- **Summary**: Event cameras are becoming increasingly popular in robotics and computer vision due to their beneficial properties, e.g., high temporal resolution, high bandwidth, almost no motion blur, and low power consumption. However, these cameras remain expensive and scarce in the market, making them inaccessible to the majority. Using event simulators minimizes the need for real event cameras to develop novel algorithms. However, due to the computational complexity of the simulation, the event streams of existing simulators cannot be generated in real-time but rather have to be pre-calculated from existing video sequences or pre-rendered and then simulated from a virtual 3D scene. Although these offline generated event streams can be used as training data for learning tasks, all response time dependent applications cannot benefit from these simulators yet, as they still require an actual event camera. This work proposes simulation methods that improve the performance of event simulation by two orders of magnitude (making them real-time capable) while remaining competitive in the quality assessment.



### Large-Field Contextual Feature Learning for Glass Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.04639v1
- **DOI**: 10.1109/TPAMI.2022.3181973
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04639v1)
- **Published**: 2022-09-10 11:08:05+00:00
- **Updated**: 2022-09-10 11:08:05+00:00
- **Authors**: Haiyang Mei, Xin Yang, Letian Yu, Qiang Zhang, Xiaopeng Wei, Rynson W. H. Lau
- **Comment**: None
- **Journal**: None
- **Summary**: Glass is very common in our daily life. Existing computer vision systems neglect it and thus may have severe consequences, e.g., a robot may crash into a glass wall. However, sensing the presence of glass is not straightforward. The key challenge is that arbitrary objects/scenes can appear behind the glass. In this paper, we propose an important problem of detecting glass surfaces from a single RGB image. To address this problem, we construct the first large-scale glass detection dataset (GDD) and propose a novel glass detection network, called GDNet-B, which explores abundant contextual cues in a large field-of-view via a novel large-field contextual feature integration (LCFI) module and integrates both high-level and low-level boundary features with a boundary feature enhancement (BFE) module. Extensive experiments demonstrate that our GDNet-B achieves satisfying glass detection results on the images within and beyond the GDD testing set. We further validate the effectiveness and generalization capability of our proposed GDNet-B by applying it to other vision tasks, including mirror segmentation and salient object detection. Finally, we show the potential applications of glass detection and discuss possible future research directions.



### LSDNet: Trainable Modification of LSD Algorithm for Real-Time Line Segment Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.04642v1
- **DOI**: 10.1109/ACCESS.2022.3169177
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04642v1)
- **Published**: 2022-09-10 11:46:21+00:00
- **Updated**: 2022-09-10 11:46:21+00:00
- **Authors**: Lev Teplyakov, Leonid Erlygin, Evgeny Shvets
- **Comment**: None
- **Journal**: IEEE Access. 2022 Apr 21;10:45256-65
- **Summary**: As of today, the best accuracy in line segment detection (LSD) is achieved by algorithms based on convolutional neural networks - CNNs. Unfortunately, these methods utilize deep, heavy networks and are slower than traditional model-based detectors. In this paper we build an accurate yet fast CNN- based detector, LSDNet, by incorporating a lightweight CNN into a classical LSD detector. Specifically, we replace the first step of the original LSD algorithm - construction of line segments heatmap and tangent field from raw image gradients - with a lightweight CNN, which is able to calculate more complex and rich features. The second part of the LSD algorithm is used with only minor modifications. Compared with several modern line segment detectors on standard Wireframe dataset, the proposed LSDNet provides the highest speed (among CNN-based detectors) of 214 FPS with a competitive accuracy of 78 Fh . Although the best-reported accuracy is 83 Fh at 33 FPS, we speculate that the observed accuracy gap is caused by errors in annotations and the actual gap is significantly lower. We point out systematic inconsistencies in the annotations of popular line detection benchmarks - Wireframe and York Urban, carefully reannotate a subset of images and show that (i) existing detectors have improved quality on updated annotations without retraining, suggesting that new annotations correlate better with the notion of correct line segment detection; (ii) the gap between accuracies of our detector and others diminishes to negligible 0.2 Fh , with our method being the fastest.



### An Interactive Automation for Human Biliary Tree Diagnosis Using Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2209.04646v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.04646v1)
- **Published**: 2022-09-10 12:17:29+00:00
- **Updated**: 2022-09-10 12:17:29+00:00
- **Authors**: Mohammad AL-Oudat, Saleh Alomari, Hazem Qattous, Mohammad Azzeh, Tariq AL-Munaizel
- **Comment**: None
- **Journal**: None
- **Summary**: The biliary tree is a network of tubes that connects the liver to the gallbladder, an organ right beneath it. The bile duct is the major tube in the biliary tree. The dilatation of a bile duct is a key indicator for more major problems in the human body, such as stones and tumors, which are frequently caused by the pancreas or the papilla of vater. The detection of bile duct dilatation can be challenging for beginner or untrained medical personnel in many circumstances. Even professionals are unable to detect bile duct dilatation with the naked eye. This research presents a unique vision-based model for biliary tree initial diagnosis. To segment the biliary tree from the Magnetic Resonance Image, the framework used different image processing approaches (MRI). After the image's region of interest was segmented, numerous calculations were performed on it to extract 10 features, including major and minor axes, bile duct area, biliary tree area, compactness, and some textural features (contrast, mean, variance and correlation). This study used a database of images from King Hussein Medical Center in Amman, Jordan, which included 200 MRI images, 100 normal cases, and 100 patients with dilated bile ducts. After the characteristics are extracted, various classifiers are used to determine the patients' condition in terms of their health (normal or dilated). The findings demonstrate that the extracted features perform well with all classifiers in terms of accuracy and area under the curve. This study is unique in that it uses an automated approach to segment the biliary tree from MRI images, as well as scientifically correlating retrieved features with biliary tree status that has never been done before in the literature.



### CoreDeep: Improving Crack Detection Algorithms Using Width Stochasticity
- **Arxiv ID**: http://arxiv.org/abs/2209.04648v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.04648v1)
- **Published**: 2022-09-10 12:21:52+00:00
- **Updated**: 2022-09-10 12:21:52+00:00
- **Authors**: Ram Krishna Pandey, Akshit Achara
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically detecting or segmenting cracks in images can help in reducing the cost of maintenance or operations. Detecting, measuring and quantifying cracks for distress analysis in challenging background scenarios is a difficult task as there is no clear boundary that separates cracks from the background. Developed algorithms should handle the inherent challenges associated with data. Some of the perceptually noted challenges are color, intensity, depth, blur, motion-blur, orientation, different region of interest (ROI) for the defect, scale, illumination, complex and challenging background, etc. These variations occur across (crack inter class) and within images (crack intra-class variabilities). Overall, there is significant background (inter) and foreground (intra-class) variability. In this work, we have attempted to reduce the effect of these variations in challenging background scenarios. We have proposed a stochastic width (SW) approach to reduce the effect of these variations. Our proposed approach improves detectability and significantly reduces false positives and negatives. We have measured the performance of our algorithm objectively in terms of mean IoU, false positives and negatives and subjectively in terms of perceptual quality.



### OmDet: Language-Aware Object Detection with Large-scale Vision-Language Multi-dataset Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2209.05946v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2209.05946v1)
- **Published**: 2022-09-10 14:25:14+00:00
- **Updated**: 2022-09-10 14:25:14+00:00
- **Authors**: Tiancheng Zhao, Peng Liu, Xiaopeng Lu, Kyusong Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Advancing object detection to open-vocabulary and few-shot transfer has long been a challenge for computer vision research. This work explores a continual learning approach that enables a detector to expand its zero/few-shot capabilities via multi-dataset vision-language pre-training. Using natural language as knowledge representation, we explore methods to accumulate "visual vocabulary" from different training datasets and unify the task as a language-conditioned detection framework. Specifically, we propose a novel language-aware detector OmDet and a novel training mechanism. The proposed multimodal detection network can resolve the technical challenges in multi-dataset joint training and it can generalize to arbitrary number of training datasets without the requirements for manual label taxonomy merging. Experiment results on COCO, Pascal VOC, and Wider Face/Pedestrian confirmed the efficacy by achieving on par or higher scores in joint training compared to training separately. Moreover, we pre-train on more than 20 million images with 4 million unique object vocabulary, and the resulting model is evaluated on 35 downstream tasks of ODinW. Results show that OmDet is able to achieve the state-of-the-art fine-tuned performance on ODinW. And analysis shows that by scaling up the proposed pre-training method, OmDet continues to improve its zero/few-shot tuning performance, suggesting a promising way for further scaling.



### APTx: better activation function than MISH, SWISH, and ReLU's variants used in deep learning
- **Arxiv ID**: http://arxiv.org/abs/2209.06119v4
- **DOI**: 10.51483/IJAIML.2.2.2022.56-61
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2209.06119v4)
- **Published**: 2022-09-10 14:26:04+00:00
- **Updated**: 2023-03-10 17:31:32+00:00
- **Authors**: Ravin Kumar
- **Comment**: 8 pages, 6 figures
- **Journal**: International Journal of Artificial Intelligence and Machine
  Learning, 2(2), 56-61 (2022)
- **Summary**: Activation Functions introduce non-linearity in the deep neural networks. This nonlinearity helps the neural networks learn faster and efficiently from the dataset. In deep learning, many activation functions are developed and used based on the type of problem statement. ReLU's variants, SWISH, and MISH are goto activation functions. MISH function is considered having similar or even better performance than SWISH, and much better than ReLU. In this paper, we propose an activation function named APTx which behaves similar to MISH, but requires lesser mathematical operations to compute. The lesser computational requirements of APTx does speed up the model training, and thus also reduces the hardware requirement for the deep learning model.



### IR-LPR: Large Scale of Iranian License Plate Recognition Dataset
- **Arxiv ID**: http://arxiv.org/abs/2209.04680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04680v1)
- **Published**: 2022-09-10 14:41:59+00:00
- **Updated**: 2022-09-10 14:41:59+00:00
- **Authors**: Mahdi Rahmani, Melika Sabaghian, Seyyede Mahila Moghadami, Mohammad Mohsen Talaie, Mahdi Naghibi, Mohammad Ali Keyvanrad
- **Comment**: This is the final draft for the paper submitted to the 12th
  International Conference on Computer and Knowledge Engineering (ICCKE 2022),
  Ferdowsi University of Mashhad, Iran
- **Journal**: None
- **Summary**: Object detection has always been practical. There are so many things in our world that recognizing them can not only increase our automatic knowledge of the surroundings, but can also be lucrative for those interested in starting a new business. One of these attractive objects is the license plate (LP). In addition to the security uses that license plate detection can have, it can also be used to create creative businesses. With the development of object detection methods based on deep learning models, an appropriate and comprehensive dataset becomes doubly important. But due to the frequent commercial use of license plate datasets, there are limited datasets not only in Iran but also in the world. The largest Iranian dataset for detection license plates has 1,466 images. Also, the largest Iranian dataset for recognizing the characters of a license plate has 5,000 images. We have prepared a complete dataset including 20,967 car images along with all the detection annotation of the whole license plate and its characters, which can be useful for various purposes. Also, the total number of license plate images for character recognition application is 27,745 images.



### Explainable Image Quality Assessments in Teledermatological Photography
- **Arxiv ID**: http://arxiv.org/abs/2209.04699v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.04699v2)
- **Published**: 2022-09-10 15:48:28+00:00
- **Updated**: 2023-01-23 15:59:20+00:00
- **Authors**: Raluca Jalaboi, Ole Winther, Alfiia Galimzianova
- **Comment**: Accepted at the Telemedicine and eHealth Journal
- **Journal**: None
- **Summary**: Image quality is a crucial factor in the effectiveness and efficiency of teledermatological consultations. However, up to 50% of images sent by patients have quality issues, thus increasing the time to diagnosis and treatment. An automated, easily deployable, explainable method for assessing image quality is necessary to improve the current teledermatological consultation flow. We introduce ImageQX, a convolutional neural network for image quality assessment with a learning mechanism for identifying the most common poor image quality explanations: bad framing, bad lighting, blur, low resolution, and distance issues. ImageQX was trained on 26,635 photographs and validated on 9,874 photographs, each annotated with image quality labels and poor image quality explanations by up to 12 board-certified dermatologists. The photographic images were taken between 2017 and 2019 using a mobile skin disease tracking application accessible worldwide. Our method achieves expert-level performance for both image quality assessment and poor image quality explanation. For image quality assessment, ImageQX obtains a macro F1-score of 0.73 +- 0.01, which places it within standard deviation of the pairwise inter-rater F1-score of 0.77 +- 0.07. For poor image quality explanations, our method obtains F1-scores of between 0.37 +- 0.01 and 0.70 +- 0.01, similar to the inter-rater pairwise F1-score of between 0.24 +- 0.15 and 0.83 +- 0.06. Moreover, with a size of only 15 MB, ImageQX is easily deployable on mobile devices. With an image quality detection performance similar to that of dermatologists, incorporating ImageQX into the teledermatology flow can enable a better, faster flow for remote consultations.



### People detection and social distancing classification in smart cities for COVID-19 by using thermal images and deep learning algorithms
- **Arxiv ID**: http://arxiv.org/abs/2209.04704v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.04704v1)
- **Published**: 2022-09-10 16:30:29+00:00
- **Updated**: 2022-09-10 16:30:29+00:00
- **Authors**: Abdussalam Elhanashi, Sergio Saponara, Alessio Gagliardi
- **Comment**: 2 pages, 2 figures , conference (ICT for Smart Cities)
- **Journal**: None
- **Summary**: COVID-19 is a disease caused by severe respiratory syndrome coronavirus. It was identified in December 2019 in Wuhan, China. It has resulted in an ongoing pandemic that caused infected cases including some deaths. Coronavirus is primarily spread between people during close contact. Motivating to this notion, this research proposes an artificial intelligence system for social distancing classification of persons by using thermal images. By exploiting YOLOv2 (you look at once), a deep learning detection technique is developed for detecting and tracking people in indoor and outdoor scenarios. An algorithm is also implemented for measuring and classifying the distance between persons and automatically check if social distancing rules are respected or not. Hence, this work aims at minimizing the spread of the COVID-19 virus by evaluating if and how persons comply with social distancing rules. The proposed approach is applied to images acquired through thermal cameras, to establish a complete AI system for people tracking, social distancing classification, and body temperature monitoring. The training phase is done with two datasets captured from different thermal cameras. Ground Truth Labeler app is used for labeling the persons in the images. The achieved results show that the proposed method is suitable for the creation of a smart surveillance system in smart cities for people detection, social distancing classification, and body temperature analysis.



### Use Classifier as Generator
- **Arxiv ID**: http://arxiv.org/abs/2209.09210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09210v1)
- **Published**: 2022-09-10 18:46:01+00:00
- **Updated**: 2022-09-10 18:46:01+00:00
- **Authors**: Haoyang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Image recognition/classification is a widely studied problem, but its reverse problem, image generation, has drawn much less attention until recently. But the vast majority of current methods for image generation require training/retraining a classifier and/or a generator with certain constraints, which can be hard to achieve. In this paper, we propose a simple approach to directly use a normally trained classifier to generate images. We evaluate our method on MNIST and show that it produces recognizable results for human eyes with limited quality with experiments.



### Anticipating the Unseen Discrepancy for Vision and Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2209.04725v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2209.04725v1)
- **Published**: 2022-09-10 19:04:40+00:00
- **Updated**: 2022-09-10 19:04:40+00:00
- **Authors**: Yujie Lu, Huiliang Zhang, Ping Nie, Weixi Feng, Wenda Xu, Xin Eric Wang, William Yang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-Language Navigation requires the agent to follow natural language instructions to reach a specific target. The large discrepancy between seen and unseen environments makes it challenging for the agent to generalize well. Previous studies propose data augmentation methods to mitigate the data bias explicitly or implicitly and provide improvements in generalization. However, they try to memorize augmented trajectories and ignore the distribution shifts under unseen environments at test time. In this paper, we propose an Unseen Discrepancy Anticipating Vision and Language Navigation (DAVIS) that learns to generalize to unseen environments via encouraging test-time visual consistency. Specifically, we devise: 1) a semi-supervised framework DAVIS that leverages visual consistency signals across similar semantic observations. 2) a two-stage learning procedure that encourages adaptation to test-time distribution. The framework enhances the basic mixture of imitation and reinforcement learning with Momentum Contrast to encourage stable decision-making on similar observations under a joint training stage and a test-time adaptation stage. Extensive experiments show that DAVIS achieves model-agnostic improvement over previous state-of-the-art VLN baselines on R2R and RxR benchmarks. Our source code and data are in supplemental materials.



### Diffusion Models in Vision: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2209.04747v5
- **DOI**: 10.1109/TPAMI.2023.3261988
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.04747v5)
- **Published**: 2022-09-10 22:00:30+00:00
- **Updated**: 2023-04-01 14:27:33+00:00
- **Authors**: Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah
- **Comment**: Accepted in IEEE Transactions on Pattern Analysis and Machine
  Intelligence. 25 pages, 3 figures
- **Journal**: None
- **Summary**: Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.



