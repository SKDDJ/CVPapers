# Arxiv Papers in cs.CV on 2022-09-09
### TEACH: Temporal Action Composition for 3D Humans
- **Arxiv ID**: http://arxiv.org/abs/2209.04066v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04066v2)
- **Published**: 2022-09-09 00:33:40+00:00
- **Updated**: 2022-09-12 16:34:20+00:00
- **Authors**: Nikos Athanasiou, Mathis Petrovich, Michael J. Black, GÃ¼l Varol
- **Comment**: 3DV 2022 Camera Ready, Affiliations corrected
- **Journal**: None
- **Summary**: Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for "TEmporal Action Compositions for Human motions", produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at our $\href{teach.is.tue.mpg.de}{\text{website}}$.



### Learning Audio-Visual embedding for Person Verification in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2209.04093v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2209.04093v2)
- **Published**: 2022-09-09 02:29:47+00:00
- **Updated**: 2022-10-26 13:55:55+00:00
- **Authors**: Peiwen Sun, Shanshan Zhang, Zishan Liu, Yougen Yuan, Taotao Zhang, Honggang Zhang, Pengfei Hu
- **Comment**: None
- **Journal**: None
- **Summary**: It has already been observed that audio-visual embedding is more robust than uni-modality embedding for person verification. Here, we proposed a novel audio-visual strategy that considers aggregators from a fusion perspective. First, we introduced weight-enhanced attentive statistics pooling for the first time in face verification. We find that a strong correlation exists between modalities during pooling, so joint attentive pooling is proposed which contains cycle consistency to learn the implicit inter-frame weight. Finally, each modality is fused with a gated attention mechanism to gain robust audio-visual embedding. All the proposed models are trained on the VoxCeleb2 dev dataset and the best system obtains 0.18%, 0.27%, and 0.49% EER on three official trial lists of VoxCeleb1 respectively, which is to our knowledge the best-published results for person verification.



### MassMIND: Massachusetts Maritime INfrared Dataset
- **Arxiv ID**: http://arxiv.org/abs/2209.04097v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.04097v1)
- **Published**: 2022-09-09 02:54:26+00:00
- **Updated**: 2022-09-09 02:54:26+00:00
- **Authors**: Shailesh Nirgudkar, Michael DeFilippo, Michael Sacarny, Michael Benjamin, Paul Robinette
- **Comment**: 10 pages, 10 figures, submitted to IJRR for review
- **Journal**: None
- **Summary**: Recent advances in deep learning technology have triggered radical progress in the autonomy of ground vehicles. Marine coastal Autonomous Surface Vehicles (ASVs) that are regularly used for surveillance, monitoring and other routine tasks can benefit from this autonomy. Long haul deep sea transportation activities are additional opportunities. These two use cases present very different terrains -- the first being coastal waters -- with many obstacles, structures and human presence while the latter is mostly devoid of such obstacles. Variations in environmental conditions are common to both terrains. Robust labeled datasets mapping such terrains are crucial in improving the situational awareness that can drive autonomy. However, there are only limited such maritime datasets available and these primarily consist of optical images. Although, Long Wave Infrared (LWIR) is a strong complement to the optical spectrum that helps in extreme light conditions, a labeled public dataset with LWIR images does not currently exist. In this paper, we fill this gap by presenting a labeled dataset of over 2,900 LWIR segmented images captured in coastal maritime environment under diverse conditions. The images are labeled using instance segmentation and classified in seven categories -- sky, water, obstacle, living obstacle, bridge, self and background. We also evaluate this dataset across three deep learning architectures (UNet, PSPNet, DeepLabv3) and provide detailed analysis of its efficacy. While the dataset focuses on the coastal terrain it can equally help deep sea use cases. Such terrain would have less traffic, and the classifier trained on cluttered environment would be able to handle sparse scenes effectively. We share this dataset with the research community with the hope that it spurs new scene understanding capabilities in the maritime environment.



### Robust and Lossless Fingerprinting of Deep Neural Networks via Pooled Membership Inference
- **Arxiv ID**: http://arxiv.org/abs/2209.04113v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.04113v2)
- **Published**: 2022-09-09 04:06:29+00:00
- **Updated**: 2022-11-01 01:20:28+00:00
- **Authors**: Hanzhou Wu
- **Comment**: https://scholar.google.com/citations?user=IdiF7M0AAAAJ&hl=en
- **Journal**: IEEE International Conference on High Performance Computing and
  Communications 2022 (AR: 117/663 = 17.6%)
- **Summary**: Deep neural networks (DNNs) have already achieved great success in a lot of application areas and brought profound changes to our society. However, it also raises new security problems, among which how to protect the intellectual property (IP) of DNNs against infringement is one of the most important yet very challenging topics. To deal with this problem, recent studies focus on the IP protection of DNNs by applying digital watermarking, which embeds source information and/or authentication data into DNN models by tuning network parameters directly or indirectly. However, tuning network parameters inevitably distorts the DNN and therefore surely impairs the performance of the DNN model on its original task regardless of the degree of the performance degradation. It has motivated the authors in this paper to propose a novel technique called pooled membership inference (PMI) so as to protect the IP of the DNN models. The proposed PMI neither alters the network parameters of the given DNN model nor fine-tunes the DNN model with a sequence of carefully crafted trigger samples. Instead, it leaves the original DNN model unchanged, but can determine the ownership of the DNN model by inferring which mini-dataset among multiple mini-datasets was once used to train the target DNN model, which differs from previous arts and has remarkable potential in practice. Experiments also have demonstrated the superiority and applicability of this work.



### Reconstruction of Three-dimensional Scroll Waves in Excitable Media from Two-Dimensional Observations using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.06860v2
- **DOI**: 10.1103/PhysRevE.107.014221
- **Categories**: **q-bio.TO**, cs.CV, physics.med-ph, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2209.06860v2)
- **Published**: 2022-09-09 04:45:29+00:00
- **Updated**: 2022-11-23 07:20:45+00:00
- **Authors**: Jan Lebert, Meenakshi Mittal, Jan Christoph
- **Comment**: None
- **Journal**: Phys. Rev. E 107, 014221 (2023)
- **Summary**: Scroll wave chaos is thought to underlie life-threatening ventricular fibrillation. However, currently there is no direct way to measure action potential wave patterns transmurally throughout the thick ventricular heart muscle. Consequently, direct observations of three-dimensional electrical scroll waves remains elusive. Here, we study whether it is possible to reconstruct simulated scroll waves and scroll wave chaos using deep learning. We trained encoding-decoding convolutional neural networks to predict three-dimensional scroll wave dynamics inside bulk-shaped excitable media from two-dimensional observations of the wave dynamics on the bulk's surface. We tested whether observations from one or two opposing surfaces would be sufficient, and whether transparency or measurements of surface deformations enhances the reconstruction. Further, we evaluated the approach's robustness against noise and tested the feasibility of predicting the bulk's thickness. We distinguished isotropic and anisotropic, as well as opaque and transparent excitable media as models for cardiac tissue and the Belousov-Zhabotinsky chemical reaction, respectively. While we demonstrate that it is possible to reconstruct three-dimensional scroll wave dynamics, we also show that it is challenging to reconstruct complicated scroll wave chaos and that prediction outcomes depend on various factors such as transparency, anisotropy and ultimately the thickness of the medium compared to the size of the scroll waves. In particular, we found that anisotropy provides crucial information for neural networks to decode depth, which facilitates the reconstructions. In the future, deep neural networks could be used to visualize intramural action potential wave patterns from epi- or endocardial measurements.



### ISS: Image as Stepping Stone for Text-Guided 3D Shape Generation
- **Arxiv ID**: http://arxiv.org/abs/2209.04145v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04145v6)
- **Published**: 2022-09-09 06:54:21+00:00
- **Updated**: 2023-02-24 01:38:20+00:00
- **Authors**: Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, Chi-Wing Fu
- **Comment**: ICLR 2023 spotlight
- **Journal**: None
- **Summary**: Text-guided 3D shape generation remains challenging due to the absence of large paired text-shape data, the substantial semantic gap between these two modalities, and the structural complexity of 3D shapes. This paper presents a new framework called Image as Stepping Stone (ISS) for the task by introducing 2D image as a stepping stone to connect the two modalities and to eliminate the need for paired text-shape data. Our key contribution is a two-stage feature-space-alignment approach that maps CLIP features to shapes by harnessing a pre-trained single-view reconstruction (SVR) model with multi-view supervisions: first map the CLIP image feature to the detail-rich shape space in the SVR model, then map the CLIP text feature to the shape space and optimize the mapping by encouraging CLIP consistency between the input text and the rendered images. Further, we formulate a text-guided shape stylization module to dress up the output shapes with novel textures. Beyond existing works on 3D shape generation from text, our new approach is general for creating shapes in a broad range of categories, without requiring paired text-shape data. Experimental results manifest that our approach outperforms the state-of-the-arts and our baselines in terms of fidelity and consistency with text. Further, our approach can stylize the generated shapes with both realistic and fantasy structures and textures.



### Domain-specific Learning of Multi-scale Facial Dynamics for Apparent Personality Traits Prediction
- **Arxiv ID**: http://arxiv.org/abs/2209.04148v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T05
- **Links**: [PDF](http://arxiv.org/pdf/2209.04148v1)
- **Published**: 2022-09-09 07:08:55+00:00
- **Updated**: 2022-09-09 07:08:55+00:00
- **Authors**: Fang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Human personality decides various aspects of their daily life and working behaviors. Since personality traits are relatively stable over time and unique for each subject, previous approaches frequently infer personality from a single frame or short-term behaviors. Moreover, most of them failed to specifically extract person-specific and unique cues for personality recognition. In this paper, we propose a novel video-based automatic personality traits recognition approach which consists of: (1) a \textbf{domain-specific facial behavior modelling} module that extracts personality-related multi-scale short-term human facial behavior features; (2) a \textbf{long-term behavior modelling} module that summarizes all short-term features of a video as a long-term/video-level personality representation and (3) a \textbf{multi-task personality traits prediction module} that models underlying relationship among all traits and jointly predict them based on the video-level personality representation. We conducted the experiments on ChaLearn First Impression dataset, and our approach achieved comparable results to the state-of-the-art. Importantly, we show that all three proposed modules brought important benefits for personality recognition.



### Generative Deformable Radiance Fields for Disentangled Image Synthesis of Topology-Varying Objects
- **Arxiv ID**: http://arxiv.org/abs/2209.04183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04183v1)
- **Published**: 2022-09-09 08:44:06+00:00
- **Updated**: 2022-09-09 08:44:06+00:00
- **Authors**: Ziyu Wang, Yu Deng, Jiaolong Yang, Jingyi Yu, Xin Tong
- **Comment**: Accepted at Pacific Graphics 2022 & COMPUTER GRAPHICS Forum, Project
  Page: https://ziyuwang98.github.io/GDRF/
- **Journal**: None
- **Summary**: 3D-aware generative models have demonstrated their superb performance to generate 3D neural radiance fields (NeRF) from a collection of monocular 2D images even for topology-varying object categories. However, these methods still lack the capability to separately control the shape and appearance of the objects in the generated radiance fields. In this paper, we propose a generative model for synthesizing radiance fields of topology-varying objects with disentangled shape and appearance variations. Our method generates deformable radiance fields, which builds the dense correspondence between the density fields of the objects and encodes their appearances in a shared template field. Our disentanglement is achieved in an unsupervised manner without introducing extra labels to previous 3D-aware GAN training. We also develop an effective image inversion scheme for reconstructing the radiance field of an object in a real monocular image and manipulating its shape and appearance. Experiments show that our method can successfully learn the generative model from unstructured monocular images and well disentangle the shape and appearance for objects (e.g., chairs) with large topological variance. The model trained on synthetic data can faithfully reconstruct the real object in a given single image and achieve high-quality texture and shape editing results.



### An Indian Roads Dataset for Supported and Suspended Traffic Lights Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.04203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04203v1)
- **Published**: 2022-09-09 09:37:50+00:00
- **Updated**: 2022-09-09 09:37:50+00:00
- **Authors**: Sarita Gautam, Anuj Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous vehicles are growing rapidly, in well-developed nations like America, Europe, and China. Tech giants like Google, Tesla, Audi, BMW, and Mercedes are building highly efficient self-driving vehicles. However, the technology is still not mainstream for developing nations like India, Thailand, Africa, etc., In this paper, we present a thorough comparison of the existing datasets based on well-developed nations as well as Indian roads. We then developed a new dataset "Indian Roads Dataset" (IRD) having more than 8000 annotations extracted from 3000+ images shot using a 64 (megapixel) camera. All the annotations are manually labelled adhering to the strict rules of annotations. Real-time video sequences have been captured from two different cities in India namely New Delhi and Chandigarh during the day and night-light conditions. Our dataset exceeds previous Indian traffic light datasets in size, annotations, and variance. We prove the amelioration of our dataset by providing an extensive comparison with existing Indian datasets. Various dataset criteria like size, capturing device, a number of cities, and variations of traffic light orientations are considered. The dataset can be downloaded from here https://sites.google.com/view/ird-dataset/home



### Selecting Related Knowledge via Efficient Channel Attention for Online Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.04212v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.04212v1)
- **Published**: 2022-09-09 09:59:54+00:00
- **Updated**: 2022-09-09 09:59:54+00:00
- **Authors**: Ya-nan Han, Jian-wei Liu
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Continual learning aims to learn a sequence of tasks by leveraging the knowledge acquired in the past in an online-learning manner while being able to perform well on all previous tasks, this ability is crucial to the artificial intelligence (AI) system, hence continual learning is more suitable for most real-word and complex applicative scenarios compared to the traditional learning pattern. However, the current models usually learn a generic representation base on the class label on each task and an effective strategy is selected to avoid catastrophic forgetting. We postulate that selecting the related and useful parts only from the knowledge obtained to perform each task is more effective than utilizing the whole knowledge. Based on this fact, in this paper we propose a new framework, named Selecting Related Knowledge for Online Continual Learning (SRKOCL), which incorporates an additional efficient channel attention mechanism to pick the particular related knowledge for every task. Our model also combines experience replay and knowledge distillation to circumvent the catastrophic forgetting. Finally, extensive experiments are conducted on different benchmarks and the competitive experimental results demonstrate that our proposed SRKOCL is a promised approach against the state-of-the-art.



### Self-supervised Learning for Heterogeneous Graph via Structure Information based on Metapath
- **Arxiv ID**: http://arxiv.org/abs/2209.04218v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.04218v1)
- **Published**: 2022-09-09 10:06:18+00:00
- **Updated**: 2022-09-09 10:06:18+00:00
- **Authors**: Shuai Ma, Jian-wei Liu, Xin Zuo
- **Comment**: 32 pages
- **Journal**: None
- **Summary**: graph neural networks (GNNs) are the dominant paradigm for modeling and handling graph structure data by learning universal node representation. The traditional way of training GNNs depends on a great many labeled data, which results in high requirements on cost and time. In some special scene, it is even unavailable and impracticable. Self-supervised representation learning, which can generate labels by graph structure data itself, is a potential approach to tackle this problem. And turning to research on self-supervised learning problem for heterogeneous graphs is more challenging than dealing with homogeneous graphs, also there are fewer studies about it. In this paper, we propose a SElfsupervised learning method for heterogeneous graph via Structure Information based on Metapath (SESIM). The proposed model can construct pretext tasks by predicting jump number between nodes in each metapath to improve the representation ability of primary task. In order to predict jump number, SESIM uses data itself to generate labels, avoiding time-consuming manual labeling. Moreover, predicting jump number in each metapath can effectively utilize graph structure information, which is the essential property between nodes. Therefore, SESIM deepens the understanding of models for graph structure. At last, we train primary task and pretext tasks jointly, and use meta-learning to balance the contribution of pretext tasks for primary task. Empirical results validate the performance of SESIM method and demonstrate that this method can improve the representation ability of traditional neural networks on link prediction task and node classification task.



### Pathology Synthesis of 3D-Consistent Cardiac MR Images using 2D VAEs and GANs
- **Arxiv ID**: http://arxiv.org/abs/2209.04223v2
- **DOI**: 10.59275/j.melba.2023-1g8b
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.04223v2)
- **Published**: 2022-09-09 10:17:49+00:00
- **Updated**: 2023-05-30 14:37:11+00:00
- **Authors**: Sina Amirrajab, Cristian Lorenz, Juergen Weese, Josien Pluim, Marcel Breeuwer
- **Comment**: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://www.melba-journal.org/2023:010
- **Journal**: Machine.Learning.for.Biomedical.Imaging. 2 (2023)
- **Summary**: We propose a method for synthesizing cardiac magnetic resonance (MR) images with plausible heart pathologies and realistic appearances for the purpose of generating labeled data for the application of supervised deep-learning (DL) training. The image synthesis consists of label deformation and label-to-image translation tasks. The former is achieved via latent space interpolation in a VAE model, while the latter is accomplished via a label-conditional GAN model. We devise three approaches for label manipulation in the latent space of the trained VAE model; i) \textbf{intra-subject synthesis} aiming to interpolate the intermediate slices of a subject to increase the through-plane resolution, ii) \textbf{inter-subject synthesis} aiming to interpolate the geometry and appearance of intermediate images between two dissimilar subjects acquired with different scanner vendors, and iii) \textbf{pathology synthesis} aiming to synthesize a series of pseudo-pathological synthetic subjects with characteristics of a desired heart disease. Furthermore, we propose to model the relationship between 2D slices in the latent space of the VAE prior to reconstruction for generating 3D-consistent subjects from stacking up 2D slice-by-slice generations. We demonstrate that such an approach could provide a solution to diversify and enrich an available database of cardiac MR images and to pave the way for the development of generalizable DL-based image analysis algorithms. We quantitatively evaluate the quality of the synthesized data in an augmentation scenario to achieve generalization and robustness to multi-vendor and multi-disease data for image segmentation. Our code is available at https://github.com/sinaamirrajab/CardiacPathologySynthesis.



### Retinal Image Restoration and Vessel Segmentation using Modified Cycle-CBAM and CBAM-UNet
- **Arxiv ID**: http://arxiv.org/abs/2209.04234v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.04234v2)
- **Published**: 2022-09-09 10:47:20+00:00
- **Updated**: 2022-10-05 11:41:51+00:00
- **Authors**: Alnur Alimanov, Md Baharul Islam
- **Comment**: 6 pages, 7 figures, conference
- **Journal**: None
- **Summary**: Clinical screening with low-quality fundus images is challenging and significantly leads to misdiagnosis. This paper addresses the issue of improving the retinal image quality and vessel segmentation through retinal image restoration. More specifically, a cycle-consistent generative adversarial network (CycleGAN) with a convolution block attention module (CBAM) is used for retinal image restoration. A modified UNet is used for retinal vessel segmentation for the restored retinal images (CBAM-UNet). The proposed model consists of two generators and two discriminators. Generators translate images from one domain to another, i.e., from low to high quality and vice versa. Discriminators classify generated and original images. The retinal vessel segmentation model uses downsampling, bottlenecking, and upsampling layers to generate segmented images. The CBAM has been used to enhance the feature extraction of these models. The proposed method does not require paired image datasets, which are challenging to produce. Instead, it uses unpaired data that consists of low- and high-quality fundus images retrieved from publicly available datasets. The restoration performance of the proposed method was evaluated using full-reference evaluation metrics, e.g., peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). The retinal vessel segmentation performance was compared with the ground-truth fundus images. The proposed method can significantly reduce the degradation effects caused by out-of-focus blurring, color distortion, low, high, and uneven illumination. Experimental results show the effectiveness of the proposed method for retinal image restoration and vessel segmentation.



### EchoCoTr: Estimation of the Left Ventricular Ejection Fraction from Spatiotemporal Echocardiography
- **Arxiv ID**: http://arxiv.org/abs/2209.04242v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04242v1)
- **Published**: 2022-09-09 11:01:59+00:00
- **Updated**: 2022-09-09 11:01:59+00:00
- **Authors**: Rand Muhtaseb, Mohammad Yaqub
- **Comment**: None
- **Journal**: None
- **Summary**: Learning spatiotemporal features is an important task for efficient video understanding especially in medical images such as echocardiograms. Convolutional neural networks (CNNs) and more recent vision transformers (ViTs) are the most commonly used methods with limitations per each. CNNs are good at capturing local context but fail to learn global information across video frames. On the other hand, vision transformers can incorporate global details and long sequences but are computationally expensive and typically require more data to train. In this paper, we propose a method that addresses the limitations we typically face when training on medical video data such as echocardiographic scans. The algorithm we propose (EchoCoTr) utilizes the strength of vision transformers and CNNs to tackle the problem of estimating the left ventricular ejection fraction (LVEF) on ultrasound videos. We demonstrate how the proposed method outperforms state-of-the-art work to-date on the EchoNet-Dynamic dataset with MAE of 3.95 and $R^2$ of 0.82. These results show noticeable improvement compared to all published research. In addition, we show extensive ablations and comparisons with several algorithms, including ViT and BERT. The code is available at https://github.com/BioMedIA-MBZUAI/EchoCoTr.



### Talking Head from Speech Audio using a Pre-trained Image Generator
- **Arxiv ID**: http://arxiv.org/abs/2209.04252v1
- **DOI**: 10.1145/3503161.3548101
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04252v1)
- **Published**: 2022-09-09 11:20:37+00:00
- **Updated**: 2022-09-09 11:20:37+00:00
- **Authors**: Mohammed M. Alghamdi, He Wang, Andrew J. Bulpitt, David C. Hogg
- **Comment**: Accepted at ACM Multimedia 2022. The Project webpage can found at
  https://mohammedalghamdi.github.io/talking-heads-acm-mm
- **Journal**: None
- **Summary**: We propose a novel method for generating high-resolution videos of talking-heads from speech audio and a single 'identity' image. Our method is based on a convolutional neural network model that incorporates a pre-trained StyleGAN generator. We model each frame as a point in the latent space of StyleGAN so that a video corresponds to a trajectory through the latent space. Training the network is in two stages. The first stage is to model trajectories in the latent space conditioned on speech utterances. To do this, we use an existing encoder to invert the generator, mapping from each video frame into the latent space. We train a recurrent neural network to map from speech utterances to displacements in the latent space of the image generator. These displacements are relative to the back-projection into the latent space of an identity image chosen from the individuals depicted in the training dataset. In the second stage, we improve the visual quality of the generated videos by tuning the image generator on a single image or a short video of any chosen identity. We evaluate our model on standard measures (PSNR, SSIM, FID and LMD) and show that it significantly outperforms recent state-of-the-art methods on one of two commonly used datasets and gives comparable performance on the other. Finally, we report on ablation experiments that validate the components of the model. The code and videos from experiments can be found at https://mohammedalghamdi.github.io/talking-heads-acm-mm



### Temporally Adjustable Longitudinal Fluid-Attenuated Inversion Recovery MRI Estimation / Synthesis for Multiple Sclerosis
- **Arxiv ID**: http://arxiv.org/abs/2209.04275v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2209.04275v1)
- **Published**: 2022-09-09 12:42:00+00:00
- **Updated**: 2022-09-09 12:42:00+00:00
- **Authors**: Jueqi Wang, Derek Berger, Erin Mazerolle, Othman Soufan, Jacob Levman
- **Comment**: 2022 MICCAI BrainLes Workshop paper
- **Journal**: None
- **Summary**: Multiple Sclerosis (MS) is a chronic progressive neurological disease characterized by the development of lesions in the white matter of the brain. T2-fluid-attenuated inversion recovery (FLAIR) brain magnetic resonance imaging (MRI) provides superior visualization and characterization of MS lesions, relative to other MRI modalities. Longitudinal brain FLAIR MRI in MS, involving repetitively imaging a patient over time, provides helpful information for clinicians towards monitoring disease progression. Predicting future whole brain MRI examinations with variable time lag has only been attempted in limited applications, such as healthy aging and structural degeneration in Alzheimer's Disease. In this article, we present novel modifications to deep learning architectures for MS FLAIR image synthesis, in order to support prediction of longitudinal images in a flexible continuous way. This is achieved with learned transposed convolutions, which support modelling time as a spatially distributed array with variable temporal properties at different spatial locations. Thus, this approach can theoretically model spatially-specific time-dependent brain development, supporting the modelling of more rapid growth at appropriate physical locations, such as the site of an MS brain lesion. This approach also supports the clinician user to define how far into the future a predicted examination should target. Accurate prediction of future rounds of imaging can inform clinicians of potentially poor patient outcomes, which may be able to contribute to earlier treatment and better prognoses. Four distinct deep learning architectures have been developed. The ISBI2015 longitudinal MS dataset was used to validate and compare our proposed approaches. Results demonstrate that a modified ACGAN achieves the best performance and reduces variability in model accuracy.



### Deep learning-based Crop Row Detection for Infield Navigation of Agri-Robots
- **Arxiv ID**: http://arxiv.org/abs/2209.04278v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.04278v2)
- **Published**: 2022-09-09 12:47:24+00:00
- **Updated**: 2023-08-10 15:19:34+00:00
- **Authors**: Rajitha de Silva, Grzegorz Cielniak, Gang Wang, Junfeng Gao
- **Comment**: Published in Journal of Field Robotics:
  https://onlinelibrary.wiley.com/doi/epdf/10.1002/rob.22238
- **Journal**: None
- **Summary**: Autonomous navigation in agricultural environments is challenged by varying field conditions that arise in arable fields. State-of-the-art solutions for autonomous navigation in such environments require expensive hardware such as RTK-GNSS. This paper presents a robust crop row detection algorithm that withstands such field variations using inexpensive cameras. Existing datasets for crop row detection does not represent all the possible field variations. A dataset of sugar beet images was created representing 11 field variations comprised of multiple grow stages, light levels, varying weed densities, curved crop rows and discontinuous crop rows. The proposed pipeline segments the crop rows using a deep learning-based method and employs the predicted segmentation mask for extraction of the central crop using a novel central crop row selection algorithm. The novel crop row detection algorithm was tested for crop row detection performance and the capability of visual servoing along a crop row. The visual servoing-based navigation was tested on a realistic simulation scenario with the real ground and plant textures. Our algorithm demonstrated robust vision-based crop row detection in challenging field conditions outperforming the baseline.



### Tracking Small and Fast Moving Objects: A Benchmark
- **Arxiv ID**: http://arxiv.org/abs/2209.04284v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04284v1)
- **Published**: 2022-09-09 13:14:44+00:00
- **Updated**: 2022-09-09 13:14:44+00:00
- **Authors**: Zhewen Zhang, Fuliang Wu, Yuming Qiu, Jingdong Liang, Shuiwang Li
- **Comment**: arXiv admin note: text overlap with arXiv:2011.10875 by other authors
- **Journal**: None
- **Summary**: With more and more large-scale datasets available for training, visual tracking has made great progress in recent years. However, current research in the field mainly focuses on tracking generic objects. In this paper, we present TSFMO, a benchmark for \textbf{T}racking \textbf{S}mall and \textbf{F}ast \textbf{M}oving \textbf{O}bjects. This benchmark aims to encourage research in developing novel and accurate methods for this challenging task particularly. TSFMO consists of 250 sequences with about 50k frames in total. Each frame in these sequences is carefully and manually annotated with a bounding box. To the best of our knowledge, TSFMO is the first benchmark dedicated to tracking small and fast moving objects, especially connected to sports. To understand how existing methods perform and to provide comparison for future research on TSFMO, we extensively evaluate 20 state-of-the-art trackers on the benchmark. The evaluation results exhibit that more effort are required to improve tracking small and fast moving objects. Moreover, to encourage future research, we proposed a novel tracker S-KeepTrack which surpasses all 20 evaluated approaches. By releasing TSFMO, we expect to facilitate future researches and applications of tracking small and fast moving objects. The TSFMO and evaluation results as well as S-KeepTrack are available at \url{https://github.com/CodeOfGithub/S-KeepTrack}.



### Towards Confidence-guided Shape Completion for Robotic Applications
- **Arxiv ID**: http://arxiv.org/abs/2209.04300v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.04300v1)
- **Published**: 2022-09-09 13:48:24+00:00
- **Updated**: 2022-09-09 13:48:24+00:00
- **Authors**: Andrea Rosasco, Stefano Berti, Fabrizio Bottarel, Michele Colledanchise, Lorenzo Natale
- **Comment**: None
- **Journal**: IEEE International Conference on Humanoid Robots, 2022
- **Summary**: Many robotic tasks involving some form of 3D visual perception greatly benefit from a complete knowledge of the working environment. However, robots often have to tackle unstructured environments and their onboard visual sensors can only provide incomplete information due to limited workspaces, clutter or object self-occlusion. In recent years, deep learning architectures for shape completion have begun taking traction as effective means of inferring a complete 3D object representation from partial visual data. Nevertheless, most of the existing state-of-the-art approaches provide a fixed output resolution in the form of voxel grids, strictly related to the size of the neural network output stage. While this is enough for some tasks, e.g. obstacle avoidance in navigation, grasping and manipulation require finer resolutions and simply scaling up the neural network outputs is computationally expensive. In this paper, we address this limitation by proposing an object shape completion method based on an implicit 3D representation providing a confidence value for each reconstructed point. As a second contribution, we propose a gradient-based method for efficiently sampling such implicit function at an arbitrary resolution, tunable at inference time. We experimentally validate our approach by comparing reconstructed shapes with ground truths, and by deploying our shape completion algorithm in a robotic grasping pipeline. In both cases, we compare results with a state-of-the-art shape completion approach.



### Saliency Guided Adversarial Training for Learning Generalizable Features with Applications to Medical Imaging Classification System
- **Arxiv ID**: http://arxiv.org/abs/2209.04326v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.04326v1)
- **Published**: 2022-09-09 14:34:52+00:00
- **Updated**: 2022-09-09 14:34:52+00:00
- **Authors**: Xin Li, Yao Qiang, Chengyin Li, Sijia Liu, Dongxiao Zhu
- **Comment**: 9 pages, 3 figures
- **Journal**: AdvML Frontiers workshop at 39th International Conference on
  Machine Learning (ICML), Baltimore, Maryland, USA, 2022
- **Summary**: This work tackles a central machine learning problem of performance degradation on out-of-distribution (OOD) test sets. The problem is particularly salient in medical imaging based diagnosis system that appears to be accurate but fails when tested in new hospitals/datasets. Recent studies indicate the system might learn shortcut and non-relevant features instead of generalizable features, so-called good features. We hypothesize that adversarial training can eliminate shortcut features whereas saliency guided training can filter out non-relevant features; both are nuisance features accounting for the performance degradation on OOD test sets. With that, we formulate a novel model training scheme for the deep neural network to learn good features for classification and/or detection tasks ensuring a consistent generalization performance on OOD test sets. The experimental results qualitatively and quantitatively demonstrate the superior performance of our method using the benchmark CXR image data sets on classification tasks.



### Bridging the Gap: Differentially Private Equivariant Deep Learning for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2209.04338v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.04338v2)
- **Published**: 2022-09-09 14:51:13+00:00
- **Updated**: 2023-06-20 16:38:13+00:00
- **Authors**: Florian A. HÃ¶lzl, Daniel Rueckert, Georgios Kaissis
- **Comment**: Accepted as extended abstract at GeoMedIA Workshop 2022
  (https://openreview.net/forum?id=rGYfMrMxI17)
- **Journal**: None
- **Summary**: Machine learning with formal privacy-preserving techniques like Differential Privacy (DP) allows one to derive valuable insights from sensitive medical imaging data while promising to protect patient privacy, but it usually comes at a sharp privacy-utility trade-off. In this work, we propose to use steerable equivariant convolutional networks for medical image analysis with DP. Their improved feature quality and parameter efficiency yield remarkable accuracy gains, narrowing the privacy-utility gap.



### EDeNN: Event Decay Neural Networks for low latency vision
- **Arxiv ID**: http://arxiv.org/abs/2209.04362v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2209.04362v2)
- **Published**: 2022-09-09 15:51:39+00:00
- **Updated**: 2023-05-09 14:22:17+00:00
- **Authors**: Celyn Walters, Simon Hadfield
- **Comment**: 14 pages, 5 figures
- **Journal**: None
- **Summary**: Despite the success of neural networks in computer vision tasks, digital 'neurons' are a very loose approximation of biological neurons. Today's learning approaches are designed to function on digital devices with digital data representations such as image frames. In contrast, biological vision systems are generally much more capable and efficient than state-of-the-art digital computer vision algorithms. Event cameras are an emerging sensor technology which imitates biological vision with asynchronously firing pixels, eschewing the concept of the image frame. To leverage modern learning techniques, many event-based algorithms are forced to accumulate events back to image frames, somewhat squandering the advantages of event cameras.   We follow the opposite paradigm and develop a new type of neural network which operates closer to the original event data stream. We demonstrate state-of-the-art performance in angular velocity regression and competitive optical flow estimation, while avoiding difficulties related to training SNN. Furthermore, the processing latency of our proposed approach is less than 1/10 any other implementation, while continuous inference increases this improvement by another order of magnitude.



### Pre-training image-language transformers for open-vocabulary tasks
- **Arxiv ID**: http://arxiv.org/abs/2209.04372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04372v1)
- **Published**: 2022-09-09 16:11:11+00:00
- **Updated**: 2022-09-09 16:11:11+00:00
- **Authors**: AJ Piergiovanni, Weicheng Kuo, Anelia Angelova
- **Comment**: None
- **Journal**: None
- **Summary**: We present a pre-training approach for vision and language transformer models, which is based on a mixture of diverse tasks. We explore both the use of image-text captioning data in pre-training, which does not need additional supervision, as well as object-aware strategies to pre-train the model. We evaluate the method on a number of textgenerative vision+language tasks, such as Visual Question Answering, visual entailment and captioning, and demonstrate large gains over standard pre-training methods.



### Energy-Aware JPEG Image Compression: A Multi-Objective Approach
- **Arxiv ID**: http://arxiv.org/abs/2209.04374v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2209.04374v1)
- **Published**: 2022-09-09 16:16:30+00:00
- **Updated**: 2022-09-09 16:16:30+00:00
- **Authors**: Seyed Jalaleddin Mousavirad, LuÃ­s A. Alexandre
- **Comment**: 42 pages, this paper is submitted to the related journal
- **Journal**: None
- **Summary**: Customer satisfaction is crucially affected by energy consumption in mobile devices. One of the most energy-consuming parts of an application is images. While different images with different quality consume different amounts of energy, there are no straightforward methods to calculate the energy consumption of an operation in a typical image. This paper, first, investigates that there is a correlation between energy consumption and image quality as well as image file size. Therefore, these two can be considered as a proxy for energy consumption. Then, we propose a multi-objective strategy to enhance image quality and reduce image file size based on the quantisation tables in JPEG image compression. To this end, we have used two general multi-objective metaheuristic approaches: scalarisation and Pareto-based. Scalarisation methods find a single optimal solution based on combining different objectives, while Pareto-based techniques aim to achieve a set of solutions. In this paper, we embed our strategy into five scalarisation algorithms, including energy-aware multi-objective genetic algorithm (EnMOGA), energy-aware multi-objective particle swarm optimisation (EnMOPSO), energy-aware multi-objective differential evolution (EnMODE), energy-aware multi-objective evolutionary strategy (EnMOES), and energy-aware multi-objective pattern search (EnMOPS). Also, two Pareto-based methods, including a non-dominated sorting genetic algorithm (NSGA-II) and a reference-point-based NSGA-II (NSGA-III) are used for the embedding scheme, and two Pareto-based algorithms, EnNSGAII and EnNSGAIII, are presented. Experimental studies show that the performance of the baseline algorithm is improved by embedding the proposed strategy into metaheuristic algorithms.



### GRASP-Net: Geometric Residual Analysis and Synthesis for Point Cloud Compression
- **Arxiv ID**: http://arxiv.org/abs/2209.04401v1
- **DOI**: 10.1145/3552457.3555727
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.04401v1)
- **Published**: 2022-09-09 17:09:02+00:00
- **Updated**: 2022-09-09 17:09:02+00:00
- **Authors**: Jiahao Pang, Muhammad Asad Lodhi, Dong Tian
- **Comment**: Accepted at ACM MM 2022 Workshop on Advances in Point Cloud
  Compression, Processing and Analysis
- **Journal**: None
- **Summary**: Point cloud compression (PCC) is a key enabler for various 3-D applications, owing to the universality of the point cloud format. Ideally, 3D point clouds endeavor to depict object/scene surfaces that are continuous. Practically, as a set of discrete samples, point clouds are locally disconnected and sparsely distributed. This sparse nature is hindering the discovery of local correlation among points for compression. Motivated by an analysis with fractal dimension, we propose a heterogeneous approach with deep learning for lossy point cloud geometry compression. On top of a base layer compressing a coarse representation of the input, an enhancement layer is designed to cope with the challenging geometric residual/details. Specifically, a point-based network is applied to convert the erratic local details to latent features residing on the coarse point cloud. Then a sparse convolutional neural network operating on the coarse point cloud is launched. It utilizes the continuity/smoothness of the coarse geometry to compress the latent features as an enhancement bit-stream that greatly benefits the reconstruction quality. When this bit-stream is unavailable, e.g., due to packet loss, we support a skip mode with the same architecture which generates geometric details from the coarse point cloud directly. Experimentation on both dense and sparse point clouds demonstrate the state-of-the-art compression performance achieved by our proposal. Our code is available at https://github.com/InterDigitalInc/GRASP-Net.



### Improved Masked Image Generation with Token-Critic
- **Arxiv ID**: http://arxiv.org/abs/2209.04439v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04439v1)
- **Published**: 2022-09-09 17:57:21+00:00
- **Updated**: 2022-09-09 17:57:21+00:00
- **Authors**: JosÃ© Lezama, Huiwen Chang, Lu Jiang, Irfan Essa
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Non-autoregressive generative transformers recently demonstrated impressive image generation performance, and orders of magnitude faster sampling than their autoregressive counterparts. However, optimal parallel sampling from the true joint distribution of visual tokens remains an open challenge. In this paper we introduce Token-Critic, an auxiliary model to guide the sampling of a non-autoregressive generative transformer. Given a masked-and-reconstructed real image, the Token-Critic model is trained to distinguish which visual tokens belong to the original image and which were sampled by the generative transformer. During non-autoregressive iterative sampling, Token-Critic is used to select which tokens to accept and which to reject and resample. Coupled with Token-Critic, a state-of-the-art generative transformer significantly improves its performance, and outperforms recent diffusion models and GANs in terms of the trade-off between generated image quality and diversity, in the challenging class-conditional ImageNet generation.



### MCIBI++: Soft Mining Contextual Information Beyond Image for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.04471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04471v1)
- **Published**: 2022-09-09 18:03:52+00:00
- **Updated**: 2022-09-09 18:03:52+00:00
- **Authors**: Zhenchao Jin, Dongdong Yu, Zehuan Yuan, Lequan Yu
- **Comment**: Accepted by TPAMI, codes are available at
  https://github.com/SegmentationBLWX/sssegmentation
- **Journal**: None
- **Summary**: Co-occurrent visual pattern makes context aggregation become an essential paradigm for semantic segmentation.The existing studies focus on modeling the contexts within image while neglecting the valuable semantics of the corresponding category beyond image. To this end, we propose a novel soft mining contextual information beyond image paradigm named MCIBI++ to further boost the pixel-level representations. Specifically, we first set up a dynamically updated memory module to store the dataset-level distribution information of various categories and then leverage the information to yield the dataset-level category representations during network forward. After that, we generate a class probability distribution for each pixel representation and conduct the dataset-level context aggregation with the class probability distribution as weights. Finally, the original pixel representations are augmented with the aggregated dataset-level and the conventional image-level contextual information. Moreover, in the inference phase, we additionally design a coarse-to-fine iterative inference strategy to further boost the segmentation results. MCIBI++ can be effortlessly incorporated into the existing segmentation frameworks and bring consistent performance improvements. Also, MCIBI++ can be extended into the video semantic segmentation framework with considerable improvements over the baseline. Equipped with MCIBI++, we achieved the state-of-the-art performance on seven challenging image or video semantic segmentation benchmarks.



### Discriminative Sampling of Proposals in Self-Supervised Transformers for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2209.09209v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09209v2)
- **Published**: 2022-09-09 18:33:23+00:00
- **Updated**: 2022-11-20 02:55:22+00:00
- **Authors**: Shakeeb Murtaza, Soufiane Belharbi, Marco Pedersoli, Aydin Sarraf, Eric Granger
- **Comment**: None
- **Journal**: None
- **Summary**: Drones are employed in a growing number of visual recognition applications. A recent development in cell tower inspection is drone-based asset surveillance, where the autonomous flight of a drone is guided by localizing objects of interest in successive aerial images. In this paper, we propose a method to train deep weakly-supervised object localization (WSOL) models based only on image-class labels to locate object with high confidence. To train our localizer, pseudo labels are efficiently harvested from a self-supervised vision transformers (SSTs). However, since SSTs decompose the scene into multiple maps containing various object parts, and do not rely on any explicit supervisory signal, they cannot distinguish between the object of interest and other objects, as required WSOL. To address this issue, we propose leveraging the multiple maps generated by the different transformer heads to acquire pseudo-labels for training a deep WSOL model. In particular, a new Discriminative Proposals Sampling (DiPS) method is introduced that relies on a CNN classifier to identify discriminative regions. Then, foreground and background pixels are sampled from these regions in order to train a WSOL model for generating activation maps that can accurately localize objects belonging to a specific class. Empirical results on the challenging TelDrone dataset indicate that our proposed approach can outperform state-of-art methods over a wide range of threshold values over produced maps. We also computed results on CUB dataset, showing that our method can be adapted for other tasks.



### Fine-grain Inference on Out-of-Distribution Data with Hierarchical Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.04493v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.04493v1)
- **Published**: 2022-09-09 18:52:36+00:00
- **Updated**: 2022-09-09 18:52:36+00:00
- **Authors**: Randolph Linderman, Jingyang Zhang, Nathan Inkawhich, Hai Li, Yiran Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning methods must be trusted to make appropriate decisions in real-world environments, even when faced with out-of-distribution (OOD) samples. Many current approaches simply aim to detect OOD examples and alert the user when an unrecognized input is given. However, when the OOD sample significantly overlaps with the training data, a binary anomaly detection is not interpretable or explainable, and provides little information to the user. We propose a new model for OOD detection that makes predictions at varying levels of granularity as the inputs become more ambiguous, the model predictions become coarser and more conservative. Consider an animal classifier that encounters an unknown bird species and a car. Both cases are OOD, but the user gains more information if the classifier recognizes that its uncertainty over the particular species is too large and predicts bird instead of detecting it as OOD. Furthermore, we diagnose the classifiers performance at each level of the hierarchy improving the explainability and interpretability of the models predictions. We demonstrate the effectiveness of hierarchical classifiers for both fine- and coarse-grained OOD tasks.



### General Place Recognition Survey: Towards the Real-world Autonomy Age
- **Arxiv ID**: http://arxiv.org/abs/2209.04497v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.04497v1)
- **Published**: 2022-09-09 19:37:05+00:00
- **Updated**: 2022-09-09 19:37:05+00:00
- **Authors**: Peng Yin, Shiqi Zhao, Ivan Cisneros, Abulikemu Abuduweili, Guoquan Huang, Micheal Milford, Changliu Liu, Howie Choset, Sebastian Scherer
- **Comment**: 20 pages, 10 figures. Submitted to IEEE T-RO survey paper
- **Journal**: None
- **Summary**: Place recognition is the fundamental module that can assist Simultaneous Localization and Mapping (SLAM) in loop-closure detection and re-localization for long-term navigation. The place recognition community has made astonishing progress over the last $20$ years, and this has attracted widespread research interest and application in multiple fields such as computer vision and robotics. However, few methods have shown promising place recognition performance in complex real-world scenarios, where long-term and large-scale appearance changes usually result in failures. Additionally, there is a lack of an integrated framework amongst the state-of-the-art methods that can handle all of the challenges in place recognition, which include appearance changes, viewpoint differences, robustness to unknown areas, and efficiency in real-world applications. In this work, we survey the state-of-the-art methods that target long-term localization and discuss future directions and opportunities.   We start by investigating the formulation of place recognition in long-term autonomy and the major challenges in real-world environments. We then review the recent works in place recognition for different sensor modalities and current strategies for dealing with various place recognition challenges. Finally, we review the existing datasets for long-term localization and introduce our datasets and evaluation API for different approaches. This paper can be a tutorial for researchers new to the place recognition community and those who care about long-term robotics autonomy. We also provide our opinion on the frequently asked question in robotics: Do robots need accurate localization for long-term autonomy? A summary of this work and our datasets and evaluation API is publicly available to the robotics community at: https://github.com/MetaSLAM/GPRS.



### Constrained Sampling for Class-Agnostic Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/2209.09195v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09195v1)
- **Published**: 2022-09-09 19:58:38+00:00
- **Updated**: 2022-09-09 19:58:38+00:00
- **Authors**: Shakeeb Murtaza, Soufiane Belharbi, Marco Pedersoli, Aydin Sarraf, Eric Granger
- **Comment**: 3 pages, 2 figures
- **Journal**: None
- **Summary**: Self-supervised vision transformers can generate accurate localization maps of the objects in an image. However, since they decompose the scene into multiple maps containing various objects, and they do not rely on any explicit supervisory signal, they cannot distinguish between the object of interest from other objects, as required in weakly-supervised object localization (WSOL). To address this issue, we propose leveraging the multiple maps generated by the different transformer heads to acquire pseudo-labels for training a WSOL model. In particular, a new discriminative proposals sampling method is introduced that relies on a pretrained CNN classifier to identify discriminative regions. Then, foreground and background pixels are sampled from these regions in order to train a WSOL model for generating activation maps that can accurately localize objects belonging to a specific class. Empirical results on the challenging CUB benchmark dataset indicate that our proposed approach can outperform state-of-art methods over a wide range of threshold values. Our method provides class activation maps with a better coverage of foreground object regions w.r.t. the background.



### DeepSTI: Towards Tensor Reconstruction using Fewer Orientations in Susceptibility Tensor Imaging
- **Arxiv ID**: http://arxiv.org/abs/2209.04504v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.04504v1)
- **Published**: 2022-09-09 20:03:53+00:00
- **Updated**: 2022-09-09 20:03:53+00:00
- **Authors**: Zhenghan Fang, Kuo-Wei Lai, Peter van Zijl, Xu Li, Jeremias Sulam
- **Comment**: None
- **Journal**: None
- **Summary**: Susceptibility tensor imaging (STI) is an emerging magnetic resonance imaging technique that characterizes the anisotropic tissue magnetic susceptibility with a second-order tensor model. STI has the potential to provide information for both the reconstruction of white matter fiber pathways and detection of myelin changes in the brain at mm resolution or less, which would be of great value for understanding brain structure and function in healthy and diseased brain. However, the application of STI in vivo has been hindered by its cumbersome and time-consuming acquisition requirement of measuring susceptibility induced MR phase changes at multiple (usually more than six) head orientations. This complexity is enhanced by the limitation in head rotation angles due to physical constraints of the head coil. As a result, STI has not yet been widely applied in human studies in vivo. In this work, we tackle these issues by proposing an image reconstruction algorithm for STI that leverages data-driven priors. Our method, called DeepSTI, learns the data prior implicitly via a deep neural network that approximates the proximal operator of a regularizer function for STI. The dipole inversion problem is then solved iteratively using the learned proximal network. Experimental results using both simulation and in vivo human data demonstrate great improvement over state-of-the-art algorithms in terms of the reconstructed tensor image, principal eigenvector maps and tractography results, while allowing for tensor reconstruction with MR phase measured at much less than six different orientations. Notably, promising reconstruction results are achieved by our method from only one orientation in human in vivo, and we demonstrate a potential application of this technique for estimating lesion susceptibility anisotropy in patients with multiple sclerosis.



### Calibrating Segmentation Networks with Margin-based Label Smoothing
- **Arxiv ID**: http://arxiv.org/abs/2209.09641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09641v1)
- **Published**: 2022-09-09 20:21:03+00:00
- **Updated**: 2022-09-09 20:21:03+00:00
- **Authors**: Balamurali Murugesan, Bingyuan Liu, Adrian Galdran, Ismail Ben Ayed, Jose Dolz
- **Comment**: Under review. The code is available at
  https://github.com/Bala93/MarginLoss. arXiv admin note: substantial text
  overlap with arXiv:2111.15430
- **Journal**: None
- **Summary**: Despite the undeniable progress in visual recognition tasks fueled by deep neural networks, there exists recent evidence showing that these models are poorly calibrated, resulting in over-confident predictions. The standard practices of minimizing the cross entropy loss during training promote the predicted softmax probabilities to match the one-hot label assignments. Nevertheless, this yields a pre-softmax activation of the correct class that is significantly larger than the remaining activations, which exacerbates the miscalibration problem. Recent observations from the classification literature suggest that loss functions that embed implicit or explicit maximization of the entropy of predictions yield state-of-the-art calibration performances. Despite these findings, the impact of these losses in the relevant task of calibrating medical image segmentation networks remains unexplored. In this work, we provide a unifying constrained-optimization perspective of current state-of-the-art calibration losses. Specifically, these losses could be viewed as approximations of a linear penalty (or a Lagrangian term) imposing equality constraints on logit distances. This points to an important limitation of such underlying equality constraints, whose ensuing gradients constantly push towards a non-informative solution, which might prevent from reaching the best compromise between the discriminative performance and calibration of the model during gradient-based optimization. Following our observations, we propose a simple and flexible generalization based on inequality constraints, which imposes a controllable margin on logit distances. Comprehensive experiments on a variety of public medical image segmentation benchmarks demonstrate that our method sets novel state-of-the-art results on these tasks in terms of network calibration, whereas the discriminative performance is also improved.



### Affinity-VAE for disentanglement, clustering and classification of objects in multidimensional image data
- **Arxiv ID**: http://arxiv.org/abs/2209.04517v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2209.04517v1)
- **Published**: 2022-09-09 20:39:22+00:00
- **Updated**: 2022-09-09 20:39:22+00:00
- **Authors**: Jola Mirecka, Marjan Famili, Anna KotaÅska, Nikolai Juraschko, Beatriz Costa-Gomes, Colin M. Palmer, Jeyan Thiyagalingam, Tom Burnley, Mark Basham, Alan R. Lowe
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we present affinity-VAE: a framework for automatic clustering and classification of objects in multidimensional image data based on their similarity. The method expands on the concept of $\beta$-VAEs with an informed similarity-based loss component driven by an affinity matrix. The affinity-VAE is able to create rotationally-invariant, morphologically homogeneous clusters in the latent representation, with improved cluster separation compared with a standard $\beta$-VAE. We explore the extent of latent disentanglement and continuity of the latent spaces on both 2D and 3D image data, including simulated biological electron cryo-tomography (cryo-ET) volumes as an example of a scientific application.



### PoliTO-IIT-CINI Submission to the EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.04525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04525v1)
- **Published**: 2022-09-09 21:03:11+00:00
- **Updated**: 2022-09-09 21:03:11+00:00
- **Authors**: Mirco Planamente, Gabriele Goletto, Gabriele Trivigno, Giuseppe Averta, Barbara Caputo
- **Comment**: 3rd place in the 2022 EPIC-KITCHENS-100 Unsupervised Domain
  Adaptation Challenge for Action Recognition. arXiv admin note: substantial
  text overlap with arXiv:2107.00337
- **Journal**: None
- **Summary**: In this report, we describe the technical details of our submission to the EPIC-Kitchens-100 Unsupervised Domain Adaptation (UDA) Challenge in Action Recognition. To tackle the domain-shift which exists under the UDA setting, we first exploited a recent Domain Generalization (DG) technique, called Relative Norm Alignment (RNA). Secondly, we extended this approach to work on unlabelled target data, enabling a simpler adaptation of the model to the target distribution in an unsupervised fashion. To this purpose, we included in our framework UDA algorithms, such as multi-level adversarial alignment and attentive entropy. By analyzing the challenge setting, we notice the presence of a secondary concurrence shift in the data, which is usually called environmental bias. It is caused by the existence of different environments, i.e., kitchens. To deal with these two shifts (environmental and temporal), we extended our system to perform Multi-Source Multi-Target Domain Adaptation. Finally, we employed distinct models in our final proposal to leverage the potential of popular video architectures, and we introduced two more losses for the ensemble adaptation. Our submission (entry 'plnet') is visible on the leaderboard and ranked in 2nd position for 'verb', and in 3rd position for both 'noun' and 'action'.



### Sparsity-guided Network Design for Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/2209.04551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04551v1)
- **Published**: 2022-09-09 23:13:25+00:00
- **Updated**: 2022-09-09 23:13:25+00:00
- **Authors**: Tianyu Ding, Luming Liang, Zhihui Zhu, Tianyi Chen, Ilya Zharkov
- **Comment**: Submitted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence. The corresponding CVPR paper can be found at arXiv:2103.10559
- **Journal**: None
- **Summary**: DNN-based frame interpolation, which generates intermediate frames from two consecutive frames, is often dependent on model architectures with a large number of features, preventing their deployment on systems with limited resources, such as mobile devices. We present a compression-driven network design for frame interpolation that leverages model pruning through sparsity-inducing optimization to greatly reduce the model size while attaining higher performance. Concretely, we begin by compressing the recently proposed AdaCoF model and demonstrating that a 10 times compressed AdaCoF performs similarly to its original counterpart, where different strategies for using layerwise sparsity information as a guide are comprehensively investigated under a variety of hyperparameter settings. We then enhance this compressed model by introducing a multi-resolution warping module, which improves visual consistency with multi-level details. As a result, we achieve a considerable performance gain with a quarter of the size of the original AdaCoF. In addition, our model performs favorably against other state-of-the-art approaches on a wide variety of datasets. We note that the suggested compression-driven framework is generic and can be easily transferred to other DNN-based frame interpolation algorithms. The source code is available at https://github.com/tding1/CDFI.



### Automatically Score Tissue Images Like a Pathologist by Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.05954v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.05954v2)
- **Published**: 2022-09-09 23:18:31+00:00
- **Updated**: 2023-03-27 21:18:14+00:00
- **Authors**: Iris Yan
- **Comment**: 19 pages, 6 figures
- **Journal**: None
- **Summary**: Cancer is the second leading cause of death in the world. Diagnosing cancer early on can save many lives. Pathologists have to look at tissue microarray (TMA) images manually to identify tumors, which can be time-consuming, inconsistent and subjective. Existing algorithms that automatically detect tumors have either not achieved the accuracy level of a pathologist or require substantial human involvements. A major challenge is that TMA images with different shapes, sizes, and locations can have the same score. Learning staining patterns in TMA images requires a huge number of images, which are severely limited due to privacy concerns and regulations in medical organizations. TMA images from different cancer types may have common characteristics that could provide valuable information, but using them directly harms the accuracy. By selective transfer learning from multiple small auxiliary sets, the proposed algorithm is able to extract knowledge from tissue images showing a ``similar" scoring pattern but with different cancer types. Remarkably, transfer learning has made it possible for the algorithm to break the critical accuracy barrier -- the proposed algorithm reports an accuracy of 75.9% on breast cancer TMA images from the Stanford Tissue Microarray Database, achieving the 75\% accuracy level of pathologists. This will allow pathologists to confidently use automatic algorithms to assist them in recognizing tumors consistently with a higher accuracy in real time.



