# Arxiv Papers in cs.CV on 2022-09-24
### DomainATM: Domain Adaptation Toolbox for Medical Data Analysis
- **Arxiv ID**: http://arxiv.org/abs/2209.11890v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11890v1)
- **Published**: 2022-09-24 00:05:25+00:00
- **Updated**: 2022-09-24 00:05:25+00:00
- **Authors**: Hao Guan, Mingxia Liu
- **Comment**: 10 pages, 13 figures
- **Journal**: None
- **Summary**: Domain adaptation (DA) is an important technique for modern machine learning-based medical data analysis, which aims at reducing distribution differences between different medical datasets. A proper domain adaptation method can significantly enhance the statistical power by pooling data acquired from multiple sites/centers. To this end, we have developed the Domain Adaptation Toolbox for Medical data analysis (DomainATM) - an open-source software package designed for fast facilitation and easy customization of domain adaptation methods for medical data analysis. The DomainATM is implemented in MATLAB with a user-friendly graphical interface, and it consists of a collection of popular data adaptation algorithms that have been extensively applied to medical image analysis and computer vision. With DomainATM, researchers are able to facilitate fast feature-level and image-level adaptation, visualization and performance evaluation of different adaptation methods for medical data analysis. More importantly, the DomainATM enables the users to develop and test their own adaptation methods through scripting, greatly enhancing its utility and extensibility. An overview characteristic and usage of DomainATM is presented and illustrated with three example experiments, demonstrating its effectiveness, simplicity, and flexibility. The software, source code, and manual are available online.



### Closing the Loop: Graph Networks to Unify Semantic Objects and Visual Features for Multi-object Scenes
- **Arxiv ID**: http://arxiv.org/abs/2209.11894v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.11894v1)
- **Published**: 2022-09-24 00:42:33+00:00
- **Updated**: 2022-09-24 00:42:33+00:00
- **Authors**: Jonathan J. Y. Kim, Martin Urschler, Patricia J. Riddle, Jörg S. Wicker
- **Comment**: 8 pages. Accepted at 2022 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS)
- **Journal**: None
- **Summary**: In Simultaneous Localization and Mapping (SLAM), Loop Closure Detection (LCD) is essential to minimize drift when recognizing previously visited places. Visual Bag-of-Words (vBoW) has been an LCD algorithm of choice for many state-of-the-art SLAM systems. It uses a set of visual features to provide robust place recognition but fails to perceive the semantics or spatial relationship between feature points. Previous work has mainly focused on addressing these issues by combining vBoW with semantic and spatial information from objects in the scene. However, they are unable to exploit spatial information of local visual features and lack a structure that unifies semantic objects and visual features, therefore limiting the symbiosis between the two components. This paper proposes SymbioLCD2, which creates a unified graph structure to integrate semantic objects and visual features symbiotically. Our novel graph-based LCD system utilizes the unified graph structure by applying a Weisfeiler-Lehman graph kernel with temporal constraints to robustly predict loop closure candidates. Evaluation of the proposed system shows that having a unified graph structure incorporating semantic objects and visual features improves LCD prediction accuracy, illustrating that the proposed graph structure provides a strong symbiosis between these two complementary components. It also outperforms other Machine Learning algorithms - such as SVM, Decision Tree, Random Forest, Neural Network and GNN based Graph Matching Networks. Furthermore, it has shown good performance in detecting loop closure candidates earlier than state-of-the-art SLAM systems, demonstrating that extended semantic and spatial awareness from the unified graph structure significantly impacts LCD performance.



### Unsupervised active speaker detection in media content using cross-modal information
- **Arxiv ID**: http://arxiv.org/abs/2209.11896v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2209.11896v1)
- **Published**: 2022-09-24 00:51:38+00:00
- **Updated**: 2022-09-24 00:51:38+00:00
- **Authors**: Rahul Sharma, Shrikanth Narayanan
- **Comment**: Under review at IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: We present a cross-modal unsupervised framework for active speaker detection in media content such as TV shows and movies. Machine learning advances have enabled impressive performance in identifying individuals from speech and facial images. We leverage speaker identity information from speech and faces, and formulate active speaker detection as a speech-face assignment task such that the active speaker's face and the underlying speech identify the same person (character). We express the speech segments in terms of their associated speaker identity distances, from all other speech segments, to capture a relative identity structure for the video. Then we assign an active speaker's face to each speech segment from the concurrently appearing faces such that the obtained set of active speaker faces displays a similar relative identity structure. Furthermore, we propose a simple and effective approach to address speech segments where speakers are present off-screen. We evaluate the proposed system on three benchmark datasets -- Visual Person Clustering dataset, AVA-active speaker dataset, and Columbia dataset -- consisting of videos from entertainment and broadcast media, and show competitive performance to state-of-the-art fully supervised methods.



### How does Imaging Impact Patient Flow in Emergency Departments?
- **Arxiv ID**: http://arxiv.org/abs/2209.12895v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.12895v1)
- **Published**: 2022-09-24 02:03:19+00:00
- **Updated**: 2022-09-24 02:03:19+00:00
- **Authors**: Vishnunarayan Girishan Prabhu, Kevin Taaffe, Marisa Shehan, Ronald Pirrallo, William Jackson, Michael Ramsay, Jessica Hobbs
- **Comment**: None
- **Journal**: None
- **Summary**: Emergency Department (ED) overcrowding continues to be a public health issue as well as a patient safety issue. The underlying factors leading to ED crowding are numerous, varied, and complex. Although lack of in-hospital beds is frequently attributed as the primary reason for crowding, ED's dependencies on other ancillary resources, including imaging, consults, and labs, also contribute to crowding. Using retrospective data associated with imaging, including delays, processing time, and the number of image orders, from a large tier 1 trauma center, we developed a discrete event simulation model to identify the impact of the imaging delays and bundling image orders on patient time in the ED. Results from sensitivity analysis show that reducing the delays associated with imaging and bundling as few as 10% of imaging orders for certain patients can significantly (p-value < 0.05) reduce the time a patient spends in the ED.



### A Simple Strategy to Provable Invariance via Orbit Mapping
- **Arxiv ID**: http://arxiv.org/abs/2209.11916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11916v1)
- **Published**: 2022-09-24 03:40:42+00:00
- **Updated**: 2022-09-24 03:40:42+00:00
- **Authors**: Kanchana Vaishnavi Gandikota, Jonas Geiping, Zorah Lähner, Adam Czapliński, Michael Moeller
- **Comment**: ACCV 2022, older version is titled "Training or Architecture? How to
  Incorporate Invariance in Neural Networks",(arXiv:2106.10044)
- **Journal**: None
- **Summary**: Many applications require robustness, or ideally invariance, of neural networks to certain transformations of input data. Most commonly, this requirement is addressed by training data augmentation, using adversarial training, or defining network architectures that include the desired invariance by design. In this work, we propose a method to make network architectures provably invariant with respect to group actions by choosing one element from a (possibly continuous) orbit based on a fixed criterion. In a nutshell, we intend to 'undo' any possible transformation before feeding the data into the actual network. Further, we empirically analyze the properties of different approaches which incorporate invariance via training or architecture, and demonstrate the advantages of our method in terms of robustness and computational efficiency. In particular, we investigate the robustness with respect to rotations of images (which can hold up to discretization artifacts) as well as the provable orientation and scaling invariance of 3D point cloud classification.



### Self-supervised Image Clustering from Multiple Incomplete Views via Constrastive Complementary Generation
- **Arxiv ID**: http://arxiv.org/abs/2209.11927v1
- **DOI**: 10.1049/cvi2.12147
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11927v1)
- **Published**: 2022-09-24 05:08:34+00:00
- **Updated**: 2022-09-24 05:08:34+00:00
- **Authors**: Jiatai Wang, Zhiwei Xu, Xuewen Yang, Dongjin Guo, Limin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Incomplete Multi-View Clustering aims to enhance clustering performance by using data from multiple modalities. Despite the fact that several approaches for studying this issue have been proposed, the following drawbacks still persist: 1) It's difficult to learn latent representations that account for complementarity yet consistency without using label information; 2) and thus fails to take full advantage of the hidden information in incomplete data results in suboptimal clustering performance when complete data is scarce. In this paper, we propose Contrastive Incomplete Multi-View Image Clustering with Generative Adversarial Networks (CIMIC-GAN), which uses GAN to fill in incomplete data and uses double contrastive learning to learn consistency on complete and incomplete data. More specifically, considering diversity and complementary information among multiple modalities, we incorporate autoencoding representation of complete and incomplete data into double contrastive learning to achieve learning consistency. Integrating GANs into the autoencoding process can not only take full advantage of new features of incomplete data, but also better generalize the model in the presence of high data missing rates. Experiments conducted on \textcolor{black}{four} extensively-used datasets show that CIMIC-GAN outperforms state-of-the-art incomplete multi-View clustering methods.



### Towards Bridging the Space Domain Gap for Satellite Pose Estimation using Event Sensing
- **Arxiv ID**: http://arxiv.org/abs/2209.11945v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.11945v1)
- **Published**: 2022-09-24 07:22:09+00:00
- **Updated**: 2022-09-24 07:22:09+00:00
- **Authors**: Mohsi Jawaid, Ethan Elms, Yasir Latif, Tat-Jun Chin
- **Comment**: 8 pages. This work has been submitted to the IEEE (ICRA 2023) for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
- **Journal**: None
- **Summary**: Deep models trained using synthetic data require domain adaptation to bridge the gap between the simulation and target environments. State-of-the-art domain adaptation methods often demand sufficient amounts of (unlabelled) data from the target domain. However, this need is difficult to fulfil when the target domain is an extreme environment, such as space. In this paper, our target problem is close proximity satellite pose estimation, where it is costly to obtain images of satellites from actual rendezvous missions. We demonstrate that event sensing offers a promising solution to generalise from the simulation to the target domain under stark illumination differences. Our main contribution is an event-based satellite pose estimation technique, trained purely on synthetic event data with basic data augmentation to improve robustness against practical (noisy) event sensors. Underpinning our method is a novel dataset with carefully calibrated ground truth, comprising of real event data obtained by emulating satellite rendezvous scenarios in the lab under drastic lighting conditions. Results on the dataset showed that our event-based satellite pose estimation method, trained only on synthetic data without adaptation, could generalise to the target domain effectively.



### Raising the Bar on the Evaluation of Out-of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.11960v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.11960v1)
- **Published**: 2022-09-24 08:48:36+00:00
- **Updated**: 2022-09-24 08:48:36+00:00
- **Authors**: Jishnu Mukhoti, Tsung-Yu Lin, Bor-Chun Chen, Ashish Shah, Philip H. S. Torr, Puneet K. Dokania, Ser-Nam Lim
- **Comment**: None
- **Journal**: None
- **Summary**: In image classification, a lot of development has happened in detecting out-of-distribution (OoD) data. However, most OoD detection methods are evaluated on a standard set of datasets, arbitrarily different from training data. There is no clear definition of what forms a ``good" OoD dataset. Furthermore, the state-of-the-art OoD detection methods already achieve near perfect results on these standard benchmarks. In this paper, we define 2 categories of OoD data using the subtly different concepts of perceptual/visual and semantic similarity to in-distribution (iD) data. We define Near OoD samples as perceptually similar but semantically different from iD samples, and Shifted samples as points which are visually different but semantically akin to iD data. We then propose a GAN based framework for generating OoD samples from each of these 2 categories, given an iD dataset. Through extensive experiments on MNIST, CIFAR-10/100 and ImageNet, we show that a) state-of-the-art OoD detection methods which perform exceedingly well on conventional benchmarks are significantly less robust to our proposed benchmark. Moreover, b) models performing well on our setup also perform well on conventional real-world OoD detection benchmarks and vice versa, thereby indicating that one might not even need a separate OoD set, to reliably evaluate performance in OoD detection.



### Approximate better, Attack stronger: Adversarial Example Generation via Asymptotically Gaussian Mixture Distribution
- **Arxiv ID**: http://arxiv.org/abs/2209.11964v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11964v1)
- **Published**: 2022-09-24 08:57:10+00:00
- **Updated**: 2022-09-24 08:57:10+00:00
- **Authors**: Zhengwei Fang, Rui Wang, Tao Huang, Liping Jing
- **Comment**: None
- **Journal**: None
- **Summary**: Strong adversarial examples are the keys to evaluating and enhancing the robustness of deep neural networks. The popular adversarial attack algorithms maximize the non-concave loss function using the gradient ascent. However, the performance of each attack is usually sensitive to, for instance, minor image transformations due to insufficient information (only one input example, few white-box source models and unknown defense strategies). Hence, the crafted adversarial examples are prone to overfit the source model, which limits their transferability to unidentified architectures. In this paper, we propose Multiple Asymptotically Normal Distribution Attacks (MultiANDA), a novel method that explicitly characterizes adversarial perturbations from a learned distribution. Specifically, we approximate the posterior distribution over the perturbations by taking advantage of the asymptotic normality property of stochastic gradient ascent (SGA), then apply the ensemble strategy on this procedure to estimate a Gaussian mixture model for a better exploration of the potential optimization space. Drawing perturbations from the learned distribution allow us to generate any number of adversarial examples for each input. The approximated posterior essentially describes the stationary distribution of SGA iterations, which captures the geometric information around the local optimum. Thus, the samples drawn from the distribution reliably maintain the transferability. Our proposed method outperforms nine state-of-the-art black-box attacks on deep learning models with or without defenses through extensive experiments on seven normally trained and seven defence models.



### Ground then Navigate: Language-guided Navigation in Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2209.11972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11972v1)
- **Published**: 2022-09-24 09:51:09+00:00
- **Updated**: 2022-09-24 09:51:09+00:00
- **Authors**: Kanishk Jain, Varun Chhangani, Amogh Tiwari, K. Madhava Krishna, Vineet Gandhi
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate the Vision-and-Language Navigation (VLN) problem in the context of autonomous driving in outdoor settings. We solve the problem by explicitly grounding the navigable regions corresponding to the textual command. At each timestamp, the model predicts a segmentation mask corresponding to the intermediate or the final navigable region. Our work contrasts with existing efforts in VLN, which pose this task as a node selection problem, given a discrete connected graph corresponding to the environment. We do not assume the availability of such a discretised map. Our work moves towards continuity in action space, provides interpretability through visual feedback and allows VLN on commands requiring finer manoeuvres like "park between the two cars". Furthermore, we propose a novel meta-dataset CARLA-NAV to allow efficient training and validation. The dataset comprises pre-recorded training sequences and a live environment for validation and testing. We provide extensive qualitative and quantitive empirical results to validate the efficacy of the proposed approach.



### Robust Hyperspectral Image Fusion with Simultaneous Guide Image Denoising via Constrained Convex Optimization
- **Arxiv ID**: http://arxiv.org/abs/2209.11979v2
- **DOI**: 10.1109/TGRS.2022.3224480
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11979v2)
- **Published**: 2022-09-24 10:58:27+00:00
- **Updated**: 2022-11-25 08:59:34+00:00
- **Authors**: Saori Takeyama, Shunsuke Ono
- **Comment**: Accepted to IEEE Transactions on Geoscience and Remote Sensing
- **Journal**: None
- **Summary**: The paper proposes a new high spatial resolution hyperspectral (HR-HS) image estimation method based on convex optimization. The method assumes a low spatial resolution HS (LR-HS) image and a guide image as observations, where both observations are contaminated by noise. Our method simultaneously estimates an HR-HS image and a noiseless guide image, so the method can utilize spatial information in a guide image even if it is contaminated by heavy noise. The proposed estimation problem adopts hybrid spatio-spectral total variation as regularization and evaluates the edge similarity between HR-HS and guide images to effectively use apriori knowledge on an HR-HS image and spatial detail information in a guide image. To efficiently solve the problem, we apply a primal-dual splitting method. Experiments demonstrate the performance of our method and the advantage over several existing methods.



### Statistical Analysis of Time-Frequency Features Based On Multivariate Synchrosqueezing Transform for Hand Gesture Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.13350v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2209.13350v1)
- **Published**: 2022-09-24 11:51:10+00:00
- **Updated**: 2022-09-24 11:51:10+00:00
- **Authors**: Lutfiye Saripinar, Deniz Hande Kisa, Mehmet Akif Ozdemir, Onan Guren
- **Comment**: Conference Paper, 5 pages, Translated Title (TR): El Hareketi
  Siniflandirmasi icin Cok-Degiskenli Senkrosikistirma Donusumune Dayali
  Zaman-Frekans Ozniteliklerinin Istatistiksel Analizi Proceedings:
  https://www.tipcih.com/books/ICMD2022_Proceedings-DRAFT.pdf Website:
  https://www.tipcih.com/
- **Journal**: 5th International Conference on Medical Devices, ICMD'2022, June
  2022, pp. 1-5
- **Summary**: In this study, the four joint time-frequency (TF) moments; mean, variance, skewness, and kurtosis of TF matrix obtained from Multivariate Synchrosqueezing Transform (MSST) are proposed as features for hand gesture recognition. A publicly available dataset containing surface EMG (sEMG) signals of 40 subjects performing 10 hand gestures, was used. The distinguishing power of the feature variables for the tested gestures was evaluated according to their p values obtained from the Kruskal-Wallis (KW) test. It is concluded that the mean, variance, skewness, and kurtosis of TF matrices can be candidate feature sets for the recognition of hand gestures.



### Deep Neural Networks for Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/2209.11990v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11990v1)
- **Published**: 2022-09-24 12:11:00+00:00
- **Updated**: 2022-09-24 12:11:00+00:00
- **Authors**: Thao Minh Le
- **Comment**: PhD thesis
- **Journal**: None
- **Summary**: Visual perception and language understanding are - fundamental components of human intelligence, enabling them to understand and reason about objects and their interactions. It is crucial for machines to have this capacity to reason using these two modalities to invent new robot-human collaborative systems. Recent advances in deep learning have built separate sophisticated representations of both visual scenes and languages. However, understanding the associations between the two modalities in a shared context for multimodal reasoning remains a challenge. Focusing on language and vision modalities, this thesis advances the understanding of how to exploit and use pivotal aspects of vision-and-language tasks with neural networks to support reasoning. We derive these understandings from a series of works, making a two-fold contribution: (i) effective mechanisms for content selection and construction of temporal relations from dynamic visual scenes in response to a linguistic query and preparing adequate knowledge for the reasoning process (ii) new frameworks to perform reasoning with neural networks by exploiting visual-linguistic associations, deduced either directly from data or guided by external priors.



### Contrastive learning for unsupervised medical image clustering and reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2209.12005v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.12005v1)
- **Published**: 2022-09-24 13:17:02+00:00
- **Updated**: 2022-09-24 13:17:02+00:00
- **Authors**: Matteo Ferrante, Tommaso Boccato, Simeon Spasov, Andrea Duggento, Nicola Toschi
- **Comment**: None
- **Journal**: None
- **Summary**: The lack of large labeled medical imaging datasets, along with significant inter-individual variability compared to clinically established disease classes, poses significant challenges in exploiting medical imaging information in a precision medicine paradigm, where in principle dense patient-specific data can be employed to formulate individual predictions and/or stratify patients into finer-grained groups which may follow more homogeneous trajectories and therefore empower clinical trials. In order to efficiently explore the effective degrees of freedom underlying variability in medical images in an unsupervised manner, in this work we propose an unsupervised autoencoder framework which is augmented with a contrastive loss to encourage high separability in the latent space. The model is validated on (medical) benchmark datasets. As cluster labels are assigned to each example according to cluster assignments, we compare performance with a supervised transfer learning baseline. Our method achieves similar performance to the supervised architecture, indicating that separation in the latent space reproduces expert medical observer-assigned labels. The proposed method could be beneficial for patient stratification, exploring new subdivisions of larger classes or pathological continua or, due to its sampling abilities in a variation setting, data augmentation in medical image processing.



### Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2209.12009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.12009v1)
- **Published**: 2022-09-24 13:40:09+00:00
- **Updated**: 2022-09-24 13:40:09+00:00
- **Authors**: Jiayi Chen, Mi Yan, Jiazhao Zhang, Yinzhen Xu, Xiaolong Li, Yijia Weng, Li Yi, Shuran Song, He Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we tackle the challenging task of jointly tracking hand object pose and reconstructing their shapes from depth point cloud sequences in the wild, given the initial poses at frame 0. We for the first time propose a point cloud based hand joint tracking network, HandTrackNet, to estimate the inter-frame hand joint motion. Our HandTrackNet proposes a novel hand pose canonicalization module to ease the tracking task, yielding accurate and robust hand joint tracking. Our pipeline then reconstructs the full hand via converting the predicted hand joints into a template-based parametric hand model MANO. For object tracking, we devise a simple yet effective module that estimates the object SDF from the first frame and performs optimization-based tracking. Finally, a joint optimization step is adopted to perform joint hand and object reasoning, which alleviates the occlusion-induced ambiguity and further refines the hand pose. During training, the whole pipeline only sees purely synthetic data, which are synthesized with sufficient variations and by depth simulation for the ease of generalization. The whole pipeline is pertinent to the generalization gaps and thus directly transferable to real in-the-wild data. We evaluate our method on two real hand object interaction datasets, e.g. HO3D and DexYCB, without any finetuning. Our experiments demonstrate that the proposed method significantly outperforms the previous state-of-the-art depth-based hand and object pose estimation and tracking methods, running at a frame rate of 9 FPS.



### Spiking SiamFC++: Deep Spiking Neural Network for Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2209.12010v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.12010v1)
- **Published**: 2022-09-24 13:54:28+00:00
- **Updated**: 2022-09-24 13:54:28+00:00
- **Authors**: Shuiying Xiang, Tao Zhang, Shuqing Jiang, Yanan Han, Yahui Zhang, Chenyang Du, Xingxing Guo, Licun Yu, Yuechun Shi, Yue Hao
- **Comment**: None
- **Journal**: None
- **Summary**: Spiking neural network (SNN) is a biologically-plausible model and exhibits advantages of high computational capability and low power consumption. While the training of deep SNN is still an open problem, which limits the real-world applications of deep SNN. Here we propose a deep SNN architecture named Spiking SiamFC++ for object tracking with end-to-end direct training. Specifically, the AlexNet network is extended in the time domain to extract the feature, and the surrogate gradient function is adopted to realize direct supervised training of the deep SNN. To examine the performance of the Spiking SiamFC++, several tracking benchmarks including OTB2013, OTB2015, VOT2015, VOT2016, and UAV123 are considered. It is found that, the precision loss is small compared with the original SiamFC++. Compared with the existing SNN-based target tracker, e.g., the SiamSNN, the precision (succession) of the proposed Spiking SiamFC++ reaches 85.24% (64.37%), which is much higher than that of 52.78% (44.32%) achieved by the SiamSNN. To our best knowledge, the performance of the Spiking SiamFC++ outperforms the existing state-of-the-art approaches in SNN-based object tracking, which provides a novel path for SNN application in the field of target tracking. This work may further promote the development of SNN algorithms and neuromorphic chips.



### Application of the nnU-Net for automatic segmentation of lung lesion on CT images, and implication on radiomic models
- **Arxiv ID**: http://arxiv.org/abs/2209.12027v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.12027v1)
- **Published**: 2022-09-24 15:04:23+00:00
- **Updated**: 2022-09-24 15:04:23+00:00
- **Authors**: Matteo Ferrante, Lisa Rinaldi, Francesca Botta, Xiaobin Hu, Andreas Dolp, Marta Minotti, Francesca De Piano, Gianluigi Funicelli, Stefania Volpe, Federica Bellerba, Paolo De Marco, Sara Raimondi, Stefania Rizzo, Kuangyu Shi, Marta Cremonesi, Barbara A. Jereczek-Fossa, Lorenzo Spaggiari, Filippo De Marinis, Roberto Orecchia, Daniela Origgi
- **Comment**: None
- **Journal**: None
- **Summary**: Lesion segmentation is a crucial step of the radiomic workflow. Manual segmentation requires long execution time and is prone to variability, impairing the realisation of radiomic studies and their robustness. In this study, a deep-learning automatic segmentation method was applied on computed tomography images of non-small-cell lung cancer patients. The use of manual vs automatic segmentation in the performance of survival radiomic models was assessed, as well. METHODS A total of 899 NSCLC patients were included (2 proprietary: A and B, 1 public datasets: C). Automatic segmentation of lung lesions was performed by training a previously developed architecture, the nnU-Net, including 2D, 3D and cascade approaches. The quality of automatic segmentation was evaluated with DICE coefficient, considering manual contours as reference. The impact of automatic segmentation on the performance of a radiomic model for patient survival was explored by extracting radiomic hand-crafted and deep-learning features from manual and automatic contours of dataset A, and feeding different machine learning algorithms to classify survival above/below median. Models' accuracies were assessed and compared. RESULTS The best agreement between automatic and manual contours with DICE=0.78 +(0.12) was achieved by averaging predictions from 2D and 3D models, and applying a post-processing technique to extract the maximum connected component. No statistical differences were observed in the performances of survival models when using manual or automatic contours, hand-crafted, or deep features. The best classifier showed an accuracy between 0.65 and 0.78. CONCLUSION The promising role of nnU-Net for automatic segmentation of lung lesions was confirmed, dramatically reducing the time-consuming physicians' workload without impairing the accuracy of survival predictive models based on radiomics.



### Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline
- **Arxiv ID**: http://arxiv.org/abs/2209.12028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.12028v1)
- **Published**: 2022-09-24 15:09:02+00:00
- **Updated**: 2022-09-24 15:09:02+00:00
- **Authors**: Lichen Zhao, Daigang Cai, Jing Zhang, Lu Sheng, Dong Xu, Rui Zheng, Yinjie Zhao, Lipeng Wang, Xibo Fan
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: Recently, 3D vision-and-language tasks have attracted increasing research interest. Compared to other vision-and-language tasks, the 3D visual question answering (VQA) task is less exploited and is more susceptible to language priors and co-reference ambiguity. Meanwhile, a couple of recently proposed 3D VQA datasets do not well support 3D VQA task due to their limited scale and annotation methods. In this work, we formally define and address a 3D grounded VQA task by collecting a new 3D VQA dataset, referred to as FE-3DGQA, with diverse and relatively free-form question-answer pairs, as well as dense and completely grounded bounding box annotations. To achieve more explainable answers, we labelled the objects appeared in the complex QA pairs with different semantic types, including answer-grounded objects (both appeared and not appeared in the questions), and contextual objects for answer-grounded objects. We also propose a new 3D VQA framework to effectively predict the completely visually grounded and explainable answer. Extensive experiments verify that our newly collected benchmark datasets can be effectively used to evaluate various 3D VQA methods from different aspects and our newly proposed framework also achieves state-of-the-art performance on the new benchmark dataset. Both the newly collected dataset and our codes will be publicly available at http://github.com/zlccccc/3DGQA.



### Controllable Face Manipulation and UV Map Generation by Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.12050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.12050v1)
- **Published**: 2022-09-24 16:49:25+00:00
- **Updated**: 2022-09-24 16:49:25+00:00
- **Authors**: Yuanming Li, Jeong-gi Kwak, David Han, Hanseok Ko
- **Comment**: None
- **Journal**: None
- **Summary**: Although manipulating facial attributes by Generative Adversarial Networks (GANs) has been remarkably successful recently, there are still some challenges in explicit control of features such as pose, expression, lighting, etc. Recent methods achieve explicit control over 2D images by combining 2D generative model and 3DMM. However, due to the lack of realism and clarity in texture reconstruction by 3DMM, there is a domain gap between the synthetic image and the rendered image of 3DMM. Since rendered 3DMM images contain facial region only without the background, directly computing the loss between these two domains is not ideal and the resultant trained model will be biased. In this study, we propose to explicitly edit the latent space of the pretrained StyleGAN by controlling the parameters of the 3DMM. To address the domain gap problem, we propose a noval network called 'Map and edit' and a simple but effective attribute editing method to avoid direct loss computation between rendered and synthesized images. Furthermore, since our model can accurately generate multi-view face images while the identity remains unchanged. As a by-product, combined with visibility masks, our proposed model can also generate texture-rich and high-resolution UV facial textures. Our model relies on pretrained StyleGAN, and the proposed model is trained in a self-supervised manner without any manual annotations or datasets.



### Global Semantic Descriptors for Zero-Shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.12061v1
- **DOI**: 10.1109/LSP.2022.3200605
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.12061v1)
- **Published**: 2022-09-24 18:15:47+00:00
- **Updated**: 2022-09-24 18:15:47+00:00
- **Authors**: Valter Estevam, Rayson Laroca, Helio Pedrini, David Menotti
- **Comment**: None
- **Journal**: IEEE Signal Processing Letters, vol. 29, pp. 1843-1847, 2022
- **Summary**: The success of Zero-shot Action Recognition (ZSAR) methods is intrinsically related to the nature of semantic side information used to transfer knowledge, although this aspect has not been primarily investigated in the literature. This work introduces a new ZSAR method based on the relationships of actions-objects and actions-descriptive sentences. We demonstrate that representing all object classes using descriptive sentences generates an accurate object-action affinity estimation when a paraphrase estimation method is used as an embedder. We also show how to estimate probabilities over the set of action classes based only on a set of sentences without hard human labeling. In our method, the probabilities from these two global classifiers (i.e., which use features computed over the entire video) are combined, producing an efficient transfer knowledge model for action classification. Our results are state-of-the-art in the Kinetics-400 dataset and are competitive on UCF-101 under the ZSAR evaluation. Our code is available at https://github.com/valterlej/objsentzsar



### Face Super-Resolution Using Stochastic Differential Equations
- **Arxiv ID**: http://arxiv.org/abs/2209.12064v1
- **DOI**: 10.1109/SIBGRAPI55357.2022.9991799
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.12064v1)
- **Published**: 2022-09-24 18:26:06+00:00
- **Updated**: 2022-09-24 18:26:06+00:00
- **Authors**: Marcelo dos Santos, Rayson Laroca, Rafael O. Ribeiro, João Neves, Hugo Proença, David Menotti
- **Comment**: Accepted for presentation at the Conference on Graphics, Patterns and
  Images (SIBGRAPI) 2022
- **Journal**: None
- **Summary**: Diffusion models have proven effective for various applications such as images, audio and graph generation. Other important applications are image super-resolution and the solution of inverse problems. More recently, some works have used stochastic differential equations (SDEs) to generalize diffusion models to continuous time. In this work, we introduce SDEs to generate super-resolution face images. To the best of our knowledge, this is the first time SDEs have been used for such an application. The proposed method provides an improved peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and consistency than the existing super-resolution methods based on diffusion models. In particular, we also assess the potential application of this method for the face recognition task. A generic facial feature extractor is used to compare the super-resolution images with the ground truth and superior results were obtained compared with other methods. Our code is publicly available at https://github.com/marcelowds/sr-sde



### NeRF-Loc: Transformer-Based Object Localization Within Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2209.12068v2
- **DOI**: 10.1109/LRA.2023.3293308
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.12068v2)
- **Published**: 2022-09-24 18:34:22+00:00
- **Updated**: 2023-07-15 08:50:01+00:00
- **Authors**: Jiankai Sun, Yan Xu, Mingyu Ding, Hongwei Yi, Chen Wang, Jingdong Wang, Liangjun Zhang, Mac Schwager
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters ( Volume: 8, Issue: 8, August
  2023)
- **Summary**: Neural Radiance Fields (NeRFs) have become a widely-applied scene representation technique in recent years, showing advantages for robot navigation and manipulation tasks. To further advance the utility of NeRFs for robotics, we propose a transformer-based framework, NeRF-Loc, to extract 3D bounding boxes of objects in NeRF scenes. NeRF-Loc takes a pre-trained NeRF model and camera view as input and produces labeled, oriented 3D bounding boxes of objects as output. Using current NeRF training tools, a robot can train a NeRF environment model in real-time and, using our algorithm, identify 3D bounding boxes of objects of interest within the NeRF for downstream navigation or manipulation tasks. Concretely, we design a pair of paralleled transformer encoder branches, namely the coarse stream and the fine stream, to encode both the context and details of target objects. The encoded features are then fused together with attention layers to alleviate ambiguities for accurate object localization. We have compared our method with conventional RGB(-D) based methods that take rendered RGB images and depths from NeRFs as inputs. Our method is better than the baselines.



### Self-supervised Learning for Unintentional Action Prediction
- **Arxiv ID**: http://arxiv.org/abs/2209.12074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.12074v1)
- **Published**: 2022-09-24 19:06:46+00:00
- **Updated**: 2022-09-24 19:06:46+00:00
- **Authors**: Olga Zatsarynna, Yazan Abu Farha, Juergen Gall
- **Comment**: Accepted to GCPR 2022
- **Journal**: None
- **Summary**: Distinguishing if an action is performed as intended or if an intended action fails is an important skill that not only humans have, but that is also important for intelligent systems that operate in human environments. Recognizing if an action is unintentional or anticipating if an action will fail, however, is not straightforward due to lack of annotated data. While videos of unintentional or failed actions can be found in the Internet in abundance, high annotation costs are a major bottleneck for learning networks for these tasks. In this work, we thus study the problem of self-supervised representation learning for unintentional action prediction. While previous works learn the representation based on a local temporal neighborhood, we show that the global context of a video is needed to learn a good representation for the three downstream tasks: unintentional action classification, localization and anticipation. In the supplementary material, we show that the learned representation can be used for detecting anomalies in videos as well.



### S^2-Transformer for Mask-Aware Hyperspectral Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2209.12075v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.12075v2)
- **Published**: 2022-09-24 19:26:46+00:00
- **Updated**: 2022-12-14 15:41:22+00:00
- **Authors**: Jiamian Wang, Kunpeng Li, Yulun Zhang, Xin Yuan, Zhiqiang Tao
- **Comment**: 11 pages, 16 figures, 6 tables, Code:
  https://github.com/Jiamian-Wang/S2-transformer-HSI
- **Journal**: None
- **Summary**: The technology of hyperspectral imaging (HSI) records the visual information upon long-range-distributed spectral wavelengths. A representative hyperspectral image acquisition procedure conducts a 3D-to-2D encoding by the coded aperture snapshot spectral imager (CASSI) and requires a software decoder for the 3D signal reconstruction. By observing this physical encoding procedure, two major challenges stand in the way of a high-fidelity reconstruction. (i) To obtain 2D measurements, CASSI dislocates multiple channels by disperser-titling and squeezes them onto the same spatial region, yielding an entangled data loss. (ii) The physical coded aperture leads to a masked data loss by selectively blocking the pixel-wise light exposure. To tackle these challenges, we propose a spatial-spectral (S^2-) Transformer network with a mask-aware learning strategy. First, we simultaneously leverage spatial and spectral attention modeling to disentangle the blended information in the 2D measurement along both two dimensions. A series of Transformer structures are systematically designed to fully investigate the spatial and spectral informative properties of the hyperspectral data. Second, the masked pixels will induce higher prediction difficulty and should be treated differently from unmasked ones. Thereby, we adaptively prioritize the loss penalty attributing to the mask structure by inferring the pixel-wise reconstruction difficulty upon the mask-encoded prediction. We theoretically discusses the distinct convergence tendencies between masked/unmasked regions of the proposed learning strategy. Extensive experiments demonstrates that the proposed method achieves superior reconstruction performance. Additionally, we empirically elaborate the behaviour of spatial and spectral attentions under the proposed architecture, and comprehensively examine the impact of the mask-aware learning.



### 3D Reconstruction using Structured Light from off-the-shelf components
- **Arxiv ID**: http://arxiv.org/abs/2209.12101v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.12101v1)
- **Published**: 2022-09-24 22:53:44+00:00
- **Updated**: 2022-09-24 22:53:44+00:00
- **Authors**: Aman Gajendra Jain, Dr. Shital Chiddarwar
- **Comment**: None
- **Journal**: None
- **Summary**: The coordinate measuring machine(CMM) has been the benchmark of accuracy in measuring solid objects from nearly past 50 years or more. However with the advent of 3D scanning technology, the accuracy and the density of point cloud generated has taken over. In this project we not only compare the different algorithms that can be used in a 3D scanning software, but also create our own 3D scanner from off-the-shelf components like camera and projector. Our objective has been : 1. To develop a prototype for 3D scanner to achieve a system that performs at optimal accuracy over a wide typology of objects. 2. To minimise the cost using off-the-shelf components. 3. To reach very close to the accuracy of CMM.



### Conversion Between CT and MRI Images Using Diffusion and Score-Matching Models
- **Arxiv ID**: http://arxiv.org/abs/2209.12104v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2209.12104v2)
- **Published**: 2022-09-24 23:50:54+00:00
- **Updated**: 2022-09-29 20:17:59+00:00
- **Authors**: Qing Lyu, Ge Wang
- **Comment**: None
- **Journal**: None
- **Summary**: MRI and CT are most widely used medical imaging modalities. It is often necessary to acquire multi-modality images for diagnosis and treatment such as radiotherapy planning. However, multi-modality imaging is not only costly but also introduces misalignment between MRI and CT images. To address this challenge, computational conversion is a viable approach between MRI and CT images, especially from MRI to CT images. In this paper, we propose to use an emerging deep learning framework called diffusion and score-matching models in this context. Specifically, we adapt denoising diffusion probabilistic and score-matching models, use four different sampling strategies, and compare their performance metrics with that using a convolutional neural network and a generative adversarial network model. Our results show that the diffusion and score-matching models generate better synthetic CT images than the CNN and GAN models. Furthermore, we investigate the uncertainties associated with the diffusion and score-matching networks using the Monte-Carlo method, and improve the results by averaging their Monte-Carlo outputs. Our study suggests that diffusion and score-matching models are powerful to generate high quality images conditioned on an image obtained using a complementary imaging modality, analytically rigorous with clear explainability, and highly competitive with CNNs and GANs for image synthesis.



