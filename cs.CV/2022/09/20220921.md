# Arxiv Papers in cs.CV on 2022-09-21
### Mutual Information Learned Classifiers: an Information-theoretic Viewpoint of Training Deep Learning Classification Systems
- **Arxiv ID**: http://arxiv.org/abs/2209.10058v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2209.10058v1)
- **Published**: 2022-09-21 01:06:30+00:00
- **Updated**: 2022-09-21 01:06:30+00:00
- **Authors**: Jirong Yi, Qiaosheng Zhang, Zhen Chen, Qiao Liu, Wei Shao
- **Comment**: 22 pages, 17 figures, 3 tables, 5 theorems
- **Journal**: None
- **Summary**: Deep learning systems have been reported to achieve state-of-the-art performances in many applications, and a key is the existence of well trained classifiers on benchmark datasets. As a main-stream loss function, the cross entropy can easily lead us to find models which demonstrate severe overfitting behavior. In this paper, we show that the existing cross entropy loss minimization problem essentially learns the label conditional entropy (CE) of the underlying data distribution of the dataset. However, the CE learned in this way does not characterize well the information shared by the label and the input. In this paper, we propose a mutual information learning framework where we train deep neural network classifiers via learning the mutual information between the label and the input. Theoretically, we give the population classification error lower bound in terms of the mutual information. In addition, we derive the mutual information lower and upper bounds for a concrete binary classification data model in $\mathbb{R}^n$, and also the error probability lower bound in this scenario. Empirically, we conduct extensive experiments on several benchmark datasets to support our theory. The mutual information learned classifiers (MILCs) achieve far better generalization performances than the conditional entropy learned classifiers (CELCs) with an improvement which can exceed more than 10\% in testing accuracy.



### Progressive with Purpose: Guiding Progressive Inpainting DNNs through Context and Structure
- **Arxiv ID**: http://arxiv.org/abs/2209.10071v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10071v2)
- **Published**: 2022-09-21 02:15:02+00:00
- **Updated**: 2023-01-03 22:15:54+00:00
- **Authors**: Kangdi Shi, Muhammad Alrabeiah, Jun Chen
- **Comment**: None
- **Journal**: None
- **Summary**: The advent of deep learning in the past decade has significantly helped advance image inpainting. Although achieving promising performance, deep learning-based inpainting algorithms still struggle from the distortion caused by the fusion of structural and contextual features, which are commonly obtained from, respectively, deep and shallow layers of a convolutional encoder. Motivated by this observation, we propose a novel progressive inpainting network that maintains the structural and contextual integrity of a processed image. More specifically, inspired by the Gaussian and Laplacian pyramids, the core of the proposed network is a feature extraction module named GLE. Stacking GLE modules enables the network to extract image features from different image frequency components. This ability is important to maintain structural and contextual integrity, for high frequency components correspond to structural information while low frequency components correspond to contextual information. The proposed network utilizes the GLE features to progressively fill in missing regions in a corrupted image in an iterative manner. Our benchmarking experiments demonstrate that the proposed method achieves clear improvement in performance over many state-of-the-art inpainting algorithms.



### Adaptive Local-Component-aware Graph Convolutional Network for One-shot Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.10073v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10073v1)
- **Published**: 2022-09-21 02:33:07+00:00
- **Updated**: 2022-09-21 02:33:07+00:00
- **Authors**: Anqi Zhu, Qiuhong Ke, Mingming Gong, James Bailey
- **Comment**: None
- **Journal**: None
- **Summary**: Skeleton-based action recognition receives increasing attention because the skeleton representations reduce the amount of training data by eliminating visual information irrelevant to actions. To further improve the sample efficiency, meta-learning-based one-shot learning solutions were developed for skeleton-based action recognition. These methods find the nearest neighbor according to the similarity between instance-level global average embedding. However, such measurement holds unstable representativity due to inadequate generalized learning on local invariant and noisy features, while intuitively, more fine-grained recognition usually relies on determining key local body movements. To address this limitation, we present the Adaptive Local-Component-aware Graph Convolutional Network, which replaces the comparison metric with a focused sum of similarity measurements on aligned local embedding of action-critical spatial/temporal segments. Comprehensive one-shot experiments on the public benchmark of NTU-RGB+D 120 indicate that our method provides a stronger representation than the global embedding and helps our model reach state-of-the-art.



### PicT: A Slim Weakly Supervised Vision Transformer for Pavement Distress Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.10074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10074v1)
- **Published**: 2022-09-21 02:33:49+00:00
- **Updated**: 2022-09-21 02:33:49+00:00
- **Authors**: Wenhao Tang, Sheng Huang, Xiaoxian Zhang, Luwen Huangfu
- **Comment**: ACM Multimedia 2022 paper, 9 pages 7 figures
- **Journal**: None
- **Summary**: Automatic pavement distress classification facilitates improving the efficiency of pavement maintenance and reducing the cost of labor and resources. A recently influential branch of this task divides the pavement image into patches and addresses these issues from the perspective of multi-instance learning. However, these methods neglect the correlation between patches and suffer from a low efficiency in the model optimization and inference. Meanwhile, Swin Transformer is able to address both of these issues with its unique strengths. Built upon Swin Transformer, we present a vision Transformer named \textbf{P}avement \textbf{I}mage \textbf{C}lassification \textbf{T}ransformer (\textbf{PicT}) for pavement distress classification. In order to better exploit the discriminative information of pavement images at the patch level, the \textit{Patch Labeling Teacher} is proposed to leverage a teacher model to dynamically generate pseudo labels of patches from image labels during each iteration, and guides the model to learn the discriminative features of patches. The broad classification head of Swin Transformer may dilute the discriminative features of distressed patches in the feature aggregation step due to the small distressed area ratio of the pavement image. To overcome this drawback, we present a \textit{Patch Refiner} to cluster patches into different groups and only select the highest distress-risk group to yield a slim head for the final image classification. We evaluate our method on CQU-BPDD. Extensive results show that \textbf{PicT} outperforms the second-best performed model by a large margin of $+2.4\%$ in P@R on detection task, $+3.9\%$ in $F1$ on recognition task, and 1.8x throughput, while enjoying 7x faster training speed using the same computing resources. Our codes and models have been released on \href{https://github.com/DearCaat/PicT}{https://github.com/DearCaat/PicT}.



### Can Shadows Reveal Biometric Information?
- **Arxiv ID**: http://arxiv.org/abs/2209.10077v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.10077v2)
- **Published**: 2022-09-21 02:36:32+00:00
- **Updated**: 2022-10-04 16:27:08+00:00
- **Authors**: Safa C. Medin, Amir Weiss, Frédo Durand, William T. Freeman, Gregory W. Wornell
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of extracting biometric information of individuals by looking at shadows of objects cast on diffuse surfaces. We show that the biometric information leakage from shadows can be sufficient for reliable identity inference under representative scenarios via a maximum likelihood analysis. We then develop a learning-based method that demonstrates this phenomenon in real settings, exploiting the subtle cues in the shadows that are the source of the leakage without requiring any labeled real data. In particular, our approach relies on building synthetic scenes composed of 3D face models obtained from a single photograph of each identity. We transfer what we learn from the synthetic data to the real data using domain adaptation in a completely unsupervised way. Our model is able to generalize well to the real domain and is robust to several variations in the scenes. We report high classification accuracies in an identity classification task that takes place in a scene with unknown geometry and occluding objects.



### Exploring Modulated Detection Transformer as a Tool for Action Recognition in Videos
- **Arxiv ID**: http://arxiv.org/abs/2209.10126v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.10126v1)
- **Published**: 2022-09-21 05:19:39+00:00
- **Updated**: 2022-09-21 05:19:39+00:00
- **Authors**: Tomás Crisol, Joel Ermantraut, Adrián Rostagno, Santiago L. Aggio, Javier Iparraguirre
- **Comment**: 5 pages, 2 figures, 1 results chart
- **Journal**: JAIIO - JORNADAS ARGENTINAS DE INFORMATICA 2022
- **Summary**: During recent years transformers architectures have been growing in popularity. Modulated Detection Transformer (MDETR) is an end-to-end multi-modal understanding model that performs tasks such as phase grounding, referring expression comprehension, referring expression segmentation, and visual question answering. One remarkable aspect of the model is the capacity to infer over classes that it was not previously trained for. In this work we explore the use of MDETR in a new task, action detection, without any previous training. We obtain quantitative results using the Atomic Visual Actions dataset. Although the model does not report the best performance in the task, we believe that it is an interesting finding. We show that it is possible to use a multi-modal model to tackle a task that it was not designed for. Finally, we believe that this line of research may lead into the generalization of MDETR in additional downstream tasks.



### Recipe Generation from Unsegmented Cooking Videos
- **Arxiv ID**: http://arxiv.org/abs/2209.10134v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10134v1)
- **Published**: 2022-09-21 05:54:13+00:00
- **Updated**: 2022-09-21 05:54:13+00:00
- **Authors**: Taichi Nishimura, Atsushi Hashimoto, Yoshitaka Ushiku, Hirotaka Kameko, Shinsuke Mori
- **Comment**: 11 pages, 9 figures. Under review
- **Journal**: None
- **Summary**: This paper tackles recipe generation from unsegmented cooking videos, a task that requires agents to (1) extract key events in completing the dish and (2) generate sentences for the extracted events. Our task is similar to dense video captioning (DVC), which aims at detecting events thoroughly and generating sentences for them. However, unlike DVC, in recipe generation, recipe story awareness is crucial, and a model should output an appropriate number of key events in the correct order. We analyze the output of the DVC model and observe that although (1) several events are adoptable as a recipe story, (2) the generated sentences for such events are not grounded in the visual content. Based on this, we hypothesize that we can obtain correct recipes by selecting oracle events from the output events of the DVC model and re-generating sentences for them. To achieve this, we propose a novel transformer-based joint approach of training event selector and sentence generator for selecting oracle events from the outputs of the DVC model and generating grounded sentences for the events, respectively. In addition, we extend the model by including ingredients to generate more accurate recipes. The experimental results show that the proposed method outperforms state-of-the-art DVC models. We also confirm that, by modeling the recipe in a story-aware manner, the proposed model output the appropriate number of events in the correct order.



### Detecting Crop Burning in India using Satellite Data
- **Arxiv ID**: http://arxiv.org/abs/2209.10148v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, econ.GN, q-fin.EC
- **Links**: [PDF](http://arxiv.org/pdf/2209.10148v1)
- **Published**: 2022-09-21 06:58:08+00:00
- **Updated**: 2022-09-21 06:58:08+00:00
- **Authors**: Kendra Walker, Ben Moscona, Kelsey Jack, Seema Jayachandran, Namrata Kala, Rohini Pande, Jiani Xue, Marshall Burke
- **Comment**: None
- **Journal**: None
- **Summary**: Crop residue burning is a major source of air pollution in many parts of the world, notably South Asia. Policymakers, practitioners and researchers have invested in both measuring impacts and developing interventions to reduce burning. However, measuring the impacts of burning or the effectiveness of interventions to reduce burning requires data on where burning occurred. These data are challenging to collect in the field, both in terms of cost and feasibility. We take advantage of data from ground-based monitoring of crop residue burning in Punjab, India to explore whether burning can be detected more effectively using accessible satellite imagery. Specifically, we used 3m PlanetScope data with high temporal resolution (up to daily) as well as publicly-available Sentinel-2 data with weekly temporal resolution but greater depth of spectral information. Following an analysis of the ability of different spectral bands and burn indices to separate burned and unburned plots individually, we built a Random Forest model with those determined to provide the greatest separability and evaluated model performance with ground-verified data. Our overall model accuracy of 82-percent is favorable given the challenges presented by the measurement. Based on insights from this process, we discuss technical challenges of detecting crop residue burning from satellite imagery as well as challenges to measuring impacts, both of burning and of policy interventions.



### RNGDet++: Road Network Graph Detection by Transformer with Instance Segmentation and Multi-scale Features Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2209.10150v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.10150v2)
- **Published**: 2022-09-21 07:06:46+00:00
- **Updated**: 2023-03-15 03:05:12+00:00
- **Authors**: Zhenhua Xu, Yuxuan Liu, Yuxiang Sun, Ming Liu, Lujia Wang
- **Comment**: Accepted by IEEE Robotics and Automation Letters (RA-L)
- **Journal**: None
- **Summary**: The road network graph is a critical component for downstream tasks in autonomous driving, such as global route planning and navigation. In the past years, road network graphs are usually annotated by human experts manually, which is time-consuming and labor-intensive. To annotate road network graphs effectively and efficiently, automatic algorithms for road network graph detection are demanded. Most existing methods either adopt a post-processing step on semantic segmentation maps to produce road network graphs, or propose graph-based algorithms to directly predict the graphs. However, these works suffer from hard-coded algorithms and inferior performance. To enhance the previous state-of-the-art (SOTA) method RNGDet, we add an instance segmentation head to better supervise the training, and enable the network to leverage multi-scale features of the backbone. Since the new proposed approach is improved from RNGDet, we name it RNGDet++. Experimental results show that our RNGDet++ outperforms baseline methods in terms of almost all evaluation metrics on two large-scale public datasets. Our code and supplementary materials are available at \url{https://tonyxuqaq.github.io/projects/RNGDetPlusPlus/}.



### Review On Deep Learning Technique For Underwater Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.10151v1
- **DOI**: 10.5121/csit.2022.121505
- **Categories**: **cs.CV**, I.4.0; I.4.10; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/2209.10151v1)
- **Published**: 2022-09-21 07:10:44+00:00
- **Updated**: 2022-09-21 07:10:44+00:00
- **Authors**: Radhwan Adnan Dakhil, Ali Retha Hasoon Khayeat
- **Comment**: 15 pages, with 9 figures,3rd International Conference on Data Science
  and Machine Learning (DSML 2022)
- **Journal**: David C. Wyld et al. (Eds): ARIA, SIPR, SOFEA, CSEN, DSML, NLP,
  EDTECH, NCWC - 2022 pp. 49-63, 2022. CS & IT - CSCP 2022
- **Summary**: Repair and maintenance of underwater structures as well as marine science rely heavily on the results of underwater object detection, which is a crucial part of the image processing workflow. Although many computer vision-based approaches have been presented, no one has yet developed a system that reliably and accurately detects and categorizes objects and animals found in the deep sea. This is largely due to obstacles that scatter and absorb light in an underwater setting. With the introduction of deep learning, scientists have been able to address a wide range of issues, including safeguarding the marine ecosystem, saving lives in an emergency, preventing underwater disasters, and detecting, spooring, and identifying underwater targets. However, the benefits and drawbacks of these deep learning systems remain unknown. Therefore, the purpose of this article is to provide an overview of the dataset that has been utilized in underwater object detection and to present a discussion of the advantages and disadvantages of the algorithms employed for this purpose.



### FT-HID: A Large Scale RGB-D Dataset for First and Third Person Human Interaction Analysis
- **Arxiv ID**: http://arxiv.org/abs/2209.10155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10155v1)
- **Published**: 2022-09-21 07:24:15+00:00
- **Updated**: 2022-09-21 07:24:15+00:00
- **Authors**: Zihui Guo, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, Wanqing Li
- **Comment**: None
- **Journal**: None
- **Summary**: Analysis of human interaction is one important research topic of human motion analysis. It has been studied either using first person vision (FPV) or third person vision (TPV). However, the joint learning of both types of vision has so far attracted little attention. One of the reasons is the lack of suitable datasets that cover both FPV and TPV. In addition, existing benchmark datasets of either FPV or TPV have several limitations, including the limited number of samples, participant subjects, interaction categories, and modalities. In this work, we contribute a large-scale human interaction dataset, namely, FT-HID dataset. FT-HID contains pair-aligned samples of first person and third person visions. The dataset was collected from 109 distinct subjects and has more than 90K samples for three modalities. The dataset has been validated by using several existing action recognition methods. In addition, we introduce a novel multi-view interaction mechanism for skeleton sequences, and a joint learning multi-stream framework for first person and third person visions. Both methods yield promising results on the FT-HID dataset. It is expected that the introduction of this vision-aligned large-scale dataset will promote the development of both FPV and TPV, and their joint learning techniques for human action analysis. The dataset and code are available at \href{https://github.com/ENDLICHERE/FT-HID}{here}.



### Position-Aware Relation Learning for RGB-Thermal Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.10158v1
- **DOI**: 10.1109/TIP.2023.3270801
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10158v1)
- **Published**: 2022-09-21 07:34:30+00:00
- **Updated**: 2022-09-21 07:34:30+00:00
- **Authors**: Heng Zhou, Chunna Tian, Zhenxi Zhang, Chengyang Li, Yuxuan Ding, Yongqiang Xie, Zhongbo Li
- **Comment**: None
- **Journal**: None
- **Summary**: RGB-Thermal salient object detection (SOD) combines two spectra to segment visually conspicuous regions in images. Most existing methods use boundary maps to learn the sharp boundary. These methods ignore the interactions between isolated boundary pixels and other confident pixels, leading to sub-optimal performance. To address this problem,we propose a position-aware relation learning network (PRLNet) for RGB-T SOD based on swin transformer. PRLNet explores the distance and direction relationships between pixels to strengthen intra-class compactness and inter-class separation, generating salient object masks with clear boundaries and homogeneous regions. Specifically, we develop a novel signed distance map auxiliary module (SDMAM) to improve encoder feature representation, which takes into account the distance relation of different pixels in boundary neighborhoods. Then, we design a feature refinement approach with directional field (FRDF), which rectifies features of boundary neighborhood by exploiting the features inside salient objects. FRDF utilizes the directional information between object pixels to effectively enhance the intra-class compactness of salient regions. In addition, we constitute a pure transformer encoder-decoder network to enhance multispectral feature representation for RGB-T SOD. Finally, we conduct quantitative and qualitative experiments on three public benchmark datasets.The results demonstrate that our proposed method outperforms the state-of-the-art methods.



### HAZE-Net: High-Frequency Attentive Super-Resolved Gaze Estimation in Low-Resolution Face Images
- **Arxiv ID**: http://arxiv.org/abs/2209.10167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10167v1)
- **Published**: 2022-09-21 07:57:07+00:00
- **Updated**: 2022-09-21 07:57:07+00:00
- **Authors**: Jun-Seok Yun, Youngju Na, Hee Hyeon Kim, Hyung-Il Kim, Seok Bong Yoo
- **Comment**: None
- **Journal**: None
- **Summary**: Although gaze estimation methods have been developed with deep learning techniques, there has been no such approach as aim to attain accurate performance in low-resolution face images with a pixel width of 50 pixels or less. To solve a limitation under the challenging low-resolution conditions, we propose a high-frequency attentive super-resolved gaze estimation network, i.e., HAZE-Net. Our network improves the resolution of the input image and enhances the eye features and those boundaries via a proposed super-resolution module based on a high-frequency attention block. In addition, our gaze estimation module utilizes high-frequency components of the eye as well as the global appearance map. We also utilize the structural location information of faces to approximate head pose. The experimental results indicate that the proposed method exhibits robust gaze estimation performance even in low-resolution face images with 28x28 pixels. The source code of this work is available at https://github.com/dbseorms16/HAZE_Net/.



### FV2ES: A Fully End2End Multimodal System for Fast Yet Effective Video Emotion Recognition Inference
- **Arxiv ID**: http://arxiv.org/abs/2209.10170v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10170v1)
- **Published**: 2022-09-21 08:05:26+00:00
- **Updated**: 2022-09-21 08:05:26+00:00
- **Authors**: Qinglan Wei, Xuling Huang, Yuan Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In the latest social networks, more and more people prefer to express their emotions in videos through text, speech, and rich facial expressions. Multimodal video emotion analysis techniques can help understand users' inner world automatically based on human expressions and gestures in images, tones in voices, and recognized natural language. However, in the existing research, the acoustic modality has long been in a marginal position as compared to visual and textual modalities. That is, it tends to be more difficult to improve the contribution of the acoustic modality for the whole multimodal emotion recognition task. Besides, although better performance can be obtained by introducing common deep learning methods, the complex structures of these training models always result in low inference efficiency, especially when exposed to high-resolution and long-length videos. Moreover, the lack of a fully end-to-end multimodal video emotion recognition system hinders its application. In this paper, we designed a fully multimodal video-to-emotion system (named FV2ES) for fast yet effective recognition inference, whose benefits are threefold: (1) The adoption of the hierarchical attention method upon the sound spectra breaks through the limited contribution of the acoustic modality and outperforms the existing models' performance on both IEMOCAP and CMU-MOSEI datasets; (2) the introduction of the idea of multi-scale for visual extraction while single-branch for inference brings higher efficiency and maintains the prediction accuracy at the same time; (3) the further integration of data pre-processing into the aligned multimodal learning model allows the significant reduction of computational costs and storage space.



### LatentGaze: Cross-Domain Gaze Estimation through Gaze-Aware Analytic Latent Code Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2209.10171v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10171v1)
- **Published**: 2022-09-21 08:05:53+00:00
- **Updated**: 2022-09-21 08:05:53+00:00
- **Authors**: Isack Lee, Jun-Seok Yun, Hee Hyeon Kim, Youngju Na, Seok Bong Yoo
- **Comment**: None
- **Journal**: None
- **Summary**: Although recent gaze estimation methods lay great emphasis on attentively extracting gaze-relevant features from facial or eye images, how to define features that include gaze-relevant components has been ambiguous. This obscurity makes the model learn not only gaze-relevant features but also irrelevant ones. In particular, it is fatal for the cross-dataset performance. To overcome this challenging issue, we propose a gaze-aware analytic manipulation method, based on a data-driven approach with generative adversarial network inversion's disentanglement characteristics, to selectively utilize gaze-relevant features in a latent code. Furthermore, by utilizing GAN-based encoder-generator process, we shift the input image from the target domain to the source domain image, which a gaze estimator is sufficiently aware. In addition, we propose gaze distortion loss in the encoder that prevents the distortion of gaze information. The experimental results demonstrate that our method achieves state-of-the-art gaze estimation accuracy in a cross-domain gaze estimation tasks. This code is available at https://github.com/leeisack/LatentGaze/.



### Learning Reconstructability for Drone Aerial Path Planning
- **Arxiv ID**: http://arxiv.org/abs/2209.10174v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10174v1)
- **Published**: 2022-09-21 08:10:26+00:00
- **Updated**: 2022-09-21 08:10:26+00:00
- **Authors**: Yilin Liu, Liqiang Lin, Yue Hu, Ke Xie, Chi-Wing Fu, Hao Zhang, Hui Huang
- **Comment**: Accepted by SIGGRAPH Asia 2022
- **Journal**: None
- **Summary**: We introduce the first learning-based reconstructability predictor to improve view and path planning for large-scale 3D urban scene acquisition using unmanned drones. In contrast to previous heuristic approaches, our method learns a model that explicitly predicts how well a 3D urban scene will be reconstructed from a set of viewpoints. To make such a model trainable and simultaneously applicable to drone path planning, we simulate the proxy-based 3D scene reconstruction during training to set up the prediction. Specifically, the neural network we design is trained to predict the scene reconstructability as a function of the proxy geometry, a set of viewpoints, and optionally a series of scene images acquired in flight. To reconstruct a new urban scene, we first build the 3D scene proxy, then rely on the predicted reconstruction quality and uncertainty measures by our network, based off of the proxy geometry, to guide the drone path planning. We demonstrate that our data-driven reconstructability predictions are more closely correlated to the true reconstruction quality than prior heuristic measures. Further, our learned predictor can be easily integrated into existing path planners to yield improvements. Finally, we devise a new iterative view planning framework, based on the learned reconstructability, and show superior performance of the new planner when reconstructing both synthetic and real scenes.



### D-InLoc++: Indoor Localization in Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/2209.10185v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10185v1)
- **Published**: 2022-09-21 08:35:32+00:00
- **Updated**: 2022-09-21 08:35:32+00:00
- **Authors**: Martina Dubenova, Anna Zderadickova, Ondrej Kafka, Tomas Pajdla, Michal Polic
- **Comment**: GCPR2022
- **Journal**: None
- **Summary**: Most state-of-the-art localization algorithms rely on robust relative pose estimation and geometry verification to obtain moving object agnostic camera poses in complex indoor environments. However, this approach is prone to mistakes if a scene contains repetitive structures, e.g., desks, tables, boxes, or moving people. We show that the movable objects incorporate non-negligible localization error and present a new straightforward method to predict the six-degree-of-freedom (6DoF) pose more robustly. We equipped the localization pipeline InLoc with real-time instance segmentation network YOLACT++. The masks of dynamic objects are employed in the relative pose estimation step and in the final sorting of camera pose proposal. At first, we filter out the matches laying on masks of the dynamic objects. Second, we skip the comparison of query and synthetic images on the area related to the moving object. This procedure leads to a more robust localization. Lastly, we describe and improve the mistakes caused by gradient-based comparison between synthetic and query images and publish a new pipeline for simulation of environments with movable objects from the Matterport scans. All the codes are available on github.com/dubenma/D-InLocpp .



### Implicit Conversion of Manifold B-Rep Solids by Neural Halfspace Representation
- **Arxiv ID**: http://arxiv.org/abs/2209.10191v1
- **DOI**: 10.1145/3550454.3555502
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10191v1)
- **Published**: 2022-09-21 08:43:06+00:00
- **Updated**: 2022-09-21 08:43:06+00:00
- **Authors**: Hao-Xiang Guo, Yang Liu, Hao Pan, Baining Guo
- **Comment**: Accepted to SIGGRAPH Asia 2022. Our supplemental material and code
  are available at https://guohaoxiang.github.io/projects/nhrep.html
- **Journal**: ACM Trans. Graph. 41, 4, Article 128 (July 2022), 13 pages
- **Summary**: We present a novel implicit representation -- neural halfspace representation (NH-Rep), to convert manifold B-Rep solids to implicit representations. NH-Rep is a Boolean tree built on a set of implicit functions represented by the neural network, and the composite Boolean function is capable of representing solid geometry while preserving sharp features. We propose an efficient algorithm to extract the Boolean tree from a manifold B-Rep solid and devise a neural network-based optimization approach to compute the implicit functions. We demonstrate the high quality offered by our conversion algorithm on ten thousand manifold B-Rep CAD models that contain various curved patches including NURBS, and the superiority of our learning approach over other representative implicit conversion algorithms in terms of surface reconstruction, sharp feature preservation, signed distance field approximation, and robustness to various surface geometry, as well as a set of applications supported by NH-Rep.



### Multi-Field De-interlacing using Deformable Convolution Residual Blocks and Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/2209.10192v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10192v1)
- **Published**: 2022-09-21 08:45:03+00:00
- **Updated**: 2022-09-21 08:45:03+00:00
- **Authors**: Ronglei Ji, A. Murat Tekalp
- **Comment**: 5 pages, 4 figures, accepted to ICIP 2022
- **Journal**: None
- **Summary**: Although deep learning has made significant impact on image/video restoration and super-resolution, learned deinterlacing has so far received less attention in academia or industry. This is despite deinterlacing is well-suited for supervised learning from synthetic data since the degradation model is known and fixed. In this paper, we propose a novel multi-field full frame-rate deinterlacing network, which adapts the state-of-the-art superresolution approaches to the deinterlacing task. Our model aligns features from adjacent fields to a reference field (to be deinterlaced) using both deformable convolution residual blocks and self attention. Our extensive experimental results demonstrate that the proposed method provides state-of-the-art deinterlacing results in terms of both numerical and perceptual performance. At the time of writing, our model ranks first in the Full FrameRate LeaderBoard at https://videoprocessing.ai/benchmarks/deinterlacer.html



### Kernel-Based Generalized Median Computation for Consensus Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.10208v1
- **DOI**: 10.1109/TPAMI.2022.3202565
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10208v1)
- **Published**: 2022-09-21 09:09:01+00:00
- **Updated**: 2022-09-21 09:09:01+00:00
- **Authors**: Andreas Nienkötter, Xiaoyi Jiang
- **Comment**: 17 pages, 5 figures, 7 tables
- **Journal**: Early Access by TPAMI 2022
  (https://ieeexplore.ieee.org/document/9869722)
- **Summary**: Computing a consensus object from a set of given objects is a core problem in machine learning and pattern recognition. One popular approach is to formulate it as an optimization problem using the generalized median. Previous methods like the Prototype and Distance-Preserving Embedding methods transform objects into a vector space, solve the generalized median problem in this space, and inversely transform back into the original space. Both of these methods have been successfully applied to a wide range of object domains, where the generalized median problem has inherent high computational complexity (typically $\mathcal{NP}$-hard) and therefore approximate solutions are required. Previously, explicit embedding methods were used in the computation, which often do not reflect the spatial relationship between objects exactly. In this work we introduce a kernel-based generalized median framework that is applicable to both positive definite and indefinite kernels. This framework computes the relationship between objects and its generalized median in kernel space, without the need of an explicit embedding. We show that the spatial relationship between objects is more accurately represented in kernel space than in an explicit vector space using easy-to-compute kernels, and demonstrate superior performance of generalized median computation on datasets of three different domains. A software toolbox resulting from our work is made publicly available to encourage other researchers to explore the generalized median computation and applications.



### HiFuse: Hierarchical Multi-Scale Feature Fusion Network for Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.10218v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10218v1)
- **Published**: 2022-09-21 09:30:20+00:00
- **Updated**: 2022-09-21 09:30:20+00:00
- **Authors**: Xiangzuo Huo, Gang Sun, Shengwei Tian, Yan Wang, Long Yu, Jun Long, Wendong Zhang, Aolun Li
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image classification has developed rapidly under the impetus of the convolutional neural network (CNN). Due to the fixed size of the receptive field of the convolution kernel, it is difficult to capture the global features of medical images. Although the self-attention-based Transformer can model long-range dependencies, it has high computational complexity and lacks local inductive bias. Much research has demonstrated that global and local features are crucial for image classification. However, medical images have a lot of noisy, scattered features, intra-class variation, and inter-class similarities. This paper proposes a three-branch hierarchical multi-scale feature fusion network structure termed as HiFuse for medical image classification as a new method. It can fuse the advantages of Transformer and CNN from multi-scale hierarchies without destroying the respective modeling so as to improve the classification accuracy of various medical images. A parallel hierarchy of local and global feature blocks is designed to efficiently extract local features and global representations at various semantic scales, with the flexibility to model at different scales and linear computational complexity relevant to image size. Moreover, an adaptive hierarchical feature fusion block (HFF block) is designed to utilize the features obtained at different hierarchical levels comprehensively. The HFF block contains spatial attention, channel attention, residual inverted MLP, and shortcut to adaptively fuse semantic information between various scale features of each branch. The accuracy of our proposed model on the ISIC2018 dataset is 7.6% higher than baseline, 21.5% on the Covid-19 dataset, and 10.4% on the Kvasir dataset. Compared with other advanced models, the HiFuse model performs the best. Our code is open-source and available from https://github.com/huoxiangzuo/HiFuse.



### Intelligent wayfinding vehicle design based on visual recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.10229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10229v1)
- **Published**: 2022-09-21 09:49:16+00:00
- **Updated**: 2022-09-21 09:49:16+00:00
- **Authors**: Zhanyu Guo, Shenyuan Guo, Jialong Wang, Yifan Feng
- **Comment**: in Chinese language
- **Journal**: None
- **Summary**: Intelligent drug delivery trolley is an advanced intelligent drug delivery equipment. Compared with traditional manual drug delivery, it has higher drug delivery efficiency and lower error rate. In this project, an intelligent drug delivery car is designed and manufactured, which can recognize the road route and the room number of the target ward through visual recognition technology. The trolley selects the corresponding route according to the identified room number, accurately transports the drugs to the target ward, and can return to the pharmacy after the drugs are delivered. The intelligent drug delivery car uses DC power supply, and the motor drive module controls two DC motors, which overcomes the problem of excessive deviation of turning angle. The trolley line inspection function uses closed-loop control to improve the accuracy of line inspection and the controllability of trolley speed. The identification of ward number is completed by the camera module with microcontroller, and has the functions of adaptive adjustment of ambient brightness, distortion correction, automatic calibration and so on. The communication between two cooperative drug delivery vehicles is realized by Bluetooth module, which achieves efficient and accurate communication and interaction. Experiments show that the intelligent drug delivery car can accurately identify the room number and plan the route to deliver drugs to the far, middle and near wards, and has the characteristics of fast speed and accurate judgment. In addition, two drug delivery trolleys can cooperate to deliver drugs to the same ward, with high efficiency and high cooperation.



### BEVStereo: Enhancing Depth Estimation in Multi-view 3D Object Detection with Dynamic Temporal Stereo
- **Arxiv ID**: http://arxiv.org/abs/2209.10248v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10248v1)
- **Published**: 2022-09-21 10:21:25+00:00
- **Updated**: 2022-09-21 10:21:25+00:00
- **Authors**: Yinhao Li, Han Bao, Zheng Ge, Jinrong Yang, Jianjian Sun, Zeming Li
- **Comment**: None
- **Journal**: None
- **Summary**: Bounded by the inherent ambiguity of depth perception, contemporary camera-based 3D object detection methods fall into the performance bottleneck. Intuitively, leveraging temporal multi-view stereo (MVS) technology is the natural knowledge for tackling this ambiguity. However, traditional attempts of MVS are flawed in two aspects when applying to 3D object detection scenes: 1) The affinity measurement among all views suffers expensive computation cost; 2) It is difficult to deal with outdoor scenarios where objects are often mobile. To this end, we introduce an effective temporal stereo method to dynamically select the scale of matching candidates, enable to significantly reduce computation overhead. Going one step further, we design an iterative algorithm to update more valuable candidates, making it adaptive to moving candidates. We instantiate our proposed method to multi-view 3D detector, namely BEVStereo. BEVStereo achieves the new state-of-the-art performance (i.e., 52.5% mAP and 61.0% NDS) on the camera-only track of nuScenes dataset. Meanwhile, extensive experiments reflect our method can deal with complex outdoor scenarios better than contemporary MVS approaches. Codes have been released at https://github.com/Megvii-BaseDetection/BEVStereo.



### Query-Guided Networks for Few-shot Fine-grained Classification and Person Search
- **Arxiv ID**: http://arxiv.org/abs/2209.10250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10250v1)
- **Published**: 2022-09-21 10:25:32+00:00
- **Updated**: 2022-09-21 10:25:32+00:00
- **Authors**: Bharti Munjal, Alessandro Flaborea, Sikandar Amin, Federico Tombari, Fabio Galasso
- **Comment**: Accepted at Pattern Recognition Journal 2022
- **Journal**: None
- **Summary**: Few-shot fine-grained classification and person search appear as distinct tasks and literature has treated them separately. But a closer look unveils important similarities: both tasks target categories that can only be discriminated by specific object details; and the relevant models should generalize to new categories, not seen during training.   We propose a novel unified Query-Guided Network (QGN) applicable to both tasks. QGN consists of a Query-guided Siamese-Squeeze-and-Excitation subnetwork which re-weights both the query and gallery features across all network layers, a Query-guided Region Proposal subnetwork for query-specific localisation, and a Query-guided Similarity subnetwork for metric learning.   QGN improves on a few recent few-shot fine-grained datasets, outperforming other techniques on CUB by a large margin. QGN also performs competitively on the person search CUHK-SYSU and PRW datasets, where we perform in-depth analysis.



### Deep Learning on Home Drone: Searching for the Optimal Architecture
- **Arxiv ID**: http://arxiv.org/abs/2209.11064v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.11064v1)
- **Published**: 2022-09-21 11:41:45+00:00
- **Updated**: 2022-09-21 11:41:45+00:00
- **Authors**: Alaa Maalouf, Yotam Gurfinkel, Barak Diker, Oren Gal, Daniela Rus, Dan Feldman
- **Comment**: None
- **Journal**: None
- **Summary**: We suggest the first system that runs real-time semantic segmentation via deep learning on a weak micro-computer such as the Raspberry Pi Zero v2 (whose price was \$15) attached to a toy-drone. In particular, since the Raspberry Pi weighs less than $16$ grams, and its size is half of a credit card, we could easily attach it to the common commercial DJI Tello toy-drone (<\$100, <90 grams, 98 $\times$ 92.5 $\times$ 41 mm). The result is an autonomous drone (no laptop nor human in the loop) that can detect and classify objects in real-time from a video stream of an on-board monocular RGB camera (no GPS or LIDAR sensors). The companion videos demonstrate how this Tello drone scans the lab for people (e.g. for the use of firefighters or security forces) and for an empty parking slot outside the lab.   Existing deep learning solutions are either much too slow for real-time computation on such IoT devices, or provide results of impractical quality. Our main challenge was to design a system that takes the best of all worlds among numerous combinations of networks, deep learning platforms/frameworks, compression techniques, and compression ratios. To this end, we provide an efficient searching algorithm that aims to find the optimal combination which results in the best tradeoff between the network running time and its accuracy/performance.



### AirFi: Empowering WiFi-based Passive Human Gesture Recognition to Unseen Environment via Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2209.10285v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2209.10285v2)
- **Published**: 2022-09-21 11:54:00+00:00
- **Updated**: 2022-12-17 01:54:12+00:00
- **Authors**: Dazhuo Wang, Jianfei Yang, Wei Cui, Lihua Xie, Sumei Sun
- **Comment**: The paper has been accepted by IEEE Transactions on Mobile Computing
- **Journal**: None
- **Summary**: WiFi-based smart human sensing technology enabled by Channel State Information (CSI) has received great attention in recent years. However, CSI-based sensing systems suffer from performance degradation when deployed in different environments. Existing works solve this problem by domain adaptation using massive unlabeled high-quality data from the new environment, which is usually unavailable in practice. In this paper, we propose a novel augmented environment-invariant robust WiFi gesture recognition system named AirFi that deals with the issue of environment dependency from a new perspective. The AirFi is a novel domain generalization framework that learns the critical part of CSI regardless of different environments and generalizes the model to unseen scenarios, which does not require collecting any data for adaptation to the new environment. AirFi extracts the common features from several training environment settings and minimizes the distribution differences among them. The feature is further augmented to be more robust to environments. Moreover, the system can be further improved by few-shot learning techniques. Compared to state-of-the-art methods, AirFi is able to work in different environment settings without acquiring any CSI data from the new environment. The experimental results demonstrate that our system remains robust in the new environment and outperforms the compared systems.



### Artificial Intelligence-Based Image Reconstruction in Cardiac Magnetic Resonance
- **Arxiv ID**: http://arxiv.org/abs/2209.10298v1
- **DOI**: 10.1007/978-3-030-92087-6_14
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10298v1)
- **Published**: 2022-09-21 12:13:18+00:00
- **Updated**: 2022-09-21 12:13:18+00:00
- **Authors**: Chen Qin, Daniel Rueckert
- **Comment**: Book Chapter in Artificial Intelligence in Cardiothoracic Imaging
- **Journal**: None
- **Summary**: Artificial intelligence (AI) and Machine Learning (ML) have shown great potential in improving the medical imaging workflow, from image acquisition and reconstruction to disease diagnosis and treatment. Particularly, in recent years, there has been a significant growth in the use of AI and ML algorithms, especially Deep Learning (DL) based methods, for medical image reconstruction. DL techniques have shown to be competitive and often superior over conventional reconstruction methods in terms of both reconstruction quality and computational efficiency. The use of DL-based image reconstruction also provides promising opportunities to transform the way cardiac images are acquired and reconstructed. In this chapter, we will review recent advances in DL-based reconstruction techniques for cardiac imaging, with emphasis on cardiac magnetic resonance (CMR) image reconstruction. We mainly focus on supervised DL methods for the application, including image post-processing techniques, model-driven approaches and k-space based methods. Current limitations, challenges and future opportunities of DL for cardiac image reconstruction are also discussed.



### I2DFormer: Learning Image to Document Attention for Zero-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.10304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10304v1)
- **Published**: 2022-09-21 12:18:31+00:00
- **Updated**: 2022-09-21 12:18:31+00:00
- **Authors**: Muhammad Ferjad Naeem, Yongqin Xian, Luc Van Gool, Federico Tombari
- **Comment**: 36th Conference on Neural Information Processing Systems (NeurIPS
  2022)
- **Journal**: None
- **Summary**: Despite the tremendous progress in zero-shot learning(ZSL), the majority of existing methods still rely on human-annotated attributes, which are difficult to annotate and scale. An unsupervised alternative is to represent each class using the word embedding associated with its semantic class name. However, word embeddings extracted from pre-trained language models do not necessarily capture visual similarities, resulting in poor zero-shot performance. In this work, we argue that online textual documents, e.g., Wikipedia, contain rich visual descriptions about object classes, therefore can be used as powerful unsupervised side information for ZSL. To this end, we propose I2DFormer, a novel transformer-based ZSL framework that jointly learns to encode images and documents by aligning both modalities in a shared embedding space. In order to distill discriminative visual words from noisy documents, we introduce a new cross-modal attention module that learns fine-grained interactions between image patches and document words. Consequently, our I2DFormer not only learns highly discriminative document embeddings that capture visual similarities but also gains the ability to localize visually relevant words in image regions. Quantitatively, we demonstrate that our I2DFormer significantly outperforms previous unsupervised semantic embeddings under both zero-shot and generalized zero-shot learning settings on three public datasets. Qualitatively, we show that our method leads to highly interpretable results where document words can be grounded in the image regions.



### KXNet: A Model-Driven Deep Neural Network for Blind Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2209.10305v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10305v2)
- **Published**: 2022-09-21 12:22:50+00:00
- **Updated**: 2022-09-22 04:27:50+00:00
- **Authors**: Jiahong Fu, Hong Wang, Qi Xie, Qian Zhao, Deyu Meng, Zongben Xu
- **Comment**: Accepted by ECCV2022
- **Journal**: None
- **Summary**: Although current deep learning-based methods have gained promising performance in the blind single image super-resolution (SISR) task, most of them mainly focus on heuristically constructing diverse network architectures and put less emphasis on the explicit embedding of the physical generation mechanism between blur kernels and high-resolution (HR) images. To alleviate this issue, we propose a model-driven deep neural network, called KXNet, for blind SISR. Specifically, to solve the classical SISR model, we propose a simple-yet-effective iterative algorithm. Then by unfolding the involved iterative steps into the corresponding network module, we naturally construct the KXNet. The main specificity of the proposed KXNet is that the entire learning process is fully and explicitly integrated with the inherent physical mechanism underlying this SISR task. Thus, the learned blur kernel has clear physical patterns and the mutually iterative process between blur kernel and HR image can soundly guide the KXNet to be evolved in the right direction. Extensive experiments on synthetic and real data finely demonstrate the superior accuracy and generality of our method beyond the current representative state-of-the-art blind SISR methods. Code is available at: https://github.com/jiahong-fu/KXNet.



### An Overview of Violence Detection Techniques: Current Challenges and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/2209.11680v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.11680v1)
- **Published**: 2022-09-21 12:27:20+00:00
- **Updated**: 2022-09-21 12:27:20+00:00
- **Authors**: Nadia Mumtaz, Naveed Ejaz, Shabana Habib, Syed Muhammad Mohsin, Prayag Tiwari, Shahab S. Band, Neeraj Kumar
- **Comment**: Artificial Intelligence Review
- **Journal**: None
- **Summary**: The Big Video Data generated in today's smart cities has raised concerns from its purposeful usage perspective, where surveillance cameras, among many others are the most prominent resources to contribute to the huge volumes of data, making its automated analysis a difficult task in terms of computation and preciseness. Violence Detection (VD), broadly plunging under Action and Activity recognition domain, is used to analyze Big Video data for anomalous actions incurred due to humans. The VD literature is traditionally based on manually engineered features, though advancements to deep learning based standalone models are developed for real-time VD analysis. This paper focuses on overview of deep sequence learning approaches along with localization strategies of the detected violence. This overview also dives into the initial image processing and machine learning-based VD literature and their possible advantages such as efficiency against the current complex models. Furthermore,the datasets are discussed, to provide an analysis of the current models, explaining their pros and cons with future directions in VD domain derived from an in-depth analysis of the previous methods.



### Understanding the Tricks of Deep Learning in Medical Image Segmentation: Challenges and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/2209.10307v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10307v2)
- **Published**: 2022-09-21 12:30:05+00:00
- **Updated**: 2023-05-08 10:23:24+00:00
- **Authors**: Dong Zhang, Yi Lin, Hao Chen, Zhuotao Tian, Xin Yang, Jinhui Tang, Kwang Ting Cheng
- **Comment**: Under submission
- **Journal**: None
- **Summary**: Over the past few years, the rapid development of deep learning technologies for computer vision has significantly improved the performance of medical image segmentation (MedISeg). However, the diverse implementation strategies of various models have led to an extremely complex MedISeg system, resulting in a potential problem of unfair result comparisons. In this paper, we collect a series of MedISeg tricks for different model implementation phases (i.e., pre-training model, data pre-processing, data augmentation, model implementation, model inference, and result post-processing), and experimentally explore the effectiveness of these tricks on consistent baselines. With the extensive experimental results on both the representative 2D and 3D medical image datasets, we explicitly clarify the effect of these tricks. Moreover, based on the surveyed tricks, we also open-sourced a strong MedISeg repository, where each component has the advantage of plug-and-play. We believe that this milestone work not only completes a comprehensive and complementary survey of the state-of-the-art MedISeg approaches, but also offers a practical guide for addressing the future medical image processing challenges including but not limited to small dataset, class imbalance learning, multi-modality learning, and domain adaptation. The code and training weights have been released at: https://github.com/hust-linyi/seg_trick.



### Rethinking the compositionality of point clouds through regularization in the hyperbolic space
- **Arxiv ID**: http://arxiv.org/abs/2209.10318v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10318v1)
- **Published**: 2022-09-21 12:45:24+00:00
- **Updated**: 2022-09-21 12:45:24+00:00
- **Authors**: Antonio Montanaro, Diego Valsesia, Enrico Magli
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: Point clouds of 3D objects exhibit an inherent compositional nature where simple parts can be assembled into progressively more complex shapes to form whole objects. Explicitly capturing such part-whole hierarchy is a long-sought objective in order to build effective models, but its tree-like nature has made the task elusive. In this paper, we propose to embed the features of a point cloud classifier into the hyperbolic space and explicitly regularize the space to account for the part-whole hierarchy. The hyperbolic space is the only space that can successfully embed the tree-like nature of the hierarchy. This leads to substantial improvements in the performance of state-of-art supervised models for point cloud classification.



### Continual VQA for Disaster Response Systems
- **Arxiv ID**: http://arxiv.org/abs/2209.10320v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10320v3)
- **Published**: 2022-09-21 12:45:51+00:00
- **Updated**: 2022-11-10 20:19:03+00:00
- **Authors**: Aditya Kane, V Manushree, Sahil Khose
- **Comment**: Accepted at Tackling Climate Change with Machine Learning workshop at
  NeurIPS 2022
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) is a multi-modal task that involves answering questions from an input image, semantically understanding the contents of the image and answering it in natural language. Using VQA for disaster management is an important line of research due to the scope of problems that are answered by the VQA system. However, the main challenge is the delay caused by the generation of labels in the assessment of the affected areas. To tackle this, we deployed pre-trained CLIP model, which is trained on visual-image pairs. however, we empirically see that the model has poor zero-shot performance. Thus, we instead use pre-trained embeddings of text and image from this model for our supervised training and surpass previous state-of-the-art results on the FloodNet dataset. We expand this to a continual setting, which is a more real-life scenario. We tackle the problem of catastrophic forgetting using various experience replay methods. Our training runs are available at: https://wandb.ai/compyle/continual_vqa_final. Our code is available at https://github.com/AdityaKane2001/continual_vqa.



### Toward 3D Spatial Reasoning for Human-like Text-based Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2209.10326v2
- **DOI**: 10.1109/TIP.2023.3276570
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2209.10326v2)
- **Published**: 2022-09-21 12:49:14+00:00
- **Updated**: 2023-06-15 02:38:25+00:00
- **Authors**: Hao Li, Jinfa Huang, Peng Jin, Guoli Song, Qi Wu, Jie Chen
- **Comment**: Accepted by TIP2023, The Arxiv version of "Weakly-Supervised 3D
  Spatial Reasoning for Text-based Visual Question Answering"
- **Journal**: None
- **Summary**: Text-based Visual Question Answering~(TextVQA) aims to produce correct answers for given questions about the images with multiple scene texts. In most cases, the texts naturally attach to the surface of the objects. Therefore, spatial reasoning between texts and objects is crucial in TextVQA. However, existing approaches are constrained within 2D spatial information learned from the input images and rely on transformer-based architectures to reason implicitly during the fusion process. Under this setting, these 2D spatial reasoning approaches cannot distinguish the fine-grain spatial relations between visual objects and scene texts on the same image plane, thereby impairing the interpretability and performance of TextVQA models. In this paper, we introduce 3D geometric information into a human-like spatial reasoning process to capture the contextual knowledge of key objects step-by-step. %we formulate a human-like spatial reasoning process by introducing 3D geometric information for capturing key objects' contextual knowledge. To enhance the model's understanding of 3D spatial relationships, Specifically, (i)~we propose a relation prediction module for accurately locating the region of interest of critical objects; (ii)~we design a depth-aware attention calibration module for calibrating the OCR tokens' attention according to critical objects. Extensive experiments show that our method achieves state-of-the-art performance on TextVQA and ST-VQA datasets. More encouragingly, our model surpasses others by clear margins of 5.7\% and 12.1\% on questions that involve spatial reasoning in TextVQA and ST-VQA valid split. Besides, we also verify the generalizability of our model on the text-based image captioning task.



### SDA-$x$Net: Selective Depth Attention Networks for Adaptive Multi-scale Feature Representation
- **Arxiv ID**: http://arxiv.org/abs/2209.10327v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10327v1)
- **Published**: 2022-09-21 12:49:55+00:00
- **Updated**: 2022-09-21 12:49:55+00:00
- **Authors**: Qingbei Guo, Xiao-Jun Wu, Zhiquan Feng, Tianyang Xu, Cong Hu
- **Comment**: 13 pages, 16 figures
- **Journal**: None
- **Summary**: Existing multi-scale solutions lead to a risk of just increasing the receptive field sizes while neglecting small receptive fields. Thus, it is a challenging problem to effectively construct adaptive neural networks for recognizing various spatial-scale objects. To tackle this issue, we first introduce a new attention dimension, i.e., depth, in addition to existing attention dimensions such as channel, spatial, and branch, and present a novel selective depth attention network to symmetrically handle multi-scale objects in various vision tasks. Specifically, the blocks within each stage of a given neural network, i.e., ResNet, output hierarchical feature maps sharing the same resolution but with different receptive field sizes. Based on this structural property, we design a stage-wise building module, namely SDA, which includes a trunk branch and a SE-like attention branch. The block outputs of the trunk branch are fused to globally guide their depth attention allocation through the attention branch. According to the proposed attention mechanism, we can dynamically select different depth features, which contributes to adaptively adjusting the receptive field sizes for the variable-sized input objects. In this way, the cross-block information interaction leads to a long-range dependency along the depth direction. Compared with other multi-scale approaches, our SDA method combines multiple receptive fields from previous blocks into the stage output, thus offering a wider and richer range of effective receptive fields. Moreover, our method can be served as a pluggable module to other multi-scale networks as well as attention networks, coined as SDA-$x$Net. Their combination further extends the range of the effective receptive fields towards small receptive fields, enabling interpretable neural networks. Our source code is available at \url{https://github.com/QingbeiGuo/SDA-xNet.git}.



### FNeVR: Neural Volume Rendering for Face Animation
- **Arxiv ID**: http://arxiv.org/abs/2209.10340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10340v1)
- **Published**: 2022-09-21 13:18:59+00:00
- **Updated**: 2022-09-21 13:18:59+00:00
- **Authors**: Bohan Zeng, Boyu Liu, Hong Li, Xuhui Liu, Jianzhuang Liu, Dapeng Chen, Wei Peng, Baochang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Face animation, one of the hottest topics in computer vision, has achieved a promising performance with the help of generative models. However, it remains a critical challenge to generate identity preserving and photo-realistic images due to the sophisticated motion deformation and complex facial detail modeling. To address these problems, we propose a Face Neural Volume Rendering (FNeVR) network to fully explore the potential of 2D motion warping and 3D volume rendering in a unified framework. In FNeVR, we design a 3D Face Volume Rendering (FVR) module to enhance the facial details for image rendering. Specifically, we first extract 3D information with a well-designed architecture, and then introduce an orthogonal adaptive ray-sampling module for efficient rendering. We also design a lightweight pose editor, enabling FNeVR to edit the facial pose in a simple yet effective way. Extensive experiments show that our FNeVR obtains the best overall quality and performance on widely used talking-head benchmarks.



### Momentum Adversarial Distillation: Handling Large Distribution Shifts in Data-Free Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2209.10359v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.10359v1)
- **Published**: 2022-09-21 13:53:56+00:00
- **Updated**: 2022-09-21 13:53:56+00:00
- **Authors**: Kien Do, Hung Le, Dung Nguyen, Dang Nguyen, Haripriya Harikumar, Truyen Tran, Santu Rana, Svetha Venkatesh
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: Data-free Knowledge Distillation (DFKD) has attracted attention recently thanks to its appealing capability of transferring knowledge from a teacher network to a student network without using training data. The main idea is to use a generator to synthesize data for training the student. As the generator gets updated, the distribution of synthetic data will change. Such distribution shift could be large if the generator and the student are trained adversarially, causing the student to forget the knowledge it acquired at previous steps. To alleviate this problem, we propose a simple yet effective method called Momentum Adversarial Distillation (MAD) which maintains an exponential moving average (EMA) copy of the generator and uses synthetic samples from both the generator and the EMA generator to train the student. Since the EMA generator can be considered as an ensemble of the generator's old versions and often undergoes a smaller change in updates compared to the generator, training on its synthetic samples can help the student recall the past knowledge and prevent the student from adapting too quickly to new updates of the generator. Our experiments on six benchmark datasets including big datasets like ImageNet and Places365 demonstrate the superior performance of MAD over competing methods for handling the large distribution shift problem. Our method also compares favorably to existing DFKD methods and even achieves state-of-the-art results in some cases.



### SoLar: Sinkhorn Label Refinery for Imbalanced Partial-Label Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.10365v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10365v1)
- **Published**: 2022-09-21 14:00:16+00:00
- **Updated**: 2022-09-21 14:00:16+00:00
- **Authors**: Haobo Wang, Mingxuan Xia, Yixuan Li, Yuren Mao, Lei Feng, Gang Chen, Junbo Zhao
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: Partial-label learning (PLL) is a peculiar weakly-supervised learning task where the training samples are generally associated with a set of candidate labels instead of single ground truth. While a variety of label disambiguation methods have been proposed in this domain, they normally assume a class-balanced scenario that may not hold in many real-world applications. Empirically, we observe degenerated performance of the prior methods when facing the combinatorial challenge from the long-tailed distribution and partial-labeling. In this work, we first identify the major reasons that the prior work failed. We subsequently propose SoLar, a novel Optimal Transport-based framework that allows to refine the disambiguated labels towards matching the marginal class prior distribution. SoLar additionally incorporates a new and systematic mechanism for estimating the long-tailed class prior distribution under the PLL setup. Through extensive experiments, SoLar exhibits substantially superior results on standardized benchmarks compared to the previous state-of-the-art PLL methods. Code and data are available at: https://github.com/hbzju/SoLar .



### Improving the Safety of 3D Object Detectors in Autonomous Driving using IoGT and Distance Measures
- **Arxiv ID**: http://arxiv.org/abs/2209.10368v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.10368v3)
- **Published**: 2022-09-21 14:03:08+00:00
- **Updated**: 2023-03-05 12:26:16+00:00
- **Authors**: Hsuan-Cheng Liao, Chih-Hong Cheng, Hasan Esen, Alois Knoll
- **Comment**: 8 pages (IEEE double column format), 8 figures, revised with clearer
  presentation and resubmitted to IROS 2023
- **Journal**: None
- **Summary**: State-of-the-art object detectors are commonly evaluated based on accuracy metrics such as mean Average Precision (mAP). In this paper, inspired by the fact that mAP is not a direct safety indicator, we propose a straightforward safety metric, especially for 3D object detectors in Autonomous Driving contexts, by combining the Intersection-over-Ground-Truth (IoGT) measure and a distance ratio. Subsequently, we formulate a safety-aware loss function by amending IoGT to commonly used accuracy-oriented loss functions. Our experiments using models from the MMDetection3D library, the nuScenes dataset, and an in-house simulation dataset demonstrate that the object detector trained with our loss function significantly reduces unsafe predictions while staying performant on accuracy and maintaining good stability in the learning process.



### DARTSRepair: Core-failure-set Guided DARTS for Network Robustness to Common Corruptions
- **Arxiv ID**: http://arxiv.org/abs/2209.10381v1
- **DOI**: 10.1016/j.patcog.2022.108864
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.10381v1)
- **Published**: 2022-09-21 14:18:49+00:00
- **Updated**: 2022-09-21 14:18:49+00:00
- **Authors**: Xuhong Ren, Jianlang Chen, Felix Juefei-Xu, Wanli Xue, Qing Guo, Lei Ma, Jianjun Zhao, Shengyong Chen
- **Comment**: To appear in Pattern Recognition (PR)
- **Journal**: None
- **Summary**: Network architecture search (NAS), in particular the differentiable architecture search (DARTS) method, has shown a great power to learn excellent model architectures on the specific dataset of interest. In contrast to using a fixed dataset, in this work, we focus on a different but important scenario for NAS: how to refine a deployed network's model architecture to enhance its robustness with the guidance of a few collected and misclassified examples that are degraded by some real-world unknown corruptions having a specific pattern (e.g., noise, blur, etc.). To this end, we first conduct an empirical study to validate that the model architectures can be definitely related to the corruption patterns. Surprisingly, by just adding a few corrupted and misclassified examples (e.g., $10^3$ examples) to the clean training dataset (e.g., $5.0 \times 10^4$ examples), we can refine the model architecture and enhance the robustness significantly. To make it more practical, the key problem, i.e., how to select the proper failure examples for the effective NAS guidance, should be carefully investigated. Then, we propose a novel core-failure-set guided DARTS that embeds a K-center-greedy algorithm for DARTS to select suitable corrupted failure examples to refine the model architecture. We use our method for DARTS-refined DNNs on the clean as well as 15 corruptions with the guidance of four specific real-world corruptions. Compared with the state-of-the-art NAS as well as data-augmentation-based enhancement methods, our final method can achieve higher accuracy on both corrupted datasets and the original clean dataset. On some of the corruption patterns, we can achieve as high as over 45% absolute accuracy improvements.



### Long-Lived Accurate Keypoints in Event Streams
- **Arxiv ID**: http://arxiv.org/abs/2209.10385v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10385v2)
- **Published**: 2022-09-21 14:25:31+00:00
- **Updated**: 2022-10-07 11:18:30+00:00
- **Authors**: Philippe Chiberre, Etienne Perot, Amos Sironi, Vincent Lepetit
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel end-to-end approach to keypoint detection and tracking in an event stream that provides better precision and much longer keypoint tracks than previous methods. This is made possible by two contributions working together.   First, we propose a simple procedure to generate stable keypoint labels, which we use to train a recurrent architecture. This training data results in detections that are very consistent over time.   Moreover, we observe that previous methods for keypoint detection work on a representation (such as the time surface) that integrates events over a period of time. Since this integration is required, we claim it is better to predict the keypoints' trajectories for the time period rather than single locations, as done in previous approaches. We predict these trajectories in the form of a series of heatmaps for the integration time period. This improves the keypoint localization.   Our architecture can also be kept very simple, which results in very fast inference times. We demonstrate our approach on the HVGA ATIS Corner dataset as well as "The Event-Camera Dataset and Simulator" dataset, and show it results in keypoint tracks that are three times longer and nearly twice as accurate as the best previous state-of-the-art methods. We believe our approach can be generalized to other event-based camera problems, and we release our source code to encourage other authors to explore it.



### 3DGTN: 3D Dual-Attention GLocal Transformer Network for Point Cloud Classification and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.11255v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11255v2)
- **Published**: 2022-09-21 14:34:21+00:00
- **Updated**: 2023-05-31 02:20:58+00:00
- **Authors**: Dening Lu, Kyle Gao, Qian Xie, Linlin Xu, Jonathan Li
- **Comment**: 10 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: Although the application of Transformers in 3D point cloud processing has achieved significant progress and success, it is still challenging for existing 3D Transformer methods to efficiently and accurately learn both valuable global features and valuable local features for improved applications. This paper presents a novel point cloud representational learning network, called 3D Dual Self-attention Global Local (GLocal) Transformer Network (3DGTN), for improved feature learning in both classification and segmentation tasks, with the following key contributions. First, a GLocal Feature Learning (GFL) block with the dual self-attention mechanism (i.e., a novel Point-Patch Self-Attention, called PPSA, and a channel-wise self-attention) is designed to efficiently learn the GLocal context information. Second, the GFL block is integrated with a multi-scale Graph Convolution-based Local Feature Aggregation (LFA) block, leading to a Global-Local (GLocal) information extraction module that can efficiently capture critical information. Third, a series of GLocal modules are used to construct a new hierarchical encoder-decoder structure to enable the learning of "GLocal" information in different scales in a hierarchical manner. The proposed framework is evaluated on both classification and segmentation datasets, demonstrating that the proposed method is capable of outperforming many state-of-the-art methods on both classification and segmentation tasks.



### IoU-Enhanced Attention for End-to-End Task Specific Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.10391v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10391v2)
- **Published**: 2022-09-21 14:36:18+00:00
- **Updated**: 2022-10-05 09:11:54+00:00
- **Authors**: Jing Zhao, Shengjian Wu, Li Sun, Qingli Li
- **Comment**: ACCV2022
- **Journal**: None
- **Summary**: Without densely tiled anchor boxes or grid points in the image, sparse R-CNN achieves promising results through a set of object queries and proposal boxes updated in the cascaded training manner. However, due to the sparse nature and the one-to-one relation between the query and its attending region, it heavily depends on the self attention, which is usually inaccurate in the early training stage. Moreover, in a scene of dense objects, the object query interacts with many irrelevant ones, reducing its uniqueness and harming the performance. This paper proposes to use IoU between different boxes as a prior for the value routing in self attention. The original attention matrix multiplies the same size matrix computed from the IoU of proposal boxes, and they determine the routing scheme so that the irrelevant features can be suppressed. Furthermore, to accurately extract features for both classification and regression, we add two lightweight projection heads to provide the dynamic channel masks based on object query, and they multiply with the output from dynamic convs, making the results suitable for the two different tasks. We validate the proposed scheme on different datasets, including MS-COCO and CrowdHuman, showing that it significantly improves the performance and increases the model convergence speed.



### Sar Ship Detection based on Swin Transformer and Feature Enhancement Feature Pyramid Network
- **Arxiv ID**: http://arxiv.org/abs/2209.10421v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.10421v1)
- **Published**: 2022-09-21 15:12:50+00:00
- **Updated**: 2022-09-21 15:12:50+00:00
- **Authors**: Xiao Ke, Xiaoling Zhang, Tianwen Zhang, Jun Shi, Shunjun Wei
- **Comment**: None
- **Journal**: None
- **Summary**: With the booming of Convolutional Neural Networks (CNNs), CNNs such as VGG-16 and ResNet-50 widely serve as backbone in SAR ship detection. However, CNN based backbone is hard to model long-range dependencies, and causes the lack of enough high-quality semantic information in feature maps of shallow layers, which leads to poor detection performance in complicated background and small-sized ships cases. To address these problems, we propose a SAR ship detection method based on Swin Transformer and Feature Enhancement Feature Pyramid Network (FEFPN). Swin Transformer serves as backbone to model long-range dependencies and generates hierarchical features maps. FEFPN is proposed to further improve the quality of feature maps by gradually enhancing the semantic information of feature maps at all levels, especially feature maps in shallow layers. Experiments conducted on SAR ship detection dataset (SSDD) reveal the advantage of our proposed methods.



### Consecutive Knowledge Meta-Adaptation Learning for Unsupervised Medical Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2209.10425v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10425v1)
- **Published**: 2022-09-21 15:19:51+00:00
- **Updated**: 2022-09-21 15:19:51+00:00
- **Authors**: Yumin Zhang, Yawen Hou, Xiuyi Chen, Hongyuan Yu, Long Xia
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based Computer-Aided Diagnosis (CAD) has attracted appealing attention in academic researches and clinical applications. Nevertheless, the Convolutional Neural Networks (CNNs) diagnosis system heavily relies on the well-labeled lesion dataset, and the sensitivity to the variation of data distribution also restricts the potential application of CNNs in CAD. Unsupervised Domain Adaptation (UDA) methods are developed to solve the expensive annotation and domain gaps problem and have achieved remarkable success in medical image analysis. Yet existing UDA approaches only adapt knowledge learned from the source lesion domain to a single target lesion domain, which is against the clinical scenario: the new unlabeled target domains to be diagnosed always arrive in an online and continual manner. Moreover, the performance of existing approaches degrades dramatically on previously learned target lesion domains, due to the newly learned knowledge overwriting the previously learned knowledge (i.e., catastrophic forgetting). To deal with the above issues, we develop a meta-adaptation framework named Consecutive Lesion Knowledge Meta-Adaptation (CLKM), which mainly consists of Semantic Adaptation Phase (SAP) and Representation Adaptation Phase (RAP) to learn the diagnosis model in an online and continual manner. In the SAP, the semantic knowledge learned from the source lesion domain is transferred to consecutive target lesion domains. In the RAP, the feature-extractor is optimized to align the transferable representation knowledge across the source and multiple target lesion domains.



### A Few Shot Multi-Representation Approach for N-gram Spotting in Historical Manuscripts
- **Arxiv ID**: http://arxiv.org/abs/2209.10441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10441v1)
- **Published**: 2022-09-21 15:35:02+00:00
- **Updated**: 2022-09-21 15:35:02+00:00
- **Authors**: Giuseppe De Gregorio, Sanket Biswas, Mohamed Ali Souibgui, Asma Bensalah, Josep Lladós, Alicia Fornés, Angelo Marcelli
- **Comment**: Accepted in ICFHR 2022
- **Journal**: None
- **Summary**: Despite recent advances in automatic text recognition, the performance remains moderate when it comes to historical manuscripts. This is mainly because of the scarcity of available labelled data to train the data-hungry Handwritten Text Recognition (HTR) models. The Keyword Spotting System (KWS) provides a valid alternative to HTR due to the reduction in error rate, but it is usually limited to a closed reference vocabulary. In this paper, we propose a few-shot learning paradigm for spotting sequences of a few characters (N-gram) that requires a small amount of labelled training data. We exhibit that recognition of important n-grams could reduce the system's dependency on vocabulary. In this case, an out-of-vocabulary (OOV) word in an input handwritten line image could be a sequence of n-grams that belong to the lexicon. An extensive experimental evaluation of our proposed multi-representation approach was carried out on a subset of Bentham's historical manuscript collections to obtain some really promising results in this direction.



### Uncertainty-aware Label Distribution Learning for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.10448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10448v1)
- **Published**: 2022-09-21 15:48:41+00:00
- **Updated**: 2022-09-21 15:48:41+00:00
- **Authors**: Nhat Le, Khanh Nguyen, Quang Tran, Erman Tjiputra, Bac Le, Anh Nguyen
- **Comment**: Accepted to WACV 2023. The first two authors contributed equally to
  this work
- **Journal**: None
- **Summary**: Despite significant progress over the past few years, ambiguity is still a key challenge in Facial Expression Recognition (FER). It can lead to noisy and inconsistent annotation, which hinders the performance of deep learning models in real-world scenarios. In this paper, we propose a new uncertainty-aware label distribution learning method to improve the robustness of deep models against uncertainty and ambiguity. We leverage neighborhood information in the valence-arousal space to adaptively construct emotion distributions for training samples. We also consider the uncertainty of provided labels when incorporating them into the label distributions. Our method can be easily integrated into a deep network to obtain more training supervision and improve recognition accuracy. Intensive experiments on several datasets under various noisy and ambiguous settings show that our method achieves competitive results and outperforms recent state-of-the-art approaches. Our code and models are available at https://github.com/minhnhatvt/label-distribution-learning-fer-tf.



### Learning from Mixed Datasets: A Monotonic Image Quality Assessment Model
- **Arxiv ID**: http://arxiv.org/abs/2209.10451v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10451v3)
- **Published**: 2022-09-21 15:53:59+00:00
- **Updated**: 2022-11-13 03:44:39+00:00
- **Authors**: Zhaopeng Feng, Keyang Zhang, Shuyue Jia, Baoliang Chen, Shiqi Wang
- **Comment**: 3 pages, 3 figures
- **Journal**: None
- **Summary**: Deep learning based image quality assessment (IQA) models usually learn to predict image quality from a single dataset, leading the model to overfit specific scenes. To account for this, mixed datasets training can be an effective way to enhance the generalization capability of the model. However, it is nontrivial to combine different IQA datasets, as their quality evaluation criteria, score ranges, view conditions, as well as subjects are usually not shared during the image quality annotation. In this paper, instead of aligning the annotations, we propose a monotonic neural network for IQA model learning with different datasets combined. In particular, our model consists of a dataset-shared quality regressor and several dataset-specific quality transformers. The quality regressor aims to obtain the perceptual qualities of each dataset while each quality transformer maps the perceptual qualities to the corresponding dataset annotations with their monotonicity maintained. The experimental results verify the effectiveness of the proposed learning strategy and our code is available at https://github.com/fzp0424/MonotonicIQA.



### Sample, Crop, Track: Self-Supervised Mobile 3D Object Detection for Urban Driving LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2209.10471v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2209.10471v1)
- **Published**: 2022-09-21 16:12:46+00:00
- **Updated**: 2022-09-21 16:12:46+00:00
- **Authors**: Sangyun Shin, Stuart Golodetz, Madhu Vankadari, Kaichen Zhou, Andrew Markham, Niki Trigoni
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has led to great progress in the detection of mobile (i.e. movement-capable) objects in urban driving scenes in recent years. Supervised approaches typically require the annotation of large training sets; there has thus been great interest in leveraging weakly, semi- or self-supervised methods to avoid this, with much success. Whilst weakly and semi-supervised methods require some annotation, self-supervised methods have used cues such as motion to relieve the need for annotation altogether. However, a complete absence of annotation typically degrades their performance, and ambiguities that arise during motion grouping can inhibit their ability to find accurate object boundaries. In this paper, we propose a new self-supervised mobile object detection approach called SCT. This uses both motion cues and expected object sizes to improve detection performance, and predicts a dense grid of 3D oriented bounding boxes to improve object discovery. We significantly outperform the state-of-the-art self-supervised mobile object detection method TCR on the KITTI tracking benchmark, and achieve performance that is within 30% of the fully supervised PV-RCNN++ method for IoUs <= 0.5.



### Show, Interpret and Tell: Entity-aware Contextualised Image Captioning in Wikipedia
- **Arxiv ID**: http://arxiv.org/abs/2209.10474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10474v1)
- **Published**: 2022-09-21 16:14:15+00:00
- **Updated**: 2022-09-21 16:14:15+00:00
- **Authors**: Khanh Nguyen, Ali Furkan Biten, Andres Mafla, Lluis Gomez, Dimosthenis Karatzas
- **Comment**: None
- **Journal**: None
- **Summary**: Humans exploit prior knowledge to describe images, and are able to adapt their explanation to specific contextual information, even to the extent of inventing plausible explanations when contextual information and images do not match. In this work, we propose the novel task of captioning Wikipedia images by integrating contextual knowledge. Specifically, we produce models that jointly reason over Wikipedia articles, Wikimedia images and their associated descriptions to produce contextualized captions. Particularly, a similar Wikimedia image can be used to illustrate different articles, and the produced caption needs to be adapted to a specific context, therefore allowing us to explore the limits of a model to adjust captions to different contextual information. A particular challenging task in this domain is dealing with out-of-dictionary words and Named Entities. To address this, we propose a pre-training objective, Masked Named Entity Modeling (MNEM), and show that this pretext task yields an improvement compared to baseline models. Furthermore, we verify that a model pre-trained with the MNEM objective in Wikipedia generalizes well to a News Captioning dataset. Additionally, we define two different test splits according to the difficulty of the captioning task. We offer insights on the role and the importance of each modality and highlight the limitations of our model. The code, models and data splits are publicly available at Upon acceptance.



### Multi-view Local Co-occurrence and Global Consistency Learning Improve Mammogram Classification Generalisation
- **Arxiv ID**: http://arxiv.org/abs/2209.10478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10478v1)
- **Published**: 2022-09-21 16:29:01+00:00
- **Updated**: 2022-09-21 16:29:01+00:00
- **Authors**: Yuanhong Chen, Hu Wang, Chong Wang, Yu Tian, Fengbei Liu, Michael Elliott, Davis J. McCarthy, Helen Frazer, Gustavo Carneiro
- **Comment**: MICCAI 2022
- **Journal**: None
- **Summary**: When analysing screening mammograms, radiologists can naturally process information across two ipsilateral views of each breast, namely the cranio-caudal (CC) and mediolateral-oblique (MLO) views. These multiple related images provide complementary diagnostic information and can improve the radiologist's classification accuracy. Unfortunately, most existing deep learning systems, trained with globally-labelled images, lack the ability to jointly analyse and integrate global and local information from these multiple views. By ignoring the potentially valuable information present in multiple images of a screening episode, one limits the potential accuracy of these systems. Here, we propose a new multi-view global-local analysis method that mimics the radiologist's reading procedure, based on a global consistency learning and local co-occurrence learning of ipsilateral views in mammograms. Extensive experiments show that our model outperforms competing methods, in terms of classification accuracy and generalisation, on a large-scale private dataset and two publicly available datasets, where models are exclusively trained and tested with global labels.



### Recurrent Super-Resolution Method for Enhancing Low Quality Thermal Facial Data
- **Arxiv ID**: http://arxiv.org/abs/2209.10489v1
- **DOI**: 10.56541/UAOV9084
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10489v1)
- **Published**: 2022-09-21 16:44:06+00:00
- **Updated**: 2022-09-21 16:44:06+00:00
- **Authors**: David O'Callaghan, Cian Ryan, Waseem Shariff, Muhammad Ali Farooq, Joseph Lemley, Peter Corcoran
- **Comment**: In proceedings of the 24th Irish Machine Vision and Image Processing
  Conference, Belfast Ireland, 31 August - 2nd September 2022
- **Journal**: None
- **Summary**: The process of obtaining high-resolution images from single or multiple low-resolution images of the same scene is of great interest for real-world image and signal processing applications. This study is about exploring the potential usage of deep learning based image super-resolution algorithms on thermal data for producing high quality thermal imaging results for in-cabin vehicular driver monitoring systems. In this work we have proposed and developed a novel multi-image super-resolution recurrent neural network to enhance the resolution and improve the quality of low-resolution thermal imaging data captured from uncooled thermal cameras. The end-to-end fully convolutional neural network is trained from scratch on newly acquired thermal data of 30 different subjects in indoor environmental conditions. The effectiveness of the thermally tuned super-resolution network is validated quantitatively as well as qualitatively on test data of 6 distinct subjects. The network was able to achieve a mean peak signal to noise ratio of 39.24 on the validation dataset for 4x super-resolution, outperforming bicubic interpolation both quantitatively and qualitatively.



### Animating Still Images
- **Arxiv ID**: http://arxiv.org/abs/2209.10497v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10497v2)
- **Published**: 2022-09-21 16:53:04+00:00
- **Updated**: 2023-01-26 07:26:02+00:00
- **Authors**: Kushagr Batra, Mridul Kavidayal
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for imparting motion to a still 2D image. Our method uses deep learning to segment a section of the image denoted as subject, then uses in-painting to complete the background, and finally adds animation to the subject by embedding the image in a triangle mesh, while preserving the rest of the image.



### Dynamic camera alignment optimization problem based on Fractal Decomposition based Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2209.11695v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11695v1)
- **Published**: 2022-09-21 16:58:47+00:00
- **Updated**: 2022-09-21 16:58:47+00:00
- **Authors**: Arcadi Llanza, Nadiya Shvai, Amir Nakib
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we tackle the Dynamic Optimization Problem (DOP) of IA in a real-world application using a Dynamic Optimization Algorithm (DOA) called Fractal Decomposition Algorithm (FDA), introduced by recently. We used FDA to perform IA on CCTV camera feed from a tunnel. As the camera viewpoint can change by multiple reasons such as wind, maintenance, etc. the alignment is required to guarantee the correct functioning of video-based traffic security system.



### Gemino: Practical and Robust Neural Compression for Video Conferencing
- **Arxiv ID**: http://arxiv.org/abs/2209.10507v3
- **DOI**: None
- **Categories**: **cs.NI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10507v3)
- **Published**: 2022-09-21 17:10:46+00:00
- **Updated**: 2023-05-11 14:24:47+00:00
- **Authors**: Vibhaalakshmi Sivaraman, Pantea Karimi, Vedantha Venkatapathy, Mehrdad Khani, Sadjad Fouladi, Mohammad Alizadeh, Frédo Durand, Vivienne Sze
- **Comment**: 13 pages, 5 appendix
- **Journal**: None
- **Summary**: Video conferencing systems suffer from poor user experience when network conditions deteriorate because current video codecs simply cannot operate at extremely low bitrates. Recently, several neural alternatives have been proposed that reconstruct talking head videos at very low bitrates using sparse representations of each frame such as facial landmark information. However, these approaches produce poor reconstructions in scenarios with major movement or occlusions over the course of a call, and do not scale to higher resolutions. We design Gemino, a new neural compression system for video conferencing based on a novel high-frequency-conditional super-resolution pipeline. Gemino upsamples a very low-resolution version of each target frame while enhancing high-frequency details (e.g., skin texture, hair, etc.) based on information extracted from a single high-resolution reference image. We use a multi-scale architecture that runs different components of the model at different resolutions, allowing it to scale to resolutions comparable to 720p, and we personalize the model to learn specific details of each person, achieving much better fidelity at low bitrates. We implement Gemino atop aiortc, an open-source Python implementation of WebRTC, and show that it operates on 1024x1024 videos in real-time on a Titan X GPU, and achieves 2.2-5x lower bitrate than traditional video codecs for the same perceptual quality.



### Learning to Relight Portrait Images via a Virtual Light Stage and Synthetic-to-Real Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2209.10510v3
- **DOI**: 10.1145/3550454.3555442
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.10510v3)
- **Published**: 2022-09-21 17:15:58+00:00
- **Updated**: 2023-08-11 03:07:28+00:00
- **Authors**: Yu-Ying Yeh, Koki Nagano, Sameh Khamis, Jan Kautz, Ming-Yu Liu, Ting-Chun Wang
- **Comment**: To appear in ACM Transactions on Graphics (SIGGRAPH Asia 2022). 21
  pages, 25 figures, 7 tables. Project page:
  https://research.nvidia.com/labs/dir/lumos/
- **Journal**: ACM Trans. Graph. 41, 6, Article 231 (December 2022), 21 pages
- **Summary**: Given a portrait image of a person and an environment map of the target lighting, portrait relighting aims to re-illuminate the person in the image as if the person appeared in an environment with the target lighting. To achieve high-quality results, recent methods rely on deep learning. An effective approach is to supervise the training of deep neural networks with a high-fidelity dataset of desired input-output pairs, captured with a light stage. However, acquiring such data requires an expensive special capture rig and time-consuming efforts, limiting access to only a few resourceful laboratories. To address the limitation, we propose a new approach that can perform on par with the state-of-the-art (SOTA) relighting methods without requiring a light stage. Our approach is based on the realization that a successful relighting of a portrait image depends on two conditions. First, the method needs to mimic the behaviors of physically-based relighting. Second, the output has to be photorealistic. To meet the first condition, we propose to train the relighting network with training data generated by a virtual light stage that performs physically-based rendering on various 3D synthetic humans under different environment maps. To meet the second condition, we develop a novel synthetic-to-real approach to bring photorealism to the relighting network output. In addition to achieving SOTA results, our approach offers several advantages over the prior methods, including controllable glares on glasses and more temporally-consistent results for relighting videos.



### Benchmarking and Analyzing 3D Human Pose and Shape Estimation Beyond Algorithms
- **Arxiv ID**: http://arxiv.org/abs/2209.10529v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10529v1)
- **Published**: 2022-09-21 17:39:53+00:00
- **Updated**: 2022-09-21 17:39:53+00:00
- **Authors**: Hui En Pang, Zhongang Cai, Lei Yang, Tianwei Zhang, Ziwei Liu
- **Comment**: Submission to 36th Conference on Neural Information Processing
  Systems (NeurIPS 2022) Track on Datasets and Benchmarks
- **Journal**: None
- **Summary**: 3D human pose and shape estimation (a.k.a. "human mesh recovery") has achieved substantial progress. Researchers mainly focus on the development of novel algorithms, while less attention has been paid to other critical factors involved. This could lead to less optimal baselines, hindering the fair and faithful evaluations of newly designed methodologies. To address this problem, this work presents the first comprehensive benchmarking study from three under-explored perspectives beyond algorithms. 1) Datasets. An analysis on 31 datasets reveals the distinct impacts of data samples: datasets featuring critical attributes (i.e. diverse poses, shapes, camera characteristics, backbone features) are more effective. Strategical selection and combination of high-quality datasets can yield a significant boost to the model performance. 2) Backbones. Experiments with 10 backbones, ranging from CNNs to transformers, show the knowledge learnt from a proximity task is readily transferable to human mesh recovery. 3) Training strategies. Proper augmentation techniques and loss designs are crucial. With the above findings, we achieve a PA-MPJPE of 47.3 mm on the 3DPW test set with a relatively simple model. More importantly, we provide strong baselines for fair comparisons of algorithms, and recommendations for building effective training configurations in the future. Codebase is available at http://github.com/smplbody/hmr-benchmarks



### FedFOR: Stateless Heterogeneous Federated Learning with First-Order Regularization
- **Arxiv ID**: http://arxiv.org/abs/2209.10537v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10537v1)
- **Published**: 2022-09-21 17:57:20+00:00
- **Updated**: 2022-09-21 17:57:20+00:00
- **Authors**: Junjiao Tian, James Seale Smith, Zsolt Kira
- **Comment**: None
- **Journal**: None
- **Summary**: Federated Learning (FL) seeks to distribute model training across local clients without collecting data in a centralized data-center, hence removing data-privacy concerns. A major challenge for FL is data heterogeneity (where each client's data distribution can differ) as it can lead to weight divergence among local clients and slow global convergence. The current SOTA FL methods designed for data heterogeneity typically impose regularization to limit the impact of non-IID data and are stateful algorithms, i.e., they maintain local statistics over time. While effective, these approaches can only be used for a special case of FL involving only a small number of reliable clients. For the more typical applications of FL where the number of clients is large (e.g., edge-device and mobile applications), these methods cannot be applied, motivating the need for a stateless approach to heterogeneous FL which can be used for any number of clients. We derive a first-order gradient regularization to penalize inconsistent local updates due to local data heterogeneity. Specifically, to mitigate weight divergence, we introduce a first-order approximation of the global data distribution into local objectives, which intuitively penalizes updates in the opposite direction of the global update. The end result is a stateless FL algorithm that achieves 1) significantly faster convergence (i.e., fewer communication rounds) and 2) higher overall converged performance than SOTA methods under non-IID data distribution. Importantly, our approach does not impose unrealistic limits on the client size, enabling learning from a large number of clients as is typical in most FL applications.



### GNPM: Geometric-Aware Neural Parametric Models
- **Arxiv ID**: http://arxiv.org/abs/2209.10621v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2209.10621v1)
- **Published**: 2022-09-21 19:23:31+00:00
- **Updated**: 2022-09-21 19:23:31+00:00
- **Authors**: Mirgahney Mohamed, Lourdes Agapito
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: We propose Geometric Neural Parametric Models (GNPM), a learned parametric model that takes into account the local structure of data to learn disentangled shape and pose latent spaces of 4D dynamics, using a geometric-aware architecture on point clouds. Temporally consistent 3D deformations are estimated without the need for dense correspondences at training time, by exploiting cycle consistency. Besides its ability to learn dense correspondences, GNPMs also enable latent-space manipulations such as interpolation and shape/pose transfer. We evaluate GNPMs on various datasets of clothed humans, and show that it achieves comparable performance to state-of-the-art methods that require dense correspondences during training.



### An Image Processing approach to identify solar plages observed at 393.37 nm by the Kodaikanal Solar Observatory
- **Arxiv ID**: http://arxiv.org/abs/2209.10631v4
- **DOI**: 10.1093/rasti/rzad027
- **Categories**: **astro-ph.SR**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10631v4)
- **Published**: 2022-09-21 19:55:10+00:00
- **Updated**: 2023-06-30 14:28:16+00:00
- **Authors**: Sarvesh Gharat, Bhaskar Bose, Abhimanyu Borthakur, Rakesh Mazumder
- **Comment**: Accepted: RAS Techniques and Instruments
- **Journal**: None
- **Summary**: Solar plages, which are bright regions on the Sun's surface, are an important indicator of solar activity. In this study, we propose an automated algorithm for identifying solar plages in Ca K wavelength solar data obtained from the Kodaikanal Solar Observatory. The algorithm successfully annotates all visually identifiable plages in an image and outputs the corresponding calculated plage index. We perform a time series analysis of the plage index (rolling mean) across multiple solar cycles to test the algorithm's reliability and robustness. The results show a strong correlation between the calculated plage index and those reported in a previous study. The correlation coefficients obtained for all the solar cycles are higher than 0.90, indicating the reliability of the model. We also suggest that adjusting the hyperparameters appropriately for a specific image using our web-based app can increase the model's efficiency. The algorithm has been deployed on the Streamlit Community Cloud platform, where users can upload images and customize the hyperparameters for desired results. The input data used in this study is freely available from the KSO data archive, and the code and the generated data are publicly available on our GitHub repository. Our proposed algorithm provides an efficient and reliable method for identifying solar plages, which can aid the study of solar activity and its impact on the Earth's climate, technology, and space weather.



### Rate-Distortion in Image Coding for Machines
- **Arxiv ID**: http://arxiv.org/abs/2209.11694v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11694v1)
- **Published**: 2022-09-21 20:24:14+00:00
- **Updated**: 2022-09-21 20:24:14+00:00
- **Authors**: Alon Harell, Anderson De Andrade, Ivan V. Bajic
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, there has been a sharp increase in transmission of images to remote servers specifically for the purpose of computer vision. In many applications, such as surveillance, images are mostly transmitted for automated analysis, and rarely seen by humans. Using traditional compression for this scenario has been shown to be inefficient in terms of bit-rate, likely due to the focus on human based distortion metrics. Thus, it is important to create specific image coding methods for joint use by humans and machines. One way to create the machine side of such a codec is to perform feature matching of some intermediate layer in a Deep Neural Network performing the machine task. In this work, we explore the effects of the layer choice used in training a learnable codec for humans and machines. We prove, using the data processing inequality, that matching features from deeper layers is preferable in the sense of rate-distortion. Next, we confirm our findings empirically by re-training an existing model for scalable human-machine coding. In our experiments we show the trade-off between the human and machine sides of such a scalable model, and discuss the benefit of using deeper layers for training in that regard.



### Automated segmentation of intracranial hemorrhages from 3D CT
- **Arxiv ID**: http://arxiv.org/abs/2209.10648v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10648v1)
- **Published**: 2022-09-21 20:37:32+00:00
- **Updated**: 2022-09-21 20:37:32+00:00
- **Authors**: Md Mahfuzur Rahman Siddiquee, Dong Yang, Yufan He, Daguang Xu, Andriy Myronenko
- **Comment**: INSTANCE22 challenge report, MICCAI2022. arXiv admin note:
  substantial text overlap with arXiv:2209.09546
- **Journal**: None
- **Summary**: Intracranial hemorrhage segmentation challenge (INSTANCE 2022) offers a platform for researchers to compare their solutions to segmentation of hemorrhage stroke regions from 3D CTs. In this work, we describe our solution to INSTANCE 2022. We use a 2D segmentation network, SegResNet from MONAI, operating slice-wise without resampling. The final submission is an ensemble of 18 models. Our solution (team name NVAUTO) achieves the top place in terms of Dice metric (0.721), and overall rank 2. It is implemented with Auto3DSeg.



### Convolutional Bayesian Kernel Inference for 3D Semantic Mapping
- **Arxiv ID**: http://arxiv.org/abs/2209.10663v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10663v2)
- **Published**: 2022-09-21 21:15:12+00:00
- **Updated**: 2023-05-31 19:41:32+00:00
- **Authors**: Joey Wilson, Yuewei Fu, Arthur Zhang, Jingyu Song, Andrew Capodieci, Paramsothy Jayakumar, Kira Barton, Maani Ghaffari
- **Comment**: None
- **Journal**: None
- **Summary**: Robotic perception is currently at a cross-roads between modern methods, which operate in an efficient latent space, and classical methods, which are mathematically founded and provide interpretable, trustworthy results. In this paper, we introduce a Convolutional Bayesian Kernel Inference (ConvBKI) layer which learns to perform explicit Bayesian inference within a depthwise separable convolution layer to maximize efficency while maintaining reliability simultaneously. We apply our layer to the task of real-time 3D semantic mapping, where we learn semantic-geometric probability distributions for LiDAR sensor information and incorporate semantic predictions into a global map. We evaluate our network against state-of-the-art semantic mapping algorithms on the KITTI data set, demonstrating improved latency with comparable semantic label inference results.



### Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking Neural Networks with Learnable Neuronal Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2209.11741v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.11741v2)
- **Published**: 2022-09-21 21:17:56+00:00
- **Updated**: 2023-03-14 16:57:14+00:00
- **Authors**: Adarsh Kumar Kosta, Kaushik Roy
- **Comment**: None
- **Journal**: None
- **Summary**: Event-based cameras have recently shown great potential for high-speed motion estimation owing to their ability to capture temporally rich information asynchronously. Spiking Neural Networks (SNNs), with their neuro-inspired event-driven processing can efficiently handle such asynchronous data, while neuron models such as the leaky-integrate and fire (LIF) can keep track of the quintessential timing information contained in the inputs. SNNs achieve this by maintaining a dynamic state in the neuron memory, retaining important information while forgetting redundant data over time. Thus, we posit that SNNs would allow for better performance on sequential regression tasks compared to similarly sized Analog Neural Networks (ANNs). However, deep SNNs are difficult to train due to vanishing spikes at later layers. To that effect, we propose an adaptive fully-spiking framework with learnable neuronal dynamics to alleviate the spike vanishing problem. We utilize surrogate gradient-based backpropagation through time (BPTT) to train our deep SNNs from scratch. We validate our approach for the task of optical flow estimation on the Multi-Vehicle Stereo Event-Camera (MVSEC) dataset and the DSEC-Flow dataset. Our experiments on these datasets show an average reduction of 13% in average endpoint error (AEE) compared to state-of-the-art ANNs. We also explore several down-scaled models and observe that our SNN models consistently outperform similarly sized ANNs offering 10%-16% lower AEE. These results demonstrate the importance of SNNs for smaller models and their suitability at the edge. In terms of efficiency, our SNNs offer substantial savings in network parameters (48.3x) and computational energy (10.2x) while attaining ~10% lower EPE compared to the state-of-the-art ANN implementations.



### NashAE: Disentangling Representations through Adversarial Covariance Minimization
- **Arxiv ID**: http://arxiv.org/abs/2209.10677v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10677v1)
- **Published**: 2022-09-21 22:02:26+00:00
- **Updated**: 2022-09-21 22:02:26+00:00
- **Authors**: Eric Yeats, Frank Liu, David Womble, Hai Li
- **Comment**: Published as a conference paper in the European Conference on
  Computer Vision (ECCV) 2022
- **Journal**: None
- **Summary**: We present a self-supervised method to disentangle factors of variation in high-dimensional data that does not rely on prior knowledge of the underlying variation profile (e.g., no assumptions on the number or distribution of the individual latent variables to be extracted). In this method which we call NashAE, high-dimensional feature disentanglement is accomplished in the low-dimensional latent space of a standard autoencoder (AE) by promoting the discrepancy between each encoding element and information of the element recovered from all other encoding elements. Disentanglement is promoted efficiently by framing this as a minmax game between the AE and an ensemble of regression networks which each provide an estimate of an element conditioned on an observation of all other elements. We quantitatively compare our approach with leading disentanglement methods using existing disentanglement metrics. Furthermore, we show that NashAE has increased reliability and increased capacity to capture salient data characteristics in the learned latent representation.



### Attention Beats Concatenation for Conditioning Neural Fields
- **Arxiv ID**: http://arxiv.org/abs/2209.10684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10684v1)
- **Published**: 2022-09-21 22:17:55+00:00
- **Updated**: 2022-09-21 22:17:55+00:00
- **Authors**: Daniel Rebain, Mark J. Matthews, Kwang Moo Yi, Gopal Sharma, Dmitry Lagun, Andrea Tagliasacchi
- **Comment**: None
- **Journal**: None
- **Summary**: Neural fields model signals by mapping coordinate inputs to sampled values. They are becoming an increasingly important backbone architecture across many fields from vision and graphics to biology and astronomy. In this paper, we explore the differences between common conditioning mechanisms within these networks, an essential ingredient in shifting neural fields from memorization of signals to generalization, where the set of signals lying on a manifold is modelled jointly. In particular, we are interested in the scaling behaviour of these mechanisms to increasingly high-dimensional conditioning variables. As we show in our experiments, high-dimensional conditioning is key to modelling complex data distributions, thus it is important to determine what architecture choices best enable this when working on such problems. To this end, we run experiments modelling 2D, 3D, and 4D signals with neural fields, employing concatenation, hyper-network, and attention-based conditioning strategies -- a necessary but laborious effort that has not been performed in the literature. We find that attention-based conditioning outperforms other approaches in a variety of settings.



### PREF: Predictability Regularized Neural Motion Fields
- **Arxiv ID**: http://arxiv.org/abs/2209.10691v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.10691v2)
- **Published**: 2022-09-21 22:32:37+00:00
- **Updated**: 2023-04-05 19:26:09+00:00
- **Authors**: Liangchen Song, Xuan Gong, Benjamin Planche, Meng Zheng, David Doermann, Junsong Yuan, Terrence Chen, Ziyan Wu
- **Comment**: Accepted at ECCV 2022 (oral). Paper + supplementary material
- **Journal**: None
- **Summary**: Knowing the 3D motions in a dynamic scene is essential to many vision applications. Recent progress is mainly focused on estimating the activity of some specific elements like humans. In this paper, we leverage a neural motion field for estimating the motion of all points in a multiview setting. Modeling the motion from a dynamic scene with multiview data is challenging due to the ambiguities in points of similar color and points with time-varying color. We propose to regularize the estimated motion to be predictable. If the motion from previous frames is known, then the motion in the near future should be predictable. Therefore, we introduce a predictability regularization by first conditioning the estimated motion on latent embeddings, then by adopting a predictor network to enforce predictability on the embeddings. The proposed framework PREF (Predictability REgularized Fields) achieves on par or better results than state-of-the-art neural motion field-based dynamic scene representation methods, while requiring no prior knowledge of the scene.



### Stochastic Future Prediction in Real World Driving Scenarios
- **Arxiv ID**: http://arxiv.org/abs/2209.10693v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.10693v2)
- **Published**: 2022-09-21 22:34:31+00:00
- **Updated**: 2022-09-27 07:30:49+00:00
- **Authors**: Adil Kaan Akan
- **Comment**: MS thesis, overlap with arXiv:2203.13641, arXiv:2203.10528,
  arXiv:2108.02760
- **Journal**: None
- **Summary**: Uncertainty plays a key role in future prediction. The future is uncertain. That means there might be many possible futures. A future prediction method should cover the whole possibilities to be robust. In autonomous driving, covering multiple modes in the prediction part is crucially important to make safety-critical decisions. Although computer vision systems have advanced tremendously in recent years, future prediction remains difficult today. Several examples are uncertainty of the future, the requirement of full scene understanding, and the noisy outputs space. In this thesis, we propose solutions to these challenges by modeling the motion explicitly in a stochastic way and learning the temporal dynamics in a latent space.



### Self-adversarial Multi-scale Contrastive Learning for Semantic Segmentation of Thermal Facial Images
- **Arxiv ID**: http://arxiv.org/abs/2209.10700v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10700v2)
- **Published**: 2022-09-21 22:58:47+00:00
- **Updated**: 2022-10-07 23:05:24+00:00
- **Authors**: Jitesh Joshi, Nadia Bianchi-Berthouze, Youngjun Cho
- **Comment**: Accepted at the British Machine Vision Conference (BMVC), 2022
- **Journal**: None
- **Summary**: Segmentation of thermal facial images is a challenging task. This is because facial features often lack salience due to high-dynamic thermal range scenes and occlusion issues. Limited availability of datasets from unconstrained settings further limits the use of the state-of-the-art segmentation networks, loss functions and learning strategies which have been built and validated for RGB images. To address the challenge, we propose Self-Adversarial Multi-scale Contrastive Learning (SAM-CL) framework as a new training strategy for thermal image segmentation. SAM-CL framework consists of a SAM-CL loss function and a thermal image augmentation (TiAug) module as a domain-specific augmentation technique. We use the Thermal-Face-Database to demonstrate effectiveness of our approach. Experiments conducted on the existing segmentation networks (UNET, Attention-UNET, DeepLabV3 and HRNetv2) evidence the consistent performance gains from the SAM-CL framework. Furthermore, we present a qualitative analysis with UBComfort and DeepBreath datasets to discuss how our proposed methods perform in handling unconstrained situations.



