# Arxiv Papers in cs.CV on 2022-09-19
### Meta-simulation for the Automated Design of Synthetic Overhead Imagery
- **Arxiv ID**: http://arxiv.org/abs/2209.08685v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08685v2)
- **Published**: 2022-09-19 00:19:16+00:00
- **Updated**: 2022-10-26 21:48:08+00:00
- **Authors**: Handi Yu, Simiao Ren, Leslie M. Collins, Jordan M. Malof
- **Comment**: None
- **Journal**: None
- **Summary**: The use of synthetic (or simulated) data for training machine learning models has grown rapidly in recent years. Synthetic data can often be generated much faster and more cheaply than its real-world counterpart. One challenge of using synthetic imagery however is scene design: e.g., the choice of content and its features and spatial arrangement. To be effective, this design must not only be realistic, but appropriate for the target domain, which (by assumption) is unlabeled. In this work, we propose an approach to automatically choose the design of synthetic imagery based upon unlabeled real-world imagery. Our approach, termed Neural-Adjoint Meta-Simulation (NAMS), builds upon the seminal recent meta-simulation approaches. In contrast to the current state-of-the-art methods, our approach can be pre-trained once offline, and then provides fast design inference for new target imagery. Using both synthetic and real-world problems, we show that NAMS infers synthetic designs that match both the in-domain and out-of-domain target imagery, and that training segmentation models with NAMS-designed imagery yields superior results compared to na\"ive randomized designs and state-of-the-art meta-simulation methods.



### Uncertainty Aware Multitask Pyramid Vision Transformer For UAV-Based Object Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2209.08686v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08686v1)
- **Published**: 2022-09-19 00:27:07+00:00
- **Updated**: 2022-09-19 00:27:07+00:00
- **Authors**: Syeda Nyma Ferdous, Xin Li, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: Object Re-IDentification (ReID), one of the most significant problems in biometrics and surveillance systems, has been extensively studied by image processing and computer vision communities in the past decades. Learning a robust and discriminative feature representation is a crucial challenge for object ReID. The problem is even more challenging in ReID based on Unmanned Aerial Vehicle (UAV) as the images are characterized by continuously varying camera parameters (e.g., view angle, altitude, etc.) of a flying drone. To address this challenge, multiscale feature representation has been considered to characterize images captured from UAV flying at different altitudes. In this work, we propose a multitask learning approach, which employs a new multiscale architecture without convolution, Pyramid Vision Transformer (PVT), as the backbone for UAV-based object ReID. By uncertainty modeling of intraclass variations, our proposed model can be jointly optimized using both uncertainty-aware object ID and camera ID information. Experimental results are reported on PRAI and VRAI, two ReID data sets from aerial surveillance, to verify the effectiveness of our proposed approach



### MECCANO: A Multimodal Egocentric Dataset for Humans Behavior Understanding in the Industrial-like Domain
- **Arxiv ID**: http://arxiv.org/abs/2209.08691v1
- **DOI**: 10.1016/S1077-3142(23)00144-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08691v1)
- **Published**: 2022-09-19 00:52:42+00:00
- **Updated**: 2022-09-19 00:52:42+00:00
- **Authors**: Francesco Ragusa, Antonino Furnari, Giovanni Maria Farinella
- **Comment**: arXiv admin note: text overlap with arXiv:2010.05654
- **Journal**: Computer Vision and Image Understanding 2023
- **Summary**: Wearable cameras allow to acquire images and videos from the user's perspective. These data can be processed to understand humans behavior. Despite human behavior analysis has been thoroughly investigated in third person vision, it is still understudied in egocentric settings and in particular in industrial scenarios. To encourage research in this field, we present MECCANO, a multimodal dataset of egocentric videos to study humans behavior understanding in industrial-like settings. The multimodality is characterized by the presence of gaze signals, depth maps and RGB videos acquired simultaneously with a custom headset. The dataset has been explicitly labeled for fundamental tasks in the context of human behavior understanding from a first person view, such as recognizing and anticipating human-object interactions. With the MECCANO dataset, we explored five different tasks including 1) Action Recognition, 2) Active Objects Detection and Recognition, 3) Egocentric Human-Objects Interaction Detection, 4) Action Anticipation and 5) Next-Active Objects Detection. We propose a benchmark aimed to study human behavior in the considered industrial-like scenario which demonstrates that the investigated tasks and the considered scenario are challenging for state-of-the-art algorithms. To support research in this field, we publicy release the dataset at https://iplab.dmi.unict.it/MECCANO/.



### An Adaptive Threshold for the Canny Edge Detection with Actor-Critic Algorithm
- **Arxiv ID**: http://arxiv.org/abs/2209.08699v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08699v1)
- **Published**: 2022-09-19 01:15:32+00:00
- **Updated**: 2022-09-19 01:15:32+00:00
- **Authors**: Keong-Hun Choi, Jong-Eun Ha
- **Comment**: None
- **Journal**: None
- **Summary**: Visual surveillance aims to perform robust foreground object detection regardless of the time and place. Object detection shows good results using only spatial information, but foreground object detection in visual surveillance requires proper temporal and spatial information processing. In deep learning-based foreground object detection algorithms, the detection ability is superior to classical background subtraction (BGS) algorithms in an environment similar to training. However, the performance is lower than that of the classical BGS algorithm in the environment different from training. This paper proposes a spatio-temporal fusion network (STFN) that could extract temporal and spatial information using a temporal network and a spatial network. We suggest a method using a semi-foreground map for stable training of the proposed STFN. The proposed algorithm shows excellent performance in an environment different from training, and we show it through experiments with various public datasets. Also, STFN can generate a compliant background image in a semi-supervised method, and it can operate in real-time on a desktop with GPU. The proposed method shows 11.28% and 18.33% higher FM than the latest deep learning method in the LASIESTA and SBI dataset, respectively.



### GLARE: A Dataset for Traffic Sign Detection in Sun Glare
- **Arxiv ID**: http://arxiv.org/abs/2209.08716v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.08716v1)
- **Published**: 2022-09-19 02:25:41+00:00
- **Updated**: 2022-09-19 02:25:41+00:00
- **Authors**: Nicholas Gray, Megan Moraes, Jiang Bian, Allen Tian, Alex Wang, Haoyi Xiong, Zhishan Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Real-time machine learning detection algorithms are often found within autonomous vehicle technology and depend on quality datasets. It is essential that these algorithms work correctly in everyday conditions as well as under strong sun glare. Reports indicate glare is one of the two most prominent environment-related reasons for crashes. However, existing datasets, such as LISA and the German Traffic Sign Recognition Benchmark, do not reflect the existence of sun glare at all. This paper presents the GLARE traffic sign dataset: a collection of images with U.S based traffic signs under heavy visual interference by sunlight. GLARE contains 2,157 images of traffic signs with sun glare, pulled from 33 videos of dashcam footage of roads in the United States. It provides an essential enrichment to the widely used LISA Traffic Sign dataset. Our experimental study shows that although several state-of-the-art baseline methods demonstrate superior performance when trained and tested against traffic sign datasets without sun glare, they greatly suffer when tested against GLARE (e.g., ranging from 9% to 21% mean mAP, which is significantly lower than the performances on LISA dataset). We also notice that current architectures have better detection accuracy (e.g., on average 42% mean mAP gain for mainstream algorithms) when trained on images of traffic signs in sun glare.



### Density-aware NeRF Ensembles: Quantifying Predictive Uncertainty in Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2209.08718v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08718v1)
- **Published**: 2022-09-19 02:28:33+00:00
- **Updated**: 2022-09-19 02:28:33+00:00
- **Authors**: Niko SÃ¼nderhauf, Jad Abou-Chakra, Dimity Miller
- **Comment**: None
- **Journal**: None
- **Summary**: We show that ensembling effectively quantifies model uncertainty in Neural Radiance Fields (NeRFs) if a density-aware epistemic uncertainty term is considered. The naive ensembles investigated in prior work simply average rendered RGB images to quantify the model uncertainty caused by conflicting explanations of the observed scene. In contrast, we additionally consider the termination probabilities along individual rays to identify epistemic model uncertainty due to a lack of knowledge about the parts of a scene unobserved during training. We achieve new state-of-the-art performance across established uncertainty quantification benchmarks for NeRFs, outperforming methods that require complex changes to the NeRF architecture and training regime. We furthermore demonstrate that NeRF uncertainty can be utilised for next-best view selection and model refinement.



### Ensembles of Compact, Region-specific & Regularized Spiking Neural Networks for Scalable Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.08723v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08723v3)
- **Published**: 2022-09-19 02:47:48+00:00
- **Updated**: 2023-05-05 07:01:17+00:00
- **Authors**: Somayeh Hussaini, Michael Milford, Tobias Fischer
- **Comment**: 8 pages, 6 figures, accepted to the IEEE International Conference on
  Robotics and Automation (ICRA) 2023
- **Journal**: None
- **Summary**: Spiking neural networks have significant potential utility in robotics due to their high energy efficiency on specialized hardware, but proof-of-concept implementations have not yet typically achieved competitive performance or capability with conventional approaches. In this paper, we tackle one of the key practical challenges of scalability by introducing a novel modular ensemble network approach, where compact, localized spiking networks each learn and are solely responsible for recognizing places in a local region of the environment only. This modular approach creates a highly scalable system. However, it comes with a high-performance cost where a lack of global regularization at deployment time leads to hyperactive neurons that erroneously respond to places outside their learned region. Our second contribution introduces a regularization approach that detects and removes these problematic hyperactive neurons during the initial environmental learning phase. We evaluate this new scalable modular system on benchmark localization datasets Nordland and Oxford RobotCar, with comparisons to standard techniques NetVLAD, DenseVLAD, and SAD, and a previous spiking neural network system. Our system substantially outperforms the previous SNN system on its small dataset, but also maintains performance on 27 times larger benchmark datasets where the operation of the previous system is computationally infeasible, and performs competitively with the conventional localization systems.



### Neural Wavelet-domain Diffusion for 3D Shape Generation
- **Arxiv ID**: http://arxiv.org/abs/2209.08725v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.08725v1)
- **Published**: 2022-09-19 02:51:48+00:00
- **Updated**: 2022-09-19 02:51:48+00:00
- **Authors**: Ka-Hei Hui, Ruihui Li, Jingyu Hu, Chi-Wing Fu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new approach for 3D shape generation, enabling direct generative modeling on a continuous implicit representation in wavelet domain. Specifically, we propose a compact wavelet representation with a pair of coarse and detail coefficient volumes to implicitly represent 3D shapes via truncated signed distance functions and multi-scale biorthogonal wavelets, and formulate a pair of neural networks: a generator based on the diffusion model to produce diverse shapes in the form of coarse coefficient volumes; and a detail predictor to further produce compatible detail coefficient volumes for enriching the generated shapes with fine structures and details. Both quantitative and qualitative experimental results manifest the superiority of our approach in generating diverse and high-quality shapes with complex topology and structures, clean surfaces, and fine details, exceeding the 3D generation capabilities of the state-of-the-art models.



### Axially Expanded Windows for Local-Global Interaction in Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2209.08726v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08726v2)
- **Published**: 2022-09-19 02:53:07+00:00
- **Updated**: 2022-11-13 02:56:49+00:00
- **Authors**: Zhemin Zhang, Xun Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Transformers have shown promising performance in various vision tasks. A challenging issue in Transformer design is that global self-attention is very expensive to compute, especially for the high-resolution vision tasks. Local self-attention performs attention computation within a local region to improve its efficiency, which leads to their receptive fields in a single attention layer are not large enough, resulting in insufficient context modeling. When observing a scene, humans usually focus on a local region while attending to non-attentional regions at coarse granularity. Based on this observation, we develop the axially expanded window self-attention mechanism that performs fine-grained self-attention within the local window and coarse-grained self-attention in the horizontal and vertical axes, and thus can effectively capturing both short- and long-range visual dependencies.



### Magnetic Resonance Fingerprinting with compressed sensing and distance metric learning
- **Arxiv ID**: http://arxiv.org/abs/2209.08734v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.08734v1)
- **Published**: 2022-09-19 03:08:26+00:00
- **Updated**: 2022-09-19 03:08:26+00:00
- **Authors**: Zhe Wang, Hongsheng Li, Qinwei Zhang, Jing Yuan, Xiaogang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic Resonance Fingerprinting (MRF) is a novel technique that simultaneously estimates multiple tissue-related parameters, such as the longitudinal relaxation time T1, the transverse relaxation time T2, off resonance frequency B0 and proton density, from a scanned object in just tens of seconds. However, the MRF method suffers from aliasing artifacts because it significantly undersamples the k-space data. In this work, we propose a compressed sensing (CS) framework for simultaneously estimating multiple tissue-related parameters based on the MRF method. It is more robust to low sampling ratio and is therefore more efficient in estimating MR parameters for all voxels of an object. Furthermore, the MRF method requires identifying the nearest atoms of the query fingerprints from the MR-signal-evolution dictionary with the L2 distance. However, we observed that the L2 distance is not always a proper metric to measure the similarities between MR Fingerprints. Adaptively learning a distance metric from the undersampled training data can significantly improve the matching accuracy of the query fingerprints. Numerical results on extensive simulated cases show that our method substantially outperforms stateof-the-art methods in terms of accuracy of parameter estimation.



### Adaptive Multi-stage Density Ratio Estimation for Learning Latent Space Energy-based Model
- **Arxiv ID**: http://arxiv.org/abs/2209.08739v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2209.08739v1)
- **Published**: 2022-09-19 03:20:15+00:00
- **Updated**: 2022-09-19 03:20:15+00:00
- **Authors**: Zhisheng Xiao, Tian Han
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: This paper studies the fundamental problem of learning energy-based model (EBM) in the latent space of the generator model. Learning such prior model typically requires running costly Markov Chain Monte Carlo (MCMC). Instead, we propose to use noise contrastive estimation (NCE) to discriminatively learn the EBM through density ratio estimation between the latent prior density and latent posterior density. However, the NCE typically fails to accurately estimate such density ratio given large gap between two densities. To effectively tackle this issue and learn more expressive prior models, we develop the adaptive multi-stage density ratio estimation which breaks the estimation into multiple stages and learn different stages of density ratio sequentially and adaptively. The latent prior model can be gradually learned using ratio estimated in previous stage so that the final latent space EBM prior can be naturally formed by product of ratios in different stages. The proposed method enables informative and much sharper prior than existing baselines, and can be trained efficiently. Our experiments demonstrate strong performances in image generation and reconstruction as well as anomaly detection.



### Integrative Feature and Cost Aggregation with Transformers for Dense Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2209.08742v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08742v2)
- **Published**: 2022-09-19 03:33:35+00:00
- **Updated**: 2022-09-20 04:51:13+00:00
- **Authors**: Sunghwan Hong, Seokju Cho, Seungryong Kim, Stephen Lin
- **Comment**: v2 includes supplementary material, while v1 does not
- **Journal**: None
- **Summary**: We present a novel architecture for dense correspondence. The current state-of-the-art are Transformer-based approaches that focus on either feature descriptors or cost volume aggregation. However, they generally aggregate one or the other but not both, though joint aggregation would boost each other by providing information that one has but other lacks, i.e., structural or semantic information of an image, or pixel-wise matching similarity. In this work, we propose a novel Transformer-based network that interleaves both forms of aggregations in a way that exploits their complementary information. Specifically, we design a self-attention layer that leverages the descriptor to disambiguate the noisy cost volume and that also utilizes the cost volume to aggregate features in a manner that promotes accurate matching. A subsequent cross-attention layer performs further aggregation conditioned on the descriptors of both images and aided by the aggregated outputs of earlier layers. We further boost the performance with hierarchical processing, in which coarser level aggregations guide those at finer levels. We evaluate the effectiveness of the proposed method on dense matching tasks and achieve state-of-the-art performance on all the major benchmarks. Extensive ablation studies are also provided to validate our design choices.



### On Robust Cross-View Consistency in Self-Supervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/2209.08747v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08747v2)
- **Published**: 2022-09-19 03:46:13+00:00
- **Updated**: 2023-07-30 07:25:17+00:00
- **Authors**: Haimei Zhao, Jing Zhang, Zhuo Chen, Bo Yuan, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Remarkable progress has been made in self-supervised monocular depth estimation (SS-MDE) by exploring cross-view consistency, e.g., photometric consistency and 3D point cloud consistency. However, they are very vulnerable to illumination variance, occlusions, texture-less regions, as well as moving objects, making them not robust enough to deal with various scenes. To address this challenge, we study two kinds of robust cross-view consistency in this paper. Firstly, the spatial offset field between adjacent frames is obtained by reconstructing the reference frame from its neighbors via deformable alignment, which is used to align the temporal depth features via a Depth Feature Alignment (DFA) loss. Secondly, the 3D point clouds of each reference frame and its nearby frames are calculated and transformed into voxel space, where the point density in each voxel is calculated and aligned via a Voxel Density Alignment (VDA) loss. In this way, we exploit the temporal coherence in both depth feature space and 3D voxel space for SS-MDE, shifting the "point-to-point" alignment paradigm to the "region-to-region" one. Compared with the photometric consistency loss as well as the rigid point cloud alignment loss, the proposed DFA and VDA losses are more robust owing to the strong representation power of deep features as well as the high tolerance of voxel density to the aforementioned challenges. Experimental results on several outdoor benchmarks show that our method outperforms current state-of-the-art techniques. Extensive ablation study and analysis validate the effectiveness of the proposed losses, especially in challenging scenes. The code and models are available at https://github.com/sunnyHelen/RCVC-depth.



### Keypoint-GraspNet: Keypoint-based 6-DoF Grasp Generation from the Monocular RGB-D input
- **Arxiv ID**: http://arxiv.org/abs/2209.08752v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.08752v4)
- **Published**: 2022-09-19 04:23:20+00:00
- **Updated**: 2023-05-01 17:53:42+00:00
- **Authors**: Yiye Chen, Yunzhi Lin, Ruinian Xu, Patricio Vela
- **Comment**: Accepted by ICRA2023. Final version. Code is available at:
  https://github.com/ivalab/KGN
- **Journal**: None
- **Summary**: Great success has been achieved in the 6-DoF grasp learning from the point cloud input, yet the computational cost due to the point set orderlessness remains a concern. Alternatively, we explore the grasp generation from the RGB-D input in this paper. The proposed solution, Keypoint-GraspNet, detects the projection of the gripper keypoints in the image space and then recover the SE(3) poses with a PnP algorithm. A synthetic dataset based on the primitive shape and the grasp family is constructed to examine our idea. Metric-based evaluation reveals that our method outperforms the baselines in terms of the grasp proposal accuracy, diversity, and the time cost. Finally, robot experiments show high success rate, demonstrating the potential of the idea in the real-world applications.



### Tree-based Text-Vision BERT for Video Search in Baidu Video Advertising
- **Arxiv ID**: http://arxiv.org/abs/2209.08759v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2209.08759v1)
- **Published**: 2022-09-19 04:49:51+00:00
- **Updated**: 2022-09-19 04:49:51+00:00
- **Authors**: Tan Yu, Jie Liu, Yi Yang, Yi Li, Hongliang Fei, Ping Li
- **Comment**: This revision is based on a manuscript submitted in October 2020, to
  ICDE 2021. We thank the Program Committee for their valuable comments
- **Journal**: None
- **Summary**: The advancement of the communication technology and the popularity of the smart phones foster the booming of video ads. Baidu, as one of the leading search engine companies in the world, receives billions of search queries per day. How to pair the video ads with the user search is the core task of Baidu video advertising. Due to the modality gap, the query-to-video retrieval is much more challenging than traditional query-to-document retrieval and image-to-image search. Traditionally, the query-to-video retrieval is tackled by the query-to-title retrieval, which is not reliable when the quality of tiles are not high. With the rapid progress achieved in computer vision and natural language processing in recent years, content-based search methods becomes promising for the query-to-video retrieval. Benefited from pretraining on large-scale datasets, some visionBERT methods based on cross-modal attention have achieved excellent performance in many vision-language tasks not only in academia but also in industry. Nevertheless, the expensive computation cost of cross-modal attention makes it impractical for large-scale search in industrial applications. In this work, we present a tree-based combo-attention network (TCAN) which has been recently launched in Baidu's dynamic video advertising platform. It provides a practical solution to deploy the heavy cross-modal attention for the large-scale query-to-video search. After launching tree-based combo-attention network, click-through rate gets improved by 2.29\% and conversion rate get improved by 2.63\%.



### Fairness on Synthetic Visual and Thermal Mask Images
- **Arxiv ID**: http://arxiv.org/abs/2209.08762v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08762v1)
- **Published**: 2022-09-19 05:04:42+00:00
- **Updated**: 2022-09-19 05:04:42+00:00
- **Authors**: Kenneth Lai, Vlad Shmerko, Svetlana Yanushkevich
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: In this paper, we study performance and fairness on visual and thermal images and expand the assessment to masked synthetic images. Using the SpeakingFace and Thermal-Mask dataset, we propose a process to assess fairness on real images and show how the same process can be applied to synthetic images. The resulting process shows a demographic parity difference of 1.59 for random guessing and increases to 5.0 when the recognition performance increases to a precision and recall rate of 99.99\%. We indicate that inherently biased datasets can deeply impact the fairness of any biometric system. A primary cause of a biased dataset is the class imbalance due to the data collection process. To address imbalanced datasets, the classes with fewer samples can be augmented with synthetic images to generate a more balanced dataset resulting in less bias when training a machine learning system. For biometric-enabled systems, fairness is of critical importance, while the related concept of Equity, Diversity, and Inclusion (EDI) is well suited for the generalization of fairness in biometrics, in this paper, we focus on the 3 most common demographic groups age, gender, and ethnicity.



### Decentralized Vehicle Coordination: The Berkeley DeepDrive Drone Dataset
- **Arxiv ID**: http://arxiv.org/abs/2209.08763v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08763v2)
- **Published**: 2022-09-19 05:06:57+00:00
- **Updated**: 2022-09-22 05:39:51+00:00
- **Authors**: Fangyu Wu, Dequan Wang, Minjune Hwang, Chenhui Hao, Jiawei Lu, Jiamu Zhang, Christopher Chou, Trevor Darrell, Alexandre Bayen
- **Comment**: 6 pages, 10 figures, 1 table
- **Journal**: None
- **Summary**: Decentralized multiagent planning has been an important field of research in robotics. An interesting and impactful application in the field is decentralized vehicle coordination in understructured road environments. For example, in an intersection, it is useful yet difficult to deconflict multiple vehicles of intersecting paths in absence of a central coordinator. We learn from common sense that, for a vehicle to navigate through such understructured environments, the driver must understand and conform to the implicit "social etiquette" observed by nearby drivers. To study this implicit driving protocol, we collect the Berkeley DeepDrive Drone dataset. The dataset contains 1) a set of aerial videos recording understructured driving, 2) a collection of images and annotations to train vehicle detection models, and 3) a kit of development scripts for illustrating typical usages. We believe that the dataset is of primary interest for studying decentralized multiagent planning employed by human drivers and, of secondary interest, for computer vision in remote sensing settings.



### S$^3$R: Self-supervised Spectral Regression for Hyperspectral Histopathology Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.08770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08770v1)
- **Published**: 2022-09-19 05:47:11+00:00
- **Updated**: 2022-09-19 05:47:11+00:00
- **Authors**: Xingran Xie, Yan Wang, Qingli Li
- **Comment**: None
- **Journal**: None
- **Summary**: Benefited from the rich and detailed spectral information in hyperspectral images (HSI), HSI offers great potential for a wide variety of medical applications such as computational pathology. But, the lack of adequate annotated data and the high spatiospectral dimensions of HSIs usually make classification networks prone to overfit. Thus, learning a general representation which can be transferred to the downstream tasks is imperative. To our knowledge, no appropriate self-supervised pre-training method has been designed for histopathology HSIs. In this paper, we introduce an efficient and effective Self-supervised Spectral Regression (S$^3$R) method, which exploits the low rank characteristic in the spectral domain of HSI. More concretely, we propose to learn a set of linear coefficients that can be used to represent one band by the remaining bands via masking out these bands. Then, the band is restored by using the learned coefficients to reweight the remaining bands. Two pre-text tasks are designed: (1)S$^3$R-CR, which regresses the linear coefficients, so that the pre-trained model understands the inherent structures of HSIs and the pathological characteristics of different morphologies; (2)S$^3$R-BR, which regresses the missing band, making the model to learn the holistic semantics of HSIs. Compared to prior arts i.e., contrastive learning methods, which focuses on natural images, S$^3$R converges at least 3 times faster, and achieves significant improvements up to 14% in accuracy when transferring to HSI classification tasks.



### TANDEM3D: Active Tactile Exploration for 3D Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.08772v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.08772v2)
- **Published**: 2022-09-19 05:54:26+00:00
- **Updated**: 2023-02-28 05:22:09+00:00
- **Authors**: Jingxi Xu, Han Lin, Shuran Song, Matei Ciocarlie
- **Comment**: 7 pages. Accepted to International Conference on Robotics and
  Automation (ICRA) 2023
- **Journal**: None
- **Summary**: Tactile recognition of 3D objects remains a challenging task. Compared to 2D shapes, the complex geometry of 3D surfaces requires richer tactile signals, more dexterous actions, and more advanced encoding techniques. In this work, we propose TANDEM3D, a method that applies a co-training framework for exploration and decision making to 3D object recognition with tactile signals. Starting with our previous work, which introduced a co-training paradigm for 2D recognition problems, we introduce a number of advances that enable us to scale up to 3D. TANDEM3D is based on a novel encoder that builds 3D object representation from contact positions and normals using PointNet++. Furthermore, by enabling 6DOF movement, TANDEM3D explores and collects discriminative touch information with high efficiency. Our method is trained entirely in simulation and validated with real-world experiments. Compared to state-of-the-art baselines, TANDEM3D achieves higher accuracy and a lower number of actions in recognizing 3D objects and is also shown to be more robust to different types and amounts of sensor noise. Video is available at https://jxu.ai/tandem3d.



### NeRF-SOS: Any-View Self-supervised Object Segmentation on Complex Scenes
- **Arxiv ID**: http://arxiv.org/abs/2209.08776v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08776v6)
- **Published**: 2022-09-19 06:03:17+00:00
- **Updated**: 2022-10-12 00:33:31+00:00
- **Authors**: Zhiwen Fan, Peihao Wang, Yifan Jiang, Xinyu Gong, Dejia Xu, Zhangyang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Neural volumetric representations have shown the potential that Multi-layer Perceptrons (MLPs) can be optimized with multi-view calibrated images to represent scene geometry and appearance, without explicit 3D supervision. Object segmentation can enrich many downstream applications based on the learned radiance field. However, introducing hand-crafted segmentation to define regions of interest in a complex real-world scene is non-trivial and expensive as it acquires per view annotation. This paper carries out the exploration of self-supervised learning for object segmentation using NeRF for complex real-world scenes. Our framework, called NeRF with Self-supervised Object Segmentation NeRF-SOS, couples object segmentation and neural radiance field to segment objects in any view within a scene. By proposing a novel collaborative contrastive loss in both appearance and geometry levels, NeRF-SOS encourages NeRF models to distill compact geometry-aware segmentation clusters from their density fields and the self-supervised pre-trained 2D visual features. The self-supervised object segmentation framework can be applied to various NeRF models that both lead to photo-realistic rendering results and convincing segmentation maps for both indoor and outdoor scenarios. Extensive results on the LLFF, Tank & Temple, and BlendedMVS datasets validate the effectiveness of NeRF-SOS. It consistently surpasses other 2D-based self-supervised baselines and predicts finer semantics masks than existing supervised counterparts. Please refer to the video on our project page for more details:https://zhiwenfan.github.io/NeRF-SOS.



### Scale Attention for Learning Deep Face Representation: A Study Against Visual Scale Variation
- **Arxiv ID**: http://arxiv.org/abs/2209.08788v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08788v1)
- **Published**: 2022-09-19 06:35:04+00:00
- **Updated**: 2022-09-19 06:35:04+00:00
- **Authors**: Hailin Shi, Hang Du, Yibo Hu, Jun Wang, Dan Zeng, Ting Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Human face images usually appear with wide range of visual scales. The existing face representations pursue the bandwidth of handling scale variation via multi-scale scheme that assembles a finite series of predefined scales. Such multi-shot scheme brings inference burden, and the predefined scales inevitably have gap from real data. Instead, learning scale parameters from data, and using them for one-shot feature inference, is a decent solution. To this end, we reform the conv layer by resorting to the scale-space theory, and achieve two-fold facilities: 1) the conv layer learns a set of scales from real data distribution, each of which is fulfilled by a conv kernel; 2) the layer automatically highlights the feature at the proper channel and location corresponding to the input pattern scale and its presence. Then, we accomplish the hierarchical scale attention by stacking the reformed layers, building a novel style named SCale AttentioN Conv Neural Network (\textbf{SCAN-CNN}). We apply SCAN-CNN to the face recognition task and push the frontier of SOTA performance. The accuracy gain is more evident when the face images are blurry. Meanwhile, as a single-shot scheme, the inference is more efficient than multi-shot fusion. A set of tools are made to ensure the fast training of SCAN-CNN and zero increase of inference cost compared with the plain CNN.



### D&D: Learning Human Dynamics from Dynamic Camera
- **Arxiv ID**: http://arxiv.org/abs/2209.08790v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.08790v1)
- **Published**: 2022-09-19 06:51:02+00:00
- **Updated**: 2022-09-19 06:51:02+00:00
- **Authors**: Jiefeng Li, Siyuan Bian, Chao Xu, Gang Liu, Gang Yu, Cewu Lu
- **Comment**: ECCV 2022 (Oral)
- **Journal**: None
- **Summary**: 3D human pose estimation from a monocular video has recently seen significant improvements. However, most state-of-the-art methods are kinematics-based, which are prone to physically implausible motions with pronounced artifacts. Current dynamics-based methods can predict physically plausible motion but are restricted to simple scenarios with static camera view. In this work, we present D&D (Learning Human Dynamics from Dynamic Camera), which leverages the laws of physics to reconstruct 3D human motion from the in-the-wild videos with a moving camera. D&D introduces inertial force control (IFC) to explain the 3D human motion in the non-inertial local frame by considering the inertial forces of the dynamic camera. To learn the ground contact with limited annotations, we develop probabilistic contact torque (PCT), which is computed by differentiable sampling from contact probabilities and used to generate motions. The contact state can be weakly supervised by encouraging the model to generate correct motions. Furthermore, we propose an attentive PD controller that adjusts target pose states using temporal information to obtain smooth and accurate pose control. Our approach is entirely neural-based and runs without offline optimization or simulation in physics engines. Experiments on large-scale 3D human motion benchmarks demonstrate the effectiveness of D&D, where we exhibit superior performance against both state-of-the-art kinematics-based and dynamics-based methods. Code is available at https://github.com/Jeffsjtu/DnD



### DifferSketching: How Differently Do People Sketch 3D Objects?
- **Arxiv ID**: http://arxiv.org/abs/2209.08791v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.08791v1)
- **Published**: 2022-09-19 06:52:18+00:00
- **Updated**: 2022-09-19 06:52:18+00:00
- **Authors**: Chufeng Xiao, Wanchao Su, Jing Liao, Zhouhui Lian, Yi-Zhe Song, Hongbo Fu
- **Comment**: SIGGRAPH Asia 2022 (Journal Track)
- **Journal**: None
- **Summary**: Multiple sketch datasets have been proposed to understand how people draw 3D objects. However, such datasets are often of small scale and cover a small set of objects or categories. In addition, these datasets contain freehand sketches mostly from expert users, making it difficult to compare the drawings by expert and novice users, while such comparisons are critical in informing more effective sketch-based interfaces for either user groups. These observations motivate us to analyze how differently people with and without adequate drawing skills sketch 3D objects. We invited 70 novice users and 38 expert users to sketch 136 3D objects, which were presented as 362 images rendered from multiple views. This leads to a new dataset of 3,620 freehand multi-view sketches, which are registered with their corresponding 3D objects under certain views. Our dataset is an order of magnitude larger than the existing datasets. We analyze the collected data at three levels, i.e., sketch-level, stroke-level, and pixel-level, under both spatial and temporal characteristics, and within and across groups of creators. We found that the drawings by professionals and novices show significant differences at stroke-level, both intrinsically and extrinsically. We demonstrate the usefulness of our dataset in two applications: (i) freehand-style sketch synthesis, and (ii) posing it as a potential benchmark for sketch-based 3D reconstruction. Our dataset and code are available at https://chufengxiao.github.io/DifferSketching/.



### AutoLV: Automatic Lecture Video Generator
- **Arxiv ID**: http://arxiv.org/abs/2209.08795v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2209.08795v1)
- **Published**: 2022-09-19 07:00:14+00:00
- **Updated**: 2022-09-19 07:00:14+00:00
- **Authors**: Wenbin Wang, Yang Song, Sanjay Jha
- **Comment**: 4 pages, 4 figures, ICIP 2022
- **Journal**: None
- **Summary**: We propose an end-to-end lecture video generation system that can generate realistic and complete lecture videos directly from annotated slides, instructor's reference voice and instructor's reference portrait video. Our system is primarily composed of a speech synthesis module with few-shot speaker adaptation and an adversarial learning-based talking-head generation module. It is capable of not only reducing instructors' workload but also changing the language and accent which can help the students follow the lecture more easily and enable a wider dissemination of lecture contents. Our experimental results show that the proposed model outperforms other current approaches in terms of authenticity, naturalness and accuracy. Here is a video demonstration of how our system works, and the outcomes of the evaluation and comparison: https://youtu.be/cY6TYkI0cog.



### Zero-shot Active Visual Search (ZAVIS): Intelligent Object Search for Robotic Assistants
- **Arxiv ID**: http://arxiv.org/abs/2209.08803v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08803v3)
- **Published**: 2022-09-19 07:18:46+00:00
- **Updated**: 2023-02-07 15:46:11+00:00
- **Authors**: Jeongeun Park, Taerim Yoon, Jejoon Hong, Youngjae Yu, Matthew Pan, Sungjoon Choi
- **Comment**: To be appear on ICRA 2023
- **Journal**: None
- **Summary**: In this paper, we focus on the problem of efficiently locating a target object described with free-form language using a mobile robot equipped with vision sensors (e.g., an RGBD camera). Conventional active visual search predefines a set of objects to search for, rendering these techniques restrictive in practice. To provide added flexibility in active visual searching, we propose a system where a user can enter target commands using free-form language; we call this system Active Visual Search in the Wild (AVSW). AVSW detects and plans to search for a target object inputted by a user through a semantic grid map represented by static landmarks (e.g., desk or bed). For efficient planning of object search patterns, AVSW considers commonsense knowledge-based co-occurrence and predictive uncertainty while deciding which landmarks to visit first. We validate the proposed method with respect to SR (success rate) and SPL (success weighted by path length) in both simulated and real-world environments. The proposed method outperforms previous methods in terms of SPL in simulated scenarios with an average gap of 0.283. We further demonstrate AVSW with a Pioneer-3AT robot in real-world studies.



### A Deep Learning Approach for Parallel Imaging and Compressed Sensing MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2209.08807v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08807v2)
- **Published**: 2022-09-19 07:26:45+00:00
- **Updated**: 2022-12-17 05:55:33+00:00
- **Authors**: Farhan Sadik, Md. Kamrul Hasan
- **Comment**: 13 pages, 11 figures
- **Journal**: None
- **Summary**: Parallel imaging accelerates MRI data acquisition by acquiring additional sensitivity information with an array of receiver coils, resulting in fewer phase encoding steps. Because of fewer data requirements than parallel imaging, compressed sensing magnetic resonance imaging (CS-MRI) has gained popularity in the field of medical imaging. Parallel imaging and compressed sensing (CS) both reduce the amount of data captured in the k-space, which speeds up traditional MRI acquisition. As acquisition time is inversely proportional to sample count, forming an image from reduced k-space samples results in faster acquisition but with aliasing artifacts. For de-aliasing the reconstructed image, this paper proposes a novel Generative Adversarial Network (GAN) called RECGAN-GR that is supervised with multi-modal losses. In comparison to existing GAN networks, our proposed method introduces a novel generator network, RemU-Net, which is integrated with dual-domain loss functions such as weighted magnitude and phase loss functions, as well as parallel imaging-based loss, GRAPPA consistency loss. As refinement learning, a k-space correction block is proposed to make the GAN network self-resistant to generating unnecessary data, which speeds up the reconstruction process. Comprehensive results show that the proposed RECGAN-GR not only improves the PSNR by 4 dB over GAN-based methods but also by 2 dB over conventional state-of-the-art CNN methods available in the literature for single-coil data. The proposed work significantly improves image quality for low-retained data, resulting in five to ten times faster acquisition.



### LMBAO: A Landmark Map for Bundle Adjustment Odometry in LiDAR SLAM
- **Arxiv ID**: http://arxiv.org/abs/2209.08810v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.08810v1)
- **Published**: 2022-09-19 07:48:28+00:00
- **Updated**: 2022-09-19 07:48:28+00:00
- **Authors**: Letian Zhang, Jinping Wang, Lu Jie, Nanjie Chen, Xiaojun Tan, Zhifei Duan
- **Comment**: 9 pages, 3 tables, 6 figures
- **Journal**: None
- **Summary**: LiDAR odometry is one of the essential parts of LiDAR simultaneous localization and mapping (SLAM). However, existing LiDAR odometry tends to match a new scan simply iteratively with previous fixed-pose scans, gradually accumulating errors. Furthermore, as an effective joint optimization mechanism, bundle adjustment (BA) cannot be directly introduced into real-time odometry due to the intensive computation of large-scale global landmarks. Therefore, this letter designs a new strategy named a landmark map for bundle adjustment odometry (LMBAO) in LiDAR SLAM to solve these problems. First, BA-based odometry is further developed with an active landmark maintenance strategy for a more accurate local registration and avoiding cumulative errors. Specifically, this paper keeps entire stable landmarks on the map instead of just their feature points in the sliding window and deletes the landmarks according to their active grade. Next, the sliding window length is reduced, and marginalization is performed to retain the scans outside the window but corresponding to active landmarks on the map, greatly simplifying the computation and improving the real-time properties. In addition, experiments on three challenging datasets show that our algorithm achieves real-time performance in outdoor driving and outperforms state-of-the-art LiDAR SLAM algorithms, including Lego-LOAM and VLOM.



### T2V-DDPM: Thermal to Visible Face Translation using Denoising Diffusion Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/2209.08814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08814v1)
- **Published**: 2022-09-19 07:59:32+00:00
- **Updated**: 2022-09-19 07:59:32+00:00
- **Authors**: Nithin Gopalakrishnan Nair, Vishal M. Patel
- **Comment**: Accepted at The IEEE conference series on Automatic Face and Gesture
  Recognition 2023
- **Journal**: None
- **Summary**: Modern-day surveillance systems perform person recognition using deep learning-based face verification networks. Most state-of-the-art facial verification systems are trained using visible spectrum images. But, acquiring images in the visible spectrum is impractical in scenarios of low-light and nighttime conditions, and often images are captured in an alternate domain such as the thermal infrared domain. Facial verification in thermal images is often performed after retrieving the corresponding visible domain images. This is a well-established problem often known as the Thermal-to-Visible (T2V) image translation. In this paper, we propose a Denoising Diffusion Probabilistic Model (DDPM) based solution for T2V translation specifically for facial images. During training, the model learns the conditional distribution of visible facial images given their corresponding thermal image through the diffusion process. During inference, the visible domain image is obtained by starting from Gaussian noise and performing denoising repeatedly. The existing inference process for DDPMs is stochastic and time-consuming. Hence, we propose a novel inference strategy for speeding up the inference time of DDPMs, specifically for the problem of T2V image translation. We achieve the state-of-the-art results on multiple datasets. The code and pretrained models are publically available at http://github.com/Nithin-GK/T2V-DDPM



### On the Shift Invariance of Max Pooling Feature Maps in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.11740v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2209.11740v1)
- **Published**: 2022-09-19 08:15:30+00:00
- **Updated**: 2022-09-19 08:15:30+00:00
- **Authors**: Hubert Leterme, KÃ©vin Polisano, ValÃ©rie Perrier, Karteek Alahari
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we aim to improve the mathematical interpretability of convolutional neural networks for image classification. When trained on natural image datasets, such networks tend to learn parameters in the first layer that closely resemble oriented Gabor filters. By leveraging the properties of discrete Gabor-like convolutions, we prove that, under specific conditions, feature maps computed by the subsequent max pooling operator tend to approximate the modulus of complex Gabor-like coefficients, and as such, are stable with respect to certain input shifts. We then compute a probabilistic measure of shift invariance for these layers. More precisely, we show that some filters, depending on their frequency and orientation, are more likely than others to produce stable image representations. We experimentally validate our theory by considering a deterministic feature extractor based on the dual-tree wavelet packet transform, a particular case of discrete Gabor-like decomposition. We demonstrate a strong correlation between shift invariance on the one hand and similarity with complex modulus on the other hand.



### A Dual-Cycled Cross-View Transformer Network for Unified Road Layout Estimation and 3D Object Detection in the Bird's-Eye-View
- **Arxiv ID**: http://arxiv.org/abs/2209.08844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08844v1)
- **Published**: 2022-09-19 08:43:38+00:00
- **Updated**: 2022-09-19 08:43:38+00:00
- **Authors**: Curie Kim, Ue-Hwan Kim
- **Comment**: None
- **Journal**: None
- **Summary**: The bird's-eye-view (BEV) representation allows robust learning of multiple tasks for autonomous driving including road layout estimation and 3D object detection. However, contemporary methods for unified road layout estimation and 3D object detection rarely handle the class imbalance of the training dataset and multi-class learning to reduce the total number of networks required. To overcome these limitations, we propose a unified model for road layout estimation and 3D object detection inspired by the transformer architecture and the CycleGAN learning framework. The proposed model deals with the performance degradation due to the class imbalance of the dataset utilizing the focal loss and the proposed dual cycle loss. Moreover, we set up extensive learning scenarios to study the effect of multi-class learning for road layout estimation in various situations. To verify the effectiveness of the proposed model and the learning scheme, we conduct a thorough ablation study and a comparative study. The experiment results attest the effectiveness of our model; we achieve state-of-the-art performance in both the road layout estimation and 3D object detection tasks.



### Masked Face Inpainting Through Residual Attention UNet
- **Arxiv ID**: http://arxiv.org/abs/2209.08850v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08850v1)
- **Published**: 2022-09-19 08:49:53+00:00
- **Updated**: 2022-09-19 08:49:53+00:00
- **Authors**: Md Imran Hosen, Md Baharul Islam
- **Comment**: 5 pages, 8 figures, Innovations in Intelligent Systems and
  Applications Conference
- **Journal**: None
- **Summary**: Realistic image restoration with high texture areas such as removing face masks is challenging. The state-of-the-art deep learning-based methods fail to guarantee high-fidelity, cause training instability due to vanishing gradient problems (e.g., weights are updated slightly in initial layers) and spatial information loss. They also depend on intermediary stage such as segmentation meaning require external mask. This paper proposes a blind mask face inpainting method using residual attention UNet to remove the face mask and restore the face with fine details while minimizing the gap with the ground truth face structure. A residual block feeds info to the next layer and directly into the layers about two hops away to solve the gradient vanishing problem. Besides, the attention unit helps the model focus on the relevant mask region, reducing resources and making the model faster. Extensive experiments on the publicly available CelebA dataset show the feasibility and robustness of our proposed model. Code is available at \url{https://github.com/mdhosen/Mask-Face-Inpainting-Using-Residual-Attention-Unet}



### Provably Uncertainty-Guided Universal Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2209.09616v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09616v7)
- **Published**: 2022-09-19 09:16:07+00:00
- **Updated**: 2023-04-03 12:53:52+00:00
- **Authors**: Yifan Wang, Lin Zhang, Ran Song, Paul L. Rosin, Yibin Li, Wei Zhang
- **Comment**: 13 pages. arXiv admin note: text overlap with arXiv:2207.09280
- **Journal**: None
- **Summary**: Universal domain adaptation (UniDA) aims to transfer the knowledge from a labeled source domain to an unlabeled target domain without any assumptions of the label sets, which requires distinguishing the unknown samples from the known ones in the target domain. A main challenge of UniDA is that the nonidentical label sets cause the misalignment between the two domains. Moreover, the domain discrepancy and the supervised objectives in the source domain easily lead the whole model to be biased towards the common classes and produce overconfident predictions for unknown samples. To address the above challenging problems, we propose a new uncertainty-guided UniDA framework. Firstly, we introduce an empirical estimation of the probability of a target sample belonging to the unknown class which fully exploits the distribution of the target samples in the latent space. Then, based on the estimation, we propose a novel neighbors searching scheme in a linear subspace with a $\delta$-filter to estimate the uncertainty score of a target sample and discover unknown samples. It fully utilizes the relationship between a target sample and its neighbors in the source domain to avoid the influence of domain misalignment. Secondly, this paper well balances the confidences of predictions for both known and unknown samples through an uncertainty-guided margin loss based on the confidences of discovered unknown samples, which can reduce the gap between the intra-class variances of known classes with respect to the unknown class. Finally, experiments on three public datasets demonstrate that our method significantly outperforms existing state-of-the-art methods.



### Flexible Neural Image Compression via Code Editing
- **Arxiv ID**: http://arxiv.org/abs/2209.09244v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.09244v1)
- **Published**: 2022-09-19 09:41:43+00:00
- **Updated**: 2022-09-19 09:41:43+00:00
- **Authors**: Chenjian Gao, Tongda Xu, Dailan He, Hongwei Qin, Yan Wang
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: Neural image compression (NIC) has outperformed traditional image codecs in rate-distortion (R-D) performance. However, it usually requires a dedicated encoder-decoder pair for each point on R-D curve, which greatly hinders its practical deployment. While some recent works have enabled bitrate control via conditional coding, they impose strong prior during training and provide limited flexibility. In this paper we propose Code Editing, a highly flexible coding method for NIC based on semi-amortized inference and adaptive quantization. Our work is a new paradigm for variable bitrate NIC. Furthermore, experimental results show that our method surpasses existing variable-rate methods, and achieves ROI coding and multi-distortion trade-off with a single decoder.



### Attentive Symmetric Autoencoder for Brain MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.08887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08887v1)
- **Published**: 2022-09-19 09:43:19+00:00
- **Updated**: 2022-09-19 09:43:19+00:00
- **Authors**: Junjia Huang, Haofeng Li, Guanbin Li, Xiang Wan
- **Comment**: MICCAI 2022, code:https://github.com/lhaof/ASA
- **Journal**: None
- **Summary**: Self-supervised learning methods based on image patch reconstruction have witnessed great success in training auto-encoders, whose pre-trained weights can be transferred to fine-tune other downstream tasks of image understanding. However, existing methods seldom study the various importance of reconstructed patches and the symmetry of anatomical structures, when they are applied to 3D medical images. In this paper we propose a novel Attentive Symmetric Auto-encoder (ASA) based on Vision Transformer (ViT) for 3D brain MRI segmentation tasks. We conjecture that forcing the auto-encoder to recover informative image regions can harvest more discriminative representations, than to recover smooth image patches. Then we adopt a gradient based metric to estimate the importance of each image patch. In the pre-training stage, the proposed auto-encoder pays more attention to reconstruct the informative patches according to the gradient metrics. Moreover, we resort to the prior of brain structures and develop a Symmetric Position Encoding (SPE) method to better exploit the correlations between long-range but spatially symmetric regions to obtain effective features. Experimental results show that our proposed attentive symmetric auto-encoder outperforms the state-of-the-art self-supervised learning methods and medical image segmentation models on three brain MRI segmentation benchmarks.



### Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2209.08891v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.08891v2)
- **Published**: 2022-09-19 09:48:17+00:00
- **Updated**: 2023-02-13 18:54:07+00:00
- **Authors**: Lukas Struppek, Dominik Hintersdorf, Felix Friedrich, Manuel Brack, Patrick Schramowski, Kristian Kersting
- **Comment**: 34 pages, 23 figures, 3 tables
- **Journal**: None
- **Summary**: Models for text-to-image synthesis, such as DALL-E~2 and Stable Diffusion, have recently drawn a lot of interest from academia and the general public. These models are capable of producing high-quality images that depict a variety of concepts and styles when conditioned on textual descriptions. However, these models adopt cultural characteristics associated with specific Unicode scripts from their vast amount of training data, which may not be immediately apparent. We show that by simply inserting single non-Latin characters in a textual description, common models reflect cultural stereotypes and biases in their generated images. We analyze this behavior both qualitatively and quantitatively, and identify a model's text encoder as the root cause of the phenomenon. Additionally, malicious users or service providers may try to intentionally bias the image generation to create racist stereotypes by replacing Latin characters with similarly-looking characters from non-Latin scripts, so-called homoglyphs. To mitigate such unnoticed script attacks, we propose a novel homoglyph unlearning method to fine-tune a text encoder, making it robust against homoglyph manipulations.



### NeuralMarker: A Framework for Learning General Marker Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2209.08896v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08896v1)
- **Published**: 2022-09-19 10:04:38+00:00
- **Updated**: 2022-09-19 10:04:38+00:00
- **Authors**: Zhaoyang Huang, Xiaokun Pan, Weihong Pan, Weikang Bian, Yan Xu, Ka Chun Cheung, Guofeng Zhang, Hongsheng Li
- **Comment**: Accepted by ToG (SIGGRAPH Asia 2022). Project Page:
  https://drinkingcoder.github.io/publication/neuralmarker/
- **Journal**: None
- **Summary**: We tackle the problem of estimating correspondences from a general marker, such as a movie poster, to an image that captures such a marker. Conventionally, this problem is addressed by fitting a homography model based on sparse feature matching. However, they are only able to handle plane-like markers and the sparse features do not sufficiently utilize appearance information. In this paper, we propose a novel framework NeuralMarker, training a neural network estimating dense marker correspondences under various challenging conditions, such as marker deformation, harsh lighting, etc. Besides, we also propose a novel marker correspondence evaluation method circumstancing annotations on real marker-image pairs and create a new benchmark. We show that NeuralMarker significantly outperforms previous methods and enables new interesting applications, including Augmented Reality (AR) and video editing.



### A model-agnostic approach for generating Saliency Maps to explain inferred decisions of Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2209.08906v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.08906v2)
- **Published**: 2022-09-19 10:28:37+00:00
- **Updated**: 2022-09-27 17:07:59+00:00
- **Authors**: Savvas Karatsiolis, Andreas Kamilaris
- **Comment**: None
- **Journal**: None
- **Summary**: The widespread use of black-box AI models has raised the need for algorithms and methods that explain the decisions made by these models. In recent years, the AI research community is increasingly interested in models' explainability since black-box models take over more and more complicated and challenging tasks. Explainability becomes critical considering the dominance of deep learning techniques for a wide range of applications, including but not limited to computer vision. In the direction of understanding the inference process of deep learning models, many methods that provide human comprehensible evidence for the decisions of AI models have been developed, with the vast majority relying their operation on having access to the internal architecture and parameters of these models (e.g., the weights of neural networks). We propose a model-agnostic method for generating saliency maps that has access only to the output of the model and does not require additional information such as gradients. We use Differential Evolution (DE) to identify which image pixels are the most influential in a model's decision-making process and produce class activation maps (CAMs) whose quality is comparable to the quality of CAMs created with model-specific algorithms. DE-CAM achieves good performance without requiring access to the internal details of the model's architecture at the cost of more computational complexity.



### HVC-Net: Unifying Homography, Visibility, and Confidence Learning for Planar Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2209.08924v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08924v1)
- **Published**: 2022-09-19 11:11:56+00:00
- **Updated**: 2022-09-19 11:11:56+00:00
- **Authors**: Haoxian Zhang, Yonggen Ling
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Robust and accurate planar tracking over a whole video sequence is vitally important for many vision applications. The key to planar object tracking is to find object correspondences, modeled by homography, between the reference image and the tracked image. Existing methods tend to obtain wrong correspondences with changing appearance variations, camera-object relative motions and occlusions. To alleviate this problem, we present a unified convolutional neural network (CNN) model that jointly considers homography, visibility, and confidence. First, we introduce correlation blocks that explicitly account for the local appearance changes and camera-object relative motions as the base of our model. Second, we jointly learn the homography and visibility that links camera-object relative motions with occlusions. Third, we propose a confidence module that actively monitors the estimation quality from the pixel correlation distributions obtained in correlation blocks. All these modules are plugged into a Lucas-Kanade (LK) tracking pipeline to obtain both accurate and robust planar object tracking. Our approach outperforms the state-of-the-art methods on public POT and TMT datasets. Its superior performance is also verified on a real-world application, synthesizing high-quality in-video advertisements.



### HiMFR: A Hybrid Masked Face Recognition Through Face Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2209.08930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08930v1)
- **Published**: 2022-09-19 11:26:49+00:00
- **Updated**: 2022-09-19 11:26:49+00:00
- **Authors**: Md Imran Hosen, Md Baharul Islam
- **Comment**: 7 pages, 6 figures, International Conference on Pattern Recognition
  Workshop: Deep Learning for Visual Detection and Recognition
- **Journal**: None
- **Summary**: To recognize the masked face, one of the possible solutions could be to restore the occluded part of the face first and then apply the face recognition method. Inspired by the recent image inpainting methods, we propose an end-to-end hybrid masked face recognition system, namely HiMFR, consisting of three significant parts: masked face detector, face inpainting, and face recognition. The masked face detector module applies a pretrained Vision Transformer (ViT\_b32) to detect whether faces are covered with masked or not. The inpainting module uses a fine-tune image inpainting model based on a Generative Adversarial Network (GAN) to restore faces. Finally, the hybrid face recognition module based on ViT with an EfficientNetB3 backbone recognizes the faces. We have implemented and evaluated our proposed method on four different publicly available datasets: CelebA, SSDMNV2, MAFA, {Pubfig83} with our locally collected small dataset, namely Face5. Comprehensive experimental results show the efficacy of the proposed HiMFR method with competitive performance. Code is available at https://github.com/mdhosen/HiMFR



### Estimating Brain Age with Global and Local Dependencies
- **Arxiv ID**: http://arxiv.org/abs/2209.08933v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08933v1)
- **Published**: 2022-09-19 11:30:18+00:00
- **Updated**: 2022-09-19 11:30:18+00:00
- **Authors**: Yanwu Yang, Xutao Guo, Zhikai Chang, Chenfei Ye, Yang Xiang, Haiyan Lv, Ting Ma
- **Comment**: None
- **Journal**: None
- **Summary**: The brain age has been proven to be a phenotype of relevance to cognitive performance and brain disease. Achieving accurate brain age prediction is an essential prerequisite for optimizing the predicted brain-age difference as a biomarker. As a comprehensive biological characteristic, the brain age is hard to be exploited accurately with models using feature engineering and local processing such as local convolution and recurrent operations that process one local neighborhood at a time. Instead, Vision Transformers learn global attentive interaction of patch tokens, introducing less inductive bias and modeling long-range dependencies. In terms of this, we proposed a novel network for learning brain age interpreting with global and local dependencies, where the corresponding representations are captured by Successive Permuted Transformer (SPT) and convolution blocks. The SPT brings computation efficiency and locates the 3D spatial information indirectly via continuously encoding 2D slices from different views. Finally, we collect a large cohort of 22645 subjects with ages ranging from 14 to 97 and our network performed the best among a series of deep learning methods, yielding a mean absolute error (MAE) of 2.855 in validation set, and 2.911 in an independent test set.



### 3D Cross-Pseudo Supervision (3D-CPS): A semi-supervised nnU-Net architecture for abdominal organ segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.08939v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08939v2)
- **Published**: 2022-09-19 11:46:43+00:00
- **Updated**: 2022-11-01 16:03:07+00:00
- **Authors**: Yongzhi Huang, Hanwen Zhang, Yan Yan, Haseeb Hassan
- **Comment**: 14 pages, 5 figures
- **Journal**: None
- **Summary**: Large curated datasets are necessary, but annotating medical images is a time-consuming, laborious, and expensive process. Therefore, recent supervised methods are focusing on utilizing a large amount of unlabeled data. However, to do so, is a challenging task. To address this problem, we propose a new 3D Cross-Pseudo Supervision (3D-CPS) method, a semi-supervised network architecture based on nnU-Net with the Cross-Pseudo Supervision method. We design a new nnU-Net based preprocessing. In addition, we set the semi-supervised loss weights to expand linearity with each epoch to prevent the model from low-quality pseudo-labels in the early training process. Our proposed method achieves an average dice similarity coefficient (DSC) of 0.881 and an average normalized surface distance (NSD) of 0.913 on the MICCAI FLARE2022 validation set (20 cases).



### Effective Adaptation in Multi-Task Co-Training for Unified Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2209.08953v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08953v1)
- **Published**: 2022-09-19 12:15:31+00:00
- **Updated**: 2022-09-19 12:15:31+00:00
- **Authors**: Xiwen Liang, Yangxin Wu, Jianhua Han, Hang Xu, Chunjing Xu, Xiaodan Liang
- **Comment**: Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: Aiming towards a holistic understanding of multiple downstream tasks simultaneously, there is a need for extracting features with better transferability. Though many latest self-supervised pre-training methods have achieved impressive performance on various vision tasks under the prevailing pretrain-finetune paradigm, their generalization capacity to multi-task learning scenarios is yet to be explored. In this paper, we extensively investigate the transfer performance of various types of self-supervised methods, e.g., MoCo and SimCLR, on three downstream tasks, including semantic segmentation, drivable area segmentation, and traffic object detection, on the large-scale driving dataset BDD100K. We surprisingly find that their performances are sub-optimal or even lag far behind the single-task baseline, which may be due to the distinctions of training objectives and architectural design lied in the pretrain-finetune paradigm. To overcome this dilemma as well as avoid redesigning the resource-intensive pre-training stage, we propose a simple yet effective pretrain-adapt-finetune paradigm for general multi-task training, where the off-the-shelf pretrained models can be effectively adapted without increasing the training overhead. During the adapt stage, we utilize learnable multi-scale adapters to dynamically adjust the pretrained model weights supervised by multi-task objectives while leaving the pretrained knowledge untouched. Furthermore, we regard the vision-language pre-training model CLIP as a strong complement to the pretrain-adapt-finetune paradigm and propose a novel adapter named LV-Adapter, which incorporates language priors in the multi-task model via task-specific prompting and alignment between visual and textual features.



### Panoramic Vision Transformer for Saliency Detection in 360Â° Videos
- **Arxiv ID**: http://arxiv.org/abs/2209.08956v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08956v1)
- **Published**: 2022-09-19 12:23:34+00:00
- **Updated**: 2022-09-19 12:23:34+00:00
- **Authors**: Heeseung Yun, Sehun Lee, Gunhee Kim
- **Comment**: Published to ECCV2022
- **Journal**: None
- **Summary**: 360$^\circ$ video saliency detection is one of the challenging benchmarks for 360$^\circ$ video understanding since non-negligible distortion and discontinuity occur in the projection of any format of 360$^\circ$ videos, and capture-worthy viewpoint in the omnidirectional sphere is ambiguous by nature. We present a new framework named Panoramic Vision Transformer (PAVER). We design the encoder using Vision Transformer with deformable convolution, which enables us not only to plug pretrained models from normal videos into our architecture without additional modules or finetuning but also to perform geometric approximation only once, unlike previous deep CNN-based approaches. Thanks to its powerful encoder, PAVER can learn the saliency from three simple relative relations among local patch features, outperforming state-of-the-art models for the Wild360 benchmark by large margins without supervision or auxiliary information like class activation. We demonstrate the utility of our saliency prediction model with the omnidirectional video quality assessment task in VQA-ODV, where we consistently improve performance without any form of supervision, including head movement.



### Latent Plans for Task-Agnostic Offline Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.08959v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.08959v1)
- **Published**: 2022-09-19 12:27:15+00:00
- **Updated**: 2022-09-19 12:27:15+00:00
- **Authors**: Erick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka Boedecker, Wolfram Burgard
- **Comment**: CoRL 2022. Project website: http://tacorl.cs.uni-freiburg.de/
- **Journal**: None
- **Summary**: Everyday tasks of long-horizon and comprising a sequence of multiple implicit subtasks still impose a major challenge in offline robot control. While a number of prior methods aimed to address this setting with variants of imitation and offline reinforcement learning, the learned behavior is typically narrow and often struggles to reach configurable long-horizon goals. As both paradigms have complementary strengths and weaknesses, we propose a novel hierarchical approach that combines the strengths of both methods to learn task-agnostic long-horizon policies from high-dimensional camera observations. Concretely, we combine a low-level policy that learns latent skills via imitation learning and a high-level policy learned from offline reinforcement learning for skill-chaining the latent behavior priors. Experiments in various simulated and real robot control tasks show that our formulation enables producing previously unseen combinations of skills to reach temporally extended goals by "stitching" together latent skills through goal chaining with an order-of-magnitude improvement in performance upon state-of-the-art baselines. We even learn one multi-task visuomotor policy for 25 distinct manipulation tasks in the real world which outperforms both imitation learning and offline reinforcement learning techniques.



### Adversarial Color Projection: A Projector-based Physical Attack to DNNs
- **Arxiv ID**: http://arxiv.org/abs/2209.09652v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09652v2)
- **Published**: 2022-09-19 12:27:32+00:00
- **Updated**: 2023-05-23 11:56:41+00:00
- **Authors**: Chengyin Hu, Weiwen Shi, Ling Tian
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2209.02430
- **Journal**: None
- **Summary**: Recent research has demonstrated that deep neural networks (DNNs) are vulnerable to adversarial perturbations. Therefore, it is imperative to evaluate the resilience of advanced DNNs to adversarial attacks. However, traditional methods that use stickers as physical perturbations to deceive classifiers face challenges in achieving stealthiness and are susceptible to printing loss. Recently, advancements in physical attacks have utilized light beams, such as lasers, to perform attacks, where the optical patterns generated are artificial rather than natural. In this work, we propose a black-box projector-based physical attack, referred to as adversarial color projection (AdvCP), which manipulates the physical parameters of color projection to perform an adversarial attack. We evaluate our approach on three crucial criteria: effectiveness, stealthiness, and robustness. In the digital environment, we achieve an attack success rate of 97.60% on a subset of ImageNet, while in the physical environment, we attain an attack success rate of 100% in the indoor test and 82.14% in the outdoor test. The adversarial samples generated by AdvCP are compared with baseline samples to demonstrate the stealthiness of our approach. When attacking advanced DNNs, experimental results show that our method can achieve more than 85% attack success rate in all cases, which verifies the robustness of AdvCP. Finally, we consider the potential threats posed by AdvCP to future vision-based systems and applications and suggest some ideas for light-based physical attacks.



### Adversarial Catoptric Light: An Effective, Stealthy and Robust Physical-World Attack to DNNs
- **Arxiv ID**: http://arxiv.org/abs/2209.11739v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.11739v2)
- **Published**: 2022-09-19 12:33:46+00:00
- **Updated**: 2023-05-23 14:05:08+00:00
- **Authors**: Chengyin Hu, Weiwen Shi
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2209.09652,
  arXiv:2209.02430
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have demonstrated exceptional success across various tasks, underscoring the need to evaluate the robustness of advanced DNNs. However, traditional methods using stickers as physical perturbations to deceive classifiers present challenges in achieving stealthiness and suffer from printing loss. Recent advancements in physical attacks have utilized light beams such as lasers and projectors to perform attacks, where the optical patterns generated are artificial rather than natural. In this study, we introduce a novel physical attack, adversarial catoptric light (AdvCL), where adversarial perturbations are generated using a common natural phenomenon, catoptric light, to achieve stealthy and naturalistic adversarial attacks against advanced DNNs in a black-box setting. We evaluate the proposed method in three aspects: effectiveness, stealthiness, and robustness. Quantitative results obtained in simulated environments demonstrate the effectiveness of the proposed method, and in physical scenarios, we achieve an attack success rate of 83.5%, surpassing the baseline. We use common catoptric light as a perturbation to enhance the stealthiness of the method and make physical samples appear more natural. Robustness is validated by successfully attacking advanced and robust DNNs with a success rate over 80% in all cases. Additionally, we discuss defense strategy against AdvCL and put forward some light-based physical attacks.



### An Overview on the Generation and Detection of Synthetic and Manipulated Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/2209.08984v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08984v1)
- **Published**: 2022-09-19 13:03:02+00:00
- **Updated**: 2022-09-19 13:03:02+00:00
- **Authors**: Lydia Abady, Edoardo Daniele Cannas, Paolo Bestagini, Benedetta Tondi, Stefano Tubaro, Mauro Barni
- **Comment**: 25 pages, 17 figures, 5 tables, APSIPA 2022
- **Journal**: None
- **Summary**: Due to the reduction of technological costs and the increase of satellites launches, satellite images are becoming more popular and easier to obtain. Besides serving benevolent purposes, satellite data can also be used for malicious reasons such as misinformation. As a matter of fact, satellite images can be easily manipulated relying on general image editing tools. Moreover, with the surge of Deep Neural Networks (DNNs) that can generate realistic synthetic imagery belonging to various domains, additional threats related to the diffusion of synthetically generated satellite images are emerging. In this paper, we review the State of the Art (SOTA) on the generation and manipulation of satellite images. In particular, we focus on both the generation of synthetic satellite imagery from scratch, and the semantic manipulation of satellite images by means of image-transfer technologies, including the transformation of images obtained from one type of sensor to another one. We also describe forensic detection techniques that have been researched so far to classify and detect synthetic image forgeries. While we focus mostly on forensic techniques explicitly tailored to the detection of AI-generated synthetic contents, we also review some methods designed for general splicing detection, which can in principle also be used to spot AI manipulate images



### MSA-GCN:Multiscale Adaptive Graph Convolution Network for Gait Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.08988v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.08988v1)
- **Published**: 2022-09-19 13:07:16+00:00
- **Updated**: 2022-09-19 13:07:16+00:00
- **Authors**: Yunfei Yin, Li Jing, Faliang Huang, Guangchao Yang, Zhuowei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Gait emotion recognition plays a crucial role in the intelligent system. Most of the existing methods recognize emotions by focusing on local actions over time. However, they ignore that the effective distances of different emotions in the time domain are different, and the local actions during walking are quite similar. Thus, emotions should be represented by global states instead of indirect local actions. To address these issues, a novel Multi Scale Adaptive Graph Convolution Network (MSA-GCN) is presented in this work through constructing dynamic temporal receptive fields and designing multiscale information aggregation to recognize emotions. In our model, a adaptive selective spatial-temporal graph convolution is designed to select the convolution kernel dynamically to obtain the soft spatio-temporal features of different emotions. Moreover, a Cross-Scale mapping Fusion Mechanism (CSFM) is designed to construct an adaptive adjacency matrix to enhance information interaction and reduce redundancy. Compared with previous state-of-the-art methods, the proposed method achieves the best performance on two public datasets, improving the mAP by 2\%. We also conduct extensive ablations studies to show the effectiveness of different components in our methods.



### EDO-Net: Learning Elastic Properties of Deformable Objects from Graph Dynamics
- **Arxiv ID**: http://arxiv.org/abs/2209.08996v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.08996v2)
- **Published**: 2022-09-19 13:20:19+00:00
- **Updated**: 2023-08-23 09:31:26+00:00
- **Authors**: Alberta Longhini, Marco Moletta, Alfredo Reichlin, Michael C. Welle, David Held, Zackory Erickson, Danica Kragic
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of learning graph dynamics of deformable objects that generalizes to unknown physical properties. Our key insight is to leverage a latent representation of elastic physical properties of cloth-like deformable objects that can be extracted, for example, from a pulling interaction. In this paper we propose EDO-Net (Elastic Deformable Object - Net), a model of graph dynamics trained on a large variety of samples with different elastic properties that does not rely on ground-truth labels of the properties. EDO-Net jointly learns an adaptation module, and a forward-dynamics module. The former is responsible for extracting a latent representation of the physical properties of the object, while the latter leverages the latent representation to predict future states of cloth-like objects represented as graphs. We evaluate EDO-Net both in simulation and real world, assessing its capabilities of: 1) generalizing to unknown physical properties, 2) transferring the learned representation to new downstream tasks.



### MoVQ: Modulating Quantized Vectors for High-Fidelity Image Generation
- **Arxiv ID**: http://arxiv.org/abs/2209.09002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09002v1)
- **Published**: 2022-09-19 13:26:51+00:00
- **Updated**: 2022-09-19 13:26:51+00:00
- **Authors**: Chuanxia Zheng, Long Tung Vuong, Jianfei Cai, Dinh Phung
- **Comment**: None
- **Journal**: None
- **Summary**: Although two-stage Vector Quantized (VQ) generative models allow for synthesizing high-fidelity and high-resolution images, their quantization operator encodes similar patches within an image into the same index, resulting in a repeated artifact for similar adjacent regions using existing decoder architectures. To address this issue, we propose to incorporate the spatially conditional normalization to modulate the quantized vectors so as to insert spatially variant information to the embedded index maps, encouraging the decoder to generate more photorealistic images. Moreover, we use multichannel quantization to increase the recombination capability of the discrete codes without increasing the cost of model and codebook. Additionally, to generate discrete tokens at the second stage, we adopt a Masked Generative Image Transformer (MaskGIT) to learn an underlying prior distribution in the compressed latent space, which is much faster than the conventional autoregressive model. Experiments on two benchmark datasets demonstrate that our proposed modulated VQGAN is able to greatly improve the reconstructed image quality as well as provide high-fidelity image generation.



### EcoFormer: Energy-Saving Attention with Linear Complexity
- **Arxiv ID**: http://arxiv.org/abs/2209.09004v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2209.09004v3)
- **Published**: 2022-09-19 13:28:32+00:00
- **Updated**: 2023-03-20 04:49:10+00:00
- **Authors**: Jing Liu, Zizheng Pan, Haoyu He, Jianfei Cai, Bohan Zhuang
- **Comment**: NeurIPS 2022 camera ready; First two authors contributed equally
- **Journal**: None
- **Summary**: Transformer is a transformative framework that models sequential data and has achieved remarkable performance on a wide range of tasks, but with high computational and energy cost. To improve its efficiency, a popular choice is to compress the models via binarization which constrains the floating-point values into binary ones to save resource consumption owing to cheap bitwise operations significantly. However, existing binarization methods only aim at minimizing the information loss for the input distribution statistically, while ignoring the pairwise similarity modeling at the core of the attention. To this end, we propose a new binarization paradigm customized to high-dimensional softmax attention via kernelized hashing, called EcoFormer, to map the original queries and keys into low-dimensional binary codes in Hamming space. The kernelized hash functions are learned to match the ground-truth similarity relations extracted from the attention map in a self-supervised way. Based on the equivalence between the inner product of binary codes and the Hamming distance as well as the associative property of matrix multiplication, we can approximate the attention in linear complexity by expressing it as a dot-product of binary codes. Moreover, the compact binary representations of queries and keys enable us to replace most of the expensive multiply-accumulate operations in attention with simple accumulations to save considerable on-chip energy footprint on edge devices. Extensive experiments on both vision and language tasks show that EcoFormer consistently achieves comparable performance with standard attentions while consuming much fewer resources. For example, based on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy footprint reduction with only a 0.33% performance drop compared to the standard attention. Code is available at https://github.com/ziplab/EcoFormer.



### BareSkinNet: De-makeup and De-lighting via 3D Face Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2209.09029v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.09029v1)
- **Published**: 2022-09-19 14:02:03+00:00
- **Updated**: 2022-09-19 14:02:03+00:00
- **Authors**: Xingchao Yang, Takafumi Taketomi
- **Comment**: accepted at PG2022
- **Journal**: None
- **Summary**: We propose BareSkinNet, a novel method that simultaneously removes makeup and lighting influences from the face image. Our method leverages a 3D morphable model and does not require a reference clean face image or a specified light condition. By combining the process of 3D face reconstruction, we can easily obtain 3D geometry and coarse 3D textures. Using this information, we can infer normalized 3D face texture maps (diffuse, normal, roughness, and specular) by an image-translation network. Consequently, reconstructed 3D face textures without undesirable information will significantly benefit subsequent processes, such as re-lighting or re-makeup. In experiments, we show that BareSkinNet outperforms state-of-the-art makeup removal methods. In addition, our method is remarkably helpful in removing makeup to generate consistent high-fidelity texture maps, which makes it extendable to many realistic face generation applications. It can also automatically build graphic assets of face makeup images before and after with corresponding 3D data. This will assist artists in accelerating their work, such as 3D makeup avatar creation.



### Fairness in Face Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.09035v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09035v1)
- **Published**: 2022-09-19 14:12:09+00:00
- **Updated**: 2022-09-19 14:12:09+00:00
- **Authors**: Meiling Fang, Wufei Yang, Arjan Kuijper, Vitomir Struc, Naser Damer
- **Comment**: None
- **Journal**: None
- **Summary**: Face presentation attack detection (PAD) is critical to secure face recognition (FR) applications from presentation attacks. FR performance has been shown to be unfair to certain demographic and non-demographic groups. However, the fairness of face PAD is an understudied issue, mainly due to the lack of appropriately annotated data. To address this issue, this work first presents a Combined Attribute Annotated PAD Dataset (CAAD-PAD) by combining several well-known PAD datasets where we provide seven human-annotated attribute labels. This work then comprehensively analyses the fairness of a set of face PADs and its relation to the nature of training data and the Operational Decision Threshold Assignment (ODTA) on different data groups by studying four face PAD approaches on our CAAD-PAD. To simultaneously represent both the PAD fairness and the absolute PAD performance, we introduce a novel metric, namely the Accuracy Balanced Fairness (ABF). Extensive experiments on CAAD-PAD show that the training data and ODTA induce unfairness on gender, occlusion, and other attribute groups. Based on these analyses, we propose a data augmentation method, FairSWAP, which aims to disrupt the identity/semantic information and guide models to mine attack cues rather than attribute-related information. Detailed experimental results demonstrate that FairSWAP generally enhances both the PAD performance and the fairness of face PAD.



### Structure-Aware 3D VR Sketch to 3D Shape Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2209.09043v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09043v1)
- **Published**: 2022-09-19 14:29:26+00:00
- **Updated**: 2022-09-19 14:29:26+00:00
- **Authors**: Ling Luo, Yulia Gryaditskaya, Tao Xiang, Yi-Zhe Song
- **Comment**: Accepted by 3DV 2022
- **Journal**: None
- **Summary**: We study the practical task of fine-grained 3D-VR-sketch-based 3D shape retrieval. This task is of particular interest as 2D sketches were shown to be effective queries for 2D images. However, due to the domain gap, it remains hard to achieve strong performance in 3D shape retrieval from 2D sketches. Recent work demonstrated the advantage of 3D VR sketching on this task. In our work, we focus on the challenge caused by inherent inaccuracies in 3D VR sketches. We observe that retrieval results obtained with a triplet loss with a fixed margin value, commonly used for retrieval tasks, contain many irrelevant shapes and often just one or few with a similar structure to the query. To mitigate this problem, we for the first time draw a connection between adaptive margin values and shape similarities. In particular, we propose to use a triplet loss with an adaptive margin value driven by a "fitting gap", which is the similarity of two shapes under structure-preserving deformations. We also conduct a user study which confirms that this fitting gap is indeed a suitable criterion to evaluate the structural similarity of shapes. Furthermore, we introduce a dataset of 202 VR sketches for 202 3D shapes drawn from memory rather than from observation. The code and data are available at https://github.com/Rowl1ng/Structure-Aware-VR-Sketch-Shape-Retrieval.



### Deep Metric Learning with Chance Constraints
- **Arxiv ID**: http://arxiv.org/abs/2209.09060v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.09060v2)
- **Published**: 2022-09-19 14:50:48+00:00
- **Updated**: 2023-01-28 22:23:40+00:00
- **Authors**: Yeti Z. Gurbuz, Ogul Can, A. Aydin Alatan
- **Comment**: Preprint. Under review
- **Journal**: None
- **Summary**: Deep metric learning (DML) aims to minimize empirical expected loss of the pairwise intra-/inter- class proximity violations in the embedding image. We relate DML to feasibility problem of finite chance constraints. We show that minimizer of proxy-based DML satisfies certain chance constraints, and that the worst case generalization performance of the proxy-based methods can be characterized by the radius of the smallest ball around a class proxy to cover the entire domain of the corresponding class samples, suggesting multiple proxies per class helps performance. To provide a scalable algorithm as well as exploiting more proxies, we consider the chance constraints implied by the minimizers of proxy-based DML instances and reformulate DML as finding a feasible point in intersection of such constraints, resulting in a problem to be approximately solved by iterative projections. Simply put, we repeatedly train a regularized proxy-based loss and re-initialize the proxies with the embeddings of the deliberately selected new samples. We apply our method with the well-accepted losses and evaluate on four popular benchmark datasets for image retrieval. Outperforming state-of-the-art, our method consistently improves the performance of the applied losses. Code is available at: https://github.com/yetigurbuz/ccp-dml



### T3VIP: Transformation-based 3D Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/2209.11693v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.11693v1)
- **Published**: 2022-09-19 15:01:09+00:00
- **Updated**: 2022-09-19 15:01:09+00:00
- **Authors**: Iman Nematollahi, Erick Rosete-Beas, Seyed Mahdi B. Azad, Raghu Rajan, Frank Hutter, Wolfram Burgard
- **Comment**: Accepted at the 2022 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS)
- **Journal**: None
- **Summary**: For autonomous skill acquisition, robots have to learn about the physical rules governing the 3D world dynamics from their own past experience to predict and reason about plausible future outcomes. To this end, we propose a transformation-based 3D video prediction (T3VIP) approach that explicitly models the 3D motion by decomposing a scene into its object parts and predicting their corresponding rigid transformations. Our model is fully unsupervised, captures the stochastic nature of the real world, and the observational cues in image and point cloud domains constitute its learning signals. To fully leverage all the 2D and 3D observational signals, we equip our model with automatic hyperparameter optimization (HPO) to interpret the best way of learning from them. To the best of our knowledge, our model is the first generative model that provides an RGB-D video prediction of the future for a static camera. Our extensive evaluation with simulated and real-world datasets demonstrates that our formulation leads to interpretable 3D models that predict future depth videos while achieving on-par performance with 2D models on RGB video prediction. Moreover, we demonstrate that our model outperforms 2D baselines on visuomotor control. Videos, code, dataset, and pre-trained models are available at http://t3vip.cs.uni-freiburg.de.



### Audio-Visual Fusion for Emotion Recognition in the Valence-Arousal Space Using Joint Cross-Attention
- **Arxiv ID**: http://arxiv.org/abs/2209.09068v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09068v1)
- **Published**: 2022-09-19 15:01:55+00:00
- **Updated**: 2022-09-19 15:01:55+00:00
- **Authors**: R Gnana Praveen, Eric Granger, Patrick Cardinal
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2203.14779,
  arXiv:2111.05222
- **Journal**: None
- **Summary**: Automatic emotion recognition (ER) has recently gained lot of interest due to its potential in many real-world applications. In this context, multimodal approaches have been shown to improve performance (over unimodal approaches) by combining diverse and complementary sources of information, providing some robustness to noisy and missing modalities. In this paper, we focus on dimensional ER based on the fusion of facial and vocal modalities extracted from videos, where complementary audio-visual (A-V) relationships are explored to predict an individual's emotional states in valence-arousal space. Most state-of-the-art fusion techniques rely on recurrent networks or conventional attention mechanisms that do not effectively leverage the complementary nature of A-V modalities. To address this problem, we introduce a joint cross-attentional model for A-V fusion that extracts the salient features across A-V modalities, that allows to effectively leverage the inter-modal relationships, while retaining the intra-modal relationships. In particular, it computes the cross-attention weights based on correlation between the joint feature representation and that of the individual modalities. By deploying the joint A-V feature representation into the cross-attention module, it helps to simultaneously leverage both the intra and inter modal relationships, thereby significantly improving the performance of the system over the vanilla cross-attention module. The effectiveness of our proposed approach is validated experimentally on challenging videos from the RECOLA and AffWild2 datasets. Results indicate that our joint cross-attentional A-V fusion model provides a cost-effective solution that can outperform state-of-the-art approaches, even when the modalities are noisy or absent.



### SOCRATES: A Stereo Camera Trap for Monitoring of Biodiversity
- **Arxiv ID**: http://arxiv.org/abs/2209.09070v2
- **DOI**: 10.3390/s22239082
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.09070v2)
- **Published**: 2022-09-19 15:03:35+00:00
- **Updated**: 2022-10-13 20:46:13+00:00
- **Authors**: Timm Haucke, Hjalmar S. KÃ¼hl, Volker Steinhage
- **Comment**: None
- **Journal**: Sensors 2022, 22(23), 9082
- **Summary**: The development and application of modern technology is an essential basis for the efficient monitoring of species in natural habitats and landscapes to trace the development of ecosystems, species communities, and populations, and to analyze reasons of changes. For estimating animal abundance using methods such as camera trap distance sampling, spatial information of natural habitats in terms of 3D (three-dimensional) measurements is crucial. Additionally, 3D information improves the accuracy of animal detection using camera trapping. This study presents a novel approach to 3D camera trapping featuring highly optimized hardware and software. This approach employs stereo vision to infer 3D information of natural habitats and is designated as StereO CameRA Trap for monitoring of biodivErSity (SOCRATES). A comprehensive evaluation of SOCRATES shows not only a $3.23\%$ improvement in animal detection (bounding box $\text{mAP}_{75}$) but also its superior applicability for estimating animal abundance using camera trap distance sampling. The software and documentation of SOCRATES is provided at https://github.com/timmh/socrates



### Multilevel Robustness for 2D Vector Field Feature Tracking, Selection, and Comparison
- **Arxiv ID**: http://arxiv.org/abs/2209.11708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11708v1)
- **Published**: 2022-09-19 15:22:58+00:00
- **Updated**: 2022-09-19 15:22:58+00:00
- **Authors**: Lin Yan, Paul Aaron Ullrich, Luke P. Van Roekel, Bei Wang, Hanqi Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Critical point tracking is a core topic in scientific visualization for understanding the dynamic behavior of time-varying vector field data. The topological notion of robustness has been introduced recently to quantify the structural stability of critical points, that is, the robustness of a critical point is the minimum amount of perturbation to the vector field necessary to cancel it. A theoretical basis has been established previously that relates critical point tracking with the notion of robustness, in particular, critical points could be tracked based on their closeness in stability, measured by robustness, instead of just distance proximities within the domain. However, in practice, the computation of classic robustness may produce artifacts when a critical point is close to the boundary of the domain; thus, we do not have a complete picture of the vector field behavior within its local neighborhood. To alleviate these issues, we introduce a multilevel robustness framework for the study of 2D time-varying vector fields. We compute the robustness of critical points across varying neighborhoods to capture the multiscale nature of the data and to mitigate the boundary effect suffered by the classic robustness computation. We demonstrate via experiments that such a new notion of robustness can be combined seamlessly with existing feature tracking algorithms to improve the visual interpretability of vector fields in terms of feature tracking, selection, and comparison for large-scale scientific simulations. We observe, for the first time, that the minimum multilevel robustness is highly correlated with physical quantities used by domain scientists in studying a real-world tropical cyclone dataset. Such observation helps to increase the physical interpretability of robustness.



### DeePhy: On Deepfake Phylogeny
- **Arxiv ID**: http://arxiv.org/abs/2209.09111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09111v1)
- **Published**: 2022-09-19 15:30:33+00:00
- **Updated**: 2022-09-19 15:30:33+00:00
- **Authors**: Kartik Narayan, Harsh Agarwal, Kartik Thakral, Surbhi Mittal, Mayank Vatsa, Richa Singh
- **Comment**: Accepted at 2022, International Joint Conference on Biometrics (IJCB
  2022)
- **Journal**: None
- **Summary**: Deepfake refers to tailored and synthetically generated videos which are now prevalent and spreading on a large scale, threatening the trustworthiness of the information available online. While existing datasets contain different kinds of deepfakes which vary in their generation technique, they do not consider progression of deepfakes in a "phylogenetic" manner. It is possible that an existing deepfake face is swapped with another face. This process of face swapping can be performed multiple times and the resultant deepfake can be evolved to confuse the deepfake detection algorithms. Further, many databases do not provide the employed generative model as target labels. Model attribution helps in enhancing the explainability of the detection results by providing information on the generative model employed. In order to enable the research community to address these questions, this paper proposes DeePhy, a novel Deepfake Phylogeny dataset which consists of 5040 deepfake videos generated using three different generation techniques. There are 840 videos of one-time swapped deepfakes, 2520 videos of two-times swapped deepfakes and 1680 videos of three-times swapped deepfakes. With over 30 GBs in size, the database is prepared in over 1100 hours using 18 GPUs of 1,352 GB cumulative memory. We also present the benchmark on DeePhy dataset using six deepfake detection algorithms. The results highlight the need to evolve the research of model attribution of deepfakes and generalize the process over a variety of deepfake generation techniques. The database is available at: http://iab-rubric.org/deephy-database



### A Closer Look at Novel Class Discovery from the Labeled Set
- **Arxiv ID**: http://arxiv.org/abs/2209.09120v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.09120v4)
- **Published**: 2022-09-19 15:41:44+00:00
- **Updated**: 2023-01-27 09:12:39+00:00
- **Authors**: Ziyun Li, Jona Otholt, Ben Dai, Di hu, Christoph Meinel, Haojin Yang
- **Comment**: Workshop on Distribution Shifts, 36th Conference on Neural
  Information Processing Systems (NeurIPS 2022)
- **Journal**: None
- **Summary**: Novel class discovery (NCD) aims to infer novel categories in an unlabeled dataset leveraging prior knowledge of a labeled set comprising disjoint but related classes. Existing research focuses primarily on utilizing the labeled set at the methodological level, with less emphasis on the analysis of the labeled set itself. Thus, in this paper, we rethink novel class discovery from the labeled set and focus on two core questions: (i) Given a specific unlabeled set, what kind of labeled set can best support novel class discovery? (ii) A fundamental premise of NCD is that the labeled set must be related to the unlabeled set, but how can we measure this relation? For (i), we propose and substantiate the hypothesis that NCD could benefit more from a labeled set with a large degree of semantic similarity to the unlabeled set. Specifically, we establish an extensive and large-scale benchmark with varying degrees of semantic similarity between labeled/unlabeled datasets on ImageNet by leveraging its hierarchical class structure. As a sharp contrast, the existing NCD benchmarks are developed based on labeled sets with different number of categories and images, and completely ignore the semantic relation. For (ii), we introduce a mathematical definition for quantifying the semantic similarity between labeled and unlabeled sets. In addition, we use this metric to confirm the validity of our proposed benchmark and demonstrate that it highly correlates with NCD performance. Furthermore, without quantitative analysis, previous works commonly believe that label information is always beneficial. However, counterintuitively, our experimental results show that using labels may lead to sub-optimal outcomes in low-similarity settings.



### Efficient approach of using CNN based pretrained model in Bangla handwritten digit recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.13005v1
- **DOI**: 10.1007/978-981-19-9819-5_50
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.13005v1)
- **Published**: 2022-09-19 15:58:53+00:00
- **Updated**: 2022-09-19 15:58:53+00:00
- **Authors**: Muntarin Islam, Shabbir Ahmed Shuvo, Musarrat Saberin Nipun, Rejwan Bin Sulaiman, Jannatul Nayeem, Zubaer Haque, Md Mostak Shaikh, Md Sakib Ullah Sourav
- **Comment**: None
- **Journal**: Computational Vision and Bio-Inspired Computing. Advances in
  Intelligent Systems and Computing, vol 1439.(2023)
- **Summary**: Due to digitalization in everyday life, the need for automatically recognizing handwritten digits is increasing. Handwritten digit recognition is essential for numerous applications in various industries. Bengali ranks the fifth largest language in the world with 265 million speakers (Native and non-native combined) and 4 percent of the world population speaks Bengali. Due to the complexity of Bengali writing in terms of variety in shape, size, and writing style, researchers did not get better accuracy using Supervised machine learning algorithms to date. Moreover, fewer studies have been done on Bangla handwritten digit recognition (BHwDR). In this paper, we proposed a novel CNN-based pre-trained handwritten digit recognition model which includes Resnet-50, Inception-v3, and EfficientNetB0 on NumtaDB dataset of 17 thousand instances with 10 classes.. The Result outperformed the performance of other models to date with 97% accuracy in the 10-digit classes. Furthermore, we have evaluated the result or our model with other research studies while suggesting future study



### Meteorological Satellite Images Prediction Based on Deep Multi-scales Extrapolation Fusion
- **Arxiv ID**: http://arxiv.org/abs/2209.11682v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11682v1)
- **Published**: 2022-09-19 16:00:17+00:00
- **Updated**: 2022-09-19 16:00:17+00:00
- **Authors**: Fang Huang, Wencong Cheng, PanFeng Wang, ZhiGang Wang, HongHong He
- **Comment**: None
- **Journal**: None
- **Summary**: Meteorological satellite imagery is critical for meteorologists. The data have played an important role in monitoring and analyzing weather and climate changes. However, satellite imagery is a kind of observation data and exists a significant time delay when transmitting the data back to Earth. It is important to make accurate predictions for meteorological satellite images, especially the nowcasting prediction up to 2 hours ahead. In recent years, there has been growing interest in the research of nowcasting prediction applications of weather radar images based on deep learning. Compared to the weather radar images prediction problem, the main challenge for meteorological satellite images prediction is the large-scale observation areas and therefore the large sizes of the observation products. Here we present a deep multi-scales extrapolation fusion method, to address the challenge of the meteorological satellite images nowcasting prediction. First, we downsample the original satellite images dataset with large size to several images datasets with smaller resolutions, then we use a deep spatiotemporal sequences prediction method to generate the multi-scales prediction images with different resolutions separately. Second, we fuse the multi-scales prediction results to the targeting prediction images with the original size by a conditional generative adversarial network. The experiments based on the FY-4A meteorological satellite data show that the proposed method can generate realistic prediction images that effectively capture the evolutions of the weather systems in detail. We believe that the general idea of this work can be potentially applied to other spatiotemporal sequence prediction tasks with a large size.



### ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver Distraction Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.09178v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09178v3)
- **Published**: 2022-09-19 16:56:51+00:00
- **Updated**: 2023-05-13 02:51:53+00:00
- **Authors**: Yunsheng Ma, Ziran Wang
- **Comment**: Accepted at the 2023 IEEE Intelligent Vehicles Symposium (IV)
- **Journal**: None
- **Summary**: Ensuring traffic safety and mitigating accidents in modern driving is of paramount importance, and computer vision technologies have the potential to significantly contribute to this goal. This paper presents a multi-modal Vision Transformer for Driver Distraction Detection (termed ViT-DD), which incorporates inductive information from training signals related to both distraction detection and driver emotion recognition. Additionally, a self-learning algorithm is developed, allowing for the seamless integration of driver data without emotion labels into the multi-task training process of ViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existing state-of-the-art methods for driver distraction detection by 6.5\% and 0.9\% on the SFDDD and AUCDD datasets, respectively. To support reproducibility and foster further advancements in this critical research area, the source code for this approach is made publicly available at https://github.com/PurdueDigitalTwin/ViT-DD.



### Neural Collapse with Normalized Features: A Geometric Analysis over the Riemannian Manifold
- **Arxiv ID**: http://arxiv.org/abs/2209.09211v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, eess.SP, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2209.09211v2)
- **Published**: 2022-09-19 17:26:32+00:00
- **Updated**: 2023-03-07 21:03:26+00:00
- **Authors**: Can Yaras, Peng Wang, Zhihui Zhu, Laura Balzano, Qing Qu
- **Comment**: The first two authors contributed to this work equally; 38 pages, 13
  figures. Accepted at NeurIPS'22
- **Journal**: None
- **Summary**: When training overparameterized deep networks for classification tasks, it has been widely observed that the learned features exhibit a so-called "neural collapse" phenomenon. More specifically, for the output features of the penultimate layer, for each class the within-class features converge to their means, and the means of different classes exhibit a certain tight frame structure, which is also aligned with the last layer's classifier. As feature normalization in the last layer becomes a common practice in modern representation learning, in this work we theoretically justify the neural collapse phenomenon for normalized features. Based on an unconstrained feature model, we simplify the empirical loss function in a multi-class classification task into a nonconvex optimization problem over the Riemannian manifold by constraining all features and classifiers over the sphere. In this context, we analyze the nonconvex landscape of the Riemannian optimization problem over the product of spheres, showing a benign global landscape in the sense that the only global minimizers are the neural collapse solutions while all other critical points are strict saddles with negative curvature. Experimental results on practical deep networks corroborate our theory and demonstrate that better representations can be learned faster via feature normalization.



### Deep Variation Prior: Joint Image Denoising and Noise Variance Estimation without Clean Data
- **Arxiv ID**: http://arxiv.org/abs/2209.09214v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.09214v1)
- **Published**: 2022-09-19 17:29:32+00:00
- **Updated**: 2022-09-19 17:29:32+00:00
- **Authors**: Rihuan Ke
- **Comment**: None
- **Journal**: None
- **Summary**: With recent deep learning based approaches showing promising results in removing noise from images, the best denoising performance has been reported in a supervised learning setup that requires a large set of paired noisy images and ground truth for training. The strong data requirement can be mitigated by unsupervised learning techniques, however, accurate modelling of images or noise variance is still crucial for high-quality solutions. The learning problem is ill-posed for unknown noise distributions. This paper investigates the tasks of image denoising and noise variance estimation in a single, joint learning framework. To address the ill-posedness of the problem, we present deep variation prior (DVP), which states that the variation of a properly learnt denoiser with respect to the change of noise satisfies some smoothness properties, as a key criterion for good denoisers. Building upon DVP, an unsupervised deep learning framework, that simultaneously learns a denoiser and estimates noise variances, is developed. Our method does not require any clean training images or an external step of noise estimation, and instead, approximates the minimum mean squared error denoisers using only a set of noisy images. With the two underlying tasks being considered in a single framework, we allow them to be optimised for each other. The experimental results show a denoising quality comparable to that of supervised learning and accurate noise variance estimates.



### 3D-PL: Domain Adaptive Depth Estimation with 3D-aware Pseudo-Labeling
- **Arxiv ID**: http://arxiv.org/abs/2209.09231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09231v1)
- **Published**: 2022-09-19 17:54:17+00:00
- **Updated**: 2022-09-19 17:54:17+00:00
- **Authors**: Yu-Ting Yen, Chia-Ni Lu, Wei-Chen Chiu, Yi-Hsuan Tsai
- **Comment**: Accepted in ECCV 2022. Project page:
  https://ccc870206.github.io/3D-PL/
- **Journal**: None
- **Summary**: For monocular depth estimation, acquiring ground truths for real data is not easy, and thus domain adaptation methods are commonly adopted using the supervised synthetic data. However, this may still incur a large domain gap due to the lack of supervision from the real data. In this paper, we develop a domain adaptation framework via generating reliable pseudo ground truths of depth from real data to provide direct supervisions. Specifically, we propose two mechanisms for pseudo-labeling: 1) 2D-based pseudo-labels via measuring the consistency of depth predictions when images are with the same content but different styles; 2) 3D-aware pseudo-labels via a point cloud completion network that learns to complete the depth values in the 3D space, thus providing more structural information in a scene to refine and generate more reliable pseudo-labels. In experiments, we show that our pseudo-labeling methods improve depth estimation in various settings, including the usage of stereo pairs during training. Furthermore, the proposed method performs favorably against several state-of-the-art unsupervised domain adaptation approaches in real-world datasets.



### Real-time Online Video Detection with Temporal Smoothing Transformers
- **Arxiv ID**: http://arxiv.org/abs/2209.09236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09236v1)
- **Published**: 2022-09-19 17:59:02+00:00
- **Updated**: 2022-09-19 17:59:02+00:00
- **Authors**: Yue Zhao, Philipp KrÃ¤henbÃ¼hl
- **Comment**: ECCV 2022; Code available at
  https://github.com/zhaoyue-zephyrus/TeSTra
- **Journal**: None
- **Summary**: Streaming video recognition reasons about objects and their actions in every frame of a video. A good streaming recognition model captures both long-term dynamics and short-term changes of video. Unfortunately, in most existing methods, the computational complexity grows linearly or quadratically with the length of the considered dynamics. This issue is particularly pronounced in transformer-based architectures. To address this issue, we reformulate the cross-attention in a video transformer through the lens of kernel and apply two kinds of temporal smoothing kernel: A box kernel or a Laplace kernel. The resulting streaming attention reuses much of the computation from frame to frame, and only requires a constant time update each frame. Based on this idea, we build TeSTra, a Temporal Smoothing Transformer, that takes in arbitrarily long inputs with constant caching and computing overhead. Specifically, it runs $6\times$ faster than equivalent sliding-window based transformers with 2,048 frames in a streaming setting. Furthermore, thanks to the increased temporal span, TeSTra achieves state-of-the-art results on THUMOS'14 and EPIC-Kitchen-100, two standard online action detection and action anticipation datasets. A real-time version of TeSTra outperforms all but one prior approaches on the THUMOS'14 dataset.



### A Simple and Powerful Global Optimization for Unsupervised Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.09341v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09341v2)
- **Published**: 2022-09-19 20:41:26+00:00
- **Updated**: 2022-10-19 14:45:08+00:00
- **Authors**: Georgy Ponimatkin, Nermin Samet, Yang Xiao, Yuming Du, Renaud Marlet, Vincent Lepetit
- **Comment**: Accepted to the IEEE Winter Conference on Applications of Computer
  Vision (WACV) 2023
- **Journal**: None
- **Summary**: We propose a simple, yet powerful approach for unsupervised object segmentation in videos. We introduce an objective function whose minimum represents the mask of the main salient object over the input sequence. It only relies on independent image features and optical flows, which can be obtained using off-the-shelf self-supervised methods. It scales with the length of the sequence with no need for superpixels or sparsification, and it generalizes to different datasets without any specific training. This objective function can actually be derived from a form of spectral clustering applied to the entire video. Our method achieves on-par performance with the state of the art on standard benchmarks (DAVIS2016, SegTrack-v2, FBMS59), while being conceptually and practically much simpler. Code is available at https://ponimatkin.github.io/ssl-vos.



### Visible-Infrared Person Re-Identification Using Privileged Intermediate Information
- **Arxiv ID**: http://arxiv.org/abs/2209.09348v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.09348v1)
- **Published**: 2022-09-19 21:08:14+00:00
- **Updated**: 2022-09-19 21:08:14+00:00
- **Authors**: Mahdi Alehdaghi, Arthur Josi, Rafael M. O. Cruz, Eric Granger
- **Comment**: None
- **Journal**: None
- **Summary**: Visible-infrared person re-identification (ReID) aims to recognize a same person of interest across a network of RGB and IR cameras. Some deep learning (DL) models have directly incorporated both modalities to discriminate persons in a joint representation space. However, this cross-modal ReID problem remains challenging due to the large domain shift in data distributions between RGB and IR modalities. % This paper introduces a novel approach for a creating intermediate virtual domain that acts as bridges between the two main domains (i.e., RGB and IR modalities) during training. This intermediate domain is considered as privileged information (PI) that is unavailable at test time, and allows formulating this cross-modal matching task as a problem in learning under privileged information (LUPI). We devised a new method to generate images between visible and infrared domains that provide additional information to train a deep ReID model through an intermediate domain adaptation. In particular, by employing color-free and multi-step triplet loss objectives during training, our method provides common feature representation spaces that are robust to large visible-infrared domain shifts. % Experimental results on challenging visible-infrared ReID datasets indicate that our proposed approach consistently improves matching accuracy, without any computational overhead at test time. The code is available at: \href{https://github.com/alehdaghi/Cross-Modal-Re-ID-via-LUPI}{https://github.com/alehdaghi/Cross-Modal-Re-ID-via-LUPI}



### E-VFIA : Event-Based Video Frame Interpolation with Attention
- **Arxiv ID**: http://arxiv.org/abs/2209.09359v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09359v3)
- **Published**: 2022-09-19 21:40:32+00:00
- **Updated**: 2023-03-01 12:52:16+00:00
- **Authors**: Onur Selim KÄ±lÄ±Ã§, Ahmet Akman, A. AydÄ±n Alatan
- **Comment**: Accepted to 2023 IEEE International Conference on Robotics and
  Automation (ICRA 2023)
- **Journal**: None
- **Summary**: Video frame interpolation (VFI) is a fundamental vision task that aims to synthesize several frames between two consecutive original video images. Most algorithms aim to accomplish VFI by using only keyframes, which is an ill-posed problem since the keyframes usually do not yield any accurate precision about the trajectories of the objects in the scene. On the other hand, event-based cameras provide more precise information between the keyframes of a video. Some recent state-of-the-art event-based methods approach this problem by utilizing event data for better optical flow estimation to interpolate for video frame by warping. Nonetheless, those methods heavily suffer from the ghosting effect. On the other hand, some of kernel-based VFI methods that only use frames as input, have shown that deformable convolutions, when backed up with transformers, can be a reliable way of dealing with long-range dependencies. We propose event-based video frame interpolation with attention (E-VFIA), as a lightweight kernel-based method. E-VFIA fuses event information with standard video frames by deformable convolutions to generate high quality interpolated frames. The proposed method represents events with high temporal resolution and uses a multi-head self-attention mechanism to better encode event-based information, while being less vulnerable to blurring and ghosting artifacts; thus, generating crispier frames. The simulation results show that the proposed technique outperforms current state-of-the-art methods (both frame and event-based) with a significantly smaller model size.



### A Trio-Method for Retinal Vessel Segmentation using Image Processing
- **Arxiv ID**: http://arxiv.org/abs/2209.11230v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11230v1)
- **Published**: 2022-09-19 22:07:34+00:00
- **Updated**: 2022-09-19 22:07:34+00:00
- **Authors**: Mahendra Kumar Gourisaria, Vinayak Singh, Manoj Sahni
- **Comment**: Accepted at 26th UK Conference on Medical Image Understanding and
  Analysis (MIUA-2022) (Abstract short paper)
- **Journal**: None
- **Summary**: Inner Retinal neurons are a most essential part of the retina and they are supplied with blood via retinal vessels. This paper primarily focuses on the segmentation of retinal vessels using a triple preprocessing approach. DRIVE database was taken into consideration and preprocessed by Gabor Filtering, Gaussian Blur, and Edge Detection by Sobel and Pruning. Segmentation was driven out by 2 proposed U-Net architectures. Both the architectures were compared in terms of all the standard performance metrics. Preprocessing generated varied interesting results which impacted the results shown by the UNet architectures for segmentation. This real-time deployment can help in the efficient pre-processing of images with better segmentation and detection.



### Gesture2Path: Imitation Learning for Gesture-aware Navigation
- **Arxiv ID**: http://arxiv.org/abs/2209.09375v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09375v1)
- **Published**: 2022-09-19 23:05:36+00:00
- **Updated**: 2022-09-19 23:05:36+00:00
- **Authors**: Catie Cuan, Edward Lee, Emre Fisher, Anthony Francis, Leila Takayama, Tingnan Zhang, Alexander Toshev, SÃ¶ren Pirk
- **Comment**: 8 pages, 12 figures
- **Journal**: None
- **Summary**: As robots increasingly enter human-centered environments, they must not only be able to navigate safely around humans, but also adhere to complex social norms. Humans often rely on non-verbal communication through gestures and facial expressions when navigating around other people, especially in densely occupied spaces. Consequently, robots also need to be able to interpret gestures as part of solving social navigation tasks. To this end, we present Gesture2Path, a novel social navigation approach that combines image-based imitation learning with model-predictive control. Gestures are interpreted based on a neural network that operates on streams of images, while we use a state-of-the-art model predictive control algorithm to solve point-to-point navigation tasks. We deploy our method on real robots and showcase the effectiveness of our approach for the four gestures-navigation scenarios: left/right, follow me, and make a circle. Our experiments indicate that our method is able to successfully interpret complex human gestures and to use them as a signal to generate socially compliant trajectories for navigation tasks. We validated our method based on in-situ ratings of participants interacting with the robots.



### LidarMultiNet: Towards a Unified Multi-Task Network for LiDAR Perception
- **Arxiv ID**: http://arxiv.org/abs/2209.09385v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09385v2)
- **Published**: 2022-09-19 23:39:15+00:00
- **Updated**: 2023-03-21 20:30:25+00:00
- **Authors**: Dongqiangzi Ye, Zixiang Zhou, Weijia Chen, Yufei Xie, Yu Wang, Panqu Wang, Hassan Foroosh
- **Comment**: Accepted to AAAI 2023 (Oral). Full-length paper extending our
  previous technical report of the 1st place solution of the 2022 Waymo Open
  Dataset 3D Semantic Segmentation challenge, including evaluations on 5 major
  benchmarks. arXiv admin note: text overlap with arXiv:2206.11428
- **Journal**: None
- **Summary**: LiDAR-based 3D object detection, semantic segmentation, and panoptic segmentation are usually implemented in specialized networks with distinctive architectures that are difficult to adapt to each other. This paper presents LidarMultiNet, a LiDAR-based multi-task network that unifies these three major LiDAR perception tasks. Among its many benefits, a multi-task network can reduce the overall cost by sharing weights and computation among multiple tasks. However, it typically underperforms compared to independently combined single-task models. The proposed LidarMultiNet aims to bridge the performance gap between the multi-task network and multiple single-task networks. At the core of LidarMultiNet is a strong 3D voxel-based encoder-decoder architecture with a Global Context Pooling (GCP) module extracting global contextual features from a LiDAR frame. Task-specific heads are added on top of the network to perform the three LiDAR perception tasks. More tasks can be implemented simply by adding new task-specific heads while introducing little additional cost. A second stage is also proposed to refine the first-stage segmentation and generate accurate panoptic segmentation results. LidarMultiNet is extensively tested on both Waymo Open Dataset and nuScenes dataset, demonstrating for the first time that major LiDAR perception tasks can be unified in a single strong network that is trained end-to-end and achieves state-of-the-art performance. Notably, LidarMultiNet reaches the official 1st place in the Waymo Open Dataset 3D semantic segmentation challenge 2022 with the highest mIoU and the best accuracy for most of the 22 classes on the test set, using only LiDAR points as input. It also sets the new state-of-the-art for a single model on the Waymo 3D object detection benchmark and three nuScenes benchmarks.



