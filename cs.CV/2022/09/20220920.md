# Arxiv Papers in cs.CV on 2022-09-20
### QuestSim: Human Motion Tracking from Sparse Sensors with Simulated Avatars
- **Arxiv ID**: http://arxiv.org/abs/2209.09391v1
- **DOI**: 10.1145/3550469.3555411
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.09391v1)
- **Published**: 2022-09-20 00:25:54+00:00
- **Updated**: 2022-09-20 00:25:54+00:00
- **Authors**: Alexander Winkler, Jungdam Won, Yuting Ye
- **Comment**: None
- **Journal**: SIGGRAPH Asia 2022 Conference Papers, December 6 to 9, 2022,
  Daegu, Republic of Korea
- **Summary**: Real-time tracking of human body motion is crucial for interactive and immersive experiences in AR/VR. However, very limited sensor data about the body is available from standalone wearable devices such as HMDs (Head Mounted Devices) or AR glasses. In this work, we present a reinforcement learning framework that takes in sparse signals from an HMD and two controllers, and simulates plausible and physically valid full body motions. Using high quality full body motion as dense supervision during training, a simple policy network can learn to output appropriate torques for the character to balance, walk, and jog, while closely following the input signals. Our results demonstrate surprisingly similar leg motions to ground truth without any observations of the lower body, even when the input is only the 6D transformations of the HMD. We also show that a single policy can be robust to diverse locomotion styles, different body sizes, and novel environments.



### Mitigating Representation Bias in Action Recognition: Algorithms and Benchmarks
- **Arxiv ID**: http://arxiv.org/abs/2209.09393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09393v1)
- **Published**: 2022-09-20 00:30:35+00:00
- **Updated**: 2022-09-20 00:30:35+00:00
- **Authors**: Haodong Duan, Yue Zhao, Kai Chen, Yuanjun Xiong, Dahua Lin
- **Comment**: ECCVW 2022
- **Journal**: None
- **Summary**: Deep learning models have achieved excellent recognition results on large-scale video benchmarks. However, they perform poorly when applied to videos with rare scenes or objects, primarily due to the bias of existing video datasets. We tackle this problem from two different angles: algorithm and dataset. From the perspective of algorithms, we propose Spatial-aware Multi-Aspect Debiasing (SMAD), which incorporates both explicit debiasing with multi-aspect adversarial training and implicit debiasing with the spatial actionness reweighting module, to learn a more generic representation invariant to non-action aspects. To neutralize the intrinsic dataset bias, we propose OmniDebias to leverage web data for joint training selectively, which can achieve higher performance with far fewer web data. To verify the effectiveness, we establish evaluation protocols and perform extensive experiments on both re-distributed splits of existing datasets and a new evaluation dataset focusing on the action with rare scenes. We also show that the debiased representation can generalize better when transferred to other datasets and tasks.



### DetCLIP: Dictionary-Enriched Visual-Concept Paralleled Pre-training for Open-world Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.09407v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09407v2)
- **Published**: 2022-09-20 02:01:01+00:00
- **Updated**: 2022-10-17 02:40:11+00:00
- **Authors**: Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, Hang Xu
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: Open-world object detection, as a more general and challenging goal, aims to recognize and localize objects described by arbitrary category names. The recent work GLIP formulates this problem as a grounding problem by concatenating all category names of detection datasets into sentences, which leads to inefficient interaction between category names. This paper presents DetCLIP, a paralleled visual-concept pre-training method for open-world detection by resorting to knowledge enrichment from a designed concept dictionary. To achieve better learning efficiency, we propose a novel paralleled concept formulation that extracts concepts separately to better utilize heterogeneous datasets (i.e., detection, grounding, and image-text pairs) for training. We further design a concept dictionary~(with descriptions) from various online sources and detection datasets to provide prior knowledge for each concept. By enriching the concepts with their descriptions, we explicitly build the relationships among various concepts to facilitate the open-domain learning. The proposed concept dictionary is further used to provide sufficient negative concepts for the construction of the word-region alignment loss\, and to complete labels for objects with missing descriptions in captions of image-text pair data. The proposed framework demonstrates strong zero-shot detection performances, e.g., on the LVIS dataset, our DetCLIP-T outperforms GLIP-T by 9.9% mAP and obtains a 13.5% improvement on rare categories compared to the fully-supervised model with the same backbone as ours.



### Bit Allocation using Optimization
- **Arxiv ID**: http://arxiv.org/abs/2209.09422v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09422v5)
- **Published**: 2022-09-20 02:40:52+00:00
- **Updated**: 2023-05-09 01:23:19+00:00
- **Authors**: Tongda Xu, Han Gao, Chenjian Gao, Yuanyuan Wang, Dailan He, Jinyong Pi, Jixiang Luo, Ziyu Zhu, Mao Ye, Hongwei Qin, Yan Wang, Jingjing Liu, Ya-Qin Zhang
- **Comment**: ICML 2023
- **Journal**: None
- **Summary**: In this paper, we consider the problem of bit allocation in Neural Video Compression (NVC). First, we reveal a fundamental relationship between bit allocation in NVC and Semi-Amortized Variational Inference (SAVI). Specifically, we show that SAVI with GoP (Group-of-Picture)-level likelihood is equivalent to pixel-level bit allocation with precise rate \& quality dependency model. Based on this equivalence, we establish a new paradigm of bit allocation using SAVI. Different from previous bit allocation methods, our approach requires no empirical model and is thus optimal. Moreover, as the original SAVI using gradient ascent only applies to single-level latent, we extend the SAVI to multi-level such as NVC by recursively applying back-propagating through gradient ascent. Finally, we propose a tractable approximation for practical implementation. Our method can be applied to scenarios where performance outweights encoding speed, and serves as an empirical bound on the R-D performance of bit allocation. Experimental results show that current state-of-the-art bit allocation algorithms still have a room of $\approx 0.5$ dB PSNR to improve compared with ours. Code is available at \url{https://github.com/tongdaxu/Bit-Allocation-Using-Optimization}.



### Data-Centric AI Paradigm Based on Application-Driven Fine-Grained Dataset Design
- **Arxiv ID**: http://arxiv.org/abs/2209.09449v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09449v3)
- **Published**: 2022-09-20 03:56:53+00:00
- **Updated**: 2022-10-17 02:42:24+00:00
- **Authors**: Huan Hu, Yajie Cui, Zhaoxiang Liu, Shiguo Lian
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has a wide range of applications in industrial scenario, but reducing false alarm (FA) remains a major difficulty. Optimizing network architecture or network parameters is used to tackle this challenge in academic circles, while ignoring the essential characteristics of data in application scenarios, which often results in increased FA in new scenarios. In this paper, we propose a novel paradigm for fine-grained design of datasets, driven by industrial applications. We flexibly select positive and negative sample sets according to the essential features of the data and application requirements, and add the remaining samples to the training set as uncertainty classes. We collect more than 10,000 mask-wearing recognition samples covering various application scenarios as our experimental data. Compared with the traditional data design methods, our method achieves better results and effectively reduces FA. We make all contributions available to the research community for broader use. The contributions will be available at https://github.com/huh30/OpenDatasets.



### Rethinking Dimensionality Reduction in Grid-based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.09464v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.09464v4)
- **Published**: 2022-09-20 04:51:54+00:00
- **Updated**: 2023-01-27 13:16:07+00:00
- **Authors**: Dihe Huang, Ying Chen, Yikang Ding, Jinli Liao, Jianlin Liu, Kai Wu, Qiang Nie, Yong Liu, Chengjie Wang, Zhiheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Bird's eye view (BEV) is widely adopted by most of the current point cloud detectors due to the applicability of well-explored 2D detection techniques. However, existing methods obtain BEV features by simply collapsing voxel or point features along the height dimension, which causes the heavy loss of 3D spatial information. To alleviate the information loss, we propose a novel point cloud detection network based on a Multi-level feature dimensionality reduction strategy, called MDRNet. In MDRNet, the Spatial-aware Dimensionality Reduction (SDR) is designed to dynamically focus on the valuable parts of the object during voxel-to-BEV feature transformation. Furthermore, the Multi-level Spatial Residuals (MSR) is proposed to fuse the multi-level spatial information in the BEV feature maps. Extensive experiments on nuScenes show that the proposed method outperforms the state-of-the-art methods. The code will be available upon publication.



### BuFF: Burst Feature Finder for Light-Constrained 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2209.09470v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09470v1)
- **Published**: 2022-09-20 05:06:33+00:00
- **Updated**: 2022-09-20 05:06:33+00:00
- **Authors**: Ahalya Ravendran, Mitch Bryson, Donald G. Dansereau
- **Comment**: 7 pages, 9 figures, 2 tables, for associated project page, see
  https://roboticimaging.org/Projects/BuFF/
- **Journal**: None
- **Summary**: Robots operating at night using conventional vision cameras face significant challenges in reconstruction due to noise-limited images. Previous work has demonstrated that burst-imaging techniques can be used to partially overcome this issue. In this paper, we develop a novel feature detector that operates directly on image bursts that enhances vision-based reconstruction under extremely low-light conditions. Our approach finds keypoints with well-defined scale and apparent motion within each burst by jointly searching in a multi-scale and multi-motion space. Because we describe these features at a stage where the images have higher signal-to-noise ratio, the detected features are more accurate than the state-of-the-art on conventional noisy images and burst-merged images and exhibit high precision, recall, and matching performance. We show improved feature performance and camera pose estimates and demonstrate improved structure-from-motion performance using our feature detector in challenging light-constrained scenes. Our feature finder provides a significant step towards robots operating in low-light scenarios and applications including night-time operations.



### Revisiting Image Pyramid Structure for High Resolution Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.09475v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09475v3)
- **Published**: 2022-09-20 05:20:07+00:00
- **Updated**: 2022-11-16 05:52:04+00:00
- **Authors**: Taehun Kim, Kunhee Kim, Joonyeong Lee, Dongmin Cha, Jiho Lee, Daijin Kim
- **Comment**: 27 pages, 15 figures, 7 tables. To appear in the 16th Asian
  Conference on Computer Vision (ACCV2022), December 4-8, 2022, Macau SAR,
  China. DOI will be added soon. Results on DIS5K are added in appendices which
  will not be in the published version
- **Journal**: None
- **Summary**: Salient object detection (SOD) has been in the spotlight recently, yet has been studied less for high-resolution (HR) images. Unfortunately, HR images and their pixel-level annotations are certainly more labor-intensive and time-consuming compared to low-resolution (LR) images and annotations. Therefore, we propose an image pyramid-based SOD framework, Inverse Saliency Pyramid Reconstruction Network (InSPyReNet), for HR prediction without any of HR datasets. We design InSPyReNet to produce a strict image pyramid structure of saliency map, which enables to ensemble multiple results with pyramid-based image blending. For HR prediction, we design a pyramid blending method which synthesizes two different image pyramids from a pair of LR and HR scale from the same image to overcome effective receptive field (ERF) discrepancy. Our extensive evaluations on public LR and HR SOD benchmarks demonstrate that InSPyReNet surpasses the State-of-the-Art (SotA) methods on various SOD metrics and boundary accuracy.



### SparCL: Sparse Continual Learning on the Edge
- **Arxiv ID**: http://arxiv.org/abs/2209.09476v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09476v1)
- **Published**: 2022-09-20 05:24:48+00:00
- **Updated**: 2022-09-20 05:24:48+00:00
- **Authors**: Zifeng Wang, Zheng Zhan, Yifan Gong, Geng Yuan, Wei Niu, Tong Jian, Bin Ren, Stratis Ioannidis, Yanzhi Wang, Jennifer Dy
- **Comment**: Published at NeurIPS 2022 as a conference paper
- **Journal**: None
- **Summary**: Existing work in continual learning (CL) focuses on mitigating catastrophic forgetting, i.e., model performance deterioration on past tasks when learning a new task. However, the training efficiency of a CL system is under-investigated, which limits the real-world application of CL systems under resource-limited scenarios. In this work, we propose a novel framework called Sparse Continual Learning(SparCL), which is the first study that leverages sparsity to enable cost-effective continual learning on edge devices. SparCL achieves both training acceleration and accuracy preservation through the synergy of three aspects: weight sparsity, data efficiency, and gradient sparsity. Specifically, we propose task-aware dynamic masking (TDM) to learn a sparse network throughout the entire CL process, dynamic data removal (DDR) to remove less informative training data, and dynamic gradient masking (DGM) to sparsify the gradient updates. Each of them not only improves efficiency, but also further mitigates catastrophic forgetting. SparCL consistently improves the training efficiency of existing state-of-the-art (SOTA) CL methods by at most 23X less training FLOPs, and, surprisingly, further improves the SOTA accuracy by at most 1.7%. SparCL also outperforms competitive baselines obtained from adapting SOTA sparse training methods to the CL setting in both efficiency and accuracy. We also evaluate the effectiveness of SparCL on a real mobile phone, further indicating the practical potential of our method.



### Interpretable Edge Enhancement and Suppression Learning for 3D Point Cloud Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.09483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09483v1)
- **Published**: 2022-09-20 05:52:28+00:00
- **Updated**: 2022-09-20 05:52:28+00:00
- **Authors**: Haoyi Xiu, Xin Liu, Weimin Wang, Kyoung-Sook Kim, Takayuki Shinohara, Qiong Chang, Masashi Matsuoka
- **Comment**: None
- **Journal**: None
- **Summary**: 3D point clouds can flexibly represent continuous surfaces and can be used for various applications; however, the lack of structural information makes point cloud recognition challenging. Recent edge-aware methods mainly use edge information as an extra feature that describes local structures to facilitate learning. Although these methods show that incorporating edges into the network design is beneficial, they generally lack interpretability, making users wonder how exactly edges help. To shed light on this issue, in this study, we propose the Diffusion Unit (DU) that handles edges in an interpretable manner while providing decent improvement. Our method is interpretable in three ways. First, we theoretically show that DU learns to perform task-beneficial edge enhancement and suppression. Second, we experimentally observe and verify the edge enhancement and suppression behavior. Third, we empirically demonstrate that this behavior contributes to performance improvement. Extensive experiments performed on challenging benchmarks verify the superiority of DU in terms of both interpretability and performance gain. Specifically, our method achieves state-of-the-art performance in object part segmentation using ShapeNet part and scene segmentation using S3DIS. Our source code will be released at https://github.com/martianxiu/DiffusionUnit.



### Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos
- **Arxiv ID**: http://arxiv.org/abs/2209.09484v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.09484v4)
- **Published**: 2022-09-20 05:52:54+00:00
- **Updated**: 2023-03-29 02:22:29+00:00
- **Authors**: Yilin Wen, Hao Pan, Lei Yang, Jia Pan, Taku Komura, Wenping Wang
- **Comment**: Accepted by CVPR 2023; Project page:
  https://fylwen.github.io/htt.html
- **Journal**: None
- **Summary**: Understanding dynamic hand motions and actions from egocentric RGB videos is a fundamental yet challenging task due to self-occlusion and ambiguity. To address occlusion and ambiguity, we develop a transformer-based framework to exploit temporal information for robust estimation. Noticing the different temporal granularity of and the semantic correlation between hand pose estimation and action recognition, we build a network hierarchy with two cascaded transformer encoders, where the first one exploits the short-term temporal cue for hand pose estimation, and the latter aggregates per-frame pose and object information over a longer time span to recognize the action. Our approach achieves competitive results on two first-person hand action benchmarks, namely FPHA and H2O. Extensive ablation studies verify our design choices.



### Self-supervised 3D Object Detection from Monocular Pseudo-LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2209.09486v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09486v1)
- **Published**: 2022-09-20 05:55:49+00:00
- **Updated**: 2022-09-20 05:55:49+00:00
- **Authors**: Curie Kim, Ue-Hwan Kim, Jong-Hwan Kim
- **Comment**: Accepted for the 2022 IEEE International Conference on Multisensor
  Fusion and Integration (MFI 2022)
- **Journal**: None
- **Summary**: There have been attempts to detect 3D objects by fusion of stereo camera images and LiDAR sensor data or using LiDAR for pre-training and only monocular images for testing, but there have been less attempts to use only monocular image sequences due to low accuracy. In addition, when depth prediction using only monocular images, only scale-inconsistent depth can be predicted, which is the reason why researchers are reluctant to use monocular images alone. Therefore, we propose a method for predicting absolute depth and detecting 3D objects using only monocular image sequences by enabling end-to-end learning of detection networks and depth prediction networks. As a result, the proposed method surpasses other existing methods in performance on the KITTI 3D dataset. Even when monocular image and 3D LiDAR are used together during training in an attempt to improve performance, ours exhibit is the best performance compared to other methods using the same input. In addition, end-to-end learning not only improves depth prediction performance, but also enables absolute depth prediction, because our network utilizes the fact that the size of a 3D object such as a car is determined by the approximate size.



### Perceptual Quality Assessment for Digital Human Heads
- **Arxiv ID**: http://arxiv.org/abs/2209.09489v5
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09489v5)
- **Published**: 2022-09-20 06:02:57+00:00
- **Updated**: 2023-02-28 12:15:46+00:00
- **Authors**: Zicheng Zhang, Yingjie Zhou, Wei Sun, Xiongkuo Min, Yuzhe Wu, Guangtao Zhai
- **Comment**: None
- **Journal**: None
- **Summary**: Digital humans are attracting more and more research interest during the last decade, the generation, representation, rendering, and animation of which have been put into large amounts of effort. However, the quality assessment of digital humans has fallen behind. Therefore, to tackle the challenge of digital human quality assessment issues, we propose the first large-scale quality assessment database for three-dimensional (3D) scanned digital human heads (DHHs). The constructed database consists of 55 reference DHHs and 1,540 distorted DHHs along with the subjective perceptual ratings. Then, a simple yet effective full-reference (FR) projection-based method is proposed to evaluate the visual quality of DHHs. The pretrained Swin Transformer tiny is employed for hierarchical feature extraction and the multi-head attention module is utilized for feature fusion. The experimental results reveal that the proposed method exhibits state-of-the-art performance among the mainstream FR metrics. The database is released at https://github.com/zzc-1998/DHHQA.



### NBD-GAP: Non-Blind Image Deblurring Without Clean Target Images
- **Arxiv ID**: http://arxiv.org/abs/2209.09498v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09498v1)
- **Published**: 2022-09-20 06:21:11+00:00
- **Updated**: 2022-09-20 06:21:11+00:00
- **Authors**: Nithin Gopalakrishnan Nair, Rajeev Yasarla, Vishal M. Patel
- **Comment**: Accepted at ICIP 2022
- **Journal**: None
- **Summary**: In recent years, deep neural network-based restoration methods have achieved state-of-the-art results in various image deblurring tasks. However, one major drawback of deep learning-based deblurring networks is that large amounts of blurry-clean image pairs are required for training to achieve good performance. Moreover, deep networks often fail to perform well when the blurry images and the blur kernels during testing are very different from the ones used during training. This happens mainly because of the overfitting of the network parameters on the training data. In this work, we present a method that addresses these issues. We view the non-blind image deblurring problem as a denoising problem. To do so, we perform Wiener filtering on a pair of blurry images with the corresponding blur kernels. This results in a pair of images with colored noise. Hence, the deblurring problem is translated into a denoising problem. We then solve the denoising problem without using explicit clean target images. Extensive experiments are conducted to show that our method achieves results that are on par to the state-of-the-art non-blind deblurring works.



### GAMA: Generative Adversarial Multi-Object Scene Attacks
- **Arxiv ID**: http://arxiv.org/abs/2209.09502v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09502v2)
- **Published**: 2022-09-20 06:40:54+00:00
- **Updated**: 2022-10-15 19:44:33+00:00
- **Authors**: Abhishek Aich, Calvin-Khang Ta, Akash Gupta, Chengyu Song, Srikanth V. Krishnamurthy, M. Salman Asif, Amit K. Roy-Chowdhury
- **Comment**: Accepted at NeurIPS 2022; First two authors contributed equally;
  Includes Supplementary Material
- **Journal**: None
- **Summary**: The majority of methods for crafting adversarial attacks have focused on scenes with a single dominant object (e.g., images from ImageNet). On the other hand, natural scenes include multiple dominant objects that are semantically related. Thus, it is crucial to explore designing attack strategies that look beyond learning on single-object scenes or attack single-object victim classifiers. Due to their inherent property of strong transferability of perturbations to unknown models, this paper presents the first approach of using generative models for adversarial attacks on multi-object scenes. In order to represent the relationships between different objects in the input scene, we leverage upon the open-sourced pre-trained vision-language model CLIP (Contrastive Language-Image Pre-training), with the motivation to exploit the encoded semantics in the language space along with the visual space. We call this attack approach Generative Adversarial Multi-object scene Attacks (GAMA). GAMA demonstrates the utility of the CLIP model as an attacker's tool to train formidable perturbation generators for multi-object scenes. Using the joint image-text features to train the generator, we show that GAMA can craft potent transferable perturbations in order to fool victim classifiers in various attack settings. For example, GAMA triggers ~16% more misclassification than state-of-the-art generative approaches in black-box settings where both the classifier architecture and data distribution of the attacker are different from the victim. Our code is available here: https://abhishekaich27.github.io/gama.html



### Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2209.09513v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2209.09513v2)
- **Published**: 2022-09-20 07:04:24+00:00
- **Updated**: 2022-10-17 07:46:47+00:00
- **Authors**: Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan
- **Comment**: Accepted to NeurIPS 2022. 22 pages, 17 figures, 9 tables. Project:
  https://scienceqa.github.io
- **Journal**: None
- **Summary**: When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40% of the data. The data and code are available at https://scienceqa.github.io.



### Review of data types and model dimensionality for cardiac DTI SMS-related artefact removal
- **Arxiv ID**: http://arxiv.org/abs/2209.09522v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09522v1)
- **Published**: 2022-09-20 07:41:24+00:00
- **Updated**: 2022-09-20 07:41:24+00:00
- **Authors**: Michael Tanzer, Sea Hee Yook, Guang Yang, Daniel Rueckert, Sonia Nielles-Vallespin
- **Comment**: 11 pages, 3 tables, 1 figure. To be published at the STACOM workshop,
  MICCAI 2022
- **Journal**: None
- **Summary**: As diffusion tensor imaging (DTI) gains popularity in cardiac imaging due to its unique ability to non-invasively assess the cardiac microstructure, deep learning-based Artificial Intelligence is becoming a crucial tool in mitigating some of its drawbacks, such as the long scan times. As it often happens in fast-paced research environments, a lot of emphasis has been put on showing the capability of deep learning while often not enough time has been spent investigating what input and architectural properties would benefit cardiac DTI acceleration the most. In this work, we compare the effect of several input types (magnitude images vs complex images), multiple dimensionalities (2D vs 3D operations), and multiple input types (single slice vs multi-slice) on the performance of a model trained to remove artefacts caused by a simultaneous multi-slice (SMS) acquisition. Despite our initial intuition, our experiments show that, for a fixed number of parameters, simpler 2D real-valued models outperform their more advanced 3D or complex counterparts. The best performance is although obtained by a real-valued model trained using both the magnitude and phase components of the acquired data. We believe this behaviour to be due to real-valued models making better use of the lower number of parameters, and to 3D models not being able to exploit the spatial information because of the low SMS acceleration factor used in our experiments.



### Graph Reasoning Transformer for Image Parsing
- **Arxiv ID**: http://arxiv.org/abs/2209.09545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09545v1)
- **Published**: 2022-09-20 08:21:37+00:00
- **Updated**: 2022-09-20 08:21:37+00:00
- **Authors**: Dong Zhang, Jinhui Tang, Kwang-Ting Cheng
- **Comment**: Accepted in ACM MM2022
- **Journal**: None
- **Summary**: Capturing the long-range dependencies has empirically proven to be effective on a wide range of computer vision tasks. The progressive advances on this topic have been made through the employment of the transformer framework with the help of the multi-head attention mechanism. However, the attention-based image patch interaction potentially suffers from problems of redundant interactions of intra-class patches and unoriented interactions of inter-class patches. In this paper, we propose a novel Graph Reasoning Transformer (GReaT) for image parsing to enable image patches to interact following a relation reasoning pattern. Specifically, the linearly embedded image patches are first projected into the graph space, where each node represents the implicit visual center for a cluster of image patches and each edge reflects the relation weight between two adjacent nodes. After that, global relation reasoning is performed on this graph accordingly. Finally, all nodes including the relation information are mapped back into the original space for subsequent processes. Compared to the conventional transformer, GReaT has higher interaction efficiency and a more purposeful interaction pattern. Experiments are carried out on the challenging Cityscapes and ADE20K datasets. Results show that GReaT achieves consistent performance gains with slight computational overheads on the state-of-the-art transformer baselines.



### Automated ischemic stroke lesion segmentation from 3D MRI
- **Arxiv ID**: http://arxiv.org/abs/2209.09546v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09546v2)
- **Published**: 2022-09-20 08:21:57+00:00
- **Updated**: 2022-09-21 08:42:03+00:00
- **Authors**: Md Mahfuzur Rahman Siddique, Dong Yang, Yufan He, Daguang Xu, Andriy Myronenko
- **Comment**: ISLES22 challenge report, MICCAI2022
- **Journal**: None
- **Summary**: Ischemic Stroke Lesion Segmentation challenge (ISLES 2022) offers a platform for researchers to compare their solutions to 3D segmentation of ischemic stroke regions from 3D MRIs. In this work, we describe our solution to ISLES 2022 segmentation task. We re-sample all images to a common resolution, use two input MRI modalities (DWI and ADC) and train SegResNet semantic segmentation network from MONAI. The final submission is an ensemble of 15 models (from 3 runs of 5-fold cross validation). Our solution (team name NVAUTO) achieves the top place in terms of Dice metric (0.824), and overall rank 2 (based on the combined metric ranking).



### Cross-modal Learning for Image-Guided Point Cloud Shape Completion
- **Arxiv ID**: http://arxiv.org/abs/2209.09552v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09552v1)
- **Published**: 2022-09-20 08:37:05+00:00
- **Updated**: 2022-09-20 08:37:05+00:00
- **Authors**: Emanuele Aiello, Diego Valsesia, Enrico Magli
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: In this paper we explore the recent topic of point cloud completion, guided by an auxiliary image. We show how it is possible to effectively combine the information from the two modalities in a localized latent space, thus avoiding the need for complex point cloud reconstruction methods from single views used by the state-of-the-art. We also investigate a novel weakly-supervised setting where the auxiliary image provides a supervisory signal to the training process by using a differentiable renderer on the completed point cloud to measure fidelity in the image space. Experiments show significant improvements over state-of-the-art supervised methods for both unimodal and multimodal completion. We also show the effectiveness of the weakly-supervised approach which outperforms a number of supervised methods and is competitive with the latest supervised models only exploiting point cloud information.



### Towards Robust Referring Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.09554v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09554v2)
- **Published**: 2022-09-20 08:48:26+00:00
- **Updated**: 2023-07-23 10:27:35+00:00
- **Authors**: Jianzong Wu, Xiangtai Li, Xia Li, Henghui Ding, Yunhai Tong, Dacheng Tao
- **Comment**: update more results
- **Journal**: None
- **Summary**: Referring Image Segmentation (RIS) is a fundamental vision-language task that outputs object masks based on text descriptions. Many works have achieved considerable progress for RIS, including different fusion method designs. In this work, we explore an essential question, ``What if the text description is wrong or misleading?'' For example, the described objects are not in the image. We term such a sentence as a negative sentence. However, existing solutions for RIS cannot handle such a setting. To this end, we propose a new formulation of RIS, named Robust Referring Image Segmentation (R-RIS). It considers the negative sentence inputs besides the regular positive text inputs. To facilitate this new task, we create three R-RIS datasets by augmenting existing RIS datasets with negative sentences and propose new metrics to evaluate both types of inputs in a unified manner. Furthermore, we propose a new transformer-based model, called RefSegformer, with a token-based vision and language fusion module. Our design can be easily extended to our R-RIS setting by adding extra blank tokens. Our proposed RefSegformer achieves state-of-the-art results on both RIS and R-RIS datasets, establishing a solid baseline for both settings. Our project page is at \url{https://github.com/jianzongwu/robust-ref-seg}.



### CoV-TI-Net: Transferred Initialization with Modified End Layer for COVID-19 Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2209.09556v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09556v1)
- **Published**: 2022-09-20 08:52:52+00:00
- **Updated**: 2022-09-20 08:52:52+00:00
- **Authors**: Sadia Khanam, Mohammad Reza Chalak Qazani, Subrota Kumar Mondal, H M Dipu Kabir, Abadhan S. Sabyasachi, Houshyar Asadi, Keshav Kumar, Farzin Tabarsinezhad, Shady Mohamed, Abbas Khorsavi, Saeid Nahavandi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes transferred initialization with modified fully connected layers for COVID-19 diagnosis. Convolutional neural networks (CNN) achieved a remarkable result in image classification. However, training a high-performing model is a very complicated and time-consuming process because of the complexity of image recognition applications. On the other hand, transfer learning is a relatively new learning method that has been employed in many sectors to achieve good performance with fewer computations. In this research, the PyTorch pre-trained models (VGG19\_bn and WideResNet -101) are applied in the MNIST dataset for the first time as initialization and with modified fully connected layers. The employed PyTorch pre-trained models were previously trained in ImageNet. The proposed model is developed and verified in the Kaggle notebook, and it reached the outstanding accuracy of 99.77% without taking a huge computational time during the training process of the network. We also applied the same methodology to the SIIM-FISABIO-RSNA COVID-19 Detection dataset and achieved 80.01% accuracy. In contrast, the previous methods need a huge compactional time during the training process to reach a high-performing model. Codes are available at the following link: github.com/dipuk0506/SpinalNet



### Calibrating Ensembles for Scalable Uncertainty Quantification in Deep Learning-based Medical Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.09563v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09563v1)
- **Published**: 2022-09-20 09:09:48+00:00
- **Updated**: 2022-09-20 09:09:48+00:00
- **Authors**: Thomas Buddenkotte, Lorena Escudero Sanchez, Mireia Crispin-Ortuzar, Ramona Woitek, Cathal McCague, James D. Brenton, Ozan Öktem, Evis Sala, Leonardo Rundo
- **Comment**: None
- **Journal**: None
- **Summary**: Uncertainty quantification in automated image analysis is highly desired in many applications. Typically, machine learning models in classification or segmentation are only developed to provide binary answers; however, quantifying the uncertainty of the models can play a critical role for example in active learning or machine human interaction. Uncertainty quantification is especially difficult when using deep learning-based models, which are the state-of-the-art in many imaging applications. The current uncertainty quantification approaches do not scale well in high-dimensional real-world problems. Scalable solutions often rely on classical techniques, such as dropout, during inference or training ensembles of identical models with different random seeds to obtain a posterior distribution. In this paper, we show that these approaches fail to approximate the classification probability. On the contrary, we propose a scalable and intuitive framework to calibrate ensembles of deep learning models to produce uncertainty quantification measurements that approximate the classification probability. On unseen test data, we demonstrate improved calibration, sensitivity (in two out of three cases) and precision when being compared with the standard approaches. We further motivate the usage of our method in active learning, creating pseudo-labels to learn from unlabeled images and human-machine collaboration.



### Sampling Agnostic Feature Representation for Long-Term Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/2209.09574v2
- **DOI**: 10.1109/TIP.2022.3207024
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09574v2)
- **Published**: 2022-09-20 09:38:48+00:00
- **Updated**: 2022-11-08 01:12:08+00:00
- **Authors**: Seongyeop Yang, Byeongkeun Kang, Yeejin Lee
- **Comment**: 11 pages, 7 figures, In IEEE Transactions on Image Processing 2022
- **Journal**: None
- **Summary**: Person re-identification is a problem of identifying individuals across non-overlapping cameras. Although remarkable progress has been made in the re-identification problem, it is still a challenging problem due to appearance variations of the same person as well as other people of similar appearance. Some prior works solved the issues by separating features of positive samples from features of negative ones. However, the performances of existing models considerably depend on the characteristics and statistics of the samples used for training. Thus, we propose a novel framework named sampling independent robust feature representation network (SirNet) that learns disentangled feature embedding from randomly chosen samples. A carefully designed sampling independent maximum discrepancy loss is introduced to model samples of the same person as a cluster. As a result, the proposed framework can generate additional hard negatives/positives using the learned features, which results in better discriminability from other identities. Extensive experimental results on large-scale benchmark datasets verify that the proposed model is more effective than prior state-of-the-art models.



### Simultaneous segmentation and classification of the retinal arteries and veins from color fundus images
- **Arxiv ID**: http://arxiv.org/abs/2209.09582v1
- **DOI**: 10.1016/j.artmed.2021.102116
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09582v1)
- **Published**: 2022-09-20 09:54:01+00:00
- **Updated**: 2022-09-20 09:54:01+00:00
- **Authors**: José Morano, Álvaro S. Hervella, Jorge Novo, José Rouco
- **Comment**: None
- **Journal**: Artificial Intelligence in Medicine, Volume 118, 2021, 102116,
  ISSN 0933-3657
- **Summary**: The study of the retinal vasculature is a fundamental stage in the screening and diagnosis of many diseases. A complete retinal vascular analysis requires to segment and classify the blood vessels of the retina into arteries and veins (A/V). Early automatic methods approached these segmentation and classification tasks in two sequential stages. However, currently, these tasks are approached as a joint semantic segmentation task, as the classification results highly depend on the effectiveness of the vessel segmentation. In that regard, we propose a novel approach for the simultaneous segmentation and classification of the retinal A/V from eye fundus images. In particular, we propose a novel method that, unlike previous approaches, and thanks to a novel loss, decomposes the joint task into three segmentation problems targeting arteries, veins and the whole vascular tree. This configuration allows to handle vessel crossings intuitively and directly provides accurate segmentation masks of the different target vascular trees. The provided ablation study on the public Retinal Images vessel Tree Extraction (RITE) dataset demonstrates that the proposed method provides a satisfactory performance, particularly in the segmentation of the different structures. Furthermore, the comparison with the state of the art shows that our method achieves highly competitive results in A/V classification, while significantly improving vascular segmentation. The proposed multi-segmentation method allows to detect more vessels and better segment the different structures, while achieving a competitive classification performance. Also, in these terms, our approach outperforms the approaches of various reference works. Moreover, in contrast with previous approaches, the proposed method allows to directly detect the vessel crossings, as well as preserving the continuity of A/V at these complex locations.



### Semi-automatic Data Annotation System for Multi-Target Multi-Camera Vehicle Tracking
- **Arxiv ID**: http://arxiv.org/abs/2209.09606v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09606v1)
- **Published**: 2022-09-20 10:37:38+00:00
- **Updated**: 2022-09-20 10:37:38+00:00
- **Authors**: Haohong Liao, Silin Zheng, Xuelin Shen, Mark Junjie Li, Xu Wang
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: Multi-target multi-camera tracking (MTMCT) plays an important role in intelligent video analysis, surveillance video retrieval, and other application scenarios. Nowadays, the deep-learning-based MTMCT has been the mainstream and has achieved fascinating improvements regarding tracking accuracy and efficiency. However, according to our investigation, the lacking of datasets focusing on real-world application scenarios limits the further improvements for current learning-based MTMCT models. Specifically, the learning-based MTMCT models training by common datasets usually cannot achieve satisfactory results in real-world application scenarios. Motivated by this, this paper presents a semi-automatic data annotation system to facilitate the real-world MTMCT dataset establishment. The proposed system first employs a deep-learning-based single-camera trajectory generation method to automatically extract trajectories from surveillance videos. Subsequently, the system provides a recommendation list in the following manual cross-camera trajectory matching process. The recommendation list is generated based on side information, including camera location, timestamp relation, and background scene. In the experimental stage, extensive results further demonstrate the efficiency of the proposed system.



### Generalisability of fetal ultrasound deep learning models to low-resource imaging settings in five African countries
- **Arxiv ID**: http://arxiv.org/abs/2209.09610v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09610v2)
- **Published**: 2022-09-20 10:56:09+00:00
- **Updated**: 2023-02-14 10:52:38+00:00
- **Authors**: Carla Sendra-Balcells, Víctor M. Campello, Jordina Torrents-Barrena, Yahya Ali Ahmed, Mustafa Elattar, Benard Ohene Botwe, Pempho Nyangulu, William Stones, Mohammed Ammar, Lamya Nawal Benamer, Harriet Nalubega Kisembo, Senai Goitom Sereke, Sikolia Z. Wanyonyi, Marleen Temmerman, Eduard Gratacós, Elisenda Bonet, Elisenda Eixarch, Kamil Mikolaj, Martin Grønnebæk Tolsgaard, Karim Lekadir
- **Comment**: 14 pages, 6 figures, accepted for publication in Scientific Reports
- **Journal**: None
- **Summary**: Most artificial intelligence (AI) research have concentrated in high-income countries, where imaging data, IT infrastructures and clinical expertise are plentiful. However, slower progress has been made in limited-resource environments where medical imaging is needed. For example, in Sub-Saharan Africa the rate of perinatal mortality is very high due to limited access to antenatal screening. In these countries, AI models could be implemented to help clinicians acquire fetal ultrasound planes for diagnosis of fetal abnormalities. So far, deep learning models have been proposed to identify standard fetal planes, but there is no evidence of their ability to generalise in centres with limited access to high-end ultrasound equipment and data. This work investigates different strategies to reduce the domain-shift effect for a fetal plane classification model trained on a high-resource clinical centre and transferred to a new low-resource centre. To that end, a classifier trained with 1,792 patients from Spain is first evaluated on a new centre in Denmark in optimal conditions with 1,008 patients and is later optimised to reach the same performance in five African centres (Egypt, Algeria, Uganda, Ghana and Malawi) with 25 patients each. The results show that a transfer learning approach can be a solution to integrate small-size African samples with existing large-scale databases in developed countries. In particular, the model can be re-aligned and optimised to boost the performance on African populations by increasing the recall to $0.92\pm0.04$ and at the same time maintaining a high precision across centres. This framework shows promise for building new AI models generalisable across clinical centres with limited data acquired in challenging and heterogeneous conditions and calls for further research to develop new solutions for usability of AI in countries with less resources.



### View-Disentangled Transformer for Brain Lesion Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.09657v1
- **DOI**: 10.1109/ISBI52829.2022.9761542
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09657v1)
- **Published**: 2022-09-20 11:58:23+00:00
- **Updated**: 2022-09-20 11:58:23+00:00
- **Authors**: Haofeng Li, Junjia Huang, Guanbin Li, Zhou Liu, Yihong Zhong, Yingying Chen, Yunfei Wang, Xiang Wan
- **Comment**: International Symposium on Biomedical Imaging (ISBI) 2022, code:
  https://github.com/lhaof/ISBI-VDFormer
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have been widely adopted in brain lesion detection and segmentation. However, locating small lesions in 2D MRI slices is challenging, and requires to balance between the granularity of 3D context aggregation and the computational complexity. In this paper, we propose a novel view-disentangled transformer to enhance the extraction of MRI features for more accurate tumour detection. First, the proposed transformer harvests long-range correlation among different positions in a 3D brain scan. Second, the transformer models a stack of slice features as multiple 2D views and enhance these features view-by-view, which approximately achieves the 3D correlation computing in an efficient way. Third, we deploy the proposed transformer module in a transformer backbone, which can effectively detect the 2D regions surrounding brain lesions. The experimental results show that our proposed view-disentangled transformer performs well for brain lesion detection on a challenging brain MRI dataset.



### Ki-Pode: Keypoint-based Implicit Pose Distribution Estimation of Rigid Objects
- **Arxiv ID**: http://arxiv.org/abs/2209.09659v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.09659v1)
- **Published**: 2022-09-20 11:59:05+00:00
- **Updated**: 2022-09-20 11:59:05+00:00
- **Authors**: Thorbjørn Mosekjær Iversen, Rasmus Laurvig Haugaard, Anders Glent Buch
- **Comment**: 11 pages, 2 figures
- **Journal**: The 33rd British Machine Vision Conference Proceedings: BMVC 2022
- **Summary**: The estimation of 6D poses of rigid objects is a fundamental problem in computer vision. Traditionally pose estimation is concerned with the determination of a single best estimate. However, a single estimate is unable to express visual ambiguity, which in many cases is unavoidable due to object symmetries or occlusion of identifying features. Inability to account for ambiguities in pose can lead to failure in subsequent methods, which is unacceptable when the cost of failure is high. Estimates of full pose distributions are, contrary to single estimates, well suited for expressing uncertainty on pose. Motivated by this, we propose a novel pose distribution estimation method. An implicit formulation of the probability distribution over object pose is derived from an intermediary representation of an object as a set of keypoints. This ensures that the pose distribution estimates have a high level of interpretability. Furthermore, our method is based on conservative approximations, which leads to reliable estimates. The method has been evaluated on the task of rotation distribution estimation on the YCB-V and T-LESS datasets and performs reliably on all objects.



### Evaluation Framework for Computer Vision-Based Guidance of the Visually Impaired
- **Arxiv ID**: http://arxiv.org/abs/2209.09676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09676v1)
- **Published**: 2022-09-20 12:26:55+00:00
- **Updated**: 2022-09-20 12:26:55+00:00
- **Authors**: Krešimir Romić, Irena Galić, Marija Habijan, Hrvoje Leventić
- **Comment**: Technical paper published at 64th International Symposium ELMAR-2022,
  ISBN: 978-1-6654-7002-5
- **Journal**: None
- **Summary**: Visually impaired persons have significant problems in their everyday movement. Therefore, some of our previous work involves computer vision in developing assistance systems for guiding the visually impaired in critical situations. Some of those situations includes crosswalks on road crossings and stairs in indoor and outdoor environment. This paper presents an evaluation framework for computer vision-based guiding of the visually impaired persons in such critical situations. Presented framework includes the interface for labeling and storing referent human decisions for guiding directions and compares them to computer vision-based decisions. Since strict evaluation methodology in this research field is not clearly defined and due to the specifics of the transfer of information to visually impaired persons, evaluation criterion for specific simplified guiding instructions is proposed.



### Detecting respiratory motion artefacts for cardiovascular MRIs to ensure high-quality segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.09678v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09678v1)
- **Published**: 2022-09-20 12:29:05+00:00
- **Updated**: 2022-09-20 12:29:05+00:00
- **Authors**: Amin Ranem, John Kalkhof, Caner Özer, Anirban Mukhopadhyay, Ilkay Oksuz
- **Comment**: None
- **Journal**: None
- **Summary**: While machine learning approaches perform well on their training domain, they generally tend to fail in a real-world application. In cardiovascular magnetic resonance imaging (CMR), respiratory motion represents a major challenge in terms of acquisition quality and therefore subsequent analysis and final diagnosis. We present a workflow which predicts a severity score for respiratory motion in CMR for the CMRxMotion challenge 2022. This is an important tool for technicians to immediately provide feedback on the CMR quality during acquisition, as poor-quality images can directly be re-acquired while the patient is still available in the vicinity. Thus, our method ensures that the acquired CMR holds up to a specific quality standard before it is used for further diagnosis. Therefore, it enables an efficient base for proper diagnosis without having time and cost-intensive re-acquisitions in cases of severe motion artefacts. Combined with our segmentation model, this can help cardiologists and technicians in their daily routine by providing a complete pipeline to guarantee proper quality assessment and genuine segmentations for cardiovascular scans. The code base is available at https://github.com/MECLabTUDA/QA_med_data/tree/dev_QA_CMRxMotion.



### Cardiac Segmentation using Transfer Learning under Respiratory Motion Artifacts
- **Arxiv ID**: http://arxiv.org/abs/2209.09714v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09714v1)
- **Published**: 2022-09-20 13:41:16+00:00
- **Updated**: 2022-09-20 13:41:16+00:00
- **Authors**: Carles Garcia-Cabrera, Eric Arazo, Kathleen M. Curran, Noel E. O'Connor, Kevin McGuinness
- **Comment**: accepted for the STACOM2022 workshop @ MICCAI2022
- **Journal**: None
- **Summary**: Methods that are resilient to artifacts in the cardiac magnetic resonance imaging (MRI) while performing ventricle segmentation, are crucial for ensuring quality in structural and functional analysis of those tissues. While there has been significant efforts on improving the quality of the algorithms, few works have tackled the harm that the artifacts generate in the predictions. In this work, we study fine tuning of pretrained networks to improve the resilience of previous methods to these artifacts. In our proposed method, we adopted the extensive usage of data augmentations that mimic those artifacts. The results significantly improved the baseline segmentations (up to 0.06 Dice score, and 4mm Hausdorff distance improvement).



### GANet: Goal Area Network for Motion Forecasting
- **Arxiv ID**: http://arxiv.org/abs/2209.09723v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09723v3)
- **Published**: 2022-09-20 13:54:12+00:00
- **Updated**: 2023-02-22 17:58:18+00:00
- **Authors**: Mingkun Wang, Xinge Zhu, Changqian Yu, Wei Li, Yuexin Ma, Ruochun Jin, Xiaoguang Ren, Dongchun Ren, Mingxu Wang, Wenjing Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting the future motion of road participants is crucial for autonomous driving but is extremely challenging due to staggering motion uncertainty. Recently, most motion forecasting methods resort to the goal-based strategy, i.e., predicting endpoints of motion trajectories as conditions to regress the entire trajectories, so that the search space of solution can be reduced. However, accurate goal coordinates are hard to predict and evaluate. In addition, the point representation of the destination limits the utilization of a rich road context, leading to inaccurate prediction results in many cases. Goal area, i.e., the possible destination area, rather than goal coordinate, could provide a more soft constraint for searching potential trajectories by involving more tolerance and guidance. In view of this, we propose a new goal area-based framework, named Goal Area Network (GANet), for motion forecasting, which models goal areas rather than exact goal coordinates as preconditions for trajectory prediction, performing more robustly and accurately. Specifically, we propose a GoICrop (Goal Area of Interest) operator to effectively extract semantic lane features in goal areas and model actors' future interactions, which benefits a lot for future trajectory estimations. GANet ranks the 1st on the leaderboard of Argoverse Challenge among all public literature (till the paper submission), and its source codes will be released.



### Metal Inpainting in CBCT Projections Using Score-based Generative Model
- **Arxiv ID**: http://arxiv.org/abs/2209.09733v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09733v1)
- **Published**: 2022-09-20 14:07:39+00:00
- **Updated**: 2022-09-20 14:07:39+00:00
- **Authors**: Siyuan Mei, Fuxin Fan, Andreas Maier
- **Comment**: None
- **Journal**: None
- **Summary**: During orthopaedic surgery, the inserting of metallic implants or screws are often performed under mobile C-arm systems. Due to the high attenuation of metals, severe metal artifacts occur in 3D reconstructions, which degrade the image quality greatly. To reduce the artifacts, many metal artifact reduction algorithms have been developed and metal inpainting in projection domain is an essential step. In this work, a score-based generative model is trained on simulated knee projections and the inpainted image is obtained by removing the noise in conditional resampling process. The result implies that the inpainted images by score-based generative model have more detailed information and achieve the lowest mean absolute error and the highest peak-signal-to-noise-ratio compared with interpolation and CNN based method. Besides, the score-based model can also recover projections with big circlar and rectangular masks, showing its generalization in inpainting task.



### wildNeRF: Complete view synthesis of in-the-wild dynamic scenes captured using sparse monocular data
- **Arxiv ID**: http://arxiv.org/abs/2209.10399v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10399v1)
- **Published**: 2022-09-20 14:37:56+00:00
- **Updated**: 2022-09-20 14:37:56+00:00
- **Authors**: Shuja Khalid, Frank Rudzicz
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel neural radiance model that is trainable in a self-supervised manner for novel-view synthesis of dynamic unstructured scenes. Our end-to-end trainable algorithm learns highly complex, real-world static scenes within seconds and dynamic scenes with both rigid and non-rigid motion within minutes. By differentiating between static and motion-centric pixels, we create high-quality representations from a sparse set of images. We perform extensive qualitative and quantitative evaluation on existing benchmarks and set the state-of-the-art on performance measures on the challenging NVIDIA Dynamic Scenes Dataset. Additionally, we evaluate our model performance on challenging real-world datasets such as Cholec80 and SurgicalActions160.



### Dynamic Graph Message Passing Networks for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.09760v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.09760v1)
- **Published**: 2022-09-20 14:41:37+00:00
- **Updated**: 2022-09-20 14:41:37+00:00
- **Authors**: Li Zhang, Mohan Chen, Anurag Arnab, Xiangyang Xue, Philip H. S. Torr
- **Comment**: PAMI extension of CVPR 2020 oral work arXiv:1908.06955
- **Journal**: None
- **Summary**: Modelling long-range dependencies is critical for scene understanding tasks in computer vision. Although convolution neural networks (CNNs) have excelled in many vision tasks, they are still limited in capturing long-range structured relationships as they typically consist of layers of local kernels. A fully-connected graph, such as the self-attention operation in Transformers, is beneficial for such modelling, however, its computational overhead is prohibitive. In this paper, we propose a dynamic graph message passing network, that significantly reduces the computational complexity compared to related works modelling a fully-connected graph. This is achieved by adaptively sampling nodes in the graph, conditioned on the input, for message passing. Based on the sampled nodes, we dynamically predict node-dependent filter weights and the affinity matrix for propagating information between them. This formulation allows us to design a self-attention module, and more importantly a new Transformer-based backbone network, that we use for both image classification pretraining, and for addressing various downstream tasks (object detection, instance and semantic segmentation). Using this model, we show significant improvements with respect to strong, state-of-the-art baselines on four different tasks. Our approach also outperforms fully-connected graphs while using substantially fewer floating-point operations and parameters. Code and models will be made publicly available at https://github.com/fudan-zvg/DGMN2



### An Outlier Exposure Approach to Improve Visual Anomaly Detection Performance for Mobile Robots
- **Arxiv ID**: http://arxiv.org/abs/2209.09786v1
- **DOI**: 10.1109/LRA.2022.3192794
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.09786v1)
- **Published**: 2022-09-20 15:18:13+00:00
- **Updated**: 2022-09-20 15:18:13+00:00
- **Authors**: Dario Mantegazza, Alessandro Giusti, Luca Maria Gambardella, Jérôme Guzzi
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters Volume 7 Issue 4 (Oct. 2022)
  11354-11361
- **Summary**: We consider the problem of building visual anomaly detection systems for mobile robots. Standard anomaly detection models are trained using large datasets composed only of non-anomalous data. However, in robotics applications, it is often the case that (potentially very few) examples of anomalies are available. We tackle the problem of exploiting these data to improve the performance of a Real-NVP anomaly detection model, by minimizing, jointly with the Real-NVP loss, an auxiliary outlier exposure margin loss. We perform quantitative experiments on a novel dataset (which we publish as supplementary material) designed for anomaly detection in an indoor patrolling scenario. On a disjoint test set, our approach outperforms alternatives and shows that exposing even a small number of anomalous frames yields significant performance improvements.



### Thermal infrared image based vehicle detection in low-level illumination conditions using multi-level GANs
- **Arxiv ID**: http://arxiv.org/abs/2209.09808v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09808v2)
- **Published**: 2022-09-20 15:56:52+00:00
- **Updated**: 2023-06-25 07:42:50+00:00
- **Authors**: Shivom Bhargava, Sanjita Prajapati, Pranamesh Chakraborty
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle detection accuracy is fairly accurate in good-illumination conditions but susceptible to poor detection accuracy under low-light conditions. The combined effect of low-light and glare from vehicle headlight or tail-light results in misses in vehicle detection more likely by state-of-the-art object detection models. However, thermal infrared images are robust to illumination changes and are based on thermal radiation. Recently, Generative Adversarial Networks (GANs) have been extensively used in image domain transfer tasks. State-of-the-art GAN models have attempted to improve vehicle detection accuracy in night-time by converting infrared images to day-time RGB images. However, these models have been found to under-perform during night-time conditions compared to day-time conditions, as day-time infrared images looks different than night-time infrared images. Therefore, this study attempts to alleviate this shortcoming by proposing three different approaches based on combination of GAN models at two different levels that try to reduce the feature distribution gap between day-time and night-time infrared images. Quantitative analysis to compare the performance of the proposed models with the state-of-the-art models has been done by testing the models using state-of-the-art object detection models. Both the quantitative and qualitative analyses have shown that the proposed models outperform the state-of-the-art GAN models for vehicle detection in night-time conditions, showing the efficacy of the proposed models.



### High-resolution synthesis of high-density breast mammograms: Application to improved fairness in deep learning based mass detection
- **Arxiv ID**: http://arxiv.org/abs/2209.09809v2
- **DOI**: 10.3389/fonc.2022.1044496
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09809v2)
- **Published**: 2022-09-20 15:57:12+00:00
- **Updated**: 2023-01-24 16:15:33+00:00
- **Authors**: Lidia Garrucho, Kaisar Kushibar, Richard Osuala, Oliver Diaz, Alessandro Catanese, Javier del Riego, Maciej Bobowicz, Fredrik Strand, Laura Igual, Karim Lekadir
- **Comment**: 9 figures, 4 tables
- **Journal**: None
- **Summary**: Computer-aided detection systems based on deep learning have shown good performance in breast cancer detection. However, high-density breasts show poorer detection performance since dense tissues can mask or even simulate masses. Therefore, the sensitivity of mammography for breast cancer detection can be reduced by more than 20% in dense breasts. Additionally, extremely dense cases reported an increased risk of cancer compared to low-density breasts. This study aims to improve the mass detection performance in highdensity breasts using synthetic high-density full-field digital mammograms (FFDM) as data augmentation during breast mass detection model training. To this end, a total of five cycle-consistent GAN (CycleGAN) models using three FFDM datasets were trained for low-to-high-density image translation in highresolution mammograms. The training images were split by breast density BIRADS categories, being BI-RADS A almost entirely fatty and BI-RADS D extremely dense breasts. Our results showed that the proposed data augmentation technique improved the sensitivity and precision of mass detection in models trained with small datasets and improved the domain generalization of the models trained with large databases. In addition, the clinical realism of the synthetic images was evaluated in a reader study involving two expert radiologists and one surgical oncologist.



### Improving Replay-Based Continual Semantic Segmentation with Smart Data Selection
- **Arxiv ID**: http://arxiv.org/abs/2209.09839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09839v1)
- **Published**: 2022-09-20 16:32:06+00:00
- **Updated**: 2022-09-20 16:32:06+00:00
- **Authors**: Tobias Kalb, Björn Mauthe, Jürgen Beyerer
- **Comment**: Accepted at 2022 IEEE Conference on Intelligent Transportation
  Systems (ITSC 2022)
- **Journal**: None
- **Summary**: Continual learning for Semantic Segmentation (CSS) is a rapidly emerging field, in which the capabilities of the segmentation model are incrementally improved by learning new classes or new domains. A central challenge in Continual Learning is overcoming the effects of catastrophic forgetting, which refers to the sudden drop in accuracy on previously learned tasks after the model is trained on new classes or domains. In continual classification this challenge is often overcome by replaying a small selection of samples from previous tasks, however replay is rarely considered in CSS. Therefore, we investigate the influences of various replay strategies for semantic segmentation and evaluate them in class- and domain-incremental settings. Our findings suggest that in a class-incremental setting, it is critical to achieve a uniform distribution for the different classes in the buffer to avoid a bias towards newly learned classes. In the domain-incremental setting, it is most effective to select buffer samples by uniformly sampling from the distribution of learned feature representations or by choosing samples with median entropy. Finally, we observe that the effective sampling methods help to decrease the representation shift significantly in early layers, which is a major cause of forgetting in domain-incremental learning.



### Exploring Inconsistent Knowledge Distillation for Object Detection with Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.09841v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09841v2)
- **Published**: 2022-09-20 16:36:28+00:00
- **Updated**: 2023-08-06 09:43:41+00:00
- **Authors**: Jiawei Liang, Siyuan Liang, Aishan Liu, Ke Ma, Jingzhi Li, Xiaochun Cao
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Knowledge Distillation (KD) for object detection aims to train a compact detector by transferring knowledge from a teacher model. Since the teacher model perceives data in a way different from humans, existing KD methods only distill knowledge that is consistent with labels annotated by human expert while neglecting knowledge that is not consistent with human perception, which results in insufficient distillation and sub-optimal performance. In this paper, we propose inconsistent knowledge distillation (IKD), which aims to distill knowledge inherent in the teacher model's counter-intuitive perceptions. We start by considering the teacher model's counter-intuitive perceptions of frequency and non-robust features. Unlike previous works that exploit fine-grained features or introduce additional regularizations, we extract inconsistent knowledge by providing diverse input using data augmentation. Specifically, we propose a sample-specific data augmentation to transfer the teacher model's ability in capturing distinct frequency components and suggest an adversarial feature augmentation to extract the teacher model's perceptions of non-robust features in the data. Extensive experiments demonstrate the effectiveness of our method which outperforms state-of-the-art KD baselines on one-stage, two-stage and anchor-free object detectors (at most +1.0 mAP). Our codes will be made available at \url{https://github.com/JWLiang007/IKD.git}.



### Frequency Dropout: Feature-Level Regularization via Randomized Filtering
- **Arxiv ID**: http://arxiv.org/abs/2209.09844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09844v1)
- **Published**: 2022-09-20 16:42:21+00:00
- **Updated**: 2022-09-20 16:42:21+00:00
- **Authors**: Mobarakol Islam, Ben Glocker
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Deep convolutional neural networks have shown remarkable performance on various computer vision tasks, and yet, they are susceptible to picking up spurious correlations from the training signal. So called `shortcuts' can occur during learning, for example, when there are specific frequencies present in the image data that correlate with the output predictions. Both high and low frequencies can be characteristic of the underlying noise distribution caused by the image acquisition rather than in relation to the task-relevant information about the image content. Models that learn features related to this characteristic noise will not generalize well to new data.   In this work, we propose a simple yet effective training strategy, Frequency Dropout, to prevent convolutional neural networks from learning frequency-specific imaging features. We employ randomized filtering of feature maps during training which acts as a feature-level regularization. In this study, we consider common image processing filters such as Gaussian smoothing, Laplacian of Gaussian, and Gabor filtering. Our training strategy is model-agnostic and can be used for any computer vision task. We demonstrate the effectiveness of Frequency Dropout on a range of popular architectures and multiple tasks including image classification, domain adaptation, and semantic segmentation using both computer vision and medical imaging datasets. Our results suggest that the proposed approach does not only improve predictive accuracy but also improves robustness against domain shift.



### Fine-grained Classification of Solder Joints with α-skew Jensen-Shannon Divergence
- **Arxiv ID**: http://arxiv.org/abs/2209.09857v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09857v1)
- **Published**: 2022-09-20 17:06:51+00:00
- **Updated**: 2022-09-20 17:06:51+00:00
- **Authors**: Furkan Ulger, Seniha Esen Yuksel, Atila Yilmaz, Dincer Gokcen
- **Comment**: Submitted to IEEE Transactions on Components, Packaging and
  Manufacturing Technology
- **Journal**: None
- **Summary**: Solder joint inspection (SJI) is a critical process in the production of printed circuit boards (PCB). Detection of solder errors during SJI is quite challenging as the solder joints have very small sizes and can take various shapes. In this study, we first show that solders have low feature diversity, and that the SJI can be carried out as a fine-grained image classification task which focuses on hard-to-distinguish object classes. To improve the fine-grained classification accuracy, penalizing confident model predictions by maximizing entropy was found useful in the literature. Inline with this information, we propose using the {\alpha}-skew Jensen-Shannon divergence ({\alpha}-JS) for penalizing the confidence in model predictions. We compare the {\alpha}-JS regularization with both existing entropyregularization based methods and the methods based on attention mechanism, segmentation techniques, transformer models, and specific loss functions for fine-grained image classification tasks. We show that the proposed approach achieves the highest F1-score and competitive accuracy for different models in the finegrained solder joint classification task. Finally, we visualize the activation maps and show that with entropy-regularization, more precise class-discriminative regions are localized, which are also more resilient to noise. Code will be made available here upon acceptance.



### Extremely Simple Activation Shaping for Out-of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.09858v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09858v2)
- **Published**: 2022-09-20 17:09:49+00:00
- **Updated**: 2023-05-01 22:27:08+00:00
- **Authors**: Andrija Djurisic, Nebojsa Bozanic, Arjun Ashok, Rosanne Liu
- **Comment**: Accepted paper at ICLR 2023. 22 pages (9 main + appendix), 9 figures
- **Journal**: None
- **Summary**: The separation between training and deployment of machine learning models implies that not all scenarios encountered in deployment can be anticipated during training, and therefore relying solely on advancements in training has its limits. Out-of-distribution (OOD) detection is an important area that stress-tests a model's ability to handle unseen situations: Do models know when they don't know? Existing OOD detection methods either incur extra training steps, additional data or make nontrivial modifications to the trained network. In contrast, in this work, we propose an extremely simple, post-hoc, on-the-fly activation shaping method, ASH, where a large portion (e.g. 90%) of a sample's activation at a late layer is removed, and the rest (e.g. 10%) simplified or lightly adjusted. The shaping is applied at inference time, and does not require any statistics calculated from training data. Experiments show that such a simple treatment enhances in-distribution and out-of-distribution distinction so as to allow state-of-the-art OOD detection on ImageNet, and does not noticeably deteriorate the in-distribution accuracy. Video, animation and code can be found at: https://andrijazz.github.io/ash



### Open-vocabulary Queryable Scene Representations for Real World Planning
- **Arxiv ID**: http://arxiv.org/abs/2209.09874v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09874v2)
- **Published**: 2022-09-20 17:29:56+00:00
- **Updated**: 2022-10-15 07:05:36+00:00
- **Authors**: Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana Gopalakrishnan, Michael S. Ryoo, Austin Stone, Daniel Kappler
- **Comment**: v2, added references to concurrent work and acknowledgments
- **Journal**: None
- **Summary**: Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: https://nlmap-saycan.github.io



### Diabetic foot ulcers monitoring by employing super resolution and noise reduction deep learning techniques
- **Arxiv ID**: http://arxiv.org/abs/2209.09880v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09880v1)
- **Published**: 2022-09-20 17:35:49+00:00
- **Updated**: 2022-09-20 17:35:49+00:00
- **Authors**: Agapi Davradou, Eftychios Protopapadakis, Maria Kaselimi, Anastasios Doulamis, Nikolaos Doulamis
- **Comment**: None
- **Journal**: None
- **Summary**: Diabetic foot ulcers (DFUs) constitute a serious complication for people with diabetes. The care of DFU patients can be substantially improved through self-management, in order to achieve early-diagnosis, ulcer prevention, and complications management in existing ulcers. In this paper, we investigate two categories of image-to-image translation techniques (ItITT), which will support decision making and monitoring of diabetic foot ulcers: noise reduction and super-resolution. In the former case, we investigated the capabilities on noise removal, for convolutional neural network stacked-autoencoders (CNN-SAE). CNN-SAE was tested on RGB images, induced with Gaussian noise. The latter scenario involves the deployment of four deep learning super-resolution models. The performance of all models, for both scenarios, was evaluated in terms of execution time and perceived quality. Results indicate that applied techniques consist a viable and easy to implement alternative that should be used by any system designed for DFU monitoring.



### Leveraging Local Patch Differences in Multi-Object Scenes for Generative Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2209.09883v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09883v2)
- **Published**: 2022-09-20 17:36:32+00:00
- **Updated**: 2022-10-03 19:30:56+00:00
- **Authors**: Abhishek Aich, Shasha Li, Chengyu Song, M. Salman Asif, Srikanth V. Krishnamurthy, Amit K. Roy-Chowdhury
- **Comment**: Accepted at WACV 2023 (Round 1), camera-ready version
- **Journal**: None
- **Summary**: State-of-the-art generative model-based attacks against image classifiers overwhelmingly focus on single-object (i.e., single dominant object) images. Different from such settings, we tackle a more practical problem of generating adversarial perturbations using multi-object (i.e., multiple dominant objects) images as they are representative of most real-world scenes. Our goal is to design an attack strategy that can learn from such natural scenes by leveraging the local patch differences that occur inherently in such images (e.g. difference between the local patch on the object `person' and the object `bike' in a traffic scene). Our key idea is to misclassify an adversarial multi-object image by confusing the victim classifier for each local patch in the image. Based on this, we propose a novel generative attack (called Local Patch Difference or LPD-Attack) where a novel contrastive loss function uses the aforesaid local differences in feature space of multi-object scenes to optimize the perturbation generator. Through various experiments across diverse victim convolutional neural networks, we show that our approach outperforms baseline generative attacks with highly transferable perturbations when evaluated under different white-box and black-box settings.



### Improving GANs with A Dynamic Discriminator
- **Arxiv ID**: http://arxiv.org/abs/2209.09897v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09897v1)
- **Published**: 2022-09-20 17:57:33+00:00
- **Updated**: 2022-09-20 17:57:33+00:00
- **Authors**: Ceyuan Yang, Yujun Shen, Yinghao Xu, Deli Zhao, Bo Dai, Bolei Zhou
- **Comment**: To appear in NeurIPS 2022
- **Journal**: None
- **Summary**: Discriminator plays a vital role in training generative adversarial networks (GANs) via distinguishing real and synthesized samples. While the real data distribution remains the same, the synthesis distribution keeps varying because of the evolving generator, and thus effects a corresponding change to the bi-classification task for the discriminator. We argue that a discriminator with an on-the-fly adjustment on its capacity can better accommodate such a time-varying task. A comprehensive empirical study confirms that the proposed training strategy, termed as DynamicD, improves the synthesis performance without incurring any additional computation cost or training objectives. Two capacity adjusting schemes are developed for training GANs under different data regimes: i) given a sufficient amount of training data, the discriminator benefits from a progressively increased learning capacity, and ii) when the training data is limited, gradually decreasing the layer width mitigates the over-fitting issue of the discriminator. Experiments on both 2D and 3D-aware image synthesis tasks conducted on a range of datasets substantiate the generalizability of our DynamicD as well as its substantial improvement over the baselines. Furthermore, DynamicD is synergistic to other discriminator-improving approaches (including data augmentation, regularizers, and pre-training), and brings continuous performance gain when combined for learning GANs.



### Text2Light: Zero-Shot Text-Driven HDR Panorama Generation
- **Arxiv ID**: http://arxiv.org/abs/2209.09898v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.09898v3)
- **Published**: 2022-09-20 17:58:44+00:00
- **Updated**: 2023-04-14 12:22:40+00:00
- **Authors**: Zhaoxi Chen, Guangcong Wang, Ziwei Liu
- **Comment**: SIGGRAPH Asia 2022; Project Page
  https://frozenburning.github.io/projects/text2light/ Codes are available at
  https://github.com/FrozenBurning/Text2Light
- **Journal**: None
- **Summary**: High-quality HDRIs(High Dynamic Range Images), typically HDR panoramas, are one of the most popular ways to create photorealistic lighting and 360-degree reflections of 3D scenes in graphics. Given the difficulty of capturing HDRIs, a versatile and controllable generative model is highly desired, where layman users can intuitively control the generation process. However, existing state-of-the-art methods still struggle to synthesize high-quality panoramas for complex scenes. In this work, we propose a zero-shot text-driven framework, Text2Light, to generate 4K+ resolution HDRIs without paired training data. Given a free-form text as the description of the scene, we synthesize the corresponding HDRI with two dedicated steps: 1) text-driven panorama generation in low dynamic range(LDR) and low resolution, and 2) super-resolution inverse tone mapping to scale up the LDR panorama both in resolution and dynamic range. Specifically, to achieve zero-shot text-driven panorama generation, we first build dual codebooks as the discrete representation for diverse environmental textures. Then, driven by the pre-trained CLIP model, a text-conditioned global sampler learns to sample holistic semantics from the global codebook according to the input text. Furthermore, a structure-aware local sampler learns to synthesize LDR panoramas patch-by-patch, guided by holistic semantics. To achieve super-resolution inverse tone mapping, we derive a continuous representation of 360-degree imaging from the LDR panorama as a set of structured latent codes anchored to the sphere. This continuous representation enables a versatile module to upscale the resolution and dynamic range simultaneously. Extensive experiments demonstrate the superior capability of Text2Light in generating high-quality HDR panoramas. In addition, we show the feasibility of our work in realistic rendering and immersive VR.



### Superpixel Generation and Clustering for Weakly Supervised Brain Tumor Segmentation in MR Images
- **Arxiv ID**: http://arxiv.org/abs/2209.09930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09930v1)
- **Published**: 2022-09-20 18:08:34+00:00
- **Updated**: 2022-09-20 18:08:34+00:00
- **Authors**: Jay J. Yoo, Khashayar Namdar, Farzad Khalvati
- **Comment**: None
- **Journal**: None
- **Summary**: Training Machine Learning (ML) models to segment tumors and other anomalies in medical images is an increasingly popular area of research but generally requires manually annotated ground truth segmentations which necessitates significant time and resources to create. This work proposes a pipeline of ML models that utilize binary classification labels, which can be easily acquired, to segment ROIs without requiring ground truth annotations. We used 2D slices of Magnetic Resonance Imaging (MRI) brain scans from the Multimodal Brain Tumor Segmentation Challenge (BraTS) 2020 dataset and labels indicating the presence of high-grade glioma (HGG) tumors to train the pipeline. Our pipeline also introduces a novel variation of deep learning-based superpixel generation, which enables training guided by clustered superpixels and simultaneously trains a superpixel clustering model. On our test set, our pipeline's segmentations achieved a Dice coefficient of 61.7%, which is a substantial improvement over the 42.8% Dice coefficient acquired when the popular Local Interpretable Model-Agnostic Explanations (LIME) method was used.



### Adversarial Bi-Regressor Network for Domain Adaptive Regression
- **Arxiv ID**: http://arxiv.org/abs/2209.09943v1
- **DOI**: 10.24963/ijcai.2022/501
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09943v1)
- **Published**: 2022-09-20 18:38:28+00:00
- **Updated**: 2022-09-20 18:38:28+00:00
- **Authors**: Haifeng Xia, Pu, Wang, Toshiaki Koike-Akino, Ye Wang, Philip Orlik, Zhengming Ding
- **Comment**: 7 pages, 5 figures; IJCAI 2022; tested in the SPAWC2021 dataset for
  indoor localization
- **Journal**: None
- **Summary**: Domain adaptation (DA) aims to transfer the knowledge of a well-labeled source domain to facilitate unlabeled target learning. When turning to specific tasks such as indoor (Wi-Fi) localization, it is essential to learn a cross-domain regressor to mitigate the domain shift. This paper proposes a novel method Adversarial Bi-Regressor Network (ABRNet) to seek more effective cross-domain regression model. Specifically, a discrepant bi-regressor architecture is developed to maximize the difference of bi-regressor to discover uncertain target instances far from the source distribution, and then an adversarial training mechanism is adopted between feature extractor and dual regressors to produce domain-invariant representations. To further bridge the large domain gap, a domain-specific augmentation module is designed to synthesize two source-similar and target-similar intermediate domains to gradually eliminate the original domain mismatch. The empirical studies on two cross-domain regressive benchmarks illustrate the power of our method on solving the domain adaptive regression (DAR) problem.



### Learning Sparse Latent Representations for Generator Model
- **Arxiv ID**: http://arxiv.org/abs/2209.09949v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09949v1)
- **Published**: 2022-09-20 18:58:24+00:00
- **Updated**: 2022-09-20 18:58:24+00:00
- **Authors**: Hanao Li, Tian Han
- **Comment**: None
- **Journal**: None
- **Summary**: Sparsity is a desirable attribute. It can lead to more efficient and more effective representations compared to the dense model. Meanwhile, learning sparse latent representations has been a challenging problem in the field of computer vision and machine learning due to its complexity. In this paper, we present a new unsupervised learning method to enforce sparsity on the latent space for the generator model with a gradually sparsified spike and slab distribution as our prior. Our model consists of only one top-down generator network that maps the latent variable to the observed data. Latent variables can be inferred following generator posterior direction using non-persistent gradient based method. Spike and Slab regularization in the inference step can push non-informative latent dimensions towards zero to induce sparsity. Extensive experiments show the model can preserve majority of the information from original images with sparse representations while demonstrating improved results compared to other existing methods. We observe that our model can learn disentangled semantics and increase explainability of the latent codes while boosting the robustness in the task of classification and denoising.



### MARIO: Modular and Extensible Architecture for Computing Visual Statistics in RoboCup SPL
- **Arxiv ID**: http://arxiv.org/abs/2209.09987v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.09987v1)
- **Published**: 2022-09-20 20:45:56+00:00
- **Updated**: 2022-09-20 20:45:56+00:00
- **Authors**: Domenico D. Bloisi, Andrea Pennisi, Cristian Zampino, Flavio Biancospino, Francesco Laus, Gianluca Di Stefano, Michele Brienza, Rocchina Romano
- **Comment**: None
- **Journal**: None
- **Summary**: This technical report describes a modular and extensible architecture for computing visual statistics in RoboCup SPL (MARIO), presented during the SPL Open Research Challenge at RoboCup 2022, held in Bangkok (Thailand). MARIO is an open-source, ready-to-use software application whose final goal is to contribute to the growth of the RoboCup SPL community. MARIO comes with a GUI that integrates multiple machine learning and computer vision based functions, including automatic camera calibration, background subtraction, homography computation, player + ball tracking and localization, NAO robot pose estimation and fall detection. MARIO has been ranked no. 1 in the Open Research Challenge.



### Subjective Assessment of High Dynamic Range Videos Under Different Ambient Conditions
- **Arxiv ID**: http://arxiv.org/abs/2209.10005v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.10005v1)
- **Published**: 2022-09-20 21:25:50+00:00
- **Updated**: 2022-09-20 21:25:50+00:00
- **Authors**: Zaixi Shang, Joshua P. Ebenezer, Alan C. Bovik, Yongjun Wu, Hai Wei, Sriram Sethuraman
- **Comment**: None
- **Journal**: None
- **Summary**: High Dynamic Range (HDR) videos can represent a much greater range of brightness and color than Standard Dynamic Range (SDR) videos and are rapidly becoming an industry standard. HDR videos have more challenging capture, transmission, and display requirements than legacy SDR videos. With their greater bit depth, advanced electro-optical transfer functions, and wider color gamuts, comes the need for video quality algorithms that are specifically designed to predict the quality of HDR videos. Towards this end, we present the first publicly released large-scale subjective study of HDR videos. We study the effect of distortions such as compression and aliasing on the quality of HDR videos. We also study the effect of ambient illumination on perceptual quality of HDR videos by conducting the study in both a dark lab environment and a brighter living-room environment. A total of 66 subjects participated in the study and more than 20,000 opinion scores were collected, which makes this the largest in-lab study of HDR video quality ever. We anticipate that the dataset will be a valuable resource for researchers to develop better models of perceptual quality for HDR videos.



### Fine-Grained VR Sketching: Dataset and Insights
- **Arxiv ID**: http://arxiv.org/abs/2209.10008v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10008v1)
- **Published**: 2022-09-20 21:30:54+00:00
- **Updated**: 2022-09-20 21:30:54+00:00
- **Authors**: Ling Luo, Yulia Gryaditskaya, Yongxin Yang, Tao Xiang, Yi-Zhe Song
- **Comment**: None
- **Journal**: 2021 International Conference on 3D Vision (3DV), pp. 1003-1013.
  IEEE, 2021
- **Summary**: We present the first fine-grained dataset of 1,497 3D VR sketch and 3D shape pairs of a chair category with large shapes diversity. Our dataset supports the recent trend in the sketch community on fine-grained data analysis, and extends it to an actively developing 3D domain. We argue for the most convenient sketching scenario where the sketch consists of sparse lines and does not require any sketching skills, prior training or time-consuming accurate drawing. We then, for the first time, study the scenario of fine-grained 3D VR sketch to 3D shape retrieval, as a novel VR sketching application and a proving ground to drive out generic insights to inform future research. By experimenting with carefully selected combinations of design factors on this new problem, we draw important conclusions to help follow-on work. We hope our dataset will enable other novel applications, especially those that require a fine-grained angle such as fine-grained 3D shape reconstruction. The dataset is available at tinyurl.com/VRSketch3DV21.



### Towards 3D VR-Sketch to 3D Shape Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2209.10020v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10020v1)
- **Published**: 2022-09-20 22:04:31+00:00
- **Updated**: 2022-09-20 22:04:31+00:00
- **Authors**: Ling Luo, Yulia Gryaditskaya, Yongxin Yang, Tao Xiang, Yi-Zhe Song
- **Comment**: None
- **Journal**: 2020 International Conference on 3D Vision (3DV), pp. 81-90. IEEE,
  2020
- **Summary**: Growing free online 3D shapes collections dictated research on 3D retrieval. Active debate has however been had on (i) what the best input modality is to trigger retrieval, and (ii) the ultimate usage scenario for such retrieval. In this paper, we offer a different perspective towards answering these questions -- we study the use of 3D sketches as an input modality and advocate a VR-scenario where retrieval is conducted. Thus, the ultimate vision is that users can freely retrieve a 3D model by air-doodling in a VR environment. As a first stab at this new 3D VR-sketch to 3D shape retrieval problem, we make four contributions. First, we code a VR utility to collect 3D VR-sketches and conduct retrieval. Second, we collect the first set of $167$ 3D VR-sketches on two shape categories from ModelNet. Third, we propose a novel approach to generate a synthetic dataset of human-like 3D sketches of different abstract levels to train deep networks. At last, we compare the common multi-view and volumetric approaches: We show that, in contrast to 3D shape to 3D shape retrieval, volumetric point-based approaches exhibit superior performance on 3D sketch to 3D shape retrieval due to the sparse and abstract nature of 3D VR-sketches. We believe these contributions will collectively serve as enablers for future attempts at this problem. The VR interface, code and datasets are available at https://tinyurl.com/3DSketch3DV.



### Fast-Image2Point: Towards Real-Time Point Cloud Reconstruction of a Single Image using 3D Supervision
- **Arxiv ID**: http://arxiv.org/abs/2209.10029v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10029v1)
- **Published**: 2022-09-20 22:39:14+00:00
- **Updated**: 2022-09-20 22:39:14+00:00
- **Authors**: AmirHossein Zamani, Amir G. Aghdam, Kamran Ghaffari T
- **Comment**: 1- The paper has 8 pages 2- The paper has already been accepted in
  IEEE International Conference on Machine Learning and Applications (ICMLA)
  2022 and it will appear in IEEE ICMLA 2022 proceedings
- **Journal**: None
- **Summary**: A key question in the problem of 3D reconstruction is how to train a machine or a robot to model 3D objects. Many tasks like navigation in real-time systems such as autonomous vehicles directly depend on this problem. These systems usually have limited computational power. Despite considerable progress in 3D reconstruction systems in recent years, applying them to real-time systems such as navigation systems in autonomous vehicles is still challenging due to the high complexity and computational demand of the existing methods. This study addresses current problems in reconstructing objects displayed in a single-view image in a faster (real-time) fashion. To this end, a simple yet powerful deep neural framework is developed. The proposed framework consists of two components: the feature extractor module and the 3D generator module. We use point cloud representation for the output of our reconstruction module. The ShapeNet dataset is utilized to compare the method with the existing results in terms of computation time and accuracy. Simulations demonstrate the superior performance of the proposed method.   Index Terms-Real-time 3D reconstruction, single-view reconstruction, supervised learning, deep neural network



### MTR-A: 1st Place Solution for 2022 Waymo Open Dataset Challenge -- Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/2209.10033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.10033v1)
- **Published**: 2022-09-20 23:03:22+00:00
- **Updated**: 2022-09-20 23:03:22+00:00
- **Authors**: Shaoshuai Shi, Li Jiang, Dengxin Dai, Bernt Schiele
- **Comment**: The 1st place solution report for Waymo Motion Prediction Challenge
  of Workshop on Autonomous Driving of CVPR 2022
- **Journal**: None
- **Summary**: In this report, we present the 1st place solution for motion prediction track in 2022 Waymo Open Dataset Challenges. We propose a novel Motion Transformer framework for multimodal motion prediction, which introduces a small set of novel motion query pairs for generating better multimodal future trajectories by jointly performing the intention localization and iterative motion refinement. A simple model ensemble strategy with non-maximum-suppression is adopted to further boost the final performance. Our approach achieves the 1st place on the motion prediction leaderboard of 2022 Waymo Open Dataset Challenges, outperforming other methods with remarkable margins. Code will be available at https://github.com/sshaoshuai/MTR.



### Traffic Accident Risk Forecasting using Contextual Vision Transformers
- **Arxiv ID**: http://arxiv.org/abs/2209.11180v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.11180v1)
- **Published**: 2022-09-20 23:38:06+00:00
- **Updated**: 2022-09-20 23:38:06+00:00
- **Authors**: Khaled Saleh, Artur Grigorev, Adriana-Simona Mihaita
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the problem of traffic accident risk forecasting has been getting the attention of the intelligent transportation systems community due to its significant impact on traffic clearance. This problem is commonly tackled in the literature by using data-driven approaches that model the spatial and temporal incident impact, since they were shown to be crucial for the traffic accident risk forecasting problem. To achieve this, most approaches build different architectures to capture the spatio-temporal correlations features, making them inefficient for large traffic accident datasets. Thus, in this work, we are proposing a novel unified framework, namely a contextual vision transformer, that can be trained in an end-to-end approach which can effectively reason about the spatial and temporal aspects of the problem while providing accurate traffic accident risk predictions. We evaluate and compare the performance of our proposed methodology against baseline approaches from the literature across two large-scale traffic accident datasets from two different geographical locations. The results have shown a significant improvement with roughly 2\% in RMSE score in comparison to previous state-of-art works (SoTA) in the literature. Moreover, our proposed approach has outperformed the SoTA technique over the two datasets while only requiring 23x fewer computational requirements.



