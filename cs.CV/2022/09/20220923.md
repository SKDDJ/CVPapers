# Arxiv Papers in cs.CV on 2022-09-23
### Learning Interpretable Dynamics from Images of a Freely Rotating 3D Rigid Body
- **Arxiv ID**: http://arxiv.org/abs/2209.11355v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.11355v3)
- **Published**: 2022-09-23 00:35:22+00:00
- **Updated**: 2023-08-23 14:51:47+00:00
- **Authors**: Justice Mason, Christine Allen-Blanchette, Nicholas Zolman, Elizabeth Davison, Naomi Leonard
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: In many real-world settings, image observations of freely rotating 3D rigid bodies, such as satellites, may be available when low-dimensional measurements are not. However, the high-dimensionality of image data precludes the use of classical estimation techniques to learn the dynamics and a lack of interpretability reduces the usefulness of standard deep learning methods. In this work, we present a physics-informed neural network model to estimate and predict 3D rotational dynamics from image sequences. We achieve this using a multi-stage prediction pipeline that maps individual images to a latent representation homeomorphic to $\mathbf{SO}(3)$, computes angular velocities from latent pairs, and predicts future latent states using the Hamiltonian equations of motion with a learned representation of the Hamiltonian. We demonstrate the efficacy of our approach on a new rotating rigid-body dataset with sequences of rotating cubes and rectangular prisms with uniform and non-uniform density.



### NasHD: Efficient ViT Architecture Performance Ranking using Hyperdimensional Computing
- **Arxiv ID**: http://arxiv.org/abs/2209.11356v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2209.11356v1)
- **Published**: 2022-09-23 00:41:31+00:00
- **Updated**: 2022-09-23 00:41:31+00:00
- **Authors**: Dongning Ma, Pengfei Zhao, Xun Jiao
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) is an automated architecture engineering method for deep learning design automation, which serves as an alternative to the manual and error-prone process of model development, selection, evaluation and performance estimation. However, one major obstacle of NAS is the extremely demanding computation resource requirements and time-consuming iterations particularly when the dataset scales. In this paper, targeting at the emerging vision transformer (ViT), we present NasHD, a hyperdimensional computing based supervised learning model to rank the performance given the architectures and configurations. Different from other learning based methods, NasHD is faster thanks to the high parallel processing of HDC architecture. We also evaluated two HDC encoding schemes: Gram-based and Record-based of NasHD on their performance and efficiency. On the VIMER-UFO benchmark dataset of 8 applications from a diverse range of domains, NasHD Record can rank the performance of nearly 100K vision transformer models with about 1 minute while still achieving comparable results with sophisticated models.



### CUTS: A Fully Unsupervised Framework for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.11359v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11359v5)
- **Published**: 2022-09-23 01:09:06+00:00
- **Updated**: 2023-08-08 07:04:02+00:00
- **Authors**: Chen Liu, Matthew Amodio, Liangbo L. Shen, Feng Gao, Arman Avesta, Sanjay Aneja, Jay C. Wang, Lucian V. Del Priore, Smita Krishnaswamy
- **Comment**: Updated Supplementary Figure S3 with the correct version. Replaced
  most figures with smaller-sized (roughly 1MB->300KB)
- **Journal**: None
- **Summary**: In this work we introduce CUTS (Contrastive and Unsupervised Training for Segmentation), a fully unsupervised deep learning framework for medical image segmentation to better utilize the vast majority of imaging data that is not labeled or annotated. We utilize self-supervision from pixels and their local neighborhoods in the images themselves. Our unsupervised approach optimizes a training objective that leverages concepts from contrastive learning and autoencoding. Our framework segments medical images with a novel two-stage approach without relying on any labeled data at any stage. The first stage involves the creation of a "pixel-centered patch" that embeds every pixel along with its surrounding patch, using a vector representation in a high-dimensional latent embedding space. The second stage utilizes diffusion condensation, a multi-scale topological data analysis approach, to dynamically coarse-grain these embedding vectors at all levels of granularity. The final outcome is a series of coarse-to-fine segmentations that highlight image structures at various scales. In this work, we show successful multi-scale segmentation on natural images, retinal fundus images, and brain MRI images. Our framework delineates structures and patterns at different scales which, in the cases of medical images, may carry distinct information relevant to clinical interpretation. Quantitatively, our framework demonstrates improvements ranging from 10% to 200% on dice coefficient and Hausdorff distance compared to existing unsupervised methods across three medical image datasets. As we tackle the problem of segmenting medical images at multiple meaningful granularities without relying on any label, we hope to demonstrate the possibility to circumvent tedious and repetitive manual annotations in future practice.



### Tensor-Based Multi-Modality Feature Selection and Regression for Alzheimer's Disease Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/2209.11372v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11372v1)
- **Published**: 2022-09-23 02:17:27+00:00
- **Updated**: 2022-09-23 02:17:27+00:00
- **Authors**: Jun Yu, Zhaoming Kong, Liang Zhan, Li Shen, Lifang He
- **Comment**: None
- **Journal**: 2022 8th International Conference on Bioinformatics and
  Biosciences
- **Summary**: The assessment of Alzheimer's Disease (AD) and Mild Cognitive Impairment (MCI) associated with brain changes remains a challenging task. Recent studies have demonstrated that combination of multi-modality imaging techniques can better reflect pathological characteristics and contribute to more accurate diagnosis of AD and MCI. In this paper, we propose a novel tensor-based multi-modality feature selection and regression method for diagnosis and biomarker identification of AD and MCI from normal controls. Specifically, we leverage the tensor structure to exploit high-level correlation information inherent in the multi-modality data, and investigate tensor-level sparsity in the multilinear regression model. We present the practical advantages of our method for the analysis of ADNI data using three imaging modalities (VBM- MRI, FDG-PET and AV45-PET) with clinical parameters of disease severity and cognitive scores. The experimental results demonstrate the superior performance of our proposed method against the state-of-the-art for the disease diagnosis and the identification of disease-specific regions and modality-related differences. The code for this work is publicly available at https://github.com/junfish/BIOS22.



### Soft-labeling Strategies for Rapid Sub-Typing
- **Arxiv ID**: http://arxiv.org/abs/2209.12684v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.12684v2)
- **Published**: 2022-09-23 03:04:27+00:00
- **Updated**: 2023-01-19 16:41:19+00:00
- **Authors**: Grant Rosario, David Noever, Matt Ciolino
- **Comment**: None
- **Journal**: None
- **Summary**: The challenge of labeling large example datasets for computer vision continues to limit the availability and scope of image repositories. This research provides a new method for automated data collection, curation, labeling, and iterative training with minimal human intervention for the case of overhead satellite imagery and object detection. The new operational scale effectively scanned an entire city (68 square miles) in grid search and yielded a prediction of car color from space observations. A partially trained yolov5 model served as an initial inference seed to output further, more refined model predictions in iterative cycles. Soft labeling here refers to accepting label noise as a potentially valuable augmentation to reduce overfitting and enhance generalized predictions to previously unseen test data. The approach takes advantage of a real-world instance where a cropped image of a car can automatically receive sub-type information as white or colorful from pixel values alone, thus completing an end-to-end pipeline without overdependence on human labor.



### LGDN: Language-Guided Denoising Network for Video-Language Modeling
- **Arxiv ID**: http://arxiv.org/abs/2209.11388v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2209.11388v3)
- **Published**: 2022-09-23 03:35:59+00:00
- **Updated**: 2022-12-05 07:20:42+00:00
- **Authors**: Haoyu Lu, Mingyu Ding, Nanyi Fei, Yuqi Huo, Zhiwu Lu
- **Comment**: Accepted by NeurIPS2022
- **Journal**: None
- **Summary**: Video-language modeling has attracted much attention with the rapid growth of web videos. Most existing methods assume that the video frames and text description are semantically correlated, and focus on video-language modeling at video level. However, this hypothesis often fails for two reasons: (1) With the rich semantics of video contents, it is difficult to cover all frames with a single video-level description; (2) A raw video typically has noisy/meaningless information (e.g., scenery shot, transition or teaser). Although a number of recent works deploy attention mechanism to alleviate this problem, the irrelevant/noisy information still makes it very difficult to address. To overcome such challenge, we thus propose an efficient and effective model, termed Language-Guided Denoising Network (LGDN), for video-language modeling. Different from most existing methods that utilize all extracted video frames, LGDN dynamically filters out the misaligned or redundant frames under the language supervision and obtains only 2--4 salient frames per video for cross-modal token-level alignment. Extensive experiments on five public datasets show that our LGDN outperforms the state-of-the-arts by large margins. We also provide detailed ablation study to reveal the critical importance of solving the noise issue, in hope of inspiring future video-language work.



### Towards Frame Rate Agnostic Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2209.11404v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11404v3)
- **Published**: 2022-09-23 04:25:19+00:00
- **Updated**: 2023-04-18 02:15:17+00:00
- **Authors**: Weitao Feng, Lei Bai, Yongqiang Yao, Fengwei Yu, Wanli Ouyang
- **Comment**: 24 pages; Author version
- **Journal**: None
- **Summary**: Multi-Object Tracking (MOT) is one of the most fundamental computer vision tasks that contributes to various video analysis applications. Despite the recent promising progress, current MOT research is still limited to a fixed sampling frame rate of the input stream. In fact, we empirically found that the accuracy of all recent state-of-the-art trackers drops dramatically when the input frame rate changes. For a more intelligent tracking solution, we shift the attention of our research work to the problem of Frame Rate Agnostic MOT (FraMOT), which takes frame rate insensitivity into consideration. In this paper, we propose a Frame Rate Agnostic MOT framework with a Periodic training Scheme (FAPS) to tackle the FraMOT problem for the first time. Specifically, we propose a Frame Rate Agnostic Association Module (FAAM) that infers and encodes the frame rate information to aid identity matching across multi-frame-rate inputs, improving the capability of the learned model in handling complex motion-appearance relations in FraMOT. Moreover, the association gap between training and inference is enlarged in FraMOT because those post-processing steps not included in training make a larger difference in lower frame rate scenarios. To address it, we propose Periodic Training Scheme (PTS) to reflect all post-processing steps in training via tracking pattern matching and fusion. Along with the proposed approaches, we make the first attempt to establish an evaluation method for this new task of FraMOT in two different modes, i.e., known frame rate and unknown frame rate, aiming to handle a more complex situation. The quantitative experiments on the challenging MOT17/20 dataset (FraMOT version) have clearly demonstrated that the proposed approaches can handle different frame rates better and thus improve the robustness against complicated scenarios.



### Recent trends and analysis of Generative Adversarial Networks in Cervical Cancer Imaging
- **Arxiv ID**: http://arxiv.org/abs/2209.12680v1
- **DOI**: 10.14704/nq.2022.20.9.NQ44381
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.12680v1)
- **Published**: 2022-09-23 05:45:40+00:00
- **Updated**: 2022-09-23 05:45:40+00:00
- **Authors**: Tamanna Sood
- **Comment**: 7 pages, proceedings of ICCS 2022
  (https://www.cecmohali.org/iccs2022/), published in neuroquantology
  (https://www.neuroquantology.com/article.php?id=7222 ,
  DOI:10.14704/nq.2022.20.9.NQ44381 )
- **Journal**: Neuro Quantology, September 2022, Volume 20, Issue 9, Page
  3302-3308
- **Summary**: Cervical cancer is one of the most common types of cancer found in females. It contributes to 6-29% of all cancers in women. It is caused by the Human Papilloma Virus (HPV). The 5-year survival chances of cervical cancer range from 17%-92% depending upon the stage at which it is detected. Early detection of this disease helps in better treatment and survival rate of the patient. Many deep learning algorithms are being used for the detection of cervical cancer these days. A special category of deep learning techniques known as Generative Adversarial Networks (GANs) are catching up with speed in the screening, detection, and classification of cervical cancer. In this work, we present a detailed analysis of the recent trends relating to the use of various GAN models, their applications, and the evaluation metrics used for their performance evaluation in the field of cervical cancer imaging.



### Learning to screen Glaucoma like the ophthalmologists
- **Arxiv ID**: http://arxiv.org/abs/2209.11431v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11431v1)
- **Published**: 2022-09-23 06:17:29+00:00
- **Updated**: 2022-09-23 06:17:29+00:00
- **Authors**: Junde Wu, Huihui Fang, Fei Li, Huazhu Fu, Yanwu Xu
- **Comment**: None
- **Journal**: None
- **Summary**: GAMMA Challenge is organized to encourage the AI models to screen the glaucoma from a combination of 2D fundus image and 3D optical coherence tomography volume, like the ophthalmologists.



### Understanding Open-Set Recognition by Jacobian Norm of Representation
- **Arxiv ID**: http://arxiv.org/abs/2209.11436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11436v1)
- **Published**: 2022-09-23 06:31:36+00:00
- **Updated**: 2022-09-23 06:31:36+00:00
- **Authors**: Jaewoo Park, Hojin Park, Eunju Jeong, Andrew Beng Jin Teoh
- **Comment**: None
- **Journal**: None
- **Summary**: In contrast to conventional closed-set recognition, open-set recognition (OSR) assumes the presence of an unknown class, which is not seen to a model during training. One predominant approach in OSR is metric learning, where a model is trained to separate the inter-class representations of known class data. Numerous works in OSR reported that, even though the models are trained only with the known class data, the models become aware of the unknown, and learn to separate the unknown class representations from the known class representations. This paper analyzes this emergent phenomenon by observing the Jacobian norm of representation. We theoretically show that minimizing the intra-class distances within the known set reduces the Jacobian norm of known class representations while maximizing the inter-class distances within the known set increases the Jacobian norm of the unknown class. The closed-set metric learning thus separates the unknown from the known by forcing their Jacobian norm values to differ. We empirically validate our theoretical framework with ample pieces of evidence using standard OSR datasets. Moreover, under our theoretical framework, we explain how the standard deep learning techniques can be helpful for OSR and use the framework as a guiding principle to develop an effective OSR model.



### Rethinking Performance Gains in Image Dehazing Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.11448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11448v1)
- **Published**: 2022-09-23 07:14:48+00:00
- **Updated**: 2022-09-23 07:14:48+00:00
- **Authors**: Yuda Song, Yang Zhou, Hui Qian, Xin Du
- **Comment**: None
- **Journal**: None
- **Summary**: Image dehazing is an active topic in low-level vision, and many image dehazing networks have been proposed with the rapid development of deep learning. Although these networks' pipelines work fine, the key mechanism to improving image dehazing performance remains unclear. For this reason, we do not target to propose a dehazing network with fancy modules; rather, we make minimal modifications to popular U-Net to obtain a compact dehazing network. Specifically, we swap out the convolutional blocks in U-Net for residual blocks with the gating mechanism, fuse the feature maps of main paths and skip connections using the selective kernel, and call the resulting U-Net variant gUNet. As a result, with a significantly reduced overhead, gUNet is superior to state-of-the-art methods on multiple image dehazing datasets. Finally, we verify these key designs to the performance gain of image dehazing networks through extensive ablation studies.



### Motion Guided Deep Dynamic 3D Garments
- **Arxiv ID**: http://arxiv.org/abs/2209.11449v1
- **DOI**: 10.1145/3550454.3555485
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.11449v1)
- **Published**: 2022-09-23 07:17:46+00:00
- **Updated**: 2022-09-23 07:17:46+00:00
- **Authors**: Meng Zhang, Duygu Ceylan, Niloy J. Mitra
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Realistic dynamic garments on animated characters have many AR/VR applications. While authoring such dynamic garment geometry is still a challenging task, data-driven simulation provides an attractive alternative, especially if it can be controlled simply using the motion of the underlying character. In this work, we focus on motion guided dynamic 3D garments, especially for loose garments. In a data-driven setup, we first learn a generative space of plausible garment geometries. Then, we learn a mapping to this space to capture the motion dependent dynamic deformations, conditioned on the previous state of the garment as well as its relative position with respect to the underlying body. Technically, we model garment dynamics, driven using the input character motion, by predicting per-frame local displacements in a canonical state of the garment that is enriched with frame-dependent skinning weights to bring the garment to the global space. We resolve any remaining per-frame collisions by predicting residual local displacements. The resultant garment geometry is used as history to enable iterative rollout prediction. We demonstrate plausible generalization to unseen body shapes and motion inputs, and show improvements over multiple state-of-the-art alternatives.



### Modular Degradation Simulation and Restoration for Under-Display Camera
- **Arxiv ID**: http://arxiv.org/abs/2209.11455v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11455v1)
- **Published**: 2022-09-23 07:36:07+00:00
- **Updated**: 2022-09-23 07:36:07+00:00
- **Authors**: Yang Zhou, Yuda Song, Xin Du
- **Comment**: None
- **Journal**: None
- **Summary**: Under-display camera (UDC) provides an elegant solution for full-screen smartphones. However, UDC captured images suffer from severe degradation since sensors lie under the display. Although this issue can be tackled by image restoration networks, these networks require large-scale image pairs for training. To this end, we propose a modular network dubbed MPGNet trained using the generative adversarial network (GAN) framework for simulating UDC imaging. Specifically, we note that the UDC imaging degradation process contains brightness attenuation, blurring, and noise corruption. Thus we model each degradation with a characteristic-related modular network, and all modular networks are cascaded to form the generator. Together with a pixel-wise discriminator and supervised loss, we can train the generator to simulate the UDC imaging degradation process. Furthermore, we present a Transformer-style network named DWFormer for UDC image restoration. For practical purposes, we use depth-wise convolution instead of the multi-head self-attention to aggregate local spatial information. Moreover, we propose a novel channel attention module to aggregate global information, which is critical for brightness recovery. We conduct evaluations on the UDC benchmark, and our method surpasses the previous state-of-the-art models by 1.23 dB on the P-OLED track and 0.71 dB on the T-OLED track, respectively.



### Segmentation-based Information Extraction and Amalgamation in Fundus Images for Glaucoma Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.11456v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11456v1)
- **Published**: 2022-09-23 07:39:17+00:00
- **Updated**: 2022-09-23 07:39:17+00:00
- **Authors**: Yanni Wang, Gang Yang, Dayong Ding, Jianchun Zao
- **Comment**: None
- **Journal**: None
- **Summary**: Glaucoma is a severe blinding disease, for which automatic detection methods are urgently needed to alleviate the scarcity of ophthalmologists. Many works have proposed to employ deep learning methods that involve the segmentation of optic disc and cup for glaucoma detection, in which the segmentation process is often considered merely as an upstream sub-task. The relationship between fundus images and segmentation masks in terms of joint decision-making in glaucoma assessment is rarely explored. We propose a novel segmentation-based information extraction and amalgamation method for the task of glaucoma detection, which leverages the robustness of segmentation masks without disregarding the rich information in the original fundus images. Experimental results on both private and public datasets demonstrate that our proposed method outperforms all models that utilize solely either fundus images or masks.



### TeST: Test-time Self-Training under Distribution Shift
- **Arxiv ID**: http://arxiv.org/abs/2209.11459v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.11459v1)
- **Published**: 2022-09-23 07:47:33+00:00
- **Updated**: 2022-09-23 07:47:33+00:00
- **Authors**: Samarth Sinha, Peter Gehler, Francesco Locatello, Bernt Schiele
- **Comment**: None
- **Journal**: WACV 2023
- **Summary**: Despite their recent success, deep neural networks continue to perform poorly when they encounter distribution shifts at test time. Many recently proposed approaches try to counter this by aligning the model to the new distribution prior to inference. With no labels available this requires unsupervised objectives to adapt the model on the observed test data. In this paper, we propose Test-Time Self-Training (TeST): a technique that takes as input a model trained on some source data and a novel data distribution at test time, and learns invariant and robust representations using a student-teacher framework. We find that models adapted using TeST significantly improve over baseline test-time adaptation algorithms. TeST achieves competitive performance to modern domain adaptation algorithms, while having access to 5-10x less data at time of adaption. We thoroughly evaluate a variety of baselines on two tasks: object detection and image segmentation and find that models adapted with TeST. We find that TeST sets the new state-of-the art for test-time domain adaptation algorithms.



### Accurate and Efficient Stereo Matching via Attention Concatenation Volume
- **Arxiv ID**: http://arxiv.org/abs/2209.12699v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.12699v2)
- **Published**: 2022-09-23 08:14:30+00:00
- **Updated**: 2022-09-27 01:32:39+00:00
- **Authors**: Gangwei Xu, Yun Wang, Junda Cheng, Jinhui Tang, Xin Yang
- **Comment**: 12 pages. arXiv admin note: substantial text overlap with
  arXiv:2203.02146
- **Journal**: None
- **Summary**: Stereo matching is a fundamental building block for many vision and robotics applications. An informative and concise cost volume representation is vital for stereo matching of high accuracy and efficiency. In this paper, we present a novel cost volume construction method, named attention concatenation volume (ACV), which generates attention weights from correlation clues to suppress redundant information and enhance matching-related information in the concatenation volume. The ACV can be seamlessly embedded into most stereo matching networks, the resulting networks can use a more lightweight aggregation network and meanwhile achieve higher accuracy. We further design a fast version of ACV to enable real-time performance, named Fast-ACV, which generates high likelihood disparity hypotheses and the corresponding attention weights from low-resolution correlation clues to significantly reduce computational and memory cost and meanwhile maintain a satisfactory accuracy. The core idea of our Fast-ACV is volume attention propagation (VAP) which can automatically select accurate correlation values from an upsampled correlation volume and propagate these accurate values to the surroundings pixels with ambiguous correlation clues. Furthermore, we design a highly accurate network ACVNet and a real-time network Fast-ACVNet based on our ACV and Fast-ACV respectively, which achieve the state-of-the-art performance on several benchmarks (i.e., our ACVNet ranks the 2nd on KITTI 2015 and Scene Flow, and the 3rd on KITTI 2012 and ETH3D among all the published methods; our Fast-ACVNet outperforms almost all state-of-the-art real-time methods on Scene Flow, KITTI 2012 and 2015 and meanwhile has better generalization ability)



### Unsupervised Hashing with Semantic Concept Mining
- **Arxiv ID**: http://arxiv.org/abs/2209.11475v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2209.11475v1)
- **Published**: 2022-09-23 08:25:24+00:00
- **Updated**: 2022-09-23 08:25:24+00:00
- **Authors**: Rong-Cheng Tu, Xian-Ling Mao, Kevin Qinghong Lin, Chengfei Cai, Weize Qin, Hongfa Wang, Wei Wei, Heyan Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, to improve the unsupervised image retrieval performance, plenty of unsupervised hashing methods have been proposed by designing a semantic similarity matrix, which is based on the similarities between image features extracted by a pre-trained CNN model. However, most of these methods tend to ignore high-level abstract semantic concepts contained in images. Intuitively, concepts play an important role in calculating the similarity among images. In real-world scenarios, each image is associated with some concepts, and the similarity between two images will be larger if they share more identical concepts. Inspired by the above intuition, in this work, we propose a novel Unsupervised Hashing with Semantic Concept Mining, called UHSCM, which leverages a VLP model to construct a high-quality similarity matrix. Specifically, a set of randomly chosen concepts is first collected. Then, by employing a vision-language pretraining (VLP) model with the prompt engineering which has shown strong power in visual representation learning, the set of concepts is denoised according to the training images. Next, the proposed method UHSCM applies the VLP model with prompting again to mine the concept distribution of each image and construct a high-quality semantic similarity matrix based on the mined concept distributions. Finally, with the semantic similarity matrix as guiding information, a novel hashing loss with a modified contrastive loss based regularization item is proposed to optimize the hashing network. Extensive experiments on three benchmark datasets show that the proposed method outperforms the state-of-the-art baselines in the image retrieval task.



### Weakly Supervised Two-Stage Training Scheme for Deep Video Fight Detection Model
- **Arxiv ID**: http://arxiv.org/abs/2209.11477v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11477v1)
- **Published**: 2022-09-23 08:29:16+00:00
- **Updated**: 2022-09-23 08:29:16+00:00
- **Authors**: Zhenting Qi, Ruike Zhu, Zheyu Fu, Wenhao Chai, Volodymyr Kindratenko
- **Comment**: Accepted by ICTAI 2022
- **Journal**: None
- **Summary**: Fight detection in videos is an emerging deep learning application with today's prevalence of surveillance systems and streaming media. Previous work has largely relied on action recognition techniques to tackle this problem. In this paper, we propose a simple but effective method that solves the task from a new perspective: we design the fight detection model as a composition of an action-aware feature extractor and an anomaly score generator. Also, considering that collecting frame-level labels for videos is too laborious, we design a weakly supervised two-stage training scheme, where we utilize multiple-instance-learning loss calculated on video-level labels to train the score generator, and adopt the self-training technique to further improve its performance. Extensive experiments on a publicly available large-scale dataset, UBI-Fights, demonstrate the effectiveness of our method, and the performance on the dataset exceeds several previous state-of-the-art approaches. Furthermore, we collect a new dataset, VFD-2000, that specializes in video fight detection, with a larger scale and more scenarios than existing datasets. The implementation of our method and the proposed dataset will be publicly available at https://github.com/Hepta-Col/VideoFightDetection.



### GIDP: Learning a Good Initialization and Inducing Descriptor Post-enhancing for Large-scale Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.11488v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11488v1)
- **Published**: 2022-09-23 09:20:09+00:00
- **Updated**: 2022-09-23 09:20:09+00:00
- **Authors**: Zhaoxin Fan, Zhenbo Song, Hongyan Liu, Jun He
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Large-scale place recognition is a fundamental but challenging task, which plays an increasingly important role in autonomous driving and robotics. Existing methods have achieved acceptable good performance, however, most of them are concentrating on designing elaborate global descriptor learning network structures. The importance of feature generalization and descriptor post-enhancing has long been neglected. In this work, we propose a novel method named GIDP to learn a Good Initialization and Inducing Descriptor Poseenhancing for Large-scale Place Recognition. In particular, an unsupervised momentum contrast point cloud pretraining module and a reranking-based descriptor post-enhancing module are proposed respectively in GIDP. The former aims at learning a good initialization for the point cloud encoding network before training the place recognition model, while the later aims at post-enhancing the predicted global descriptor through reranking at inference time. Extensive experiments on both indoor and outdoor datasets demonstrate that our method can achieve state-of-the-art performance using simple and general point cloud encoding backbones.



### Grouped Adaptive Loss Weighting for Person Search
- **Arxiv ID**: http://arxiv.org/abs/2209.11492v1
- **DOI**: 10.1145/3503161.3547779
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11492v1)
- **Published**: 2022-09-23 09:32:54+00:00
- **Updated**: 2022-09-23 09:32:54+00:00
- **Authors**: Yanling Tian, Di Chen, Yunan Liu, Shanshan Zhang, Jian Yang
- **Comment**: Accepted by ACM MM
- **Journal**: None
- **Summary**: Person search is an integrated task of multiple sub-tasks such as foreground/background classification, bounding box regression and person re-identification. Therefore, person search is a typical multi-task learning problem, especially when solved in an end-to-end manner. Recently, some works enhance person search features by exploiting various auxiliary information, e.g. person joint keypoints, body part position, attributes, etc., which brings in more tasks and further complexifies a person search model. The inconsistent convergence rate of each task could potentially harm the model optimization. A straightforward solution is to manually assign different weights to different tasks, compensating for the diverse convergence rates. However, given the special case of person search, i.e. with a large number of tasks, it is impractical to weight the tasks manually. To this end, we propose a Grouped Adaptive Loss Weighting (GALW) method which adjusts the weight of each task automatically and dynamically. Specifically, we group tasks according to their convergence rates. Tasks within the same group share the same learnable weight, which is dynamically assigned by considering the loss uncertainty. Experimental results on two typical benchmarks, CUHK-SYSU and PRW, demonstrate the effectiveness of our method.



### Comparison of synthetic dataset generation methods for medical intervention rooms using medical clothing detection as an example
- **Arxiv ID**: http://arxiv.org/abs/2209.11493v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5; I.2
- **Links**: [PDF](http://arxiv.org/pdf/2209.11493v1)
- **Published**: 2022-09-23 09:36:23+00:00
- **Updated**: 2022-09-23 09:36:23+00:00
- **Authors**: Patrick Schülein, Hannah Teufel, Ronja Vorpahl, Indira Emter, Yannick Bukschat, Marcus Pfister, Anke Siebert, Nils Rathmann, Steffen Diehl, Marcus Vetter
- **Comment**: None
- **Journal**: None
- **Summary**: The availability of real data from areas with high privacy requirements, such as the medical intervention space, is low and the acquisition legally complex. Therefore, this work presents a way to create a synthetic dataset for the medical context, using medical clothing as an example. The goal is to close the reality gap between the synthetic and real data. For this purpose, methods of 3D-scanned clothing and designed clothing are compared in a Domain-Randomization and Structured-Domain-Randomization scenario using an Unreal-Engine plugin or Unity. Additionally a Mixed-Reality dataset in front of a greenscreen and a target domain dataset were used. Our experiments show, that Structured-Domain-Randomization of designed clothing together with Mixed-Reality data provide a baseline achieving 72.0% mAP on a test dataset of the clinical target domain. When additionally using 15% of available target domain train data, the gap towards 100% (660 images) target domain train data could be nearly closed 80.05% mAP (81.95% mAP). Finally we show that when additionally using 100% target domain train data the accuracy could be increased to 83.35% mAP.



### Marine Video Kit: A New Marine Video Dataset for Content-based Analysis and Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2209.11518v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2209.11518v4)
- **Published**: 2022-09-23 10:57:50+00:00
- **Updated**: 2022-12-06 05:29:30+00:00
- **Authors**: Quang-Trung Truong, Tuan-Anh Vu, Tan-Sang Ha, Lokoc Jakub, Yue Him Wong Tim, Ajay Joneja, Sai-Kit Yeung
- **Comment**: Camera Ready for MMM 2023, Bergen, Norway
- **Journal**: None
- **Summary**: Effective analysis of unusual domain specific video collections represents an important practical problem, where state-of-the-art general purpose models still face limitations. Hence, it is desirable to design benchmark datasets that challenge novel powerful models for specific domains with additional constraints. It is important to remember that domain specific data may be noisier (e.g., endoscopic or underwater videos) and often require more experienced users for effective search. In this paper, we focus on single-shot videos taken from moving cameras in underwater environments, which constitute a nontrivial challenge for research purposes. The first shard of a new Marine Video Kit dataset is presented to serve for video retrieval and other computer vision challenges. Our dataset is used in a special session during Video Browser Showdown 2023. In addition to basic meta-data statistics, we present several insights based on low-level features as well as semantic annotations of selected keyframes. The analysis also contains experiments showing limitations of respected general purpose models for retrieval. Our dataset and code are publicly available at https://hkust-vgd.github.io/marinevideokit.



### Vector Quantized Semantic Communication System
- **Arxiv ID**: http://arxiv.org/abs/2209.11519v2
- **DOI**: 10.1109/LWC.2023.3255221
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2209.11519v2)
- **Published**: 2022-09-23 10:58:23+00:00
- **Updated**: 2023-04-12 18:00:52+00:00
- **Authors**: Qifan Fu, Huiqiang Xie, Zhijin Qin, Gregory Slabaugh, Xiaoming Tao
- **Comment**: This five pages article has been accepted for publication in IEEE
  Wireless Communications Letters. This is the author's version which has not
  been fully edited and content may change prior to final publication. Citation
  information: DOI 10.1109/LWC.2023.3255221
- **Journal**: None
- **Summary**: Although analog semantic communication systems have received considerable attention in the literature, there is less work on digital semantic communication systems. In this paper, we develop a deep learning (DL)-enabled vector quantized (VQ) semantic communication system for image transmission, named VQ-DeepSC. Specifically, we propose a convolutional neural network (CNN)-based transceiver to extract multi-scale semantic features of images and introduce multi-scale semantic embedding spaces to perform semantic feature quantization, rendering the data compatible with digital communication systems. Furthermore, we employ adversarial training to improve the quality of received images by introducing a PatchGAN discriminator. Experimental results demonstrate that the proposed VQ-DeepSC is more robustness than BPG in digital communication systems and has comparable MS-SSIM performance to the DeepJSCC method.



### WS-3D-Lane: Weakly Supervised 3D Lane Detection With 2D Lane Labels
- **Arxiv ID**: http://arxiv.org/abs/2209.11523v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.11523v2)
- **Published**: 2022-09-23 11:07:08+00:00
- **Updated**: 2023-01-17 05:18:16+00:00
- **Authors**: Jianyong Ai, Wenbo Ding, Jiuhua Zhao, Jiachen Zhong
- **Comment**: 7 pages, 8 figures. Accepted by ICRA 2023
- **Journal**: None
- **Summary**: Compared to 2D lanes, real 3D lane data is difficult to collect accurately. In this paper, we propose a novel method for training 3D lanes with only 2D lane labels, called weakly supervised 3D lane detection WS-3D-Lane. By assumptions of constant lane width and equal height on adjacent lanes, we indirectly supervise 3D lane heights in the training. To overcome the problem of the dynamic change of the camera pitch during data collection, a camera pitch self-calibration method is proposed. In anchor representation, we propose a double-layer anchor with a improved non-maximum suppression (NMS) method, which enables the anchor-based method to predict two lane lines that are close. Experiments are conducted on the base of 3D-LaneNet under two supervision methods. Under weakly supervised setting, our WS-3D-Lane outperforms previous 3D-LaneNet: F-score rises to 92.3% on Apollo 3D synthetic dataset, and F1 rises to 74.5% on ONCE-3DLanes. Meanwhile, WS-3D-Lane in purely supervised setting makes more increments and outperforms state-of-the-art. To the best of our knowledge, WS-3D-Lane is the first try of 3D lane detection under weakly supervised setting.



### Statistical shape representations for temporal registration of plant components in 3D
- **Arxiv ID**: http://arxiv.org/abs/2209.11526v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11526v2)
- **Published**: 2022-09-23 11:11:10+00:00
- **Updated**: 2023-06-06 20:17:36+00:00
- **Authors**: Karoline Heiwolt, Cengiz Öztireli, Grzegorz Cielniak
- **Comment**: 6 pages plus references, 7 figures, presented at ICRA 2023
- **Journal**: None
- **Summary**: Plants are dynamic organisms and understanding temporal variations in vegetation is an essential problem for robots in the wild. However, associating repeated 3D scans of plants across time is challenging. A key step in this process is re-identifying and tracking the same individual plant components over time. Previously, this has been achieved by comparing their global spatial or topological location. In this work, we demonstrate how using shape features improves temporal organ matching. We present a landmark-free shape compression algorithm, which allows for the extraction of 3D shape features of leaves, characterises leaf shape and curvature efficiently in few parameters, and makes the association of individual leaves in feature space possible. The approach combines 3D contour extraction and further compression using Principal Component Analysis (PCA) to produce a shape space encoding, which is entirely learned from data and retains information about edge contours and 3D curvature. Our evaluation on temporal scan sequences of tomato plants shows, that incorporating shape features improves temporal leaf-matching. A combination of shape, location, and rotation information proves most informative for recognition of leaves over time and yields a true positive rate of 75%, a 15% improvement on sate-of-the-art methods. This is essential for robotic crop monitoring, which enables whole-of-lifecycle phenotyping.



### Deep Learning-based Anonymization of Chest Radiographs: A Utility-preserving Measure for Patient Privacy
- **Arxiv ID**: http://arxiv.org/abs/2209.11531v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.11531v2)
- **Published**: 2022-09-23 11:36:32+00:00
- **Updated**: 2023-07-24 13:04:48+00:00
- **Authors**: Kai Packhäuser, Sebastian Gündel, Florian Thamm, Felix Denzinger, Andreas Maier
- **Comment**: Accepted at MICCAI 2023
- **Journal**: None
- **Summary**: Robust and reliable anonymization of chest radiographs constitutes an essential step before publishing large datasets of such for research purposes. The conventional anonymization process is carried out by obscuring personal information in the images with black boxes and removing or replacing meta-information. However, such simple measures retain biometric information in the chest radiographs, allowing patients to be re-identified by a linkage attack. Therefore, there is an urgent need to obfuscate the biometric information appearing in the images. We propose the first deep learning-based approach (PriCheXy-Net) to targetedly anonymize chest radiographs while maintaining data utility for diagnostic and machine learning purposes. Our model architecture is a composition of three independent neural networks that, when collectively used, allow for learning a deformation field that is able to impede patient re-identification. Quantitative results on the ChestX-ray14 dataset show a reduction of patient re-identification from 81.8% to 57.7% (AUC) after re-training with little impact on the abnormality classification performance. This indicates the ability to preserve underlying abnormality patterns while increasing patient privacy. Lastly, we compare our proposed anonymization approach with two other obfuscation-based methods (Privacy-Net, DP-Pix) and demonstrate the superiority of our method towards resolving the privacy-utility trade-off for chest radiographs.



### MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier
- **Arxiv ID**: http://arxiv.org/abs/2209.11549v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.11549v3)
- **Published**: 2022-09-23 12:15:40+00:00
- **Updated**: 2023-06-30 07:14:12+00:00
- **Authors**: Mozhdeh Rouhsedaghat, Masoud Monajatipoor, C. -C. Jay Kuo, Iacopo Masi
- **Comment**: Accepted to the Thirty-Seventh Conference on Artificial Intelligence
  (AAAI) 2023 - 12 pages, 9 figures
- **Journal**: None
- **Summary**: We offer a method for one-shot mask-guided image synthesis that allows controlling manipulations of a single image by inverting a quasi-robust classifier equipped with strong regularizers. Our proposed method, entitled MAGIC, leverages structured gradients from a pre-trained quasi-robust classifier to better preserve the input semantics while preserving its classification accuracy, thereby guaranteeing credibility in the synthesis. Unlike current methods that use complex primitives to supervise the process or use attention maps as a weak supervisory signal, MAGIC aggregates gradients over the input, driven by a guide binary mask that enforces a strong, spatial prior. MAGIC implements a series of manipulations with a single framework achieving shape and location control, intense non-rigid shape deformations, and copy/move operations in the presence of repeating objects and gives users firm control over the synthesis by requiring to simply specify binary guide masks. Our study and findings are supported by various qualitative comparisons with the state-of-the-art on the same images sampled from ImageNet and quantitative analysis using machine perception along with a user survey of 100+ participants that endorse our synthesis quality. Project page at https://mozhdehrouhsedaghat.github.io/magic.html. Code is available at https://github.com/mozhdehrouhsedaghat/magic



### Query-based Hard-Image Retrieval for Object Detection at Test Time
- **Arxiv ID**: http://arxiv.org/abs/2209.11559v2
- **DOI**: 10.1609/aaai.v37i12.26717
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.11559v2)
- **Published**: 2022-09-23 12:33:31+00:00
- **Updated**: 2023-06-29 11:55:58+00:00
- **Authors**: Edward Ayers, Jonathan Sadeghi, John Redford, Romain Mueller, Puneet K. Dokania
- **Comment**: None
- **Journal**: Proceedings of the AAAI Conference on Artificial Intelligence,
  37(12), 14692-14700 (2023)
- **Summary**: There is a longstanding interest in capturing the error behaviour of object detectors by finding images where their performance is likely to be unsatisfactory. In real-world applications such as autonomous driving, it is also crucial to characterise potential failures beyond simple requirements of detection performance. For example, a missed detection of a pedestrian close to an ego vehicle will generally require closer inspection than a missed detection of a car in the distance. The problem of predicting such potential failures at test time has largely been overlooked in the literature and conventional approaches based on detection uncertainty fall short in that they are agnostic to such fine-grained characterisation of errors. In this work, we propose to reformulate the problem of finding "hard" images as a query-based hard image retrieval task, where queries are specific definitions of "hardness", and offer a simple and intuitive method that can solve this task for a large family of queries. Our method is entirely post-hoc, does not require ground-truth annotations, is independent of the choice of a detector, and relies on an efficient Monte Carlo estimation that uses a simple stochastic model in place of the ground-truth. We show experimentally that it can be applied successfully to a wide variety of queries for which it can reliably identify hard images for a given detector without any labelled data. We provide results on ranking and classification tasks using the widely used RetinaNet, Faster-RCNN, Mask-RCNN, and Cascade Mask-RCNN object detectors. The code for this project is available at https://github.com/fiveai/hardest.



### Multi-Modal Cross-Domain Alignment Network for Video Moment Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2209.11572v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2209.11572v2)
- **Published**: 2022-09-23 12:58:20+00:00
- **Updated**: 2022-11-15 13:43:47+00:00
- **Authors**: Xiang Fang, Daizong Liu, Pan Zhou, Yuchong Hu
- **Comment**: Accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: As an increasingly popular task in multimedia information retrieval, video moment retrieval (VMR) aims to localize the target moment from an untrimmed video according to a given language query. Most previous methods depend heavily on numerous manual annotations (i.e., moment boundaries), which are extremely expensive to acquire in practice. In addition, due to the domain gap between different datasets, directly applying these pre-trained models to an unseen domain leads to a significant performance drop. In this paper, we focus on a novel task: cross-domain VMR, where fully-annotated datasets are available in one domain (``source domain''), but the domain of interest (``target domain'') only contains unannotated datasets. As far as we know, we present the first study on cross-domain VMR. To address this new task, we propose a novel Multi-Modal Cross-Domain Alignment (MMCDA) network to transfer the annotation knowledge from the source domain to the target domain. However, due to the domain discrepancy between the source and target domains and the semantic gap between videos and queries, directly applying trained models to the target domain generally leads to a performance drop. To solve this problem, we develop three novel modules: (i) a domain alignment module is designed to align the feature distributions between different domains of each modality; (ii) a cross-modal alignment module aims to map both video and query features into a joint embedding space and to align the feature distributions between different modalities in the target domain; (iii) a specific alignment module tries to obtain the fine-grained similarity between a specific frame and the given query for optimal localization. By jointly training these three modules, our MMCDA can learn domain-invariant and semantic-aligned cross-modal representations.



### Towards Complete-View and High-Level Pose-based Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.11577v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11577v1)
- **Published**: 2022-09-23 13:13:59+00:00
- **Updated**: 2022-09-23 13:13:59+00:00
- **Authors**: Honghu Pan, Yongyong Chen, Tingyang Xu, Yunqi He, Zhenyu He
- **Comment**: None
- **Journal**: None
- **Summary**: The model-based gait recognition methods usually adopt the pedestrian walking postures to identify human beings.   However, existing methods did not explicitly resolve the large intra-class variance of human pose due to camera views changing.   In this paper, we propose to generate multi-view pose sequences for each single-view pose sample by learning full-rank transformation matrices via lower-upper generative adversarial network (LUGAN).   By the prior of camera imaging, we derive that the spatial coordinates between cross-view poses satisfy a linear transformation of a full-rank matrix, thereby, this paper employs the adversarial training to learn transformation matrices from the source pose and target views to obtain the target pose sequences.   To this end, we implement a generator composed of graph convolutional (GCN) layers, fully connected (FC) layers and two-branch convolutional (CNN) layers: GCN layers and FC layers encode the source pose sequence and target view, then CNN branches learn a lower triangular matrix and an upper triangular matrix, respectively, finally they are multiplied to formulate the full-rank transformation matrix.   For the purpose of adversarial training, we further devise a condition discriminator that distinguishes whether the pose sequence is true or generated.   To enable the high-level correlation learning, we propose a plug-and-play module, named multi-scale hypergraph convolution (HGC), to replace the spatial graph convolutional layer in baseline, which could simultaneously model the joint-level, part-level and body-level correlations.   Extensive experiments on two large gait recognition datasets, i.e., CASIA-B and OUMVLP-Pose, demonstrate that our method outperforms the baseline model and existing pose-based methods by a large margin.



### Pose-Aided Video-based Person Re-Identification via Recurrent Graph Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/2209.11582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11582v1)
- **Published**: 2022-09-23 13:20:33+00:00
- **Updated**: 2022-09-23 13:20:33+00:00
- **Authors**: Honghu Pan, Qiao Liu, Yongyong Chen, Yunqi He, Yuan Zheng, Feng Zheng, Zhenyu He
- **Comment**: None
- **Journal**: None
- **Summary**: Existing methods for video-based person re-identification (ReID) mainly learn the appearance feature of a given pedestrian via a feature extractor and a feature aggregator.   However, the appearance models would fail when different pedestrians have similar appearances.   Considering that different pedestrians have different walking postures and body proportions, we propose to learn the discriminative pose feature beyond the appearance feature for video retrieval.   Specifically, we implement a two-branch architecture to separately learn the appearance feature and pose feature, and then concatenate them together for inference.   To learn the pose feature, we first detect the pedestrian pose in each frame through an off-the-shelf pose detector, and construct a temporal graph using the pose sequence.   We then exploit a recurrent graph convolutional network (RGCN) to learn the node embeddings of the temporal pose graph, which devises a global information propagation mechanism to simultaneously achieve the neighborhood aggregation of intra-frame nodes and message passing among inter-frame graphs.   Finally, we propose a dual-attention method consisting of node-attention and time-attention to obtain the temporal graph representation from the node embeddings, where the self-attention mechanism is employed to learn the importance of each node and each frame.   We verify the proposed method on three video-based ReID datasets, i.e., Mars, DukeMTMC and iLIDS-VID, whose experimental results demonstrate that the learned pose feature can effectively improve the performance of existing appearance models.



### Multi-Granularity Graph Pooling for Video-based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2209.11584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11584v1)
- **Published**: 2022-09-23 13:26:05+00:00
- **Updated**: 2022-09-23 13:26:05+00:00
- **Authors**: Honghu Pan, Yongyong Chen, Zhenyu He
- **Comment**: None
- **Journal**: None
- **Summary**: The video-based person re-identification (ReID) aims to identify the given pedestrian video sequence across multiple non-overlapping cameras.   To aggregate the temporal and spatial features of the video samples, the graph neural networks (GNNs) are introduced.   However, existing graph-based models, like STGCN, perform the \textit{mean}/\textit{max pooling} on node features to obtain the graph representation, which neglect the graph topology and node importance.   In this paper, we propose the graph pooling network (GPNet) to learn the multi-granularity graph representation for the video retrieval, where the \textit{graph pooling layer} is implemented to downsample the graph.   We first construct a multi-granular graph, whose node features denote image embedding learned by backbone, and edges are established between the temporal and Euclidean neighborhood nodes.   We then implement multiple graph convolutional layers to perform the neighborhood aggregation on the graphs.   To downsample the graph, we propose a multi-head full attention graph pooling (MHFAPool) layer, which integrates the advantages of existing node clustering and node selection pooling methods.   Specifically, MHFAPool takes the main eigenvector of full attention matrix as the aggregation coefficients to involve the global graph information in each pooled nodes.   Extensive experiments demonstrate that our GPNet achieves the competitive results on four widely-used datasets, i.e., MARS, DukeMTMC-VideoReID, iLIDS-VID and PRID-2011.



### I-SPLIT: Deep Network Interpretability for Split Computing
- **Arxiv ID**: http://arxiv.org/abs/2209.11607v1
- **DOI**: 10.1109/ICPR56361.2022.9956625
- **Categories**: **cs.CV**, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.11607v1)
- **Published**: 2022-09-23 14:26:56+00:00
- **Updated**: 2022-09-23 14:26:56+00:00
- **Authors**: Federico Cunico, Luigi Capogrosso, Francesco Setti, Damiano Carra, Franco Fummi, Marco Cristani
- **Comment**: ICPR 2022
- **Journal**: None
- **Summary**: This work makes a substantial step in the field of split computing, i.e., how to split a deep neural network to host its early part on an embedded device and the rest on a server. So far, potential split locations have been identified exploiting uniquely architectural aspects, i.e., based on the layer sizes. Under this paradigm, the efficacy of the split in terms of accuracy can be evaluated only after having performed the split and retrained the entire pipeline, making an exhaustive evaluation of all the plausible splitting points prohibitive in terms of time. Here we show that not only the architecture of the layers does matter, but the importance of the neurons contained therein too. A neuron is important if its gradient with respect to the correct class decision is high. It follows that a split should be applied right after a layer with a high density of important neurons, in order to preserve the information flowing until then. Upon this idea, we propose Interpretable Split (I-SPLIT): a procedure that identifies the most suitable splitting points by providing a reliable prediction on how well this split will perform in terms of classification accuracy, beforehand of its effective implementation. As a further major contribution of I-SPLIT, we show that the best choice for the splitting point on a multiclass categorization problem depends also on which specific classes the network has to deal with. Exhaustive experiments have been carried out on two networks, VGG16 and ResNet-50, and three datasets, Tiny-Imagenet-200, notMNIST, and Chest X-Ray Pneumonia. The source code is available at https://github.com/vips4/I-Split.



### A direct time-of-flight image sensor with in-pixel surface detection and dynamic vision
- **Arxiv ID**: http://arxiv.org/abs/2209.11772v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/2209.11772v1)
- **Published**: 2022-09-23 14:38:00+00:00
- **Updated**: 2022-09-23 14:38:00+00:00
- **Authors**: Istvan Gyongy, Ahmet T. Erdogan, Neale A. W. Dutton, Germán Mora Martín, Alistair Gorman, Hanning Mai, Francesco Mattioli Della Rocca, Robert K. Henderson
- **Comment**: 24 pages, 16 figures. The visualisations may be viewed by clicking on
  the hyperlinks in the text
- **Journal**: None
- **Summary**: 3D flash LIDAR is an alternative to the traditional scanning LIDAR systems, promising precise depth imaging in a compact form factor, and free of moving parts, for applications such as self-driving cars, robotics and augmented reality (AR). Typically implemented using single-photon, direct time-of-flight (dToF) receivers in image sensor format, the operation of the devices can be hindered by the large number of photon events needing to be processed and compressed in outdoor scenarios, limiting frame rates and scalability to larger arrays. We here present a 64x32 pixel (256x128 SPAD) dToF imager that overcomes these limitations by using pixels with embedded histogramming, which lock onto and track the return signal. This reduces the size of output data frames considerably, enabling maximum frame rates in the 10 kFPS range or 100 kFPS for direct depth readings. The sensor offers selective readout of pixels detecting surfaces, or those sensing motion, leading to reduced power consumption and off-chip processing requirements. We demonstrate the application of the sensor in mid-range LIDAR.



### View-Invariant Skeleton-based Action Recognition via Global-Local Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.11634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11634v1)
- **Published**: 2022-09-23 15:00:57+00:00
- **Updated**: 2022-09-23 15:00:57+00:00
- **Authors**: Cunling Bian, Wei Feng, Fanbo Meng, Song Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Skeleton-based human action recognition has been drawing more interest recently due to its low sensitivity to appearance changes and the accessibility of more skeleton data. However, even the 3D skeletons captured in practice are still sensitive to the viewpoint and direction gave the occlusion of different human-body joints and the errors in human joint localization. Such view variance of skeleton data may significantly affect the performance of action recognition. To address this issue, we propose in this paper a new view-invariant representation learning approach, without any manual action labeling, for skeleton-based human action recognition. Specifically, we leverage the multi-view skeleton data simultaneously taken for the same person in the network training, by maximizing the mutual information between the representations extracted from different views, and then propose a global-local contrastive loss to model the multi-scale co-occurrence relationships in both spatial and temporal domains. Extensive experimental results show that the proposed method is robust to the view difference of the input skeleton data and significantly boosts the performance of unsupervised skeleton-based human action methods, resulting in new state-of-the-art accuracies on two challenging multi-view benchmarks of PKUMMD and NTU RGB+D.



### Image-to-Image Translation for Autonomous Driving from Coarsely-Aligned Image Pairs
- **Arxiv ID**: http://arxiv.org/abs/2209.11673v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.11673v1)
- **Published**: 2022-09-23 16:03:18+00:00
- **Updated**: 2022-09-23 16:03:18+00:00
- **Authors**: Youya Xia, Josephine Monica, Wei-Lun Chao, Bharath Hariharan, Kilian Q Weinberger, Mark Campbell
- **Comment**: Submitted to the International Conference on Robotics and Automation
  (ICRA) 2023
- **Journal**: None
- **Summary**: A self-driving car must be able to reliably handle adverse weather conditions (e.g., snowy) to operate safely. In this paper, we investigate the idea of turning sensor inputs (i.e., images) captured in an adverse condition into a benign one (i.e., sunny), upon which the downstream tasks (e.g., semantic segmentation) can attain high accuracy. Prior work primarily formulates this as an unpaired image-to-image translation problem due to the lack of paired images captured under the exact same camera poses and semantic layouts. While perfectly-aligned images are not available, one can easily obtain coarsely-paired images. For instance, many people drive the same routes daily in both good and adverse weather; thus, images captured at close-by GPS locations can form a pair. Though data from repeated traversals are unlikely to capture the same foreground objects, we posit that they provide rich contextual information to supervise the image translation model. To this end, we propose a novel training objective leveraging coarsely-aligned image pairs. We show that our coarsely-aligned training scheme leads to a better image translation quality and improved downstream tasks, such as semantic segmentation, monocular depth estimation, and visual localization.



### PNeRF: Probabilistic Neural Scene Representations for Uncertain 3D Visual Mapping
- **Arxiv ID**: http://arxiv.org/abs/2209.11677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11677v1)
- **Published**: 2022-09-23 16:05:12+00:00
- **Updated**: 2022-09-23 16:05:12+00:00
- **Authors**: Yassine Ahmine, Arnab Dey, Andrew I. Comport
- **Comment**: 7 Pages, 6 Figures, 5 Tables. Submitted to IEEE International
  Conference on Robotics and Automation 2023 (ICRA 2023)
- **Journal**: None
- **Summary**: Recently neural scene representations have provided very impressive results for representing 3D scenes visually, however, their study and progress have mainly been limited to visualization of virtual models in computer graphics or scene reconstruction in computer vision without explicitly accounting for sensor and pose uncertainty. Using this novel scene representation in robotics applications, however, would require accounting for this uncertainty in the neural map. The aim of this paper is therefore to propose a novel method for training {\em probabilistic neural scene representations} with uncertain training data that could enable the inclusion of these representations in robotics applications. Acquiring images using cameras or depth sensors contains inherent uncertainty, and furthermore, the camera poses used for learning a 3D model are also imperfect. If these measurements are used for training without accounting for their uncertainty, then the resulting models are non-optimal, and the resulting scene representations are likely to contain artifacts such as blur and un-even geometry. In this work, the problem of uncertainty integration to the learning process is investigated by focusing on training with uncertain information in a probabilistic manner. The proposed method involves explicitly augmenting the training likelihood with an uncertainty term such that the learnt probability distribution of the network is minimized with respect to the training uncertainty. It will be shown that this leads to more accurate image rendering quality, in addition to more precise and consistent geometry. Validation has been carried out on both synthetic and real datasets showing that the proposed approach outperforms state-of-the-art methods. The results show notably that the proposed method is capable of rendering novel high-quality views even when the training data is limited.



### Multivariate Wasserstein Functional Connectivity for Autism Screening
- **Arxiv ID**: http://arxiv.org/abs/2209.11703v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11703v1)
- **Published**: 2022-09-23 16:23:05+00:00
- **Updated**: 2022-09-23 16:23:05+00:00
- **Authors**: Oleg Kachan, Alexander Bernstein
- **Comment**: None
- **Journal**: None
- **Summary**: Most approaches to the estimation of brain functional connectivity from the functional magnetic resonance imaging (fMRI) data rely on computing some measure of statistical dependence, or more generally, a distance between univariate representative time series of regions of interest (ROIs) consisting of multiple voxels. However, summarizing a ROI's multiple time series with its mean or the first principal component (1PC) may result to the loss of information as, for example, 1PC explains only a small fraction of variance of the multivariate signal of the neuronal activity.   We propose to compare ROIs directly, without the use of representative time series, defining a new measure of multivariate connectivity between ROIs, not necessarily consisting of the same number of voxels, based on the Wasserstein distance. We assess the proposed Wasserstein functional connectivity measure on the autism screening task, demonstrating its superiority over commonly used univariate and multivariate functional connectivity measures.



### Best Prompts for Text-to-Image Models and How to Find Them
- **Arxiv ID**: http://arxiv.org/abs/2209.11711v3
- **DOI**: 10.1145/3539618.3592000
- **Categories**: **cs.HC**, cs.CL, cs.CV, H.5.2; H.3.3
- **Links**: [PDF](http://arxiv.org/pdf/2209.11711v3)
- **Published**: 2022-09-23 16:39:13+00:00
- **Updated**: 2023-06-01 15:13:20+00:00
- **Authors**: Nikita Pavlichenko, Dmitry Ustalov
- **Comment**: 13 pages (6 main pages), 7 figures, 4 tables, accepted at SIGIR '23
  Short Paper Track
- **Journal**: None
- **Summary**: Recent progress in generative models, especially in text-guided diffusion models, has enabled the production of aesthetically-pleasing imagery resembling the works of professional human artists. However, one has to carefully compose the textual description, called the prompt, and augment it with a set of clarifying keywords. Since aesthetics are challenging to evaluate computationally, human feedback is needed to determine the optimal prompt formulation and keyword combination. In this paper, we present a human-in-the-loop approach to learning the most useful combination of prompt keywords using a genetic algorithm. We also show how such an approach can improve the aesthetic appeal of images depicting the same descriptions.



### Boost CTR Prediction for New Advertisements via Modeling Visual Content
- **Arxiv ID**: http://arxiv.org/abs/2209.11727v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11727v1)
- **Published**: 2022-09-23 17:08:54+00:00
- **Updated**: 2022-09-23 17:08:54+00:00
- **Authors**: Tan Yu, Zhipeng Jin, Jie Liu, Yi Yang, Hongliang Fei, Ping Li
- **Comment**: None
- **Journal**: None
- **Summary**: Existing advertisements click-through rate (CTR) prediction models are mainly dependent on behavior ID features, which are learned based on the historical user-ad interactions. Nevertheless, behavior ID features relying on historical user behaviors are not feasible to describe new ads without previous interactions with users. To overcome the limitations of behavior ID features in modeling new ads, we exploit the visual content in ads to boost the performance of CTR prediction models. Specifically, we map each ad into a set of visual IDs based on its visual content. These visual IDs are further used for generating the visual embedding for enhancing CTR prediction models. We formulate the learning of visual IDs into a supervised quantization problem. Due to a lack of class labels for commercial images in advertisements, we exploit image textual descriptions as the supervision to optimize the image extractor for generating effective visual IDs. Meanwhile, since the hard quantization is non-differentiable, we soften the quantization operation to make it support the end-to-end network training. After mapping each image into visual IDs, we learn the embedding for each visual ID based on the historical user-ad interactions accumulated in the past. Since the visual ID embedding depends only on the visual content, it generalizes well to new ads. Meanwhile, the visual ID embedding complements the ad behavior ID embedding. Thus, it can considerably boost the performance of the CTR prediction models previously relying on behavior ID features for both new ads and ads that have accumulated rich user behaviors. After incorporating the visual ID embedding in the CTR prediction model of Baidu online advertising, the average CTR of ads improves by 1.46%, and the total charge increases by 1.10%.



### Dual-Cycle: Self-Supervised Dual-View Fluorescence Microscopy Image Reconstruction using CycleGAN
- **Arxiv ID**: http://arxiv.org/abs/2209.11729v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11729v1)
- **Published**: 2022-09-23 17:23:19+00:00
- **Updated**: 2022-09-23 17:23:19+00:00
- **Authors**: Tomas Kerepecky, Jiaming Liu, Xue Wen Ng, David W. Piston, Ulugbek S. Kamilov
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: Three-dimensional fluorescence microscopy often suffers from anisotropy, where the resolution along the axial direction is lower than that within the lateral imaging plane. We address this issue by presenting Dual-Cycle, a new framework for joint deconvolution and fusion of dual-view fluorescence images. Inspired by the recent Neuroclear method, Dual-Cycle is designed as a cycle-consistent generative network trained in a self-supervised fashion by combining a dual-view generator and prior-guided degradation model. We validate Dual-Cycle on both synthetic and real data showing its state-of-the-art performance without any external training data.



### Semantic scene descriptions as an objective of human vision
- **Arxiv ID**: http://arxiv.org/abs/2209.11737v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2209.11737v1)
- **Published**: 2022-09-23 17:34:33+00:00
- **Updated**: 2022-09-23 17:34:33+00:00
- **Authors**: Adrien Doerig, Tim C Kietzmann, Emily Allen, Yihan Wu, Thomas Naselaris, Kendrick Kay, Ian Charest
- **Comment**: None
- **Journal**: None
- **Summary**: Interpreting the meaning of a visual scene requires not only identification of its constituent objects, but also a rich semantic characterization of object interrelations. Here, we study the neural mechanisms underlying visuo-semantic transformations by applying modern computational techniques to a large-scale 7T fMRI dataset of human brain responses elicited by complex natural scenes. Using semantic embeddings obtained by applying linguistic deep learning models to human-generated scene descriptions, we identify a widely distributed network of brain regions that encode semantic scene descriptions. Importantly, these semantic embeddings better explain activity in these regions than traditional object category labels. In addition, they are effective predictors of activity despite the fact that the participants did not actively engage in a semantic task, suggesting that visuo-semantic transformations are a default mode of vision. In support of this view, we then show that highly accurate reconstructions of scene captions can be directly linearly decoded from patterns of brain activity. Finally, a recurrent convolutional neural network trained on semantic embeddings further outperforms semantic embeddings in predicting brain activity, providing a mechanistic model of the brain's visuo-semantic transformations. Together, these experimental and computational results suggest that transforming visual input into rich semantic scene descriptions may be a central objective of the visual system, and that focusing efforts on this new objective may lead to improved models of visual information processing in the human brain.



### A Neural Template Matching Method to Detect Knee Joint Areas
- **Arxiv ID**: http://arxiv.org/abs/2209.11791v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11791v1)
- **Published**: 2022-09-23 18:10:07+00:00
- **Updated**: 2022-09-23 18:10:07+00:00
- **Authors**: Juha Tiirola
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, new methods are considered to detect knee joint areas in bilateral PA fixed flexion knee X-ray images. The methods are of template matching type where the distance criterion is based on the negative normalized cross-correlation. The manual annotations are made on only one side of a single bilateral image when the templates are selected. The best matching patch search is formulated as an unconstrained continuous domain minimization problem. For the minimization problem different optimization methods are considered. The main method of the paper is a trainable optimizer where the method is taught to take zoomed and possibly rotated patches from its input images which look like the template. In the experiments, we compare the minimum values found by different optimization methods. We also look at some test images to examine the correspondence between the minimum value and how well the knee area is localized. It seems that making annotations only to a single image enables to detect knee joint areas quite precisely.



### Descriptor Distillation: a Teacher-Student-Regularized Framework for Learning Local Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2209.11795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11795v1)
- **Published**: 2022-09-23 18:22:04+00:00
- **Updated**: 2022-09-23 18:22:04+00:00
- **Authors**: Yuzhen Liu, Qiulei Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Learning a fast and discriminative patch descriptor is a challenging topic in computer vision. Recently, many existing works focus on training various descriptor learning networks by minimizing a triplet loss (or its variants), which is expected to decrease the distance between each positive pair and increase the distance between each negative pair. However, such an expectation has to be lowered due to the non-perfect convergence of network optimizer to a local solution. Addressing this problem and the open computational speed problem, we propose a Descriptor Distillation framework for local descriptor learning, called DesDis, where a student model gains knowledge from a pre-trained teacher model, and it is further enhanced via a designed teacher-student regularizer. This teacher-student regularizer is to constrain the difference between the positive (also negative) pair similarity from the teacher model and that from the student model, and we theoretically prove that a more effective student model could be trained by minimizing a weighted combination of the triplet loss and this regularizer, than its teacher which is trained by minimizing the triplet loss singly. Under the proposed DesDis, many existing descriptor networks could be embedded as the teacher model, and accordingly, both equal-weight and light-weight student models could be derived, which outperform their teacher in either accuracy or speed. Experimental results on 3 public datasets demonstrate that the equal-weight student models, derived from the proposed DesDis framework by utilizing three typical descriptor learning networks as teacher models, could achieve significantly better performances than their teachers and several other comparative methods. In addition, the derived light-weight models could achieve 8 times or even faster speeds than the comparative methods under similar patch verification performances



### Composite Layers for Deep Anomaly Detection on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2209.11796v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.11796v1)
- **Published**: 2022-09-23 18:32:18+00:00
- **Updated**: 2022-09-23 18:32:18+00:00
- **Authors**: Alberto Floris, Luca Frittoli, Diego Carrera, Giacomo Boracchi
- **Comment**: Submitted to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Deep neural networks require specific layers to process point clouds, as the scattered and irregular location of points prevents us from using convolutional filters. Here we introduce the composite layer, a new convolutional operator for point clouds. The peculiarity of our composite layer is that it extracts and compresses the spatial information from the position of points before combining it with their feature vectors. Compared to well-known point-convolutional layers such as those of ConvPoint and KPConv, our composite layer provides additional regularization and guarantees greater flexibility in terms of design and number of parameters. To demonstrate the design flexibility, we also define an aggregate composite layer that combines spatial information and features in a nonlinear manner, and we use these layers to implement a convolutional and an aggregate CompositeNet. We train our CompositeNets to perform classification and, most remarkably, unsupervised anomaly detection. Our experiments on synthetic and real-world datasets show that, in both tasks, our CompositeNets outperform ConvPoint and achieve similar results as KPConv despite having a much simpler architecture. Moreover, our CompositeNets substantially outperform existing solutions for anomaly detection on point clouds.



### Expanding the Deployment Envelope of Behavior Prediction via Adaptive Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.11820v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.11820v4)
- **Published**: 2022-09-23 19:23:07+00:00
- **Updated**: 2023-05-23 05:37:16+00:00
- **Authors**: Boris Ivanovic, James Harrison, Marco Pavone
- **Comment**: 12 pages, 13 figures, 2 tables. ICRA 2023
- **Journal**: None
- **Summary**: Learning-based behavior prediction methods are increasingly being deployed in real-world autonomous systems, e.g., in fleets of self-driving vehicles, which are beginning to commercially operate in major cities across the world. Despite their advancements, however, the vast majority of prediction systems are specialized to a set of well-explored geographic regions or operational design domains, complicating deployment to additional cities, countries, or continents. Towards this end, we present a novel method for efficiently adapting behavior prediction models to new environments. Our approach leverages recent advances in meta-learning, specifically Bayesian regression, to augment existing behavior prediction models with an adaptive layer that enables efficient domain transfer via offline fine-tuning, online adaptation, or both. Experiments across multiple real-world datasets demonstrate that our method can efficiently adapt to a variety of unseen environments.



### Wide-Area Geolocalization with a Limited Field of View Camera
- **Arxiv ID**: http://arxiv.org/abs/2209.11854v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11854v2)
- **Published**: 2022-09-23 20:59:26+00:00
- **Updated**: 2023-05-18 14:41:01+00:00
- **Authors**: Lena M. Downes, Ted J. Steiner, Rebecca L. Russell, Jonathan P. How
- **Comment**: 7 pages, 10 figures. Accepted to ICRA 2023
- **Journal**: None
- **Summary**: Cross-view geolocalization, a supplement or replacement for GPS, localizes an agent within a search area by matching images taken from a ground-view camera to overhead images taken from satellites or aircraft. Although the viewpoint disparity between ground and overhead images makes cross-view geolocalization challenging, significant progress has been made assuming that the ground agent has access to a panoramic camera. For example, our prior work (WAG) introduced changes in search area discretization, training loss, and particle filter weighting that enabled city-scale panoramic cross-view geolocalization. However, panoramic cameras are not widely used in existing robotic platforms due to their complexity and cost. Non-panoramic cross-view geolocalization is more applicable for robotics, but is also more challenging. This paper presents Restricted FOV Wide-Area Geolocalization (ReWAG), a cross-view geolocalization approach that generalizes WAG for use with standard, non-panoramic ground cameras by creating pose-aware embeddings and providing a strategy to incorporate particle pose into the Siamese network. ReWAG is a neural network and particle filter system that is able to globally localize a mobile agent in a GPS-denied environment with only odometry and a 90 degree FOV camera, achieving similar localization accuracy as what WAG achieved with a panoramic camera and improving localization accuracy by a factor of 100 compared to a baseline vision transformer (ViT) approach. A video highlight that demonstrates ReWAG's convergence on a test path of several dozen kilometers is available at https://youtu.be/U_OBQrt8qCE.



### Transformer-Based Microbubble Localization
- **Arxiv ID**: http://arxiv.org/abs/2209.11859v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11859v1)
- **Published**: 2022-09-23 21:13:19+00:00
- **Updated**: 2022-09-23 21:13:19+00:00
- **Authors**: Sepideh K. Gharamaleki, Brandon Helfield, Hassan Rivaz
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Ultrasound Localization Microscopy (ULM) is an emerging technique that employs the localization of echogenic microbubbles (MBs) to finely sample and image the microcirculation beyond the diffraction limit of ultrasound imaging. Conventional MB localization methods are mainly based on considering a specific Point Spread Function (PSF) for MBs, which leads to loss of information caused by overlapping MBs, non-stationary PSFs, and harmonic MB echoes. Therefore, it is imperative to devise methods that can accurately localize MBs while being resilient to MB nonlinearities and variations of MB concentrations that distort MB PSFs. This paper proposes a transformer-based MB localization approach to address this issue. We adopted DEtection TRansformer (DETR) arXiv:2005.12872 , which is an end-to-end object recognition method that detects a unique bounding box for each of the detected objects using set-based Hungarian loss and bipartite matching. To the authors' knowledge, this is the first time transformers have been used for MB localization. To appraise the proposed strategy, the pre-trained DETR network's performance has been tested for detecting MBs using transfer learning principles. We have fine-tuned the network on a subset of randomly selected frames of the dataset provided by the IEEE IUS Ultra-SR challenge organizers and then tested on the rest using cross-validation. For the simulation dataset, the paper supports the deployment of transformer-based solutions for MB localization at high accuracy.



### Leveraging Self-Supervised Training for Unintentional Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.11870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11870v1)
- **Published**: 2022-09-23 21:36:36+00:00
- **Updated**: 2022-09-23 21:36:36+00:00
- **Authors**: Enea Duka, Anna Kukleva, Bernt Schiele
- **Comment**: Accepted at ECCVW2022
- **Journal**: None
- **Summary**: Unintentional actions are rare occurrences that are difficult to define precisely and that are highly dependent on the temporal context of the action. In this work, we explore such actions and seek to identify the points in videos where the actions transition from intentional to unintentional. We propose a multi-stage framework that exploits inherent biases such as motion speed, motion direction, and order to recognize unintentional actions. To enhance representations via self-supervised training for the task of unintentional action recognition we propose temporal transformations, called Temporal Transformations of Inherent Biases of Unintentional Actions (T2IBUA). The multi-stage approach models the temporal information on both the level of individual frames and full clips. These enhanced representations show strong performance for unintentional action recognition tasks. We provide an extensive ablation study of our framework and report results that significantly improve over the state-of-the-art.



### Enhancing Data Diversity for Self-training Based Unsupervised Cross-modality Vestibular Schwannoma and Cochlea Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.11879v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.11879v2)
- **Published**: 2022-09-23 22:26:51+00:00
- **Updated**: 2022-10-26 14:31:35+00:00
- **Authors**: Han Liu, Yubo Fan, Ipek Oguz, Benoit M. Dawant
- **Comment**: Accepted by BrainLes MICCAI proceedings
- **Journal**: None
- **Summary**: Automatic segmentation of vestibular schwannoma (VS) and cochlea from magnetic resonance imaging can facilitate VS treatment planning. Unsupervised segmentation methods have shown promising results without requiring the time-consuming and laborious manual labeling process. In this paper, we present an approach for VS and cochlea segmentation in an unsupervised domain adaptation setting. Specifically, we first develop a cross-site cross-modality unpaired image translation strategy to enrich the diversity of the synthesized data. Then, we devise a rule-based offline augmentation technique to further minimize the domain gap. Lastly, we adopt a self-configuring segmentation framework empowered by self-training to obtain the final results. On the CrossMoDA 2022 validation leaderboard, our method has achieved competitive VS and cochlea segmentation performance with mean Dice scores of 0.8178 $\pm$ 0.0803 and 0.8433 $\pm$ 0.0293, respectively.



### JPEG Artifact Correction using Denoising Diffusion Restoration Models
- **Arxiv ID**: http://arxiv.org/abs/2209.11888v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.11888v2)
- **Published**: 2022-09-23 23:47:00+00:00
- **Updated**: 2022-11-23 20:11:40+00:00
- **Authors**: Bahjat Kawar, Jiaming Song, Stefano Ermon, Michael Elad
- **Comment**: Presented at NeurIPS 2022 Workshop on Score-Based Methods. Code:
  https://github.com/bahjat-kawar/ddrm-jpeg
- **Journal**: None
- **Summary**: Diffusion models can be used as learned priors for solving various inverse problems. However, most existing approaches are restricted to linear inverse problems, limiting their applicability to more general cases. In this paper, we build upon Denoising Diffusion Restoration Models (DDRM) and propose a method for solving some non-linear inverse problems. We leverage the pseudo-inverse operator used in DDRM and generalize this concept for other measurement operators, which allows us to use pre-trained unconditional diffusion models for applications such as JPEG artifact correction. We empirically demonstrate the effectiveness of our approach across various quality factors, attaining performance levels that are on par with state-of-the-art methods trained specifically for the JPEG restoration task.



