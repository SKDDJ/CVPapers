# Arxiv Papers in cs.CV on 2022-09-17
### Delving Globally into Texture and Structure for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2209.08217v1
- **DOI**: 10.1145/3503161.3548265
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08217v1)
- **Published**: 2022-09-17 02:19:26+00:00
- **Updated**: 2022-09-17 02:19:26+00:00
- **Authors**: Haipeng Liu, Yang Wang, Meng Wang, Yong Rui
- **Comment**: 9 pages, 10 figures, accepted by ACM Multimedia 2022
- **Journal**: None
- **Summary**: Image inpainting has achieved remarkable progress and inspired abundant methods, where the critical bottleneck is identified as how to fulfill the high-frequency structure and low-frequency texture information on the masked regions with semantics. To this end, deep models exhibit powerful superiority to capture them, yet constrained on the local spatial regions. In this paper, we delve globally into texture and structure information to well capture the semantics for image inpainting. As opposed to the existing arts trapped on the independent local patches, the texture information of each patch is reconstructed from all other patches across the whole image, to match the coarsely filled information, specially the structure information over the masked regions. Unlike the current decoder-only transformer within the pixel level for image inpainting, our model adopts the transformer pipeline paired with both encoder and decoder. On one hand, the encoder captures the texture semantic correlations of all patches across image via self-attention module. On the other hand, an adaptive patch vocabulary is dynamically established in the decoder for the filled patches over the masked regions. Building on this, a structure-texture matching attention module anchored on the known regions comes up to marry the best of these two worlds for progressive inpainting via a probabilistic diffusion process. Our model is orthogonal to the fashionable arts, such as Convolutional Neural Networks (CNNs), Attention and Transformer model, from the perspective of texture and structure information for image inpainting. The extensive experiments over the benchmarks validate its superiority. Our code is available at https://github.com/htyjers/DGTS-Inpainting.



### Neural Implicit Surface Reconstruction using Imaging Sonar
- **Arxiv ID**: http://arxiv.org/abs/2209.08221v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.08221v1)
- **Published**: 2022-09-17 02:23:09+00:00
- **Updated**: 2022-09-17 02:23:09+00:00
- **Authors**: Mohamad Qadri, Michael Kaess, Ioannis Gkioulekas
- **Comment**: 8 pages, 8 figures. This paper is under review
- **Journal**: None
- **Summary**: We present a technique for dense 3D reconstruction of objects using an imaging sonar, also known as forward-looking sonar (FLS). Compared to previous methods that model the scene geometry as point clouds or volumetric grids, we represent the geometry as a neural implicit function. Additionally, given such a representation, we use a differentiable volumetric renderer that models the propagation of acoustic waves to synthesize imaging sonar measurements. We perform experiments on real and synthetic datasets and show that our algorithm reconstructs high-fidelity surface geometry from multi-view FLS images at much higher quality than was possible with previous techniques and without suffering from their associated memory overhead.



### Few-Shot Classification with Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.08224v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08224v1)
- **Published**: 2022-09-17 02:39:09+00:00
- **Updated**: 2022-09-17 02:39:09+00:00
- **Authors**: Zhanyuan Yang, Jinghua Wang, Yingying Zhu
- **Comment**: To appear in ECCV 2022
- **Journal**: None
- **Summary**: A two-stage training paradigm consisting of sequential pre-training and meta-training stages has been widely used in current few-shot learning (FSL) research. Many of these methods use self-supervised learning and contrastive learning to achieve new state-of-the-art results. However, the potential of contrastive learning in both stages of FSL training paradigm is still not fully exploited. In this paper, we propose a novel contrastive learning-based framework that seamlessly integrates contrastive learning into both stages to improve the performance of few-shot classification. In the pre-training stage, we propose a self-supervised contrastive loss in the forms of feature vector vs. feature map and feature map vs. feature map, which uses global and local information to learn good initial representations. In the meta-training stage, we propose a cross-view episodic training mechanism to perform the nearest centroid classification on two different views of the same episode and adopt a distance-scaled contrastive loss based on them. These two strategies force the model to overcome the bias between views and promote the transferability of representations. Extensive experiments on three benchmark datasets demonstrate that our method achieves competitive results.



### Learning Distinct and Representative Styles for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2209.08231v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08231v2)
- **Published**: 2022-09-17 03:25:46+00:00
- **Updated**: 2023-08-15 07:24:55+00:00
- **Authors**: Qi Chen, Chaorui Deng, Qi Wu
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: Over the years, state-of-the-art (SoTA) image captioning methods have achieved promising results on some evaluation metrics (e.g., CIDEr). However, recent findings show that the captions generated by these methods tend to be biased toward the "average" caption that only captures the most general mode (a.k.a, language pattern) in the training corpus, i.e., the so-called mode collapse problem. Affected by it, the generated captions are limited in diversity and usually less informative than natural image descriptions made by humans. In this paper, we seek to avoid this problem by proposing a Discrete Mode Learning (DML) paradigm for image captioning. Our innovative idea is to explore the rich modes in the training caption corpus to learn a set of "mode embeddings", and further use them to control the mode of the generated captions for existing image captioning models. Specifically, the proposed DML optimizes a dual architecture that consists of an image-conditioned discrete variational autoencoder (CdVAE) branch and a mode-conditioned image captioning (MIC) branch. The CdVAE branch maps each image caption to one of the mode embeddings stored in a learned codebook, and is trained with a pure non-autoregressive generation objective to make the modes distinct and representative. The MIC branch can be simply modified from an existing image captioning model, where the mode embedding is added to the original word embeddings as the control signal. In the experiments, we apply the proposed DML to two widely used image captioning models, Transformer and AoANet. The results show that the learned mode embedding successfully facilitates these models to generate high-quality image captions with different modes, further leading to better performance for both diversity and quality on the MSCOCO dataset.



### Understanding the Impact of Image Quality and Distance of Objects to Object Detection Performance
- **Arxiv ID**: http://arxiv.org/abs/2209.08237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08237v1)
- **Published**: 2022-09-17 04:05:01+00:00
- **Updated**: 2022-09-17 04:05:01+00:00
- **Authors**: Yu Hao, Haoyang Pei, Yixuan Lyu, Zhongzheng Yuan, John-Ross Rizzo, Yao Wang, Yi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has made great strides for object detection in images. The detection accuracy and computational cost of object detection depend on the spatial resolution of an image, which may be constrained by both the camera and storage considerations. Compression is often achieved by reducing either spatial or amplitude resolution or, at times, both, both of which have well-known effects on performance. Detection accuracy also depends on the distance of the object of interest from the camera. Our work examines the impact of spatial and amplitude resolution, as well as object distance, on object detection accuracy and computational cost. We develop a resolution-adaptive variant of YOLOv5 (RA-YOLO), which varies the number of scales in the feature pyramid and detection head based on the spatial resolution of the input image. To train and evaluate this new method, we created a dataset of images with diverse spatial and amplitude resolutions by combining images from the TJU and Eurocity datasets and generating different resolutions by applying spatial resizing and compression. We first show that RA-YOLO achieves a good trade-off between detection accuracy and inference time over a large range of spatial resolutions. We then evaluate the impact of spatial and amplitude resolutions on object detection accuracy using the proposed RA-YOLO model. We demonstrate that the optimal spatial resolution that leads to the highest detection accuracy depends on the 'tolerated' image size. We further assess the impact of the distance of an object to the camera on the detection accuracy and show that higher spatial resolution enables a greater detection range. These results provide important guidelines for choosing the image spatial resolution and compression settings predicated on available bandwidth, storage, desired inference time, and/or desired detection range, in practical applications.



### Deep Plug-and-Play Prior for Hyperspectral Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/2209.08240v1
- **DOI**: 10.1016/j.neucom.2022.01.057
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.08240v1)
- **Published**: 2022-09-17 04:41:43+00:00
- **Updated**: 2022-09-17 04:41:43+00:00
- **Authors**: Zeqiang Lai, Kaixuan Wei, Ying Fu
- **Comment**: code at https://github.com/Zeqiang-Lai/DPHSIR
- **Journal**: Neurocomputing 481 (2022) 281-293
- **Summary**: Deep-learning-based hyperspectral image (HSI) restoration methods have gained great popularity for their remarkable performance but often demand expensive network retraining whenever the specifics of task changes. In this paper, we propose to restore HSIs in a unified approach with an effective plug-and-play method, which can jointly retain the flexibility of optimization-based methods and utilize the powerful representation capability of deep neural networks. Specifically, we first develop a new deep HSI denoiser leveraging gated recurrent convolution units, short- and long-term skip connections, and an augmented noise level map to better exploit the abundant spatio-spectral information within HSIs. It, therefore, leads to the state-of-the-art performance on HSI denoising under both Gaussian and complex noise settings. Then, the proposed denoiser is inserted into the plug-and-play framework as a powerful implicit HSI prior to tackle various HSI restoration tasks. Through extensive experiments on HSI super-resolution, compressed sensing, and inpainting, we demonstrate that our approach often achieves superior performance, which is competitive with or even better than the state-of-the-art on each task, via a single model without any task-specific training.



### Fast, Accurate and Object Boundary-Aware Surface Normal Estimation from Depth Maps
- **Arxiv ID**: http://arxiv.org/abs/2209.08241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08241v1)
- **Published**: 2022-09-17 04:43:09+00:00
- **Updated**: 2022-09-17 04:43:09+00:00
- **Authors**: Saed Moradi, Alireza Memarmoghadam, Denis Laurendeau
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a fast and accurate surface normal estimation method which can be directly used on depth maps (organized point clouds). The surface normal estimation process is formulated as a closed-form expression. In order to reduce the effect of measurement noise, the averaging operation is utilized in multi-direction manner. The multi-direction normal estimation process is reformulated in the next step to be implemented efficiently. Finally, a simple yet effective method is proposed to remove erroneous normal estimation at depth discontinuities. The proposed method is compared to well-known surface normal estimation algorithms. The results show that the proposed algorithm not only outperforms the baseline algorithms in term of accuracy, but also is fast enough to be used in real-time applications.



### Mitigating Both Covariate and Conditional Shift for Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2209.08253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.08253v1)
- **Published**: 2022-09-17 05:13:56+00:00
- **Updated**: 2022-09-17 05:13:56+00:00
- **Authors**: Jianxin Lin, Yongqiang Tang, Junping Wang, Wensheng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Domain generalization (DG) aims to learn a model on several source domains, hoping that the model can generalize well to unseen target domains. The distribution shift between domains contains the covariate shift and conditional shift, both of which the model must be able to handle for better generalizability. In this paper, a novel DG method is proposed to deal with the distribution shift via Visual Alignment and Uncertainty-guided belief Ensemble (VAUE). Specifically, for the covariate shift, a visual alignment module is designed to align the distribution of image style to a common empirical Gaussian distribution so that the covariate shift can be eliminated in the visual space. For the conditional shift, we adopt an uncertainty-guided belief ensemble strategy based on the subjective logic and Dempster-Shafer theory. The conditional distribution given a test sample is estimated by the dynamic combination of that of source domains. Comprehensive experiments are conducted to demonstrate the superior performance of the proposed method on four widely used datasets, i.e., Office-Home, VLCS, TerraIncognita, and PACS.



### Can segmentation models be trained with fully synthetically generated data?
- **Arxiv ID**: http://arxiv.org/abs/2209.08256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08256v1)
- **Published**: 2022-09-17 05:24:04+00:00
- **Updated**: 2022-09-17 05:24:04+00:00
- **Authors**: Virginia Fernandez, Walter Hugo Lopez Pinaya, Pedro Borges, Petru-Daniel Tudosiu, Mark S Graham, Tom Vercauteren, M Jorge Cardoso
- **Comment**: 12 pages, 2 (+2 App.) figures, 3 tables. Accepted at Simulation and
  Synthesis in Medical Imaging workshop (MICCAI 2022)
- **Journal**: None
- **Summary**: In order to achieve good performance and generalisability, medical image segmentation models should be trained on sizeable datasets with sufficient variability. Due to ethics and governance restrictions, and the costs associated with labelling data, scientific development is often stifled, with models trained and tested on limited data. Data augmentation is often used to artificially increase the variability in the data distribution and improve model generalisability. Recent works have explored deep generative models for image synthesis, as such an approach would enable the generation of an effectively infinite amount of varied data, addressing the generalisability and data access problems. However, many proposed solutions limit the user's control over what is generated. In this work, we propose brainSPADE, a model which combines a synthetic diffusion-based label generator with a semantic image generator. Our model can produce fully synthetic brain labels on-demand, with or without pathology of interest, and then generate a corresponding MRI image of an arbitrary guided style. Experiments show that brainSPADE synthetic data can be used to train segmentation models with performance comparable to that of models trained on real data.



### A study on the deviations in performance of FNNs and CNNs in the realm of grayscale adversarial images
- **Arxiv ID**: http://arxiv.org/abs/2209.08262v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.08262v1)
- **Published**: 2022-09-17 06:25:14+00:00
- **Updated**: 2022-09-17 06:25:14+00:00
- **Authors**: Durga Shree Nagabushanam, Steve Mathew, Chiranji Lal Chowdhary
- **Comment**: 19 pages, 12 tables, 4 figures
- **Journal**: None
- **Summary**: Neural Networks are prone to having lesser accuracy in the classification of images with noise perturbation. Convolutional Neural Networks, CNNs are known for their unparalleled accuracy in the classification of benign images. But our study shows that they are extremely vulnerable to noise addition while Feed-forward Neural Networks, FNNs show very less correspondence with noise perturbation, maintaining their accuracy almost undisturbed. FNNs are observed to be better at classifying noise-intensive, single-channeled images that are just sheer noise to human vision. In our study, we have used the hand-written digits dataset, MNIST with the following architectures: FNNs with 1 and 2 hidden layers and CNNs with 3, 4, 6 and 8 convolutions and analyzed their accuracies. FNNs stand out to show that irrespective of the intensity of noise, they have a classification accuracy of more than 85%. In our analysis of CNNs with this data, the deceleration of classification accuracy of CNN with 8 convolutions was half of that of the rest of the CNNs. Correlation analysis and mathematical modelling of the accuracy trends act as roadmaps to these conclusions.



### Scalable SoftGroup for 3D Instance Segmentation on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2209.08263v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08263v2)
- **Published**: 2022-09-17 06:36:18+00:00
- **Updated**: 2022-11-13 04:57:35+00:00
- **Authors**: Thang Vu, Kookhoi Kim, Tung M. Luu, Thanh Nguyen, Junyeong Kim, Chang D. Yoo
- **Comment**: Technical report. Extension of arXiv:2203.01509
- **Journal**: None
- **Summary**: This paper considers a network referred to as SoftGroup for accurate and scalable 3D instance segmentation. Existing state-of-the-art methods produce hard semantic predictions followed by grouping to obtain instance segmentation results. However, the errors stemming from hard decisions propagate into grouping that results in low overlaps of the predicted instances with the ground truth and substantial false positives. To address the aforementioned problems, SoftGroup allows each point to be associated with multiple classes to mitigate the problem stemming from semantic prediction errors and suppresses false positive instances by learning to categorize them as background. Regarding scalability, the existing fast methods require computational time on the order of tens of seconds on large-scale scenes, which is unsatisfactory and far from applicable for real-time. Our finding is that the $k$-Nearest Neighbor ($k$-NN) module, which serves as the prerequisite of grouping, introduces computational bottleneck. SoftGroup is extended to resolve this computational bottleneck, which is referred to as SoftGroup++. The proposed SoftGroup++ reduces time complexity with octree $k$-NN and reduces search space with class-aware pyramid scaling and late devoxelization. Experimental results on various indoor and outdoor datasets demonstrate the efficacy and generality of the proposed SoftGroup and SoftGroup++. Their performances surpass the strongest baseline by a large margin (6\% $\sim$ 16\%) in terms of AP$_{50}$. On datasets with large-scale scenes, SoftGroup++ achieves 6$\times$ speed boost on average compared to SoftGroup. Furthermore, SoftGroup can be extended to perform object detection and panoptic segmentation with nontrivial improvements over existing methods. The source code and trained models are available at \url{https://github.com/thangvubk/SoftGroup}.



### 6DOF Pose Estimation of a 3D Rigid Object based on Edge-enhanced Point Pair Features
- **Arxiv ID**: http://arxiv.org/abs/2209.08266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08266v1)
- **Published**: 2022-09-17 07:05:50+00:00
- **Updated**: 2022-09-17 07:05:50+00:00
- **Authors**: Chenyi Liu, Fei Chen, Lu Deng, Renjiao Yi, Lintao Zheng, Chenyang Zhu, Jia Wang, Kai Xu
- **Comment**: 16 pages,20 figures
- **Journal**: None
- **Summary**: The point pair feature (PPF) is widely used for 6D pose estimation. In this paper, we propose an efficient 6D pose estimation method based on the PPF framework. We introduce a well-targeted down-sampling strategy that focuses more on edge area for efficient feature extraction of complex geometry. A pose hypothesis validation approach is proposed to resolve the symmetric ambiguity by calculating edge matching degree. We perform evaluations on two challenging datasets and one real-world collected dataset, demonstrating the superiority of our method on pose estimation of geometrically complex, occluded, symmetrical objects. We further validate our method by applying it to simulated punctures.



### CARNet:Compression Artifact Reduction for Point Cloud Attribute
- **Arxiv ID**: http://arxiv.org/abs/2209.08276v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08276v1)
- **Published**: 2022-09-17 08:05:35+00:00
- **Updated**: 2022-09-17 08:05:35+00:00
- **Authors**: Dandan Ding, Junzhe Zhang, Jianqiang Wang, Zhan Ma
- **Comment**: 13pages, 8figures
- **Journal**: None
- **Summary**: A learning-based adaptive loop filter is developed for the Geometry-based Point Cloud Compression (G-PCC) standard to reduce attribute compression artifacts. The proposed method first generates multiple Most-Probable Sample Offsets (MPSOs) as potential compression distortion approximations, and then linearly weights them for artifact mitigation. As such, we drive the filtered reconstruction as close to the uncompressed PCA as possible. To this end, we devise a Compression Artifact Reduction Network (CARNet) which consists of two consecutive processing phases: MPSOs derivation and MPSOs combination. The MPSOs derivation uses a two-stream network to model local neighborhood variations from direct spatial embedding and frequency-dependent embedding, where sparse convolutions are utilized to best aggregate information from sparsely and irregularly distributed points. The MPSOs combination is guided by the least square error metric to derive weighting coefficients on the fly to further capture content dynamics of input PCAs. The CARNet is implemented as an in-loop filtering tool of the GPCC, where those linear weighting coefficients are encapsulated into the bitstream with negligible bit rate overhead. Experimental results demonstrate significant improvement over the latest GPCC both subjectively and objectively.



### MiNL: Micro-images based Neural Representation for Light Fields
- **Arxiv ID**: http://arxiv.org/abs/2209.08277v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08277v1)
- **Published**: 2022-09-17 08:06:38+00:00
- **Updated**: 2022-09-17 08:06:38+00:00
- **Authors**: Hanxin Zhu, Henan Wang, Zhibo Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional representations for light fields can be separated into two types: explicit representation and implicit representation. Unlike explicit representation that represents light fields as Sub-Aperture Images (SAIs) based arrays or Micro-Images (MIs) based lenslet images, implicit representation treats light fields as neural networks, which is inherently a continuous representation in contrast to discrete explicit representation. However, at present almost all the implicit representations for light fields utilize SAIs to train an MLP to learn a pixel-wise mapping from 4D spatial-angular coordinate to pixel colors, which is neither compact nor of low complexity. Instead, in this paper we propose MiNL, a novel MI-wise implicit neural representation for light fields that train an MLP + CNN to learn a mapping from 2D MI coordinates to MI colors. Given the micro-image's coordinate, MiNL outputs the corresponding micro-image's RGB values. Light field encoding in MiNL is just training a neural network to regress the micro-images and the decoding process is a simple feedforward operation. Compared with common pixel-wise implicit representation, MiNL is more compact and efficient that has faster decoding speed (\textbf{$\times$80$\sim$180} speed-up) as well as better visual quality (\textbf{1$\sim$4dB} PSNR improvement on average).



### Self-supervised learning of hologram reconstruction using physics consistency
- **Arxiv ID**: http://arxiv.org/abs/2209.08288v2
- **DOI**: 10.1038/s42256-023-00704-7
- **Categories**: **cs.CV**, cs.LG, eess.IV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2209.08288v2)
- **Published**: 2022-09-17 09:02:10+00:00
- **Updated**: 2023-06-17 08:53:11+00:00
- **Authors**: Luzhe Huang, Hanlong Chen, Tairan Liu, Aydogan Ozcan
- **Comment**: 45 pages, 6 Figures
- **Journal**: Nature Machine Intelligence (2023)
- **Summary**: The past decade has witnessed transformative applications of deep learning in various computational imaging, sensing and microscopy tasks. Due to the supervised learning schemes employed, these methods mostly depend on large-scale, diverse, and labeled training data. The acquisition and preparation of such training image datasets are often laborious and costly, also leading to biased estimation and limited generalization to new sample types. Here, we report a self-supervised learning model, termed GedankenNet, that eliminates the need for labeled or experimental training data, and demonstrate its effectiveness and superior generalization on hologram reconstruction tasks. Without prior knowledge about the sample types to be imaged, the self-supervised learning model was trained using a physics-consistency loss and artificial random images that are synthetically generated without any experiments or resemblance to real-world samples. After its self-supervised training, GedankenNet successfully generalized to experimental holograms of various unseen biological samples, reconstructing the phase and amplitude images of different types of objects using experimentally acquired test holograms. Without access to experimental data or knowledge of real samples of interest or their spatial features, GedankenNet's self-supervised learning achieved complex-valued image reconstructions that are consistent with the Maxwell's equations, and its output inference and object solutions accurately represent the wave propagation in free-space. GedankenNet framework also exhibits resilience to random, unknown perturbations in the physical forward model, including changes in the hologram distances, pixel size and illumination wavelength. This self-supervised learning of image reconstruction tasks creates new opportunities for various inverse problems in holography, microscopy and computational imaging fields.



### Continuously Controllable Facial Expression Editing in Talking Face Videos
- **Arxiv ID**: http://arxiv.org/abs/2209.08289v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.08289v1)
- **Published**: 2022-09-17 09:05:47+00:00
- **Updated**: 2022-09-17 09:05:47+00:00
- **Authors**: Zhiyao Sun, Yu-Hui Wen, Tian Lv, Yanan Sun, Ziyang Zhang, Yaoyuan Wang, Yong-Jin Liu
- **Comment**: Demo video: https://youtu.be/WD-bNVya6kM
- **Journal**: None
- **Summary**: Recently audio-driven talking face video generation has attracted considerable attention. However, very few researches address the issue of emotional editing of these talking face videos with continuously controllable expressions, which is a strong demand in the industry. The challenge is that speech-related expressions and emotion-related expressions are often highly coupled. Meanwhile, traditional image-to-image translation methods cannot work well in our application due to the coupling of expressions with other attributes such as poses, i.e., translating the expression of the character in each frame may simultaneously change the head pose due to the bias of the training data distribution. In this paper, we propose a high-quality facial expression editing method for talking face videos, allowing the user to control the target emotion in the edited video continuously. We present a new perspective for this task as a special case of motion information editing, where we use a 3DMM to capture major facial movements and an associated texture map modeled by a StyleGAN to capture appearance details. Both representations (3DMM and texture map) contain emotional information and can be continuously modified by neural networks and easily smoothed by averaging in coefficient/latent spaces, making our method simple yet effective. We also introduce a mouth shape preservation loss to control the trade-off between lip synchronization and the degree of exaggeration of the edited expression. Extensive experiments and a user study show that our method achieves state-of-the-art performance across various evaluation criteria.



### Changer: Feature Interaction is What You Need for Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.08290v1
- **DOI**: 10.1109/TGRS.2023.3277496
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08290v1)
- **Published**: 2022-09-17 09:13:02+00:00
- **Updated**: 2022-09-17 09:13:02+00:00
- **Authors**: Sheng Fang, Kaiyu Li, Zhe Li
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Change detection is an important tool for long-term earth observation missions. It takes bi-temporal images as input and predicts "where" the change has occurred. Different from other dense prediction tasks, a meaningful consideration for change detection is the interaction between bi-temporal features. With this motivation, in this paper we propose a novel general change detection architecture, MetaChanger, which includes a series of alternative interaction layers in the feature extractor. To verify the effectiveness of MetaChanger, we propose two derived models, ChangerAD and ChangerEx with simple interaction strategies: Aggregation-Distribution (AD) and "exchange". AD is abstracted from some complex interaction methods, and "exchange" is a completely parameter\&computation-free operation by exchanging bi-temporal features. In addition, for better alignment of bi-temporal features, we propose a flow dual-alignment fusion (FDAF) module which allows interactive alignment and feature fusion. Crucially, we observe Changer series models achieve competitive performance on different scale change detection datasets. Further, our proposed ChangerAD and ChangerEx could serve as a starting baseline for future MetaChanger design.



### Active-Passive SimStereo -- Benchmarking the Cross-Generalization Capabilities of Deep Learning-based Stereo Methods
- **Arxiv ID**: http://arxiv.org/abs/2209.08305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08305v1)
- **Published**: 2022-09-17 10:30:32+00:00
- **Updated**: 2022-09-17 10:30:32+00:00
- **Authors**: Laurent Jospin, Allen Antony, Lian Xu, Hamid Laga, Farid Boussaid, Mohammed Bennamoun
- **Comment**: 22 pages, 12 figures, accepted in NeurIPS 2022 Datasets and
  Benchmarks Track
- **Journal**: None
- **Summary**: In stereo vision, self-similar or bland regions can make it difficult to match patches between two images. Active stereo-based methods mitigate this problem by projecting a pseudo-random pattern on the scene so that each patch of an image pair can be identified without ambiguity. However, the projected pattern significantly alters the appearance of the image. If this pattern acts as a form of adversarial noise, it could negatively impact the performance of deep learning-based methods, which are now the de-facto standard for dense stereo vision. In this paper, we propose the Active-Passive SimStereo dataset and a corresponding benchmark to evaluate the performance gap between passive and active stereo images for stereo matching algorithms. Using the proposed benchmark and an additional ablation study, we show that the feature extraction and matching modules of a selection of twenty selected deep learning-based stereo matching methods generalize to active stereo without a problem. However, the disparity refinement modules of three of the twenty architectures (ACVNet, CascadeStereo, and StereoNet) are negatively affected by the active stereo patterns due to their reliance on the appearance of the input images.



### Learning to Weight Samples for Dynamic Early-exiting Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.08310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08310v1)
- **Published**: 2022-09-17 10:46:32+00:00
- **Updated**: 2022-09-17 10:46:32+00:00
- **Authors**: Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfen Cao, Wenhui Huang, Chao Deng, Gao Huang
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Early exiting is an effective paradigm for improving the inference efficiency of deep networks. By constructing classifiers with varying resource demands (the exits), such networks allow easy samples to be output at early exits, removing the need for executing deeper layers. While existing works mainly focus on the architectural design of multi-exit networks, the training strategies for such models are largely left unexplored. The current state-of-the-art models treat all samples the same during training. However, the early-exiting behavior during testing has been ignored, leading to a gap between training and testing. In this paper, we propose to bridge this gap by sample weighting. Intuitively, easy samples, which generally exit early in the network during inference, should contribute more to training early classifiers. The training of hard samples (mostly exit from deeper layers), however, should be emphasized by the late classifiers. Our work proposes to adopt a weight prediction network to weight the loss of different training samples at each exit. This weight prediction network and the backbone model are jointly optimized under a meta-learning framework with a novel optimization objective. By bringing the adaptive behavior during inference into the training phase, we show that the proposed weighting mechanism consistently improves the trade-off between classification accuracy and inference efficiency. Code is available at https://github.com/LeapLabTHU/L2W-DEN.



### RGB-Event Fusion for Moving Object Detection in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2209.08323v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.08323v2)
- **Published**: 2022-09-17 12:59:08+00:00
- **Updated**: 2023-03-09 14:32:56+00:00
- **Authors**: Zhuyun Zhou, Zongwei Wu, Rémi Boutteau, Fan Yang, Cédric Demonceaux, Dominique Ginhac
- **Comment**: ICRA'23
- **Journal**: None
- **Summary**: Moving Object Detection (MOD) is a critical vision task for successfully achieving safe autonomous driving. Despite plausible results of deep learning methods, most existing approaches are only frame-based and may fail to reach reasonable performance when dealing with dynamic traffic participants. Recent advances in sensor technologies, especially the Event camera, can naturally complement the conventional camera approach to better model moving objects. However, event-based works often adopt a pre-defined time window for event representation, and simply integrate it to estimate image intensities from events, neglecting much of the rich temporal information from the available asynchronous events. Therefore, from a new perspective, we propose RENet, a novel RGB-Event fusion Network, that jointly exploits the two complementary modalities to achieve more robust MOD under challenging scenarios for autonomous driving. Specifically, we first design a temporal multi-scale aggregation module to fully leverage event frames from both the RGB exposure time and larger intervals. Then we introduce a bi-directional fusion module to attentively calibrate and fuse multi-modal features. To evaluate the performance of our network, we carefully select and annotate a sub-MOD dataset from the commonly used DSEC dataset. Extensive experiments demonstrate that our proposed method performs significantly better than the state-of-the-art RGB-Event fusion alternatives. The source code and dataset are publicly available at: https://github.com/ZZY-Zhou/RENet.



### Non-Imaging Medical Data Synthesis for Trustworthy AI: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/2209.09239v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.09239v1)
- **Published**: 2022-09-17 13:34:17+00:00
- **Updated**: 2022-09-17 13:34:17+00:00
- **Authors**: Xiaodan Xing, Huanjun Wu, Lichao Wang, Iain Stenson, May Yong, Javier Del Ser, Simon Walsh, Guang Yang
- **Comment**: 35 pages, Submitted to ACM Computing Surveys
- **Journal**: None
- **Summary**: Data quality is the key factor for the development of trustworthy AI in healthcare. A large volume of curated datasets with controlled confounding factors can help improve the accuracy, robustness and privacy of downstream AI algorithms. However, access to good quality datasets is limited by the technical difficulty of data acquisition and large-scale sharing of healthcare data is hindered by strict ethical restrictions. Data synthesis algorithms, which generate data with a similar distribution as real clinical data, can serve as a potential solution to address the scarcity of good quality data during the development of trustworthy AI. However, state-of-the-art data synthesis algorithms, especially deep learning algorithms, focus more on imaging data while neglecting the synthesis of non-imaging healthcare data, including clinical measurements, medical signals and waveforms, and electronic healthcare records (EHRs). Thus, in this paper, we will review the synthesis algorithms, particularly for non-imaging medical data, with the aim of providing trustworthy AI in this domain. This tutorial-styled review paper will provide comprehensive descriptions of non-imaging medical data synthesis on aspects including algorithms, evaluations, limitations and future research directions.



### Lightweight Spatial-Channel Adaptive Coordination of Multilevel Refinement Enhancement Network for Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2209.08337v1
- **DOI**: 10.1016/j.knosys.2022.109824
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08337v1)
- **Published**: 2022-09-17 14:15:42+00:00
- **Updated**: 2022-09-17 14:15:42+00:00
- **Authors**: Yuxi Cai, Huicheng Lai, Zhenghong Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Benefiting from the vigorous development of deep learning, many CNN-based image super-resolution methods have emerged and achieved better results than traditional algorithms. However, it is difficult for most algorithms to adaptively adjust the spatial region and channel features at the same time, let alone the information exchange between them. In addition, the exchange of information between attention modules is even less visible to researchers. To solve these problems, we put forward a lightweight spatial-channel adaptive coordination of multilevel refinement enhancement networks(MREN). Specifically, we construct a space-channel adaptive coordination block, which enables the network to learn the spatial region and channel feature information of interest under different receptive fields. In addition, the information of the corresponding feature processing level between the spatial part and the channel part is exchanged with the help of jump connection to achieve the coordination between the two. We establish a communication bridge between attention modules through a simple linear combination operation, so as to more accurately and continuously guide the network to pay attention to the information of interest. Extensive experiments on several standard test sets have shown that our MREN achieves superior performance over other advanced algorithms with a very small number of parameters and very low computational complexity.



### OA-SLAM: Leveraging Objects for Camera Relocalization in Visual SLAM
- **Arxiv ID**: http://arxiv.org/abs/2209.08338v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08338v1)
- **Published**: 2022-09-17 14:20:08+00:00
- **Updated**: 2022-09-17 14:20:08+00:00
- **Authors**: Matthieu Zins, Gilles Simon, Marie-Odile Berger
- **Comment**: ISMAR 2022
- **Journal**: None
- **Summary**: In this work, we explore the use of objects in Simultaneous Localization and Mapping in unseen worlds and propose an object-aided system (OA-SLAM). More precisely, we show that, compared to low-level points, the major benefit of objects lies in their higher-level semantic and discriminating power. Points, on the contrary, have a better spatial localization accuracy than the generic coarse models used to represent objects (cuboid or ellipsoid). We show that combining points and objects is of great interest to address the problem of camera pose recovery. Our main contributions are: (1) we improve the relocalization ability of a SLAM system using high-level object landmarks; (2) we build an automatic system, capable of identifying, tracking and reconstructing objects with 3D ellipsoids; (3) we show that object-based localization can be used to reinitialize or resume camera tracking. Our fully automatic system allows on-the-fly object mapping and enhanced pose tracking recovery, which we think, can significantly benefit to the AR community. Our experiments show that the camera can be relocalized from viewpoints where classical methods fail. We demonstrate that this localization allows a SLAM system to continue working despite a tracking loss, which can happen frequently with an uninitiated user. Our code and test data are released at gitlab.inria.fr/tangram/oa-slam.



### Data Efficient Visual Place Recognition Using Extremely JPEG-Compressed Images
- **Arxiv ID**: http://arxiv.org/abs/2209.08343v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08343v2)
- **Published**: 2022-09-17 14:46:28+00:00
- **Updated**: 2023-03-01 13:13:47+00:00
- **Authors**: Mihnea-Alexandru Tomita, Bruno Ferrarini, Michael Milford, Klaus McDonald-Maier, Shoaib Ehsan
- **Comment**: The paper is currently under-review. 8 pages, 8 figures
- **Journal**: None
- **Summary**: Visual Place Recognition (VPR) is the ability of a robotic platform to correctly interpret visual stimuli from its on-board cameras in order to determine whether it is currently located in a previously visited place, despite different viewpoint, illumination and appearance changes. JPEG is a widely used image compression standard that is capable of significantly reducing the size of an image at the cost of image clarity. For applications where several robotic platforms are simultaneously deployed, the visual data gathered must be transmitted remotely between each robot. Hence, JPEG compression can be employed to drastically reduce the amount of data transmitted over a communication channel, as working with limited bandwidth for VPR can be proven to be a challenging task. However, the effects of JPEG compression on the performance of current VPR techniques have not been previously studied. For this reason, this paper presents an in-depth study of JPEG compression in VPR related scenarios. We use a selection of well-established VPR techniques on well-established benchmark datasets with various amounts of compression applied. We show that by introducing compression, the VPR performance is drastically reduced, especially in the higher spectrum of compression. Moreover, this paper demonstrates how fine-tuning a CNN can be utilised as an optimisation method for JPEG compressed data to perform more consistently with the image transformations detected in extremely JPEG compressed images.



### Shape Completion with Points in the Shadow
- **Arxiv ID**: http://arxiv.org/abs/2209.08345v3
- **DOI**: 10.1145/3550469.3555389
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08345v3)
- **Published**: 2022-09-17 14:58:56+00:00
- **Updated**: 2022-10-04 13:58:33+00:00
- **Authors**: Bowen Zhang, Xi Zhao, He Wang, Ruizhen Hu
- **Comment**: SIGGRAPH Aisa 2022 Conference Paper
- **Journal**: None
- **Summary**: Single-view point cloud completion aims to recover the full geometry of an object based on only limited observation, which is extremely hard due to the data sparsity and occlusion. The core challenge is to generate plausible geometries to fill the unobserved part of the object based on a partial scan, which is under-constrained and suffers from a huge solution space. Inspired by the classic shadow volume technique in computer graphics, we propose a new method to reduce the solution space effectively. Our method considers the camera a light source that casts rays toward the object. Such light rays build a reasonably constrained but sufficiently expressive basis for completion. The completion process is then formulated as a point displacement optimization problem. Points are initialized at the partial scan and then moved to their goal locations with two types of movements for each point: directional movements along the light rays and constrained local movement for shape refinement. We design neural networks to predict the ideal point movements to get the completion results. We demonstrate that our method is accurate, robust, and generalizable through exhaustive evaluation and comparison. Moreover, it outperforms state-of-the-art methods qualitatively and quantitatively on MVP datasets.



### Human Pose Driven Object Effects Recommendation
- **Arxiv ID**: http://arxiv.org/abs/2209.08353v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.08353v1)
- **Published**: 2022-09-17 15:32:54+00:00
- **Updated**: 2022-09-17 15:32:54+00:00
- **Authors**: Zhaoxin Fan, Fengxin Li, Hongyan Liu, Jun He, Xiaoyong Du
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we research the new topic of object effects recommendation in micro-video platforms, which is a challenging but important task for many practical applications such as advertisement insertion. To avoid the problem of introducing background bias caused by directly learning video content from image frames, we propose to utilize the meaningful body language hidden in 3D human pose for recommendation. To this end, in this work, a novel human pose driven object effects recommendation network termed PoseRec is introduced. PoseRec leverages the advantages of 3D human pose detection and learns information from multi-frame 3D human pose for video-item registration, resulting in high quality object effects recommendation performance. Moreover, to solve the inherent ambiguity and sparsity issues that exist in object effects recommendation, we further propose a novel item-aware implicit prototype learning module and a novel pose-aware transductive hard-negative mining module to better learn pose-item relationships. What's more, to benchmark methods for the new research topic, we build a new dataset for object effects recommendation named Pose-OBE. Extensive experiments on Pose-OBE demonstrate that our method can achieve superior performance than strong baselines.



### Towards Connectivity-Aware Pulmonary Airway Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.08355v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.08355v3)
- **Published**: 2022-09-17 15:47:01+00:00
- **Updated**: 2023-03-09 04:06:16+00:00
- **Authors**: Minghui Zhang, Yun Gu
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Detailed pulmonary airway segmentation is a clinically important task for endobronchial intervention and treatment of peripheral pulmonary lesions. Convolutional Neural Networks (CNNs) are promising for automated analysis of medical imaging, which however performs poorly on airway segmentation. Specifically, breakage of small bronchi distals cannot be effectively eliminated in the prediction results of CNNs, which is detrimental to use as a reference for bronchoscopic-assisted surgery. In this paper, we proposed a connectivity-aware segmentation framework to improve the performance of airway segmentation. A Connectivity-Aware Surrogate (CAS) module is first proposed to balance the training progress within-class distribution. Furthermore, a Local-Sensitive Distance (LSD) module is designed to identify the breakage and minimize the variation of the distance map between the prediction and ground-truth. The proposed method is validated with the publically available reference airway segmentation datasets. The detected rate of branch and length on public EXACT'09 and BAS datasets are 82.1%/79.6% and 96.5%/91.5% respectively, demonstrating the effectiveness of the method in terms of improving the connectedness of the segmentation performance.



### Uncertainty Guided Policy for Active Robotic 3D Reconstruction using Neural Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2209.08409v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.08409v1)
- **Published**: 2022-09-17 21:28:57+00:00
- **Updated**: 2022-09-17 21:28:57+00:00
- **Authors**: Soomin Lee, Le Chen, Jiahao Wang, Alexander Liniger, Suryansh Kumar, Fisher Yu
- **Comment**: 8 pages, 9 figure; Accepted for publication at IEEE Robotics and
  Automation Letters (RA-L) 2022
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of active robotic 3D reconstruction of an object. In particular, we study how a mobile robot with an arm-held camera can select a favorable number of views to recover an object's 3D shape efficiently. Contrary to the existing solution to this problem, we leverage the popular neural radiance fields-based object representation, which has recently shown impressive results for various computer vision tasks. However, it is not straightforward to directly reason about an object's explicit 3D geometric details using such a representation, making the next-best-view selection problem for dense 3D reconstruction challenging. This paper introduces a ray-based volumetric uncertainty estimator, which computes the entropy of the weight distribution of the color samples along each ray of the object's implicit neural representation. We show that it is possible to infer the uncertainty of the underlying 3D geometry given a novel view with the proposed estimator. We then present a next-best-view selection policy guided by the ray-based volumetric uncertainty in neural radiance fields-based representations. Encouraging experimental results on synthetic and real-world data suggest that the approach presented in this paper can enable a new research direction of using an implicit 3D object representation for the next-best-view problem in robot vision applications, distinguishing our approach from the existing approaches that rely on explicit 3D geometric modeling.



### Spatial-Temporal Deep Embedding for Vehicle Trajectory Reconstruction from High-Angle Video
- **Arxiv ID**: http://arxiv.org/abs/2209.08417v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.08417v1)
- **Published**: 2022-09-17 22:32:05+00:00
- **Updated**: 2022-09-17 22:32:05+00:00
- **Authors**: Tianya T. Zhang Ph. D., Peter J. Jin Ph. D., Han Zhou, Benedetto Piccoli, Ph. D
- **Comment**: None
- **Journal**: None
- **Summary**: Spatial-temporal Map (STMap)-based methods have shown great potential to process high-angle videos for vehicle trajectory reconstruction, which can meet the needs of various data-driven modeling and imitation learning applications. In this paper, we developed Spatial-Temporal Deep Embedding (STDE) model that imposes parity constraints at both pixel and instance levels to generate instance-aware embeddings for vehicle stripe segmentation on STMap. At pixel level, each pixel was encoded with its 8-neighbor pixels at different ranges, and this encoding is subsequently used to guide a neural network to learn the embedding mechanism. At the instance level, a discriminative loss function is designed to pull pixels belonging to the same instance closer and separate the mean value of different instances far apart in the embedding space. The output of the spatial-temporal affinity is then optimized by the mutex-watershed algorithm to obtain final clustering results. Based on segmentation metrics, our model outperformed five other baselines that have been used for STMap processing and shows robustness under the influence of shadows, static noises, and overlapping. The designed model is applied to process all public NGSIM US-101 videos to generate complete vehicle trajectories, indicating a good scalability and adaptability. Last but not least, the strengths of the scanline method with STDE and future directions were discussed. Code, STMap dataset and video trajectory are made publicly available in the online repository. GitHub Link: shorturl.at/jklT0.



### Automated Segmentation and Recurrence Risk Prediction of Surgically Resected Lung Tumors with Adaptive Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.08423v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.08423v1)
- **Published**: 2022-09-17 23:06:22+00:00
- **Updated**: 2022-09-17 23:06:22+00:00
- **Authors**: Marguerite B. Basta, Sarfaraz Hussein, Hsiang Hsu, Flavio P. Calmon
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Lung cancer is the leading cause of cancer related mortality by a significant margin. While new technologies, such as image segmentation, have been paramount to improved detection and earlier diagnoses, there are still significant challenges in treating the disease. In particular, despite an increased number of curative resections, many postoperative patients still develop recurrent lesions. Consequently, there is a significant need for prognostic tools that can more accurately predict a patient's risk for recurrence.   In this paper, we explore the use of convolutional neural networks (CNNs) for the segmentation and recurrence risk prediction of lung tumors that are present in preoperative computed tomography (CT) images. First, expanding upon recent progress in medical image segmentation, a residual U-Net is used to localize and characterize each nodule. Then, the identified tumors are passed to a second CNN for recurrence risk prediction. The system's final results are produced with a random forest classifier that synthesizes the predictions of the second network with clinical attributes. The segmentation stage uses the LIDC-IDRI dataset and achieves a dice score of 70.3%. The recurrence risk stage uses the NLST dataset from the National Cancer institute and achieves an AUC of 73.0%. Our proposed framework demonstrates that first, automated nodule segmentation methods can generalize to enable pipelines for a wide range of multitask systems and second, that deep learning and image processing have the potential to improve current prognostic tools. To the best of our knowledge, it is the first fully automated segmentation and recurrence risk prediction system.



### Introspective Learning : A Two-Stage Approach for Inference in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.08425v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.08425v1)
- **Published**: 2022-09-17 23:31:03+00:00
- **Updated**: 2022-09-17 23:31:03+00:00
- **Authors**: Mohit Prabhushankar, Ghassan AlRegib
- **Comment**: Accepted at NeurIPS 2022
- **Journal**: None
- **Summary**: In this paper, we advocate for two stages in a neural network's decision making process. The first is the existing feed-forward inference framework where patterns in given data are sensed and associated with previously learned patterns. The second stage is a slower reflection stage where we ask the network to reflect on its feed-forward decision by considering and evaluating all available choices. Together, we term the two stages as introspective learning. We use gradients of trained neural networks as a measurement of this reflection. A simple three-layered Multi Layer Perceptron is used as the second stage that predicts based on all extracted gradient features. We perceptually visualize the post-hoc explanations from both stages to provide a visual grounding to introspection. For the application of recognition, we show that an introspective network is 4% more robust and 42% less prone to calibration errors when generalizing to noisy data. We also illustrate the value of introspective networks in downstream tasks that require generalizability and calibration including active learning, out-of-distribution detection, and uncertainty estimation. Finally, we ground the proposed machine introspection to human introspection for the application of image quality assessment.



### DytanVO: Joint Refinement of Visual Odometry and Motion Segmentation in Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/2209.08430v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.08430v4)
- **Published**: 2022-09-17 23:56:03+00:00
- **Updated**: 2023-04-29 04:37:57+00:00
- **Authors**: Shihao Shen, Yilin Cai, Wenshan Wang, Sebastian Scherer
- **Comment**: Accepted to ICRA 2023
- **Journal**: None
- **Summary**: Learning-based visual odometry (VO) algorithms achieve remarkable performance on common static scenes, benefiting from high-capacity models and massive annotated data, but tend to fail in dynamic, populated environments. Semantic segmentation is largely used to discard dynamic associations before estimating camera motions but at the cost of discarding static features and is hard to scale up to unseen categories. In this paper, we leverage the mutual dependence between camera ego-motion and motion segmentation and show that both can be jointly refined in a single learning-based framework. In particular, we present DytanVO, the first supervised learning-based VO method that deals with dynamic environments. It takes two consecutive monocular frames in real-time and predicts camera ego-motion in an iterative fashion. Our method achieves an average improvement of 27.7% in ATE over state-of-the-art VO solutions in real-world dynamic environments, and even performs competitively among dynamic visual SLAM systems which optimize the trajectory on the backend. Experiments on plentiful unseen environments also demonstrate our method's generalizability.



