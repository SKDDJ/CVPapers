# Arxiv Papers in cs.CV on 2022-09-11
### Scattering Model Guided Adversarial Examples for SAR Target Recognition: Attack and Defense
- **Arxiv ID**: http://arxiv.org/abs/2209.04779v1
- **DOI**: 10.1109/TGRS.2022.3213305
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04779v1)
- **Published**: 2022-09-11 03:41:12+00:00
- **Updated**: 2022-09-11 03:41:12+00:00
- **Authors**: Bowen Peng, Bo Peng, Jie Zhou, Jianyue Xie, Li Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) based Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) systems have shown to be highly vulnerable to adversarial perturbations that are deliberately designed yet almost imperceptible but can bias DNN inference when added to targeted objects. This leads to serious safety concerns when applying DNNs to high-stake SAR ATR applications. Therefore, enhancing the adversarial robustness of DNNs is essential for implementing DNNs to modern real-world SAR ATR systems. Toward building more robust DNN-based SAR ATR models, this article explores the domain knowledge of SAR imaging process and proposes a novel Scattering Model Guided Adversarial Attack (SMGAA) algorithm which can generate adversarial perturbations in the form of electromagnetic scattering response (called adversarial scatterers). The proposed SMGAA consists of two parts: 1) a parametric scattering model and corresponding imaging method and 2) a customized gradient-based optimization algorithm. First, we introduce the effective Attributed Scattering Center Model (ASCM) and a general imaging method to describe the scattering behavior of typical geometric structures in the SAR imaging process. By further devising several strategies to take the domain knowledge of SAR target images into account and relax the greedy search procedure, the proposed method does not need to be prudentially finetuned, but can efficiently to find the effective ASCM parameters to fool the SAR classifiers and facilitate the robust model training. Comprehensive evaluations on the MSTAR dataset show that the adversarial scatterers generated by SMGAA are more robust to perturbations and transformations in the SAR processing chain than the currently studied attacks, and are effective to construct a defensive model against the malicious scatterers.



### MAiVAR: Multimodal Audio-Image and Video Action Recognizer
- **Arxiv ID**: http://arxiv.org/abs/2209.04780v1
- **DOI**: 10.1109/VCIP56404.2022.10008833
- **Categories**: **cs.CV**, I.2.10; I.5.4; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/2209.04780v1)
- **Published**: 2022-09-11 03:52:27+00:00
- **Updated**: 2022-09-11 03:52:27+00:00
- **Authors**: Muhammad Bilal Shaikh, Douglas Chai, Syed Mohammed Shamsul Islam, Naveed Akhtar
- **Comment**: Peer reviewed & accepted at IEEE VCIP 2022 (http://www.vcip2022.org/)
- **Journal**: 2022 IEEE International Conference on Visual Communications and
  Image Processing (VCIP)
- **Summary**: Currently, action recognition is predominately performed on video data as processed by CNNs. We investigate if the representation process of CNNs can also be leveraged for multimodal action recognition by incorporating image-based audio representations of actions in a task. To this end, we propose Multimodal Audio-Image and Video Action Recognizer (MAiVAR), a CNN-based audio-image to video fusion model that accounts for video and audio modalities to achieve superior action recognition performance. MAiVAR extracts meaningful image representations of audio and fuses it with video representation to achieve better performance as compared to both modalities individually on a large-scale action recognition dataset.



### Learning to diagnose common thorax diseases on chest radiographs from radiology reports in Vietnamese
- **Arxiv ID**: http://arxiv.org/abs/2209.04794v1
- **DOI**: 10.1371/journal.pone.0276545
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.04794v1)
- **Published**: 2022-09-11 06:06:03+00:00
- **Updated**: 2022-09-11 06:06:03+00:00
- **Authors**: Thao T. B. Nguyen, Tam M. Vo, Thang V. Nguyen, Hieu H. Pham, Ha Q. Nguyen
- **Comment**: This work has been provisionally accepted for publication by Plos One
  journal
- **Journal**: None
- **Summary**: We propose a data collecting and annotation pipeline that extracts information from Vietnamese radiology reports to provide accurate labels for chest X-ray (CXR) images. This can benefit Vietnamese radiologists and clinicians by annotating data that closely match their endemic diagnosis categories which may vary from country to country. To assess the efficacy of the proposed labeling technique, we built a CXR dataset containing 9,752 studies and evaluated our pipeline using a subset of this dataset. With an F1-score of at least 0.9923, the evaluation demonstrates that our labeling tool performs precisely and consistently across all classes. After building the dataset, we train deep learning models that leverage knowledge transferred from large public CXR datasets. We employ a variety of loss functions to overcome the curse of imbalanced multi-label datasets and conduct experiments with various model architectures to select the one that delivers the best performance. Our best model (CheXpert-pretrained EfficientNet-B2) yields an F1-score of 0.6989 (95% CI 0.6740, 0.7240), AUC of 0.7912, sensitivity of 0.7064 and specificity of 0.8760 for the abnormal diagnosis in general. Finally, we demonstrate that our coarse classification (based on five specific locations of abnormalities) yields comparable results to fine classification (twelve pathologies) on the benchmark CheXpert dataset for general anomaly detection while delivering better performance in terms of the average performance of all classes.



### Multiple Object Tracking in Recent Times: A Literature Review
- **Arxiv ID**: http://arxiv.org/abs/2209.04796v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04796v1)
- **Published**: 2022-09-11 06:12:28+00:00
- **Updated**: 2022-09-11 06:12:28+00:00
- **Authors**: Mk Bashar, Samia Islam, Kashifa Kawaakib Hussain, Md. Bakhtiar Hasan, A. B. M. Ashikur Rahman, Md. Hasanul Kabir
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple object tracking gained a lot of interest from researchers in recent years, and it has become one of the trending problems in computer vision, especially with the recent advancement of autonomous driving. MOT is one of the critical vision tasks for different issues like occlusion in crowded scenes, similar appearance, small object detection difficulty, ID switching, etc. To tackle these challenges, as researchers tried to utilize the attention mechanism of transformer, interrelation of tracklets with graph convolutional neural network, appearance similarity of objects in different frames with the siamese network, they also tried simple IOU matching based CNN network, motion prediction with LSTM. To take these scattered techniques under an umbrella, we have studied more than a hundred papers published over the last three years and have tried to extract the techniques that are more focused on by researchers in recent times to solve the problems of MOT. We have enlisted numerous applications, possibilities, and how MOT can be related to real life. Our review has tried to show the different perspectives of techniques that researchers used overtimes and give some future direction for the potential researchers. Moreover, we have included popular benchmark datasets and metrics in this review.



### Pathfinding in Random Partially Observable Environments with Vision-Informed Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.04801v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.04801v1)
- **Published**: 2022-09-11 06:32:00+00:00
- **Updated**: 2022-09-11 06:32:00+00:00
- **Authors**: Anthony Dowling
- **Comment**: None
- **Journal**: None
- **Summary**: Deep reinforcement learning is a technique for solving problems in a variety of environments, ranging from Atari video games to stock trading. This method leverages deep neural network models to make decisions based on observations of a given environment with the goal of maximizing a reward function that can incorporate cost and rewards for reaching goals. With the aim of pathfinding, reward conditions can include reaching a specified target area along with costs for movement. In this work, multiple Deep Q-Network (DQN) agents are trained to operate in a partially observable environment with the goal of reaching a target zone in minimal travel time. The agent operates based on a visual representation of its surroundings, and thus has a restricted capability to observe the environment. A comparison between DQN, DQN-GRU, and DQN-LSTM is performed to examine each models capabilities with two different types of input. Through this evaluation, it is been shown that with equivalent training and analogous model architectures, a DQN model is able to outperform its recurrent counterparts.



### OAIR: Object-Aware Image Retargeting Using PSO and Aesthetic Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2209.04804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04804v1)
- **Published**: 2022-09-11 07:16:59+00:00
- **Updated**: 2022-09-11 07:16:59+00:00
- **Authors**: Mohammad Reza Naderi, Mohammad Hossein Givkashi, Nader Karimi, Shahram Shirani, Shadrokh Samavi
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: Image retargeting aims at altering an image size while preserving important content and minimizing noticeable distortions. However, previous image retargeting methods create outputs that suffer from artifacts and distortions. Besides, most previous works attempt to retarget the background and foreground of the input image simultaneously. Simultaneous resizing of the foreground and background causes changes in the aspect ratios of the objects. The change in the aspect ratio is specifically not desirable for human objects. We propose a retargeting method that overcomes these problems. The proposed approach consists of the following steps. Firstly, an inpainting method uses the input image and the binary mask of foreground objects to produce a background image without any foreground objects. Secondly, the seam carving method resizes the background image to the target size. Then, a super-resolution method increases the input image quality, and we then extract the foreground objects. Finally, the retargeted background and the extracted super-resolued objects are fed into a particle swarm optimization algorithm (PSO). The PSO algorithm uses aesthetic quality assessment as its objective function to identify the best location and size for the objects to be placed in the background. We used image quality assessment and aesthetic quality assessment measures to show our superior results compared to popular image retargeting techniques.



### Lexicon and Attention based Handwritten Text Recognition System
- **Arxiv ID**: http://arxiv.org/abs/2209.04817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04817v1)
- **Published**: 2022-09-11 09:26:45+00:00
- **Updated**: 2022-09-11 09:26:45+00:00
- **Authors**: Lalita Kumari, Sukhdeep Singh, VVS Rathore, Anuj Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: The handwritten text recognition problem is widely studied by the researchers of computer vision community due to its scope of improvement and applicability to daily lives, It is a sub-domain of pattern recognition. Due to advancement of computational power of computers since last few decades neural networks based systems heavily contributed towards providing the state-of-the-art handwritten text recognizers. In the same direction, we have taken two state-of-the art neural networks systems and merged the attention mechanism with it. The attention technique has been widely used in the domain of neural machine translations and automatic speech recognition and now is being implemented in text recognition domain. In this study, we are able to achieve 4.15% character error rate and 9.72% word error rate on IAM dataset, 7.07% character error rate and 16.14% word error rate on GW dataset after merging the attention and word beam search decoder with existing Flor et al. architecture. To analyse further, we have also used system similar to Shi et al. neural network system with greedy decoder and observed 23.27% improvement in character error rate from the base model.



### Local-Aware Global Attention Network for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/2209.04821v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04821v2)
- **Published**: 2022-09-11 09:43:42+00:00
- **Updated**: 2023-04-04 11:26:56+00:00
- **Authors**: Nathanael L. Baisa
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2108.02234
- **Journal**: None
- **Summary**: Learning representative, robust and discriminative information from images is essential for effective person re-identification (Re-Id). In this paper, we propose a compound approach for end-to-end discriminative deep feature learning for person Re-Id based on both body and hand images. We carefully design the Local-Aware Global Attention Network (LAGA-Net), a multi-branch deep network architecture consisting of one branch for spatial attention, one branch for channel attention, one branch for global feature representations and another branch for local feature representations. The attention branches focus on the relevant features of the image while suppressing the irrelevant backgrounds. In order to overcome the weakness of the attention mechanisms, equivariant to pixel shuffling, we integrate relative positional encodings into the spatial attention module to capture the spatial positions of pixels. The global branch intends to preserve the global context or structural information. For the the local branch, which intends to capture the fine-grained information, we perform uniform partitioning to generate stripes on the conv-layer horizontally. We retrieve the parts by conducting a soft partition without explicitly partitioning the images or requiring external cues such as pose estimation. A set of ablation study shows that each component contributes to the increased performance of the LAGA-Net. Extensive evaluations on four popular body-based person Re-Id benchmarks and two publicly available hand datasets demonstrate that our proposed method consistently outperforms existing state-of-the-art methods.



### Continual Learning for Pose-Agnostic Object Recognition in 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2209.04840v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04840v1)
- **Published**: 2022-09-11 11:31:39+00:00
- **Updated**: 2022-09-11 11:31:39+00:00
- **Authors**: Xihao Wang, Xian Wei
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Continual Learning aims to learn multiple incoming new tasks continually, and to keep the performance of learned tasks at a consistent level. However, existing research on continual learning assumes the pose of the object is pre-defined and well-aligned. For practical application, this work focuses on pose-agnostic continual learning tasks, where the object's pose changes dynamically and unpredictably. The point cloud augmentation adopted from past approaches would sharply rise with the task increment in the continual learning process. To address this problem, we inject the equivariance as the additional prior knowledge into the networks. We proposed a novel continual learning model that effectively distillates previous tasks' geometric equivariance information. The experiments show that our method overcomes the challenge of pose-agnostic scenarios in several mainstream point cloud datasets. We further conduct ablation studies to evaluate the validation of each component of our approach.



### Deep Lossy Plus Residual Coding for Lossless and Near-lossless Image Compression
- **Arxiv ID**: http://arxiv.org/abs/2209.04847v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.04847v1)
- **Published**: 2022-09-11 12:11:56+00:00
- **Updated**: 2022-09-11 12:11:56+00:00
- **Authors**: Yuanchao Bai, Xianming Liu, Kai Wang, Xiangyang Ji, Xiaolin Wu, Wen Gao
- **Comment**: arXiv admin note: text overlap with arXiv:2103.17015
- **Journal**: None
- **Summary**: Lossless and near-lossless image compression is of paramount importance to professional users in many technical fields, such as medicine, remote sensing, precision engineering and scientific research. But despite rapidly growing research interests in learning-based image compression, no published method offers both lossless and near-lossless modes. In this paper, we propose a unified and powerful deep lossy plus residual (DLPR) coding framework for both lossless and near-lossless image compression. In the lossless mode, the DLPR coding system first performs lossy compression and then lossless coding of residuals. We solve the joint lossy and residual compression problem in the approach of VAEs, and add autoregressive context modeling of the residuals to enhance lossless compression performance. In the near-lossless mode, we quantize the original residuals to satisfy a given $\ell_\infty$ error bound, and propose a scalable near-lossless compression scheme that works for variable $\ell_\infty$ bounds instead of training multiple networks. To expedite the DLPR coding, we increase the degree of algorithm parallelization by a novel design of coding context, and accelerate the entropy coding with adaptive residual interval. Experimental results demonstrate that the DLPR coding system achieves both the state-of-the-art lossless and near-lossless image compression performance with competitive coding speed.



### OpenMixup: Open Mixup Toolbox and Benchmark for Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.04851v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04851v1)
- **Published**: 2022-09-11 12:46:01+00:00
- **Updated**: 2022-09-11 12:46:01+00:00
- **Authors**: Siyuan Li, Zedong Wang, Zicheng Liu, Di Wu, Stan Z. Li
- **Comment**: Preprint of 12 pages. The source code is available at
  https://github.com/Westlake-AI/openmixup
- **Journal**: None
- **Summary**: With the remarkable progress of deep neural networks in computer vision, data mixing augmentation techniques are widely studied to alleviate problems of degraded generalization when the amount of training data is limited. However, mixup strategies have not been well assembled in current vision toolboxes. In this paper, we propose \texttt{OpenMixup}, an open-source all-in-one toolbox for supervised, semi-, and self-supervised visual representation learning with mixup. It offers an integrated model design and training platform, comprising a rich set of prevailing network architectures and modules, a collection of data mixing augmentation methods as well as practical model analysis tools. In addition, we also provide standard mixup image classification benchmarks on various datasets, which expedites practitioners to make fair comparisons among state-of-the-art methods under the same settings. The source code and user documents are available at \url{https://github.com/Westlake-AI/openmixup}.



### Inverse Image Frequency for Long-tailed Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.04861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04861v1)
- **Published**: 2022-09-11 13:31:43+00:00
- **Updated**: 2022-09-11 13:31:43+00:00
- **Authors**: Konstantinos Panagiotis Alexandridis, Shan Luo, Anh Nguyen, Jiankang Deng, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: The long-tailed distribution is a common phenomenon in the real world. Extracted large scale image datasets inevitably demonstrate the long-tailed property and models trained with imbalanced data can obtain high performance for the over-represented categories, but struggle for the under-represented categories, leading to biased predictions and performance degradation. To address this challenge, we propose a novel de-biasing method named Inverse Image Frequency (IIF). IIF is a multiplicative margin adjustment transformation of the logits in the classification layer of a convolutional neural network. Our method achieves stronger performance than similar works and it is especially useful for downstream tasks such as long-tailed instance segmentation as it produces fewer false positive detections. Our extensive experiments show that IIF surpasses the state of the art on many long-tailed benchmarks such as ImageNet-LT, CIFAR-LT, Places-LT and LVIS, reaching 55.8% top-1 accuracy with ResNet50 on ImageNet-LT and 26.2% segmentation AP with MaskRCNN on LVIS. Code available at https://github.com/kostas1515/iif



### Instruction-driven history-aware policies for robotic manipulations
- **Arxiv ID**: http://arxiv.org/abs/2209.04899v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.04899v3)
- **Published**: 2022-09-11 16:28:25+00:00
- **Updated**: 2022-12-17 18:12:32+00:00
- **Authors**: Pierre-Louis Guhur, Shizhe Chen, Ricardo Garcia, Makarand Tapaswi, Ivan Laptev, Cordelia Schmid
- **Comment**: Accepted in CoRL 2022 (oral); project page at
  https://guhur.github.io/hiveformer/
- **Journal**: None
- **Summary**: In human environments, robots are expected to accomplish a variety of manipulation tasks given simple natural language instructions. Yet, robotic manipulation is extremely challenging as it requires fine-grained motor control, long-term memory as well as generalization to previously unseen tasks and environments. To address these challenges, we propose a unified transformer-based approach that takes into account multiple inputs. In particular, our transformer architecture integrates (i) natural language instructions and (ii) multi-view scene observations while (iii) keeping track of the full history of observations and actions. Such an approach enables learning dependencies between history and instructions and improves manipulation precision using multiple views. We evaluate our method on the challenging RLBench benchmark and on a real-world robot. Notably, our approach scales to 74 diverse RLBench tasks and outperforms the state of the art. We also address instruction-conditioned tasks and demonstrate excellent generalization to previously unseen variations.



### Automatic Detection of Sentimentality from Facial Expressions
- **Arxiv ID**: http://arxiv.org/abs/2209.04908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04908v1)
- **Published**: 2022-09-11 17:36:41+00:00
- **Updated**: 2022-09-11 17:36:41+00:00
- **Authors**: Mina Bishay, Jay Turcot, Graham Page, Mohammad Mavadati
- **Comment**: Accepted in ICIP 2022
- **Journal**: None
- **Summary**: Emotion recognition has received considerable attention from the Computer Vision community in the last 20 years. However, most of the research focused on analyzing the six basic emotions (e.g. joy, anger, surprise), with a limited work directed to other affective states. In this paper, we tackle sentimentality (strong feeling of heartwarming or nostalgia), a new emotional state that has few works in the literature, and no guideline defining its facial markers. To this end, we first collect a dataset of 4.9K videos of participants watching some sentimental and non-sentimental ads, and then we label the moments evoking sentimentality in the ads. Second, we use the ad-level labels and the facial Action Units (AUs) activation across different frames for defining some weak frame-level sentimentality labels. Third, we train a Multilayer Perceptron (MLP) using the AUs activation for sentimentality detection. Finally, we define two new ad-level metrics for evaluating our model performance. Quantitative and qualitative results show promising results for sentimentality detection. To the best of our knowledge this is the first work to address the problem of sentimentality detection.



### Diversity and Novelty MasterPrints: Generating Multiple DeepMasterPrints for Increased User Coverage
- **Arxiv ID**: http://arxiv.org/abs/2209.04909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04909v1)
- **Published**: 2022-09-11 17:38:27+00:00
- **Updated**: 2022-09-11 17:38:27+00:00
- **Authors**: M Charity, Nasir Memon, Zehua Jiang, Abhi Sen, Julian Togelius
- **Comment**: None
- **Journal**: None
- **Summary**: This work expands on previous advancements in genetic fingerprint spoofing via the DeepMasterPrints and introduces Diversity and Novelty MasterPrints. This system uses quality diversity evolutionary algorithms to generate dictionaries of artificial prints with a focus on increasing coverage of users from the dataset. The Diversity MasterPrints focus on generating solution prints that match with users not covered by previously found prints, and the Novelty MasterPrints explicitly search for prints with more that are farther in user space than previous prints. Our multi-print search methodologies outperform the singular DeepMasterPrints in both coverage and generalization while maintaining quality of the fingerprint image output.



### Vec2Face-v2: Unveil Human Faces from their Blackbox Features via Attention-based Network in Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.04920v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04920v1)
- **Published**: 2022-09-11 19:14:21+00:00
- **Updated**: 2022-09-11 19:14:21+00:00
- **Authors**: Thanh-Dat Truong, Chi Nhan Duong, Ngan Le, Marios Savvides, Khoa Luu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2003.06958
- **Journal**: None
- **Summary**: In this work, we investigate the problem of face reconstruction given a facial feature representation extracted from a blackbox face recognition engine. Indeed, it is very challenging problem in practice due to the limitations of abstracted information from the engine. We therefore introduce a new method named Attention-based Bijective Generative Adversarial Networks in a Distillation framework (DAB-GAN) to synthesize faces of a subject given his/her extracted face recognition features. Given any unconstrained unseen facial features of a subject, the DAB-GAN can reconstruct his/her faces in high definition. The DAB-GAN method includes a novel attention-based generative structure with the new defined Bijective Metrics Learning approach. The framework starts by introducing a bijective metric so that the distance measurement and metric learning process can be directly adopted in image domain for an image reconstruction task. The information from the blackbox face recognition engine will be optimally exploited using the global distillation process. Then an attention-based generator is presented for a highly robust generator to synthesize realistic faces with ID preservation. We have evaluated our method on the challenging face recognition databases, i.e. CelebA, LFW, AgeDB, CFP-FP, and consistently achieved the state-of-the-art results. The advancement of DAB-GAN is also proven on both image realism and ID preservation properties.



### Synthetic Wavelength Imaging -- Utilizing Spectral Correlations for High-Precision Time-of-Flight Sensing
- **Arxiv ID**: http://arxiv.org/abs/2209.04941v1
- **DOI**: None
- **Categories**: **physics.optics**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.04941v1)
- **Published**: 2022-09-11 21:39:59+00:00
- **Updated**: 2022-09-11 21:39:59+00:00
- **Authors**: Florian Willomitzer
- **Comment**: None
- **Journal**: None
- **Summary**: This book chapter describes how spectral correlations in scattered light fields can be utilized for high-precision time-of-flight sensing. The chapter should serve as a gentle introduction and is intended for computational imaging scientists and students new to the fascinating topic of synthetic wavelength imaging. Technical details (such as detector or light source specifications) will be largely omitted. Instead, the similarities between different methods will be emphasized to "draw the bigger picture."



### Learning When to Say "I Don't Know"
- **Arxiv ID**: http://arxiv.org/abs/2209.04944v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.04944v2)
- **Published**: 2022-09-11 21:50:03+00:00
- **Updated**: 2023-02-15 16:30:29+00:00
- **Authors**: Nicholas Kashani Motlagh, Jim Davis, Tim Anderson, Jeremy Gwinnup
- **Comment**: International Symposium on Visual Computing, October 2022
- **Journal**: None
- **Summary**: We propose a new Reject Option Classification technique to identify and remove regions of uncertainty in the decision space for a given neural classifier and dataset. Such existing formulations employ a learned rejection (remove)/selection (keep) function and require either a known cost for rejecting examples or strong constraints on the accuracy or coverage of the selected examples. We consider an alternative formulation by instead analyzing the complementary reject region and employing a validation set to learn per-class softmax thresholds. The goal is to maximize the accuracy of the selected examples subject to a natural randomness allowance on the rejected examples (rejecting more incorrect than correct predictions). We provide results showing the benefits of the proposed method over na\"ively thresholding calibrated/uncalibrated softmax scores with 2-D points, imagery, and text classification datasets using state-of-the-art pretrained models. Source code is available at https://github.com/osu-cvl/learning-idk.



### Unsupervised Learning of 3D Scene Flow with 3D Odometry Assistance
- **Arxiv ID**: http://arxiv.org/abs/2209.04945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.04945v1)
- **Published**: 2022-09-11 21:53:43+00:00
- **Updated**: 2022-09-11 21:53:43+00:00
- **Authors**: Guangming Wang, Zhiheng Feng, Chaokang Jiang, Hesheng Wang
- **Comment**: 12 pages, 9 figures, under review
- **Journal**: None
- **Summary**: Scene flow represents the 3D motion of each point in the scene, which explicitly describes the distance and the direction of each point's movement. Scene flow estimation is used in various applications such as autonomous driving fields, activity recognition, and virtual reality fields. As it is challenging to annotate scene flow with ground truth for real-world data, this leaves no real-world dataset available to provide a large amount of data with ground truth for scene flow estimation. Therefore, many works use synthesized data to pre-train their network and real-world LiDAR data to finetune. Unlike the previous unsupervised learning of scene flow in point clouds, we propose to use odometry information to assist the unsupervised learning of scene flow and use real-world LiDAR data to train our network. Supervised odometry provides more accurate shared cost volume for scene flow. In addition, the proposed network has mask-weighted warp layers to get a more accurate predicted point cloud. The warp operation means applying an estimated pose transformation or scene flow to a source point cloud to obtain a predicted point cloud and is the key to refining scene flow from coarse to fine. When performing warp operations, the points in different states use different weights for the pose transformation and scene flow transformation. We classify the states of points as static, dynamic, and occluded, where the static masks are used to divide static and dynamic points, and the occlusion masks are used to divide occluded points. The mask-weighted warp layer indicates that static masks and occlusion masks are used as weights when performing warp operations. Our designs are proved to be effective in ablation experiments. The experiment results show the promising prospect of an odometry-assisted unsupervised learning method for 3D scene flow in real-world data.



