# Arxiv Papers in cs.CV on 2022-09-30
### Embedded System Performance Analysis for Implementing a Portable Drowsiness Detection System for Drivers
- **Arxiv ID**: http://arxiv.org/abs/2209.15148v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.15148v4)
- **Published**: 2022-09-30 00:22:57+00:00
- **Updated**: 2022-12-27 01:39:48+00:00
- **Authors**: Minjeong Kim, Jimin Koo
- **Comment**: 26 pages, 13 figures, 4 tables
- **Journal**: None
- **Summary**: Drowsiness on the road is a widespread problem with fatal consequences; thus, a multitude of systems and techniques have been proposed. Among existing methods, Ghoddoosian et al. utilized temporal blinking patterns to detect early signs of drowsiness, but their algorithm was tested only on a powerful desktop computer, which is not practical to apply in a moving vehicle setting. In this paper, we propose an efficient platform to run Ghoddosian's algorithm, detail the performance tests we ran to determine this platform, and explain our threshold optimization logic. After considering the Jetson Nano and Beelink (Mini PC), we concluded that the Mini PC is the most efficient and practical to run our embedded system in a vehicle. To determine this, we ran communication speed tests and evaluated total processing times for inference operations. Based on our experiments, the average total processing time to run the drowsiness detection model was 94.27 ms for Jetson Nano and 22.73 ms for the Beelink (Mini PC). Considering the portability and power efficiency of each device, along with the processing time results, the Beelink (Mini PC) was determined to be most suitable. Also, we propose a threshold optimization algorithm, which determines whether the driver is drowsy or alert based on the trade-off between the sensitivity and specificity of the drowsiness detection model. Our study will serve as a crucial next step for drowsiness detection research and its application in vehicles. Through our experiment, we have determinend a favorable platform that can run drowsiness detection algorithms in real-time and can be used as a foundation to further advance drowsiness detection research. In doing so, we have bridged the gap between an existing embedded system and its actual implementation in vehicles to bring drowsiness technology a step closer to prevalent real-life implementation.



### MonoNeuralFusion: Online Monocular Neural 3D Reconstruction with Geometric Priors
- **Arxiv ID**: http://arxiv.org/abs/2209.15153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15153v1)
- **Published**: 2022-09-30 00:44:26+00:00
- **Updated**: 2022-09-30 00:44:26+00:00
- **Authors**: Zi-Xin Zou, Shi-Sheng Huang, Yan-Pei Cao, Tai-Jiang Mu, Ying Shan, Hongbo Fu
- **Comment**: 12 pages, 12 figures
- **Journal**: None
- **Summary**: High-fidelity 3D scene reconstruction from monocular videos continues to be challenging, especially for complete and fine-grained geometry reconstruction. The previous 3D reconstruction approaches with neural implicit representations have shown a promising ability for complete scene reconstruction, while their results are often over-smooth and lack enough geometric details. This paper introduces a novel neural implicit scene representation with volume rendering for high-fidelity online 3D scene reconstruction from monocular videos. For fine-grained reconstruction, our key insight is to incorporate geometric priors into both the neural implicit scene representation and neural volume rendering, thus leading to an effective geometry learning mechanism based on volume rendering optimization. Benefiting from this, we present MonoNeuralFusion to perform the online neural 3D reconstruction from monocular videos, by which the 3D scene geometry is efficiently generated and optimized during the on-the-fly 3D monocular scanning. The extensive comparisons with state-of-the-art approaches show that our MonoNeuralFusion consistently generates much better complete and fine-grained reconstruction results, both quantitatively and qualitatively.



### MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features
- **Arxiv ID**: http://arxiv.org/abs/2209.15159v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.15159v2)
- **Published**: 2022-09-30 01:04:10+00:00
- **Updated**: 2022-10-06 14:19:13+00:00
- **Authors**: Shakti N. Wadekar, Abhishek Chaurasia
- **Comment**: 20 pages, 7 figures
- **Journal**: None
- **Summary**: MobileViT (MobileViTv1) combines convolutional neural networks (CNNs) and vision transformers (ViTs) to create light-weight models for mobile vision tasks. Though the main MobileViTv1-block helps to achieve competitive state-of-the-art results, the fusion block inside MobileViTv1-block, creates scaling challenges and has a complex learning task. We propose changes to the fusion block that are simple and effective to create MobileViTv3-block, which addresses the scaling and simplifies the learning task. Our proposed MobileViTv3-block used to create MobileViTv3-XXS, XS and S models outperform MobileViTv1 on ImageNet-1k, ADE20K, COCO and PascalVOC2012 datasets. On ImageNet-1K, MobileViTv3-XXS and MobileViTv3-XS surpasses MobileViTv1-XXS and MobileViTv1-XS by 2% and 1.9% respectively. Recently published MobileViTv2 architecture removes fusion block and uses linear complexity transformers to perform better than MobileViTv1. We add our proposed fusion block to MobileViTv2 to create MobileViTv3-0.5, 0.75 and 1.0 models. These new models give better accuracy numbers on ImageNet-1k, ADE20K, COCO and PascalVOC2012 datasets as compared to MobileViTv2. MobileViTv3-0.5 and MobileViTv3-0.75 outperforms MobileViTv2-0.5 and MobileViTv2-0.75 by 2.1% and 1.0% respectively on ImageNet-1K dataset. For segmentation task, MobileViTv3-1.0 achieves 2.07% and 1.1% better mIOU compared to MobileViTv2-1.0 on ADE20K dataset and PascalVOC2012 dataset respectively. Our code and the trained models are available at: https://github.com/micronDLA/MobileViTv3



### Distilling Style from Image Pairs for Global Forward and Inverse Tone Mapping
- **Arxiv ID**: http://arxiv.org/abs/2209.15165v2
- **DOI**: 10.1145/3565516.3565520
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15165v2)
- **Published**: 2022-09-30 01:26:16+00:00
- **Updated**: 2022-10-04 16:10:28+00:00
- **Authors**: Aamir Mustafa, Param Hanji, Rafal K. Mantiuk
- **Comment**: Published in European Conference on Visual Media Production (CVMP
  '22)
- **Journal**: None
- **Summary**: Many image enhancement or editing operations, such as forward and inverse tone mapping or color grading, do not have a unique solution, but instead a range of solutions, each representing a different style. Despite this, existing learning-based methods attempt to learn a unique mapping, disregarding this style. In this work, we show that information about the style can be distilled from collections of image pairs and encoded into a 2- or 3-dimensional vector. This gives us not only an efficient representation but also an interpretable latent space for editing the image style. We represent the global color mapping between a pair of images as a custom normalizing flow, conditioned on a polynomial basis of the pixel color. We show that such a network is more effective than PCA or VAE at encoding image style in low-dimensional space and lets us obtain an accuracy close to 40 dB, which is about 7-10 dB improvement over the state-of-the-art methods.



### Understanding Pure CLIP Guidance for Voxel Grid NeRF Models
- **Arxiv ID**: http://arxiv.org/abs/2209.15172v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.15172v1)
- **Published**: 2022-09-30 01:47:47+00:00
- **Updated**: 2022-09-30 01:47:47+00:00
- **Authors**: Han-Hung Lee, Angel X. Chang
- **Comment**: None
- **Journal**: None
- **Summary**: We explore the task of text to 3D object generation using CLIP. Specifically, we use CLIP for guidance without access to any datasets, a setting we refer to as pure CLIP guidance. While prior work has adopted this setting, there is no systematic study of mechanics for preventing adversarial generations within CLIP. We illustrate how different image-based augmentations prevent the adversarial generation problem, and how the generated results are impacted. We test different CLIP model architectures and show that ensembling different models for guidance can prevent adversarial generations within bigger models and generate sharper results. Furthermore, we implement an implicit voxel grid model to show how neural networks provide an additional layer of regularization, resulting in better geometrical structure and coherency of generated objects. Compared to prior work, we achieve more coherent results with higher memory efficiency and faster training speeds.



### Physical Adversarial Attack meets Computer Vision: A Decade Survey
- **Arxiv ID**: http://arxiv.org/abs/2209.15179v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15179v2)
- **Published**: 2022-09-30 01:59:53+00:00
- **Updated**: 2022-11-05 13:44:24+00:00
- **Authors**: Hui Wei, Hao Tang, Xuemei Jia, Hanxun Yu, Zhubo Li, Zhixiang Wang, Shin'ichi Satoh, Zheng Wang
- **Comment**: 32 pages. Under Review
- **Journal**: None
- **Summary**: Although Deep Neural Networks (DNNs) have achieved impressive results in computer vision, their exposed vulnerability to adversarial attacks remains a serious concern. A series of works has shown that by adding elaborate perturbations to images, DNNs could have catastrophic degradation in performance metrics. And this phenomenon does not only exist in the digital space but also in the physical space. Therefore, estimating the security of these DNNs-based systems is critical for safely deploying them in the real world, especially for security-critical applications, e.g., autonomous cars, video surveillance, and medical diagnosis. In this paper, we focus on physical adversarial attacks and provide a comprehensive survey of over 150 existing papers. We first clarify the concept of the physical adversarial attack and analyze its characteristics. Then, we define the adversarial medium, essential to perform attacks in the physical world. Next, we present the physical adversarial attack methods in task order: classification, detection, and re-identification, and introduce their performance in solving the trilemma: effectiveness, stealthiness, and robustness. In the end, we discuss the current challenges and potential future directions.



### SCI: A Spectrum Concentrated Implicit Neural Compression for Biomedical Data
- **Arxiv ID**: http://arxiv.org/abs/2209.15180v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, I.4.2; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2209.15180v5)
- **Published**: 2022-09-30 02:05:39+00:00
- **Updated**: 2022-11-23 14:39:38+00:00
- **Authors**: Runzhao Yang, Tingxiong Xiao, Yuxiao Cheng, Qianni Cao, Jinyuan Qu, Jinli Suo, Qionghai Dai
- **Comment**: accepted to AAAI2023
- **Journal**: None
- **Summary**: Massive collection and explosive growth of biomedical data, demands effective compression for efficient storage, transmission and sharing. Readily available visual data compression techniques have been studied extensively but tailored for natural images/videos, and thus show limited performance on biomedical data which are of different features and larger diversity. Emerging implicit neural representation (INR) is gaining momentum and demonstrates high promise for fitting diverse visual data in target-data-specific manner, but a general compression scheme covering diverse biomedical data is so far absent. To address this issue, we firstly derive a mathematical explanation for INR's spectrum concentration property and an analytical insight on the design of INR based compressor. Further, we propose a Spectrum Concentrated Implicit neural compression (SCI) which adaptively partitions the complex biomedical data into blocks matching INR's concentrated spectrum envelop, and design a funnel shaped neural network capable of representing each block with a small number of parameters. Based on this design, we conduct compression via optimization under given budget and allocate the available parameters with high representation accuracy. The experiments show SCI's superior performance to state-of-the-art methods including commercial compressors, data-driven ones, and INR based counterparts on diverse biomedical data. The source code can be found at https://github.com/RichealYoung/ImplicitNeuralCompression.git.



### An Interactive Interpretability System for Breast Cancer Screening with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2210.08979v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.08979v1)
- **Published**: 2022-09-30 02:19:49+00:00
- **Updated**: 2022-09-30 02:19:49+00:00
- **Authors**: Yuzhe Lu, Adam Perer
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods, in particular convolutional neural networks, have emerged as a powerful tool in medical image computing tasks. While these complex models provide excellent performance, their black-box nature may hinder real-world adoption in high-stakes decision-making. In this paper, we propose an interactive system to take advantage of state-of-the-art interpretability techniques to assist radiologists with breast cancer screening. Our system integrates a deep learning model into the radiologists' workflow and provides novel interactions to promote understanding of the model's decision-making process. Moreover, we demonstrate that our system can take advantage of user interactions progressively to provide finer-grained explainability reports with little labeling overhead. Due to the generic nature of the adopted interpretability technique, our system is domain-agnostic and can be used for many different medical image computing tasks, presenting a novel perspective on how we can leverage visual analytics to transform originally static interpretability techniques to augment human decision making and promote the adoption of medical AI.



### Self-Distillation for Further Pre-training of Transformers
- **Arxiv ID**: http://arxiv.org/abs/2210.02871v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.02871v3)
- **Published**: 2022-09-30 02:25:12+00:00
- **Updated**: 2023-06-09 08:57:07+00:00
- **Authors**: Seanie Lee, Minki Kang, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi
- **Comment**: ICLR 2023
- **Journal**: None
- **Summary**: Pre-training a large transformer model on a massive amount of unlabeled data and fine-tuning it on labeled datasets for diverse downstream tasks has proven to be a successful strategy, for a variety of vision and natural language processing tasks. However, direct fine-tuning of the pre-trained model may be suboptimal if there exist large discrepancies across data domains for pre-training and fine-tuning. To tackle this issue, several previous studies have proposed further pre-training strategies, where we continue to pre-train the model on the target unlabeled dataset before fine-tuning. However, all of them solely focus on language models and we empirically find that a Vision Transformer is vulnerable to overfitting as we continue to pretrain the model on target unlabeled data. In order to tackle this limitation, we propose self-distillation as a regularization for a further pre-training stage. Specifically, we first further pre-train the initial pre-trained model on the target unlabeled data and then consider it as a teacher for self-distillation. Then we take the same initial pre-trained model as a student and enforce its hidden representations to be close to those of the teacher while optimizing the student with a masked auto-encoding objective. We empirically validate the efficacy of self-distillation on a variety of benchmark datasets for image and text classification tasks. Experimentally, we show that our proposed method outperforms all the relevant baselines. Theoretically, we analyze the proposed method with a simplified model to understand how self-distillation for further pre-training can potentially help improve the performance of the downstream tasks.



### Multi-Prompt Alignment for Multi-Source Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2209.15210v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15210v3)
- **Published**: 2022-09-30 03:40:10+00:00
- **Updated**: 2023-05-17 17:27:45+00:00
- **Authors**: Haoran Chen, Zuxuan Wu, Xintong Han, Yu-Gang Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Most existing methods for unsupervised domain adaptation (UDA) rely on a shared network to extract domain-invariant features. However, when facing multiple source domains, optimizing such a network involves updating the parameters of the entire network, making it both computationally expensive and challenging, particularly when coupled with min-max objectives. Inspired by recent advances in prompt learning that adapts high-capacity models for downstream tasks in a computationally economic way, we introduce Multi-Prompt Alignment (MPA), a simple yet efficient framework for multi-source UDA. Given a source and target domain pair, MPA first trains an individual prompt to minimize the domain gap through a contrastive loss. Then, MPA denoises the learned prompts through an auto-encoding process and aligns them by maximizing the agreement of all the reconstructed prompts. Moreover, we show that the resulting subspace acquired from the auto-encoding process can easily generalize to a streamlined set of target domains, making our method more efficient for practical usage. Extensive experiments show that MPA achieves state-of-the-art results on three popular datasets with an impressive average accuracy of 54.1% on DomainNet.



### Dual Progressive Transformations for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.15211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15211v1)
- **Published**: 2022-09-30 03:42:52+00:00
- **Updated**: 2022-09-30 03:42:52+00:00
- **Authors**: Dongjian Huo, Yukun Su, Qingyao Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly supervised semantic segmentation (WSSS), which aims to mine the object regions by merely using class-level labels, is a challenging task in computer vision. The current state-of-the-art CNN-based methods usually adopt Class-Activation-Maps (CAMs) to highlight the potential areas of the object, however, they may suffer from the part-activated issues. To this end, we try an early attempt to explore the global feature attention mechanism of vision transformer in WSSS task. However, since the transformer lacks the inductive bias as in CNN models, it can not boost the performance directly and may yield the over-activated problems. To tackle these drawbacks, we propose a Convolutional Neural Networks Refined Transformer (CRT) to mine a globally complete and locally accurate class activation maps in this paper. To validate the effectiveness of our proposed method, extensive experiments are conducted on PASCAL VOC 2012 and CUB-200-2011 datasets. Experimental evaluations show that our proposed CRT achieves the new state-of-the-art performance on both the weakly supervised semantic segmentation task the weakly supervised object localization task, which outperform others by a large margin.



### INT: Towards Infinite-frames 3D Detection with An Efficient Framework
- **Arxiv ID**: http://arxiv.org/abs/2209.15215v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15215v2)
- **Published**: 2022-09-30 04:03:40+00:00
- **Updated**: 2023-02-13 08:43:11+00:00
- **Authors**: Jianyun Xu, Zhenwei Miao, Da Zhang, Hongyu Pan, Kaixuan Liu, Peihan Hao, Jun Zhu, Zhengyang Sun, Hongmin Li, Xin Zhan
- **Comment**: accepted by ECCV2022
- **Journal**: None
- **Summary**: It is natural to construct a multi-frame instead of a single-frame 3D detector for a continuous-time stream. Although increasing the number of frames might improve performance, previous multi-frame studies only used very limited frames to build their systems due to the dramatically increased computational and memory cost. To address these issues, we propose a novel on-stream training and prediction framework that, in theory, can employ an infinite number of frames while keeping the same amount of computation as a single-frame detector. This infinite framework (INT), which can be used with most existing detectors, is utilized, for example, on the popular CenterPoint, with significant latency reductions and performance improvements. We've also conducted extensive experiments on two large-scale datasets, nuScenes and Waymo Open Dataset, to demonstrate the scheme's effectiveness and efficiency. By employing INT on CenterPoint, we can get around 7% (Waymo) and 15% (nuScenes) performance boost with only 2~4ms latency overhead, and currently SOTA on the Waymo 3D Detection leaderboard.



### Your Out-of-Distribution Detection Method is Not Robust!
- **Arxiv ID**: http://arxiv.org/abs/2209.15246v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.15246v1)
- **Published**: 2022-09-30 05:49:00+00:00
- **Updated**: 2022-09-30 05:49:00+00:00
- **Authors**: Mohammad Azizmalayeri, Arshia Soltani Moakhar, Arman Zarei, Reihaneh Zohrabi, Mohammad Taghi Manzuri, Mohammad Hossein Rohban
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: Out-of-distribution (OOD) detection has recently gained substantial attention due to the importance of identifying out-of-domain samples in reliability and safety. Although OOD detection methods have advanced by a great deal, they are still susceptible to adversarial examples, which is a violation of their purpose. To mitigate this issue, several defenses have recently been proposed. Nevertheless, these efforts remained ineffective, as their evaluations are based on either small perturbation sizes, or weak attacks. In this work, we re-examine these defenses against an end-to-end PGD attack on in/out data with larger perturbation sizes, e.g. up to commonly used $\epsilon=8/255$ for the CIFAR-10 dataset. Surprisingly, almost all of these defenses perform worse than a random detection under the adversarial setting. Next, we aim to provide a robust OOD detection method. In an ideal defense, the training should expose the model to almost all possible adversarial perturbations, which can be achieved through adversarial training. That is, such training perturbations should based on both in- and out-of-distribution samples. Therefore, unlike OOD detection in the standard setting, access to OOD, as well as in-distribution, samples sounds necessary in the adversarial training setup. These tips lead us to adopt generative OOD detection methods, such as OpenGAN, as a baseline. We subsequently propose the Adversarially Trained Discriminator (ATD), which utilizes a pre-trained robust model to extract robust features, and a generator model to create OOD samples. Using ATD with CIFAR-10 and CIFAR-100 as the in-distribution data, we could significantly outperform all previous methods in the robust AUROC while maintaining high standard AUROC and classification accuracy. The code repository is available at https://github.com/rohban-lab/ATD .



### Hyperspectral and LiDAR data for the prediction via machine learning of tree species, volume and biomass: a possible contribution for updating forest management plans
- **Arxiv ID**: http://arxiv.org/abs/2209.15248v1
- **DOI**: 10.1007/978-3-031-17439-1_17
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15248v1)
- **Published**: 2022-09-30 06:06:25+00:00
- **Updated**: 2022-09-30 06:06:25+00:00
- **Authors**: Daniele Michelini, Michele Dalponte, Angelo Carriero, Erico Kutchart, Salvatore Eugenio Pappalardo, Massimo De Marchi, Francesco Pirotti
- **Comment**: None
- **Journal**: None
- **Summary**: This work intends to lay the foundations for identifying the prevailing forest types and the delineation of forest units within private forest inventories in the Autonomous Province of Trento (PAT), using currently available remote sensing solutions. In particular, data from LiDAR and hyperspectral surveys of 2014 made available by PAT were acquired and processed. Such studies are very important in the context of forest management scenarios. The method includes defining tree species ground-truth by outlining single tree crowns with polygons and labeling them. Successively two supervised machine learning classifiers, K-Nearest Neighborhood and Support Vector Machine (SVM) were used. The results show that, by setting specific hyperparameters, the SVM methodology gave the best results in classification of tree species. Biomass was estimated using canopy parameters and the Jucker equation for the above ground biomass (AGB) and that of Scrinzi for the tariff volume. Predicted values were compared with 11 field plots of fixed radius where volume and biomass were field-estimated in 2017. Results show significant coefficients of correlation: 0.94 for stem volume and 0.90 for total aboveground tree biomass.



### Traffic Sign Classification Using Deep and Quantum Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.15251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15251v1)
- **Published**: 2022-09-30 06:16:03+00:00
- **Updated**: 2022-09-30 06:16:03+00:00
- **Authors**: Sylwia Kuros, Tomasz Kryjak
- **Comment**: Accepted for the ICCVG 2022 conference
- **Journal**: None
- **Summary**: Quantum Neural Networks (QNNs) are an emerging technology that can be used in many applications including computer vision. In this paper, we presented a traffic sign classification system implemented using a hybrid quantum-classical convolutional neural network. Experiments on the German Traffic Sign Recognition Benchmark dataset indicate that currently QNN do not outperform classical DCNN (Deep Convolutuional Neural Networks), yet still provide an accuracy of over 90% and are a definitely promising solution for advanced computer vision.



### PointPillars Backbone Type Selection For Fast and Accurate LiDAR Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.15252v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.15252v1)
- **Published**: 2022-09-30 06:18:14+00:00
- **Updated**: 2022-09-30 06:18:14+00:00
- **Authors**: Konrad Lis, Tomasz Kryjak
- **Comment**: Accepted for the ICCVG 2022 conference
- **Journal**: None
- **Summary**: 3D object detection from LiDAR sensor data is an important topic in the context of autonomous cars and drones. In this paper, we present the results of experiments on the impact of backbone selection of a deep convolutional neural network on detection accuracy and computation speed. We chose the PointPillars network, which is characterised by a simple architecture, high speed, and modularity that allows for easy expansion. During the experiments, we paid particular attention to the change in detection efficiency (measured by the mAP metric) and the total number of multiply-addition operations needed to process one point cloud. We tested 10 different convolutional neural network architectures that are widely used in image-based detection problems. For a backbone like MobilenetV1, we obtained an almost 4x speedup at the cost of a 1.13% decrease in mAP. On the other hand, for CSPDarknet we got an acceleration of more than 1.5x at an increase in mAP of 0.33%. We have thus demonstrated that it is possible to significantly speed up a 3D object detector in LiDAR point clouds with a small decrease in detection efficiency. This result can be used when PointPillars or similar algorithms are implemented in embedded systems, including SoC FPGAs. The code is available at https://github.com/vision-agh/pointpillars\_backbone.



### S2P: State-conditioned Image Synthesis for Data Augmentation in Offline Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.15256v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.15256v1)
- **Published**: 2022-09-30 06:32:26+00:00
- **Updated**: 2022-09-30 06:32:26+00:00
- **Authors**: Daesol Cho, Dongseok Shim, H. Jin Kim
- **Comment**: NeurIPS 2022, first two authors contributed equally
- **Journal**: None
- **Summary**: Offline reinforcement learning (Offline RL) suffers from the innate distributional shift as it cannot interact with the physical environment during training. To alleviate such limitation, state-based offline RL leverages a learned dynamics model from the logged experience and augments the predicted state transition to extend the data distribution. For exploiting such benefit also on the image-based RL, we firstly propose a generative model, S2P (State2Pixel), which synthesizes the raw pixel of the agent from its corresponding state. It enables bridging the gap between the state and the image domain in RL algorithms, and virtually exploring unseen image distribution via model-based transition in the state space. Through experiments, we confirm that our S2P-based image synthesis not only improves the image-based offline RL performance but also shows powerful generalization capability on unseen tasks.



### Mind Reader: Reconstructing complex images from brain activities
- **Arxiv ID**: http://arxiv.org/abs/2210.01769v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV, cs.HC, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2210.01769v1)
- **Published**: 2022-09-30 06:32:46+00:00
- **Updated**: 2022-09-30 06:32:46+00:00
- **Authors**: Sikun Lin, Thomas Sprague, Ambuj K Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding how the brain encodes external stimuli and how these stimuli can be decoded from the measured brain activities are long-standing and challenging questions in neuroscience. In this paper, we focus on reconstructing the complex image stimuli from fMRI (functional magnetic resonance imaging) signals. Unlike previous works that reconstruct images with single objects or simple shapes, our work aims to reconstruct image stimuli that are rich in semantics, closer to everyday scenes, and can reveal more perspectives. However, data scarcity of fMRI datasets is the main obstacle to applying state-of-the-art deep learning models to this problem. We find that incorporating an additional text modality is beneficial for the reconstruction problem compared to directly translating brain signals to images. Therefore, the modalities involved in our method are: (i) voxel-level fMRI signals, (ii) observed images that trigger the brain signals, and (iii) textual description of the images. To further address data scarcity, we leverage an aligned vision-language latent space pre-trained on massive datasets. Instead of training models from scratch to find a latent space shared by the three modalities, we encode fMRI signals into this pre-aligned latent space. Then, conditioned on embeddings in this space, we reconstruct images with a generative model. The reconstructed images from our pipeline balance both naturalness and fidelity: they are photo-realistic and capture the ground truth image contents well.



### Energy Efficient Hardware Acceleration of Neural Networks with Power-of-Two Quantisation
- **Arxiv ID**: http://arxiv.org/abs/2209.15257v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.15257v1)
- **Published**: 2022-09-30 06:33:40+00:00
- **Updated**: 2022-09-30 06:33:40+00:00
- **Authors**: Dominika Przewlocka-Rus, Tomasz Kryjak
- **Comment**: Accepted for the ICCVG 2022 conference
- **Journal**: None
- **Summary**: Deep neural networks virtually dominate the domain of most modern vision systems, providing high performance at a cost of increased computational complexity.Since for those systems it is often required to operate both in real-time and with minimal energy consumption (e.g., for wearable devices or autonomous vehicles, edge Internet of Things (IoT), sensor networks), various network optimisation techniques are used, e.g., quantisation, pruning, or dedicated lightweight architectures. Due to the logarithmic distribution of weights in neural network layers, a method providing high performance with significant reduction in computational precision (for 4-bit weights and less) is the Power-of-Two (PoT) quantisation (and therefore also with a logarithmic distribution). This method introduces additional possibilities of replacing the typical for neural networks Multiply and ACcumulate (MAC -- performing, e.g., convolution operations) units, with more energy-efficient Bitshift and ACcumulate (BAC). In this paper, we show that a hardware neural network accelerator with PoT weights implemented on the Zynq UltraScale + MPSoC ZCU104 SoC FPGA can be at least $1.4x$ more energy efficient than the uniform quantisation version. To further reduce the actual power requirement by omitting part of the computation for zero weights, we also propose a new pruning method adapted to logarithmic quantisation.



### Transformers for Object Detection in Large Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2209.15258v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.15258v1)
- **Published**: 2022-09-30 06:35:43+00:00
- **Updated**: 2022-09-30 06:35:43+00:00
- **Authors**: Felicia Ruppel, Florian Faion, Claudius Gläser, Klaus Dietmayer
- **Comment**: Accepted for publication at the 2022 25th IEEE International
  Conference on Intelligent Transportation Systems (ITSC 2022), Sep 18- Oct 12,
  2022, in Macau, China
- **Journal**: None
- **Summary**: We present TransLPC, a novel detection model for large point clouds that is based on a transformer architecture. While object detection with transformers has been an active field of research, it has proved difficult to apply such models to point clouds that span a large area, e.g. those that are common in autonomous driving, with lidar or radar data. TransLPC is able to remedy these issues: The structure of the transformer model is modified to allow for larger input sequence lengths, which are sufficient for large point clouds. Besides this, we propose a novel query refinement technique to improve detection accuracy, while retaining a memory-friendly number of transformer decoder queries. The queries are repositioned between layers, moving them closer to the bounding box they are estimating, in an efficient manner. This simple technique has a significant effect on detection accuracy, which is evaluated on the challenging nuScenes dataset on real-world lidar data. Besides this, the proposed method is compatible with existing transformer-based solutions that require object detection, e.g. for joint multi-object tracking and detection, and enables them to be used in conjunction with large point clouds.



### Minimalistic Unsupervised Learning with the Sparse Manifold Transform
- **Arxiv ID**: http://arxiv.org/abs/2209.15261v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2209.15261v2)
- **Published**: 2022-09-30 06:38:30+00:00
- **Updated**: 2023-04-27 22:05:23+00:00
- **Authors**: Yubei Chen, Zeyu Yun, Yi Ma, Bruno Olshausen, Yann LeCun
- **Comment**: This paper is published at ICLR 2023
- **Journal**: The Eleventh International Conference on Learning Representations
  (2023)
- **Summary**: We describe a minimalistic and interpretable method for unsupervised learning, without resorting to data augmentation, hyperparameter tuning, or other engineering designs, that achieves performance close to the SOTA SSL methods. Our approach leverages the sparse manifold transform, which unifies sparse coding, manifold learning, and slow feature analysis. With a one-layer deterministic sparse manifold transform, one can achieve 99.3% KNN top-1 accuracy on MNIST, 81.1% KNN top-1 accuracy on CIFAR-10 and 53.2% on CIFAR-100. With a simple gray-scale augmentation, the model gets 83.2% KNN top-1 accuracy on CIFAR-10 and 57% on CIFAR-100. These results significantly close the gap between simplistic "white-box" methods and the SOTA methods. Additionally, we provide visualization to explain how an unsupervised representation transform is formed. The proposed method is closely connected to latent-embedding self-supervised methods and can be treated as the simplest form of VICReg. Though there remains a small performance gap between our simple constructive model and SOTA methods, the evidence points to this as a promising direction for achieving a principled and white-box approach to unsupervised learning.



### Diffusion-based Image Translation using Disentangled Style and Content Representation
- **Arxiv ID**: http://arxiv.org/abs/2209.15264v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2209.15264v2)
- **Published**: 2022-09-30 06:44:37+00:00
- **Updated**: 2023-02-01 10:27:16+00:00
- **Authors**: Gihyun Kwon, Jong Chul Ye
- **Comment**: ICLR 2023 camera ready
- **Journal**: None
- **Summary**: Diffusion-based image translation guided by semantic texts or a single target image has enabled flexible style transfer which is not limited to the specific domains. Unfortunately, due to the stochastic nature of diffusion models, it is often difficult to maintain the original content of the image during the reverse diffusion. To address this, here we present a novel diffusion-based unsupervised image translation method using disentangled style and content representation.   Specifically, inspired by the splicing Vision Transformer, we extract intermediate keys of multihead self attention layer from ViT model and used them as the content preservation loss. Then, an image guided style transfer is performed by matching the [CLS] classification token from the denoised samples and target image, whereas additional CLIP loss is used for the text-driven style transfer. To further accelerate the semantic change during the reverse diffusion, we also propose a novel semantic divergence loss and resampling strategy. Our experimental results show that the proposed method outperforms state-of-the-art baseline models in both text-guided and image-guided translation tasks.



### Generative Model Watermarking Based on Human Visual System
- **Arxiv ID**: http://arxiv.org/abs/2209.15268v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/2209.15268v1)
- **Published**: 2022-09-30 06:59:11+00:00
- **Updated**: 2022-09-30 06:59:11+00:00
- **Authors**: Li Zhang, Yong Liu, Shaoteng Liu, Tianshu Yang, Yexin Wang, Xinpeng Zhang, Hanzhou Wu
- **Comment**: https://scholar.google.com/citations?user=IdiF7M0AAAAJ&hl=en
- **Journal**: 19th International Forum of Digital Multimedia Communication
  (2022)
- **Summary**: Intellectual property protection of deep neural networks is receiving attention from more and more researchers, and the latest research applies model watermarking to generative models for image processing. However, the existing watermarking methods designed for generative models do not take into account the effects of different channels of sample images on watermarking. As a result, the watermarking performance is still limited. To tackle this problem, in this paper, we first analyze the effects of embedding watermark information on different channels. Then, based on the characteristics of human visual system (HVS), we introduce two HVS-based generative model watermarking methods, which are realized in RGB color space and YUV color space respectively. In RGB color space, the watermark is embedded into the R and B channels based on the fact that HVS is more sensitive to G channel. In YUV color space, the watermark is embedded into the DCT domain of U and V channels based on the fact that HVS is more sensitive to brightness changes. Experimental results demonstrate the effectiveness of the proposed work, which improves the fidelity of the model to be protected and has good universality compared with previous methods.



### ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2209.15270v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15270v1)
- **Published**: 2022-09-30 07:20:07+00:00
- **Updated**: 2022-09-30 07:20:07+00:00
- **Authors**: Bin Shan, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang
- **Comment**: 14 pages, 6 figures
- **Journal**: None
- **Summary**: Recent Vision-Language Pre-trained (VLP) models based on dual encoder have attracted extensive attention from academia and industry due to their superior performance on various cross-modal tasks and high computational efficiency. They attempt to learn cross-modal representation using contrastive learning on image-text pairs, however, the built inter-modal correlations only rely on a single view for each modality. Actually, an image or a text contains various potential views, just as humans could capture a real-world scene via diverse descriptions or photos. In this paper, we propose ERNIE-ViL 2.0, a Multi-View Contrastive learning framework to build intra-modal and inter-modal correlations between diverse views simultaneously, aiming at learning a more robust cross-modal representation. Specifically, we construct multiple views within each modality to learn the intra-modal correlation for enhancing the single-modal representation. Besides the inherent visual/textual views, we construct sequences of object tags as a special textual view to narrow the cross-modal semantic gap on noisy image-text pairs. Pre-trained with 29M publicly available datasets, ERNIE-ViL 2.0 achieves competitive results on English cross-modal retrieval. Additionally, to generalize our method to Chinese cross-modal tasks, we train ERNIE-ViL 2.0 through scaling up the pre-training datasets to 1.5B Chinese image-text pairs, resulting in significant improvements compared to previous SOTA results on Chinese cross-modal retrieval. We release our pre-trained models in https://github.com/PaddlePaddle/ERNIE.



### Application-Driven AI Paradigm for Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.15271v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.15271v1)
- **Published**: 2022-09-30 07:22:01+00:00
- **Updated**: 2022-09-30 07:22:01+00:00
- **Authors**: Zezhou Chen, Yajie Cui, Kaikai Zhao, Zhaoxiang Liu, Shiguo Lian
- **Comment**: None
- **Journal**: None
- **Summary**: Human action recognition in computer vision has been widely studied in recent years. However, most algorithms consider only certain action specially with even high computational cost. That is not suitable for practical applications with multiple actions to be identified with low computational cost. To meet various application scenarios, this paper presents a unified human action recognition framework composed of two modules, i.e., multi-form human detection and corresponding action classification. Among them, an open-source dataset is constructed to train a multi-form human detection model that distinguishes a human being's whole body, upper body or part body, and the followed action classification model is adopted to recognize such action as falling, sleeping or on-duty, etc. Some experimental results show that the unified framework is effective for various application scenarios. It is expected to be a new application-driven AI paradigm for human action recognition.



### Learning Transferable Spatiotemporal Representations from Natural Script Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2209.15280v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.15280v3)
- **Published**: 2022-09-30 07:39:48+00:00
- **Updated**: 2023-03-12 13:49:09+00:00
- **Authors**: Ziyun Zeng, Yuying Ge, Xihui Liu, Bin Chen, Ping Luo, Shu-Tao Xia, Yixiao Ge
- **Comment**: Accepted by CVPR 2023; camera-ready version
- **Journal**: None
- **Summary**: Pre-training on large-scale video data has become a common recipe for learning transferable spatiotemporal representations in recent years. Despite some progress, existing methods are mostly limited to highly curated datasets (e.g., K400) and exhibit unsatisfactory out-of-the-box representations. We argue that it is due to the fact that they only capture pixel-level knowledge rather than spatiotemporal semantics, which hinders further progress in video understanding. Inspired by the great success of image-text pre-training (e.g., CLIP), we take the first step to exploit language semantics to boost transferable spatiotemporal representation learning. We introduce a new pretext task, Turning to Video for Transcript Sorting (TVTS), which sorts shuffled ASR scripts by attending to learned video representations. We do not rely on descriptive captions and learn purely from video, i.e., leveraging the natural transcribed speech knowledge to provide noisy but useful semantics over time. Our method enforces the vision model to contextualize what is happening over time so that it can re-organize the narrative transcripts, and can seamlessly apply to large-scale uncurated video data in the real world. Our method demonstrates strong out-of-the-box spatiotemporal representations on diverse benchmarks, e.g., +13.6% gains over VideoMAE on SSV2 via linear probing. The code is available at https://github.com/TencentARC/TVTS.



### Verifiable and Energy Efficient Medical Image Analysis with Quantised Self-attentive Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.15287v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.15287v1)
- **Published**: 2022-09-30 07:51:29+00:00
- **Updated**: 2022-09-30 07:51:29+00:00
- **Authors**: Rakshith Sathish, Swanand Khare, Debdoot Sheet
- **Comment**: Accepted at MICCAI 2022 FAIR Workshop
- **Journal**: None
- **Summary**: Convolutional Neural Networks have played a significant role in various medical imaging tasks like classification and segmentation. They provide state-of-the-art performance compared to classical image processing algorithms. However, the major downside of these methods is the high computational complexity, reliance on high-performance hardware like GPUs and the inherent black-box nature of the model. In this paper, we propose quantised stand-alone self-attention based models as an alternative to traditional CNNs. In the proposed class of networks, convolutional layers are replaced with stand-alone self-attention layers, and the network parameters are quantised after training. We experimentally validate the performance of our method on classification and segmentation tasks. We observe a $50-80\%$ reduction in model size, $60-80\%$ lesser number of parameters, $40-85\%$ fewer FLOPs and $65-80\%$ more energy efficiency during inference on CPUs. The code will be available at \href {https://github.com/Rakshith2597/Quantised-Self-Attentive-Deep-Neural-Network}{https://github.com/Rakshith2597/Quantised-Self-Attentive-Deep-Neural-Network}.



### Hiding Visual Information via Obfuscating Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/2209.15304v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15304v4)
- **Published**: 2022-09-30 08:23:26+00:00
- **Updated**: 2023-08-28 03:16:50+00:00
- **Authors**: Zhigang Su, Dawei Zhou, Nannan Wangu, Decheng Li, Zhen Wang, Xinbo Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Growing leakage and misuse of visual information raise security and privacy concerns, which promotes the development of information protection. Existing adversarial perturbations-based methods mainly focus on the de-identification against deep learning models. However, the inherent visual information of the data has not been well protected. In this work, inspired by the Type-I adversarial attack, we propose an adversarial visual information hiding method to protect the visual privacy of data. Specifically, the method generates obfuscating adversarial perturbations to obscure the visual information of the data. Meanwhile, it maintains the hidden objectives to be correctly predicted by models. In addition, our method does not modify the parameters of the applied model, which makes it flexible for different scenarios. Experimental results on the recognition and classification tasks demonstrate that the proposed method can effectively hide visual information and hardly affect the performances of models. The code is available in the supplementary material.



### Effective Early Stopping of Point Cloud Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.15308v1
- **DOI**: 10.1007/978-3-031-13448-7_13
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.15308v1)
- **Published**: 2022-09-30 08:34:22+00:00
- **Updated**: 2022-09-30 08:34:22+00:00
- **Authors**: Thanasis Zoumpekas, Maria Salamó, Anna Puig
- **Comment**: None
- **Journal**: None
- **Summary**: Early stopping techniques can be utilized to decrease the time cost, however currently the ultimate goal of early stopping techniques is closely related to the accuracy upgrade or the ability of the neural network to generalize better on unseen data without being large or complex in structure and not directly with its efficiency. Time efficiency is a critical factor in neural networks, especially when dealing with the segmentation of 3D point cloud data, not only because a neural network itself is computationally expensive, but also because point clouds are large and noisy data, making learning processes even more costly. In this paper, we propose a new early stopping technique based on fundamental mathematics aiming to upgrade the trade-off between the learning efficiency and accuracy of neural networks dealing with 3D point clouds. Our results show that by employing our early stopping technique in four distinct and highly utilized neural networks in segmenting 3D point clouds, the training time efficiency of the models is greatly improved, with efficiency gain values reaching up to 94\%, while the models achieving in just a few epochs approximately similar segmentation accuracy metric values like the ones that are obtained in the training of the neural networks in 200 epochs. Also, our proposal outperforms four conventional early stopping approaches in segmentation accuracy, implying a promising innovative early stopping technique in point cloud segmentation.



### Did You Get What You Paid For? Rethinking Annotation Cost of Deep Learning Based Computer Aided Detection in Chest Radiographs
- **Arxiv ID**: http://arxiv.org/abs/2209.15314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15314v1)
- **Published**: 2022-09-30 08:42:22+00:00
- **Updated**: 2022-09-30 08:42:22+00:00
- **Authors**: Tae Soo Kim, Geonwoon Jang, Sanghyup Lee, Thijs Kooi
- **Comment**: MICCAI 2022, Contains Supplemental Material
- **Journal**: None
- **Summary**: As deep networks require large amounts of accurately labeled training data, a strategy to collect sufficiently large and accurate annotations is as important as innovations in recognition methods. This is especially true for building Computer Aided Detection (CAD) systems for chest X-rays where domain expertise of radiologists is required to annotate the presence and location of abnormalities on X-ray images. However, there lacks concrete evidence that provides guidance on how much resource to allocate for data annotation such that the resulting CAD system reaches desired performance. Without this knowledge, practitioners often fall back to the strategy of collecting as much detail as possible on as much data as possible which is cost inefficient. In this work, we investigate how the cost of data annotation ultimately impacts the CAD model performance on classification and segmentation of chest abnormalities in frontal-view X-ray images. We define the cost of annotation with respect to the following three dimensions: quantity, quality and granularity of labels. Throughout this study, we isolate the impact of each dimension on the resulting CAD model performance on detecting 10 chest abnormalities in X-rays. On a large scale training data with over 120K X-ray images with gold-standard annotations, we find that cost-efficient annotations provide great value when collected in large amounts and lead to competitive performance when compared to models trained with only gold-standard annotations. We also find that combining large amounts of cost efficient annotations with only small amounts of expensive labels leads to competitive CAD models at a much lower cost.



### Convolutional Neural Networks Quantization with Attention
- **Arxiv ID**: http://arxiv.org/abs/2209.15317v1
- **DOI**: 10.1142/S0129065722500514
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.15317v1)
- **Published**: 2022-09-30 08:48:31+00:00
- **Updated**: 2022-09-30 08:48:31+00:00
- **Authors**: Binyi Wu, Bernd Waschneck, Christian Georg Mayr
- **Comment**: Preprint of an article published in International Journal of Neural
  Systems, [10.1142/S0129065722500514] \c{opyright} [copyright World Scientific
  Publishing Company]
  [https://www.worldscientific.com/doi/10.1142/S0129065722500514]
- **Journal**: None
- **Summary**: It has been proven that, compared to using 32-bit floating-point numbers in the training phase, Deep Convolutional Neural Networks (DCNNs) can operate with low precision during inference, thereby saving memory space and power consumption. However, quantizing networks is always accompanied by an accuracy decrease. Here, we propose a method, double-stage Squeeze-and-Threshold (double-stage ST). It uses the attention mechanism to quantize networks and achieve state-of-art results. Using our method, the 3-bit model can achieve accuracy that exceeds the accuracy of the full-precision baseline model. The proposed double-stage ST activation quantization is easy to apply: inserting it before the convolution.



### SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.15323v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2209.15323v2)
- **Published**: 2022-09-30 09:03:22+00:00
- **Updated**: 2023-03-28 12:04:49+00:00
- **Authors**: Rita Ramos, Bruno Martins, Desmond Elliott, Yova Kementchedjhieva
- **Comment**: Accepted to CVPR 2023
- **Journal**: None
- **Summary**: Recent advances in image captioning have focused on scaling the data and model size, substantially increasing the cost of pre-training and finetuning. As an alternative to large models, we present SmallCap, which generates a caption conditioned on an input image and related captions retrieved from a datastore. Our model is lightweight and fast to train, as the only learned parameters are in newly introduced cross-attention layers between a pre-trained CLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains without additional finetuning and can exploit large-scale data in a training-free fashion since the contents of the datastore can be readily replaced. Our experiments show that SmallCap, trained only on COCO, has competitive performance on this benchmark, and also transfers to other domains without retraining, solely through retrieval from target-domain data. Further improvement is achieved through the training-free exploitation of diverse human-labeled and web data, which proves to be effective for a range of domains, including the nocaps benchmark, designed to test generalization to unseen visual concepts.



### Towards a Fully Autonomous UAV Controller for Moving Platform Detection and Landing
- **Arxiv ID**: http://arxiv.org/abs/2210.08120v1
- **DOI**: 10.1109/VLSID2022.2022.00044
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2210.08120v1)
- **Published**: 2022-09-30 09:16:04+00:00
- **Updated**: 2022-09-30 09:16:04+00:00
- **Authors**: Michalis Piponidis, Panayiotis Aristodemou, Theocharis Theocharides
- **Comment**: 2022 35th International Conference on VLSI Design and 2022 21st
  International Conference on Embedded Systems (VLSID)
- **Journal**: None
- **Summary**: While Unmanned Aerial Vehicles (UAVs) are increasingly deployed in several missions, their inability of reliable and consistent autonomous landing poses a major setback for deploying such systems truly autonomously. In this paper we present an autonomous UAV landing system for landing on a moving platform. In contrast to existing attempts, the proposed system relies only on the camera sensor, and has been designed as lightweight as possible. The proposed system can be deployed on a low power platform as part of the drone payload, whilst being indifferent to any external communication or any other sensors. The system relies on a Neural Network (NN) based controller, for which a target and environment agnostic simulator was created, used in training and testing of the proposed system, via Reinforcement Learning (RL) and Proximal Policy optimization (PPO) to optimally control and steer the drone towards landing on the target. Through real-world testing, the system was evaluated with an average deviation of 15cm from the center of the target, for 40 landing attempts.



### Towards End-to-end Handwritten Document Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.15362v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15362v2)
- **Published**: 2022-09-30 10:31:22+00:00
- **Updated**: 2022-10-20 14:05:59+00:00
- **Authors**: Denis Coquenet
- **Comment**: Ph.D Thesis
- **Journal**: None
- **Summary**: Handwritten text recognition has been widely studied in the last decades for its numerous applications. Nowadays, the state-of-the-art approach consists in a three-step process. The document is segmented into text lines, which are then ordered and recognized. However, this three-step approach has many drawbacks. The three steps are treated independently whereas they are closely related. Errors accumulate from one step to the other. The ordering step is based on heuristic rules which prevent its use for documents with a complex layouts or for heterogeneous documents. The need for additional physical segmentation annotations for training the segmentation stage is inherent to this approach. In this thesis, we propose to tackle these issues by performing the handwritten text recognition of whole document in an end-to-end way. To this aim, we gradually increase the difficulty of the recognition task, moving from isolated lines to paragraphs, and then to whole documents. We proposed an approach at the line level, based on a fully convolutional network, in order to design a first generic feature extraction step for the handwriting recognition task. Based on this preliminary work, we studied two different approaches to recognize handwritten paragraphs. We reached state-of-the-art results at paragraph level on the RIMES 2011, IAM and READ 2016 datasets and outperformed the line-level state of the art on these datasets. We finally proposed the first end-to-end approach dedicated to the recognition of both text and layout, at document level. Characters and layout tokens are sequentially predicted following a learned reading order. We proposed two new metrics we used to evaluate this task on the RIMES 2009 and READ 2016 dataset, at page level and double-page level.



### Inharmonious Region Localization by Magnifying Domain Discrepancy
- **Arxiv ID**: http://arxiv.org/abs/2209.15368v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15368v1)
- **Published**: 2022-09-30 10:41:16+00:00
- **Updated**: 2022-09-30 10:41:16+00:00
- **Authors**: Jing Liang, Li Niu, Penghao Wu, Fengjun Guo, Teng Long
- **Comment**: None
- **Journal**: None
- **Summary**: Inharmonious region localization aims to localize the region in a synthetic image which is incompatible with surrounding background. The inharmony issue is mainly attributed to the color and illumination inconsistency produced by image editing techniques. In this work, we tend to transform the input image to another color space to magnify the domain discrepancy between inharmonious region and background, so that the model can identify the inharmonious region more easily. To this end, we present a novel framework consisting of a color mapping module and an inharmonious region localization network, in which the former is equipped with a novel domain discrepancy magnification loss and the latter could be an arbitrary localization network. Extensive experiments on image harmonization dataset show the superiority of our designed framework. Our code is available at https://github.com/bcmi/MadisNet-Inharmonious-Region-Localization.



### Automatic Context-Driven Inference of Engagement in HMI: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2209.15370v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.15370v1)
- **Published**: 2022-09-30 10:46:13+00:00
- **Updated**: 2022-09-30 10:46:13+00:00
- **Authors**: Hanan Salam, Oya Celiktutan, Hatice Gunes, Mohamed Chetouani
- **Comment**: None
- **Journal**: None
- **Summary**: An integral part of seamless human-human communication is engagement, the process by which two or more participants establish, maintain, and end their perceived connection. Therefore, to develop successful human-centered human-machine interaction applications, automatic engagement inference is one of the tasks required to achieve engaging interactions between humans and machines, and to make machines attuned to their users, hence enhancing user satisfaction and technology acceptance. Several factors contribute to engagement state inference, which include the interaction context and interactants' behaviours and identity. Indeed, engagement is a multi-faceted and multi-modal construct that requires high accuracy in the analysis and interpretation of contextual, verbal and non-verbal cues. Thus, the development of an automated and intelligent system that accomplishes this task has been proven to be challenging so far. This paper presents a comprehensive survey on previous work in engagement inference for human-machine interaction, entailing interdisciplinary definition, engagement components and factors, publicly available datasets, ground truth assessment, and most commonly used features and methods, serving as a guide for the development of future human-machine interaction interfaces with reliable context-aware engagement inference capability. An in-depth review across embodied and disembodied interaction modes, and an emphasis on the interaction context of which engagement perception modules are integrated sets apart the presented survey from existing surveys.



### NBV-SC: Next Best View Planning based on Shape Completion for Fruit Mapping and Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2209.15376v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.15376v3)
- **Published**: 2022-09-30 11:09:54+00:00
- **Updated**: 2023-08-30 11:04:14+00:00
- **Authors**: Rohit Menon, Tobias Zaenker, Nils Dengler, Maren Bennewitz
- **Comment**: Agricultural Automation, Viewpoint Planning, Active Perception, Shape
  Completion
- **Journal**: None
- **Summary**: Active perception for fruit mapping and harvesting is a difficult task since occlusions occur frequently and the location as well as size of fruits change over time. State-of-the-art viewpoint planning approaches utilize computationally expensive ray casting operations to find good viewpoints aiming at maximizing information gain and covering the fruits in the scene. In this paper, we present a novel viewpoint planning approach that explicitly uses information about the predicted fruit shapes to compute targeted viewpoints that observe as yet unobserved parts of the fruits. Furthermore, we formulate the concept of viewpoint dissimilarity to reduce the sampling space for more efficient selection of useful, dissimilar viewpoints. Our simulation experiments with a UR5e arm equipped with an RGB-D sensor provide a quantitative demonstration of the efficacy of our iterative next best view planning method based on shape completion. In comparative experiments with a state-of-the-art viewpoint planner, we demonstrate improvement not only in the estimation of the fruit sizes, but also in their reconstruction, while significantly reducing the planning time. Finally, we show the viability of our approach for mapping sweet peppers plants with a real robotic system in a commercial glasshouse.



### DELAD: Deep Landweber-guided deconvolution with Hessian and sparse prior
- **Arxiv ID**: http://arxiv.org/abs/2209.15377v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.15377v1)
- **Published**: 2022-09-30 11:15:03+00:00
- **Updated**: 2022-09-30 11:15:03+00:00
- **Authors**: Tomas Chobola, Anton Theileis, Jan Taucher, Tingying Peng
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: We present a model for non-blind image deconvolution that incorporates the classic iterative method into a deep learning application. Instead of using large over-parameterised generative networks to create sharp picture representations, we build our network based on the iterative Landweber deconvolution algorithm, which is integrated with trainable convolutional layers to enhance the recovered image structures and details. Additional to the data fidelity term, we also add Hessian and sparse constraints as regularization terms to improve the image reconstruction quality. Our proposed model is \textit{self-supervised} and converges to a solution based purely on the input blurred image and respective blur kernel without the requirement of any pre-training. We evaluate our technique using standard computer vision benchmarking datasets as well as real microscope images obtained by our enhanced depth-of-field (EDOF) underwater microscope, demonstrating the capabilities of our model in a real-world application. The quantitative results demonstrate that our approach is competitive with state-of-the-art non-blind image deblurring methods despite having a fraction of the parameters and not being pre-trained, demonstrating the efficiency and efficacy of embedding a classic deconvolution approach inside a deep network.



### Semi-Supervised Single-View 3D Reconstruction via Prototype Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/2209.15383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15383v1)
- **Published**: 2022-09-30 11:19:25+00:00
- **Updated**: 2022-09-30 11:19:25+00:00
- **Authors**: Zhen Xing, Hengduo Li, Zuxuan Wu, Yu-Gang Jiang
- **Comment**: Accepeted by ECCV2022
- **Journal**: None
- **Summary**: The performance of existing single-view 3D reconstruction methods heavily relies on large-scale 3D annotations. However, such annotations are tedious and expensive to collect. Semi-supervised learning serves as an alternative way to mitigate the need for manual labels, but remains unexplored in 3D reconstruction. Inspired by the recent success of semi-supervised image classification tasks, we propose SSP3D, a semi-supervised framework for 3D reconstruction. In particular, we introduce an attention-guided prototype shape prior module for guiding realistic object reconstruction. We further introduce a discriminator-guided module to incentivize better shape generation, as well as a regularizer to tolerate noisy training samples. On the ShapeNet benchmark, the proposed approach outperforms previous supervised methods by clear margins under various labeling ratios, (i.e., 1%, 5% , 10% and 20%). Moreover, our approach also performs well when transferring to real-world Pix3D datasets under labeling ratios of 10%. We also demonstrate our method could transfer to novel categories with few novel supervised data. Experiments on the popular ShapeNet dataset show that our method outperforms the zero-shot baseline by over 12% and we also perform rigorous ablations and analysis to validate our approach.



### Evaluation of importance estimators in deep learning classifiers for Computed Tomography
- **Arxiv ID**: http://arxiv.org/abs/2209.15398v1
- **DOI**: 10.1007/978-3-031-15565-9_1
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2209.15398v1)
- **Published**: 2022-09-30 11:57:25+00:00
- **Updated**: 2022-09-30 11:57:25+00:00
- **Authors**: Lennart Brocki, Wistan Marchadour, Jonas Maison, Bogdan Badic, Panagiotis Papadimitroulas, Mathieu Hatt, Franck Vermet, Neo Christopher Chung
- **Comment**: 4th International Workshop on EXplainable and TRAnsparent AI and
  Multi-Agent Systems (EXTRAAMAS 2022) - International Conference on Autonomous
  Agents and Multi-Agent Systems (AAMAS)
- **Journal**: 2022 EXTRAAMAS 2022, Lecture Notes in Computer Science (LNAI,
  volume 13283)
- **Summary**: Deep learning has shown superb performance in detecting objects and classifying images, ensuring a great promise for analyzing medical imaging. Translating the success of deep learning to medical imaging, in which doctors need to understand the underlying process, requires the capability to interpret and explain the prediction of neural networks. Interpretability of deep neural networks often relies on estimating the importance of input features (e.g., pixels) with respect to the outcome (e.g., class probability). However, a number of importance estimators (also known as saliency maps) have been developed and it is unclear which ones are more relevant for medical imaging applications. In the present work, we investigated the performance of several importance estimators in explaining the classification of computed tomography (CT) images by a convolutional deep network, using three distinct evaluation metrics. First, the model-centric fidelity measures a decrease in the model accuracy when certain inputs are perturbed. Second, concordance between importance scores and the expert-defined segmentation masks is measured on a pixel level by a receiver operating characteristic (ROC) curves. Third, we measure a region-wise overlap between a XRAI-based map and the segmentation mask by Dice Similarity Coefficients (DSC). Overall, two versions of SmoothGrad topped the fidelity and ROC rankings, whereas both Integrated Gradients and SmoothGrad excelled in DSC evaluation. Interestingly, there was a critical discrepancy between model-centric (fidelity) and human-centric (ROC and DSC) evaluation. Expert expectation and intuition embedded in segmentation maps does not necessarily align with how the model arrived at its prediction. Understanding this difference in interpretability would help harnessing the power of deep learning in medicine.



### Rethinking the Learning Paradigm for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.15402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15402v1)
- **Published**: 2022-09-30 12:00:54+00:00
- **Updated**: 2022-09-30 12:00:54+00:00
- **Authors**: Weijie Wang, Nicu Sebe, Bruno Lepri
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the subjective crowdsourcing annotations and the inherent inter-class similarity of facial expressions, the real-world Facial Expression Recognition (FER) datasets usually exhibit ambiguous annotation. To simplify the learning paradigm, most previous methods convert ambiguous annotation results into precise one-hot annotations and train FER models in an end-to-end supervised manner. In this paper, we rethink the existing training paradigm and propose that it is better to use weakly supervised strategies to train FER models with original ambiguous annotation.



### Entropy-driven Unsupervised Keypoint Representation Learning in Videos
- **Arxiv ID**: http://arxiv.org/abs/2209.15404v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.15404v2)
- **Published**: 2022-09-30 12:03:52+00:00
- **Updated**: 2023-06-06 07:23:21+00:00
- **Authors**: Ali Younes, Simone Schaub-Meyer, Georgia Chalvatzaki
- **Comment**: 29 pages, 14 figures, Accepted at ICML 2023
- **Journal**: None
- **Summary**: Extracting informative representations from videos is fundamental for effectively learning various downstream tasks. We present a novel approach for unsupervised learning of meaningful representations from videos, leveraging the concept of image spatial entropy (ISE) that quantifies the per-pixel information in an image. We argue that \textit{local entropy} of pixel neighborhoods and their temporal evolution create valuable intrinsic supervisory signals for learning prominent features. Building on this idea, we abstract visual features into a concise representation of keypoints that act as dynamic information transmitters, and design a deep learning model that learns, purely unsupervised, spatially and temporally consistent representations \textit{directly} from video frames. Two original information-theoretic losses, computed from local entropy, guide our model to discover consistent keypoint representations; a loss that maximizes the spatial information covered by the keypoints and a loss that optimizes the keypoints' information transportation over time. We compare our keypoint representation to strong baselines for various downstream tasks, \eg, learning object dynamics. Our empirical results show superior performance for our information-driven keypoints that resolve challenges like attendance to static and dynamic objects or objects abruptly entering and leaving the scene.



### Semi-Supervised Domain Generalization for Cardiac Magnetic Resonance Image Segmentation with High Quality Pseudo Labels
- **Arxiv ID**: http://arxiv.org/abs/2209.15451v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.15451v2)
- **Published**: 2022-09-30 12:57:41+00:00
- **Updated**: 2022-10-30 08:41:48+00:00
- **Authors**: Wanqin Ma, Huifeng Yao, Yiqun Lin, Jiarong Guo, Xiaomeng Li
- **Comment**: Accepted by International Workshop on Statistical Atlases and
  Computational Models of the Heart (STACOM2022) of MICCAI2022
- **Journal**: None
- **Summary**: Developing a deep learning method for medical segmentation tasks heavily relies on a large amount of labeled data. However, the annotations require professional knowledge and are limited in number. Recently, semi-supervised learning has demonstrated great potential in medical segmentation tasks. Most existing methods related to cardiac magnetic resonance images only focus on regular images with similar domains and high image quality. A semi-supervised domain generalization method was developed in [2], which enhances the quality of pseudo labels on varied datasets. In this paper, we follow the strategy in [2] and present a domain generalization method for semi-supervised medical segmentation. Our main goal is to improve the quality of pseudo labels under extreme MRI Analysis with various domains. We perform Fourier transformation on input images to learn low-level statistics and cross-domain information. Then we feed the augmented images as input to the double cross pseudo supervision networks to calculate the variance among pseudo labels. We evaluate our method on the CMRxMotion dataset [1]. With only partially labeled data and without domain labels, our approach consistently generates accurate segmentation results of cardiac magnetic resonance images with different respiratory motions. Code will be available after the conference.



### Road Network Deterioration Monitoring Using Aerial Images and Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/2209.15455v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2209.15455v1)
- **Published**: 2022-09-30 13:05:03+00:00
- **Updated**: 2022-09-30 13:05:03+00:00
- **Authors**: Nicolas Parra-A, Vladimir Vargas-Calderón, Herbert Vinck-Posada, Nicanor Vinck
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Road maintenance is an essential process for guaranteeing the quality of transportation in any city. A crucial step towards effective road maintenance is the ability to update the inventory of the road network. We present a proof of concept of a protocol for maintaining said inventory based on the use of unmanned aerial vehicles to quickly collect images which are processed by a computer vision program that automatically identifies potholes and their severity. Our protocol aims to provide information to local governments to prioritise the road network maintenance budget, and to be able to detect early stages of road deterioration so as to minimise maintenance expenditure.



### Melanoma Skin Cancer and Nevus Mole Classification using Intensity Value Estimation with Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2209.15465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15465v1)
- **Published**: 2022-09-30 13:35:24+00:00
- **Updated**: 2022-09-30 13:35:24+00:00
- **Authors**: N. I. Md. Ashafuddula, Rafiqul Islam
- **Comment**: The paper has been accepted for publication in Computer Science
  journal: http://journals.agh.edu.pl/csci
- **Journal**: None
- **Summary**: Melanoma skin cancer is one of the most dangerous and life-threatening cancer. Exposure to ultraviolet rays may damage the skin cell's DNA, which causes melanoma skin cancer. However, it is difficult to detect and classify melanoma and nevus mole at the immature stages. In this work, an automatic deep learning system is developed based on the intensity value estimation with a convolutional neural network model (CNN) to detect and classify melanoma and nevus mole more accurately. Since intensity levels are the most distinctive features for object or region of interest identification, the high-intensity pixel values are selected from the extracted lesion images. Incorporating those high-intensity features into the CNN improves the overall performance of the proposed model than the state-of-the-art methods for detecting melanoma skin cancer. To evaluate the system, we used 5-fold cross-validation. Experimental results show that a superior percentage of accuracy (92.58%), sensitivity (93.76%), specificity (91.56%), and precision (90.68%) are achieved.



### Two-headed eye-segmentation approach for biometric identification
- **Arxiv ID**: http://arxiv.org/abs/2209.15471v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.15471v1)
- **Published**: 2022-09-30 13:52:03+00:00
- **Updated**: 2022-09-30 13:52:03+00:00
- **Authors**: Wiktor Lazarski, Maciej Zieba, Tanguy Jeanneau, Tobias Zillig, Christian Brendel
- **Comment**: None
- **Journal**: None
- **Summary**: Iris-based identification systems are among the most popular approaches for person identification. Such systems require good-quality segmentation modules that ideally identify the regions for different eye components. This paper introduces the new two-headed architecture, where the eye components and eyelashes are segmented using two separate decoding modules. Moreover, we investigate various training scenarios by adopting different training losses. Thanks to the two-headed approach, we were also able to examine the quality of the model with the convex prior, which enforces the convexity of the segmented shapes. We conducted an extensive evaluation of various learning scenarios on real-life conditions high-resolution near-infrared iris images.



### Reliable Face Morphing Attack Detection in On-The-Fly Border Control Scenario with Variation in Image Resolution and Capture Distance
- **Arxiv ID**: http://arxiv.org/abs/2209.15474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15474v1)
- **Published**: 2022-09-30 13:58:43+00:00
- **Updated**: 2022-09-30 13:58:43+00:00
- **Authors**: Jag Mohan Singh, Raghavendra Ramachandra
- **Comment**: The paper is accepted at the International Joint Conference on
  Biometrics (IJCB) 2022
- **Journal**: None
- **Summary**: Face Recognition Systems (FRS) are vulnerable to various attacks performed directly and indirectly. Among these attacks, face morphing attacks are highly potential in deceiving automatic FRS and human observers and indicate a severe security threat, especially in the border control scenario. This work presents a face morphing attack detection, especially in the On-The-Fly (OTF) Automatic Border Control (ABC) scenario. We present a novel Differential-MAD (D-MAD) algorithm based on the spherical interpolation and hierarchical fusion of deep features computed from six different pre-trained deep Convolutional Neural Networks (CNNs). Extensive experiments are carried out on the newly generated face morphing dataset (SCFace-Morph) based on the publicly available SCFace dataset by considering the real-life scenario of Automatic Border Control (ABC) gates. Experimental protocols are designed to benchmark the proposed and state-of-the-art (SOTA) D-MAD techniques for different camera resolutions and capture distances. Obtained results have indicated the superior performance of the proposed D-MAD method compared to the existing methods.



### Point Cloud Quality Assessment using 3D Saliency Maps
- **Arxiv ID**: http://arxiv.org/abs/2209.15475v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.15475v1)
- **Published**: 2022-09-30 13:59:09+00:00
- **Updated**: 2022-09-30 13:59:09+00:00
- **Authors**: Zhengyu Wang, Yujie Zhang, Qi Yang, Yiling Xu, Jun Sun, Shan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Point cloud quality assessment (PCQA) has become an appealing research field in recent days. Considering the importance of saliency detection in quality assessment, we propose an effective full-reference PCQA metric which makes the first attempt to utilize the saliency information to facilitate quality prediction, called point cloud quality assessment using 3D saliency maps (PQSM). Specifically, we first propose a projection-based point cloud saliency map generation method, in which depth information is introduced to better reflect the geometric characteristics of point clouds. Then, we construct point cloud local neighborhoods to derive three structural descriptors to indicate the geometry, color and saliency discrepancies. Finally, a saliency-based pooling strategy is proposed to generate the final quality score. Extensive experiments are performed on four independent PCQA databases. The results demonstrate that the proposed PQSM shows competitive performances compared to multiple state-of-the-art PCQA metrics.



### Impact of Face Image Quality Estimation on Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.15489v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15489v1)
- **Published**: 2022-09-30 14:23:47+00:00
- **Updated**: 2022-09-30 14:23:47+00:00
- **Authors**: Carlos Aravena, Diego Pasmino, Juan E. Tapia, Christoph Busch
- **Comment**: None
- **Journal**: None
- **Summary**: Non-referential face image quality assessment methods have gained popularity as a pre-filtering step on face recognition systems. In most of them, the quality score is usually designed with face matching in mind. However, a small amount of work has been done on measuring their impact and usefulness on Presentation Attack Detection (PAD). In this paper, we study the effect of quality assessment methods on filtering bona fide and attack samples, their impact on PAD systems, and how the performance of such systems is improved when training on a filtered (by quality) dataset. On a Vision Transformer PAD algorithm, a reduction of 20% of the training dataset by removing lower quality samples allowed us to improve the BPCER by 3% in a cross-dataset test.



### Learning Second Order Local Anomaly for General Face Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.15490v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15490v1)
- **Published**: 2022-09-30 14:27:11+00:00
- **Updated**: 2022-09-30 14:27:11+00:00
- **Authors**: Jianwei Fei, Yunshu Dai, Peipeng Yu, Tianrun Shen, Zhihua Xia, Jian Weng
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a novel method to improve the generalization ability of CNN-based face forgery detectors. Our method considers the feature anomalies of forged faces caused by the prevalent blending operations in face forgery algorithms. Specifically, we propose a weakly supervised Second Order Local Anomaly (SOLA) learning module to mine anomalies in local regions using deep feature maps. SOLA first decomposes the neighborhood of local features by different directions and distances and then calculates the first and second order local anomaly maps which provide more general forgery traces for the classifier. We also propose a Local Enhancement Module (LEM) to improve the discrimination between local features of real and forged regions, so as to ensure accuracy in calculating anomalies. Besides, an improved Adaptive Spatial Rich Model (ASRM) is introduced to help mine subtle noise features via learnable high pass filters. With neither pixel level annotations nor external synthetic data, our method using a simple ResNet18 backbone achieves competitive performances compared with state-of-the-art works when evaluated on unseen forgeries.



### A Closer Look at Temporal Ordering in the Segmentation of Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/2209.15501v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15501v2)
- **Published**: 2022-09-30 14:44:19+00:00
- **Updated**: 2022-10-07 18:04:47+00:00
- **Authors**: Anil Batra, Shreyank N Gowda, Frank Keller, Laura Sevilla-Lara
- **Comment**: Accepted at BMVC 2022
- **Journal**: None
- **Summary**: Understanding the steps required to perform a task is an important skill for AI systems. Learning these steps from instructional videos involves two subproblems: (i) identifying the temporal boundary of sequentially occurring segments and (ii) summarizing these steps in natural language. We refer to this task as Procedure Segmentation and Summarization (PSS). In this paper, we take a closer look at PSS and propose three fundamental improvements over current methods. The segmentation task is critical, as generating a correct summary requires each step of the procedure to be correctly identified. However, current segmentation metrics often overestimate the segmentation quality because they do not consider the temporal order of segments. In our first contribution, we propose a new segmentation metric that takes into account the order of segments, giving a more reliable measure of the accuracy of a given predicted segmentation. Current PSS methods are typically trained by proposing segments, matching them with the ground truth and computing a loss. However, much like segmentation metrics, existing matching algorithms do not consider the temporal order of the mapping between candidate segments and the ground truth. In our second contribution, we propose a matching algorithm that constrains the temporal order of segment mapping, and is also differentiable. Lastly, we introduce multi-modal feature training for PSS, which further improves segmentation. We evaluate our approach on two instructional video datasets (YouCook2 and Tasty) and observe an improvement over the state-of-the-art of $\sim7\%$ and $\sim2.5\%$ for procedure segmentation and summarization, respectively.



### Sphere-Guided Training of Neural Implicit Surfaces
- **Arxiv ID**: http://arxiv.org/abs/2209.15511v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.15511v2)
- **Published**: 2022-09-30 15:00:03+00:00
- **Updated**: 2023-04-13 13:03:58+00:00
- **Authors**: Andreea Dogaru, Andrei Timotei Ardelean, Savva Ignatyev, Egor Zakharov, Evgeny Burnaev
- **Comment**: Accepted at CVPR 2023
- **Journal**: None
- **Summary**: In recent years, neural distance functions trained via volumetric ray marching have been widely adopted for multi-view 3D reconstruction. These methods, however, apply the ray marching procedure for the entire scene volume, leading to reduced sampling efficiency and, as a result, lower reconstruction quality in the areas of high-frequency details. In this work, we address this problem via joint training of the implicit function and our new coarse sphere-based surface reconstruction. We use the coarse representation to efficiently exclude the empty volume of the scene from the volumetric ray marching procedure without additional forward passes of the neural surface network, which leads to an increased fidelity of the reconstructions compared to the base systems. We evaluate our approach by incorporating it into the training procedures of several implicit surface modeling methods and observe uniform improvements across both synthetic and real-world datasets. Our codebase can be accessed via the project page: https://andreeadogaru.github.io/SphereGuided



### Medical Image Understanding with Pretrained Vision Language Models: A Comprehensive Study
- **Arxiv ID**: http://arxiv.org/abs/2209.15517v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15517v2)
- **Published**: 2022-09-30 15:06:13+00:00
- **Updated**: 2023-02-07 16:11:42+00:00
- **Authors**: Ziyuan Qin, Huahui Yi, Qicheng Lao, Kang Li
- **Comment**: Accepted to ICLR2023
- **Journal**: None
- **Summary**: The large-scale pre-trained vision language models (VLM) have shown remarkable domain transfer capability on natural images. However, it remains unknown whether this capability can also apply to the medical image domain. This paper thoroughly studies the knowledge transferability of pre-trained VLMs to the medical domain, where we show that well-designed medical prompts are the key to elicit knowledge from pre-trained VLMs. We demonstrate that by prompting with expressive attributes that are shared between domains, the VLM can carry the knowledge across domains and improve its generalization. This mechanism empowers VLMs to recognize novel objects with fewer or without image samples. Furthermore, to avoid the laborious manual designing process, we develop three approaches for automatic generation of medical prompts, which can inject expert-level medical knowledge and image-specific information into the prompts for fine-grained grounding. We conduct extensive experiments on thirteen different medical datasets across various modalities, showing that our well-designed prompts greatly improve the zero-shot performance compared to the default prompts, and our fine-tuned models surpass the supervised models by a significant margin.



### Slimmable Networks for Contrastive Self-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.15525v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15525v2)
- **Published**: 2022-09-30 15:15:05+00:00
- **Updated**: 2023-05-23 12:20:31+00:00
- **Authors**: Shuai Zhao, Xiaohan Wang, Linchao Zhu, Yi Yang
- **Comment**: preprint,work in progress
- **Journal**: None
- **Summary**: Self-supervised learning makes significant progress in pre-training large models, but struggles with small models. Previous solutions to this problem rely mainly on knowledge distillation, which involves a two-stage procedure: first training a large teacher model and then distilling it to improve the generalization ability of smaller ones. In this work, we present a one-stage solution to obtain pre-trained small models without the need for extra teachers, namely, slimmable networks for contrastive self-supervised learning (\emph{SlimCLR}). A slimmable network consists of a full network and several weight-sharing sub-networks, which can be pre-trained once to obtain various networks, including small ones with low computation costs. However, interference between weight-sharing networks leads to severe performance degradation in self-supervised cases, as evidenced by \emph{gradient magnitude imbalance} and \emph{gradient direction divergence}. The former indicates that a small proportion of parameters produce dominant gradients during backpropagation, while the main parameters may not be fully optimized. The latter shows that the gradient direction is disordered, and the optimization process is unstable. To address these issues, we introduce three techniques to make the main parameters produce dominant gradients and sub-networks have consistent outputs. These techniques include slow start training of sub-networks, online distillation, and loss re-weighting according to model sizes. Furthermore, theoretical results are presented to demonstrate that a single slimmable linear layer is sub-optimal during linear evaluation. Thus a switchable linear probe layer is applied during linear evaluation. We instantiate SlimCLR with typical contrastive learning frameworks and achieve better performance than previous arts with fewer parameters and FLOPs.



### TT-NF: Tensor Train Neural Fields
- **Arxiv ID**: http://arxiv.org/abs/2209.15529v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2209.15529v1)
- **Published**: 2022-09-30 15:17:39+00:00
- **Updated**: 2022-09-30 15:17:39+00:00
- **Authors**: Anton Obukhov, Mikhail Usvyatsov, Christos Sakaridis, Konrad Schindler, Luc Van Gool
- **Comment**: Preprint, under review
- **Journal**: None
- **Summary**: Learning neural fields has been an active topic in deep learning research, focusing, among other issues, on finding more compact and easy-to-fit representations. In this paper, we introduce a novel low-rank representation termed Tensor Train Neural Fields (TT-NF) for learning neural fields on dense regular grids and efficient methods for sampling from them. Our representation is a TT parameterization of the neural field, trained with backpropagation to minimize a non-convex objective. We analyze the effect of low-rank compression on the downstream task quality metrics in two settings. First, we demonstrate the efficiency of our method in a sandbox task of tensor denoising, which admits comparison with SVD-based schemes designed to minimize reconstruction error. Furthermore, we apply the proposed approach to Neural Radiance Fields, where the low-rank structure of the field corresponding to the best quality can be discovered only through learning.



### The More Secure, The Less Equally Usable: Gender and Ethnicity (Un)fairness of Deep Face Recognition along Security Thresholds
- **Arxiv ID**: http://arxiv.org/abs/2209.15550v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15550v1)
- **Published**: 2022-09-30 15:51:52+00:00
- **Updated**: 2022-09-30 15:51:52+00:00
- **Authors**: Andrea Atzori, Gianni Fenu, Mirko Marras
- **Comment**: Accepted as a full paper at the 2nd International Workshop on
  Artificial Intelligence Methods for Smart Cities (AISC 2022)
- **Journal**: None
- **Summary**: Face biometrics are playing a key role in making modern smart city applications more secure and usable. Commonly, the recognition threshold of a face recognition system is adjusted based on the degree of security for the considered use case. The likelihood of a match can be for instance decreased by setting a high threshold in case of a payment transaction verification. Prior work in face recognition has unfortunately showed that error rates are usually higher for certain demographic groups. These disparities have hence brought into question the fairness of systems empowered with face biometrics. In this paper, we investigate the extent to which disparities among demographic groups change under different security levels. Our analysis includes ten face recognition models, three security thresholds, and six demographic groups based on gender and ethnicity. Experiments show that the higher the security of the system is, the higher the disparities in usability among demographic groups are. Compelling unfairness issues hence exist and urge countermeasures in real-world high-stakes environments requiring severe security levels.



### Towards a Unified View of Affinity-Based Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2209.15555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15555v1)
- **Published**: 2022-09-30 16:12:25+00:00
- **Updated**: 2022-09-30 16:12:25+00:00
- **Authors**: Vladimir Li, Atsuto Maki
- **Comment**: None
- **Journal**: None
- **Summary**: Knowledge transfer between artificial neural networks has become an important topic in deep learning. Among the open questions are what kind of knowledge needs to be preserved for the transfer, and how it can be effectively achieved. Several recent work have shown good performance of distillation methods using relation-based knowledge. These algorithms are extremely attractive in that they are based on simple inter-sample similarities. Nevertheless, a proper metric of affinity and use of it in this context is far from well understood. In this paper, by explicitly modularising knowledge distillation into a framework of three components, i.e. affinity, normalisation, and loss, we give a unified treatment of these algorithms as well as study a number of unexplored combinations of the modules. With this framework we perform extensive evaluations of numerous distillation objectives for image classification, and obtain a few useful insights for effective design choices while demonstrating how relation-based knowledge distillation could achieve comparable performance to the state of the art in spite of the simplicity.



### Automated Characterization of Catalytically Active Inclusion Body Production in Biotechnological Screening Systems
- **Arxiv ID**: http://arxiv.org/abs/2209.15584v1
- **DOI**: 10.1109/EMBC48229.2022.9871325
- **Categories**: **q-bio.QM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.15584v1)
- **Published**: 2022-09-30 16:53:16+00:00
- **Updated**: 2022-09-30 16:53:16+00:00
- **Authors**: Karina Ruzaeva, Kira Küsters, Wolfgang Wiechert, Benjamin Berkels, Marco Oldiges, Katharina Nöh
- **Comment**: None
- **Journal**: 2022 IEEE 44th International Engineering in Medicine and Biology
  Conference (EMBC)
- **Summary**: We here propose an automated pipeline for the microscopy image-based characterization of catalytically active inclusion bodies (CatIBs), which includes a fully automatic experimental high-throughput workflow combined with a hybrid approach for multi-object microbial cell segmentation. For automated microscopy, a CatIB producer strain was cultivated in a microbioreactor from which samples were injected into a flow chamber. The flow chamber was fixed under a microscope and an integrated camera took a series of images per sample. To explore heterogeneity of CatIB development during the cultivation and track the size and quantity of CatIBs over time, a hybrid image processing pipeline approach was developed, which combines an ML-based detection of in-focus cells with model-based segmentation. The experimental setup in combination with an automated image analysis unlocks high-throughput screening of CatIB production, saving time and resources.   Biotechnological relevance - CatIBs have wide application in synthetic chemistry and biocatalysis, but also could have future biomedical applications such as therapeutics. The proposed hybrid automatic image processing pipeline can be adjusted to treat comparable biological microorganisms, where fully data-driven ML-based segmentation approaches are not feasible due to the lack of training data. Our work is the first step towards image-based bioprocess control.



### Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods
- **Arxiv ID**: http://arxiv.org/abs/2209.15589v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.15589v4)
- **Published**: 2022-09-30 17:04:55+00:00
- **Updated**: 2022-10-18 21:46:25+00:00
- **Authors**: Skanda Koppula, Yazhe Li, Evan Shelhamer, Andrew Jaegle, Nikhil Parthasarathy, Relja Arandjelovic, João Carreira, Olivier Hénaff
- **Comment**: 11 pages. 36th Conference on Neural Information Processing Systems,
  Workshop on Self-Supervised Learning (2022)
- **Journal**: None
- **Summary**: Self-supervised methods have achieved remarkable success in transfer learning, often achieving the same or better accuracy than supervised pre-training. Most prior work has done so by increasing pre-training computation by adding complex data augmentation, multiple views, or lengthy training schedules. In this work, we investigate a related, but orthogonal question: given a fixed FLOP budget, what are the best datasets, models, and (self-)supervised training methods for obtaining high accuracy on representative visual tasks? Given the availability of large datasets, this setting is often more relevant for both academic and industry labs alike. We examine five large-scale datasets (JFT-300M, ALIGN, ImageNet-1K, ImageNet-21K, and COCO) and six pre-training methods (CLIP, DINO, SimCLR, BYOL, Masked Autoencoding, and supervised). In a like-for-like fashion, we characterize their FLOP and CO$_2$ footprints, relative to their accuracy when transferred to a canonical image segmentation task. Our analysis reveals strong disparities in the computational efficiency of pre-training methods and their dependence on dataset quality. In particular, our results call into question the commonly-held assumption that self-supervised methods inherently scale to large, uncurated data. We therefore advocate for (1) paying closer attention to dataset curation and (2) reporting of accuracies in context of the total computational cost.



### Combining Efficient and Precise Sign Language Recognition: Good pose estimation library is all you need
- **Arxiv ID**: http://arxiv.org/abs/2210.00893v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/2210.00893v1)
- **Published**: 2022-09-30 17:30:32+00:00
- **Updated**: 2022-09-30 17:30:32+00:00
- **Authors**: Matyáš Boháček, Zhuo Cao, Marek Hrúz
- **Comment**: 5 pages, 2 figures, CVPR 2022 AVA workshop extended abstract
- **Journal**: None
- **Summary**: Sign language recognition could significantly improve the user experience for d/Deaf people with the general consumer technology, such as IoT devices or videoconferencing. However, current sign language recognition architectures are usually computationally heavy and require robust GPU-equipped hardware to run in real-time. Some models aim for lower-end devices (such as smartphones) by minimizing their size and complexity, which leads to worse accuracy. This highly scrutinizes accurate in-the-wild applications. We build upon the SPOTER architecture, which belongs to the latter group of light methods, as it came close to the performance of large models employed for this task. By substituting its original third-party pose estimation module with the MediaPipe library, we achieve an overall state-of-the-art result on the WLASL100 dataset. Significantly, our method beats previous larger architectures while still being twice as computationally efficient and almost $11$ times faster on inference when compared to a relevant benchmark. To demonstrate our method's combined efficiency and precision, we built an online demo that enables users to translate sign lemmas of American sign language in their browsers. This is the first publicly available online application demonstrating this task to the best of our knowledge.



### Bias Mimicking: A Simple Sampling Approach for Bias Mitigation
- **Arxiv ID**: http://arxiv.org/abs/2209.15605v8
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15605v8)
- **Published**: 2022-09-30 17:33:00+00:00
- **Updated**: 2023-04-27 17:29:44+00:00
- **Authors**: Maan Qraitem, Kate Saenko, Bryan A. Plummer
- **Comment**: Accepted at CVPR 2023
- **Journal**: None
- **Summary**: Prior work has shown that Visual Recognition datasets frequently underrepresent bias groups $B$ (\eg Female) within class labels $Y$ (\eg Programmers). This dataset bias can lead to models that learn spurious correlations between class labels and bias groups such as age, gender, or race. Most recent methods that address this problem require significant architectural changes or additional loss functions requiring more hyper-parameter tuning. Alternatively, data sampling baselines from the class imbalance literature (\eg Undersampling, Upweighting), which can often be implemented in a single line of code and often have no hyperparameters, offer a cheaper and more efficient solution. However, these methods suffer from significant shortcomings. For example, Undersampling drops a significant part of the input distribution per epoch while Oversampling repeats samples, causing overfitting. To address these shortcomings, we introduce a new class-conditioned sampling method: Bias Mimicking. The method is based on the observation that if a class $c$ bias distribution, \ie $P_D(B|Y=c)$ is mimicked across every $c^{\prime}\neq c$, then $Y$ and $B$ are statistically independent. Using this notion, BM, through a novel training procedure, ensures that the model is exposed to the entire distribution per epoch without repeating samples. Consequently, Bias Mimicking improves underrepresented groups' accuracy of sampling methods by 3\% over four benchmarks while maintaining and sometimes improving performance over nonsampling methods. Code: \url{https://github.com/mqraitem/Bias-Mimicking}



### Towards Multi-spatiotemporal-scale Generalized PDE Modeling
- **Arxiv ID**: http://arxiv.org/abs/2209.15616v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.15616v2)
- **Published**: 2022-09-30 17:40:05+00:00
- **Updated**: 2022-11-15 23:07:10+00:00
- **Authors**: Jayesh K. Gupta, Johannes Brandstetter
- **Comment**: None
- **Journal**: None
- **Summary**: Partial differential equations (PDEs) are central to describing complex physical system simulations. Their expensive solution techniques have led to an increased interest in deep neural network based surrogates. However, the practical utility of training such surrogates is contingent on their ability to model complex multi-scale spatio-temporal phenomena. Various neural network architectures have been proposed to target such phenomena, most notably Fourier Neural Operators (FNOs), which give a natural handle over local & global spatial information via parameterization of different Fourier modes, and U-Nets which treat local and global information via downsampling and upsampling paths. However, generalizing across different equation parameters or time-scales still remains a challenge. In this work, we make a comprehensive comparison between various FNO, ResNet, and U-Net like approaches to fluid mechanics problems in both vorticity-stream and velocity function form. For U-Nets, we transfer recent architectural improvements from computer vision, most notably from object segmentation and generative modeling. We further analyze the design considerations for using FNO layers to improve performance of U-Net architectures without major degradation of computational cost. Finally, we show promising results on generalization to different PDE parameters and time-scales with a single surrogate model. Source code for our PyTorch benchmark framework is available at https://github.com/microsoft/pdearena.



### Point normal orientation and surface reconstruction by incorporating isovalue constraints to Poisson equation
- **Arxiv ID**: http://arxiv.org/abs/2209.15619v3
- **DOI**: 10.1016/j.cagd.2023.102195
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.15619v3)
- **Published**: 2022-09-30 17:47:48+00:00
- **Updated**: 2023-04-30 14:01:45+00:00
- **Authors**: Dong Xiao, Zuoqiang Shi, Siyu Li, Bailin Deng, Bin Wang
- **Comment**: Accepted by Computer Aided Geometric Design from GMP 2023
- **Journal**: None
- **Summary**: Oriented normals are common pre-requisites for many geometric algorithms based on point clouds, such as Poisson surface reconstruction. However, it is not trivial to obtain a consistent orientation. In this work, we bridge orientation and reconstruction in the implicit space and propose a novel approach to orient point cloud normals by incorporating isovalue constraints to the Poisson equation. In implicit surface reconstruction, the reconstructed shape is represented as an isosurface of an implicit function defined in the ambient space. Therefore, when such a surface is reconstructed from a set of sample points, the implicit function values at the points should be close to the isovalue corresponding to the surface. Based on this observation and the Poisson equation, we propose an optimization formulation that combines isovalue constraints with local consistency requirements for normals. We optimize normals and implicit functions simultaneously and solve for a globally consistent orientation. Thanks to the sparsity of the linear system, our method can work on an average laptop with reasonable computational time. Experiments show that our method can achieve high performance in non-uniform and noisy data and manage varying sampling densities, artifacts, multiple connected components, and nested surfaces. The source code is available at \url{https://github.com/Submanifold/IsoConstraints}.



### ExtrudeNet: Unsupervised Inverse Sketch-and-Extrude for Shape Parsing
- **Arxiv ID**: http://arxiv.org/abs/2209.15632v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.15632v1)
- **Published**: 2022-09-30 17:58:11+00:00
- **Updated**: 2022-09-30 17:58:11+00:00
- **Authors**: Daxuan Ren, Jianmin Zheng, Jianfei Cai, Jiatong Li, Junzhe Zhang
- **Comment**: Accepted to ECCV 2022
- **Journal**: None
- **Summary**: Sketch-and-extrude is a common and intuitive modeling process in computer aided design. This paper studies the problem of learning the shape given in the form of point clouds by inverse sketch-and-extrude. We present ExtrudeNet, an unsupervised end-to-end network for discovering sketch and extrude from point clouds. Behind ExtrudeNet are two new technical components: 1) an effective representation for sketch and extrude, which can model extrusion with freeform sketches and conventional cylinder and box primitives as well; and 2) a numerical method for computing the signed distance field which is used in the network learning. This is the first attempt that uses machine learning to reverse engineer the sketch-and-extrude modeling process of a shape in an unsupervised fashion. ExtrudeNet not only outputs a compact, editable and interpretable representation of the shape that can be seamlessly integrated into modern CAD software, but also aligns with the standard CAD modeling process facilitating various editing applications, which distinguishes our work from existing shape parsing research. Code is released at https://github.com/kimren227/ExtrudeNet.



### Improving 3D-aware Image Synthesis with A Geometry-aware Discriminator
- **Arxiv ID**: http://arxiv.org/abs/2209.15637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15637v1)
- **Published**: 2022-09-30 17:59:37+00:00
- **Updated**: 2022-09-30 17:59:37+00:00
- **Authors**: Zifan Shi, Yinghao Xu, Yujun Shen, Deli Zhao, Qifeng Chen, Dit-Yan Yeung
- **Comment**: Accepted by NeurIPS 2022. Project page:
  https://vivianszf.github.io/geod
- **Journal**: None
- **Summary**: 3D-aware image synthesis aims at learning a generative model that can render photo-realistic 2D images while capturing decent underlying 3D shapes. A popular solution is to adopt the generative adversarial network (GAN) and replace the generator with a 3D renderer, where volume rendering with neural radiance field (NeRF) is commonly used. Despite the advancement of synthesis quality, existing methods fail to obtain moderate 3D shapes. We argue that, considering the two-player game in the formulation of GANs, only making the generator 3D-aware is not enough. In other words, displacing the generative mechanism only offers the capability, but not the guarantee, of producing 3D-aware images, because the supervision of the generator primarily comes from the discriminator. To address this issue, we propose GeoD through learning a geometry-aware discriminator to improve 3D-aware GANs. Concretely, besides differentiating real and fake samples from the 2D image space, the discriminator is additionally asked to derive the geometry information from the inputs, which is then applied as the guidance of the generator. Such a simple yet effective design facilitates learning substantially more accurate 3D shapes. Extensive experiments on various generator architectures and training datasets verify the superiority of GeoD over state-of-the-art alternatives. Moreover, our approach is registered as a general framework such that a more capable discriminator (i.e., with a third task of novel view synthesis beyond domain classification and geometry extraction) can further assist the generator with a better multi-view consistency.



### F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models
- **Arxiv ID**: http://arxiv.org/abs/2209.15639v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.15639v2)
- **Published**: 2022-09-30 17:59:52+00:00
- **Updated**: 2023-02-23 19:14:52+00:00
- **Authors**: Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, Anelia Angelova
- **Comment**: Accepted to ICLR 2023 (https://iclr.cc/Conferences/2023). 20 pages, 7
  figures
- **Journal**: None
- **Summary**: We present F-VLM, a simple open-vocabulary object detection method built upon Frozen Vision and Language Models. F-VLM simplifies the current multi-stage training pipeline by eliminating the need for knowledge distillation or detection-tailored pretraining. Surprisingly, we observe that a frozen VLM: 1) retains the locality-sensitive features necessary for detection, and 2) is a strong region classifier. We finetune only the detector head and combine the detector and VLM outputs for each region at inference time. F-VLM shows compelling scaling behavior and achieves +6.5 mask AP improvement over the previous state of the art on novel categories of LVIS open-vocabulary detection benchmark. In addition, we demonstrate very competitive results on COCO open-vocabulary detection benchmark and cross-dataset transfer detection, in addition to significant training speed-up and compute savings. Code will be released at the https://sites.google.com/view/f-vlm/home



### VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training
- **Arxiv ID**: http://arxiv.org/abs/2210.00030v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.00030v2)
- **Published**: 2022-09-30 18:14:07+00:00
- **Updated**: 2023-03-07 02:29:59+00:00
- **Authors**: Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, Amy Zhang
- **Comment**: ICLR 2023, Notable-Top-25% (Spotlight). Project website:
  https://sites.google.com/view/vip-rl
- **Journal**: None
- **Summary**: Reward and representation learning are two long-standing challenges for learning an expanding set of robot manipulation skills from sensory observations. Given the inherent cost and scarcity of in-domain, task-specific robot data, learning from large, diverse, offline human videos has emerged as a promising path towards acquiring a generally useful visual representation for control; however, how these human videos can be used for general-purpose reward learning remains an open question. We introduce $\textbf{V}$alue-$\textbf{I}$mplicit $\textbf{P}$re-training (VIP), a self-supervised pre-trained visual representation capable of generating dense and smooth reward functions for unseen robotic tasks. VIP casts representation learning from human videos as an offline goal-conditioned reinforcement learning problem and derives a self-supervised dual goal-conditioned value-function objective that does not depend on actions, enabling pre-training on unlabeled human videos. Theoretically, VIP can be understood as a novel implicit time contrastive objective that generates a temporally smooth embedding, enabling the value function to be implicitly defined via the embedding distance, which can then be used to construct the reward for any goal-image specified downstream task. Trained on large-scale Ego4D human videos and without any fine-tuning on in-domain, task-specific data, VIP's frozen representation can provide dense visual reward for an extensive set of simulated and $\textbf{real-robot}$ tasks, enabling diverse reward-based visual control methods and significantly outperforming all prior pre-trained representations. Notably, VIP can enable simple, $\textbf{few-shot}$ offline RL on a suite of real-world robot tasks with as few as 20 trajectories.



### Differentially Private Bias-Term only Fine-tuning of Foundation Models
- **Arxiv ID**: http://arxiv.org/abs/2210.00036v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.00036v2)
- **Published**: 2022-09-30 18:30:48+00:00
- **Updated**: 2022-10-04 17:51:09+00:00
- **Authors**: Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, George Karypis
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of differentially private (DP) fine-tuning of large pre-trained models -- a recent privacy-preserving approach suitable for solving downstream tasks with sensitive data. Existing work has demonstrated that high accuracy is possible under strong privacy constraint, yet requires significant computational overhead or modifications to the network architecture.   We propose differentially private bias-term fine-tuning (DP-BiTFiT), which matches the state-of-the-art accuracy for DP algorithms and the efficiency of the standard BiTFiT. DP-BiTFiT is model agnostic (not modifying the network architecture), parameter efficient (only training about $0.1\%$ of the parameters), and computation efficient (almost removing the overhead caused by DP, in both the time and space complexity). On a wide range of tasks, DP-BiTFiT is $2\sim 30\times$ faster and uses $2\sim 8\times$ less memory than DP full fine-tuning, even faster than the standard full fine-tuning. This amazing efficiency enables us to conduct DP fine-tuning on language and vision tasks with long-sequence texts and high-resolution images, which were computationally difficult using existing methods.



### Differentially Private Optimization on Large Model at Small Cost
- **Arxiv ID**: http://arxiv.org/abs/2210.00038v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.00038v1)
- **Published**: 2022-09-30 18:38:53+00:00
- **Updated**: 2022-09-30 18:38:53+00:00
- **Authors**: Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, George Karypis
- **Comment**: None
- **Journal**: None
- **Summary**: Differentially private (DP) optimization is the standard paradigm to learn large neural networks that are accurate and privacy-preserving. The computational cost for DP deep learning, however, is notoriously heavy due to the per-sample gradient clipping. Existing DP implementations are $2-1000\times$ more costly in time and space complexity than the standard (non-private) training. In this work, we develop a novel Book-Keeping (BK) technique that implements existing DP optimizers (thus achieving the same accuracy), with a substantial improvement on the computational cost. Specifically, BK enables DP training on large models and high dimensional data to be roughly as efficient as the standard training, whereas previous DP algorithms can be inefficient or incapable of training due to memory error. The computational advantage of BK is supported by the complexity analysis as well as extensive experiments on vision and language tasks. Our implementation achieves state-of-the-art (SOTA) accuracy with very small extra cost: on GPT2 and at the same memory cost, BK has 1.0$\times$ the time complexity of the standard training (0.75$\times$ training speed in practice), and 0.6$\times$ the time complexity of the most efficient DP implementation (1.24$\times$ training speed in practice). We will open-source the codebase for the BK algorithm.



### MaskTune: Mitigating Spurious Correlations by Forcing to Explore
- **Arxiv ID**: http://arxiv.org/abs/2210.00055v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.00055v2)
- **Published**: 2022-09-30 19:36:12+00:00
- **Updated**: 2022-10-08 19:38:28+00:00
- **Authors**: Saeid Asgari Taghanaki, Aliasghar Khani, Fereshte Khani, Ali Gholami, Linh Tran, Ali Mahdavi-Amiri, Ghassan Hamarneh
- **Comment**: Accepted to NeurIPS 2022
- **Journal**: None
- **Summary**: A fundamental challenge of over-parameterized deep learning models is learning meaningful data representations that yield good performance on a downstream task without over-fitting spurious input features. This work proposes MaskTune, a masking strategy that prevents over-reliance on spurious (or a limited number of) features. MaskTune forces the trained model to explore new features during a single epoch finetuning by masking previously discovered features. MaskTune, unlike earlier approaches for mitigating shortcut learning, does not require any supervision, such as annotating spurious features or labels for subgroup samples in a dataset. Our empirical results on biased MNIST, CelebA, Waterbirds, and ImagenNet-9L datasets show that MaskTune is effective on tasks that often suffer from the existence of spurious correlations. Finally, we show that MaskTune outperforms or achieves similar performance to the competing methods when applied to the selective classification (classification with rejection option) task. Code for MaskTune is available at https://github.com/aliasgharkhani/Masktune.



### D-Align: Dual Query Co-attention Network for 3D Object Detection Based on Multi-frame Point Cloud Sequence
- **Arxiv ID**: http://arxiv.org/abs/2210.00087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00087v1)
- **Published**: 2022-09-30 20:41:25+00:00
- **Updated**: 2022-09-30 20:41:25+00:00
- **Authors**: Junhyung Lee, Junho Koh, Youngwoo Lee, Jun Won Choi
- **Comment**: None
- **Journal**: None
- **Summary**: LiDAR sensors are widely used for 3D object detection in various mobile robotics applications. LiDAR sensors continuously generate point cloud data in real-time. Conventional 3D object detectors detect objects using a set of points acquired over a fixed duration. However, recent studies have shown that the performance of object detection can be further enhanced by utilizing spatio-temporal information obtained from point cloud sequences. In this paper, we propose a new 3D object detector, named D-Align, which can effectively produce strong bird's-eye-view (BEV) features by aligning and aggregating the features obtained from a sequence of point sets. The proposed method includes a novel dual-query co-attention network that uses two types of queries, including target query set (T-QS) and support query set (S-QS), to update the features of target and support frames, respectively. D-Align aligns S-QS to T-QS based on the temporal context features extracted from the adjacent feature maps and then aggregates S-QS with T-QS using a gated attention mechanism. The dual queries are updated through multiple attention layers to progressively enhance the target frame features used to produce the detection results. Our experiments on the nuScenes dataset show that the proposed D-Align method greatly improved the performance of a single frame-based baseline method and significantly outperformed the latest 3D object detectors.



### Federated Training of Dual Encoding Models on Small Non-IID Client Datasets
- **Arxiv ID**: http://arxiv.org/abs/2210.00092v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.00092v2)
- **Published**: 2022-09-30 21:03:43+00:00
- **Updated**: 2023-04-10 18:27:06+00:00
- **Authors**: Raviteja Vemulapalli, Warren Richard Morningstar, Philip Andrew Mansfield, Hubert Eichner, Karan Singhal, Arash Afkanpour, Bradley Green
- **Comment**: ICLR 2023 Workshop on Pitfalls of Limited Data and Computation for
  Trustworthy ML
- **Journal**: None
- **Summary**: Dual encoding models that encode a pair of inputs are widely used for representation learning. Many approaches train dual encoding models by maximizing agreement between pairs of encodings on centralized training data. However, in many scenarios, datasets are inherently decentralized across many clients (user devices or organizations) due to privacy concerns, motivating federated learning. In this work, we focus on federated training of dual encoding models on decentralized data composed of many small, non-IID (independent and identically distributed) client datasets. We show that existing approaches that work well in centralized settings perform poorly when naively adapted to this setting using federated averaging. We observe that, we can simulate large-batch loss computation on individual clients for loss functions that are based on encoding statistics. Based on this insight, we propose a novel federated training approach, Distributed Cross Correlation Optimization (DCCO), which trains dual encoding models using encoding statistics aggregated across clients, without sharing individual data samples. Our experimental results on two datasets demonstrate that the proposed DCCO approach outperforms federated variants of existing approaches by a large margin.



### Adaptive Weight Decay: On The Fly Weight Decay Tuning for Improving Robustness
- **Arxiv ID**: http://arxiv.org/abs/2210.00094v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.00094v1)
- **Published**: 2022-09-30 21:13:00+00:00
- **Updated**: 2022-09-30 21:13:00+00:00
- **Authors**: Amin Ghiasi, Ali Shafahi, Reza Ardekani
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce adaptive weight decay, which automatically tunes the hyper-parameter for weight decay during each training iteration. For classification problems, we propose changing the value of the weight decay hyper-parameter on the fly based on the strength of updates from the classification loss (i.e., gradient of cross-entropy), and the regularization loss (i.e., $\ell_2$-norm of the weights). We show that this simple modification can result in large improvements in adversarial robustness -- an area which suffers from robust overfitting -- without requiring extra data. Specifically, our reformulation results in 20% relative robustness improvement for CIFAR-100, and 10% relative robustness improvement on CIFAR-10 comparing to traditional weight decay. In addition, this method has other desirable properties, such as less sensitivity to learning rate, and smaller weight norms, which the latter contributes to robustness to overfitting to label noise, and pruning.



### Image-Based Detection of Modifications in Gas Pump PCBs with Deep Convolutional Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/2210.00100v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.00100v2)
- **Published**: 2022-09-30 21:31:47+00:00
- **Updated**: 2022-10-07 00:15:00+00:00
- **Authors**: Diulhio Candido de Oliveira, Bogdan Tomoyuki Nassu, Marco Aurelio Wehrmeister
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce an approach for detecting modifications in assembled printed circuit boards based on photographs taken without tight control over perspective and illumination conditions. One instance of this problem is the visual inspection of gas pumps PCBs, which can be modified by fraudsters wishing to deceive costumers or evade taxes. Given the uncontrolled environment and the huge number of possible modifications, we address the problem as a case of anomaly detection, proposing an approach that is directed towards the characteristics of that scenario, while being well-suited for other similar applications. The proposed approach employs a deep convolutional autoencoder trained to reconstruct images of an unmodified board, but which remains unable to do the same for images showing modifications. By comparing the input image with its reconstruction, it is possible to segment anomalies and modifications in a pixel-wise manner. Experiments performed on a dataset built to represent real-world situations (and which we will make publicly available) show that our approach outperforms other state-of-the-art approaches for anomaly segmentation in the considered scenario, while producing comparable results on the popular MVTec-AD dataset for a more general object anomaly detection task.



### Contrastive Corpus Attribution for Explaining Representations
- **Arxiv ID**: http://arxiv.org/abs/2210.00107v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.00107v2)
- **Published**: 2022-09-30 21:59:10+00:00
- **Updated**: 2023-06-12 22:23:56+00:00
- **Authors**: Chris Lin, Hugh Chen, Chanwoo Kim, Su-In Lee
- **Comment**: Updated for the final camera-ready version of ICLR 2023
- **Journal**: None
- **Summary**: Despite the widespread use of unsupervised models, very few methods are designed to explain them. Most explanation methods explain a scalar model output. However, unsupervised models output representation vectors, the elements of which are not good candidates to explain because they lack semantic meaning. To bridge this gap, recent works defined a scalar explanation output: a dot product-based similarity in the representation space to the sample being explained (i.e., an explicand). Although this enabled explanations of unsupervised models, the interpretation of this approach can still be opaque because similarity to the explicand's representation may not be meaningful to humans. To address this, we propose contrastive corpus similarity, a novel and semantically meaningful scalar explanation output based on a reference corpus and a contrasting foil set of samples. We demonstrate that contrastive corpus similarity is compatible with many post-hoc feature attribution methods to generate COntrastive COrpus Attributions (COCOA) and quantitatively verify that features important to the corpus are identified. We showcase the utility of COCOA in two ways: (i) we draw insights by explaining augmentations of the same image in a contrastive learning setting (SimCLR); and (ii) we perform zero-shot object localization by explaining the similarity of image representations to jointly learned text representations (CLIP).



### Robust Person Identification: A WiFi Vision-based Approach
- **Arxiv ID**: http://arxiv.org/abs/2210.00127v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2210.00127v1)
- **Published**: 2022-09-30 22:54:30+00:00
- **Updated**: 2022-09-30 22:54:30+00:00
- **Authors**: Yili Ren, Jie Yang
- **Comment**: 18 pages, USENIX Security '23
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) has become increasingly important as it supports a wide range of security applications. Traditional person Re-ID mainly relies on optical camera-based systems, which incur several limitations due to the changes in the appearance of people, occlusions, and human poses. In this work, we propose a WiFi vision-based system, 3D-ID, for person Re-ID in 3D space. Our system leverages the advances of WiFi and deep learning to help WiFi devices see, identify, and recognize people. In particular, we leverage multiple antennas on next-generation WiFi devices and 2D AoA estimation of the signal reflections to enable WiFi to visualize a person in the physical environment. We then leverage deep learning to digitize the visualization of the person into 3D body representation and extract both the static body shape and dynamic walking patterns for person Re-ID. Our evaluation results under various indoor environments show that the 3D-ID system achieves an overall rank-1 accuracy of 85.3%. Results also show that our system is resistant to various attacks. The proposed 3D-ID is thus very promising as it could augment or complement camera-based systems.



### An In-depth Study of Stochastic Backpropagation
- **Arxiv ID**: http://arxiv.org/abs/2210.00129v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00129v1)
- **Published**: 2022-09-30 23:05:06+00:00
- **Updated**: 2022-09-30 23:05:06+00:00
- **Authors**: Jun Fang, Mingze Xu, Hao Chen, Bing Shuai, Zhuowen Tu, Joseph Tighe
- **Comment**: NeurIPS 2022
- **Journal**: None
- **Summary**: In this paper, we provide an in-depth study of Stochastic Backpropagation (SBP) when training deep neural networks for standard image classification and object detection tasks. During backward propagation, SBP calculates the gradients by only using a subset of feature maps to save the GPU memory and computational cost. We interpret SBP as an efficient way to implement stochastic gradient decent by performing backpropagation dropout, which leads to considerable memory saving and training process speedup, with a minimal impact on the overall model accuracy. We offer some good practices to apply SBP in training image recognition models, which can be adopted in learning a wide range of deep neural networks. Experiments on image classification and object detection show that SBP can save up to 40% of GPU memory with less than 1% accuracy degradation.



### Alignment-guided Temporal Attention for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2210.00132v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2210.00132v2)
- **Published**: 2022-09-30 23:10:47+00:00
- **Updated**: 2022-12-30 19:03:06+00:00
- **Authors**: Yizhou Zhao, Zhenyang Li, Xun Guo, Yan Lu
- **Comment**: Accepted by NeurIPS 2022
- **Journal**: None
- **Summary**: Temporal modeling is crucial for various video learning tasks. Most recent approaches employ either factorized (2D+1D) or joint (3D) spatial-temporal operations to extract temporal contexts from the input frames. While the former is more efficient in computation, the latter often obtains better performance. In this paper, we attribute this to a dilemma between the sufficiency and the efficiency of interactions among various positions in different frames. These interactions affect the extraction of task-relevant information shared among frames. To resolve this issue, we prove that frame-by-frame alignments have the potential to increase the mutual information between frame representations, thereby including more task-relevant information to boost effectiveness. Then we propose Alignment-guided Temporal Attention (ATA) to extend 1-dimensional temporal attention with parameter-free patch-level alignments between neighboring frames. It can act as a general plug-in for image backbones to conduct the action recognition task without any model-specific design. Extensive experiments on multiple benchmarks demonstrate the superiority and generality of our module.



### IMB-NAS: Neural Architecture Search for Imbalanced Datasets
- **Arxiv ID**: http://arxiv.org/abs/2210.00136v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2210.00136v1)
- **Published**: 2022-09-30 23:15:28+00:00
- **Updated**: 2022-09-30 23:15:28+00:00
- **Authors**: Rahul Duggal, Shengyun Peng, Hao Zhou, Duen Horng Chau
- **Comment**: None
- **Journal**: None
- **Summary**: Class imbalance is a ubiquitous phenomenon occurring in real world data distributions. To overcome its detrimental effect on training accurate classifiers, existing work follows three major directions: class re-balancing, information transfer, and representation learning. In this paper, we propose a new and complementary direction for improving performance on long tailed datasets - optimizing the backbone architecture through neural architecture search (NAS). We find that an architecture's accuracy obtained on a balanced dataset is not indicative of good performance on imbalanced ones. This poses the need for a full NAS run on long tailed datasets which can quickly become prohibitively compute intensive. To alleviate this compute burden, we aim to efficiently adapt a NAS super-network from a balanced source dataset to an imbalanced target one. Among several adaptation strategies, we find that the most effective one is to retrain the linear classification head with reweighted loss, while freezing the backbone NAS super-network trained on a balanced source dataset. We perform extensive experiments on multiple datasets and provide concrete insights to optimize architectures for long tailed datasets.



