# Arxiv Papers in cs.CV on 2022-09-27
### EEG-based Image Feature Extraction for Visual Classification using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.13090v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.13090v1)
- **Published**: 2022-09-27 00:50:56+00:00
- **Updated**: 2022-09-27 00:50:56+00:00
- **Authors**: Alankrit Mishra, Nikhil Raj, Garima Bajwa
- **Comment**: 8 pages, 4 figures, to be published in 2022 International Conference
  on Intelligent Data Science Technologies and Applications (IDSTA)
- **Journal**: None
- **Summary**: While capable of segregating visual data, humans take time to examine a single piece, let alone thousands or millions of samples. The deep learning models efficiently process sizeable information with the help of modern-day computing. However, their questionable decision-making process has raised considerable concerns. Recent studies have identified a new approach to extract image features from EEG signals and combine them with standard image features. These approaches make deep learning models more interpretable and also enables faster converging of models with fewer samples. Inspired by recent studies, we developed an efficient way of encoding EEG signals as images to facilitate a more subtle understanding of brain signals with deep learning models. Using two variations in such encoding methods, we classified the encoded EEG signals corresponding to 39 image classes with a benchmark accuracy of 70% on the layered dataset of six subjects, which is significantly higher than the existing work. Our image classification approach with combined EEG features achieved an accuracy of 82% compared to the slightly better accuracy of a pure deep learning approach; nevertheless, it demonstrates the viability of the theory.



### WaterNeRF: Neural Radiance Fields for Underwater Scenes
- **Arxiv ID**: http://arxiv.org/abs/2209.13091v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.13091v1)
- **Published**: 2022-09-27 00:53:26+00:00
- **Updated**: 2022-09-27 00:53:26+00:00
- **Authors**: Advaith Venkatramanan Sethuraman, Manikandasriram Srinivasan Ramanagopal, Katherine A. Skinner
- **Comment**: None
- **Journal**: None
- **Summary**: Underwater imaging is a critical task performed by marine robots for a wide range of applications including aquaculture, marine infrastructure inspection, and environmental monitoring. However, water column effects, such as attenuation and backscattering, drastically change the color and quality of imagery captured underwater. Due to varying water conditions and range-dependency of these effects, restoring underwater imagery is a challenging problem. This impacts downstream perception tasks including depth estimation and 3D reconstruction. In this paper, we advance state-of-the-art in neural radiance fields (NeRFs) to enable physics-informed dense depth estimation and color correction. Our proposed method, WaterNeRF, estimates parameters of a physics-based model for underwater image formation, leading to a hybrid data-driven and model-based solution. After determining the scene structure and radiance field, we can produce novel views of degraded as well as corrected underwater images, along with dense depth of the scene. We evaluate the proposed method qualitatively and quantitatively on a real underwater dataset.



### Efficient Noise Filtration of Images by Low-Rank Singular Vector Approximations of Geodesics' Gramian Matrix
- **Arxiv ID**: http://arxiv.org/abs/2209.13094v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA, 68U10, 94A08, 68T10, I.4.3; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2209.13094v3)
- **Published**: 2022-09-27 01:03:36+00:00
- **Updated**: 2022-11-22 01:52:40+00:00
- **Authors**: Kelum Gajamannage, Yonggi Park, Mallikarjunaiah Muddamallappa, Sunil Mathur
- **Comment**: 19 pages, 3 figures, submitted to ACM Transactions on Architecture
  and Code Optimization
- **Journal**: None
- **Summary**: Modern society is interested in capturing high-resolution and fine-quality images due to the surge of sophisticated cameras. However, the noise contamination in the images not only inferior people's expectations but also conversely affects the subsequent processes if such images are utilized in computer vision tasks such as remote sensing, object tracking, etc. Even though noise filtration plays an essential role, real-time processing of a high-resolution image is limited by the hardware limitations of the image-capturing instruments. Geodesic Gramian Denoising (GGD) is a manifold-based noise filtering method that we introduced in our past research which utilizes a few prominent singular vectors of the geodesics' Gramian matrix for the noise filtering process. The applicability of GDD is limited as it encounters $\mathcal{O}(n^6)$ when denoising a given image of size $n\times n$ since GGD computes the prominent singular vectors of a $n^2 \times n^2$ data matrix that is implemented by singular value decomposition (SVD). In this research, we increase the efficiency of our GGD framework by replacing its SVD step with four diverse singular vector approximation techniques. Here, we compare both the computational time and the noise filtering performance between the four techniques integrated into GGD.



### BayesNetCNN: incorporating uncertainty in neural networks for image-based classification tasks
- **Arxiv ID**: http://arxiv.org/abs/2209.13096v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.13096v1)
- **Published**: 2022-09-27 01:07:19+00:00
- **Updated**: 2022-09-27 01:07:19+00:00
- **Authors**: Matteo Ferrante, Tommaso Boccato, Nicola Toschi
- **Comment**: None
- **Journal**: None
- **Summary**: The willingness to trust predictions formulated by automatic algorithms is key in a vast number of domains. However, a vast number of deep architectures are only able to formulate predictions without an associated uncertainty. In this paper, we propose a method to convert a standard neural network into a Bayesian neural network and estimate the variability of predictions by sampling different networks similar to the original one at each forward pass. We couple our methods with a tunable rejection-based approach that employs only the fraction of the dataset that the model is able to classify with an uncertainty below a user-set threshold. We test our model in a large cohort of brain images from Alzheimer's Disease patients, where we tackle discrimination of patients from healthy controls based on morphometric images only. We demonstrate how combining the estimated uncertainty with a rejection-based approach increases classification accuracy from 0.86 to 0.95 while retaining 75% of the test set. In addition, the model can select cases to be recommended for manual evaluation based on excessive uncertainty. We believe that being able to estimate the uncertainty of a prediction, along with tools that can modulate the behavior of the network to a degree of confidence that the user is informed about (and comfortable with) can represent a crucial step in the direction of user compliance and easier integration of deep learning tools into everyday tasks currently performed by human operators.



### Simultaneous Acquisition of High Quality RGB Image and Polarization Information using a Sparse Polarization Sensor
- **Arxiv ID**: http://arxiv.org/abs/2209.13106v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13106v1)
- **Published**: 2022-09-27 01:41:58+00:00
- **Updated**: 2022-09-27 01:41:58+00:00
- **Authors**: Teppei Kurita, Yuhi Kondo, Legong Sun, Yusuke Moriuchi
- **Comment**: Accepted to IEEE Winter Conference on Applications of Computer Vision
  (WACV) 2023
- **Journal**: None
- **Summary**: This paper proposes a novel polarization sensor structure and network architecture to obtain a high-quality RGB image and polarization information. Conventional polarization sensors can simultaneously acquire RGB images and polarization information, but the polarizers on the sensor degrade the quality of the RGB images. There is a trade-off between the quality of the RGB image and polarization information as fewer polarization pixels reduce the degradation of the RGB image but decrease the resolution of polarization information. Therefore, we propose an approach that resolves the trade-off by sparsely arranging polarization pixels on the sensor and compensating for low-resolution polarization information with higher resolution using the RGB image as a guide. Our proposed network architecture consists of an RGB image refinement network and a polarization information compensation network. We confirmed the superiority of our proposed network in compensating the differential component of polarization intensity by comparing its performance with state-of-the-art methods for similar tasks: depth completion. Furthermore, we confirmed that our approach could simultaneously acquire higher quality RGB images and polarization information than conventional polarization sensors, resolving the trade-off between the quality of RGB images and polarization information. The baseline code and newly generated real and synthetic large-scale polarization image datasets are available for further research and development.



### FG-UAP: Feature-Gathering Universal Adversarial Perturbation
- **Arxiv ID**: http://arxiv.org/abs/2209.13113v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13113v1)
- **Published**: 2022-09-27 02:03:42+00:00
- **Updated**: 2022-09-27 02:03:42+00:00
- **Authors**: Zhixing Ye, Xinwen Cheng, Xiaolin Huang
- **Comment**: 27 pages, 4 figures
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are susceptible to elaborately designed perturbations, whether such perturbations are dependent or independent of images. The latter one, called Universal Adversarial Perturbation (UAP), is very attractive for model robustness analysis, since its independence of input reveals the intrinsic characteristics of the model. Relatively, another interesting observation is Neural Collapse (NC), which means the feature variability may collapse during the terminal phase of training. Motivated by this, we propose to generate UAP by attacking the layer where NC phenomenon happens. Because of NC, the proposed attack could gather all the natural images' features to its surrounding, which is hence called Feature-Gathering UAP (FG-UAP).   We evaluate the effectiveness our proposed algorithm on abundant experiments, including untargeted and targeted universal attacks, attacks under limited dataset, and transfer-based black-box attacks among different architectures including Vision Transformers, which are believed to be more robust. Furthermore, we investigate FG-UAP in the view of NC by analyzing the labels and extracted features of adversarial examples, finding that collapse phenomenon becomes stronger after the model is corrupted. The code will be released when the paper is accepted.



### Spatio-Temporal Relation Learning for Video Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.13116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13116v1)
- **Published**: 2022-09-27 02:19:31+00:00
- **Updated**: 2022-09-27 02:19:31+00:00
- **Authors**: Hui Lv, Zhen Cui, Biao Wang, Jian Yang
- **Comment**: 8 pages, 5 figures,Journal
- **Journal**: None
- **Summary**: Anomaly identification is highly dependent on the relationship between the object and the scene, as different/same object actions in same/different scenes may lead to various degrees of normality and anomaly. Therefore, object-scene relation actually plays a crucial role in anomaly detection but is inadequately explored in previous works. In this paper, we propose a Spatial-Temporal Relation Learning (STRL) framework to tackle the video anomaly detection task. First, considering dynamic characteristics of the objects as well as scene areas, we construct a Spatio-Temporal Auto-Encoder (STAE) to jointly exploit spatial and temporal evolution patterns for representation learning. For better pattern extraction, two decoding branches are designed in the STAE module, i.e. an appearance branch capturing spatial cues by directly predicting the next frame, and a motion branch focusing on modeling the dynamics via optical flow prediction. Then, to well concretize the object-scene relation, a Relation Learning (RL) module is devised to analyze and summarize the normal relations by introducing the Knowledge Graph Embedding methodology. Specifically in this process, the plausibility of object-scene relation is measured by jointly modeling object/scene features and optimizable object-scene relation maps. Extensive experiments are conducted on three public datasets, and the superior performance over the state-of-the-art methods demonstrates the effectiveness of our method.



### Deep Unfolding for Iterative Stripe Noise Removal
- **Arxiv ID**: http://arxiv.org/abs/2209.14973v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.14973v1)
- **Published**: 2022-09-27 02:53:03+00:00
- **Updated**: 2022-09-27 02:53:03+00:00
- **Authors**: Zeshan Fayyaz, Daniel Platnick, Hannan Fayyaz, Nariman Farsad
- **Comment**: Accepted in the International Joint Conference on Neural Networks
  (IJCNN) track of the 2022 IEEE World Congress on Computational Intelligence
  (IEEE WCCI 2022)
- **Journal**: None
- **Summary**: The non-uniform photoelectric response of infrared imaging systems results in fixed-pattern stripe noise being superimposed on infrared images, which severely reduces image quality. As the applications of degraded infrared images are limited, it is crucial to effectively preserve original details. Existing image destriping methods struggle to concurrently remove all stripe noise artifacts, preserve image details and structures, and balance real-time performance. In this paper we propose a novel algorithm for destriping degraded images, which takes advantage of neighbouring column signal correlation to remove independent column stripe noise. This is achieved through an iterative deep unfolding algorithm where the estimated noise of one network iteration is used as input to the next iteration. This progression substantially reduces the search space of possible function approximations, allowing for efficient training on larger datasets. The proposed method allows for a more precise estimation of stripe noise to preserve scene details more accurately. Extensive experimental results demonstrate that the proposed model outperforms existing destriping methods on artificially corrupted images on both quantitative and qualitative assessments.



### 3D Scene Flow Estimation on Pseudo-LiDAR: Bridging the Gap on Estimating Point Motion
- **Arxiv ID**: http://arxiv.org/abs/2209.13130v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.13130v1)
- **Published**: 2022-09-27 03:27:09+00:00
- **Updated**: 2022-09-27 03:27:09+00:00
- **Authors**: Chaokang Jiang, Guangming Wang, Yanzi Miao, Hesheng Wang
- **Comment**: 9 pages, 5 figures; This paper has been accepted by IEEE Transactions
  on Industrial Informatics
- **Journal**: None
- **Summary**: 3D scene flow characterizes how the points at the current time flow to the next time in the 3D Euclidean space, which possesses the capacity to infer autonomously the non-rigid motion of all objects in the scene. The previous methods for estimating scene flow from images have limitations, which split the holistic nature of 3D scene flow by estimating optical flow and disparity separately. Learning 3D scene flow from point clouds also faces the difficulties of the gap between synthesized and real data and the sparsity of LiDAR point clouds. In this paper, the generated dense depth map is utilized to obtain explicit 3D coordinates, which achieves direct learning of 3D scene flow from 2D images. The stability of the predicted scene flow is improved by introducing the dense nature of 2D pixels into the 3D space. Outliers in the generated 3D point cloud are removed by statistical methods to weaken the impact of noisy points on the 3D scene flow estimation task. Disparity consistency loss is proposed to achieve more effective unsupervised learning of 3D scene flow. The proposed method of self-supervised learning of 3D scene flow on real-world images is compared with a variety of methods for learning on the synthesized dataset and learning on LiDAR point clouds. The comparisons of multiple scene flow metrics are shown to demonstrate the effectiveness and superiority of introducing pseudo-LiDAR point cloud to scene flow estimation.



### Hitchhiker's Guide to Super-Resolution: Introduction and Recent Advances
- **Arxiv ID**: http://arxiv.org/abs/2209.13131v2
- **DOI**: 10.1109/TPAMI.2023.3243794
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.13131v2)
- **Published**: 2022-09-27 03:28:34+00:00
- **Updated**: 2023-02-14 11:09:24+00:00
- **Authors**: Brian Moser, Federico Raue, Stanislav Frolov, Jörn Hees, Sebastian Palacio, Andreas Dengel
- **Comment**: accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence, 2023
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2023
- **Summary**: With the advent of Deep Learning (DL), Super-Resolution (SR) has also become a thriving research area. However, despite promising results, the field still faces challenges that require further research e.g., allowing flexible upsampling, more effective loss functions, and better evaluation metrics. We review the domain of SR in light of recent advances, and examine state-of-the-art models such as diffusion (DDPM) and transformer-based SR models. We present a critical discussion on contemporary strategies used in SR, and identify promising yet unexplored research directions. We complement previous surveys by incorporating the latest developments in the field such as uncertainty-driven losses, wavelet networks, neural architecture search, novel normalization methods, and the latests evaluation techniques. We also include several visualizations for the models and methods throughout each chapter in order to facilitate a global understanding of the trends in the field. This review is ultimately aimed at helping researchers to push the boundaries of DL applied to SR.



### Searching a High-Performance Feature Extractor for Text Recognition Network
- **Arxiv ID**: http://arxiv.org/abs/2209.13139v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.13139v1)
- **Published**: 2022-09-27 03:49:04+00:00
- **Updated**: 2022-09-27 03:49:04+00:00
- **Authors**: Hui Zhang, Quanming Yao, James T. Kwok, Xiang Bai
- **Comment**: None
- **Journal**: None
- **Summary**: Feature extractor plays a critical role in text recognition (TR), but customizing its architecture is relatively less explored due to expensive manual tweaking. In this work, inspired by the success of neural architecture search (NAS), we propose to search for suitable feature extractors. We design a domain-specific search space by exploring principles for having good feature extractors. The space includes a 3D-structured space for the spatial model and a transformed-based space for the sequential model. As the space is huge and complexly structured, no existing NAS algorithms can be applied. We propose a two-stage algorithm to effectively search in the space. In the first stage, we cut the space into several blocks and progressively train each block with the help of an auxiliary head. We introduce the latency constraint into the second stage and search sub-network from the trained supernet via natural gradient descent. In experiments, a series of ablation studies are performed to better understand the designed space, search algorithm, and searched architectures. We also compare the proposed method with various state-of-the-art ones on both hand-written and scene TR tasks. Extensive results show that our approach can achieve better recognition performance with less latency.



### Observation Centric and Central Distance Recovery on Sports Player Tracking
- **Arxiv ID**: http://arxiv.org/abs/2209.13154v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13154v1)
- **Published**: 2022-09-27 04:48:11+00:00
- **Updated**: 2022-09-27 04:48:11+00:00
- **Authors**: Hsiang-Wei Huang, Cheng-Yen Yang, Jenq-Neng Hwang, Pyong-Kun Kim, Kwangju Kim, Kyoungoh Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-Object Tracking over humans has improved rapidly with the development of object detection and re-identification. However, multi-actor tracking over humans with similar appearance and nonlinear movement can still be very challenging even for the state-of-the-art tracking algorithm. Current motion-based tracking algorithms often use Kalman Filter to predict the motion of an object, however, its linear movement assumption can cause failure in tracking when the target is not moving linearly. And for multi-players tracking over the sports field, because the players in the same team are usually wearing the same color of jersey, making re-identification even harder both in the short term and long term in the tracking process. In this work, we proposed a motionbased tracking algorithm and three post-processing pipelines for three sports including basketball, football, and volleyball, we successfully handle the tracking of the non-linear movement of players on the sports fields. Experiments result on the testing set of ECCV DeeperAction Challenge SportsMOT Dataset demonstrate the effectiveness of our method, which achieves a HOTA of 73.968, ranking 3rd place on the 2022 Sportsmot workshop final leaderboard.



### Ki-67 Index Measurement in Breast Cancer Using Digital Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2209.13155v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.13155v1)
- **Published**: 2022-09-27 04:48:57+00:00
- **Updated**: 2022-09-27 04:48:57+00:00
- **Authors**: Hsiang-Wei Huang, Wen-Tsung Huang, Hsun-Heng Tsai
- **Comment**: None
- **Journal**: None
- **Summary**: Ki-67 is a nuclear protein that can be produced during cell proliferation. The Ki67 index is a valuable prognostic variable in several kinds of cancer. In breast cancer, the index is even routinely checked in many patients. Currently, pathologists use the immunohistochemistry method to calculate the percentage of Ki-67 positive malignant cells as Ki-67 index. The higher score usually means more aggressive tumor behavior. In clinical practice, the measurement of Ki-67 index relies on visual identifying method and manual counting. However, visual and manual assessment method is timeconsuming and leads to poor reproducibility because of different scoring standards or limited tumor area under assessment. Here, we use digital image processing technics including image binarization and image morphological operations to create a digital image analysis method to interpretate Ki-67 index. Then, 10 breast cancer specimens are used as validation with high accuracy (correlation efficiency r = 0.95127). With the assistance of digital image analysis, pathologists can interpretate the Ki67 index more efficiently, precisely with excellent reproducibility.



### Towards Multimodal Multitask Scene Understanding Models for Indoor Mobile Agents
- **Arxiv ID**: http://arxiv.org/abs/2209.13156v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.13156v1)
- **Published**: 2022-09-27 04:49:19+00:00
- **Updated**: 2022-09-27 04:49:19+00:00
- **Authors**: Yao-Hung Hubert Tsai, Hanlin Goh, Ali Farhadi, Jian Zhang
- **Comment**: Submitted to ICRA2023
- **Journal**: None
- **Summary**: The perception system in personalized mobile agents requires developing indoor scene understanding models, which can understand 3D geometries, capture objectiveness, analyze human behaviors, etc. Nonetheless, this direction has not been well-explored in comparison with models for outdoor environments (e.g., the autonomous driving system that includes pedestrian prediction, car detection, traffic sign recognition, etc.). In this paper, we first discuss the main challenge: insufficient, or even no, labeled data for real-world indoor environments, and other challenges such as fusion between heterogeneous sources of information (e.g., RGB images and Lidar point clouds), modeling relationships between a diverse set of outputs (e.g., 3D object locations, depth estimation, and human poses), and computational efficiency. Then, we describe MMISM (Multi-modality input Multi-task output Indoor Scene understanding Model) to tackle the above challenges. MMISM considers RGB images as well as sparse Lidar points as inputs and 3D object detection, depth completion, human pose estimation, and semantic segmentation as output tasks. We show that MMISM performs on par or even better than single-task models; e.g., we improve the baseline 3D object detection results by 11.7% on the benchmark ARKitScenes dataset.



### A Morphology Focused Diffusion Probabilistic Model for Synthesis of Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/2209.13167v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.13167v2)
- **Published**: 2022-09-27 05:58:35+00:00
- **Updated**: 2022-09-29 02:13:12+00:00
- **Authors**: Puria Azadi Moghadam, Sanne Van Dalen, Karina C. Martin, Jochen Lennerz, Stephen Yip, Hossein Farahani, Ali Bashashati
- **Comment**: None
- **Journal**: None
- **Summary**: Visual microscopic study of diseased tissue by pathologists has been the cornerstone for cancer diagnosis and prognostication for more than a century. Recently, deep learning methods have made significant advances in the analysis and classification of tissue images. However, there has been limited work on the utility of such models in generating histopathology images. These synthetic images have several applications in pathology including utilities in education, proficiency testing, privacy, and data sharing. Recently, diffusion probabilistic models were introduced to generate high quality images. Here, for the first time, we investigate the potential use of such models along with prioritized morphology weighting and color normalization to synthesize high quality histopathology images of brain cancer. Our detailed results show that diffusion probabilistic models are capable of synthesizing a wide range of histopathology images and have superior performance compared to generative adversarial networks.



### Globally Optimal Event-Based Divergence Estimation for Ventral Landing
- **Arxiv ID**: http://arxiv.org/abs/2209.13168v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13168v1)
- **Published**: 2022-09-27 06:00:52+00:00
- **Updated**: 2022-09-27 06:00:52+00:00
- **Authors**: Sofia McLeod, Gabriele Meoni, Dario Izzo, Anne Mergy, Daqi Liu, Yasir Latif, Ian Reid, Tat-Jun Chin
- **Comment**: Accepted in the ECCV 2022 workshop on AI for Space, 18 pages, 6
  figures
- **Journal**: None
- **Summary**: Event sensing is a major component in bio-inspired flight guidance and control systems. We explore the usage of event cameras for predicting time-to-contact (TTC) with the surface during ventral landing. This is achieved by estimating divergence (inverse TTC), which is the rate of radial optic flow, from the event stream generated during landing. Our core contributions are a novel contrast maximisation formulation for event-based divergence estimation, and a branch-and-bound algorithm to exactly maximise contrast and find the optimal divergence value. GPU acceleration is conducted to speed up the global algorithm. Another contribution is a new dataset containing real event streams from ventral landing that was employed to test and benchmark our method. Owing to global optimisation, our algorithm is much more capable at recovering the true divergence, compared to other heuristic divergence estimators or event-based optic flow methods. With GPU acceleration, our method also achieves competitive runtimes.



### RepsNet: Combining Vision with Language for Automated Medical Reports
- **Arxiv ID**: http://arxiv.org/abs/2209.13171v1
- **DOI**: 10.1007/978-3-031-16443-9_68
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13171v1)
- **Published**: 2022-09-27 06:11:22+00:00
- **Updated**: 2022-09-27 06:11:22+00:00
- **Authors**: Ajay Kumar Tanwani, Joelle Barral, Daniel Freedman
- **Comment**: None
- **Journal**: MICCAI 2022, pp. 714--724
- **Summary**: Writing reports by analyzing medical images is error-prone for inexperienced practitioners and time consuming for experienced ones. In this work, we present RepsNet that adapts pre-trained vision and language models to interpret medical images and generate automated reports in natural language. RepsNet consists of an encoder-decoder model: the encoder aligns the images with natural language descriptions via contrastive learning, while the decoder predicts answers by conditioning on encoded images and prior context of descriptions retrieved by nearest neighbor search. We formulate the problem in a visual question answering setting to handle both categorical and descriptive natural language answers. We perform experiments on two challenging tasks of medical visual question answering (VQA-Rad) and report generation (IU-Xray) on radiology image datasets. Results show that RepsNet outperforms state-of-the-art methods with 81.08 % classification accuracy on VQA-Rad 2018 and 0.58 BLEU-1 score on IU-Xray. Supplementary details are available at https://sites.google.com/view/repsnet



### Dynamics-Aware Spatiotemporal Occupancy Prediction in Urban Environments
- **Arxiv ID**: http://arxiv.org/abs/2209.13172v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, I.2.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2209.13172v1)
- **Published**: 2022-09-27 06:12:34+00:00
- **Updated**: 2022-09-27 06:12:34+00:00
- **Authors**: Maneekwan Toyungyernsub, Esen Yel, Jiachen Li, Mykel J. Kochenderfer
- **Comment**: Accepted at 2022 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2022)
- **Journal**: None
- **Summary**: Detection and segmentation of moving obstacles, along with prediction of the future occupancy states of the local environment, are essential for autonomous vehicles to proactively make safe and informed decisions. In this paper, we propose a framework that integrates the two capabilities together using deep neural network architectures. Our method first detects and segments moving objects in the scene, and uses this information to predict the spatiotemporal evolution of the environment around autonomous vehicles. To address the problem of direct integration of both static-dynamic object segmentation and environment prediction models, we propose using occupancy-based environment representations across the whole framework. Our method is validated on the real-world Waymo Open Dataset and demonstrates higher prediction accuracy than baseline methods.



### Progress and Prospects for Fairness in Healthcare and Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2209.13177v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13177v5)
- **Published**: 2022-09-27 06:29:18+00:00
- **Updated**: 2023-05-25 00:14:59+00:00
- **Authors**: Zikang Xu, Jun Li, Qingsong Yao, S. Kevin Zhou
- **Comment**: 21 pages, 11 figures
- **Journal**: None
- **Summary**: Machine learning-enabled medical imaging analysis has become a vital part of the current automatic diagnosis system. However, machine learning models have been shown to demonstrate a systematic bias towards certain subgroups of people, e.g., giving a worse predictive performance to old females. It is harmful and dangerous in such a sensitive area and therefore researchers have been working on developing bias mitigation algorithms to address the fairness issue in the general machine learning field. However, given the specific characteristics of medical imaging, fairness in medical image analysis (MedIA) requires additional efforts. Hence, in this survey, we give a comprehensive review of the current progress of fairness study and that in MedIA. Specifically, we first discuss the definitions of fairness and analyze the source of bias in medical imaging. Then, we discuss current research on fairness for MedIA and present a collection of public medical imaging datasets that can be used for evaluating fairness in MedIA. Furthermore, we conduct extensive experiments to evaluate the fairness of several different tasks for medical imaging, including classification, object detection, and landmark detection. Finally, we discuss the challenges and potential future directions in developing fair MedIA.



### A Novel Dataset for Evaluating and Alleviating Domain Shift for Human Detection in Agricultural Fields
- **Arxiv ID**: http://arxiv.org/abs/2209.13202v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2209.13202v1)
- **Published**: 2022-09-27 07:04:28+00:00
- **Updated**: 2022-09-27 07:04:28+00:00
- **Authors**: Paraskevi Nousi, Emmanouil Mpampis, Nikolaos Passalis, Ole Green, Anastasios Tefas
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we evaluate the impact of domain shift on human detection models trained on well known object detection datasets when deployed on data outside the distribution of the training set, as well as propose methods to alleviate such phenomena based on the available annotations from the target domain. Specifically, we introduce the OpenDR Humans in Field dataset, collected in the context of agricultural robotics applications, using the Robotti platform, allowing for quantitatively measuring the impact of domain shift in such applications. Furthermore, we examine the importance of manual annotation by evaluating three distinct scenarios concerning the training data: a) only negative samples, i.e., no depicted humans, b) only positive samples, i.e., only images which contain humans, and c) both negative and positive samples. Our results indicate that good performance can be achieved even when using only negative samples, if additional consideration is given to the training process. We also find that positive samples increase performance especially in terms of better localization. The dataset is publicly available for download at https://github.com/opendr-eu/datasets.



### NEURAL MARIONETTE: A Transformer-based Multi-action Human Motion Synthesis System
- **Arxiv ID**: http://arxiv.org/abs/2209.13204v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.13204v1)
- **Published**: 2022-09-27 07:10:20+00:00
- **Updated**: 2022-09-27 07:10:20+00:00
- **Authors**: Weiqiang Wang, Xuefei Zhe, Huan Chen, Di Kang, Tingguang Li, Ruizhi Chen, Linchao Bao
- **Comment**: None
- **Journal**: None
- **Summary**: We present a neural network-based system for long-term, multi-action human motion synthesis. The system, dubbed as NEURAL MARIONETTE, can produce high-quality and meaningful motions with smooth transitions from simple user input, including a sequence of action tags with expected action duration, and optionally a hand-drawn moving trajectory if the user specifies. The core of our system is a novel Transformer-based motion generation model, namely MARIONET, which can generate diverse motions given action tags. Different from existing motion generation models, MARIONET utilizes contextual information from the past motion clip and future action tag, dedicated to generating actions that can smoothly blend historical and future actions. Specifically, MARIONET first encodes target action tag and contextual information into an action-level latent code. The code is unfolded into frame-level control signals via a time unrolling module, which could be then combined with other frame-level control signals like the target trajectory. Motion frames are then generated in an auto-regressive way. By sequentially applying MARIONET, the system NEURAL MARIONETTE can robustly generate long-term, multi-action motions with the help of two simple schemes, namely "Shadow Start" and "Action Revision". Along with the novel system, we also present a new dataset dedicated to the multi-action motion synthesis task, which contains both action tags and their contextual information. Extensive experiments are conducted to study the action accuracy, naturalism, and transition smoothness of the motions generated by our system.



### Neural Frank-Wolfe Policy Optimization for Region-of-Interest Intra-Frame Coding with HEVC/H.265
- **Arxiv ID**: http://arxiv.org/abs/2209.13210v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.13210v1)
- **Published**: 2022-09-27 07:26:35+00:00
- **Updated**: 2022-09-27 07:26:35+00:00
- **Authors**: Yung-Han Ho, Chia-Hao Kao, Wen-Hsiao Peng, Ping-Chun Hsieh
- **Comment**: Accepted by VCIP 2022. arXiv admin note: text overlap with
  arXiv:2203.05127
- **Journal**: None
- **Summary**: This paper presents a reinforcement learning (RL) framework that utilizes Frank-Wolfe policy optimization to solve Coding-Tree-Unit (CTU) bit allocation for Region-of-Interest (ROI) intra-frame coding. Most previous RL-based methods employ the single-critic design, where the rewards for distortion minimization and rate regularization are weighted by an empirically chosen hyper-parameter. Recently, the dual-critic design is proposed to update the actor by alternating the rate and distortion critics. However, its convergence is not guaranteed. To address these issues, we introduce Neural Frank-Wolfe Policy Optimization (NFWPO) in formulating the CTU-level bit allocation as an action-constrained RL problem. In this new framework, we exploit a rate critic to predict a feasible set of actions. With this feasible set, a distortion critic is invoked to update the actor to maximize the ROI-weighted image quality subject to a rate constraint. Experimental results produced with x265 confirm the superiority of the proposed method to the other baselines.



### Im2Oil: Stroke-Based Oil Painting Rendering with Linearly Controllable Fineness Via Adaptive Sampling
- **Arxiv ID**: http://arxiv.org/abs/2209.13219v1
- **DOI**: 10.1145/3503161.3547759
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2209.13219v1)
- **Published**: 2022-09-27 07:41:04+00:00
- **Updated**: 2022-09-27 07:41:04+00:00
- **Authors**: Zhengyan Tong, Xiaohang Wang, Shengchao Yuan, Xuanhong Chen, Junjie Wang, Xiangzhong Fang
- **Comment**: ACM MM 2022 oral paper, accepted by the 30th ACM International
  Conference on Multimedia
- **Journal**: None
- **Summary**: This paper proposes a novel stroke-based rendering (SBR) method that translates images into vivid oil paintings. Previous SBR techniques usually formulate the oil painting problem as pixel-wise approximation. Different from this technique route, we treat oil painting creation as an adaptive sampling problem. Firstly, we compute a probability density map based on the texture complexity of the input image. Then we use the Voronoi algorithm to sample a set of pixels as the stroke anchors. Next, we search and generate an individual oil stroke at each anchor. Finally, we place all the strokes on the canvas to obtain the oil painting. By adjusting the hyper-parameter maximum sampling probability, we can control the oil painting fineness in a linear manner. Comparison with existing state-of-the-art oil painting techniques shows that our results have higher fidelity and more realistic textures. A user opinion test demonstrates that people behave more preference toward our oil paintings than the results of other methods. More interesting results and the code are in https://github.com/TZYSJTU/Im2Oil.



### View-aware Salient Object Detection for 360° Omnidirectional Image
- **Arxiv ID**: http://arxiv.org/abs/2209.13222v1
- **DOI**: 10.1109/TMM.2022.3209015
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13222v1)
- **Published**: 2022-09-27 07:44:08+00:00
- **Updated**: 2022-09-27 07:44:08+00:00
- **Authors**: Junjie Wu, Changqun Xia, Tianshu Yu, Jia Li
- **Comment**: This paper has been accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Image-based salient object detection (ISOD) in 360{\deg} scenarios is significant for understanding and applying panoramic information. However, research on 360{\deg} ISOD has not been widely explored due to the lack of large, complex, high-resolution, and well-labeled datasets. Towards this end, we construct a large scale 360{\deg} ISOD dataset with object-level pixel-wise annotation on equirectangular projection (ERP), which contains rich panoramic scenes with not less than 2K resolution and is the largest dataset for 360{\deg} ISOD by far to our best knowledge. By observing the data, we find current methods face three significant challenges in panoramic scenarios: diverse distortion degrees, discontinuous edge effects and changeable object scales. Inspired by humans' observing process, we propose a view-aware salient object detection method based on a Sample Adaptive View Transformer (SAVT) module with two sub-modules to mitigate these issues. Specifically, the sub-module View Transformer (VT) contains three transform branches based on different kinds of transformations to learn various features under different views and heighten the model's feature toleration of distortion, edge effects and object scales. Moreover, the sub-module Sample Adaptive Fusion (SAF) is to adjust the weights of different transform branches based on various sample features and make transformed enhanced features fuse more appropriately. The benchmark results of 20 state-of-the-art ISOD methods reveal the constructed dataset is very challenging. Moreover, exhaustive experiments verify the proposed approach is practical and outperforms the state-of-the-art methods.



### A Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective
- **Arxiv ID**: http://arxiv.org/abs/2209.13232v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13232v3)
- **Published**: 2022-09-27 08:10:14+00:00
- **Updated**: 2022-10-23 09:46:40+00:00
- **Authors**: Chaoqi Chen, Yushuang Wu, Qiyuan Dai, Hong-Yu Zhou, Mutian Xu, Sibei Yang, Xiaoguang Han, Yizhou Yu
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Graph Neural Networks (GNNs) have gained momentum in graph representation learning and boosted the state of the art in a variety of areas, such as data mining (\emph{e.g.,} social network analysis and recommender systems), computer vision (\emph{e.g.,} object detection and point cloud learning), and natural language processing (\emph{e.g.,} relation extraction and sequence learning), to name a few. With the emergence of Transformers in natural language processing and computer vision, graph Transformers embed a graph structure into the Transformer architecture to overcome the limitations of local neighborhood aggregation while avoiding strict structural inductive biases. In this paper, we present a comprehensive review of GNNs and graph Transformers in computer vision from a task-oriented perspective. Specifically, we divide their applications in computer vision into five categories according to the modality of input data, \emph{i.e.,} 2D natural images, videos, 3D data, vision + language, and medical images. In each category, we further divide the applications according to a set of vision tasks. Such a task-oriented taxonomy allows us to examine how each task is tackled by different GNN-based approaches and how well these approaches perform. Based on the necessary preliminaries, we provide the definitions and challenges of the tasks, in-depth coverage of the representative approaches, as well as discussions regarding insights, limitations, and future directions.



### Genetic Programming-Based Evolutionary Deep Learning for Data-Efficient Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.13233v1
- **DOI**: 10.1109/TEVC.2022.3214503
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13233v1)
- **Published**: 2022-09-27 08:10:16+00:00
- **Updated**: 2022-09-27 08:10:16+00:00
- **Authors**: Ying Bi, Bing Xue, Mengjie Zhang
- **Comment**: Accepted by IEEE Transactions on Evolutionary Computation
- **Journal**: IEEE Transactions on Evolutionary Computation, 2022,
  https://ieeexplore.ieee.org/document/9919314
- **Summary**: Data-efficient image classification is a challenging task that aims to solve image classification using small training data. Neural network-based deep learning methods are effective for image classification, but they typically require large-scale training data and have major limitations such as requiring expertise to design network architectures and having poor interpretability. Evolutionary deep learning is a recent hot topic that combines evolutionary computation with deep learning. However, most evolutionary deep learning methods focus on evolving architectures of neural networks, which still suffer from limitations such as poor interpretability. To address this, this paper proposes a new genetic programming-based evolutionary deep learning approach to data-efficient image classification. The new approach can automatically evolve variable-length models using many important operators from both image and classification domains. It can learn different types of image features from colour or gray-scale images, and construct effective and diverse ensembles for image classification. A flexible multi-layer representation enables the new approach to automatically construct shallow or deep models/trees for different tasks and perform effective transformations on the input data via multiple internal nodes. The new approach is applied to solve five image classification tasks with different training set sizes. The results show that it achieves better performance in most cases than deep learning methods for data-efficient image classification. A deep analysis shows that the new approach has good convergence and evolves models with high interpretability, different lengths/sizes/shapes, and good transferability.



### RIGA: Rotation-Invariant and Globally-Aware Descriptors for Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/2209.13252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13252v1)
- **Published**: 2022-09-27 08:45:56+00:00
- **Updated**: 2022-09-27 08:45:56+00:00
- **Authors**: Hao Yu, Ji Hou, Zheng Qin, Mahdi Saleh, Ivan Shugurov, Kai Wang, Benjamin Busam, Slobodan Ilic
- **Comment**: None
- **Journal**: None
- **Summary**: Successful point cloud registration relies on accurate correspondences established upon powerful descriptors. However, existing neural descriptors either leverage a rotation-variant backbone whose performance declines under large rotations, or encode local geometry that is less distinctive. To address this issue, we introduce RIGA to learn descriptors that are Rotation-Invariant by design and Globally-Aware. From the Point Pair Features (PPFs) of sparse local regions, rotation-invariant local geometry is encoded into geometric descriptors. Global awareness of 3D structures and geometric context is subsequently incorporated, both in a rotation-invariant fashion. More specifically, 3D structures of the whole frame are first represented by our global PPF signatures, from which structural descriptors are learned to help geometric descriptors sense the 3D world beyond local regions. Geometric context from the whole scene is then globally aggregated into descriptors. Finally, the description of sparse regions is interpolated to dense point descriptors, from which correspondences are extracted for registration. To validate our approach, we conduct extensive experiments on both object- and scene-level data. With large rotations, RIGA surpasses the state-of-the-art methods by a margin of 8\degree in terms of the Relative Rotation Error on ModelNet40 and improves the Feature Matching Recall by at least 5 percentage points on 3DLoMatch.



### Exploring the Algorithm-Dependent Generalization of AUPRC Optimization with List Stability
- **Arxiv ID**: http://arxiv.org/abs/2209.13262v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.13262v1)
- **Published**: 2022-09-27 09:06:37+00:00
- **Updated**: 2022-09-27 09:06:37+00:00
- **Authors**: Peisong Wen, Qianqian Xu, Zhiyong Yang, Yuan He, Qingming Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Stochastic optimization of the Area Under the Precision-Recall Curve (AUPRC) is a crucial problem for machine learning. Although various algorithms have been extensively studied for AUPRC optimization, the generalization is only guaranteed in the multi-query case. In this work, we present the first trial in the single-query generalization of stochastic AUPRC optimization. For sharper generalization bounds, we focus on algorithm-dependent generalization. There are both algorithmic and theoretical obstacles to our destination. From an algorithmic perspective, we notice that the majority of existing stochastic estimators are biased only when the sampling strategy is biased, and is leave-one-out unstable due to the non-decomposability. To address these issues, we propose a sampling-rate-invariant unbiased stochastic estimator with superior stability. On top of this, the AUPRC optimization is formulated as a composition optimization problem, and a stochastic algorithm is proposed to solve this problem. From a theoretical perspective, standard techniques of the algorithm-dependent generalization analysis cannot be directly applied to such a listwise compositional optimization problem. To fill this gap, we extend the model stability from instancewise losses to listwise losses and bridge the corresponding generalization and stability. Additionally, we construct state transition matrices to describe the recurrence of the stability, and simplify calculations by matrix spectrum. Practically, experimental results on three image retrieval datasets on speak to the effectiveness and soundness of our framework.



### Deep Unfolding of the DBFB Algorithm with Application to ROI CT Imaging with Limited Angular Density
- **Arxiv ID**: http://arxiv.org/abs/2209.13264v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/2209.13264v3)
- **Published**: 2022-09-27 09:10:57+00:00
- **Updated**: 2023-05-17 15:27:50+00:00
- **Authors**: Marion Savanier, Emilie Chouzenoux, Jean-Christophe Pesquet, Cyril Riddell
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new method for reconstructing regions of interest (ROI) from a limited number of computed tomography (CT) measurements. Classical model-based iterative reconstruction methods lead to images with predictable features. Still, they often suffer from tedious parameterization and slow convergence. On the contrary, deep learning methods are fast, and they can reach high reconstruction quality by leveraging information from large datasets, but they lack interpretability. At the crossroads of both methods, deep unfolding networks have been recently proposed. Their design includes the physics of the imaging system and the steps of an iterative optimization algorithm. Motivated by the success of these networks for various applications, we introduce an unfolding neural network called U-RDBFB designed for ROI CT reconstruction from limited data. Few-view truncated data are effectively handled thanks to a robust non-convex data fidelity term combined with a sparsity-inducing regularization function. We unfold the Dual Block coordinate Forward-Backward (DBFB) algorithm, embedded in an iterative reweighted scheme, allowing the learning of key parameters in a supervised manner. Our experiments show an improvement over several state-of-the-art methods, including a model-based iterative scheme, a multi-scale deep learning architecture, and other deep unfolding methods.



### Inducing Data Amplification Using Auxiliary Datasets in Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/2209.14053v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.14053v1)
- **Published**: 2022-09-27 09:21:40+00:00
- **Updated**: 2022-09-27 09:21:40+00:00
- **Authors**: Saehyung Lee, Hyungyu Lee
- **Comment**: Accepted at WACV 2023
- **Journal**: None
- **Summary**: Several recent studies have shown that the use of extra in-distribution data can lead to a high level of adversarial robustness. However, there is no guarantee that it will always be possible to obtain sufficient extra data for a selected dataset. In this paper, we propose a biased multi-domain adversarial training (BiaMAT) method that induces training data amplification on a primary dataset using publicly available auxiliary datasets, without requiring the class distribution match between the primary and auxiliary datasets. The proposed method can achieve increased adversarial robustness on a primary dataset by leveraging auxiliary datasets via multi-domain learning. Specifically, data amplification on both robust and non-robust features can be accomplished through the application of BiaMAT as demonstrated through a theoretical and empirical analysis. Moreover, we demonstrate that while existing methods are vulnerable to negative transfer due to the distributional discrepancy between auxiliary and primary data, the proposed method enables neural networks to flexibly leverage diverse image datasets for adversarial training by successfully handling the domain discrepancy through the application of a confidence-based selection strategy. The pre-trained models and code are available at: \url{https://github.com/Saehyung-Lee/BiaMAT}.



### Orbeez-SLAM: A Real-time Monocular Visual SLAM with ORB Features and NeRF-realized Mapping
- **Arxiv ID**: http://arxiv.org/abs/2209.13274v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.13274v2)
- **Published**: 2022-09-27 09:37:57+00:00
- **Updated**: 2023-01-31 05:39:28+00:00
- **Authors**: Chi-Ming Chung, Yang-Che Tseng, Ya-Ching Hsu, Xiang-Qian Shi, Yun-Hung Hua, Jia-Fong Yeh, Wen-Chin Chen, Yi-Ting Chen, Winston H. Hsu
- **Comment**: None
- **Journal**: None
- **Summary**: A spatial AI that can perform complex tasks through visual signals and cooperate with humans is highly anticipated. To achieve this, we need a visual SLAM that easily adapts to new scenes without pre-training and generates dense maps for downstream tasks in real-time. None of the previous learning-based and non-learning-based visual SLAMs satisfy all needs due to the intrinsic limitations of their components. In this work, we develop a visual SLAM named Orbeez-SLAM, which successfully collaborates with implicit neural representation and visual odometry to achieve our goals. Moreover, Orbeez-SLAM can work with the monocular camera since it only needs RGB inputs, making it widely applicable to the real world. Results show that our SLAM is up to 800x faster than the strong baseline with superior rendering outcomes. Code link: https://github.com/MarvinChung/Orbeez-SLAM.



### A comparative study of attention mechanism and generative adversarial network in facade damage segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.13283v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.13283v1)
- **Published**: 2022-09-27 09:59:32+00:00
- **Updated**: 2022-09-27 09:59:32+00:00
- **Authors**: Fangzheng Lin, Jiesheng Yang, Jiangpeng Shu, Raimar J. Scherer
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation profits from deep learning and has shown its possibilities in handling the graphical data from the on-site inspection. As a result, visual damage in the facade images should be detected. Attention mechanism and generative adversarial networks are two of the most popular strategies to improve the quality of semantic segmentation. With specific focuses on these two strategies, this paper adopts U-net, a representative convolutional neural network, as the primary network and presents a comparative study in two steps. First, cell images are utilized to respectively determine the most effective networks among the U-nets with attention mechanism or generative adversarial networks. Subsequently, selected networks from the first test and their combination are applied for facade damage segmentation to investigate the performances of these networks. Besides, the combined effect of the attention mechanism and the generative adversarial network is discovered and discussed.



### Frame Interpolation for Dynamic Scenes with Implicit Flow Encoding
- **Arxiv ID**: http://arxiv.org/abs/2209.13284v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13284v2)
- **Published**: 2022-09-27 10:00:05+00:00
- **Updated**: 2022-11-16 05:59:28+00:00
- **Authors**: Pedro Figueirêdo, Avinash Paliwal, Nima Khademi Kalantari
- **Comment**: Accepted to WACV 2023. Project website:
  https://people.engr.tamu.edu/nimak/Papers/WACV2023_Interp . Code:
  https://github.com/pedrovfigueiredo/frameintIFE . YouTube:
  https://youtu.be/Re_c-CBlSfI
- **Journal**: None
- **Summary**: In this paper, we propose an algorithm to interpolate between a pair of images of a dynamic scene. While in the past years significant progress in frame interpolation has been made, current approaches are not able to handle images with brightness and illumination changes, which are common even when the images are captured shortly apart. We propose to address this problem by taking advantage of the existing optical flow methods that are highly robust to the variations in the illumination. Specifically, using the bidirectional flows estimated using an existing pre-trained flow network, we predict the flows from an intermediate frame to the two input images. To do this, we propose to encode the bidirectional flows into a coordinate-based network, powered by a hypernetwork, to obtain a continuous representation of the flow across time. Once we obtain the estimated flows, we use them within an existing blending network to obtain the final intermediate frame. Through extensive experiments, we demonstrate that our approach is able to produce significantly better results than state-of-the-art frame interpolation algorithms.



### When Handcrafted Features and Deep Features Meet Mismatched Training and Test Sets for Deepfake Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.13289v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13289v1)
- **Published**: 2022-09-27 10:15:46+00:00
- **Updated**: 2022-09-27 10:15:46+00:00
- **Authors**: Ying Xu, Sule Yildirim Yayilgan
- **Comment**: None
- **Journal**: None
- **Summary**: The accelerated growth in synthetic visual media generation and manipulation has now reached the point of raising significant concerns and posing enormous intimidations towards society. There is an imperative need for automatic detection networks towards false digital content and avoid the spread of dangerous artificial information to contend with this threat. In this paper, we utilize and compare two kinds of handcrafted features(SIFT and HoG) and two kinds of deep features(Xception and CNN+RNN) for the deepfake detection task. We also check the performance of these features when there are mismatches between training sets and test sets. Evaluation is performed on the famous FaceForensics++ dataset, which contains four sub-datasets, Deepfakes, Face2Face, FaceSwap and NeuralTextures. The best results are from Xception, where the accuracy could surpass over 99\% when the training and test set are both from the same sub-dataset. In comparison, the results drop dramatically when the training set mismatches the test set. This phenomenon reveals the challenge of creating a universal deepfake detection system.



### Video-based estimation of pain indicators in dogs
- **Arxiv ID**: http://arxiv.org/abs/2209.13296v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13296v2)
- **Published**: 2022-09-27 10:38:59+00:00
- **Updated**: 2022-11-26 15:48:32+00:00
- **Authors**: Hongyi Zhu, Yasemin Salgırlı, Pınar Can, Durmuş Atılgan, Albert Ali Salah
- **Comment**: 20 pages, 7 figures
- **Journal**: None
- **Summary**: Dog owners are typically capable of recognizing behavioral cues that reveal subjective states of their dogs, such as pain. But automatic recognition of the pain state is very challenging. This paper proposes a novel video-based, two-stream deep neural network approach for this problem. We extract and preprocess body keypoints, and compute features from both keypoints and the RGB representation over the video. We propose an approach to deal with self-occlusions and missing keypoints. We also present a unique video-based dog behavior dataset, collected by veterinary professionals, and annotated for presence of pain, and report good classification results with the proposed approach. This study is one of the first works on machine learning based estimation of dog pain state.



### Passive Non-line-of-sight Imaging for Moving Targets with an Event Camera
- **Arxiv ID**: http://arxiv.org/abs/2209.13300v1
- **DOI**: 10.3788/COL202321.061103
- **Categories**: **cs.CV**, eess.IV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/2209.13300v1)
- **Published**: 2022-09-27 10:56:14+00:00
- **Updated**: 2022-09-27 10:56:14+00:00
- **Authors**: Conghe Wang, Yutong He, Xia Wang, Honghao Huang, Changda Yan, Xin Zhang, Hongwei Chen
- **Comment**: None
- **Journal**: [J]. Chinese Optics Letters, 2023, 21(6): 061103
- **Summary**: Non-line-of-sight (NLOS) imaging is an emerging technique for detecting objects behind obstacles or around corners. Recent studies on passive NLOS mainly focus on steady-state measurement and reconstruction methods, which show limitations in recognition of moving targets. To the best of our knowledge, we propose a novel event-based passive NLOS imaging method. We acquire asynchronous event-based data which contains detailed dynamic information of the NLOS target, and efficiently ease the degradation of speckle caused by movement. Besides, we create the first event-based NLOS imaging dataset, NLOS-ES, and the event-based feature is extracted by time-surface representation. We compare the reconstructions through event-based data with frame-based data. The event-based method performs well on PSNR and LPIPS, which is 20% and 10% better than frame-based method, while the data volume takes only 2% of traditional method.



### Embracing Consistency: A One-Stage Approach for Spatio-Temporal Video Grounding
- **Arxiv ID**: http://arxiv.org/abs/2209.13306v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13306v2)
- **Published**: 2022-09-27 11:13:04+00:00
- **Updated**: 2022-12-01 04:41:09+00:00
- **Authors**: Yang Jin, Yongzhi Li, Zehuan Yuan, Yadong Mu
- **Comment**: 18 pages, 7 figures, Accepted by Neurips 2022, Spotlight Presentation
- **Journal**: None
- **Summary**: Spatio-Temporal video grounding (STVG) focuses on retrieving the spatio-temporal tube of a specific object depicted by a free-form textual expression. Existing approaches mainly treat this complicated task as a parallel frame-grounding problem and thus suffer from two types of inconsistency drawbacks: feature alignment inconsistency and prediction inconsistency. In this paper, we present an end-to-end one-stage framework, termed Spatio-Temporal Consistency-Aware Transformer (STCAT), to alleviate these issues. Specially, we introduce a novel multi-modal template as the global objective to address this task, which explicitly constricts the grounding region and associates the predictions among all video frames. Moreover, to generate the above template under sufficient video-textual perception, an encoder-decoder architecture is proposed for effective global context modeling. Thanks to these critical designs, STCAT enjoys more consistent cross-modal feature alignment and tube prediction without reliance on any pre-trained object detectors. Extensive experiments show that our method outperforms previous state-of-the-arts with clear margins on two challenging video benchmarks (VidSTG and HC-STVG), illustrating the superiority of the proposed framework to better understanding the association between vision and natural language. Code is publicly available at https://github.com/jy0205/STCAT.



### Text-Adaptive Multiple Visual Prototype Matching for Video-Text Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2209.13307v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/2209.13307v1)
- **Published**: 2022-09-27 11:13:48+00:00
- **Updated**: 2022-09-27 11:13:48+00:00
- **Authors**: Chengzhi Lin, Ancong Wu, Junwei Liang, Jun Zhang, Wenhang Ge, Wei-Shi Zheng, Chunhua Shen
- **Comment**: NIPS2022
- **Journal**: NIPS2022
- **Summary**: Cross-modal retrieval between videos and texts has gained increasing research interest due to the rapid emergence of videos on the web. Generally, a video contains rich instance and event information and the query text only describes a part of the information. Thus, a video can correspond to multiple different text descriptions and queries. We call this phenomenon the ``Video-Text Correspondence Ambiguity'' problem. Current techniques mostly concentrate on mining local or multi-level alignment between contents of a video and text (\textit{e.g.}, object to entity and action to verb). It is difficult for these methods to alleviate the video-text correspondence ambiguity by describing a video using only one single feature, which is required to be matched with multiple different text features at the same time. To address this problem, we propose a Text-Adaptive Multiple Visual Prototype Matching model, which automatically captures multiple prototypes to describe a video by adaptive aggregation of video token features. Given a query text, the similarity is determined by the most similar prototype to find correspondence in the video, which is termed text-adaptive matching. To learn diverse prototypes for representing the rich information in videos, we propose a variance loss to encourage different prototypes to attend to different contents of the video. Our method outperforms state-of-the-art methods on four public video retrieval datasets.



### Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection
- **Arxiv ID**: http://arxiv.org/abs/2210.00875v3
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2210.00875v3)
- **Published**: 2022-09-27 12:56:56+00:00
- **Updated**: 2023-04-05 13:32:57+00:00
- **Authors**: Yiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, Bo Li
- **Comment**: This work is accepted by the NeurIPS 2022 (Oral, TOP 2%). The first
  two authors contributed equally to this work. 25 pages. We have fixed some
  typos in the previous version
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have demonstrated their superiority in practice. Arguably, the rapid development of DNNs is largely benefited from high-quality (open-sourced) datasets, based on which researchers and developers can easily evaluate and improve their learning methods. Since the data collection is usually time-consuming or even expensive, how to protect their copyrights is of great significance and worth further exploration. In this paper, we revisit dataset ownership verification. We find that existing verification methods introduced new security risks in DNNs trained on the protected dataset, due to the targeted nature of poison-only backdoor watermarks. To alleviate this problem, in this work, we explore the untargeted backdoor watermarking scheme, where the abnormal model behaviors are not deterministic. Specifically, we introduce two dispersibilities and prove their correlation, based on which we design the untargeted backdoor watermark under both poisoned-label and clean-label settings. We also discuss how to use the proposed untargeted backdoor watermark for dataset ownership verification. Experiments on benchmark datasets verify the effectiveness of our methods and their resistance to existing backdoor defenses. Our codes are available at \url{https://github.com/THUYimingLi/Untargeted_Backdoor_Watermark}.



### SuperYOLO: Super Resolution Assisted Object Detection in Multimodal Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/2209.13351v2
- **DOI**: 10.1109/TGRS.2023.3258666
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13351v2)
- **Published**: 2022-09-27 12:58:58+00:00
- **Updated**: 2023-04-08 09:50:26+00:00
- **Authors**: Jiaqing Zhang, Jie Lei, Weiying Xie, Zhenman Fang, Yunsong Li, Qian Du
- **Comment**: The article is accepted by IEEE Transactions on Geoscience and Remote
  Sensing
- **Journal**: None
- **Summary**: Accurately and timely detecting multiscale small objects that contain tens of pixels from remote sensing images (RSI) remains challenging. Most of the existing solutions primarily design complex deep neural networks to learn strong feature representations for objects separated from the background, which often results in a heavy computation burden. In this article, we propose an accurate yet fast object detection method for RSI, named SuperYOLO, which fuses multimodal data and performs high-resolution (HR) object detection on multiscale objects by utilizing the assisted super resolution (SR) learning and considering both the detection accuracy and computation cost. First, we utilize a symmetric compact multimodal fusion (MF) to extract supplementary information from various data for improving small object detection in RSI. Furthermore, we design a simple and flexible SR branch to learn HR feature representations that can discriminate small objects from vast backgrounds with low-resolution (LR) input, thus further improving the detection accuracy. Moreover, to avoid introducing additional computation, the SR branch is discarded in the inference stage, and the computation of the network model is reduced due to the LR input. Experimental results show that, on the widely used VEDAI RS dataset, SuperYOLO achieves an accuracy of 75.09% (in terms of mAP50 ), which is more than 10% higher than the SOTA large models, such as YOLOv5l, YOLOv5x, and RS designed YOLOrs. Meanwhile, the parameter size and GFLOPs of SuperYOLO are about 18 times and 3.8 times less than YOLOv5x. Our proposed model shows a favorable accuracy and speed tradeoff compared to the state-of-the-art models. The code will be open-sourced at https://github.com/icey-zhang/SuperYOLO.



### Suppress with a Patch: Revisiting Universal Adversarial Patch Attacks against Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.13353v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13353v2)
- **Published**: 2022-09-27 12:59:19+00:00
- **Updated**: 2022-12-22 08:53:12+00:00
- **Authors**: Svetlana Pavlitskaya, Jonas Hendl, Sebastian Kleim, Leopold Müller, Fabian Wylczoch, J. Marius Zöllner
- **Comment**: Accepted for publication at ICECCME 2022
- **Journal**: None
- **Summary**: Adversarial patch-based attacks aim to fool a neural network with an intentionally generated noise, which is concentrated in a particular region of an input image. In this work, we perform an in-depth analysis of different patch generation parameters, including initialization, patch size, and especially positioning a patch in an image during training. We focus on the object vanishing attack and run experiments with YOLOv3 as a model under attack in a white-box setting and use images from the COCO dataset. Our experiments have shown, that inserting a patch inside a window of increasing size during training leads to a significant increase in attack strength compared to a fixed position. The best results were obtained when a patch was positioned randomly during training, while patch position additionally varied within a batch.



### Draw Your Art Dream: Diverse Digital Art Synthesis with Multimodal Guided Diffusion
- **Arxiv ID**: http://arxiv.org/abs/2209.13360v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13360v2)
- **Published**: 2022-09-27 13:10:25+00:00
- **Updated**: 2022-09-28 05:31:18+00:00
- **Authors**: Nisha Huang, Fan Tang, Weiming Dong, Changsheng Xu
- **Comment**: Accepted by ACM MM 2022
- **Journal**: None
- **Summary**: Digital art synthesis is receiving increasing attention in the multimedia community because of engaging the public with art effectively. Current digital art synthesis methods usually use single-modality inputs as guidance, thereby limiting the expressiveness of the model and the diversity of generated results. To solve this problem, we propose the multimodal guided artwork diffusion (MGAD) model, which is a diffusion-based digital artwork generation approach that utilizes multimodal prompts as guidance to control the classifier-free diffusion model. Additionally, the contrastive language-image pretraining (CLIP) model is used to unify text and image modalities. Extensive experimental results on the quality and quantity of the generated digital art paintings confirm the effectiveness of the combination of the diffusion model and multimodal guidance. Code is available at https://github.com/haha-lisa/MGAD-multimodal-guided-artwork-diffusion.



### DELTAR: Depth Estimation from a Light-weight ToF Sensor and RGB Image
- **Arxiv ID**: http://arxiv.org/abs/2209.13362v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13362v1)
- **Published**: 2022-09-27 13:11:37+00:00
- **Updated**: 2022-09-27 13:11:37+00:00
- **Authors**: Yijin Li, Xinyang Liu, Wenqi Dong, Han Zhou, Hujun Bao, Guofeng Zhang, Yinda Zhang, Zhaopeng Cui
- **Comment**: Accepted to ECCV 2022. Project Page: https://zju3dv.github.io/deltar/
- **Journal**: None
- **Summary**: Light-weight time-of-flight (ToF) depth sensors are small, cheap, low-energy and have been massively deployed on mobile devices for the purposes like autofocus, obstacle detection, etc. However, due to their specific measurements (depth distribution in a region instead of the depth value at a certain pixel) and extremely low resolution, they are insufficient for applications requiring high-fidelity depth such as 3D reconstruction. In this paper, we propose DELTAR, a novel method to empower light-weight ToF sensors with the capability of measuring high resolution and accurate depth by cooperating with a color image. As the core of DELTAR, a feature extractor customized for depth distribution and an attention-based neural architecture is proposed to fuse the information from the color and ToF domain efficiently. To evaluate our system in real-world scenarios, we design a data collection device and propose a new approach to calibrate the RGB camera and ToF sensor. Experiments show that our method produces more accurate depth than existing frameworks designed for depth completion and depth super-resolution and achieves on par performance with a commodity-level RGB-D sensor. Code and data are available at https://zju3dv.github.io/deltar/.



### OBBStacking: An Ensemble Method for Remote Sensing Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.13369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13369v1)
- **Published**: 2022-09-27 13:17:06+00:00
- **Updated**: 2022-09-27 13:17:06+00:00
- **Authors**: Haoning Lin, Changhao Sun, Yunpeng Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Ensemble methods are a reliable way to combine several models to achieve superior performance. However, research on the application of ensemble methods in the remote sensing object detection scenario is mostly overlooked. Two problems arise. First, one unique characteristic of remote sensing object detection is the Oriented Bounding Boxes (OBB) of the objects and the fusion of multiple OBBs requires further research attention. Second, the widely used deep learning object detectors provide a score for each detected object as an indicator of confidence, but how to use these indicators effectively in an ensemble method remains a problem. Trying to address these problems, this paper proposes OBBStacking, an ensemble method that is compatible with OBBs and combines the detection results in a learned fashion. This ensemble method helps take 1st place in the Challenge Track \textit{Fine-grained Object Recognition in High-Resolution Optical Images}, which was featured in \textit{2021 Gaofen Challenge on Automated High-Resolution Earth Observation Image Interpretation}. The experiments on DOTA dataset and FAIR1M dataset demonstrate the improved performance of OBBStacking and the features of OBBStacking are analyzed.



### StyleMask: Disentangling the Style Space of StyleGAN2 for Neural Face Reenactment
- **Arxiv ID**: http://arxiv.org/abs/2209.13375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13375v1)
- **Published**: 2022-09-27 13:22:35+00:00
- **Updated**: 2022-09-27 13:22:35+00:00
- **Authors**: Stella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, Georgios Tzimiropoulos
- **Comment**: Accepted for publication in IEEE FG 2023. Code:
  https://github.com/StelaBou/StyleMask
- **Journal**: None
- **Summary**: In this paper we address the problem of neural face reenactment, where, given a pair of a source and a target facial image, we need to transfer the target's pose (defined as the head pose and its facial expressions) to the source image, by preserving at the same time the source's identity characteristics (e.g., facial shape, hair style, etc), even in the challenging case where the source and the target faces belong to different identities. In doing so, we address some of the limitations of the state-of-the-art works, namely, a) that they depend on paired training data (i.e., source and target faces have the same identity), b) that they rely on labeled data during inference, and c) that they do not preserve identity in large head pose changes. More specifically, we propose a framework that, using unpaired randomly generated facial images, learns to disentangle the identity characteristics of the face from its pose by incorporating the recently introduced style space $\mathcal{S}$ of StyleGAN2, a latent representation space that exhibits remarkable disentanglement properties. By capitalizing on this, we learn to successfully mix a pair of source and target style codes using supervision from a 3D model. The resulting latent code, that is subsequently used for reenactment, consists of latent units corresponding to the facial pose of the target only and of units corresponding to the identity of the source only, leading to notable improvement in the reenactment performance compared to recent state-of-the-art methods. In comparison to state of the art, we quantitatively and qualitatively show that the proposed method produces higher quality results even on extreme pose variations. Finally, we report results on real images by first embedding them on the latent space of the pretrained generator. We make the code and pretrained models publicly available at: https://github.com/StelaBou/StyleMask



### Neural Network Panning: Screening the Optimal Sparse Network Before Training
- **Arxiv ID**: http://arxiv.org/abs/2209.13378v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.13378v1)
- **Published**: 2022-09-27 13:31:43+00:00
- **Updated**: 2022-09-27 13:31:43+00:00
- **Authors**: Xiatao Kang, Ping Li, Jiayi Yao, Chengxi Li
- **Comment**: Accepted by ACCV 2022
- **Journal**: None
- **Summary**: Pruning on neural networks before training not only compresses the original models, but also accelerates the network training phase, which has substantial application value. The current work focuses on fine-grained pruning, which uses metrics to calculate weight scores for weight screening, and extends from the initial single-order pruning to iterative pruning. Through these works, we argue that network pruning can be summarized as an expressive force transfer process of weights, where the reserved weights will take on the expressive force from the removed ones for the purpose of maintaining the performance of original networks. In order to achieve optimal expressive force scheduling, we propose a pruning scheme before training called Neural Network Panning which guides expressive force transfer through multi-index and multi-process steps, and designs a kind of panning agent based on reinforcement learning to automate processes. Experimental results show that Panning performs better than various available pruning before training methods.



### CCTCOVID: COVID-19 Detection from Chest X-Ray Images Using Compact Convolutional Transformers
- **Arxiv ID**: http://arxiv.org/abs/2209.13399v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.13399v1)
- **Published**: 2022-09-27 14:02:40+00:00
- **Updated**: 2022-09-27 14:02:40+00:00
- **Authors**: Abdolreza Marefat, Mahdieh Marefat, Javad Hasannataj Joloudari, Mohammad Ali Nematollahi, Reza Lashgari
- **Comment**: None
- **Journal**: None
- **Summary**: COVID-19 is a novel virus that attacks the upper respiratory tract and the lungs. Its person-to-person transmissibility is considerably rapid and this has caused serious problems in approximately every facet of individuals lives. While some infected individuals may remain completely asymptomatic, others have been frequently witnessed to have mild to severe symptoms. In addition to this, thousands of death cases around the globe indicated that detecting COVID-19 is an urgent demand in the communities. Practically, this is prominently done with the help of screening medical images such as Computed Tomography (CT) and X-ray images. However, the cumbersome clinical procedures and a large number of daily cases have imposed great challenges on medical practitioners. Deep Learning-based approaches have demonstrated a profound potential in a wide range of medical tasks. As a result, we introduce a transformer-based method for automatically detecting COVID-19 from X-ray images using Compact Convolutional Transformers (CCT). Our extensive experiments prove the efficacy of the proposed method with an accuracy of 98% which outperforms the previous works.



### A Pathologist-Informed Workflow for Classification of Prostate Glands in Histopathology
- **Arxiv ID**: http://arxiv.org/abs/2209.13408v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13408v1)
- **Published**: 2022-09-27 14:08:19+00:00
- **Updated**: 2022-09-27 14:08:19+00:00
- **Authors**: Alessandro Ferrero, Beatrice Knudsen, Deepika Sirohi, Ross Whitaker
- **Comment**: Published as a workshop paper at MICCAI MOVI 2022
- **Journal**: First International Workshop, MOVI 2022, Held in Conjunction with
  MICCAI 2022, Singapore, September 18, 2022, Proceedings,
  https://link.springer.com/book/10.1007/978-3-031-16961-8
- **Summary**: Pathologists diagnose and grade prostate cancer by examining tissue from needle biopsies on glass slides. The cancer's severity and risk of metastasis are determined by the Gleason grade, a score based on the organization and morphology of prostate cancer glands. For diagnostic work-up, pathologists first locate glands in the whole biopsy core, and -- if they detect cancer -- they assign a Gleason grade. This time-consuming process is subject to errors and significant inter-observer variability, despite strict diagnostic criteria. This paper proposes an automated workflow that follows pathologists' \textit{modus operandi}, isolating and classifying multi-scale patches of individual glands in whole slide images (WSI) of biopsy tissues using distinct steps: (1) two fully convolutional networks segment epithelium versus stroma and gland boundaries, respectively; (2) a classifier network separates benign from cancer glands at high magnification; and (3) an additional classifier predicts the grade of each cancer gland at low magnification. Altogether, this process provides a gland-specific approach for prostate cancer grading that we compare against other machine-learning-based grading methods.



### UAV-based Visual Remote Sensing for Automated Building Inspection
- **Arxiv ID**: http://arxiv.org/abs/2209.13418v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.13418v1)
- **Published**: 2022-09-27 14:18:14+00:00
- **Updated**: 2022-09-27 14:18:14+00:00
- **Authors**: Kushagra Srivastava, Dhruv Patel, Aditya Kumar Jha, Mohhit Kumar Jha, Jaskirat Singh, Ravi Kiran Sarvadevabhatla, Pradeep Kumar Ramancharla, Harikumar Kandath, K. Madhava Krishna
- **Comment**: Paper accepted at CVCIE Workshop at ECCV, 2022 and the project page
  is https://uvrsabi.github.io/
- **Journal**: None
- **Summary**: Unmanned Aerial Vehicle (UAV) based remote sensing system incorporated with computer vision has demonstrated potential for assisting building construction and in disaster management like damage assessment during earthquakes. The vulnerability of a building to earthquake can be assessed through inspection that takes into account the expected damage progression of the associated component and the component's contribution to structural system performance. Most of these inspections are done manually, leading to high utilization of manpower, time, and cost. This paper proposes a methodology to automate these inspections through UAV-based image data collection and a software library for post-processing that helps in estimating the seismic structural parameters. The key parameters considered here are the distances between adjacent buildings, building plan-shape, building plan area, objects on the rooftop and rooftop layout. The accuracy of the proposed methodology in estimating the above-mentioned parameters is verified through field measurements taken using a distance measuring sensor and also from the data obtained through Google Earth. Additional details and code can be accessed from https://uvrsabi.github.io/ .



### Stacking Ensemble Learning in Deep Domain Adaptation for Ophthalmic Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.13420v1
- **DOI**: 10.1007/978-3-030-87000-3_18
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13420v1)
- **Published**: 2022-09-27 14:19:00+00:00
- **Updated**: 2022-09-27 14:19:00+00:00
- **Authors**: Yeganeh Madadi, Vahid Seydi, Jian Sun, Edward Chaum, Siamak Yousefi
- **Comment**: None
- **Journal**: OMIA 2021: Ophthalmic Medical Image Analysis
- **Summary**: Domain adaptation is an attractive approach given the availability of a large amount of labeled data with similar properties but different domains. It is effective in image classification tasks where obtaining sufficient label data is challenging. We propose a novel method, named SELDA, for stacking ensemble learning via extending three domain adaptation methods for effectively solving real-world problems. The major assumption is that when base domain adaptation models are combined, we can obtain a more accurate and robust model by exploiting the ability of each of the base models. We extend Maximum Mean Discrepancy (MMD), Low-rank coding, and Correlation Alignment (CORAL) to compute the adaptation loss in three base models. Also, we utilize a two-fully connected layer network as a meta-model to stack the output predictions of these three well-performing domain adaptation models to obtain high accuracy in ophthalmic image classification tasks. The experimental results using Age-Related Eye Disease Study (AREDS) benchmark ophthalmic dataset demonstrate the effectiveness of the proposed model.



### UniCLIP: Unified Framework for Contrastive Language-Image Pre-training
- **Arxiv ID**: http://arxiv.org/abs/2209.13430v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13430v2)
- **Published**: 2022-09-27 14:36:16+00:00
- **Updated**: 2022-10-31 12:36:18+00:00
- **Authors**: Janghyeon Lee, Jongsuk Kim, Hyounguk Shon, Bumsoo Kim, Seung Hwan Kim, Honglak Lee, Junmo Kim
- **Comment**: Neural Information Processing Systems (NeurIPS) 2022
- **Journal**: None
- **Summary**: Pre-training vision-language models with contrastive objectives has shown promising results that are both scalable to large uncurated datasets and transferable to many downstream applications. Some following works have targeted to improve data efficiency by adding self-supervision terms, but inter-domain (image-text) contrastive loss and intra-domain (image-image) contrastive loss are defined on individual spaces in those works, so many feasible combinations of supervision are overlooked. To overcome this issue, we propose UniCLIP, a Unified framework for Contrastive Language-Image Pre-training. UniCLIP integrates the contrastive loss of both inter-domain pairs and intra-domain pairs into a single universal space. The discrepancies that occur when integrating contrastive loss between different domains are resolved by the three key components of UniCLIP: (1) augmentation-aware feature embedding, (2) MP-NCE loss, and (3) domain dependent similarity measure. UniCLIP outperforms previous vision-language pre-training methods on various single- and multi-modality downstream tasks. In our experiments, we show that each component that comprises UniCLIP contributes well to the final performance.



### OmniNeRF: Hybriding Omnidirectional Distance and Radiance fields for Neural Surface Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2209.13433v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13433v1)
- **Published**: 2022-09-27 14:39:23+00:00
- **Updated**: 2022-09-27 14:39:23+00:00
- **Authors**: Jiaming Shen, Bolin Song, Zirui Wu, Yi Xu
- **Comment**: Accepted by CMSDA 2022
- **Journal**: None
- **Summary**: 3D reconstruction from images has wide applications in Virtual Reality and Automatic Driving, where the precision requirement is very high. Ground-breaking research in the neural radiance field (NeRF) by utilizing Multi-Layer Perceptions has dramatically improved the representation quality of 3D objects. Some later studies improved NeRF by building truncated signed distance fields (TSDFs) but still suffer from the problem of blurred surfaces in 3D reconstruction. In this work, this surface ambiguity is addressed by proposing a novel way of 3D shape representation, OmniNeRF. It is based on training a hybrid implicit field of Omni-directional Distance Field (ODF) and neural radiance field, replacing the apparent density in NeRF with omnidirectional information. Moreover, we introduce additional supervision on the depth map to further improve reconstruction quality. The proposed method has been proven to effectively deal with NeRF defects at the edges of the surface reconstruction, providing higher quality 3D scene reconstruction results.



### Scaling Laws For Deep Learning Based Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/2209.13435v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2209.13435v2)
- **Published**: 2022-09-27 14:44:57+00:00
- **Updated**: 2023-02-23 13:31:20+00:00
- **Authors**: Tobit Klug, Reinhard Heckel
- **Comment**: None
- **Journal**: Published as a conference paper at ICLR 2023
- **Summary**: Deep neural networks trained end-to-end to map a measurement of a (noisy) image to a clean image perform excellent for a variety of linear inverse problems. Current methods are only trained on a few hundreds or thousands of images as opposed to the millions of examples deep networks are trained on in other domains. In this work, we study whether major performance gains are expected from scaling up the training set size. We consider image denoising, accelerated magnetic resonance imaging, and super-resolution and empirically determine the reconstruction quality as a function of training set size, while simultaneously scaling the network size. For all three tasks we find that an initially steep power-law scaling slows significantly already at moderate training set sizes. Interpolating those scaling laws suggests that even training on millions of images would not significantly improve performance. To understand the expected behavior, we analytically characterize the performance of a linear estimator learned with early stopped gradient descent. The result formalizes the intuition that once the error induced by learning the signal model is small relative to the error floor, more training examples do not improve performance.



### Design Perspectives of Multitask Deep Learning Models and Applications
- **Arxiv ID**: http://arxiv.org/abs/2209.13444v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, 68Txx, I.2
- **Links**: [PDF](http://arxiv.org/pdf/2209.13444v1)
- **Published**: 2022-09-27 15:04:31+00:00
- **Updated**: 2022-09-27 15:04:31+00:00
- **Authors**: Yeshwant Singh, Anupam Biswas, Angshuman Bora, Debashish Malakar, Subham Chakraborty, Suman Bera
- **Comment**: To be published in Wiley&IEEE Book "Machine Learning Algorithms for
  Signal and Image Processing"
- **Journal**: None
- **Summary**: In recent years, multi-task learning has turned out to be of great success in various applications. Though single model training has promised great results throughout these years, it ignores valuable information that might help us estimate a metric better. Under learning-related tasks, multi-task learning has been able to generalize the models even better. We try to enhance the feature mapping of the multi-tasking models by sharing features among related tasks and inductive transfer learning. Also, our interest is in learning the task relationships among various tasks for acquiring better benefits from multi-task learning. In this chapter, our objective is to visualize the existing multi-tasking models, compare their performances, the methods used to evaluate the performance of the multi-tasking models, discuss the problems faced during the design and implementation of these models in various domains, and the advantages and milestones achieved by them



### EgoSpeed-Net: Forecasting Speed-Control in Driver Behavior from Egocentric Video Data
- **Arxiv ID**: http://arxiv.org/abs/2209.13459v1
- **DOI**: 10.1145/3557915.3560946
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13459v1)
- **Published**: 2022-09-27 15:25:57+00:00
- **Updated**: 2022-09-27 15:25:57+00:00
- **Authors**: Yichen Ding, Ziming Zhang, Yanhua Li, Xun Zhou
- **Comment**: In Proceedings of the 30th ACM SIGSPATIAL, International Conference
  on Advances in Geographic Information Systems (2022) [accepted as a full
  paper]
- **Journal**: None
- **Summary**: Speed-control forecasting, a challenging problem in driver behavior analysis, aims to predict the future actions of a driver in controlling vehicle speed such as braking or acceleration. In this paper, we try to address this challenge solely using egocentric video data, in contrast to the majority of works in the literature using either third-person view data or extra vehicle sensor data such as GPS, or both. To this end, we propose a novel graph convolutional network (GCN) based network, namely, EgoSpeed-Net. We are motivated by the fact that the position changes of objects over time can provide us very useful clues for forecasting the speed change in future. We first model the spatial relations among the objects from each class, frame by frame, using fully-connected graphs, on top of which GCNs are applied for feature extraction. Then we utilize a long short-term memory network to fuse such features per class over time into a vector, concatenate such vectors and forecast a speed-control action using a multilayer perceptron classifier. We conduct extensive experiments on the Honda Research Institute Driving Dataset and demonstrate the superior performance of EgoSpeed-Net.



### AdaFocusV3: On Unified Spatial-temporal Dynamic Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.13465v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13465v1)
- **Published**: 2022-09-27 15:30:52+00:00
- **Updated**: 2022-09-27 15:30:52+00:00
- **Authors**: Yulin Wang, Yang Yue, Xinhong Xu, Ali Hassani, Victor Kulikov, Nikita Orlov, Shiji Song, Humphrey Shi, Gao Huang
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Recent research has revealed that reducing the temporal and spatial redundancy are both effective approaches towards efficient video recognition, e.g., allocating the majority of computation to a task-relevant subset of frames or the most valuable image regions of each frame. However, in most existing works, either type of redundancy is typically modeled with another absent. This paper explores the unified formulation of spatial-temporal dynamic computation on top of the recently proposed AdaFocusV2 algorithm, contributing to an improved AdaFocusV3 framework. Our method reduces the computational cost by activating the expensive high-capacity network only on some small but informative 3D video cubes. These cubes are cropped from the space formed by frame height, width, and video duration, while their locations are adaptively determined with a light-weighted policy network on a per-sample basis. At test time, the number of the cubes corresponding to each video is dynamically configured, i.e., video cubes are processed sequentially until a sufficiently reliable prediction is produced. Notably, AdaFocusV3 can be effectively trained by approximating the non-differentiable cropping operation with the interpolation of deep features. Extensive empirical results on six benchmark datasets (i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2 and Diving48) demonstrate that our model is considerably more efficient than competitive baselines.



### Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with Extremely Limited Labels
- **Arxiv ID**: http://arxiv.org/abs/2209.13476v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13476v5)
- **Published**: 2022-09-27 15:50:31+00:00
- **Updated**: 2023-03-17 00:41:23+00:00
- **Authors**: Chenyu You, Weicheng Dai, Fenglin Liu, Yifei Min, Haoran Su, Xiaoran Zhang, Xiaoxiao Li, David A. Clifton, Lawrence Staib, James S. Duncan
- **Comment**: In this version: Add theoretical analysis and correct some typos
- **Journal**: None
- **Summary**: Recent studies on contrastive learning have achieved remarkable performance solely by leveraging few labels in the context of medical image segmentation. Existing methods mainly focus on instance discrimination and invariant mapping. However, they face three common pitfalls: (1) tailness: medical image data usually follows an implicit long-tail class distribution. Blindly leveraging all pixels in training hence can lead to the data imbalance issues, and cause deteriorated performance; (2) consistency: it remains unclear whether a segmentation model has learned meaningful and yet consistent anatomical features due to the intra-class variations between different anatomical features; and (3) diversity: the intra-slice correlations within the entire dataset have received significantly less attention. This motivates us to seek a principled approach for strategically making use of the dataset itself to discover similar yet distinct samples from different anatomical views. In this paper, we introduce a novel semi-supervised 2D medical image segmentation framework termed Mine yOur owN Anatomy (MONA), and make three contributions. First, prior work argues that every pixel equally matters to the model training; we observe empirically that this alone is unlikely to define meaningful anatomical features, mainly due to lacking the supervision signal. We show two simple solutions towards learning invariances - through the use of stronger data augmentations and nearest neighbors. Second, we construct a set of objectives that encourage the model to be capable of decomposing medical images into a collection of anatomical features in an unsupervised manner. Lastly, we both empirically and theoretically, demonstrate the efficacy of our MONA on three benchmark datasets with different labeled settings, achieving new state-of-the-art under different labeled semi-supervised settings



### Unsupervised Domain Adaptation with Histogram-gated Image Translation for Delayered IC Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2209.13479v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13479v1)
- **Published**: 2022-09-27 15:53:22+00:00
- **Updated**: 2022-09-27 15:53:22+00:00
- **Authors**: Yee-Yang Tee, Deruo Cheng, Chye-Soon Chee, Tong Lin, Yiqiong Shi, Bah-Hwee Gwee
- **Comment**: 7 pages, 4 figures, To be presented at IEEE PAINE 2022 (oral)
- **Journal**: None
- **Summary**: Deep learning has achieved great success in the challenging circuit annotation task by employing Convolutional Neural Networks (CNN) for the segmentation of circuit structures. The deep learning approaches require a large amount of manually annotated training data to achieve a good performance, which could cause a degradation in performance if a deep learning model trained on a given dataset is applied to a different dataset. This is commonly known as the domain shift problem for circuit annotation, which stems from the possibly large variations in distribution across different image datasets. The different image datasets could be obtained from different devices or different layers within a single device. To address the domain shift problem, we propose Histogram-gated Image Translation (HGIT), an unsupervised domain adaptation framework which transforms images from a given source dataset to the domain of a target dataset, and utilize the transformed images for training a segmentation network. Specifically, our HGIT performs generative adversarial network (GAN)-based image translation and utilizes histogram statistics for data curation. Experiments were conducted on a single labeled source dataset adapted to three different target datasets (without labels for training) and the segmentation performance was evaluated for each target dataset. We have demonstrated that our method achieves the best performance compared to the reported domain adaptation techniques, and is also reasonably close to the fully supervised benchmark.



### Critical Evaluation of LOCO dataset with Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.13499v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.13499v1)
- **Published**: 2022-09-27 16:17:01+00:00
- **Updated**: 2022-09-27 16:17:01+00:00
- **Authors**: Recep Savas, Johannes Hinckeldeyn
- **Comment**: 30 pages, 16 figures
- **Journal**: Hamburg International Conference on Logistics (HICL 2022)
- **Summary**: Purpose: Object detection is rapidly evolving through machine learning technology in automation systems. Well prepared data is necessary to train the algorithms. Accordingly, the objective of this paper is to describe a re-evaluation of the so-called Logistics Objects in Context (LOCO) dataset, which is the first dataset for object detection in the field of intralogistics.   Methodology: We use an experimental research approach with three steps to evaluate the LOCO dataset. Firstly, the images on GitHub were analyzed to understand the dataset better. Secondly, Google Drive Cloud was used for training purposes to revisit the algorithmic implementation and training. Lastly, the LOCO dataset was examined, if it is possible to achieve the same training results in comparison to the original publications.   Findings: The mean average precision, a common benchmark in object detection, achieved in our study was 64.54%, and shows a significant increase from the initial study of the LOCO authors, achieving 41%. However, improvement potential is seen specifically within object types of forklifts and pallet truck.   Originality: This paper presents the first critical replication study of the LOCO dataset for object detection in intralogistics. It shows that the training with better hyperparameters based on LOCO can even achieve a higher accuracy than presented in the original publication. However, there is also further room for improving the LOCO dataset.



### Dense-TNT: Efficient Vehicle Type Classification Neural Network Using Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/2209.13500v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.13500v1)
- **Published**: 2022-09-27 16:17:53+00:00
- **Updated**: 2022-09-27 16:17:53+00:00
- **Authors**: Ruikang Luo, Yaofeng Song, Han Zhao, Yicheng Zhang, Yi Zhang, Nanbin Zhao, Liping Huang, Rong Su
- **Comment**: 10 pages, 8 figures, 5 tables
- **Journal**: None
- **Summary**: Accurate vehicle type classification serves a significant role in the intelligent transportation system. It is critical for ruler to understand the road conditions and usually contributive for the traffic light control system to response correspondingly to alleviate traffic congestion. New technologies and comprehensive data sources, such as aerial photos and remote sensing data, provide richer and high-dimensional information. Also, due to the rapid development of deep neural network technology, image based vehicle classification methods can better extract underlying objective features when processing data. Recently, several deep learning models have been proposed to solve the problem. However, traditional pure convolutional based approaches have constraints on global information extraction, and the complex environment, such as bad weather, seriously limits the recognition capability. To improve the vehicle type classification capability under complex environment, this study proposes a novel Densely Connected Convolutional Transformer in Transformer Neural Network (Dense-TNT) framework for the vehicle type classification by stacking Densely Connected Convolutional Network (DenseNet) and Transformer in Transformer (TNT) layers. Three-region vehicle data and four different weather conditions are deployed for recognition capability evaluation. Experimental findings validate the recognition ability of our proposed vehicle classification model with little decay, even under the heavy foggy weather condition.



### Visual Object Tracking in First Person Vision
- **Arxiv ID**: http://arxiv.org/abs/2209.13502v1
- **DOI**: 10.1007/s11263-022-01694-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13502v1)
- **Published**: 2022-09-27 16:18:47+00:00
- **Updated**: 2022-09-27 16:18:47+00:00
- **Authors**: Matteo Dunnhofer, Antonino Furnari, Giovanni Maria Farinella, Christian Micheloni
- **Comment**: International Journal of Computer Vision (IJCV). arXiv admin note:
  substantial text overlap with arXiv:2108.13665
- **Journal**: None
- **Summary**: The understanding of human-object interactions is fundamental in First Person Vision (FPV). Visual tracking algorithms which follow the objects manipulated by the camera wearer can provide useful information to effectively model such interactions. In the last years, the computer vision community has significantly improved the performance of tracking algorithms for a large variety of target objects and scenarios. Despite a few previous attempts to exploit trackers in the FPV domain, a methodical analysis of the performance of state-of-the-art trackers is still missing. This research gap raises the question of whether current solutions can be used ``off-the-shelf'' or more domain-specific investigations should be carried out. This paper aims to provide answers to such questions. We present the first systematic investigation of single object tracking in FPV. Our study extensively analyses the performance of 42 algorithms including generic object trackers and baseline FPV-specific trackers. The analysis is carried out by focusing on different aspects of the FPV setting, introducing new performance measures, and in relation to FPV-specific tasks. The study is made possible through the introduction of TREK-150, a novel benchmark dataset composed of 150 densely annotated video sequences. Our results show that object tracking in FPV poses new challenges to current visual trackers. We highlight the factors causing such behavior and point out possible research directions. Despite their difficulties, we prove that trackers bring benefits to FPV downstream tasks requiring short-term object tracking. We expect that generic object tracking will gain popularity in FPV as new and FPV-specific methodologies are investigated.



### CrossDTR: Cross-view and Depth-guided Transformers for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2209.13507v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.13507v3)
- **Published**: 2022-09-27 16:23:12+00:00
- **Updated**: 2023-02-03 10:39:37+00:00
- **Authors**: Ching-Yu Tseng, Yi-Rong Chen, Hsin-Ying Lee, Tsung-Han Wu, Wen-Chin Chen, Winston H. Hsu
- **Comment**: Accepted by IEEE International Conference on Robotics and Automation
  (ICRA) 2023. The code is available at https://github.com/sty61010/CrossDTR
- **Journal**: None
- **Summary**: To achieve accurate 3D object detection at a low cost for autonomous driving, many multi-camera methods have been proposed and solved the occlusion problem of monocular approaches. However, due to the lack of accurate estimated depth, existing multi-camera methods often generate multiple bounding boxes along a ray of depth direction for difficult small objects such as pedestrians, resulting in an extremely low recall. Furthermore, directly applying depth prediction modules to existing multi-camera methods, generally composed of large network architectures, cannot meet the real-time requirements of self-driving applications. To address these issues, we propose Cross-view and Depth-guided Transformers for 3D Object Detection, CrossDTR. First, our lightweight depth predictor is designed to produce precise object-wise sparse depth maps and low-dimensional depth embeddings without extra depth datasets during supervision. Second, a cross-view depth-guided transformer is developed to fuse the depth embeddings as well as image features from cameras of different views and generate 3D bounding boxes. Extensive experiments demonstrated that our method hugely surpassed existing multi-camera methods by 10 percent in pedestrian detection and about 3 percent in overall mAP and NDS metrics. Also, computational analyses showed that our method is 5 times faster than prior approaches. Our codes will be made publicly available at https://github.com/sty61010/CrossDTR.



### Motion Transformer with Global Intention Localization and Local Movement Refinement
- **Arxiv ID**: http://arxiv.org/abs/2209.13508v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13508v2)
- **Published**: 2022-09-27 16:23:14+00:00
- **Updated**: 2023-03-18 23:05:00+00:00
- **Authors**: Shaoshuai Shi, Li Jiang, Dengxin Dai, Bernt Schiele
- **Comment**: Accepted by NeurIPS 2022 as Oral Presentation
- **Journal**: None
- **Summary**: Predicting multimodal future behavior of traffic participants is essential for robotic vehicles to make safe decisions. Existing works explore to directly predict future trajectories based on latent features or utilize dense goal candidates to identify agent's destinations, where the former strategy converges slowly since all motion modes are derived from the same feature while the latter strategy has efficiency issue since its performance highly relies on the density of goal candidates. In this paper, we propose Motion TRansformer (MTR) framework that models motion prediction as the joint optimization of global intention localization and local movement refinement. Instead of using goal candidates, MTR incorporates spatial intention priors by adopting a small set of learnable motion query pairs. Each motion query pair takes charge of trajectory prediction and refinement for a specific motion mode, which stabilizes the training process and facilitates better multimodal predictions. Experiments show that MTR achieves state-of-the-art performance on both the marginal and joint motion prediction challenges, ranking 1st on the leaderboards of Waymo Open Motion Dataset. The source code is available at https://github.com/sshaoshuai/MTR.



### StyleSwap: Style-Based Generator Empowers Robust Face Swapping
- **Arxiv ID**: http://arxiv.org/abs/2209.13514v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2209.13514v1)
- **Published**: 2022-09-27 16:35:16+00:00
- **Updated**: 2022-09-27 16:35:16+00:00
- **Authors**: Zhiliang Xu, Hang Zhou, Zhibin Hong, Ziwei Liu, Jiaming Liu, Zhizhi Guo, Junyu Han, Jingtuo Liu, Errui Ding, Jingdong Wang
- **Comment**: Accepted to ECCV 2022. Demo videos and code can be found at
  https://hangz-nju-cuhk.github.io/projects/StyleSwap
- **Journal**: None
- **Summary**: Numerous attempts have been made to the task of person-agnostic face swapping given its wide applications. While existing methods mostly rely on tedious network and loss designs, they still struggle in the information balancing between the source and target faces, and tend to produce visible artifacts. In this work, we introduce a concise and effective framework named StyleSwap. Our core idea is to leverage a style-based generator to empower high-fidelity and robust face swapping, thus the generator's advantage can be adopted for optimizing identity similarity. We identify that with only minimal modifications, a StyleGAN2 architecture can successfully handle the desired information from both source and target. Additionally, inspired by the ToRGB layers, a Swapping-Driven Mask Branch is further devised to improve information blending. Furthermore, the advantage of StyleGAN inversion can be adopted. Particularly, a Swapping-Guided ID Inversion strategy is proposed to optimize identity similarity. Extensive experiments validate that our framework generates high-quality face swapping results that outperform state-of-the-art methods both qualitatively and quantitatively.



### Formal Conceptual Views in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2209.13517v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, 68T07 68T30 03G10
- **Links**: [PDF](http://arxiv.org/pdf/2209.13517v1)
- **Published**: 2022-09-27 16:38:24+00:00
- **Updated**: 2022-09-27 16:38:24+00:00
- **Authors**: Johannes Hirth, Tom Hanika
- **Comment**: 17 pages, 8 figures, 9 tables
- **Journal**: None
- **Summary**: Explaining neural network models is a challenging task that remains unsolved in its entirety to this day. This is especially true for high dimensional and complex data. With the present work, we introduce two notions for conceptual views of a neural network, specifically a many-valued and a symbolic view. Both provide novel analysis methods to enable a human AI analyst to grasp deeper insights into the knowledge that is captured by the neurons of a network. We test the conceptual expressivity of our novel views through different experiments on the ImageNet and Fruit-360 data sets. Furthermore, we show to which extent the views allow to quantify the conceptual similarity of different learning architectures. Finally, we demonstrate how conceptual views can be applied for abductive learning of human comprehensible rules from neurons. In summary, with our work, we contribute to the most relevant task of globally explaining neural networks models.



### FreeSeg: Free Mask from Interpretable Contrastive Language-Image Pretraining for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.13558v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13558v2)
- **Published**: 2022-09-27 17:16:20+00:00
- **Updated**: 2022-12-14 11:28:25+00:00
- **Authors**: Yi Li, Huifeng Yao, Hualiang Wang, Xiaomeng Li
- **Comment**: This paper contains some immature results
- **Journal**: None
- **Summary**: Fully supervised semantic segmentation learns from dense masks, which requires heavy annotation cost for closed set. In this paper, we use natural language as supervision without any pixel-level annotation for open world segmentation. We call the proposed framework as FreeSeg, where the mask is freely available from raw feature map of pretraining model. Compared with zero-shot or openset segmentation, FreeSeg doesn't require any annotated masks, and it widely predicts categories beyond class-agnostic unsupervised segmentation. Specifically, FreeSeg obtains free mask from Image-Text Similarity Map (ITSM) of Interpretable Contrastive Language-Image Pretraining (ICLIP). And our core improvements are the smoothed min pooling for dense ICLIP, with the partial label and pixel strategies for segmentation. Furthermore, FreeSeg is very straight forward without complex design like grouping, clustering or retrieval. Besides the simplicity, the performances of FreeSeg surpass previous state-of-the-art at large margins, e.g. 13.4% higher at mIoU on VOC dataset in the same settings.



### Efficient Non-Parametric Optimizer Search for Diverse Tasks
- **Arxiv ID**: http://arxiv.org/abs/2209.13575v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2209.13575v1)
- **Published**: 2022-09-27 17:51:31+00:00
- **Updated**: 2022-09-27 17:51:31+00:00
- **Authors**: Ruochen Wang, Yuanhao Xiong, Minhao Cheng, Cho-Jui Hsieh
- **Comment**: Accepted at NeurIPS 2022. Code will be released prior to the
  conference. This is only a preprint, not the final camera ready version
- **Journal**: None
- **Summary**: Efficient and automated design of optimizers plays a crucial role in full-stack AutoML systems. However, prior methods in optimizer search are often limited by their scalability, generability, or sample efficiency. With the goal of democratizing research and application of optimizer search, we present the first efficient, scalable and generalizable framework that can directly search on the tasks of interest. We first observe that optimizer updates are fundamentally mathematical expressions applied to the gradient. Inspired by the innate tree structure of the underlying math expressions, we re-arrange the space of optimizers into a super-tree, where each path encodes an optimizer. This way, optimizer search can be naturally formulated as a path-finding problem, allowing a variety of well-established tree traversal methods to be used as the search algorithm. We adopt an adaptation of the Monte Carlo method to tree search, equipped with rejection sampling and equivalent-form detection that leverage the characteristics of optimizer update rules to further boost the sample efficiency. We provide a diverse set of tasks to benchmark our algorithm and demonstrate that, with only 128 evaluations, the proposed framework can discover optimizers that surpass both human-designed counterparts and prior optimizer search methods.



### Learning State-Aware Visual Representations from Audible Interactions
- **Arxiv ID**: http://arxiv.org/abs/2209.13583v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13583v1)
- **Published**: 2022-09-27 17:57:13+00:00
- **Updated**: 2022-09-27 17:57:13+00:00
- **Authors**: Himangi Mittal, Pedro Morgado, Unnat Jain, Abhinav Gupta
- **Comment**: NeurIPS 2022. Code available at https://github.com/HimangiM/RepLAI
- **Journal**: None
- **Summary**: We propose a self-supervised algorithm to learn representations from egocentric video data. Recently, significant efforts have been made to capture humans interacting with their own environments as they go about their daily activities. In result, several large egocentric datasets of interaction-rich multi-modal data have emerged. However, learning representations from videos can be challenging. First, given the uncurated nature of long-form continuous videos, learning effective representations require focusing on moments in time when interactions take place. Second, visual representations of daily activities should be sensitive to changes in the state of the environment. However, current successful multi-modal learning frameworks encourage representation invariance over time. To address these challenges, we leverage audio signals to identify moments of likely interactions which are conducive to better learning. We also propose a novel self-supervised objective that learns from audible state changes caused by interactions. We validate these contributions extensively on two large-scale egocentric datasets, EPIC-Kitchens-100 and the recently released Ego4D, and show improvements on several downstream tasks, including action recognition, long-term action anticipation, and object state change classification.



### Learning-Based Dimensionality Reduction for Computing Compact and Effective Local Feature Descriptors
- **Arxiv ID**: http://arxiv.org/abs/2209.13586v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.13586v1)
- **Published**: 2022-09-27 17:59:04+00:00
- **Updated**: 2022-09-27 17:59:04+00:00
- **Authors**: Hao Dong, Xieyuanli Chen, Mihai Dusmanu, Viktor Larsson, Marc Pollefeys, Cyrill Stachniss
- **Comment**: 7 pages, 7 figures
- **Journal**: None
- **Summary**: A distinctive representation of image patches in form of features is a key component of many computer vision and robotics tasks, such as image matching, image retrieval, and visual localization. State-of-the-art descriptors, from hand-crafted descriptors such as SIFT to learned ones such as HardNet, are usually high dimensional; 128 dimensions or even more. The higher the dimensionality, the larger the memory consumption and computational time for approaches using such descriptors. In this paper, we investigate multi-layer perceptrons (MLPs) to extract low-dimensional but high-quality descriptors. We thoroughly analyze our method in unsupervised, self-supervised, and supervised settings, and evaluate the dimensionality reduction results on four representative descriptors. We consider different applications, including visual localization, patch verification, image matching and retrieval. The experiments show that our lightweight MLPs achieve better dimensionality reduction than PCA. The lower-dimensional descriptors generated by our approach outperform the original higher-dimensional descriptors in downstream tasks, especially for the hand-crafted ones. The code will be available at https://github.com/PRBonn/descriptor-dr.



### Sauron U-Net: Simple automated redundancy elimination in medical image segmentation via filter pruning
- **Arxiv ID**: http://arxiv.org/abs/2209.13590v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2209.13590v1)
- **Published**: 2022-09-27 17:59:41+00:00
- **Updated**: 2022-09-27 17:59:41+00:00
- **Authors**: Juan Miguel Valverde, Artem Shatillo, Jussi Tohka
- **Comment**: None
- **Journal**: None
- **Summary**: We present Sauron, a filter pruning method that eliminates redundant feature maps by discarding the corresponding filters with automatically-adjusted layer-specific thresholds. Furthermore, Sauron minimizes a regularization term that, as we show with various metrics, promotes the formation of feature maps clusters. In contrast to most filter pruning methods, Sauron is single-phase, similarly to typical neural network optimization, requiring fewer hyperparameters and design decisions. Additionally, unlike other cluster-based approaches, our method does not require pre-selecting the number of clusters, which is non-trivial to determine and varies across layers. We evaluated Sauron and three state-of-the-art filter pruning methods on three medical image segmentation tasks. This is an area where filter pruning has received little attention and where it can help building efficient models for medical grade computers that cannot use cloud services due to privacy considerations. Sauron achieved models with higher performance and pruning rate than the competing pruning methods. Additionally, since Sauron removes filters during training, its optimization accelerated over time. Finally, we show that the feature maps of a Sauron-pruned model were highly interpretable. The Sauron code is publicly available at https://github.com/jmlipman/SauronUNet.



### Scalable and Equivariant Spherical CNNs by Discrete-Continuous (DISCO) Convolutions
- **Arxiv ID**: http://arxiv.org/abs/2209.13603v3
- **DOI**: None
- **Categories**: **cs.CV**, astro-ph.IM, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13603v3)
- **Published**: 2022-09-27 18:00:01+00:00
- **Updated**: 2023-01-28 15:27:15+00:00
- **Authors**: Jeremy Ocampo, Matthew A. Price, Jason D. McEwen
- **Comment**: 19 pages, 7 figures, accepted by ICLR 2023
- **Journal**: None
- **Summary**: No existing spherical convolutional neural network (CNN) framework is both computationally scalable and rotationally equivariant. Continuous approaches capture rotational equivariance but are often prohibitively computationally demanding. Discrete approaches offer more favorable computational performance but at the cost of equivariance. We develop a hybrid discrete-continuous (DISCO) group convolution that is simultaneously equivariant and computationally scalable to high-resolution. While our framework can be applied to any compact group, we specialize to the sphere. Our DISCO spherical convolutions exhibit $\text{SO}(3)$ rotational equivariance, where $\text{SO}(n)$ is the special orthogonal group representing rotations in $n$-dimensions. When restricting rotations of the convolution to the quotient space $\text{SO}(3)/\text{SO}(2)$ for further computational enhancements, we recover a form of asymptotic $\text{SO}(3)$ rotational equivariance. Through a sparse tensor implementation we achieve linear scaling in number of pixels on the sphere for both computational cost and memory usage. For 4k spherical images we realize a saving of $10^9$ in computational cost and $10^4$ in memory usage when compared to the most efficient alternative equivariant spherical convolution. We apply the DISCO spherical CNN framework to a number of benchmark dense-prediction problems on the sphere, such as semantic segmentation and depth estimation, on all of which we achieve the state-of-the-art performance.



### LapGM: A Multisequence MR Bias Correction and Normalization Model
- **Arxiv ID**: http://arxiv.org/abs/2209.13619v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/2209.13619v1)
- **Published**: 2022-09-27 18:28:02+00:00
- **Updated**: 2022-09-27 18:28:02+00:00
- **Authors**: Luciano Vinas, Arash A. Amini, Jade Fischer, Atchar Sudhyadhom
- **Comment**: None
- **Journal**: None
- **Summary**: A spatially regularized Gaussian mixture model, LapGM, is proposed for the bias field correction and magnetic resonance normalization problem. The proposed spatial regularizer gives practitioners fine-tuned control between balancing bias field removal and preserving image contrast preservation for multi-sequence, magnetic resonance images. The fitted Gaussian parameters of LapGM serve as control values which can be used to normalize image intensities across different patient scans. LapGM is compared to well-known debiasing algorithm N4ITK in both the single and multi-sequence setting. As a normalization procedure, LapGM is compared to known techniques such as: max normalization, Z-score normalization, and a water-masked region-of-interest normalization. Lastly a CUDA-accelerated Python package $\texttt{lapgm}$ is provided from the authors for use.



### Reconstruction-guided attention improves the robustness and shape processing of neural networks
- **Arxiv ID**: http://arxiv.org/abs/2209.13620v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2209.13620v2)
- **Published**: 2022-09-27 18:32:22+00:00
- **Updated**: 2023-02-07 23:14:54+00:00
- **Authors**: Seoyoung Ahn, Hossein Adeli, Gregory J. Zelinsky
- **Comment**: paper accepted to SVRHM, Neurips workshop 2022
- **Journal**: None
- **Summary**: Many visual phenomena suggest that humans use top-down generative or reconstructive processes to create visual percepts (e.g., imagery, object completion, pareidolia), but little is known about the role reconstruction plays in robust object recognition. We built an iterative encoder-decoder network that generates an object reconstruction and used it as top-down attentional feedback to route the most relevant spatial and feature information to feed-forward object recognition processes. We tested this model using the challenging out-of-distribution digit recognition dataset, MNIST-C, where 15 different types of transformation and corruption are applied to handwritten digit images. Our model showed strong generalization performance against various image perturbations, on average outperforming all other models including feedforward CNNs and adversarially trained networks. Our model is particularly robust to blur, noise, and occlusion corruptions, where shape perception plays an important role. Ablation studies further reveal two complementary roles of spatial and feature-based attention in robust object recognition, with the former largely consistent with spatial masking benefits in the attention literature (the reconstruction serves as a mask) and the latter mainly contributing to the model's inference speed (i.e., number of time steps to reach a certain confidence threshold) by reducing the space of possible object hypotheses. We also observed that the model sometimes hallucinates a non-existing pattern out of noise, leading to highly interpretable human-like errors. Our study shows that modeling reconstruction-based feedback endows AI systems with a powerful attention mechanism, which can help us understand the role of generating perception in human visual processing.



### Rethinking Clustering-Based Pseudo-Labeling for Unsupervised Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/2209.13635v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2209.13635v1)
- **Published**: 2022-09-27 19:04:36+00:00
- **Updated**: 2022-09-27 19:04:36+00:00
- **Authors**: Xingping Dong, Jianbing Shen, Ling Shao
- **Comment**: Accepted by European Conference on Computer Vision (ECCV), 2022
- **Journal**: None
- **Summary**: The pioneering method for unsupervised meta-learning, CACTUs, is a clustering-based approach with pseudo-labeling. This approach is model-agnostic and can be combined with supervised algorithms to learn from unlabeled data. However, it often suffers from label inconsistency or limited diversity, which leads to poor performance. In this work, we prove that the core reason for this is lack of a clustering-friendly property in the embedding space. We address this by minimizing the inter- to intra-class similarity ratio to provide clustering-friendly embedding features, and validate our approach through comprehensive experiments. Note that, despite only utilizing a simple clustering algorithm (k-means) in our embedding space to obtain the pseudo-labels, we achieve significant improvement. Moreover, we adopt a progressive evaluation mechanism to obtain more diverse samples in order to further alleviate the limited diversity problem. Finally, our approach is also model-agnostic and can easily be integrated into existing supervised methods. To demonstrate its generalization ability, we integrate it into two representative algorithms: MAML and EP. The results on three main few-shot benchmarks clearly show that the proposed method achieves significant improvement compared to state-of-the-art models. Notably, our approach also outperforms the corresponding supervised method in two tasks.



### 3D Rendering Framework for Data Augmentation in Optical Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/2209.14970v1
- **DOI**: 10.1109/ISSCS52333.2021.9497438
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.14970v1)
- **Published**: 2022-09-27 19:31:23+00:00
- **Updated**: 2022-09-27 19:31:23+00:00
- **Authors**: Andreas Spruck, Maximiliane Hawesch, Anatol Maier, Christian Riess, Jürgen Seiler, André Kaup
- **Comment**: IEEE International Symposium on Signals, Circuits and Systems
  (ISSCS), 1-4, July 2021
- **Journal**: None
- **Summary**: In this paper, we propose a data augmentation framework for Optical Character Recognition (OCR). The proposed framework is able to synthesize new viewing angles and illumination scenarios, effectively enriching any available OCR dataset. Its modular structure allows to be modified to match individual user requirements. The framework enables to comfortably scale the enlargement factor of the available dataset. Furthermore, the proposed method is not restricted to single frame OCR but can also be applied to video OCR. We demonstrate the performance of our framework by augmenting a 15% subset of the common Brno Mobile OCR dataset. Our proposed framework is capable of leveraging the performance of OCR applications especially for small datasets. Applying the proposed method, improvements of up to 2.79 percentage points in terms of Character Error Rate (CER), and up to 7.88 percentage points in terms of Word Error Rate (WER) are achieved on the subset. Especially the recognition of challenging text lines can be improved. The CER may be decreased by up to 14.92 percentage points and the WER by up to 18.19 percentage points for this class. Moreover, we are able to achieve smaller error rates when training on the 15% subset augmented with the proposed method than on the original non-augmented full dataset.



### CEC-CNN: A Consecutive Expansion-Contraction Convolutional Network for Very Small Resolution Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2209.13661v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13661v1)
- **Published**: 2022-09-27 20:01:12+00:00
- **Updated**: 2022-09-27 20:01:12+00:00
- **Authors**: Ioannis Vezakis, Antonios Vezakis, Sofia Gourtsoyianni, Vassilis Koutoulidis, George K. Matsopoulos, Dimitrios Koutsouris
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNNs) for image classification successively alternate convolutions and downsampling operations, such as pooling layers or strided convolutions, resulting in lower resolution features the deeper the network gets. These downsampling operations save computational resources and provide some translational invariance as well as a bigger receptive field at the next layers. However, an inherent side-effect of this is that high-level features, produced at the deep end of the network, are always captured in low resolution feature maps. The inverse is also true, as shallow layers always contain small scale features. In biomedical image analysis engineers are often tasked with classifying very small image patches which carry only a limited amount of information. By their nature, these patches may not even contain objects, with the classification depending instead on the detection of subtle underlying patterns with an unknown scale in the image's texture. In these cases every bit of information is valuable; thus, it is important to extract the maximum number of informative features possible. Driven by these considerations, we introduce a new CNN architecture which preserves multi-scale features from deep, intermediate, and shallow layers by utilizing skip connections along with consecutive contractions and expansions of the feature maps. Using a dataset of very low resolution patches from Pancreatic Ductal Adenocarcinoma (PDAC) CT scans we demonstrate that our network can outperform current state of the art models.



### Mixed-domain Training Improves Multi-Mission Terrain Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2209.13674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2209.13674v1)
- **Published**: 2022-09-27 20:25:24+00:00
- **Updated**: 2022-09-27 20:25:24+00:00
- **Authors**: Grace Vincent, Alice Yepremyan, Jingdao Chen, Edwin Goh
- **Comment**: Accepted to ECCV 2022 AI4Space Workshop
- **Journal**: None
- **Summary**: Planetary rover missions must utilize machine learning-based perception to continue extra-terrestrial exploration with little to no human presence. Martian terrain segmentation has been critical for rover navigation and hazard avoidance to perform further exploratory tasks, e.g. soil sample collection and searching for organic compounds. Current Martian terrain segmentation models require a large amount of labeled data to achieve acceptable performance, and also require retraining for deployment across different domains, i.e. different rover missions, or different tasks, i.e. geological identification and navigation. This research proposes a semi-supervised learning approach that leverages unsupervised contrastive pretraining of a backbone for a multi-mission semantic segmentation for Martian surfaces. This model will expand upon the current Martian segmentation capabilities by being able to deploy across different Martian rover missions for terrain navigation, by utilizing a mixed-domain training set that ensures feature diversity. Evaluation results of using average pixel accuracy show that a semi-supervised mixed-domain approach improves accuracy compared to single domain training and supervised training by reaching an accuracy of 97% for the Mars Science Laboratory's Curiosity Rover and 79.6% for the Mars 2020 Perseverance Rover. Further, providing different weighting methods to loss functions improved the models correct predictions for minority or rare classes by over 30% using the recall metric compared to standard cross-entropy loss. These results can inform future multi-mission and multi-task semantic segmentation for rover missions in a data-efficient manner.



### V2XP-ASG: Generating Adversarial Scenes for Vehicle-to-Everything Perception
- **Arxiv ID**: http://arxiv.org/abs/2209.13679v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2209.13679v3)
- **Published**: 2022-09-27 20:34:41+00:00
- **Updated**: 2023-03-14 04:58:08+00:00
- **Authors**: Hao Xiang, Runsheng Xu, Xin Xia, Zhaoliang Zheng, Bolei Zhou, Jiaqi Ma
- **Comment**: ICRA 2023, see https://github.com/XHwind/V2XP-ASG
- **Journal**: None
- **Summary**: Recent advancements in Vehicle-to-Everything communication technology have enabled autonomous vehicles to share sensory information to obtain better perception performance. With the rapid growth of autonomous vehicles and intelligent infrastructure, the V2X perception systems will soon be deployed at scale, which raises a safety-critical question: \textit{how can we evaluate and improve its performance under challenging traffic scenarios before the real-world deployment?} Collecting diverse large-scale real-world test scenes seems to be the most straightforward solution, but it is expensive and time-consuming, and the collections can only cover limited scenarios. To this end, we propose the first open adversarial scene generator V2XP-ASG that can produce realistic, challenging scenes for modern LiDAR-based multi-agent perception systems. V2XP-ASG learns to construct an adversarial collaboration graph and simultaneously perturb multiple agents' poses in an adversarial and plausible manner. The experiments demonstrate that V2XP-ASG can effectively identify challenging scenes for a large range of V2X perception systems. Meanwhile, by training on the limited number of generated challenging scenes, the accuracy of V2X perception systems can be further improved by 12.3\% on challenging and 4\% on normal scenes. Our code will be released at https://github.com/XHwind/V2XP-ASG.



### What Does DALL-E 2 Know About Radiology?
- **Arxiv ID**: http://arxiv.org/abs/2209.13696v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2209.13696v1)
- **Published**: 2022-09-27 21:15:47+00:00
- **Updated**: 2022-09-27 21:15:47+00:00
- **Authors**: Lisa C. Adams, Felix Busch, Daniel Truhn, Marcus R. Makowski, Hugo JWL. Aerts, Keno K. Bressem
- **Comment**: 4 Figures
- **Journal**: None
- **Summary**: Generative models such as DALL-E 2 could represent a promising future tool for image generation, augmentation, and manipulation for artificial intelligence research in radiology provided that these models have sufficient medical domain knowledge. Here we show that DALL-E 2 has learned relevant representations of X-ray images with promising capabilities in terms of zero-shot text-to-image generation of new images, continuation of an image beyond its original boundaries, or removal of elements, while pathology generation or CT, MRI, and ultrasound images are still limited. The use of generative models for augmenting and generating radiological data thus seems feasible, even if further fine-tuning and adaptation of these models to the respective domain is required beforehand.



### An Overview of the Data-Loader Landscape: Comparative Performance Analysis
- **Arxiv ID**: http://arxiv.org/abs/2209.13705v1
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, cs.LG, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/2209.13705v1)
- **Published**: 2022-09-27 21:40:56+00:00
- **Updated**: 2022-09-27 21:40:56+00:00
- **Authors**: Iason Ofeidis, Diego Kiedanski, Leandros Tassiulas
- **Comment**: 17 pages, 28 figures
- **Journal**: None
- **Summary**: Dataloaders, in charge of moving data from storage into GPUs while training machine learning models, might hold the key to drastically improving the performance of training jobs. Recent advances have shown promise not only by considerably decreasing training time but also by offering new features such as loading data from remote storage like S3. In this paper, we are the first to distinguish the dataloader as a separate component in the Deep Learning (DL) workflow and to outline its structure and features. Finally, we offer a comprehensive comparison of the different dataloading libraries available, their trade-offs in terms of functionality, usability, and performance and the insights derived from them.



### Deep Learning Based Detection of Enlarged Perivascular Spaces on Brain MRI
- **Arxiv ID**: http://arxiv.org/abs/2209.13727v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13727v2)
- **Published**: 2022-09-27 22:35:41+00:00
- **Updated**: 2022-10-14 21:13:03+00:00
- **Authors**: Tanweer Rashid, Hangfan Liu, Jeffrey B. Ware, Karl Li, Jose Rafael Romero, Elyas Fadaee, Ilya M. Nasrallah, Saima Hilal, R. Nick Bryan, Timothy M. Hughes, Christos Davatzikos, Lenore Launer, Sudha Seshadri, Susan R. Heckbert, Mohamad Habes
- **Comment**: None
- **Journal**: None
- **Summary**: BACKGROUND AND PURPOSE: Deep learning has been demonstrated effective in many neuroimaging applications. However, in many scenarios, the number of imaging sequences capturing information related to small vessel disease lesions is insufficient to support data-driven techniques. Additionally, cohort-based studies may not always have the optimal or essential imaging sequences for accurate lesion detection. Therefore, it is necessary to determine which imaging sequences are crucial for precise detection. This study introduces a novel deep learning framework to detect enlarged perivascular spaces (ePVS) and aims to find the optimal combination of MRI sequences for deep learning-based quantification. MATERIALS AND METHODS: We implemented an effective lightweight U-Net adapted for ePVS detection and comprehensively investigated different combinations of information from SWI, FLAIR, T1-weighted (T1w), and T2-weighted (T2w) MRI sequences. The training data included 21 participants, which were randomly selected from the MESA cohort. Participants had ePVS 683 lesions on average. For T1w, T2w, and FLAIR images, the MESA study collected 3D isotropic MRI scans at six different sites with Siemens scanners. Our training data included participants from all these sites and all the scanner models, and the proposed model was applied to the whole brain instead of selective regions. RESULTS: The experimental results showed that T2w MRI is the most important for accurate ePVS detection, and the incorporation of SWI, FLAIR and T1w MRI in the deep neural network had minor improvements in accuracy and resulted in the highest sensitivity and precision (sensitivity =0.82, precision =0.83). The proposed method achieved comparable accuracy at a minimal time cost compared to manual reading.



### Towards Regression-Free Neural Networks for Diverse Compute Platforms
- **Arxiv ID**: http://arxiv.org/abs/2209.13740v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2209.13740v1)
- **Published**: 2022-09-27 23:19:16+00:00
- **Updated**: 2022-09-27 23:19:16+00:00
- **Authors**: Rahul Duggal, Hao Zhou, Shuo Yang, Jun Fang, Yuanjun Xiong, Wei Xia
- **Comment**: To be presented at ECCV 2022
- **Journal**: None
- **Summary**: With the shift towards on-device deep learning, ensuring a consistent behavior of an AI service across diverse compute platforms becomes tremendously important. Our work tackles the emergent problem of reducing predictive inconsistencies arising as negative flips: test samples that are correctly predicted by a less accurate model, but incorrectly by a more accurate one. We introduce REGression constrained Neural Architecture Search (REG-NAS) to design a family of highly accurate models that engender fewer negative flips. REG-NAS consists of two components: (1) A novel architecture constraint that enables a larger model to contain all the weights of the smaller one thus maximizing weight sharing. This idea stems from our observation that larger weight sharing among networks leads to similar sample-wise predictions and results in fewer negative flips; (2) A novel search reward that incorporates both Top-1 accuracy and negative flips in the architecture search metric. We demonstrate that \regnas can successfully find desirable architectures with few negative flips in three popular architecture search spaces. Compared to the existing state-of-the-art approach, REG-NAS enables 33-48% relative reduction of negative flips.



