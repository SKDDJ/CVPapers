# Arxiv Papers in cs.CV on 2022-08-13
### Defense against Backdoor Attacks via Identifying and Purifying Bad Neurons
- **Arxiv ID**: http://arxiv.org/abs/2208.06537v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.06537v1)
- **Published**: 2022-08-13 01:10:20+00:00
- **Updated**: 2022-08-13 01:10:20+00:00
- **Authors**: Mingyuan Fan, Yang Liu, Cen Chen, Ximeng Liu, Wenzhong Guo
- **Comment**: None
- **Journal**: None
- **Summary**: The opacity of neural networks leads their vulnerability to backdoor attacks, where hidden attention of infected neurons is triggered to override normal predictions to the attacker-chosen ones. In this paper, we propose a novel backdoor defense method to mark and purify the infected neurons in the backdoored neural networks. Specifically, we first define a new metric, called benign salience. By combining the first-order gradient to retain the connections between neurons, benign salience can identify the infected neurons with higher accuracy than the commonly used metric in backdoor defense. Then, a new Adaptive Regularization (AR) mechanism is proposed to assist in purifying these identified infected neurons via fine-tuning. Due to the ability to adapt to different magnitudes of parameters, AR can provide faster and more stable convergence than the common regularization mechanism in neuron purifying. Extensive experimental results demonstrate that our method can erase the backdoor in neural networks with negligible performance degradation.



### MaskBlock: Transferable Adversarial Examples with Bayes Approach
- **Arxiv ID**: http://arxiv.org/abs/2208.06538v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.06538v1)
- **Published**: 2022-08-13 01:20:39+00:00
- **Updated**: 2022-08-13 01:20:39+00:00
- **Authors**: Mingyuan Fan, Cen Chen, Ximeng Liu, Wenzhong Guo
- **Comment**: Under Review
- **Journal**: None
- **Summary**: The transferability of adversarial examples (AEs) across diverse models is of critical importance for black-box adversarial attacks, where attackers cannot access the information about black-box models. However, crafted AEs always present poor transferability. In this paper, by regarding the transferability of AEs as generalization ability of the model, we reveal that vanilla black-box attacks craft AEs via solving a maximum likelihood estimation (MLE) problem. For MLE, the results probably are model-specific local optimum when available data is small, i.e., limiting the transferability of AEs. By contrast, we re-formulate crafting transferable AEs as the maximizing a posteriori probability estimation problem, which is an effective approach to boost the generalization of results with limited available data. Because Bayes posterior inference is commonly intractable, a simple yet effective method called MaskBlock is developed to approximately estimate. Moreover, we show that the formulated framework is a generalization version for various attack methods. Extensive experiments illustrate MaskBlock can significantly improve the transferability of crafted adversarial examples by up to about 20%.



### ExpansionNet v2: Block Static Expansion in fast end to end training for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/2208.06551v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06551v3)
- **Published**: 2022-08-13 02:50:35+00:00
- **Updated**: 2022-08-19 19:44:50+00:00
- **Authors**: Jia Cheng Hu, Roberto Cavicchioli, Alessandro Capotondi
- **Comment**: None
- **Journal**: None
- **Summary**: Expansion methods explore the possibility of performance bottlenecks in the input length in Deep Learning methods. In this work, we introduce the Block Static Expansion which distributes and processes the input over a heterogeneous and arbitrarily big collection of sequences characterized by a different length compared to the input one. Adopting this method we introduce a model called ExpansionNet v2, which is trained using our novel training strategy, designed to be not only effective but also 6 times faster compared to the standard approach of recent works in Image Captioning. The model achieves the state of art performance over the MS-COCO 2014 captioning challenge with a score of 143.7 CIDEr-D in the offline test split, 140.8 CIDEr-D in the online evaluation server and 72.9 All-CIDEr on the nocaps validation set. Source code available at: https://github.com/jchenghu/ExpansionNet_v2



### Memory Efficient Temporal & Visual Graph Model for Unsupervised Video Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2208.06554v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06554v1)
- **Published**: 2022-08-13 02:56:10+00:00
- **Updated**: 2022-08-13 02:56:10+00:00
- **Authors**: Xinyue Hu, Lin Gu, Liangchen Liu, Ruijiang Li, Chang Su, Tatsuya Harada, Yingying Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Existing video domain adaption (DA) methods need to store all temporal combinations of video frames or pair the source and target videos, which are memory cost expensive and can't scale up to long videos. To address these limitations, we propose a memory-efficient graph-based video DA approach as follows. At first our method models each source or target video by a graph: nodes represent video frames and edges represent the temporal or visual similarity relationship between frames. We use a graph attention network to learn the weight of individual frames and simultaneously align the source and target video into a domain-invariant graph feature space. Instead of storing a large number of sub-videos, our method only constructs one graph with a graph attention mechanism for one video, reducing the memory cost substantially. The extensive experiments show that, compared with the state-of-art methods, we achieved superior performance while reducing the memory cost significantly.



### Finding Point with Image: An End-to-End Benchmark for Vision-based UAV Localization
- **Arxiv ID**: http://arxiv.org/abs/2208.06561v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06561v1)
- **Published**: 2022-08-13 03:25:50+00:00
- **Updated**: 2022-08-13 03:25:50+00:00
- **Authors**: Ming Dai, Jiahao Chen, Yusheng Lu, Wenlong Hao, Enhui Zheng
- **Comment**: 13 pages, 12 figures
- **Journal**: None
- **Summary**: In the past, image retrieval was the mainstream solution for cross-view geolocation and UAV visual localization tasks. In a nutshell, the way of image retrieval is to obtain the final required information, such as GPS, through a transitional perspective. However, the way of image retrieval is not completely end-to-end. And there are some redundant operations such as the need to prepare the feature library in advance, and the sampling interval problem of the gallery construction, which make it difficult to implement large-scale applications. In this article we propose an end-to-end positioning scheme, Finding Point with Image (FPI), which aims to directly find the corresponding location in the image of source B (satellite-view) through the image of source A (drone-view). To verify the feasibility of our framework, we construct a new dataset (UL14), which is designed to solve the UAV visual self-localization task. At the same time, we also build a transformer-based baseline to achieve end-to-end training. In addition, the previous evaluation methods are no longer applicable under the framework of FPI. Thus, Metre-level Accuracy (MA) and Relative Distance Score (RDS) are proposed to evaluate the accuracy of UAV localization. At the same time, we preliminarily compare FPI and image retrieval method, and the structure of FPI achieves better performance in both speed and efficiency. In particular, the task of FPI remains great challenges due to the large differences between different views and the drastic spatial scale transformation.



### Enhanced Vehicle Re-identification for ITS: A Feature Fusion approach using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.06579v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06579v1)
- **Published**: 2022-08-13 05:59:16+00:00
- **Updated**: 2022-08-13 05:59:16+00:00
- **Authors**: Ashutosh Holla B, Manohara Pai M. M, Ujjwal Verma, Radhika M. Pai
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, the development of robust Intelligent transportation systems (ITS) is tackled across the globe to provide better traffic efficiency by reducing frequent traffic problems. As an application of ITS, vehicle re-identification has gained ample interest in the domain of computer vision and robotics. Convolutional neural network (CNN) based methods are developed to perform vehicle re-identification to address key challenges such as occlusion, illumination change, scale, etc. The advancement of transformers in computer vision has opened an opportunity to explore the re-identification process further to enhance performance. In this paper, a framework is developed to perform the re-identification of vehicles across CCTV cameras. To perform re-identification, the proposed framework fuses the vehicle representation learned using a CNN and a transformer model. The framework is tested on a dataset that contains 81 unique vehicle identities observed across 20 CCTV cameras. From the experiments, the fused vehicle re-identification framework yields an mAP of 61.73% which is significantly better when compared with the standalone CNN or transformer model.



### Confidence Matters: Inspecting Backdoors in Deep Neural Networks via Distribution Transfer
- **Arxiv ID**: http://arxiv.org/abs/2208.06592v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.06592v1)
- **Published**: 2022-08-13 08:16:28+00:00
- **Updated**: 2022-08-13 08:16:28+00:00
- **Authors**: Tong Wang, Yuan Yao, Feng Xu, Miao Xu, Shengwei An, Ting Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Backdoor attacks have been shown to be a serious security threat against deep learning models, and detecting whether a given model has been backdoored becomes a crucial task. Existing defenses are mainly built upon the observation that the backdoor trigger is usually of small size or affects the activation of only a few neurons. However, the above observations are violated in many cases especially for advanced backdoor attacks, hindering the performance and applicability of the existing defenses. In this paper, we propose a backdoor defense DTInspector built upon a new observation. That is, an effective backdoor attack usually requires high prediction confidence on the poisoned training samples, so as to ensure that the trained model exhibits the targeted behavior with a high probability. Based on this observation, DTInspector first learns a patch that could change the predictions of most high-confidence data, and then decides the existence of backdoor by checking the ratio of prediction changes after applying the learned patch on the low-confidence data. Extensive evaluations on five backdoor attacks, four datasets, and three advanced attacking types demonstrate the effectiveness of the proposed defense.



### Self-supervised Matting-specific Portrait Enhancement and Generation
- **Arxiv ID**: http://arxiv.org/abs/2208.06601v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.06601v1)
- **Published**: 2022-08-13 09:00:02+00:00
- **Updated**: 2022-08-13 09:00:02+00:00
- **Authors**: Yangyang Xu Zeyang Zhou, Shengfeng He
- **Comment**: None
- **Journal**: None
- **Summary**: We resolve the ill-posed alpha matting problem from a completely different perspective. Given an input portrait image, instead of estimating the corresponding alpha matte, we focus on the other end, to subtly enhance this input so that the alpha matte can be easily estimated by any existing matting models. This is accomplished by exploring the latent space of GAN models. It is demonstrated that interpretable directions can be found in the latent space and they correspond to semantic image transformations. We further explore this property in alpha matting. Particularly, we invert an input portrait into the latent code of StyleGAN, and our aim is to discover whether there is an enhanced version in the latent space which is more compatible with a reference matting model. We optimize multi-scale latent vectors in the latent spaces under four tailored losses, ensuring matting-specificity and subtle modifications on the portrait. We demonstrate that the proposed method can refine real portrait images for arbitrary matting models, boosting the performance of automatic alpha matting by a large margin. In addition, we leverage the generative property of StyleGAN, and propose to generate enhanced portrait data which can be treated as the pseudo GT. It addresses the problem of expensive alpha matte annotation, further augmenting the matting performance of existing models. Code is available at~\url{https://github.com/cnnlstm/StyleGAN_Matting}.



### Combating Label Distribution Shift for Active Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2208.06604v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2208.06604v1)
- **Published**: 2022-08-13 09:06:45+00:00
- **Updated**: 2022-08-13 09:06:45+00:00
- **Authors**: Sehyun Hwang, Sohyun Lee, Sungyeon Kim, Jungseul Ok, Suha Kwak
- **Comment**: ECCV 2022 accepted
- **Journal**: None
- **Summary**: We consider the problem of active domain adaptation (ADA) to unlabeled target data, of which subset is actively selected and labeled given a budget constraint. Inspired by recent analysis on a critical issue from label distribution mismatch between source and target in domain adaptation, we devise a method that addresses the issue for the first time in ADA. At its heart lies a novel sampling strategy, which seeks target data that best approximate the entire target distribution as well as being representative, diverse, and uncertain. The sampled target data are then used not only for supervised learning but also for matching label distributions of source and target domains, leading to remarkable performance improvement. On four public benchmarks, our method substantially outperforms existing methods in every adaptation scenario.



### A Study of Demographic Bias in CNN-based Brain MR Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.06613v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06613v1)
- **Published**: 2022-08-13 10:07:54+00:00
- **Updated**: 2022-08-13 10:07:54+00:00
- **Authors**: Stefanos Ioannou, Hana Chockler, Alexander Hammers, Andrew P. King
- **Comment**: Accepted for publication at MICCAI MLCN 2022
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are increasingly being used to automate the segmentation of brain structures in magnetic resonance (MR) images for research studies. In other applications, CNN models have been shown to exhibit bias against certain demographic groups when they are under-represented in the training sets. In this work, we investigate whether CNN models for brain MR segmentation have the potential to contain sex or race bias when trained with imbalanced training sets. We train multiple instances of the FastSurferCNN model using different levels of sex imbalance in white subjects. We evaluate the performance of these models separately for white male and white female test sets to assess sex bias, and furthermore evaluate them on black male and black female test sets to assess potential racial bias. We find significant sex and race bias effects in segmentation model performance. The biases have a strong spatial component, with some brain regions exhibiting much stronger bias than others. Overall, our results suggest that race bias is more significant than sex bias. Our study demonstrates the importance of considering race and sex balance when forming training sets for CNN-based brain MR segmentation, to avoid maintaining or even exacerbating existing health inequalities through biased research study findings.



### A Unified Two-Stage Group Semantics Propagation and Contrastive Learning Network for Co-Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.06615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06615v1)
- **Published**: 2022-08-13 10:14:50+00:00
- **Updated**: 2022-08-13 10:14:50+00:00
- **Authors**: Zhenshan Tan, Cheng Chen, Keyu Wen, Yuzhuo Qin, Xiaodong Gu
- **Comment**: None
- **Journal**: None
- **Summary**: Co-saliency detection (CoSOD) aims at discovering the repetitive salient objects from multiple images. Two primary challenges are group semantics extraction and noise object suppression. In this paper, we present a unified Two-stage grOup semantics PropagatIon and Contrastive learning NETwork (TopicNet) for CoSOD. TopicNet can be decomposed into two substructures, including a two-stage group semantics propagation module (TGSP) to address the first challenge and a contrastive learning module (CLM) to address the second challenge. Concretely, for TGSP, we design an image-to-group propagation module (IGP) to capture the consensus representation of intra-group similar features and a group-to-pixel propagation module (GPP) to build the relevancy of consensus representation. For CLM, with the design of positive samples, the semantic consistency is enhanced. With the design of negative samples, the noise objects are suppressed. Experimental results on three prevailing benchmarks reveal that TopicNet outperforms other competitors in terms of various evaluation metrics.



### Online Refinement of a Scene Recognition Model for Mobile Robots by Observing Human's Interaction with Environments
- **Arxiv ID**: http://arxiv.org/abs/2208.06636v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.06636v1)
- **Published**: 2022-08-13 12:48:18+00:00
- **Updated**: 2022-08-13 12:48:18+00:00
- **Authors**: Shigemichi Matsuzaki, Hiroaki Masuzawa, Jun Miura
- **Comment**: Accepted to IEEE SMC 2022
- **Journal**: None
- **Summary**: This paper describes a method of online refinement of a scene recognition model for robot navigation considering traversable plants, flexible plant parts which a robot can push aside while moving. In scene recognition systems that consider traversable plants growing out to the paths, misclassification may lead the robot to getting stuck due to the traversable plants recognized as obstacles. Yet, misclassification is inevitable in any estimation methods. In this work, we propose a framework that allows for refining a semantic segmentation model on the fly during the robot's operation. We introduce a few-shot segmentation based on weight imprinting for online model refinement without fine-tuning. Training data are collected via observation of a human's interaction with the plant parts. We propose novel robust weight imprinting to mitigate the effect of noise included in the masks generated by the interaction. The proposed method was evaluated through experiments using real-world data and shown to outperform an ordinary weight imprinting and provide competitive results to fine-tuning with model distillation while requiring less computational cost.



### Recent Progress in Transformer-based Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/2208.06643v4
- **DOI**: 10.1016/j.compbiomed.2023.107268
- **Categories**: **eess.IV**, cs.CV, I.2.m, I.4.9, I.5.4, J.0
- **Links**: [PDF](http://arxiv.org/pdf/2208.06643v4)
- **Published**: 2022-08-13 13:13:41+00:00
- **Updated**: 2023-07-25 08:53:16+00:00
- **Authors**: Zhaoshan Liu, Qiujie Lv, Ziduo Yang, Yifan Li, Chau Hung Lee, Lei Shen
- **Comment**: Computers in Biology and Medicine Accepted
- **Journal**: None
- **Summary**: The transformer is primarily used in the field of natural language processing. Recently, it has been adopted and shows promise in the computer vision (CV) field. Medical image analysis (MIA), as a critical branch of CV, also greatly benefits from this state-of-the-art technique. In this review, we first recap the core component of the transformer, the attention mechanism, and the detailed structures of the transformer. After that, we depict the recent progress of the transformer in the field of MIA. We organize the applications in a sequence of different tasks, including classification, segmentation, captioning, registration, detection, enhancement, localization, and synthesis. The mainstream classification and segmentation tasks are further divided into eleven medical image modalities. A large number of experiments studied in this review illustrate that the transformer-based method outperforms existing methods through comparisons with multiple evaluation metrics. Finally, we discuss the open challenges and future opportunities in this field. This task-modality review with the latest contents, detailed information, and comprehensive comparison may greatly benefit the broad MIA community.



### ULDGNN: A Fragmented UI Layer Detector Based on Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.06658v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.06658v1)
- **Published**: 2022-08-13 14:14:37+00:00
- **Updated**: 2022-08-13 14:14:37+00:00
- **Authors**: Jiazhi Li, Tingting Zhou, Yunnong Chen, Yanfang Chang, Yankun Zhen, Lingyun Sun, Liuqing Chen
- **Comment**: None
- **Journal**: None
- **Summary**: While some work attempt to generate front-end code intelligently from UI screenshots, it may be more convenient to utilize UI design drafts in Sketch which is a popular UI design software, because we can access multimodal UI information directly such as layers type, position, size, and visual images. However, fragmented layers could degrade the code quality without being merged into a whole part if all of them are involved in the code generation. In this paper, we propose a pipeline to merge fragmented layers automatically. We first construct a graph representation for the layer tree of a UI draft and detect all fragmented layers based on the visual features and graph neural networks. Then a rule-based algorithm is designed to merge fragmented layers. Through experiments on a newly constructed dataset, our approach can retrieve most fragmented layers in UI design drafts, and achieve 87% accuracy in the detection task, and the post-processing algorithm is developed to cluster associative layers under simple and general circumstances.



### Entropy Induced Pruning Framework for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.06660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06660v1)
- **Published**: 2022-08-13 14:35:08+00:00
- **Updated**: 2022-08-13 14:35:08+00:00
- **Authors**: Yiheng Lu, Ziyu Guan, Yaming Yang, Maoguo Gong, Wei Zhao, Kaiyuan Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Structured pruning techniques have achieved great compression performance on convolutional neural networks for image classification task. However, the majority of existing methods are weight-oriented, and their pruning results may be unsatisfactory when the original model is trained poorly. That is, a fully-trained model is required to provide useful weight information. This may be time-consuming, and the pruning results are sensitive to the updating process of model parameters. In this paper, we propose a metric named Average Filter Information Entropy (AFIE) to measure the importance of each filter. It is calculated by three major steps, i.e., low-rank decomposition of the "input-output" matrix of each convolutional layer, normalization of the obtained eigenvalues, and calculation of filter importance based on information entropy. By leveraging the proposed AFIE, the proposed framework is able to yield a stable importance evaluation of each filter no matter whether the original model is trained fully. We implement our AFIE based on AlexNet, VGG-16, and ResNet-50, and test them on MNIST, CIFAR-10, and ImageNet, respectively. The experimental results are encouraging. We surprisingly observe that for our methods, even when the original model is only trained with one epoch, the importance evaluation of each filter keeps identical to the results when the model is fully-trained. This indicates that the proposed pruning strategy can perform effectively at the beginning stage of the training process for the original model.



### SSP-Pose: Symmetry-Aware Shape Prior Deformation for Direct Category-Level Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.06661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06661v1)
- **Published**: 2022-08-13 14:37:31+00:00
- **Updated**: 2022-08-13 14:37:31+00:00
- **Authors**: Ruida Zhang, Yan Di, Fabian Manhardt, Federico Tombari, Xiangyang Ji
- **Comment**: Accepted by IROS 2022
- **Journal**: None
- **Summary**: Category-level pose estimation is a challenging problem due to intra-class shape variations. Recent methods deform pre-computed shape priors to map the observed point cloud into the normalized object coordinate space and then retrieve the pose via post-processing, i.e., Umeyama's Algorithm. The shortcomings of this two-stage strategy lie in two aspects: 1) The surrogate supervision on the intermediate results can not directly guide the learning of pose, resulting in large pose error after post-processing. 2) The inference speed is limited by the post-processing step. In this paper, to handle these shortcomings, we propose an end-to-end trainable network SSP-Pose for category-level pose estimation, which integrates shape priors into a direct pose regression network. SSP-Pose stacks four individual branches on a shared feature extractor, where two branches are designed to deform and match the prior model with the observed instance, and the other two branches are applied for directly regressing the totally 9 degrees-of-freedom pose and performing symmetry reconstruction and point-wise inlier mask prediction respectively. Consistency loss terms are then naturally exploited to align the outputs of different branches and promote the performance. During inference, only the direct pose regression branch is needed. In this manner, SSP-Pose not only learns category-level pose-sensitive characteristics to boost performance but also keeps a real-time inference speed. Moreover, we utilize the symmetry information of each category to guide the shape prior deformation, and propose a novel symmetry-aware loss to mitigate the matching ambiguity. Extensive experiments on public datasets demonstrate that SSP-Pose produces superior performance compared with competitors with a real-time inference speed at about 25Hz.



### Self-Contained Entity Discovery from Captioned Videos
- **Arxiv ID**: http://arxiv.org/abs/2208.06662v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06662v1)
- **Published**: 2022-08-13 14:39:01+00:00
- **Updated**: 2022-08-13 14:39:01+00:00
- **Authors**: Melika Ayoughi, Pascal Mettes, Paul Groth
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces the task of visual named entity discovery in videos without the need for task-specific supervision or task-specific external knowledge sources. Assigning specific names to entities (e.g. faces, scenes, or objects) in video frames is a long-standing challenge. Commonly, this problem is addressed as a supervised learning objective by manually annotating faces with entity labels. To bypass the annotation burden of this setup, several works have investigated the problem by utilizing external knowledge sources such as movie databases. While effective, such approaches do not work when task-specific knowledge sources are not provided and can only be applied to movies and TV series. In this work, we take the problem a step further and propose to discover entities in videos from videos and corresponding captions or subtitles. We introduce a three-stage method where we (i) create bipartite entity-name graphs from frame-caption pairs, (ii) find visual entity agreements, and (iii) refine the entity assignment through entity-level prototype construction. To tackle this new problem, we outline two new benchmarks SC-Friends and SC-BBT based on the Friends and Big Bang Theory TV series. Experiments on the benchmarks demonstrate the ability of our approach to discover which named entity belongs to which face or scene, with an accuracy close to a supervised oracle, just from the multimodal information present in videos. Additionally, our qualitative examples show the potential challenges of self-contained discovery of any visual entity for future work. The code and the data are available on GitHub.



### Bidirectional Feature Globalization for Few-shot Semantic Segmentation of 3D Point Cloud Scenes
- **Arxiv ID**: http://arxiv.org/abs/2208.06671v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06671v3)
- **Published**: 2022-08-13 15:04:20+00:00
- **Updated**: 2022-11-02 13:49:44+00:00
- **Authors**: Yongqiang Mao, Zonghao Guo, Xiaonan Lu, Zhiqiang Yuan, Haowen Guo
- **Comment**: 3DV2022 Oral
- **Journal**: None
- **Summary**: Few-shot segmentation of point cloud remains a challenging task, as there is no effective way to convert local point cloud information to global representation, which hinders the generalization ability of point features. In this study, we propose a bidirectional feature globalization (BFG) approach, which leverages the similarity measurement between point features and prototype vectors to embed global perception to local point features in a bidirectional fashion. With point-to-prototype globalization (Po2PrG), BFG aggregates local point features to prototypes according to similarity weights from dense point features to sparse prototypes. With prototype-to-point globalization (Pr2PoG), the global perception is embedded to local point features based on similarity weights from sparse prototypes to dense point features. The sparse prototypes of each class embedded with global perception are summarized to a single prototype for few-shot 3D segmentation based on the metric learning framework. Extensive experiments on S3DIS and ScanNet demonstrate that BFG significantly outperforms the state-of-the-art methods.



### DS-MVSNet: Unsupervised Multi-view Stereo via Depth Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2208.06674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06674v1)
- **Published**: 2022-08-13 15:25:51+00:00
- **Updated**: 2022-08-13 15:25:51+00:00
- **Authors**: Jingliang Li, Zhengda Lu, Yiqun Wang, Ying Wang, Jun Xiao
- **Comment**: Accepted by ACM MM 2022
- **Journal**: None
- **Summary**: In recent years, supervised or unsupervised learning-based MVS methods achieved excellent performance compared with traditional methods. However, these methods only use the probability volume computed by cost volume regularization to predict reference depths and this manner cannot mine enough information from the probability volume. Furthermore, the unsupervised methods usually try to use two-step or additional inputs for training which make the procedure more complicated. In this paper, we propose the DS-MVSNet, an end-to-end unsupervised MVS structure with the source depths synthesis. To mine the information in probability volume, we creatively synthesize the source depths by splattering the probability volume and depth hypotheses to source views. Meanwhile, we propose the adaptive Gaussian sampling and improved adaptive bins sampling approach that improve the depths hypotheses accuracy. On the other hand, we utilize the source depths to render the reference images and propose depth consistency loss and depth smoothness loss. These can provide additional guidance according to photometric and geometric consistency in different views without additional inputs. Finally, we conduct a series of experiments on the DTU dataset and Tanks & Temples dataset that demonstrate the efficiency and robustness of our DS-MVSNet compared with the state-of-the-art methods.



### A new way of video compression via forward-referencing using deep learning
- **Arxiv ID**: http://arxiv.org/abs/2208.06678v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.06678v1)
- **Published**: 2022-08-13 16:19:11+00:00
- **Updated**: 2022-08-13 16:19:11+00:00
- **Authors**: S. M. A. K. Rajin, M. Murshed, M. Paul, S. W. Teng, J. Ma
- **Comment**: None
- **Journal**: None
- **Summary**: To exploit high temporal correlations in video frames of the same scene, the current frame is predicted from the already-encoded reference frames using block-based motion estimation and compensation techniques. While this approach can efficiently exploit the translation motion of the moving objects, it is susceptible to other types of affine motion and object occlusion/deocclusion. Recently, deep learning has been used to model the high-level structure of human pose in specific actions from short videos and then generate virtual frames in future time by predicting the pose using a generative adversarial network (GAN). Therefore, modelling the high-level structure of human pose is able to exploit semantic correlation by predicting human actions and determining its trajectory. Video surveillance applications will benefit as stored big surveillance data can be compressed by estimating human pose trajectories and generating future frames through semantic correlation. This paper explores a new way of video coding by modelling human pose from the already-encoded frames and using the generated frame at the current time as an additional forward-referencing frame. It is expected that the proposed approach can overcome the limitations of the traditional backward-referencing frames by predicting the blocks containing the moving objects with lower residuals. Experimental results show that the proposed approach can achieve on average up to 2.83 dB PSNR gain and 25.93\% bitrate savings for high motion video sequences



### Modeling biological face recognition with deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/2208.06681v3
- **DOI**: 10.1162/jocn_a_02040
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06681v3)
- **Published**: 2022-08-13 16:45:30+00:00
- **Updated**: 2023-08-19 07:07:10+00:00
- **Authors**: Leonard E. van Dyck, Walter R. Gruber
- **Comment**: 41 pages, 2 figures, 1 table
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) have become the state-of-the-art computational models of biological object recognition. Their remarkable success has helped vision science break new ground and recent efforts have started to transfer this achievement to research on biological face recognition. In this regard, face detection can be investigated by comparing face-selective biological neurons and brain areas to artificial neurons and model layers. Similarly, face identification can be examined by comparing in vivo and in silico multidimensional "face spaces". In this review, we summarize the first studies that use DCNNs to model biological face recognition. On the basis of a broad spectrum of behavioral and computational evidence, we conclude that DCNNs are useful models that closely resemble the general hierarchical organization of face recognition in the ventral visual pathway and the core face network. In two exemplary spotlights, we emphasize the unique scientific contributions of these models. First, studies on face detection in DCNNs indicate that elementary face selectivity emerges automatically through feedforward processing even in the absence of visual experience. Second, studies on face identification in DCNNs suggest that identity-specific experience and generative mechanisms facilitate this particular challenge. Taken together, as this novel modeling approach enables close control of predisposition (i.e., architecture) and experience (i.e., training data), it may be suited to inform long-standing debates on the substrates of biological face recognition.



### UAV-CROWD: Violent and non-violent crowd activity simulator from the perspective of UAV
- **Arxiv ID**: http://arxiv.org/abs/2208.06702v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.06702v1)
- **Published**: 2022-08-13 18:28:37+00:00
- **Updated**: 2022-08-13 18:28:37+00:00
- **Authors**: Mahieyin Rahmun, Tonmoay Deb, Shahriar Ali Bijoy, Mayamin Hamid Raha
- **Comment**: None
- **Journal**: None
- **Summary**: Unmanned Aerial Vehicle (UAV) has gained significant traction in the recent years, particularly the context of surveillance. However, video datasets that capture violent and non-violent human activity from aerial point-of-view is scarce. To address this issue, we propose a novel, baseline simulator which is capable of generating sequences of photo-realistic synthetic images of crowds engaging in various activities that can be categorized as violent or non-violent. The crowd groups are annotated with bounding boxes that are automatically computed using semantic segmentation. Our simulator is capable of generating large, randomized urban environments and is able to maintain an average of 25 frames per second on a mid-range computer with 150 concurrent crowd agents interacting with each other. We also show that when synthetic data from the proposed simulator is augmented with real world data, binary video classification accuracy is improved by 5% on average across two different models.



### Simulating Personal Food Consumption Patterns using a Modified Markov Chain
- **Arxiv ID**: http://arxiv.org/abs/2208.06709v1
- **DOI**: 10.1145/3552484.3555747
- **Categories**: **cs.CV**, I.6.8
- **Links**: [PDF](http://arxiv.org/pdf/2208.06709v1)
- **Published**: 2022-08-13 18:50:23+00:00
- **Updated**: 2022-08-13 18:50:23+00:00
- **Authors**: Xinyue Pan, Jiangpeng He, Andrew Peng, Fengqing Zhu
- **Comment**: 9 pages, 6 figures,In Proceedings of the 7th International Workshop
  on Multimedia Assisted Dietary Management (MADiMa 22)
- **Journal**: None
- **Summary**: Food image classification serves as the foundation of image-based dietary assessment to predict food categories. Since there are many different food classes in real life, conventional models cannot achieve sufficiently high accuracy. Personalized classifiers aim to largely improve the accuracy of food image classification for each individual. However, a lack of public personal food consumption data proves to be a challenge for training such models. To address this issue, we propose a novel framework to simulate personal food consumption data patterns, leveraging the use of a modified Markov chain model and self-supervised learning. Our method is capable of creating an accurate future data pattern from a limited amount of initial data, and our simulated data patterns can be closely correlated with the initial data pattern. Furthermore, we use Dynamic Time Warping distance and Kullback-Leibler divergence as metrics to evaluate the effectiveness of our method on the public Food-101 dataset. Our experimental results demonstrate promising performance compared with random simulation and the original Markov chain method.



### Progressive Multi-scale Light Field Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.06710v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2208.06710v1)
- **Published**: 2022-08-13 19:02:34+00:00
- **Updated**: 2022-08-13 19:02:34+00:00
- **Authors**: David Li, Amitabh Varshney
- **Comment**: Accepted to 3DV 2022. Website at
  https://augmentariumlab.github.io/multiscale-lfn/
- **Journal**: None
- **Summary**: Neural representations have shown great promise in their ability to represent radiance and light fields while being very compact compared to the image set representation. However, current representations are not well suited for streaming as decoding can only be done at a single level of detail and requires downloading the entire neural network model. Furthermore, high-resolution light field networks can exhibit flickering and aliasing as neural networks are sampled without appropriate filtering. To resolve these issues, we present a progressive multi-scale light field network that encodes a light field with multiple levels of detail. Lower levels of detail are encoded using fewer neural network weights enabling progressive streaming and reducing rendering time. Our progressive multi-scale light field network addresses aliasing by encoding smaller anti-aliased representations at its lower levels of detail. Additionally, per-pixel level of detail enables our representation to support dithered transitions and foveated rendering.



### Machine Learning Based Radiomics for Glial Tumor Classification and Comparison with Volumetric Analysis
- **Arxiv ID**: http://arxiv.org/abs/2208.06739v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/2208.06739v1)
- **Published**: 2022-08-13 22:09:53+00:00
- **Updated**: 2022-08-13 22:09:53+00:00
- **Authors**: Sevcan Turk, Kaya Oguz, Mehmet Orman, Emre Caliskan, Yesim Ertan, Erkin Ozgiray, Taner Akalin, Ashok Srinivasan, Omer Kitis
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose; The purpose of this study is to classify glial tumors into grade II, III and IV categories noninvasively by application of machine learning to multi-modal MRI features in comparison with volumetric analysis. Methods; We retrospectively studied 57 glioma patients with pre and postcontrast T1 weighted, T2 weighted, FLAIR images, and ADC maps acquired on a 3T MRI. The tumors were segmented into enhancing and nonenhancing portions, tumor necrosis, cyst and edema using semiautomated segmentation of ITK-SNAP open source tool. We measured total tumor volume, enhancing-nonenhancing tumor, edema, necrosis volume and the ratios to the total tumor volume. Training of a support vector machine (SVM) classifier and artificial neural network (ANN) was performed with labeled data designed to answer the question of interest. Specificity, sensitivity, and AUC of the predictions were computed by means of ROC analysis. Differences in continuous measures between groups were assessed by using Kruskall Wallis, with post hoc Dunn correction for multiple comparisons. Results; When we compared the volume ratios between groups, there was statistically significant difference between grade IV and grade II-III glial tumors. Edema and tumor necrosis volume ratios for grade IV glial tumors were higher than that of grade II and III. Volumetric ratio analysis could not distinguish grade II and III tumors successfully. However, SVM and ANN correctly classified each group with accuracies up to 98% and 96%. Conclusion; Application of machine learning methods to MRI features can be used to classify brain tumors noninvasively and more readily in clinical settings.



