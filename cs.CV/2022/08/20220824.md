# Arxiv Papers in cs.CV on 2022-08-24
### A new explainable DTM generation algorithm with airborne LIDAR data: grounds are smoothly connected eventually
- **Arxiv ID**: http://arxiv.org/abs/2208.11243v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.11243v1)
- **Published**: 2022-08-24 00:26:03+00:00
- **Updated**: 2022-08-24 00:26:03+00:00
- **Authors**: Hunsoo Song, Jinha Jung
- **Comment**: None
- **Journal**: None
- **Summary**: The digital terrain model (DTM) is fundamental geospatial data for various studies in urban, environmental, and Earth science. The reliability of the results obtained from such studies can be considerably affected by the errors and uncertainties of the underlying DTM. Numerous algorithms have been developed to mitigate the errors and uncertainties of DTM. However, most algorithms involve tricky parameter selection and complicated procedures that make the algorithm's decision rule obscure, so it is often difficult to explain and predict the errors and uncertainties of the resulting DTM. Also, previous algorithms often consider the local neighborhood of each point for distinguishing non-ground objects, which limits both search radius and contextual understanding and can be susceptible to errors particularly if point density varies. This study presents an open-source DTM generation algorithm for airborne LiDAR data that can consider beyond the local neighborhood and whose results are easily explainable, predictable, and reliable. The key assumption of the algorithm is that grounds are smoothly connected while non-grounds are surrounded by areas having sharp elevation changes. The robustness and uniqueness of the proposed algorithm were evaluated in geographically complex environments through tiling evaluation compared to other state-of-the-art algorithms.



### SwinFIR: Revisiting the SwinIR with Fast Fourier Convolution and Improved Training for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2208.11247v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11247v2)
- **Published**: 2022-08-24 01:04:47+00:00
- **Updated**: 2023-05-06 12:14:47+00:00
- **Authors**: Dafeng Zhang, Feiyu Huang, Shizhuo Liu, Xiaobing Wang, Zhezhu Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Transformer-based methods have achieved impressive image restoration performance due to their capacities to model long-range dependency compared to CNN-based methods. However, advances like SwinIR adopts the window-based and local attention strategy to balance the performance and computational overhead, which restricts employing large receptive fields to capture global information and establish long dependencies in the early layers. To further improve the efficiency of capturing global information, in this work, we propose SwinFIR to extend SwinIR by replacing Fast Fourier Convolution (FFC) components, which have the image-wide receptive field. We also revisit other advanced techniques, i.e, data augmentation, pre-training, and feature ensemble to improve the effect of image reconstruction. And our feature ensemble method enables the performance of the model to be considerably enhanced without increasing the training and testing time. We applied our algorithm on multiple popular large-scale benchmarks and achieved state-of-the-art performance comparing to the existing methods. For example, our SwinFIR achieves the PSNR of 32.83 dB on Manga109 dataset, which is 0.8 dB higher than the state-of-the-art SwinIR method.



### Learnable human mesh triangulation for 3D human pose and shape estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.11251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11251v1)
- **Published**: 2022-08-24 01:11:57+00:00
- **Updated**: 2022-08-24 01:11:57+00:00
- **Authors**: Sungho Chun, Sungbum Park, Ju Yong Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Compared to joint position, the accuracy of joint rotation and shape estimation has received relatively little attention in the skinned multi-person linear model (SMPL)-based human mesh reconstruction from multi-view images. The work in this field is broadly classified into two categories. The first approach performs joint estimation and then produces SMPL parameters by fitting SMPL to resultant joints. The second approach regresses SMPL parameters directly from the input images through a convolutional neural network (CNN)-based model. However, these approaches suffer from the lack of information for resolving the ambiguity of joint rotation and shape reconstruction and the difficulty of network learning. To solve the aforementioned problems, we propose a two-stage method. The proposed method first estimates the coordinates of mesh vertices through a CNN-based model from input images, and acquires SMPL parameters by fitting the SMPL model to the estimated vertices. Estimated mesh vertices provide sufficient information for determining joint rotation and shape, and are easier to learn than SMPL parameters. According to experiments using Human3.6M and MPI-INF-3DHP datasets, the proposed method significantly outperforms the previous works in terms of joint rotation and shape estimation, and achieves competitive performance in terms of joint location estimation.



### FashionVQA: A Domain-Specific Visual Question Answering System
- **Arxiv ID**: http://arxiv.org/abs/2208.11253v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11253v1)
- **Published**: 2022-08-24 01:18:13+00:00
- **Updated**: 2022-08-24 01:18:13+00:00
- **Authors**: Min Wang, Ata Mahjoubfar, Anupama Joshi
- **Comment**: None
- **Journal**: None
- **Summary**: Humans apprehend the world through various sensory modalities, yet language is their predominant communication channel. Machine learning systems need to draw on the same multimodal richness to have informed discourses with humans in natural language; this is particularly true for systems specialized in visually-dense information, such as dialogue, recommendation, and search engines for clothing. To this end, we train a visual question answering (VQA) system to answer complex natural language questions about apparel in fashion photoshoot images. The key to the successful training of our VQA model is the automatic creation of a visual question-answering dataset with 168 million samples from item attributes of 207 thousand images using diverse templates. The sample generation employs a strategy that considers the difficulty of the question-answer pairs to emphasize challenging concepts. Contrary to the recent trends in using several datasets for pretraining the visual question answering models, we focused on keeping the dataset fixed while training various models from scratch to isolate the improvements from model architecture changes. We see that using the same transformer for encoding the question and decoding the answer, as in language models, achieves maximum accuracy, showing that visual language models (VLMs) make the best visual question answering systems for our dataset. The accuracy of the best model surpasses the human expert level, even when answering human-generated questions that are not confined to the template formats. Our approach for generating a large-scale multimodal domain-specific dataset provides a path for training specialized models capable of communicating in natural language. The training of such domain-expert models, e.g., our fashion VLM model, cannot rely solely on the large-scale general-purpose datasets collected from the web.



### 3D-FM GAN: Towards 3D-Controllable Face Manipulation
- **Arxiv ID**: http://arxiv.org/abs/2208.11257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11257v1)
- **Published**: 2022-08-24 01:33:13+00:00
- **Updated**: 2022-08-24 01:33:13+00:00
- **Authors**: Yuchen Liu, Zhixin Shu, Yijun Li, Zhe Lin, Richard Zhang, S. Y. Kung
- **Comment**: Accepted to ECCV2022. Project webpage:
  https://lychenyoko.github.io/3D-FM-GAN-Webpage/
- **Journal**: None
- **Summary**: 3D-controllable portrait synthesis has significantly advanced, thanks to breakthroughs in generative adversarial networks (GANs). However, it is still challenging to manipulate existing face images with precise 3D control. While concatenating GAN inversion and a 3D-aware, noise-to-image GAN is a straight-forward solution, it is inefficient and may lead to noticeable drop in editing quality. To fill this gap, we propose 3D-FM GAN, a novel conditional GAN framework designed specifically for 3D-controllable face manipulation, and does not require any tuning after the end-to-end learning phase. By carefully encoding both the input face image and a physically-based rendering of 3D edits into a StyleGAN's latent spaces, our image generator provides high-quality, identity-preserved, 3D-controllable face manipulation. To effectively learn such novel framework, we develop two essential training strategies and a novel multiplicative co-modulation architecture that improves significantly upon naive schemes. With extensive evaluations, we show that our method outperforms the prior arts on various tasks, with better editability, stronger identity preservation, and higher photo-realism. In addition, we demonstrate a better generalizability of our design on large pose editing and out-of-domain images.



### Applying Eigencontours to PolarMask-Based Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.11258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11258v1)
- **Published**: 2022-08-24 01:33:18+00:00
- **Updated**: 2022-08-24 01:33:18+00:00
- **Authors**: Wonhui Park, Dongkwon Jin, Chang-Su Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Eigencontours are the first data-driven contour descriptors based on singular value decomposition. Based on the implementation of ESE-Seg, eigencontours were applied to the instance segmentation task successfully. In this report, we incorporate eigencontours into the PolarMask network for instance segmentation. Experimental results demonstrate that the proposed algorithm yields better results than PolarMask on two instance segmentation datasets of COCO2017 and SBD. Also, we analyze the characteristics of eigencontours qualitatively. Our codes are available at https://github.com/dnjs3594/Eigencontours.



### AT-DDPM: Restoring Faces degraded by Atmospheric Turbulence using Denoising Diffusion Probabilistic Models
- **Arxiv ID**: http://arxiv.org/abs/2208.11284v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11284v2)
- **Published**: 2022-08-24 03:13:04+00:00
- **Updated**: 2022-09-20 06:13:41+00:00
- **Authors**: Nithin Gopalakrishnan Nair, Kangfu Mei, Vishal M. Patel
- **Comment**: Accepted to IEEE WACV 2023
- **Journal**: None
- **Summary**: Although many long-range imaging systems are designed to support extended vision applications, a natural obstacle to their operation is degradation due to atmospheric turbulence. Atmospheric turbulence causes significant degradation to image quality by introducing blur and geometric distortion. In recent years, various deep learning-based single image atmospheric turbulence mitigation methods, including CNN-based and GAN inversion-based, have been proposed in the literature which attempt to remove the distortion in the image. However, some of these methods are difficult to train and often fail to reconstruct facial features and produce unrealistic results especially in the case of high turbulence. Denoising Diffusion Probabilistic Models (DDPMs) have recently gained some traction because of their stable training process and their ability to generate high quality images. In this paper, we propose the first DDPM-based solution for the problem of atmospheric turbulence mitigation. We also propose a fast sampling technique for reducing the inference times for conditional DDPMs. Extensive experiments are conducted on synthetic and real-world data to show the significance of our model. To facilitate further research, all codes and pretrained models are publically available at http://github.com/Nithin-GK/AT-DDPM



### Multi-Modality Abdominal Multi-Organ Segmentation with Deep Supervised 3D Segmentation Model
- **Arxiv ID**: http://arxiv.org/abs/2208.12041v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.12041v1)
- **Published**: 2022-08-24 03:37:54+00:00
- **Updated**: 2022-08-24 03:37:54+00:00
- **Authors**: Satoshi Kondo, Satoshi Kasai
- **Comment**: None
- **Journal**: None
- **Summary**: To promote the development of medical image segmentation technology, AMOS, a large-scale abdominal multi-organ dataset for versatile medical image segmentation, is provided and AMOS 2022 challenge is held by using the dataset. In this report, we present our solution for the AMOS 2022 challenge. We employ residual U-Net with deep super vision as our base model. The experimental results show that the mean scores of Dice similarity coefficient and normalized surface dice are 0.8504 and 0.8476 for CT only task and CT/MRI task, respectively.



### Semi-Supervised and Unsupervised Deep Visual Learning: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2208.11296v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11296v1)
- **Published**: 2022-08-24 04:26:21+00:00
- **Updated**: 2022-08-24 04:26:21+00:00
- **Authors**: Yanbei Chen, Massimiliano Mancini, Xiatian Zhu, Zeynep Akata
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI), 2022
- **Journal**: None
- **Summary**: State-of-the-art deep learning models are often trained with a large amount of costly labeled training data. However, requiring exhaustive manual annotations may degrade the model's generalizability in the limited-label regime. Semi-supervised learning and unsupervised learning offer promising paradigms to learn from an abundance of unlabeled visual data. Recent progress in these paradigms has indicated the strong benefits of leveraging unlabeled data to improve model generalization and provide better model initialization. In this survey, we review the recent advanced deep learning algorithms on semi-supervised learning (SSL) and unsupervised learning (UL) for visual recognition from a unified perspective. To offer a holistic understanding of the state-of-the-art in these areas, we propose a unified taxonomy. We categorize existing representative SSL and UL with comprehensive and insightful analysis to highlight their design rationales in different learning scenarios and applications in different computer vision tasks. Lastly, we discuss the emerging trends and open challenges in SSL and UL to shed light on future critical research directions.



### E-NeRF: Neural Radiance Fields from a Moving Event Camera
- **Arxiv ID**: http://arxiv.org/abs/2208.11300v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2208.11300v2)
- **Published**: 2022-08-24 04:53:32+00:00
- **Updated**: 2023-01-24 05:57:03+00:00
- **Authors**: Simon Klenk, Lukas Koestler, Davide Scaramuzza, Daniel Cremers
- **Comment**: revised RAL version + added suppl. material
- **Journal**: None
- **Summary**: Estimating neural radiance fields (NeRFs) from "ideal" images has been extensively studied in the computer vision community. Most approaches assume optimal illumination and slow camera motion. These assumptions are often violated in robotic applications, where images may contain motion blur, and the scene may not have suitable illumination. This can cause significant problems for downstream tasks such as navigation, inspection, or visualization of the scene. To alleviate these problems, we present E-NeRF, the first method which estimates a volumetric scene representation in the form of a NeRF from a fast-moving event camera. Our method can recover NeRFs during very fast motion and in high-dynamic-range conditions where frame-based approaches fail. We show that rendering high-quality frames is possible by only providing an event stream as input. Furthermore, by combining events and frames, we can estimate NeRFs of higher quality than state-of-the-art approaches under severe motion blur. We also show that combining events and frames can overcome failure cases of NeRF estimation in scenarios where only a few input views are available without requiring additional regularization.



### Modeling Paragraph-Level Vision-Language Semantic Alignment for Multi-Modal Summarization
- **Arxiv ID**: http://arxiv.org/abs/2208.11303v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.11303v3)
- **Published**: 2022-08-24 05:18:23+00:00
- **Updated**: 2023-05-10 15:54:12+00:00
- **Authors**: Chenhao Cui, Xinnian Liang, Shuangzhi Wu, Zhoujun Li
- **Comment**: None
- **Journal**: None
- **Summary**: Most current multi-modal summarization methods follow a cascaded manner, where an off-the-shelf object detector is first used to extract visual features, then these features are fused with language representations to generate the summary with an encoder-decoder model. The cascaded way cannot capture the semantic alignments between images and paragraphs, which are crucial to a precise summary. In this paper, we propose ViL-Sum to jointly model paragraph-level \textbf{Vi}sion-\textbf{L}anguage Semantic Alignment and Multi-Modal \textbf{Sum}marization. The core of ViL-Sum is a joint multi-modal encoder with two well-designed tasks, image reordering and image selection. The joint multi-modal encoder captures the interactions between modalities, where the reordering task guides the model to learn paragraph-level semantic alignment and the selection task guides the model to selected summary-related images in the final summary. Experimental results show that our proposed ViL-Sum significantly outperforms current state-of-the-art methods. In further analysis, we find that two well-designed tasks and joint multi-modal encoder can effectively guide the model to learn reasonable paragraphs-images and summary-images relations.



### Visual Subtitle Feature Enhanced Video Outline Generation
- **Arxiv ID**: http://arxiv.org/abs/2208.11307v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2208.11307v2)
- **Published**: 2022-08-24 05:26:26+00:00
- **Updated**: 2022-09-01 07:26:33+00:00
- **Authors**: Qi Lv, Ziqiang Cao, Wenrui Xie, Derui Wang, Jingwen Wang, Zhiwei Hu, Tangkun Zhang, Ba Yuan, Yuanhang Li, Min Cao, Wenjie Li, Sujian Li, Guohong Fu
- **Comment**: None
- **Journal**: None
- **Summary**: With the tremendously increasing number of videos, there is a great demand for techniques that help people quickly navigate to the video segments they are interested in. However, current works on video understanding mainly focus on video content summarization, while little effort has been made to explore the structure of a video. Inspired by textual outline generation, we introduce a novel video understanding task, namely video outline generation (VOG). This task is defined to contain two sub-tasks: (1) first segmenting the video according to the content structure and then (2) generating a heading for each segment. To learn and evaluate VOG, we annotate a 10k+ dataset, called DuVOG. Specifically, we use OCR tools to recognize subtitles of videos. Then annotators are asked to divide subtitles into chapters and title each chapter. In videos, highlighted text tends to be the headline since it is more likely to attract attention. Therefore we propose a Visual Subtitle feature Enhanced video outline generation model (VSENet) which takes as input the textual subtitles together with their visual font sizes and positions. We consider the VOG task as a sequence tagging problem that extracts spans where the headings are located and then rewrites them to form the final outlines. Furthermore, based on the similarity between video outlines and textual outlines, we use a large number of articles with chapter headings to pretrain our model. Experiments on DuVOG show that our model largely outperforms other baseline methods, achieving 77.1 of F1-score for the video segmentation level and 85.0 of ROUGE-L_F0.5 for the headline generation level.



### Deep model with built-in cross-attention alignment for acoustic echo cancellation
- **Arxiv ID**: http://arxiv.org/abs/2208.11308v2
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/2208.11308v2)
- **Published**: 2022-08-24 05:29:47+00:00
- **Updated**: 2023-03-14 08:16:51+00:00
- **Authors**: Evgenii Indenbom, Nicolae-Cătălin Ristea, Ando Saabas, Tanel Pärnamaa, Jegor Gužvin
- **Comment**: None
- **Journal**: None
- **Summary**: With recent research advances, deep learning models have become an attractive choice for acoustic echo cancellation (AEC) in real-time teleconferencing applications. Since acoustic echo is one of the major sources of poor audio quality, a wide variety of deep models have been proposed. However, an important but often omitted requirement for good echo cancellation quality is the synchronization of the microphone and far end signals. Typically implemented using classical algorithms based on cross-correlation, the alignment module is a separate functional block with known design limitations. In our work we propose a deep learning architecture with built-in self-attention based alignment, which is able to handle unaligned inputs, improving echo cancellation performance while simplifying the communication pipeline. Moreover, we show that our approach achieves significant improvements for difficult delay estimation cases on real recordings from AEC Challenge data set.



### RZSR: Reference-based Zero-Shot Super-Resolution with Depth Guided Self-Exemplars
- **Arxiv ID**: http://arxiv.org/abs/2208.11313v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11313v1)
- **Published**: 2022-08-24 05:48:17+00:00
- **Updated**: 2022-08-24 05:48:17+00:00
- **Authors**: Jun-Sang Yoo, Dong-Wook Kim, Yucheng Lu, Seung-Won Jung
- **Comment**: Accepted by IEEE Transactions on Multimedia (TMM)
- **Journal**: None
- **Summary**: Recent methods for single image super-resolution (SISR) have demonstrated outstanding performance in generating high-resolution (HR) images from low-resolution (LR) images. However, most of these methods show their superiority using synthetically generated LR images, and their generalizability to real-world images is often not satisfactory. In this paper, we pay attention to two well-known strategies developed for robust super-resolution (SR), i.e., reference-based SR (RefSR) and zero-shot SR (ZSSR), and propose an integrated solution, called reference-based zero-shot SR (RZSR). Following the principle of ZSSR, we train an image-specific SR network at test time using training samples extracted only from the input image itself. To advance ZSSR, we obtain reference image patches with rich textures and high-frequency details which are also extracted only from the input image using cross-scale matching. To this end, we construct an internal reference dataset and retrieve reference image patches from the dataset using depth information. Using LR patches and their corresponding HR reference patches, we train a RefSR network that is embodied with a non-local attention module. Experimental results demonstrate the superiority of the proposed RZSR compared to the previous ZSSR methods and robustness to unseen images compared to other fully supervised SISR methods.



### Modality Mixer for Multi-modal Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2208.11314v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11314v2)
- **Published**: 2022-08-24 05:56:00+00:00
- **Updated**: 2023-02-21 07:18:46+00:00
- **Authors**: Sumin Lee, Sangmin Woo, Yeonju Park, Muhammad Adi Nugroho, Changick Kim
- **Comment**: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
  2023
- **Journal**: None
- **Summary**: In multi-modal action recognition, it is important to consider not only the complementary nature of different modalities but also global action content. In this paper, we propose a novel network, named Modality Mixer (M-Mixer) network, to leverage complementary information across modalities and temporal context of an action for multi-modal action recognition. We also introduce a simple yet effective recurrent unit, called Multi-modal Contextualization Unit (MCU), which is a core component of M-Mixer. Our MCU temporally encodes a sequence of one modality (e.g., RGB) with action content features of other modalities (e.g., depth, IR). This process encourages M-Mixer to exploit global action content and also to supplement complementary information of other modalities. As a result, our proposed method outperforms state-of-the-art methods on NTU RGB+D 60, NTU RGB+D 120, and NW-UCLA datasets. Moreover, we demonstrate the effectiveness of M-Mixer by conducting comprehensive ablation studies.



### Comparison of Object Detection Algorithms for Street-level Objects
- **Arxiv ID**: http://arxiv.org/abs/2208.11315v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11315v1)
- **Published**: 2022-08-24 05:57:12+00:00
- **Updated**: 2022-08-24 05:57:12+00:00
- **Authors**: Martinus Grady Naftali, Jason Sebastian Sulistyawan, Kelvin Julian
- **Comment**: 11 pages, 9 figures, 5 tables
- **Journal**: None
- **Summary**: Object detection for street-level objects can be applied to various use cases, from car and traffic detection to the self-driving car system. Therefore, finding the best object detection algorithm is essential to apply it effectively. Many object detection algorithms have been released, and many have compared object detection algorithms, but few have compared the latest algorithms, such as YOLOv5, primarily which focus on street-level objects. This paper compares various one-stage detector algorithms; SSD MobileNetv2 FPN-lite 320x320, YOLOv3, YOLOv4, YOLOv5l, and YOLOv5s for street-level object detection within real-time images. The experiment utilizes a modified Udacity Self Driving Car Dataset with 3,169 images. Dataset is split into train, validation, and test; Then, it is preprocessed and augmented using rescaling, hue shifting, and noise. Each algorithm is then trained and evaluated. Based on the experiments, the algorithms have produced decent results according to the inference time and the values of their precision, recall, F1-Score, and Mean Average Precision (mAP). The results also shows that YOLOv5l outperforms the other algorithms in terms of accuracy with a mAP@.5 of 0.593, MobileNetv2 FPN-lite has the fastest inference time among the others with only 3.20ms inference time. It is also found that YOLOv5s is the most efficient, with it having a YOLOv5l accuracy and a speed almost as quick as the MobileNetv2 FPN-lite. This shows that various algorithm are suitable for street-level object detection and viable enough to be used in self-driving car.



### Robust Motion Averaging for Multi-view Registration of Point Sets Based Maximum Correntropy Criterion
- **Arxiv ID**: http://arxiv.org/abs/2208.11327v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11327v1)
- **Published**: 2022-08-24 06:49:43+00:00
- **Updated**: 2022-08-24 06:49:43+00:00
- **Authors**: Yugeng Huang, Haitao Liu, Tian Huang
- **Comment**: None
- **Journal**: None
- **Summary**: As an efficient algorithm to solve the multi-view registration problem,the motion averaging (MA) algorithm has been extensively studied and many MA-based algorithms have been introduced. They aim at recovering global motions from relative motions and exploiting information redundancy to average accumulative errors. However, one property of these methods is that they use Guass-Newton method to solve a least squares problem for the increment of global motions, which may lead to low efficiency and poor robustness to outliers. In this paper, we propose a novel motion averaging framework for the multi-view registration with Laplacian kernel-based maximum correntropy criterion (LMCC). Utilizing the Lie algebra motion framework and the correntropy measure, we propose a new cost function that takes all constraints supplied by relative motions into account. Obtaining the increment used to correct the global motions, can further be formulated as an optimization problem aimed at maximizing the cost function. By virtue of the quadratic technique, the optimization problem can be solved by dividing into two subproblems, i.e., computing the weight for each relative motion according to the current residuals and solving a second-order cone program problem (SOCP) for the increment in the next iteration. We also provide a novel strategy for determining the kernel width which ensures that our method can efficiently exploit information redundancy supplied by relative motions in the presence of many outliers. Finally, we compare the proposed method with other MA-based multi-view registration methods to verify its performance. Experimental tests on synthetic and real data demonstrate that our method achieves superior performance in terms of efficiency, accuracy and robustness.



### K-Order Graph-oriented Transformer with GraAttention for 3D Pose and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/2208.11328v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11328v2)
- **Published**: 2022-08-24 06:54:03+00:00
- **Updated**: 2022-09-24 02:20:27+00:00
- **Authors**: Weixi Zhao, Weiqiang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel attention-based 2D-to-3D pose estimation network for graph-structured data, named KOG-Transformer, and a 3D pose-to-shape estimation network for hand data, named GASE-Net. Previous 3D pose estimation methods have focused on various modifications to the graph convolution kernel, such as abandoning weight sharing or increasing the receptive field. Some of these methods employ attention-based non-local modules as auxiliary modules. In order to better model the relationship between nodes in graph-structured data and fuse the information of different neighbor nodes in a differentiated way, we make targeted modifications to the attention module and propose two modules designed for graph-structured data, graph relative positional encoding multi-head self-attention (GR-MSA) and K-order graph-oriented multi-head self-attention (KOG-MSA). By stacking GR-MSA and KOG-MSA, we propose a novel network KOG-Transformer for 2D-to-3D pose estimation. Furthermore, we propose a network for shape estimation on hand data, called GraAttention shape estimation network (GASE-Net), which takes a 3D pose as input and gradually models the shape of the hand from sparse to dense. We have empirically shown the superiority of KOG-Transformer through extensive experiments. Experimental results show that KOG-Transformer significantly outperforms the previous state-of-the-art methods on the benchmark dataset Human3.6M. We evaluate the effect of GASE-Net on two public available hand datasets, ObMan and InterHand2.6M. GASE-Net can predict the corresponding shape for input pose with strong generalization ability.



### Automatic detection of faults in race walking from a smartphone camera: a comparison of an Olympic medalist and university athletes
- **Arxiv ID**: http://arxiv.org/abs/2208.12646v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.12646v1)
- **Published**: 2022-08-24 07:04:36+00:00
- **Updated**: 2022-08-24 07:04:36+00:00
- **Authors**: Tomohiro Suzuki, Kazuya Takeda, Keisuke Fujii
- **Comment**: 16 pages, 9 figures
- **Journal**: None
- **Summary**: Automatic fault detection is a major challenge in many sports. In race walking, referees visually judge faults according to the rules. Hence, ensuring objectivity and fairness while judging is important. To address this issue, some studies have attempted to use sensors and machine learning to automatically detect faults. However, there are problems associated with sensor attachments and equipment such as a high-speed camera, which conflict with the visual judgement of referees, and the interpretability of the fault detection models. In this study, we proposed a fault detection system for non-contact measurement. We used pose estimation and machine learning models trained based on the judgements of multiple qualified referees to realize fair fault judgement. We verified them using smartphone videos of normal race walking and walking with intentional faults in several athletes including the medalist of the Tokyo Olympics. The validation results show that the proposed system detected faults with an average accuracy of over 90%. We also revealed that the machine learning model detects faults according to the rules of race walking. In addition, the intentional faulty walking movement of the medalist was different from that of university walkers. This finding informs realization of a more general fault detection model. The code and data are available at https://github.com/SZucchini/racewalk-aijudge.



### Monetisation of and Access to in-Vehicle data and resources: the 5GMETA approach
- **Arxiv ID**: http://arxiv.org/abs/2208.11335v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/2208.11335v1)
- **Published**: 2022-08-24 07:16:42+00:00
- **Updated**: 2022-08-24 07:16:42+00:00
- **Authors**: Djibrilla Amadou Kountche, Fatma Raissi, Mandimby Ranaivo Rakotondravelona, Edoardo Bonetto, Daniele Brevi, Angel Martin, Oihana Otaegui, Gorka Velez
- **Comment**: ITS World Congress 2021
- **Journal**: None
- **Summary**: Today's vehicles are increasingly embedded with computers and sensors which produce huge amount of data. The data are exploited for internal purposes and with the development of connected infrastructures and smart cities, the vehicles interact with each other as well as with road users generating other types of data. The access to these data and in-vehicle resources and their monetisation faces many challenges which are presented in this paper. Furthermore, the most important commercial solution compared to the open and novel approach faced in the H2020 5GMETA project.



### A Spatio-Temporal Attentive Network for Video-Based Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2208.11339v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11339v1)
- **Published**: 2022-08-24 07:40:34+00:00
- **Updated**: 2022-08-24 07:40:34+00:00
- **Authors**: Marco Avvenuti, Marco Bongiovanni, Luca Ciampi, Fabrizio Falchi, Claudio Gennaro, Nicola Messina
- **Comment**: Accepted at IEEE ISCC 2022
- **Journal**: None
- **Summary**: Automatic people counting from images has recently drawn attention for urban monitoring in modern Smart Cities due to the ubiquity of surveillance camera networks. Current computer vision techniques rely on deep learning-based algorithms that estimate pedestrian densities in still, individual images. Only a bunch of works take advantage of temporal consistency in video sequences. In this work, we propose a spatio-temporal attentive neural network to estimate the number of pedestrians from surveillance videos. By taking advantage of the temporal correlation between consecutive frames, we lowered state-of-the-art count error by 5% and localization error by 7.5% on the widely-used FDST benchmark.



### Discovering Transferable Forensic Features for CNN-generated Images Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.11342v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11342v1)
- **Published**: 2022-08-24 07:48:07+00:00
- **Updated**: 2022-08-24 07:48:07+00:00
- **Authors**: Keshigeyan Chandrasegaran, Ngoc-Trung Tran, Alexander Binder, Ngai-Man Cheung
- **Comment**: ECCV 2022 Oral; 35 pages
- **Journal**: None
- **Summary**: Visual counterfeits are increasingly causing an existential conundrum in mainstream media with rapid evolution in neural image synthesis methods. Though detection of such counterfeits has been a taxing problem in the image forensics community, a recent class of forensic detectors -- universal detectors -- are able to surprisingly spot counterfeit images regardless of generator architectures, loss functions, training datasets, and resolutions. This intriguing property suggests the possible existence of transferable forensic features (T-FF) in universal detectors. In this work, we conduct the first analytical study to discover and understand T-FF in universal detectors. Our contributions are 2-fold: 1) We propose a novel forensic feature relevance statistic (FF-RS) to quantify and discover T-FF in universal detectors and, 2) Our qualitative and quantitative investigations uncover an unexpected finding: color is a critical T-FF in universal detectors. Code and models are available at https://keshik6.github.io/transferable-forensic-features/



### ICANet: A Method of Short Video Emotion Recognition Driven by Multimodal Data
- **Arxiv ID**: http://arxiv.org/abs/2208.11346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11346v1)
- **Published**: 2022-08-24 07:54:03+00:00
- **Updated**: 2022-08-24 07:54:03+00:00
- **Authors**: Xuecheng Wu, Mengmeng Tian, Lanhang Zhai
- **Comment**: 4 pages, 5 figures
- **Journal**: None
- **Summary**: With the fast development of artificial intelligence and short videos, emotion recognition in short videos has become one of the most important research topics in human-computer interaction. At present, most emotion recognition methods still stay in a single modality. However, in daily life, human beings will usually disguise their real emotions, which leads to the problem that the accuracy of single modal emotion recognition is relatively terrible. Moreover, it is not easy to distinguish similar emotions. Therefore, we propose a new approach denoted as ICANet to achieve multimodal short video emotion recognition by employing three different modalities of audio, video and optical flow, making up for the lack of a single modality and then improving the accuracy of emotion recognition in short videos. ICANet has a better accuracy of 80.77% on the IEMOCAP benchmark, exceeding the SOTA methods by 15.89%.



### Self-Filtering: A Noise-Aware Sample Selection for Label Noise with Confidence Penalization
- **Arxiv ID**: http://arxiv.org/abs/2208.11351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11351v1)
- **Published**: 2022-08-24 08:02:36+00:00
- **Updated**: 2022-08-24 08:02:36+00:00
- **Authors**: Qi Wei, Haoliang Sun, Xiankai Lu, Yilong Yin
- **Comment**: 14 pages
- **Journal**: European Conference on Computer Vision 2022
- **Summary**: Sample selection is an effective strategy to mitigate the effect of label noise in robust learning. Typical strategies commonly apply the small-loss criterion to identify clean samples. However, those samples lying around the decision boundary with large losses usually entangle with noisy examples, which would be discarded with this criterion, leading to the heavy degeneration of the generalization performance. In this paper, we propose a novel selection strategy, \textbf{S}elf-\textbf{F}il\textbf{t}ering (SFT), that utilizes the fluctuation of noisy examples in historical predictions to filter them, which can avoid the selection bias of the small-loss criterion for the boundary examples. Specifically, we introduce a memory bank module that stores the historical predictions of each example and dynamically updates to support the selection for the subsequent learning iteration. Besides, to reduce the accumulated error of the sample selection bias of SFT, we devise a regularization term to penalize the confident output distribution. By increasing the weight of the misclassified categories with this term, the loss function is robust to label noise in mild conditions. We conduct extensive experiments on three benchmarks with variant noise types and achieve the new state-of-the-art. Ablation studies and further analysis verify the virtue of SFT for sample selection in robust learning.



### Research on Mask Wearing Detection of Natural Population Based on Improved YOLOv4
- **Arxiv ID**: http://arxiv.org/abs/2208.11353v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11353v1)
- **Published**: 2022-08-24 08:04:11+00:00
- **Updated**: 2022-08-24 08:04:11+00:00
- **Authors**: Xuecheng Wu, Mengmeng Tian, Lanhang Zhai
- **Comment**: 4 pages, 1 figures
- **Journal**: None
- **Summary**: Recently, the domestic COVID-19 epidemic situation has been serious, but in some public places, some people do not wear masks or wear masks incorrectly, which requires the relevant staff to instantly remind and supervise them to wear masks correctly. However, in the face of such important and complicated work, it is necessary to carry out automated mask wearing detection in public places. This paper proposes a new mask wearing detection method based on the improved YOLOv4. Specifically, firstly, we add the Coordinate Attention Module to the backbone to coordinate feature fusion and representation. Secondly, we conduct a series of network structural improvements to enhance the model performance and robustness. Thirdly, we deploy the K-means clustering algorithm to make the nine anchor boxes more suitable for our NPMD dataset. The experimental results show that the improved YOLOv4 performs better, exceeding the baseline by 4.06% AP with a comparable speed of 64.37 FPS.



### Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2208.11356v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.11356v2)
- **Published**: 2022-08-24 08:09:25+00:00
- **Updated**: 2023-03-24 02:06:36+00:00
- **Authors**: Gongjie Zhang, Zhipeng Luo, Zichen Tian, Jingyi Zhang, Xiaoqin Zhang, Shijian Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-scale features have been proven highly effective for object detection but often come with huge and even prohibitive extra computation costs, especially for the recent Transformer-based detectors. In this paper, we propose Iterative Multi-scale Feature Aggregation (IMFA) -- a generic paradigm that enables efficient use of multi-scale features in Transformer-based object detectors. The core idea is to exploit sparse multi-scale features from just a few crucial locations, and it is achieved with two novel designs. First, IMFA rearranges the Transformer encoder-decoder pipeline so that the encoded features can be iteratively updated based on the detection predictions. Second, IMFA sparsely samples scale-adaptive features for refined detection from just a few keypoint locations under the guidance of prior detection predictions. As a result, the sampled multi-scale features are sparse yet still highly beneficial for object detection. Extensive experiments show that the proposed IMFA boosts the performance of multiple Transformer-based object detectors significantly yet with only slight computational overhead.



### On the Design of Privacy-Aware Cameras: a Study on Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.11372v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/2208.11372v1)
- **Published**: 2022-08-24 08:45:31+00:00
- **Updated**: 2022-08-24 08:45:31+00:00
- **Authors**: Marcela Carvalho, Oussama Ennaffi, Sylvain Chateau, Samy Ait Bachir
- **Comment**: Accepted in ECCV 2022 International Workshop on Distributed Cameras
- **Journal**: None
- **Summary**: In spite of the legal advances in personal data protection, the issue of private data being misused by unauthorized entities is still of utmost importance. To prevent this, Privacy by Design is often proposed as a solution for data protection. In this paper, the effect of camera distortions is studied using Deep Learning techniques commonly used to extract sensitive data. To do so, we simulate out-of-focus images corresponding to a realistic conventional camera with fixed focal length, aperture, and focus, as well as grayscale images coming from a monochrome camera. We then prove, through an experimental study, that we can build a privacy-aware camera that cannot extract personal information such as license plate numbers. At the same time, we ensure that useful non-sensitive data can still be extracted from distorted images. Code is available at https://github.com/upciti/privacy-by-design-semseg .



### WiCV 2022: The Tenth Women In Computer Vision Workshop
- **Arxiv ID**: http://arxiv.org/abs/2208.11388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11388v1)
- **Published**: 2022-08-24 09:12:36+00:00
- **Updated**: 2022-08-24 09:12:36+00:00
- **Authors**: Doris Antensteiner, Silvia Bucci, Arushi Goel, Marah Halawa, Niveditha Kalavakonda, Tejaswi Kasarla, Miaomiao Liu, Nermin Samet, Ivaxi Sheth
- **Comment**: Report on WiCV Workshop at CVPR 2022. arXiv admin note: substantial
  text overlap with arXiv:2203.05825, arXiv:2101.03787
- **Journal**: None
- **Summary**: In this paper, we present the details of Women in Computer Vision Workshop - WiCV 2022, organized alongside the hybrid CVPR 2022 in New Orleans, Louisiana. It provides a voice to a minority (female) group in the computer vision community and focuses on increasing the visibility of these researchers, both in academia and industry. WiCV believes that such an event can play an important role in lowering the gender imbalance in the field of computer vision. WiCV is organized each year where it provides a) opportunity for collaboration between researchers from minority groups, b) mentorship to female junior researchers, c) financial support to presenters to overcome monetary burden and d) large and diverse choice of role models, who can serve as examples to younger researchers at the beginning of their careers. In this paper, we present a report on the workshop program, trends over the past years, a summary of statistics regarding presenters, attendees, and sponsorship for the WiCV 2022 workshop.



### Event-based Image Deblurring with Dynamic Motion Awareness
- **Arxiv ID**: http://arxiv.org/abs/2208.11398v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11398v1)
- **Published**: 2022-08-24 09:39:55+00:00
- **Updated**: 2022-08-24 09:39:55+00:00
- **Authors**: Patricia Vitoria, Stamatios Georgoulis, Stepan Tulyakov, Alfredo Bochicchio, Julius Erbach, Yuanyou Li
- **Comment**: None
- **Journal**: ECCVW 2022
- **Summary**: Non-uniform image deblurring is a challenging task due to the lack of temporal and textural information in the blurry image itself. Complementary information from auxiliary sensors such event sensors are being explored to address these limitations. The latter can record changes in a logarithmic intensity asynchronously, called events, with high temporal resolution and high dynamic range. Current event-based deblurring methods combine the blurry image with events to jointly estimate per-pixel motion and the deblur operator. In this paper, we argue that a divide-and-conquer approach is more suitable for this task. To this end, we propose to use modulated deformable convolutions, whose kernel offsets and modulation masks are dynamically estimated from events to encode the motion in the scene, while the deblur operator is learned from the combination of blurry image and corresponding events. Furthermore, we employ a coarse-to-fine multi-scale reconstruction approach to cope with the inherent sparsity of events in low contrast regions. Importantly, we introduce the first dataset containing pairs of real RGB blur images and related events during the exposure time. Our results show better overall robustness when using events, with improvements in PSNR by up to 1.57dB on synthetic data and 1.08 dB on real event data.



### Radial Basis Function Networks for Convolutional Neural Networks to Learn Similarity Distance Metric and Improve Interpretability
- **Arxiv ID**: http://arxiv.org/abs/2208.11401v1
- **DOI**: 10.1109/ACCESS.2020.3007337
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11401v1)
- **Published**: 2022-08-24 09:48:00+00:00
- **Updated**: 2022-08-24 09:48:00+00:00
- **Authors**: Mohammadreza Amirian, Friedhelm Schwenker
- **Comment**: 12 pages, 8 figures
- **Journal**: IEEE Access (2020)
- **Summary**: Radial basis function neural networks (RBFs) are prime candidates for pattern classification and regression and have been used extensively in classical machine learning applications. However, RBFs have not been integrated into contemporary deep learning research and computer vision using conventional convolutional neural networks (CNNs) due to their lack of adaptability with modern architectures. In this paper, we adapt RBF networks as a classifier on top of CNNs by modifying the training process and introducing a new activation function to train modern vision architectures end-to-end for image classification. The specific architecture of RBFs enables the learning of a similarity distance metric to compare and find similar and dissimilar images. Furthermore, we demonstrate that using an RBF classifier on top of any CNN architecture provides new human-interpretable insights about the decision-making process of the models. Finally, we successfully apply RBFs to a range of CNN architectures and evaluate the results on benchmark computer vision datasets.



### Adaptive QoS of WebRTC for Vehicular Media Communications
- **Arxiv ID**: http://arxiv.org/abs/2208.11405v1
- **DOI**: 10.1109/BMSB55706.2022.9828782
- **Categories**: **cs.NI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.11405v1)
- **Published**: 2022-08-24 09:51:59+00:00
- **Updated**: 2022-08-24 09:51:59+00:00
- **Authors**: Ángel Martín, Daniel Mejías, Zaloa Fernández, Roberto Viola, Josu Pérez, Mikel García, Gorka Velez, Jon Montalbán, Pablo Angueira
- **Comment**: None
- **Journal**: 2022 IEEE International Symposium on Broadband Multimedia Systems
  and Broadcasting (BMSB), 2022, pp. 1-6
- **Summary**: Vehicles shipping sensors for onboard systems are gaining connectivity. This enables information sharing to realize a more comprehensive understanding of the environment. However, peer communication through public cellular networks brings multiple networking hurdles to address, needing in-network systems to relay communications and connect parties that cannot connect directly. Web Real-Time Communication (WebRTC) is a good candidate for media streaming across vehicles as it enables low latency communications, while bringing standard protocols to security handshake, discovering public IPs and transverse Network Address Translation (NAT) systems. However, the end-to-end Quality of Service (QoS) adaptation in an infrastructure where transmission and reception are decoupled by a relay, needs a mechanism to adapt the video stream to the network capacity efficiently. To this end, this paper investigates a mechanism to apply changes on resolution, framerate and bitrate by exploiting the Real Time Transport Control Protocol (RTCP) metrics, such as bandwidth and round-trip time. The solution aims to ensure that the receiving onboard system gets relevant information in time. The impact on end-to-end throughput efficiency and reaction time when applying different approaches to QoS adaptation are analyzed in a real 5G testbed.



### Self-Supervised Endoscopic Image Key-Points Matching
- **Arxiv ID**: http://arxiv.org/abs/2208.11424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11424v1)
- **Published**: 2022-08-24 10:47:21+00:00
- **Updated**: 2022-08-24 10:47:21+00:00
- **Authors**: Manel Farhat, Houda Chaabouni-Chouayakh, Achraf Ben-Hamadou
- **Comment**: 35 pages
- **Journal**: None
- **Summary**: Feature matching and finding correspondences between endoscopic images is a key step in many clinical applications such as patient follow-up and generation of panoramic image from clinical sequences for fast anomalies localization. Nonetheless, due to the high texture variability present in endoscopic images, the development of robust and accurate feature matching becomes a challenging task. Recently, deep learning techniques which deliver learned features extracted via convolutional neural networks (CNNs) have gained traction in a wide range of computer vision tasks. However, they all follow a supervised learning scheme where a large amount of annotated data is required to reach good performances, which is generally not always available for medical data databases. To overcome this limitation related to labeled data scarcity, the self-supervised learning paradigm has recently shown great success in a number of applications. This paper proposes a novel self-supervised approach for endoscopic image matching based on deep learning techniques. When compared to standard hand-crafted local feature descriptors, our method outperformed them in terms of precision and recall. Furthermore, our self-supervised descriptor provides a competitive performance in comparison to a selection of state-of-the-art deep learning based supervised methods in terms of precision and matching score.



### YOLOPv2: Better, Faster, Stronger for Panoptic Driving Perception
- **Arxiv ID**: http://arxiv.org/abs/2208.11434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11434v1)
- **Published**: 2022-08-24 11:00:27+00:00
- **Updated**: 2022-08-24 11:00:27+00:00
- **Authors**: Cheng Han, Qichao Zhao, Shuyi Zhang, Yinzi Chen, Zhenlin Zhang, Jinwei Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last decade, multi-tasking learning approaches have achieved promising results in solving panoptic driving perception problems, providing both high-precision and high-efficiency performance. It has become a popular paradigm when designing networks for real-time practical autonomous driving system, where computation resources are limited. This paper proposed an effective and efficient multi-task learning network to simultaneously perform the task of traffic object detection, drivable road area segmentation and lane detection. Our model achieved the new state-of-the-art (SOTA) performance in terms of accuracy and speed on the challenging BDD100K dataset. Especially, the inference time is reduced by half compared to the previous SOTA model. Code will be released in the near future.



### Bidirectional Contrastive Split Learning for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/2208.11435v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11435v3)
- **Published**: 2022-08-24 11:01:47+00:00
- **Updated**: 2023-08-03 04:28:15+00:00
- **Authors**: Yuwei Sun, Hideya Ochiai
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) based on multi-modal data facilitates real-life applications such as home robots and medical diagnoses. One significant challenge is to devise a robust decentralized learning framework for various client models where centralized data collection is refrained due to confidentiality concerns. This work aims to tackle privacy-preserving VQA by decoupling a multi-modal model into representation modules and a contrastive module and leveraging inter-module gradients sharing and inter-client weight sharing. To this end, we propose Bidirectional Contrastive Split Learning (BiCSL) to train a global multi-modal model on the entire data distribution of decentralized clients. We employ the contrastive loss that enables a more efficient self-supervised learning of decentralized modules. Comprehensive experiments are conducted on the VQA-v2 dataset based on five SOTA VQA models, demonstrating the effectiveness of the proposed method. Furthermore, we inspect BiCSL's robustness against a dual-key backdoor attack on VQA. Consequently, BiCSL shows much better robustness to the multi-modal adversarial attack compared to the centralized learning method, which provides a promising approach to decentralized multi-modal learning.



### Trace and Detect Adversarial Attacks on CNNs using Feature Response Maps
- **Arxiv ID**: http://arxiv.org/abs/2208.11436v1
- **DOI**: 10.21256/zhaw-3863
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11436v1)
- **Published**: 2022-08-24 11:05:04+00:00
- **Updated**: 2022-08-24 11:05:04+00:00
- **Authors**: Mohammadreza Amirian, Friedhelm Schwenker, Thilo Stadelmann
- **Comment**: 13 pages, 6 figures
- **Journal**: 8th IAPR TC3 Workshop on Artificial Neural Networks in Pattern
  Recognition 8th IAPR TC3 Workshop on Artificial Neural Networks in Pattern
  Recognition (ANNPR 2018)
- **Summary**: The existence of adversarial attacks on convolutional neural networks (CNN) questions the fitness of such models for serious applications. The attacks manipulate an input image such that misclassification is evoked while still looking normal to a human observer -- they are thus not easily detectable. In a different context, backpropagated activations of CNN hidden layers -- "feature responses" to a given input -- have been helpful to visualize for a human "debugger" what the CNN "looks at" while computing its output. In this work, we propose a novel detection method for adversarial examples to prevent attacks. We do so by tracking adversarial perturbations in feature responses, allowing for automatic detection using average local spatial entropy. The method does not alter the original network architecture and is fully human-interpretable. Experiments confirm the validity of our approach for state-of-the-art attacks on large-scale models trained on ImageNet.



### Dynamic Template Initialization for Part-Aware Person Re-ID
- **Arxiv ID**: http://arxiv.org/abs/2208.11440v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.11440v1)
- **Published**: 2022-08-24 11:20:48+00:00
- **Updated**: 2022-08-24 11:20:48+00:00
- **Authors**: Kalana Abeywardena, Shechem Sumanthiran, Sanoojan Baliah, Nadarasar Bahavan, Nalith Udugampola, Ajith Pasqual, Chamira Edussooriya, Ranga Rodrigo
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Many of the existing Person Re-identification (Re-ID) approaches depend on feature maps which are either partitioned to localize parts of a person or reduced to create a global representation. While part localization has shown significant success, it uses either na{\i}ve position-based partitions or static feature templates. These, however, hypothesize the pre-existence of the parts in a given image or their positions, ignoring the input image-specific information which limits their usability in challenging scenarios such as Re-ID with partial occlusions and partial probe images. In this paper, we introduce a spatial attention-based Dynamic Part Template Initialization module that dynamically generates part-templates using mid-level semantic features at the earlier layers of the backbone. Following a self-attention layer, human part-level features of the backbone are used to extract the templates of diverse human body parts using a simplified cross-attention scheme which will then be used to identify and collate representations of various human parts from semantically rich features, increasing the discriminative ability of the entire model. We further explore adaptive weighting of part descriptors to quantify the absence or occlusion of local attributes and suppress the contribution of the corresponding part descriptors to the matching criteria. Extensive experiments on holistic, occluded, and partial Re-ID task benchmarks demonstrate that our proposed architecture is able to achieve competitive performance. Codes will be included in the supplementary material and will be made publicly available.



### Hybrid Fusion Based Interpretable Multimodal Emotion Recognition with Insufficient Labelled Data
- **Arxiv ID**: http://arxiv.org/abs/2208.11450v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11450v1)
- **Published**: 2022-08-24 11:35:51+00:00
- **Updated**: 2022-08-24 11:35:51+00:00
- **Authors**: Puneet Kumar, Sarthak Malik, Balasubramanian Raman
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a multimodal emotion recognition system, VIsual Spoken Textual Additive Net (VISTA Net), to classify the emotions reflected by a multimodal input containing image, speech, and text into discrete classes. A new interpretability technique, K-Average Additive exPlanation (KAAP), has also been developed to identify the important visual, spoken, and textual features leading to predicting a particular emotion class. The VISTA Net fuses the information from image, speech & text modalities using a hybrid of early and late fusion. It automatically adjusts the weights of their intermediate outputs while computing the weighted average without human intervention. The KAAP technique computes the contribution of each modality and corresponding features toward predicting a particular emotion class. To mitigate the insufficiency of multimodal emotion datasets labeled with discrete emotion classes, we have constructed a large-scale IIT-R MMEmoRec dataset consisting of real-life images, corresponding speech & text, and emotion labels ('angry,' 'happy,' 'hate,' and 'sad.'). The VISTA Net has resulted in 95.99% emotion recognition accuracy on considering image, speech, and text modalities, which is better than the performance on considering the inputs of any one or two modalities.



### Q-Net: Query-Informed Few-Shot Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.11451v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11451v3)
- **Published**: 2022-08-24 11:36:53+00:00
- **Updated**: 2023-03-07 09:58:15+00:00
- **Authors**: Qianqian Shen, Yanan Li, Jiyong Jin, Bin Liu
- **Comment**: Accpeted by Intelligent Systems Conference (IntelliSys) 2023
- **Journal**: None
- **Summary**: Deep learning has achieved tremendous success in computer vision, while medical image segmentation (MIS) remains a challenge, due to the scarcity of data annotations. Meta-learning techniques for few-shot segmentation (Meta-FSS) have been widely used to tackle this challenge, while they neglect possible distribution shifts between the query image and the support set. In contrast, an experienced clinician can perceive and address such shifts by borrowing information from the query image, then fine-tune or calibrate her prior cognitive model accordingly. Inspired by this, we propose Q-Net, a Query-informed Meta-FSS approach, which mimics in spirit the learning mechanism of an expert clinician. We build Q-Net based on ADNet, a recently proposed anomaly detection-inspired method. Specifically, we add two query-informed computation modules into ADNet, namely a query-informed threshold adaptation module and a query-informed prototype refinement module. Combining them with a dual-path extension of the feature extraction module, Q-Net achieves state-of-the-art performance on widely used abdominal and cardiac magnetic resonance (MR) image datasets. Our work sheds light on a novel way to improve Meta-FSS techniques by leveraging query information.



### Symbolic Replay: Scene Graph as Prompt for Continual Learning on VQA Task
- **Arxiv ID**: http://arxiv.org/abs/2208.12037v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.12037v2)
- **Published**: 2022-08-24 12:00:02+00:00
- **Updated**: 2022-08-29 10:22:20+00:00
- **Authors**: Stan Weixian Lei, Difei Gao, Jay Zhangjie Wu, Yuxuan Wang, Wei Liu, Mengmi Zhang, Mike Zheng Shou
- **Comment**: 18 pages, 13 figures
- **Journal**: None
- **Summary**: VQA is an ambitious task aiming to answer any image-related question. However, in reality, it is hard to build such a system once for all since the needs of users are continuously updated, and the system has to implement new functions. Thus, Continual Learning (CL) ability is a must in developing advanced VQA systems. Recently, a pioneer work split a VQA dataset into disjoint answer sets to study this topic. However, CL on VQA involves not only the expansion of label sets (new Answer sets). It is crucial to study how to answer questions when deploying VQA systems to new environments (new Visual scenes) and how to answer questions requiring new functions (new Question types). Thus, we propose CLOVE, a benchmark for Continual Learning On Visual quEstion answering, which contains scene- and function-incremental settings for the two aforementioned CL scenarios. In terms of methodology, the main difference between CL on VQA and classification is that the former additionally involves expanding and preventing forgetting of reasoning mechanisms, while the latter focusing on class representation. Thus, we propose a real-data-free replay-based method tailored for CL on VQA, named Scene Graph as Prompt for Symbolic Replay. Using a piece of scene graph as a prompt, it replays pseudo scene graphs to represent the past images, along with correlated QA pairs. A unified VQA model is also proposed to utilize the current and replayed data to enhance its QA ability. Finally, experimental results reveal challenges in CLOVE and demonstrate the effectiveness of our method. The dataset and code will be available at https://github.com/showlab/CLVQA.



### Tracking by weakly-supervised learning and graph optimization for whole-embryo C. elegans lineages
- **Arxiv ID**: http://arxiv.org/abs/2208.11467v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11467v1)
- **Published**: 2022-08-24 12:17:59+00:00
- **Updated**: 2022-08-24 12:17:59+00:00
- **Authors**: Peter Hirsch, Caroline Malin-Mayor, Anthony Santella, Stephan Preibisch, Dagmar Kainmueller, Jan Funke
- **Comment**: Accepted at MICCAI 2022, Code: https://github.com/funkelab/linajea
- **Journal**: None
- **Summary**: Tracking all nuclei of an embryo in noisy and dense fluorescence microscopy data is a challenging task. We build upon a recent method for nuclei tracking that combines weakly-supervised learning from a small set of nuclei center point annotations with an integer linear program (ILP) for optimal cell lineage extraction. Our work specifically addresses the following challenging properties of C. elegans embryo recordings: (1) Many cell divisions as compared to benchmark recordings of other organisms, and (2) the presence of polar bodies that are easily mistaken as cell nuclei. To cope with (1), we devise and incorporate a learnt cell division detector. To cope with (2), we employ a learnt polar body detector. We further propose automated ILP weights tuning via a structured SVM, alleviating the need for tedious manual set-up of a respective grid search. Our method outperforms the previous leader of the cell tracking challenge on the Fluo-N3DH-CE embryo dataset. We report a further extensive quantitative evaluation on two more C. elegans datasets. We will make these datasets public to serve as an extended benchmark for future method development. Our results suggest considerable improvements yielded by our method, especially in terms of the correctness of division event detection and the number and length of fully correct track segments. Code: https://github.com/funkelab/linajea



### Weakly Supervised Airway Orifice Segmentation in Video Bronchoscopy
- **Arxiv ID**: http://arxiv.org/abs/2208.11468v1
- **DOI**: 10.1117/12.2654229
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11468v1)
- **Published**: 2022-08-24 12:18:25+00:00
- **Updated**: 2022-08-24 12:18:25+00:00
- **Authors**: Ron Keuth, Mattias Heinrich, Martin Eichenlaub, Marian Himstedt
- **Comment**: 5 Pages, 2 figures, only supplemental file, submitted to SPIE MI
- **Journal**: SPIE Medical Imaging 2023: Image Processing
- **Summary**: Video bronchoscopy is routinely conducted for biopsies of lung tissue suspected for cancer, monitoring of COPD patients and clarification of acute respiratory problems at intensive care units. The navigation within complex bronchial trees is particularly challenging and physically demanding, requiring long-term experiences of physicians. This paper addresses the automatic segmentation of bronchial orifices in bronchoscopy videos. Deep learning-based approaches to this task are currently hampered due to the lack of readily-available ground truth segmentation data. Thus, we present a data-driven pipeline consisting of a k-means followed by a compact marker-based watershed algorithm which enables to generate airway instance segmentation maps from given depth images. In this way, these traditional algorithms serve as weak supervision for training a shallow CNN directly on RGB images solely based on a phantom dataset. We evaluate generalization capabilities of this model on two in-vivo datasets covering 250 frames on 21 different bronchoscopies. We demonstrate that its performance is comparable to those models being directly trained on in-vivo data, reaching an average error of 11 vs 5 pixels for the detected centers of the airway segmentation by an image resolution of 128x128. Our quantitative and qualitative results indicate that in the context of video bronchoscopy, phantom data and weak supervision using non-learning-based approaches enable to gain a semantic understanding of airway structures.



### A Deep Learning Approach Using Masked Image Modeling for Reconstruction of Undersampled K-spaces
- **Arxiv ID**: http://arxiv.org/abs/2208.11472v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, J.3; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2208.11472v1)
- **Published**: 2022-08-24 12:27:54+00:00
- **Updated**: 2022-08-24 12:27:54+00:00
- **Authors**: Kyler Larsen, Arghya Pal, Yogesh Rathi
- **Comment**: 15 pages, 13 figures
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) scans are time consuming and precarious, since the patients remain still in a confined space for extended periods of time. To reduce scanning time, some experts have experimented with undersampled k spaces, trying to use deep learning to predict the fully sampled result. These studies report that as many as 20 to 30 minutes could be saved off a scan that takes an hour or more. However, none of these studies have explored the possibility of using masked image modeling (MIM) to predict the missing parts of MRI k spaces. This study makes use of 11161 reconstructed MRI and k spaces of knee MRI images from Facebook's fastmri dataset. This tests a modified version of an existing model using baseline shifted window (Swin) and vision transformer architectures that makes use of MIM on undersampled k spaces to predict the full k space and consequently the full MRI image. Modifications were made using pytorch and numpy libraries, and were published to a github repository. After the model reconstructed the k space images, the basic Fourier transform was applied to determine the actual MRI image. Once the model reached a steady state, experimentation with hyperparameters helped to achieve pinpoint accuracy for the reconstructed images. The model was evaluated through L1 loss, gradient normalization, and structural similarity values. The model produced reconstructed images with L1 loss values averaging to <0.01 and gradient normalization values <0.1 after training finished. The reconstructed k spaces yielded structural similarity values of over 99% for both training and validation with the fully sampled k spaces, while validation loss continually decreased under 0.01. These data strongly support the idea that the algorithm works for MRI reconstruction, as they indicate the model's reconstructed image aligns extremely well with the original, fully sampled k space.



### SubFace: Learning with Softmax Approximation for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/2208.11483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11483v1)
- **Published**: 2022-08-24 12:31:08+00:00
- **Updated**: 2022-08-24 12:31:08+00:00
- **Authors**: Hongwei Xu, Suncheng Xiang, Dahong Qian
- **Comment**: None
- **Journal**: None
- **Summary**: The softmax-based loss functions and its variants (e.g., cosface, sphereface, and arcface) significantly improve the face recognition performance in wild unconstrained scenes. A common practice of these algorithms is to perform optimizations on the multiplication between the embedding features and the linear transformation matrix. However in most cases, the dimension of embedding features is given based on traditional design experience, and there is less-studied on improving performance using the feature itself when giving a fixed size. To address this challenge, this paper presents a softmax approximation method called SubFace, which employs the subspace feature to promote the performance of face recognition. Specifically, we dynamically select the non-overlapping subspace features in each batch during training, and then use the subspace features to approximate full-feature among softmax-based loss, so the discriminability of the deep model can be significantly enhanced for face recognition. Comprehensive experiments conducted on benchmark datasets demonstrate that our method can significantly improve the performance of vanilla CNN baseline, which strongly proves the effectiveness of subspace strategy with the margin-based loss.



### Semi-supervised Semantic Segmentation with Mutual Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2208.11499v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11499v3)
- **Published**: 2022-08-24 12:47:58+00:00
- **Updated**: 2023-08-30 06:57:57+00:00
- **Authors**: Jianlong Yuan, Jinchao Ge, Zhibin Wang, Yifan Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Consistency regularization has been widely studied in recent semisupervised semantic segmentation methods, and promising performance has been achieved. In this work, we propose a new consistency regularization framework, termed mutual knowledge distillation (MKD), combined with data and feature augmentation. We introduce two auxiliary mean-teacher models based on consistency regularization. More specifically, we use the pseudo-labels generated by a mean teacher to supervise the student network to achieve a mutual knowledge distillation between the two branches. In addition to using image-level strong and weak augmentation, we also discuss feature augmentation. This involves considering various sources of knowledge to distill the student network. Thus, we can significantly increase the diversity of the training samples. Experiments on public benchmarks show that our framework outperforms previous state-of-the-art (SOTA) methods under various semi-supervised settings. Code is available at semi-mmseg.



### Prostate Lesion Detection and Salient Feature Assessment Using Zone-Based Classifiers
- **Arxiv ID**: http://arxiv.org/abs/2208.11522v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.11522v1)
- **Published**: 2022-08-24 13:08:56+00:00
- **Updated**: 2022-08-24 13:08:56+00:00
- **Authors**: Haoli Yin, Nithin Buduma
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-parametric magnetic resonance imaging (mpMRI) has a growing role in detecting prostate cancer lesions. Thus, it is pertinent that medical professionals who interpret these scans reduce the risk of human error by using computer-aided detection systems. The variety of algorithms used in system implementation, however, has yielded mixed results. Here we investigate the best machine learning classifier for each prostate zone. We also discover salient features to clarify the models' classification rationale. Of the data provided, we gathered and augmented T2 weighted images and apparent diffusion coefficient map images to extract first through third order statistical features as input to machine learning classifiers. For our deep learning classifier, we used a convolutional neural net (CNN) architecture for automatic feature extraction and classification. The interpretability of the CNN results was improved by saliency mapping to understand the classification mechanisms within. Ultimately, we concluded that effective detection of peripheral and anterior fibromuscular stroma (AS) lesions depended more on statistical distribution features, whereas those in the transition zone (TZ) depended more on textural features. Ensemble algorithms worked best for PZ and TZ zones, while CNNs were best in the AS zone. These classifiers can be used to validate a radiologist's predictions and reduce inter-reader variability in patients suspected to have prostate cancer. The salient features reported in this study can also be investigated further to better understand hidden features and biomarkers of prostate lesions with mpMRIs.



### Fast and Precise Binary Instance Segmentation of 2D Objects for Automotive Applications
- **Arxiv ID**: http://arxiv.org/abs/2208.11527v1
- **DOI**: 10.24132/csrn.3201.38
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11527v1)
- **Published**: 2022-08-24 13:19:34+00:00
- **Updated**: 2022-08-24 13:19:34+00:00
- **Authors**: Darshan Ganganna Ravindra, Laslo Dinges, Al-Hamadi Ayoub, Vasili Baranau
- **Comment**: 4 pages, 4 figures, WSCG 2022 conference [WSCG 2022 Proceedings, CSRN
  3201, ISSN 2464-4617]
- **Journal**: Journal of WSCG, Vol.30, 2022, 302-305 ISSN 1213-6972
- **Summary**: In this paper, we focus on improving binary 2D instance segmentation to assist humans in labeling ground truth datasets with polygons. Humans labeler just have to draw boxes around objects, and polygons are generated automatically. To be useful, our system has to run on CPUs in real-time. The most usual approach for binary instance segmentation involves encoder-decoder networks. This report evaluates state-of-the-art encoder-decoder networks and proposes a method for improving instance segmentation quality using these networks. Alongside network architecture improvements, our proposed method relies upon providing extra information to the network input, so-called extreme points, i.e. the outermost points on the object silhouette. The user can label them instead of a bounding box almost as quickly. The bounding box can be deduced from the extreme points as well. This method produces better IoU compared to other state-of-the-art encoder-decoder networks and also runs fast enough when it is deployed on a CPU.



### A novel method for data augmentation: Nine Dot Moving Least Square (ND-MLS)
- **Arxiv ID**: http://arxiv.org/abs/2208.11532v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11532v1)
- **Published**: 2022-08-24 13:26:53+00:00
- **Updated**: 2022-08-24 13:26:53+00:00
- **Authors**: Wen Yang, Rui Wang, Yanchao Zhang
- **Comment**: 16 pages,13 figures
- **Journal**: None
- **Summary**: Data augmentation greatly increases the amount of data obtained based on labeled data to save on expenses and labor for data collection and labeling. We present a new approach for data augmentation called nine-dot MLS (ND-MLS). This approach is proposed based on the idea of image defor-mation. Images are deformed based on control points, which are calculated by ND-MLS. The method can generate over 2000 images for one exist-ing dataset in a short time. To verify this data augmentation method, extensive tests were performed covering 3 main tasks of computer vision, namely, classification, detection and segmentation. The results show that 1) in classification, 10 images per category were used for training, and VGGNet can obtain 92% top-1 acc on the MNIST dataset of handwritten digits by ND-MLS. In the Omniglot dataset, the few-shot accuracy usu-ally decreases with the increase in character categories. However, the ND-MLS method has stable performance and obtains 96.5 top-1 acc in Res-Net on 100 different handwritten character classification tasks; 2) in segmentation, under the premise of only ten original images, DeepLab obtains 93.5%, 85%, and 73.3% m_IOU(10) on the bottle, horse, and grass test datasets, respectively, while the cat test dataset obtains 86.7% m_IOU(10) with the SegNet model; 3) with only 10 original images from each category in object detection, YOLO v4 obtains 100% and 97.2% bottle and horse detection, respectively, while the cat dataset obtains 93.6% with YOLO v3. In summary, ND-MLS can perform well on classification, object detec-tion, and semantic segmentation tasks by using only a few data.



### ssFPN: Scale Sequence (S^2) Feature Based-Feature Pyramid Network for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.11533v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11533v2)
- **Published**: 2022-08-24 13:29:12+00:00
- **Updated**: 2022-08-25 04:22:56+00:00
- **Authors**: Hye-Jin Park, Young-Ju Choi, Young-Woon Lee, Byung-Gyu Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Feature Pyramid Network (FPN) has been an essential module for object detection models to consider various scales of an object. However, average precision (AP) on small objects is relatively lower than AP on medium and large objects. The reason is why the deeper layer of CNN causes information loss as feature extraction level. We propose a new scale sequence (S^2) feature extraction of FPN to strengthen feature information of small objects. We consider FPN structure as scale-space and extract scale sequence (S^2) feature by 3D convolution on the level axis of FPN. It is basically scale invariant feature and is built on high-resolution pyramid feature map for small objects. Furthermore, the proposed S^2 feature can be extended to most object detection models based on FPN. We demonstrate the proposed S2 feature can improve the performance of both one-stage and two-stage detectors on MS COCO dataset. Based on the proposed S2 feature, we achieve upto 1.3% and 1.1% of AP improvement for YOLOv4-P5 and YOLOv4-P6, respectively. For Faster RCNN and Mask R-CNN, we observe upto 2.0% and 1.6% of AP improvement with the suggested S^2 feature, respectively.



### PeRFception: Perception using Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2208.11537v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11537v1)
- **Published**: 2022-08-24 13:32:46+00:00
- **Updated**: 2022-08-24 13:32:46+00:00
- **Authors**: Yoonwoo Jeong, Seungjoo Shin, Junha Lee, Christopher Choy, Animashree Anandkumar, Minsu Cho, Jaesik Park
- **Comment**: Project Page: https://postech-cvlab.github.io/PeRFception/
- **Journal**: None
- **Summary**: The recent progress in implicit 3D representation, i.e., Neural Radiance Fields (NeRFs), has made accurate and photorealistic 3D reconstruction possible in a differentiable manner. This new representation can effectively convey the information of hundreds of high-resolution images in one compact format and allows photorealistic synthesis of novel views. In this work, using the variant of NeRF called Plenoxels, we create the first large-scale implicit representation datasets for perception tasks, called the PeRFception, which consists of two parts that incorporate both object-centric and scene-centric scans for classification and segmentation. It shows a significant memory compression rate (96.4\%) from the original dataset, while containing both 2D and 3D information in a unified form. We construct the classification and segmentation models that directly take as input this implicit format and also propose a novel augmentation technique to avoid overfitting on backgrounds of images. The code and data are publicly available in https://postech-cvlab.github.io/PeRFception .



### Unsupervised Structure-Consistent Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/2208.11546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11546v1)
- **Published**: 2022-08-24 13:47:15+00:00
- **Updated**: 2022-08-24 13:47:15+00:00
- **Authors**: Shima Shahfar, Charalambos Poullis
- **Comment**: structure-consistent image-to-image translation \and style transfer
  \and training class imbalance
- **Journal**: None
- **Summary**: The Swapping Autoencoder achieved state-of-the-art performance in deep image manipulation and image-to-image translation. We improve this work by introducing a simple yet effective auxiliary module based on gradient reversal layers. The auxiliary module's loss forces the generator to learn to reconstruct an image with an all-zero texture code, encouraging better disentanglement between the structure and texture information. The proposed attribute-based transfer method enables refined control in style transfer while preserving structural information without using a semantic mask. To manipulate an image, we encode both the geometry of the objects and the general style of the input images into two latent codes with an additional constraint that enforces structure consistency. Moreover, due to the auxiliary loss, training time is significantly reduced. The superiority of the proposed model is demonstrated in complex domains such as satellite images where state-of-the-art are known to fail. Lastly, we show that our model improves the quality metrics for a wide range of datasets while achieving comparable results with multi-modal image generation techniques.



### Improving video retrieval using multilingual knowledge transfer
- **Arxiv ID**: http://arxiv.org/abs/2208.11553v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11553v5)
- **Published**: 2022-08-24 13:55:15+00:00
- **Updated**: 2023-01-03 09:05:59+00:00
- **Authors**: Avinash Madasu, Estelle Aflalo, Gabriela Ben Melech Stan, Shao-Yen Tseng, Gedas Bertasius, Vasudev Lal
- **Comment**: None
- **Journal**: None
- **Summary**: Video retrieval has seen tremendous progress with the development of vision-language models. However, further improving these models require additional labelled data which is a huge manual effort. In this paper, we propose a framework MKTVR, that utilizes knowledge transfer from a multilingual model to boost the performance of video retrieval. We first use state-of-the-art machine translation models to construct pseudo ground-truth multilingual video-text pairs. We then use this data to learn a video-text representation where English and non-English text queries are represented in a common embedding space based on pretrained multilingual models. We evaluate our proposed approach on four English video retrieval datasets such as MSRVTT, MSVD, DiDeMo and Charades. Experimental results demonstrate that our approach achieves state-of-the-art results on all datasets outperforming previous models. Finally, we also evaluate our model on a multilingual video-retrieval dataset encompassing six languages and show that our model outperforms previous multilingual video retrieval models in a zero-shot setting.



### Contrastive learning-based pretraining improves representation and transferability of diabetic retinopathy classification models
- **Arxiv ID**: http://arxiv.org/abs/2208.11563v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/2208.11563v1)
- **Published**: 2022-08-24 14:07:45+00:00
- **Updated**: 2022-08-24 14:07:45+00:00
- **Authors**: Minhaj Nur Alam, Rikiya Yamashita, Vignav Ramesh, Tejas Prabhune, Jennifer I. Lim, R. V. P. Chan, Joelle Hallak, Theodore Leng, Daniel Rubin
- **Comment**: None
- **Journal**: None
- **Summary**: Self supervised contrastive learning based pretraining allows development of robust and generalized deep learning models with small, labeled datasets, reducing the burden of label generation. This paper aims to evaluate the effect of CL based pretraining on the performance of referrable vs non referrable diabetic retinopathy (DR) classification. We have developed a CL based framework with neural style transfer (NST) augmentation to produce models with better representations and initializations for the detection of DR in color fundus images. We compare our CL pretrained model performance with two state of the art baseline models pretrained with Imagenet weights. We further investigate the model performance with reduced labeled training data (down to 10 percent) to test the robustness of the model when trained with small, labeled datasets. The model is trained and validated on the EyePACS dataset and tested independently on clinical data from the University of Illinois, Chicago (UIC). Compared to baseline models, our CL pretrained FundusNet model had higher AUC (CI) values (0.91 (0.898 to 0.930) vs 0.80 (0.783 to 0.820) and 0.83 (0.801 to 0.853) on UIC data). At 10 percent labeled training data, the FundusNet AUC was 0.81 (0.78 to 0.84) vs 0.58 (0.56 to 0.64) and 0.63 (0.60 to 0.66) in baseline models, when tested on the UIC dataset. CL based pretraining with NST significantly improves DL classification performance, helps the model generalize well (transferable from EyePACS to UIC data), and allows training with small, annotated datasets, therefore reducing ground truth annotation burden of the clinicians.



### Apple Counting using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.11566v1
- **DOI**: 10.1109/IROS.2018.8594304
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11566v1)
- **Published**: 2022-08-24 14:13:40+00:00
- **Updated**: 2022-08-24 14:13:40+00:00
- **Authors**: Nicolai Häni, Pravakar Roy, Volkan Isler
- **Comment**: None
- **Journal**: 2018 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)
- **Summary**: Estimating accurate and reliable fruit and vegetable counts from images in real-world settings, such as orchards, is a challenging problem that has received significant recent attention. Estimating fruit counts before harvest provides useful information for logistics planning. While considerable progress has been made toward fruit detection, estimating the actual counts remains challenging. In practice, fruits are often clustered together. Therefore, methods that only detect fruits fail to offer general solutions to estimate accurate fruit counts. Furthermore, in horticultural studies, rather than a single yield estimate, finer information such as the distribution of the number of apples per cluster is desirable. In this work, we formulate fruit counting from images as a multi-class classification problem and solve it by training a Convolutional Neural Network. We first evaluate the per-image accuracy of our method and compare it with a state-of-the-art method based on Gaussian Mixture Models over four test datasets. Even though the parameters of the Gaussian Mixture Model-based method are specifically tuned for each dataset, our network outperforms it in three out of four datasets with a maximum of 94\% accuracy. Next, we use the method to estimate the yield for two datasets for which we have ground truth. Our method achieved 96-97\% accuracies. For additional details please see our video here: https://www.youtube.com/watch?v=Le0mb5P-SYc}{https://www.youtube.com/watch?v=Le0mb5P-SYc.



### Cats: Complementary CNN and Transformer Encoders for Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.11572v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.11572v1)
- **Published**: 2022-08-24 14:25:11+00:00
- **Updated**: 2022-08-24 14:25:11+00:00
- **Authors**: Hao Li, Dewei Hu, Han Liu, Jiacheng Wang, Ipek Oguz
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep learning methods have achieved state-of-the-art performance in many medical image segmentation tasks. Many of these are based on convolutional neural networks (CNNs). For such methods, the encoder is the key part for global and local information extraction from input images; the extracted features are then passed to the decoder for predicting the segmentations. In contrast, several recent works show a superior performance with the use of transformers, which can better model long-range spatial dependencies and capture low-level details. However, transformer as sole encoder underperforms for some tasks where it cannot efficiently replace the convolution based encoder. In this paper, we propose a model with double encoders for 3D biomedical image segmentation. Our model is a U-shaped CNN augmented with an independent transformer encoder. We fuse the information from the convolutional encoder and the transformer, and pass it to the decoder to obtain the results. We evaluate our methods on three public datasets from three different challenges: BTCV, MoDA and Decathlon. Compared to the state-of-the-art models with and without transformers on each task, our proposed method obtains higher Dice scores across the board.



### Active Gaze Control for Foveal Scene Exploration
- **Arxiv ID**: http://arxiv.org/abs/2208.11594v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/2208.11594v1)
- **Published**: 2022-08-24 14:59:28+00:00
- **Updated**: 2022-08-24 14:59:28+00:00
- **Authors**: Alexandre M. F. Dias, Luís Simões, Plinio Moreno, Alexandre Bernardino
- **Comment**: 6 pages, 8 figures, ICDL 2022 (International Conference on
  Development and Learning, formerly ICDL-EpiRob)
- **Journal**: None
- **Summary**: Active perception and foveal vision are the foundations of the human visual system. While foveal vision reduces the amount of information to process during a gaze fixation, active perception will change the gaze direction to the most promising parts of the visual field. We propose a methodology to emulate how humans and robots with foveal cameras would explore a scene, identifying the objects present in their surroundings with in least number of gaze shifts. Our approach is based on three key methods. First, we take an off-the-shelf deep object detector, pre-trained on a large dataset of regular images, and calibrate the classification outputs to the case of foveated images. Second, a body-centered semantic map, encoding the objects classifications and corresponding uncertainties, is sequentially updated with the calibrated detections, considering several data fusion techniques. Third, the next best gaze fixation point is determined based on information-theoretic metrics that aim at minimizing the overall expected uncertainty of the semantic map. When compared to the random selection of next gaze shifts, the proposed method achieves an increase in detection F1-score of 2-3 percentage points for the same number of gaze shifts and reduces to one third the number of required gaze shifts to attain similar performance.



### Motion Robust High-Speed Light-Weighted Object Detection With Event Camera
- **Arxiv ID**: http://arxiv.org/abs/2208.11602v2
- **DOI**: 10.1109/TIM.2023.3269780
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11602v2)
- **Published**: 2022-08-24 15:15:24+00:00
- **Updated**: 2023-06-26 01:18:16+00:00
- **Authors**: Bingde Liu, Chang Xu, Wen Yang, Huai Yu, Lei Yu
- **Comment**: Published in: IEEE Transactions on Instrumentation and Measurement
  (Volume: 72) 2023
- **Journal**: None
- **Summary**: In this work, we propose a motion robust and high-speed detection pipeline which better leverages the event data. First, we design an event stream representation called temporal active focus (TAF), which efficiently utilizes the spatial-temporal asynchronous event stream, constructing event tensors robust to object motions. Then, we propose a module called the bifurcated folding module (BFM), which encodes the rich temporal information in the TAF tensor at the input layer of the detector. Following this, we design a high-speed lightweight detector called agile event detector (AED) plus a simple but effective data augmentation method, to enhance the detection accuracy and reduce the model's parameter. Experiments on two typical real-scene event camera object detection datasets show that our method is competitive in terms of accuracy, efficiency, and the number of parameters. By classifying objects into multiple motion levels based on the optical flow density metric, we further illustrated the robustness of our method for objects with different velocities relative to the camera. The codes and trained models are available at https://github.com/HarmoniaLeo/FRLW-EvD .



### Learning crop type mapping from regional label proportions in large-scale SAR and optical imagery
- **Arxiv ID**: http://arxiv.org/abs/2208.11607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11607v1)
- **Published**: 2022-08-24 15:23:26+00:00
- **Updated**: 2022-08-24 15:23:26+00:00
- **Authors**: Laura E. C. La Rosa, Dario A. B. Oliveira, Pedram Ghamisi
- **Comment**: None
- **Journal**: None
- **Summary**: The application of deep learning algorithms to Earth observation (EO) in recent years has enabled substantial progress in fields that rely on remotely sensed data. However, given the data scale in EO, creating large datasets with pixel-level annotations by experts is expensive and highly time-consuming. In this context, priors are seen as an attractive way to alleviate the burden of manual labeling when training deep learning methods for EO. For some applications, those priors are readily available. Motivated by the great success of contrastive-learning methods for self-supervised feature representation learning in many computer-vision tasks, this study proposes an online deep clustering method using crop label proportions as priors to learn a sample-level classifier based on government crop-proportion data for a whole agricultural region. We evaluate the method using two large datasets from two different agricultural regions in Brazil. Extensive experiments demonstrate that the method is robust to different data types (synthetic-aperture radar and optical images), reporting higher accuracy values considering the major crop types in the target regions. Thus, it can alleviate the burden of large-scale image annotation in EO applications.



### Sliding Window Recurrent Network for Efficient Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2208.11608v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.11608v1)
- **Published**: 2022-08-24 15:23:44+00:00
- **Updated**: 2022-08-24 15:23:44+00:00
- **Authors**: Wenyi Lian, Wenjing Lian
- **Comment**: Participated in the AIM 2022 Real-Time Video SR Challenge
- **Journal**: None
- **Summary**: Video super-resolution (VSR) is the task of restoring high-resolution frames from a sequence of low-resolution inputs. Different from single image super-resolution, VSR can utilize frames' temporal information to reconstruct results with more details. Recently, with the rapid development of convolution neural networks (CNN), the VSR task has drawn increasing attention and many CNN-based methods have achieved remarkable results. However, only a few VSR approaches can be applied to real-world mobile devices due to the computational resources and runtime limitations. In this paper, we propose a \textit{Sliding Window based Recurrent Network} (SWRN) which can be real-time inference while still achieving superior performance. Specifically, we notice that video frames should have both spatial and temporal relations that can help to recover details, and the key point is how to extract and aggregate information. Address it, we input three neighboring frames and utilize a hidden state to recurrently store and update the important temporal information. Our experiment on REDS dataset shows that the proposed method can be well adapted to mobile devices and produce visually pleasant results.



### Fast Nearest Convolution for Real-Time Efficient Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/2208.11609v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.11609v1)
- **Published**: 2022-08-24 15:23:51+00:00
- **Updated**: 2022-08-24 15:23:51+00:00
- **Authors**: Ziwei Luo, Youwei Li, Lei Yu, Qi Wu, Zhihong Wen, Haoqiang Fan, Shuaicheng Liu
- **Comment**: AIM & Mobile AI 2022
- **Journal**: None
- **Summary**: Deep learning-based single image super-resolution (SISR) approaches have drawn much attention and achieved remarkable success on modern advanced GPUs. However, most state-of-the-art methods require a huge number of parameters, memories, and computational resources, which usually show inferior inference times when applying them to current mobile device CPUs/NPUs. In this paper, we propose a simple plain convolution network with a fast nearest convolution module (NCNet), which is NPU-friendly and can perform a reliable super-resolution in real-time. The proposed nearest convolution has the same performance as the nearest upsampling but is much faster and more suitable for Android NNAPI. Our model can be easily deployed on mobile devices with 8-bit quantization and is fully compatible with all major mobile AI accelerators. Moreover, we conduct comprehensive experiments on different tensor operations on a mobile device to illustrate the efficiency of our network architecture. Our NCNet is trained and validated on the DIV2K 3x dataset, and the comparison with other efficient SR methods demonstrated that the NCNet can achieve high fidelity SR results while using fewer inference times. Our codes and pretrained models are publicly available at \url{https://github.com/Algolzw/NCNet}.



### Unrestricted Black-box Adversarial Attack Using GAN with Limited Queries
- **Arxiv ID**: http://arxiv.org/abs/2208.11613v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11613v1)
- **Published**: 2022-08-24 15:28:46+00:00
- **Updated**: 2022-08-24 15:28:46+00:00
- **Authors**: Dongbin Na, Sangwoo Ji, Jong Kim
- **Comment**: Accepted to the ECCV 2022 Workshop on Adversarial Robustness in the
  Real World
- **Journal**: None
- **Summary**: Adversarial examples are inputs intentionally generated for fooling a deep neural network. Recent studies have proposed unrestricted adversarial attacks that are not norm-constrained. However, the previous unrestricted attack methods still have limitations to fool real-world applications in a black-box setting. In this paper, we present a novel method for generating unrestricted adversarial examples using GAN where an attacker can only access the top-1 final decision of a classification model. Our method, Latent-HSJA, efficiently leverages the advantages of a decision-based attack in the latent space and successfully manipulates the latent vectors for fooling the classification model.   With extensive experiments, we demonstrate that our proposed method is efficient in evaluating the robustness of classification models with limited queries in a black-box setting. First, we demonstrate that our targeted attack method is query-efficient to produce unrestricted adversarial examples for a facial identity recognition model that contains 307 identities. Then, we demonstrate that the proposed method can also successfully attack a real-world celebrity recognition service.



### Generative Adversarial Network (GAN) based Image-Deblurring
- **Arxiv ID**: http://arxiv.org/abs/2208.11622v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.11622v1)
- **Published**: 2022-08-24 15:46:09+00:00
- **Updated**: 2022-08-24 15:46:09+00:00
- **Authors**: Yuhong Lu, Nicholas Polydorides
- **Comment**: 90 pages, 35 figures, MS Thesis at the University of Edinburgh
- **Journal**: None
- **Summary**: This thesis analyzes the challenging problem of Image Deblurring based on classical theorems and state-of-art methods proposed in recent years. By spectral analysis we mathematically show the effective of spectral regularization methods, and point out the linking between the spectral filtering result and the solution of the regularization optimization objective. For ill-posed problems like image deblurring, the optimization objective contains a regularization term (also called the regularization functional) that encodes our prior knowledge into the solution. We demonstrate how to craft a regularization term by hand using the idea of maximum a posterior estimation. Then, we point out the limitations of such regularization-based methods, and step into the neural-network based methods.   Based on the idea of Wasserstein generative adversarial models, we can train a CNN to learn the regularization functional. Such data-driven approaches are able to capture the complexity, which may not be analytically modellable. Besides, in recent years with the improvement of architectures, the network has been able to output an image closely approximating the ground truth given the blurry observation. The Generative Adversarial Network (GAN) works on this Image-to-Image translation idea. We analyze the DeblurGAN-v2 method proposed by Orest Kupyn et al. [14] in 2019 based on numerical tests. And, based on the experimental results and our knowledge, we put forward some suggestions for improvement on this method.



### GAN-based generative modelling for dermatological applications -- comparative study
- **Arxiv ID**: http://arxiv.org/abs/2208.11702v1
- **DOI**: 10.1007/978-3-031-37649-8_10
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11702v1)
- **Published**: 2022-08-24 15:59:39+00:00
- **Updated**: 2022-08-24 15:59:39+00:00
- **Authors**: Sandra Carrasco Limeros, Sylwia Majchrowska, Mohamad Khir Zoubi, Anna Rosén, Juulia Suvilehto, Lisa Sjöblom, Magnus Kjellberg
- **Comment**: 16 pages, 5 figures, 2 tables
- **Journal**: Digital Interaction and Machine Intelligence. MIDI 2022. Lecture
  Notes in Networks and Systems, vol 710. Springer, Cham
- **Summary**: The lack of sufficiently large open medical databases is one of the biggest challenges in AI-powered healthcare. Synthetic data created using Generative Adversarial Networks (GANs) appears to be a good solution to mitigate the issues with privacy policies. The other type of cure is decentralized protocol across multiple medical institutions without exchanging local data samples. In this paper, we explored unconditional and conditional GANs in centralized and decentralized settings. The centralized setting imitates studies on large but highly unbalanced skin lesion dataset, while the decentralized one simulates a more realistic hospital scenario with three institutions. We evaluated models' performance in terms of fidelity, diversity, speed of training, and predictive ability of classifiers trained on the generated synthetic data. In addition we provided explainability through exploration of latent space and embeddings projection focused both on global and local explanations. Calculated distance between real images and their projections in the latent space proved the authenticity and generalization of trained GANs, which is one of the main concerns in this type of applications. The open source code for conducted studies is publicly available at \url{https://github.com/aidotse/stylegan2-ada-pytorch}.



### Detecting the unknown in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.11641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11641v1)
- **Published**: 2022-08-24 16:27:38+00:00
- **Updated**: 2022-08-24 16:27:38+00:00
- **Authors**: Dario Fontanel, Matteo Tarantino, Fabio Cermelli, Barbara Caputo
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection methods have witnessed impressive improvements in the last years thanks to the design of novel neural network architectures and the availability of large scale datasets. However, current methods have a significant limitation: they are able to detect only the classes observed during training time, that are only a subset of all the classes that a detector may encounter in the real world. Furthermore, the presence of unknown classes is often not considered at training time, resulting in methods not even able to detect that an unknown object is present in the image. In this work, we address the problem of detecting unknown objects, known as open-set object detection. We propose a novel training strategy, called UNKAD, able to predict unknown objects without requiring any annotation of them, exploiting non annotated objects that are already present in the background of training images. In particular, exploiting the four-steps training strategy of Faster R-CNN, UNKAD first identifies and pseudo-labels unknown objects and then uses the pseudo-annotations to train an additional unknown class. While UNKAD can directly detect unknown objects, we further combine it with previous unknown detection techniques, showing that it improves their performance at no costs.



### Lane Change Classification and Prediction with Action Recognition Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.11650v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11650v2)
- **Published**: 2022-08-24 16:40:27+00:00
- **Updated**: 2022-09-12 18:55:49+00:00
- **Authors**: Kai Liang, Jun Wang, Abhir Bhalerao
- **Comment**: Accepted by ECCV
- **Journal**: None
- **Summary**: Anticipating lane change intentions of surrounding vehicles is crucial for efficient and safe driving decision making in an autonomous driving system. Previous works often adopt physical variables such as driving speed, acceleration and so forth for lane change classification. However, physical variables do not contain semantic information. Although 3D CNNs have been developing rapidly, the number of methods utilising action recognition models and appearance feature for lane change recognition is low, and they all require additional information to pre-process data. In this work, we propose an end-to-end framework including two action recognition methods for lane change recognition, using video data collected by cameras. Our method achieves the best lane change classification results using only the RGB video data of the PREVENTION dataset. Class activation maps demonstrate that action recognition models can efficiently extract lane change motions. A method to better extract motion clues is also proposed in this paper.



### AGO-Net: Association-Guided 3D Point Cloud Object Detection Network
- **Arxiv ID**: http://arxiv.org/abs/2208.11658v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.11658v1)
- **Published**: 2022-08-24 16:54:38+00:00
- **Updated**: 2022-08-24 16:54:38+00:00
- **Authors**: Liang Du, Xiaoqing Ye, Xiao Tan, Edward Johns, Bo Chen, Errui Ding, Xiangyang Xue, Jianfeng Feng
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: The human brain can effortlessly recognize and localize objects, whereas current 3D object detection methods based on LiDAR point clouds still report inferior performance for detecting occluded and distant objects: the point cloud appearance varies greatly due to occlusion, and has inherent variance in point densities along the distance to sensors. Therefore, designing feature representations robust to such point clouds is critical. Inspired by human associative recognition, we propose a novel 3D detection framework that associates intact features for objects via domain adaptation. We bridge the gap between the perceptual domain, where features are derived from real scenes with sub-optimal representations, and the conceptual domain, where features are extracted from augmented scenes that consist of non-occlusion objects with rich detailed information. A feasible method is investigated to construct conceptual scenes without external datasets. We further introduce an attention-based re-weighting module that adaptively strengthens the feature adaptation of more informative regions. The network's feature enhancement ability is exploited without introducing extra cost during inference, which is plug-and-play in various 3D detection frameworks. We achieve new state-of-the-art performance on the KITTI 3D detection benchmark in both accuracy and speed. Experiments on nuScenes and Waymo datasets also validate the versatility of our method.



### Cross-Camera View-Overlap Recognition
- **Arxiv ID**: http://arxiv.org/abs/2208.11661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11661v1)
- **Published**: 2022-08-24 16:55:52+00:00
- **Updated**: 2022-08-24 16:55:52+00:00
- **Authors**: Alessio Xompero, Andrea Cavallaro
- **Comment**: 17 pages, 5 figures, 2 tables. Accepted to International Workshop on
  Distributed Smart Cameras (IWDSC) at the 2022 European Conference on Computer
  Vision (ECCV2022)
- **Journal**: None
- **Summary**: We propose a decentralised view-overlap recognition framework that operates across freely moving cameras without the need of a reference 3D map. Each camera independently extracts, aggregates into a hierarchical structure, and shares feature-point descriptors over time. A view overlap is recognised by view-matching and geometric validation to discard wrongly matched views. The proposed framework is generic and can be used with different descriptors. We conduct the experiments on publicly available sequences as well as new sequences we collected with hand-held cameras. We show that Oriented FAST and Rotated BRIEF (ORB) features with Bags of Binary Words within the proposed framework lead to higher precision and a higher or similar accuracy compared to NetVLAD, RootSIFT, and SuperGlue.



### Efficient Heterogeneous Video Segmentation at the Edge
- **Arxiv ID**: http://arxiv.org/abs/2208.11666v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11666v1)
- **Published**: 2022-08-24 17:01:09+00:00
- **Updated**: 2022-08-24 17:01:09+00:00
- **Authors**: Jamie Menjay Lin, Siargey Pisarchyk, Juhyun Lee, David Tian, Tingbo Hou, Karthik Raveendran, Raman Sarokin, George Sung, Trent Tolley, Matthias Grundmann
- **Comment**: Published as a workshop paper at CVPRW CV4ARVR 2022
- **Journal**: None
- **Summary**: We introduce an efficient video segmentation system for resource-limited edge devices leveraging heterogeneous compute. Specifically, we design network models by searching across multiple dimensions of specifications for the neural architectures and operations on top of already light-weight backbones, targeting commercially available edge inference engines. We further analyze and optimize the heterogeneous data flows in our systems across the CPU, the GPU and the NPU. Our approach has empirically factored well into our real-time AR system, enabling remarkably higher accuracy with quadrupled effective resolutions, yet at much shorter end-to-end latency, much higher frame rate, and even lower power consumption on edge platforms.



### Learned Lossless JPEG Transcoding via Joint Lossy and Residual Compression
- **Arxiv ID**: http://arxiv.org/abs/2208.11673v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.11673v1)
- **Published**: 2022-08-24 17:12:00+00:00
- **Updated**: 2022-08-24 17:12:00+00:00
- **Authors**: Xiaoshuai Fan, Xin Li, Zhibo Chen
- **Comment**: Accepted by VCIP2022
- **Journal**: None
- **Summary**: As a commonly-used image compression format, JPEG has been broadly applied in the transmission and storage of images. To further reduce the compression cost while maintaining the quality of JPEG images, lossless transcoding technology has been proposed to recompress the compressed JPEG image in the DCT domain. Previous works, on the other hand, typically reduce the redundancy of DCT coefficients and optimize the probability prediction of entropy coding in a hand-crafted manner that lacks generalization ability and flexibility. To tackle the above challenge, we propose the learned lossless JPEG transcoding framework via Joint Lossy and Residual Compression. Instead of directly optimizing the entropy estimation, we focus on the redundancy that exists in the DCT coefficients. To the best of our knowledge, we are the first to utilize the learned end-to-end lossy transform coding to reduce the redundancy of DCT coefficients in a compact representational domain. We also introduce residual compression for lossless transcoding, which adaptively learns the distribution of residual DCT coefficients before compressing them using context-based entropy coding. Our proposed transcoding architecture shows significant superiority in the compression of JPEG images thanks to the collaboration of learned lossy transform coding and residual entropy coding. Extensive experiments on multiple datasets have demonstrated that our proposed framework can achieve about 21.49% bits saving in average based on JPEG compression, which outperforms the typical lossless transcoding framework JPEG-XL by 3.51%.



### TMIC: App Inventor Extension for the Deployment of Image Classification Models Exported from Teachable Machine
- **Arxiv ID**: http://arxiv.org/abs/2208.12637v2
- **DOI**: None
- **Categories**: **cs.CY**, cs.AI, cs.CV, cs.LG, I.2; K.3
- **Links**: [PDF](http://arxiv.org/pdf/2208.12637v2)
- **Published**: 2022-08-24 17:34:47+00:00
- **Updated**: 2022-08-30 22:08:48+00:00
- **Authors**: Fabiano Pereira de Oliveira, Christiane Gresse von Wangenheim, Jean C. R. Hauck
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: TMIC is an App Inventor extension for the deployment of ML models for image classification developed with Google Teachable Machine in educational settings. Google Teachable Machine, is an intuitive visual tool that provides workflow-oriented support for the development of ML models for image classification. Aiming at the usage of models developed with Google Teachable Machine, the extension TMIC enables the deployment of the trained models exported as TensorFlow.js to Google Cloud as part of App Inventor, one of the most popular block-based programming environments for teaching computing in K-12. The extension was created with the App Inventor extension framework based on the extension PIC and is available under the BSD 3 license. It can be used for teaching ML in K-12, in introductory courses in higher education or by anyone interested in creating intelligent apps with image classification. The extension TMIC is being developed by the initiative Computa\c{c}\~ao na Escola of the Department of Informatics and Statistics at the Federal University of Santa Catarina/Brazil as part of a research effort aiming at introducing AI education in K-12.



### ForestEyes Project: Conception, Enhancements, and Challenges
- **Arxiv ID**: http://arxiv.org/abs/2208.11687v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.11687v1)
- **Published**: 2022-08-24 17:48:12+00:00
- **Updated**: 2022-08-24 17:48:12+00:00
- **Authors**: Fernanda B. J. R. Dallaqua, Álvaro Luiz Fazenda, Fabio A. Faria
- **Comment**: 30 pages, Published at Elsevier Future Generation Computer System
  (FGCS), Volume 124, November 2021, Pages 422-435
- **Journal**: None
- **Summary**: Rainforests play an important role in the global ecosystem. However, significant regions of them are facing deforestation and degradation due to several reasons. Diverse government and private initiatives were created to monitor and alert for deforestation increases from remote sensing images, using different ways to deal with the notable amount of generated data. Citizen Science projects can also be used to reach the same goal. Citizen Science consists of scientific research involving nonprofessional volunteers for analyzing, collecting data, and using their computational resources to outcome advancements in science and to increase the public's understanding of problems in specific knowledge areas such as astronomy, chemistry, mathematics, and physics. In this sense, this work presents a Citizen Science project called ForestEyes, which uses volunteer's answers through the analysis and classification of remote sensing images to monitor deforestation regions in rainforests. To evaluate the quality of those answers, different campaigns/workflows were launched using remote sensing images from Brazilian Legal Amazon and their results were compared to an official groundtruth from the Amazon Deforestation Monitoring Project PRODES. In this work, the first two workflows that enclose the State of Rond\^onia in the years 2013 and 2016 received more than $35,000$ answers from $383$ volunteers in the $2,050$ created tasks in only two and a half weeks after their launch. For the other four workflows, even enclosing the same area (Rond\^onia) and different setups (e.g., image segmentation method, image resolution, and detection target), they received $51,035$ volunteers' answers gathered from $281$ volunteers in $3,358$ tasks. In the performed experiments...



### Bugs in the Data: How ImageNet Misrepresents Biodiversity
- **Arxiv ID**: http://arxiv.org/abs/2208.11695v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11695v1)
- **Published**: 2022-08-24 17:55:48+00:00
- **Updated**: 2022-08-24 17:55:48+00:00
- **Authors**: Alexandra Sasha Luccioni, David Rolnick
- **Comment**: None
- **Journal**: None
- **Summary**: ImageNet-1k is a dataset often used for benchmarking machine learning (ML) models and evaluating tasks such as image recognition and object detection. Wild animals make up 27% of ImageNet-1k but, unlike classes representing people and objects, these data have not been closely scrutinized. In the current paper, we analyze the 13,450 images from 269 classes that represent wild animals in the ImageNet-1k validation set, with the participation of expert ecologists. We find that many of the classes are ill-defined or overlapping, and that 12% of the images are incorrectly labeled, with some classes having >90% of images incorrect. We also find that both the wildlife-related labels and images included in ImageNet-1k present significant geographical and cultural biases, as well as ambiguities such as artificial animals, multiple species in the same image, or the presence of humans. Our findings highlight serious issues with the extensive use of this dataset for evaluating ML systems, the use of such algorithms in wildlife-related tasks, and more broadly the ways in which ML datasets are commonly created and curated.



### gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window
- **Arxiv ID**: http://arxiv.org/abs/2208.11718v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.11718v1)
- **Published**: 2022-08-24 18:00:46+00:00
- **Updated**: 2022-08-24 18:00:46+00:00
- **Authors**: Mocho Go, Hideyuki Tachibana
- **Comment**: 13 pages, 6 figures
- **Journal**: None
- **Summary**: Following the success in language domain, the self-attention mechanism (transformer) is adopted in the vision domain and achieving great success recently. Additionally, as another stream, multi-layer perceptron (MLP) is also explored in the vision domain. These architectures, other than traditional CNNs, have been attracting attention recently, and many methods have been proposed. As one that combines parameter efficiency and performance with locality and hierarchy in image recognition, we propose gSwin, which merges the two streams; Swin Transformer and (multi-head) gMLP. We showed that our gSwin can achieve better accuracy on three vision tasks, image classification, object detection and semantic segmentation, than Swin Transformer, with smaller model size.



### Full Body Video-Based Self-Avatars for Mixed Reality: from E2E System to User Study
- **Arxiv ID**: http://arxiv.org/abs/2208.12639v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.12639v1)
- **Published**: 2022-08-24 20:59:17+00:00
- **Updated**: 2022-08-24 20:59:17+00:00
- **Authors**: Diego Gonzalez Morin, Ester Gonzalez-Sosa, Pablo Perez, Alvaro Villegas
- **Comment**: Diego Gonzalez-Morin and Ester Gonzalez-Sosa contribute equally
- **Journal**: None
- **Summary**: In this work we explore the creation of self-avatars through video pass-through in Mixed Reality (MR) applications. We present our end-to-end system, including: custom MR video pass-through implementation on a commercial head mounted display (HMD), our deep learning-based real-time egocentric body segmentation algorithm, and our optimized offloading architecture, to communicate the segmentation server with the HMD. To validate this technology, we designed an immersive VR experience where the user has to walk through a narrow tiles path over an active volcano crater. The study was performed under three body representation conditions: virtual hands, video pass-through with color-based full-body segmentation and video pass-through with deep learning full-body segmentation. This immersive experience was carried out by 30 women and 28 men. To the best of our knowledge, this is the first user study focused on evaluating video-based self-avatars to represent the user in a MR scene. Results showed no significant differences between the different body representations in terms of presence, with moderate improvements in some Embodiment components between the virtual hands and full-body representations. Visual Quality results showed better results from the deep-learning algorithms in terms of the whole body perception and overall segmentation quality. We provide some discussion regarding the use of video-based self-avatars, and some reflections on the evaluation methodology. The proposed E2E solution is in the boundary of the state of the art, so there is still room for improvement before it reaches maturity. However, this solution serves as a crucial starting point for novel MR distributed solutions.



### Comprehensive Dataset of Face Manipulations for Development and Evaluation of Forensic Tools
- **Arxiv ID**: http://arxiv.org/abs/2208.11776v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.11776v2)
- **Published**: 2022-08-24 21:17:28+00:00
- **Updated**: 2022-10-25 18:47:57+00:00
- **Authors**: Brian DeCann, Kirill Trapeznikov
- **Comment**: Includes distribution statement
- **Journal**: None
- **Summary**: Digital media (e.g., photographs, video) can be easily created, edited, and shared. Tools for editing digital media are capable of doing so while also maintaining a high degree of photo-realism. While many types of edits to digital media are generally benign, others can also be applied for malicious purposes. State-of-the-art face editing tools and software can, for example, artificially make a person appear to be smiling at an inopportune time, or depict authority figures as frail and tired in order to discredit individuals. Given the increasing ease of editing digital media and the potential risks from misuse, a substantial amount of effort has gone into media forensics. To this end, we created a challenge dataset of edited facial images to assist the research community in developing novel approaches to address and classify the authenticity of digital media. Our dataset includes edits applied to controlled, portrait-style frontal face images and full-scene in-the-wild images that may include multiple (i.e., more than one) face per image. The goals of our dataset is to address the following challenge questions: (1) Can we determine the authenticity of a given image (edit detection)? (2) If an image has been edited, can we \textit{localize} the edit region? (3) If an image has been edited, can we deduce (classify) what edit type was performed? The majority of research in image forensics generally attempts to answer item (1), detection. To the best of our knowledge, there are no formal datasets specifically curated to evaluate items (2) and (3), localization and classification, respectively. Our hope is that our prepared evaluation protocol will assist researchers in improving the state-of-the-art in image forensics as they pertain to these challenges.



### Learning from Unlabeled 3D Environments for Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/2208.11781v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.11781v1)
- **Published**: 2022-08-24 21:50:20+00:00
- **Updated**: 2022-08-24 21:50:20+00:00
- **Authors**: Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, Ivan Laptev
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: In vision-and-language navigation (VLN), an embodied agent is required to navigate in realistic 3D environments following natural language instructions. One major bottleneck for existing VLN approaches is the lack of sufficient training data, resulting in unsatisfactory generalization to unseen environments. While VLN data is typically collected manually, such an approach is expensive and prevents scalability. In this work, we address the data scarcity issue by proposing to automatically create a large-scale VLN dataset from 900 unlabeled 3D buildings from HM3D. We generate a navigation graph for each building and transfer object predictions from 2D to generate pseudo 3D object labels by cross-view consistency. We then fine-tune a pretrained language model using pseudo object labels as prompts to alleviate the cross-modal gap in instruction generation. Our resulting HM3D-AutoVLN dataset is an order of magnitude larger than existing VLN datasets in terms of navigation environments and instructions. We experimentally demonstrate that HM3D-AutoVLN significantly increases the generalization ability of resulting VLN models. On the SPL metric, our approach improves over state of the art by 7.1% and 8.1% on the unseen validation splits of REVERIE and SOON datasets respectively.



