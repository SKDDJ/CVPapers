# Arxiv Papers in cs.CV on 2022-08-14
### Predicting skull fractures via CNN with classification algorithms
- **Arxiv ID**: http://arxiv.org/abs/2208.06756v1
- **DOI**: 10.1145/3542954.3543017
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.06756v1)
- **Published**: 2022-08-14 01:37:23+00:00
- **Updated**: 2022-08-14 01:37:23+00:00
- **Authors**: Md Moniruzzaman Emon, Tareque Rahman Ornob, Moqsadur Rahman
- **Comment**: None
- **Journal**: ICCA 2022: 2nd International Conference on Computing Advancements
- **Summary**: Computer Tomography (CT) images have become quite important to diagnose diseases. CT scan slice contains a vast amount of data that may not be properly examined with the requisite precision and speed using normal visual inspection. A computer-assisted skull fracture classification expert system is needed to assist physicians. Convolutional Neural Networks (CNNs) are the most extensively used deep learning models for image categorization since most often time they outperform other models in terms of accuracy and results. The CNN models were then developed and tested, and several convolutional neural network (CNN) architectures were compared. ResNet50, which was used for feature extraction combined with a gradient boosted decision tree machine learning algorithm to act as a classifier for the categorization of skull fractures from brain CT scans into three fracture categories, had the best overall F1-score of 96%, Hamming Score of 95%, Balanced accuracy Score of 94% & ROC AUC curve of 96% for the classification of skull fractures.



### MAFNet: A Multi-Attention Fusion Network for RGB-T Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/2208.06761v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06761v1)
- **Published**: 2022-08-14 02:42:09+00:00
- **Updated**: 2022-08-14 02:42:09+00:00
- **Authors**: Pengyu Chen, Junyu Gao, Yuan Yuan, Qi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: RGB-Thermal (RGB-T) crowd counting is a challenging task, which uses thermal images as complementary information to RGB images to deal with the decreased performance of unimodal RGB-based methods in scenes with low-illumination or similar backgrounds. Most existing methods propose well-designed structures for cross-modal fusion in RGB-T crowd counting. However, these methods have difficulty in encoding cross-modal contextual semantic information in RGB-T image pairs. Considering the aforementioned problem, we propose a two-stream RGB-T crowd counting network called Multi-Attention Fusion Network (MAFNet), which aims to fully capture long-range contextual information from the RGB and thermal modalities based on the attention mechanism. Specifically, in the encoder part, a Multi-Attention Fusion (MAF) module is embedded into different stages of the two modality-specific branches for cross-modal fusion at the global level. In addition, a Multi-modal Multi-scale Aggregation (MMA) regression head is introduced to make full use of the multi-scale and contextual information across modalities to generate high-quality crowd density maps. Extensive experiments on two popular datasets show that the proposed MAFNet is effective for RGB-T crowd counting and achieves the state-of-the-art performance.



### Flow-Guided Transformer for Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/2208.06768v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06768v1)
- **Published**: 2022-08-14 03:10:01+00:00
- **Updated**: 2022-08-14 03:10:01+00:00
- **Authors**: Kaidong Zhang, Jingjing Fu, Dong Liu
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: We propose a flow-guided transformer, which innovatively leverage the motion discrepancy exposed by optical flows to instruct the attention retrieval in transformer for high fidelity video inpainting. More specially, we design a novel flow completion network to complete the corrupted flows by exploiting the relevant flow features in a local temporal window. With the completed flows, we propagate the content across video frames, and adopt the flow-guided transformer to synthesize the rest corrupted regions. We decouple transformers along temporal and spatial dimension, so that we can easily integrate the locally relevant completed flows to instruct spatial attention only. Furthermore, we design a flow-reweight module to precisely control the impact of completed flows on each spatial transformer. For the sake of efficiency, we introduce window partition strategy to both spatial and temporal transformers. Especially in spatial transformer, we design a dual perspective spatial MHSA, which integrates the global tokens to the window-based attention. Extensive experiments demonstrate the effectiveness of the proposed method qualitatively and quantitatively. Codes are available at https://github.com/hitachinsk/FGT.



### TL;DW? Summarizing Instructional Videos with Task Relevance & Cross-Modal Saliency
- **Arxiv ID**: http://arxiv.org/abs/2208.06773v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.06773v1)
- **Published**: 2022-08-14 04:07:40+00:00
- **Updated**: 2022-08-14 04:07:40+00:00
- **Authors**: Medhini Narasimhan, Arsha Nagrani, Chen Sun, Michael Rubinstein, Trevor Darrell, Anna Rohrbach, Cordelia Schmid
- **Comment**: Accepted to ECCV 2022. Website: https://medhini.github.io/ivsum/
- **Journal**: None
- **Summary**: YouTube users looking for instructions for a specific task may spend a long time browsing content trying to find the right video that matches their needs. Creating a visual summary (abridged version of a video) provides viewers with a quick overview and massively reduces search time. In this work, we focus on summarizing instructional videos, an under-explored area of video summarization. In comparison to generic videos, instructional videos can be parsed into semantically meaningful segments that correspond to important steps of the demonstrated task. Existing video summarization datasets rely on manual frame-level annotations, making them subjective and limited in size. To overcome this, we first automatically generate pseudo summaries for a corpus of instructional videos by exploiting two key assumptions: (i) relevant steps are likely to appear in multiple videos of the same task (Task Relevance), and (ii) they are more likely to be described by the demonstrator verbally (Cross-Modal Saliency). We propose an instructional video summarization network that combines a context-aware temporal video encoder and a segment scoring transformer. Using pseudo summaries as weak supervision, our network constructs a visual summary for an instructional video given only video and transcribed speech. To evaluate our model, we collect a high-quality test set, WikiHow Summaries, by scraping WikiHow articles that contain video demonstrations and visual depictions of steps allowing us to obtain the ground-truth summaries. We outperform several baselines and a state-of-the-art video summarization model on this new benchmark.



### HDR-Plenoxels: Self-Calibrating High Dynamic Range Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2208.06787v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2208.06787v2)
- **Published**: 2022-08-14 06:12:22+00:00
- **Updated**: 2022-11-18 13:32:35+00:00
- **Authors**: Kim Jun-Seong, Kim Yu-Ji, Moon Ye-Bin, Tae-Hyun Oh
- **Comment**: Accepted at ECCV 2022. [Project page] https://hdr-plenoxels.github.io
  [Code] https://github.com/postech-ami/HDR-Plenoxels
- **Journal**: None
- **Summary**: We propose high dynamic range (HDR) radiance fields, HDR-Plenoxels, that learn a plenoptic function of 3D HDR radiance fields, geometry information, and varying camera settings inherent in 2D low dynamic range (LDR) images. Our voxel-based volume rendering pipeline reconstructs HDR radiance fields with only multi-view LDR images taken from varying camera settings in an end-to-end manner and has a fast convergence speed. To deal with various cameras in real-world scenarios, we introduce a tone mapping module that models the digital in-camera imaging pipeline (ISP) and disentangles radiometric settings. Our tone mapping module allows us to render by controlling the radiometric settings of each novel view. Finally, we build a multi-view dataset with varying camera conditions, which fits our problem setting. Our experiments show that HDR-Plenoxels can express detail and high-quality HDR novel views from only LDR images with various cameras.



### Light Weight Character and Shape Recognition for Autonomous Drones
- **Arxiv ID**: http://arxiv.org/abs/2208.06804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06804v1)
- **Published**: 2022-08-14 08:22:41+00:00
- **Updated**: 2022-08-14 08:22:41+00:00
- **Authors**: Neetigya Poddar, Shruti Jain
- **Comment**: None
- **Journal**: None
- **Summary**: There has been an extensive use of Unmanned Aerial Vehicles in search and rescue missions to distribute first aid kits and food packets. It is important that these UAVs are able to identify and distinguish the markers from one another for effective distribution. One of the common ways to mark the locations is via the use of characters superimposed on shapes of various colors which gives rise to wide variety of markers based on combination of different shapes, characters, and their respective colors.   In this paper, we propose an object detection and classification pipeline which prevents false positives and minimizes misclassification of alphanumeric characters and shapes in aerial images. Our method makes use of traditional computer vision techniques and unsupervised machine learning methods for identifying region proposals, segmenting the image targets and removing false positives. We make use of a computationally light model for classification, making it easy to be deployed on any aerial vehicle.



### Semi-Supervised Video Inpainting with Cycle Consistency Constraints
- **Arxiv ID**: http://arxiv.org/abs/2208.06807v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06807v2)
- **Published**: 2022-08-14 08:46:37+00:00
- **Updated**: 2022-11-17 10:54:47+00:00
- **Authors**: Zhiliang Wu, Hanyu Xuan, Changchang Sun, Kang Zhang, Yan Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based video inpainting has yielded promising results and gained increasing attention from researchers. Generally, these methods usually assume that the corrupted region masks of each frame are known and easily obtained. However, the annotation of these masks are labor-intensive and expensive, which limits the practical application of current methods. Therefore, we expect to relax this assumption by defining a new semi-supervised inpainting setting, making the networks have the ability of completing the corrupted regions of the whole video using the annotated mask of only one frame. Specifically, in this work, we propose an end-to-end trainable framework consisting of completion network and mask prediction network, which are designed to generate corrupted contents of the current frame using the known mask and decide the regions to be filled of the next frame, respectively. Besides, we introduce a cycle consistency loss to regularize the training parameters of these two networks. In this way, the completion network and the mask prediction network can constrain each other, and hence the overall performance of the trained model can be maximized. Furthermore, due to the natural existence of prior knowledge (e.g., corrupted contents and clear borders), current video inpainting datasets are not suitable in the context of semi-supervised video inpainting. Thus, we create a new dataset by simulating the corrupted video of real-world scenarios. Extensive experimental results are reported to demonstrate the superiority of our model in the video inpainting task. Remarkably, although our model is trained in a semi-supervised manner, it can achieve comparable performance as fully-supervised methods.



### Multi-Attribute Open Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/2208.06809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06809v1)
- **Published**: 2022-08-14 09:04:52+00:00
- **Updated**: 2022-08-14 09:04:52+00:00
- **Authors**: Piyapat Saranrittichai, Chaithanya Kumar Mummadi, Claudia Blaiotta, Mauricio Munoz, Volker Fischer
- **Comment**: Accepted for publication at German Conference for Pattern Recognition
  (GCPR) 2022
- **Journal**: None
- **Summary**: Open Set Recognition (OSR) extends image classification to an open-world setting, by simultaneously classifying known classes and identifying unknown ones. While conventional OSR approaches can detect Out-of-Distribution (OOD) samples, they cannot provide explanations indicating which underlying visual attribute(s) (e.g., shape, color or background) cause a specific sample to be unknown. In this work, we introduce a novel problem setup that generalizes conventional OSR to a multi-attribute setting, where multiple visual attributes are simultaneously recognized. Here, OOD samples can be not only identified but also categorized by their unknown attribute(s). We propose simple extensions of common OSR baselines to handle this novel scenario. We show that these baselines are vulnerable to shortcuts when spurious correlations exist in the training dataset. This leads to poor OOD performance which, according to our experiments, is mainly due to unintended cross-attribute correlations of the predicted confidence scores. We provide an empirical evidence showing that this behavior is consistent across different baselines on both synthetic and real world datasets.



### Contrastive Learning for Joint Normal Estimation and Point Cloud Filtering
- **Arxiv ID**: http://arxiv.org/abs/2208.06811v2
- **DOI**: 10.1109/TVCG.2023.3263866
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06811v2)
- **Published**: 2022-08-14 09:16:25+00:00
- **Updated**: 2023-05-03 01:08:53+00:00
- **Authors**: Dasith de Silva Edirimuni, Xuequan Lu, Gang Li, Antonio Robles-Kelly
- **Comment**: This paper has been accepted to the IEEE TVCG journal. Our source
  code is available at https://github.com/ddsediri/CLJNEPCF
- **Journal**: None
- **Summary**: Point cloud filtering and normal estimation are two fundamental research problems in the 3D field. Existing methods usually perform normal estimation and filtering separately and often show sensitivity to noise and/or inability to preserve sharp geometric features such as corners and edges. In this paper, we propose a novel deep learning method to jointly estimate normals and filter point clouds. We first introduce a 3D patch based contrastive learning framework, with noise corruption as an augmentation, to train a feature encoder capable of generating faithful representations of point cloud patches while remaining robust to noise. These representations are consumed by a simple regression network and supervised by a novel joint loss, simultaneously estimating point normals and displacements that are used to filter the patch centers. Experimental results show that our method well supports the two tasks simultaneously and preserves sharp features and fine details. It generally outperforms state-of-the-art techniques on both tasks. Our source code is available at https://github.com/ddsediri/CLJNEPCF.



### Remote Photoplethysmography from Low Resolution videos: An end-to-end solution using Efficient ConvNets
- **Arxiv ID**: http://arxiv.org/abs/2208.06817v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.06817v1)
- **Published**: 2022-08-14 10:04:25+00:00
- **Updated**: 2022-08-14 10:04:25+00:00
- **Authors**: Bharath Ramakrishnan, Ruijia Deng
- **Comment**: arXiv admin note: text overlap with arXiv:2208.04947
- **Journal**: None
- **Summary**: Measurement of the cardiac pulse from facial video has become an interesting pursuit of research over the last few years. This is mainly due to the increasing importance of obtaining the heart rate of an individual in a non-invasive manner, which can be highly useful for applications in gaming and the medical industry. Another instrumental area of research over the past few years has been the advent of Deep Learning and using Deep Neural networks to enhance task performance. In this work, we propose to use efficient convolutional networks to accurately measure the heart rate of user from low resolution facial videos. Furthermore, to ensure that we are able to obtain the heart rate in real time, we compress the deep learning model by pruning it, thereby reducing its memory footprint. We benchmark the performance of our approach on the MAHNOB dataset and compare its performance across multiple approaches.



### HighlightNet: Highlighting Low-Light Potential Features for Real-Time UAV Tracking
- **Arxiv ID**: http://arxiv.org/abs/2208.06818v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.06818v1)
- **Published**: 2022-08-14 10:09:35+00:00
- **Updated**: 2022-08-14 10:09:35+00:00
- **Authors**: Changhong Fu, Haolin Dong, Junjie Ye, Guangze Zheng, Sihang Li, Jilin Zhao
- **Comment**: accepted by IROS2022
- **Journal**: None
- **Summary**: Low-light environments have posed a formidable challenge for robust unmanned aerial vehicle (UAV) tracking even with state-of-the-art (SOTA) trackers since the potential image features are hard to extract under adverse light conditions. Besides, due to the low visibility, accurate online selection of the object also becomes extremely difficult for human monitors to initialize UAV tracking in ground control stations. To solve these problems, this work proposes a novel enhancer, i.e., HighlightNet, to light up potential objects for both human operators and UAV trackers. By employing Transformer, HighlightNet can adjust enhancement parameters according to global features and is thus adaptive for the illumination variation. Pixel-level range mask is introduced to make HighlightNet more focused on the enhancement of the tracking object and regions without light sources. Furthermore, a soft truncation mechanism is built to prevent background noise from being mistaken for crucial features. Evaluations on image enhancement benchmarks demonstrate HighlightNet has advantages in facilitating human perception. Experiments on the public UAVDark135 benchmark show that HightlightNet is more suitable for UAV tracking tasks than other SOTA low-light enhancers. In addition, real-world tests on a typical UAV platform verify HightlightNet's practicability and efficiency in nighttime aerial tracking-related applications. The code and demo videos are available at https://github.com/vision4robotics/HighlightNet.



### Surrogate-assisted Multi-objective Neural Architecture Search for Real-time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.06820v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2208.06820v1)
- **Published**: 2022-08-14 10:18:51+00:00
- **Updated**: 2022-08-14 10:18:51+00:00
- **Authors**: Zhichao Lu, Ran Cheng, Shihua Huang, Haoming Zhang, Changxiao Qiu, Fan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: The architectural advancements in deep neural networks have led to remarkable leap-forwards across a broad array of computer vision tasks. Instead of relying on human expertise, neural architecture search (NAS) has emerged as a promising avenue toward automating the design of architectures. While recent achievements in image classification have suggested opportunities, the promises of NAS have yet to be thoroughly assessed on more challenging tasks of semantic segmentation. The main challenges of applying NAS to semantic segmentation arise from two aspects: (i) high-resolution images to be processed; (ii) additional requirement of real-time inference speed (i.e., real-time semantic segmentation) for applications such as autonomous driving. To meet such challenges, we propose a surrogate-assisted multi-objective method in this paper. Through a series of customized prediction models, our method effectively transforms the original NAS task into an ordinary multi-objective optimization problem. Followed by a hierarchical pre-screening criterion for in-fill selection, our method progressively achieves a set of efficient architectures trading-off between segmentation accuracy and inference speed. Empirical evaluations on three benchmark datasets together with an application using Huawei Atlas 200 DK suggest that our method can identify architectures significantly outperforming existing state-of-the-art architectures designed both manually by human experts and automatically by other NAS methods.



### Fast Learning Radiance Fields by Shooting Much Fewer Rays
- **Arxiv ID**: http://arxiv.org/abs/2208.06821v2
- **DOI**: 10.1109/TIP.2023.3267049
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06821v2)
- **Published**: 2022-08-14 10:25:53+00:00
- **Updated**: 2023-04-06 06:05:32+00:00
- **Authors**: Wenyuan Zhang, Ruofan Xing, Yunfan Zeng, Yu-Shen Liu, Kanle Shi, Zhizhong Han
- **Comment**: Accepted by lEEE Transactions on lmage Processing 2023. Project Page:
  https://zparquet.github.io/Fast-Learning . Code:
  https://github.com/zParquet/Fast-Learning
- **Journal**: None
- **Summary**: Learning radiance fields has shown remarkable results for novel view synthesis. The learning procedure usually costs lots of time, which motivates the latest methods to speed up the learning procedure by learning without neural networks or using more efficient data structures. However, these specially designed approaches do not work for most of radiance fields based methods. To resolve this issue, we introduce a general strategy to speed up the learning procedure for almost all radiance fields based methods. Our key idea is to reduce the redundancy by shooting much fewer rays in the multi-view volume rendering procedure which is the base for almost all radiance fields based methods. We find that shooting rays at pixels with dramatic color change not only significantly reduces the training burden but also barely affects the accuracy of the learned radiance fields. In addition, we also adaptively subdivide each view into a quadtree according to the average rendering error in each node in the tree, which makes us dynamically shoot more rays in more complex regions with larger rendering error. We evaluate our method with different radiance fields based methods under the widely used benchmarks. Experimental results show that our method achieves comparable accuracy to the state-of-the-art with much faster training.



### BDSL 49: A Comprehensive Dataset of Bangla Sign Language
- **Arxiv ID**: http://arxiv.org/abs/2208.06827v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06827v1)
- **Published**: 2022-08-14 10:54:49+00:00
- **Updated**: 2022-08-14 10:54:49+00:00
- **Authors**: Ayman Hasib, Saqib Sizan Khan, Jannatul Ferdous Eva, Mst. Nipa Khatun, Ashraful Haque, Nishat Shahrin, Rashik Rahman, Hasan Murad, Md. Rajibul Islam, Molla Rashied Hussein
- **Comment**: 16 pages; 6 figures; Submitted to Data in Brief, a multidisciplinary,
  open-access and peer-reviewed journal for reviewing
- **Journal**: None
- **Summary**: Language is a method by which individuals express their thoughts. Each language has its own set of alphabetic and numeric characters. People can communicate with one another through either oral or written communication. However, each language has a sign language counterpart. Individuals who are deaf and/or mute communicate through sign language. The Bangla language also has a sign language, which is called BDSL. The dataset is about Bangla hand sign images. The collection contains 49 individual Bangla alphabet images in sign language. BDSL49 is a dataset that consists of 29,490 images with 49 labels. Images of 14 different adult individuals, each with a distinct background and appearance, have been recorded during data collection. Several strategies have been used to eliminate noise from datasets during preparation. This dataset is available to researchers for free. They can develop automated systems using machine learning, computer vision, and deep learning techniques. In addition, two models were used in this dataset. The first is for detection, while the second is for recognition.



### Shuffle Instances-based Vision Transformer for Pancreatic Cancer ROSE Image Classification
- **Arxiv ID**: http://arxiv.org/abs/2208.06833v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.06833v1)
- **Published**: 2022-08-14 11:37:04+00:00
- **Updated**: 2022-08-14 11:37:04+00:00
- **Authors**: Tianyi Zhang, Youdan Feng, Yunlu Feng, Yu Zhao, Yanli Lei, Nan Ying, Zhiling Yan, Yufang He, Guanglei Zhang
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2206.03080
- **Journal**: None
- **Summary**: The rapid on-site evaluation (ROSE) technique can signifi-cantly accelerate the diagnosis of pancreatic cancer by im-mediately analyzing the fast-stained cytopathological images. Computer-aided diagnosis (CAD) can potentially address the shortage of pathologists in ROSE. However, the cancerous patterns vary significantly between different samples, making the CAD task extremely challenging. Besides, the ROSE images have complicated perturbations regarding color distribution, brightness, and contrast due to different staining qualities and various acquisition device types. To address these challenges, we proposed a shuffle instances-based Vision Transformer (SI-ViT) approach, which can reduce the perturbations and enhance the modeling among the instances. With the regrouped bags of shuffle instances and their bag-level soft labels, the approach utilizes a regression head to make the model focus on the cells rather than various perturbations. Simultaneously, combined with a classification head, the model can effectively identify the general distributive patterns among different instances. The results demonstrate significant improvements in the classification accuracy with more accurate attention regions, indicating that the diverse patterns of ROSE images are effectively extracted, and the complicated perturbations are significantly reduced. It also suggests that the SI-ViT has excellent potential in analyzing cytopathological images. The code and experimental results are available at https://github.com/sagizty/MIL-SI.



### Underwater Ranker: Learn Which Is Better and How to Be Better
- **Arxiv ID**: http://arxiv.org/abs/2208.06857v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06857v2)
- **Published**: 2022-08-14 14:13:13+00:00
- **Updated**: 2022-11-26 16:33:53+00:00
- **Authors**: Chunle Guo, Ruiqi Wu, Xin Jin, Linghao Han, Zhi Chai, Weidong Zhang, Chongyi Li
- **Comment**: 9 pages, 10 figures
- **Journal**: None
- **Summary**: In this paper, we present a ranking-based underwater image quality assessment (UIQA) method, abbreviated as URanker. The URanker is built on the efficient conv-attentional image Transformer. In terms of underwater images, we specially devise (1) the histogram prior that embeds the color distribution of an underwater image as histogram token to attend global degradation and (2) the dynamic cross-scale correspondence to model local degradation. The final prediction depends on the class tokens from different scales, which comprehensively considers multi-scale dependencies. With the margin ranking loss, our URanker can accurately rank the order of underwater images of the same scene enhanced by different underwater image enhancement (UIE) algorithms according to their visual quality. To achieve that, we also contribute a dataset, URankerSet, containing sufficient results enhanced by different UIE algorithms and the corresponding perceptual rankings, to train our URanker. Apart from the good performance of URanker, we found that a simple U-shape UIE network can obtain promising performance when it is coupled with our pre-trained URanker as additional supervision. In addition, we also propose a normalization tail that can significantly improve the performance of UIE networks. Extensive experiments demonstrate the state-of-the-art performance of our method. The key designs of our method are discussed. We will release our dataset and code.



### HyP$^2$ Loss: Beyond Hypersphere Metric Space for Multi-label Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/2208.06866v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.06866v1)
- **Published**: 2022-08-14 15:06:27+00:00
- **Updated**: 2022-08-14 15:06:27+00:00
- **Authors**: Chengyin Xu, Zenghao Chai, Zhengzhuo Xu, Chun Yuan, Yanbo Fan, Jue Wang
- **Comment**: Accepted by ACM International Conference on Multimedia (ACM MM) 2022
- **Journal**: None
- **Summary**: Image retrieval has become an increasingly appealing technique with broad multimedia application prospects, where deep hashing serves as the dominant branch towards low storage and efficient retrieval. In this paper, we carried out in-depth investigations on metric learning in deep hashing for establishing a powerful metric space in multi-label scenarios, where the pair loss suffers high computational overhead and converge difficulty, while the proxy loss is theoretically incapable of expressing the profound label dependencies and exhibits conflicts in the constructed hypersphere space. To address the problems, we propose a novel metric learning framework with Hybrid Proxy-Pair Loss (HyP$^2$ Loss) that constructs an expressive metric space with efficient training complexity w.r.t. the whole dataset. The proposed HyP$^2$ Loss focuses on optimizing the hypersphere space by learnable proxies and excavating data-to-data correlations of irrelevant pairs, which integrates sufficient data correspondence of pair-based methods and high-efficiency of proxy-based methods. Extensive experiments on four standard multi-label benchmarks justify the proposed method outperforms the state-of-the-art, is robust among different hash bits and achieves significant performance gains with a faster, more stable convergence speed. Our code is available at https://github.com/JerryXu0129/HyP2-Loss.



### SketchSampler: Sketch-based 3D Reconstruction via View-dependent Depth Sampling
- **Arxiv ID**: http://arxiv.org/abs/2208.06880v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06880v2)
- **Published**: 2022-08-14 16:37:51+00:00
- **Updated**: 2022-12-26 03:24:19+00:00
- **Authors**: Chenjian Gao, Qian Yu, Lu Sheng, Yi-Zhe Song, Dong Xu
- **Comment**: 16 pages, 7 figures, accepted by ECCV 2022
- **Journal**: None
- **Summary**: Reconstructing a 3D shape based on a single sketch image is challenging due to the large domain gap between a sparse, irregular sketch and a regular, dense 3D shape. Existing works try to employ the global feature extracted from sketch to directly predict the 3D coordinates, but they usually suffer from losing fine details that are not faithful to the input sketch. Through analyzing the 3D-to-2D projection process, we notice that the density map that characterizes the distribution of 2D point clouds (i.e., the probability of points projected at each location of the projection plane) can be used as a proxy to facilitate the reconstruction process. To this end, we first translate a sketch via an image translation network to a more informative 2D representation that can be used to generate a density map. Next, a 3D point cloud is reconstructed via a two-stage probabilistic sampling process: first recovering the 2D points (i.e., the x and y coordinates) by sampling the density map; and then predicting the depth (i.e., the z coordinate) by sampling the depth values at the ray determined by each 2D point. Extensive experiments are conducted, and both quantitative and qualitative results show that our proposed approach significantly outperforms other baseline methods.



### CoShNet: A Hybrid Complex Valued Neural Network using Shearlets
- **Arxiv ID**: http://arxiv.org/abs/2208.06882v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06882v2)
- **Published**: 2022-08-14 16:58:05+00:00
- **Updated**: 2022-10-29 14:16:38+00:00
- **Authors**: Manny Ko, Ujjawal K. Panchal, Héctor Andrade-Loarca, Andres Mendez-Vazquez
- **Comment**: 16 pages, 11 figures
- **Journal**: None
- **Summary**: In a hybrid neural network, the expensive convolutional layers are replaced by a non-trainable fixed transform with a great reduction in parameters. In previous works, good results were obtained by replacing the convolutions with wavelets. However, wavelet based hybrid network inherited wavelet's lack of vanishing moments along curves and its axis-bias. We propose to use Shearlets with its robust support for important image features like edges, ridges and blobs. The resulting network is called Complex Shearlets Network (CoShNet). It was tested on Fashion-MNIST against ResNet-50 and Resnet-18, obtaining 92.2% versus 90.7% and 91.8% respectively. The proposed network has 49.9k parameters versus ResNet-18 with 11.18m and use 52 times fewer FLOPs. Finally, we trained in under 20 epochs versus 200 epochs required by ResNet and do not need any hyperparameter tuning nor regularization.   Code: https://github.com/Ujjawal-K-Panchal/coshnet



### Global Priors Guided Modulation Network for Joint Super-Resolution and Inverse Tone-Mapping
- **Arxiv ID**: http://arxiv.org/abs/2208.06885v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2208.06885v2)
- **Published**: 2022-08-14 17:10:34+00:00
- **Updated**: 2022-11-10 05:12:31+00:00
- **Authors**: Gang He, Shaoyi Long, Li Xu, Chang Wu, Jinjia Zhou, Ming Sun, Xing Wen, Yurong Dai
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Joint super-resolution and inverse tone-mapping (SR-ITM) aims to enhance the visual quality of videos that have quality deficiencies in resolution and dynamic range. This problem arises when using 4K high dynamic range (HDR) TVs to watch a low-resolution standard dynamic range (LR SDR) video. Previous methods that rely on learning local information typically cannot do well in preserving color conformity and long-range structural similarity, resulting in unnatural color transition and texture artifacts. In order to tackle these challenges, we propose a global priors guided modulation network (GPGMNet) for joint SR-ITM. In particular, we design a global priors extraction module (GPEM) to extract color conformity prior and structural similarity prior that are beneficial for ITM and SR tasks, respectively. To further exploit the global priors and preserve spatial information, we devise multiple global priors guided spatial-wise modulation blocks (GSMBs) with a few parameters for intermediate feature modulation, in which the modulation parameters are generated by the shared global priors and the spatial features map from the spatial pyramid convolution block (SPCB). With these elaborate designs, the GPGMNet can achieve higher visual quality with lower computational complexity. Extensive experiments demonstrate that our proposed GPGMNet is superior to the state-of-the-art methods. Specifically, our proposed model exceeds the state-of-the-art by 0.64 dB in PSNR, with 69$\%$ fewer parameters and 3.1$\times$ speedup. The code will be released soon.



### AVisT: A Benchmark for Visual Object Tracking in Adverse Visibility
- **Arxiv ID**: http://arxiv.org/abs/2208.06888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06888v1)
- **Published**: 2022-08-14 17:49:37+00:00
- **Updated**: 2022-08-14 17:49:37+00:00
- **Authors**: Mubashir Noman, Wafa Al Ghallabi, Daniya Najiha, Christoph Mayer, Akshay Dudhane, Martin Danelljan, Hisham Cholakkal, Salman Khan, Luc Van Gool, Fahad Shahbaz Khan
- **Comment**: None
- **Journal**: None
- **Summary**: One of the key factors behind the recent success in visual tracking is the availability of dedicated benchmarks. While being greatly benefiting to the tracking research, existing benchmarks do not pose the same difficulty as before with recent trackers achieving higher performance mainly due to (i) the introduction of more sophisticated transformers-based methods and (ii) the lack of diverse scenarios with adverse visibility such as, severe weather conditions, camouflage and imaging effects.   We introduce AVisT, a dedicated benchmark for visual tracking in diverse scenarios with adverse visibility. AVisT comprises 120 challenging sequences with 80k annotated frames, spanning 18 diverse scenarios broadly grouped into five attributes with 42 object categories. The key contribution of AVisT is diverse and challenging scenarios covering severe weather conditions such as, dense fog, heavy rain and sandstorm; obstruction effects including, fire, sun glare and splashing water; adverse imaging effects such as, low-light; target effects including, small targets and distractor objects along with camouflage. We further benchmark 17 popular and recent trackers on AVisT with detailed analysis of their tracking performance across attributes, demonstrating a big room for improvement in performance. We believe that AVisT can greatly benefit the tracking community by complementing the existing benchmarks, in developing new creative tracking solutions in order to continue pushing the boundaries of the state-of-the-art. Our dataset along with the complete tracking performance evaluation is available at: https://github.com/visionml/pytracking



### The SVD of Convolutional Weights: A CNN Interpretability Framework
- **Arxiv ID**: http://arxiv.org/abs/2208.06894v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, 68T07, 68T01, 05C65
- **Links**: [PDF](http://arxiv.org/pdf/2208.06894v1)
- **Published**: 2022-08-14 18:23:02+00:00
- **Updated**: 2022-08-14 18:23:02+00:00
- **Authors**: Brenda Praggastis, Davis Brown, Carlos Ortiz Marrero, Emilie Purvine, Madelyn Shapiro, Bei Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks used for image classification often use convolutional filters to extract distinguishing features before passing them to a linear classifier. Most interpretability literature focuses on providing semantic meaning to convolutional filters to explain a model's reasoning process and confirm its use of relevant information from the input domain. Fully connected layers can be studied by decomposing their weight matrices using a singular value decomposition, in effect studying the correlations between the rows in each matrix to discover the dynamics of the map. In this work we define a singular value decomposition for the weight tensor of a convolutional layer, which provides an analogous understanding of the correlations between filters, exposing the dynamics of the convolutional map. We validate our definition using recent results in random matrix theory. By applying the decomposition across the linear layers of an image classification network we suggest a framework against which interpretability methods might be applied using hypergraphs to model class separation. Rather than looking to the activations to explain the network, we use the singular vectors with the greatest corresponding singular values for each linear layer to identify those features most important to the network. We illustrate our approach with examples and introduce the DeepDataProfiler library, the analysis tool used for this study.



### MTCSNN: Multi-task Clinical Siamese Neural Network for Diabetic Retinopathy Severity Prediction
- **Arxiv ID**: http://arxiv.org/abs/2208.06917v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06917v2)
- **Published**: 2022-08-14 20:55:13+00:00
- **Updated**: 2022-10-27 20:02:00+00:00
- **Authors**: Chao Feng, Jui Po Hung, Aishan Li, Jieping Yang, Xinyu Zhang
- **Comment**: This paper is not sufficiently exhaustive and lacks some analysis.
  Besides, certain methods of this paper are from the first author's other
  co-first authoring research paper. There exist disputes among authors, thus
  we decide to withdraw this paper currently
- **Journal**: None
- **Summary**: Diabetic Retinopathy (DR) has become one of the leading causes of vision impairment in working-aged people and is a severe problem worldwide. However, most of the works ignored the ordinal information of labels. In this project, we propose a novel design MTCSNN, a Multi-task Clinical Siamese Neural Network for Diabetic Retinopathy severity prediction task. The novelty of this project is to utilize the ordinal information among labels and add a new regression task, which can help the model learn more discriminative feature embedding for fine-grained classification tasks. We perform comprehensive experiments over the RetinaMNIST, comparing MTCSNN with other models like ResNet-18, 34, 50. Our results indicate that MTCSNN outperforms the benchmark models in terms of AUC and accuracy on the test dataset.



### Gradient Mask: Lateral Inhibition Mechanism Improves Performance in Artificial Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/2208.06918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06918v1)
- **Published**: 2022-08-14 20:55:50+00:00
- **Updated**: 2022-08-14 20:55:50+00:00
- **Authors**: Lei Jiang, Yongqing Liu, Shihai Xiao, Yansong Chua
- **Comment**: None
- **Journal**: None
- **Summary**: Lateral inhibitory connections have been observed in the cortex of the biological brain, and has been extensively studied in terms of its role in cognitive functions. However, in the vanilla version of backpropagation in deep learning, all gradients (which can be understood to comprise of both signal and noise gradients) flow through the network during weight updates. This may lead to overfitting. In this work, inspired by biological lateral inhibition, we propose Gradient Mask, which effectively filters out noise gradients in the process of backpropagation. This allows the learned feature information to be more intensively stored in the network while filtering out noisy or unimportant features. Furthermore, we demonstrate analytically how lateral inhibition in artificial neural networks improves the quality of propagated gradients. A new criterion for gradient quality is proposed which can be used as a measure during training of various convolutional neural networks (CNNs). Finally, we conduct several different experiments to study how Gradient Mask improves the performance of the network both quantitatively and qualitatively. Quantitatively, accuracy in the original CNN architecture, accuracy after pruning, and accuracy after adversarial attacks have shown improvements. Qualitatively, the CNN trained using Gradient Mask has developed saliency maps that focus primarily on the object of interest, which is useful for data augmentation and network interpretability.



### Visual Localization via Few-Shot Scene Region Classification
- **Arxiv ID**: http://arxiv.org/abs/2208.06933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06933v1)
- **Published**: 2022-08-14 22:39:02+00:00
- **Updated**: 2022-08-14 22:39:02+00:00
- **Authors**: Siyan Dong, Shuzhe Wang, Yixin Zhuang, Juho Kannala, Marc Pollefeys, Baoquan Chen
- **Comment**: 3DV 2022
- **Journal**: None
- **Summary**: Visual (re)localization addresses the problem of estimating the 6-DoF (Degree of Freedom) camera pose of a query image captured in a known scene, which is a key building block of many computer vision and robotics applications. Recent advances in structure-based localization solve this problem by memorizing the mapping from image pixels to scene coordinates with neural networks to build 2D-3D correspondences for camera pose optimization. However, such memorization requires training by amounts of posed images in each scene, which is heavy and inefficient. On the contrary, few-shot images are usually sufficient to cover the main regions of a scene for a human operator to perform visual localization. In this paper, we propose a scene region classification approach to achieve fast and effective scene memorization with few-shot images. Our insight is leveraging a) pre-learned feature extractor, b) scene region classifier, and c) meta-learning strategy to accelerate training while mitigating overfitting. We evaluate our method on both indoor and outdoor benchmarks. The experiments validate the effectiveness of our method in the few-shot setting, and the training time is significantly reduced to only a few minutes. Code available at: \url{https://github.com/siyandong/SRC}



