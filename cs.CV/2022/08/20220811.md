# Arxiv Papers in cs.CV on 2022-08-11
### Memorizing Complementation Network for Few-Shot Class-Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.05610v1
- **DOI**: 10.1109/TIP.2023.3236160
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.05610v1)
- **Published**: 2022-08-11 02:32:41+00:00
- **Updated**: 2022-08-11 02:32:41+00:00
- **Authors**: Zhong Ji, Zhishen Hou, Xiyao Liu, Yanwei Pang, Xuelong Li
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot Class-Incremental Learning (FSCIL) aims at learning new concepts continually with only a few samples, which is prone to suffer the catastrophic forgetting and overfitting problems. The inaccessibility of old classes and the scarcity of the novel samples make it formidable to realize the trade-off between retaining old knowledge and learning novel concepts. Inspired by that different models memorize different knowledge when learning novel concepts, we propose a Memorizing Complementation Network (MCNet) to ensemble multiple models that complements the different memorized knowledge with each other in novel tasks. Additionally, to update the model with few novel samples, we develop a Prototype Smoothing Hard-mining Triplet (PSHT) loss to push the novel samples away from not only each other in current task but also the old distribution. Extensive experiments on three benchmark datasets, e.g., CIFAR100, miniImageNet and CUB200, have demonstrated the superiority of our proposed method.



### FIGO: Enhanced Fingerprint Identification Approach Using GAN and One Shot Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/2208.05615v2
- **DOI**: 10.1109/ISDFS58141.2023.10131805
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.05615v2)
- **Published**: 2022-08-11 02:45:42+00:00
- **Updated**: 2023-05-29 03:21:23+00:00
- **Authors**: Ibrahim Yilmaz, Mahmoud Abouyoussef
- **Comment**: None
- **Journal**: None
- **Summary**: Fingerprint evidence plays an important role in a criminal investigation for the identification of individuals. Although various techniques have been proposed for fingerprint classification and feature extraction, automated fingerprint identification of fingerprints is still in its earliest stage. The performance of traditional \textit{Automatic Fingerprint Identification System} (AFIS) depends on the presence of valid minutiae points and still requires human expert assistance in feature extraction and identification stages. Based on this motivation, we propose a Fingerprint Identification approach based on Generative adversarial network and One-shot learning techniques (FIGO). Our solution contains two components: fingerprint enhancement tier and fingerprint identification tier. First, we propose a Pix2Pix model to transform low-quality fingerprint images to a higher level of fingerprint images pixel by pixel directly in the fingerprint enhancement tier. With the proposed enhancement algorithm, the fingerprint identification model's performance is significantly improved. Furthermore, we develop another existing solution based on Gabor filters as a benchmark to compare with the proposed model by observing the fingerprint device's recognition accuracy. Experimental results show that our proposed Pix2pix model has better support than the baseline approach for fingerprint identification. Second, we construct a fully automated fingerprint feature extraction model using a one-shot learning approach to differentiate each fingerprint from the others in the fingerprint identification process. Two twin convolutional neural networks (CNNs) with shared weights and parameters are used to obtain the feature vectors in this process. Using the proposed method, we demonstrate that it is possible to learn necessary information from only one training sample with high accuracy.



### OpenMedIA: Open-Source Medical Image Analysis Toolbox and Benchmark under Heterogeneous AI Computing Platforms
- **Arxiv ID**: http://arxiv.org/abs/2208.05616v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.05616v2)
- **Published**: 2022-08-11 02:51:14+00:00
- **Updated**: 2022-09-08 01:44:10+00:00
- **Authors**: Jia-Xin Zhuang, Xiansong Huang, Yang Yang, Jiancong Chen, Yue Yu, Wei Gao, Ge Li, Jie Chen, Tong Zhang
- **Comment**: 12 pages, 1 figure
- **Journal**: None
- **Summary**: In this paper, we present OpenMedIA, an open-source toolbox library containing a rich set of deep learning methods for medical image analysis under heterogeneous Artificial Intelligence (AI) computing platforms. Various medical image analysis methods, including 2D/3D medical image classification, segmentation, localisation, and detection, have been included in the toolbox with PyTorch and/or MindSpore implementations under heterogeneous NVIDIA and Huawei Ascend computing systems. To our best knowledge, OpenMedIA is the first open-source algorithm library providing compared PyTorch and MindSpore implementations and results on several benchmark datasets. The source codes and models are available at https://git.openi.org.cn/OpenMedIA.



### Language-Guided Face Animation by Recurrent StyleGAN-based Generator
- **Arxiv ID**: http://arxiv.org/abs/2208.05617v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05617v1)
- **Published**: 2022-08-11 02:57:30+00:00
- **Updated**: 2022-08-11 02:57:30+00:00
- **Authors**: Tiankai Hang, Huan Yang, Bei Liu, Jianlong Fu, Xin Geng, Baining Guo
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works on language-guided image manipulation have shown great power of language in providing rich semantics, especially for face images. However, the other natural information, motions, in language is less explored. In this paper, we leverage the motion information and study a novel task, language-guided face animation, that aims to animate a static face image with the help of languages. To better utilize both semantics and motions from languages, we propose a simple yet effective framework. Specifically, we propose a recurrent motion generator to extract a series of semantic and motion information from the language and feed it along with visual information to a pre-trained StyleGAN to generate high-quality frames. To optimize the proposed framework, three carefully designed loss functions are proposed including a regularization loss to keep the face identity, a path length regularization loss to ensure motion smoothness, and a contrastive loss to enable video synthesis with various language guidance in one single model. Extensive experiments with both qualitative and quantitative evaluations on diverse domains (\textit{e.g.,} human face, anime face, and dog face) demonstrate the superiority of our model in generating high-quality and realistic videos from one still image with the guidance of language. Code will be available at https://github.com/TiankaiHang/language-guided-animation.git.



### ARMANI: Part-level Garment-Text Alignment for Unified Cross-Modal Fashion Design
- **Arxiv ID**: http://arxiv.org/abs/2208.05621v1
- **DOI**: 10.1145/3503161.3548230
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05621v1)
- **Published**: 2022-08-11 03:44:02+00:00
- **Updated**: 2022-08-11 03:44:02+00:00
- **Authors**: Xujie Zhang, Yu Sha, Michael C. Kampffmeyer, Zhenyu Xie, Zequn Jie, Chengwen Huang, Jianqing Peng, Xiaodan Liang
- **Comment**: Accepted by ACMMM22
- **Journal**: None
- **Summary**: Cross-modal fashion image synthesis has emerged as one of the most promising directions in the generation domain due to the vast untapped potential of incorporating multiple modalities and the wide range of fashion image applications. To facilitate accurate generation, cross-modal synthesis methods typically rely on Contrastive Language-Image Pre-training (CLIP) to align textual and garment information. In this work, we argue that simply aligning texture and garment information is not sufficient to capture the semantics of the visual information and therefore propose MaskCLIP. MaskCLIP decomposes the garments into semantic parts, ensuring fine-grained and semantically accurate alignment between the visual and text information. Building on MaskCLIP, we propose ARMANI, a unified cross-modal fashion designer with part-level garment-text alignment. ARMANI discretizes an image into uniform tokens based on a learned cross-modal codebook in its first stage and uses a Transformer to model the distribution of image tokens for a real image given the tokens of the control signals in its second stage. Contrary to prior approaches that also rely on two-stage paradigms, ARMANI introduces textual tokens into the codebook, making it possible for the model to utilize fine-grain semantic information to generate more realistic images. Further, by introducing a cross-modal Transformer, ARMANI is versatile and can accomplish image synthesis from various control signals, such as pure text, sketch images, and partial images. Extensive experiments conducted on our newly collected cross-modal fashion dataset demonstrate that ARMANI generates photo-realistic images in diverse synthesis tasks and outperforms existing state-of-the-art cross-modal image synthesis approaches.Our code is available at https://github.com/Harvey594/ARMANI.



### Locality-aware Attention Network with Discriminative Dynamics Learning for Weakly Supervised Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.05636v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05636v1)
- **Published**: 2022-08-11 04:27:33+00:00
- **Updated**: 2022-08-11 04:27:33+00:00
- **Authors**: Yujiang Pu, Xiaoyu Wu
- **Comment**: Accepted by ICME 2022
- **Journal**: None
- **Summary**: Video anomaly detection is recently formulated as a multiple instance learning task under weak supervision, in which each video is treated as a bag of snippets to be determined whether contains anomalies. Previous efforts mainly focus on the discrimination of the snippet itself without modeling the temporal dynamics, which refers to the variation of adjacent snippets. Therefore, we propose a Discriminative Dynamics Learning (DDL) method with two objective functions, i.e., dynamics ranking loss and dynamics alignment loss. The former aims to enlarge the score dynamics gap between positive and negative bags while the latter performs temporal alignment of the feature dynamics and score dynamics within the bag. Moreover, a Locality-aware Attention Network (LA-Net) is constructed to capture global correlations and re-calibrate the location preference across snippets, followed by a multilayer perceptron with causal convolution to obtain anomaly scores. Experimental results show that our method achieves significant improvements on two challenging benchmarks, i.e., UCF-Crime and XD-Violence.



### Adaptive and Implicit Regularization for Matrix Completion
- **Arxiv ID**: http://arxiv.org/abs/2208.05640v1
- **DOI**: 10.1137/22M1489228
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.05640v1)
- **Published**: 2022-08-11 05:00:58+00:00
- **Updated**: 2022-08-11 05:00:58+00:00
- **Authors**: Zhemin Li, Tao Sun, Hongxia Wang, Bao Wang
- **Comment**: None
- **Journal**: SIAM Journal on Imaging Sciences (2022)
- **Summary**: The explicit low-rank regularization, e.g., nuclear norm regularization, has been widely used in imaging sciences. However, it has been found that implicit regularization outperforms explicit ones in various image processing tasks. Another issue is that the fixed explicit regularization limits the applicability to broad images since different images favor different features captured by different explicit regularizations. As such, this paper proposes a new adaptive and implicit low-rank regularization that captures the low-rank prior dynamically from the training data. The core of our new adaptive and implicit low-rank regularization is parameterizing the Laplacian matrix in the Dirichlet energy-based regularization, which we call the regularization AIR. Theoretically, we show that the adaptive regularization of \ReTwo{AIR} enhances the implicit regularization and vanishes at the end of training. We validate AIR's effectiveness on various benchmark tasks, indicating that the AIR is particularly favorable for the scenarios when the missing entries are non-uniform. The code can be found at https://github.com/lizhemin15/AIR-Net.



### Towards Automated Key-Point Detection in Images with Partial Pool View
- **Arxiv ID**: http://arxiv.org/abs/2208.05641v1
- **DOI**: 10.1145/3552437.3555705
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05641v1)
- **Published**: 2022-08-11 05:06:00+00:00
- **Updated**: 2022-08-11 05:06:00+00:00
- **Authors**: T. J. Woinoski, I. V. Bajic
- **Comment**: None
- **Journal**: Proceedings of the 5th International ACM Workshop on Multimedia
  Content Analysis in Sports (MMSports '22), October 10, 2022, Lisboa, Portugal
- **Summary**: Sports analytics has been an up-and-coming field of research among professional sporting organizations and academic institutions alike. With the insurgence and collection of athlete data, the primary goal of such analysis is to improve athletes' performance in a measurable and quantifiable manner. This work is aimed at alleviating some of the challenges encountered in the collection of adequate swimming data. Past works on this subject have shown that the detection and tracking of swimmers is feasible, but not without challenges. Among these challenges are pool localization and determining the relative positions of the swimmers relative to the pool. This work presents two contributions towards solving these challenges. First, we present a pool model with invariant key-points relevant for swimming analytics. Second, we study the detectability of such key-points in images with partial pool view, which are challenging but also quite common in swimming race videos.



### Self-Knowledge Distillation via Dropout
- **Arxiv ID**: http://arxiv.org/abs/2208.05642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05642v1)
- **Published**: 2022-08-11 05:08:55+00:00
- **Updated**: 2022-08-11 05:08:55+00:00
- **Authors**: Hyoje Lee, Yeachan Park, Hyun Seo, Myungjoo Kang
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: To boost the performance, deep neural networks require deeper or wider network structures that involve massive computational and memory costs. To alleviate this issue, the self-knowledge distillation method regularizes the model by distilling the internal knowledge of the model itself. Conventional self-knowledge distillation methods require additional trainable parameters or are dependent on the data. In this paper, we propose a simple and effective self-knowledge distillation method using a dropout (SD-Dropout). SD-Dropout distills the posterior distributions of multiple models through a dropout sampling. Our method does not require any additional trainable modules, does not rely on data, and requires only simple operations. Furthermore, this simple method can be easily combined with various self-knowledge distillation approaches. We provide a theoretical and experimental analysis of the effect of forward and reverse KL-divergences in our work. Extensive experiments on various vision tasks, i.e., image classification, object detection, and distribution shift, demonstrate that the proposed method can effectively improve the generalization of a single network. Further experiments show that the proposed method also improves calibration performance, adversarial robustness, and out-of-distribution detection ability.



### PPMN: Pixel-Phrase Matching Network for One-Stage Panoptic Narrative Grounding
- **Arxiv ID**: http://arxiv.org/abs/2208.05647v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.05647v1)
- **Published**: 2022-08-11 05:42:12+00:00
- **Updated**: 2022-08-11 05:42:12+00:00
- **Authors**: Zihan Ding, Zi-han Ding, Tianrui Hui, Junshi Huang, Xiaoming Wei, Xiaolin Wei, Si Liu
- **Comment**: Accepted by ACM MM 2022
- **Journal**: None
- **Summary**: Panoptic Narrative Grounding (PNG) is an emerging task whose goal is to segment visual objects of things and stuff categories described by dense narrative captions of a still image. The previous two-stage approach first extracts segmentation region proposals by an off-the-shelf panoptic segmentation model, then conducts coarse region-phrase matching to ground the candidate regions for each noun phrase. However, the two-stage pipeline usually suffers from the performance limitation of low-quality proposals in the first stage and the loss of spatial details caused by region feature pooling, as well as complicated strategies designed for things and stuff categories separately. To alleviate these drawbacks, we propose a one-stage end-to-end Pixel-Phrase Matching Network (PPMN), which directly matches each phrase to its corresponding pixels instead of region proposals and outputs panoptic segmentation by simple combination. Thus, our model can exploit sufficient and finer cross-modal semantic correspondence from the supervision of densely annotated pixel-phrase pairs rather than sparse region-phrase pairs. In addition, we also propose a Language-Compatible Pixel Aggregation (LCPA) module to further enhance the discriminative ability of phrase features through multi-round refinement, which selects the most compatible pixels for each phrase to adaptively aggregate the corresponding visual context. Extensive experiments show that our method achieves new state-of-the-art performance on the PNG benchmark with 4.0 absolute Average Recall gains.



### Diverse Generative Perturbations on Attention Space for Transferable Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/2208.05650v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05650v2)
- **Published**: 2022-08-11 06:00:40+00:00
- **Updated**: 2022-12-02 13:12:55+00:00
- **Authors**: Woo Jae Kim, Seunghoon Hong, Sung-Eui Yoon
- **Comment**: ICIP 2022 (Oral)
- **Journal**: None
- **Summary**: Adversarial attacks with improved transferability - the ability of an adversarial example crafted on a known model to also fool unknown models - have recently received much attention due to their practicality. Nevertheless, existing transferable attacks craft perturbations in a deterministic manner and often fail to fully explore the loss surface, thus falling into a poor local optimum and suffering from low transferability. To solve this problem, we propose Attentive-Diversity Attack (ADA), which disrupts diverse salient features in a stochastic manner to improve transferability. Primarily, we perturb the image attention to disrupt universal features shared by different models. Then, to effectively avoid poor local optima, we disrupt these features in a stochastic manner and explore the search space of transferable perturbations more exhaustively. More specifically, we use a generator to produce adversarial perturbations that each disturbs features in different ways depending on an input latent code. Extensive experimental evaluations demonstrate the effectiveness of our method, outperforming the transferability of state-of-the-art methods. Codes are available at https://github.com/wkim97/ADA.



### PA-Seg: Learning from Point Annotations for 3D Medical Image Segmentation using Contextual Regularization and Cross Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/2208.05669v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05669v2)
- **Published**: 2022-08-11 07:00:33+00:00
- **Updated**: 2023-02-13 08:40:44+00:00
- **Authors**: Shuwei Zhai, Guotai Wang, Xiangde Luo, Qiang Yue, Kang Li, Shaoting Zhang
- **Comment**: 12 pages, 10 figures, 4 tables; Accepted by IEEE TMI
- **Journal**: None
- **Summary**: The success of Convolutional Neural Networks (CNNs) in 3D medical image segmentation relies on massive fully annotated 3D volumes for training that are time-consuming and labor-intensive to acquire. In this paper, we propose to annotate a segmentation target with only seven points in 3D medical images, and design a two-stage weakly supervised learning framework PA-Seg. In the first stage, we employ geodesic distance transform to expand the seed points to provide more supervision signal. To further deal with unannotated image regions during training, we propose two contextual regularization strategies, i.e., multi-view Conditional Random Field (mCRF) loss and Variance Minimization (VM) loss, where the first one encourages pixels with similar features to have consistent labels, and the second one minimizes the intensity variance for the segmented foreground and background, respectively. In the second stage, we use predictions obtained by the model pre-trained in the first stage as pseudo labels. To overcome noises in the pseudo labels, we introduce a Self and Cross Monitoring (SCM) strategy, which combines self-training with Cross Knowledge Distillation (CKD) between a primary model and an auxiliary model that learn from soft labels generated by each other. Experiments on public datasets for Vestibular Schwannoma (VS) segmentation and Brain Tumor Segmentation (BraTS) demonstrated that our model trained in the first stage outperformed existing state-of-the-art weakly supervised approaches by a large margin, and after using SCM for additional training, the model's performance was close to its fully supervised counterpart on the BraTS dataset.



### WeightMom: Learning Sparse Networks using Iterative Momentum-based pruning
- **Arxiv ID**: http://arxiv.org/abs/2208.05970v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.05970v1)
- **Published**: 2022-08-11 07:13:59+00:00
- **Updated**: 2022-08-11 07:13:59+00:00
- **Authors**: Elvis Johnson, Xiaochen Tang, Sriramacharyulu Samudrala
- **Comment**: submitted to ICTAI 2022
- **Journal**: None
- **Summary**: Deep Neural Networks have been used in a wide variety of applications with significant success. However, their highly complex nature owing to comprising millions of parameters has lead to problems during deployment in pipelines with low latency requirements. As a result, it is more desirable to obtain lightweight neural networks which have the same performance during inference time. In this work, we propose a weight based pruning approach in which the weights are pruned gradually based on their momentum of the previous iterations. Each layer of the neural network is assigned an importance value based on their relative sparsity, followed by the magnitude of the weight in the previous iterations. We evaluate our approach on networks such as AlexNet, VGG16 and ResNet50 with image classification datasets such as CIFAR-10 and CIFAR-100. We found that the results outperformed the previous approaches with respect to accuracy and compression ratio. Our method is able to obtain a compression of 15% for the same degradation in accuracy on both the datasets.



### Semi-supervised Vision Transformers at Scale
- **Arxiv ID**: http://arxiv.org/abs/2208.05688v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.05688v1)
- **Published**: 2022-08-11 08:11:54+00:00
- **Updated**: 2022-08-11 08:11:54+00:00
- **Authors**: Zhaowei Cai, Avinash Ravichandran, Paolo Favaro, Manchen Wang, Davide Modolo, Rahul Bhotika, Zhuowen Tu, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We study semi-supervised learning (SSL) for vision transformers (ViT), an under-explored topic despite the wide adoption of the ViT architectures to different tasks. To tackle this problem, we propose a new SSL pipeline, consisting of first un/self-supervised pre-training, followed by supervised fine-tuning, and finally semi-supervised fine-tuning. At the semi-supervised fine-tuning stage, we adopt an exponential moving average (EMA)-Teacher framework instead of the popular FixMatch, since the former is more stable and delivers higher accuracy for semi-supervised vision transformers. In addition, we propose a probabilistic pseudo mixup mechanism to interpolate unlabeled samples and their pseudo labels for improved regularization, which is important for training ViTs with weak inductive bias. Our proposed method, dubbed Semi-ViT, achieves comparable or better performance than the CNN counterparts in the semi-supervised classification setting. Semi-ViT also enjoys the scalability benefits of ViTs that can be readily scaled up to large-size models with increasing accuracies. For example, Semi-ViT-Huge achieves an impressive 80% top-1 accuracy on ImageNet using only 1% labels, which is comparable with Inception-v4 using 100% ImageNet labels.



### General Cutting Planes for Bound-Propagation-Based Neural Network Verification
- **Arxiv ID**: http://arxiv.org/abs/2208.05740v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2208.05740v2)
- **Published**: 2022-08-11 10:31:28+00:00
- **Updated**: 2022-12-04 09:39:50+00:00
- **Authors**: Huan Zhang, Shiqi Wang, Kaidi Xu, Linyi Li, Bo Li, Suman Jana, Cho-Jui Hsieh, J. Zico Kolter
- **Comment**: Accepted by NeurIPS 2022. GCP-CROWN is part of the alpha-beta-CROWN
  verifier, the VNN-COMP 2022 winner
- **Journal**: None
- **Summary**: Bound propagation methods, when combined with branch and bound, are among the most effective methods to formally verify properties of deep neural networks such as correctness, robustness, and safety. However, existing works cannot handle the general form of cutting plane constraints widely accepted in traditional solvers, which are crucial for strengthening verifiers with tightened convex relaxations. In this paper, we generalize the bound propagation procedure to allow the addition of arbitrary cutting plane constraints, including those involving relaxed integer variables that do not appear in existing bound propagation formulations. Our generalized bound propagation method, GCP-CROWN, opens up the opportunity to apply general cutting plane methods for neural network verification while benefiting from the efficiency and GPU acceleration of bound propagation methods. As a case study, we investigate the use of cutting planes generated by off-the-shelf mixed integer programming (MIP) solver. We find that MIP solvers can generate high-quality cutting planes for strengthening bound-propagation-based verifiers using our new formulation. Since the branching-focused bound propagation procedure and the cutting-plane-focused MIP solver can run in parallel utilizing different types of hardware (GPUs and CPUs), their combination can quickly explore a large number of branches with strong cutting planes, leading to strong verification performance. Experiments demonstrate that our method is the first verifier that can completely solve the oval20 benchmark and verify twice as many instances on the oval21 benchmark compared to the best tool in VNN-COMP 2021, and also noticeably outperforms state-of-the-art verifiers on a wide range of benchmarks. GCP-CROWN is part of the $\alpha,\!\beta$-CROWN verifier, the VNN-COMP 2022 winner. Code is available at http://PaperCode.cc/GCP-CROWN



### On the Pros and Cons of Momentum Encoder in Self-Supervised Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/2208.05744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05744v1)
- **Published**: 2022-08-11 10:44:49+00:00
- **Updated**: 2022-08-11 10:44:49+00:00
- **Authors**: Trung Pham, Chaoning Zhang, Axi Niu, Kang Zhang, Chang D. Yoo
- **Comment**: 35 pages
- **Journal**: None
- **Summary**: Exponential Moving Average (EMA or momentum) is widely used in modern self-supervised learning (SSL) approaches, such as MoCo, for enhancing performance. We demonstrate that such momentum can also be plugged into momentum-free SSL frameworks, such as SimCLR, for a performance boost. Despite its wide use as a fundamental component in modern SSL frameworks, the benefit caused by momentum is not well understood. We find that its success can be at least partly attributed to the stability effect. In the first attempt, we analyze how EMA affects each part of the encoder and reveal that the portion near the encoder's input plays an insignificant role while the latter parts have much more influence. By monitoring the gradient of the overall loss with respect to the output of each block in the encoder, we observe that the final layers tend to fluctuate much more than other layers during backpropagation, i.e. less stability. Interestingly, we show that using EMA to the final part of the SSL encoder, i.e. projector, instead of the whole deep network encoder can give comparable or preferable performance. Our proposed projector-only momentum helps maintain the benefit of EMA but avoids the double forward computation.



### FDNeRF: Few-shot Dynamic Neural Radiance Fields for Face Reconstruction and Expression Editing
- **Arxiv ID**: http://arxiv.org/abs/2208.05751v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05751v2)
- **Published**: 2022-08-11 11:05:59+00:00
- **Updated**: 2022-09-12 10:38:46+00:00
- **Authors**: Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao
- **Comment**: Accepted at SIGGRAPH Asia 2022. Project page:
  https://fdnerf.github.io
- **Journal**: None
- **Summary**: We propose a Few-shot Dynamic Neural Radiance Field (FDNeRF), the first NeRF-based method capable of reconstruction and expression editing of 3D faces based on a small number of dynamic images. Unlike existing dynamic NeRFs that require dense images as input and can only be modeled for a single identity, our method enables face reconstruction across different persons with few-shot inputs. Compared to state-of-the-art few-shot NeRFs designed for modeling static scenes, the proposed FDNeRF accepts view-inconsistent dynamic inputs and supports arbitrary facial expression editing, i.e., producing faces with novel expressions beyond the input ones. To handle the inconsistencies between dynamic inputs, we introduce a well-designed conditional feature warping (CFW) module to perform expression conditioned warping in 2D feature space, which is also identity adaptive and 3D constrained. As a result, features of different expressions are transformed into the target ones. We then construct a radiance field based on these view-consistent features and use volumetric rendering to synthesize novel views of the modeled faces. Extensive experiments with quantitative and qualitative evaluation demonstrate that our method outperforms existing dynamic and few-shot NeRFs on both 3D face reconstruction and expression editing tasks. Code is available at https://github.com/FDNeRF/FDNeRF.



### MixSKD: Self-Knowledge Distillation from Mixup for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/2208.05768v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05768v1)
- **Published**: 2022-08-11 11:57:26+00:00
- **Updated**: 2022-08-11 11:57:26+00:00
- **Authors**: Chuanguang Yang, Zhulin An, Helong Zhou, Linhang Cai, Xiang Zhi, Jiwen Wu, Yongjun Xu, Qian Zhang
- **Comment**: 22 pages, ECCV-2022
- **Journal**: None
- **Summary**: Unlike the conventional Knowledge Distillation (KD), Self-KD allows a network to learn knowledge from itself without any guidance from extra networks. This paper proposes to perform Self-KD from image Mixture (MixSKD), which integrates these two techniques into a unified framework. MixSKD mutually distills feature maps and probability distributions between the random pair of original images and their mixup images in a meaningful way. Therefore, it guides the network to learn cross-image knowledge by modelling supervisory signals from mixup images. Moreover, we construct a self-teacher network by aggregating multi-stage feature maps for providing soft labels to supervise the backbone classifier, further improving the efficacy of self-boosting. Experiments on image classification and transfer learning to object detection and semantic segmentation demonstrate that MixSKD outperforms other state-of-the-art Self-KD and data augmentation methods. The code is available at https://github.com/winycg/Self-KD-Lib.



### PSUMNet: Unified Modality Part Streams are All You Need for Efficient Pose-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2208.05775v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.05775v1)
- **Published**: 2022-08-11 12:12:07+00:00
- **Updated**: 2022-08-11 12:12:07+00:00
- **Authors**: Neel Trivedi, Ravi Kiran Sarvadevabhatla
- **Comment**: Accepted at ECCV 2022 WCPA (https://sites.google.com/view/wcpa2022/)
  . Code and models at https://github.com/skelemoa/psumnet
- **Journal**: None
- **Summary**: Pose-based action recognition is predominantly tackled by approaches which treat the input skeleton in a monolithic fashion, i.e. joints in the pose tree are processed as a whole. However, such approaches ignore the fact that action categories are often characterized by localized action dynamics involving only small subsets of part joint groups involving hands (e.g. `Thumbs up') or legs (e.g. `Kicking'). Although part-grouping based approaches exist, each part group is not considered within the global pose frame, causing such methods to fall short. Further, conventional approaches employ independent modality streams (e.g. joint, bone, joint velocity, bone velocity) and train their network multiple times on these streams, which massively increases the number of training parameters. To address these issues, we introduce PSUMNet, a novel approach for scalable and efficient pose-based action recognition. At the representation level, we propose a global frame based part stream approach as opposed to conventional modality based streams. Within each part stream, the associated data from multiple modalities is unified and consumed by the processing pipeline. Experimentally, PSUMNet achieves state of the art performance on the widely used NTURGB+D 60/120 dataset and dense joint skeleton dataset NTU 60-X/120-X. PSUMNet is highly efficient and outperforms competing methods which use 100%-400% more parameters. PSUMNet also generalizes to the SHREC hand gesture dataset with competitive performance. Overall, PSUMNet's scalability, performance and efficiency makes it an attractive choice for action recognition and for deployment on compute-restricted embedded and edge devices. Code and pretrained models can be accessed at https://github.com/skelemoa/psumnet



### Unsupervised Face Morphing Attack Detection via Self-paced Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.05787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05787v1)
- **Published**: 2022-08-11 12:21:50+00:00
- **Updated**: 2022-08-11 12:21:50+00:00
- **Authors**: Meiling Fang, Fadi Boutros, Naser Damer
- **Comment**: Accepted at IJCB2022
- **Journal**: None
- **Summary**: The supervised-learning-based morphing attack detection (MAD) solutions achieve outstanding success in dealing with attacks from known morphing techniques and known data sources. However, given variations in the morphing attacks, the performance of supervised MAD solutions drops significantly due to the insufficient diversity and quantity of the existing MAD datasets. To address this concern, we propose a completely unsupervised MAD solution via self-paced anomaly detection (SPL-MAD) by leveraging the existing large-scale face recognition (FR) datasets and the unsupervised nature of convolutional autoencoders. Using general FR datasets that might contain unintentionally and unlabeled manipulated samples to train an autoencoder can lead to a diverse reconstruction behavior of attack and bona fide samples. We analyze this behavior empirically to provide a solid theoretical ground for designing our unsupervised MAD solution. This also results in proposing to integrate our adapted modified self-paced learning paradigm to enhance the reconstruction error separability between the bona fide and attack samples in a completely unsupervised manner. Our experimental results on a diverse set of MAD evaluation datasets show that the proposed unsupervised SPL-MAD solution outperforms the overall performance of a wide range of supervised MAD solutions and provides higher generalizability on unknown attacks.



### Towards Sequence-Level Training for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/2208.05810v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.05810v3)
- **Published**: 2022-08-11 13:15:36+00:00
- **Updated**: 2022-10-16 16:05:12+00:00
- **Authors**: Minji Kim, Seungkwan Lee, Jungseul Ok, Bohyung Han, Minsu Cho
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Despite the extensive adoption of machine learning on the task of visual object tracking, recent learning-based approaches have largely overlooked the fact that visual tracking is a sequence-level task in its nature; they rely heavily on frame-level training, which inevitably induces inconsistency between training and testing in terms of both data distributions and task objectives. This work introduces a sequence-level training strategy for visual tracking based on reinforcement learning and discusses how a sequence-level design of data sampling, learning objectives, and data augmentation can improve the accuracy and robustness of tracking algorithms. Our experiments on standard benchmarks including LaSOT, TrackingNet, and GOT-10k demonstrate that four representative tracking models, SiamRPN++, SiamAttn, TransT, and TrDiMP, consistently improve by incorporating the proposed methods in training without modifying architectures.



### Seeing your sleep stage: cross-modal distillation from EEG to infrared video
- **Arxiv ID**: http://arxiv.org/abs/2208.05814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2208.05814v1)
- **Published**: 2022-08-11 13:21:31+00:00
- **Updated**: 2022-08-11 13:21:31+00:00
- **Authors**: Jianan Han, Shaoxing Zhang, Aidong Men, Yang Liu, Ziming Yao, Yan Yan, Qingchao Chen
- **Comment**: We have submitted this paper to an academic journal
- **Journal**: None
- **Summary**: It is inevitably crucial to classify sleep stage for the diagnosis of various diseases. However, existing automated diagnosis methods mostly adopt the "gold-standard" lectroencephalogram (EEG) or other uni-modal sensing signal of the PolySomnoGraphy (PSG) machine in hospital, that are expensive, importable and therefore unsuitable for point-of-care monitoring at home. To enable the sleep stage monitoring at home, in this paper, we analyze the relationship between infrared videos and the EEG signal and propose a new task: to classify the sleep stage using infrared videos by distilling useful knowledge from EEG signals to the visual ones. To establish a solid cross-modal benchmark for this application, we develop a new dataset termed as Seeing your Sleep Stage via Infrared Video and EEG ($S^3VE$). $S^3VE$ is a large-scale dataset including synchronized infrared video and EEG signal for sleep stage classification, including 105 subjects and 154,573 video clips that is more than 1100 hours long. Our contributions are not limited to datasets but also about a novel cross-modal distillation baseline model namely the structure-aware contrastive distillation (SACD) to distill the EEG knowledge to infrared video features. The SACD achieved the state-of-the-art performances on both our $S^3VE$ and the existing cross-modal distillation benchmark. Both the benchmark and the baseline methods will be released to the community. We expect to raise more attentions and promote more developments in the sleep stage classification and more importantly the cross-modal distillation from clinical signal/media to the conventional media.



### Hybrid Transformer Network for Deepfake Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.05820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05820v1)
- **Published**: 2022-08-11 13:30:42+00:00
- **Updated**: 2022-08-11 13:30:42+00:00
- **Authors**: Sohail Ahmed Khan, Duc-Tien Dang-Nguyen
- **Comment**: Accepted for publication at ACM International Conference on
  Content-Based Multimedia Indexing
- **Journal**: None
- **Summary**: Deepfake media is becoming widespread nowadays because of the easily available tools and mobile apps which can generate realistic looking deepfake videos/images without requiring any technical knowledge. With further advances in this field of technology in the near future, the quantity and quality of deepfake media is also expected to flourish, while making deepfake media a likely new practical tool to spread mis/disinformation. Because of these concerns, the deepfake media detection tools are becoming a necessity. In this study, we propose a novel hybrid transformer network utilizing early feature fusion strategy for deepfake video detection. Our model employs two different CNN networks, i.e., (1) XceptionNet and (2) EfficientNet-B4 as feature extractors. We train both feature extractors along with the transformer in an end-to-end manner on FaceForensics++, DFDC benchmarks. Our model, while having relatively straightforward architecture, achieves comparable results to other more advanced state-of-the-art approaches when evaluated on FaceForensics++ and DFDC benchmarks. Besides this, we also propose novel face cut-out augmentations, as well as random cut-out augmentations. We show that the proposed augmentations improve the detection performance of our model and reduce overfitting. In addition to that, we show that our model is capable of learning from considerably small amount of data.



### K-UNN: k-Space Interpolation With Untrained Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2208.05827v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05827v1)
- **Published**: 2022-08-11 13:53:48+00:00
- **Updated**: 2022-08-11 13:53:48+00:00
- **Authors**: Zhuo-Xu Cui, Sen Jia, Qingyong Zhu, Congcong Liu, Zhilang Qiu, Yuanyuan Liu, Jing Cheng, Haifeng Wang, Yanjie Zhu, Dong Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, untrained neural networks (UNNs) have shown satisfactory performances for MR image reconstruction on random sampling trajectories without using additional full-sampled training data. However, the existing UNN-based approach does not fully use the MR image physical priors, resulting in poor performance in some common scenarios (e.g., partial Fourier, regular sampling, etc.) and the lack of theoretical guarantees for reconstruction accuracy. To bridge this gap, we propose a safeguarded k-space interpolation method for MRI using a specially designed UNN with a tripled architecture driven by three physical priors of the MR images (or k-space data), including sparsity, coil sensitivity smoothness, and phase smoothness. We also prove that the proposed method guarantees tight bounds for interpolated k-space data accuracy. Finally, ablation experiments show that the proposed method can more accurately characterize the physical priors of MR images than existing traditional methods. Additionally, under a series of commonly used sampling trajectories, experiments also show that the proposed method consistently outperforms traditional parallel imaging methods and existing UNNs, and even outperforms the state-of-the-art supervised-trained k-space deep learning methods in some cases.



### Joint reconstruction-segmentation on graphs
- **Arxiv ID**: http://arxiv.org/abs/2208.05834v2
- **DOI**: 10.1137/22M151546X
- **Categories**: **cs.CV**, 05C99, 34B45, 35R02, 65F60, 94A08
- **Links**: [PDF](http://arxiv.org/pdf/2208.05834v2)
- **Published**: 2022-08-11 14:01:38+00:00
- **Updated**: 2023-01-20 17:58:02+00:00
- **Authors**: Jeremy Budd, Yves van Gennip, Jonas Latz, Simone Parisotto, Carola-Bibiane Schönlieb
- **Comment**: 33 pages, 20 figures
- **Journal**: SIAM Journal on Imaging Sciences, 16(2), 911-947 (2023)
- **Summary**: Practical image segmentation tasks concern images which must be reconstructed from noisy, distorted, and/or incomplete observations. A recent approach for solving such tasks is to perform this reconstruction jointly with the segmentation, using each to guide the other. However, this work has so far employed relatively simple segmentation methods, such as the Chan--Vese algorithm. In this paper, we present a method for joint reconstruction-segmentation using graph-based segmentation methods, which have been seeing increasing recent interest. Complications arise due to the large size of the matrices involved, and we show how these complications can be managed. We then analyse the convergence properties of our scheme. Finally, we apply this scheme to distorted versions of ``two cows'' images familiar from previous graph-based segmentation literature, first to a highly noised version and second to a blurred version, achieving highly accurate segmentations in both cases. We compare these results to those obtained by sequential reconstruction-segmentation approaches, finding that our method competes with, or even outperforms, those approaches in terms of reconstruction and segmentation accuracy.



### Differencing based Self-supervised pretraining for Scene Change Detection
- **Arxiv ID**: http://arxiv.org/abs/2208.05838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05838v1)
- **Published**: 2022-08-11 14:06:32+00:00
- **Updated**: 2022-08-11 14:06:32+00:00
- **Authors**: Vijaya Raghavan T. Ramkumar, Elahe Arani, Bahram Zonooz
- **Comment**: Published at Conference on Lifelong Learning Agents (CoLLAs 2022)
- **Journal**: None
- **Summary**: Scene change detection (SCD), a crucial perception task, identifies changes by comparing scenes captured at different times. SCD is challenging due to noisy changes in illumination, seasonal variations, and perspective differences across a pair of views. Deep neural network based solutions require a large quantity of annotated data which is tedious and expensive to obtain. On the other hand, transfer learning from large datasets induces domain shift. To address these challenges, we propose a novel \textit{Differencing self-supervised pretraining (DSP)} method that uses feature differencing to learn discriminatory representations corresponding to the changed regions while simultaneously tackling the noisy changes by enforcing temporal invariance across views. Our experimental results on SCD datasets demonstrate the effectiveness of our method, specifically to differences in camera viewpoints and lighting conditions. Compared against the self-supervised Barlow Twins and the standard ImageNet pretraining that uses more than a million additional labeled images, DSP can surpass it without using any additional data. Our results also demonstrate the robustness of DSP to natural corruptions, distribution shift, and learning under limited labeled data.



### A Comprehensive Analysis of AI Biases in DeepFake Detection With Massively Annotated Databases
- **Arxiv ID**: http://arxiv.org/abs/2208.05845v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.05845v2)
- **Published**: 2022-08-11 14:28:21+00:00
- **Updated**: 2023-03-23 17:56:52+00:00
- **Authors**: Ying Xu, Philipp Terhörst, Kiran Raja, Marius Pedersen
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, image and video manipulations with Deepfake have become a severe concern for security and society. Many detection models and datasets have been proposed to detect Deepfake data reliably. However, there is an increased concern that these models and training databases might be biased and, thus, cause Deepfake detectors to fail. In this work, we investigate the bias issue caused by public Deepfake datasets by (a) providing large-scale demographic and non-demographic attribute annotations of 47 different attributes for five popular Deepfake datasets and (b) comprehensively analysing AI-bias of three state-of-the-art Deepfake detection backbone models on these datasets. The investigation analyses the influence of a large variety of distinctive attributes (from over 65M labels) on the detection performance, including demographic (age, gender, ethnicity) and non-demographic (hair, skin, accessories, etc.) information. The results indicate that investigated databases lack diversity and, more importantly, show that the utilised Deepfake detection backbone models are strongly biased towards many investigated attributes. The Deepfake detection backbone methods, which are trained with biased datasets, might output incorrect detection results, thereby leading to generalisability, fairness, and security issues. We hope that the findings of this study and the annotation databases will help to evaluate and mitigate bias in future Deepfake detection techniques. The annotation datasets are publicly available.



### MultiMatch: Multi-task Learning for Semi-supervised Domain Generalization
- **Arxiv ID**: http://arxiv.org/abs/2208.05853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05853v1)
- **Published**: 2022-08-11 14:44:33+00:00
- **Updated**: 2022-08-11 14:44:33+00:00
- **Authors**: Lei Qi, Hongpeng Yang, Yinghuan Shi, Xin Geng
- **Comment**: None
- **Journal**: None
- **Summary**: Domain generalization (DG) aims at learning a model on source domains to well generalize on the unseen target domain. Although it has achieved great success, most of existing methods require the label information for all training samples in source domains, which is time-consuming and expensive in the real-world application. In this paper, we resort to solving the semi-supervised domain generalization (SSDG) task, where there are a few label information in each source domain. To address the task, we first analyze the theory of the multi-domain learning, which highlights that 1) mitigating the impact of domain gap and 2) exploiting all samples to train the model can effectively reduce the generalization error in each source domain so as to improve the quality of pseudo-labels. According to the analysis, we propose MultiMatch, i.e., extending FixMatch to the multi-task learning framework, producing the high-quality pseudo-label for SSDG. To be specific, we consider each training domain as a single task (i.e., local task) and combine all training domains together (i.e., global task) to train an extra task for the unseen test domain. In the multi-task framework, we utilize the independent BN and classifier for each task, which can effectively alleviate the interference from different domains during pseudo-labeling. Also, most of parameters in the framework are shared, which can be trained by all training samples sufficiently. Moreover, to further boost the pseudo-label accuracy and the model's generalization, we fuse the predictions from the global task and local task during training and testing, respectively. A series of experiments validate the effectiveness of the proposed method, and it outperforms the existing semi-supervised methods and the SSDG method on several benchmark DG datasets.



### Face Morphing Attacks and Face Image Quality: The Effect of Morphing and the Unsupervised Attack Detection by Quality
- **Arxiv ID**: http://arxiv.org/abs/2208.05864v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05864v2)
- **Published**: 2022-08-11 15:12:50+00:00
- **Updated**: 2022-08-12 07:15:35+00:00
- **Authors**: Biying Fu, Naser Damer
- **Comment**: accepted at IET Biometrics journal
- **Journal**: None
- **Summary**: Morphing attacks are a form of presentation attacks that gathered increasing attention in recent years. A morphed image can be successfully verified to multiple identities. This operation, therefore, poses serious security issues related to the ability of a travel or identity document to be verified to belong to multiple persons. Previous works touched on the issue of the quality of morphing attack images, however, with the main goal of quantitatively proofing the realistic appearance of the produced morphing attacks. We theorize that the morphing processes might have an effect on both, the perceptual image quality and the image utility in face recognition (FR) when compared to bona fide samples. Towards investigating this theory, this work provides an extensive analysis of the effect of morphing on face image quality, including both general image quality measures and face image utility measures. This analysis is not limited to a single morphing technique, but rather looks at six different morphing techniques and five different data sources using ten different quality measures. This analysis reveals consistent separability between the quality scores of morphing attack and bona fide samples measured by certain quality measures. Our study goes further to build on this effect and investigate the possibility of performing unsupervised morphing attack detection (MAD) based on quality scores. Our study looks intointra and inter-dataset detectability to evaluate the generalizability of such a detection concept on different morphing techniques and bona fide sources. Our final results point out that a set of quality measures, such as MagFace and CNNNIQA, can be used to perform unsupervised and generalized MAD with a correct classification accuracy of over 70%.



### TotalSegmentator: robust segmentation of 104 anatomical structures in CT images
- **Arxiv ID**: http://arxiv.org/abs/2208.05868v2
- **DOI**: 10.1148/ryai.230024
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.05868v2)
- **Published**: 2022-08-11 15:16:40+00:00
- **Updated**: 2023-06-16 14:26:43+00:00
- **Authors**: Jakob Wasserthal, Hanns-Christian Breit, Manfred T. Meyer, Maurice Pradella, Daniel Hinck, Alexander W. Sauter, Tobias Heye, Daniel Boll, Joshy Cyriac, Shan Yang, Michael Bach, Martin Segeroth
- **Comment**: Accepted at Radiology: Artificial Intelligence
- **Journal**: Radiol Artif Intell 2023;5(5):e230024
- **Summary**: We present a deep learning segmentation model that can automatically and robustly segment all major anatomical structures in body CT images. In this retrospective study, 1204 CT examinations (from the years 2012, 2016, and 2020) were used to segment 104 anatomical structures (27 organs, 59 bones, 10 muscles, 8 vessels) relevant for use cases such as organ volumetry, disease characterization, and surgical or radiotherapy planning. The CT images were randomly sampled from routine clinical studies and thus represent a real-world dataset (different ages, pathologies, scanners, body parts, sequences, and sites). The authors trained an nnU-Net segmentation algorithm on this dataset and calculated Dice similarity coefficients (Dice) to evaluate the model's performance. The trained algorithm was applied to a second dataset of 4004 whole-body CT examinations to investigate age dependent volume and attenuation changes. The proposed model showed a high Dice score (0.943) on the test set, which included a wide range of clinical data with major pathologies. The model significantly outperformed another publicly available segmentation model on a separate dataset (Dice score, 0.932 versus 0.871, respectively). The aging study demonstrated significant correlations between age and volume and mean attenuation for a variety of organ groups (e.g., age and aortic volume; age and mean attenuation of the autochthonous dorsal musculature). The developed model enables robust and accurate segmentation of 104 anatomical structures. The annotated dataset (https://doi.org/10.5281/zenodo.6802613) and toolkit (https://www.github.com/wasserth/TotalSegmentator) are publicly available.



### Uncertainty-Aware Blob Detection with an Application to Integrated-Light Stellar Population Recoveries
- **Arxiv ID**: http://arxiv.org/abs/2208.05881v2
- **DOI**: 10.1051/0004-6361/202244739
- **Categories**: **astro-ph.GA**, astro-ph.IM, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/2208.05881v2)
- **Published**: 2022-08-11 15:34:27+00:00
- **Updated**: 2023-03-03 10:37:13+00:00
- **Authors**: Fabian Parzer, Prashin Jethwa, Alina Boecker, Mayte Alfaro-Cuello, Otmar Scherzer, Glenn van de Ven
- **Comment**: Revision submitted to A&A, comments welcome
- **Journal**: None
- **Summary**: Context. Blob detection is a common problem in astronomy. One example is in stellar population modelling, where the distribution of stellar ages and metallicities in a galaxy is inferred from observations. In this context, blobs may correspond to stars born in-situ versus those accreted from satellites, and the task of blob detection is to disentangle these components. A difficulty arises when the distributions come with significant uncertainties, as is the case for stellar population recoveries inferred from modelling spectra of unresolved stellar systems. There is currently no satisfactory method for blob detection with uncertainties. Aims. We introduce a method for uncertainty-aware blob detection developed in the context of stellar population modelling of integrated-light spectra of stellar systems. Methods. We develop theory and computational tools for an uncertainty-aware version of the classic Laplacian-of-Gaussians method for blob detection, which we call ULoG. This identifies significant blobs considering a variety of scales. As a prerequisite to apply ULoG to stellar population modelling, we introduce a method for efficient computation of uncertainties for spectral modelling. This method is based on the truncated Singular Value Decomposition and Markov Chain Monte Carlo sampling (SVD-MCMC). Results. We apply the methods to data of the star cluster M54. We show that the SVD-MCMC inferences match those from standard MCMC, but are a factor 5-10 faster to compute. We apply ULoG to the inferred M54 age/metallicity distributions, identifying between 2 or 3 significant, distinct populations amongst its stars.



### Optimal Transport Features for Morphometric Population Analysis
- **Arxiv ID**: http://arxiv.org/abs/2208.05891v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM, I.4.9; J.3; I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/2208.05891v1)
- **Published**: 2022-08-11 15:46:32+00:00
- **Updated**: 2022-08-11 15:46:32+00:00
- **Authors**: Samuel Gerber, Marc Niethammer, Ebrahim Ebrahim, Joseph Piven, Stephen R. Dager, Martin Styner, Stephen Aylward, Andinet Enquobahrie
- **Comment**: submitted to Medical Image Analysis
- **Journal**: None
- **Summary**: Brain pathologies often manifest as partial or complete loss of tissue. The goal of many neuroimaging studies is to capture the location and amount of tissue changes with respect to a clinical variable of interest, such as disease progression. Morphometric analysis approaches capture local differences in the distribution of tissue or other quantities of interest in relation to a clinical variable. We propose to augment morphometric analysis with an additional feature extraction step based on unbalanced optimal transport. The optimal transport feature extraction step increases statistical power for pathologies that cause spatially dispersed tissue loss, minimizes sensitivity to shifts due to spatial misalignment or differences in brain topology, and separates changes due to volume differences from changes due to tissue location. We demonstrate the proposed optimal transport feature extraction step in the context of a volumetric morphometric analysis of the OASIS-1 study for Alzheimer's disease. The results demonstrate that the proposed approach can identify tissue changes and differences that are not otherwise measurable.



### Heatmap Regression for Lesion Detection using Pointwise Annotations
- **Arxiv ID**: http://arxiv.org/abs/2208.05939v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.05939v1)
- **Published**: 2022-08-11 17:26:09+00:00
- **Updated**: 2022-08-11 17:26:09+00:00
- **Authors**: Chelsea Myers-Colet, Julien Schroeter, Douglas L. Arnold, Tal Arbel
- **Comment**: Accepted at Medical Image Learning with Limited & Noisy Data
  (MILLanD), a workshop hosted with the conference on Medical Image Computing
  and Computer Assisted Interventions (MICCAI) 2022
- **Journal**: None
- **Summary**: In many clinical contexts, detecting all lesions is imperative for evaluating disease activity. Standard approaches pose lesion detection as a segmentation problem despite the time-consuming nature of acquiring segmentation labels. In this paper, we present a lesion detection method which relies only on point labels. Our model, which is trained via heatmap regression, can detect a variable number of lesions in a probabilistic manner. In fact, our proposed post-processing method offers a reliable way of directly estimating the lesion existence uncertainty. Experimental results on Gad lesion detection show our point-based method performs competitively compared to training on expensive segmentation labels. Finally, our detection model provides a suitable pre-training for segmentation. When fine-tuning on only 17 segmentation samples, we achieve comparable performance to training with the full dataset.



### PointTree: Transformation-Robust Point Cloud Encoder with Relaxed K-D Trees
- **Arxiv ID**: http://arxiv.org/abs/2208.05962v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.05962v1)
- **Published**: 2022-08-11 17:59:09+00:00
- **Updated**: 2022-08-11 17:59:09+00:00
- **Authors**: Jun-Kun Chen, Yu-Xiong Wang
- **Comment**: ECCV 2022
- **Journal**: None
- **Summary**: Being able to learn an effective semantic representation directly on raw point clouds has become a central topic in 3D understanding. Despite rapid progress, state-of-the-art encoders are restrictive to canonicalized point clouds, and have weaker than necessary performance when encountering geometric transformation distortions. To overcome this challenge, we propose PointTree, a general-purpose point cloud encoder that is robust to transformations based on relaxed K-D trees. Key to our approach is the design of the division rule in K-D trees by using principal component analysis (PCA). We use the structure of the relaxed K-D tree as our computational graph, and model the features as border descriptors which are merged with pointwise-maximum operation. In addition to this novel architecture design, we further improve the robustness by introducing pre-alignment -- a simple yet effective PCA-based normalization scheme. Our PointTree encoder combined with pre-alignment consistently outperforms state-of-the-art methods by large margins, for applications from object classification to semantic segmentation on various transformed versions of the widely-benchmarked datasets. Code and pre-trained models are available at https://github.com/immortalCO/PointTree.



### RelPose: Predicting Probabilistic Relative Rotation for Single Objects in the Wild
- **Arxiv ID**: http://arxiv.org/abs/2208.05963v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.05963v2)
- **Published**: 2022-08-11 17:59:59+00:00
- **Updated**: 2022-10-03 00:44:14+00:00
- **Authors**: Jason Y. Zhang, Deva Ramanan, Shubham Tulsiani
- **Comment**: In ECCV 2022. V2: updated references
- **Journal**: None
- **Summary**: We describe a data-driven method for inferring the camera viewpoints given multiple images of an arbitrary object. This task is a core component of classic geometric pipelines such as SfM and SLAM, and also serves as a vital pre-processing requirement for contemporary neural approaches (e.g. NeRF) to object reconstruction and view synthesis. In contrast to existing correspondence-driven methods that do not perform well given sparse views, we propose a top-down prediction based approach for estimating camera viewpoints. Our key technical insight is the use of an energy-based formulation for representing distributions over relative camera rotations, thus allowing us to explicitly represent multiple camera modes arising from object symmetries or views. Leveraging these relative predictions, we jointly estimate a consistent set of camera rotations from multiple images. We show that our approach outperforms state-of-the-art SfM and SLAM methods given sparse images on both seen and unseen categories. Further, our probabilistic approach significantly outperforms directly regressing relative poses, suggesting that modeling multimodality is important for coherent joint reconstruction. We demonstrate that our system can be a stepping stone toward in-the-wild reconstruction from multi-view datasets. The project page with code and videos can be found at https://jasonyzhang.com/relpose.



### Anomaly segmentation model for defects detection in electroluminescence images of heterojunction solar cells
- **Arxiv ID**: http://arxiv.org/abs/2208.05994v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.05994v3)
- **Published**: 2022-08-11 18:02:36+00:00
- **Updated**: 2022-10-01 09:21:26+00:00
- **Authors**: Alexey Korovin, Artem Vasilyev, Fedor Egorov, Dmitry Saykin, Evgeny Terukov, Igor Shakhray, Leonid Zhukov, Semen Budennyy
- **Comment**: None
- **Journal**: None
- **Summary**: Efficient defect detection in solar cell manufacturing is crucial for stable green energy technology manufacturing. This paper presents a deep-learning-based automatic detection model SeMaCNN for classification and semantic segmentation of electroluminescent images for solar cell quality evaluation and anomalies detection. The core of the model is an anomaly detection algorithm based on Mahalanobis distance that can be trained in a semi-supervised manner on imbalanced data with small number of digital electroluminescence images with relevant defects. This is particularly valuable for prompt model integration into the industrial landscape. The model has been trained with the on-plant collected dataset consisting of 68 748 electroluminescent images of heterojunction solar cells with a busbar grid. Our model achieves the accuracy of 92.5%, F1 score 95.8%, recall 94.8%, and precision 96.9% within the validation subset consisting of 1049 manually annotated images. The model was also tested on the open ELPV dataset and demonstrates stable performance with accuracy 94.6% and F1 score 91.1%. The SeMaCNN model demonstrates a good balance between its performance and computational costs, which make it applicable for integrating into quality control systems of solar cell manufacturing.



### Handling big tabular data of ICT supply chains: a multi-task, machine-interpretable approach
- **Arxiv ID**: http://arxiv.org/abs/2208.06031v1
- **DOI**: 10.1109/GLOBECOM48099.2022.10001253
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06031v1)
- **Published**: 2022-08-11 20:29:45+00:00
- **Updated**: 2022-08-11 20:29:45+00:00
- **Authors**: Bin Xiao, Murat Simsek, Burak Kantarci, Ala Abu Alkheir
- **Comment**: 6 pages, 7 tables, 4 figures, IEEE Global Communications Conference
  (Globecom), 2022
- **Journal**: None
- **Summary**: Due to the characteristics of Information and Communications Technology (ICT) products, the critical information of ICT devices is often summarized in big tabular data shared across supply chains. Therefore, it is critical to automatically interpret tabular structures with the surging amount of electronic assets. To transform the tabular data in electronic documents into a machine-interpretable format and provide layout and semantic information for information extraction and interpretation, we define a Table Structure Recognition (TSR) task and a Table Cell Type Classification (CTC) task. We use a graph to represent complex table structures for the TSR task. Meanwhile, table cells are categorized into three groups based on their functional roles for the CTC task, namely Header, Attribute, and Data. Subsequently, we propose a multi-task model to solve the defined two tasks simultaneously by using the text modal and image modal features. Our experimental results show that our proposed method can outperform state-of-the-art methods on ICDAR2013 and UNLV datasets.



### Shifted Windows Transformers for Medical Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/2208.06034v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2208.06034v1)
- **Published**: 2022-08-11 20:39:26+00:00
- **Updated**: 2022-08-11 20:39:26+00:00
- **Authors**: Caner Ozer, Arda Guler, Aysel Turkvatan Cansever, Deniz Alis, Ercan Karaarslan, Ilkay Oksuz
- **Comment**: 10 pages, 3 figures, 4 tables. Accepted in 13th Machine Learning in
  Medical Imaging (MLMI 2022) workshop
- **Journal**: None
- **Summary**: To maintain a standard in a medical imaging study, images should have necessary image quality for potential diagnostic use. Although CNN-based approaches are used to assess the image quality, their performance can still be improved in terms of accuracy. In this work, we approach this problem by using Swin Transformer, which improves the poor-quality image classification performance that causes the degradation in medical image quality. We test our approach on Foreign Object Classification problem on Chest X-Rays (Object-CXR) and Left Ventricular Outflow Tract Classification problem on Cardiac MRI with a four-chamber view (LVOT). While we obtain a classification accuracy of 87.1% and 95.48% on the Object-CXR and LVOT datasets, our experimental results suggest that the use of Swin Transformer improves the Object-CXR classification performance while obtaining a comparable performance for the LVOT dataset. To the best of our knowledge, our study is the first vision transformer application for medical image quality assessment.



### Region-Based Evidential Deep Learning to Quantify Uncertainty and Improve Robustness of Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2208.06038v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.06038v1)
- **Published**: 2022-08-11 21:04:15+00:00
- **Updated**: 2022-08-11 21:04:15+00:00
- **Authors**: Hao Li, Yang Nan, Javier Del Ser, Guang Yang
- **Comment**: 16 Pages, 3 figures, submitted to NCAA
- **Journal**: None
- **Summary**: Despite recent advances in the accuracy of brain tumor segmentation, the results still suffer from low reliability and robustness. Uncertainty estimation is an efficient solution to this problem, as it provides a measure of confidence in the segmentation results. The current uncertainty estimation methods based on quantile regression, Bayesian neural network, ensemble, and Monte Carlo dropout are limited by their high computational cost and inconsistency. In order to overcome these challenges, Evidential Deep Learning (EDL) was developed in recent work but primarily for natural image classification. In this paper, we proposed a region-based EDL segmentation framework that can generate reliable uncertainty maps and robust segmentation results. We used the Theory of Evidence to interpret the output of a neural network as evidence values gathered from input features. Following Subjective Logic, evidence was parameterized as a Dirichlet distribution, and predicted probabilities were treated as subjective opinions. To evaluate the performance of our model on segmentation and uncertainty estimation, we conducted quantitative and qualitative experiments on the BraTS 2020 dataset. The results demonstrated the top performance of the proposed method in quantifying segmentation uncertainty and robustly segmenting tumors. Furthermore, our proposed new framework maintained the advantages of low computational cost and easy implementation and showed the potential for clinical application.



### MILAN: Masked Image Pretraining on Language Assisted Representation
- **Arxiv ID**: http://arxiv.org/abs/2208.06049v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.06049v3)
- **Published**: 2022-08-11 21:58:36+00:00
- **Updated**: 2022-12-19 19:21:47+00:00
- **Authors**: Zejiang Hou, Fei Sun, Yen-Kuang Chen, Yuan Xie, Sun-Yuan Kung
- **Comment**: add new experiments and improved results. provide repo link
- **Journal**: None
- **Summary**: Self-attention based transformer models have been dominating many computer vision tasks in the past few years. Their superb model qualities heavily depend on the excessively large labeled image datasets. In order to reduce the reliance on large labeled datasets, reconstruction based masked autoencoders are gaining popularity, which learn high quality transferable representations from unlabeled images. For the same purpose, recent weakly supervised image pretraining methods explore language supervision from text captions accompanying the images. In this work, we propose masked image pretraining on language assisted representation, dubbed as MILAN. Instead of predicting raw pixels or low level features, our pretraining objective is to reconstruct the image features with substantial semantic signals that are obtained using caption supervision. Moreover, to accommodate our reconstruction target, we propose a more effective prompting decoder architecture and a semantic aware mask sampling mechanism, which further advance the transfer performance of the pretrained model. Experimental results demonstrate that MILAN delivers higher accuracy than the previous works. When the masked autoencoder is pretrained and finetuned on ImageNet-1K dataset with an input resolution of 224x224, MILAN achieves a top-1 accuracy of 85.4% on ViT-Base, surpassing previous state-of-the-arts by 1%. In the downstream semantic segmentation task, MILAN achieves 52.7 mIoU using ViT-Base on ADE20K dataset, outperforming previous masked pretraining results by 4 points.



### Optimizing Anchor-based Detectors for Autonomous Driving Scenes
- **Arxiv ID**: http://arxiv.org/abs/2208.06062v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2208.06062v1)
- **Published**: 2022-08-11 22:44:59+00:00
- **Updated**: 2022-08-11 22:44:59+00:00
- **Authors**: Xianzhi Du, Wei-Chih Hung, Tsung-Yi Lin
- **Comment**: None
- **Journal**: None
- **Summary**: This paper summarizes model improvements and inference-time optimizations for the popular anchor-based detectors in the scenes of autonomous driving. Based on the high-performing RCNN-RS and RetinaNet-RS detection frameworks designed for common detection scenes, we study a set of framework improvements to adapt the detectors to better detect small objects in crowd scenes. Then, we propose a model scaling strategy by scaling input resolution and model size to achieve a better speed-accuracy trade-off curve. We evaluate our family of models on the real-time 2D detection track of the Waymo Open Dataset (WOD). Within the 70 ms/frame latency constraint on a V100 GPU, our largest Cascade RCNN-RS model achieves 76.9% AP/L1 and 70.1% AP/L2, attaining the new state-of-the-art on WOD real-time 2D detection. Our fastest RetinaNet-RS model achieves 6.3 ms/frame while maintaining a reasonable detection precision at 50.7% AP/L1 and 42.9% AP/L2.



### ICIP 2022 Challenge on Parasitic Egg Detection and Classification in Microscopic Images: Dataset, Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/2208.06063v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2208.06063v2)
- **Published**: 2022-08-11 22:50:51+00:00
- **Updated**: 2022-10-17 21:55:42+00:00
- **Authors**: Nantheera Anantrasirichai, Thanarat H. Chalidabhongse, Duangdao Palasuwan, Korranat Naruenatthanaset, Thananop Kobchaisawat, Nuntiporn Nunthanasup, Kanyarat Boonpeng, Xudong Ma, Alin Achim
- **Comment**: The 29th IEEE International Conference on Image Processing
- **Journal**: None
- **Summary**: Manual examination of faecal smear samples to identify the existence of parasitic eggs is very time-consuming and can only be done by specialists. Therefore, an automated system is required to tackle this problem since it can relate to serious intestinal parasitic infections. This paper reviews the ICIP 2022 Challenge on parasitic egg detection and classification in microscopic images. We describe a new dataset for this application, which is the largest dataset of its kind. The methods used by participants in the challenge are summarised and discussed along with their results.



### Deep is a Luxury We Don't Have
- **Arxiv ID**: http://arxiv.org/abs/2208.06066v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2208.06066v1)
- **Published**: 2022-08-11 23:43:52+00:00
- **Updated**: 2022-08-11 23:43:52+00:00
- **Authors**: Ahmed Taha, Yen Nhi Truong Vu, Brent Mombourquette, Thomas Paul Matthews, Jason Su, Sadanand Singh
- **Comment**: MICCAI 2022 + Extra Experiments
- **Journal**: None
- **Summary**: Medical images come in high resolutions. A high resolution is vital for finding malignant tissues at an early stage. Yet, this resolution presents a challenge in terms of modeling long range dependencies. Shallow transformers eliminate this problem, but they suffer from quadratic complexity. In this paper, we tackle this complexity by leveraging a linear self-attention approximation. Through this approximation, we propose an efficient vision model called HCT that stands for High resolution Convolutional Transformer. HCT brings transformers' merits to high resolution images at a significantly lower cost. We evaluate HCT using a high resolution mammography dataset. HCT is significantly superior to its CNN counterpart. Furthermore, we demonstrate HCT's fitness for medical images by evaluating its effective receptive field.Code available at https://bit.ly/3ykBhhf



